{"id": "1312.1146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2013", "title": "Case-Based Merging Techniques in OAKPLAN", "abstract": "belta Case - based planning can vagabond take emilion advantage goddaughter of monocotyledons former problem - solving experiences by po\u0142aniec storing in huden a unobjective plan jepson library previously garage-rock generated strategoi plans that can reconfiguring be reused imabari to solve seabrook similar planning problems nutfield in the future. Although 1.9-meter comparative worst - case d\u2019ivoire complexity ognyan analyses 61.57 of plan generation and reuse techniques corpore reveal jaca that franco-belgian it is somprasong not possible to achieve sal provable calle efficiency kary gain of eternals reuse tawke over generation, we show that glaucidium the sports-oriented case - guohong based +2 planning unscrewing approach gilo can usasa be brennen an 10.53 effective kyowa alternative to plan startling generation patriarca when similar chico reuse candidates inhabitation can 65-58 be campbellsville chosen.", "histories": [["v1", "Wed, 4 Dec 2013 13:10:45 GMT  (342kb,D)", "http://arxiv.org/abs/1312.1146v1", "preliminary version"]], "COMMENTS": "preliminary version", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["anna roub\\'i\\v{c}kov\\'a", "ivan serina"], "accepted": false, "id": "1312.1146"}, "pdf": {"name": "1312.1146.pdf", "metadata": {"source": "CRF", "title": "Case-Based Merging Techniques in OAKPLAN", "authors": [], "emails": [], "sections": [{"heading": null, "text": "Keywords: Case-Based Planning, Domain-Independent Planning, Merging Techniques, Case-Based Reasoning, Heuristic Search for Planning, Kernel Functions.\nCC\u00a9 BY:\u00a9 $\\\u00a9 =\u00a9\nar X\niv :1\n31 2.\n11 46\nv1 [\ncs .A\nI] 4\nD ec\n2 01\n3\nContents"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Preliminaries 3", "text": "2.1 Planning formalism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Kernel Functions for Labeled Graphs . . . . . . . . . . . . . . . . . . . . . . . . 8"}, {"heading": "3 Case-Based Planning and OAKPLAN 11", "text": "3.1 Plan Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.1.1 Object Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Plan Evaluation Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.2.1 Application of plan merging techniques . . . . . . . . . . . . . . . . . . 20 3.3 Plan Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.4 Plan Revision & Case Base Update . . . . . . . . . . . . . . . . . . . . . . . . . 23"}, {"heading": "4 Experimental Results 25", "text": "5 Related Work 28"}, {"heading": "1 Introduction", "text": "This report describes a case-based planning system called OAKPLAN, with a special emphasis on newly implemented merging techniques. This extension was motivated by the observation that, especially in some domains, the performance of the system can be greatly improved by remembering some elementary solutions for a simple problems, which can be combined to address significantly more complex scenarios."}, {"heading": "2 Preliminaries", "text": "In the following we present some notations that will be used in the paper with an analysis of the computational complexity of the problems considered."}, {"heading": "2.1 Planning formalism", "text": "Similarly to Bylander\u2019s work [9], we define an instance of propositional planning problem as:\nDefinition 1 An instance of propositional STRIPS planning problem is a tuple \u03a0 = \u27e8Pr,I,G,Op\u27e9 where:\n\u2022 Pr is a finite set of ground atomic propositional formulae;\n\u2022 I \u2286 Pr is the initial state;\n\u2022 G \u2286 Pr is the goal specification;\n\u2022 Op is a finite set of operators, where each operator o \u2208 Op has the form op \u21d2 o+, o\u2212 such that\n\u2013 op \u2286 Pr are the propositional preconditions, \u2013 o+ \u2286 Pr are the positive postconditions (add list), \u2013 o\u2212 \u2286 Pr are the negative postconditions (delete list)\nand o+ \u2229 o\u2212 = \u2205.\nWe assume that the set of propositions Pr has a particular structure. Let O be a set of typed constants ci, with the understanding that distinct constants denote distinct objects (corresponding to individual entities following the Conceptual Graphs notation [12]). Let P be a set of predicate symbols, then Pr(O,P) is the set of all ground atomic formulae over this signature. Note that we use a many-sorted logic formalisation since it can significantly increase the efficiency of a deductive inference system by eliminating useless branches of the search space of a domain [13, 14, 61]. A fact is an assertion that some individual entities exist and that these entities are related by some relationships.\nA plan \u03c0 is a partially ordered sequence of actions \u03c0 = (a1, . . . , am,C), where ai is an action (completely instantiated operator) of \u03c0 and C defines the ordering relations between the actions of \u03c0. A linearisation of a partially ordered plan is a total order over the actions of the plan that is consistent with the existing partial order. In a totally ordered plan \u03c0 = (a\u20321, . . . , a\u2032m), a precondition f of a plan action a\u2032i is supported if (i) f is added by an earlier action a \u2032 j and not deleted by an intervening action a\u2032k with j < k < i or (ii) f is true in the initial state and /\u2203 a\u2032k\nwith k < i s.t. f \u2208 del(a\u2032k). In a partially ordered plan, a precondition of an action is possibly supported if there exists a linearisation in which it is supported, while an action precondition is necessarily supported if it is supported in all linearisations. An action precondition is necessarily unsupported if it is not possibly supported. A valid plan is a plan in which all action preconditions are necessarily supported.\nThe complexity of STRIPS planning problems has been studied extensively in the literature. Bylander [9] has defined PLANSAT as the decision problem of determining whether an instance \u03a0 of propositional STRIPS planning has a solution or not. PLANMIN is defined as the problem of determining if there exists a solution of length k or less, i.e., it is the decision problem corresponding to the problem of generating plans with minimal length. Based on this framework, he has analysed the computational complexity of a general propositional planning problem and a number of generalisations and restricted problems. In its most general form, both PLANSAT and PLANMIN are PSPACE-complete. Severe restrictions on the form of the operators are necessary to guarantee polynomial time or even NP-completeness [9].\nTo address the high complexity of the planning problems, different heuristical approaches arise. The case based approach relies on encountering problems similar to each other and tries to reuse previously found plans to solve new problems. If successful, this can save a considerable amount of resources. Clearly, to apply such a technique, we need similar problems to have similar solution \u2014 in other words, we need the world to be regular. When solving a new problem, the planner searches previously solved problems and retrieves the most suitable one which is then adapted to solve the current problem.\nIn general, solved problems are stored in a case base, which is a collection of cases; a case is a pair consisting of a problem description and its solution. Following the formalisation of Liberatore [37], we define a case base as follows:\nDefinition 2 A case base, or a plan library, is a set {\u27e8\u03a0i, \u03c0i\u27e9,1 \u2264 i \u2264 n} where each \u27e8\u03a0i, \u03c0i\u27e9 is a planning case with \u03a0i being an instance of propositional STRIPS planning problem and \u03c0i a solution to \u03a0i.\nNote that different planners implement cases differently \u2014 several solution plans may be stored instead of just one, justifications may be added, some planners even avoid storing a plan as a set of actions and store its derivational trace instead.\nIn order to realise the benefits of remembering and reusing past solutions, a case-based system needs to efficiently implement several procedures, such as those for retrieving analogous cases (Retrieve), adapting them to suit the new problem (Reuse, Revise), and building and maintaining a case base of sufficient size and coverage to yield useful analogues (Retain). When a case-based system faces a new problem, it performs these procedures in a sequence that starts by querying the case base and ends by (possibly) updating it (Fig. 1).\nRegardless of the underlying formalisation, there are two main approaches to case-based planning, whose fundamental difference is in the way they adapt the cases to solve the current problem. A conservative or a transformational plan adaptation inisists on reusing as much of the starting plan \u03c0i as possible. It turns out that such adaptation may be very expensive. Moreover, the quality of the solution strongly depends on the correspondence between the retrieved case and the current problem, which is influenced by the way the case is retrieved from the case base as well as by the case base itself, or rather by its competence. Intuitively, a competence is a feature of the case base referring to the variety of problems for which the case\nbase contains a reasonably good case to adapt to a solution; the more problems the case base can address, the higher competence it has. The generative approach, on the contrary, treats the case as a hint that can even be fully ignored and the solution can be generated from scratch. Hence the competence of the case base is not so crucial for the quality of the solution plan. On the other hand, such process may be worse than traditional plan-generation in terms of complexity, as it first needs to search the case base to possibly ignore what has been found and to generate a whole new plan. This shows that the competence of the case base is also important here, though it influences the complexity of finding the solution rather than its quality.\nIt is important to remark that our approach is more related to the generative case-based approach than to the transformational, in that the plan library is only assumed to be used when it is useful in the process of searching for a new plan, and is not necessarily used to provide a starting point of the search process. If a planning case \u27e8\u03a0\u2217, \u03c0\u2217\u27e9 is also known, the current planning problem \u03a0 cannot become more difficult, as we can simply disregard the case and find the plan using \u03a0 only.\nEssential to this trivial result is that, similarly to most modern plan adaptation and casebased planning approaches [3, 24, 56, 57], we do not enforce plan adaptation to be conservative, in the sense that we do not require to reuse as much of the starting plan \u03c0\u2217 to solve the new plan. The computational complexity of plan Reuse and Modification for STRIPS planning problems has been analysed in a number of papers [9, 10, 35, 37, 44].\nMoreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient than plan generation in the average case [8, 18, 23, 24, 44, 54, 57].\nIt is crucial that the system has at its disposal a competent case base. Intuitivelly, the competence of the case base measures how often the case base provides a good reuse candidate. Of course, the number of successful reuses also depends on other components of ths system (how exactly is the case base queried, what is the reuse strategy, when do we consider the solution a \u201csuccessful reuse\u201d, etc.).\nThe competence of the case base grows as we keep adding solutions to new diverse problems, but only to a certain point when the query over the case base takes too much time to be answered. If the system works in the environment where the problems tend to be very complex, it is unfortunately very unlikely that the case base could contain sufficient number of cases to cover majority or at least a significant part of the kinds of problems to be solved. One of the promising directions for such scenarios is to attempt to build a case base of \u201celementary\u201d cases which address only simple sub-problems and which are combined together to address also complex problems. The number of such elementary problems is lower and hence it is easier to achieve a competent case base.\nClearly such modification needs to be accompanied by the change of the retrieval policy \u2014 we no longer look for the most similar case, but for a suitable set of partial solutions. The partial solutions are then put together; depending on the interaction between the subgoals a simple concatenation may be sufficient, but in case the subgoals interfere the concatenation may be very inefficient (e.g. in the logistics domain)."}, {"heading": "2.2 Graphs", "text": "Graphs provide a rich means for modelling structured objects and they are widely used in reallife applications to represent molecules, images, or networks. On a very basic level, a graph can be defined by a set of entities and a set of connections between these entities. Due to their universal definition, graphs have found widespread application for many years as tools for the representation of complex relations. Furthermore, it is often useful to compare objects represented as graphs to determine the degree of similarity between the objects. More formally:\nDefinition 3 A labeled graph G is a 3-tuple G = (V,E,\u03bb), in which\n\u2022 V is the set of vertices,\n\u2022 E \u2286 V \u00d7 V is the set of directed edges or arcs,\n\u2022 \u03bb \u2236 V \u222aE \u2192 \u2118s(L\u03bb) is a function assigning labels to vertices and edges;\nwhere L\u03bb is a finite set of symbolic labels and \u2118s(L\u03bb) represents the set of all the multisets on L\u03bb. Note that our label function considers multisets of symbolic labels, with the corresponding operations of union, intersection and join [6], since in our context they are more suitable than standard sets of symbolic labels in order to compare vertices or edges accurately as described later. The above definition corresponds to the case of directed graphs; undirected graphs are obtained if we require for each edge [v1, v2] \u2208 E the existence of an edge [v2, v1] \u2208 E with the same label. \u2223G\u2223 = \u2223V \u2223 + \u2223E\u2223 denotes the size of the graph G, while an empty graph such that \u2223G\u2223 = 0 will be denoted by \u2205. An arc e = [v, u] \u2208 E is considered to be directed from v to u; v is called the source node and u is called the target node of the arc; u is said to be a direct successor of v, v is said to be a direct predecessor of u, while v is said to be adjacent to the vertex u and vice versa.\nHere we present the notion of graph union which is essential for the definition of the graphs used by our matching functions:\nDefinition 4 The union of two graphs G1 = (V1,E1, \u03bb1) and G2 = (V2,E2, \u03bb2), denoted by G1 \u222aG2, is the graph G = (V,E,\u03bb) defined by\n\u2022 V = V1 \u222a V2,\n\u2022 E = E1 \u222aE2,\n\u2022 \u03bb(x) = \u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23a9 \u03bb1(x) if x \u2208 (V1/V2) \u2228 x \u2208 (E1/E2) \u03bb2(x) if x \u2208 (V2/V1) \u2228 x \u2208 (E2/E1) \u03bb1(x) \u228e \u03bb2(x) otherwise\nwhere \u228e indicates the join, sometimes called sum, of two multisets [6], while \u03bb(\u22c5) associates a multiset of symbolic labels to a vertex or to an edge.\nIn many applications it is necessary to compare objects represented as graphs and determine the similarity among these objects. This is often accomplished by using graph matching, or isomorphism techniques. Graph isomorphism can be formulated as the problem of identifying a one-to-one correspondence between the vertices of two graphs such that an edge only exists between two vertices in one graph if an edge exists between the two corresponding vertices in the other graph. Graph matching can be formulated as the problem involving the maximum common subgraph (MCS) between the collection of graphs being considered. This is often referred to as the maximum common substructure problem and denotes the largest substructure common to the collection of graphs under consideration. More precisely:\nDefinition 5 Two labeled graphs G = (V,E,\u03bb) and G\u2032 = (V \u2032,E\u2032, \u03bb\u2032) are isomorphic if there exists a bijective function f \u2236 V \u2192 V \u2032 such that\n\u2022 \u2200v \u2208 V,\u03bb(v) = \u03bb\u2032(f(v)),\n\u2022 \u2200[v1, v2] \u2208 E,\u03bb([v1, v2]) = \u03bb\u2032([f(v1), f(v2)]),\n\u2022 [u, v] \u2208 E if and only if [f(u), f(v)] \u2208 E\u2032.\nWe shall say that f is an isomorphism function.\nDefinition 6 An Induced Subgraph of a graph G = (V,E,\u03bb) is a graph S = (V \u2032,E\u2032, \u03bb\u2032) such that\n\u2022 V \u2032 \u2286 V and \u2200v \u2208 V \u2032, \u03bb\u2032(v) \u2286 \u03bb(v),\n\u2022 E\u2032 \u2286 E and \u2200e \u2208 E\u2032, \u03bb\u2032(e) \u2286 \u03bb(e)\n\u2022 \u2200v, u \u2208 V \u2032, [v, u] \u2208 E\u2032 if and only if [v, u] \u2208 E\nA graph G is a Common Induced Subgraph (CIS) of graphs G1 and G2 if G is isomorphic to induced subgraphs of G1 and G2. A common induced subgraph G = (V,E,\u03bb) of G1 and G2 is called Maximum Common Induced Subgraph (MCIS) if there exists no other common induced subgraph ofG1 andG2 with\u2211v\u2208V \u2223\u03bb(v)\u2223 greater thanG. Similarly, a common induced subgraph G = (V,E,\u03bb) of G1 and G2 is called Maximum Common Edge Subgraph (MCES), if there exists no other common induced subgraph of G1 and G2 with \u2211e\u2208E \u2223\u03bb(e)\u2223 greater than G. Note that, since we are considering multiset labeled graphs, we require a stronger condition than standard MCIS and MCES for labeled graph, in fact we want to maximise the\ntotal cardinality of the multiset labels of vertices/edges involved instead of the simple number of vertices/edges.\nAs it is well known, subgraph isomorphism and MCS between two or among more graphs are NP-complete problems [19], while it is still an open question if also graph isomorphism is an NP-complete problem. As a consequence, worst-case time requirements of matching algorithms increase exponentially with the size of the input graphs, restricting the applicability of many graph based techniques to very small graphs."}, {"heading": "2.3 Kernel Functions for Labeled Graphs", "text": "In recent years, a large number of graph matching methods based on different matching paradigms have been proposed, ranging from the spectral decomposition of graph matrices to the training of artificial neural networks and from continuous optimisation algorithms to optimal tree search procedures.\nThe basic limitation of graph matching is due to the lack of any mathematical structure in the space of graphs. Kernel machines, a new class of algorithms for pattern analysis and classification, offer an elegant solution to this problem [48]. The basic idea of kernel machines is to address a pattern recognition problem in a related vector space instead of the original pattern space. That is, rather than defining mathematical operations in the space of graphs, all graphs are mapped into a vector space where these operations are readily available. Obviously, the difficulty is to find a mapping that preserves the structural similarity of graphs, at least to a certain extent. In other words, if two graphs are structurally similar, the two vectors representing these graphs should be similar as well, since the objective is to obtain a vector space embedding that preserves the characteristics of the original space of graphs.\nA key result from the theory underlying kernel machines states that an explicit mapping from the pattern space into a vector space is not required. Instead, from the definition of a kernel function it follows that there exists such a vector space embedding and that the kernel function can be used to extract the information from vectors that is relevant for recognition. As a matter of fact, the family of kernel machines consists of all the algorithms that can be formulated in terms of such a kernel function, including standard methods for pattern analysis and classification such as principal component analysis and nearest-neighbour classification. Hence, from the definition of a graph similarity measure, we obtain an implicit embedding of the entire space of graphs into a vector space.\nA kernel function can be thought of as a special similarity measure with well defined mathematical properties [48]. Considering the graph formalism, it is possible to define a kernel function which measures the degree of similarity between two graphs. Each structure could be represented by means of its similarity to all the other structures in the graph space. Moreover a kernel function implicitly defines a dot product in some space [48]; i.e., by defining a kernel function between two graphs we implicitly define a vector representation of them without the need to explicitly know about it.\nFrom a technical point of view a kernel function is a special similarity measure k \u2236 X \u00d7X \u2192 IR between patterns lying in some arbitrary domainX , which represents a dot product, denoted by \u27e8\u22c5, \u22c5\u27e9, in some Hilbert space H [48]; thus, for two arbitrary patterns x,x\u2032 \u2208 X it holds that k(x,x\u2032) = \u27e8\u03c6(x), \u03c6(x\u2032)\u27e9, where \u03c6 \u2236 X \u2192 H is an arbitrary mapping of patterns from the domain X into the feature space H. In principle the patterns in domain X do not necessarily have to be vectors; they could be strings, graphs, trees, text documents or other objects. The vector representation of these objects is then given by the map \u03c6. Instead of performing the expensive transformation step explicitly, the kernel can be calculated directly, thus performing\nthe feature transformation only implicitly: this is known as kernel trick. This means that any set, whether a linear space or not, that admits a positive definite kernel can be embedded into a linear space.\nMore specifically, kernel methods manage non-linear complex tasks making use of linear methods in a new space. For instance, take into consideration a classification problem with a training set S = {(u1, y1), . . . , (un, yn)}, (ui, yi) \u2208 X \u00d7 Y , for i = 1, . . . , n, where X is an inner-product space (i.e. IRd) and Y = {\u22121,+1}. In this case, the learning phase corresponds to building a function f \u2208 Y X from the training set S by associating a class y \u2208 Y to a pattern u \u2208 X so that the generalisation error of f is as low as possible.\nA functional form for f consists in the hyperplane f(u) = sign(\u27e8w,u\u27e9+ b), where sign(\u22c5) refers to the function returning the sign of its argument. The decision function f produces a prediction that depends on which side of the hyperplane \u27e8w,u\u27e9 + b = 0 the input pattern u lies. The individuation of the best hyperplane corresponds to a convex quadratic optimisation problem in which the solution vector w is a linear combination of the training vectors:\nw = \u2211ni=1 \u03b1iyiui, for some \u03b1i \u2208 IR+, i = 1, . . . , n. In this way the linear classifier f may be rewritten as\nf(u) = sign( n\n\u2211 i=1 \u03b1iyi\u27e8ui, u\u27e9 + b)\nAs regards complex classification problems, the set of all possible linear decision surfaces might not be rich enough in order to provide a good classification, independently from the values of the parameters w \u2208 X and b \u2208 IR (see Figure 2). The aim of the kernel trick is that of overcoming this limitation by adopting a linear approach to transformed data \u03c6(u1), . . . , \u03c6(un) rather than raw data. Here \u03c6 indicates an embedding function from the input space X to a feature space H, provided with a dot product. This transformation enables us to give an alternative kernel representation of the data which is equivalent to a mapping into a highdimensional space where the two classes of data are more readily separable. The mapping is achieved through a replacement of the inner product:\n\u27e8ui, u\u27e9\u2192 \u27e8\u03c6(ui), \u03c6(u)\u27e9\nand the separating function can be rewritten as:\nf(u) = sign( n\n\u2211 i=1 \u03b1iyi\u27e8\u03c6(ui), \u03c6(u)\u27e9 + b) (1)\nThe main idea behind the kernel approach consists in replacing the dot product in the feature space using a kernel k(u, v) = \u27e8\u03c6(v), \u03c6(u)\u27e9; the functional form of the mapping \u03c6(\u22c5) does not actually need to be known since it is implicitly defined by the choice of the kernel. A positive definite kernel [20] is:\nDefinition 7 Let X be a set. A symmetric function k \u2236 X \u00d7X \u2192 IR is a positive definite kernel function on X iff \u2200n \u2208 IN , \u2200x1, . . . , xn \u2208 X , and \u2200c1, . . . , cn \u2208 IR\n\u2211 i,j\u2208{1,...,n} cicjk(xi, xj) \u2265 0\nwhere IN is the set of positive integers. For a given set Su = {u1, . . . , un}, the matrix K = (k(ui, uj))i,j is known as Gram matrix of k with respect to Su. Positive definite kernels are also called Mercer kernels.\nTheorem 1 ( Mercer\u2019s property [41]) For any positive definite kernel function k \u2208 IRX\u00d7X , there exists a mapping \u03c6 \u2208 HX into the feature space H equipped with the inner product \u27e8\u22c5, \u22c5\u27e9H, such that:\n\u2200u, v \u2208 X , k(u, v) = \u27e8\u03c6(u), \u03c6(v)\u27e9H\nThe kernel approach replaces all inner products in Equation 1 and all related expressions to compute the real coefficients \u03b1i and b, by means of a Mercer kernel k. For any input pattern u, the relating decision function f is given by:\nf(u) = sign( n\n\u2211 i=1 \u03b1iyik(ui, u) + b) (2)\nThis approach transforms the input patterns u1, . . . , un into the corresponding vectors \u03c6(u1), . . . , \u03c6(un) \u2208 H through the mapping \u03c6 \u2208 HX (cf. Mercer\u2019s property, Theorem 1), and uses hyperplanes in the feature space H for the purpose of classification (see Figure 2). The dot product \u27e8u, v\u27e9 = \u2211di=1 uivi of IRd is actually a Mercer kernel, while other commonly used Mercer kernels, like polynomial and Gaussian kernels, generally correspond to nonlinear mappings \u03c6 into high-dimensional feature spaces H. On the other hand the Gram matrix implicitly defines the geometry of the embedding space and permits the use of linear techniques in the feature space so as to derive complex decision surfaces in the input space X .\nWhile it is not always easy to prove positive definiteness for a given kernel, positive definite kernels are characterised by interesting closure properties. More precisely, they are closed under sum, direct sum, multiplication by a scalar, tensor product, zero extension, pointwise limits, and exponentiation [48]. Well-known examples of kernel functions are: \u25cf Radial Basis Functions kRBF (x,x\u2032) = exp (\u2212\u2223\u2223x\u2212x \u2032\u2223\u22232 2\u03c32\n); \u25cf Homogenous polynomial kernels kpoly(x,x\u2032) = \u27e8x,x\u2032\u27e9d (d \u2208 IN ); \u25cf Sigmoidal kernels kSig(x,x\u2032) = tanh (\u03ba(x \u22c5 x\u2032) + \u03b8) \u25cf Inv. multiquadratic kernels kinv(x,x\u2032) = 1\u221a\u2223\u2223x\u2212x\u2032\u2223\u22232+c2\nA remarkable contribution to graph kernels is the work on convolution kernels, that provides a general framework to deal with complex objects consisting of simpler parts [30]. Convolution kernels derive the similarity of complex objects from the similarity of their parts. Given two kernels k1 and k2 over the same set of objects, new kernels may be built by using operations such as convex linear combinations and convolutions. The convolution of k1 and k2 is a new kernel k with the form\nk1 \u22c6 k2(u, v) = \u2211 {u1,u2}=u;{v1,v2}=v k1(u1, v1)k2(u2, v2)\nwhere u = {u1, u2} refers to a partition of u into two substructures u1 and u2 [30, 48]. The kind of substructures depends on the domain of course and could be, for instance, subgraphs or subsets or substrings in the case of kernels defined over graphs, sets or strings, respectively. Different kernels can be obtained by considering different classes of subgraphs (e.g. directed/undirected, labeled/unlabeled, paths/trees/cycles, deterministic/random walks) and various ways of listing and counting them [34, 45, 46]. The consideration of space and time complexity so as to compute convolution/spectral kernels is important, owing to the combinatorial explosion linked to variable-size substructures.\nIn the following section we present our Optimal Assignment Kernel as a symmetric and positive definite similarity measure for directed graph structures and it will be used in order to define the correspondence between the vertices of two directed graphs. For an introduction to kernel functions related concepts and notation, the reader is referred to Scholkopf and Smola\u2019s book [48]."}, {"heading": "3 Case-Based Planning and OAKPLAN", "text": "Here we provide a detailed description of the case-based approach to planning and its implementation in the state-of-the-art CBP system OAKPLAN.\nA case-based planning system solves planning problems by making use of stored plans that were used to solve analogous problems. CBP is a type of case-based reasoning, which involves the use of stored experiences (cases); moreover there is strong evidence that people frequently employ this kind of analogical reasoning [21, 47, 60]. When a CBP system solves a new planning problem, the new plan is added to its case base for potential reuse in the future. Thus we can say that the system learns from experience.\nIn general the following steps are executed when a new planning problem must be solved by a CBP system:\n1. Plan Retrieval to retrieve cases from memory that are analogous to the current (target) problem (see section 3.1 for a description of our approach).\n2. Plan Evaluation to evaluate the new plans by execution, simulated execution, or analysis and choose one of them (see section 3.2).\n3. Plan Adaptation to repair any faults found in the new plan (see section 3.3).\n4. Plan Revision to test the solution new plan \u03c0 for success and repair it if a failure occurs during execution (see section 3.4).\n5. Plan Storage to eventually store \u03c0 as a new case in the case base (see section 3.4).\nIn order to realise the benefits of remembering and reusing past plans, a CBP system needs efficient methods for retrieving analogous cases and for adapting retrieved plans together with a case base of sufficient size and coverage to yield useful analogues. The ability of the system to search in the library for a plan suitable to adaptation1 depends both on the efficiency/accuracy of the implemented retrieval algorithm and on the data structures used to represent the elements of the case base.\nA planning case of the case base corresponds to a planning problem \u03a0 (defined by an initial state I , a goal state G and a set of operators O) a solution \u03c0 of \u03a0 and additional data structures derived by \u03a0 and stored in the case base so as to avoid their recomputation. The case base competence should increase progressively during the use of the case base system itself, every time solution plans enhancing the competence of the case base are produced.\nThe possibility of solving a large number of problems depends both on the size and on the competence of the library with respect to the agent activity. Furthermore this competence could be increased during the agent activity, in fact the solution plans of new problems could be added to the library.\nSimilarly to the Aamodt & Plaza\u2019s classic model of the problem solving cycle in CBR [1], Figure 3 shows the main steps of our case-based planning cycle and the interactions of the different steps with the case base. In the following we illustrate the main steps of our case-based planning approach, examining the different implementation choices adopted."}, {"heading": "3.1 Plan Retrieval", "text": "Although the plan adaptation phase is the central component of a CBP system, the retrieval phase critically affects the system performance too. As a matter of fact the retrieval time is a component of the total adaptation time and the quality of the retrieved plan is fundamental for the performance of the successive adaptation phase. With OAKPLAN a number of functions for the management of the plan library and matching functions for the selection of the candidate plan for adaptation have been implemented.\n1A plan suitable to adaptation has an adaptation cost that is lower with respect to the other candidates of the case base and with respect to plan generation.\nThe retrieval phase has to consider all the elements of the plan library in order to choose a good one that will allow the system to solve the new problem easily. Hence it is necessary to design a similarity metric and reduce the number of cases that must be evaluated accurately so as to improve the efficiency of the retrieval phase. Anyway the efficiency of a plan adaptation system is undoubtedly linked to the distance between the problem to solve and the plan to adapt. In order to find a plan which is useful for adaptation we have to reach the following objectives:\n\u2022 The retrieval phase must identify the candidates for adaptation. The retrieval time should be as small as possible as it will be added to the adaptation time and so particular attention has been given to the creation of efficient data structures for this phase.\n\u2022 The selected cases should actually contain the plans that are easier to adapt; since we assume that the world is regular, i.e. that similar problems have similar solutions, we look for the cases that are the most similar to the problem to solve (with respect to all the other candidates of the case base). In this sense, it is important to define a metric able to give an accurate measure of the similarity between the planning problem to solve and the cases of the plan library.\nTo the end of applying the reuse technique, it is necessary to provide a plan library from which \u201csufficiently similar\u201d reuse candidates can be chosen. In this case, \u201csufficiently similar\u201d means that reuse candidates have a large number of initial and goal facts in common with the new instance. However, one may also want to consider the reuse candidates that are similar to the new instance after the objects of the selected candidates have been systematically renamed. As a matter of fact, every plan reuse system should contain a matching component that tries to find a mapping between the objects of the reuse candidate and the objects of the new instance such that the number of common goal facts is maximised and the additional planning effort to achieve the initial state of the plan library is minimised. Following Nebel & Koehler\u2019s formalisation [44], we will have a closer look at this matching problem."}, {"heading": "3.1.1 Object Matching", "text": "As previously said we use a many-sorted logic in order to reduce the search space for the matching process; moreover we assume that the operators are ordinary STRIPS operators using variables, i.e. we require that if there exists an operator ok mentioning the typed constants {c1 \u2236 t1, ..., cn \u2236 tn} \u2286 O, then there also exists an operator ol over the arbitrary set of typed constants {d1 \u2236 t1, ..., dn \u2236 tn} \u2286 O such that ol becomes identical to ok if the di\u2019s are replaced by ci\u2019s. If there are two instances\n\u03a0\u2032 = \u27e8Pr(O\u2032,P\u2032),I \u2032,G\u2032,Op\u2032 \u27e9\n\u03a0 = \u27e8Pr(O,P),I,G,Op\u27e9\nsuch that (without loss of generality) O\u2032 \u2286O\nP\u2032 = P\nOp\u2032 \u2286 Op\nthen a mapping, or matching function, from \u03a0\u2032 to \u03a0 is a function\n\u00b5 \u2236O\u2032 \u2192O\nThe mapping is extended to ground atomic formulae and sets of such formulae in the canonical way, i.e.,\n\u00b5(p(c1 \u2236 t1, ..., cn \u2236 tn)) = p(\u00b5(c1) \u2236 t1, ..., \u00b5(cn) \u2236 tn)\n\u00b5({p1(..), ..., pm(..)}) = {\u00b5(p1(..)), ..., \u00b5(pm(..))} If there exists a bijective matching function \u00b5 from \u03a0\u2032 to \u03a0 such that \u00b5(G\u2032) = G and \u00b5(I \u2032) = I , then it is obvious that a solution plan \u03c0\u2032 for \u03a0\u2032 can be directly reused for solving \u03a0 since \u03a0\u2032 and \u03a0 are identical within a renaming of constant symbols, i.e., \u00b5(\u03c0\u2032) solves \u03a0. Even if \u00b5 does not match all goal and initial-state facts, \u00b5(\u03c0\u2032) can still be used as a starting point for the adaptation process that can solve \u03a0.\nIn order to measure the similarity between two objects, it is intuitive and usual to compare the features which are common to both objects [39]. The Jaccard similarity coefficient used in information retrieval is particularly interesting. Here we examine an extended version that considers two pairs of disjoint sets:\ncomplete_simil\u00b5(\u03a0\u2032,\u03a0) = \u2223\u00b5(G\u2032) \u2229 G\u2223 + \u2223\u00b5(I \u2032) \u2229 I \u2223 \u2223\u00b5(G\u2032) \u222a G\u2223 + \u2223\u00b5(I \u2032) \u222a I \u2223 (3)\nIn the following we present a variant of the previous function so as to overcome the problems related to the presence of irrelevant facts in the initial state description of the current planning problem \u03a0 and additional goals that are present in \u03a0\u2032. In fact while the irrelevant facts can be filtered out from the initial state description of the case-based planning problem \u03a0\u2032 using the corresponding solution plan \u03c0\u2032, this is not possible for the initial state description of the current planning problem \u03a0. Similarly, we do not want to consider possible \u201cirrelevant\u201d additional goals of G\u2032; this could happen when \u03a0\u2032 solves a more difficult planning problem with respect to \u03a0. We define the following similarity function so as to address these issues:\nsimil\u00b5(\u03a0\u2032,\u03a0) = \u2223\u00b5(G\u2032) \u2229 G\u2223 + \u2223\u00b5(I \u2032) \u2229 I \u2223\n\u2223G\u2223 + \u2223\u00b5(I \u2032)\u2223 . (4)\nUsing simil\u00b5 we obtain a value equal to 1 when there exists a mapping \u00b5 s.t. \u2200f \u2208 I \u2032, \u00b5(f) \u2208 I (to guarantee the applicability of \u03c0\u2032) and \u2200g \u2208 G, \u2203g\u2032 \u2208 G\u2032 s.t. g = \u00b5(g\u2032) (to guarantee the achievement of the goals of the current planning problem). Note that these similarity functions are not metric functions, although we could define a distance function in terms of the similarity as Dist\u00b5(\u03a0\u2032,\u03a0) = 1 \u2212 simil\u00b5(\u03a0\u2032,\u03a0). and it easy to show that this distance function is indeed a metric.\nFinally we define the following optimisation problem, which we call obj_match: Instance: Two planning instances, \u03a0\u2032 and \u03a0, and a real number k \u2208 [0,1]. Question: Does a mapping \u00b5 from \u03a0\u2032 to \u03a0 such that simil\u00b5(\u03a0\u2032,\u03a0) = k exist and there\nis no mapping \u00b5\u2032 from \u03a0\u2032 to \u03a0 with simil\u00b5\u2032(\u03a0\u2032,\u03a0) > k?\nIt should be noted that this matching problem has to be solved for each potentially relevant candidate in the plan library to select the corresponding best reuse candidate. Of course, one may use structuring and indexing techniques to avoid considering all plans in the library. Nevertheless, it seems unavoidable solving this problem a considerable number of times before\nan appropriate reuse candidate is identified. For this reason, the efficiency of the matching component is crucial for the overall system performance. Unfortunately, similarly to Nebel & Koehler\u2019s analysis [44], it is quite easy to show that this matching problem is an NP-hard problem.\nTheorem 2 obj_match is NP-hard.\nThe proof of this theorem and of the following ones can be found in Appendix ??. This NPhardness result implies that matching may be indeed a bottleneck for plan reuse systems. As a matter of fact, it seems to be the case that planning instances with complex goal or initial-state descriptions may not benefit from plan-reuse techniques because matching and retrieval are too expensive. In fact existing similarity metrics address the problem heuristically, considering approximations of it [43, 58]. However, this theorem is interesting because it captures the limit case for such approximations.\nPlanning Encoding Graph. We define a particular labeled graph data structure called Planning Encoding Graph which encodes the initial and goal facts of a single planning problem \u03a0 to perform an efficient matching between the objects of a planning case and the objects of the current planning problem. The vertices of this graph belong to a set V\u03a0 whose elements are the representation of the objects O of the current planning problem \u03a0 and of the predicate symbols P of \u03a0:\nV\u03a0 =O \u222a \u22c3 p\u2208P Ip \u222a \u22c3 q\u2208P Gq\ni.e. for each predicate we define two additional nodes, one associated to the corresponding initial fact predicate called Ip and the other associated to the corresponding goal fact predicate called Gq. The labels of this graph are derived from the predicates of our facts and the sorts of our many-sorted logic. The representation of an entity (an object using planning terminology) of the application domain is traditionally called a concept in the conceptual graph community [12]. Following this notation a Planning Encoding Graph is composed of three kinds of nodes: concept nodes representing entities (objects) that occur in the application domain, initial fact relation nodes representing relationships that hold between the objects of the initial facts and goal fact relation nodes representing relationships that hold between the objects of the goal facts.\nThe Planning Encoding Graph of a planning problem \u03a0(I,G) is built using the corresponding initial and goal facts. In particular for each propositional initial fact p = p(c1 \u2236 t1, ..., cn \u2236 tn) \u2208 I we define a data structure called Initial Fact Encoding Graph which corresponds to a graph that represents p. More precisely:\nDefinition 8 Given a propositional typed initial fact p = p(c1 \u2236 t1, ..., cn \u2236 tn) \u2208 I of \u03a0, the Initial Fact Encoding Graph EI(p) = (Vp,Ep, \u03bbp) of fact p is a directed labeled graph where\n\u2022 Vp = {Ip, c1, ..., cn} \u2286V\u03a0;\n\u2022 Ep = {[Ip, c1], [c1, c2], [c1, c3], ..., [c1, cn], [c2, c3], [c2, c4], ..., [cn\u22121, cn]} =\n= [Ip, c1] \u222a \u22c3 i=1,...,n; j=i+1,...,n [ci, cj]\n\u2022 \u03bbp(Ip) = {Ip}, \u03bbp(ci) = {ti} with i = 1, ..., n;\n\u2022 \u03bbp([Ip, c1]) = {I0,1p }; \u2200[ci, cj] \u2208 Ep, \u03bbp([ci, cj]) = {Ii,jp }; i.e. the first node of the graph EI(p), see Figure 4, is the initial fact relation node Ip labeled with the multiset \u03bbp(Ip) = {(Ip,1)} = {Ip},2 it is connected to a direct edge to the second node of the graph, the concept node c1, which is labeled by sort t1 (i.e. \u03bbp(c1) = {(t1,1)} = {t1}); the node c1 is connected with the third node of the graph c2 which is labeled by sort t2 (i.e. \u03bbp(c2) = {(t2,1)} = {t2}) and with all the remaining concept nodes, the third node of the graph c2 is connected with c3, c4, ...,cn and so on. The first edge of the graph [Ip, c1] is labeled by the multiset {I0,1p ,1} = {I0,1p }, similarly a generic edge [ci, cj] \u2208 Ep is labeled by the multiset {Ii,jp }.\nFor example, in Figure 5 we can see the Initial Fact Encoding Graph of the fact \u201cp = (on A B)\u201d of the BlocksWorld domain. The first node is named as \u201cIon\u201d and its label is the multiset \u03bbp(Ion) = {(Ion,1)} = {Ion}, the second node represents the object \u201cA\u201d with label \u03bbp(A) = {(Obj,1)} = {Obj} and finally the third node represents the object \u201cB\u201d and its label is \u03bbp(B) = {Obj}; the label of the [Ion,A] arc is the multiset {(I0,1on ,1)} = {I0,1on } and the label of the [A,B] arc is the multiset {(I1,2on ,1)} = {I1,2on }.\nSimilarly to Definition 8 we define the Goal Fact Encoding Graph EG(q) of the fact q = q(c\u20321 \u2236 t\u20321, ..., c\u2032m \u2236 t\u2032m) \u2208 G using {Gq} for the labeling procedure.\nGiven a planning problem \u03a0 with initial and goal states I and G, the Planning Encoding Graph of \u03a0, that we indicate as E\u03a0, is a directed labeled graph derived by the encoding graphs of the initial and goal facts:\nE\u03a0(I,G) = \u22c3 p\u2208I EI(p) \u222a \u22c3 q\u2208G EG(q) (5)\n2In the following we indicate the multiset {(x,1)} as {x} for sake of simplicity.\ni.e. the Planning Encoding Graph of \u03a0(I,G) is a graph obtained by merging the Initial and Goal Fact Encoding Graphs. For simplicity in the following we visualise it as a three-level graph. The first level is derived from the predicate symbols of the initial facts, the second level encodes the objects of the initial and goal states and the third level shows the goal fact nodes derived from the predicate symbols of the goal facts.3\nFigure 6 illustrates the Planning Encoding Graph for the Sussman anomaly planning problem in the BlocksWorld domain. The nodes of the first and third levels are the initial and goal fact relation nodes: the vertices Ion, Iclear and Ion\u2212table are derived by the predicates of the initial facts, while Gon by the predicates of the goal facts. The nodes of the second level are concept nodes which represent the objects of the current planning problem A, B and C, where the label \u201cObj\u201d corresponds to their type. The initial fact \u201c(on C A)\u201d determines two arcs, one connecting Ion to the vertex C and the second connecting C to A; the labels of these arcs are derived from the predicate symbol \u201con\u201d determining the multisets {I0,1on } and {I1,2on } respectively. In the same way the other arcs are defined. Moreover since there is no overlapping among the edges of the Initial and Goal Fact Encoding Graphs, the multiplicity of the edge label multisets is equal to 1; on the contrary the label multisets of the vertices associated to the objects are:\n\u03bb(A) = {(Obj,3)}, \u03bb(B) = {(Obj,4)} and \u03bb(C) = {(Obj,3)}. Moreover it could be useful to point out that if an object c appears more than once in an initial (goal) fact p(c1....cn) of a planning problem \u03a0, then the corresponding Initial (Goal) Fact Encoding Graph is built as usual (instantiating n nodes, one each ci), while during the construction of the Planning Encoding Graph obtained by merging the Initial and Goal Encoding Graphs of \u03a0, the nodes that correspond to the same object are merged into a single vertex node.\n3Following the conceptual graph notation, the first and third level nodes correspond to initial and goal fact relation nodes, while the nodes of the second level correspond to concept nodes representing the objects of the initial and goal states.\nThis graph representation can give us a detailed description of the \u201ctopology\u201d of a planning problem without requiring any a priori assumptions on the relevance of certain problem descriptors for the whole graph. Furthermore it allows us to use Graph Theory based techniques in order to define effective matching functions. In fact a matching function from \u03a0\u2032 to \u03a0 can be derived by solving the Maximum Common Subgraph problem on the corresponding Planning Encoding Graphs. A number of exact and approximate algorithms have been proposed in the literature so as to solve this graph problem efficiently. With respect to normal conceptual graphs [12] used for Graph-based Knowledge Representation, we use a richer label representation based on multisets. A single relation node is used to represent each predicate of the initial and goal facts which reduces the total number of nodes in the graphs considerably. This is extremely important from a computational point of view since, as we will see in the following sections, the matching process must be repeated several times and it directly influences the total retrieval time.\nIn the following we examine a procedure based on graph degree sequences that is useful to derive an upper bound on the size of the MCES of two graphs in an efficient way. Then we present an algorithm based on Kernel Functions that allows to compute an approximate matching of two graphs in polynomial time."}, {"heading": "3.2 Plan Evaluation Phase", "text": "The purpose of plan evaluation is that of defining the capacity of a plan \u03c0 to resolve a particular planning problem. It is performed by simulating the execution of \u03c0 and identifying the unsupported preconditions of its actions; in the same way the presence of unsupported goals is identified. The plan evaluation function could be easily defined as the number of inconsistencies in the current planning problem. Unfortunately this kind of evaluation considers a uniform cost in order to resolve the different inconsistencies and this assumption is generally too restrictive. Then our system considers a more accurate inconsistency evaluation criterion so as to improve the plan evaluation metric. The inconsistencies related to unsupported facts are evaluated by computing a relaxed plan starting from the corresponding state and using the RELAXEDPLAN algorithm in LPG [22]. The number of actions in the relaxed plan determines\nthe difficulty to make the selected inconsistencies supported; the number of actions in the final relaxed plan determines the accuracy of the input plan \u03c0 to solve the corresponding planning problem.\nFigure 7 describes the main steps of the RELAXEDPLAN function.4 It constructs a relaxed plan through a backward process where Bestaction(g) is the action a\u2032 chosen to achieve a (sub)goal g, and such that: (i) g is an effect of a\u2032; (ii) all preconditions of a\u2032 are reachable from the current state INIT; (iii) the reachability of the preconditions of a\u2032 requires a minimum number of actions, evaluated as the maximum of the heuristically estimated minimum number of actions required to support each precondition p of a\u2032 from INIT; (iv) a\u2032 subverts a minimum number of supported precondition nodes in A (i.e., the size of the set Threats(a\u2032) is minimal).\nFigure 8 describes the main steps of the EVALUATEPLAN function. For all actions of \u03c0 (if any), it checks if at least one precondition is not supported. In this case it uses the RELAXEDPLAN algorithm (step 4) so as to identify the additional actions required to satisfy the unsupported preconditions. If Rplan contains a number of actions greater than Climit we can stop the evaluation, otherwise we update the current state CState (step 7). Finally we examine the goal facts G (step 8) to identify the additional actions required to satisfy them, if necessary.\nIn order to improve the efficiency of the system and reuse as many possible parts of previously executed plans we have adopted plan merging techniques [?], which are based on the well-known divide and conquer strategy.\nIn order to apply this strategy, our system must accomplish two further subtasks: problem decomposition and plan merging. The problem decomposition is performed identifying the set of actions and the initial facts needed for a single goal and storing them in the case base as a new problem instance (if not already present); moreover these new instances remain related to the original solution plan in order to maintain a statistic of their effective usage. The stored (sub)cases are then used in the merging phase in order to identify a single global plan that satisfies all goals. We progressively identify the unsatisfied goals and the corresponding (sub)cases that allow to satisfy them, giving the preference to the (sub)plans that allow us to improve the plan metric and that have been successful in a greater number of times in analogous situations.\n4RELAXEDPLAN is described in detail in [22]. It also computes an estimation of the earliest time when all facts in G can be achieved, which is not described in this paper for sake of simplicity."}, {"heading": "3.2.1 Application of plan merging techniques", "text": "We have used case-based plan merging techniques to store plans. Moreover, in order to reuse as many as possible parts of previously executed plans, we decompose the solution plans into subparts that allow us to satisfy every single goal or a set of interrelated goals and we store these subparts in the case base, if they are not already present.\nWhen a new e-learning planning problem must be solved, we search in the case base if a plan that already solves all goals exists. If such a plan does not exist we apply plan merging techniques that progressively identify (sub)plans in the case base that can satisfy the goals. This phase consists in reusing parts of the retrieved plans to complete a new one. Figure 9 describes the process for merging plans of the library in order to find a plan \u03c0 that solves the current planning problem \u03a0 or that represents a quasi-solution [?] for it. At step 1 we search in the library the plan that satisfies all the goals with the lowest heuristic adaptation cost, where the function EvPlan(I, \u03c0,G) determines the adaptation effort by estimating the number of actions that are necessary to transform \u03c0 into a solution of the problem \u03a0(I,G).5 This step corresponds to the extraction of the best plan of the library (if it exists) as proposed by the standard OAKPLAN system. At steps 3.x, we progressively analyse the unsatisfied goals and the unsatisfied preconditions of the current plan \u03c0, trying to identify in the library a subplan \u03c0f that can be merged with \u03c0 in order to satisfy f (and other unsatisfied facts if possible) and reduce, at the same time, the global heuristic adaptation cost, where merge identifies the best part of \u03c0 where the actions of \u03c0f can be inserted in producing a new global plan.6 If such a plan exists, we merge it with \u03c0 at step 3.3 and we restart from step 3 reconsidering all the unsatisfied facts. The repeat loop halts when all the goals and preconditions are satisfied, i.e. when we have found a solution plan, or when there is not a suitable plan that can be extracted from the library that satisfies the remaining unsupported facts. In this case, the plan \u03c0 does not represent a solution plan. However, it can be used as a starting point for a local search process to find a solution plan for the current planning problem.\nFigure 10 describes the main steps of the retrieval phase. We initially compute a relaxed plan \u03c0R for \u03a0 (step 1.1) using the EVALUATEPLAN function on the empty plan which is needed so as to define the generation cost of the current planning problem \u03a0 (step 4.1)7 and an estimate of the initial state relevant facts (step 1.2). In fact we use the relaxed plan \u03c0R so as to filter out\n5See EVALUATEPLAN for a more detailed description. 6In our tests we have considered the earliest and the latest part of \u03c0 where f can be satisfied. 7The \u03b1G coefficient gives more or less importance to plan adaptation vs plan generation; if \u03b1G > 1 then it is\nmore likely to perform plan adaptation than plan generation.\nthe irrelevant facts from the initial state description.8 This could be easily done by considering all the preconditions of the actions of \u03c0R:\nI\u03c0R = I \u2229 \u22c3 a\u2208\u03c0R pre(a).\nThen in step 1.3 the Planning Encoding Graph of the current planning problem \u03a0 and the degree sequences that will be used in the screening procedure are precomputed. Note that the degree sequences are computed considering the Planning Encoding Graph E\u03a0R of the planning problem \u03a0R(I\u03c0R ,G) which uses I\u03c0R instead of I as initial state. This could be extremely useful in practical applications when automated tools are used to define the initial state description without distinguishing among relevant and irrelevant initial facts.\nSteps 1.4 \u2013 1.7 examine all the planning cases of the case base so as to reduce the set of candidate plans to a suitable number. It is important to point out that in this phase it is not necessary to retrieve the complete planning encoding graphs of the case base candidates G\u03a0\u2032 but only their sorted degree sequences Li\u03a0\u2032 which are precomputed and stored in the case base.\n8In the relaxed planning graph analysis the negative effects of the domain operators are not considered and a solution plan \u03c0R of a relaxed planning problem can be computed in polynomial time [31].\nOn the contrary the planning encoding graph and the degree sequences of the input planning problem are only computed in the initial preprocessing phase (step 1.3).\nAll the cases with a similarity value sufficiently close9 to the best degree sequences similarity value (best_ds_simil) are examined further on (steps 2.1\u20132.4) using the Kbase kernel function. Then all the cases selected at steps 2.x with a similarity value sufficiently close to the best simil\u00b5base similarity value (best_\u00b5base_simil) (step 3.1) are accurately evaluated using the KN kernel function, while the corresponding \u00b5N function is defined at step 3.2. In steps 3.3\u20133.5 we select the best matching function found for \u03a0i and the best similarity value found until now.\nWe use the relaxed plan \u03c0R in order to define an estimate of the generation cost of the current planning problem \u03a0 (step 4.1). The best_cost value allows to select a good candidate plan for adaptation (which could also be the empty plan). This value is also useful during the computation of the adaptation cost through EVALUATEPLAN, in fact if such a limit is exceeded then it is wasteful to use CPU time and memory to carry out the estimate and the current evaluation could be terminated. The computation of the adaptation cost of the empty plan allows to choose between an adaptive approach and a generative approach, if no plan gives an adaptation cost smaller than the empty plan.\nFor all the cases previously selected with a similarity value sufficiently close to best_simil (step 4.2) the adaptation cost is determined (step 4.4). If a case of the case base determines an adaptation cost which is lower than best_cost\u22c5simil\u00b5i(\u03a0i,\u03a0) then it is selected as the current best case and also the best_cost and the best_plan are updated (steps 4.5\u20134.7). Note that we store the encoded plan \u00b5i(\u03c0i) in best_plan since this is the plan that can be used by the adaptation phase for solving the current planning problem \u03a0. Moreover we use the simil\u00b5i(\u03a0i,\u03a0) value in steps 4.4 \u2013 4.6 as an indicator of the effective ability of the selected plan to solve the current planning problem maintaining the original plan structure and at the same time obtaining low distance values."}, {"heading": "3.3 Plan Adaptation", "text": "As previously exposed, the plan adaptation system is a fundamental component of a casebased planner. It consists in reusing and modifying previously generated plans to solve a new problem and overcome the limitation of planning from scratch. As a matter of fact, in planning from scratch if a planner receives exactly the same planning problem it will repeat the very same planning operations. In our context the input plan is provided by the plan retrieval phase previously described; but the applicability of a plan adaption system is more general. For example the need for adapting a precomputed plan can arise in a dynamic environment when the execution of a planned action fails, when the new information changing the description of the world prevents the applicability of some planned actions, or when the goal state is modified by adding new goals or removing existing ones [18, 22].\nDifferent approaches have been considered in the literature for plan adaptation; strategies vary from attempting to reuse the structure of an existing plan by constructing bridges that link together the fragments of the plan that fail in the face of new initial conditions [27, 28, 29, 32], to more dynamic plan modification approaches that use a series of plan modification operators to attempt to repair a plan [38, 57]. From a theoretical point of view, in the worst case, plan adaptation is not more efficient than a complete regeneration of the plan [44] when a conservative adaptation strategy is adopted. However adapting an existing plan can be in practice more\n9In our experiments we used limit = 0.1.\nefficient than generating a new one from scratch, and, in addition, this worst case scenario does not always hold, as exposed in [3] for the Derivation Analogy adaptation approach. Plan adaptation can also be more convenient when the new plan has to be as \u201csimilar\u201d as possible to the original one.\nOur work uses the LPG-adapt system given its good performance in many planning domains but other plan adaptation systems could be used as well. LPG-adapt is a local-search-based planner that modifies plan candidates incrementally in a search for a flawless candidate. We describe the main components of the LPG-adapt system in the following section. It is important to point out that this paper relates to the description of a new efficient case-based planner and in particular to the definition of effective plan matching functions, no significant changes were made to the plan adaptation component (for a detailed description of it see [18])."}, {"heading": "3.4 Plan Revision & Case Base Update", "text": "Any kind of planning system that works in dynamic environments has to take into account failures that may arise during plan generation and execution. In this respect case-based planning is not an exception; this capability is called plan revision and it is divided in two subtasks: evaluation and repair. The evaluation step verifies the presence of failures that may occur during plan execution when the plan does not produce the expected result. When a failure is discovered, the system may react by looking for a repair or aborting the plan. In this first hypothesis the LPG-adapt system is invoked on the remaining part of the plan; in the latter hypothesis the system repeats the CBP cycle so as to search a new solution.\nAfter finding the plan from the library and after repairing it with the LPG-adapt techniques the solution plan can be inserted into the library or be discarded. The case base maintenance is clearly important for the performance of the system and different strategies have been proposed in the literature [51, 56]. Furthermore our attention has been oriented towards the improvement of the competence of the case base; a solved planning problem is not added to the case base only if there is a case that solves the same planning problem with a solution of a better quality.10 Such a check has been introduced to the end of keeping only the best solution plans for certain 10In our experiments we have considered only the number of actions for distinguishing between two plans that solve the same planning problem but other and more accurate metrics could be easily added, i.e. consider for example actions with not unary costs.\nkinds of problems in the library as there can be different plans that can solve the same problems with different sets of actions.\nFigure 11 describes the main steps of the function used to evaluate the insertion of a planning problem \u03a0 solved in the case base. First of all we compute the set of initial state relevant facts I\u03c0 using the input plan \u03c0; this set corresponds to a subset of the facts of I relevant for the execution of \u03c0. It can be easily computed, as described in section 3.2, using the preconditions of the actions in \u03c0:\nI\u03c0 = I \u2229 \u22c3 a\u2208\u03c0 pre(a).\nNote that I\u03c0 identifies all the facts required for the execution of the plan \u03c0 and that this definition is consistent with the procedure used in the RETRIEVEPLAN algorithm for the relaxed plan \u03c0R.11 Then we compute the Planning Encoding Graph E\u03c0 of the new planning problem \u03a0\u03c0(I\u03c0,G) having I\u03c0 as initial state instead of I . At steps 3\u20136 the algorithm examines all the cases of the case base and if it finds a case that solves \u03a0 with a plan of a better quality with respect to \u03c0 then it stops and exits. In order to do so we use the similarity function complete_simil\u00b5i , described in section 3.1.1, which compares all the initial and goal facts of two planning problems. Otherwise if there is no case that can solve \u03a0\u03c0 with a plan of a better quality with respect to \u03c0 then we insert the solved problem in the case base. As we can observe at step 8, a planning case is made up not only by \u03a0\u03c0 and \u03c0, but also other additional data structures are precomputed and added to the case base so that their recomputation during the Retrieval Phase can be avoided.\nRecently, the system was extended with a set of maintenance policies guided by the cases\u2019 similarity, as is described in [25, 26].\nFigure 12 describes the algorithm for updating the plan library with parts of an input plan \u03c0. In short, UPDATELIBRARY identifies the subplans of \u03c0 that can be inserted in the plan library to increase the competence of the library in itself [52, 56]. Here \u03c0g represents the subplan of \u03c0 that satisfies g starting from I . Note that it can be easily identified considering the set of causal links C\u03c0 of \u03c0 computed at step 1. In a similar way, it is possible to compute the set of facts Ig that are necessary to apply the actions of \u03c0g.\nAt step 2 we identify the set of facts that will be examined for the insertion in the library. In particular we consider all the goalsG, the elements of F and the subsets of interacting goals Gi. The F set represents a set of facts, different by the input goals, that could be useful for the following merging phase such as unsupported facts of a previous adaptation phase. Moreover, the sets of interacting goals Gi can be easily computed considering the actions in the subplans \u03c0gj that are in common to the different goals.\n11We have used this simple definition instead of using the causal links in \u03c0 in order to compute the set of relevant facts since it allows to obtain slightly better performance than the corresponding version based on causal links.\nThe CHECK&INSERT((IGi ,Gi), \u03c0) function (step 3.1) searches if there not exists a casebase element (\u03a0j , \u03c0j) whose goals and initial state perfectly match with the current goals and initial state, respectively. In this case, we insert the current planning problem \u03a0Gi = (IGi ,Gi) and its solution plan \u03c0Gi in the library. Otherwise, we have to decide whether to insert (\u03a0Gi , \u03c0Gi) and remove (\u03a0j , \u03c0j), or simply skip the insertion of (\u03a0Gi , \u03c0Gi). In our tests we have used an update policy that maintains the plan with the lowest number of actions, but other policies could be used as well considering, for example, the plan qualities, their makespan, or the robustness to exogenous events. Moreover, CHECK&INSERT ignores too small and too big plans \u03c0; in fact, a small plan \u03c0 could determine the inclusion in the library of a high number of very small plan fragments that have to be considered in the merging phase, while a big plan \u03c0 could determine the insertion in the library of very big subplans that are difficult to merge.12"}, {"heading": "4 Experimental Results", "text": "In this section, we present an experimental study aimed at testing the effectiveness of OAKPLAN in a number of standard benchmark domains.\nWe have experimented with several courses, but here focus on a real, large-size Moodle course of around 90 LOs on Natural Sciences inspired on http://www.profesorenlinea.cl. We have created nine initial configurations (with 10, 20. . . 90 fictitious students, respectively), and defined 10 variants per configuration (plus an additional variant for the 90 problem), thus considering 100 planning problems in total (the 91 variants plus the 9 initial configurations). Each variant artificially simulates the changes that may occur during the route execution in an incremental way. That is, in the first variant some equipment is no longer available. The second variant maintains these changes and includes restrictions on the students\u2019 availability; and so on for the other variants.\nIn addition to OAKPLAN and our case base planner OAKPLAN-merge, we have used two state of the art planners, SGPLAN6 and LPG13. All tests were performed on an Intel(R) Xeon(TM) CPU 2.40GHz with 2GB of RAM, and censored after 10 minutes. In our tests, the solution plans (i.e. the learning routes) inserted in the case base were obtained by using the best quality plans generated by LPG and SGPLAN6 on the initial-configuration planning problems used to create the corresponding variants.\nFigure 13 depicts the results: the time taken to produce a solution \u2014the first one for LPG, OAKPLAN and OAKPLAN-merge \u2014 (top); the quality of the generated routes (middle); and\n12In our tests we have used 5 \u2264 \u2223\u03c0\u2223 \u2264 200. 13For a further description of these planners see http://ipc.icaps-conference.org.\nthe stability, in terms of distance of the new routes to the original ones (bottom). We show the best distance and plan quality across all plans produced in the entire optimization phase14.\nIn the plots on the left we compare OAKPLAN (with and without merging techniques) vs. LPG and SGPLAN6 using a \u201ccomplete\u201d case-base that contains all the base problems and the corresponding solutions (the case-base for the merging variants contains also the selected subplans of the base problems). Here we can observe the general good behaviour of the casebased techniques, which are comparable in terms of CPU-time to SGPLAN6 and slightly better than the other planners in terms of plan quality for the bigger instances. OAKPLAN-merge is slightly slower than OAKPLAN since it tries to reuse not only the base problems but also their subplans. The results demonstrate that the case-based approach is at least as fast as replanning, and sometimes faster. Obviously, plan retrieval techniques show less useful when the changes are significant and fixing the route requires more effort than simply discarding it and rebuilding a new one from scratch. But the benefits for investing this effort can be seen in terms of stability. On the other hand, the retrieval and adaptation process sometimes comes at a price in terms of quality, as the route is adapted to fit a new configuration rather than constructed expressly for it. But our experiments show that the quality for the case-based approach can be better than for replanning, particularly in the most complex problems (see Figure 13).\nFinally, the best values for stability are achieved in OAKPLAN-merge. While replanning generates routes that are consistently very different to the original ones, the differences between the retrieved plan and the solution plans are very small. This high value is not particularly surprising since LPG and SGPLAN6 do not know the target plans used for this comparison. These distance values are interesting since they are a clear indicator of the good behaviour of case-based techniques and show that the generative approach is not feasible when we want to preserve the stability of the plans produced. Moreover, we can observe the extremely good behaviour of OAKPLAN-merge; its distance values are obtained considering the number of different actions w.r.t. the matching plan provided by the retrieval phase, which is not necessarily obtained directly by the solution plan stored in the case base (as in OAKPLAN), but also using the different subplans (highlighted to the teachers) obtained by the analysis of the case-based elements that best fit the current initial state and goals. This indicator is very appealing in an e-learning setting as the students/teachers do not want to deal with an entirely new learning route after a little change happens during execution. Quite the contrary, students and teachers prefer a kind of inertia in the learning routes that facilitates the learning process.\nIn the plots on the right of Figure 13 we analyse the behaviour of OAKPLAN and OAKPLANmerge to study the impact of using a case base considering: i) a case base created using only the smallest base problem (with 10 students), ii) a case base where the base problems are progressively inserted after the corresponding variants have been evaluated (it initially contains only the smallest base problem). In the first case, we primarily want to evaluate the ability of the merging techniques to reuse the solutions available in the case base at the increase of the \u201cdifferences\u201d (in terms of number of students) among the current situation and the elements stored in the case base. In particular, we want to examine the scalability in terms of students which is extremely important in our context where a teacher could decide to evaluate the effectiveness of an e-learning course considering a limited number of students before using it for the whole class.\n14Note that the first plan generated by LPG, OAKPLAN and OAKPLAN-merge, the best quality plan and the best distance plan could be different plans. It depends on the teacher\u2019s preferences to give more importance to the plan quality or to the plan stability by selecting the most appropriate solution plan during the validation process.\nHere we can observe the general good behaviour of OAKPLAN-merge, while OAKPLAN without merging techniques is able to solve only 50 variants with high distance values w.r.t. the solution plan of the base problems. In fact, OAKPLAN can retrieve from the case base only the base problem with 10 students, whose solution plan is very different w.r.t. the final solution plan., so we have not plotted the distance w.r.t. the solution plan provided by the retrieval phase since it corresponds to the solution plan of the base problem with 10 students which is obviously extremely different w.r.t. the final solution plan.\nRegarding the tests with the incremental case base, we want to analyse the behaviour of OAKPLAN considering a case base that contains elements which are structurally not to much different w.r.t. the current situation. For example, considering the solution of the variants with 50 students, the case base does not contain the base problem with 50 students but it contains the base problems with 10, 20, 30 and 40 students. Here we want to examine the situation where a teacher has already used a course in different classes and wants to reuse the stored experiences in a new (slightly bigger) class. As expected, the behaviour of OAKPLAN-merge does not change significantly neither in term of CPU-time nor in term of distance and plan quality; on the contrary, the CPU-time of OAKPLAN without merging techniques decreases significantly since it can replan starting from a case base element with a slightly lower number of students w.r.t. the current situation; moreover it is now able to solve all the variants considered. Regarding the plan differences w.r.t. the solution of the base problems we can observe values which are similar to the ones obtained for replanning. This is not surprising since the elements stored in the case base are different w.r.t. the current situation and OAKPLAN does not know the target solution plan; on the contrary the performances of OAKPLAN-merge are extremely good, both considering the case base with only the smallest base problems and the incremental case base. However, it is important to point out that in this case the comparison in terms of plan distances are performed considering directly the plan provided by the retrieval phase. It is up to the teacher to decide if (s)he wants to validate elements that considers only previously executed courses or also subparts of them.\nGlobally we can observe that the use of plan merging techniques are potentially very effective in our execution context, allowing to obtain efficiently new e-learning routes which are very similar to the combination of previously executed ones (or subpart of them). This is extremely important since it allows to highlight to the teachers the changes w.r.t. already executed learning routes, facilitating the validation process. Moreover, the stored plans can also contain some notes, regarding for example the pedagogical motivations associated to the selection or combination of specific LOs, annotated by the teacher during the creation of the original learning route or during previously executions of the learning route; in this case, these notes can be easily reexamined by the teachers facilitating the learning route validation process."}, {"heading": "5 Related Work", "text": "In the following section we examine the most relevant case-based planners considering their retrieval, adaptation and storage capabilities. Moreover, we present an empirical comparison of the performance of OAKPLAN vs. the FAR-OFF system and some comments on the advantages of OAKPLAN with respect to other case-based planners.\nSome CBP systems designed in the past do not consider any generative planning in their structure, and find a solution only by the cases stored in the case base. These CBP systems are called reuse-only systems. As reuse-only systems cannot find any planning solution from scratch, they cannot find a solution unless they find a proper case in the case base that can\nbe adapted through single rules. An alternative approach to reuse-only systems is the reuseoptional approach, which uses a generative planning system that is responsible to adapt the retrieved cases. This feature allows a CBP system to solve problems that cannot be solved only by using stored cases and simple rules in the adaptation phase. Empirically, a great number of reuse-optional CBP systems has shown that the use of a case base can permit them to perform better in processing time and in a number of planning solutions than the generative planning that they incorporate.\nObviously the retrieval phase critically affects the systems performance; it must search in a space of cases in order to choose a good one that will allow the system to solve a new problem easily. In order to improve efficiency in the retrieval phase, it is necessary either to reduce the search space or to design an accurate similarity metric. Reducing the search space, only a suitable subset of cases will be available for the search process and an accurate similarity metric will choose the most similar case to decrease the adaptation phase effort. In the literature there are different domain dependent and a few domain independent plan adaptation and case-based planning systems, which mostly use a search engine based on a space of states [24, 29, 56, 57]. An alternative approach to planning with states is that of plan-space planning or hierarchical systems [4] that search in a space of plans and have no goals, but only tasks to be achieved. Since tasks are semantically different from goals, the similarity metric designed for these CBP systems is also different from the similarity rules designed for state-space based CBP systems. For a detailed analysis of case-based and plan adaptation techniques see the papers of Spalazzi [53] and Munoz-Avila & Cox [42].\nThe CHEF system [27] is the first application of CBR in planning and it is a reuse-only system which is important especially from a historical point of view. It solves problems in the domain of Szechwan cooking and is equipped with a set of plan repair rules that describe how a specific failure can be repaired. Given a goal to produce a dish with particular properties, CHEF first tries to anticipate any problems or conflicts that may arise from the new goal and repairs problems that did not arise in the baseline scenario. It then executes the plan and, if execution results in a failure, a repair algorithm analyses the failure and builds an explanation of the reason why the failure has occurred. This explanation includes a description of the steps and states leading towards the failure as well as the goals that these steps tried to realise. Based on the explanation, a set of plan repair strategies is selected and instantiated to the specific situation of the failure. After choosing the best of these instantiated repair strategies, CHEF implements it and uses the result of the failure analysis to improve the index of this solution so that it will not be retrieved in situations where it will fail again.\nMuch attention has been given to research that designs suitable similarity metrics. It focuses on choosing the most adaptable case as the most similar one, such as the DIAL [36] and D\u00c9J\u00c0VU [50] systems. The DIAL system is a case-based planner that works in disaster domains where cases are schema-based episodes and uses a similarity assessment approach, called RCR, which considers an adaptability estimate to choose cases in the retrieval phase. Our similarity functions differ from the RCR method since they are based on a domain knowledge that is available in action definitions, while the RCR method uses the experience learned from the adaptation of previous utilisation of cases. They also differ in their applicability because the RCR method considers specifically disaster domains while our approach is suitable for domain independent planning.\nSimilarly, the D\u00c9J\u00c0VU system operates in design domains and uses an adaptation-guided retrieval (AGR) procedure to choose cases that are easier to be adapted. The AGR approach in the D\u00c9J\u00c0VU system uses additional domain knowledge, called capability knowledge, which\nis similar to that used to solve conflicts in partial-order planning systems. This additional knowledge allows to identify the type and the functionality of a set of transformations, which are performed by actions, through a collection of agents called specialists and strategies. It must be well specified so as to maximise the AGR performance. Our similarity functions differ from the AGR approach because we do not use any domain knowledge besides that obtained from actions and states, which is the minimal knowledge required to define a domain for planning systems.\nThe PLEXUS system [2] confronts with the problem of \u201cadaptive planning\u201d, but also addresses the problem of runtime adaptation to plan failure. PLEXUS approaches plan adaptation with a combination of tactical control and situation matching. When plan failure is detected it is classified as either beginning a failing precondition, a failing outcome, a case of differing goals or an out-of-order step. If we ignore how to manage incomplete knowledge, the repair strategy involves the fact of replacing a failed plan step with one that might achieve the same purpose. It uses a semantic network to represent abstraction classes of actions that achieve the same purpose.\nThe GORDIUS [49] system is a transformational planner that combines small plan fragments for different (hopefully independent) aspects of the current problem. It does not perform an anticipation analysis on the plan, on the contrary it accepts the fact that the retrieved plan will be flawed and counts on its repair heuristics to patch it; in fact, much of the GORDIUS work is devoted to developing a set of repair operators for quantified and metric variables. The previous approaches differ with respect to OAKPLAN fundamentally because they are domain dependent planners; on the contrary OAKPLAN uses only the domain and planning problems descriptions.\nThree interesting works developed at the same time adopt similar assumptions: the PRIAR system [33], the SPA system [29] and the Prodigy/Analogy system [58, 59]. PRIAR uses a variant of Nonlin [55], a hierarchical planner, whereas SPA uses a constraint posting technique similar to Chapman\u2019s Tweak [11] as modified by McAllester and Rosenblitt [40]. PRIAR\u2019s plan representation and thus its algorithms are more complicated than those of SPA. There are three different types of validations (filter condition, precondition, and phantom goal) as well as different reduction levels for the plan that represents a hierarchical decomposition of its structure, along with five different strategies for repairing validation failures. In contrast to this representation the plan representation of SPA consists of causal links and step order constraints. The main idea behind the SPA system that separates it from the systems mentioned above is that the process of plan adaptation is a fairly simple extension of the process of plan generation. In the SPA view, plan generation is just a special case of plan adaptation (one in which there is no retrieved structure to exploit). With respect to our approach that defines a matching function \u00b5 from \u03a0 to \u03a0\u2032 that maximises the similarity function simil\u00b5, it should be noted that in PRIAR and SPA the conditions for the initial state match are slightly more complicated. In PRIAR the number of inconsistencies in the validation structure of the plan library is minimised; in SPA the number of violations of preconditions in the plan library is maximised. Moreover the problem-independent matching strategy implemented in SPA runs in exponential time in the number of objects since it simply evaluates all possible mappings. On the contrary we compute an approximate matching function in polynomial time and use an accurate plan evaluation function on a subset of the plans in the library.\nThe Prodigy/Analogy system also uses a search oriented approach to planning. A library plan (case) in a transformational or case-based planning framework stores a solution to a prior problem along with a summary of the new problems for which it would be a suitable solution,\nbut it contains little information on the process that generates the solution. On the other hand derivational analogy stores substantial descriptions of the adaptation process decisions in the solution, whereas Veloso\u2019s system records more information at each choice point than SPA does, like a list of failed alternatives. An interesting similarity rule in the plan-space approach is presented in the CAPLAN/CBC system [43] which extends the similarity rule introduced by the Prodigy/Analogy system [58, 59] by using feature weights in order to reduce the errors in the retrieval phase. These feature weights are learned and recomputed according to the performance of the previous retrieved cases and we can note that this approach is similar to the RCR method used by the DIAL system in disaster domains. There are two important differences between our approach and the similarity rules of CAPLAN/CBC, one of which is that the former is designed for state-space planning and the latter for plan-space planning. Another difference is that our retrieval function does not need to learn any knowledge to present an accurate estimate: our retrieval method only needs the knowledge that can be extracted from the problem description and the actions of the planning cases.\nO-Plan [15, 16] is based on the strategy of using plan repair rules as well. The effects of every action are confirmed while execution is performed. A repair plan formed by additional actions is added to the plan every time a failing effect is necessary in order to execute some other actions. We call repair plans the prebuilt ones which are in a position to repair a series of failure conditions. For instance, we can have repair plans including a plan to replace either a flat tyre or a broken engine. When an erroneous condition is met, the plan is no longer executed but a repair plan is inserted and executed. When the repair plan is complete, the regular plan is executed once more. Failures are repaired by O-Plan by adding actions. It follows that it does not use either unrefinements or requires a history. However it is not complete and there are some failures which cannot be repaired.\nMLR [44] is another case-based system and it is based on a proof system. While retrieving a plan from the library that has to be adapted to the current world state, it makes an effort to employ the retrieval plan as if it were a proof to set the goal conditions from the start. Should this happen, there is no need for any iteration to use the plan, otherwise, the outcome is a failed proof that can provide refitting information. On the basis of the failed proof, a plan skeleton is built through a modification strategy and it makes use of the failed proof to obtain the parts of the plan that are useful and removes the useless parts. After the computation of this skeleton, gaps are filled through a refinement strategy which makes use of the proof system. Although our object matching function is inspired to the Nebel & Koehler\u2019s formalisation, our approach significantly differs from theirs since they do not present an effective domain independent matching function. In fact, their experiments exhibit an exponential run time behaviour for the matching algorithm they use, instead we show that the retrieval and matching processes can be performed efficiently also for huge plan libraries. The matching function formalisation proposed by Nebel & Koehler also tries to maximise first the cardinality of the common goal facts set and second the cardinality of the common initial facts set. On the contrary we try to identify the matching function \u00b5 that maximise the simil\u00b5 similarity value which considers both the initial and goal relevant facts and an accurate evaluation function based on a simulated execution of the candidate plans is used to select the best plan that has to be adapted.\nNebel & Koehler [44] present an interesting comparison of the MLR, SPA and PRIAR performance in the BlocksWorld domain considering planning instances with up to 8 blocks. They show that also for these small sized instances and using a single reuse candidate the matching costs are already greater than adaptation costs. When the modification tasks become more difficult, since the reuse candidate and the new planning instance are structurally less similar, the\nsavings of plan modification become less predictable and the matching and adaptation effort is higher than the generation from scratch. On the contrary OAKPLAN shows good performance with respect to plan generation and our tests in the BlocksWorld domain consider instances with up to 140 blocks and a plan library with ten thousands cases.\nThe LPA* algorithm is used by the SHERPA replanner [38]. This algorithm was originally bound to repair path plan and backtrack to a partial plan having the same heuristic value as before the unexpected changes did in the world using the unrefinement step once. SHERPA is not useful to solve every repair problem, owing to the unrefinement strategy and the single application thereof. Its use is restricted to those problems whose actions are no longer present in the domain description. It follows that through the unrefinement step unavailaible actions are removed.\nThe Replan [7] model of plans is similar to the plans used in the hierarchical task network (HTN) formalism [17]. A task network is a description of a possible way to fulfil a task by doing some subtasks, or, eventually (primitive) actions. For each task at least one of such task networks exists. A plan is created by choosing the right task networks for each (abstract) task chosen, until each network consists of only (primitive) actions. Throughout this planning process, Replan constructs a derivation tree that includes all tasks chosen, and shows how a plan is derived. Plan repair within Replan is called partialisation. For each invalidated leaf node of the derivation tree, the (smallest) subtree that contains this node is removed. Initially, such an invalid leave node is a primitive action and the root of the corresponding subtree is the task containing this action. Subsequently a new refinement is generated for this task. If the refinement fails, a new round is started in which task subtrees that are higher in the hierarchy are removed and regenerated. In the worst case, this process continues until the whole derivation tree is discarded.\nA very interesting case-based planner is the FAR-OFF15 (Fast and Accurate Retrieval on Fast Forward) system [56]. It uses a generative planning system based on the FF planner [31] to adapt similar cases and a similarity metric, called ADG (Action Distance-Guided), which, like EVALUATEPLAN, determines the adaptation effort by estimating the number of actions that is necessary to transform a case into a solution of the problem. The ADG similarity metric calculates two estimate values of the distance between states. The first value, called initial similarity value, estimates the distance between the current initial state I and the initial state of the case I\u03c0 building a relaxed plan having I as initial state and I\u03c0 as goal state. Similarly the second value, called goal similarity value, estimates the distance between the final state of the case and the goals of the current planning problem. Our EVALUATEPLAN procedure evaluates instead every single inconsistency that a case base solution plan determines in the current world state I .\nThe FAR-OFF system uses a new competence-based method, called Footprint-based Retrieval [51], to reduce the space of cases that will be evaluated by ADG. The Footprint-based Retrieval is a competence-based method for determining groups of footprint cases that represent a smaller case base with the same competence of the original one. Each footprint case has a set of similar cases called Related Set [51]. The union of footprint cases and Related Set is the original case base. On the contrary OAKPLAN uses a much more simple procedure based on the similds function to filter out irrelevant cases. The use of Footprint-based Retrieval techniques and case base maintenance policies in OAKPLAN is left for future work. It is important to point out that the retrieval phase of FAR-OFF does not use any kind of abstraction to match cases and problems.\n15FAR-OFF is available at http://www.fei.edu.br/\u223cflaviot/faroff.\nThe FAR-OFF system retrieves the most similar case, or the ordered k most similar cases, and shifts to the adaptation phase. Its adaptation process does not modify the retrieved case, but only completes it; it will only find a plan that begins from the current initial state and then goes to the initial state of the case, and another plan that begins from the state obtained by applying all the actions of the case and goes to a state that satisfies the current goals G. Obviously, the completing of cases leads the FAR-OFF system to find longer solution plans than generative planners, but it avoids wasting time in manipulating case actions in order to find shorter solutions length. To complete cases, the FAR-OFF system uses a FF-based generative planning system, where the solution is obtained by merging both plans that are found by the FF-based generative planning and the solution plan of the planning case selected. On the contrary OAKPLAN uses the LPG-adapt adaptation system, which uses a local search approach and works on the whole input plan so as to adapt and find a solution to the current planning problem.\nIn Figure 14 we can observe the behaviour of OAKPLAN vs FAR-OFF considering different variants of the greater case bases provided with the FAR-OFF system in the Logistics domain;16 similar results have been obtained in the BlocksWorld, DriverLog and ZenoTravel domains. Globally, we can observe that FAR-OFF is always faster than OAKPLAN both considering the retrieval and the total adaptation time although also the OAKPLAN CPU-time is always lower than 0.6 seconds. Considering OAKPLAN, most of the CPU-time is devoted to the computation of the matching functions which are not computed by FAR-OFF since it simply considers the identity matching function that directly assigns the objects of the case base to those of the current planning problem with the same name. In fact, it does not consider objects which are not already present in the case base and, to overcome this limitation, the variants used in this test are directly obtained by the problems stored in the case bases.\nRegarding the plan qualities17 and the plan distances, it is important to point out that for each variant solved by OAKPLAN we consider only the first solution produced since FAR-OFF does not perform a plan optimisation process. However OAKPLAN is able to obtain better plans both considering the plan quality and the plan distance values. Globally, OAKPLAN is able to find plans with 20% better quality and 24% better plan distances. Moreover further improvements on plan qualities and distance values of OAKPLAN could be obtained by performing the optimisation process of LPG-adapt.\nFinally, note that in this experiment we have used the case bases provided by FAR-OFF which contain 700 elements each and the corresponding cases are generated by creating randomly planning problems all with the same configuration: same objects, trucks and airplanes simply disposed in different ways. This kind of experiment is highly unfavourable to OAKPLAN since our first screening procedure cannot filter out a significant number of cases as they all have the same structure. On the contrary, in the experiments described in the previous sections the case bases used by OAKPLAN in the standard configuration (not the \u201csmall\u201d versions) are not constrained to a particular planning problem but they have been generated by considering all the different planning problems configurations used in the International Planning Competitions. This is a much more realistic situation, where the cases are added to the case base when the planning problems provided by the users are resolved as time goes by.\n16We have used the case bases for the logistics-16-0, logistics-17-0 and logistics-18-0 Logistics IPC2 problems. For each problem considered the FAR-OFF system must have a case base with the same structure to perform tests. More than 700 cases belong to each case base and for each case base we have selected two planning cases and randomly generated 36 variants.\n17In STRIPS domains the plan quality is obtained by considering the number of actions in the solution plan."}], "references": [{"title": "Case-based reasoning: foundational issues, methodological variations, and system approaches", "author": ["A. Aamodt", "E. Plaza"], "venue": "AI Commun.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "An adaptive planner", "author": ["R. Alterman"], "venue": "J. Allen, J. Hendler, and A. Tate, editors, Readings in Planning, pages 660\u2013664. Kaufmann, San Mateo, CA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "On the complexity of plan adaptation by derivational analogy in a universal classical planning framework", "author": ["T. Au", "H. Mu\u00f1oz-Avila", "D.S. Nau"], "venue": "Proceedings of the 6th European Conference on Advances in Case-Based Reasoning, pages 13\u201327, London, UK", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Building and refining abstract planning cases by change of representation language", "author": ["R. Bergmann", "W. Wilke"], "venue": "Journal of Artificial Intelligence Research, 3:53\u2013118", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "editors", "author": ["S. Biundo", "K.L. Myers", "K. Rajan"], "venue": "Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiset theory", "author": ["W.D. Blizard"], "venue": "Notre Dame Journal of Formal Logic, 30(1):36\u201366", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "A replanning algorithm for a reactive agent architecture", "author": ["G. Boella", "R. Damiano"], "venue": "D. Scott, editor, AIMSA, volume 2443 of Lecture Notes in Computer Science, pages 183\u2013192. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "An average case analysis of planning", "author": ["T. Bylander"], "venue": "Proceedings of the Eleventh National Conference of the American Association for Artificial Intelligence (AAAI-93), pages 480\u2013485, Washington, D.C.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "The computational complexity of propositional STRIPS planning", "author": ["T. Bylander"], "venue": "Artificial Intelligence, 69:165\u2013204", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "A probabilistic analysis of propositional STRIPS planning", "author": ["T. Bylander"], "venue": "Artificial Intelligence, 81(1-2):241\u2013271", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Planning for conjunctive goals", "author": ["D. Chapman"], "venue": "Artificial Intelligence, 32(3):333\u2013377", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs", "author": ["M. Chein", "M. Mugnier"], "venue": "Springer Publishing Company, Incorporated", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Using many-sorted logic in the object-oriented data model for fast robot task planning", "author": ["Y.P. Chien", "A. Hudli", "M. Palakal"], "venue": "Journal of Intelligent and Robotic Systems, 23(1):1\u201325", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Many sorted logic=unsorted logic+control? In Proceedings of Expert Systems \u201986", "author": ["A.G. Cohn"], "venue": "The 6Th Annual Technical Conference on Research and development in expert systems III, pages 184\u2013194, New York, NY, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1987}, {"title": "O-plan: The open planning architecture", "author": ["K. Currie", "A. Tate"], "venue": "Artificial Intelligence, 52(1):49\u2013 86", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1991}, {"title": "and A", "author": ["B. Drabble", "J. Dalton"], "venue": "Tate. Repairing plans on the fly", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "HTN planning: Complexity and expressivity", "author": ["Kutluhan Erol", "James Hendler", "Dana S. Nau"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Plan stability: Replanning versus plan repair", "author": ["M. Fox", "A. Gerevini", "D. Long", "I. Serina"], "venue": "Proceedings of International Conference on AI Planning and Scheduling (ICAPS). AAAI Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Computers and Intractability : A Guide to the Theory of NP- Completeness (Series of Books in the Mathematical Sciences)", "author": ["M.R. Garey", "D.S. Johnson"], "venue": "W. H. Freeman", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1979}, {"title": "A survey of kernels for structured data", "author": ["T. G\u00e4rtner"], "venue": "SIGKDD Explor. Newsl., 5(1):49\u201358", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "The mechanisms of analogical learning", "author": ["D. Gentner"], "venue": "B. G. Buchanan and D. C. Wilkins, editors, Readings in Knowledge Acquisition and Learning: Automating the Construction and Improvement of Expert Systems, pages 673\u2013694. Kaufmann, San Mateo, CA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Planning through stochastic local search and temporal action graphs", "author": ["A. Gerevini", "A. Saetti", "I. Serina"], "venue": "Journal of Artificial Intelligence Research (JAIR), 20:pp. 239\u2013290", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Plan adaptation through planning graph analysis", "author": ["A. Gerevini", "I. Serina"], "venue": "Lecture Notes in Artificial Intelligence (AI*IA 99), pages 356\u2013367. Springer-Verlag", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast plan adaptation through planning graphs: Local and systematic search techniques", "author": ["A. Gerevini", "I. Serina"], "venue": "Proceedings of the 5th International Conference on Artificial Intelligence Planning and Scheduling (AIPS-00), pages 112\u2013121. AAAI Press/MIT Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Offline and online plan library maintenance in case-based planning", "author": ["A.E. Gerevini", "A. Roub\u00ed\u010dkov\u00e1", "A. Saetti", "I. Serina"], "venue": "Proceedings of the 13th Conference of the Italian Association for Artificial Intelligence. Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "On the plan-library maintenance problem in a case-based planner", "author": ["A.E. Gerevini", "A. Roub\u00ed\u010dkov\u00e1", "A. Saetti", "I. Serina"], "venue": "Proceedings of the 21st International Conference on Case-Based Reasoning. Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Explaining and repairing plans that fail", "author": ["K. Hammond"], "venue": "Artificial Intelligence, 45:173\u2013228", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1990}, {"title": "Systematic adaptation for case-based planning", "author": ["S. Hanks", "D.S. Weld"], "venue": "J. Hendler, editor, AIPS-92: Proc. of the First International Conference on Artificial Intelligence Planning Systems, pages 96\u2013105. Kaufmann, San Mateo, CA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1992}, {"title": "A domain-independent algorithm for plan adaptation", "author": ["S. Hanks", "D.S. Weld"], "venue": "Journal of Artificial Intelligence Research (JAIR), 2:319\u2013360", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Convolution kernels on discrete structures", "author": ["D. Haussler"], "venue": "Technical Report UCS-CRL-99-10, UC Santa Cruz", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "The FF planning system: Fast plan generation through heuristic search", "author": ["J. Hoffmann", "B. Nebel"], "venue": "Journal of Artificial Intelligence Research (JAIR), 14:253\u2013302", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "A theory of plan modification", "author": ["S. Kambhampati"], "venue": "In Proceedings of the Eighth National Conference on Artificial Intelligence", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1990}, {"title": "A validation-structure-based theory of plan modification and reuse", "author": ["S. Kambhampati", "J.A. Hendler"], "venue": "Artificial Intelligence, 55:193\u2013258", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1992}, {"title": "Marginalized kernels between labeled graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "T. Fawcett and N. Mishra, editors, ICML, pages 321\u2013328. AAAI Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "An analysis on transformational analogy: General framework and complexity", "author": ["V. Kuchibatla", "H. Mu\u00f1oz-Avila"], "venue": "ECCBR, volume 4106 of Lecture Notes in Computer Science, pages 458\u2013473. Springer", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Case-based similarity assessment: Estimating adaptability from experience", "author": ["D.B. Leake", "A. Kinley", "D.C. Wilson"], "venue": "Proceedings of the 14th National Conference on Artificial Intelligence -AAAI\u201997, pages 674\u2013679, Menlo Park, CA, USA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "On the complexity of case-based planning", "author": ["P. Liberatore"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence, 17(3):283\u2013295", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "J. W. Shavlik, editor, ICML, pages 296\u2013304. Morgan Kaufmann", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1998}, {"title": "Systematic nonlinear planning", "author": ["D. McAllester", "D. Rosenblitt"], "venue": "In Proceedings of the Ninth National Conference on Artificial Intelligence", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1991}, {"title": "Functions of positive and negative type and their connection with the theory of integral equations", "author": ["J. Mercer"], "venue": "Philos. Trans. Roy. Soc. London, A 209:415\u2013446", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1909}, {"title": "Case-based plan adaptation: An analysis and review", "author": ["H. Mu\u00f1oz-Avila", "M. Cox"], "venue": "IEEE Intelligent Systems, 23(4):75\u201381", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Feature weighting by explaining case-based planning episodes", "author": ["H. Mu\u00f1oz-Avila", "J. H\u00fcllen"], "venue": "EWCBR \u201996: Proceedings of the Third European Workshop on Advances in Case-Based Reasoning, pages 280\u2013294, London, UK", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1996}, {"title": "Plan reuse versus plan generation: A complexity-theoretic perspective", "author": ["B. Nebel", "J. Koehler"], "venue": "Artificial Intelligence- Special Issue on Planning and Scheduling, 76:427\u2013454", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1995}, {"title": "A convolution edit kernel for error-tolerant graph matching", "author": ["M. Neuhaus", "H. Bunke"], "venue": "volume 4, pages 220\u2013223, Washington, DC, USA", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Bridging the gap between Graph Edit Distance and Kernel Machines", "author": ["M. Neuhaus", "H. Bunke"], "venue": "World Scientific", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Some psychological results on case-based reasoning", "author": ["B.H. Ross"], "venue": "Proc. of a Workshop on Case-Based Reasoning, pages 144\u2013147, Pensacola Beach, FL", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning with Kernels: Support Vector Machines", "author": ["B. Scholkopf", "A.J. Smola"], "venue": "Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2001}, {"title": "A theory of debugging plans and interpretations", "author": ["R.G. Simmons"], "venue": "Proc. of AAAI-88, pages 94\u201399, St. Paul, MN", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1988}, {"title": "Adaptation-guided retrieval: Questioning the similarity assumption in reasoning", "author": ["B. Smyth", "M.T. Keane"], "venue": "Artificial Intelligence, 102(2):249\u2013293", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1998}, {"title": "Footprint-based retrieval", "author": ["B. Smyth", "E. McKenna"], "venue": "K. D. Althoff, R. Bergmann, and K. Branting, editors, ICCBR, volume 1650 of Lecture Notes in Computer Science, pages 343\u2013 357. Springer", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1999}, {"title": "Competence models and the maintenance problem", "author": ["Barry Smyth", "Elizabeth McKenna"], "venue": "Computational Intelligence,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "A survey on case-based planning", "author": ["L. Spalazzi"], "venue": "Artificial Intelligence Review, 16(1):3\u201336", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2001}, {"title": "Domain independent approaches for finding diverse plans", "author": ["B. Srivastava", "T.A. Nguyen", "A. Gerevini", "S. Kambhampati", "M.B. Do", "I. Serina"], "venue": "M. M. Veloso, editor, IJCAI, pages 2016\u2013 2022", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}, {"title": "Generating project networks", "author": ["A. Tate"], "venue": "Proceedings of the Fifth International Joint Conference on Artificial Intelligence (IJCAI-77), pages 888\u2013889, Cambridge, MA", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1977}, {"title": "The FAR-OFF system: A heuristic search case-based planning", "author": ["F. Tonidandel", "M. Rillo"], "venue": "M. Ghallab, J. Hertzberg, and P. Traverso, editors, AIPS, pages 302\u2013311. AAAI", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning by analogical reasoning in general problem solving", "author": ["M. Veloso"], "venue": "Technical report, CMU- CS-92-174, Department of Computer Science, Carnegie Mellon University", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1992}, {"title": "Planning and Learning by Analogical Reasoning", "author": ["M. Veloso"], "venue": "volume 886 of Lecture Notes in Artificial Intelligence and Lecture Notes in Computer Science. Springer-Verlag Inc., New York, USA", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1994}, {"title": "editors", "author": ["S. Vosniadou", "A. Ortony"], "venue": "Similarity and analogical reasoning. Cambridge University Press, New York, NY, USA", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1989}, {"title": "A mechanical solution of Schubert\u2019s steamroller by many-sorted resolution", "author": ["C. Walther"], "venue": "Artificial Intelligence, 26(2):217\u2013224", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1985}], "referenceMentions": [{"referenceID": 8, "context": "Similarly to Bylander\u2019s work [9], we define an instance of propositional planning problem as:", "startOffset": 29, "endOffset": 32}, {"referenceID": 11, "context": "Let O be a set of typed constants ci, with the understanding that distinct constants denote distinct objects (corresponding to individual entities following the Conceptual Graphs notation [12]).", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "Note that we use a many-sorted logic formalisation since it can significantly increase the efficiency of a deductive inference system by eliminating useless branches of the search space of a domain [13, 14, 61].", "startOffset": 198, "endOffset": 210}, {"referenceID": 13, "context": "Note that we use a many-sorted logic formalisation since it can significantly increase the efficiency of a deductive inference system by eliminating useless branches of the search space of a domain [13, 14, 61].", "startOffset": 198, "endOffset": 210}, {"referenceID": 58, "context": "Note that we use a many-sorted logic formalisation since it can significantly increase the efficiency of a deductive inference system by eliminating useless branches of the search space of a domain [13, 14, 61].", "startOffset": 198, "endOffset": 210}, {"referenceID": 8, "context": "Bylander [9] has defined PLANSAT as the decision problem of determining whether an instance \u03a0 of propositional STRIPS planning has a solution or not.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Severe restrictions on the form of the operators are necessary to guarantee polynomial time or even NP-completeness [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 36, "context": "Following the formalisation of Liberatore [37], we define a case base as follows:", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "Essential to this trivial result is that, similarly to most modern plan adaptation and casebased planning approaches [3, 24, 56, 57], we do not enforce plan adaptation to be conservative, in the sense that we do not require to reuse as much of the starting plan \u03c0\u2217 to solve the new plan.", "startOffset": 117, "endOffset": 132}, {"referenceID": 23, "context": "Essential to this trivial result is that, similarly to most modern plan adaptation and casebased planning approaches [3, 24, 56, 57], we do not enforce plan adaptation to be conservative, in the sense that we do not require to reuse as much of the starting plan \u03c0\u2217 to solve the new plan.", "startOffset": 117, "endOffset": 132}, {"referenceID": 54, "context": "Essential to this trivial result is that, similarly to most modern plan adaptation and casebased planning approaches [3, 24, 56, 57], we do not enforce plan adaptation to be conservative, in the sense that we do not require to reuse as much of the starting plan \u03c0\u2217 to solve the new plan.", "startOffset": 117, "endOffset": 132}, {"referenceID": 8, "context": "The computational complexity of plan Reuse and Modification for STRIPS planning problems has been analysed in a number of papers [9, 10, 35, 37, 44].", "startOffset": 129, "endOffset": 148}, {"referenceID": 9, "context": "The computational complexity of plan Reuse and Modification for STRIPS planning problems has been analysed in a number of papers [9, 10, 35, 37, 44].", "startOffset": 129, "endOffset": 148}, {"referenceID": 34, "context": "The computational complexity of plan Reuse and Modification for STRIPS planning problems has been analysed in a number of papers [9, 10, 35, 37, 44].", "startOffset": 129, "endOffset": 148}, {"referenceID": 36, "context": "The computational complexity of plan Reuse and Modification for STRIPS planning problems has been analysed in a number of papers [9, 10, 35, 37, 44].", "startOffset": 129, "endOffset": 148}, {"referenceID": 42, "context": "The computational complexity of plan Reuse and Modification for STRIPS planning problems has been analysed in a number of papers [9, 10, 35, 37, 44].", "startOffset": 129, "endOffset": 148}, {"referenceID": 7, "context": "Moreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient than plan generation in the average case [8, 18, 23, 24, 44, 54, 57].", "startOffset": 155, "endOffset": 182}, {"referenceID": 17, "context": "Moreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient than plan generation in the average case [8, 18, 23, 24, 44, 54, 57].", "startOffset": 155, "endOffset": 182}, {"referenceID": 22, "context": "Moreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient than plan generation in the average case [8, 18, 23, 24, 44, 54, 57].", "startOffset": 155, "endOffset": 182}, {"referenceID": 23, "context": "Moreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient than plan generation in the average case [8, 18, 23, 24, 44, 54, 57].", "startOffset": 155, "endOffset": 182}, {"referenceID": 42, "context": "Moreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient than plan generation in the average case [8, 18, 23, 24, 44, 54, 57].", "startOffset": 155, "endOffset": 182}, {"referenceID": 52, "context": "Moreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient than plan generation in the average case [8, 18, 23, 24, 44, 54, 57].", "startOffset": 155, "endOffset": 182}, {"referenceID": 5, "context": "Note that our label function considers multisets of symbolic labels, with the corresponding operations of union, intersection and join [6], since in our context they are more suitable than standard sets of symbolic labels in order to compare vertices or edges accurately as described later.", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "where \u228e indicates the join, sometimes called sum, of two multisets [6], while \u03bb(\u22c5) associates a multiset of symbolic labels to a vertex or to an edge.", "startOffset": 67, "endOffset": 70}, {"referenceID": 18, "context": "As it is well known, subgraph isomorphism and MCS between two or among more graphs are NP-complete problems [19], while it is still an open question if also graph isomorphism is an NP-complete problem.", "startOffset": 108, "endOffset": 112}, {"referenceID": 46, "context": "Kernel machines, a new class of algorithms for pattern analysis and classification, offer an elegant solution to this problem [48].", "startOffset": 126, "endOffset": 130}, {"referenceID": 46, "context": "A kernel function can be thought of as a special similarity measure with well defined mathematical properties [48].", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "Moreover a kernel function implicitly defines a dot product in some space [48]; i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 46, "context": "From a technical point of view a kernel function is a special similarity measure k \u2236 X \u00d7X \u2192 IR between patterns lying in some arbitrary domainX , which represents a dot product, denoted by \u27e8\u22c5, \u22c5\u27e9, in some Hilbert space H [48]; thus, for two arbitrary patterns x,x\u2032 \u2208 X it holds that k(x,x\u2032) = \u27e8\u03c6(x), \u03c6(x\u2032)\u27e9, where \u03c6 \u2236 X \u2192 H is an arbitrary mapping of patterns from the domain X into the feature space H.", "startOffset": 221, "endOffset": 225}, {"referenceID": 19, "context": "A positive definite kernel [20] is:", "startOffset": 27, "endOffset": 31}, {"referenceID": 39, "context": "Theorem 1 ( Mercer\u2019s property [41]) For any positive definite kernel function k \u2208 IRX\u00d7X , there exists a mapping \u03c6 \u2208 HX into the feature space H equipped with the inner product \u27e8\u22c5, \u22c5\u27e9H, such that: \u2200u, v \u2208 X , k(u, v) = \u27e8\u03c6(u), \u03c6(v)\u27e9H", "startOffset": 30, "endOffset": 34}, {"referenceID": 46, "context": "More precisely, they are closed under sum, direct sum, multiplication by a scalar, tensor product, zero extension, pointwise limits, and exponentiation [48].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "A remarkable contribution to graph kernels is the work on convolution kernels, that provides a general framework to deal with complex objects consisting of simpler parts [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 29, "context": "where u = {u1, u2} refers to a partition of u into two substructures u1 and u2 [30, 48].", "startOffset": 79, "endOffset": 87}, {"referenceID": 46, "context": "where u = {u1, u2} refers to a partition of u into two substructures u1 and u2 [30, 48].", "startOffset": 79, "endOffset": 87}, {"referenceID": 33, "context": "directed/undirected, labeled/unlabeled, paths/trees/cycles, deterministic/random walks) and various ways of listing and counting them [34, 45, 46].", "startOffset": 134, "endOffset": 146}, {"referenceID": 43, "context": "directed/undirected, labeled/unlabeled, paths/trees/cycles, deterministic/random walks) and various ways of listing and counting them [34, 45, 46].", "startOffset": 134, "endOffset": 146}, {"referenceID": 44, "context": "directed/undirected, labeled/unlabeled, paths/trees/cycles, deterministic/random walks) and various ways of listing and counting them [34, 45, 46].", "startOffset": 134, "endOffset": 146}, {"referenceID": 46, "context": "For an introduction to kernel functions related concepts and notation, the reader is referred to Scholkopf and Smola\u2019s book [48].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "CBP is a type of case-based reasoning, which involves the use of stored experiences (cases); moreover there is strong evidence that people frequently employ this kind of analogical reasoning [21, 47, 60].", "startOffset": 191, "endOffset": 203}, {"referenceID": 45, "context": "CBP is a type of case-based reasoning, which involves the use of stored experiences (cases); moreover there is strong evidence that people frequently employ this kind of analogical reasoning [21, 47, 60].", "startOffset": 191, "endOffset": 203}, {"referenceID": 57, "context": "CBP is a type of case-based reasoning, which involves the use of stored experiences (cases); moreover there is strong evidence that people frequently employ this kind of analogical reasoning [21, 47, 60].", "startOffset": 191, "endOffset": 203}, {"referenceID": 0, "context": "Similarly to the Aamodt & Plaza\u2019s classic model of the problem solving cycle in CBR [1], Figure 3 shows the main steps of our case-based planning cycle and the interactions of the different steps with the case base.", "startOffset": 84, "endOffset": 87}, {"referenceID": 42, "context": "Following Nebel & Koehler\u2019s formalisation [44], we will have a closer look at this matching problem.", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "In order to measure the similarity between two objects, it is intuitive and usual to compare the features which are common to both objects [39].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "Finally we define the following optimisation problem, which we call obj_match: Instance: Two planning instances, \u03a0\u2032 and \u03a0, and a real number k \u2208 [0,1].", "startOffset": 145, "endOffset": 150}, {"referenceID": 42, "context": "Unfortunately, similarly to Nebel & Koehler\u2019s analysis [44], it is quite easy to show that this matching problem is an NP-hard problem.", "startOffset": 55, "endOffset": 59}, {"referenceID": 41, "context": "In fact existing similarity metrics address the problem heuristically, considering approximations of it [43, 58].", "startOffset": 104, "endOffset": 112}, {"referenceID": 55, "context": "In fact existing similarity metrics address the problem heuristically, considering approximations of it [43, 58].", "startOffset": 104, "endOffset": 112}, {"referenceID": 11, "context": "The representation of an entity (an object using planning terminology) of the application domain is traditionally called a concept in the conceptual graph community [12].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "Bestaction(g) is the action that is heuristically chosen to support g as described in [22].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "With respect to normal conceptual graphs [12] used for Graph-based Knowledge Representation, we use a richer label representation based on multisets.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "The inconsistencies related to unsupported facts are evaluated by computing a relaxed plan starting from the corresponding state and using the RELAXEDPLAN algorithm in LPG [22].", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "RELAXEDPLAN is described in detail in [22].", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "In the relaxed planning graph analysis the negative effects of the domain operators are not considered and a solution plan \u03c0R of a relaxed planning problem can be computed in polynomial time [31].", "startOffset": 191, "endOffset": 195}, {"referenceID": 17, "context": "For example the need for adapting a precomputed plan can arise in a dynamic environment when the execution of a planned action fails, when the new information changing the description of the world prevents the applicability of some planned actions, or when the goal state is modified by adding new goals or removing existing ones [18, 22].", "startOffset": 330, "endOffset": 338}, {"referenceID": 21, "context": "For example the need for adapting a precomputed plan can arise in a dynamic environment when the execution of a planned action fails, when the new information changing the description of the world prevents the applicability of some planned actions, or when the goal state is modified by adding new goals or removing existing ones [18, 22].", "startOffset": 330, "endOffset": 338}, {"referenceID": 26, "context": "Different approaches have been considered in the literature for plan adaptation; strategies vary from attempting to reuse the structure of an existing plan by constructing bridges that link together the fragments of the plan that fail in the face of new initial conditions [27, 28, 29, 32], to more dynamic plan modification approaches that use a series of plan modification operators to attempt to repair a plan [38, 57].", "startOffset": 273, "endOffset": 289}, {"referenceID": 27, "context": "Different approaches have been considered in the literature for plan adaptation; strategies vary from attempting to reuse the structure of an existing plan by constructing bridges that link together the fragments of the plan that fail in the face of new initial conditions [27, 28, 29, 32], to more dynamic plan modification approaches that use a series of plan modification operators to attempt to repair a plan [38, 57].", "startOffset": 273, "endOffset": 289}, {"referenceID": 28, "context": "Different approaches have been considered in the literature for plan adaptation; strategies vary from attempting to reuse the structure of an existing plan by constructing bridges that link together the fragments of the plan that fail in the face of new initial conditions [27, 28, 29, 32], to more dynamic plan modification approaches that use a series of plan modification operators to attempt to repair a plan [38, 57].", "startOffset": 273, "endOffset": 289}, {"referenceID": 31, "context": "Different approaches have been considered in the literature for plan adaptation; strategies vary from attempting to reuse the structure of an existing plan by constructing bridges that link together the fragments of the plan that fail in the face of new initial conditions [27, 28, 29, 32], to more dynamic plan modification approaches that use a series of plan modification operators to attempt to repair a plan [38, 57].", "startOffset": 273, "endOffset": 289}, {"referenceID": 42, "context": "From a theoretical point of view, in the worst case, plan adaptation is not more efficient than a complete regeneration of the plan [44] when a conservative adaptation strategy is adopted.", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "efficient than generating a new one from scratch, and, in addition, this worst case scenario does not always hold, as exposed in [3] for the Derivation Analogy adaptation approach.", "startOffset": 129, "endOffset": 132}, {"referenceID": 17, "context": "It is important to point out that this paper relates to the description of a new efficient case-based planner and in particular to the definition of effective plan matching functions, no significant changes were made to the plan adaptation component (for a detailed description of it see [18]).", "startOffset": 288, "endOffset": 292}, {"referenceID": 49, "context": "The case base maintenance is clearly important for the performance of the system and different strategies have been proposed in the literature [51, 56].", "startOffset": 143, "endOffset": 151}, {"referenceID": 54, "context": "The case base maintenance is clearly important for the performance of the system and different strategies have been proposed in the literature [51, 56].", "startOffset": 143, "endOffset": 151}, {"referenceID": 24, "context": "Recently, the system was extended with a set of maintenance policies guided by the cases\u2019 similarity, as is described in [25, 26].", "startOffset": 121, "endOffset": 129}, {"referenceID": 25, "context": "Recently, the system was extended with a set of maintenance policies guided by the cases\u2019 similarity, as is described in [25, 26].", "startOffset": 121, "endOffset": 129}, {"referenceID": 50, "context": "In short, UPDATELIBRARY identifies the subplans of \u03c0 that can be inserted in the plan library to increase the competence of the library in itself [52, 56].", "startOffset": 146, "endOffset": 154}, {"referenceID": 54, "context": "In short, UPDATELIBRARY identifies the subplans of \u03c0 that can be inserted in the plan library to increase the competence of the library in itself [52, 56].", "startOffset": 146, "endOffset": 154}, {"referenceID": 23, "context": "In the literature there are different domain dependent and a few domain independent plan adaptation and case-based planning systems, which mostly use a search engine based on a space of states [24, 29, 56, 57].", "startOffset": 193, "endOffset": 209}, {"referenceID": 28, "context": "In the literature there are different domain dependent and a few domain independent plan adaptation and case-based planning systems, which mostly use a search engine based on a space of states [24, 29, 56, 57].", "startOffset": 193, "endOffset": 209}, {"referenceID": 54, "context": "In the literature there are different domain dependent and a few domain independent plan adaptation and case-based planning systems, which mostly use a search engine based on a space of states [24, 29, 56, 57].", "startOffset": 193, "endOffset": 209}, {"referenceID": 3, "context": "An alternative approach to planning with states is that of plan-space planning or hierarchical systems [4] that search in a space of plans and have no goals, but only tasks to be achieved.", "startOffset": 103, "endOffset": 106}, {"referenceID": 51, "context": "For a detailed analysis of case-based and plan adaptation techniques see the papers of Spalazzi [53] and Munoz-Avila & Cox [42].", "startOffset": 96, "endOffset": 100}, {"referenceID": 40, "context": "For a detailed analysis of case-based and plan adaptation techniques see the papers of Spalazzi [53] and Munoz-Avila & Cox [42].", "startOffset": 123, "endOffset": 127}, {"referenceID": 26, "context": "The CHEF system [27] is the first application of CBR in planning and it is a reuse-only system which is important especially from a historical point of view.", "startOffset": 16, "endOffset": 20}, {"referenceID": 35, "context": "It focuses on choosing the most adaptable case as the most similar one, such as the DIAL [36] and D\u00c9J\u00c0VU [50] systems.", "startOffset": 89, "endOffset": 93}, {"referenceID": 48, "context": "It focuses on choosing the most adaptable case as the most similar one, such as the DIAL [36] and D\u00c9J\u00c0VU [50] systems.", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "The PLEXUS system [2] confronts with the problem of \u201cadaptive planning\u201d, but also addresses the problem of runtime adaptation to plan failure.", "startOffset": 18, "endOffset": 21}, {"referenceID": 47, "context": "The GORDIUS [49] system is a transformational planner that combines small plan fragments for different (hopefully independent) aspects of the current problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": "Three interesting works developed at the same time adopt similar assumptions: the PRIAR system [33], the SPA system [29] and the Prodigy/Analogy system [58, 59].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "Three interesting works developed at the same time adopt similar assumptions: the PRIAR system [33], the SPA system [29] and the Prodigy/Analogy system [58, 59].", "startOffset": 116, "endOffset": 120}, {"referenceID": 55, "context": "Three interesting works developed at the same time adopt similar assumptions: the PRIAR system [33], the SPA system [29] and the Prodigy/Analogy system [58, 59].", "startOffset": 152, "endOffset": 160}, {"referenceID": 56, "context": "Three interesting works developed at the same time adopt similar assumptions: the PRIAR system [33], the SPA system [29] and the Prodigy/Analogy system [58, 59].", "startOffset": 152, "endOffset": 160}, {"referenceID": 53, "context": "PRIAR uses a variant of Nonlin [55], a hierarchical planner, whereas SPA uses a constraint posting technique similar to Chapman\u2019s Tweak [11] as modified by McAllester and Rosenblitt [40].", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "PRIAR uses a variant of Nonlin [55], a hierarchical planner, whereas SPA uses a constraint posting technique similar to Chapman\u2019s Tweak [11] as modified by McAllester and Rosenblitt [40].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "PRIAR uses a variant of Nonlin [55], a hierarchical planner, whereas SPA uses a constraint posting technique similar to Chapman\u2019s Tweak [11] as modified by McAllester and Rosenblitt [40].", "startOffset": 182, "endOffset": 186}, {"referenceID": 41, "context": "An interesting similarity rule in the plan-space approach is presented in the CAPLAN/CBC system [43] which extends the similarity rule introduced by the Prodigy/Analogy system [58, 59] by using feature weights in order to reduce the errors in the retrieval phase.", "startOffset": 96, "endOffset": 100}, {"referenceID": 55, "context": "An interesting similarity rule in the plan-space approach is presented in the CAPLAN/CBC system [43] which extends the similarity rule introduced by the Prodigy/Analogy system [58, 59] by using feature weights in order to reduce the errors in the retrieval phase.", "startOffset": 176, "endOffset": 184}, {"referenceID": 56, "context": "An interesting similarity rule in the plan-space approach is presented in the CAPLAN/CBC system [43] which extends the similarity rule introduced by the Prodigy/Analogy system [58, 59] by using feature weights in order to reduce the errors in the retrieval phase.", "startOffset": 176, "endOffset": 184}, {"referenceID": 14, "context": "O-Plan [15, 16] is based on the strategy of using plan repair rules as well.", "startOffset": 7, "endOffset": 15}, {"referenceID": 15, "context": "O-Plan [15, 16] is based on the strategy of using plan repair rules as well.", "startOffset": 7, "endOffset": 15}, {"referenceID": 42, "context": "MLR [44] is another case-based system and it is based on a proof system.", "startOffset": 4, "endOffset": 8}, {"referenceID": 42, "context": "Nebel & Koehler [44] present an interesting comparison of the MLR, SPA and PRIAR performance in the BlocksWorld domain considering planning instances with up to 8 blocks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "The Replan [7] model of plans is similar to the plans used in the hierarchical task network (HTN) formalism [17].", "startOffset": 11, "endOffset": 14}, {"referenceID": 16, "context": "The Replan [7] model of plans is similar to the plans used in the hierarchical task network (HTN) formalism [17].", "startOffset": 108, "endOffset": 112}, {"referenceID": 54, "context": "A very interesting case-based planner is the FAR-OFF15 (Fast and Accurate Retrieval on Fast Forward) system [56].", "startOffset": 108, "endOffset": 112}, {"referenceID": 30, "context": "It uses a generative planning system based on the FF planner [31] to adapt similar cases and a similarity metric, called ADG (Action Distance-Guided), which, like EVALUATEPLAN, determines the adaptation effort by estimating the number of actions that is necessary to transform a case into a solution of the problem.", "startOffset": 61, "endOffset": 65}, {"referenceID": 49, "context": "The FAR-OFF system uses a new competence-based method, called Footprint-based Retrieval [51], to reduce the space of cases that will be evaluated by ADG.", "startOffset": 88, "endOffset": 92}, {"referenceID": 49, "context": "Each footprint case has a set of similar cases called Related Set [51].", "startOffset": 66, "endOffset": 70}], "year": 2013, "abstractText": "Case-based planning can take advantage of former problem-solving experiences by storing in a plan library previously generated plans that can be reused to solve similar planning problems in the future. Although comparative worst-case complexity analyses of plan generation and reuse techniques reveal that it is not possible to achieve provable efficiency gain of reuse over generation, we show that the case-based planning approach can be an effective alternative to plan generation when similar reuse candidates can be chosen.", "creator": "TeX"}}}