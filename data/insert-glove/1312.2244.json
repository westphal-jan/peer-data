{"id": "1312.2244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2013", "title": "Time-dependent Hierarchical Dirichlet Model for Timeline Generation", "abstract": "Timeline Generation aims mosto at summarizing news chartwell from different 17,250 epochs trc and telling readers how an cuneo event 13-minute evolves. 39.79 It is tapuskovic a femoralis new perotistas challenge that muwaffaq combines spessart salience ranking merhige with ack novelty vallehermoso detection. hanza For long - term stationed public 97.75 events, the instilled main ifw topic usually includes 1-800-253-6476 various aspects across different englishsptimes.com epochs liotta and each miren aspect kamboh has pratten its own evolving antidiuretic pattern. solloway Existing santolan approaches neglect preto such queralt hierarchical friendsville topic troiano structure andru involved in tranquillitatis the news corpus abourezk in timeline temmu generation. test-playing In grayrigg this yuria paper, we voislav develop a zailckas novel concurso time - dependent Hierarchical Dirichlet Model (fatherly HDM) valeur for kaercher timeline autocracies generation. kathoey Our gelsenkirchen model can aptly renacimiento detect ausseill different priester levels doyne of bechly topic hiri information impelled across rungkat corpus withe and steeler such 2,110 structure eluard is vollebaek further used fortepiano for sentence wbns-tv selection. Based on the 1.3128 topic mined fro HDM, sentences are bonavista selected subalpine by considering munakata different aspects such as relevance, coherence and coverage. launchings We develop lycos experimental systems tutted to evaluate 8 mundine long - term events that azeotropic public maniatis concern. Performance kinglets comparison things between different hasselblad systems bsaa demonstrates the bhang effectiveness sumaye of gulbrandsen our model grunted in terms dextran of based ROUGE calamai metrics.", "histories": [["v1", "Sun, 8 Dec 2013 19:15:15 GMT  (635kb,D)", "http://arxiv.org/abs/1312.2244v1", null], ["v2", "Mon, 21 Apr 2014 11:44:47 GMT  (1146kb,D)", "http://arxiv.org/abs/1312.2244v2", null], ["v3", "Tue, 14 Mar 2017 01:34:50 GMT  (1598kb,D)", "http://arxiv.org/abs/1312.2244v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["rumeng li", "tao wang", "xun wang"], "accepted": false, "id": "1312.2244"}, "pdf": {"name": "1312.2244.pdf", "metadata": {"source": "CRF", "title": "Time-dependent Hierarchical Dirichlet Model for Timeline Generation", "authors": ["Tao Wang"], "emails": ["pwangtao@whu.edu.cn"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors I.7.5 [Document Capture]: [Document Analysis]\nGeneral Terms Design, Algorithms, Theory, Experimentation\nKeywords Timeline generation, Dirichlet Process, Topic Model"}, {"heading": "1. INTRODUCTION", "text": "Faced with thousands of news articles, readers usually try to ask the general questions such as the beginning, the evolutionary pattern and the end of an event. General search engines simply return top ranking articles according to query relevance and fail to trace how a specific event goes. Even though the documents can be ranked in a chronological order, readers are tired of reading the overwhelming number\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nof documents in details. Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.\nTimeline generation for an evolving topic cares for not only the salience, but also the novelty and evolutionary process of the events. In this paper, we focus on extractive approach for timeline generation, which tries to select desired sentences from news corpus. There are two main challenges for long-term timeline generation. First, for extractive methods, we want to select the sentences that can best generalize topics at each time, so how to model information hidden within the news corpus becomes important. As for long-term event, one main topic is usually described from various specific aspects and each aspect has its own evolutionary pattern. For example, the news about \u201cGreek Debtgovernment Crisis\u201c may involve different sub-events such as \u201cthe cause of debt crisis\u201c, \u201cGreek public protests\u201c, \u201cRescue packages\u201c while each of them contains different stages. Detecting such hierarchical structure is important for later sentence selection. Second, it is very common that the themes of a corpus evolve over time, and thus topics of adjacent time epochs usually exhibit strong correlations. How to incorporate time dependency into the hierarchical structure is also a hard problem.\nThe problem of timelime generation was firstly proposed by Allan et al.[27] by extracting clusters of noun phases and name entities. Later they built up a system to provide timelines which consist of one sentence per date by considering usefulness and novelty[3]. Chieu et al.[10] built a similar system in unit of sentences with interest and burstiness. However, these methods seldom explored the evolutionary characteristics of news. Recently, Yan et al.[38] extended the graph based sentence ranking algorithm used in traditional multi-document summarization (MDS) to timeline generation by projecting sentences from different times into one plane. They further explored the timeline task from the optimization of a function by considering the combination of different respects such as relevance, coverage, coherence and diversity[40]. Time dependency is considered in Yan et al\u2019s work[38, 40]. However, their approaches treat timeline generation as a sentence ranking or optimization problem and seldom explore the topic information lied within the corpus or the structure of news information.\nTopic detection or mining is not a new task and many algorithms have been proposed in TDT (Topic Detection and Tracking) task[11, 13], or summarization task[32, 18]. Among existing approaches, topic models such as Latent Dirichlet Allocation (LDA)[6] or Hierarchical Dirichlet Pro-\nar X\niv :1\n31 2.\n22 44\nv1 [\ncs .C\nL ]\n8 D\nec 2\n01 3\ncesses (HDP) have their advantages [29] in capturing latent topics within document collection due its clear probabilistic interpretions.\nAs we get back to the two challenges previously discussed, we find that topic models show promising properties to to handle them: First, Blei et al. [15] introduced a hLDA model which organizes topics into a tree structure with depth L. Topics located close to each other in the tree tend to share a similar topic-word distribution and the children of a certain node tend to be treated as subtopics talking about minor aspects. Such tree structure can well help model the hierarchical topic structure in the long term event. One major problem hLDA is that LDA based models require users to specify a cluster number at each time epoch for each corpus, which is awkward in real case. In addition, the timedependent nature of new corpus data can not be addressed in hLDA model. Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].\nIn this paper, we develop a novel topic approach denoted as time-dependent Hierarchical Dirichlet Model (HDM) for timeline generation. Our HDM model can be viewed as an extended combination of hLDA and multi-level timecorrelated Dirichlet Process (DP) and tries to model the topic distribution in a hierarchical tree structure. Such structure is famed as a time-correlated Dirichlet process in both horizontal (how many children does a node have) and vertical (how deep is the tree) direction. We approach HDM by specifying a generative probabilistic model for hierarchical structure and taking a Bayesian perspective on the problem of learning the tree structure automatically from the data. The non-parametric nature of DP means that we do not have to know a priori the number of topics or depth as they will readily accommodates growing data collections. From that structure, we can clearly identify the evolution pattern of different aspects involved in the event by locating the topic position in the tree and further, based on that, we construct a timeline ranking framework by considering the relevance, coherence and coverage of each sentence. We build an experimental system on 8 real long-term events that public concern. The effectiveness of model is verified through ROUGE matrix.\nThe rest of paper is organized as follows. Section 2 describes related work. Section 3 briefly illustrates Dirichlet Process (DP) and Hierarchical Dirichlet Processes (HDP). The details of HDM is shown in Section 4. We present experimental results in Section 5 and conclude this paper in Section 6."}, {"heading": "2. RELATED WORK", "text": "Timeline Generation. The task of timeline generation is firstly proposed by Swan and Allan[27] by using named entities for sentence selection in different time epochs. Since then, researchers tried to generate timeline by considering different respects such as usefulness and novelty[3] or interest and burstiness[10]. Yan et al.[39] extended the graph based sentence ranking algorithm in multi-document summarization (MDS) to timeline generation by projecting sentences from different temporal corpus into one plane. Another work from Yan et al.[38] transforms\nthis task to an optimization problem by considering the relevance, coverage and coherence of the sentences. Existing approaches seldom explore the topic information lied in the corpus or the structure of news information. Li et al [17] exploited topic model to capture the dynamic evolution of topics in timeline generation.\nTopic Models. Recently, bayesian topic models such as LDA[6] or HDP[29] have shown the power text mining for its clear probabilistic interpretation. In topic model, each document is denoted as a mixture of different topics and each topic is presented as a distribution over words. hLDA [15] extends LDA model to a multi-level tree structure where each node is associated with a topic distribution over words. It can be inferred from a nested Chinese restaurant process based one MCMC strategy. HDP can be used as an LDA-based topic model where the number of clusters can be automatically inferred from data. Therefore, HDP is more practical when users have little knowledge about the content to be analyzed.\nLearning from time-correlated corpus. Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37]. Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] . In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42]. For instance, Wang et al. [34] focused on detecting the simultaneous busting of some topics in multiple text streams. Zhang et al. [35] further extended [34] where they adjusted the timestamps of all documents to synchronize multiple streams and then learned a common topic model. Wang et al. [42] introduced an approach based on Hierarchical Dirichlet Process [29] to discover interesting cluster evolutionary pattern from correlated time-varying text corpora."}, {"heading": "3. DP AND HDP", "text": "Dirichlet Process(DP) can be considered as a distribution over distributions[12]. A DP denoted by DP (\u03b1,G0) is parameterized by a base measure G0 and a concentration parameter. We write G \u223c DP (\u03b1,G0) for a draw of distribution G from the Dirichlet process. G itself is a distribution over a given parameter space \u03b8, therefore we can draw parameters \u03b81:N from G following a Polya urn distribution [4] also known as a Chinese Restaurant process [29] or through a stick breaking construction [26]. Under the framework of Gibbes sampling, we have:\n\u03b8i|\u03b81:i\u22121, G0, \u03b1 \u223c \u2211 k mk i\u2212 1 + \u03b1\u03b4(\u03c6k) +\n\u03b1\ni\u2212 1 + \u03b1G0 (1)\nwhere \u03c61:k denotes the distinct values among parameter \u03b8 and mk is the number of parameters \u03b8 having value \u03c6k.\nAs for HDP, we model each document as a DP and each word w in document d will be associated with a topic sampled from the random Gd, where Gd \u223c DP (\u03b1,G0). The random measure Gd represents the document-specific mixing vector over infinite number of topics. In HDP, the document specific measures Gd are tied together by modeling the\nbase measure G0 itself as a random measure sampled from DP (\u03b3,H).\nHDP model can be more easily explained from the Chinese Restaurant Process metaphor. where each document is referred to a restaurant and words are compared to customers. Customers in the restaurant sit around different tables and each table b is associated with a dish (topic) \u03c6b according to the dish menu. To associate a topic with customer w, we proceed as follows: In restaurant d, the customer can sit on table bdb that has ndb with probability ndb u\u22121+\u03b1 and shares the dish \u03c6db or picks a new table b new with probability \u03b1 i\u22121+\u03b1 . If he chooses a new table, he has to order a dish for that table from the global menu. A dish \u03c6k that is served at mk across all the restaurants is ordered from the global menu with probability mk\u2211\nk\u2032 mk\u2032+\u03b3 , or a new dish knew\nthat has never been served in any restaurant with probability \u03b3\u2211\nk\u2032 mk\u2032+\u03b3 and \u03c6new \u223c H. Let \u03b8di denotes the dish\nassociated with customer wdi, then putting them together we have\n\u03b8di|\u03b8d,1:i\u22121,\u03b1,\u03c6 \u223c \u2211 b ndb i\u2212 1 + \u03b1\u03b4\u03c6db +\n\u03b1\ni\u2212 1 + \u03b1\u03b4\u03c6 new db (2)\n\u03c6newdb \u223c \u2211 k mk\u2211 k\u2032 mk\u2032 + \u03b3 \u03b4\u03c6k + \u03b3\u2211 k\u2032 mk\u2032 + \u03b3 H (3)\nFigure 1: Graphical representation of (a) DP and (b) HDP"}, {"heading": "4. HDM FOR TIMELINE GENERATION", "text": ""}, {"heading": "4.1 Problem Formulation", "text": "Here we firstly give a standard formulation of the task. Given a query, Q = {wqi}i=ni=1 , where wqi is the word in the query, we get a set of query related documents from the Internet. The corpus is divided into a series of document collections according to the published time as C = {Ct}t=Tt=1 , where Ct = {Dti}i=N t\ni=1 , corresponding to the document collection published at time t. Dti denotes document i at time t and Nt denotes the number of documents published at time t. Document Dti is formulated as a collection of words Dti = {wtin} n=Nti n=1 . V denotes the vocabulary size. The output of the algorithm is a series of timelines I = {It}t=Tt=1 and It \u2282 Ct."}, {"heading": "4.2 HDM", "text": "HDM can be viewed as an extension of Hierarchical LDA model [15] where we wish to discover common usage patterns (or topics) given a collection of documents and try to organize these topics into a hierarchy. However, HDM is different from hLDA for its non-parametric nature and capability of modeling time-correlated data.\nHDM can be better explained from a Chinese restaurant perspective. There are a series of restaurants in the city and each restaurant is associated with a restaurant-specific dish menu. Such menu is drawn from a global dish menu shared across all restaurants in the whole city. In each restaurant, there are infinite number of tables. When a customer comes\ninto the restaurant, he would choose a table and a dish according the local restaurant dish menu. An demonstration of restaurant menu is illustrated in Figure 3. From Figure 3, we observe that the Chinese restaurant interpretation for HDM is different from other topic models such as LDA, HDP and sLDA in that the dishes in the menu are organized as a hierarchical tree structure. The intuitive idea for the construction of such menu is that it can be used to represent the hierarchical topic structure in the corpus. For instance, the route of meat \u2192 pork\u2192 tenderloin \u2192 grilled tenderloin can be compared to the hierarchical structure of topic \u2192 subtopic \u2192 three-level topic \u2192 forth-level topic described in Section 1, say Greek crisis \u2192 Greek public protests \u2192 Indignant Citizens Movement \u2192 people get killed.\nThere is a pool of dishes H, from which the everyday city dish menu Gt is drawn, where t denotes the index of the day. It is worth noting that the global dish menu changes each day in that residents in the city are fed up with having the same dishes every day. Another thing to mention is residents in the town would preserve part of dishes they already had yesterday on today\u2019s menu since some of them are so delicious and they want to have them again. So Gt, which denotes the city dish menu for day t is drawn from the combination of base menu H and yesterday\u2019s menu Gt\u22121 as follows:\nGt \u223c DP (\u03b1, F (\u2206)Gt\u22121 + (1\u2212 F (\u2206))H) (4)\nwhere F (\u2206) = exp(\u2212\u2206/\u03bb), and it controls the influence of neighboring data. \u03bb is the decay factor of the time-decaying kernel. Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].\nEach restaurant has its own specific menu, which is drawn from that day\u2019s global menu Gt with G i t \u223c DP (\u03b3,Gt). For each day, the customer would enter a restaurant and choose a dish according to the restaurant dish menu Git, where i denotes the index of the restaurant. First, he would decide the table he would sit at according to the dish served at that table. Since the customer is indecisive, he can not specifically figure out what he wants to eat immediately, but can tell the category of food he wants, say meat or fish (See Figure 3). So he first chooses from pizza, meat or fish according to HDP model, shown in Section 2. If he decides to have meat today, he would further decide what kind of meat he would like to have, turkey, chicken or pork, also according to HDP algorithm. Such process goes on and on until he finally get to the end of menu. It is worth noting that, the costumer\nis a bit critical since he may not feel like having any type of dish (or sub-dishes) appeared on the restaurant menu. If that is the case, the servant would give him the city menu Gt, letting him choose and then again base menu H if he is still not satisfied. The problem is, sometimes the customer is so critical that even the global menu H can not satisfy him. In that case, the cook in the restaurant would invent a new kind of meat (if he already decided to have meat) or a new cooking method for tenderloin (if he already decided to have tenderloin) and then add this newly invented dish to the menu list.\nGetting back to our data, each sentence is compared to customer in the Chinese Restaurant metaphor1, document to restaurant and document collection at each time to restaurants at that day. For each sentence s, it is linked with a vector of dish (topic) \u03b81, \u03b82, ..., \u03b8s. \u03b8l l \u2208 [1, s] is drawn successively from Git given \u03b8l\u22121. In Chinese restaurant metaphor, \u03b81 can be compared to meat, \u03b82 to pork, \u03b83 to tenderloin and so on. The plate diagram and generative story for our model are given in Figure 4 and Figure 5."}, {"heading": "4.3 Inference", "text": "For model inference, we use a straightforward Gibbs sampler based on the Chinese Restaurant Metaphor. Due to the\n1We follow the strict assumption proposed by Grubber et al. [16] that words in the same sentence are all generated from the same topic.\nspace limit, we just briefly describe this part and skip the details that can be found in Teh et al\u2019s work [29]. sample table B for current customer s:\nP (Bs = B|w,B \u2212 bs) \u221d{ nBP (s|w, zB) if B is used \u03b3P (s|w, znew) if B is new\n(5)\nwhere nB denotes the number of customers sitting at table B and zB denotes the dish served at table B. P (s|w, zB) and P (s|w, znew) denote the probability of sentence s generated by topic zB and new topic respectively, which would be described in detail in Appendix A. sample dish zB for the new table: If the customer chooses a new table, we have to sample a dish for this table according to the dish list.\nP (zBnew = z \u2032|w \u2208 s, z) \u221d{ mzt \u2032P (s|w, z) if z is used\n\u03b1P (s|w, znew) if z is new (6)\nAccording to Equ 4, we have mzt \u2032 = F (\u2206)mzt\u22121 + (1 \u2212 F (\u2206))mzt where m z t denotes the number of tables serving dish z at time t. re-sample dish zB for each table: Since the process of dish sampling actually changes the component member of tables after each iteration, we need to re-sample dish for each table as follows:\nP (zB = z|w, z,B) \u221d{ mzt \u2032P (B|w, z) if z is used\n\u03b1P (B|w, znew) if z is new (7)\nwhere P (B|w, z) and P (B|w, znew) denotes the probability that all sentences around table B are generated by topic z and new topic respectively, which will be described in Appendix A."}, {"heading": "4.4 Tree-based Sentence Selection", "text": "In HDM, each node from the tree is associated with a distribution over vocabularies and each sentence is represented by a path in the tree. We assume that sentences (or words) sharing similar paths should be more similar to each other because of the sharing topics. Let L denote the set of leaf nodes in the tree. The similarity between two words w1 and w2 is obtained by first calculating the Jensen-Shannon\ndivergence between them2.\nJS(w1||w2) = KL(w1|| w1 + w2\n2 ) +KL(w2|| w1 + w2 2 )\n. KL(w1||w2) = \u2211\nnodei\u2208L\nP (w1|nodei)log P (w1|nodei) P (w2|node)\nLet Ti denote a collection of words. Ti = {w|w \u2208 Ti}. q(T |nodei) = 1|T | \u2211 w\u2208T p(w|nodei). And the KL divergence between two collections of words are defined as follows:\nKL(T1||T2) = \u2211\nnodei\u2208L\nq(T1|nodei)log q(T1|nodei) q(T2|nodei)\nJS divergence is then transformed into a similarity measure by an decreasing logistic function \u03b61(x) = 1/(1 + e\nx) and increasing logistic function \u03b62(x) = e\nx/(1 + ex) to refine the relevance in the range of (0,1). A good timeline should properly consider the following key requirements[20]: (1)Focus: The timeline for epoch t should be related to the given query. Since the Query distribution is too sparse, we adopt the strategy taken in Yan et al.\u2018s work[40]. Query expansion is introduced by pseudo-relevance feedback to enlarge Q. We retrieve top \u2212 \u03ba snippets, which is denoted as Q\u2018 and use Q\u2018 to approximate Q.\n\u03beF (It) = \u03b61[JS(It||Q\u2032)]\n(2)Coherence: A timeline consists of a series of individual but correlated sentences. News evolves over time and a good component timeline should be coherent with neighboring documents so that the timeline can track the evolutionary pattern of news rather than peaks and bounds.\n\u03beCH(It) = \u03b61[JS(It||Ct\u22121)]\n(3)Coverage: Timeline candidate should keep alignment with source alignment at epoch t:\n\u03beCV (It) = \u03b61[JS(It||Ct)]\n(4)Non-redundancy: Non-redundancy measures the novelty degree of any of the sentence s compared with other sentences within Ii.\n\u03beNR(It) = 1 |It| \u2211 s\u2208Is \u03b62(JS(s||It \u2212 s)) (8)\nGiven the source collection, timeline is scored based on the weighted combination of these four requirements. The score function is illustrated in Equ.(14).\nS(It) = w1 \u00b7 \u03beF (It) + w2 \u00b7 \u03beCH(It) + w3 \u00b7 \u03beCV (It) + w4\u03beNR(It) (9)\u2211\ni wi = 1. Given the dataset, our timeline generation problem is transformed to the optimization problem as follows:\nIi = argmax I?i\nS(I?i ) (10)\nWe applied the sentence selection strategy in Yan et al\u2019s work [40] that literately generate I?i to approximate Ii by maximizing function S based on the timeline generation in the last iteration.\n2Since Kullback-Leibler divergence is asymmetric, we adopt Jensen-Shannon divergence in this paper, which is symmetric and nonnegative."}, {"heading": "5. EXPERIMENTS", "text": ""}, {"heading": "5.1 Datasets and Experiments Setup", "text": "There is no existing standard evaluation datasets for timeline generation. In this paper, we build 8 datasets talking about the real long-term events that draw public attention and use the golden standards to evaluate the performance of different models. The events include \u201cIraq War\u201c, \u201cApple Inc\u201c, \u201cFinancial Crisis\u201c, \u201cGreek Debt Crisis\u201c, \u201cAfghanistan War\u201c, \u201cArab Spring\u201c, \u201cAmerican Presidential Election 2012\u201c and \u201cNorth Korea\u2019s nuclear crisis\u201c. We downloaded 9935 news articles from different resources. The details of datasets are illustrated at Table1 and Table2. Dataset 1 \u201dIraq War\u201d, 2 \u201dApple Inc\u201d, 6 \u201cAfghanistan War\u201c and 8 \u201dNorth Korea\u2019s Nuclear Crisis\u201d are used as training sets for parameter tuning and the rest are used for testing.\nPreprocessing: We firstly remove stop-words in documents using a stop-word list of 598 words and the remaining words are stemmed by Porter Stemmer3. Besides stemming and stop-word removal, we also extract text snippets with the toolkit provided by Yan et al.[39]. Then we compress the corpus by discarding non-events texts. Timeline for each day is truncated to the same length of 30 words.\nPost-precessing: Since timeline candidates are extracted directly from original news corpus, sentences tend to contain less meaningful information and are sometimes redundant for readers to read. So we apply sentence compression algorithm, making the timeline much conciser. Sentence compression techniques have been well explored in many existing works[14, 21, 41]. We adopt the strategy introduced by [21] as shown in Figure 7."}, {"heading": "5.2 Evaluation Metrics", "text": "We adopt ROUGE toolkit (version 1.5.5) for performance evaluation. The timeline quality is measured by counting the number of overlapping units, such as N-gram, word sequences and word pairs between candidate timeline CT and the ground-truth timelines GT. Several automatic evaluation methods are implemented in ROUGE and each of\n3 http://tartarus.org/\u223cmartin/PorterStemmer/.\nthe methods can generate scores of recall, precision and Fmeasure. Take ROUGE-N as an example: ROUGE \u2212N \u2212R = \u2211 I\u2208GT \u2211 gramN\u2208I\nCntmatch(gramN )\u2211 I\u2208GT \u2211 gramN\u2208I Cnt(gramN )\nROUGE \u2212N \u2212 P = \u2211 I\u2208CT \u2211 gramN\u2208I\nCntmatch(gramN )\u2211 I\u2208CT \u2211 gramN\u2208I Cnt(gramN )\nROUGE\u2212N\u2212F = 2ROUGE \u2212N \u2212R \u00b7ROUGE \u2212N \u2212 P ROUGE \u2212N \u2212R+ROUGE \u2212N \u2212 P\nwhere N denotes the length of N-grams, Cntmatch(gramN ) is the maximum number of N-grams co-occurring in a candidate timeline and the reference timelines. In this paper, we only report ROUGE-F measure scores: ROUGE1-F, ROUGE-2-F and ROUGE-W-F, where ROUGE-W-F is based on the weighted longest common subsequence and the weight W is set to be 1.2.\nPart of reference timelines in ROUGE evaluation are downloads from Wikipedia4, while others are manually generated by using Amazon Mechanical Turk5. As for Turk, 16 Turkers are involved in this work. For each epoch."}, {"heading": "5.3 Parameter Tuning", "text": "Keeping other parameters fixed, we vary one parameter at a time to examine the changes of its performance. The first group of key parameters is w1, w2 and w3 where w4 = 1 \u2212 w1 \u2212 w2 \u2212 w3. We first gradually change w1 from 0 to 1 at the interval of 0.1 and set w2, w3 and w3 to the same value of (1\u2212w1)/3. Similar way is applied for training of w2 and w3. Experimental results are shown in Figure 7 and we notice that coherence and non-redundancy facilitate the performance while relevance demonstrates a relatively weaker influence. We set w1 = 0.1, w2 = 0.3, w3 = 0.3 and hence w4 = 0.3 according to ROUGE scores. \u03bb controls the rate of the exponential decay and further controls the trade off between influences from base menu list H and yesterday\u2019s menu list Gt\u22121 on Gt. We performed experiments setting different values of \u03bb to [ 1\n100 , 1 50 , 1 10 , 1, 10, 50, 100]. It is worth\nnoting that when \u03bb is set to 1 100\n(an approximation of 0), the model degenerates into a standard HDP model where no neighbor influence is considered. When \u03bb is set to 100 (an approximation of \u221e), each day\u2019s dish menu is identical to yesterday. We find that experimental performance peaks when the value of \u03bb is round 1.\n4http://www.wikipedia.org/ 5http://mturk.com"}, {"heading": "5.4 Performance Comparison with Baselines", "text": "We implement the following algorithms as the baseline systems. For fairness we conduct the same preprocessing and post-processing for all algorithms.\nRandom: The methods that select sentences randomly for timeline generation.\nCentroid: The method that applies MEAD algorithm [23], which has been widely used in MDS for sentence selection according to centroid value, positional value, and firstsentence overlap.\nGMDS: The Graph-based MDS sentence selection strategy proposed by Wan et al.[30] that constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on centrality.\nChieu: A timeline system proposed by Chieu et al.[10] by considering interest and burstiness.\nETTS: A timeline system proposed by Yan et al.[40] by projecting sentences from different temporal corpus into one plane and developing the graph based ranking algorithm.\nWe average F-score performance in terms of ROUGE-1, ROUGE-2, and ROUGE-W on all sets. The overall results are shown in Figure 8 and the details are shown in Table 3. We observe that since traditional MDS methods such as Centriod and GMS only consider sentence ranking and selection strategy, they seldom try to consider the evolutionary pattern of topics. Many sentences are missed, resulting in a low recall. Chieu does not capture time attributes, so its results are worse than ETTS, which uses local and global\nmeasures to capture the time dependencies. However, since ETTS is a sentence ranking algorithm in nature, its sentence selection strategy may be biased for neglecting the evolutionary pattern of topic information across the corpus. tHDT achieves the best results for its capability in detecting the hierarchical structures in the corpus. HDM outperforms ETTS by 6.2%,12.9% and 11.7% with regard to the overall performance in ROUGE-1,ROUGE-2 and ROUGEW respectively."}, {"heading": "5.5 Comparison with Other Topic Models", "text": "To illustrate the effectiveness of our topic model, we provide six other baseline systems which adopt different aspect modeling techniques.\ntHDP: HDP is a time-dependent HDP model without considering hierarchical structure of topics. It is a simple version HDM.\nStand-HDP: Stand-HDP is standard HDP model that treats different epochs as a series of independent HDPs without considering time dependence.\nDyn-LDA: A dynamic LDA[5] where topic-word distribution and popularity are linked across epochs by including Markovian assumption.\nStand-LDA: Standard LDA topic model without considering background or temporal information6.\nThe overall results are shown in Figure 9 and details are listed in Table 4. As we can see, HDM is better than tHDP and HDP, which verifies HDM\u2019s ability in modeling hierarchical topic structure in long-term news corpus. It is also better than baselines based on LDA and Dyn-LDA. Compared with Table 3 and Figure 8, we find that most topic based models can achieve better results than baselines in\n6Topic number is set to 80 in Dyn-LDA and Stand-LDA.\nSection 5.4. This illustrates the advantages of topic modeling in timeline generation over methods that only focus on sentence selection strategy."}, {"heading": "5.6 Qualitative Evaluation of Topic Modeling", "text": "Finally, we present qualitative evaluations for topic modeling results of HDM. Figure 10 (a) presents first level topics extracted from Greek Debt Crisis dataset and correspondent top words for each topic. intensity corresponds to the number of sentences assigned to the certain topic at a specific time. As we can see, each topic does describe real aspect within the event. According to the top word, we find that Topic 1 talks about general concept for Greek Debt Crisis, Topic 2 talks about response or rescue from international community, Topic 3 discusses protests from Greek public and Topic 4 discusses measures that Greek government take. We also find an interesting phenomena that protests (Topic 3) usually peaks right after domestic measures (Topic 4) or inter national response (Topic 2), which makes sense in real case where people go on protests for the measure of policy of the government. Figure 10 (b) presents sub-topics (secondlevel topics) for Topic 3, denoted as Topic 3-1, 3-2 and 3-3. According to top words and peak time, we can find Topic 3-1 presents strike and protest on May 2010 while Topic 3-2 corresponds to the demonstration around May 2011. These two subtopics capture the differences and specifics between the two procedures of protest and demonstration within Greek debt crisis. We can see top words in Topic 3-1 tend to be more violent than words in Topic 3-2, including the information that people got killed in May, 20107. Topic 3-3 tend to describe the general or background perspective in Greek protest. Figure 11 describes presents the hierarchical topic structure for American Election 2012, where the labels are manually given.\n7http://www.nytimes.com/2010/05/06/world/europe/06greece.html"}, {"heading": "6. CONCLUSION", "text": "In this paper, we develop a novel topic model denoted as time-dependent Hierarchical Dirichlet Model (HDM) to explore the hierarchical topic structure for timeline generation. Our model aptly combines Dirichlet Tree with Dirichlet Processes and can automatically learns the structure of trees across corpus. Different levels of Markovian time dependency and background information are considered for tree structure construction. We build an experimental system on 8 real long-term events that public concern. Experimental results illustrate the effectiveness of our proposed model."}, {"heading": "7. REFERENCES", "text": "[1] A. Ahmed and E. P. Xing. Dynamic non-parametric\nmixture models and the recurrent chinese restaurant process. Carnegie Mellon University, School of Computer Science, Machine Learning Department, 2007.\n[2] A. Ahmed and E. P. Xing. Timeline: A dynamic hierarchical dirichlet process model for recovering birth/death and evolution of topics in text stream. arXiv preprint arXiv:1203.3463, 2012.\n[3] J. Allan, R. Gupta, and V. Khandelwal. Temporal summaries of new topics. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10\u201318. ACM, 2001.\n[4] D. Blackwell and J. B. MacQueen. Ferguson distributions via po\u0301lya urn schemes. The annals of statistics, pages 353\u2013355, 1973.\n[5] D. M. Blei and J. D. Lafferty. Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning, pages 113\u2013120. ACM, 2006.\n[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022, 2003.\n[7] F. Caron, M. Davy, and A. Doucet. Generalized polya urn for time-varying dirichlet process mixtures. arXiv preprint arXiv:1206.5254, 2012.\n[8] D. Chakrabarti, R. Kumar, and A. Tomkins. Evolutionary clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 554\u2013560. ACM, 2006.\n[9] Y. Chi, X. Song, D. Zhou, K. Hino, and B. L. Tseng. Evolutionary spectral clustering by incorporating temporal smoothness. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 153\u2013162. ACM, 2007.\n[10] H. L. Chieu and Y. K. Lee. Query based event extraction along a timeline. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 425\u2013432. ACM, 2004.\n[11] A. Feng and J. Allan. Finding and linking incidents in news. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 821\u2013830. ACM, 2007.\n[12] T. S. Ferguson. A bayesian analysis of some nonparametric problems. The annals of statistics, pages 209\u2013230, 1973.\n[13] G. P. C. Fung, J. X. Yu, H. Liu, and P. S. Yu. Time-dependent event hierarchy construction. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 300\u2013309. ACM, 2007.\n[14] D. Gillick and B. Favre. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 10\u201318. Association for Computational Linguistics, 2009.\n[15] D. M. B. T. L. Griffiths and M. I. J. J. B. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference, volume 16, page 17. MIT Press, 2004.\n[16] A. Gruber, Y. Weiss, and M. Rosen-Zvi. Hidden topic markov models. In International Conference on Artificial Intelligence and Statistics, pages 163\u2013170, 2007.\n[17] J. Li and S. Li. Evolutionary hierarchical dirichlet process for timeline summarization.\n[18] J. Li and S. Li. A novel feature-based bayesian model for query focused multi-document summarization.\n[19] J. Li, S. Li, X. Wang, and B. Chang. Update summarization using a multi-level hierarchical dirichlet process model.\n[20] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th international conference on World wide web, pages 71\u201380. ACM, 2009.\n[21] P. Li, Y. Wang, W. Gao, and J. Jiang. Generating aspect-oriented multi-document summarization with event-aspect model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1137\u20131146. Association for Computational Linguistics, 2011.\n[22] I. Pruteanu-Malinici, L. Ren, J. Paisley, E. Wang, and L. Carin. Hierarchical bayesian modeling of topics in time-stamped documents. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(6):996\u20131011, 2010.\n[23] D. R. Radev, H. Jing, M. Stys\u0301, and D. Tam. Centroid-based summarization of multiple documents. Information Processing & Management, 40(6):919\u2013938, 2004.\n[24] L. Ren, D. B. Dunson, and L. Carin. The dynamic hierarchical dirichlet process. In Proceedings of the 25th international conference on Machine learning, pages 824\u2013831. ACM, 2008.\n[25] K. Salomatin, Y. Yang, and A. Lad. Multi-field correlated topic modeling. In SDM, pages 628\u2013637, 2009.\n[26] J. Sethuraman. A constructive definition of dirichlet priors. Technical report, DTIC Document, 1991.\n[27] R. Swan and J. Allan. Automatic generation of overview timelines. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 49\u201356. ACM, 2000.\n[28] Y. W. Teh and G. Haffari. Hierarchical dirichlet trees for information retrieval. 2009.\n[29] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566\u20131581, 2006.\n[30] X. Wan and J. Yang. Multi-document summarization using cluster-based link analysis. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 299\u2013306. ACM, 2008.\n[31] C. Wang, D. Blei, and D. Heckerman. Continuous time dynamic topic models. arXiv preprint arXiv:1206.3298, 2012.\n[32] J. L. S. L. X. Wang and Y. T. B. Chang. Update summarization using a multi-level hierarchical\ndirichlet process model.\n[33] X. Wang and A. McCallum. Topics over time: a non-markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 424\u2013433. ACM, 2006.\n[34] X. Wang, C. Zhai, X. Hu, and R. Sproat. Mining correlated bursty topic patterns from coordinated text streams. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 784\u2013793. ACM, 2007.\n[35] X. Wang, K. Zhang, X. Jin, and D. Shen. Mining common topics from multiple asynchronous text streams. In Proceedings of the Second ACM\nInternational Conference on Web Search and Data Mining, pages 192\u2013201. ACM, 2009.\n[36] T. Xu, Z. Zhang, P. S. Yu, and B. Long. Dirichlet process based evolutionary clustering. In Data Mining, 2008. ICDM\u201908. Eighth IEEE International Conference on, pages 648\u2013657. IEEE, 2008.\n[37] T. Xu, Z. Zhang, P. S. Yu, and B. Long. Evolutionary clustering by hierarchical dirichlet process with hidden markov state. In Data Mining, 2008. ICDM\u201908. Eighth IEEE International Conference on, pages 658\u2013667. IEEE, 2008.\n[38] R. Yan, L. Kong, C. Huang, X. Wan, X. Li, and Y. Zhang. Timeline generation through evolutionary trans-temporal summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 433\u2013443. Association for Computational Linguistics, 2011.\n[39] R. Yan, Y. Li, Y. Zhang, and X. Li. Event recognition from news webpages through latent ingredients extraction. Information Retrieval Technology, pages 490\u2013501, 2010.\n[40] R. Yan, X. Wan, J. Otterbacher, L. Kong, X. Li, and Y. Zhang. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 745\u2013754. ACM, 2011.\n[41] D. Zajic, B. J. Dorr, J. Lin, and R. Schwartz. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing & Management, 43(6):1549\u20131570, 2007.\n[42] J. Zhang, Y. Song, C. Zhang, and S. Liu. Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1079\u20131088. ACM, 2010."}], "references": [{"title": "Dynamic non-parametric mixture models and the recurrent chinese restaurant process", "author": ["A. Ahmed", "E.P. Xing"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Timeline: A dynamic hierarchical dirichlet process model for recovering birth/death and evolution of topics in text stream", "author": ["A. Ahmed", "E.P. Xing"], "venue": "arXiv preprint arXiv:1203.3463,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Temporal summaries of new topics", "author": ["J. Allan", "R. Gupta", "V. Khandelwal"], "venue": "In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Ferguson distributions via p\u00f3lya urn schemes", "author": ["D. Blackwell", "J.B. MacQueen"], "venue": "The annals of statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1973}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Generalized polya urn for time-varying dirichlet process mixtures", "author": ["F. Caron", "M. Davy", "A. Doucet"], "venue": "arXiv preprint arXiv:1206.5254,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Evolutionary clustering", "author": ["D. Chakrabarti", "R. Kumar", "A. Tomkins"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Evolutionary spectral clustering by incorporating temporal smoothness", "author": ["Y. Chi", "X. Song", "D. Zhou", "K. Hino", "B.L. Tseng"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Query based event extraction along a timeline", "author": ["H.L. Chieu", "Y.K. Lee"], "venue": "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Finding and linking incidents in news", "author": ["A. Feng", "J. Allan"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "A bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The annals of statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1973}, {"title": "Time-dependent event hierarchy construction", "author": ["G.P.C. Fung", "J.X. Yu", "H. Liu", "P.S. Yu"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "A scalable global model for summarization", "author": ["D. Gillick", "B. Favre"], "venue": "In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D.M.B.T.L. Griffiths", "M.I.J.J.B. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Hidden topic markov models", "author": ["A. Gruber", "Y. Weiss", "M. Rosen-Zvi"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Enhancing diversity, coverage and balance for summarization through structure learning", "author": ["L. Li", "K. Zhou", "G.-R. Xue", "H. Zha", "Y. Yu"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Generating aspect-oriented multi-document summarization with event-aspect model", "author": ["P. Li", "Y. Wang", "W. Gao", "J. Jiang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Hierarchical bayesian modeling of topics in time-stamped documents", "author": ["I. Pruteanu-Malinici", "L. Ren", "J. Paisley", "E. Wang", "L. Carin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Centroid-based summarization of multiple documents", "author": ["D.R. Radev", "H. Jing", "M. Sty\u015b", "D. Tam"], "venue": "Information Processing & Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "The dynamic hierarchical dirichlet process", "author": ["L. Ren", "D.B. Dunson", "L. Carin"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Multi-field correlated topic modeling", "author": ["K. Salomatin", "Y. Yang", "A. Lad"], "venue": "In SDM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "A constructive definition of dirichlet priors", "author": ["J. Sethuraman"], "venue": "Technical report, DTIC Document,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "Automatic generation of overview timelines", "author": ["R. Swan", "J. Allan"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Hierarchical dirichlet trees for information", "author": ["Y.W. Teh", "G. Haffari"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Multi-document summarization using cluster-based link analysis", "author": ["X. Wan", "J. Yang"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Continuous time dynamic topic models", "author": ["C. Wang", "D. Blei", "D. Heckerman"], "venue": "arXiv preprint arXiv:1206.3298,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "Mining correlated bursty topic patterns from coordinated text streams", "author": ["X. Wang", "C. Zhai", "X. Hu", "R. Sproat"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Dirichlet process based evolutionary clustering", "author": ["T. Xu", "Z. Zhang", "P.S. Yu", "B. Long"], "venue": "In Data Mining,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Evolutionary clustering by hierarchical dirichlet process with hidden markov state", "author": ["T. Xu", "Z. Zhang", "P.S. Yu", "B. Long"], "venue": "In Data Mining,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Timeline generation through evolutionary trans-temporal summarization", "author": ["R. Yan", "L. Kong", "C. Huang", "X. Wan", "X. Li", "Y. Zhang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Event recognition from news webpages through latent ingredients extraction", "author": ["R. Yan", "Y. Li", "Y. Zhang", "X. Li"], "venue": "Information Retrieval Technology,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution", "author": ["R. Yan", "X. Wan", "J. Otterbacher", "L. Kong", "X. Li", "Y. Zhang"], "venue": "In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Multi-candidate reduction: Sentence compression as a tool for document summarization tasks", "author": ["D. Zajic", "B.J. Dorr", "J. Lin", "R. Schwartz"], "venue": "Information Processing & Management,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora", "author": ["J. Zhang", "Y. Song", "C. Zhang", "S. Liu"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 9, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 32, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 34, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 23, "context": "[27] by extracting clusters of noun phases and name entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Later they built up a system to provide timelines which consist of one sentence per date by considering usefulness and novelty[3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "[10] built a similar system in unit of sentences with interest and burstiness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[38] extended the graph based sentence ranking algorithm used in traditional multi-document summarization (MDS) to timeline generation by projecting sentences from different times into one plane.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "They further explored the timeline task from the optimization of a function by considering the combination of different respects such as relevance, coverage, coherence and diversity[40].", "startOffset": 181, "endOffset": 185}, {"referenceID": 32, "context": "Time dependency is considered in Yan et al\u2019s work[38, 40].", "startOffset": 49, "endOffset": 57}, {"referenceID": 34, "context": "Time dependency is considered in Yan et al\u2019s work[38, 40].", "startOffset": 49, "endOffset": 57}, {"referenceID": 10, "context": "Topic detection or mining is not a new task and many algorithms have been proposed in TDT (Topic Detection and Tracking) task[11, 13], or summarization task[32, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 12, "context": "Topic detection or mining is not a new task and many algorithms have been proposed in TDT (Topic Detection and Tracking) task[11, 13], or summarization task[32, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 5, "context": "Among existing approaches, topic models such as Latent Dirichlet Allocation (LDA)[6] or Hierarchical Dirichlet Proar X iv :1 31 2.", "startOffset": 81, "endOffset": 84}, {"referenceID": 25, "context": "cesses (HDP) have their advantages [29] in capturing latent topics within document collection due its clear probabilistic interpretions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "[15] introduced a hLDA model which organizes topics into a tree structure with depth L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 1, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 4, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 8, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 20, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 21, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 24, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 28, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 36, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 23, "context": "The task of timeline generation is firstly proposed by Swan and Allan[27] by using named entities for sentence selection in different time epochs.", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "Since then, researchers tried to generate timeline by considering different respects such as usefulness and novelty[3] or interest and burstiness[10].", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Since then, researchers tried to generate timeline by considering different respects such as usefulness and novelty[3] or interest and burstiness[10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 33, "context": "[39] extended the graph based sentence ranking algorithm in multi-document summarization (MDS) to timeline generation by projecting sentences from different temporal corpus into one plane.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[38] transforms this task to an optimization problem by considering the relevance, coverage and coherence of the sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Recently, bayesian topic models such as LDA[6] or HDP[29] have shown the power text mining for its clear probabilistic interpretation.", "startOffset": 43, "endOffset": 46}, {"referenceID": 25, "context": "Recently, bayesian topic models such as LDA[6] or HDP[29] have shown the power text mining for its clear probabilistic interpretation.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "hLDA [15] extends LDA model to a multi-level tree structure where each node is associated with a topic distribution over words.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 7, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 8, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 30, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 31, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 0, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 39, "endOffset": 50}, {"referenceID": 31, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 39, "endOffset": 50}, {"referenceID": 30, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 39, "endOffset": 50}, {"referenceID": 4, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 160, "endOffset": 171}, {"referenceID": 27, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 160, "endOffset": 171}, {"referenceID": 28, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 160, "endOffset": 171}, {"referenceID": 6, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 18, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 20, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 29, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 36, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 29, "context": "[34] focused on detecting the simultaneous busting of some topics in multiple text streams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[35] further extended [34] where they adjusted the timestamps of all documents to synchronize multiple streams and then learned a common topic model.", "startOffset": 22, "endOffset": 26}, {"referenceID": 36, "context": "[42] introduced an approach based on Hierarchical Dirichlet Process [29] to discover interesting cluster evolutionary pattern from correlated time-varying text corpora.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[42] introduced an approach based on Hierarchical Dirichlet Process [29] to discover interesting cluster evolutionary pattern from correlated time-varying text corpora.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "Dirichlet Process(DP) can be considered as a distribution over distributions[12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "G itself is a distribution over a given parameter space \u03b8, therefore we can draw parameters \u03b81:N from G following a Polya urn distribution [4] also known as a Chinese Restaurant process [29] or through a stick breaking construction [26].", "startOffset": 139, "endOffset": 142}, {"referenceID": 25, "context": "G itself is a distribution over a given parameter space \u03b8, therefore we can draw parameters \u03b81:N from G following a Polya urn distribution [4] also known as a Chinese Restaurant process [29] or through a stick breaking construction [26].", "startOffset": 186, "endOffset": 190}, {"referenceID": 22, "context": "G itself is a distribution over a given parameter space \u03b8, therefore we can draw parameters \u03b81:N from G following a Polya urn distribution [4] also known as a Chinese Restaurant process [29] or through a stick breaking construction [26].", "startOffset": 232, "endOffset": 236}, {"referenceID": 14, "context": "HDM can be viewed as an extension of Hierarchical LDA model [15] where we wish to discover common usage patterns (or topics) given a collection of documents and try to organize these topics into a hierarchy.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 1, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 20, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 36, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 15, "context": "[16] that words in the same sentence are all generated from the same topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "space limit, we just briefly describe this part and skip the details that can be found in Teh et al\u2019s work [29].", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "A good timeline should properly consider the following key requirements[20]: (1)Focus: The timeline for epoch t should be related to the given query.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "\u2018s work[40].", "startOffset": 7, "endOffset": 11}, {"referenceID": 34, "context": "We applied the sentence selection strategy in Yan et al\u2019s work [40] that literately generate I i to approximate Ii by maximizing function S based on the timeline generation in the last iteration.", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "[39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Sentence compression techniques have been well explored in many existing works[14, 21, 41].", "startOffset": 78, "endOffset": 90}, {"referenceID": 17, "context": "Sentence compression techniques have been well explored in many existing works[14, 21, 41].", "startOffset": 78, "endOffset": 90}, {"referenceID": 35, "context": "Sentence compression techniques have been well explored in many existing works[14, 21, 41].", "startOffset": 78, "endOffset": 90}, {"referenceID": 17, "context": "We adopt the strategy introduced by [21] as shown in Figure 7.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 9, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 9, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 19, "context": "Centroid: The method that applies MEAD algorithm [23], which has been widely used in MDS for sentence selection according to centroid value, positional value, and firstsentence overlap.", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "[30] that constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on centrality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] by considering interest and burstiness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[40] by projecting sentences from different temporal corpus into one plane and developing the graph based ranking algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Dyn-LDA: A dynamic LDA[5] where topic-word distribution and popularity are linked across epochs by including Markovian assumption.", "startOffset": 22, "endOffset": 25}], "year": 2017, "abstractText": "Timeline Generation aims at summarizing news from different epochs and telling readers how an event evolves. It is a new challenge that combines salience ranking with novelty detection. For long-term public events, the main topic usually includes various aspects across different epochs and each aspect has its own evolving pattern. Existing approaches neglect such hierarchical topic structure involved in the news corpus in timeline generation. In this paper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for timeline generation. Our model can aptly detect different levels of topic information across corpus and such structure is further used for sentence selection. Based on the topic mined fro HDM, sentences are selected by considering different aspects such as relevance, coherence and coverage. We develop experimental systems to evaluate 8 long-term events that public concern. Performance comparison between different systems demonstrates the effectiveness of our model in terms of ROUGE metrics.", "creator": "LaTeX with hyperref package"}}}