{"id": "1312.4405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Learning Deep Representations By Distributed Random Samplings", "abstract": "sherrod In 9:44 this guttata paper, homoeosoma we p.e. propose 3,688 an fruitlands extremely bricks simple isba deep anadol model titirangi for the crisply unsupervised nonlinear amica dimensionality reduction - - sandercock deep dourthe distributed absa random hotlines samplings, eee which enounters performs like a morariu stack a-24 of unsupervised bootstrap coal aggregating. wallbank First, cabinetry its buffenbarger network double-acting structure 60,000-70 is novel: each layer of the avoids network 4-4 is a group pizzicato of mutually ghb independent $ k $ - 83-member centers roorda clusterings. gardeja Second, its learning wisecracks method horrem is extremely simple: the $ loepfe k $ caldiero centers of agonizingly each clustering slaf are 28.92 only $ k $ randomly selected examples from the fuwa training data; for small - busu scale data humberstone sets, zeljeznicar the $ k $ goza centers are further sunspot randomly reconstructed zulqarnain by a 54.34 simple electrowinning cyclic - gousmi shift operation. Experimental fkm results 3-of-6 on 896,000 nonlinear mudie dimensionality yamun reduction fortus show lakeridge that ensuite the mantels proposed method can caenogastropoda learn sea-water abstract representations siwu on both dodman large - mchedlidze scale playsets and coskun small - scale problems, tikhomirov and papert meanwhile leive is much faster purples than geldern deep neural brdy networks actor-director on zuiderzee large - 11,360 scale cri problems.", "histories": [["v1", "Mon, 16 Dec 2013 15:40:05 GMT  (2558kb,D)", "http://arxiv.org/abs/1312.4405v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiao-lei zhang"], "accepted": false, "id": "1312.4405"}, "pdf": {"name": "1312.4405.pdf", "metadata": {"source": "META", "title": "Learning Deep Representations By Distributed Random Samplings", "authors": ["Xiao-Lei Zhang"], "emails": ["HUOSHAN6@126.COM"], "sections": [{"heading": "1. Introduction", "text": "Deep learning is one of the most powerful representation learning techniques. Recent advanced work starts from Hinton and Salakhutdinov (Hinton & Salakhutdinov, 2006). The method, named deep belief networks (DBN), contains two phases \u2013 the bottom-up greedy layer-wise pretraining phase and the top-down fine-tuning phase.\nIn this paper, we propose a simple deep learning method \u2013 deep distributed random samplings (DDRS), for the unsupervised dimensionality reduction. The time and storage complexities of DDRS scale linearly with the size of the data set.\nIn the remainder of the paper, we will describe DDRS in Section 2, and report the experimental results in Section 3. Finally, we conclude this paper in Section 4. The theoretical justification, motivation, related work and several supplemental experiments are in the supplementary material.\nA very basic draft that is used to declare the ownership of this algorithm. Full version will be updated later.\nLayer 1\nLayer 2\nLayer 3\nFigure 1. Architecture of the proposed DDRS. The clusterings in the same layer are drawn in different colors. Each clustering contains two computation nodes. The first node is the k-randomlysampled examples, which is represented as a square. The second node is the one-hot sparse encoding, which is represented as the dotted lines from the square."}, {"heading": "2. Algorithm description", "text": "DDRS is trained layer-wisely (Figure 1). The output of one layer is the input of its upper layer. Each layer consists of V independent k-centers clusterings. Given a set of d-dimensional input data X = {x1, . . . ,xn}, the training process of the v-th clustering is as follows:\n1. Random feature selection: randomly select badc dimensions of X to form a subset of X , denoted as X (v) = { x (v) 1 , . . . ,x (v) n } , where a represents the\nfraction of the selected dimensions. 2. Random sampling: randomly select k examples\nfrom X (v), denoted as Wv = [wv,1, . . . ,wv,k], as the k centers of the v-th clustering. 3. Random reconstruction: Randomly select brbadcc dimensions of Wv , and do one-step cyclic-shift on the selected dimensions as in Figure 2, where r represents the fraction of the randomly selected features over the badc-dimensional centers. 4. Sparse representation learning: (i) calculate the similarities hv between the input x(v) and the k centers in terms of some predefined similarity measurement at the bottom layer, such as the Euclidean distance, and in terms of hv = WTv x\n(v) at all other layers; (ii) enforce one-hot encoding on hv by setting the\nar X\niv :1\n31 2.\n44 05\nv1 [\ncs .L\nG ]\n1 6\nD ec\n2 01\n3\nentry that corresponds to the closest center to x(v) to 1 and all other entries to 0.\nThe outputs of all clusterings of each layer are concatenated to a long sparse vector, i.e. h\u0304 = [hT1 , . . . ,h T V ] T ."}, {"heading": "3. Empirical evaluation", "text": "In this section, we will focus on the unsupervised dimensionality reduction problem. When we evaluate the running time, the experiments are run with one-core PC with 8 GB memory. The experiments are conduced on three data sets, which are the MNIST handwritten digits, and a small-scale data set respectively. The analysis on how the parameters affect the performance is attached in the supplementary material.\nThe bottom layer of DDRS uses the linear kernel to calculate the similarities between the input data and the centers in all data sets. Because DDRS learns only a sparse high-dimensional representation, we use principle component analysis (PCA) to project it to a low-dimensional subspace. Only a few largest eigenvalues and their corresponding eigenvectors are preserved for constructing the subspace."}, {"heading": "3.1. Results on the MNIST digits", "text": "MNIST handwritten digit data set is a benchmark data set that contains 10 hand written integer digits ranging from 0 to 9. It consists of 60,000 training examples and 10,000 test examples. Each example has 784 dimensions. We normalize each example to [0, 1] by dividing each entry of the example by 255.\nThe parameter setting of DDRS is as follows. The learned representations are projected to {2, 3, 5, 10, 20, 30} lowdimensional subspaces respectively.\nThe parameter settings of DBN are the same as in (Hinton & Salakhutdinov, 2006) except that the number of the units in the linear output layer are set to {2, 3, 5, 10, 20, 30} respectively. The CPU time consumed on pretraining and\nfine-tuning are recorded separately.\nWe use the training set to train the models, and evaluate the effectiveness of the learned representations on the test set by the k-means clustering, where the Euclidean distance is used to measure the similarity between any two examples in the low-dimensional subspace. The normalized mutual information (NMI) (Strehl & Ghosh, 2003) is used as the evaluation metric, and the results are average ones over 10 independent runs. We will compare DDRS with DBN and PCA.\nThe results of using the k-means clustering are summarized in Figures 3. From the figures, we can see that (i) when the dimensions are restricted to 2 to 5, DDRS is as good as DBN as long as they are in the same depth, and significantly outperforms PCA; (ii) when the dimensions are enlarged to 10 to 30, DDRS performs better than PCA and DBN; (iii) when the dimensions are gradually increased, all methods are getting worse and worse.\nThe CPU time is recorded in Table 1. From the table, we can see that DDRS is about 20 times faster than DBN.\nWe also compared DDRS with other well-known dimensionality reduction methods on small subsets of MNIST in the supplementary material, since all of the supplemental competitive methods cannot handle large scale problems. DDRS is at least as good as the best competitors."}, {"heading": "3.2. Results on a small-scale data set", "text": "Sometimes, we have to deal with very small-scale but highdimensional data sets. In this subsection, we will study such a very small-scale problem. The data set is a two-class classification problem that consists of 38 training examples and 34 test examples.\nThe parameter settings of DDRS is as follows. The learned representations are projected to {2, 3, 5, 10} lowdimensional subspaces.\nWe will compare DDRS with PCA and some methods\nTable 1. CPU time (in hours) comparison on MNIST.\nDBNpretraining DBNfine_tunning DDRS\nTime 2.94 74.54 5.16\nbased on graphs. The competitive methods are all one-layer nonlinear dimension reduction methods that have an O(n2) complexity.\nWe run the experiment 10 times and report the average performance. When k-means clustering is used for evaluation, for each single experimental running, we run k-means on the entire data set 50 times and record the average NMI performance. Figure 4 shows the performance of the k-means clusterings using the features learned by the competitive methods. From the figures, we observe that DDRS can learn a feature that is at least as good as the best competitor, and the experimental phenomena is similar with those on the MNIST data set."}, {"heading": "4. Conclusions", "text": "DDRS is simple, fast, and effective."}, {"heading": "Acknowledgement", "text": "The author would like to thank Prof. DeLiang Wang for providing the Ohio Supercomputing Center, Columbus, OH, USA, for the experimental running."}], "references": [{"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["Strehl", "Alexander", "Ghosh", "Joydeep"], "venue": "JMLR, 3:583\u2013617,", "citeRegEx": "Strehl et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2003}], "referenceMentions": [], "year": 2013, "abstractText": "In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction \u2013 deep distributed random samplings. First, its network structure is novel: each layer of the network is a group of mutually independent k-centers clusterings. Second, its learning method is extremely simple: the k centers of each clustering are only k randomly selected examples from the training data; for small-scale data sets, the k centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems.", "creator": "LaTeX with hyperref package"}}}