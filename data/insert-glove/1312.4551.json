{"id": "1312.4551", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs", "abstract": "We present an chainstore asymptotic analysis of rakiura Viterbi Training (VT) and contrast it dilettante with a sheehy-skeffington more zoete conventional Maximum rsu Likelihood (coryell ML) tsavo approach to parameter gulan estimation okamba in Hidden barrs Markov kwoh Models. While ML zalla estimator works hont by (locally) maximizing 30.92 the likelihood marinov of the bagrov observed data, VT globose seeks habia to nebulas maximize the nandana probability kangchu of inquires the snowmen most likely early-twentieth hidden interesante state dipset sequence. We develop congregationalists an encampment analytical kreisher framework based 77.67 on a generating cinch function formalism hakman and hound illustrate eike it outsourcing on an kalev/cramo exactly solvable model shenkar of HMM with one 300,000-dollar unambiguous mascoma symbol. gosingan For this mullane particular model the elizur ML stepanovs objective torga function right-wing is contrivances continuously degenerate. haat VT deerskins objective, 1.097 in jand contrast, exeter is shown to bloodlust have only finite gallium degeneracy. yasini Furthermore, emmitt VT converges poundbury faster and results babitsky in lampard sparser (+.06 simpler) lelean models, jcummings thus ellipse realizing an automatic 1,078 Occam ' 44-member s razor all-academic for concentric HMM 40-31 learning. For arends more general scenario royal VT cofinancing can usl be delimited worse compared parasail to ML but rcsb still lappets capable viacom of correctly unteroffizier recovering reidsville most of straddle the machover parameters.", "histories": [["v1", "Mon, 16 Dec 2013 21:03:28 GMT  (170kb)", "http://arxiv.org/abs/1312.4551v1", "Appeared in Neural Information Processing Systems (NIPS) 2011"]], "COMMENTS": "Appeared in Neural Information Processing Systems (NIPS) 2011", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["armen e allahverdyan", "aram galstyan"], "accepted": true, "id": "1312.4551"}, "pdf": {"name": "1312.4551.pdf", "metadata": {"source": "CRF", "title": "Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs", "authors": ["Armen Allahverdyan"], "emails": ["aarmen@yerphi.am", "galstyan@isi.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n45 51\nv1 [\nst at\n.M L\n] 1\n6 D"}, {"heading": "1 Introduction", "text": "Hidden Markov Models (HMM) provide one of the simplest examples of structured data observed through a noisy channel. The inference problems of HMM naturally divide into two classes [20, 9]: i) recovering the hidden sequence of states given the observed sequence, and ii) estimating the model parameters (transition probabilities of the hidden Markov chain and/or conditional probabilities of observations) from the observed sequence. The first class of problems is usually solved via the maximum a posteriori (MAP) method and its computational implementation known as Viterbi algorithm [20, 9]. For the parameter estimation problem, the prevailing method is maximum likelihood (ML) estimation, which finds the parameters by maximizing the likelihood of the observed data. Since global optimization is generally intractable, in practice it is implemented through an expectation\u2013 maximization (EM) procedure known as Baum\u2013Welch algorithm [20, 9].\nAn alternative approach to parameter learning is Viterbi Training (VT), also known in the literature as segmental K-means, Baum\u2013Viterbi algorithm, classification EM, hard EM, etc. Instead of maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. Maximizing VT objective function is hard [8], so in practice it is implemented via an EM-style iterations between calculating the MAP sequence and adjusting the model parameters based on the sequence statistics. It is known that VT lacks some of the desired features of ML estimation such as consistency [17], and in fact, can produce biased estimates [9]. However, it has been shown to perform well in practice, which explains its widespread use in applications such as speech recognition [16], unsupervised dependency parsing [24], grammar induction [6], ion channel modeling [19]. It is generally assumed that VT is more robust and faster but usually less accurate, although for certain tasks it outperforms conventional EM [24].\nThe current understanding of when and under what circumstances one method should be preferred over the other is not well\u2013established. For HMMs with continuos observations, Ref. [18] established\nan upper bound on the difference between the ML and VT objective functions, and showed that both approaches produce asymptotically similar estimates when the dimensionality of the observation space is very large. Note, however, that this asymptotic limit is not very interesting as it makes the structure imposed by the Markovian process irrelevant. A similar attempt to compare both approaches on discrete models (for stochastic context free grammars) was presented in [23]. However, the established bound was very loose.\nOur goal here is to understand, both qualitatively and quantitatively, the difference between the two estimation methods. We develop an analytical approach based on generating functions for examining the asymptotic properties of both approaches. Previously, a similar approach was used for calculating entropy rate of a hidden Markov process [1]. Here we provide a non-trivial extension of the methods that allows to perform comparative asymptotic analysis of ML and VT estimation. It is shown that both estimation methods correspond to certain free-energy minimization problem at different temperatures. Furthermore, we demonstrate the approach on a particular class of HMM with one unambiguous symbol and obtain a closed\u2013form solution to the estimation problem. This class of HMMs is sufficiently rich so as to include models where not all parameters can be determined from the observations, i.e., the model is not identifiable [7, 14, 9].\nWe find that for the considered model VT is a better option if the ML objective is degenerate (i.e., not all parameters can be obtained from observations). Namely, not only VT recovers the identifiable parameters but it also provides a simple (in the sense that non-identifiable parameters are set to zero) and optimal (in the sense of the MAP performance) solution. Hence, VT realizes an automatic Occam\u2019s razor for the HMM learning. In addition, we show that the VT algorithm for this model converges faster than the conventional EM approach. Whenever the ML objective is not degenerate, VT leads generally to inferior results that, nevertheless, may be partially correct in the sense of recovering certain (not all) parameters."}, {"heading": "2 Hidden Markov Process", "text": "Let S = {S0,S1,S2, ...} be a discrete-time, stationary, Markov process with conditional probability Pr[Sk+l = sk|Sk\u22121+l = sk\u22121] = p(sk|sk\u22121), (1)\nwhere l is an integer. Each realization sk of the random variable Sk takes values 1, ..., L. We assume that S is mixing: it has a unique stationary distribution pst(s), \u2211L r=1p(s|r)pst(r) = pst(s), that is established from any initial probability in the long time limit.\nLet random variables Xi, with realizations xi = 1, ..,M , be noisy observations of Si: the (timeinvariant) conditional probability of observing Xi = xi given the realization Si = si of the Markov process is \u03c0(xk|sk). Defining x \u2261 (xN , ..., x1), s \u2261 (sN , ..., s0), the joint probability of S,X reads\nP (s,x) = TsN sN\u22121(xN )...Ts1 s0(x1) pst(s0), (2)\nwhere the L\u00d7 L transfer-matrix T (x) with matrix elements Tsi si\u22121(x) is defined as Tsi si\u22121(x) = \u03c0(x|si) p(si|si\u22121). (3)\nX = {X1,X2, ...} is called a hidden Markov process. Generally, it is not Markov, but it inherits stationarity and mixing from S [9]. The probabilities for X can be represented as follows:\nP (x) = \u2211\nss\u2032 [T(x)]ss\u2032 pst(s\n\u2032), T(x) \u2261 T (xN )T (xN\u22121) . . . T (x1), (4)\nwhere T(x) is a product of transfer matrices."}, {"heading": "3 Parameter Estimation", "text": ""}, {"heading": "3.1 Maximum Likelihood Estimation", "text": "The unknown parameters of an HMM are the transition probabilities p(s|s\u2032) of the Markov process and the observation probabilities \u03c0(x|s); see (2). They have to be estimated from the observed sequence x. This is standardly done via the maximum-likelihood approach: one starts with some\ntrial values p\u0302(s|s\u2032), \u03c0\u0302(x|s) of the parameters and calculates the (log)-likelihood ln P\u0302 (x), where P\u0302 means the probability (4) calculated at the trial values of the parameters. Next, one maximizes ln P\u0302 (x) over p\u0302(s|s\u2032) and \u03c0\u0302(x|s) for the given observed sequence x (in practice this is done via the Baum-Welch algorithm [20, 9]). The rationale of this approach is as follows. Provided that the length N of the observed sequence is long, and recaling that X is mixing (due to the analogous feature of S) we get probability-one convergence (law of large numbers) [9]:\nln P\u0302 (x) \u2192 \u2211\ny\nP (y) ln P\u0302 (y), (5)\nwhere the average is taken over the true probability P (...) that generated x. Since the relative entropy is non-negative, \u2211 x P (x) ln[P (x)/P\u0302 (x)] \u2265 0, the global maximum of \u2211 x P (x) ln P\u0302 (x) as a function of p\u0302(s|s\u2032) and \u03c0\u0302(x|s) is reached for p\u0302(s|s\u2032) = p(s|s\u2032) and \u03c0\u0302(x|s) = \u03c0(x|s). This argument is silent on how unique this global maximum is and how difficult to reach it."}, {"heading": "3.2 Viterbi Training", "text": "An alternative approach to the parameter learning employs the maximal a posteriori (MAP) estimation and proceeds as follows: Instead of maximizing the likelihood of observed data (5) one tries to maximize the probability of the most likely sequence [20, 9]. Given the joint probability P\u0302 (s,x) at trial values of parameters, and given the observed sequence x, one estimates the generating statesequence s via maximizing the a posteriori probability\nP\u0302 (s|x) = P\u0302 (s,x)/P\u0302 (x) (6)\nover s. Since P\u0302 (x) does not depend on s, one can maximize ln P\u0302 (s,x). If the number of observations is sufficiently large N \u2192 \u221e, one can substitute maxs ln P\u0302 (s,x) by its average over P (...) [see (5)] and instead maximize (over model parameters)\n\u2211 x P (x)maxs ln P\u0302 (s,x). (7)\nTo relate (7) to the free energy concept (see e.g. [2, 4]), we define an auxiliary (Gibbsian) probability\n\u03c1\u0302\u03b2(s|x) = P\u0302 \u03b2(s,x)/ [\u2211\ns\u2032 P\u0302 \u03b2(s\u2032,x)\n] , (8)\nwhere \u03b2 > 0 is a parameter. As a function of s (and for a fixed x), \u03c1\u0302\u03b2\u2192\u221e(s|x) concentrates on those s that maximize ln P\u0302 (s,x):\n\u03c1\u0302\u03b2\u2192\u221e(s|x) \u2192 1 N \u2211 j \u03b4[s, s\u0303[j](x)], (9)\nwhere \u03b4(s, s\u2032) is the Kronecker delta, s\u0303[j](x) are equivalent outcomes of the maximization, and N is the number of such outcomes. Further, define\nF\u03b2 \u2261 \u2212 1\n\u03b2 \u2211 x P (x) ln \u2211 s P\u0302 \u03b2(s,x). (10)\nWithin statistical mechanics Eqs. 8 and 10 refer to, respectively, the Gibbs distribution and free energy of a physical system with Hamiltonian H = \u2212 lnP (s,x) coupled to a thermal bath at inverse temperature \u03b2 = 1/T [2, 4]. It is then clear that ML and Viterbi Training correspond to minimizing the free energy Eq. 10 at \u03b2 = 1 and \u03b2 = \u221e, respectively. Note that \u03b22\u2202\u03b2F = \u2212\u2211\nx P (x) \u2211 s \u03c1\u03b2(s|x) ln \u03c1\u03b2(s|x) \u2265 0, which yields F1 \u2264 F\u221e."}, {"heading": "3.3 Local Optimization", "text": "As we mentioned, global maximization of neither objective is feasible in the general case. Instead, in practice this maximization is locally implemented via an EM-type algorithm [20, 9]: for a given observed sequence x, and for some initial values of the parameters, one calculates the expected value of the objective function under the trial parameters (E-step), and adjusts the parameters to maximize this expectation (M-step). The resulting estimates of the parameters are now employed as new trial parameters and the previous step is repeated. This recursion continues till convergence.\nFor our purposes, this procedure can be understood as calculating certain statistics of the hidden sequence averaged over the Gibbs distribution Eqs. 8. Indeed, let us introduce f\u03b3(s) \u2261 e\u03b2\u03b3 \u2211 N i=1\n\u03b4(si+1,a)\u03b4(si,b) and define \u03b2F\u03b2(\u03b3) \u2261 \u2212 \u2211 x P (x) ln \u2211 s P\u0302 \u03b2(s,x)f\u03b3(s). (11)\nThen, for instance, the (iterative) Viterbi estimate of the transition probabilities are given as follows:\nP\u0303 (Sk+1 = a, Sk = b) = \u2212\u2202\u03b3 [F\u221e(\u03b3)]|\u03b3\u21920. (12) Conditional probabilities for observations are calculated similarly via a different indicator function."}, {"heading": "4 Generating Function", "text": "Note from (4) that both P (x) and P\u0302 (x) are obtained as matrix-products. For a large number of multipliers the behavior of such products is governed by the multiplicative law of large numbers. We now recall its formulation from [10]: for N \u2192 \u221e and x generated by the mixing process X there is a probability-one convergence:\n1 N ln ||T(x)|| \u2192 1 N \u2211 y P (y) ln \u03bb[T(y)], (13)\nwhere ||...|| is a matrix norm in the linear space of L \u00d7 L matrices, and \u03bb[T(x)] is the maximal eigenvalue of T(x). Note that (13) does not depend on the specific norm chosen, because all norms in the finite-dimensional linear space are equivalent; they differ by a multiplicative factor that disappears for N \u2192 \u221e [10]. Eqs. (4, 13) also imply \u2211\nx \u03bb[T(x)] \u2192 1. Altogether, we calculate (5) via\nits probability-one limit 1\nN \u2211 x P (x) ln P\u0302 (x) \u2192 1 N \u2211 x \u03bb[T(x)] ln \u03bb[T\u0302(x)]. (14)\nNote that the multiplicative law of large numbers is normally formulated for the maximal singular value. Its reformulation in terms of the maximal eigenvalue needs additional arguments [1].\nIntroducing the generating function\n\u039bN (n,N) = \u2211\nx\n\u03bb[T(x)]\u03bbn [ T\u0302(x) ] , (15)\nwhere n is a non-negative number, and where \u039bN (n,N) means \u039b(n,N) in degree of N , one represents (14) as\n1\nN \u2211 x\n\u03bb[T(x)] ln \u03bb[T\u0302(x)] = \u2202n\u039b(n,N)|n=0, (16) where we took into account \u039b(0, N) = 1, as follows from (15).\nThe behavior of \u039bN (n,N) is better understood after expressing it via the zeta-function \u03be(z, n) [1]\n\u03be(z, n) = exp [ \u2212 \u2211\u221e\nm=1\nzm m \u039bm(n,m)\n] , (17)\nwhere \u039bm(n,m) \u2265 0 is given by (15). Since for a large N , \u039bN (n,N) \u2192 \u039bN (n) [this is the content of (13)], the zeta-function \u03be(z, n) has a zero at z = 1\u039b(n) :\n\u03be(1/\u039b(n), n) = 0. (18)\nIndeed for z close (but smaller than) 1\u039b(n) , the series \u2211\u221e m=1 zm m \u039bm(n,m) \u2192 \u2211\u221em=1 [z\u039b(n)]m m almost diverges and one has \u03be(z, n) \u2192 1 \u2212 z\u039b(n). Recalling that \u039b(0) = 1 and taking n \u2192 0 in 0 = d dn\u03be( 1 \u039b(n) , n), we get from (16)\n1\nN \u2211 x \u03bb[T(x)] ln \u03bb[T\u0302(x)] = \u2202n\u03be(1, 0) \u2202z\u03be(1, 0) . (19)\nFor calculating \u2212\u03b2F\u03b2 in (10) we have instead of (19)\n\u2212\u03b2F\u03b2 N = \u2202n\u03be\n[\u03b2](1, 0)\n\u2202z\u03be[\u03b2](1, 0) , (20)\nwhere \u03be[\u03b2](z, n) employs T\u0302 \u03b2si si\u22121(x) = \u03c0\u0302 \u03b2(x|si) p\u0302\u03b2(si|si\u22121) instead of T\u0302si si\u22121(x) in (19).\nThough in this paper we restricted ourselves to the limit N \u2192 \u221e, we stress that the knowledge of the generating function \u039bN(n,N) allows to analyze the learning algorithms for any finite N ."}, {"heading": "5 Hidden Markov Model with One Unambiguous Symbol", "text": ""}, {"heading": "5.1 Definition", "text": "Given a L-state Markov process S, the observed process X has two states 1 and 2; see Fig. 1. All internal states besides one are observed as 2, while the internal state 1 produces, respectively, 1 and 2 with probabilities 1 \u2212 \u01eb and \u01eb. For L = 3 we obtain from (1) \u03c0(1|1) = 1 \u2212 \u03c0(2|1) = 1 \u2212 \u01eb, \u03c0(1|2) = \u03c0(1|3) = \u03c0(2|1) = 0, \u03c0(2|2) = \u03c0(2|3) = 1. Hence 1 is unambiguous: if it is observed, the unobserved process S was certainly in 1; see Fig. 1. The simplest example of such HMM exists already for L = 2; see [12] for analytical features of entropy for this case. We, however, describe in detail the L = 3 situation, since this case will be seen to be generic (in contrast to L = 2) and it allows straightforward generalizations to L > 3. The transition matrix (1) of a general L = 3 Markov process reads\nP \u2261 { p(s|s\u2032) }3s,s\u2032=1 = (\np0 q1 r1 p1 q0 r2 p2 q2 r0\n) , ( p0 q0 r0 ) = ( 1\u2212 p1 \u2212 p2 1\u2212 r1 \u2212 r2 1\u2212 r1 \u2212 r2 ) (21)\nwhere, e.g., q1 = p(1|2) is the transition probability 2 \u2192 1; see Fig. 1. The corresponding transfer matrices read from (3)\nT (1) = (1\u2212 \u01eb) (\np0 q1 r1 0 0 0 0 0 0\n) , T (2) = P\u2212 T (1). (22)\nEq. (22) makes straightforward the reconstruction of the transfer-matrices for L \u2265 4. It should also be obvious that for all L only the first row of T (1) consists of non-zero elements.\nFor \u01eb = 0 we get from (22) the simplest example of an aggregated HMM, where several Markov states are mapped into one observed state. This model plays a special role for the HMM theory, since it was employed in the pioneering study of the non-identifiability problem [7]."}, {"heading": "5.2 Solution of the Model", "text": "For this model \u03be(z, n) can be calculated exactly, because T (1) has only one non-zero row. Using the method outlined in the supplementary material (see also [1, 3]) we get\n\u03be(z, n) = 1\u2212 z(t0t\u0302n0 + \u03c40\u03c4\u0302n0 ) + \u2211\u221e\nk=2 [\u03c4 \u03c4\u0302n t\u0302nk\u22122tk\u22122 \u2212 t\u0302nk\u22121tk\u22121]zk (23)\nwhere \u03c4 and \u03c4\u0302 are the largest eigenvalues of T (2) and T\u0302 (2), respectively\ntk \u2261 \u30081|T (1)T (2)k|1\u3009 = \u2211L\n\u03b1=1 \u03c4k\u03b1\u03c8\u03b1, (24)\n\u03c8\u03b1 \u2261 \u30081|T (1)|R\u03b1\u3009\u3008L\u03b1|1\u3009, \u30081| \u2261 (1, 0, . . . , 0). (25) Here |R\u03b1\u3009 and \u3008L\u03b1| are, respectively right and left eigenvalues of T (2), while \u03c41, . . . , \u03c4L (\u03c4L \u2261 \u03c4 ) are the eigenvalues of T (2). Eqs. (24, 25) obviously extend to hatted quantities.\nWe get from (23, 19):\n\u03be(1, n) = (1\u2212 \u03c4\u0302n\u03c4) ( 1\u2212 \u2211\u221e k=0 t\u0302nk tk ) , (26)\n\u2202n\u03be(1, 0) \u2202z\u03be(1, 0) = \u2211\u221e k=0tk ln[t\u0302k]\u2211\u221e k=0(k + 1)tk . (27)\nNote that for \u01eb = 0, tk are return probabilities to the state 1 of the L-state Markov process. For \u01eb > 0 this interpretation does not hold, but tk still has a meaning of probability as \u2211\u221e k=0tk = 1.\nTurning to equations (19, 27) for the free energy, we note that as a function of trial values it depends on the following 2L parameters:\n(\u03c4\u03021, . . . , \u03c4\u0302L, \u03c8\u03021, . . . , \u03c8\u0302L). (28)\nAs a function of the true values, the free energy depends on the same 2L parameters (28) [without hats], though concrete dependencies are different. For the studied class of HMM there are at most L(L \u2212 1) + 1 unknown parameters: L(L \u2212 1) transition probabilities of the unobserved Markov chain, and one parameter \u01eb coming from observations. We checked numerically that the Jacobian of the transformation from the unknown parameters to the parameters (28) has rank 2L \u2212 1. Any 2L\u2212 1 parameters among (28) can be taken as independent ones. For L > 2 the number of effective independent parameters that affect the free energy is smaller than the number of parameters. So if the number of unknown parameters is larger than 2L \u2212 1, neither of them can be found explicitly. One can only determine the values of the effective parameters."}, {"heading": "6 The Simplest Non-Trivial Scenario", "text": "The following example allows the full analytical treatment, but is generic in the sense that it contains all the key features of the more general situation given above (21). Assume that L = 3 and\nq0 = q\u03020 = r0 = r\u03020 = 0, \u01eb = \u01eb\u0302 = 0. (29)\nNote the following explicit expressions\nt0 = p0, t1 = p1q1 + p2r1, t2 = p1r1q2 + p2q1r2, (30)\n\u03c4 = \u03c43 = \u221a q2r2, \u03c42 = \u2212\u03c4, \u03c41 = 0, (31)\n\u03c83 \u2212 \u03c82 = t1/\u03c4, \u03c83 + \u03c82 = t2/\u03c42, (32)\nEqs. (30\u201332) with obvious changes si \u2192 s\u0302i for every symbol si hold for t\u0302k, \u03c4\u0302k and \u03c8\u0302k. Note a consequence of \u22112 k=0pk = \u22112 k=0qk = \u22112 k=0rk = 1:\n\u03c42(1 \u2212 t0) = 1\u2212 t0 \u2212 t1 \u2212 t2. (33)"}, {"heading": "6.1 Optimization of F1", "text": "Eqs. (27) and (30\u201332) imply \u2211\u221e\nk=0(k + 1)tk = \u00b5 1\u2212\u03c42 ,\n\u00b5 \u2261 1\u2212 \u03c42 + t2 + (1\u2212 t0)(1 + \u03c42) > 0, (34)\n\u2212\u00b5F1 N = t1 ln t\u03021 + t2 ln t\u03022 + (1 \u2212 \u03c42)t0 ln t\u03020 + (1\u2212 t0)\u03c42 ln \u03c4\u03022 . (35)\nThe free energyF1 depends on three independent parameters t\u03020, t\u03021, t\u03022 [recall (33)]. Hence, minimizing F1 we get t\u0302i = ti (i = 0, 1, 2), but we do not obtain a definite solution for the unknown parameters: any four numbers p\u03021, p\u03022, q\u03021, r\u03021 satisfying three equations t0 = 1\u2212 p\u03021 \u2212 p\u03022, t1 = p\u03021q\u03021 + p\u03022r\u03021, t2 = p\u03021r\u03021(1\u2212 q\u03021) + p\u03022q\u03021(1 \u2212 r\u03021), minimize F1."}, {"heading": "6.2 Optimization of F\u221e", "text": "In deriving (35) we used no particular feature of {p\u0302k}2k=0, {q\u0302k}2k=1, {r\u0302k}2k=1. Hence, as seen from (20), the free energy at \u03b2 > 0 is recovered from (35) by equating its LHS to \u2212\u03b2F\u03b2\nN and by taking in\nits RHS: t\u03020 \u2192 p\u0302\u03b20 , \u03c4\u03022 \u2192 q\u0302\u03b22 r\u0302\u03b22 , t\u03021 \u2192 p\u0302\u03b21 q\u0302\u03b21 + p\u0302\u03b22 r\u0302\u03b21 , t\u03022 \u2192 p\u0302\u03b21 r\u0302\u03b21 q\u0302\u03b22 + p\u0302\u03b22 q\u0302\u03b21 r\u0302\u03b22 . The zero-temperature free energy reads from (35)\n\u2212\u00b5F\u221e N\n= (1 \u2212 \u03c42)t0 ln t\u03020 + (1\u2212 t0)\u03c42 ln \u03c4\u03022 + t1 lnmax[p\u03021q\u03021, p\u03022r\u03021] + t2 lnmax[p\u03022q\u03021r\u03022, p\u03021r\u03021q\u03022]. (36)\nWe now minimize F\u221e over the trial parameters p\u03021, p\u03022, q\u03021, r\u03021. This is not what is done by the VT algorithm; see the discussion after (12). But at any rate both procedures aim to minimize the same target. VT recursion for this models will be studied in section 6.3 \u2014 it leads to the same result. Minimizing F\u221e over the trial parameters produces four distinct solutions:\n{\u03c3\u0302i}4i=1 = {p\u03021 = 0, p\u03022 = 0, q\u03021 = 0, r\u03021 = 0}. (37) For each of these four solutions: t\u0302i = ti (i = 0, 1, 2) and F1 = F\u221e. The easiest way to get these results is to minimize F\u221e under conditions t\u0302i = ti (for i = 0, 1, 2), obtain F1 = F\u221e and then to conclude that due to the inequality F1 \u2264 F\u221e the conditional minimization led to the global minimization. The logics of (37) is that the unambiguous state tends to get detached from the ambiguous ones, since the probabilities nullifying in (37) refer to transitions from or to the unambiguous state.\nNote that although minimizing either F\u221e and F1 produces correct values of the independent variables t0, t1, t2, in the present situation minimizing F\u221e is preferable, because it leads to the four-fold degenerate set of solutions (37) instead of the continuously degenerate set. For instance, if the solution with p\u03021 = 0 is chosen we get for other parameters\np\u03022 = 1\u2212 t0, q\u03021 = t2\n1\u2212 t0 \u2212 t1 , r\u03021 = t1 1\u2212 t0 . (38)\nFurthermore, a more elaborate analysis reveals that for each fixed set of correct parameters only one among the four solutions Eq. 37 provides the best value for the quality of the MAP reconstruction, i.e. for the overlap between the original and MAP-decoded sequences.\nFinally, we note that minimizing F\u221e allows one to get the correct values t0, t1, t2 of the independent variables t\u03020, t\u03021 and t\u03022 only if their number is less than the number of unknown parameters. This is not a drawback, since once the number of unknown parameters is sufficiently small [less than four for the present case (29)] their exact values are obtained by minimizing F1. Even then, the minimization of F\u221e can provide partially correct answers. Assume in (36) that the parameter r\u03021 is known, r\u03021 = r1. Now F\u221e has three local minima given by p\u03021 = 0, p\u03022 = 0 and q\u03021 = 0; cf. with (37). The minimum with p\u03022 = 0 is the global one and it allows to obtain the exact values of the two effective parameters: t\u03020 = 1 \u2212 p\u03021 = t0 and t\u03021 = p\u03021q\u03021 = t1. These effective parameters are recovered, because they do not depend on the known parameter r\u03021 = r1. Two other minima have greater values of F\u221e, and they allow to recover only one effective parameter: t\u03020 = 1 \u2212 p\u03021 = t0. If in addition to r\u03021 also q\u03021 is known, the two local minimia of F\u221e (p\u03021 = 0 and p\u03022 = 0) allow to recover t\u03020 = t0 only. In contrast, if p\u03021 = p1 (or p\u03022 = p2) is known exactly, there are three local minima again\u2014p\u03022 = 0, q\u03021 = 0, r\u03021 = 0\u2014but now none of effective parameters is equal to its true value: t\u0302i 6= ti (i = 0, 1, 2)."}, {"heading": "6.3 Viterbi EM", "text": "Recall the description of the VT algorithm given after (12). For calculating P\u0303 (Sk+1 = a, Sk = b) via (11, 12) we modify the transfer matrix element in (15, 17) as T\u0302ab(k) \u2192 T\u0302ab(k)e\u03b3 , which produces from (11, 12) for the MAP-estimates of the transition probabilities\np\u03031 = t1\u03c7\u03021 + t2\u03c7\u03022\nt1 + t2 + t0(1\u2212 \u03c42) , p\u03032 = 1\u2212 t0 \u2212 p\u03031, (39)\nq\u03031 = t1\u03c7\u03021 + t2(1\u2212 \u03c7\u03022)\nt1\u03c7\u03021 + t2 + (1\u2212 t0)\u03c42 , q\u03032 = 1\u2212 q\u03031 (40)\nr\u03031 = t1(1\u2212 \u03c7\u03021) + t2\u03c7\u03022\nt2 + t1(1 \u2212 \u03c7\u03021) + (1\u2212 t0)\u03c42 r\u03032 = 1\u2212 r\u03031, (41)\nwhere \u03c7\u03021 \u2261 p\u0302 \u03b2 1 q\u0302 \u03b2 1\np\u0302 \u03b2 1 q\u0302 \u03b2 1 +p\u0302\u03b2 2 r\u0302 \u03b2 1\n, \u03c7\u03022 \u2261 p\u0302 \u03b2 1 r\u0302 \u03b2 1 q\u0302 \u03b2 2\np\u0302 \u03b2 1 r\u0302 \u03b2 1 q\u0302 \u03b2 2 +p\u0302\u03b2 2 r\u0302 \u03b2 2 q\u0302 \u03b2 1 . The \u03b2 \u2192 \u221e limit of \u03c7\u03021 and \u03c7\u03022 is obvious: each of them is equal to 0 or 1 depending on the ratios p\u03021q\u03021\np\u03022 r\u03021 and p\u03021 r\u03021q\u03022 p\u03022 r\u03022q\u03021 . The EM approach amounts to\nstarting with some trial values p\u03021, p\u03022, q\u03021, r\u03021 and using p\u03031, p\u03032, q\u03031, r\u03031 as new trial parameters (and so on). We see from (39\u201341) that the algorithm converges just in one step: (39\u201341) are equal to the parameters given by one of four solutions (37)\u2014which one among the solutions (37) is selected depends on the on initial trial parameters in (39\u201341)\u2014recovering the correct effective parameters (30\u201332); e.g. cf. (38) with (39, 41) under \u03c7\u03021 = \u03c7\u03022 = 0. Hence, VT converges in one step in contrast to the Baum-Welch algorithm (that uses EM to locally minimize F1) which, for the present model, obviously does not converge in one step. There is possibly a deeper point in the one-step convergence that can explain why in practice VT converges faster than the Baum-Welch algorithm [9, 21]: recall that, e.g. the Newton method for local optimization works precisely in one step for quadratic functions, but generally there is a class of functions, where it performs faster than (say) the steepest descent method. Further research should show whether our situation is similar: the VT works just in one step for this exactly solvable HMM model that belongs to a class of models, where VT generally performs faster than ML.\nWe conclude this section by noting that the solvable case (29) is generic: its key results extend to the general situation defined above (21). We checked this fact numerically for several values of L. In particular, the minimization of F\u221e nullifies as many trial parameters as necessary to express the remaining parameters via independent effective parameters t0, t1, . . .. Hence for L = 3 and \u01eb = 0 two such trial parameters are nullified; cf. with discussion around (28). If the true error probability \u01eb 6= 0, the trial value \u01eb\u0302 is among the nullified parameters. Again, there is a discrete degeneracy in solutions provided by minimizing F\u221e."}, {"heading": "7 Summary", "text": "We presented a method for analyzing two basic techniques for parameter estimation in HMMs, and illustrated it on a specific class of HMMs with one unambiguous symbol. The virtue of this class of models is that it is exactly solvable, hence the sought quantities can be obtained in a closed form via generating functions. This is a rare occasion, because characteristics of HMM such as likelihood or entropy are notoriously difficult to calculate explicitly [1]. An important feature of the example considered here is that the set of unknown parameters is not completely identifiable in the maximum likelihood sense [7, 14]. This corresponds to the zero eigenvalue of the Hessian for the ML (maximum-likelihood) objective function. In practice, one can have weaker degeneracy of the objective function resulting in very small values for the Hessian eigenvalues. This scenario occurs often in various models of physics and computational biology [11]. Hence, it is a drawback that the theory of HMM learning was developed assuming complete identifiably [5].\nOne of our main result is that in contrast to the ML approach that produces continuously degenerate solutions, VT results in finitely degenerate solution that is sparse, i.e., some [non-identifiable] parameters are set to zero, and, furthermore, converges faster. Note that sparsity might be a desired feature in many practical applications. For instance, imposing sparsity on conventional EM-type learning has been shown to produce better results part of speech tagging applications [25]. Whereas [25] had to impose sparsity via an additional penalty term in the objective function, in our case sparsity is a natural outcome of maximizing the likelihood of the best sequence. While our results were obtained on a class of exactly-solvable model, it is plausible that they hold more generally.\nThe fact that VT provides simpler and more definite solutions\u2014among all choices of the parameters compatible with the observed data\u2014can be viewed as a type of the Occam\u2019s razor for the parameter learning. Note finally that statistical mechanics intuition behind these results is that the aposteriori likelihood is (negative) zero-temperature free energy of a certain physical system. Minimizing this free energy makes physical sense: this is the premise of the second law of thermodynamics that ensures relaxation towards a more equilibrium state. In that zero-temperature equilibrium state certain types of motion are frozen, which means nullifying the corresponding transition probabilities. In that way the second law relates to the Occam\u2019s razor. Other connections of this type are discussed in [15]."}, {"heading": "Acknowledgments", "text": "This research was supported in part by the US ARO MURI grant No. W911NF0610094 and US DTRA grant HDTRA1-10-1-0086."}], "references": [{"title": "Entropy of Hidden Markov Processes via Cycle Expansion", "author": ["A.E. Allahverdyan"], "venue": "J. Stat. Phys. 133, 535 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "On Maximum a Posteriori Estimation of Hidden Markov Processes", "author": ["A.E. Allahverdyan", "A. Galstyan"], "venue": "Proc. of UAI, ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "E", "author": ["R. Artuso"], "venue": "Aurell and P. Cvitanovic, Recycling of strange sets, Nonlinearity 3, 325 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Bioinformatics", "author": ["P. Baldi", "S. Brunak"], "venue": "MIT Press, Cambridge, USA ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Ann. Math. Stat. 37, 1554 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1966}, {"title": "Estimation of stochastic context-free grammars and their use as language models", "author": ["J.M. Benedi", "J.A. Sanchez"], "venue": "Comp. Speech and Lang. 19, pp. 249-274 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "On the identifiability problem for functions of finite Markov chains", "author": ["D. Blackwell", "L. Koopmans"], "venue": "Ann. Math. Statist. 28, 1011 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1957}, {"title": "Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization", "author": ["S.B. Cohen", "N.A. Smith"], "venue": "Procs. of ACL ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Hidden Markov processes", "author": ["Y. Ephraim", "N. Merhav"], "venue": "IEEE Trans. Inf. Th., 48, 1518-1569, ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Lyapunov indices of a product of random matrices", "author": ["L.Y. Goldsheid", "G.A. Margulis"], "venue": "Russ. Math. Surveys 44, 11 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1989}, {"title": "Universally Sloppy Parameter Sensitivities in Systems Biology Models", "author": ["R.N. Gutenkunst"], "venue": "PLoS Computational Biology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Analyticity of entropy rate of hidden Markov chains", "author": ["G. Han", "B. Marcus"], "venue": "IEEE Trans. Inf. Th., 52, 5251 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Matrix Analysis (Cambridge", "author": ["R.A. Horn", "C.R. Johnson"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Identifiability of Hidden Markov Information Sources", "author": ["H. Ito", "S. Amari", "K. Kobayashi"], "venue": "IEEE Trans. Inf. Th., 38, 324 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "On causally asymmetric versions of Occam\u2019s Razor and their relation to thermodynamics", "author": ["D. Janzing"], "venue": "arXiv:0708.3411 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "The segmental k-means algorithm for estimating parameters of hidden Markov models", "author": ["B.H. Juang", "L.R. Rabiner"], "venue": "IEEE Trans. Acoustics, Speech, and Signal Processing, ASSP-38, no.9, pp.1639-1641, ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Maximum-Likelihood Estimation for Hidden Markov Models", "author": ["B.G. Leroux"], "venue": "Stochastic Processes and Their Applications, 40, 127 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Maximum likelihood hidden Markov modeling using a dominant sequence of states", "author": ["N. Merhav", "Y. Ephraim"], "venue": "IEEE Transactions on Signal Processing, vol.39, no.9, pp.2111-2115 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1991}, {"title": "Restoration of single-channel currents using the segmental k-means method based on hidden Markov modeling", "author": ["F. Qin"], "venue": "Biophys J 86, 14881501 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proc. IEEE, 77, 257 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "Comparative Study of the Baum-Welch and Viterbi Training Algorithms", "author": ["L.J. Rodriguez", "I. Torres"], "venue": "Pattern Recognition and Image Analysis, Lecture Notes in Computer Science, 2652/2003, 847 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Statistical Mechanics", "author": ["D. Ruelle"], "venue": "Thermodynamic Formalism, ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1978}, {"title": "Comparison between the inside-outside algorithm and the Viterbi algorithm for stochastic context-free grammars", "author": ["J. Sanchez", "J. Benedi", "F. Casacuberta"], "venue": "Adv. in Struct. and Synt. Pattern Recognition ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Viterbi Training Improves Unsupervised Dependency Parsing", "author": ["V.I. Spitkovsky", "H. Alshawi", "D. Jurafsky", "C.D. Manning"], "venue": "Proc. of the 14th Conference on Computational Natural Language Learning ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "The inference problems of HMM naturally divide into two classes [20, 9]: i) recovering the hidden sequence of states given the observed sequence, and ii) estimating the model parameters (transition probabilities of the hidden Markov chain and/or conditional probabilities of observations) from the observed sequence.", "startOffset": 64, "endOffset": 71}, {"referenceID": 8, "context": "The inference problems of HMM naturally divide into two classes [20, 9]: i) recovering the hidden sequence of states given the observed sequence, and ii) estimating the model parameters (transition probabilities of the hidden Markov chain and/or conditional probabilities of observations) from the observed sequence.", "startOffset": 64, "endOffset": 71}, {"referenceID": 19, "context": "The first class of problems is usually solved via the maximum a posteriori (MAP) method and its computational implementation known as Viterbi algorithm [20, 9].", "startOffset": 152, "endOffset": 159}, {"referenceID": 8, "context": "The first class of problems is usually solved via the maximum a posteriori (MAP) method and its computational implementation known as Viterbi algorithm [20, 9].", "startOffset": 152, "endOffset": 159}, {"referenceID": 19, "context": "Since global optimization is generally intractable, in practice it is implemented through an expectation\u2013 maximization (EM) procedure known as Baum\u2013Welch algorithm [20, 9].", "startOffset": 164, "endOffset": 171}, {"referenceID": 8, "context": "Since global optimization is generally intractable, in practice it is implemented through an expectation\u2013 maximization (EM) procedure known as Baum\u2013Welch algorithm [20, 9].", "startOffset": 164, "endOffset": 171}, {"referenceID": 7, "context": "Maximizing VT objective function is hard [8], so in practice it is implemented via an EM-style iterations between calculating the MAP sequence and adjusting the model parameters based on the sequence statistics.", "startOffset": 41, "endOffset": 44}, {"referenceID": 16, "context": "It is known that VT lacks some of the desired features of ML estimation such as consistency [17], and in fact, can produce biased estimates [9].", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "It is known that VT lacks some of the desired features of ML estimation such as consistency [17], and in fact, can produce biased estimates [9].", "startOffset": 140, "endOffset": 143}, {"referenceID": 15, "context": "However, it has been shown to perform well in practice, which explains its widespread use in applications such as speech recognition [16], unsupervised dependency parsing [24], grammar induction [6], ion channel modeling [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "However, it has been shown to perform well in practice, which explains its widespread use in applications such as speech recognition [16], unsupervised dependency parsing [24], grammar induction [6], ion channel modeling [19].", "startOffset": 171, "endOffset": 175}, {"referenceID": 5, "context": "However, it has been shown to perform well in practice, which explains its widespread use in applications such as speech recognition [16], unsupervised dependency parsing [24], grammar induction [6], ion channel modeling [19].", "startOffset": 195, "endOffset": 198}, {"referenceID": 18, "context": "However, it has been shown to perform well in practice, which explains its widespread use in applications such as speech recognition [16], unsupervised dependency parsing [24], grammar induction [6], ion channel modeling [19].", "startOffset": 221, "endOffset": 225}, {"referenceID": 23, "context": "It is generally assumed that VT is more robust and faster but usually less accurate, although for certain tasks it outperforms conventional EM [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "[18] established", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "A similar attempt to compare both approaches on discrete models (for stochastic context free grammars) was presented in [23].", "startOffset": 120, "endOffset": 124}, {"referenceID": 0, "context": "Previously, a similar approach was used for calculating entropy rate of a hidden Markov process [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": ", the model is not identifiable [7, 14, 9].", "startOffset": 32, "endOffset": 42}, {"referenceID": 13, "context": ", the model is not identifiable [7, 14, 9].", "startOffset": 32, "endOffset": 42}, {"referenceID": 8, "context": ", the model is not identifiable [7, 14, 9].", "startOffset": 32, "endOffset": 42}, {"referenceID": 8, "context": "Generally, it is not Markov, but it inherits stationarity and mixing from S [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 19, "context": "Next, one maximizes ln P\u0302 (x) over p\u0302(s|s\u2032) and \u03c0\u0302(x|s) for the given observed sequence x (in practice this is done via the Baum-Welch algorithm [20, 9]).", "startOffset": 145, "endOffset": 152}, {"referenceID": 8, "context": "Next, one maximizes ln P\u0302 (x) over p\u0302(s|s\u2032) and \u03c0\u0302(x|s) for the given observed sequence x (in practice this is done via the Baum-Welch algorithm [20, 9]).", "startOffset": 145, "endOffset": 152}, {"referenceID": 8, "context": "Provided that the length N of the observed sequence is long, and recaling that X is mixing (due to the analogous feature of S) we get probability-one convergence (law of large numbers) [9]: ln P\u0302 (x) \u2192 \u2211", "startOffset": 185, "endOffset": 188}, {"referenceID": 19, "context": "An alternative approach to the parameter learning employs the maximal a posteriori (MAP) estimation and proceeds as follows: Instead of maximizing the likelihood of observed data (5) one tries to maximize the probability of the most likely sequence [20, 9].", "startOffset": 249, "endOffset": 256}, {"referenceID": 8, "context": "An alternative approach to the parameter learning employs the maximal a posteriori (MAP) estimation and proceeds as follows: Instead of maximizing the likelihood of observed data (5) one tries to maximize the probability of the most likely sequence [20, 9].", "startOffset": 249, "endOffset": 256}, {"referenceID": 1, "context": "[2, 4]), we define an auxiliary (Gibbsian) probability", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[2, 4]), we define an auxiliary (Gibbsian) probability", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "8 and 10 refer to, respectively, the Gibbs distribution and free energy of a physical system with Hamiltonian H = \u2212 lnP (s,x) coupled to a thermal bath at inverse temperature \u03b2 = 1/T [2, 4].", "startOffset": 183, "endOffset": 189}, {"referenceID": 3, "context": "8 and 10 refer to, respectively, the Gibbs distribution and free energy of a physical system with Hamiltonian H = \u2212 lnP (s,x) coupled to a thermal bath at inverse temperature \u03b2 = 1/T [2, 4].", "startOffset": 183, "endOffset": 189}, {"referenceID": 19, "context": "Instead, in practice this maximization is locally implemented via an EM-type algorithm [20, 9]: for a given observed sequence x, and for some initial values of the parameters, one calculates the expected value of the objective function under the trial parameters (E-step), and adjusts the parameters to maximize this expectation (M-step).", "startOffset": 87, "endOffset": 94}, {"referenceID": 8, "context": "Instead, in practice this maximization is locally implemented via an EM-type algorithm [20, 9]: for a given observed sequence x, and for some initial values of the parameters, one calculates the expected value of the objective function under the trial parameters (E-step), and adjusts the parameters to maximize this expectation (M-step).", "startOffset": 87, "endOffset": 94}, {"referenceID": 9, "context": "We now recall its formulation from [10]: for N \u2192 \u221e and x generated by the mixing process X there is a probability-one convergence: 1 N ln ||T(x)|| \u2192 1 N \u2211", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Note that (13) does not depend on the specific norm chosen, because all norms in the finite-dimensional linear space are equivalent; they differ by a multiplicative factor that disappears for N \u2192 \u221e [10].", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "Its reformulation in terms of the maximal eigenvalue needs additional arguments [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "The behavior of \u039b (n,N) is better understood after expressing it via the zeta-function \u03be(z, n) [1]", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "The simplest example of such HMM exists already for L = 2; see [12] for analytical features of entropy for this case.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "This model plays a special role for the HMM theory, since it was employed in the pioneering study of the non-identifiability problem [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Using the method outlined in the supplementary material (see also [1, 3]) we get \u03be(z, n) = 1\u2212 z(t0t\u0302n0 + \u03c40\u03c4\u0302 0 ) + \u2211\u221e", "startOffset": 66, "endOffset": 72}, {"referenceID": 2, "context": "Using the method outlined in the supplementary material (see also [1, 3]) we get \u03be(z, n) = 1\u2212 z(t0t\u0302n0 + \u03c40\u03c4\u0302 0 ) + \u2211\u221e", "startOffset": 66, "endOffset": 72}, {"referenceID": 8, "context": "There is possibly a deeper point in the one-step convergence that can explain why in practice VT converges faster than the Baum-Welch algorithm [9, 21]: recall that, e.", "startOffset": 144, "endOffset": 151}, {"referenceID": 20, "context": "There is possibly a deeper point in the one-step convergence that can explain why in practice VT converges faster than the Baum-Welch algorithm [9, 21]: recall that, e.", "startOffset": 144, "endOffset": 151}, {"referenceID": 0, "context": "This is a rare occasion, because characteristics of HMM such as likelihood or entropy are notoriously difficult to calculate explicitly [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "An important feature of the example considered here is that the set of unknown parameters is not completely identifiable in the maximum likelihood sense [7, 14].", "startOffset": 153, "endOffset": 160}, {"referenceID": 13, "context": "An important feature of the example considered here is that the set of unknown parameters is not completely identifiable in the maximum likelihood sense [7, 14].", "startOffset": 153, "endOffset": 160}, {"referenceID": 10, "context": "This scenario occurs often in various models of physics and computational biology [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "Hence, it is a drawback that the theory of HMM learning was developed assuming complete identifiably [5].", "startOffset": 101, "endOffset": 104}, {"referenceID": 14, "context": "Other connections of this type are discussed in [15].", "startOffset": 48, "endOffset": 52}], "year": 2013, "abstractText": "We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only finite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam\u2019s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}