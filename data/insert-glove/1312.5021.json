{"id": "1312.5021", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Efficient Online Bootstrapping for Large Scale Learning", "abstract": "Bootstrapping 4,255 is pear a useful patagones technique lenzner for baptist estimating the uncertainty of saltoun a predictor, snaked for example, confidence intervals pathologie for prediction. It tajfel is toiv typically soldatov used 62.7 on small washam to jump5 moderate cabiallavetta sized raaj datasets, due to nfu its iprs high 1,740 computation cost. 111.7 This matures work describes effervescence a highly holborne scalable 70-100 online bootstrapping strategy, implemented cotsen inside Vowpal Wabbit, that is laotians several times beese faster 3,145 than kamenge traditional faton strategies. Our experiments dsta indicate that, ahmedinejad in addition stevenston to providing exhaustively a svilanovic black box - otieno like birdsall method for estimating quas uncertainty, sweetens our rieders implementation menza of kumar online banqueting bootstrapping may also help best-studied to train models non-active with runcie better somebody prediction nicaea performance perwira due 87-74 to model abramoff averaging.", "histories": [["v1", "Wed, 18 Dec 2013 02:10:21 GMT  (87kb,D)", "http://arxiv.org/abs/1312.5021v1", "5 pages, appeared at Big Learning Workshop at Neural Information Processing Systems 2013"]], "COMMENTS": "5 pages, appeared at Big Learning Workshop at Neural Information Processing Systems 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhen qin", "vaclav petricek", "nikos karampatziakis", "lihong li", "john langford"], "accepted": false, "id": "1312.5021"}, "pdf": {"name": "1312.5021.pdf", "metadata": {"source": "CRF", "title": "Efficient Online Bootstrapping for Large Scale Learning", "authors": ["Zhen Qin", "Vaclav Petricek", "Nikos Karampatziakis", "Lihong Li"], "emails": ["zqin001@cs.ucr.edu", "vpetricek@eharmony.com", "nikosk@microsoft.com", "lihongli@microsoft.com", "jcl@microsoft.com"], "sections": [{"heading": null, "text": "Bootstrapping is a useful technique for estimating the uncertainty of a predictor, for example, confidence intervals for prediction. It is typically used on small to moderate sized datasets, due to its high computation cost. This work describes a highly scalable online bootstrapping strategy, implemented inside Vowpal Wabbit, that is several times faster than traditional strategies. Our experiments indicate that, in addition to providing a black box-like method for estimating uncertainty, our implementation of online bootstrapping may also help to train models with better prediction performance due to model averaging."}, {"heading": "1 Introduction", "text": "Bootstrapping is a very common method for sample statistics estimation. It generates N distinct datasets from the original training data by sampling examples with replacement; each resampled set is used to train one separate model. However, instantiating the individual bootstrapped samples is costly, both in terms of storage and processing. A naive implementation would require N times the original running time for training only, plus the resampling overhead. This makes bootstrapping formidable for large-scale learning tasks.\nVowpal Wabbit1 (VW) [4] is a very fast open-source implementation of an online out-of-core learner. Among the many efficient tricks within, it allocates a fixed (user-specifiable) memory size for learner representation, implements a hashing trick that hashes feature names to numeric indexes, and executes parallel threads for example parsing and learning. VW is able to learn a tera-feature (1012) dataset on 1000 nodes in one hour [1].\nIn this work, we extend an online version of bootstrapping for examples with unit weights proposed by Oza and Russell [6] to arbitrary positive real-valued weights, taking advantage of the good support of handling varying weights in VW [2]. We provide an efficient implementation of this algorithm that works as a reduction, and therefore may be used with binary and multiclass classifiers, regressors, as well as contextual bandit learners. Our memory efficient strategy scales well to large-scale data. All of our code is a part of the open-source Vowpal Wabbit project [4].\n1https://github.com/JohnLangford/vowpal_wabbit\nar X\niv :1\n31 2.\n50 21\nv1 [\ncs .L\nG ]\n1 8"}, {"heading": "2 Background on Online Bootstrapping", "text": "Online bootstrapping via sampling from Poisson distribution was first proposed in [6]. This is a very effective online approximation to batch bootstrapping, leveraging the following argument: Bootstrapping a dataset D with n examples means sampling n examples from D with replacement. Each example i will appear Zi times in the bootstrapped sample where Zi is a random variable. In the case of all examples with unit weight, Zi is distributed as a Binom(n, 1/n), because during resampling the i-th example will have n chances to be picked, each with probability 1/n. This Binom(n, 1/n) distribution converges to a Poisson distribution with rate 1, even for modest n (see Fig. 1). Poisson distribution is much easier to sample from, making it particularly suitable for large-scale learning.\nPark et al [7] proposed reservoir sampling with replacement for sampling streaming data - a technique that could be used to implement bootstrapping without approximation. However reservoir sampling is more complex and expensive. Kleiner et al [3] propose a different bootstrap approximation which divides the large dataset into many little and possibly non-overlapping subsamples, but each set of subsamples are still processed in a batch manner, thus it is not applicable for typical online settings."}, {"heading": "3 Efficient Online Bootstrapping", "text": "Sampling importance weights from a Poisson distribution allows us to implement an efficient online approximation to bootstrapping. Fig. 2 shows the basic algorithm.\nInput: example E with importance weight W, user-specified number of bootstrapping rounds N\nWe implemented online bootstrapping as a top-level reduction in VW (see Fig. 2). This way bootstrapping can be combined with any other algorithm implementing learn(). Parameter i is passed to the base learner to indicate an offset for storing feature weight of the current bootstrapping submodel. This architecture has three benefits: i) It keeps bootstrapping code separate from the learning code, capitalizing on any improvements in the base learning algorithm, ii) Weights for the same feature inside different bootstraps can be co-located in memory, which keeps memory access local and\nmaximizes cache hits, and iii) Each example needs to be parsed only once for all bootstrapped submodels. Only the importance weight of example is modified by drawing repeatedly from a Poisson distribution. This greatly reduces example parsing overhead.\nThere are two alternatives to implement online bootstrapping for non-unitary weights: i) sample the new importance weight directly from Poisson(W ) or ii) sample from a Poisson(1) and multiply it with W . Option i) suffers when W 1 as it almost always rejects in this case (most weights are zero). Option ii) is preferable and can be implemented very efficiently by a lookup table of the Poisson(1) probabilities.2.\nDuring prediction (testing), each example is again parsed only once and fed into N learners online. Besides estimating statistics from these N predictions, user can specify different ensemble methods to get one final prediction. The current implementation supports mean (for regression) and majority voting (for classification). Implementation of other statistics including quantiles is straightforward."}, {"heading": "4 Experiments", "text": "We show the efficiency of our online bootstrapping strategy, as well as how it helps to improve prediction accuracy. All experiments are conducted on a single desktop. We first show speed comparison on two datasets. The 75K dataset contains 74746 examples and 3000 features per example on average. For this dataset, we run 20 online passes to mitigate setting up overhead. The RCV1 [5] training dataset contains 781265 examples and 80 features per example on average. We only run single pass for this dataset. Running time for batch bootstrapping is estimated as t \u00d7 n where n is number of bootstrap samples and t is time for n = 1. We believe this estimation is optimistic, as it is not even clear how to do batch resampling on large datasets. We show results in Fig. 3. It is clear there is more performance gain for the RCV1 training dataset. Also the running time does not differ too much with different number of bootstrapping rounds for the RCV1 training dataset. These can be explained as a lot of computation power is spent on example parsing, while one benefit of our approach is to avoid repetitive example parsing. Thus our strategy is particularly helpful for a dataset with many examples.\nNext we show that online bootstrapping may improve predictions. We train on the RCV1 training dataset and report the online holdout validation loss3 at the end of each pass. Experiments are conducted with 20 bootstraps and 224 bits used for learner representation. We use square loss for all experiments. Fig. 4 shows that with bootstrapping the online validation loss is indeed lower. We save the best model according to validation loss and test generalization on a separate test set containing 23149 examples. In Tbl. 1, we measure classification accuracy and can see that the model trained\n2Only 20 entries are needed before the probability drops below machine precision 3A feature available in VW. This loss is calculated online on a consistent set of examples across passes that\nare used for model evaluation but not for model updating\nwith online bootstrapping performs better. We further add bootstrapping to a well tuned learner 4 and observe that online bootstrapping can improve performance upon such a competitive baseline.\nFigure 4: Bootstrapping helps predictive performance compared to a single model.\nBootstrapping learns more parameters than a single model and therefore for a fixed model size increases the chance of hashing collisions. This effectively leads to learning N simpler models versus a single more complex model. We ran three tests on the RCV1 dataset to investigate this trade-off: i) single model (default), ii) single model with extra quadratic features and iii) bootstrapped model with extra quadratics. We ran multiple passes and saved the best model based on holdout loss. The holdout loss results are summarized in Tbl. 2. We can see that on this dataset even with larger number of features, bootstrapping helps to improve model performance."}, {"heading": "5 Conclusions", "text": "In this work we show a highly effective and efficient online bootstrapping strategy that we implemented in the open-source Vowpal Wabbit online learning package. It is fast, feasible for large scale datasets, improves predictions of the resulting model, and provides a blackbox-like way to obtain uncertainty estimates that works with a variety of existing learning algorithms.\n4Single pass learning with options -b 23 -l 0.25 --ngram 2 --skips 4"}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u0131\u0301k", "John Langford"], "venue": "CoRR, abs/1110.4198,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Online importance weight aware updates", "author": ["Nikos Karampatziakis", "John Langford"], "venue": "In UAI, pages 392\u2013399,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The Big Data Bootstrap", "author": ["Ariel Kleiner", "Ameet Talwalkar", "Purnamrita Sarkar", "Michael I. Jordan"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Vowpal wabbit open source project", "author": ["John Langford", "Lihong Li", "Alexander Strehl"], "venue": "In Technical Report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Online bagging and boosting", "author": ["Nikunj C. Oza", "Stuart Russell"], "venue": "Artificial Intelligence and Statistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Sampling streaming data with replacement", "author": ["Byung-Hoon Park", "George Ostrouchov", "Nagiza F. Samatova"], "venue": "Comput. Stat. Data Anal.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Vowpal Wabbit1 (VW) [4] is a very fast open-source implementation of an online out-of-core learner.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "VW is able to learn a tera-feature (10) dataset on 1000 nodes in one hour [1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "In this work, we extend an online version of bootstrapping for examples with unit weights proposed by Oza and Russell [6] to arbitrary positive real-valued weights, taking advantage of the good support of handling varying weights in VW [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "In this work, we extend an online version of bootstrapping for examples with unit weights proposed by Oza and Russell [6] to arbitrary positive real-valued weights, taking advantage of the good support of handling varying weights in VW [2].", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": "All of our code is a part of the open-source Vowpal Wabbit project [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Online bootstrapping via sampling from Poisson distribution was first proposed in [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "Park et al [7] proposed reservoir sampling with replacement for sampling streaming data - a technique that could be used to implement bootstrapping without approximation.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Kleiner et al [3] propose a different bootstrap approximation which divides the large dataset into many little and possibly non-overlapping subsamples, but each set of subsamples are still processed in a batch manner, thus it is not applicable for typical online settings.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "The RCV1 [5] training dataset contains 781265 examples and 80 features per example on average.", "startOffset": 9, "endOffset": 12}], "year": 2013, "abstractText": "Bootstrapping is a useful technique for estimating the uncertainty of a predictor, for example, confidence intervals for prediction. It is typically used on small to moderate sized datasets, due to its high computation cost. This work describes a highly scalable online bootstrapping strategy, implemented inside Vowpal Wabbit, that is several times faster than traditional strategies. Our experiments indicate that, in addition to providing a black box-like method for estimating uncertainty, our implementation of online bootstrapping may also help to train models with better prediction performance due to model averaging.", "creator": "LaTeX with hyperref package"}}}