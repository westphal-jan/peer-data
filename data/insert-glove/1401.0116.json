{"id": "1401.0116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2013", "title": "Controlled Sparsity Kernel Learning", "abstract": "4:35 Multiple Kernel peel Learning (haakonsson MKL) conness on Support Vector provoking Machines (SVMs) kedu has been interschool a popular front of research in joselin recent times due youmei to its boroondara success in application problems like wenguang Object Categorization. bitmap This euro209 success is reinhilt due to the christian fact that MKL has the ability protectress to choose from tsutsumi a mittelfranken variety of hardback feature encourage kernels to identify ub the optimal coby kernel combination. firepower But the caribou initial suspensive formulation 7,825 of 107.92 MKL 81-3 was 51-seat only able to 15utl select bemusing the vuyani best of interchurch the quipped features and handback misses ibrahimov out many steel-reinforced other informative widell kernels transamination presented. multilinear To overcome xiangqi this, reato the kijevo Lp norm bela\u00fande based formulation was ariyoshi proposed rashtrakuta by tebessa Kloft awarded et. mogra al. This formulation is tpr capable of faiza choosing a 69-29 non - sparse fyrstenberg set of hernes kernels shearer through a control parameter pesky p. hegde Unfortunately, the mirvis parameter p quaked does zintan not sde have a direct pichincha meaning to flag-captain the number of golden-winged kernels ekadasi selected. We \u0161prem have observed that stricter control 7.41 over the sysop number coeur of kernels pricked selected gives 5.625 us s\u00f8nderborg an edge nachman over mery these techniques 38-6 in terms of accuracy vesicles of classification and sekely also helps 50.65 us sm-2 to danjuma fine 450s tune 3.0-liter the notational algorithms to laham the presold time 98-91 requirements at whiti hand. beehler In this aerovironment work, we propose takaki a 2323 Controlled seaon Sparsity equitably Kernel shanthakumaran Learning (ambikapur CSKL) \u015ferefliko\u00e7hisar formulation that 30-seat can strictly control mhow the jawless number heraldist of kernels trinitas which xboxes we wish nyora to rinsed select. The CSKL 1477 formulation cuenco introduces inner-city a parameter ecw t which whitecoat directly corresponds to the number t3ss of kernels selected. It is web-only important to note that naimoli a 50-over search in brwilson t space emmenecker is finite and excises fast as compared to michaux p. bioequivalent We chhean have also provided an evy efficient deadshot Reduced Gradient immunomodulatory Descent acacias based steatosis algorithm memorandums to 94.39 solve isoenzymes the CSKL formulation, which kaikai is proven pago to widzew converge. taluka Through our experiments on climaco the marudu Caltech101 Object vibe Categorization dataset, we stoudamire have noncapital also shotley shown higuita that 23-12 one privilegium can pollards achieve belkovsky better lan\u00fas accuracies entorhinal than the saint-brieuc previous bnb formulations peap through aplus the eaw right choice of aiding t.", "histories": [["v1", "Tue, 31 Dec 2013 09:13:09 GMT  (2517kb,D)", "http://arxiv.org/abs/1401.0116v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dinesh govindaraj", "raman sankaran", "sreedal menon", "chiranjib bhattacharyya"], "accepted": false, "id": "1401.0116"}, "pdf": {"name": "1401.0116.pdf", "metadata": {"source": "CRF", "title": "Controlled Sparsity Kernel Learning\u2217", "authors": ["Dinesh Govindaraj", "Raman Sankaran"], "emails": ["dinesh.govindaraj@alcatel-lucent.com", "ramans@csa.iisc.ernet.in", "sreedal.menon@alcatel-lucent.com", "chiru@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p doesnot have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can acheive better accuracies than the previous formulations through the right choice of t."}, {"heading": "1 Introduction", "text": "Support Vector Machines(SVMs) [15] have emerged as powerful tools for classification problems. The key to\n\u2217Work Done in the year 2011\naccurate classification using SVMs is the choice of Kernel functions(for definition of Kernel function please see [16]). This issue was first studied in Lanckriet et. al. [2] where the problem of Multiple kernel learning(MKL) was first introduced. They have been successfully applied to a variety of domains e.g. text, object recognition [10, 13], protein structures[20]. Even though the idea was to explore the space of all possible linear combinations of the specified kernels, the functional framework associated with it could only select the best kernel from the set of specified kernels. Recently, many other approaches have been proposed to overcome this limitation[6, 7]. While some of them select all the kernels and some have sparse solutions that choose a subset of the specified kernels in a weighted combination, none of them have explicit control over sparsity.\nDue to lack of explicit control, in many application scenarios Non-Sparse solutions end up selecting some bad kernels also which leads to reduction in the discriminative power of the combination kernel. We show experimental evidence of this phenomenon. One might argue that if the kernels given were all good kernels, this problem will not persist. But that does not take away the fact that a lower-accuracy good kernel can still bring down the accuracy of a better kernel. In most of the recent publications, we do not get a glimpse of the original problem as the space of kernels explored is very small and most of the kernels have almost equal power of representation.\nWhile sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution. This inherits most of the problems of selecting one and selecting all kernels due to the lack of control. The most relevant problem is that it misses out on some\nar X\niv :1\n40 1.\n01 16\nv1 [\ncs .L\nG ]\n3 1\nD ec\nimportant features by selecting lesser number of kernels than optimal. In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6]. This is due to the fact that different kernels represent different features necessary for the task and dropping some of them or most of them will lead to a bad combination kernel. These flaws are shown in our experiments as well.\nThis work builds a variable sparsity solution that has explicit control over the number of kernels selected overcoming all these problems. We show the effect and need of strict control of sparsity through our experiments on the application of Object Recognition. Along the way, we have also extended the MKL framework to nu-SVMs which allow us better control of the number of support vectors and training error as well.\nIn the following section 2, we present a review of the existing work on MKL. Section 3 introduces the CSKL formulation for the C-SVM and \u03bd-SVM. Section 4 presents the algorithms to solve the proposed formulations. Section 5 demonstrates the usefulness of the CSKL formulation on a toy dataset and the Caltech101 real world Object Categorization dataset."}, {"heading": "2 Related Work", "text": "Multiple Kernel Learning(MKL) was initially proposed by Lanckriet et. al. [2]. They introduced an SemiDefinite programming(SDP) approach to solve for the combination kernel. As SDP becomes intractable with increase in size and number of kernels, Bach et.al [3] reformulated MKL by considering each feature as a block and applying the l1 norm across the blocks and l2 norm within each block. For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process. [4] provides an Semi-Infinite Linear Programming(SLIP) based algorithm which decreases the training time to large extent. SimpleMKL [5] proposed by Rakotomamonjy et.al. derived a formulation which is equivalent to the block l1 norm based formulation and provided a Reduced Gradient Descent based algorithm that is faster than the SLIP algorithm proposed previously. The dual of the SimpleMKL formulation is given by,\nmax \u03b1i\n\u2211 i \u03b1i \u2212 1 2 \u2211 i,j \u03b1i\u03b1jyiyj ( \u2211 m \u03b3mKm (xi, xj))\ns. t. \u2211 i \u03b1iyi = 0\n0 \u2264 \u03b1i \u2264 C\u2200i\u2211 m \u03b3i = 1, \u03b3i \u2265 0,\u2200i(2.1)\nWhile all these approaches discussed a sparse solution to MKL, on understanding the need for non-sparse solutions, researchers have been exploring the space of non-sparse formulations in recent times. To acheive\nnon-sparsity, [6] group the kernels and apply l\u221e norm across the groups and l1 norm within the groups. They have also proposed a Mirror Descent Algorithm for solving MKL formulations which is much faster than SimpleMKL. Especially when number of kernels are high. Kloft et.al.[7] apply general lp norm to kernels and they show that Non-Sparse MKL generalizes much better than sparse MKL. The dual of the lp norm based MKL formulation as proposed by Kloft et.al. looks like\nmax \u03b1i\n\u2211 i \u03b1i \u2212 1 2 (\u2211 m (\u2211 i,j \u03b1i\u03b1jyiyjKm (xi, xj) ) p p\u22121 ) p\u22121 p\ns. t. \u2211 i \u03b1iyi = 0\n0 \u2264 \u03b1i \u2264 C\u2200i(2.2)\nThey have shown that when p = 1, the formulation is equivalent to SimpleMKL and as p moves to \u221e, it explores non-sparse solutions. But the value of p lacks a direct meaning or implication to the number of kernels selected.\nEven though the details of sparse and non-sparse solutions have been explored, none of these formulations have explicit control of sparsity for their solutions. As we have demonstrated in the experiments section, strict control of sparsity is highly valuable. Hence we propose a formulation, where we can parametrically control the total number of kernels selected and an efficient reduced gradient descent based algorithm to solve it. We have also experimentally shown that our formulation will be able to better state-of-the-art performance on the Caltech101[19] dataset for object categorization through strict control of sparsity."}, {"heading": "3 Controlled Sparsity Kernel Learning", "text": "In this section we introduce the new Controlled Sparsity Kernel Learning (CSKL) formulation and prove that this formulation can explicitly control the sparsity of kernel selection through a parameter t. We derive the CSKL formulation by modifying the dual of MKL [2]. Lets start with the MKL dual [2]\nmin \u03b3\u22650,K \u03c9(K)(= max \u03b1\u2208Sm \u22121 2 \u03b1>Y KY \u03b1+ m\u2211 i=0 \u03b1i)\ntr(K) = \u03b4\nK = n\u2211 i=1 \u03b3iKi(3.3)\nwhere Sm = {\u03b1 \u2208 Rm|0 \u2264 \u03b1 \u2264 C, yTC = 0}. Denote by dj tj \u03b4 = \u03b1\n>Y KjY \u03b1 and tj = Trace(Kj). As \u03c9(K) is convex, one can interchange the min and the\nmax. Now, the dual looks like\nmax \u03b1\u2208Sn,d min \u03b3\u22650 \u2212 \u2211 j \u03b3jdj tj \u03b4 + \u2211 i \u03b1i\n\u03b3>t = \u03b4\ndj = \u03b1 >Y KjY \u03b1(3.4)\nDefine \u03b3\u2032j = \u03b3j tj \u03b4 then the constraint \u03b3 >t = \u03b4 can be rewritten as \u2211 j \u03b3 \u2032 j = 1. The l\u221e norm can be represented as (3.5) \u2016v\u2016inf = max\u2211 i \u03b3i\u22641,\u03b3i\u22650 \u2211 j \u03b3jvj\nfor any vj \u2265 0 . Given this, Equation (3.4) can be restated as\nmax \u03b1\u2208Sn,d \u2212\u2016d\u2016\u221e + \u2211 i \u03b1i\ndj = \u03b1 >Y KjY \u03b1(3.6)\nThis formulation (3.6) results in a sparse selection of kernels as shown in [7]. Similarly, equation (2.2) can also be rewritten as,\nmax \u03b1\u2208Sn,d \u2212\u2016d\u2016p\u2217 + \u2211 i \u03b1i\ndj = \u03b1 >Y KjY \u03b1(3.7)\np\u2217 = p\np\u2212 1 (3.8)\nThe above formulation(3.7) is referred to as Lp MKL throughout this paper. Even though above formulation (3.7) uses generic norm over d, there is no guarantee of explicit control over sparsity. In next section, we derive our CSKL formulation by modifying the norm on d.\n3.1 CSKL formulation Let v \u2208 Rn+ denote the space of n dimensional vectors with all components positive, i.e. vi > 0. Let v(i) be the ith largest component of v, i.e. v(1) \u2265 v(2) . . . , v(n) Consider the following convex function on gt : Rn+ \u2192 R, gt(v) =\u2211t i=1 v(i) where t is a positive integer less than n.\nWe present our first claim by this theorem\nTheorem 3.1. If v \u2208 Rn+ such that v(n) > 0 and gt(v) defined as before then\ngt(v) = max \u03b3\n\u03b3T v\ns.t. n\u2211 i=1 \u03b3i = t, 0 \u2264 \u03b3i \u2264 1,\u2200i(3.9)\nand at optimality \u03b3i = 1,whenever vi > v(t) and \u03b3i = 0,whenever vi < v(t)\nProof. We begin by constructing the Lagrangian of the problem\nL(\u03b3, a, \u00b5, \u03b2) =\u03b3>v \u2212 a ( n\u2211 i=0 \u03b3i \u2212 t ) (3.10)\n+ n\u2211 i=1 \u03b2i\u03b3i \u2212 n\u2211 i=0 \u00b5i (\u03b3i \u2212 1)(3.11)\nwhere the lagrange multipliers are \u00b5, \u03b2 and a. Apart from the feasibility conditions on \u03b3 and the nonnegativity constraints on the lagrange multipliers \u00b5 and \u03b2 the KKT conditions reads as\n\u2202L\n\u2202\u03b3i = 0 =\u21d2 a+ \u00b5i = \u03b2i + vi(3.12)\n\u03b1 ( n\u2211 i=0 \u03b3i \u2212 t ) = 0(3.13)\n\u03b2i\u03b3i = 0(3.14) \u00b5i (\u03b3i \u2212 1) = 0(3.15)\nThe proof hinges on that fact that a = v(t) satisfies the KKT conditions. We note that both \u03b2i and \u00b5i cannot be simultaneously positive. If vi < a, then (3.12) could be obtained by setting \u03b2i > 0 and \u00b5i = 0. As \u03b2i > 0 then \u03b3i = 0. Again if vi > a, then (3.12) could be obtained by setting \u03b2i = 0 and \u00b5i > 0. As \u00b5i > 0 then \u03b3i = 1. Interestingly note that if vi = a as both \u00b5i = \u03b2i = 0 and 1 > \u03b3i > 0. Let us now suppose that a = v(t) The constraint\n\u2211n i=1 \u03b3i = t can now be written as\u2211\ni:vi<v(t) \u03b3i\ufe38 \ufe37\ufe37 \ufe38 T1\n+ \u2211\ni:vi=v(t) \u03b3i\ufe38 \ufe37\ufe37 \ufe38 T2\n+ \u2211\ni:vi>v(t) \u03b3i\ufe38 \ufe37\ufe37 \ufe38 T3 = t\nDue to observations made before it is straightforward to see that T1 = 0 and T3 \u2264 t \u2212 1. One can always choose feasible \u03b3i \u2200 vi = v(t) such that T2 = t \u2212 T3 This establishes the fact that a = v(t) indeed satisfies the KKT conditions and for which \u03b3i = 1(\u03b3i = 0) if vi > v(t)(vi < v(t)).\nAs KKT conditions are necessary and sufficient for this problem [14] we see that at optimality \u03b3>v =\u2211t i=1 v(i) obtained by substituting the \u03b3 obtained before. This completes the proof.\nBy introducing gt(d) to the dual (As in Eqn. 3.6) we get the following CSKL formulation,\nmax \u03b1\u2208Sn,d \u2212gt(d) + \u2211 i \u03b1i\ndj = \u03b1 >Y KjY \u03b1\n(3.16)\nNote that CSKL formulation (3.16) explicitly controls the sparsity of kernel selection by varying t as is evident from Theorem 3.1.\n3.1.1 \u03bd-CSKL A variant of SVM is the \u03bd-SVM [1] where parameter C is replaced by a parameter \u03bd = [0, 1]. Here, the parameter \u03bd is lower bound on the fraction of number support vector and an upper bound on the fraction of margin errors. In this section we extend our CSKL formulation to \u03bd-SVM. The dual of \u03bd-SVM is given by,\nmax \u03b1 \u2212 1 2 \u03b1>Y KY \u03b1\ns.t. 0 \u2264 \u03b1i \u2264 1\nm , m\u2211 i=1 \u03b1iyi = 0, m\u2211 i=1 \u03b1i \u2265 \u03bd\n(3.17)\nIntroducing MKL to the dual of \u03bd-SVM and rewriting it similar to equation (3.6).\nmax \u03b1\n\u2212 \u2016d\u2016\u221e\ns.t. 0 \u2264 \u03b1i \u2264 1\nm , m\u2211 i=1 \u03b1iyi = 0, m\u2211 i=1 \u03b1i \u2265 \u03bd\ndj = \u03b1 >Y KjY \u03b1(3.18)\nWe now introduce our CSKL formulation in the setting of \u03bd-SVM.\nmax \u03b1\n\u2212 gt(d)\ns.t. 0 \u2264 \u03b1i \u2264 1\nm , m\u2211 i=1 \u03b1iyi = 0, \u03bd \u2265 m\u2211 i=1 \u03b1i\ndj = \u03b1 >Y KjY \u03b1(3.19)\nThe above formulation is denoted as \u03bd-CSKL throughout this paper."}, {"heading": "4 Algorithms for solving CSKL formulations", "text": "We present an alternating optimization scheme for solving the \u03bd \u2212 CSKL formulation. For a fixed \u03b3, we solve the following maximization for \u03b1,\nmax \u03b1\n\u2212 \u03b3T d\ns.t. 0 \u2264 \u03b1i \u2264 1\nm , m\u2211 i=1 \u03b1iyi = 0, \u03bd \u2265 m\u2211 i=1 \u03b1i\ndj = \u03b1 >Y KjY \u03b1(4.20)\nNote that in above problem \u03b3 should satisfy the conditions \u2211n i=1 \u03b3i = t, 0 \u2264 \u03b3i \u2264 1,\u2200i. We can\nuse standard Sequential Minimal Optimization(SMO) solver for the above problem. Once optimal \u03b1\u2217 is calculated, we compute d as, dj = \u03b1 \u2217>Y KjY \u03b1 \u2217. Next step is to solve for \u03b3. We can find the optimal \u03b3 by solving gt(d) using Reduced Gradient Descent or a Linear Programming based Gradient Descent.\nAlgorithm 1 \u03bd-CSKL Algorithm\nRequire: xTKjx > 0,\u2200x 6= 0,\u2200j INPUT: N = number of kernels \u03b3 = tN objOld = 0 while \u03b4 \u2264 do\nSolve \u03b1 using SMO solver with kernel K =\u2211N j \u03b3jKj Compute dj = \u03b1 TY KjY \u03b1 Solve gt(d) using Reduced Gradient Descent or Linear Programming based solver obj = \u2212 12\u03b1 TY (\u2211N j=1 \u03b3jKj ) Y \u03b1 \u03b4 = obj \u2212 objOld objOld = obj\nend while\nIn Algorithm 2, we present our Reduced Gradient Algorithm to solve \u03b3. The SVM solver is used to obtain J (see Algorithm 2). The Descent Direction D is defined as per Algorithm 3. In the case of \u03bd-CSKL, the value of J (\u03b1, \u03b3) = \u2212\u03b3T d while in the case of C-CSKL it is \u2212\u03b3T d + \u03b1T e while the rest of the framework remains the same.\nDue to our assumptions on K, in both the cases, J is convex and differentiable with Lipschitz gradient wrt. \u03b3 [17]. For such functions the Reduced Gradient Method converges with bounds as defined in [18].\nWe also present a linear programming based approach to solve for gt(d). We use some standard LP Solver to solve the following linear program for finding descent direction D for gt(d).\nmin D\n\u03c6TD\nDT 1 = 0, \u2212 \u03b3 \u2265 D \u2265 1\u2212 \u03b3(4.21)\nwhere \u03c6m = \u2202J \u2202\u03b3m\nand step size (S) can be found by using line search. \u03b3 is updates as \u03b3new = \u03b3 + SD. Though this algorithm is found to converge for K > 0 we have no bounds on its convergence as yet."}, {"heading": "5 Experiments.", "text": "To illustrate the benefits of CSKL formulation, we give results on Synthetic data and the Caltech101 [19] realworld Object Categorization dataset. We compare CSKL\nAlgorithm 2 Reduced Gradient Algorithm for Solving gt(d)\nJ(\u03b1, \u03b3) = \u2212 12\u03b1 TY (\u2211N j=1 \u03b3jKj ) Y \u03b1 Set \u00b5 = argminm (abs (\u03b3m \u2212 0.5)) Set Jnew = J \u2212 1, \u03b3new = \u03b3 Compute \u03c6m =\n\u03b4J \u03b4\u03b3m for m = 1, . . . , N\nCompute the descend direction D(d, \u00b5, \u03c6) Set Dnew = D while Jnew < J do \u03b3 = \u03b3new D = Dnew \u03bd1 = argmin\n(m|Dm<0) \u2212 \u03b3mDm\n\u03bd2 = argmin (m|Dm>0)\n1\u2212\u03b3m Dm\nSmax = min ( \u2212 \u03b3\u03bd1D\u03bd1 , 1\u2212\u03b3\u03bd2 D\u03bd2 ) \u03bd = argmin\n\u03bd1,\u03bd2\n( \u2212 \u03b3\u03bd1D\u03bd1 , 1\u2212\u03b3\u03bd2 D\u03bd2 ) \u03b3new = \u03b3 + SmaxD, Dnew (\u00b5) = D\u00b5 \u2212D\u03bd Dnew (\u03bd) = 0 Compute Jnew(\u03b1, \u03b3new)\nend while Linesearch along D for S \u2208 [0, Smax] \u03b3 \u2190 \u03b3 + SD\nAlgorithm 3 Calculating the Descent Direction\nINPUT: Kernel Weights \u03b3, Selected Pivot \u00b5, Calculated Gradients \u03c6 OUTPUT: Optimal Descent Direction D(\u03b3, \u00b5, \u03c6) for m = 1 to N do Dm = 0 if (\u03b3m == 0 & \u03c6m \u2212 \u03c6\u00b5 > 0) then Dm = 0\nelse if (\u03b3m == 1 & \u03c6m \u2212 \u03c6\u00b5 < 0) then Dm = 0 else if (\u03b3m > 0 & m! = \u00b5) then Dm = \u2212\u03c6m \u2212 \u03c6\u00b5 else if (m == \u00b5) then Dm = \u2211 \u03bd!=\u00b5,\u03b3\u03bd>0\n(\u03c6\u03bd \u2212 \u03c6\u00b5) end if\nend for\nalgorithm with SimpleMKL [Equation 2.1] 1 which is a sparse selection algorithm, and Lp MKL [Equation 2.2] with p = 2 which is a non-sparse selection algorithm 2.\n5.1 Datasets In this section, we describe the datasets we used for our experiments.\n5.1.1 Synthetic Dataset To show the effect of noisy kernels, we generated n = 18 kernels out of which 16 are informative kernels and 2 are noisy kernels. To build these kernels, we sampled m = 500 datapoints with dimension d = 3 from two independent Gaussian distributions with covariance as the identity matrix and different means(\u00b51 = 0 and \u00b52 = 3, Datapoints sampled from different Gaussians are assumed to belong to different classes). We generated four kernels (two gaussian(\u03c3 = and \u03c3 =) and two polynomial kernels(\u03c3 = and \u03c3 =)) for each dimension seprately and all together(4\u22173+4 = 16). On top of this, we also added two carefully chosen noisy kernels to this kernel set.\n5.1.2 Caltech101 The Caltech101 dataset has 102 categories of images such as airplanes, cars, leopards, etc. It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier. Using the method followed by [10] 3, we extract the following 4 descriptors : PhowColor, PhowGray, GeometricBlur and SelfSimilarity. Each descriptor gives rise to a distance matrix. We create multiple Gaussian kernels for each descriptor by varying the Gaussian width parameter used to generate the kernel. We currently used 5 width values in the log space of -4 to 0. Hence we arrive at a total of 20 kernels. The number of binary classification problems are 5151 and 102, for 1-vs-1 and 1-vs-rest classification approaches respectively.\n5.2 Need for Control Over Sparsity The key result we wish to establish is that by suitable variation of parameter t in CSKL, one can combine good kernels and eliminate noisy kernels and achieve better generalization than other MKL formulations.\nIn the Synthetic dataset setting t = 1 will facilitate sparse selection, and t = n facilitates a complete non-sparse selection. As shown in the figure 1, the CSKL formulation clearly outperforms both sparse and non-sparse MKL by setting t = 4. It is clear\n1Implementation downloaded from http://asi.insarouen.fr/enseignants/\u223carakotom/code/mklindex.html 2Implementation available in the Shogun toolbox : http://www.shogun-toolbox.org/ 3http://www.robots.ox.ac.uk/ vgg/software/MKL/v1.0/\nthat setting t = 4 in CSKL gives better generalization performance than both t = 1 and t = 18. This clearly shows neither sparse nor complete non-sparse is good for this dataset. CSKL is the only formulation which can capture all good kernels but still eliminate the noisy kernels by tuning parameter t.\nIn order to demonstrate that neither sparse nor non-sparse solutions are always the best in real-world datasets, we take all the binary 1-vs-1 and 1-vsRest classifiers in Caltech101 dataset and compare SimpleMKL and L2 MKL solutions. Figures 2, 3 show the ratio of improvement in accuracy of L2 MKL over SimpleMKL. It is evident from the figures that neither of the algorithms are always the best. Thus, depending on the binary classification problem, we need to have different controls on the sparsity to achieve the stateof-the-art performance. Clearly this motivates that, to achieve the desired sparsity, we can use the CSKL formulation instead of either SimpleMKL or L2 MKL. In next section we show how CSKL can achieve better performance than other algorithms.\n5.3 Performance of CSKL We apply our CSKL algorithm, and compare its performance against the other state-of-the-art algorithms SimpleMKL and L2 MKL. We take the highest accuracy achieved by CSKL across various values of parameter t for the comparison.\nFigures 4, 5 show the ratio of improvement in accuracy of CSKL over L2 MKL and SimpleMKL.\nWe also present here overall performance of CSKL on Caltech101 dataset. Figure 8 shows the performance of CSKL as t is varied. For comparison, we have shown a straight line which shows the average accuracy achieved by SimpleMKL and L2 MKL. Figure 8 clearly shows that all the 20 kernels are not necessary, since the CSKL accuracy more or less saturates after t > 4. The result also shows that a sparse selection algorithm like SimpleMKL wont be most efficient algorithm in terms of the accuracy achieved. And the performance of CSKL is almost equal to that of L2 MKL, but the latter selects all the provided kernels, while we can achieve competitive accuracy with the former itself at t = 4. Note that no other formulation can give this flexibility to users to select exactly four best performing kernels. It is natural to use use t = 4 here because number of descriptors used is four. Hence the experiments demonstrated in this section provide a proper justification for the usage of the CSKL formulation.\n5.4 Discussion To analyze more on why CSKL achives better accuracy, we plot the histogram of number of descriptors selected when CSKL outperforms L2\nMKL and SimpleMKL in the binary classification problems. We see that, in the figures 9, 11, SimpleMKL only selects one or two descriptors whereas CSKL select all the descriptors. For the cases where SimpleMKL doesnt perform the best, non-sparse combination might be a better choice, and this is emperically confirmed in the figures 9, 11. Similarly from the figures 10, 12, we see that L2 MKL selects all the descriptors whereas CSKL does not select all descriptors most of the cases. These are expected, since, for the cases where L2 MKL perform low, it may mean that a non-sparse classification is preferable. And the same is reflected in the figures 10, 12.\nOut of the 5151 binary classification problems in the 1-vs-1 setting, \u03bd-CSKL performed better in 5112 and 1498 cases against L2 MKL and SimpleMKL respectively. Similarly in the 1-vs-Rest setting, out of the 102 classification problems, the numbers turned out to be 97 and 91 against L2 MKL and SimpleMKL respectively.\nFinally, Figure 13 shows the number of descriptors selected as t is increased. We can infer that as t increased beyond 4, all the descriptors are selected. This is also not surprising since all the 4 descriptors used in our experiment are independent and experimentally they have been shown to aid the accuracy of the Object Categorization problem."}, {"heading": "6 Conclusion.", "text": "As we have seen, both Sparse and Non-Sparse MKL have their handicaps depending on the classification problem at hand. Niehter of them are always the best. Also, in such problems the time taken to calculate the features is one the biggest bottlenecks. For all these reasons, a formulation with strict control of sparsity would be the best solution to have. One can then tune the sparsity parameter t and select the best set of kernels for any particular classification problem. We have described one such formulation in this paper along with the associated solution algorithms. We have also shown the superior performance of this formulation with respect to both the Sparse and Non-Sparse formulations of MKL for the application problem of Object Categorization."}], "references": [{"title": "Learning the Kernel Matrix with Semidefinite Programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Multiple Kernel Learning", "author": ["F. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "Conic Duality, and the SMO Algorithm, International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y Grandvalet"], "venue": "Journal of Machine Learning Research, vol 9, pages 2491-2521", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation", "author": ["Saketha Nath Jagarlapudi", "Dinesh Govindaraj", "Raman S", "Chiranjib Bhattacharyya", "Aharon Ben-Tal", "K.R. Ramakrishnan"], "venue": "Proceedings of the Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Efficient and Accurate Lp-Norm Multiple Kernel Learning", "author": ["Marius Kloft", "Ulf Brefeld", "Soeren Sonnenburg", "Pavel Laskov", "Klaus-Robert Mller", "Alexander Zien"], "venue": "Proceedings of the Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Composite Kernel Learning", "author": ["M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy"], "venue": "Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A Visual Vocabulary for Flower Classification", "author": ["Maria-Elena Nilsback", "Andrew Zisserman"], "venue": "Proceedings  of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Learning the Discriminative Power Invariance Trade-off", "author": ["M. Varma", "D. Ray"], "venue": "Proceedings of the International Conference on Computer Vision", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated Flower Classification over a Large Number of Classes", "author": ["Maria-Elena Nilsback", "Andrew Zisserman"], "venue": "Proceedings of the Sixth Indian Conference on Computer Vision, Graphics & Image Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "On Feature Combination for Multiclass Object Classification", "author": ["Peter Gehler", "Sebastian Nowozin"], "venue": "Proceedings of the Twelfth IEEE International Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Support kernel machines for object recognition", "author": ["A. Kumar", "C. Sminchisescu"], "venue": "IEEE International Conference on Computer Vision", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning with Kernels Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Bernhard Schlkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Numerical Optimization: Theoretical and Practical Aspects (Universitext)", "author": ["Bonnans", "J. Fr\u00e9d\u00e9ric", "Gilbert", "Jean Charles", "Lemar\u00e9chal", "Claude", "Sagastiz\u00e1bal", "Claudia A"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories", "author": ["R. Fergus L. Fei-Fei", "P. Perona"], "venue": "In IEEE", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Probabilistic multi-class multi-kernel learning: On protein fold recognition and remote homology", "author": ["Damoulas", "T. Girolami M"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "\u2217Work Done in the year 2011 accurate classification using SVMs is the choice of Kernel functions(for definition of Kernel function please see [16]).", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "[2] where the problem of Multiple kernel learning(MKL) was first introduced.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "text, object recognition [10, 13], protein structures[20].", "startOffset": 25, "endOffset": 33}, {"referenceID": 10, "context": "text, object recognition [10, 13], protein structures[20].", "startOffset": 25, "endOffset": 33}, {"referenceID": 14, "context": "text, object recognition [10, 13], protein structures[20].", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "Recently, many other approaches have been proposed to overcome this limitation[6, 7].", "startOffset": 78, "endOffset": 84}, {"referenceID": 4, "context": "Recently, many other approaches have been proposed to overcome this limitation[6, 7].", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "While sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution.", "startOffset": 22, "endOffset": 28}, {"referenceID": 2, "context": "While sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution.", "startOffset": 22, "endOffset": 28}, {"referenceID": 9, "context": "In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 3, "context": "In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 0, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "al [3] reformulated MKL by considering each feature as a block and applying the l1 norm across the blocks and l2 norm within each block.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process.", "startOffset": 39, "endOffset": 48}, {"referenceID": 3, "context": "For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process.", "startOffset": 39, "endOffset": 48}, {"referenceID": 2, "context": "SimpleMKL [5] proposed by Rakotomamonjy et.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "To acheive non-sparsity, [6] group the kernels and apply l\u221e norm across the groups and l1 norm within the groups.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "[7] apply general lp norm to kernels and they show that Non-Sparse MKL generalizes much better than sparse MKL.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We have also experimentally shown that our formulation will be able to better state-of-the-art performance on the Caltech101[19] dataset for object categorization through strict control of sparsity.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "We derive the CSKL formulation by modifying the dual of MKL [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "Lets start with the MKL dual [2]", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "6) results in a sparse selection of kernels as shown in [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "\u03b3 [17].", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "To illustrate the benefits of CSKL formulation, we give results on Synthetic data and the Caltech101 [19] realworld Object Categorization dataset.", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.", "startOffset": 21, "endOffset": 33}, {"referenceID": 10, "context": "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.", "startOffset": 21, "endOffset": 33}, {"referenceID": 9, "context": "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.", "startOffset": 21, "endOffset": 33}, {"referenceID": 7, "context": "Using the method followed by [10] , we extract the following 4 descriptors : PhowColor, PhowGray, GeometricBlur and SelfSimilarity.", "startOffset": 29, "endOffset": 33}], "year": 2014, "abstractText": "Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p doesnot have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can acheive better accuracies than the previous formulations through the right choice of t.", "creator": "LaTeX with hyperref package"}}}