{"id": "1401.3879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Soft Constraints of Difference and Equality", "abstract": "In many combinatorial playbills problems one may proviso need to model the purdy diversity bolona or similarity publicaciones of assignments in a rafanelli solution. For two-mile example, acetylated one carbomb may wish to maximise or mawa minimise the number of quotation distinct values weigl in 107-run a andokides solution. h\u1ed3ng To ozgur formulate michinoku problems seidl of this type, 32.19 we impedances can squamous use soft variants recalculate of the well known ehrenkranz AllDifferent ulis and AllEqual dried constraints. 86.43 We dreamz present 41-33 a taxonomy adefemi of eral six dakosaurus soft hortensis global laitman constraints, generated by mintenko combining 96-page the cytoxan two latter boondox ones lg.philips and akqa the two standard cibitoke cost functions, shunters which are either maximised or bolten minimised. xuewen We characterise the gersende complexity gears of sipadan achieving produjo arc deputising and bounds consistency on these constraints, resolving 114-111 those mogale cases ditzler for culter which NP - hardness was neither 431st proven doonside nor gimp disproven. In rechy particular, privates we explore troyes in davidtz depth the 117.84 constraint ensuring that at petriceicu least k dyas pairs subtilis of variables b\u00e1nffy have keselowski a common ravenal value. malayan We zorya show suevi that hatt achieving sequential arc boozed consistency kugel is sabretooth NP - hard, altenglan however transmittance achieving bounds consistency 17-foot can leonians be done in polynomial miksa time 208,000 through mamed dynamic deanna programming. Moreover, azamiya we kansans show that the wsk maximum number of 339th pairs of 33-hour equal d-beat variables almoravids can be approximated 1,676 by brenthia a factor 1 / 2 with vuvuzela a shanghainese linear time lpga greedy algorithm. grigore Finally, vorstadt we eunuchs provide ballparks a 62.94 fixed parameter grigull tractable algorithm fsp with djaanfari respect to nard\u00f2 the number 9,300 of cerri values cooney appearing organogenesis in intuits more than haydon two takamasa distinct domains. Interestingly, nypa this whoa taxonomy shows that enforcing plotline equality is punggol harder mazdak than enforcing x\u00e1 difference.", "histories": [["v1", "Thu, 16 Jan 2014 05:11:58 GMT  (656kb)", "http://arxiv.org/abs/1401.3879v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["emmanuel hebrard", "d\\'aniel marx", "barry o'sullivan", "igor razgon"], "accepted": false, "id": "1401.3879"}, "pdf": {"name": "1401.3879.pdf", "metadata": {"source": "CRF", "title": "Soft Constraints of Difference and Equality", "authors": ["Emmanuel Hebrard", "Barry O\u2019Sullivan", "Igor Razgon"], "emails": ["hebrard@laas.fr", "dmarx@cs.bme.hu", "b.osullivan@cs.ucc.ie", "ir45@mcs.le.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Constraints for reasoning about equality and difference within assignments to a set of variables are ubiquitous in constraint programming. In many settings, one needs to enforce a given degree of diversity or similarity in a solution. For example, in a university timetabling problem we will want to ensure that all courses taken by a particular student are held at different times. Similarly, in meeting scheduling we will want to ensure that the participants of the same meeting are scheduled to meet at the same time and in the same place. Sometimes, when the problem is over-constrained, we might wish to maximise the extent to which these constraints are satisfied. Consider again our timetabling example: we might wish to\nc\u00a92011 AI Access Foundation. All rights reserved.\nmaximise the number of courses that are scheduled at different times when a student\u2019s preferences cannot all be met.\nIn a constraint programming setting requirements on the diversity and similarity amongst variables can be specified using global constraints. One of the most commonly used global constraints is the AllDifferent (Re\u0301gin, 1994), which enforces that all variables take pairwise different values. A soft version of the AllDifferent constraint, named SoftAllDiff, has been proposed by Petit, Re\u0301gin, and Bessiere (2001). They proposed two cost metrics for measuring the degree of satisfaction of the constraint, which are to be minimised or maximised: graph- and variable-based cost. These two cost metrics are generic and widely used (e.g., van Hoeve, 2004). The former counts the number of equalities, whilst the latter counts the number of variables to change in order to satisfy the corresponding hard constraint. When we wish to enforce that a set of variables take equal values, we can use the AllEqual, or its soft variant for the graph-based cost, the SoftAllEqual constraint (Hebrard, O\u2019Sullivan, & Razgon, 2008), or its soft variant for the variable-based cost, the AtMostNValue constraint (Beldiceanu, 2001).\nWhen considering these two constraints (AllDifferent and AllEqual), these two costs (graph-based and variable-based) and objectives (minimisation and maximisation) we can define eight algorithmic problems related to constraints of difference and equality. In fact, because the graph-based costs of AllDifferent and AllEqual are dual, only six distinct problems are thus defined. The structure of this class of constraints is illustrated in Figure 1. For each one, we give the complexity of the best known algorithm for achieving ac and bc. Three of these problems were studied in the past: minimising the cost of SoftAllDiff variable (Petit et al., 2001) and graph-based cost (van Hoeve, 2004) is polynomial whilst maximising the variable-based cost of SoftAllDiff is NP-hard (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) for ac and polynomial (Beldiceanu, 2001) for bc. A fourth one, maximising the variable-based cost of the SoftAllEqual constraint, can directly be mapped to a known problem: the Global Cardinality constraint. In this paper,1 we introduce two efficient algorithms for achieving, respectively, Arc consistency (ac) and Bounds consistency (bc) on the fifth case, minimising the variable-based cost for SoftAllEqual. Moreover, the computational complexity of the last remaining case, maximising the graph-based cost for SoftAllDiff (or, equivalently, minimising the graph-based cost for SoftAllEqual) was still unknown. Informally, this problem is to maximise the number of pairs of variables assigned to a common value. It turns out to be a challenging and interesting problem, in that it is hard but yet can be addressed in several ways. In particular, we show that:\n\u2022 Finding a solution with at least k pairs of equal variables is NP-complete, hence achieving ac on the corresponding constraint is NP-hard.\n\u2022 When domains are contiguous, it can be solved in a polynomial number of steps through dynamic programming, hence achieving bc on the corresponding constraint is polynomial.\n\u2022 There exists a linear approximation by a factor of 12 for the general case.\n1. Part of the material presented in this paper is based on two conference publications (Hebrard et al., 2008; Hebrard, Marx, O\u2019Sullivan, & Razgon, 2009).\n\u2022 If no value appears in the domains of more than two distinct variables, then the problem can be solved by a general matching, thus defining another tractable class.\n\u2022 There exists a fixed parameter tractable algorithm for this problem for a parameter k equal to the number of values that appear in more than two distinct domains.\nMoreover, we show that the constraint defined by setting a lower bound on the graphbased cost of SoftAllEqual can be used to efficiently find a set of similar solutions to a set of problems, for instance to promote stability or regularity. Similarly, the dual constraint (SoftAllDiff) can be used to find a set of diverse solutions, for instance to sample a set of configurations. Notice that these two applications have motivated, in part, our choice of cost metrics.\nThe remainder of this paper is organised as follows. In Section 2 we introduce the necessary technical background. A complete taxonomy of constraints of equality and difference, based on results by other authors as well as original material is presented in Section 3. Then, in the following sections, we present the new results allowing us to close the gaps in this taxonomy. First, in Section 4 we present two efficient algorithm for achieving ac and bc when minimising the variable-based cost of SoftAllEqual. Second, in Section 5 we give a proof of NP-hardness for the problem of achieving ac when maximising the graph-based cost of SoftAllDiff. Third, in Section 6 we present a polynomial algorithm to achieve bc on the same constraint. Finally, in the remaining sections, we explore the algorithmic properties of this preference cost. In Section 7, we show that a natural greedy algorithm approximates the maximum number of equalities within a factor of 12 , and that its complexity can be brought down to linear time. Next, in Section 8, we identify a polynomial class for this constraint. Then, in Section 9, we identify a parameter based on this class and show that the SoftAllEqualG constraint is fixed-parameter tractable with respect to this parameter. Finally, in Section 10, we show how the results obtained in this paper can be applied to sample solutions or, conversely, to promote stability. In particular, we describe two constructions using SoftAllDiffminG and SoftAllEqual min G respectively. Concluding remarks are made in Section 11."}, {"heading": "2. Background", "text": "In this section we present the necessary background required by the reader and introduce the notation we use throughout the paper."}, {"heading": "2.1 Constraint Satisfaction", "text": "A constraint satisfaction problem (CSP) is a triplet P = (X ,D, C) where X is a set of variables, D is a mapping of variables to finite sets of values and C is a set of constraints that specify allowed combinations of values for subsets of variables. Without loss of generality, we assume D(X) \u2282 Z for all X \u2208 X , and we denote by min(X) and max(X) the minimum and maximum values in D(X), respectively. An assignment of a set of variables X is a set of pairs S such that |X | = |S| and for each X \u2208 X , there exists (X, v) \u2208 S with v \u2208 D(X). A constraint C \u2208 C is arc consistent (ac) iff, when a variable in the scope of C is assigned any value, there exists an assignment to the other variables in C such that C is satisfied. This satisfying assignment is called a domain support for the value. Similarly, we call a\nrange support an assignment satisfying C, but where values, instead of being taken from the domain of each variable (v \u2208 D(X)), can be any integer between the minimum and maximum of this domain following the natural order on Z (v \u2208 [min(X), . . . ,max(X)]) . A constraint C \u2208 C is range consistent (rc) iff every value of every variable in the scope of C has a range support. A constraint C \u2208 C is bounds consistent (bc) iff for every variable X in the scope of C, min(X) and max(X) have a range support. Given a CSP P = (X ,D, C), we shall use the following notation throughout the paper: n shall denote the number of variables, i.e., n = |X |; m shall denote the number of distinct unary assignments, i.e., m = \u2211 X\u2208X |D(X)|; \u039b shall denote the total set of values, i.e., \u039b = \u22c3 X\u2208X D(X); finally, \u03bb shall denote the total number of distinct values, i.e., \u03bb = |\u039b|."}, {"heading": "2.2 Soft Global Constraints", "text": "Adding a cost variable to a constraint to represent its degree of violation is now common practice in constraint programming. This model was introduced by Petit, Re\u0301gin, and Bessiere (2000). It offers the advantage of unifying hard and soft constraints since arc consistency, along with other types of consistencies, can be applied to such constraints with no extra effort. As a consequence, classical constraint solvers can model over-constrained problems in this way without modification. This approach was applied to a number of other constraints, for instance by van Hoeve, Pesant, and Rousseau (2006). Several cost metrics have been explored for the AllDifferent constraint, as well as several others (e.g., Beldiceanu & Petit, 2004). It is important, if one uses such a unifying model, that the cost metric chosen can be evaluated in polynomial time given a complete assignment of the variables that are constrained. This is the case for the two metrics considered in this paper for the constraints AllDifferent and AllEqual.\nThe variable-based cost counts how many variables need to change in order to obtain a valid assignment for the hard constraint. It can be viewed as the smallest Hamming distance with respect to a satisfying assignment. The graph-based cost counts how many times a component of a decomposition of the constraint is violated. Typically these components correspond to edges of a decomposition graph, e.g. for an AllDifferent constraint, the decomposition graph is a clique and an edge is violated if and only if both variables connected by this edge share the same value. The following example, still for the AllDifferent constraint, shows two solutions involving four variables X1, . . . , X4 each with domain {a, b}:\nS1 = {(X1, a), (X2, b), (X3, a), (X4, b)}.\nS2 = {(X1, a), (X2, b), (X3, b), (X4, b)}.\nIn both solutions, at least two variables must change (e.g., X3 and X4) to obtain a valid solution. Therefore, the variable-based cost is 2 for S1 and S2. However, in S1 only two edges are violated, (X1, X3) and (X2, X4), whilst in S2, three edges are violated, (X2, X3), (X2, X4) and (X3, X4). Thus, the graph-based cost of S1 is 2 whereas it is 3 for S2."}, {"heading": "2.3 Parameterised Complexity", "text": "We shall use the notion of parameterised complexity in Section 9. We refer the reader to Niedermeier\u2019s (2006) book for a comprehensive introduction. Given a problem A, a\nparameterised version of A is obtained by specifying a parameter of this problem and getting as additional input a non-negative integer k which restricts the value of this parameter. The resulting parameterised problem \u3008A, k\u3009 is fixed-parameter tractable (FPT) with respect to k if it can be solved in time f(k) \u2217 nO(1), where f(k) is a function depending only on k. When the size of the problem is significantly larger than the parameter k, a fixed-parameter algorithm essentially has polynomial behaviour. For instance if f(k) = 2k then, as long as k is bounded by log n, the problem can be solved in polynomial time."}, {"heading": "3. Taxonomy", "text": "In this section we introduce a taxonomy of soft constraints based on AllDifferent and AllEqual. We consider the eight algorithmic problems related to constraints of difference and equality defined by combining these two constraints, two costs (graph-based and variable-based), and two objectives (minimisation and maximisation). In fact, because the graph-based costs of AllDifferent and AllEqual are dual, only six different problems are defined. Observe that we consider only costs defined through inequalities, rather than equalities. There are several reasons for doing so. First, reasoning about the lower bound or the upper bound of the cost variable can yield two extremely different problems, and hence different algorithmic solutions. For instance, we shall see that in some cases the problem is tractable in one direction, and NP-hard in the other direction. When reasoning about cost equality, one will often separate the inference procedures relative to the lower bound, upper bound, and intermediate values. Reasoning about lower and upper bounds is sufficient to model an equality although it might hinder domain filtering when intermediate values for the cost are forbidden. We thus cover equalities in a restricted way, albeit arguably reasonable in practice. Indeed, when dealing with costs and objectives, reasoning about inequalities and bounds is more useful in practice than imposing (dis)equalities.\nWe close the last remaining cases: the complexity of achieving ac and bc SoftAllEqualminV in Section 4, that of achieving ac on SoftAllEqualminG in Section 5 and that of achieving bc on SoftAllEqualminG in Section 6. Based on these results, Figure 1 can now be completed (fourth and fifth columns).\nThe next six paragraphs correspond to the six columns of Figure 1, that is, to the twelve elements of the taxonomy. For each of them, we briefly outline the current state of the art, using the following assignment as a recurring example to illustrate the various costs:\nS3 = {(X1, a), (X2, a), (X3, a), (X4, a), (X5, b), (X6, b), (X7, c)}.\n3.1 SoftAllDiff: Variable-based cost, Minimisation\nDefinition 1 (SoftAllDiffminV )\nSoftAllDiffminV ({X1, . . . , Xn}, N)\u21d4 N \u2265 n\u2212 |{v | \u2203Xi = v}|.\nHere the cost to minimise is the number of variables that need to be changed in order to obtain a solution satisfying an AllDifferent constraint. For instance, the cost of S3 is 4 since three of the four variables assigned to a as well as one of the variables assigned to b must change. This objective function was first studied by Petit et al. (2001), and an algorithm for achieving ac in O(n \u221a m) was introduced. To the best of our knowledge, no\nalgorithm with better time complexity for the special case of bounds consistency has been proposed for this constraint. Notice however that Mehlhorn and Thiel\u2019s (2000) algorithm achieves bc on the AllDifferent constraint with an O(n log n) time complexity. The question of whether this algorithm could be adapted to achieve bc on SoftAllDiffminV remains open.\n3.2 SoftAllDiff: Variable-based cost, Maximisation\nDefinition 2 (SoftAllDiffmaxV )\nSoftAllDiffmaxV ({X1, . . . , Xn}, N)\u21d4 N \u2264 n\u2212 |{v | \u2203Xi = v}|.\nHere the same cost is to be maximised. In other words, we want to minimise the number of distinct values assigned to the given set of variables, since the complement of this number to n is exactly the number of variables to modify in order to obtain a solution satisfying an AllDifferent constraint. For instance, the cost of S3 is 4 and the number of distinct values is 7 \u2212 4 = 3. This constraint was studied under the name AtMostNValue. An algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that achieving ac is NP-hard was given by Bessiere et al. (2006).\n3.3 SoftAllDiff: Graph-based cost, Minimisation & SoftAllEqual: Graph-based cost, Maximisation\nDefinition 3 (SoftAllDiffminG \u2261 SoftAllEqualmaxG )\nSoftAllDiffminG ({X1, . . . , Xn}, N)\u21d4 N \u2265 |{{i, j} | Xi = Xj & i < j}|.\nHere the cost to minimise is the number of violated constraints when decomposing AllDifferent into a clique of binary NotEqual constraints. For instance, the cost of S3 is 7 since four variables share the value a (six violations) and two share the value b (one violation). Clearly, it is equivalent to maximising the number of violated binary Equal constraints in a decomposition of a global AllEqual. Indeed, these two costs are complementary to ( n 2 ) of each other (on S3: 7 + 14 = 21). An algorithm in O(nm) for achieving ac on this constraint was introduced by van Hoeve (2004). Again, to our knowledge there is no algorithm improving this complexity for the special case of bc.\n3.4 SoftAllEqual: Graph-based cost, Minimisation & SoftAllDiff: Graph-based cost Maximisation\nDefinition 4 (SoftAllEqualminG \u2261 SoftAllDiffmaxG )\nSoftAllEqualminG ({X1, . . . , Xn}, N)\u21d4 N \u2265 |{{i, j} | Xi 6= Xj & i < j}|.\nHere we consider the same two complementary costs, however we aim at optimising in the opposite way. In Section 5 we show that achieving ac on this constraint is NP-hard and, in Section 6 we show that, when domains are contiguous intervals, computing the optimal cost can be done in O(min(n\u03bb2, n3)). As a consequence, bc can be achieved in polynomial time.\n3.5 SoftAllEqual: Variable-based cost, Minimisation\nDefinition 5 (SoftAllEqualminV )\nSoftAllEqualminV ({X1, . . . , Xn}, N)\u21d4 N \u2265 n\u2212max v\u2208\u039b (|{i | Xi = v}|).\nHere the cost to minimise is the number of variables that need to be changed in order to obtain a solution satisfying an AllEqual constraint. For instance, the cost of S3 is 3 since four variables already share the same value. This is equivalent to maximising the number of variables sharing a given value. Therefore this bound can be computed trivially by counting the occurrences of every value in the domains. However, pruning the domains according to this bound without degrading the time complexity is not as trivial. In Section 4, we introduce two filtering algorithms, achieving ac and rc in the same complexity as that of counting values.\n3.6 SoftAllEqual: Variable-based cost, Maximisation\nDefinition 6 (SoftAllEqualmaxV )\nSoftAllEqualmaxV ({X1, . . . , Xn}, N)\u21d4 N \u2264 n\u2212max v\u2208\u039b (|{i | Xi = v}|).\nHere the same cost has to be maximised. In other words we want to minimise the maximum cardinality of each value. For instance, the cost of S3 is 3, that is, the complement to n of the maximum cardinality of a value (3 = 7 \u2212 4). This is exactly equivalent to applying a Global Cardinality constraint (considering only the upper bounds on the cardinalities). Two algorithms, for achieving ac and bc on this constraint and running in O( \u221a nm) and O(n log n) respectively, was introduced by Quimper et al. (2004).\n4. The Complexity of Arc and Bounds Consistency on SoftAllEqualminV\nHere we show how to achieve ac, rc and bc on the SoftAllEqualminV constraints (see Definition 5). This constraint is satisfied if and only if n minus the cardinality of any set of variables assigned to a single value is less than or equal to the value of the cost variable N . In other words, it is satisfied if there are at least k variables sharing a value, where k = n \u2212 max(N). Therefore, for simplicity sake, we shall consider the following equivalent formulation, where N is a lower bound on the complement to n of the same cost (N \u2032 = n\u2212N):\nN \u2032 \u2264 max v\u2208\u039b (|{i | Xi = v}|).\nWe shall see that to filter the domain of N \u2032 and the Xi\u2019s we need to compute two properties:\n1. An upper bound k\u2217 on the number of occurrences amongst all values.\n2. The set of values that can actually appear k\u2217 times.\nComputing the set of values that appear in the largest possible number of variable domains can be performed trivially in O(m), by counting the number of occurrences of every value, i.e., the number of variables whose domain contains v.\nHowever, if domains are discrete intervals defined by lower and upper bounds, it can be done even more efficiently. Given two integers a and b, a \u2264 b, we say that the set of all integers x, a \u2264 x \u2264 b, is an interval and denote it by [a, b]. In the rest of this section we shall assume that the overall set of values values \u039b = \u22c3 X\u2208X D(X) is the interval [1, \u03bb].\nDefinition 7 (Occurrence function and derivative) Given a constraint network P = (X ,D, C), the occurrence function occ is the mapping from values in \u039b to N defined as follows:\nocc(v) = |{X | X \u2208 X & v \u2208 doms(X)}|.\nThe \u201c derivative\u201d of occ, \u03b4occ, maps each value v \u2208 \u039b to the difference between the value of occ(v \u2212 1) and occ(v):\n\u03b4occ(0) = 0, \u03b4occ(v) = occ(v)\u2212 occ(v \u2212 1).\nWe give an example of the occurrence function for a set of variables with interval domains in Figure 2.\nAlgorithm 1 computes occ\u22121, that is, the inverse of the occurrence function, which maps every element in the interval [1, n] to the set of values appearing that many times. It runs\nAlgorithm 1: Computing the inverse occurrence function. Data: A set of variables: X Result: occ\u22121 : [1, n] 7\u2192 2\u039b\n\u03b4occ(v)\u2190 \u2205; 1 foreach X \u2208 X do\n\u03b4occ(min(X))\u2190 \u03b4occ(min(X)) + 1; \u03b4occ(max(X) + 1)\u2190 \u03b4occ(max(X) + 1)\u2212 1;\n2 \u2200x \u2208 [1, n], occ\u22121(x)\u2190 \u2205; x\u2190 0; pop first element (v, a) of \u03b4occ; repeat\npop first element (w, b) of \u03b4occ; x\u2190 x+ \u03b4occ(a); occ\u22121(x)\u2190 occ\u22121(x) \u222a [a, b\u2212 1]; a\u2190 b;\nuntil \u03b4occ = \u2205;\nin O(n log n) worst-case time complexity if we assume it is easy to extract both an upper bound (k\u2217 \u2265 N \u2032) and the set of values that can appear k\u2217 times from occ\u22121.\nThe idea behind this algorithm, which we shall reuse throughout this paper, is that when domains are given as discrete intervals one can compute the non-null values of the derivative \u03b4occ of the occurrence function occ in O(n log n) time. The procedure is closely related to the concept of sweep algorithms (Beldiceanu & Carlsson, 2001) used, for instance, to implement filtering algorithms for the Cumulative constraint. Instead of scanning the entire horizon, one can jump from an event to the next, assuming that nothing changes between two events. As in the case of the Cumulative constraint, events here correspond to start and end points of the domains. In fact, it is possible to compute the same lower bound, with the same complexity, by using Petit, Re\u0301gin, and Bessiere\u2019s (2002) Range-based Max-CSP Algorithm (RMA)2 on a reformulation as a Max-CSP. Given a set of variables X , we add an extra variable Z whose domain is the union of all domains in X : D(Z) = \u039b = \u22c3 X\u2208X D(X). Then\n2. We thank the anonymous reviewer who made this observation.\nwe link it to other variables in X through binary equality constraints:\n\u2200X \u2208 X , Z = X.\nThere is a one-to-one mapping between the solutions of this Max-CSP and the satisfying assignments of a SoftAllEqualminV constraint on (X , N), where the value of N corresponds to the number of violated constraints in the Max-CSP. The lower bound on the number of violations computed by RMA and the lower bound k\u2217 on N computed in Algorithm 1 are, therefore, the same. Moreover the procedures are essentially equivalent, i.e., modulo the modelling step. Algorithm 1 can be seen as a particular case of RMA: the same ordered set of intervals is computed, and subsequently associated with a violation cost. However, we use our formalism, since the notion of occurrence function and its derivative is important and used throughout the paper.\nWe first define a simple data structure that we shall use to compute and represent the function \u03b4occ. A specific data structure is required since indexing the image of \u03b4occ(v) by the value v would add a factor of \u03bb to the (space and therefore time) complexity. The non-zero values of \u03b4occ are stored as a list of pairs whose first element is a value v \u2208 [1, . . . , \u03bb] and second element stands for \u03b4occ(v). The list is maintained in increasing order of the pair\u2019s first element. Given an ordered list \u03b4occ = [(v1, o1), . . . , (vk, ok)], the assignment operation \u03b4occ(vi)\u2190 oi can therefore been done in O(log |\u03b4occ|) steps as follows:\n1. The rank r of the pair (vj , oj) such that vj is minimum and vj \u2265 vi is computed through a dichotomic search.\n2. If vi = vj , the pair (vj , oj) is removed.\n3. The pair (vi, oi) is inserted at rank r.\nMoreover, one can access the element with minimum (resp. maximum) first element in constant time since it is first (resp. last) in the list. Finally, the value of \u03b4occ(vi) is oi if there exists a pair (vj , oj) in the list, and 0 otherwise. Computing this value can also be done in logarithmic time.\nThe derivative \u03b4occ(v) is computed in Loop 1 of Algorithm 1 using the assignment operator defined above. Observe that if D(X) = [a, b], then X contributes only to two values of \u03b4occ: it increases \u03b4occ(a) by 1 and decreases \u03b4occ(b + 1) by 1. For every value w such that there is no X with min(X) = w or max(X) + 1 = w, \u03b4occ(w) is null. In other words, we can define \u03b4occ(v) for any value v, as follows:\n\u03b4occ(v) = (|{i | min(Xi) = v}| \u2212 |{i | max(Xi) = v \u2212 1}|).\nTherefore, by going through every variable X \u2208 X , we can compute the non-null values of \u03b4occ in time O(n log n) using the simple list structure described above.\nThen, starting from Line 2, we compute occ\u22121 by going through the non-zero values v of the derivative, i.e. such that \u03b4occ(v) 6= 0, in increasing order of v. Recall that we use an ordered list, so this is trivially done in linear time. By definition, the occurrence function is constant on the interval defined by two such successive values. Since the number of non-zero values of \u03b4occ is bounded by O(n), the overall worst-case time complexity is in O(n log n). We use Figure 3 (a,c & d) to illustrate an execution of Algorithm 1. First, six variables and\ntheir domains are represented in Figure 3(a). Then, in Figures 3(c) and 3(d) we show the derivative and the inverse, respectively, of the occurrence function.\nAlternatively, when \u03bb < n log n, it is possible to compute occ\u22121 in O(n+\u03bb) by replacing the data structure used to store \u03b4occ by a simple array, indexed by values in [1, \u03bb]. Accessing and updating a value of \u03b4occ can thus be done in constant time.\nNow we show how to prune the variables in X with respect to this bound without degrading the time complexity. According to the method used we can, therefore, achieve ac or rc in a worst-case time complexity of O(m) or O(min(n+ \u03bb, n log n), respectively.\nTheorem 1 Enforcing ac (resp. rc) on SoftAllEqualminV can be achieved in in O(m) steps (resp. O(min(n+ \u03bb, n log n)).\nProof. We suppose, without loss of generality, that the current lower bound on N \u2032 is k. We first compute the inverse occurrence function either by counting values, or considering interval domains using Algorithm 1. From this we can define the set of values with highest number of occurrences. Let this number of occurrences be k\u2217, and the corresponding set of values be V (i.e. occ\u22121(k\u2217) = V ). Then there are three cases to consider:\n1. First, if every value appears in strictly fewer than k domains (k\u2217 < k) then the constraint is violated.\n2. Second, if at least one value v appears in the domains of at least k + 1 variables (k\u2217 > k), then we can build a support for every value w \u2208 D(X). Let v \u2208 V , we assign all the variables in X \\X with v when possible. The resulting assignment has at least k occurrences of v, hence it is consistent. Consequently, since k\u2217 > k, every value is consistent.\n3. Otherwise, if neither of the two cases above hold, we know that no value appears in more than k domains, and that at least one appears k times. Recall that V denotes the set of such values. In this case, the pair (X, v) is inconsistent if and only if v 6\u2208 V & V \u2282 D(X). We first suppose that this condition does not hold and show that we can build a support. If v \u2208 V then clearly we can assign every possible variable to v and achieve a cost of k. If V 6\u2282 D(X), then we consider w such that w \u2208 V and w 6\u2208 D(X). By assigning every variable with w when possible we achieve a cost of k no matter what value is assigned to X.\nNow we suppose that v 6\u2208 V & V \u2282 D(X) holds and show that (X, v) does not have an ac support. Indeed, once X is assigned to v the domains are such that no value appears in k domains or more, since every value in V has now one fewer occurrence, hence we are back to Case 1.\nComputing the set V of values satisfying the condition above can be done easily once the inverse occurrence function has been computed. On the one hand, if this function occ\u22121 has been computed by counting every value in every domain, then the supports used in the proofs are all domain supports, hence ac is achieved. On the other hand, if domains are approximated by their bounds and Algorithm 1 is used instead, the supports are all range supports, hence rc is achieved. In Case 3, the domain can be pruned down to the set V of values whose number of occurrences is k, as illustrated in Figure 3 (b). 2\nCorollary 1 Enforcing bc on SoftAllEqualminV can be achieved in O(min(n+\u03bb, n log n) steps.\nProof. This is a direct implication of Theorem 1. 2\nThe proof of Theorem 1 yields a domain filtering procedure. Algorithm 2 achieves either ac or rc depending on the version of Algorithm 1 used in Line 1 to compute the inverse occurrence function. The later function occ\u22121 is then used in Line 2, 3 and 4 to, respectively, catch a global inconsistency, prune the upper bound of N \u2032 and prune the domains of the variables in X .\nFigure 3(b) illustrates the pruning that one can achieve on X provided that the lower bound on N \u2032 is equal to 4. Dashed lines represent inconsistent intervals. The set V of values used in Line 4 of Algorithm 2 is occ\u22121(4) = {[15, 40] \u222a [70, 70]}.\nAlgorithm 2: Propagation of SoftAllEqualminV ({X1, . . . , Xn}, N \u2032). 1 occ\u22121 \u2190 Algorithm 1; ub\u2190 n; while occ\u22121(ub) = \u2205 do\nub\u2190 ub\u2212 1; 2 if min(N \u2032) > ub then fail;\nelse 3 max(N \u2032)\u2190 ub;\nif min(N \u2032) = max(N \u2032) then V \u2190 occ\u22121(min(N \u2032));\n4 foreach X \u2208 X do if V \u2282 D(X) then D(X)\u2190 V ;\n5. The Complexity of Arc Consistency on SoftAllEqualminG\nHere we show that achieving ac on SoftAllEqualminG is NP-hard. In order to achieve ac we need to compute an arc consistent lower bound on the cost variable N constrained as follows:\nN \u2264 |{{i, j} | Xi 6= Xj & i < j}|.\nIn other words, we want to find an assignment of the variables in X minimising the number of pairwise disequalities, or maximising the number of pairwise equalities. We consider the corresponding decision problem (SoftAllEqualminG -decision), and show that it is NP-hard through a reduction from 3dMatching (Garey & Johnson, 1979).\nDefinition 8 (SoftAllEqualminG -decision) Data: An integer N , a set X of variables. Question: Does there exist a mapping s : X 7\u2192 \u039b such that \u2200X \u2208 X , s[X] \u2208 D(X) and |{{i, j} | s[Xi] = s[Xj ] & i 6= j}| \u2265 N?\nDefinition 9 (3dMatching) Data: An integer K, three disjoint sets X,Y, Z, and T \u2286 X \u00d7 Y \u00d7 Z. Question: Does there exist M \u2286 T such that |M | \u2265 K and \u2200m1,m2 \u2208M, \u2200i \u2208 {1, 2, 3}, m1[i] 6= m2[i]?\nTheorem 2 (The Complexity of SoftAllEqualminG ) Finding a satisfying assignment for the SoftAllEqualminG constraint is NP-complete even if no value appears in more than three domains.\nProof. The problem SoftAllEqualminG -decision is clearly in NP: checking the number of equalities in an assignment can be done in O(n2) time.\nWe use a reduction from 3dMatching to show completeness. Let P = (X,Y, Z, T,K) be an instance of 3dMatching, where: K is an integer; X,Y, Z are three disjoint sets such that X \u222a Y \u222a Z = {x1, . . . , xn}; and T = {t1, . . . , tm} is a set of triplets over X \u00d7 Y \u00d7 Z. We build an instance I of SoftAllEqualminG as follows:\n1. Let n = |X|+ |Y |+ |Z|, we build n variables {X1, . . . , Xn}.\n2. For each tl = \u3008xi, xj , xk\u3009 \u2208 T , we have l \u2208 D(Xi), l \u2208 D(Xj) and l \u2208 D(Xk).\n3. For each pair (i, j) such that 1 \u2264 i < j \u2264 n, we put the value (|T |+ (i\u2212 1) \u2217 n+ j) in both D(Xi) and D(Xj).\nWe show there exists a matching of P of size K if and only if there exists a solution of I with b3K+n2 c equalities. We refer to \u201ca matching of P\u201d and to a \u201csolution of I\u201d as \u201ca matching\u201d and \u201ca solution\u201d throughout this proof, respectively. \u21d2: We show that if there exists a matching of cardinality K then there exists a solution with at least b3K+n2 c equalities. Let M be a matching of cardinality K. We build a solution as follows. For all tl = \u3008xi, xj , xk\u3009 \u2208 M we assign Xi, Xj and Xk to l (item 2 above). Observe that there remain exactly n\u2212 3K unassigned variables after this process. We pick an arbitrary pair of unassigned variables and assign them with their common value (item 3 above), until at most one variable is left (if one variable is left we assign it to an arbitrary value). Therefore, the solution obtained in this way has exactly b3K+n2 c equalities, 3K from the variables corresponding to the matching and bn\u22123K2 c for the remaining variables. \u21d0: We show that if the cardinality of the maximal matching is K, then there is no solution with more than b3K+n2 c equalities. Let S be a solution. Furthermore, let L be the number of values appearing three times in S. Observe that this set of values corresponds to a matching. Indeed, a value l appears in three domains D(Xi),D(Xj) and D(Xk) if and only if there exists a triplet tl = \u3008xi, xj , xk\u3009 \u2208 T (item 2 above). Since a variable can only be assigned to a single value, the values appearing three times in a solution form a matching. Moreover, since no value appears in more than three domains, all other values can appear at most twice. Hence the number of equalities in S is less than or equal to b3L+n2 c, where L is the size of a matching. It follows that if there is no matching of cardinality greater than K, there is no solution with more than b3K+n2 c equalities. 2\nCohen, Cooper, Jeavons, and Krokhin (2004) showed that the language of soft binary equality constraints is NP-complete, for as few as three distinct values. On the one hand, Theorem 2 applies to a more specific class of problems where the constraint network formed by the soft binary constraints is a clique. On the other hand, the proof requires an unbounded number of values, these two results are therefore incomparable. However, we shall see in Section 9 that this problem is fixed parameter tractable with respect to the number of values, hence polynomial when it is bounded.\n6. The Complexity of Bounds Consistency on SoftAllEqualminG\nIn this section we introduce an efficient algorithm that, assuming the domains are discrete intervals, computes the maximum possible pairs of equal values in an assignment. We therefore need to solve the optimisation version of the problem defined in the previous section (Definition 8):\nDefinition 10 (SoftAllEqualminG -optimisation) Data: A set X of variables. Question: What is the maximum integer K such that there exists a mapping s : X 7\u2192 \u039b satisfying \u2200X \u2208 X , s[X] \u2208 D(X) and |{{i, j} | s[Xi] = s[Xj ] & i 6= j}| = K?\nThe algorithm we introduce allows us to close the last remaining open complexity question in Figure 1: bc on the SoftAllEqualminG constraint. We then improve it by reducing the time complexity thanks to a preprocessing step.\nWe use the same terminology as in Section 4, and refer to the set of all integers x such that a \u2264 x \u2264 b as the interval [a, b]. Let X be the set of variables of the considered CSP and assume that the domains of all the variables of X are sub-intervals of [1, \u03bb]. We denote by ME(X ) the set of all assignments P to the variables of X such that the number of pairs of equal values of P is the maximum possible. The subset of X containing all the variables whose domains are subsets of [a, b] is denoted by Xa,b. The subset of Xa,b including all the variables containing the given value c in their domains is denoted by Xa,b,c. Finally the number of pairs of equal values in an element of ME(Xa,b) is denoted by Ca,b(X ) or just Ca,b if the considered set of variables is clear from the context. For notational convenience, if b < a, then we set Xa,b = \u2205 and Ca,b = 0. The value C1,\u03bb(X ) is the number of equal pairs of values in an element of ME(X ).\nTheorem 3 C1,\u03bb(X ) can be computed in O((n+ \u03bb)\u03bb2) steps.\nProof. The problem is solved by a dynamic programming approach: for every a, b such that 1 \u2264 a \u2264 b \u2264 \u03bb, we compute Ca,b. The main observation that makes it possible to use dynamic programming is the following: in every P \u2208ME(Xa,b) there is a value c (a \u2264 c \u2264 b) such that every variable X \u2208 Xa,b,c is assigned value c. To see this, let value c be a value that is assigned by P to a maximum number of variables. Suppose that there is a variable X with c \u2208 D(X) that is assigned by P to a different value, say c\u2032. Suppose that c and c\u2032 appear on x and y variables, respectively. By changing the value of X from c\u2032 to c, we increase the number of equalities by x \u2212 (y \u2212 1) \u2265 1 (since x \u2265 y), contradicting the optimality of P .\nNotice that Xa,b \\ Xa,b,c is the disjoint union of Xa,c\u22121 and Xc+1,b (if c \u2212 1 < a or c + 1 > b, then the corresponding set is empty). These two sets are independent in the sense that there is no value that can appear on variables from both sets. Thus it can be assumed that P \u2208 ME(Xa,b) restricted to Xa,c\u22121 and Xc+1,b are elements of ME(Xa,c\u22121) and ME(Xc+1,b), respectively. Taking into consideration all possible values c, we get\nCa,b = max c,a\u2264c\u2264b\n(( |Xa,b,c|\n2\n) + Ca,c\u22121 + Cc+1,b ) . (1)\nIn the first step of Algorithm 3, we compute |Xa,b,c| for all values of a, b, c. For each triple a, b, c, it is easy to compute |Xa,b,c| in time O(n), hence all these values can be computed in time O(n\u03bb3). However, the running time can be reduced to O((n+ \u03bb)\u03bb2) by using the same idea as in Algorithm 1. For each pair a, b, we compute the number of occurrences of each value c by first computing a derivative \u03b4a,b. More precisely, we define \u03b4a,b(c) = |Xa,b,c|\u2212 |Xa,b,c\u22121| and compute \u03b4a,b(c) for every a < c \u2264 b (Algorithm 3, Line 1-2). Thus by going through all the variables, we can compute the \u03b4a,b(c) values for a fixed a, b and for all a \u2264 c \u2264 b in time O(n) and we can also compute |Xa,b,a| in the same time bound. Now it is possible to compute the values |Xa,b,c|, a < c \u2264 b in time O(\u03bb) by using the equality |Xa,b,c| = |Xa,b,c\u22121|+ \u03b4a,b(c) iteratively (Algorithm 3, Line 3).\nIn the second step of the algorithm, we compute all the values Ca,b. We compute these values in increasing order of b\u2212 a. If a = b, then Ca,b = (|Xa,a,a|\n2\n) . Otherwise, values Ca,c\u22121\nand Cc+1,b are already available for every a \u2264 c \u2264 b, hence Ca,b can be determined in time O(\u03bb) using Eq. (1) (Algorithm 3, Line 4). Thus all the values Ca,b can be computed in time\nAlgorithm 3: Computing the maximum number of equalities. Data: A set of variables: X Result: C1,\u03bb(X ) \u2200 1 \u2264 a, b, c \u2264 \u03bb, \u03b4a,b(c)\u2190 |Xa,b,c| \u2190 Ca,b \u2190 0; foreach k \u2208 [0, \u03bb\u2212 1] do\nforeach a \u2208 [1, \u03bb\u2212 k] do b\u2190 a+ k; foreach X \u2208 Xa,b do\n1 \u03b4a,b(min(X))\u2190 \u03b4a,b(min(X)) + 1; 2 \u03b4a,b(max(X) + 1)\u2190 \u03b4a,b(max(X) + 1)\u2212 1; foreach c \u2208 [a, b] do 3 |Xa,b,c| \u2190 |Xa,b,c\u22121|+ \u03b4a,b(c); 4 Ca,b \u2190 max(Ca,b, ( (|Xa,b,c| 2 ) + Ca,c\u22121 + Cc+1,b));\nreturn C1,\u03bb;\nO(\u03bb3), including C1,\u03bb, which is the value of the optimum solution of the problem. Using standard techniques (storing for each Ca,b a value c that minimises (1)), a third step of the algorithm can actually produce a variable assignment that obtains the maximum value. 2\nAlgorithm 3 computes the largest number of equalities one can achieve by assigning a set of variables with interval domains. It can therefore be used to find an optimal solution to either SoftAllDiffmaxG or SoftAllEqual min G . Notice that for the latter one needs\nto take the complement to ( n 2 ) in order to get the value of the violation cost. Clearly, it follows that achieving range or bounds consistency on these two constraints can be done\nin polynomial time, since Algorithm 3 can be used as an oracle for testing the existence of a range support. We give an example of the execution of Algorithm 3 in Figure 4. A set of ten variables, from X1 to X10 are represented. Then we give the table Ca,b for all pairs a, b \u2208 [1, \u03bb].\nThe complexity can be further reduced if \u03bb n. Here again, we will use the occurrence function, albeit in a slightly different way. The intuition is that some values and intervals of values are dominated by other. When the occurrence function is monotonically increasing, it means that we are moving toward dominating values (they can be taken by a larger set of variables), and conversely, a monotonic decrease denotes dominated values. Notice that since we are considering discrete values, some variations may not be apparent in the occurrence function. For instance, consider two variables X and Y with respective domains [a, b] and [b + 1, c] such that a \u2264 b \u2264 c. The occurrence function for these two variables is constant on [a, c]. However, for our purpose, we need to distinguish between \u201ctrue\u201d monotonicity and that induced by the discrete nature of the problem. We therefore consider some rational values when defining the occurrence function. In the example above, by introducing an extra point b + 12 to the occurrence function, we can now capture the fact that in fact it is not monotonic on [a, c].\nLet X be a set of variables with interval domains in [1, \u03bb]. Consider the occurrence function occ : Q 7\u2192 [0..n], where Q \u2282 Q is a set of values of the form a/2 for some a \u2208 N, such that min(Q) = 1 and max(Q) = \u03bb. Intuitively, the value of occ(a) is the number of variables whose domain interval encloses the value a, more formally:\n\u2200a \u2208 Q, occ(a) = |{X | X \u2208 X ,min(X) \u2264 a \u2264 max(X)}|.\nSuch a function, along with the corresponding set of intervals, is depicted in Figure 5. A crest of the function occ is an interval [a, b] \u2286 Q such that for some c \u2208 [a, b], occ is monotonically increasing on [a, c] and monotonically decreasing on [c, b]. For instance, on the set intervals represented in Figure 5, [1, 15] is a crest since it is monotonically increasing on [1, 12] and monotonically decreasing on [12, 15].\nLet I be a partition of [1, \u03bb] into a set of intervals such that every element of I is a crest. For instance, I = {[1, 15], [16, 20], [21, 29], [30, 42]} is such a partition for the set of intervals shown in Figure 5. We shall map each element of I to an integer corresponding to its rank in the natural order. We denote by RI(X ) the reduction of X by the partition I. The reduction has as many variables as X (equation 2 below) but the domains are replaced with the set of intervals in I that overlap with the corresponding variable in X (equation 3 below). Observe that the domains remain intervals after the reduction.\nRI(X ) = {X \u20321, . . . , X \u2032|X |}. (2)\n\u2200X \u2032i \u2208 RI(X ), D(X \u2032i) = {I | I \u2208 I & D(Xi) \u2229 I 6= \u2205}. (3)\nFor instance, the set of intervals depicted in Figure 5 can be reduced to the set shown in Figure 4, where each element in I is mapped to an integer in [1, 4].\nTheorem 4 If I is a partition of [1, \u03bb] such that every element of I is a crest of occ, then ME(X ) = ME(RI(X )).\nProof. First, we show that for any optimal solution s \u2208ME(X ), we can produce a solution s\u2032 \u2208ME(RI(X )) that has at least as many equalities as s. Indeed, for any value a, consider every variable X assigned to this value, that is, such that s[X] = a. Let I \u2208 I be the crest containing a, by definition we have I \u2208 D(X \u2032). Therefore we can assign all these variables to the same value I.\nNow we show the opposite, that is, given a solution to the reduced problem, one can build a solution to the original problem with at least as many equalities. The key observation is that, for a given crest [a, b], all intervals overlapping with [a, b] have a common value. Indeed, suppose that this is not the case, that is, there exists [c1, d1] and [c2, d2] both overlapping with [a, b] and such that d1 < c2. Then occ(d1) > occ(d1 + 1 2) and similarly occ(c2\u2212 12) < occ(c2). However, since a \u2264 d1 < c2 \u2264 b, [a, b] would not satisfy the conditions for being a crest, hence a contradiction. Therefore, for a given crest I, and for every variable X \u2032 such that s\u2032[X \u2032] = I, we can assign X to this common value, hence obtaining as many equalities. 2\nWe show that this transformation can be achieved in O(n log n) steps. We once again use the derivative of the occurrence function (\u03b4occ), however, defined on Q rather than [1, \u03bb]:\n\u03b4occ(v)\u2190 (|{i | min(Xi) = v}| \u2212 |{i | max(Xi) = v \u2212 1\n2 }|).\nMoreover, we can compute it in O(n log n) steps as shown in Algorithm 4. We first compute the non-null values of \u03b4occ by looping through each variable X \u2208 X (Line 1). We use the\nsame data structure as for Algorithm 1, hence the complexity of this step isO(n log n). Next, we create the partition into crests by going through the derivative once and identifying the inflection points. The variable polarity (Line 3) is used to keep track of the evolution of the function occ. The decreasing phases are denoted by polarity = neg whilst the increasing phases correspond to polarity = pos. We know that a value v is the end of a crest interval when the variable polarity switches from neg to pos. Clearly, the number of elements in \u03b4occ is bounded by 2n. Recall that the list data structure is sorted. Therefore, going through the values \u03b4occ(v) in increasing order of v can be done in linear time, hence the overall O(n log n) worst-case time complexity.\nAlgorithm 4: Computing a partition into crests. Data: A set of variables: X Result: I \u03b4occ \u2190 \u2205;\n1 foreach X \u2208 X do \u03b4occ(min(X))\u2190 \u03b4occ(min(X)) + 1; \u03b4occ(max(X) +\n1 2 )\u2190 \u03b4occ(max(X) + 12 )\u2212 1;\nI \u2190 \u2205; min\u2190 max\u2190 1;\n2 while \u03b4occ 6= \u2205 do 3 polarity \u2190 pos;\nk = 1; repeat\npick and remove the first element (a, k) of \u03b4occ; max\u2190 round(a)\u2212 1; if polarity = pos & k < 0 then polarity \u2190 neg;\nuntil polarity = pos or k < 0 ; add [min,max] to I; min\u2190 max+ 1;\nreturn I\nTherefore, we can replace every crest by a single value at the preprocessing stage and then run Algorithm 3. Moreover, observe that the number of crests is bounded by n, since each needs at least one interval to start and one interval to end. Thus we obtain the following theorem, where n stands for the number of variables, \u03bb for the number of distinct values, and m for the sum of all domain sizes.\nTheorem 5 Enforcing rc on SoftAllEqualminG can be achieved in O(min(\u03bb2, n2)nm) steps.\nProof. If \u03bb \u2264 n then one can achieve range consistency by iteratively calling Algorithm 3 after assigning each of the O(m) unit assignments ((X, v) \u2200X \u2208 X , v \u2208 D(X)). The resulting complexity is O(n\u03bb2)m (see Theorem 3, the term \u03bb3 is absorbed by n\u03bb2 due to \u03bb \u2264 n).\nOtherwise, if \u03bb > n, the same procedure is used, but after applying the reformulation described in Algorithm 4. The complexity of the Algorithm 4 is O(n log n), and since after the reformulation we have \u03bb = O(n), the resulting complexity is O(n3m). 2"}, {"heading": "7. Approximation Algorithm", "text": "We have completed the taxonomy of soft global constraints introduced in Section 3. However, in this section and in the rest of the paper we refine our analysis of the problem of maximising the number of pairs of variables sharing a value, that is, SoftAllEqualminG - optimisation (Definition 10).\nGiven a solution s over a set of variable X , we denote by obj(s) the number of equalities in X . obj(s) = |{{i, j} | s[Xi] = s[X[j] & i 6= j}|. Furthermore, we shall denote as s\u2217 and obj(s\u2217) an optimal solution and the number of equalities in this solution, respectively. We first study a natural greedy algorithm for approximating the maximum number of equalities in a set of variables (Algorithm 5). This algorithm picks the value that occurs in the largest number of domains, and assigns as many variables as possible to this value (this can be achieved in O(m)). Then it recursively repeats the process on the resulting sub-problem until all variables are assigned (at most O(n) times). We show that, surprisingly, this straightforward algorithm approximates the maximum number of equalities with a factor of 12 in the worst case. Moreover, it can be implemented to run in O(m) amortised time. We use the following data structures3:\n\u2022 var : \u039b 7\u2192 2X maps every value v to the set of variables whose domains contain v.\n\u2022 occ : \u039b 7\u2192 N maps every value v to the number of variables whose domains contain v.\n\u2022 val : N 7\u2192 2\u039b maps every integer i \u2208 [0..n] to the set of values appearing in exactly i domains.\nThese data structures are initialised in Lines 1, 2 and 3 of Algorithm 5, respectively. Then, Algorithm 6 recursively chooses the value with largest number of occurrences (Line 2), makes the corresponding assignments (Line 7) while updating the current state of the data structures (Loop 3).\nAlgorithm 5: Computing a lower bound on the maximum number of equalities. Data: A set of variables: X Result: An integer E such that obj(s\u2217)/2 \u2264 E \u2264 obj(s\u2217)\n1 var(v)\u2190 \u2205, \u2200v \u2208 \u22c3 X\u2208X D(X);\nforeach X \u2208 X do foreach v \u2208 D(X) do\nadd X to var(v); 2 occ(v)\u2190 |var(v)|, \u2200v \u2208 \u22c3 X\u2208X D(X); 3 val(k)\u2190 \u2205, \u2200k \u2208 [0..n]; foreach v \u2208 \u22c3 X\u2208X D(X) do\nadd v to val(|var(v)|); return AssignAndRecurse(var, val, occ, n);\nTheorem 6 (Algorithm Correctness) Algorithm 5 approximates the optimal satisfying assignment of the SoftAllEqualG constraint within a factor of 1 2 and - provided that the data-structure for representing domains respects some assumptions - runs in O(m).\n3. We describe these structures at a lower level in the subsequent proof of complexity.\nAlgorithm 6: procedure AssignAndRecurse of Algorithm 5.\nData: A mapping: var : \u039b 7\u2192 2X , A mapping: val : N 7\u2192 2\u039b, A mapping: occ : \u039b 7\u2192 [0..n], An integer: k\n1 while val(k) = \u2205 do k \u2190 k \u2212 1; if k \u2264 1 then\nreturn 0;\nelse 2 pick and remove any v \u2208 val(k); 3 foreach X \u2208 var(v) do if v \u2208 D(X) then foreach w 6= v \u2208 D(X) do 4 remove w from val(occ(w)); 5 occ(w)\u2190 occ(w)\u2212 1; 6 add w to val(occ(w));\n7 assign X with v;\nreturn k(k\u22121) 2 +AssignAndRecurse(var, val, k);\nProof. We first prove the correctness of the approximation ratio, the soundness of the algorithm and then the complexity of the algorithm.\nApproximation Factor. We proceed using induction on the number of distinct values \u03bb in the current subproblem involving all unassigned variables. Let s be the solution computed by Algorithm 5 and let s\u2217 be an optimal solution. We denote as P (\u03bb) the proposition \u201cIf there are no more than \u03bb values in the union of the domains of X , then obj(s) \u2265 obj(s\u2217)/2\u201d. P (1) implies that every unassigned variable can be assigned to a unique value v. Algorithm 6 therefore chooses this value and assigns all variables to it. In this case obj(s) = obj(s\u2217).\nNow we suppose that P (\u03bb) holds and we show that P (\u03bb+ 1) also holds. Let the set of variables X of the problem be such that | \u22c3 X\u2208X D(X)| = \u03bb+ 1 and let v be the first value chosen by Algorithm 6. We partition the variables into two subset Xv and X\u0304v depending on the presence of the value v in their domains.\n\u2022 Xv = {X \u2208 X | v \u2208 D(X)} is the set of variables whose domains contain v.\n\u2022 X\u0304v = X \\ Xv is the complementary set of variables which do not contain the value v.\nUsing these notations, we will partition the equalities into two subsets in order to count them. The first subset of equalities are those involving at least one variable in Xv, the second subset are those restricted to variables in X\u0304v.\nWe first compute a bound on the number of equalities that one can achieve on X . Let k = |Xv|, let s\u2217v be an optimal solution on X\u0304v and let obj(s\u2217v) be the number of equalities in s\u2217v. For each variable X \u2208 Xv, given any value w in D(X), there are no more than k variables in X containing w. Indeed, v was chosen for maximising this criterion and belongs to the domains of exactly k variables. Therefore, there are at most k(k\u2212 1) equalities that involve at least a variable in Xv, since each one can be involved in at most k\u2212 1 equalities, and there are k of them. Consequently, on the set of variables X , one can achieve at most k(k \u2212 1) + obj(s\u2217v) equalities.\nOn the other hand, Algorithm 6 assigns every variable in Xv to v and therefore produces k(k \u2212 1)/2 equalities involving at least one variable in Xv. Moreover, observe that since v does not belong to any domain in X\u0304v, the number of distinct values in X\u0304v is at most \u03bb.\nThe induction hypothesis P (\u03bb) can therefore be used, hence we know that the number of equalities achieved by Algorithm 5 on the subset X\u0304v is at least obj(s\u2217v)/2. Consequently, on the set of variables X , Algorithm 5 achieves at least k(k \u2212 1)/2 + obj(s\u2217v)/2 equalities.\nSince the lower bound on the number of equalities achieved by the greedy algorithm is half of the upper bound computed above, we can conclude that if P (\u03bb + 1) holds, then P (\u03bb+ 1) also holds.\nCorrectness. Here we show that the mappings occ and val are correctly updated in a call to Algorithm 6. The domain of a variable X changes only when it is assigned to a value v in Line 7. In that case, the occurrence of every value w \u2208 D(X) such that w 6= v is decreased by one when assigning X to v. Indeed, for every such value w, occ(w) is decremented and w is removed from val(occ(w) + 1) and added to val(occ(w)).\nComplexity. Now we show that Algorithm 5 runs in O(m) steps under the following assumptions:\n\u2022 The values are consecutive and taken from the set {1, . . . , \u03bb}.\n\u2022 Assigning a variable to a value can be done in constant time.\n\u2022 Checking membership of a value in a variable\u2019s domain can be done in constant time.\nNotice that if the first assumption does not hold, one can rename values. However, it would require a further O(\u03bb log \u03bb) time complexity to sort them, as well as O(n\u03bb) to create a new set of domains.\nFor every 0 \u2264 k \u2264 n, we use a doubly linked list to represent val(k). Moreover we use a single array index with \u03bb+ 1 elements to store the current position of every value v in the list it appears in (observe that each value appears in exactly one list). To add a value v in val(k) we simply append it at the tail of the list and set its index to the previous length. To remove a value v from val(k), we delete the element at position index[v] in val(k). The total space complexity for this data-structure is therefore O(\u03bb). For each value v, the set of variables var(k) is implemented as a simple list, hence a O(m) space complexity. The mapping occ(v) is represented as an array with one element per value, hence a O(\u03bb) space complexity.\nInitialising all three mappings is done in linear time since each addition requires only constant time. This step can therefore be achieved in O(m) steps. In Line 1 of Algorithm 6, k can be decremented at most n times in total, hence Line 2 is executed at most n times in total.\nObserve that no value is chosen more than once in Line 2. Moreover, the total space complexity of var is O(m). Therefore, the total number of steps in Loop 3 is O(m).\nLast, observe that no pair variable/value (X,w) will be explored more than once in Lines 4, 5 and 6. Indeed, since X is assigned to v in Line 7, it will never pass the condition in Line 3 since subsequent chosen values will not be equal to v. The overall time complexity is thus in O(m).\n2\nTheorem 7 (Tightness of the Approximation Ratio) The approximation factor of 12 for Algorithm 5 is tight.\nProof. Let {X1, . . . , X4} be a set of four variables with domains as follows:\nX1 \u2208 {a}; X2 \u2208 {b}; X3 \u2208 {a, c}; X4 \u2208 {b, c}.\nEvery value appears in exactly two domains, hence Algorithm 5 can choose any value. We suppose that the value c is chosen first. At this point no other value can contribute to an equality, hence Algorithm 5 returns 1. However, it is possible to achieve two equalities with the following solution: X1 = a, X3 = a, X2 = b, X4 = b. 2"}, {"heading": "8. Tractable Class", "text": "In this section we explore further the connection between the SoftAllEqualminG constraint and vertex matching. We showed earlier that the general case was linked to 3dMatching. We now show that the particular case where no value appears in more than two domains solving the SoftAllEqualG constraint is equivalent to the vertex matching problem on general graphs, and therefore can be solved by a polynomial time algorithm. We shall then use this tractable class to show that SoftAllEqualG is NP-hard only if an unbounded number of values appear in more than two domains.\nDefinition 11 (The VertexMatching Problem) Data: An integer K, an undirected graph G = (V,E). Question: Does there exist M \u2286 E such that |M | \u2265 K and \u2200e1, e2 \u2208 M , e1 and e2 do not share a vertex.\nTheorem 8 (Tractable Class of SoftAllEqualminG ) If all triplets of variables X,Y, Z \u2208 X are such that D(X)\u2229D(Y )\u2229D(Z) = \u2205 then finding an optimal satisfying assignment to SoftAllEqualminG is in P .\nProof. In order to solve this problem, we build a graph GX = (V,E) with a vertex xi for each variable Xi \u2208 X , that is, V = {xi | Xi \u2208 X}. Then for each pair {i, j} such that D(Xi) \u2229 D(Xj) 6= \u2205, we create an undirected edge {i, j}; let E = {{i, j} | i 6= j & D(Xi) \u2229 D(Xj) 6= \u2205}.\nWe first show that if there exists a matching of cardinality K, then there exists a solution with at least K equalities. Let M be a matching of cardinality K of GX , for each edge e = (i, j) \u2208 M we assign Xi and Xj to any value v \u2208 D(Xi) \u2229 D(Xj) (by construction, we know that there exists such a value). Observe that no variable is considered twice since it would mean that two edges of the matching have a common vertex. The obtained solution therefore has at least |M | equalities.\nNow we show that if there exists a solution S with K equalities, then there exists a matching of cardinality K. Let S be a solution, and let M = {{i, j} | S[Xi] = S[Xj ]}. Observe that M is a matching of GX . Indeed, suppose that two edges sharing a vertex (say {i, j}, {j, k}) are both in M . It follows that S[Xi] = S[Xj ] = S[Xk], however this is in contradiction with the hypothesis. We can therefore compute a solution S maximising the number of equalities by computing a maximal matching in GX . 2\nThis tractable class can be generalised by restricting the number of occurrences of values in the domains of variables. The notion of heavy values is key to this result.\nDefinition 12 (Heavy Value) A heavy value is a value that occurs more than twice in the domains of the variables of the problem.\nTheorem 9 (Tractable Class with Heavy Values) If the domain D(Xi) of each variable Xi contains at most one heavy value then finding an optimal satisfying assignment of SoftAllEqualminG is in P .\nProof. Consider a two stage algorithm. In the first stage, we explore every heavy value w and assign w to every variable whose domain contains it. Notice that no variable will be assigned twice. In the second stage, the CSP created by the domains of unassigned variables consists of only values having at most two occurrences, so we solve this CSP by transforming it to the matching problem as suggested in the proof of Theorem 8.\nWe show that there exists an optimal solution where each variable that can be assigned to a heavy value is assigned to this value. Let s\u2217 be an optimal solution and w be a heavy value over a set T of variables of cardinality t. We suppose that only z < t of them are assigned to w in s\u2217. Consider the solution s\u2032 obtained by assigning all these t variables to w: we add exactly t(t \u2212 1)/2 \u2212 z(z \u2212 1)/2 equalities. However, we potentially remove t\u2212z equalities since values other than w do not appear more than twice. We therefore have obj(s\u2032)\u2212obj(s\u2217) \u2265 t2\u22123t\u2212z2 +3z, which is non-negative for t \u2265 3 and z < t. By iteratively applying this transformation, we obtain an optimal solution where each variable that can be assigned to a heavy value is assigned to this value. The first stage of the algorithm is thus correct. The second stage is correct by Theorem 8. 2"}, {"heading": "9. Parameterised Complexity", "text": "We further advance our analysis of the complexity of the SoftAllEqualminG constraint by introducing a fixed-parameter tractable (FPT) algorithm with respect to the number of values. This result is important because it shows that the complexity of propagating this constraint grows only polynomially in the number of variables. It may therefore be possible to achieve ac at a reasonable computational cost even for a very large set of variables, provided that the total number of distinct values is relatively small.\nWe first show that the SoftAllEqualminG -optimisation problem is FPT with respect to the number of values \u03bb. We use the tractable class introduced in the previous section to generalise this result, showing that the problem is FPT with respect to the number of heavy values occurring in domains containing two or more heavy values. We begin with a definition.\nDefinition 13 (Solution from a Total Order) A solution s\u227a is induced by a total order \u227a over the values if and only if\ns[X] = v \u21d2 \u2200w \u227a v, w 6\u2208 D(X).\nWe now prove the following key lemma.\nLemma 1 There exists a total order \u227a over the set of values, such that the solution s\u227a induced by \u227a is optimal.\nProof. Let s\u2217 be an optimal solution, v be a value, and occ(s\u2217, v) be the number of variables assigned to v in s\u2217. Moreover, let \u227aocc be a total order such that values are ranked by decreasing number of occurrences (occ(s\u2217, v)) and ties are broken arbitrarily. We show that \u227aocc induces s\u2217.\nConsider, without loss of generality, a pair of values v, w such that v \u227aocc w. By definition we have occ(s\u2217, v) \u2265 occ(s\u2217, w). We suppose that the hypothesis is falsified and show that this leads to a contradiction. Suppose that there exists a variable X such that {v, w} \u2286 D(X) and s\u2217[X] = w (that is, \u227aocc does not induce s\u2217). The objective value of the solution s\u2032 such that s\u2032[X] = v and s\u2032[Y ] = s\u2217[Y ] \u2200y 6= x is given by: obj(s\u2032) = obj(s\u2217) + occ(s\u2217, v)\u2212 (occ(s\u2217, w)\u2212 1). Therefore, obj(s\u2032) > obj(s\u2217). However, s\u2217 is optimal, hence this is a contradiction. 2\nAn interesting consequence of Lemma 1 is that searching over the space of total orders on values is enough to compute an optimal solution. Moreover, the fixed-parameter tractability of the SoftAllEqualminG constraint follows easily from the same lemma.\nTheorem 10 (FPT \u2013 number of values) Finding an optimal satisfying assignment of the SoftAllEqualminG constraint is fixed-parameter tractable with respect to \u03bb, the number of values in the domains of the constrained variables.\nProof. Explore all possible \u03bb! permutations of values. For each permutation create a solution induced by this permutation. Compute the cost of this solution. Return the solution having the highest cost. According to Lemma 1, this solution is optimal. Creating an induced solution can be done by selecting for each domain the first value in the order. Clearly, this can be done in O(m). Computing the cost of the given solution can be done by computing the number of occurrences occ(w) and then summing up occ(w) \u2217 (occ(w)\u2212 1)/2 for all values w. Clearly, this can be done in O(m) as well. Hence the theorem follows. 2\nWe can also derive the following corollary from Lemma 1:\nCorollary 2 The number of optimal solutions of the CSP with the SoftAllEqualG is at most \u03bb!.\nProof. According to Lemma 1, each optimal solution is induced by an order over the values of the given problem. Clearly each order induces exactly one solution. Thus the number of optimal solution does not exceed the number of total orders which is at most \u03bb!. 2\nCorollary 2 shows that the number of optimal solutions of the considered problem does not depend on the number of variables and they all can be explored by considering all possible orders of values. We believe this fact is interesting from the practical point of view because in essence it means that even enumerating all optimal solutions is scalable with respect to the number of variables. Moreover, we can show that SoftAllEqualminG is fixed-parameter tractable with respect to the number of conflicting values, defined as follows.\nDefinition 14 (Conflicting Value) A value w of a given CSP is a conflicting value if and only if it is a heavy value and there is a domain D(X) that contains w and another heavy value.\nTheorem 11 (FPT \u2013 number of conflicting values) Let k be the number of conflicting values of a CSP comprising only one SoftAllEqualG constraint. Then the CSP can be solved in time O(k! \u221a n\u03bb), hence SoftAllEqualminG is fixed-parameter tractable with respect to k.\nProof. Consider all the permutations of the conflicting values. For each permutation perform the following two steps. In the first step for each variable X where there are two or more conflicting values, remove all the conflicting values except the one which is the first in the order among the conflicting values of D(X) according to the given permutation. In the second stage we obtain a problem where each domain contains exactly one heavy value. Solve this problem polynomially by the algorithm provided in the proof of Theorem 9.\nLet s be the solution obtained by this algorithm. We show that this solution is optimal. Let p\u2217 be a permutation of all the values of the considered CSP so that the solution s\u2217 induced by p\u2217 has the highest possible cost. By Lemma 1, s\u2217 is an optimal solution. Let p1 be the permutation of the conflicting values which is induced by p\u2217 and let s1 be the solution obtained by the algorithm above with respect to p1. By definition of s, obj(s) \u2265 obj(s1). We show that obj(s1) \u2265 obj(s\u2217) from which the optimality of s immediately follows.\nObserve that there is no X such that s\u2217[X] = w and w was removed from D(X) in the first stage of the above algorithm where the permutation p1 is considered. Indeed, w can only be removed from D(X) if it is preceded in p1 by a value v \u2208 D(X). It follows that w is also preceded in p\u2217 by v and consequently s\u2217(X) 6= w. Thus s\u2217 is a solution of the CSP obtained as a result of the first stage. However s1 is an optimal solution of that CSP by Theorem 9 and, consequently, obj(s1) \u2265 obj(s\u2217) as required.\nRegarding the runtime, observe that the execution of the algorithm consists of k! running an algorithm for finding the largest bipartite matching of the given graph. This graph has n vertices (corresponding to the variables). Moreover, each edge is associated with a value and no two edges are associated with the same value (because when the matching applies each value has at most two occurrences). It follows that the graph has at most \u03bb edges. According to Micali and Vazirani (1980), the largest matching can be found in O( \u221a n\u03bb), hence the upper bound. 2 This result shows that the complexity of propagating the SoftAllEqualminG constraint comes primarily from the number of (conflicting) values, whereas other factors, such as the number of variables, have little impact. Notice that detecting conflicting values can be done in linear time (O(m)), by first counting occurrences of every value, then flagging any value with at least two occurrences as \u201cheavy\u201d and finally flagging heavy values as \u201cconflicting\u201d in every domain containing at least two of them.\nObserve, moreover, the \u201cexponential\u201d part of this algorithm is based on the exploration of all possible orders over the given set of conflicting values. In fact the ordering relation between two values matters only if these values belong to a domain of the same variable. In other words consider a graph H on values of the given CSP instance. Two values a and b are connected by an edge if and only if they belong to the domain of the same variable. Instead of considering all possible orders over the given set of values we may consider all possible ways of transforming the given graph into an acyclic digraph. The upper bound on the number of possible transformations is 2E(H) where E(H) is the number of edges of\nH. For sparse graphs such a bound is much more optimistic that k!. For example, if the average degree of a vertex is 4 then the number of considered partial orders is 22k = 4k."}, {"heading": "10. Finding a Set of Similar or Diverse Solutions", "text": "Problems of similarity and diversity have a wide range of applications. Finding several diverse solutions can be used to sample the solution space, for instance for product recommendation (Shimazu, 2001), case-based reasoning (Smyth & McClave, 2001; Aha & Watson, 2001) or constraint elicitation (Bessie\u0300re, Coletta, Koriche, & O\u2019Sullivan, 2005; Gama, Camacho, Brazdil, Jorge, & Torgo, 2005).\nConversely, similarity is important for problems with a periodic aspect. For instance, a schedule or timetable may need to be computed on a weekly basis, but the constraints might change slightly from week to week. In this type of problems the regularity of the solutions, that is, the similarity between each week\u2019s solution, is a very valuable property (Groe\u0308r, Golden, & Wasil, 2009).\nFinally, finding similar solutions to a set of variants of a problem can be useful to find solutions that are robust to uncertainty. Suppose, for example, that we are to solve a Travelling Salesman Problem (TSP), however, the costs associated with a set of k \u2212 1 links between pairs of cities are uncertain or variable over time. We would like to find an optimal, or near-optimal, route such that when the cost of traversing a link changes, a limited amount of re-routing is sufficient to obtain another near-optimal solution. For that purpose, one can build a similar structure as that pictured in Figure 6 by duplicating the TSP once per uncertain link, the last being the original formulation. In each duplicate, the cost of the corresponding link is then set to some expected upper bound. If we minimise the distance between solutions, we obtain a solution with good properties of robustness: if the cost associated with the ith link increases, the solution of the ith duplicate is a valid alternative avoiding this link (if it degrades the solution quality too much) whilst requiring a small amount of re-routing.\nWe therefore want to find a set of k solutions \u2014 either pairwise similar or different \u2014 to a set of k problems, distinct or not. A heuristic method was introduced to solve the problem of finding k solutions of a constraint network, such that the minimum (resp. maximum) distance between all pairs of solutions is maximum (resp. minimum) by Hebrard, Hnich, O\u2019Sullivan, and Walsh (2005). Since reasoning on the maximum minimum distance is NPhard (Frances & Litman, 1997), it was proposed to use the sum of the Hamming distances instead. In this section, we first formally define the notion of Hamming distance between variables and between solutions. Next, we show that the constraints studied in this paper can help achieve ac and rc in polynomial time for respectively maximising and minimising the sum of pairwise distances between solutions to a set of problem instances."}, {"heading": "10.1 Hamming Distance:", "text": "The Hamming distance between the instantiation of two variables X and Y is defined as follows:\n\u2206h(X,Y ) = { 1 iff X 6= Y 0 otherwise\nWhereas the Hamming distance between two solutions si and sj (over the sets of variables {Xi1, . . . , Xin} and {X j 1 , . . . , X j n}, respectively) is defined as:\n\u2206h(si, sj) = \u2211\n1\u2264`\u2264n \u2206h(X\ni `, X j ` )\nGiven a problem P with n variables {X1, . . . , Xn}, we duplicate P k times, with identical constraints if we seek a set of diverse solutions to P , or altered constraints to model the expected scenarios if we seek for a set of similar solutions for some variations of P (see Figure 6).\nThen the objective to maximise or minimise is the sum of the pairwise distances between the (sub-)solutions of the duplicated problems:\u2211\n1\u2264i<j\u2264k \u2206h(si, sj) (4)"}, {"heading": "10.2 Constraint Formulation:", "text": "The first approaches to this problem relied on heuristic methods (Hebrard et al., 2005; Hentenryck, Coffrin, & Gutkovich, 2009), It was also shown that when the problem P allows it, knowledge compilation methods could efficiently solve this problem (Hadzic, Holland, & O\u2019Sullivan, 2009).\nHere we show that one can achieve arc or bound consistency for maximising this objective function. Whilst arc consistency is NP-hard for minimisation, bounds consistency can be achieved in polynomial time both for minimisation and maximisation. First, we decompose the objective function described previously (Equation 4) using the SoftAllEqualminG or SoftAllEqualmaxG constraints for optimising, respectively, solution similarity or diversity. Then we shall see that achieving ac (resp. bc) on this decomposition is equivalent to achieving ac (resp. bc) on the global constraint defined by bounding the objective.\nRemember that each row in Figure 6 represents a duplicate of the original set of variables {X1, . . . , Xn}. The objective function is defined as the sum of the Hamming distances between every pair of rows. However, consider now Figure 6 by vertical slices. Each column corresponds to the set of duplicates {Xji | 1 \u2264 j \u2264 k} of an original original variable Xi. One can compute the contribution of this set of variables to the sum of Hamming distances between pairs of rows as the number of pairwise disequalities in the set: |{{j, k} | Xji 6= Xki & j < k}|. Notice that this is precisely the definition of the cost to minimise (resp. maximise) in SoftAllEqualminG (resp. SoftAllEqual max G ).\nTherefore we can model the objective function as the constraint networks shown in Figure 7, respectively for minimisation and maximisation. Notice that, to simplify the model, we use the following, equivalent formulation for SoftAllEqualmaxG , rather than Definition 3:\nSecond, notice that the constraint networks depicted in Figure 7 are such that no two constraints share more than one variable, and there is no Berge-cycle (Berge, 1970) in the constraint hypergraph, that is, a sequence C1, X1, C2, . . . , Xk, Ck+1 such that:\n\u2022 X1, . . . , Xk are distinct variables,\n\u2022 C1, . . . , Ck+1 are distinct constraints,\n\u2022 k \u2265 2 and C1 = Ck+1,\n\u2022 Xi is in the scope of Ci and Ci+1.\nIndeed, the SoftAllEqual constraints do not share any variable, and the overlap with the sum constraint is limited to a single variable with each SoftAllEqual. The constraint hypergraph is therefore Berge-acyclic, and in such constraint networks it was shown that propagating ac is sufficient to filter all globally inconsistent values (Janssen & Vilarem, 1988; Je\u0301gou, 1991).\nTherefore, when every constraint in this network is ac (resp. rc), the network is globally arc consistent (resp. globally range consistent). We can view these two constraint networks as two global constraints, respectively CN div and CN sim, over the set variables {Xji | 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 k} and a variable N to represent the objective:\nCN div({Xji | 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 k}, N)\u21d4 N \u2264 \u2211\n1\u2264i\u2264n Ni & \u22001 \u2264 i \u2264 n SoftAllEqualmaxG (X1i , . . . , Xki , Ni)\nCN sim({Xji | 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 k}, N)\u21d4 N \u2265 \u2211\n1\u2264i\u2264n Ni & \u22001 \u2264 i \u2264 n SoftAllEqualminG (X1i , . . . , Xki , Ni)\nTheorems 12 and 13 (where m = \u2211\n1\u2264i\u2264n |D(X1i )| denotes the sum of the domain sizes in one copy of the problem) follow from, respectively, (van Hoeve, 2004) and Theorem 5:\nTheorem 12 Enforcing ac on CN div({Xji | 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 k}, N) can be achieved in O(k2m) steps.\nProof. Since the constraint network equivalent to CN div is Berge-acyclic, we know that it is ac iff every constraint in the decomposition is ac. Moreover, we describe a filtering algorithm that requires only a bounded number of calls to the propagator of each constraint in the decomposition.\nWe assume that no variable\u2019s domain is completely wiped out during the process. If it was the case, the process would be interrupted earlier (as soon as an inconsistency is detected while achieving ac on a component).\nWe introduce some terminology:\n\u2022 property (1) denotes the fact that for all 1 \u2264 i \u2264 n the domains of the variables in Xji are consistent with the upper bounds of Ni,\n\u2022 property (2) denotes the fact that for all 1 \u2264 i \u2264 n the domains of the variables in Xji are consistent with the lower bounds of Ni,\n\u2022 property (3) denotes the fact that the sum constraint (N \u2264 \u2211\n1\u2264i\u2264nNi) is bc (or equivalently ac).\nFirst, the domain of some variable Xji for 1 \u2264 i \u2264 n and 1 \u2264 j \u2264 k might have changed, as well as the lower bound of N . A change on the upper bound of N either results on an immediate failure, or bears no consequences.\n1. For every i \u2208 [1..n], we update the upper bound of the variable Ni by calling the procedure proposed by van Hoeve (2004) to find the maximum possible number of disequalities. Hence property (1) holds.\n2. We achieve bc (equivalent to ac in this case) on the sum constraint. Notice that only the upper bound of N and the lower bounds of Ni for some 1 \u2264 i \u2264 n will be updated, therefore property (1) and (3) hold.\n3. For every i \u2208 [1..n], we prune the domains of the variables in {Xji | 1 \u2264 j \u2264 k} by calling the filtering procedure proposed by van Hoeve (2004). Since this domain reduction will not trigger any further changes in the bounds of Ni, we know property (1), (2) and (3) hold, hence CN div is ac.\nThe first phase requires O( \u2211\n1\u2264i\u2264n k 2|D(X1i )|), that is O(k2m) steps. The second phase\nrequires O(n) steps. Finally, the third phase, like the first, requires O(k2m) steps. Hence an overall O(k2m) time complexity.\nTheorem 13 Enforcing rc on CN sim({Xji | 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 k}, N) can be achieved in O(k4m) steps.\nProof. This proof is very similar to that of Theorem 12, if we swap upper and lower bounds, and if we use the procedure described in Section 6 for phase (1) and (3).\nThe first phase requires O(k3n) steps. The second phase requires O(n) steps. Finally, the third phase requires O( \u2211 1\u2264i\u2264n k\n4|D(X1i )|), that is O(k4m) steps. Hence an overall O(k4m) time complexity."}, {"heading": "11. Conclusion", "text": "In many applications we are concerned with stating constraints on the similarity and diversity amongst assignments to variables. To formulate such problems we can use soft variants of the well known AllDifferent and AllEqual constraints. In this paper we considered the global constraints AllDifferent and AllEqual, and their optimisation variants, SoftAllDiff and SoftAllEqual, respectively. Furthermore, we considered two cost functions, based either on the Hamming distance to a satisfying assignment or on the number of violations on the decomposition graph. We have shown that the constraint ensuring an upper bound on the Hamming distance with a solution satisfying the AllEqual constraint can be propagated efficiently, both for arc and bounds consistency. Then we have shown that, on the one hand, deciding the existence of an assignment minimising the number of violation in the decomposition graph of the AllEqual constraint is NP-complete, hence propagating arc consistency on the constraint ensuring this property is NP-hard. On the other hand, propagating bounds consistency on the same constraint can be done in polynomial time. Moreover, we have shown that this problem is fixed parameter tractable in the number of distinct values of the problem. This work complements nicely some earlier results of Cohen et al. (2004) showing that the language of soft binary equality constraints was NP-complete, for as few as three distinct values in the domains. In this paper we have shown that the problem remains NP-complete even if the graph of soft binary equality constraints forms a clique, however, becomes polynomial if the number of values is bounded.\nThis paper therefore provides a comprehensive complexity analysis of achieving ac and bc on an important class of soft constraints of difference and equality. Interestingly, this taxonomy shows that enforcing equality is harder than enforcing difference."}, {"heading": "Acknowledgments", "text": "Hebrard, O\u2019Sullivan and Razgon are supported by Science Foundation Ireland (Grant Number 05/IN/I886). Marx is supported in part by the ERC Advanced grant DMMCA, the Alexander von Humboldt Foundation, and the Hungarian National Research Fund (Grant Number OTKA 67651)."}], "references": [{"title": "Case-Based Reasoning Research and Development, 4th International Conference on Case-Based Reasoning", "author": ["D.W. Aha", "I. Watson"], "venue": "ICCBR", "citeRegEx": "Aha and Watson,? \\Q2001\\E", "shortCiteRegEx": "Aha and Watson", "year": 2001}, {"title": "Sweep as a Generic Pruning Technique Applied to the Non-Overlapping Rectangles Constraint", "author": ["N. Beldiceanu", "M. Carlsson"], "venue": "Proceedings of the 7th International Conference on Principles and Practice of Constraint Programming (CP01),", "citeRegEx": "Beldiceanu and Carlsson,? \\Q2001\\E", "shortCiteRegEx": "Beldiceanu and Carlsson", "year": 2001}, {"title": "Cost evaluation of soft global constraints", "author": ["N. Beldiceanu", "T. Petit"], "venue": "Proceedings of the 6th International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems (CPAIOR-04),", "citeRegEx": "Beldiceanu and Petit,? \\Q2004\\E", "shortCiteRegEx": "Beldiceanu and Petit", "year": 2004}, {"title": "Pruning for the Minimum Constraint Family and for the Number of Distinct Values Constraint Family", "author": ["N. Beldiceanu"], "venue": "Proceedings of the 7th International Conference on Principles and Practice of Constraint Programming (CP01),", "citeRegEx": "Beldiceanu,? \\Q2001\\E", "shortCiteRegEx": "Beldiceanu", "year": 2001}, {"title": "Graphs and Hypergraphs. Dunod", "author": ["C. Berge"], "venue": null, "citeRegEx": "Berge,? \\Q1970\\E", "shortCiteRegEx": "Berge", "year": 1970}, {"title": "A SAT-Based Version Space Algorithm for Acquiring Constraint Satisfaction Problems", "author": ["C. Bessi\u00e8re", "R. Coletta", "F. Koriche", "B. O\u2019Sullivan"], "venue": "In Gama et al. (Gama et al.,", "citeRegEx": "Bessi\u00e8re et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bessi\u00e8re et al\\.", "year": 2005}, {"title": "Filtering algorithms for the nvalue", "author": ["C. Bessiere", "E. Hebrard", "B. Hnich", "Z. Kiziltan", "T. Walsh"], "venue": "constraint. Constraints,", "citeRegEx": "Bessiere et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bessiere et al\\.", "year": 2006}, {"title": "A maximal tractable class of soft constraints", "author": ["D. Cohen", "M. Cooper", "P. Jeavons", "A. Krokhin"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2004}, {"title": "Computers and Intractability: A Guide to the Theory of NP-completeness. W.H", "author": ["M.R. Garey", "D.S. Johnson"], "venue": null, "citeRegEx": "Garey and Johnson,? \\Q1979\\E", "shortCiteRegEx": "Garey and Johnson", "year": 1979}, {"title": "The Consistent Vehicle Routing Problem", "author": ["C. Gro\u00ebr", "B. Golden", "E. Wasil"], "venue": "Manufacturing & Service Operations Management,", "citeRegEx": "Gro\u00ebr et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gro\u00ebr et al\\.", "year": 2009}, {"title": "Reasoning about Optimal Collections of Solutions", "author": ["T. Hadzic", "A. Holland", "B. O\u2019Sullivan"], "venue": "Proceedings of the 15th International Conference on Principles and Practice of Constraint Programming (CP-09),", "citeRegEx": "Hadzic et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hadzic et al\\.", "year": 2009}, {"title": "Finding Diverse and Similar Solutions in Constraint Programming", "author": ["E. Hebrard", "B. Hnich", "B. O\u2019Sullivan", "T. Walsh"], "venue": "Proceedings of the 20th National Conference on Artificial Intelligence and the Seventeenth Conference on Innovative Applications of Artificial Intelligence (AAAI-05 / IAAI-05),", "citeRegEx": "Hebrard et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hebrard et al\\.", "year": 2005}, {"title": "Constraints of Difference and Equality: A Complete Taxonomic Characterisation", "author": ["E. Hebrard", "D. Marx", "B. O\u2019Sullivan", "I. Razgon"], "venue": "Proceedings of the 15th International Conference on Principles and Practice of Constraint Programming", "citeRegEx": "Hebrard et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hebrard et al\\.", "year": 2009}, {"title": "A Soft Constraint of Equality: Complexity and approximability", "author": ["E. Hebrard", "B. O\u2019Sullivan", "I. Razgon"], "venue": "Proceedings of the 14th International Conference on Principles and Practice of Constraint Programming (CP-08),", "citeRegEx": "Hebrard et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hebrard et al\\.", "year": 2008}, {"title": "Constraint-based local search for the automatic generation of architectural tests", "author": ["P.V. Hentenryck", "C. Coffrin", "B. Gutkovich"], "venue": "Proceedings of the 15th International Conference on Principles and Practice of Constraint Programming (CP09),", "citeRegEx": "Hentenryck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hentenryck et al\\.", "year": 2009}, {"title": "Probl\u00e8mes de satisfaction de contraintes: techniques de r\u00e9solution et application \u00e0 la synth\u00e8se de peptides", "author": ["P. Janssen", "Vilarem", "M.-C"], "venue": "C.R.I.M. Research Report.", "citeRegEx": "Janssen et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Janssen et al\\.", "year": 1988}, {"title": "Contribution \u00e0 l\u00e9tude des probl\u00e8mes de satisfaction de contraintes: algorithmes de propagation et de r\u00e9solution", "author": ["P. J\u00e9gou"], "venue": "Propagation de contraintes dans les re\u0301seaux dynamiques. Ph.D. thesis", "citeRegEx": "J\u00e9gou,? \\Q1991\\E", "shortCiteRegEx": "J\u00e9gou", "year": 1991}, {"title": "Faster Algorithms for Bound-Consistency of the Sortedness and the Alldifferent Constraint", "author": ["K. Mehlhorn", "S. Thiel"], "venue": "Proceedings of the 6th International Conference on Principles and Practice of Constraint Programming (CP-00),", "citeRegEx": "Mehlhorn and Thiel,? \\Q2000\\E", "shortCiteRegEx": "Mehlhorn and Thiel", "year": 2000}, {"title": "Invitation to Fixed-Parameter Algorithms", "author": ["R. Niedermeier"], "venue": null, "citeRegEx": "Niedermeier,? \\Q2006\\E", "shortCiteRegEx": "Niedermeier", "year": 2006}, {"title": "Meta-constraints on Violations for over Constrained Problems", "author": ["T. Petit", "R\u00e9gin", "J.-C", "C. Bessiere"], "venue": "In 12th IEEE International Conference on Tools with Artificial Intelligence", "citeRegEx": "Petit et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Petit et al\\.", "year": 2000}, {"title": "Specific Filtering Algorithms for OverConstrained Problems", "author": ["T. Petit", "R\u00e9gin", "J.-C", "C. Bessiere"], "venue": "Proceedings of the 7th International Conference on Principles and Practice of Constraint Programming (CP-01),", "citeRegEx": "Petit et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Petit et al\\.", "year": 2001}, {"title": "Range-Based Algorithm for Max-CSP", "author": ["T. Petit", "R\u00e9gin", "J.-C", "C. Bessiere"], "venue": "Proceedings of the 8th International Conference on Principles and Practice of Constraint Programming (CP-02),", "citeRegEx": "Petit et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Petit et al\\.", "year": 2002}, {"title": "Improved Algorithms for the Global Cardinality Constraint", "author": ["Quimper", "C.-G", "A. Lopez-Ortiz", "P. van Beek", "A. Golynski"], "venue": "Proceedings of the 10th International Conference on Principles and Practice of Constraint Programming (CP-", "citeRegEx": "Quimper et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quimper et al\\.", "year": 2004}, {"title": "A Filtering Algorithm for Constraints of Difference in CSPs", "author": ["R\u00e9gin", "J.-C"], "venue": "Proceedings of the 12th National Conference on Artificial Intelligence", "citeRegEx": "R\u00e9gin and J..C.,? \\Q1994\\E", "shortCiteRegEx": "R\u00e9gin and J..C.", "year": 1994}, {"title": "Expertclerk: Navigating Shoppers Buying Process with the Combination of Asking and Proposing", "author": ["H. Shimazu"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence", "citeRegEx": "Shimazu,? \\Q2001\\E", "shortCiteRegEx": "Shimazu", "year": 2001}, {"title": "Similarity vs. diversity", "author": ["B. Smyth", "P. McClave"], "venue": null, "citeRegEx": "Smyth and McClave,? \\Q2001\\E", "shortCiteRegEx": "Smyth and McClave", "year": 2001}, {"title": "A hyper-arc consistency algorithm for the soft alldifferent constraint", "author": ["van Hoeve", "W.-J"], "venue": "Proceedings of the 10th International Conference on Principles and Practice of Constraint Programming (CP-04),", "citeRegEx": "Hoeve and W..J.,? \\Q2004\\E", "shortCiteRegEx": "Hoeve and W..J.", "year": 2004}, {"title": "On Global Warming: Flow-Based Soft Global Constraints", "author": ["van Hoeve", "W.-J", "G. Pesant", "Rousseau", "L.-M"], "venue": "Journal of Heuristics,", "citeRegEx": "Hoeve et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hoeve et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "When we wish to enforce that a set of variables take equal values, we can use the AllEqual, or its soft variant for the graph-based cost, the SoftAllEqual constraint (Hebrard, O\u2019Sullivan, & Razgon, 2008), or its soft variant for the variable-based cost, the AtMostNValue constraint (Beldiceanu, 2001).", "startOffset": 282, "endOffset": 300}, {"referenceID": 20, "context": "Three of these problems were studied in the past: minimising the cost of SoftAllDiff variable (Petit et al., 2001) and graph-based cost (van Hoeve, 2004) is polynomial whilst maximising the variable-based cost of SoftAllDiff is NP-hard (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) for ac and polynomial (Beldiceanu, 2001) for bc.", "startOffset": 94, "endOffset": 114}, {"referenceID": 3, "context": ", 2001) and graph-based cost (van Hoeve, 2004) is polynomial whilst maximising the variable-based cost of SoftAllDiff is NP-hard (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) for ac and polynomial (Beldiceanu, 2001) for bc.", "startOffset": 203, "endOffset": 221}, {"referenceID": 13, "context": "Part of the material presented in this paper is based on two conference publications (Hebrard et al., 2008; Hebrard, Marx, O\u2019Sullivan, & Razgon, 2009).", "startOffset": 85, "endOffset": 150}, {"referenceID": 18, "context": "We refer the reader to Niedermeier\u2019s (2006) book for a comprehensive introduction.", "startOffset": 23, "endOffset": 44}, {"referenceID": 19, "context": "This objective function was first studied by Petit et al. (2001), and an algorithm for achieving ac in O(n \u221a m) was introduced.", "startOffset": 45, "endOffset": 65}, {"referenceID": 20, "context": "References: [1] (Petit et al., 2001), [2] (Bessiere et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": ", 2001), [2] (Bessiere et al., 2006), [3] (Beldiceanu, 2001), [4] (van Hoeve, 2004), [5] (Hebrard et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 3, "context": ", 2006), [3] (Beldiceanu, 2001), [4] (van Hoeve, 2004), [5] (Hebrard et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 13, "context": ", 2006), [3] (Beldiceanu, 2001), [4] (van Hoeve, 2004), [5] (Hebrard et al., 2008), [6] (Hebrard et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 12, "context": ", 2008), [6] (Hebrard et al., 2009), [7] (present paper), [8] (Quimper et al.", "startOffset": 13, "endOffset": 35}, {"referenceID": 22, "context": ", 2009), [7] (present paper), [8] (Quimper et al., 2004).", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": "Notice however that Mehlhorn and Thiel\u2019s (2000) algorithm achieves bc on the AllDifferent constraint with an O(n log n) time complexity.", "startOffset": 20, "endOffset": 48}, {"referenceID": 3, "context": "An algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that achieving ac is NP-hard was given by Bessiere et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 3, "context": "An algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that achieving ac is NP-hard was given by Bessiere et al. (2006).", "startOffset": 57, "endOffset": 153}, {"referenceID": 22, "context": "Two algorithms, for achieving ac and bc on this constraint and running in O( \u221a nm) and O(n log n) respectively, was introduced by Quimper et al. (2004).", "startOffset": 130, "endOffset": 152}, {"referenceID": 3, "context": "The procedure is closely related to the concept of sweep algorithms (Beldiceanu & Carlsson, 2001) used, for instance, to implement filtering algorithms for the Cumulative constraint. Instead of scanning the entire horizon, one can jump from an event to the next, assuming that nothing changes between two events. As in the case of the Cumulative constraint, events here correspond to start and end points of the domains. In fact, it is possible to compute the same lower bound, with the same complexity, by using Petit, R\u00e9gin, and Bessiere\u2019s (2002) Range-based Max-CSP Algorithm (RMA)2 on a reformulation as a Max-CSP.", "startOffset": 69, "endOffset": 549}, {"referenceID": 24, "context": "Finding several diverse solutions can be used to sample the solution space, for instance for product recommendation (Shimazu, 2001), case-based reasoning (Smyth & McClave, 2001; Aha & Watson, 2001) or constraint elicitation (Bessi\u00e8re, Coletta, Koriche, & O\u2019Sullivan, 2005; Gama, Camacho, Brazdil, Jorge, & Torgo, 2005).", "startOffset": 116, "endOffset": 131}, {"referenceID": 24, "context": "Finding several diverse solutions can be used to sample the solution space, for instance for product recommendation (Shimazu, 2001), case-based reasoning (Smyth & McClave, 2001; Aha & Watson, 2001) or constraint elicitation (Bessi\u00e8re, Coletta, Koriche, & O\u2019Sullivan, 2005; Gama, Camacho, Brazdil, Jorge, & Torgo, 2005). Conversely, similarity is important for problems with a periodic aspect. For instance, a schedule or timetable may need to be computed on a weekly basis, but the constraints might change slightly from week to week. In this type of problems the regularity of the solutions, that is, the similarity between each week\u2019s solution, is a very valuable property (Gro\u00ebr, Golden, & Wasil, 2009). Finally, finding similar solutions to a set of variants of a problem can be useful to find solutions that are robust to uncertainty. Suppose, for example, that we are to solve a Travelling Salesman Problem (TSP), however, the costs associated with a set of k \u2212 1 links between pairs of cities are uncertain or variable over time. We would like to find an optimal, or near-optimal, route such that when the cost of traversing a link changes, a limited amount of re-routing is sufficient to obtain another near-optimal solution. For that purpose, one can build a similar structure as that pictured in Figure 6 by duplicating the TSP once per uncertain link, the last being the original formulation. In each duplicate, the cost of the corresponding link is then set to some expected upper bound. If we minimise the distance between solutions, we obtain a solution with good properties of robustness: if the cost associated with the ith link increases, the solution of the ith duplicate is a valid alternative avoiding this link (if it degrades the solution quality too much) whilst requiring a small amount of re-routing. We therefore want to find a set of k solutions \u2014 either pairwise similar or different \u2014 to a set of k problems, distinct or not. A heuristic method was introduced to solve the problem of finding k solutions of a constraint network, such that the minimum (resp. maximum) distance between all pairs of solutions is maximum (resp. minimum) by Hebrard, Hnich, O\u2019Sullivan, and Walsh (2005). Since reasoning on the maximum minimum distance is NPhard (Frances & Litman, 1997), it was proposed to use the sum of the Hamming distances instead.", "startOffset": 117, "endOffset": 2211}, {"referenceID": 11, "context": "The first approaches to this problem relied on heuristic methods (Hebrard et al., 2005; Hentenryck, Coffrin, & Gutkovich, 2009), It was also shown that when the problem P allows it, knowledge compilation methods could efficiently solve this problem (Hadzic, Holland, & O\u2019Sullivan, 2009).", "startOffset": 65, "endOffset": 127}, {"referenceID": 4, "context": "Second, notice that the constraint networks depicted in Figure 7 are such that no two constraints share more than one variable, and there is no Berge-cycle (Berge, 1970) in the constraint hypergraph, that is, a sequence C1, X1, C2, .", "startOffset": 156, "endOffset": 169}, {"referenceID": 16, "context": "The constraint hypergraph is therefore Berge-acyclic, and in such constraint networks it was shown that propagating ac is sufficient to filter all globally inconsistent values (Janssen & Vilarem, 1988; J\u00e9gou, 1991).", "startOffset": 176, "endOffset": 214}, {"referenceID": 7, "context": "This work complements nicely some earlier results of Cohen et al. (2004) showing that the language of soft binary equality constraints was NP-complete, for as few as three distinct values in the domains.", "startOffset": 53, "endOffset": 73}], "year": 2011, "abstractText": "In many combinatorial problems one may need to model the diversity or similarity of sets of assignments. For example, one may wish to maximise or minimise the number of distinct values in a solution. To formulate problems of this type we can use soft variants of the well known AllDifferent and AllEqual constraints. We present a taxonomy of six soft global constraints, generated by combining the two latter ones and the two standard cost functions, which are either maximised or minimised. We characterise the complexity of achieving arc and bounds consistency on these constraints, resolving those cases for which NP-hardness was neither proven nor disproven. In particular, we explore in depth the constraint ensuring that at least k pairs of variables have a common value. We show that achieving arc consistency is NP-hard, however bounds consistency can be achieved in polynomial time through dynamic programming. Moreover, we show that the maximum number of pairs of equal variables can be approximated by a factor of 12 with a linear time greedy algorithm. Finally, we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains. Interestingly, this taxonomy shows that enforcing equality is harder than enforcing difference.", "creator": "TeX"}}}