{"id": "1401.3886", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference", "abstract": "Previous studies have prueba demonstrated that lautrec encoding a Bayesian muslin network sethe into a gjds.com SAT formula well-protected and then degryse performing weighted joliette model counting acheulean using camisole a backtracking 17.62 search danto algorithm antibodies can be herstmonceux an shivaram effective ausiello method for yorck exact annum inference. In mal\u00ebsi this chomedey paper, we amikam present techniques dembowski for otomis improving ungraded this corruptions approach for Bayesian networks undivided with noisy - servat OR invasions and noisy - ausa MAX kojonup relations - - - two mladi relations that stik are 72.97 widely oester used in practice salzkammergut as they 1.120 can loblaws dramatically reduce mogudu the 38-9 number of 88.43 probabilities arenberg one needs to kilbey specify. loathe In ockenga particular, ariwibowo we fidy present two SAT tarchi encodings out-of-body for miyavi noisy - OR formula and jinggang two waa encodings for noisy - 5.34 MAX that exploit shirting the mseleku structure hugh or penalty semantics of the relations compiegne to improve both bigg time garter and space efficiency, and jairus we j\u00f8rgen prove the correctness basevi of the encodings. tryin We experimentally evaluated surgutneftegaz our petrick techniques biogenetic on large - scale pipp real and randomly intermingled generated 372.5 Bayesian networks. 51.54 On caudillo these benchmarks, bushati our techniques gave schagen speedups scholia of liopleurodon up sarsa to two zalkind orders 5,057 of 32.78 magnitude over laurine the best previous 6,055 approaches demchugdongrub for 68.22 networks gizella with noisy - OR / youyu MAX relations -2 and leitz scaled 46-28 up to larger networks. velopark As 4.8-billion well, our moonshine techniques huanggang extend the unthank weighted nightstands model lieutenant-governor counting approach vlj for exact inference to networks fajer that hirofumi were previously intractable for wholeheartedly the isoleucine approach.", "histories": [["v1", "Thu, 16 Jan 2014 05:15:08 GMT  (291kb)", "http://arxiv.org/abs/1401.3886v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["wei li", "pascal poupart", "peter van beek"], "accepted": false, "id": "1401.3886"}, "pdf": {"name": "1401.3886.pdf", "metadata": {"source": "CRF", "title": "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference", "authors": ["Wei Li", "Pascal Poupart", "Peter van Beek"], "emails": ["wei.li@autodesk.com", "ppoupart@cs.uwaterloo.ca", "vanbeek@cs.uwaterloo.ca"], "sections": [{"heading": "1. Introduction", "text": "Bayesian networks are a fundamental building block of many AI applications. A Bayesian network consists of a directed acyclic graph where the nodes represent the random variables and each node is labeled with a conditional probability table (CPT) that represents the strengths of the influences of the parent nodes on the child node (Pearl, 1988). In general, assuming random variables with domain size d, the CPT of a child node with n parents requires one to specify dn+1 probabilities. This presents a practical difficulty and has led to the introduction of patterns for CPTs that require one to specify many fewer parameters (e.g., Good, 1961; Pearl, 1988; D\u0301\u0131ez & Druzdzel, 2006).\nPerhaps the most widely used patterns in practice are the noisy-OR relation and its generalization, the noisy-MAX relation (Good, 1961; Pearl, 1988). These relations assume a form of causal independence and allow one to specify a CPT with just n parameters in the case of the noisy-OR and (d\u22121)2n parameters in the case of the noisy-MAX, where n is the number of parents of the node and d is the size of the domains of the random variables. The noisy-OR/MAX relations have been successfully applied in the knowledge engineering of\nc\u00a92011 AI Access Foundation. All rights reserved.\nlarge real-world Bayesian networks, such as the Quick Medical Reference-Decision Theoretic (QMR-DT) project (Miller, Masarie, & Myers, 1986) and the Computer-based Patient Case Simulation system (Parker & Miller, 1987). As well, Zagorecki and Druzdzel (1992) show that in three real-world Bayesian networks, noisy-OR/MAX relations were a good fit for up to 50% of the CPTs in these networks and that converting some CPTs to noisy-OR/MAX relations gave good approximations when answering probabilistic queries. This is surprising, as the CPTs in these networks were not specified using the noisy-OR/MAX assumptions and were specified as full CPTs. Their results provide additional evidence for the usefulness of noisy-OR/MAX relations.\nWe consider here the problem of exact inference in Bayesian networks that contain noisy-OR/MAX relations. One method for solving such networks is to replace each noisyOR/MAX by its full CPT representation and then use any of the well-known algorithms for answering probabilistic queries such as variable elimination or tree clustering/jointree. However, in general\u2014and in particular, for the networks that we use in our experimental evaluation\u2014this method is impractical. A more fruitful approach for solving such networks is to take advantage of the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen, Kjaerulff, Jensen, Jensen, Falck, Andreassen, & Andersen, 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gala\u0301n, 2003; Chavira, Allen, & Darwiche, 2005).\nPrevious studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a DPLL-based algorithm can be an effective method for exact inference, where DPLL is a backtracking algorithm specialized for SAT that includes unit propagation, conflict recording, backjumping, and component caching (Sang, Beame, & Kautz, 2005a). In this paper, we present techniques for improving this weighted model counting approach for Bayesian networks with noisy-OR and noisy-MAX relations. In particular, we present two CNF encodings for noisy-OR and two CNF encodings for noisy-MAX that exploit their semantics to improve both the time and space efficiency of probabilistic inference. In our encodings, we pay particular attention to reducing the treewidth of the CNF formula. We also explore alternative search ordering heuristics for the DPLL-based backtracking algorithm.\nWe experimentally evaluated our encodings on large-scale real and randomly generated Bayesian networks using the Cachet weighted model counting solver (Sang, Bacchus, Beame, Kautz, & Pitassi, 2004). While our experimental results must be interpreted with some care as we are comparing not only our encodings but also implementations of systems with conflicting design goals, on these benchmarks our techniques gave speedups of up to three orders of magnitude over the best previous approaches for networks with noisy-OR and noisy-MAX. As well, on these benchmarks there were many networks that could not be solved at all by previous approaches within resource limits, but could be solved quite quickly by Cachet using our encodings. Thus, our noisy-OR and noisy-MAX encodings extend the model counting approach for exact inference to networks that were previously intractable for the approach."}, {"heading": "2. Background", "text": "In this section, we review noisy-OR/MAX relations and the needed background on weighted model counting approaches to exact inference in Bayesian networks (for more on these topics see, for example, Koller & Friedman, 2009; Darwiche, 2009; Chavira & Darwiche, 2008)."}, {"heading": "2.1 Patterns for CPTs: Noisy-OR and Noisy-MAX", "text": "With the noisy-OR relation one assumes that there are different causes X1, . . . , Xn leading to an effect Y (see Figure 1), where all random variables are assumed to have Booleanvalued domains. Each cause Xi is either present or absent, and each Xi in isolation is likely to cause Y and the likelihood is not diminished if more than one cause is present. Further, one assumes that all possible causes are given and when all causes are absent, the effect is absent. Finally, one assumes that the mechanism or reason that inhibits a Xi from causing Y is independent of the mechanism or reason that inhibits a Xj , j = i, from causing Y .\nA noisy-OR relation specifies a CPT using n parameters, q1, . . . , qn, one for each parent, where qi is the probability that Y is false given that Xi is true and all of the other parents are false, P (Y = 0 | Xi = 1,Xj = 0[\u2200j,j =i]) = qi. (1) From these parameters, the full CPT representation of size 2n+1 can be generated using,\nP (Y = 0 | X1, . . . ,Xn) = \u220f i\u2208Tx qi (2)\nand P (Y = 1 | X1, . . . ,Xn) = 1\u2212 \u220f i\u2208Tx qi (3) where Tx = {i | Xi = 1} and P (Y = 0 | X1, . . . ,Xn) = 1 if Tx is empty. The last condition (when Tx is empty) corresponds to the assumptions that all possible causes are given and that when all causes are absent, the effect is absent; i.e., P (Y = 0 | X1 = 0, . . . ,Xn = 0) = 1. These assumptions are not as restrictive as may first appear. One can always introduce an\nadditional random variable X0 that is a parent of Y but itself has no parents. The variable X0 represents all of the other reasons that could cause Y to occur. The node X0 and the prior probability P (X0) are referred to as a leak node and the leak probability, respectively. In what follows, we continue to refer to all possible causes as X1, . . . , Xn where it is understood that one of these causes could be a leak node.\nExample 1. Consider the Bayesian network shown in Figure 2. Suppose that the random variables are Boolean representing the presence or the absence of the disease or the symptom, that there is a noisy-OR at node Nausea and at node Headache, and that the parameters for the noisy-ORs are as given in Table 1. The full CPT for the node Nausea is given by,\nC F M P (Nausea = 0 | C,F,M) P (Nausea = 1 | C,F,M) 0 0 0 1.00 0.00 0 0 1 0.40 0.60 0 1 0 0.50 0.50 0 1 1 0.20 = 0.5 \u00d7 0.4 0.80 1 0 0 0.60 0.40 1 0 1 0.24 = 0.6 \u00d7 0.4 0.76 1 1 0 0.30 = 0.6 \u00d7 0.5 0.70 1 1 1 0.12 = 0.6 \u00d7 0.5\u00d7 0.4 0.88\nwhere C, F , and M are short for Cold , Flu, and Malaria, respectively.\nAn alternative way to view a noisy-OR relation is as a decomposed probabilistic model. In the decomposed model shown in Figure 3, one only has to specify a small conditional probability table at each node Yi given by P (Yi | Xi), instead of an exponentially large CPT given by P (Y | X1, . . . ,Xn). In the decomposed model, P (Yi = 0 | Xi = 0) = 1, P (Yi = 0 | Xi = 1) = qi, and the CPT at node Y is now deterministic and is given by the OR logical relation. The OR operator can be converted into a full CPT as follows,\nP (Y | Y1, . . . , Yn) = { 1, if Y = Y1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Yn, 0, otherwise.\nThe probability distribution of an effect variable Y is given by,\nP (Y | X1, . . . ,Xn) = \u2211\nY =Y1\u2228\u00b7\u00b7\u00b7\u2228Yn\n( n\u220f\ni=1\nP (Yi | Xi) ) ,\nwhere the sum is over all configurations or possible values for Y1, . . . , Yn such that the OR of these Boolean values is equal to the value for Y . Similarly, in Pearl\u2019s (1988) decomposed model, one only has to specify n probabilities to fully specify the model (see Figure 4); i.e., one specifies the prior probabilities P (Ii), 1 \u2264 i \u2264 n. In this model, causes always lead to effects unless they are prevented or inhibited from doing so. The random variables Ii model this prevention or inhibition.\nThese two decomposed probabilistic models (Figure 3, and Figure 4) can be shown to be equivalent in the sense that the conditional probability distribution P (Y | X1, . . . ,Xn) induced by both of these networks is just the original distribution for the network shown in Figure 1. It is important to note that both of these models would still have an exponentially large CPT associated with the effect node Y if the deterministic OR node were to be replaced\nby its full CPT representation. In other words, these decomposed models address ease of modeling or representation issues, but do not address efficiency of reasoning issues.\nThe noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; D\u0301\u0131ez, 1993) is a generalization of the noisy-OR to non-Boolean domains. With the noisy-MAX relation, one again assumes that there are different causes X1,. . . , Xn leading to an effect Y (see Figure 1), but now the random variables may have multi-valued (non-Boolean) domains. The domains of the variables are assumed to be ordered and the values are referred to as the degree or the severity of the variable. Each domain has a distinguished lowest degree 0 representing the fact that a cause or effect is absent. As with the noisy-OR relation, one assumes that all possible causes are given and when all causes are absent, the effect is absent. Again, these assumptions are not as restrictive as first appears, as one can incorporate a leak node. As well, one assumes that the mechanism or reason that inhibits a Xi from causing Y is independent of the mechanism or reason that inhibits a Xj , j = i, from causing Y .\nLet dX be the number of values in the domain of some random variable X. For simplicity of notation and without loss of generality, we assume that the domain of a variable X is given by the set of integers {0, 1, . . . , dX \u2212 1}. A noisy-MAX relation with causes X1, . . . , Xn and effect Y specifies a CPT using the parameters,\nP (Y = y | Xi = xi,Xj = 0[\u2200j,j =i]) = qxii,y i = 1, . . . , n, (4) y = 0, . . . , dY \u2212 1, xi = 1, . . . , dXi \u2212 1.\nIf all of the domain sizes are equal to d, a total of (d \u2212 1)2n non-redundant probabilities must be specified. From these parameters, the full CPT representation of size dn+1 can be generated using,\nP (Y \u2264 y | X) = n\u220f\ni=1 xi =0\ny\u2211 y\u2032=0 qxii,y\u2032 (5)\nand\nP (Y = y | X) = {\nP (Y \u2264 0 | X) if y = 0, P (Y \u2264 y | X)\u2212 P (Y \u2264 y \u2212 1 | X) if y > 0. (6)\nwhere X represents a certain configuration of the parents of Y , X = x1, . . . , xn, and P (Y = 0 | X1 = 0, . . . ,Xn = 0) = 1; i.e., if all causes are absent, the effect is absent.\nExample 2. Consider once again the Bayesian network shown in Figure 2. Suppose that the diseases are Boolean random variables and the symptoms Nausea and Headache have domains {absent = 0, mild = 1, severe = 2}, there is a noisy-MAX at node Nausea and at node Headache, and the parameters for the noisy-MAX at node Nausea are as given in Table 2. The full CPT for the node Nausea is given by,\nC F M P (N = a | C,F,M) P (N = m | C,F,M) P (N = s | C,F,M) 0 0 0 1.000 0.000 0.000 0 0 1 0.100 0.400 0.500 0 1 0 0.500 0.200 0.300 0 1 1 0.050 = 0.5\u00d7 0.1 0.300 0.650 1 0 0 0.700 0.200 0.100 1 0 1 0.070 = 0.7\u00d7 0.1 0.380 0.550 1 1 0 0.350 = 0.7\u00d7 0.5 0.280 0.370 1 1 1 0.035 = 0.7\u00d7 0.5 \u00d7 0.1 0.280 0.685\nwhere C, F , M , and N are short for the variables Cold , Flu, Malaria, and Nausea, respectively, and a, m, and s are short for the values absent, mild, and severe, respectively. As an example calculation, P (Nausea = mild | Cold = 0,Flu = 1,Malaria = 1) = ((0.5 + 0.2) \u00d7 (0.1 + 0.4)) \u2212 (0.05) = 0.3 As a second example, P (Nausea = mild | Cold = 1,Flu = 1,Malaria = 1) = ((0.7 + 0.2) \u00d7 (0.5 + 0.2) \u00d7 (0.1 + 0.4)) \u2212 (0.035) = 0.28\nAs with the noisy-OR relation, an alternative view of a noisy-MAX relation is as a decomposed probabilistic model (see Figure 3). In the decomposed model, one only has to specify a small conditional probability table at each node Yi given by P (Yi | Xi), where P (Yi = 0 | Xi = 0) = 1 and P (Yi = y | Xi = x) = qxi,y. Each Yi models the effect of the\ncause Xi on the effect Y in isolation; i.e., the degree or the severity of the effect in the case where only the cause Xi is not absent and all other causes are absent. The CPT at node Y is now deterministic and is given by the MAX arithmetic relation. This corresponds to the assumption that the severity or the degree reached by the effect Y is the maximum of the degrees produced by each cause if they were acting independently; i.e., the maximum of the Yi\u2019s. This assumption is only valid if the effects do not accumulate. The MAX operator can be converted into a full CPT as follows,\nP (Y | Y1, . . . , Yn) = { 1, if Y = max{Y1, . . . , Yn}, 0, otherwise.\nThe probability distribution of an effect variable Y is given by,\nP (Y | X1, . . . ,Xn) = \u2211\nY =max{Y1,...,Yn}\n( n\u220f\ni=1\nP (Yi | Xi) ) ,\nwhere the sum is over all configurations or possible values for Y1, . . . , Yn such that the maximum of these values is equal to the value for Y . In both cases, however, making the CPTs explicit is often not possible in practice, as their size is exponential in the number of causes and the number of values in the domains of the random variables."}, {"heading": "2.2 Weighted Model Counting for Probabilistic Inference", "text": "In what follows, we consider propositional formulas in conjunctive normal form (CNF). A literal is a Boolean variable (also called a proposition) or its negation and a clause is a disjunction of literals. A clause with one literal is called a unit clause and the literal in the unit clause is called a unit literal. A propositional formula F is in conjunctive normal form if it is a conjunction of clauses.\nExample 3. For example, (x \u2228 \u00acy) is a clause, and the formula,\nF = (x \u2228 \u00acy) \u2227 (x \u2228 y \u2228 z) \u2227 (y \u2228 \u00acz \u2228 w) \u2227 (\u00acw \u2228 \u00acz \u2228 v) \u2227 (\u00acv \u2228 u),\nis in CNF, where u, v, w, x, y, and z are propositions.\nGiven a propositional formula in conjunctive normal form, the problem of determining whether there exists a variable assignment that makes the formula evaluate to true is called the Boolean satisfiability problem or SAT. A variable assignment that makes a formula evaluate to true is also called a model. The problem of counting the number of models of a formula is called model counting.\nLet F denote a propositional formula. We use the value 0 interchangeably with the Boolean value false and the value 1 interchangeably with the Boolean value true. The notation F |v=false represents a new formula, called the residual formula, obtained by removing all clauses that contain the literal \u00acv (as these clauses evaluate to true) and deleting the literal v from all clauses. Similarly, the notation F |v=true represents the residual formula obtained by removing all clauses that contain the literal v and deleting the literal \u00acv from all\nclauses. Let s be a set of instantiated variables in F . The residual formula F |s is obtained by cumulatively reducing F by each of the variables in s.\nExample 4. Consider once again the propositional formula F given in Example 3. Suppose x is assigned false. The residual formula is given by,\nF |x=0 = (\u00acy) \u2227 (y \u2228 z) \u2227 (y \u2228 \u00acz \u2228 w) \u2227 (\u00acw \u2228 \u00acz \u2228 v) \u2227 (\u00acv \u2228 u).\nAs is clear, a CNF formula is satisfied if and only if each of its clauses is satisfied and a clause is satisfied if and only if at least one of its literals is equivalent to 1. In a unit clause, there is no choice and the value of the literal is said to be forced or implied. The process of unit propagation assigns all unit literals to the value 1. As well, the formula is simplified by removing the variables of the unit literals from the remaining clauses and removing clauses that evaluate to true (i.e., the residual formula is obtained) and the process continues looking for new unit clauses and updating the formula until no unit clause remains.\nExample 5. Consider again the propositional formula F |x=0 given in Example 4, where x has been assigned false. The unit clause (\u00acy) forces y to be assigned false. The residual formula is given by,\nF |x=0,y=0 = (z) \u2227 (\u00acz \u2228 w) \u2227 (\u00acw \u2228 \u00acz \u2228 v) \u2227 (\u00acv \u2228 u).\nIn turn, the unit clause (z) forces z to be assigned true. Similarly, the assignments w = 1, v = 1, and u = 1 are forced.\nThere are natural polynomial-time reductions between the Bayesian inference problem and model counting problems (Bacchus, Dalmao, & Pitassi, 2003). In particular, exact inference in Bayesian networks can be reduced to the weighted model counting of CNFs (Darwiche, 2002; Littman, 1999; Sang et al., 2005a). Weighted model counting is a generalization of model counting.\nA weighted model counting problem consists of a CNF formula F and for each variable v in F , a weight for each literal: weight(v) and weight(\u00acv). Let s be an assignment of a value to every variable in the formula F that satisfies the formula; i.e., s is a model of the formula. The weight of s is the product of the weights of the literals in s. The solution of a weighted model counting problem is the sum of the weights of all satisfying assignments; i.e.,\nweight(F ) = \u2211\ns \u220f l\u2208s weight(l),\nwhere the sum is over all possible models and the product is over the literals in that model. Chavira and Darwiche (2002, 2008) proposed an encoding of a Bayesian network into weighted model counting of a propositional formula in conjunctive normal form. Chavira and Darwiche\u2019s encoding proceeds as follows. At each step, we illustrate the encoding using the Bayesian network shown in Figure 2. For simplicity, we assume the random variables are all Boolean and we omit the node Headache. To improve clarity, we refer to the random variables in the Bayesian network as \u201cnodes\u201d and reserve the word \u201cvariables\u201d for the Boolean variables in the resulting propositional formula.\n\u2022 For each value of each node in the Bayesian network, an indicator variable is created,\nC : IC0 , IC1 , M : IM0 , IM1 , F : IF0 , IF1 , N : IN0 , IN1 .\n\u2022 For each node, indicator clauses are generated which ensure that in each model exactly one of the corresponding indicator variables for each node is true,\nC : (IC0 \u2228 IC1) \u2227 (\u00acIC0 \u2228 \u00acIC1), M : (IM0 \u2228 IM1) \u2227 (\u00acIM0 \u2228 \u00acIM1), F : (IF0 \u2228 IF1) \u2227 (\u00acIF0 \u2228 \u00acIF1), N : (IN0 \u2228 IN1) \u2227 (\u00acIN0 \u2228 \u00acIN1).\n\u2022 For each conditional probability table (CPT) and for each parameter (probability) value in the CPT, a parameter variable is created,\nC : PC0 , PC1 , F : PF0 , PF1 , M : PM0 , PM1 ,\nN : PN0|C0,F0,M0, PN1|C0,F0,M0, . . ., . . ., PN0|C1,F1,M1, PN1|C1,F1,M1.\n\u2022 For each parameter variable, a parameter clause is generated. A parameter clause asserts that the conjunction of the corresponding indicator variables implies the parameter variable and vice-versa,\nC : IC0 \u21d4 PC0 , IC1 \u21d4 PC1 , F : IF0 \u21d4 PF0, IF1 \u21d4 PF1, M : IM0 \u21d4 PM0 , IM1 \u21d4 PM1 , N : IC0 \u2227 IF0 \u2227 IM0 \u2227 IN0 \u21d4 PN0|C0,F0,M0, IC0 \u2227 IF0 \u2227 IM0 \u2227 IN1 \u21d4 PN1|C0,F0,M0,\n. . ., . . ., IC1 \u2227 IF1 \u2227 IM1 \u2227 IN0 \u21d4 PN0|C1,F1,M1, IC1 \u2227 IF1 \u2227 IM1 \u2227 IN1 \u21d4 PN1|C1,F1,M1.\n\u2022 A weight is assigned to each literal in the propositional formula. Each positive literal of a parameter variable is assigned a weight equal to the corresponding probability entry in the CPT table,\nC : weight(PC0 ) = P (C = 0), weight(PC1 ) = P (C = 1), F : weight(PF0) = P (F = 0), weight(PF1) = P (F = 1), M : weight(PM0 ) = P (M = 0), weight(PM1 ) = P (M = 1), N : weight(PN0 |C0,F0,M0) = P (N = 0 | C = 0, F = 0,M = 0), weight(PN1 |C0,F0,M0) = P (N = 1 | C = 0, F = 0,M = 0), . . . , weight(PN0 |C1,F1,M1) = P (N = 0 | C = 1, F = 1,M = 1), weight(PN1 |C1,F1,M1) = P (N = 1 | C = 1, F = 1,M = 1).\nAll other literals (both positive and negative) are assigned a weight of 1; i.e., weight(IC0) = weight(\u00acIC0) = \u00b7 \u00b7 \u00b7 = weight(IN1) = weight(\u00acIN1) = 1 and weight(\u00acPC0 ) = \u00b7 \u00b7 \u00b7 =\nweight(\u00acPN1 |C1,F1,M1) = 1. The basic idea is that the indicator variables specify the state of the world\u2014i.e., a value for each random variable in the Bayesian network\u2014 and then the weights of the literals multiplied together give the probability of that state of the world.\nSang, Beame, and Kautz (2005a) introduced an alternative encoding of a Bayesian network into weighted model counting of a CNF formula. Sang et al.\u2019s encoding creates fewer variables and clauses, but the size of generated clauses of multi-valued variables can be larger. As with Chavira and Darwiche\u2019s encoding presented above, we illustrate Sang et al.\u2019s encoding using the Bayesian network shown in Figure 2, once again assuming the random variables are all Boolean and omitting the node Headache.\n\u2022 As in Chavira and Darwiche\u2019s encoding, for each node, indicator variables are created and indicator clauses are generated which ensure that in each model exactly one of the corresponding indicator variables for each node is true.\n\u2022 Let the values of the nodes be linearly ordered. For each CPT entry P (Y = y | X) such that y is not the last value in the domain of Y , a parameter variable Py|X is created; e.g.,\nC : PC0 , F : PF0 , M : PM0 ,\nN : PN0|C0,F0,M0, PN0|C0,F0,M1, PN0|C0,F1,M0, PN0|C0,F1,M1, PN0|C1,F0,M0, PN0|C1,F0,M1, PN0|C1,F1,M0, PN0|C1,F1,M1.\n\u2022 For each CPT entry P (Y = yi | X), a parameter clause is generated. Let the ordered domain of Y be {y1, . . . , yk} and let X = x1, . . . , xl. If yi is not the last value in the domain of Y , the clause is given by,\nIx1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 Ixl \u2227 \u00acPy1|X \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u00acPyi\u22121|X \u2227 Pyi|X \u21d2 Iyi .\nIf yi is the last value in the domain of Y , the clause is given by,\nIx1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 Ixl \u2227 \u00acPy1|X \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u00acPyk\u22121|X \u21d2 Iyk .\nFor our running example, the following parameter clauses would be generated,\nC : PC0 \u21d2 IC0 \u00acPC0 \u21d2 IC1 F : PF0 \u21d2 IF0 \u00acPF0 \u21d2 IF1 M : PM0 \u21d2 IM0 \u00acPM0 \u21d2 IM1 N : IC0 \u2227 IF0 \u2227 IM0 \u2227 PN0|C0,F0,M0 \u21d2 IN0, IC0 \u2227 IF0 \u2227 IM0 \u2227 \u00acPN0|C0,F0,M0 \u21d2 IN1 ,\n. . ., . . ., IC1 \u2227 IF1 \u2227 IM1 \u2227 PN0|C1,F1,M1 \u21d2 IN0, IC1 \u2227 IF1 \u2227 IM1 \u2227 \u00acPN0|C1,F1,M1 \u21d2 IN1 .\n\u2022 A weight is assigned to each literal in the propositional formula. As in Chavira and Darwiche\u2019s encoding, the weight of literals for indicator variables is always 1. The\nweight of literals for each parameter variable Py|X is given by,\nweight(Py|X) = P (y | X), weight(\u00acPy|X) = 1\u2212 P (y | X).\nLet F be the CNF encoding of a Bayesian network (either Chavira and Darwiche\u2019s encoding or Sang et al.\u2019s encoding). A general query P (Q | E) on the network can be answered by,\nweight(F \u2227 Q \u2227 E) weight(F \u2227 E) , (7)\nwhere Q and E are propositional formulas which enforce the appropriate values for the indicator variables that correspond to the known values of the random variables.\nA backtracking algorithm used to enumerate the (weighted) models of a CNF formula is often referred to as DPLL or DPLL-based (in honor of Davis, Putnam, Logemann, and Loveland, the authors of some of the earliest work in the field: Davis & Putnam, 1960; Davis, Logemann, & Loveland, 1962), and usually includes such techniques as unit propagation, conflict recording, backjumping, and component caching."}, {"heading": "3. Related Work", "text": "In this section, we relate our work to previously proposed methods for exact inference in Bayesian networks that contain noisy-OR/MAX relations.\nOne method for solving such networks is to replace each noisy-OR/MAX by its full CPT representation and then use any of the well-known algorithms for answering probabilistic queries such as variable elimination or tree clustering/jointree. However, in general\u2014and in particular, for the networks that we use in our experimental evaluation\u2014this method is impractical. A more fruitful approach for solving such networks is to take advantage of the structure or the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gala\u0301n, 2003; Chavira et al., 2005).\nQuickscore (Heckerman, 1989) was the first efficient exact inference algorithm for Booleanvalued two-layer noisy-OR networks. Chavira, Allen and Darwiche (2005) present a method for multi-layer noisy-OR networks and show that their approach is significantly faster than Quickscore on randomly generated two-layer networks. Their approach proceeds as follows: (i) transform the noisy-OR network into a Bayesian network with full CPTs using Pearl\u2019s decomposition (see Figure 4), (ii) translate the network with full CPTs into CNF using a general encoding (see Section 2), (iii) simplify the resulting CNF by taking advantage of determinism (zero parameters and one parameters), and (iv) compile the CNF into an arithmetic circuit. One of our encodings for the noisy-OR (called WMC1) is similar to their more indirect (but also more general) proposal for encoding noisy-ORs (steps (i)\u2013(iii)). We perform a detailed comparison in Section 4.1. In our experiments, we perform a detailed empirical comparison of their approach using compilation (steps (i)\u2013(iv)) against Cachet using our encodings on large Bayesian networks.\nMany alternative methods have been proposed to decompose a noisy-OR/MAX by adding hidden or auxiliary nodes and then solving using adaptations of variable elimination or tree clustering (e.g., Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gala\u0301n, 2003).\nOlesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering.\nTo take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR. By introducing one auxiliary variable Y \u2032, D\u0301\u0131ez\u2019s method leads to a complexity of O(nd2) for singly connected networks, where n is the number of causes and d is the size of the domains of the random variables. However, for networks with loops it needs to be integrated with local conditioning. Takikawa and D\u2019Ambrosio (1999) proposed a similar multiplicative factorization approach. The complexity of their approach is O(max(2d, nd2)). However, Takikawa and D\u2019Ambrosio\u2019s approach allows more efficient elimination orderings in the variable elimination algorithm, while D\u0301\u0131ez\u2019s method enforces more restrictions on the orderings. More recently, D\u0301\u0131ez and Gala\u0301n (2003) proposed a multiplicative factorization that improves on this previous work, as it has the advantages of both methods. We use their auxiliary graph as the starting point for the remaining three of our CNF encodings (WMC2, MAX1, and MAX2). In our experiments, we perform a detailed empirical comparison of their approach using variable elimination against our proposals on large Bayesian networks.\nIn our work, we build upon the DPLL-based weighted model counting approach of Sang, Beame, and Kautz (2005a). Their general encoding assumes full CPTs and yields a parameter clause for each CPT parameter. However, this approach is impractical for large-scale noisy-OR/MAX networks. Our special-purpose encodings extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach."}, {"heading": "4. Efficient Encodings of Noisy-OR into CNF", "text": "In this section, we present techniques for improving the weighted model counting approach for Bayesian networks with noisy-OR relations. In particular, we present two CNF encodings for noisy-OR relations that exploit their structure or semantics. For the noisy-OR relation we take advantage of the Boolean domains to simplify the encodings. We use as a running example the Bayesian network shown in Figure 2. In the subsequent section, we generalize to the noisy-MAX relation."}, {"heading": "4.1 Weighted CNF Encoding 1: An Additive Encoding", "text": "Let there be causes X1, . . . , Xn leading to an effect Y and let there be a noisy-OR relation at node Y (see Figure 1), where all random variables are assumed to have Boolean-valued domains.\nIn our first weighted model encoding method (WMC1), we introduce an indicator variable IY for Y and an indicator variable IXi for each parent of Y . We also introduce a parameter variable Pqi for each parameter qi, 1 \u2264 i \u2264 n in the noisy-OR (see Equation 1). The weights of these variables are as follows,\nweight(IXi ) = weight(\u00acIXi ) = 1, weight(IY ) = weight(\u00acIY ) = 1,\nweight(Pqi ) = 1\u2212 qi, weight(\u00acPqi ) = qi.\nThe noisy-OR relation can then be encoded as the formula,\n(IX1 \u2227 Pq1) \u2228 (IX2 \u2227 Pq2) \u2228 \u00b7 \u00b7 \u00b7 \u2228 (IXn \u2227 Pqn) \u21d4 IY . (8)\nThe formula can be seen to be an encoding of Pearl\u2019s well-known decomposition for noisyOR (see Figure 4).\nExample 6. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-ORs shown in Table 1. The WMC1 encoding introduces the five Boolean indicator variables IC , IF , IM , IN , and IH , each with weight 1; and the six parameter variables P0.6, P0.5, P0.4, P0.3, P0.2, and P0.1, each with weight(Pqi ) = 1 \u2212 qi and weight(\u00acPqi) = qi. Using Equation 8, the noisy-OR at node Nausea can be encoded as,\n(IC \u2227 P0.6) \u2228 (IF \u2227 P0.5) \u2228 (IM \u2227 P0.4) \u21d4 IN .\nTo illustrate the weighted model counting of the formula, suppose that nausea and malaria are absent and cold and flu are present (i.e., Nausea = 0, Malaria = 0, Cold = 1, and Flu = 1; and for the corresponding indicator variables IN and IM are false and IC and IF are true). The formula can be simplified to,\n(P0.6) \u2228 (P0.5) \u21d4 0.\nThere is just one model for this formula, the model that sets P0.6 to false and P0.5 to false. Hence, the weighted model count of this formula is weight(\u00acP0.6)\u00d7weight(\u00acP0.5) = 0.6\u00d70.5 = 0.3, which is just the entry in the penultimate row of the full CPT shown in Example 2.\nTowards converting Equation 8 into CNF, we also introduce an auxiliary indicator variable wi for each conjunction such that wi \u21d4 IXi\u2227Pqi . This dramatically reduces the number\nof clauses generated. Equation 8 is then transformed into,\n(\u00acIY \u2228 ((w1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 wn) \u2227 (\u00acIX1 \u2228 \u00acPq1 \u2228 w1) \u2227 (IX1 \u2228 \u00acw1) \u2227 (Pq1 \u2228 \u00acw1) \u2227 \u00b7 \u00b7 \u00b7 \u2227 (\u00acIXn \u2228 \u00acPqn \u2228 wn) \u2227 (IXn \u2228 \u00acwn) \u2227 (Pqn \u2228 \u00acwn))) \u2227\n(IY \u2228 ((\u00acIX1 \u2228 \u00acPq1) \u2227 \u00b7 \u00b7 \u00b7 \u2227 (\u00acIXn \u2228 \u00acPqn))).\nThe formula is not in CNF, but can be easily transformed into CNF using the distributive law. It can be seen that the WMC1 encoding can also easily encode evidence\u2014i.e, if IY = 0 or IY = 1, the formula can be further simplified\u2014before the final translation into CNF. Note that we have made the definitions of the auxiliary variables (i.e., wi \u21d4 IXi \u2227Pqi) conditional on IY being true, rather than just introducing separate clauses to define each auxiliary variable. This allows the formula to be further simplified in the presence of evidence and only introduces the wi if they are actually needed. In particular, if we know that IY is false, all of the clauses involving the auxiliary variables wi, including the definitions of the wi, disappear when the formula is simplified.\nExample 7. Consider once again the Bayesian network shown in Figure 2. To illustrate the encoding of evidence, suppose that nausea is present (i.e., Nausea = 1) and headache is not present (i.e., Headache = 0). The corresponding constraints for the evidence are as follows.\n(IC \u2227 P0.6) \u2228 (IF \u2227 P0.5) \u2228 (IM \u2227 P0.4) \u21d4 1 (9) (IC \u2227 P0.3) \u2228 (IF \u2227 P0.2) \u2228 (IM \u2227 P0.1) \u21d4 0 (10)\nThe above constraints can be converted into CNF clauses. Constraint Equation 9 gives the clauses,\n(w1 \u2228 w2 \u2228 w3) \u2227 (\u00acIC \u2228 \u00acP0.6 \u2228 w1) \u2227 (IC \u2228 \u00acw1) \u2227 (P0.6 \u2228 \u00acw1) \u2227 (\u00acIF \u2228 \u00acP0.5 \u2228 w2) \u2227 (IF \u2228 \u00acw2) \u2227 (P0.5 \u2228 \u00acw2) \u2227 (\u00acIM \u2228 \u00acP0.4 \u2228 w3) \u2227 (IM \u2228 \u00acw3) \u2227 (P0.4 \u2228 \u00acw3)\nand constraint Equation 10 gives the clauses,\n(\u00acIC \u2228 \u00acP0.3) \u2227 (\u00acIF \u2228 \u00acP0.2) \u2227 (\u00acIM \u2228 \u00acP0.1).\nTo show the correctness of encoding WMC1 of a noisy-OR, we first show that each entry in the full CPT representation of a noisy-OR relation can be determined using the weighted model count of the encoding. As always, let there be causes X1, . . . , Xn leading to an effect Y and let there be a noisy-OR relation at node Y , where all random variables have Boolean-valued domains.\nLemma 1. Each entry in the full CPT representation of a noisy-OR at a node Y , P (Y = y | X1 = x1, . . . ,Xn = xn), can be determined using the weighted model count of Equation 8 created using the encoding WMC1.\nProof. Let FY be the encoding of the noisy-OR at node Y using WMC1 and let s be the set of assignments to the indicator variables IY , IX1 , . . . , IXn corresponding to the desired entry in the CPT (e.g., if Y = 0, IY is instantiated to false; otherwise it is instantiated to true). For each Xi = 0, the disjunct (IXi \u2227Pqi) in Equation 8 is false and would be removed in the residual formula FY |s; and for each Xi = 1, the disjunct reduces to (Pqi). If IY = 0, each of the disjuncts in Equation 8 must be false and there is only a single model of the formula. Hence,\nweight(FY |s) = \u220f i\u2208Tx weight(\u00acPqi ) = \u220f i\u2208Tx qi = P (Y = 0 | X),\nwhere Tx = {i | Xi = 1} and P (Y = 0 | X) = 1 if Tx is empty. If IY = 1, at least one of the disjuncts in Equation 8 must be true and there are, therefore, 2|Tx| \u2212 1 models. It can be seen that if we sum over all 2|Tx| possible assignments, the weight of the formula is 1. Hence, subtracting off the one possible assignment that is not a model gives,\nweight(FY |s) = 1\u2212 \u220f i\u2208Tx weight(\u00acPqi ) = 1\u2212 \u220f i\u2208Tx qi = P (Y = 1 | X).\nA noisy-OR Bayesian network over a set of random variables Z1, . . . , Zn is a Bayesian network where there are noisy-OR relations at one or more of the Zi and full CPTs at the remaining nodes. The next step in the proof of correctness is to show that each entry in the joint probability distribution represented by a noisy-OR Bayesian network can be determined using weighted model counting. In what follows, we assume that noisy-OR nodes are encoded using WMC1 and the remaining nodes are encoded using Sang et al.\u2019s general encoding discussed in Section 2.2. Similar results can be stated using Chavira and Darwiche\u2019s general encoding.\nLemma 2. Each entry in the joint probability distribution, P (Z1 = z1, . . . Zn = zn), represented by a noisy-OR Bayesian network can be determined using weighted model counting and encoding WMC1.\nProof. Let F be the encoding of the Bayesian network using WMC1 for the noisy-OR nodes and let s be the set of assignments to the indicator variables IZ1, . . . , IZn corresponding to the desired entry in the joint probability distribution. Any entry in the joint probability\ndistribution can be expressed as the product,\nP (X1, . . . ,Xn) = n\u220f\ni=1\nP (Xi | parents(Xi)),\nwhere n is the size of the Bayesian network and parents(Xi) is the set of parents of Xi in the directed graph; i.e., the entry in the joint probability distribution is determined by multiplying the corresponding CPT entries. For those nodes with full CPTs, s determines the correct entry in each CPT by Lemma 2 in Sang et al. (2005a) and for those nodes with noisy-ORs, s determines the correct probability by Lemma 1 above. Thus, weight(F \u2227 s) is the multiplication of the corresponding CPT entries; i.e., the entry in the joint probability distribution.\nThe final step in the proof of correctness is to show that queries of interest can be correctly answered.\nTheorem 1. Given a noisy-OR Bayesian network, general queries of the form P (Q | E) can be determined using weighted model counting and encoding WMC1.\nProof. Let F be the CNF encoding of a noisy-OR Bayesian network. A general query P (Q | E) on the network can be answered by,\nP (Q \u2227 E) P (E) = weight(F \u2227 Q \u2227 E) weight(F \u2227 E) ,\nwhere Q and E are propositional formulas that enforce the appropriate values for the indicator variables that correspond to the known values of the random variables. By definition, the function weight computes the weighted sum of the solutions of its argument. By Lemma 2, this is equal to the sum of the probabilities of those sets of assignments that satisfy the restrictions Q \u2227 E and E, respectively, which in turn is equal to the sum of the entries in the joint probability distribution that are consistent with Q \u2227 E and E, respectively.\nAs Sang et al. (2005a) note, the weighted model counting approach supports queries and evidence in arbitrary propositional form and such queries are not supported by any other exact inference method.\nOur WMC1 encoding for noisy-OR is essentially similar to a more indirect but also more general proposal by Chavira and Darwiche (2005) (see Darwiche, 2009, pp. 313-323 for a detailed exposition of their proposal). Their approach proceeds as follows: (i) transform the noisy-OR network into a Bayesian network with full CPTs using Pearl\u2019s decomposition (see Figure 4), (ii) translate the network with full CPTs into CNF using a general encoding (see Section 2), and (iii) simplify the resulting CNF by taking advantage of determinism. Simplifying the resulting CNF proceeds as follows. Suppose we have in an encoding the sentence (Ia \u2227 I\u00acb) \u21d4 P\u00acb|a. If the parameter corresponding to P\u00acb|a is zero, the sentence can be replaced by \u00ac(Ia \u2227 I\u00acb) and P\u00acb|a can be removed from the encoding. If the parameter corresponding to P\u00acb|a is one, the entire sentence can be removed from the encoding. Applying their method to a noisy-OR (see Figure 1) results in the following,\n(\u00acIX1 \u2228 \u00acPq1 \u2228 w1) \u2227 (\u00acIX1 \u2228 Pq1 \u2228 \u00acw1) \u2227 (IX1 \u2228 \u00acPq1 \u2228 \u00acw1) \u2227 (IX1 \u2228 Pq1 \u2228 \u00acw1) \u2227 . . . (\u00acIXn \u2228 \u00acPqn \u2228 wn) \u2227 (\u00acIXn \u2228 Pqn \u2228 \u00acwn) \u2227 (IXn \u2228 \u00acPqn \u2228 \u00acwn) \u2227 (IXn \u2228 Pqn \u2228 \u00acwn) \u2227\n(\u00acIY \u2228 (w1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 wn)) \u2227 (IY \u2228 ((\u00acw1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acwn\u22121 \u2228 \u00acwn) \u2227\n(\u00acw1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acwn\u22121 \u2228 wn) \u2227 . . . (w1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 wn\u22121 \u2228 \u00acwn))),\nwhere we have simplified the expression by substituting equivalent literals and by using the fact that the random variables are Boolean (e.g., we use IX1 and \u00acIX1 rather than IX1=0 and IX1=1). Three differences can be noted. First, in our encoding the definitions of the wi are conditional on IY being true, rather than being introduced as separate clauses. Second, our definitions of the wi are more succinct. Third, in our encoding there are a linear number of clauses conditioned on IY whereas in the Chavira et al. encoding there are 2n\u22121 clauses. We note, however, that Chavira, Allen, and Darwiche (2005) discuss a direct translation of a noisy-OR to CNF based on Pearl\u2019s decomposition that is said to compactly represent the noisy-OR (i.e., not an exponential number of clauses), but the specific details of the CNF formula are not given."}, {"heading": "4.2 Weighted CNF Encoding 2: A Multiplicative Encoding", "text": "Again, let there be causes X1, . . . , Xn leading to an effect Y and let there be a noisyOR relation at node Y (see Figure 1), where all random variables are assumed to have Boolean-valued domains.\nOur second weighted model encoding method (WMC2) takes as its starting point D\u0301\u0131ez and Gala\u0301n\u2019s (2003) directed auxiliary graph transformation of a Bayesian network with a noisy-OR/MAX relation. D\u0301\u0131ez and Gala\u0301n note that for the noisy-OR relation, Equation (6) can be represented as a product of matrices,(\nP (Y = 0 | X) P (Y = 1 | X)\n) = ( 1 0 \u22121 1 )( P (Y \u2264 0 | X) P (Y \u2264 1 | X) ) .\nBased on this factorization, one can integrate a noisy-OR node into a regular Bayesian network by introducing a hidden node Y \u2032 for each noisy-OR node Y . The transformation first creates a graph with the same set of nodes and arcs as the original network. Then, for each node Y with a noisy-OR relation, we add a hidden node Y \u2032 with the same domain as Y , add an arc Y \u2032 \u2192 Y , redirect each arc Xi \u2192 Y to Xi \u2192 Y \u2032, and associate with Y a factorization table,\nY \u2032 = 0 Y \u2032 = 1 Y = 0 1 0 Y = 1 \u22121 1.\nThis auxiliary graph is not a Bayesian network as it contains parameters that are less than 0. So the CNF encoding methods for general Bayesian networks (see Section 2) cannot be applied here.\nWe introduce indicator variables IY \u2032 and IY for Y \u2032 and Y , and an indicator variable IXi for each parent of Y \u2032. The weights of these variables are as follows,\nweight(IY \u2032) = weight(\u00acIY \u2032) = 1, weight(IY ) = weight(\u00acIY ) = 1, weight(IXi ) = weight(\u00acIXi ) = 1.\nFor each arc Xi \u2192 Y \u2032, 1 \u2264 i \u2264 n, we create two parameter variables P 0Xi,Y \u2032 and P 1Xi,Y \u2032 . The weights of these variables are as follows,\nweight(P 0Xi ,Y \u2032) = 1, weight(P 1 Xi ,Y \u2032) = qi,\nweight(\u00acP 0Xi ,Y \u2032) = 0, weight(\u00acP 1Xi,Y \u2032) = 1\u2212 qi. For each factorization table, we introduce two variables, uY and wY , where the weights of these variables are given by,\nweight(uY ) = 1, weight(\u00acuY ) = 0, weight(wY ) = \u22121, weight(\u00acwY ) = 2.\nFor the first row of a factorization table, we generate the clause,\n(\u00acIY \u2032 \u2228 IY ), (11)\nand for the second row, we generate the clauses,\n(\u00acIY \u2032 \u2228 \u00acIY \u2228 uY ) \u2227 (IY \u2032 \u2228 \u00acIY \u2228 wY ). (12)\nFinally, for every parent Xi of Y \u2032, we generate the clauses,\n(IY \u2032 \u2228 IXi \u2228 P 0Xi,Y \u2032) \u2227 (IY \u2032 \u2228 \u00acIXi \u2228 P 1Xi,Y \u2032). (13)\nWe now have a conjunction of clauses; i.e., CNF.\nExample 8. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-ORs shown in Table 1. The auxiliary graph transformation is shown in Figure 5. The WMC2 encoding introduces the seven Boolean indicator variables IC , IF , IM , I \u2032N , IN , I \u2032 H , and IH ; the twelve parameter variables,\nP 0C,N \u2032 P 1 C,N \u2032 P 0 C,H\u2032 P 1 C,H\u2032 P 0F,N \u2032 P 1 F,N \u2032 P 0 F,H\u2032 P 1 F,H\u2032 P 0M,N \u2032 P 1 M,N \u2032 P 0 M,H\u2032 P 1 M,H\u2032;\nand the four factorization variables uN , wN , uH , and wH . The noisy-OR at node Nausea can be encoded as the set of clauses,\n\u00acIN \u2032 \u2228 IN IN \u2032 \u2228 IC \u2228 P 0C,N \u2032 IN \u2032 \u2228 \u00acIC \u2228 P 1C,N \u2032 \u00acIN \u2032 \u2228 \u00acIN \u2228 uN IN \u2032 \u2228 IF \u2228 P 0F,N \u2032 IN \u2032 \u2228 \u00acIF \u2228 P 1F,N \u2032 IN \u2032 \u2228 \u00acIN \u2228 wN IN \u2032 \u2228 IM \u2228 P 0M,N \u2032 IN \u2032 \u2228 \u00acIM \u2228 P 1M,N \u2032\nTo illustrate the weighted model counting of the formula, suppose that nausea and malaria are absent and cold and flu are present (i.e., Nausea = 0, Malaria = 0, Cold = 1, and Flu = 1; and for the corresponding indicator variables IN and IM are false and IC and IF are true). The formula can be simplified to,\nP 1C,N \u2032 \u2227 P 1F,N \u2032 \u2227 P 0M,N \u2032 .\n(To see this, note that clauses that evaluate to true are removed and literals that evaluate to false are removed from a clause. As a result of simplifying the first clause, IN \u2032 is forced to be false and is removed from the other clauses.) There is just one model for this formula, the model that sets each of the conjuncts to true. Hence, the weighted model count of this formula is weight(P 1C,N \u2032 )\u00d7 weight(P 1F,N \u2032 )\u00d7 weight(P 0M,N \u2032 ) = 0.6\u00d7 0.5 \u00d7 1.0 = 0.3, which is just the entry in the penultimate row of the full CPT shown in Example 2.\nOnce again, it can be seen that WMC2 can also easily encode evidence into the CNF formula; i.e., if IY = 0 or IY = 1, the formula can be further simplified.\nExample 9. Consider once again the Bayesian network shown in Figure 2. To illustrate the encoding of evidence, suppose that nausea is present (i.e., Nausea = 1) and headache is not present (i.e., Headache = 0). The WMC2 encoding results in the following set of clauses,\nIN \u2032 \u2228 IC \u2228 P 0C,N \u2032 IN \u2032 \u2228 \u00acIC \u2228 P 1C,N \u2032 \u00acIN \u2032 \u2228 uN IN \u2032 \u2228 IF \u2228 P 0F,N \u2032 IN \u2032 \u2228 \u00acIF \u2228 P 1F,N \u2032 IN \u2032 \u2228 wN IN \u2032 \u2228 IM \u2228 P 0M,N \u2032 IN \u2032 \u2228 \u00acIM \u2228 P 1M,N \u2032\n\u00acIH\u2032 IC \u2228 P 0C,H\u2032 \u00acIC \u2228 P 1C,H\u2032 IF \u2228 P 0F,H\u2032 \u00acIF \u2228 P 1F,H\u2032 IM \u2228 P 0M,H\u2032 \u00acIM \u2228 P 1M,H\u2032\nTo show the correctness of encoding WMC2 of a noisy-OR, we first show that each entry in the full CPT representation of a noisy-OR relation can be determined using the weighted model count of the encoding. As always, let there be causes X1, . . . , Xn leading to an effect Y and let there be a noisy-OR relation at node Y , where all random variables have Boolean-valued domains.\nLemma 3. Each entry in the full CPT representation of a noisy-OR at a node Y , P (Y = y | X1 = x1, . . . ,Xn = xn), can be determined using the weighted model count of Equations 11\u221213 created using the encoding WMC2. Proof. Let FY be the encoding of the noisy-OR at node Y using WMC2 and let s be the set of assignments to the indicator variables IY , IX1 , . . . , IXn corresponding to the desired entry in the CPT. For each Xi = 0, the clauses in Equation 13 reduce to (IY \u2032 \u2228P 0Xi,Y \u2032), and for each Xi = 1, the clauses reduce to (IY \u2032 \u2228P 1Xi,Y \u2032). If IY = 0, the clauses in Equations 11 & 12 reduce to (\u00acIY \u2032). Hence,\nweight(FY |s) = weight(\u00acIY \u2032) \u220f i \u2208Tx weight(P 0Xi ,Y \u2032)) \u220f i\u2208Tx weight(P 1Xi ,Y \u2032))\n= \u220f i\u2208Tx qi = P (Y = 0 | X),\nwhere Tx = {i | Xi = 1} and P (Y = 0 | X) = 1 if Tx is empty. If IY = 1, the clauses in Equations 11 & 12 reduce to (\u00acIY \u2032 \u2228 uY ) \u2227 (IY \u2032 \u2228 wY ). Hence,\nweight(FY |s) = weight(\u00acIY \u2032)weight(\u00acuY )weight(wY ) \u220f i\u2208Tx qi +\nweight(\u00acIY \u2032)weight(uY )weight(wY ) \u220f i\u2208Tx qi + weight(IY \u2032)weight(uY )weight(\u00acwY ) + weight(IY \u2032)weight(uY )weight(wY )\n= 1\u2212 \u220f i\u2208Tx qi = P (Y = 1 | X).\nThe remainder of the proof of correctness for encoding WMC2 is similar to that of encoding WMC1.\nLemma 4. Each entry in the joint probability distribution, P (Z1 = z1, . . . Zn = zn), represented by a noisy-OR Bayesian network can be determined using weighted model counting and encoding WMC2.\nTheorem 2. Given a noisy-OR Bayesian network, general queries of the form P (Q | E) can be determined using weighted model counting and encoding WMC2."}, {"heading": "5. Efficient Encodings of Noisy-MAX into CNF", "text": "In this section, we present techniques for improving the weighted model counting approach for Bayesian networks with noisy-MAX relations. In particular, we present two CNF encodings for noisy-MAX relations that exploit their structure or semantics. We again use as a running example the Bayesian network shown in Figure 2.\nLet there be causes X1, . . . , Xn leading to an effect Y and let there be a noisy-MAX relation at node Y (see Figure 1), where the random variables may have multi-valued (nonBoolean) domains. Let dX be the number of values in the domain of some random variable X.\nThe WMC2 multiplicative encoding above can be extended to noisy-MAX by introducing more indicator variables to represent variables with multiple values. In this section, we explain the extension and present two noisy-MAX encodings based on two different weight definitions of the parameter variables. The two noisy-MAX encodings are denoted MAX1 and MAX2, respectively. We begin by presenting those parts of the encodings that MAX1 and MAX2 have in common. As with WMC2, these two noisy-MAX encodings take as their starting point D\u0301\u0131ez and Gala\u0301n\u2019s (2003) directed auxiliary graph transformation of a Bayesian network with noisy-OR/MAX. D\u0301\u0131ez and Gala\u0301n show that for the noisy-MAX relation, Equation (6) can be factorized as a product of matrices,\nP (Y = y | X) = y\u2211\ny\u2032=0\nMY (y, y\u2032) \u00b7 P (Y \u2264 y\u2032 | X) (14)\nwhere MY is a dY \u00d7 dY matrix given by,\nMY (y, y\u2032) = \u23a7\u23aa\u23a8 \u23aa\u23a9 1, if y\u2032 = y, \u22121, if y\u2032 = y \u2212 1, 0, otherwise.\nFor each noisy-MAX node Y , we introduce dY indicator variables IY0 ... IYdY \u22121, to represent each value in the domain of Y , and ( dY 2 ) +1 clauses to ensure that exactly one of these variables is true. As in WMC2, we introduce a hidden node Y \u2032 with the same domain as Y , corresponding indicator variables to represent each value in the domain of Y \u2032, and clauses to ensure that exactly one domain value is selected in each model. For each parent Xi, 1 \u2264 i \u2264 n, of Y , we define indicator variables Ii,x, where x = 0, . . . , dXi \u2212 1, and add\nclauses that ensure that exactly one of the indicator variables corresponding to each Xi is true. Each indicator variable and each negation of an indicator variable has weight 1.\nExample 10. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. As the node Nausea has domain {absent = 0, mild = 1, severe = 2} and the parents Cold, Flu, and Malaria are Boolean valued, both the MAX1 and MAX2 encodings introduce the Boolean indicator variables INa , INm , INs, IN \u2032a , IN \u2032m , IN \u2032s , IC0 , IC1 , IF0, IF1 , IM0 , and IM1. The weights of these variables and their negations are 1. Four clauses are added over the indicator variables for Nausea,\n(INa \u2228 INm \u2228 INs) \u2227 (\u00acINa \u2228 \u00acINm) \u2227 (\u00acINa \u2228 \u00acINs) \u2227 (\u00acINm \u2228 \u00acINs).\nSimilar clauses are added over the indicator variables for the hidden node N \u2032 and over the indicator variables for the parents Cold, Flu, and Malaria, respectively.\nFor each factorization table, we introduce two auxiliary variables, uY and wY , where the weights of these variables are given by,\nweight(uY ) = 1, weight(\u00acuY ) = 0, weight(wY ) = \u22121, weight(\u00acwY ) = 2.\nFor each factorization table, a clause is added for each entry in the matrix,\nMY (y, y\u2032) = \u23a7\u23aa\u23a8 \u23aa\u23a9 1, add (\u00acIy\u2032 \u2228 \u00acIy \u2228 uY ) if y\u2032 = y, \u22121, add (\u00acIy\u2032 \u2228 \u00acIy \u2228 wY ) if y\u2032 = y \u2212 1, 0, add (\u00acIy\u2032 \u2228 \u00acIy \u2228 \u00acuY ) otherwise.\nExample 11. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. As Nausea has domain {absent = 0, mild = 1, severe = 2}, the factorization table MN is given by,\nN \u2032 = absent N \u2032 = mild N \u2032 = severe N = absent 1 0 0 N = mild \u22121 1 0 N = severe 0 \u22121 1.\nAuxiliary variables uN and wN are introduced and the following clauses, shown in row order, would be added for the factorization table MN ,\n\u00acINa \u2228 \u00acIN \u2032a \u2228 uN \u00acINa \u2228 \u00acIN \u2032m \u2228 \u00acuN \u00acINa \u2228 \u00acIN \u2032s \u2228 \u00acuN \u00acINm \u2228 \u00acIN \u2032a \u2228 wN \u00acINm \u2228 \u00acIN \u2032m \u2228 uN \u00acINm \u2228 \u00acIN \u2032s \u2228 \u00acuN \u00acINs \u2228 \u00acIN \u2032a \u2228 \u00acuN \u00acINs \u2228 \u00acIN \u2032m \u2228 wN \u00acINs \u2228 \u00acIN \u2032s \u2228 uN .\nThat completes the description of those parts of the encodings that are common to both MAX1 and MAX2."}, {"heading": "5.1 Weighted CNF Encoding 1 for Noisy-MAX", "text": "Our first weighted model counting encoding for noisy-MAX relations (MAX1) is based on an additive definition of noisy-MAX. Recall the decomposed probabilistic model for the noisyMAX relation discussed at the end of Section 2.1. It can be shown that for the noisy-MAX, P (Y \u2264 y | X1, . . . ,Xn) can be determined using,\nP (Y \u2264 y | X1, . . . ,Xn) = \u2211 Yi\u2264y n\u220f i=1 P (Yi | Xi) = \u2211 Yi\u2264y n\u220f i=1\nXi =0\nqXii,Yi (15)\nwhere the qXii,Yi are the parameters to the noisy-MAX, and the sum is over all the configurations or possible values for Y1, . . . , Yn, such that each of these values is less than or equal to the value y. Note that the outer operator is summation; hence, we refer to MAX1 as an additive encoding. Substituting the above into Equation 14 gives,\nP (Y = y | X1, . . . ,Xn) = y\u2211\ny\u2032=0\nMY (y, y\u2032) \u00b7 \u239b \u239c\u239c\u239d \u2211\nYi\u2264y\u2032 n\u220f i=1\nXi =0\nqXii,Yi \u239e \u239f\u239f\u23a0 . (16)\nIt is this equation that we encode into CNF. The encoding of the factorization table MY is common to both encodings and has been explained above. It remains to encode the computation for P (Y \u2264 y | X1, . . . ,Xn).\nFor each parent Xi, 1 \u2264 i \u2264 n, of Y we introduce dY indicator variables, Ii,y, to represent the effect of Xi on Y , where 0 \u2264 y \u2264 dY \u22121, and add clauses that ensure that exactly one of the indicator variables correspond to each Xi is true. Note that these indicators variables are in addition to the indicator variables common to both encodings and explained above. As always with indicator variables, the weights of Ii,y and \u00acIi,y are both 1.\nFor each parameter qxi,y to the noisy-MAX, we introduce a corresponding parameter variable P xi,y. The weight of each parameter variable is given by,\nweight(P xi,y) = q x i,y weight(\u00acP xi,y) = 1\nwhere 1 \u2264 i \u2264 n, 0 \u2264 y \u2264 dY \u2212 1, and 1 \u2264 x \u2264 dXi \u2212 1. The relation between Xi and Y is represented by the parameter clauses1,\n(Ii,x \u2227 Ii,y) \u21d4 P xi,y where 1 \u2264 i \u2264 n, 0 \u2264 y \u2264 dY \u2212 1, and 1 \u2264 x \u2264 dXi \u2212 1. Example 12. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. For the noisy-MAX at node Nausea, the encoding introduces the indicator variables IC,Na, IC,Nm, IC,Ns, IF,Na , IF,Nm, IF,Ns, IM,Na, IM,Nm, and IM,Ns, all with weight 1, and the clauses,\n1. To improve readability, in this section the propositional formulas are sometimes written in a more natural but non-clausal form. We continue to refer to them as clauses when the translation to clause form is straightforward.\nIC,Na \u2228 IC,Nm \u2228 IC,Ns IF,Na \u2228 IF,Nm \u2228 IF,Ns IM,Na \u2228 IM,Nm \u2228 IM,Ns \u00acIC,Na \u2228 \u00acIC,Nm \u00acIF,Na \u2228 \u00acIF,Nm \u00acIM,Na \u2228 \u00acIM,Nm \u00acIC,Na \u2228 \u00acIC,Ns \u00acIF,Na \u2228 \u00acIF,Ns \u00acIM,Na \u2228 \u00acIM,Ns \u00acIC,Nm \u2228 \u00acIC,Ns \u00acIF,Nm \u2228 \u00acIF,Ns \u00acIM,Nm \u2228 \u00acIM,Ns\nAs well, the following parameter variables and associated weights would be introduced,\nweight(P 1C,Na) = 0.7 weight(P 1 F,Na ) = 0.5 weight(P 1M,Na) = 0.1 weight(P 1C,Nm) = 0.2 weight(P 1 F,Nm\n) = 0.2 weight(P 1M,Nm) = 0.4 weight(P 1C,Ns ) = 0.1 weight(P 1 F,Ns ) = 0.3 weight(P 1M,Ns) = 0.5,\nalong with the following parameter clauses,\n(IC1 \u2227 IC,Na) \u21d4 P 1C,Na (IF1 \u2227 IF,Na) \u21d4 P 1F,Na (IM1 \u2227 IM,Na) \u21d4 P 1M,Na (IC1 \u2227 IC,Nm) \u21d4 P 1C,Nm (IF1 \u2227 IF,Nm) \u21d4 P 1F,Nm (IM1 \u2227 IM,Nm) \u21d4 P 1M,Nm (IC1 \u2227 IC,Ns) \u21d4 P 1C,Ns (IF1 \u2227 IF,Ns) \u21d4 P 1F,Ns (IM1 \u2227 IM,Ns) \u21d4 P 1M,Ns\nIt remains to relate (i) the indicator variables, Ii,x, which represent the value of the parent variable Xi, where x = 0, . . . , dXi\u22121; (ii) the indicator variables, Ii,y, which represent the effect of Xi on Y , where y = 0, . . . , dY \u2212 1; and (iii) the indicator variables, IY \u2032\ny\u2032 , which\nrepresent the value of the hidden variable Y \u2032, where y\u2032 = 0, . . . , dY \u22121. Causal independent clauses define the relation between (i) and (ii) and assert that if the cause Xi is absent (Xi = 0), then Xi has no effect on Y ; i.e.,\nIi,x0 \u21d2 Ii,y0 where 1 \u2264 i \u2264 n. Value constraint clauses define the relation between (ii) and (iii) and assert that if the hidden variable Y \u2032 takes on a value y\u2032, then the effect of Xi on Y cannot be that Y takes on a higher degree or more severe value y; i.e.,\nIY \u2032 y\u2032 \u21d2 \u00acIi,Yy\nwhere 1 \u2264 i \u2264 n, 0 \u2264 y\u2032 \u2264 dY \u2212 1, and y\u2032 < y \u2264 dY \u2212 1. Example 13. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. For the noisy-MAX at node Nausea, the encoding introduces the causal independence clauses,\nIC0 \u21d2 IC,Na IF0 \u21d2 IF,Na IM0 \u21d2 IM,Na the value constraint clauses for N \u2032 = absent,\nIN \u2032a \u21d2 \u00acIC,Nm IN \u2032a \u21d2 \u00acIF,Nm IN \u2032a \u21d2 \u00acIM,Nm IN \u2032a \u21d2 \u00acIC,Ns IN \u2032a \u21d2 \u00acIF,Ns IN \u2032a \u21d2 \u00acIM,Ns\nand the value constraint clauses for N \u2032 = mild,\nIN \u2032m \u21d2 \u00acIC,Ns IN \u2032m \u21d2 \u00acIF,Ns IN \u2032m \u21d2 \u00acIM,Ns"}, {"heading": "5.2 Weighted CNF Encoding 2 for Noisy-MAX", "text": "Our second weighted model counting encoding for noisy-MAX relations (MAX2) is based on a multiplicative definition of noisy-MAX. Equation 5 states that P (Y \u2264 y | X1, . . . ,Xn) can be determined using,\nP (Y \u2264 y | X) = n\u220f\ni=1 xi =0\ny\u2211 y\u2032=0 qxii,y\u2032 . (17)\nNote that the outer operator is multiplication; hence we refer to MAX2 as a multiplicative encoding. Substituting the above into Equation 14 gives,\nP (Y = y | X1, . . . ,Xn) = y\u2211\ny\u2032=0\nMY (y, y\u2032) \u00b7 \u239b \u239c\u239c\u239d\nn\u220f i=1 xi =0 y\u2032\u2211 y\u2032\u2032=0 qxii,y\u2032\u2032\n\u239e \u239f\u239f\u23a0 . (18)\nIt is this equation that we encode into CNF. The encoding of the factorization table MY is common to both encodings and has been explained above. It remains to encode the computation for P (Y \u2264 y | X1, . . . ,Xn).\nFor each parameter qxi,y to the noisy-MAX, we introduce a corresponding parameter variable P xi,y. The weight of each parameter variable pre-computes the summation in Equation 17,\nweight(P xi,y) = y\u2211\ny\u2032=0\nqxi,y\u2032 weight(\u00acP xi,y) = 1\nwhere 1 \u2264 i \u2264 n, 0 \u2264 y \u2264 dY \u2212 1, and 1 \u2264 x \u2264 dXi \u2212 1. The relation between Xi and Y \u2032 is represented by the parameter clauses,\n(Ii,x \u2227 Iy\u2032) \u21d4 P xi,y,\nwhere 0 \u2264 y \u2264 dY \u2212 1 and 0 \u2264 x \u2264 dXi \u2212 1. Example 14. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. For the noisy-MAX at node Nausea, the following parameter variables and associated weights would be introduced,\nweight(P 1C,Na ) = 0.7 weight(P 1 F,Na ) = 0.5 weight(P 1M,Na) = 0.1 weight(P 1C,Nm) = 0.9 weight(P 1 F,Nm\n) = 0.7 weight(P 1M,Nm) = 0.5 weight(P 1C,Ns ) = 1 weight(P 1 F,Ns ) = 1 weight(P 1M,Ns ) = 1,\nalong with the following parameter clauses,\n(IC1 \u2227 IN \u2032a) \u21d4 P 1C,Na (IF1 \u2227 IN \u2032a) \u21d4 P 1F,Na (IM1 \u2227 IN \u2032a) \u21d4 P 1M,Na (IC1 \u2227 IN \u2032m) \u21d4 P 1C,Nm (IF1 \u2227 IN \u2032m) \u21d4 P 1F,Nm (IM1 \u2227 IN \u2032m) \u21d4 P 1M,Nm (IC1 \u2227 IN \u2032s) \u21d4 P 1C,Ns (IF1 \u2227 IN \u2032s) \u21d4 P 1F,Ns (IM1 \u2227 IN \u2032s) \u21d4 P 1M,Ns\nAs stated so far, the encoding is sufficient for correctly determining each entry in the full CPT representation of a noisy-MAX relation using weighted model counting. However, to\nimprove the efficiency of the encoding, we add redundant clauses. The redundant clauses do not change the set of solutions to the encoding, and thus do not change the weighted model count. They do, however, increase propagation and thus the overall speed of computation in the special case where all of the causes are absent. To this end, for each noisy-MAX node Y , we introduce an auxiliary variable IvY with weights given by,\nweight(IvY ) = 1, weight(\u00acIvY ) = 0, and we introduce the clauses,(\nn\u2227 i Ii,0\n) \u21d2 (IY \u20320 \u21d2 IvY ),\n( n\u2227 i Ii,0 ) \u21d2 (IY0 \u21d2 IvY ),\nand the clauses,( n\u2227 i Ii,0 ) \u21d2 (Iy\u2032 \u21d2 \u00acIvY ),\n( n\u2227 i Ii,0 ) \u21d2 (Iy \u21d2 \u00acIvY ),\nwhere 1 \u2264 y\u2032 \u2264 dY \u2212 1 and 1 \u2264 y \u2264 dY \u2212 1. Example 15. Consider once again the Bayesian network shown in Figure 2. For the noisy-MAX at node Nausea, an auxiliary variable IvN is introduced with weight(IvN ) = 1 and weight(\u00acIvN ) = 0 along with the following redundant clauses,\n(IC0 \u2227 IF0 \u2227 IM0) \u21d2 (IN \u2032a \u21d2 IvN ) (IC0 \u2227 IF0 \u2227 IM0) \u21d2 (INa \u21d2 IvN ) (IC0 \u2227 IF0 \u2227 IM0) \u21d2 (IN \u2032m \u21d2 \u00acIvN ) (IC0 \u2227 IF0 \u2227 IM0) \u21d2 (INm \u21d2 \u00acIvN ) (IC0 \u2227 IF0 \u2227 IM0) \u21d2 (IN \u2032s \u21d2 \u00acIvN ) (IC0 \u2227 IF0 \u2227 IM0) \u21d2 (INs \u21d2 \u00acIvN )."}, {"heading": "6. Experimental Evaluation", "text": "In this section, we empirically evaluate the effectiveness of our encodings. We use the Cachet solver2 in our experiments as it is one of the fastest weighted model counting solvers. We compare against Ace (version 2) (Chavira et al., 2005) and D\u0301\u0131ez and Gala\u0301n\u2019s (2003) approach using variable elimination.\nWe chose to compare against Ace for two reasons. First, Ace did well in the 2008 exact inference competition (no winner was declared, but Ace performed better on more classes of problems than all other entries). Second, other methods that are publicly available or that did well at the competition, such as Smile/GeNIe (Druzdzel, 2005) or Cachet using a general encoding on the full CPT representation, currently do not take any computational advantage of noisy-OR and noisy-MAX and thus would be \u201cstraw\u201d algorithms. A strength of Ace is that it does take advantage of local structure and determinism and it specifically takes advantage of the semantics of the noisy-OR and noisy-MAX to speed up computation. The comparison to Ace, while revealing, is not without its methodological difficulties however (see Section 6.4 for a discussion).\n2. http://www.cs.rochester.edu/u/kautz/Cachet/index.htm\nWe chose to compare against D\u0301\u0131ez and Gala\u0301n\u2019s (2003) approach, which consists of variable elimination applied to an auxiliary network that permits exploitation of causal independence, as they show that the approach is more efficient than previous proposals for noisy-MAX. To our knowledge, this work has not been subsequently superseded; i.e., it is still the state-of-the-art on improving variable elimination for noisy-MAX for exact inference. No implementation of D\u0301\u0131ez and Gala\u0301n\u2019s approach is publicly available, and so we implemented it ourselves. Our implementation uses algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) as the base data structure to represent conditional probability tables. Algebraic decision diagrams permit a compact representation by aggregating identical probability values and speed up computation by exploiting context-specific independence (Boutilier, Friedman, Goldszmidt, & Koller, 1996), taking advantage of determinism and caching intermediate results to avoid duplicate computation. While ADDs are more complicated than table based representations, their ability to exploit structure often yields a speed up that is greater than the incurred overhead. In fact, ADDs are currently the preferred data structure for inference in factored partially observable Markov decision processes (Shani, Brafman, Shimony, & Poupart, 2008). The variable elimination heuristic that we used is a greedy one that first eliminates all variables that appear in deterministic potentials of one variable (this is equivalent to unit propagation) and then eliminates the variable that creates the smallest algebraic decision diagram with respect to the eliminated algebraic decision diagrams. In order to avoid creating an algebraic decision diagram for each variable when searching for the next variable to eliminate, the size of a new algebraic decision diagram is estimated by the smallest of two upper bounds: (i) the cross product of the domain size of the variables of the new algebraic decision diagram and (ii) the product of the sizes (e.g., the number of nodes) of the eliminated algebraic decision diagrams.\nGood variable ordering heuristics play an important role in the success of modern DPLLbased model counting solvers. Here, we evaluate two heuristics: Variable State Aware Decaying Sum (VSADS) and Tree Decomposition Variable Group Ordering (DTree). The VSADS heuristic is one of the current best performing dynamic heuristics designed for DPLL-based model counting engines (Sang, Beame, & Kautz, 2005b). It can be viewed as a scoring system that attempts to satisfy the most recent conflict clauses and also considers the number of occurrences of a variable at the same time. Compared with the VSADS heuristic, the DTree heuristic (Huang & Darwiche, 2003) can be described as a mixed variable ordering heuristic. DTree first uses a binary tree decomposition to generate ordered variable groups. The decomposition is done prior to search. The order of the variables within a group is then decided dynamically during the backtracking search using a dynamic heuristic.\nAll of the experiments were performed on a Pentium workstation with 3GHz hyperthreading CPU and 2GB RAM."}, {"heading": "6.1 Experiment 1: Random Two-Layer Networks", "text": "In our first set of experiments, we used randomly generated two-layer networks to compare the time and space efficiency of the WMC1 and WMC2 encodings.\nBoth the WMC1 and WMC2 encodings can answer probabilistic queries using Equation 7. Both encodings lead to quick factorization given evidence during the encoding. The\nclauses from negative evidence can be represented compactly in the resulting CNF, even with a large number of parents. In the WMC2 encoding, positive evidence can be represented by just three Boolean variables (see Example 9 for an illustration of which variables are deleted and which are kept for the case of positive evidence), whereas the WMC1 encoding requires n Boolean variables, one for each parent (see Example 7). In the WMC2 encoding, we use two parameter variables (P 0Xi,Y \u2032 and P 1 Xi,Y \u2032) to represent every arc, while the WMC1 encoding only needs one."}, {"heading": "30 3686 10 0.2 30 6590 11 0.1 30 31.7 30", "text": ""}, {"heading": "35 3716 11 0.6 30 6605 11 0.2 30 32.5 30", "text": ""}, {"heading": "40 3746 13 21.4 30 6620 11 0.5 30 32.7 30", "text": ""}, {"heading": "45 3776 14 38.8 30 6635 13 2.0 30 35.7 30", "text": ""}, {"heading": "50 3806 19 75.3 30 6650 13 6.1 30 40.9 30", "text": ""}, {"heading": "55 3836 22 175.2 30 6665 16 71.0 30 166.0 30", "text": ""}, {"heading": "60 3916 24 17 6680 16 27 21", "text": "Each random network contains 500 diseases and 500 symptoms. Each symptom has six possible diseases uniformly distributed in the disease set. Table 3 shows the treewidth of the encoded CNF for the WMC1 and WMC2 encodings. The first column shows the amount of positive evidence in the symptom variables. The remainder of the evidence variables are negative symptoms. It can be seen that although the WMC1 encoding generates fewer variables than the WMC2 encoding, the CNF created by the WMC2 encoding has smaller width. The probability of evidence (PE) is computed using the tree decomposition guided variable ordering (Huang & Darwiche, 2003) and the results are compared against Ace3 (a more detailed experimental analysis is given in the next experiments)."}, {"heading": "6.2 Experiment 2: QMR-DT", "text": "In our second set of experiments, we used a Bayesian network called QMR-DT. In comparison to randomly generated problems, QMR-DT presents a real-world inference task with various structural and sparsity properties. For example, in the empirical distribution of diseases, a small proportion of the symptoms are connected with a large number of diseases (see Figure 6).\n3. http://reasoning.cs.ucla.edu/ace/\nThe network we used was aQMR-DT, an anonymized version of QMR-DT4. Symptom vectors with k positive symptoms were generated for each experiment. For each evidence vector, the symptom variables were sorted into ascending order by their parent (disease) number, the first k variables were chosen as positive symptoms, and the remaining symptom variables were set to negative. The goal of the method is to generate instances of increasing difficulty.\nWe report the runtime to answer the probability of evidence (PE) queries. We also experimented with an implementation of Quickscore5, but found that it could not solve any of the test cases shown in Figure 7. The approach based on weighted model counting also outperforms variable elimination on QMR-DT. The model counting time for 2560 positive symptoms, when using the WMC1 encoding and the VSADS dynamic variable ordering heuristic, is 25 seconds. This same instance could not be solved within one hour by variable elimination.\nWe tested two different heuristics on each encoding: the VSADS dynamic variable order heuristic and DTree (Huang & Darwiche, 2003), the semi-static tree decomposition-based heuristic. The runtime using an encoding and the DTree heuristic is the sum of two parts: the preprocessing time by DTree and the runtime of model counting on the encoding. In this experiment, DTree had a faster runtime than VSADS in the model counting process. However, the overhead of preprocessing for large size networks is too high to achieve better overall performance.\nThe WMC2 encoding generates twice as many variables as the WMC1 encoding. Although the WMC2 encoding is more promising than the WMC1 encoding on smaller size\n4. http://www.utoronto.ca/morrislab/aQMR.html 5. http://www.cs.ubc.ca/\u223cmurphyk/Software/BNT/bnt.html\nnetworks (see Table 3), here the WMC2 encoding is less efficient than the WMC1 encoding. The overhead of the tree decomposition ordering on the WMC2 encoding is also higher than on the WMC1 encoding. Our results also show that dynamic variable ordering does not work well on the WMC2 encoding. Model counting using the WMC2 encoding and the VSADS heuristic cannot solve networks when the amount of positive evidence is greater than 1500 symptoms.\nThe experimental results also show that our approach is more efficient than Ace. For example, using Ace, a CNF of QMR-DT with 30 positive symptoms creates 2.8 \u00d7 105 variables, 2.8\u00d7 105 clauses and 3.8\u00d7 105 literals. Also, it often requires more than 1GB of memory to finish the compilation process. With the WMC1 encoding, the same network and the same evidence create only 4.6\u00d7104 variables, 4.6\u00d7104 clauses and 1.1\u00d7105 literals. Cachet, the weighted model counting engine, needs less than 250MB of memory in most cases to solve these instances. And in our experiments, Ace could not solve QMR-DT with more than 500 positive symptoms within an hour."}, {"heading": "6.3 Experiment 3: Random Multi-Layer Networks", "text": "In our third set of experiments, we used randomly generated multi-layer Bayesian networks. To test randomly generated multi-layer networks, we constructed a set of acyclic Bayesian networks using the same method as D\u0301\u0131ez and Gala\u0301n (2003): create n binary variables; randomly select m pairs of nodes and add arcs between them, where an arc is added from Xi to Xj if i < j; and assign a noisy-OR distribution or a noisy-MAX distribution to each node with parents.\nFigure 8 shows the effect of the number of hidden variables on the average time to answer probability of evidence (PE) queries for random noisy-OR Bayesian networks. Each data point is an average over 30 randomly generated instances, where each instance had 3000 nodes in total.\nThe results from the two layer QMR-DT and the multiple layer random noisy-OR show that on average, the approach based on weighted model counting performed significantly better than variable elimination and significantly better than Ace. All the approaches benefit from the large amount of evidence, but the weighted model counting approach explores the determinism more efficiently with dynamic decomposition and unit propagation. In comparison to variable elimination, the weighted model counting approach encodes the local dependencies among parameters and the evidence into clauses/constraints. The\ntopological or structural features of the CNF, such as connectivity, can then be explored dynamically during DPLL\u2019s simplification process.\nHeuristics based primarily on conflict analysis have been successfully applied in modern SAT solvers. However, Sang, Beame, and Kautz (2005b) note that for model counting it is often the case that there are few conflicts in those parts of the search tree where there are large numbers of solutions and in these parts a heuristic based purely on conflict analysis will make nearly random decisions. Sang et al.\u2019s (2005b) VSADS heuristic, which combines both conflict analysis and literal counting, avoids this pitfall and can be seen to work very well on these large Bayesian networks with large amounts of evidence. DTree is also a good choice due to its divide-and-conquer nature. However, when we use DTree to decompose the CNF generated from QMR-DT, usually the first variable group contains more than 500 disease variables. As well, the overhead of preprocessing affects the overall efficiency of this approach.\nSimilarly, we performed an experiment with 100 five-valued random variables. Figure 9 shows the effect of the number of arcs on the average time to answer probability of evidence (PE) queries for random noisy-MAX Bayesian networks. Each data point is an average over 50 randomly generated instances. It can be seen that on these instances our CNF encoding MAX2 out performs our encoding MAX1 and significantly outperforms Chavira, Allen, and Darwiche\u2019s Ace (2005). It has been recognized that for noisy-MAX relations, the multiplicative factorization has significant advantages over the additive factorization (Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gala\u0301n, 2003). Hence, one would expect that the\nCNF encoding based on the multiplicative factorization (encoding MAX2) would perform better than the CNF encoding based on the additive factorization (encoding MAX1). The primary disadvantage of encoding MAX1 is that it must encode in the CNF summing over all configurations. As a result, MAX1 generates much larger CNFs than MAX2, including more variables and more clauses. In encoding MAX2, the weight of a parameter variable represents the maximum effect of each cause and hence minimizes the add computations."}, {"heading": "6.4 Discussion", "text": "We experimentally evaluated our four SAT encodings\u2014WMC1 and WMC2 for noisy-OR and MAX1 and MAX2 for noisy-MAX\u2014on a variety of Bayesian networks using the Cachet weighted modeling counting solver. The WMC1 and MAX1 encodings can be characterized as additive encodings and the WMC2 and MAX2 encodings as multiplicative encodings. In our experiments, the multiplicative encodings gave SAT instances with smaller treewidth. For noisy-OR, the additive encoding (WMC1) gave smaller SAT instances than the multiplicative encoding (WMC2). For noisy-MAX, it was the reverse and the additive encoding (MAX1) gave larger SAT instances than the multiplicative encoding (MAX2). With regards to speedups, in the experiments for the noisy-OR, the results were mixed as to which encoding is better; sometimes it was WMC1 and other times WMC2. In the experiments for the noisy-MAX, the results suggest that the multiplicative encoding (MAX2) is better. Here the reduced treewidth and the reduced size of the MAX2 encoding were important, and WMC2 was able to solve many more instances.\nWe also compared against D\u0301\u0131ez and Gala\u0301n\u2019s (2003) approach using variable elimination (hereafter, D&G) and against Ace (Chavira et al., 2005). In our experiments, our approach dominated D&G and Ace with speedups of up to three orders of magnitude. As well, our approach could solve many instances which D&G and Ace could not solve within the resource limits. However, our results should be interpreted with some care for at least three reasons. First, it is well known that the efficiency of variable elimination is sensitive to the variable elimination heuristic that is used and to how it is implemented. While we were careful to optimize our implementation and to use a high-quality heuristic, there is still the possibility that a different implementation or a different heuristic would lead to different results. Second, Cachet, which is based on search, is designed to answer a single query and our experiments are based on answering a single query. However, Ace uses a compilation strategy which is designed to answer multiple queries efficiently. The compilation step can take a considerable number of resources (both time and space) which does not payoff in our experimental design. Third, although Ace can be viewed as a weighted model counting solver, we are not comparing just encodings in our experiments. As Chavira and Darwiche (2008) note, Cachet and Ace differ in many ways including using different methods for decomposition, variable splitting, and caching. As well, Ace uses other optimizations that Cachet does not, including encoding equal parameters, eclauses (a succinct way of encoding that only one literal is true in a disjunction), and structured resolution. (We refer the reader to Chavira & Darwiche, 2008 for an experimental comparison of search and compilation, and an extensive discussion of the difficulty of comparing the two approaches and their advantages and disadvantages.) Nevertheless, in our experiments we demonstrated instances of noisy-OR networks (see Figure 7) and noisy-MAX networks (see Figure 9) that could not\nbe solved at all by D&G and by Ace within the resource limits, but could be solved quite quickly by Cachet using our encodings."}, {"heading": "7. Conclusions and Future Work", "text": "Large graphical models, such as QMR-DT, are often intractable for exact inference when there is a large amount of positive evidence. We presented time and space efficient CNF encodings for noisy-OR/MAX relations. We also explored alternative search ordering heuristics for the DPLL-based backtracking algorithm on these encodings. In our experiments, we showed that together our techniques extend the model counting approach for exact inference to networks that were previously intractable for the approach. As well, while our experimental results must be interpreted with some care as we are comparing not only our encodings but also implementations of systems with conflicting design goals, on these benchmarks our techniques gave speedups of up to three orders of magnitude over the best previous approaches and scaled up to larger instances. Future work could include developing specific CNF encodings of other causal independence relations (see Koller & Friedman, 2009, pp. 175\u2013185)."}, {"heading": "Acknowledgments", "text": "A preliminary version of this paper appeared as: Wei Li, Pascal Poupart, and Peter van Beek. Exploiting Causal Independence Using Weighted Model Counting. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pages 337\u2013343, 2008. The authors wish to thank the anonymous referees for their helpful comments."}], "references": [{"title": "DPLL with caching: A new algorithm for #SAT and Bayesian inference", "author": ["F. Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "Electronic Colloquium on Computational Complexity,", "citeRegEx": "Bacchus et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2003}, {"title": "Algebraic decision diagrams and their applications", "author": ["R.I. Bahar", "E.A. Frohm", "C.M. Gaona", "G.D. Hachtel", "E. Macii", "A. Pardo", "F. Somenzi"], "venue": "In Proceedings of the 1993 IEEE/ACM International Conference on Computer-Aided Design (ICCAD-93),", "citeRegEx": "Bahar et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bahar et al\\.", "year": 1993}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Boutilier et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Exploiting evidence in probabilistic inference", "author": ["M. Chavira", "D. Allen", "A. Darwiche"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Chavira et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chavira et al\\.", "year": 2005}, {"title": "Compiling Bayesian networks with local structure", "author": ["M. Chavira", "A. Darwiche"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Chavira and Darwiche,? \\Q2005\\E", "shortCiteRegEx": "Chavira and Darwiche", "year": 2005}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Chavira and Darwiche,? \\Q2008\\E", "shortCiteRegEx": "Chavira and Darwiche", "year": 2008}, {"title": "Symbolic probabilistic inference in large BN2O networks", "author": ["B. D\u2019Ambrosio"], "venue": "In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "D.Ambrosio,? \\Q1994\\E", "shortCiteRegEx": "D.Ambrosio", "year": 1994}, {"title": "Modeling and Reasoning with Bayesian Networks. Cambridge", "author": ["A. Darwiche"], "venue": null, "citeRegEx": "Darwiche,? \\Q2009\\E", "shortCiteRegEx": "Darwiche", "year": 2009}, {"title": "A logical approach to factoring belief networks", "author": ["A. Darwiche"], "venue": "In Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning", "citeRegEx": "Darwiche,? \\Q2002\\E", "shortCiteRegEx": "Darwiche", "year": 2002}, {"title": "A machine program for theorem proving", "author": ["M. Davis", "G. Logemann", "D. Loveland"], "venue": "Communications of the ACM,", "citeRegEx": "Davis et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1962}, {"title": "A computing procedure for quantification theory", "author": ["M. Davis", "H. Putnam"], "venue": "J. ACM,", "citeRegEx": "Davis and Putnam,? \\Q1960\\E", "shortCiteRegEx": "Davis and Putnam", "year": 1960}, {"title": "Parameter adjustement in Bayes networks. The generalized noisy ORgate", "author": ["F.J. D\u0301\u0131ez"], "venue": "In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "D\u0301\u0131ez,? \\Q1993\\E", "shortCiteRegEx": "D\u0301\u0131ez", "year": 1993}, {"title": "Canonical probabilistic models for knowledge engineering", "author": ["F.J. D\u0301\u0131ez", "M.J. Druzdzel"], "venue": "Tech. rep. CISIAD-06-01,", "citeRegEx": "D\u0301\u0131ez and Druzdzel,? \\Q2006\\E", "shortCiteRegEx": "D\u0301\u0131ez and Druzdzel", "year": 2006}, {"title": "Efficient computation for the noisy MAX", "author": ["F.J. D\u0301\u0131ez", "S.F. Gal\u00e1n"], "venue": "International J. of Intelligent Systems,", "citeRegEx": "D\u0301\u0131ez and Gal\u00e1n,? \\Q2003\\E", "shortCiteRegEx": "D\u0301\u0131ez and Gal\u00e1n", "year": 2003}, {"title": "Intelligent decision support systems based on SMILE", "author": ["M.J. Druzdzel"], "venue": "Software 2.0,", "citeRegEx": "Druzdzel,? \\Q2005\\E", "shortCiteRegEx": "Druzdzel", "year": 2005}, {"title": "A causal calculus", "author": ["I.J. Good"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Good,? \\Q1961\\E", "shortCiteRegEx": "Good", "year": 1961}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "In Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Heckerman,? \\Q1989\\E", "shortCiteRegEx": "Heckerman", "year": 1989}, {"title": "Causal independence for probability assessment and inference using Bayesian networks", "author": ["D. Heckerman", "J. Breese"], "venue": "IEEE, Systems, Man, and Cyber.,", "citeRegEx": "Heckerman and Breese,? \\Q1996\\E", "shortCiteRegEx": "Heckerman and Breese", "year": 1996}, {"title": "Causal independence for knowledge acquisition and inference", "author": ["D. Heckerman"], "venue": "In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (UAI93)", "citeRegEx": "Heckerman,? \\Q1993\\E", "shortCiteRegEx": "Heckerman", "year": 1993}, {"title": "Some practical issues in constructing belief networks", "author": ["M. Henrion"], "venue": "In Proceedings of the Third Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Henrion,? \\Q1987\\E", "shortCiteRegEx": "Henrion", "year": 1987}, {"title": "A structure-based variable ordering heuristic for SAT", "author": ["J. Huang", "A. Darwiche"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Huang and Darwiche,? \\Q2003\\E", "shortCiteRegEx": "Huang and Darwiche", "year": 2003}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Initial experiments in stochastic satisfiability", "author": ["M.L. Littman"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence", "citeRegEx": "Littman,? \\Q1999\\E", "shortCiteRegEx": "Littman", "year": 1999}, {"title": "Quick medical reference for diagnostic assistance", "author": ["R.A. Miller", "F.E. Masarie", "J.D. Myers"], "venue": "Medical Computing,", "citeRegEx": "Miller et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1986}, {"title": "A MUNIN network for the median nerve: A case study on", "author": ["K.G. Olesen", "U. Kjaerulff", "F. Jensen", "F.V. Jensen", "B. Falck", "S. Andreassen", "S.K. Andersen"], "venue": "loops. Appl. Artificial Intelligence,", "citeRegEx": "Olesen et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Olesen et al\\.", "year": 1989}, {"title": "Using causal knowledge to create simulated patient cases: The CPCS project as an extension of INTERNIST-1", "author": ["R. Parker", "R. Miller"], "venue": "In The 11th Symposium Computer Applications in Medical Care,", "citeRegEx": "Parker and Miller,? \\Q1987\\E", "shortCiteRegEx": "Parker and Miller", "year": 1987}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Combining component caching and clause learning for effective model counting", "author": ["T. Sang", "F. Bacchus", "P. Beame", "H. Kautz", "T. Pitassi"], "venue": "In Proceedings of the 7th International Conference on Theory and Applications of Satisfiability Testing (SAT04)", "citeRegEx": "Sang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2004}, {"title": "Solving Bayesian networks by weighted model counting", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Heuristics for fast exact model counting", "author": ["T. Sang", "P. Beame", "H.A. Kautz"], "venue": "In Proceedings of the 8th International Conference on Theory and Applications of Satisfiability Testing", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Efficient ADD operations for point-based algorithms", "author": ["G. Shani", "R.I. Brafman", "S.E. Shimony", "P. Poupart"], "venue": "In Proceedings of the Eighteenth International Conference on Automated Planning and Scheduling", "citeRegEx": "Shani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shani et al\\.", "year": 2008}, {"title": "Multiplicative factorization of noisy-MAX", "author": ["M. Takikawa", "B. D\u2019Ambrosio"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Takikawa and D.Ambrosio,? \\Q1999\\E", "shortCiteRegEx": "Takikawa and D.Ambrosio", "year": 1999}, {"title": "Knowledge engineering for Bayesian networks: How common are noisy-MAX distributions in practice", "author": ["A. Zagorecki", "M.J. Druzdzel"], "venue": "In Proceedings of the 10th European Conference on Artificial Intelligence", "citeRegEx": "Zagorecki and Druzdzel,? \\Q1992\\E", "shortCiteRegEx": "Zagorecki and Druzdzel", "year": 1992}, {"title": "Exploiting causal independence in Bayesian network inference", "author": ["N.L. Zhang", "D. Poole"], "venue": "J. of Artificial Intelligence Research,", "citeRegEx": "Zhang and Poole,? \\Q1996\\E", "shortCiteRegEx": "Zhang and Poole", "year": 1996}], "referenceMentions": [{"referenceID": 26, "context": "A Bayesian network consists of a directed acyclic graph where the nodes represent the random variables and each node is labeled with a conditional probability table (CPT) that represents the strengths of the influences of the parent nodes on the child node (Pearl, 1988).", "startOffset": 257, "endOffset": 270}, {"referenceID": 26, "context": "This presents a practical difficulty and has led to the introduction of patterns for CPTs that require one to specify many fewer parameters (e.g., Good, 1961; Pearl, 1988; D\u0301\u0131ez & Druzdzel, 2006).", "startOffset": 140, "endOffset": 195}, {"referenceID": 15, "context": "Perhaps the most widely used patterns in practice are the noisy-OR relation and its generalization, the noisy-MAX relation (Good, 1961; Pearl, 1988).", "startOffset": 123, "endOffset": 148}, {"referenceID": 26, "context": "Perhaps the most widely used patterns in practice are the noisy-OR relation and its generalization, the noisy-MAX relation (Good, 1961; Pearl, 1988).", "startOffset": 123, "endOffset": 148}, {"referenceID": 6, "context": "A more fruitful approach for solving such networks is to take advantage of the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen, Kjaerulff, Jensen, Jensen, Falck, Andreassen, & Andersen, 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira, Allen, & Darwiche, 2005).", "startOffset": 161, "endOffset": 405}, {"referenceID": 10, "context": "As well, Zagorecki and Druzdzel (1992) show that in three real-world Bayesian networks, noisy-OR/MAX relations were a good fit for up to 50% of the CPTs in these networks and that converting some CPTs to noisy-OR/MAX relations gave good approximations when answering probabilistic queries.", "startOffset": 23, "endOffset": 39}, {"referenceID": 7, "context": "In this section, we review noisy-OR/MAX relations and the needed background on weighted model counting approaches to exact inference in Bayesian networks (for more on these topics see, for example, Koller & Friedman, 2009; Darwiche, 2009; Chavira & Darwiche, 2008).", "startOffset": 154, "endOffset": 264}, {"referenceID": 26, "context": "Similarly, in Pearl\u2019s (1988) decomposed model, one only has to specify n probabilities to fully specify the model (see Figure 4); i.", "startOffset": 14, "endOffset": 29}, {"referenceID": 26, "context": "Figure 4: Pearl\u2019s (1988) decomposed form of the noisy-OR relation.", "startOffset": 10, "endOffset": 25}, {"referenceID": 15, "context": "The noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; D\u0301\u0131ez, 1993) is a generalization of the noisy-OR to non-Boolean domains.", "startOffset": 23, "endOffset": 80}, {"referenceID": 19, "context": "The noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; D\u0301\u0131ez, 1993) is a generalization of the noisy-OR to non-Boolean domains.", "startOffset": 23, "endOffset": 80}, {"referenceID": 11, "context": "The noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; D\u0301\u0131ez, 1993) is a generalization of the noisy-OR to non-Boolean domains.", "startOffset": 23, "endOffset": 80}, {"referenceID": 8, "context": "In particular, exact inference in Bayesian networks can be reduced to the weighted model counting of CNFs (Darwiche, 2002; Littman, 1999; Sang et al., 2005a).", "startOffset": 106, "endOffset": 157}, {"referenceID": 22, "context": "In particular, exact inference in Bayesian networks can be reduced to the weighted model counting of CNFs (Darwiche, 2002; Littman, 1999; Sang et al., 2005a).", "startOffset": 106, "endOffset": 157}, {"referenceID": 24, "context": "A more fruitful approach for solving such networks is to take advantage of the structure or the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005).", "startOffset": 178, "endOffset": 359}, {"referenceID": 6, "context": "A more fruitful approach for solving such networks is to take advantage of the structure or the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005).", "startOffset": 178, "endOffset": 359}, {"referenceID": 3, "context": "A more fruitful approach for solving such networks is to take advantage of the structure or the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005).", "startOffset": 178, "endOffset": 359}, {"referenceID": 16, "context": "Quickscore (Heckerman, 1989) was the first efficient exact inference algorithm for Booleanvalued two-layer noisy-OR networks.", "startOffset": 11, "endOffset": 28}, {"referenceID": 3, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005). Quickscore (Heckerman, 1989) was the first efficient exact inference algorithm for Booleanvalued two-layer noisy-OR networks. Chavira, Allen and Darwiche (2005) present a method for multi-layer noisy-OR networks and show that their approach is significantly faster than Quickscore on randomly generated two-layer networks.", "startOffset": 123, "endOffset": 307}, {"referenceID": 6, "context": "Many alternative methods have been proposed to decompose a noisy-OR/MAX by adding hidden or auxiliary nodes and then solving using adaptations of variable elimination or tree clustering (e.g., Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003).", "startOffset": 186, "endOffset": 307}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators.", "startOffset": 8, "endOffset": 124}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators.", "startOffset": 8, "endOffset": 470}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering.", "startOffset": 8, "endOffset": 772}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR.", "startOffset": 8, "endOffset": 1189}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR. By introducing one auxiliary variable Y \u2032, D\u0301\u0131ez\u2019s method leads to a complexity of O(nd2) for singly connected networks, where n is the number of causes and d is the size of the domains of the random variables. However, for networks with loops it needs to be integrated with local conditioning. Takikawa and D\u2019Ambrosio (1999) proposed a similar multiplicative factorization approach.", "startOffset": 8, "endOffset": 1559}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR. By introducing one auxiliary variable Y \u2032, D\u0301\u0131ez\u2019s method leads to a complexity of O(nd2) for singly connected networks, where n is the number of causes and d is the size of the domains of the random variables. However, for networks with loops it needs to be integrated with local conditioning. Takikawa and D\u2019Ambrosio (1999) proposed a similar multiplicative factorization approach. The complexity of their approach is O(max(2d, nd2)). However, Takikawa and D\u2019Ambrosio\u2019s approach allows more efficient elimination orderings in the variable elimination algorithm, while D\u0301\u0131ez\u2019s method enforces more restrictions on the orderings. More recently, D\u0301\u0131ez and Gal\u00e1n (2003) proposed a multiplicative factorization that improves on this previous work, as it has the advantages of both methods.", "startOffset": 8, "endOffset": 1901}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR. By introducing one auxiliary variable Y \u2032, D\u0301\u0131ez\u2019s method leads to a complexity of O(nd2) for singly connected networks, where n is the number of causes and d is the size of the domains of the random variables. However, for networks with loops it needs to be integrated with local conditioning. Takikawa and D\u2019Ambrosio (1999) proposed a similar multiplicative factorization approach. The complexity of their approach is O(max(2d, nd2)). However, Takikawa and D\u2019Ambrosio\u2019s approach allows more efficient elimination orderings in the variable elimination algorithm, while D\u0301\u0131ez\u2019s method enforces more restrictions on the orderings. More recently, D\u0301\u0131ez and Gal\u00e1n (2003) proposed a multiplicative factorization that improves on this previous work, as it has the advantages of both methods. We use their auxiliary graph as the starting point for the remaining three of our CNF encodings (WMC2, MAX1, and MAX2). In our experiments, we perform a detailed empirical comparison of their approach using variable elimination against our proposals on large Bayesian networks. In our work, we build upon the DPLL-based weighted model counting approach of Sang, Beame, and Kautz (2005a). Their general encoding assumes full CPTs and yields a parameter clause for each CPT parameter.", "startOffset": 8, "endOffset": 2407}, {"referenceID": 27, "context": "For those nodes with full CPTs, s determines the correct entry in each CPT by Lemma 2 in Sang et al. (2005a) and for those nodes with noisy-ORs, s determines the correct probability by Lemma 1 above.", "startOffset": 89, "endOffset": 109}, {"referenceID": 22, "context": "As Sang et al. (2005a) note, the weighted model counting approach supports queries and evidence in arbitrary propositional form and such queries are not supported by any other exact inference method.", "startOffset": 3, "endOffset": 23}, {"referenceID": 4, "context": "Our WMC1 encoding for noisy-OR is essentially similar to a more indirect but also more general proposal by Chavira and Darwiche (2005) (see Darwiche, 2009, pp.", "startOffset": 107, "endOffset": 135}, {"referenceID": 3, "context": "Third, in our encoding there are a linear number of clauses conditioned on IY whereas in the Chavira et al. encoding there are 2n\u22121 clauses. We note, however, that Chavira, Allen, and Darwiche (2005) discuss a direct translation of a noisy-OR to CNF based on Pearl\u2019s decomposition that is said to compactly represent the noisy-OR (i.", "startOffset": 93, "endOffset": 200}, {"referenceID": 11, "context": "Our second weighted model encoding method (WMC2) takes as its starting point D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) directed auxiliary graph transformation of a Bayesian network with a noisy-OR/MAX relation.", "startOffset": 77, "endOffset": 102}, {"referenceID": 11, "context": "Figure 5: D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) transformation of a noisy-OR relation applied to the Bayesian network shown in Figure 2.", "startOffset": 10, "endOffset": 35}, {"referenceID": 11, "context": "As with WMC2, these two noisy-MAX encodings take as their starting point D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) directed auxiliary graph transformation of a Bayesian network with noisy-OR/MAX.", "startOffset": 73, "endOffset": 98}, {"referenceID": 3, "context": "We compare against Ace (version 2) (Chavira et al., 2005) and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 35, "endOffset": 57}, {"referenceID": 14, "context": "Second, other methods that are publicly available or that did well at the competition, such as Smile/GeNIe (Druzdzel, 2005) or Cachet using a general encoding on the full CPT representation, currently do not take any computational advantage of noisy-OR and noisy-MAX and thus would be \u201cstraw\u201d algorithms.", "startOffset": 107, "endOffset": 123}, {"referenceID": 3, "context": "We compare against Ace (version 2) (Chavira et al., 2005) and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 36, "endOffset": 87}, {"referenceID": 9, "context": "We chose to compare against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach, which consists of variable elimination applied to an auxiliary network that permits exploitation of causal independence, as they show that the approach is more efficient than previous proposals for noisy-MAX.", "startOffset": 28, "endOffset": 53}, {"referenceID": 11, "context": "Effect of amount of positive symptoms on the time to answer probability of evidence queries, for the WMC1 encoding and the DTree variable ordering heuristic, the WMC1 encoding and the VSADS variable ordering heuristic, the WMC2 encoding and the DTree variable ordering heuristic, and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 284, "endOffset": 309}, {"referenceID": 11, "context": "To test randomly generated multi-layer networks, we constructed a set of acyclic Bayesian networks using the same method as D\u0301\u0131ez and Gal\u00e1n (2003): create n binary variables; randomly select m pairs of nodes and add arcs between them, where an arc is added from Xi to Xj if i < j; and assign a noisy-OR distribution or a noisy-MAX distribution to each node with parents.", "startOffset": 124, "endOffset": 147}, {"referenceID": 11, "context": "Effect of number of hidden variables on average time to answer probability of evidence queries, for the WMC1 encoding and the VSADS variable ordering heuristic, the WMC1 encoding and the DTree variable ordering heuristic, and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 226, "endOffset": 251}, {"referenceID": 27, "context": "Sang et al.\u2019s (2005b) VSADS heuristic, which combines both conflict analysis and literal counting, avoids this pitfall and can be seen to work very well on these large Bayesian networks with large amounts of evidence.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "Effect of number of arcs on average time to answer probability of evidence queries, for the MAX1 encoding for noisy-MAX, the MAX2 encoding for noisy-MAX, and Chavira, Allen, and Darwiche\u2019s Ace (2005).", "startOffset": 178, "endOffset": 200}, {"referenceID": 6, "context": "It can be seen that on these instances our CNF encoding MAX2 out performs our encoding MAX1 and significantly outperforms Chavira, Allen, and Darwiche\u2019s Ace (2005). It has been recognized that for noisy-MAX relations, the multiplicative factorization has significant advantages over the additive factorization (Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003).", "startOffset": 142, "endOffset": 164}, {"referenceID": 3, "context": "We also compared against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination (hereafter, D&G) and against Ace (Chavira et al., 2005).", "startOffset": 119, "endOffset": 141}, {"referenceID": 6, "context": "We also compared against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination (hereafter, D&G) and against Ace (Chavira et al.", "startOffset": 25, "endOffset": 50}, {"referenceID": 3, "context": "We also compared against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination (hereafter, D&G) and against Ace (Chavira et al., 2005). In our experiments, our approach dominated D&G and Ace with speedups of up to three orders of magnitude. As well, our approach could solve many instances which D&G and Ace could not solve within the resource limits. However, our results should be interpreted with some care for at least three reasons. First, it is well known that the efficiency of variable elimination is sensitive to the variable elimination heuristic that is used and to how it is implemented. While we were careful to optimize our implementation and to use a high-quality heuristic, there is still the possibility that a different implementation or a different heuristic would lead to different results. Second, Cachet, which is based on search, is designed to answer a single query and our experiments are based on answering a single query. However, Ace uses a compilation strategy which is designed to answer multiple queries efficiently. The compilation step can take a considerable number of resources (both time and space) which does not payoff in our experimental design. Third, although Ace can be viewed as a weighted model counting solver, we are not comparing just encodings in our experiments. As Chavira and Darwiche (2008) note, Cachet and Ace differ in many ways including using different methods for decomposition, variable splitting, and caching.", "startOffset": 120, "endOffset": 1350}], "year": 2011, "abstractText": "Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference. In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations\u2014 two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two SAT encodings for noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks. On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisyOR/MAX relations and scaled up to larger networks. As well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.", "creator": "gnuplot 4.4 patchlevel 2"}}}