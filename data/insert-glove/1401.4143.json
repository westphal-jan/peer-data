{"id": "1401.4143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Convex Optimization for Binary Classifier Aggregation in Multiclass Problems", "abstract": "petitgout Multiclass problems are freakishness often decomposed nucleolar into fressange multiple binary rosentraub problems that stana are solved by scrapper individual binary classifiers whose results are lc50 integrated tauber into yotel a obeyed final answer. blithfield Various methods, including positrons all - pairs (geballe APs ), one - lewisia versus - all (chaume OVA ), and error possesses correcting output vence code (ECOC ), have brosque been masaru studied, swishing to formula decompose meeker multiclass problems makepeace into binary ostrogoth problems. However, edge little study has been made to optimally holidayed aggregate 68.64 binary problems atun to determine kadina a 3,725 final xinpei answer mainstay to the animax multiclass dazz problem. livre In this paper amidase we present ibr a obrenovi\u0107 convex antiterrorist optimization method for beteen an galeano optimal aggregation jantjes of binary troeger classifiers 67.5 to estimate peet class letsie membership ouendan probabilities in snorre multiclass ctx-5000 problems. cvw-14 We cityhopper model the garganas class membership 156 probability bimantoro as a softmax function which nigmatulin takes zoysia a 8.61 conic combination of groover discrepancies induced 80.4 by individual cinnamon binary mercat classifiers, hobeika as an input. 467.5 With this tonelli model, marrerok we lawas formulate the banham regularized capri maximum 35.96 likelihood estimation as vivier a convex optimization geum problem, which is solved by revolution the unroasted primal - wookiee dual seance interior edin point roesmanhadi method. middle-range Connections of our salable method landlord to large margin classifiers are presented, satow showing zagorski that the trockel large margin formulation pellicano can rhinns be \u03ba considered minted as miret a interrogatives limiting case of counter-proposal our convex lashof formulation. towered Numerical experiments benediction on darken synthetic p\u0159emyslid and sunnyow real - world data toc sets demonstrate 2411 that pi\u0144cz\u00f3w our spoons method ipsilateral outperforms eclipsed existing brauner aggregation cummerbund methods raisonne as istithmar well concluded as werling direct hillandale methods, in terms of wiredu the whithorn classification accuracy 1,313 and the 5-to-2 quality undertones of en-route class jarious membership wenches probability estimates.", "histories": [["v1", "Thu, 16 Jan 2014 19:49:02 GMT  (413kb)", "http://arxiv.org/abs/1401.4143v1", "Appeared in Proceedings of the 2014 SIAM International Conference on Data Mining (SDM 2014)"]], "COMMENTS": "Appeared in Proceedings of the 2014 SIAM International Conference on Data Mining (SDM 2014)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sunho park", "taehyun hwang", "seungjin choi"], "accepted": false, "id": "1401.4143"}, "pdf": {"name": "1401.4143.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sunho Park", "TaeHyun Hwang", "Seungjin Choi"], "emails": ["taehyun.hwang}@utsouthwestern.edu", "seungjin@postech.ac.kr"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\naAppeared in Proceedings of the 2014 SIAM International Conference on Data Mining (SDM 2014).\n2 S. Park et al. CONTENTS\nContents"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Preliminaries 5", "text": ""}, {"heading": "3 Convex Optimization for Binary Classifier Aggregation 6", "text": "3.1 Convex Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Primal-Dual Interior Point Method . . . . . . . . . . . . . . . . . . . . . . . . 8"}, {"heading": "4 Connections to Large Margin Classifiers 12", "text": ""}, {"heading": "5 Experiments 15", "text": "5.1 Experimental Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.2 Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 5.3 Real-World Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"}, {"heading": "6 Conclusions 27", "text": ""}, {"heading": "7 Appendix 27", "text": "7.1 Derivations of gradient and Hessian of the objective function (10) . . . . . . . 27 7.2 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 7.3 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nMachine Learning Group Department of Computer Science Pohang University of Science and Technology San 31 Hyoja-dong, Nam-gu Pohang 790-784, Korea\nConvex Binary Classifier Aggregation 3"}, {"heading": "1 Introduction", "text": "Multiclass classification is an important supervised learning problem, the goal of which is to assign data points to a finite set of K classes, which is solved by one of two different approaches (direct and indirect methods). Direct approach involves constructing a discriminant function directly for the multiclass problem. For example, the multiclass SVM [1, 2] models a K-way classifier which directly separates the correct class label from the rest of class labels in the large margin framework. Alternatively, in indirect approach, one decomposes the multiclass problem into multiple binary classification problems which are solved by individual binary classifiers whose results are integrated into a final answer. All-pairs (APs) and one-versus-all (OVA) are well-known methods for decomposing multiclass problems into binary problems.\nIn this paper we consider the indirect approach where binary-decomposition methods enjoy several advantages over direct methods in multiclass problems. It is much easier and simpler to learn a set of binary classifiers than to train one unique classifier which separates all classes simultaneously [3]. For example, a digit recognition problem can be decomposed into a set of simpler sub-problems, which can be easily solved by linear classifiers [4]. Even in such a case, the performance is comparable to that obtained by a more complex classifier. A comparison study [5] observed that the direct methods, such as multiclass SVM [1, 2], generally require more training time than binary-decomposition methods. It was also observed in [5] that APs-based decomposition methods show higher predictive accuracy than the multiclass SVM for most of cases. Moreover, in the case of binary-decomposition methods, binary classifiers can be independently trained on different processors, which is well suited to parallel processing in the training phase.\nReducing multiclass problems to multiple binary problems can be viewed as encoding, since binary codewords are assigned to class labels. Several encoding methods are widely used, including APs, OVA, and error correcting output code (ECOC) [6]. Aggregation of binary classifiers involves combining prediction results determined by binary classifiers into a final answer to the multiclass problem. Aggregation methods can be categorized into two types: hard decoding and probabilistic decoding.\nIn hard decoding, one seeks a codeword which best matches binary predictions, to determine a most probable label. Hamming distance is often used as a discrepancy measure between a codeword and binary predictions, in the case where individual binary classifiers yield binary outputs. Various loss functions (such as exponential loss and logistic loss) are considered in the case where binary classifiers yield a score whose magnitude is a measure of confidence in the prediction, referred to be as loss-based decoding [7]. In many applications, however, class membership probabilities need to be computed, which is not possible in the hard decoding. For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]). To this end, one needs to correctly calculate class membership probabilities for the given data point.\nIn probabilistic decoding, we are given binary class membership probability estimates (scores in [0,1]) determined by binary classifiers. One couples these probability estimates to determine a set of class membership probabilities for multiclass problems. In the case of APs, Hastie and Tibshirani [12] developed a method, pairwise coupling, in which pairwise class membership probability estimates are combined to form a joint probability estimates for all K classes, fitting the Bradley-Terry model [13] by minimizing a KL-divergence criterion. This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [14, 15], where a generalized Bradley-Terry model [15] was considered to relate probability estimates obtained by binary classifiers to class membership probability estimates.\nThe (generalized) Bradley-Terry model provides a natural way to relate probability estimates computed by binary classifiers to class membership probabilities, but there are some\n4 S. Park et al. 1 INTRODUCTION\ndrawbacks. Most of aforementioned methods based on the Bradley-Terry model treat all binary classifiers equally, leading to the performance degradation in the presence of bad binary classifiers. This problem is alleviated by introducing confidence weights placed on individual binary classifiers that are optimally tuned based on training data [16]. However, the method in [16] involves a huge number of parameters, NK +M , where N is the number of training data points and M is the number of binary classifiers. In other words, the computational complexity scales linearly with the number of training data points, which makes the method prohibitive even for mid-scale problems. Moreover, additional iterative optimization is required to estimate the class memberships probabilities for test data.\nTakenouchi and Ishii [17] proposed a different type of decoding method in which misclassification in binary classifier is formulated as a bit inversion error problem, as in information transmission theory. The dependency between classifiers are directly modeled by Bolzmann machine and the hard decoding problem (which can also be extended to probabilistic decoding) is formulated as a probabilistic inference problem in Bolzmann machine. The method provides a new viewpoint to the multiclass problems in the context of information transmission theory. However it involves exponential-order computational complexity, due to the partition function in the Bolzmann machine, requiring approximate inference techniques such as Monte Carlo Markov Chain (MCMC) or mean filed approximation. It might suffer from multiple local minima and is sensitive to initial conditions.\nRecently, we have developed a Bayesian aggregation method [18] for probabilistic decoding. In contrast to most of existing probabilistic decoding methods where the Bradley-Terry model was used to relate binary probability estimates to class membership probabilities, we directly modeled class membership probabilities as softmax function whose input argument is a linear combination of discrepancies induced by binary classifiers. In this way, aggregation weights are the only parameters to be tuned (M), while the existing method [16] scales linearly with the number of samples (NK +M). Based on the likelihood modeled by the softmax function and the appropriate prior on the aggregation weights, we formulated the problem of estimating aggregation weights as variational logistic regression in which predictive distribution yielded class membership probabilities. In such a case, regularization parameter was learned in Bayesian framework and over-fitting was alleviated, compared to maximum likelihood methods.\nThere are two computational issues in the Bayesian aggregation framework: (1) the solution suffers from local minima; (2) the evaluation of class membership probabilities for data instances requires additional computations (through variational optimization). To solve these problems, one can consider the maximum likelihood estimation instead of full Bayesian learning [18], in which class membership probabilities can be easily computed by evaluating the softmax function with the learned aggregation weights. In our previous work [19], we proposed the \u21131 norm regularized maximum likelihood method to determine the optimal aggregation weights, which is a convex problem. We then convert the convex optimization problem to an equivalent geometric programming in order to make use of an off-the-shelf optimization toolbox. With this approach, a global solution is determined and class membership probabilities can be easily evaluated without additional optimizations. However, our previous method [19] still has several limitations: (1) the optimization problem can be directly solved by the standard convex optimization algorithms without transforming it to geometric programming; (2) it only allows \u21131 norm regularization. In contrast to [19] where the problem was converted to geometric programming, we directly solve the optimization problem using primal-dual interior point method that is an efficient solver for convex optimization problems, which allows us to use various types of regularization. Especially, when \u21132 norm regularization considered, we can provide an interesting connection of our method to the large margin formulation. The main contribution of this paper is summarized below.\nConvex Binary Classifier Aggregation 5\n\u2022 Our method is more computationally efficient than the existing probabilistic decoding methods. In our formulation, the aggregation weights are the only parameters to be tuned (M), while the existing method [16] scales linearly with the number of samples (NK +M).\n\u2022 We formulate the regularized maximum likelihood estimation as a convex optimization, so a global solution is found. We use the primal-dual interior point method to solve this optimization problem.\n\u2022 Connections of our method to large margin classifiers are presented, showing that the large margin formulation can be considered as a limiting case of our convex formulation. Moreover, we present data-dependent generalization error bound, based on margins and Rademacher complexity, extending existing work on binary problems [20] to our multiclass problems which are solved by aggregating binary solutions.\nThe rest of this paper is organized as follows. The next section describes notations and preliminaries which are needed to explain our method. Section 3 provides the main contribution, in which we describe our model and show how an optimal aggregation of binary classifiers is formulated as a convex optimization, which is solved by the primal-dual interior point method. Connections to large margin classifiers and generalization error bound are described in Section 4. Experiments on synthetic and real-world data sets are provided in Section 5, demonstrating that our method outperforms existing aggregation methods in terms of the classification accuracy and the quality of class membership probabilities. Finally conclusions are drawn in Section 6. In addition, the appendix provides details about the proof of propositions in Section 4."}, {"heading": "2 Preliminaries", "text": "We are given N training examples {(xi, yi)} N i=1, where xi \u2208 X \u2282 R D are data vectors and yi \u2208 Y = {1, . . . ,K} (K \u2265 3) are class labels associated with xi. Multiclass prediction involves estimating the class membership probabilities of xi,\nPk,i , P (yi = k |xi), (1)\nfor k = 1, . . . ,K, and i = 1, . . . , N. A class label for xi is determined by\ny\u0302i = argmax k Pk,i.\nWe denote by pi = [P1,i, . . . , PK,i] \u22a4 \u2208 RK the class membership probability vector for data point xi. We also define the data matrix as X = [x1, . . . ,xN ] and the class label vector as y = [y1, ..., yN ]\n\u22a4. Multiclass problems are decomposed into a set of binary problems that are solved by individual binary classifiers. Such decomposition can be viewed as encoding and various methods are widely used:\n\u2022 OVA involves a set of K binary functions, each of which discriminates one class from the other classes.\n\u2022 APs learns a set of K(K\u22121)2 binary classifiers, each of which distinguishes each pair of classes.\n\u2022 ECOC assigns a binary codeword to each class such that Hamming distances between codewords are maximized (to increase the separability) and the length of codewords determines the number of binary functions to be learned.\n6 S. Park et al.3 CONVEX OPTIMIZATION FOR BINARY CLASSIFIER AGGREGATION\nAforementioned encoding methods yield a code matrix C = [Cj,k] \u2208 R M\u00d7K where M is the number of binary classifiers involved and K is the number of class labels. For instance, Table 1 shows the 3\u00d73 code matrix for a 3-class problem in the case of APs coding. According to the code matrix, multiclass problem is reduced to a set of binary problems that are solved independently. Each column in the code matrix C, denoted by ci, corresponds to codeword, while each row defines a binary problem to be solved by a binary classifier (BCi). For instance, BC2 discriminates class 2 from class 3, while samples in class 1 is not used.\nGiven the code matrix C, the jth binary classifier is trained using examples {(xi, Cj,yi)}, where binary values of target Cj,yi , associated with data xi, are determined by the code matrix. For instance, in the case of the 2nd binary classifier in Table 1, the binary target value for xi is C2,2 = 1 when xi belongs to \u2019class 2\u2019 and is C2,3 = 0 if xi belongs to \u2019class 3\u2019.\nWe assume that each binary classifier yields a probabilistic prediction, the value of which ranges between 0 and 1. For example, we can use probabilistic SVM [21]. We denote by Qj,i the probabilistic prediction by binary classifier j for the class label of xi:\nQj,i , P (Cj,yi = 1 |xi), (2)\nfor j = 1, . . . ,M, and i = 1, . . . , N. We denote by qi = [Q1,i, . . . , QM,i] \u22a4 \u2208 RM the probability estimates computed by M binary classifiers for data point xi. In the paper, our goal is to estimate class membership probabilities pi using a collection of binary classifiers\u2019 probability estimates, qi."}, {"heading": "3 Convex Optimization for Binary Classifier Aggrega-", "text": "tion\nIn this section we present our main contribution, convex formulation, where an optimal aggregation of binary classifiers into a final answer to multiclass problems is formulated as a convex optimization, which is solved by the primal-dual interior point method. We make use of the softmax function to relate class membership probabilities with binary probability estimates. The softmax model takes a conic combination the discrepancies between codewords and the probability estimates of binary classifiers, as input arguments, to represent class membership probabilities. This approach provides a simple model to evaluate class membership probabilities, compared to the (generalized) Bradley-Terry model-based method [16]."}, {"heading": "3.1 Convex Formulation", "text": "Given probabilistic predictions of binary classifiers, qi, for data point xi, we evaluate which codeword ck is closest to qi in the sense of a pre-specified discrepancy measure, in order\n3.1 Convex Formulation Convex Binary Classifier Aggregation 7\nto guess a class label for xi. To this end, we define the discrepancy \u03c1(ck, qi,w) as a conic combination of errors induced by M binary classifiers:\n\u03c1(ck, qi,w) = M\u2211\nj=1\nwj d(Cj,k, Qj,i), (3)\nwhere\nd(Cj,k, Qj,i) = \u2212Cj,k logQj,i \u2212 (1\u2212 Cj,k) log(1 \u2212Qj,i), (4)\nis the cross-entropy error function for two classes where the model probability for membership of one class is Qj,i and the corresponding true probability is Cj,k, while the model probability for membership of the other class is 1\u2212Qj,i and the corresponding true probability is 1\u2212Cj,k. Coefficients wj \u2265 0 for j = 1, . . . ,M are aggregation weights. In the case of Cj,k = \u25b3, we do not care what a probability estimate of the corresponding binary classifier yields, so we set d(\u25b3, Qj,i) = 0. Our method to be described below is not restricted to the case of cross-entropy error function. For any proper loss function, it holds. For instance, we can also choose the exponential loss function that was used in loss-based decoding [7]\nde(Cj,k, Qj,i) = exp { \u2212C\u0303j,k(Qj,i \u2212 1/2) } , (5)\nwhere C\u0303j,k=1,-1, or 0, when Cj,k=1, 0, or \u25b3, respectively. We define aggregation weight vector as w = [w1, . . . , wM ]\n\u22a4 \u2208 RM . Given data point xi and the probabilistic predictions qi determined by M binary classifiers, we model class membership probability using the softmax function that takes the form:\nP (yi = k |w,xi) = exp {\u2212\u03c1(ck, qi,w)}\u2211K j=1 exp {\u2212\u03c1(cj , qi,w)} , (6)\nwhere \u03c1(ck, qi,w) is given in (3). The index k yielding the smallest \u03c1(ck, qi,w) leads to the highest class membership probability. The prediction based on the loss-based decoding [7] is a special case of our model. Fixing aggregation weights with w1 = \u00b7 \u00b7 \u00b7 = wM = 1/M , the prediction y\u0302i = argmaxk P (yi = k |w,xi) under the model (6) with the exponential loss function (5) leads to the results determined by the loss-based decoding. In contrast, as will be explained below, we attempt to optimally tune aggregation weights using a convex optimization.\nWe re-arrange the class membership model probability (6) by multiplying its numerator and denominator by exp {\u03c1(ck, qi,w)}:\nP (yi = k |w,xi) = 1\n\u2211K j=1 exp { w\u22a4\u03d5 j,k i } , (7)\nwhere \u03d5j,ki \u2208 R M are M -dimensional vectors, the lth entry of which is given by\n[\u03d5j,ki ]l = d(Cl,k, Ql,i)\u2212 d(Cl,j , Ql,i). (8)\nThat is, \u03d5j,ki contains differences between two discrepancies, each of which is induced when qi is compared to codewords ck and cj , respectively. With these relations (7) and (8), we write the likelihood as\np(y |w,X)\n=\nN\u220f\ni=1\nK\u220f\nk=1\n  1\u2211K\nj=1 exp { w\u22a4\u03d5 j,k i }\n  \u03b4(k,yi)\n, (9)\n8 S. Park et al.3 CONVEX OPTIMIZATION FOR BINARY CLASSIFIER AGGREGATION\nwhere \u03b4(k, j) is the Kronecker delta which equals 1 if j = k and otherwise 0. We impose \u21132 norm regularization on the aggregation weight vector w and consider the negative log-likelihood, leading to the following minimization problem:\nminimize f0(w)\nsubject to wj \u2265 0, j = 1, . . . ,M, (10)\nwhere\nf0(w) , \u2212 1\nN log p(y |w,X) +\n\u03bb 2 \u2016w\u201622\n= 1\nN\nN\u2211\ni=1\nlog\n  K\u2211\nj=1\nexp { w\u22a4\u03d5\nj,yi i\n}  + \u03bb\n2\nM\u2211\nj=1\nw2j ,\nwhere \u03bb > 0 is a regularization parameter. Note that the term log (\u2211K j=1 exp { w\u22a4\u03d5 j,yi i }) associated with data point xi in the objective function (11) is called log-sum-exp which is a well known convex function used for geometric programming [22, 23]. The objective function and constraints wj \u2265 0 for j = 1, . . . ,M are convex in our formulation (10), so we apply a convex optimization method to determine optimal aggregation weights w.\nNote that maximum likelihood estimation is often interpreted as the minimization of Kullback-Leibler (KL) divergence between oracle and model. We define the true class label matrix as T = [Tk,i] \u2208 R\nK\u00d7N , where each column vector ti follows the 1-of-K encoding to represent true class label for xi, such that only element associated with yi is 1 and all remaining elements equal 0. We denote by pwi \u2208 R\nK the K-dimensional class membership model probability vector given the aggregation weight vector w, in which the kth element is p(yi = k |w,xi) in (6). With these definitions, we write the KL-divergence between the oracle and the model as\nN\u2211\ni=1\nKL [ ti \u2016p w i ] = N\u2211\ni=1\nK\u2211\nk=1\nTk,i log Tk,i\np(yi = k |w,xi)\n=\nN\u2211\ni=1\nlog\n( K\u2211\nk=1\nexp { w\u22a4\u03d5\nk,yi i\n}) . (11)\nIt follows from (11) and (9) that the maximization of the likelihood (9) (with the regularization ignored) equals the minimization of the KL-divergence (11). Thus the optimal aggregation weightw\u22c6 determined by the maximum likelihood estimation enforces P (yi = k |w\n\u22c6,xi) to be as close as to 1 for k = yi and 0 for k 6= yi. We can also easily predict class labels for test data points, by evaluating corresponding class membership model probabilities (6) using the optimal aggregation weight vector."}, {"heading": "3.2 Primal-Dual Interior Point Method", "text": "We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w\u22c6. The primal-dual interior point method exhibits better than linear convergence and outperforms the standard interior point methods in most of applications such as linear, quadratic, geometric and semidefinite programming [22]. We explain the primal-dual interior point method briefly in this section to make our paper self-contained and the algorithm is outlined in Algorithm 1. Readers who are familiar with the primal-dual interior point method can skip this section and more details can be found in [22].\n3.2 Primal-Dual Interior Point Method Convex Binary Classifier Aggregation 9\nWe first examine Karush-Kuhn-Tucker (KKT) optimality conditions for the problem (10). We denote dual variables by z = [z1, . . . , zM ]\n\u22a4 (zj \u2265 0 for j = 1, . . . ,M). Then the Lagrangian is written as\nL(w, z) , f0(w)\u2212\nM\u2211\nj=1\nzjwj . (12)\nKKT optimality conditions are given by\nwj \u2265 0, j = 1, ...,M, (13)\nzj \u2265 0, j = 1, ...,M, (14)\n\u2207f0(w)\u2212 z = 0, (15)\nzjwj = 0, j = 1, ...,M, (16)\nwhere \u2207f0(w) represents the gradient of f0(w) with respect to w. One can easily see that Slater\u2019s constraint qualification holds for the problem (10), since any point on the positive orthant (w \u2208 RM++) could be a strictly feasible solution to the problem [22]. In such a case, there exist optimal primal-dual points satisfying the KKT conditions (13) - (16) and the optimal duality gap is zero. We define w\u22c6 and z\u22c6 to be optimal primal and dual points, respectively. Then we have\n\u03b7 , f0(w \u22c6)\u2212 g(z\u22c6) = 0, (17)\nwhere g(z) is the Lagrange dual function, i.e., g(z) , infw L(w, z). The primal-dual interior point method finds the optimal primal solution w\u22c6 and dual solutionz\u22c6, which satisfy the KKT conditions (13) - (16).\nWe augment the objective function f0(w) by a logarithmic barrier [24] such that the constrained optimization problem (10) is converted to an unconstrained optimization:\nminimize f0(w)\u2212 \u00b5\nM\u2211\nj=1\nlog(wj), (18)\nwhere \u00b5 is the barrier parameter. The accuracy of approximation increases as the barrier parameter \u00b5 approaches zero. The (primal-dual) interior point methods solve the barrier subproblems for a sequence of the barrier parameters {\u00b5} that converge to 0. The logarithmic barrier penalizes the points that are close to zero, so the primal solution for each barrier sub-problem is strictly feasible, i.e., w(\u00b5) \u227b 0 (where w(\u00b5) represents the solution to the optimization (18) when a fixed value of \u00b5 is given and \u227b 0 means that each entry in vector is greater than 0) and eventually converges to the optimal solution as \u00b5 approaches 0.\nThe optimality conditions for the barrier sub-problem (18) can be interpreted as the perturbed KKT conditions. Given the barrier parameter \u00b5, the optimality conditions for (18) are given by\n\u2207f0(w)\u2212 \u00b5w \u22121 = 0. (19)\nwhere w\u22121 = [1/w1, ..., 1/wM ] \u22a4 \u2208 RM . Comparing (19) and (15), we have zj(\u00b5) = \u00b5/wj(\u00b5), where z(\u00b5) is the dual solution for the barrier sub-problem when \u00b5 is given. With w \u227b 0, the optimality conditions for the barrier sub-problem (18) are equivalently expressed as\nzj \u2265 0, j = 1, ...,M, (20)\n\u2207f0(w)\u2212 z = 0, (21)\nzjwj = \u00b5, j = 1, ...,M. (22)\n10 S. Park et al.3 CONVEX OPTIMIZATION FOR BINARY CLASSIFIER AGGREGATION\nThe main difference between these conditions and the KKT conditions (13), (14), (15), and (16) is in complementary slackness conditions, i.e., zjwj = 0 is replaced with zjwj = \u00b5. Furthermore these conditions (20), (21), and (22) can explain that w(\u00b5) converges to the optimal solution as \u00b5 approaches zero:\nf0(w(\u00b5))\u2212 p \u22c6 \u2264\nM\u2211\nj=1\nzj(\u00b5)wj(\u00b5)\n= \u00b5M, (23)\nwhere p\u22c6 is a dual optimum, p\u22c6 , supz g(z) = g(z \u22c6). We use \u03b7\u0302 = z(\u00b5)\u22a4w(\u00b5) to measure the duality gap of the barrier sub-problem with the given \u00b5. The iterative update rule for primal-dual interior point method is derived by approximately solving the sequence of the perturbed KKT conditions (20), (21), and (22) using the Newton method. Given the barrier parameter \u00b5, the method tries to compute the Newton step at the current solutions, w(\u00b5) and z(\u00b5). With abuse of notations, we denote the current primal and dual variables by w and z without \u00b5. Then, it follows from the perturbed KKT conditions (20), (21), and (22) that the residual r\u00b5(w, z) is defined as\nr\u00b5(w, z) =\n[ \u2207f0(w)\u2212 z\ndiag(z)w \u2212 \u00b51\n] , (24)\nwhere diag(\u00b7) takes a vector and return a diagonal matrix with entries of the vector placed on the diagonal, and 1 is the vector of all ones. The residual is not necessary 0 at each iteration, except in the limits as the algorithm converges [22]. With a first order approximation of r\u00b5(w, z)=0, we can obtain the Newton step by solving the following linear equations\n[ \u22072f0(w) \u2212I diag(z) diag(w) ] [ \u2206w \u2206z ] = \u2212 [ \u2207f0(w)\u2212 z diag(z)w \u2212 \u00b51 ] , (25)\nwhere I is an M \u00d7M identity matrix. Calculating the Newton step is further simplified by eliminating the variable \u2206z that can be expressed as\n\u2206z = \u2212diag(w)\u22121diag(z)\u2206w \u2212 diag(w)\u22121 ( diag(z)w \u2212 \u00b51 ) . (26)\nSubstituting this into (25), the linear equations are simplified as\nH\u2206w = \u2212g, (27)\nwhere\nH = \u22072f0(w) + diag(z)[diag(w)] \u22121,\ng = \u2207f0(w)\u2212 \u00b5w \u22121.\nNote that the derivations of gradient and Hessian of the objective function (10), \u2207f0(w) and \u22072f0(w), are provided in Appendix 7.1. We use the preconditioned conjugate gradient (PCG) to solve the linear system (27). Denoting by P \u2208 RM\u00d7M the pre-conditioning matrix, the PCG algorithm yields an approximate solution within a smaller number of steps than M , when P\u22121/2HP\u22121/2 has just a few extreme eigenvalues. As proposed in [27], we construct P as a diagonal matrix in which the diagonal entries are set to that of H .\nGiven the Newton steps, \u2206w and \u2206z, we update the primal-dual variables:\nw(\u00b5+) = w + s\u2206w, and z(\u00b5+) = z + s\u2206z, (28)\nwhere s is a step length and \u00b5+ denotes the updated barrier parameter. The step length s can be computed by a backtracking line search as described in [22]. It should be carefully\n3.2 Primal-Dual Interior Point Method Convex Binary Classifier Aggregation 11\nchosen so that w(\u00b5+) \u227b 0 and z(\u00b5+) 0 are always satisfied. To do this, we first compute smax to ensure z(\u00b5+) 0:\nsmax = sup{s \u2208 [0, 1]|z + s\u2206z 0}\n= min{1,min{\u2212zj/\u2206zj |\u2206zj < 0}}. (29)\nThen, we start the backtracking with s = 0.99smax, and multiply s by \u03b2 until we have w(\u00b5+) \u227b 0 and\n\u2016r\u00b5(w(\u00b5 +), z(\u00b5+))\u20162 \u2264 \u2016r\u00b5(w, z)\u20162(1\u2212 \u03b1s), (30)\nwhere \u03b1 is a small constant (\u03b1 = 0.01 was used in our experiments). In order to update the barrier parameter \u00b5, we use an adaptively strategies that determines it according to the reduction of the duality gap as in [27]:\n\u00b5+ = { \u03b7\u0302/(2M) if s \u2265 smin; \u00b5 otherwise.\n(31)\nwhere we use smin = 0.5. The primal-dual interior point method to solve the convex optimization problem (10) is summarized in Algorithm 1. The most dominant operation in Algorithm 1 is to compute the Newton step \u2206w at each iteration, which involves calculating Hessian matrix of the objective function, \u22072f0(w), and solving the linear system (27). Given a set of {\u03d5 j,yi i }, whose construction time is O(MNK), we can form \u22072f0(w) at a cost of O(M 2NK). For convenience, we assume that the linear system is solved by the standard matrix inversion at a cost of O(M3), while the PCG algorithm generally requires less computational cost. Thus the total cost of computing the Newton direction is O(M2NK + M3), which is the same as O(M2NK) when there are more training points than binary classifiers involved into the binary decomposition for a multiclass problem.\nAlgorithm 1: Primal-dual interior point method for convex aggregation\nData: X,y,C Result: Optimal aggregation weights w\u22c6 Binary classifications Solve the set of binary classification problems:\nobtain qi for i = 1, ..., N , compute {\u03d5k,yii } K k=1 for i = 1, ..., N by (8),\nPrimal-dual interior point method Initialize parameters\nset \u03b1 = 0.01 and \u03b2 = 0.5, \u00b5 = (w\u22a4z)/2M , set wj = 1/M, and zj = 1, for j = 1, ...,M , set tolerance parameters \u01ebfea = \u01eb = 10\n\u22124. repeat\n1. Update the barrier parameter \u00b5 using (31). 2. Calculate the Newton steps, \u2206w and \u2206z:\n- compute \u2207f0(w) and \u2207 2f0(w) (see Appendix 7.1),\n- solve r\u00b5(w, z) = 0 using by Newton method. 3. Update with line search:\n- determine the learning step s, - update the primal-dual variables by (28).\nuntil \u2016r\u00b5(w(\u00b5 +), z(\u00b5+))\u2016 \u2264 \u01ebfea and \u03b7\u0302 \u2264 \u01eb;\n12 S. Park et al. 4 CONNECTIONS TO LARGE MARGIN CLASSIFIERS"}, {"heading": "4 Connections to Large Margin Classifiers", "text": "In this section we show the connections of our convex formulation to large margin classifiers, in which the discrepancy differences {\u03d5k,yii } K k=1 induced by binary classifiers (instead of training examples xi) are inputs to a large margin classifier. Following the work in [28] where a close relation between large margin and logistic regression formulations is shown in the case of binary classification, we provide its multiclass extension in this section. We emphasize that the large margin formulation can be understood as a limiting case of our convex formulation.\nWe assume that we assign data point xi to class y\u0302i if\ny\u0302i = argmin k \u03c1(ck, qi,w),\nwhere \u03c1(ck, qi,w) = \u2211M\nj=1 wjd(Cj,k, Qj,i) is defined in (3). Then the misclassification error is given by\nE(w) = 1\nN\nN\u2211\ni=1\n1 ( y\u0302i 6= yi ) , (32)\nwhere 1(\u03c0) is the 0-1 loss function which equals 1 if the predicate \u03c0 is true, otherwise 0. A direct optimization of the misclassification error (32) is not an easy task due to the discrete nature of the 0-1 loss. The multiclass hinge loss function, which is a convex upper bound on the 0-1 loss 1 ( y\u0302i 6= yi ) [2], was considered as a surrogate function:\nE(w) \u2264 1\nN\nN\u2211\ni=1\nh ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n) , (33)\nwhere the hinge loss function h ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n) is given by\nh ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n)\n= max k\u2208Y\\yi\n[ 1\u2212 ( \u03c1(ck, qi,w)\u2212 \u03c1(cyi , qi,w) )] +\n= max k\u2208Y\n{ (1\u2212 \u03b4(yi, k)) +w \u22a4\u03d5 k,yi i } . (34)\nwhere [a]+ = max{a, 0} and \u03d5 yi,yi i = 0 is used to arrive at the second equality. To validate the inequality (33), we define margin as\n\u03bdw(xi, yi) = min k 6=yi \u03c1(ck, qi,w)\u2212 \u03c1(cyi , qi,w). (35)\nThe multiclass hinge loss function (34) yields 0 only when the margin is greater than or equal to 1. When the margin is between 0 and 1, the predicted class label y\u0302i is still correct, i.e., y\u0302i = yi but the loss 1\u2212\u03bdw(xi, yi) (which is less than 1) is produced by the hinge loss function.\nThe negative value of margin, where y\u0302i 6= yi, implies h ( \u03d5 1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w ) \u2265 1, leading to (33). Pictorial illustration is shown in Figure 1. Thus, the problem of estimating aggregation weights can be formulated as the large margin learning with the nonnegativity constraints on aggregation weights:\nminimizew fLM (w)\nsubject to wj \u2265 0, j = 1, ...,M, (36)\nConvex Binary Classifier Aggregation 13\ni ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n) = 0\nwhen mink 6=yi \u03c1(ck, qi,w) \u2212 \u03c1(cyi , qi,w) \u2265 1; (b) h ( \u03d5 1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w ) = 1 \u2212 mink 6=yi \u03c1(ck, qi,w) + \u03c1(cyi , qi,w) when 0 \u2264 mink 6=yi \u03c1(ck, qi,w) \u2212 \u03c1(cyi , qi,w) < 1; (c)\nh ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w ) \u2265 1 when mink 6=yi \u03c1(ck, qi,w)\u2212 \u03c1(cyi , qi,w) < 0.\nwhere\nfLM (w)\n= 1\nN\nN\u2211\ni=1\nh ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n) + \u03bb\n2 \u2016w\u201622.\nIn this setting, we seek an aggregation weight vector w such that the empirical misclassification is minimized, while the margin is maximized. The projected subgradient methods [29] can be applied to directly solve the primal form (36). Note that the projected subgradient method is a first-order method which exploits the gradient only, thus its performance much depends on the problem scaling or conditioning [26, 30]. On the other hand, the primal-dual interior point method used in our aggregation method is a second-order method where the gradient and Hessian information are exploited, so its performance is not affected by the problem scaling. In general, (projected) subgradient methods are slower than (primal dual) interior point methods [30]. Thus, our convex formulation (10) benefits from (primal-dual) interior point methods, compared to the large margin formulation (36).\nWe now show a close connection between our convex formulation (10) and the large margin formulation (36). To this end, we slightly modify the class membership model probability (6), introducing misclassification cost 1\u2212 \u03b4(yi, j) and stiffness parameter \u03c4 > 0:\nP (yi = k |w,xi)\n= exp\n{ \u03c4 ( (1 \u2212 \u03b4(yi, k))\u2212 \u03c1(ck, qi,w) )}\n\u2211K j=1 exp { \u03c4 ( (1\u2212 \u03b4(yi, j))\u2212 \u03c1(cj , qi,w) )} ,\n= 1\n\u2211K j=1 exp { \u03c4 ( (\u03b4(yi, k)\u2212 \u03b4(yi, j)) +w\u22a4\u03d5 j,k i )} ,\n(37)\n14 S. Park et al. 4 CONNECTIONS TO LARGE MARGIN CLASSIFIERS\nThis modification leads to the following minimization problem for estimating aggregation weights:\nminimize f\u03c4 (w),\nsubject to wj \u2265 0, j = 1, . . . ,M, (38)\nwhere\nf\u03c4 (w)\n= 1\n\u03c4N\nN\u2211\ni=1\nlog ( K\u2211\nj=1\nexp { \u03c4 ( (1\u2212 \u03b4(yi, j)) +w \u22a4\u03d5 j,yi i )}) + \u03bb\n2 \u2016w\u201622.\nThe objective function f\u03c4 (w) is nothing but the negative log-likelihood defined by the modification (37), with \u21132 norm regularization. The parameter \u03c4 controls the stiffness and 1 \u2212 \u03b4(yi, j) leads to a shift of the loss function f0(w) (11) by 1 when the Kronecker delta equals 0. Proposition 1 outlines that the large margin formulation can be interpreted as a limiting case of our convex formulation (38).\nProposition 1. The sequence of functions {f\u03c4 (w)} (\u03c4 = 1, 2, ...) uniformly converges to the objective function fLM (w) in the large margin formulation (36). That is, given any \u01eb > 0, there exists a natural number \u039e = \u039e(\u01eb) such that\n|f\u03c4 (w)\u2212 fLM (w)| < \u01eb, for \u2200 \u03c4 > \u039e and \u2200 w \u2208 R M .\nProof. See Appendix 7.2. We would like to point out a few things about our convex formulations (10) and (38),\nand the large margin formulation (36).\n\u2022 A special case of (38) when fixing \u03c4 = 1 and neglecting the misclassification loss 1\u2212 \u03b4(yi, j), leads to our original convex formulation (10).\n\u2022 In contrast to the large margin formulation (36), the convex formulation (38) allows us to calculate the gradient and Hessian, so the primal-dual interior point method can be used to find the optimal value of w, as in the case of (10), while the subgradient method is used to solve the large margin formulation (36).\n\u2022 Solving (38) requires a successive application of the primal-dual interior point method, gradually increasing the value of \u03c4 . Starting from \u03c4 = 1, the primal-dual interior point method is used to determine the optimal w. This optimal value of w is used as an initial condition at the next iteration with increasing \u03c4 .\n\u2022 In our experience with extensive numerical experiments, these three formulations (10), (36) and (38) yield similar performance in terms of classification accuracy. However, we prefer our original convex formulation (10) to others, due to its computational efficiency and implementation simplicity.\nIn addition, we present a data-dependent generalization error bound, based on the large margin formulation (36) using the Rademacher complexity [20]. Our result is an extension of existing work on binary problems [20] to the multiclass problems solved by aggregating binary solutions. To this end, we treat the margin (50) as a decision function for multiclass problems, so that a class of functions is given by F = {f : X \u00d7Y \u2192 R | f(x, y) = w\u22a4\u03d5k,yx }, where w \u2208 RM+ the aggregation weight vector and k = argmink 6=y \u03c1(ck, qx,w). Note that \u03d5k,yx can be considered as feature mapping, as in kernel methods. Applying Theorem 7 in [20] to our problem, with the empirical Rademacher complexity, yields the following proposition.\nConvex Binary Classifier Aggregation 15\nProposition 2. Let Px,y be a probability distribution on X \u00d7Y, from which (x, y) is drawn. Given \u01eb > 0, with probability \u2265 1\u2212\u01eb over training samples {(xi, yi)} N i=1 drawn independently from Px,y, for every aggregation weight vector w \u2208 R M + for \u2016w\u20162 \u2264 B,\nP (y 6= y\u0302) \u2264 1\nN\nN\u2211\ni=1\n\u03c6 ( \u03bdw(xi, yi) ) + 2B\nN\n( N\u2211\ni=1\nmin k 6=yi\n\u2016\u03d5k,yii \u2016 2 2\n)1/2 + \u221a 9 log(2/\u01eb)\n2N ,\nwhere \u03bdw(xi, yi) is the margin (50) and \u03c6(z) = min(1,max(0, 1\u2212 z)), for z \u2208 R, is the ramp loss that is a clipped version of hinge loss [31].\nProof. See Appendix 7.3. In Proposition 2, the generalization error P (y 6= y\u0302) is upper-bounded by a sum of three terms, each of which is the average empirical loss, the empirical Rademacher complexity of the function class F , and a constant term depending on \u01eb (confidence parameter) as well as the number of training samples N . Lemma 22 in [20] was used to compute the empirical Rademacher complexity in our problem. Note that the average empirical loss in Proposition 2 is not convex. Thus Replacing the ramp loss \u03c6 by the multiclass hinge loss function h (34) that is a convex upper bound on \u03c6, with regularization, yields the large margin formulation (36) which can be solved by convex optimization.\nProposition 2 theoretically supports the validity of our aggregation method, including the large margin formulation (36) and the convex formulation (38). The aggregation weights are determined by minimizing the average multiclass hinge loss, equivalently maximizing the margin. Thus, our aggregation method yields the lower generalization error, since Proposition 2 implies the larger the margin the lower the generalization error of classifiers. This is also applied to our convex formulation (10), due to its strong connection to the formulation (38). Note that our generalization error bound is similar to the ones for boosting with loss-based decoding [7], while the error bound in [7] is based on VC dimension which does not depend on sample distribution in contrast to Rademacher complexity. In addition, it also follows from Proposition 2 that our aggregation method yields the lower generalization error, compared to the loss-based decoding, because our method minimize the average multiclass hinge loss while the loss-based decoding use fixed aggregation weights (wj = 1/M for j = 1, . . . ,M)."}, {"heading": "5 Experiments", "text": "We evaluated the performance of our method on several data sets in terms of the classification accuracy and the quality of class membership probability estimates, compared to existing multiclass classification methods. They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].} We carried out numerical experiments on two synthetic and eight real-world data sets, in order to show the usefulness and high performance of our convex aggregation method."}, {"heading": "5.1 Experimental Setting", "text": "M-SVM [2] aims to directly construct a K-way classifier which separates the correct class labels from the rest of class labels by maximizing the margin defined as gyi(xi)\u2212maxk 6=yi gk(xi), where {gk} K k=1 are classification functions for each class. Thus the prediction for a new point xo is made by y\u0302o = maxk gk(xo). In the experiments, we used a linear kernel and the regularization parameter \u03bbM , which trades off the empirical misclassification error and the\n16 S. Park et al. 5 EXPERIMENTS\nmargin, was obtained by maximizing the classification accuracy on randomly chosen validation set on a parameter space \u03bbM \u2208 {10\n0, 101, ..., 106}. We used a toolbox available at http://svmlight.joachims.org/svm multiclass.html.\nLMR [32] is a variant of generalized liner model (GML) [33] that generalizes linear regression by allowing a linear model to be related with the response variables (characterized by exponential family distribution) through the response function. Especially, multinomial regression (MR) defines a linear model which is related with the categorical response variable (class labels). In this case, the response function turns out to be the class membership probability in multiclass problems:\nP (y = k|x) = exp{\u03b3k0 + \u03b3 \u22a4 k x}\u2211K\nj=1 exp{\u03b3j0 + \u03b3 \u22a4 j x}\n. (39)\nwhere parameters are \u03b3j0 \u2208 R, \u03b3j \u2208 R D, for j = 1, ...,K. LMR [32] determines the parameters by maximum likelihood with the \u21131 norm (lasso) regularization. We used a Matlab toolbox which is available at http://www-stat.stanford.edu/ tibs/glmnet-matlab/.\nFor binary-decomposition methods, we used three encoding schemes, OVA, APs and ECOC, where the code matrix C is determined as in Section 2. The most simple case is OVA encoding: the code matrix C is set to a K \u00d7 K identity matrix. In APs encoding, we learned a set of M = K(K\u22121)2 binary classifiers, each of which distinguishes each pair of classes. The code matrix for APs is a M\u00d7K rectangle matrix in which each column includes only one 1 and 0. In the case of ECOC encoding, we used two strategies to generate the code matrix C: complete code and sparse random code [7]. For K < 8, we used the complete code, yielding M = 2K\u22121\u2212 1 binary classifiers and generating a binary code matrix without don\u2019t care terms (\u25b3). For K \u2265 8, we generated a spare random code matrix as in [7], in which M = \u230815 log2 K\u2309, and entries of the code matrix are chosen as \u25b3 with probability 1/2 and 0 or 1 with probability 1/4 for each. To increase the separability between codewords, Hamming distance \u03ba between each pair of columns in C should be large. We selected the matrix with a maximum \u03ba by generating 20,000 random matrices and ensuring that each column has at least one 0 and one 1.\nWe used two linear SVMs to implement the base binary classifier, LibSVM and Liblinear [34]. In fact, the loss-based decoding and our method do not require that the binary classifier yields probability estimates. However, for fair comparison with GBTM and WMAP which are based on the probability estimates of binary classifiers, we converted the score obtained by SVM into the probability. In the case of LibSVM, Platt\u2019s sigmoid model [21] is used to calculate the binary class membership probability:\nQj,i = 1\n1 + exp {\u2212Agj(xi) +B} , (40)\nwhere gj is the function learned by the jth SVM and A,B \u2208 R are parameters are tuned by the regularized maximum likelihood framework [21, 35]. Note that, in the case of Liblinear the binary class membership probabilities are directly calculated by \u21132 norm regularized logistic regression. For linear kernel case, the regularization parameter \u03bbB , only user parameter to be set, is obtained by maximizing the classification accuracy on randomly chosen validation set on a parameter space [2\u22123, 2\u22122, ..., 23, 24].\nAs mentioned in Section 3.1, the loss-based decoding [7] can be implemented as a special case of our aggregation method, where the aggregation weights are set to w\u03031 = ... = w\u0303M = 1/M . In this setting, the method can be easily extended to probabilistic decoding. The class membership probability of the instance xi in the loss-based decoding is given by\nP (yi = k|x, w\u0303) = exp{\u2212\u03c1e(ck, qi, w\u0303)}\u2211K j=1 exp{\u2212\u03c1e(cj , qi, w\u0303)} , (41)\n5.2 Synthetic Data Convex Binary Classifier Aggregation 17\nwhere \u03c1e(ck, q, w\u0303) = 1 M \u2211M j=1 de(Cj,k, Qj,i) and de is the exponential loss defined in (5).\nWMAP [16] is an existing optimal aggregation method, which also tunes aggregation weights based on training data. The method is also based on the generalized Bradley-Terry models to connect class membership probabilities to the probability estimates obtained by binary classifiers. The aggregation weights are assigned to each classifier an learned by maximizing the objective function which represents the concordance between the class membership probability estimates and the target labels [16]. Note that, since the aggregation weights are indirectly related with the objective function, the calculation of the gradient of the objective function with respect to the aggregation weights is tricky and involves the optimization of class membership probabilities for whole training data. It usually takes too much of time to compute the gradient at each iteration, so WMAP is prohibitive even for mid-scale problems. In the experiments, we estimated the aggregation weights using WMAP only for small datasets, otherwise class membership probabilities for the test data were just estimated with the fixed aggregation weights, wj = Nj/ \u2211M j=1 Nj, where Nj is the number of training points involved in the jth binary classification problem. Some user parameters were manually set, choosing the values yielding the best performance after several trials were made.\nGBTM is also a probabilistic decoding method based on generalized Bradely-Terry models [15], which can be understood as a special case of WMAP with the uniform aggregation weighs. Similar to WMAP, class membership probabilities are computed by minimizing KL divergence between the generalized Bradely-Terry models and the probability estimates obtained by binary classifiers. However, the method does not includes the learning procedure of aggregation weights: it only provides the fixed-point type update rule for computing class membership probabilities for test points. We implemented the method according to Algorithm 2 in [15]. Both GBTM and WMAP were implemented in Matlab.\nFor our method, we need to determine the regularization parameter \u03bb in (10). To do this, we investigated the regularization path, which explains how the value of \u03bb affects the optimal solution of aggregation weights. For example, we examined \u2019Vowel\u2019 dataset from UCI repository [36], which contains 11 classes and about 1000 examples. In this case, we used LibSVM with linear kernel for the base binary classier and ECOC (sparse random code) encoding for binary-decomposition, in which the number of classifiers is \u230815 log2 K\u2309 = 52, so is the dimension of w. The regularization path to this problem is shown in Figure 2. We also reported the training classification accuracy of this dataset: the square in the figure indicates the classification accuracy for the training data at each value of \u03bb. The method showed reasonable performance for \u03bb \u2264 10\u22122. With extensive numerical experiments, we found that our method shows the stable performance for the wide range of the value of \u03bb. For simplicity, we set \u03bb = 10\u22124 for all experiments. The primal-dual interior point algorithm in Table 1 was implemented in Matlab. All experiments were run on Intel i7 quad-core 2.67GHz cpu with 12GB main memory."}, {"heading": "5.2 Synthetic Data", "text": "In this subsection, we show that how our method improves the overall classification accuracy of the loss-based decoding by adapting the aggregation weights on the given dataset. We first considered a 3 class problem, in which each class includes 100 training examples. The APs encoding was used for the binary-decomposition, so three binary classification problems were defined based on the code matrix C which equals Table 1. We assumed that one of three classifiers fails to correctly separate the given pair of classes. To realize this assumption, we directly generated the probability estimates for three binary classifiers, i.e., {Qj,i} for i = 1, ..., 300 and j = 1, ..., 3. Denote Ik by a set of indexes of the training examples with class label k. We assumed that the first and second classifiers are well designed for their purposes, but, the third classifier, BC3, was designed to fail to achieve its goal. Thus, {Qj,i}\n18 S. Park et al. 5 EXPERIMENTS\nwere generated as follows\n\u2022 from BC1 (the classifier that separates classes 1 and 2): Q1,i = 0.9 + 0.1u1,i for i \u2208 I1, Q1,i = 0.1 + 0.1u1,i for i \u2208 I2\n\u2022 from BC2 (the classifier that separates classes 1 and 3): Q2,i = 0.6 + 0.4u2,i for i \u2208 I1, Q2,i = 0.1 + 0.1u2,i for i \u2208 I3\n\u2022 from BC3 (the classifier that separates classes 2 and 3): Q3,i = 0.5 + 0.5r(v3,i)u3,i for i \u2208 I2, Q3,i = 0.5 + 0.5r(v3,1)u3,i for i \u2208 I3.\nHere, uj,i and vj,i were generated from the uniform distribution, uj,i, vj,i \u223c U[0 1] and r(a) is a binary function that is 1 if a > 0.5, otherwise -1. For each binary classifier, the probabilistic estimates for the training examples associated with unused class label \u25b3 were assumed to be generated from the uniform distribution. For example, in the case of BC1, Q1,i = u1,i for i \u2208 I3, where u1,i \u223c U[0 1]. Note that the classifier BC3 totally fails to separate classes 2 and 3.\nThe loss-based decoding method gives undesirable classification results due to the incorrect classifier, BC3. In this case, the training classification accuracy is 0.780, and the method often misclassify classes 2 and 3. We can check this point from the confusion matrix of the loss-based decoding method on this dataset, shown in Table 2.\nOur method can give the better solution for this problem by adapting the aggregation weights based on the observed data. When the results from a certain classifiers are unreliable, our method can remove the effect of this incorrect classifier on the overall classification accuracy by setting the corresponding aggregation weight to as close as to zero. We obtained the optimal aggregation weight vector w\u22c6 by solving the optimization problem (10). Figure 3 shows the progress of primal-dual interior point method for this dataset. As mentioned\n5.2 Synthetic Data Convex Binary Classifier Aggregation 19\nin Section 3.1, the initial solution of the algorithm was set to a uniform weight vector, w1 = w2 = w3 = 1/3, in which our method produces the identical prediction to the lossbased decoding. As the algorithm converges, the classification accuracy evaluated using w(\u00b5) (the solution of each iteration) increases. The final solution of our method, w\u22c6, is given by\nw\u22c6 = [8.3146, 8.2828, 0.0161]\u22a4. (42)\nAs we expected, the aggregation weight for BC3, w3, becomes close to zero. With the optimal aggregation weight vector w\u22c6, the training classification accuracy is 0.940, which is much higher than that of loss-based decoding. The confusion matrix of our method is also given in Table 3.\nWe further compared the performance of the loss-based decoding and our method, in the case where the number of classes is increased with the fixed number of training data. The data instances evenly sampled from K number of 2-dimensional Gaussian distributions, the mean vectors of which are chosen as D independent uniform [0, 20] random variables. We allowed the overlap of classes, thus the separation of classes might be harder as the number of classes is increased. For each trial 1300 samples were drawn from each Gaussian distribution, in which 300 samples were used for training and 1000 samples for test. As the number of classes increase, the data points in each class became spares. With this synthetic\n20 S. Park et al. 5 EXPERIMENTS\ndata, we can examine the performance of our method for sparse dataset varying with the number of classes. We used the Liblinear (linear SVM) for the base binary classifier.\nFigure 4 (b)-(d) represent the classification accuracy averaged over 20 independent runs, when the number of classes, K, varies from 3 to 50 in the cases of APs, OVA and ECOC, respectively. Our method improves the classification accuracy of the loss-based decoding in the all cases. In addition, we can confirm that our method well works for the case where the number of data points in each class is relatively small compared to the number of classes."}, {"heading": "5.3 Real-World Data", "text": "To compare the performance of each method on real-world problems, we used two cancer datasets and six UCI data sets [36]. The cancer data sets are acute lymphoblastic leukemia (ALL) [37] and global cancer map (GCM) [38], which were used to evaluate the performance of WMAP [16]. The detailed descriptions for the datasets are summarized in Table 4. All datasets were pre-processed such that all attributes are normalized to have unit variance, in order for attributes to reside in similar dynamic ranges. Especially, for two cancer datasets, we only selected a subset of genes as classification features: we chose 1, 000 genes as the input features by using a gene ranking based on the ratio of between group- to within group sum of squares. In addition to the cancer datasets, the input dimensionality of \u2019isolet\u2019 dataset was reduced into 25 (equals to K \u2212 1) by Fisher linear discriminant analysis in order to reduce the computational complexity without loss of classification performance.\n5.3 Real-World Data Convex Binary Classifier Aggregation 21\nIn addition to classification accuracy, we evaluated mean square error (MSE) to examine the quality of class membership probability estimates obtained by the method. For a new test point xo, we usually have a corresponding true label, but not class membership probabilities. As in [10, 39], we can assume that the class membership probabilities are given by to = [Tj,o] \u2208 R\nK where Tj,o is defined to be 1 if the label of xo is j and 0 otherwise. Then, MSE is calculated as\nMSE(xo) =\nK\u2211\nj=1\n( Tj,o \u2212 P\u0302 (yo = j|xo) )2 , (43)\nwhere P\u0302 (yo = j|xo) is the class membership probability estimated by the method. Note that MSE is also called Brier score [40], and the lower value the better performance.\nFor binary-decomposition methods, the base binary classifier was chosen according to the scale of dataset: LibSVM for the datasets {GCM, ALL}, and Liblinear [34] for other datasets. After learning of the binary classifiers, our method and WMAP were applied to learn the aggregation weights. Due to computational complexity, WMAP was applied to the small datasets, {GCM, ALL}. Otherwise, class membership probabilities for test dataset are calculated in the WMAP framework with the fixed aggregation weights. Each experiment was repeated 20 times by the random 10-fold cross-validation, in which the original data are randomly split into 10 subsets with the equal size, and then 1 subset is used for the validation data, and 9 subsets for the training data.\nTable 5 summarizes the average accuracy for the different methods, M-SVM, LMR, loss-based decoding, GBTM, WMAP, and our method. The aggregation methods with APs encoding usually show the higher classification accuracy than two direct methods. This result is consistent with the comparison study in [5] which reported that APs-based decomposition methods generally show higher predictive accuracy than the direct methods for multiclass problems, such as M-SVM. In addition, our aggregation method shows superior performance than other methods across most of cases.\nTable 6 shows the average MSE, which measures the quality of class membership probability estimate obtained by each method. At first we can check out that our method significantly improves the quality of class membership probability estimates of the loss-based decoding by tuning the aggregation weights. In addition, LMR generally shows high performance for most data sets, however, its performance does not exceed that of our method with APs encoding. We finally conclude that our aggregation method outperforms other methods, including the direct method, LMR, and two aggregation methods, GBTM and WMAP, in terms of the quality of class membership probability estimates. Note that, M-SVM is not considered in these cases due to its deterministic nature.\nWe also evaluated the performance of our method in terms of training time, reported in Table 7. As mentioned before, WMAP is prohibitive even for medium-scale problems: for GCM dataset, it averagely took 360.297 and 92.971 second to learn the aggregation weights\n22 S. Park et al. 5 EXPERIMENTS\nin AP and OVA encodings, respectively. On the other hand, our method was terminated in seconds for most cases. Furthermore, our method has an additional computational advantage over the probabilistic decoding method based on (generalized) Bredely-Terry model, such as GBTM and WMAP, which involve additional optimizations to estimate the class membership probabilities for test data. In our method they are easily calculated by evaluating the softmax function (6) with the learnt aggregation weights. For example, we compared the test time of 3 probabilistic decoding methods, GBTM, WMAP and our method, on 6 UCI datasets. As shown in Table 8, our method is remarkably faster than other methods. As a results, we can confirm the superiority of our method in terms of computational efficiency as well as classification performance. Our method becomes more useful for largescale multiclass problems which involve evaluating class membership probabilities for the data points.\n5 .3\nR ea l-W o rld D a ta\nC o n v ex B in a ry C la ssifi er A g g reg a tio n 2 3\nTable 5: Comparison of classification performance for two direct methods (M-SVM and LMR), and four aggregation methods (loss-based decoding, GBTM, WMAP, and our method), in which results are the average accuracy and the number in parenthesis represents the standard deviation. The numbers in bold face denote the best performance for each dataset.\nDataset M-SVM LMR Encoding Loss-based GBTM WMAP Our method\nGCM APs 0.650(0.097) 0.632(0.092) 0.695(0.109) 0.726(0.100)\n0.708 0.684 OVA 0.784(0.101) 0.784(0.101) 0.784(0.101) 0.795(0.090) (0.110) (0.109) ECOC 0.758(0.096) 0.771(0.101) 0.721(0.123) 0.766(0.100)\nALL APs 0.978(0.033) 0.978(0.033) 0.978(0.033) 0.972(0.035)\n0.972 0.976 OVA 0.978(0.030) 0.978(0.030) 0.976(0.030) 0.978(0.030) (0.037) (0.027) ECOC 0.980(0.028) 0.980(0.028) 0.978(0.027) 0.980(0.028)\nglass APs 0.564(0.102) 0.576(0.094) 0.564(0.099) 0.602(0.098)\n0.638 0.631 OVA 0.600(0.098) 0.600(0.098) 0.598(0.102) 0.610(0.108) (0.091) (0.091) ECOC 0.629(0.109) 0.629(0.109) 0.626(0.111) 0.640(0.088)\nsegment APs 0.950(0.017) 0.947(0.017) 0.948(0.017) 0.952(0.017)\n0.950 0.907 OVA 0.910(0.024) 0.910(0.024) 0.910(0.024) 0.917(0.022) (0.020) (0.019) ECOC 0.905(0.022) 0.905(0.022) 0.905(0.022) 0.951(0.016)\nsatimage APs 0.862(0.011) 0.861(0.011) 0.861(0.011) 0.862(0.011)\n0.847 0.846 OVA 0.832(0.014) 0.832(0.014) 0.832(0.014) 0.836(0.013) (0.011) (0.015) ECOC 0.818(0.011) 0.818(0.011) 0.818(0.011) 0.856(0.013)\npendigits APs 0.979(0.005) 0.977(0.006) 0.977(0.006) 0.979(0.005)\n0.956 0.934 OVA 0.931(0.007) 0.931(0.007) 0.931(0.007) 0.934(0.008) (0.007) (0.007) ECOC 0.918(0.025) 0.928(0.009) 0.928(0.009) 0.958(0.008)\nisolet APs 0.974(0.005) 0.973(0.005) 0.973(0.005) 0.978(0.004)\n0.976 0.960 OVA 0.972(0.005) 0.972(0.005) 0.972(0.005) 0.972(0.005) (0.004) (0.007) ECOC 0.959(0.008) 0.965(0.006) 0.965(0.006) 0.969(0.006)\nletter APs 0.837(0.006) 0.830(0.007) 0.831(0.007) 0.844(0.007)\n0.784 0.748 OVA 0.723(0.008) 0.723(0.008) 0.723(0.008) 0.723(0.009) (0.009) (0.010) ECOC 0.565(0.024) 0.619(0.016) 0.618(0.016) 0.635(0.027)\n24 S. Park et al. 5 EXPERIMENTS\nWe additionally evaluated the classification performance of the aggregation methods with nonlinear binary classifiers. We used the LibSVM with a rbf kernel function for a base binary classifier, where the rbf-kernel width \u03b3 and the regularization parameter \u03bbB were determined by maximizing the classification accuracy of randomly chosen validation set on the 2-dimensional grid space (\u03b3, \u03bbB), \u03b3 \u2208 [2 \u22126, 2\u22125, ..., 22, 23] and \u03bbB \u2208 [2 \u22123, 2\u22122, ..., 23, 24]. Note that, the results on 2 cancer datasets (GCM and ALL) are not included in here because the performance on these datasets were considerable worse than using the linear kernel.\nThe average classification accuracy and MSE are shown in Table 9 and 10, respectively. As similar to the linear kernel case, our method showed the stable performance for the wide range of the value of \u03bb. However, there was the slight degradation of performance of our method on the datasets {glass, satimage} due to overfitting, so we just increased the regularization parameter as \u03bb = 10\u22121 for these datasets. We can see that all probabilistic decoding methods yield the similar classification performance for most cases. Although our method improves the results of the loss-based decoding in terms of MSE, its performance is not significantly better than that of GBTM and WMAP. The reason being is that optimal tuning of aggregation weights favors for good binary classifiers while de-emphasizing no good binary classifiers. When suitably-chosen nonlinear kernels are used, all binary classifiers are already good, so no much performance gain is shown even when optimal tuning of aggregation weights is made.\n5.3 Real-World Data Convex Binary Classifier Aggregation 25\n26 S. Park et al. 5 EXPERIMENTS\nConvex Binary Classifier Aggregation 27"}, {"heading": "6 Conclusions", "text": "We have presented a method for optimally combining the results of binary classifiers into a final answer to multiclass problems. The softmax function was used to model the class membership probability, taking a conic combination of discrepancies induced by binary classifiers and returning a guess of class membership. The corresponding log-likelihood was a convex function in the form of log-sum-exp, leading to a convex formulation for optimal binary classifier aggregation. The primal-dual interior point method was adopted to solve the convex optimization problem.\nOur method has several advantages over an existing optimal aggregationmethod, WMAP [16] which optimally combines binary class membership probability estimates to form a joint probability estimates for all K classes, fitting the generalized Bradley-Terry model. In WMAP, both aggregation weights and class membership probabilities are treated as parameters to be estimated, so the computational complexity grows linearly with the number of training examples. In contrast, our method has a few strong points: (1) aggregation weights are the only parameters to be tuned (low complexity); (2) the convex formulation yields the global solution; (3) class membership probabilities for test data are easily evaluated without further optimizations. In addition, our method is available for any types of discrepancy measures, while the aggregation methods based on the (generalized) Bradley-Terry model always require that the binary classifier yields the probability estimates.\nThe (primal-dual) interior point method still suffers from computational burden in large scale problems. We may use a stochastic approximation of interior point methods [41] to improve the scalability. It is our future work to adopt more efficient optimization to speed up the computation and to improve the scalability, in our convex aggregation method."}, {"heading": "7 Appendix", "text": "7.1 Derivations of gradient and Hessian of the objective function (10)\nIn this section, we include the gradient and Hessian of the objective function (10), which can be easily calculated based on the derivations of gradient and Hessian of the log-sum-exp function, described in Appendix in [22].\nWe first compute \u03d5j,yii for j = 1, ...,K and i = 1, ..., N by (8). Then we define \u03a8i \u2208 R K\u00d7M and ui \u2208 R K\u00d71 for the ith data point:\n\u03a8i =   (\u03d51,yii ) \u22a4 ...\n(\u03d5K,yii ) \u22a4\n  .\nui = [ exp{w\u22a4\u03d51,yii }, ..., exp{w \u22a4\u03d5 M,yi i } ]\u22a4 . (44)\nThe objective function is given by\nf0(w) = 1\nN\nN\u2211\ni=1\nlog\n( K\u2211\nk=1\n[ui]k\n) + \u03bb\n2 w\u22a4w. (45)\nThe gradient is computed as\n\u2207f0(w) =\nN\u2211\ni=1\n( 1\n1\u22a4ui \u03a8\u22a4i ui\n) + \u03bbw.\n28 S. Park et al. 7 APPENDIX\nThe Hessian is obtained by\n\u22072f0(w)\n=\nN\u2211\ni=1\n\u03a8\u22a4i\n( 1\n1\u22a4ui diag(ui)\u2212\n1\n(1\u22a4ui)2 ui(ui)\n\u22a4 ) \u03a8i + diag(\u03bb),\nwhere \u03bb = [\u03bb, ..., \u03bb]\u22a4 \u2208 RM ."}, {"heading": "7.2 Proof of Proposition 1", "text": "Proof. Let us define l\u03c4 as the modified log-sum-exp function with \u03c4 , such that\nl\u03c4\n( \u03d5\n1,yi i ,\u03d5 2,yi i , ...,\u03d5 K,yi i ,w\n)\n= 1\n\u03c4 log\n( K\u2211\nk=1\nexp { \u03c4 ( (1 \u2212 \u03b4(yi, k)) +w \u22a4\u03d5 k,yi i )}) .\nWe first show that the sequence of functions { l\u03c4 ( \u03d5 1,yi i ,\u03d5 2,yi i , ...,\u03d5 K,yi i ,w )} for \u03c4 = 1, 2, . . . , uniformly converges to the multiclass hinge loss function h ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n) . Then,\nwe can easily prove that the sequence of functions {f\u03c4 (w)} also uniformly converges to fLM (w). This proposition is an extension of the results in [42] which provide a connection between the hinge loss function and logistic loss function (that is a special case of the log-sum-exp function) in the case of binary problems.\nFor all \u03be \u2208 RK , we are given the following inequalities for the log-sum-exp function [22]:\nmax{\u03be1, \u03be2, ..., \u03beK} \u2264 log\n( K\u2211\nk=1\nexp{\u03bek}\n)\n\u2264 max{\u03be1, \u03be2, ..., \u03beK}+ logK. (46)\nSubstituting \u03bek = \u03c4 ( (1 \u2212 \u03b4(yi, k)) +w \u22a4\u03d5 k,yi i ) into (46), one can easily see that\nh ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n)\n\u2264 l\u03c4\n( \u03d5\n1,yi i ,\u03d5 2,yi i , ...,\u03d5 K,yi i ,w\n)\n\u2264 h ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n) + logK\n\u03c4 . (47)\nIt follows from this inequality that we have\nlogK\n\u03c4 = max\nw,\u03d51,yi i ,..,\u03d5K,yi i\n{ l\u03c4 ( \u03d5 1,yi i ,\u03d5 2,yi i , ...,\u03d5 K,yi i ,w )\n\u2212h ( \u03d5\n1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w\n)} . (48)\nThus, for any given \u01eb > 0, we can choose sufficiently large \u03c4 such that\n\u2223\u2223\u2223l\u03c4 ( \u03d5 1,yi i ,\u03d5 2,yi i , ...,\u03d5 K,yi i ,w ) \u2212 h ( \u03d5 1,yi i ,\u03d5 2,yi i , . . . ,\u03d5 K,yi i ,w )\u2223\u2223\u2223 < \u01eb.\nSumming over all training data points, we obtain the inequality logK/\u03c4 \u2265 maxw {f\u03c4 (w)\u2212 fLM (w)}, which implies the uniform convergence of f\u03c4 (w) to fLM (w).\n7.3 Proof of Proposition 2 Convex Binary Classifier Aggregation 29"}, {"heading": "7.3 Proof of Proposition 2", "text": "Proof. We first give the detailed explanations for some notations used in the proof. We can consider the margin of each example, \u03bdw(xi, yi), as a decision function for multiclass problems. For example, the data point xi is misclassified if and only if \u03bdw(xi, yi) \u2264 0. Thus the empirical misclassification error can be defined by\n1\nN\nN\u2211\ni=1\n1 ( \u03bdw(xi, yi) \u2264 0 ) , (49)\nwhere 1(\u03c0) is the 0-1 loss function which equals 1 if the predicate \u03c0 is true, otherwise 0. In addition, due to the definition of the margin, we have\n\u03bdw(xi, yi) = min k 6=yi \u03c1(ck, qi,w)\u2212 \u03c1(cyi , qi,w)\n= w\u22a4\u03d5k\u0304i,yii (50)\nwhere k\u0304i = argmink 6=yi \u03c1(ck, qi,w). Note that, since \u03d5 k,y x can be considered as feature mapping, as in kernel methods, the hypothesis set considered here is defined as a bounded linear function class, i.e., F = {f : X \u00d7 Y \u2192 R | f(x, y) = w\u22a4\u03d5k\u0304i,yii for some w \u2208 R\nM + , \u2016w\u20162 \u2264 B}. In this work, we aim at finding an aggregation weight vector among the function class F , which minimizes the expected misclassification error (generalization error), E [ 1 ( \u03bdw(x, y) \u2264 0 )] ( = P (y 6= y\u0302) ) . However, a direct optimization involving the\n0-1 loss is not an easy task because of its discrete nature. We instead consider a ramp loss \u03c6 : R \u2192 [0, 1], which is a continuous upper bound on 0-1 loss function:\n\u03c6(z) =    0 if z \u2265 1 1\u2212 z if 0 < z < 1\n1 if z \u2264 0 . (51)\nNote that, \u03c6 is a clipped version of hinge loss [31]. Finally, we define the empirical Rademacher complexity [20] of a class of functions we are interested in. Let G be a class of functions mapping from X \u00d7 Y to R and given samples {(xi, yi)} N i=1, the empirical Rademacher complexity of the class G is given by [20]\nR\u0302N(G) = E\u03c3 [ sup h\u2208G ( 1 N N\u2211\ni=1\n\u03c3ih(xi, yi) )] , (52)\nwhere \u03c3i \u2208 {\u22121, 1} are independent uniform random variables. Note that, the empirical Rademacher complexity is based on the training examples and thus is practically computable. In addition, it can be viewed as the correlation between a random binary noise and functions in the function class G, in the supermum sense. In our case, the empirical Rademacher can be calculated based on Lemma 22 in [20]. Defining a new index\n30 S. Park et al. REFERENCES\nk\u2032i = argmink 6=yi \u2016\u03d5 k,yi i \u20162, the empirical Rademacher complexity of the class F is given by\nR\u0302N (F) = E\u03c3\n[ sup\nw:\u2016w\u20162\u2264B and w\u2208RM+\nw\u22a4 ( 1 N N\u2211\ni=1\n\u03c3i\u03d5 k\u0304i,yi i\n)]\n\u2264 B\nN E\u03c3\n  \u221a\u221a\u221a\u221a N\u2211\ni=1\nN\u2211\nj=1\n\u03c3i\u03c3j ( \u03d5 k\u2032 i ,yi\ni\n)\u22a4 \u03d5 k\u2032 i ,yj\nj\n \n\u2264 B\nN\n\u221a\u221a\u221a\u221a\u221aE\u03c3   N\u2211\ni=1\nN\u2211\nj=1\n\u03c3i\u03c3j ( \u03d5 k\u2032i,yi i )\u22a4 \u03d5 k\u2032i,yj j\n \n= B\nN\n( N\u2211\ni=1\nmin k 6=yi\n\u2016\u03d5k,yii \u2016 2 2\n)1/2 , (53)\nwhere Cauchy-Schwarz and Jensen\u2019s inequalities are used to arrive at the second and the third inequalities respectively.\nApplying Theorem 7 and 8 in [20], yields the following generalization bound. With the empirical Rademacher complexity in (53), for any \u01eb > 0, with probability greater than 1\u2212 \u01eb over samples of length N , every aggregation weight vector w \u2208 F satisfies\nE [ 1 ( \u03bdw(x, y) \u2264 0 )] \u2264 1\nN\nN\u2211\ni=1\n\u03c6 ( \u03bdw(xi, yi) ) + 2 R\u0302N(F) +\n\u221a 9 ln(2/\u01eb)\n2N . (54)\nUsing P (y 6= y\u0302) = E [ 1 ( \u03bdw(x, y) \u2264 0 )] , we directly obtain Proposition 2."}], "references": [{"title": "Support vector machines for multi-class pattern recognition", "author": ["J. Weston", "C. Watkins"], "venue": "Proceedings of the Seventh European Symposium On Artificial Neural Networks", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "On the algorithmic implementation of multiclass kernelbased vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research, 2:265\u2013292", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "A review on the combination of binary classifiers in multiclass problems", "author": ["A.C. Lorena", "A.C. Carvalho", "J.M. Gama"], "venue": "Artificial Intelligence Review, 30:19\u201337", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Handwritten digit recognition by neural networks with single-layer training", "author": ["S. Knerr", "L. Personnaz", "G. Dreyfus"], "venue": "IEEE Transactions on Neural Networks, 3:962\u2013 968", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["C.W. Hsu", "C.J. Lin"], "venue": "IEEE Transactions on Neural Networks, 13:415\u2013425", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research, 2:263\u2013286", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research, 1:113\u2013141", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "MetaCost: A general method for making classifiers cost-sensitive", "author": ["P. Domingos"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 155\u2013164", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Class probability estimation and cost-sensitive classification decisions", "author": ["D.D. Margineantu"], "venue": "Proceedings of the European Conference on Machine Learning (ECML), pages 270\u2013281", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), Williams Town, MA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiely & Sons", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification by pairwise coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "The Annals of Statistics, 26(2):451\u2013471", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Rank analysis of incomplete block designs: The method of paired comparisons", "author": ["R.A. Bradley", "M.E. Terry"], "venue": "Biometrika, 39:324\u2013345", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1952}, {"title": "Reducing multiclass to binary by coupling probability estimates", "author": ["B. Zadrozny"], "venue": "Advances in Neural Information Processing Systems (NIPS), volume 14. MIT Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Generalized Bradley-Terry models and multiclass probability estimates", "author": ["T.K. Huang", "R.C. Weng", "C.J. Lin"], "venue": "Journal of Machine Learning Research, 7:85\u2013115", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimal aggregation of binary classifiers for multiclass cancer diagnosis using gene expression profiles", "author": ["N. Yukinawa", "S. Oba", "K. Kato", "S. Ishii"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics, 6(2):333\u2013343", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "A multiclass classification method based on decoding of binary classifiers", "author": ["T. Takenouchi", "S. Ishii"], "venue": "Neural Computation, 21(7):2049\u20132081", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian aggregation of binary classifiers", "author": ["S. Park", "S. Choi"], "venue": "Proceedings of the IEEE International Conference on Data Mining (ICDM)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Geometric programming for classifier aggregation", "author": ["S. Park", "S. Choi"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Prague, Czech Republic", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J.C. Platt"], "venue": "Advances in Large Margin Classifiers, pages 61\u201374. MIT Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A tutorial on geometric programming", "author": ["S. Boyd", "S.J. Kim", "L. Vandenberghe", "A. Hassibi"], "venue": "Optimization and Engineering, 8(1):67\u2013127", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear Programming: Sequential Unconstrained Minimization Techniques", "author": ["Anthony V. Fiacco", "Garth P. McCormick"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1968}, {"title": "A new polynomial-time algorithm for linear programming", "author": ["N. Karmarkar"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1984}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "An interior-point method for large-scale l1regularized logistic regression", "author": ["K.M. Koh", "S.J. Kim", "S. Boyd"], "venue": "Journal of Machine Learning Research, 8:1519\u20131555", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Text categorization based on regularized linear classification methods", "author": ["T. Zhang", "F.J. Oles"], "venue": "Information Retrieval, 4:5\u201331", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Large scale transductive SVMs", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research, 7:1687\u20131712", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hestie", "R. Tibshirani"], "venue": "Journal of Statistical Software, 33(1)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized linear models", "author": ["J. Nelder", "W.M. Wedderburn"], "venue": "Journal of the Royal Statistical Society A, 135:370\u2013384", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1972}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research, 9:1871\u2013 1874", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "A note on Platt\u2019s probabilistic outputs for support vector machine", "author": ["H.T. Lin", "C.J. Lin", "R.C. Weng"], "venue": "Machine Learning, 68(3):267\u2013276", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Classification", "author": ["E.J. Yeoh", "M.E. Ross", "S.A. Shurtleff", "W.K. Williams", "D. Patel", "R. Mahfouz", "F.G. Behm", "S.C. Raimondi", "M.V. Relling", "A. Patel", "C. Cheng", "D. Campana", "D. Wilkins", "X. Zhou", "J. Li", "H. Liu", "C.H. Pui", "W.E. Evans", "C. Naeve", "L. Wong", "J.R. Downing"], "venue": "subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling. Cancer Cell, 1(2):133\u2013143", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Multiclass cancer diagnosis using tumor gene expression signatures", "author": ["S. Ramaswamy", "P. Tamayo", "R. Rifkin", "S. Mukherjee", "C.H. Yeang", "M. Angelo", "C. Ladd", "M. Reich", "E. Latulippe", "J.P. Mesirov", "T. Poggio", "W. Gerald", "M. Loda", "E.S. Lander", "T.R. Golub"], "venue": "Proceedings of the National Academy of Sciences, USA, 98(26):15149\u201315154", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), Edmonton, Canada", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Verification of forecasts expressed in terms of probability", "author": ["G.W. Brier"], "venue": "Monthly Weather Review, 78:1\u20133", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1950}, {"title": "and N", "author": ["P. Carbonetto", "M. Schmidt"], "venue": "de Freitas. An interior-point stochastic approximation method and an L1-regularized delta rule. In Advances in Neural Information Processing Systems (NIPS), volume 21. MIT Press", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Modified logistic regression: An approximation to SVM and its applications in large-scale text categorization", "author": ["J. Zhang", "R. Jin", "Y. Yang", "A.G. Hauptmann"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), Washington, DC", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "For example, the multiclass SVM [1, 2] models a K-way classifier which directly separates the correct class label from the rest of class labels in the large margin framework.", "startOffset": 32, "endOffset": 38}, {"referenceID": 1, "context": "For example, the multiclass SVM [1, 2] models a K-way classifier which directly separates the correct class label from the rest of class labels in the large margin framework.", "startOffset": 32, "endOffset": 38}, {"referenceID": 2, "context": "It is much easier and simpler to learn a set of binary classifiers than to train one unique classifier which separates all classes simultaneously [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "For example, a digit recognition problem can be decomposed into a set of simpler sub-problems, which can be easily solved by linear classifiers [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 4, "context": "A comparison study [5] observed that the direct methods, such as multiclass SVM [1, 2], generally require more training time than binary-decomposition methods.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "A comparison study [5] observed that the direct methods, such as multiclass SVM [1, 2], generally require more training time than binary-decomposition methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 1, "context": "A comparison study [5] observed that the direct methods, such as multiclass SVM [1, 2], generally require more training time than binary-decomposition methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 4, "context": "It was also observed in [5] that APs-based decomposition methods show higher predictive accuracy than the multiclass SVM for most of cases.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "Several encoding methods are widely used, including APs, OVA, and error correcting output code (ECOC) [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "Various loss functions (such as exponential loss and logistic loss) are considered in the case where binary classifiers yield a score whose magnitude is a measure of confidence in the prediction, referred to be as loss-based decoding [7].", "startOffset": 234, "endOffset": 237}, {"referenceID": 7, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 53, "endOffset": 63}, {"referenceID": 8, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 53, "endOffset": 63}, {"referenceID": 9, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 53, "endOffset": 63}, {"referenceID": 10, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 208, "endOffset": 212}, {"referenceID": 0, "context": "In probabilistic decoding, we are given binary class membership probability estimates (scores in [0,1]) determined by binary classifiers.", "startOffset": 97, "endOffset": 102}, {"referenceID": 11, "context": "In the case of APs, Hastie and Tibshirani [12] developed a method, pairwise coupling, in which pairwise class membership probability estimates are combined to form a joint probability estimates for all K classes, fitting the Bradley-Terry model [13] by minimizing a KL-divergence criterion.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "In the case of APs, Hastie and Tibshirani [12] developed a method, pairwise coupling, in which pairwise class membership probability estimates are combined to form a joint probability estimates for all K classes, fitting the Bradley-Terry model [13] by minimizing a KL-divergence criterion.", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [14, 15], where a generalized Bradley-Terry model [15] was considered to relate probability estimates obtained by binary classifiers to class membership probability estimates.", "startOffset": 78, "endOffset": 86}, {"referenceID": 14, "context": "This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [14, 15], where a generalized Bradley-Terry model [15] was considered to relate probability estimates obtained by binary classifiers to class membership probability estimates.", "startOffset": 78, "endOffset": 86}, {"referenceID": 14, "context": "This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [14, 15], where a generalized Bradley-Terry model [15] was considered to relate probability estimates obtained by binary classifiers to class membership probability estimates.", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "This problem is alleviated by introducing confidence weights placed on individual binary classifiers that are optimally tuned based on training data [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "However, the method in [16] involves a huge number of parameters, NK +M , where N is the number of training data points and M is the number of binary classifiers.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Takenouchi and Ishii [17] proposed a different type of decoding method in which misclassification in binary classifier is formulated as a bit inversion error problem, as in information transmission theory.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "Recently, we have developed a Bayesian aggregation method [18] for probabilistic decoding.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "In this way, aggregation weights are the only parameters to be tuned (M), while the existing method [16] scales linearly with the number of samples (NK +M).", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "To solve these problems, one can consider the maximum likelihood estimation instead of full Bayesian learning [18], in which class membership probabilities can be easily computed by evaluating the softmax function with the learned aggregation weights.", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "In our previous work [19], we proposed the l1 norm regularized maximum likelihood method to determine the optimal aggregation weights, which is a convex problem.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "However, our previous method [19] still has several limitations: (1) the optimization problem can be directly solved by the standard convex optimization algorithms without transforming it to geometric programming; (2) it only allows l1 norm regularization.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "In contrast to [19] where the problem was converted to geometric programming, we directly solve the optimization problem using primal-dual interior point method that is an efficient solver for convex optimization problems, which allows us to use various types of regularization.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "In our formulation, the aggregation weights are the only parameters to be tuned (M), while the existing method [16] scales linearly with the number of samples (NK +M).", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Moreover, we present data-dependent generalization error bound, based on margins and Rademacher complexity, extending existing work on binary problems [20] to our multiclass problems which are solved by aggregating binary solutions.", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "For example, we can use probabilistic SVM [21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "This approach provides a simple model to evaluate class membership probabilities, compared to the (generalized) Bradley-Terry model-based method [16].", "startOffset": 145, "endOffset": 149}, {"referenceID": 6, "context": "For instance, we can also choose the exponential loss function that was used in loss-based decoding [7]", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "The prediction based on the loss-based decoding [7] is a special case of our model.", "startOffset": 48, "endOffset": 51}, {"referenceID": 21, "context": "associated with data point xi in the objective function (11) is called log-sum-exp which is a well known convex function used for geometric programming [22, 23].", "startOffset": 152, "endOffset": 160}, {"referenceID": 22, "context": "associated with data point xi in the objective function (11) is called log-sum-exp which is a well known convex function used for geometric programming [22, 23].", "startOffset": 152, "endOffset": 160}, {"referenceID": 21, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 23, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 24, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 25, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 21, "context": "The primal-dual interior point method exhibits better than linear convergence and outperforms the standard interior point methods in most of applications such as linear, quadratic, geometric and semidefinite programming [22].", "startOffset": 220, "endOffset": 224}, {"referenceID": 21, "context": "Readers who are familiar with the primal-dual interior point method can skip this section and more details can be found in [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "One can easily see that Slater\u2019s constraint qualification holds for the problem (10), since any point on the positive orthant (w \u2208 R++) could be a strictly feasible solution to the problem [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 23, "context": "We augment the objective function f0(w) by a logarithmic barrier [24] such that the constrained optimization problem (10) is converted to an unconstrained optimization:", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "The residual is not necessary 0 at each iteration, except in the limits as the algorithm converges [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 26, "context": "As proposed in [27], we construct P as a diagonal matrix in which the diagonal entries are set to that of H .", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "The step length s can be computed by a backtracking line search as described in [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "To do this, we first compute smax to ensure z(\u03bc) 0: smax = sup{s \u2208 [0, 1]|z + s\u2206z 0} = min{1,min{\u2212zj/\u2206zj |\u2206zj < 0}}.", "startOffset": 67, "endOffset": 73}, {"referenceID": 26, "context": "In order to update the barrier parameter \u03bc, we use an adaptively strategies that determines it according to the reduction of the duality gap as in [27]:", "startOffset": 147, "endOffset": 151}, {"referenceID": 27, "context": "Following the work in [28] where a close relation between large margin and logistic regression formulations is shown in the case of binary classification, we provide its multiclass extension in this section.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "The multiclass hinge loss function, which is a convex upper bound on the 0-1 loss 1 ( \u0177i 6= yi ) [2], was considered as a surrogate function:", "startOffset": 97, "endOffset": 100}, {"referenceID": 28, "context": "The projected subgradient methods [29] can be applied to directly solve the primal form (36).", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "Note that the projected subgradient method is a first-order method which exploits the gradient only, thus its performance much depends on the problem scaling or conditioning [26, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 19, "context": "In addition, we present a data-dependent generalization error bound, based on the large margin formulation (36) using the Rademacher complexity [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "Our result is an extension of existing work on binary problems [20] to the multiclass problems solved by aggregating binary solutions.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Applying Theorem 7 in [20] to our problem, with the empirical Rademacher complexity, yields the following proposition.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "where \u03bdw(xi, yi) is the margin (50) and \u03c6(z) = min(1,max(0, 1\u2212 z)), for z \u2208 R, is the ramp loss that is a clipped version of hinge loss [31].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "Lemma 22 in [20] was used to compute the empirical Rademacher complexity in our problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "Note that our generalization error bound is similar to the ones for boosting with loss-based decoding [7], while the error bound in [7] is based on VC dimension which does not depend on sample distribution in contrast to Rademacher complexity.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "Note that our generalization error bound is similar to the ones for boosting with loss-based decoding [7], while the error bound in [7] is based on VC dimension which does not depend on sample distribution in contrast to Rademacher complexity.", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 81, "endOffset": 84}, {"referenceID": 30, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 183, "endOffset": 186}, {"referenceID": 14, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 299, "endOffset": 303}, {"referenceID": 1, "context": "1 Experimental Setting M-SVM [2] aims to directly construct a K-way classifier which separates the correct class labels from the rest of class labels by maximizing the margin defined as gyi(xi)\u2212maxk 6=yi gk(xi), where {gk} K k=1 are classification functions for each class.", "startOffset": 29, "endOffset": 32}, {"referenceID": 30, "context": "LMR [32] is a variant of generalized liner model (GML) [33] that generalizes linear regression by allowing a linear model to be related with the response variables (characterized by exponential family distribution) through the response function.", "startOffset": 4, "endOffset": 8}, {"referenceID": 31, "context": "LMR [32] is a variant of generalized liner model (GML) [33] that generalizes linear regression by allowing a linear model to be related with the response variables (characterized by exponential family distribution) through the response function.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "LMR [32] determines the parameters by maximum likelihood with the l1 norm (lasso) regularization.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "In the case of ECOC encoding, we used two strategies to generate the code matrix C: complete code and sparse random code [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "For K \u2265 8, we generated a spare random code matrix as in [7], in which M = \u230815 log2 K\u2309, and entries of the code matrix are chosen as \u25b3 with probability 1/2 and 0 or 1 with probability 1/4 for each.", "startOffset": 57, "endOffset": 60}, {"referenceID": 32, "context": "We used two linear SVMs to implement the base binary classifier, LibSVM and Liblinear [34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "In the case of LibSVM, Platt\u2019s sigmoid model [21] is used to calculate the binary class membership probability:", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "where gj is the function learned by the jth SVM and A,B \u2208 R are parameters are tuned by the regularized maximum likelihood framework [21, 35].", "startOffset": 133, "endOffset": 141}, {"referenceID": 33, "context": "where gj is the function learned by the jth SVM and A,B \u2208 R are parameters are tuned by the regularized maximum likelihood framework [21, 35].", "startOffset": 133, "endOffset": 141}, {"referenceID": 6, "context": "1, the loss-based decoding [7] can be implemented as a special case of our aggregation method, where the aggregation weights are set to w\u03031 = .", "startOffset": 27, "endOffset": 30}, {"referenceID": 15, "context": "WMAP [16] is an existing optimal aggregation method, which also tunes aggregation weights based on training data.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "The aggregation weights are assigned to each classifier an learned by maximizing the objective function which represents the concordance between the class membership probability estimates and the target labels [16].", "startOffset": 210, "endOffset": 214}, {"referenceID": 14, "context": "GBTM is also a probabilistic decoding method based on generalized Bradely-Terry models [15], which can be understood as a special case of WMAP with the uniform aggregation weighs.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "We implemented the method according to Algorithm 2 in [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Here, uj,i and vj,i were generated from the uniform distribution, uj,i, vj,i \u223c U[0 1] and r(a) is a binary function that is 1 if a > 0.", "startOffset": 80, "endOffset": 85}, {"referenceID": 0, "context": "For example, in the case of BC1, Q1,i = u1,i for i \u2208 I3, where u1,i \u223c U[0 1].", "startOffset": 71, "endOffset": 76}, {"referenceID": 19, "context": "The data instances evenly sampled from K number of 2-dimensional Gaussian distributions, the mean vectors of which are chosen as D independent uniform [0, 20] random variables.", "startOffset": 151, "endOffset": 158}, {"referenceID": 34, "context": "The cancer data sets are acute lymphoblastic leukemia (ALL) [37] and global cancer map (GCM) [38], which were used to evaluate the performance of WMAP [16].", "startOffset": 60, "endOffset": 64}, {"referenceID": 35, "context": "The cancer data sets are acute lymphoblastic leukemia (ALL) [37] and global cancer map (GCM) [38], which were used to evaluate the performance of WMAP [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "The cancer data sets are acute lymphoblastic leukemia (ALL) [37] and global cancer map (GCM) [38], which were used to evaluate the performance of WMAP [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "As in [10, 39], we can assume that the class membership probabilities are given by to = [Tj,o] \u2208 R K where Tj,o is defined to be 1 if the label of xo is j and 0 otherwise.", "startOffset": 6, "endOffset": 14}, {"referenceID": 36, "context": "As in [10, 39], we can assume that the class membership probabilities are given by to = [Tj,o] \u2208 R K where Tj,o is defined to be 1 if the label of xo is j and 0 otherwise.", "startOffset": 6, "endOffset": 14}, {"referenceID": 37, "context": "Note that MSE is also called Brier score [40], and the lower value the better performance.", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "For binary-decomposition methods, the base binary classifier was chosen according to the scale of dataset: LibSVM for the datasets {GCM, ALL}, and Liblinear [34] for other datasets.", "startOffset": 157, "endOffset": 161}, {"referenceID": 4, "context": "This result is consistent with the comparison study in [5] which reported that APs-based decomposition methods generally show higher predictive accuracy than the direct methods for multiclass problems, such as M-SVM.", "startOffset": 55, "endOffset": 58}, {"referenceID": 15, "context": "Our method has several advantages over an existing optimal aggregationmethod, WMAP [16] which optimally combines binary class membership probability estimates to form a joint probability estimates for all K classes, fitting the generalized Bradley-Terry model.", "startOffset": 83, "endOffset": 87}, {"referenceID": 38, "context": "We may use a stochastic approximation of interior point methods [41] to improve the scalability.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "1 Derivations of gradient and Hessian of the objective function (10) In this section, we include the gradient and Hessian of the objective function (10), which can be easily calculated based on the derivations of gradient and Hessian of the log-sum-exp function, described in Appendix in [22].", "startOffset": 288, "endOffset": 292}, {"referenceID": 39, "context": "This proposition is an extension of the results in [42] which provide a connection between the hinge loss function and logistic loss function (that is a special case of the log-sum-exp function) in the case of binary problems.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "For all \u03be \u2208 R , we are given the following inequalities for the log-sum-exp function [22]:", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "We instead consider a ramp loss \u03c6 : R \u2192 [0, 1], which is a continuous upper bound on 0-1 loss function:", "startOffset": 40, "endOffset": 46}, {"referenceID": 29, "context": "Note that, \u03c6 is a clipped version of hinge loss [31].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "Finally, we define the empirical Rademacher complexity [20] of a class of functions we are interested in.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "Let G be a class of functions mapping from X \u00d7 Y to R and given samples {(xi, yi)} N i=1, the empirical Rademacher complexity of the class G is given by [20]", "startOffset": 153, "endOffset": 157}, {"referenceID": 19, "context": "In our case, the empirical Rademacher can be calculated based on Lemma 22 in [20].", "startOffset": 77, "endOffset": 81}], "year": 2014, "abstractText": "Multiclass problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods, including all-pairs (APs), one-versus-all (OVA), and error correcting output code (ECOC), have been studied, to decompose multiclass problems into binary problems. However, little study has been made to optimally aggregate binary problems to determine a final answer to the multiclass problem. In this paper we present a convex optimization method for an optimal aggregation of binary classifiers to estimate class membership probabilities in multiclass problems. We model the class membership probability as a softmax function which takes a conic combination of discrepancies induced by individual binary classifiers, as an input. With this model, we formulate the regularized maximum likelihood estimation as a convex optimization problem, which is solved by the primal-dual interior point method. Connections of our method to large margin classifiers are presented, showing that the large margin formulation can be considered as a limiting case of our convex formulation. Numerical experiments on synthetic and real-world data sets demonstrate that our method outperforms existing aggregation methods as well as direct methods, in terms of the classification accuracy and the quality of class membership probability estimates. aAppeared in Proceedings of the 2014 SIAM International Conference on Data Mining (SDM 2014). 2 S. Park et al. CONTENTS", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}