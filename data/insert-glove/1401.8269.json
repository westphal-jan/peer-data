{"id": "1401.8269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2014", "title": "Experiments with Three Approaches to Recognizing Lexical Entailment", "abstract": "Inference ai'i in 33-11 natural language often 28sec involves schnepp recognizing lexical endear entailment (RLE ); 66-million that is, yacimientos identifying whether saltwell one zamosc word entails exposing another. gastos For shizuku example, \" cynically buy \" entails \" 50.68 own \". squirm Two general faegre strategies for 11-style RLE have zucca been katastralgemeinden proposed: oguchi One strategy is to darold manually mot\u00f6rhead construct an flecktones asymmetric similarity silberling measure 69.28 for context feebles vectors (directional supplementation similarity) predefined and keverne another geographia is to tahirih treat RLE as a problem of learning to recognize semantic paibool relations using hirakata supervised machine learning techniques (tattooists relation pauperism classification ). lerebours In -53 this 62.13 paper, 45402 we firmest experiment rzewski with two recent elita state - of - the - art alodersptimes.com representatives upl of threw the two 220-280 general strategies. The dematteis first approach is an demotions asymmetric similarity amateur measure (dome-like an 42,083 instance safarov of matar the 35.57 directional campau similarity s\u014dry\u016b strategy ), designed shutouts to hurghada capture the temiscaming degree 45-3 to pens\u00e9es which the belardi contexts of comr a word, matanog a, moonbase form bova a izen subset of gosar the hovell contexts of refocusing another 93.30 word, prefabricated b. massachussets The clerks second 1985-1994 approach (an 64.95 instance of self-assembling the rgc relation classification 533 strategy) represents a word carnea pair, unsortable a: deathbed b, trebnje with festivali a 7071 feature vector elaine that is deduplication the concatenation tandems of the 1,785 context from vectors daimler-benz of a and umaglesi b, thornham and highsmith then mihdar applies supervised learning gunk to non-fictional a v\u00e9liz training set doillon of gunowners labeled overath feature re-launching vectors. welf Additionally, we introduce a akhmad third approach that batina is mongla a new instance of tidelands the gunboat relation meghana classification word-for-word strategy. The third kittner approach krawczyk represents redburn a objected word fti pair, nadzeya a: b, with a 13:04 feature vector in which qudratullah the oprah features are the www.mtv.com differences 6-pica in haakon the keeravani similarities of strike-shortened a and b 50.3 to populli a 55.36 set sryanglobe.com of jausiers reference words. spermicides All byproducts three approaches drac use demaret vector space gribeauval models (anti-partisan VSMs) of semantics, dary based honeyguide on mutti word - bilking context 86.25 matrices. We moshin perform an extensive evaluation exstein of the three on-demand approaches using melchett three beta-blockers different popadich datasets. 720s The proposed dreyse new approach (sparrer similarity differences) shukarno performs 1974/75 significantly luv better than the other two approaches 0212 on hlv some peronne datasets hansens and 1961-1965 there 14k is vickerman no irwan dataset margit for which it non-aryans is furnivall significantly worse. m\u00fcnsterland Our vitesse results celerant suggest it xwb is beneficial lipunan to cordish make connections between the avie research admissions in lexical entailment and the flatts research williford in africanum semantic relation classification.", "histories": [["v1", "Fri, 31 Jan 2014 19:42:19 GMT  (58kb)", "http://arxiv.org/abs/1401.8269v1", "to appear in Natural Language Engineering"]], "COMMENTS": "to appear in Natural Language Engineering", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["peter d turney", "saif m mohammad"], "accepted": false, "id": "1401.8269"}, "pdf": {"name": "1401.8269.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["saif.mohammad}@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\n82 69\nv1 [\ncs .C\nL ]"}, {"heading": "1 Introduction", "text": "Recognizing textual entailment (RTE) is a popular task in natural language processing research, due to its relevance for text summarization, information retrieval, information extraction, question answering, machine translation, paraphrasing, and other applications (Androutsopoulos and Malakasiotis 2010). RTE involves pairs of sentences, such as the following (Dagan, Dolan, Magnini, and Roth 2009):\nwas recovered Saturday, almost three months after it was stolen from an Oslo museum.\nHypothesis: Edvard Munch painted \u2018The Scream\u2019.\nThe objective is to develop algorithms that can determine whether the text sentence entails the hypothesis sentence. In RTE, the gold standard for entailment is established by common sense, rather than formal logic. The text entails the hypothesis if the meaning of the hypothesis can be inferred from the meaning of the text, according to typical human interpretations of the text and the hypothesis (Dagan et al. 2009). In the above example, the text entails the hypothesis. In recent years, the RTE pairs have grown richer and more challenging. The text may now be a whole paragraph.\nIn many cases, to recognize when one sentence entails another, we must first be able to recognize when one word entails another (Geffet and Dagan 2005). Consider this sentence pair (our example):\nText: George was bitten by a dog. Hypothesis: George was attacked by an animal.\nTo recognize that the text entails the hypothesis, we must first recognize that bitten entails attacked and dog entails animal. This is the problem of recognizing lexical entailment (RLE). (We discuss the definition of lexical entailment in Section 2.)\nVector space models (VSMs) of semantics have been particularly useful for lexical semantics (Turney and Pantel 2010), hence it is natural to apply them to RLE. In this paper, we experiment with three VSM algorithms for lexical entailment. All three use word\u2013context matrices, in which a word corresponds to a row vector, called a context vector. For a given word, the corresponding context vector represents the distribution of the word over various contexts. The contexts consist of the words that occur near the given word in a large corpus of text. These models are inspired by the distributional hypothesis (Harris 1954; Firth 1957):\nDistributional hypothesis: Words that occur in similar contexts tend to\nhave similar meanings.\nThe first of the three algorithms, balAPinc (balanced average precision for distributional inclusion), attempts to address the problem of RLE with an asymmetric similarity measure for context vectors (Kotlerman, Dagan, Szpektor, and Zhitomirsky-Geffet 2010). The idea is to design a measure that captures the context inclusion hypothesis (Geffet and Dagan 2005):\nContext inclusion hypothesis: If a word a tends to occur in a subset of\nthe contexts in which a word b occurs (b contextually includes a), then a (the narrower term) tends to entail b (the broader term).\nThis is our paraphrase of what Geffet and Dagan (2005) call the distributional inclusion hypothesis. We prefer to call it context inclusion rather than distributional inclusion, to be clear about what is included. In the text\u2013hypothesis example above,\nanimal.\nThe intent of balAPinc is to take the context vectors a and b for the words a and b and calculate a numerical score that measures the degree to which b contextually includes a. The context inclusion hypothesis is inspired by model theory in formal logic (Hunter 1996). Let a and b be assertions in formal logic. In model theory, \u2018a |= b\u2019 means a entails b. If a |= b, then the set of models in which a is true is a subset of the set of models in which b is true. That is, the models of b include the models of a.\nThe second and third algorithms approach the task of recognizing lexical entailment by using techniques from research in semantic relation classification. Semantic relation classification is the task of learning to recognize when a word pair is an instance of a given semantic relation class (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju, Nakov, Nastase, Szpakowicz, Turney, and Yuret 2007).\nAn important subclass of lexical entailment is covered by the hyponymy\u2013 hypernymy semantic relation. If a word pair a : b is an instance of the hyponym\u2013 hypernym relation (dog:animal), then a |= b. There is a relatively large body of work on semantic relation classification in general, with good results on the hyponym\u2013 hypernym relation in particular (Hearst 1992; Snow, Jurafsky, and Ng 2006). Since semantic relation classification algorithms have worked well for this important subclass of lexical entailment, it seems plausible that this approach can be expanded to cover other subclasses of lexical entailment, and perhaps all subclasses of lexical entailment. (We say more about this in Section 3.)\nThe second of the three algorithms represents a word pair, a : b, with a feature vector that is the concatenation of the context vector a for a and the context vector b for b (Baroni, Bernardi, Do, and Shan 2012). For example, the concatenation of the two three-dimensional vectors \u30081, 2, 3\u3009 and \u30084, 5, 6\u3009 is the six-dimensional vector \u30081, 2, 3, 4, 5, 6\u3009. This algorithm was not given a name by Baroni et al. (2012). For ease of reference, we will call it ConVecs (concatenated vectors).\nConVecs is based on the context combination hypothesis (Baroni et al. 2012):\nContext combination hypothesis: The tendency of a to entail b is cor-\nrelated with some learnable function of the contexts in which a occurs and the contexts in which b occurs; some conjunctions of contexts tend to indicate entailment and others tend to indicate a lack of entailment.\nThis hypothesis implies that the contexts of a (the elements in the context vector a) and the contexts of b (elements in b) are suitable features for a feature vector representation of the word pair a : b. That is, if this hypothesis is correct, concatenated context vectors are an appropriate representation of word pairs for supervised machine learning of lexical entailment. This hypothesis was not explicitly stated by Baroni et al. (2012) but it is implicit in their approach.\nIn the semantic relation classification literature, vector concatentation (but not necessarily with context vectors) is a common way to construct feature vectors\n2002; Nastase and Szpakowicz 2003). Context concatentation is a first-order feature vector representation of word pairs. We call it first-order because the features are directly based on the elements of the context vectors.\nThis paper introduces a new algorithm, SimDiffs (similarity differences), as the third of the three algorithms we evaluate. SimDiffs uses a second-order feature vector representation of a :b, in which the features are differences in the similarities of a and b to a set of reference words, R. The similarities are given by cosines of the first-order context vectors for a, b, and the reference words, r \u2208 R. (We use a set of common words for R, as described in Section 6.3. We do not experiment with other choices for R.)\nSimDiffs is dependent on the similarity differences hypothesis (introduced here):\nSimilarity differences hypothesis: The tendency of a to entail b is corre-\nlated with some learnable function of the differences in their similarities, sim(a, r) \u2212 sim(b, r), to a set of reference words, r \u2208 R; some differences tend to indicate entailment and others tend to indicate a lack of entailment.\nFor example, consider dog |= animal versus table 6|= animal. Suppose that life is one of the reference words. We see that dog and animal are similar with respect to the reference word life; the difference in their similarities is small. On the other hand, table and animal are dissimilar with respect to life; there is a large difference in their similarities. Some differences are important for entailment (such as whether something is animate or inanimate) and others usually have little effect (such as the colour of a thing). Given labeled training data, we may be able to learn how differences in similarities affect lexical entailment.\nWe empirically evaluate the three algorithms, balAPinc, ConVecs, and SimDiffs, using three different datasets. We find that SimDiffs performs significantly better than the other two algorithms in some cases and there is no case for which it is significantly worse. ConVecs is significantly worse than balAPinc and SimDiffs on one dataset, whereas balAPinc is significantly worse than ConVecs on one dataset and significantly worse than SimDiffs on two datasets.\nSection 2 defines lexical entailment in terms of semantic relations between words. There is some disagreement about whether lexical entailment should be approached as a semantic relation classification task. We address this issue in Section 3. Past work on RLE is examined in Section 4. Performance measures for RLE algorithms are presented in Section 5. We describe the three algorithms in detail in Section 6. The three algorithms are evaluated using three datasets, which are presented in Section 7. We use the datasets of Kotlerman et al. (2010), Baroni et al. (2012), and Jurgens, Mohammad, Turney, and Holyoak (2012). The experimental results are reported in Section 8. We discuss some implications of the experiments in Section 9. Limitations of this work are considered in Section 10 and we conclude in Section 11.\nLet w and v be two words. Zhitomirsky-Geffet and Dagan (2009, p. 442) define substitutable lexical entailment as follows:\n... w entails v, if the following two conditions are fulfilled:\n1. Word meaning entailment: the meaning of a possible sense of w implies a possible sense of v; 2. Substitutability: w can substitute for v in some naturally occurring sentence, such that the meaning of the modified sentence would entail the meaning of the original one.\nWe call this the substitutional definition of lexical entailment.\nWe present a different definition of lexical entailment here. The idea is that whether one word entails another depends on the semantic relation between the words. We discuss some objections to this idea in Section 3.\nLet x and y be two words. To be able to say that x entails y outside of the context of a specific sentence, it must be the case that there is a strong semantic relation between x and y, and the entailment must follow from the nature of that semantic relation. We say that x entails y if the following three conditions are fulfilled:\n1. Typical relation: Given x and y, there should be a typical semantic relation\nR(x, y) that comes to mind. Let R(x, y) be the typical semantic relation between x and y. If there is no typical semantic relation between x and y, then x cannot entail y outside of a specific context. 2. Semantic relation entailment: If x and y typically have the semantic relation\nR(x, y), then it should follow from the meaning of the semantic relation that x implies y. 3. Relation entailment agreement: If x and y have two or more typical semantic\nrelations and the relations do not agree on whether x implies y, then assume that x does not imply y.\nWe call this the relational definition of lexical entailment.\nIn the first condition of the relational definition, the typical relation between x and y is the relation that naturally comes to mind when x and y are presented together. If x and y have multiple senses, the juxtaposition of x and y may suggest a semantic relation and it may also constrain the possible senses of the words. The constrained senses of the words are not necessarily the most frequent or prototypical senses of the words.\nFor example, consider the words lion and cat. The word cat has the senses house cat (a specific type of cat) and feline (the general class of cats, including domestic cats and wild cats). When the words lion and cat are juxtaposed, the relation that naturally comes to mind (for us) is hyponym\u2013hypernym (a lion is a type of cat) and the sense of cat is constrained to feline, although the house cat sense is more frequent and prototypical than the feline sense.\nContext determines the sense of an ambiguous word, but lexical entailment considers word pairs outside of the context of sentences. Since word senses can affect entailment, any approach to lexical entailment must decide how to handle ambiguous words. The substitutional definition of lexical entailment invites us to imagine\nsenses of the two words. The relational definition of lexical entailment invites us to imagine a semantic relation that connects the two words and constrains their possible senses.\nThe second condition of the relational definition determines whether one word entails another, based on their semantic relation. Since a hyponym implies its hypernym, lion entails cat. The second condition excludes semantic relations that do not imply entailment. For example, antonymy is excluded (e.g., tall does not imply short) and the hyponym\u2013hypernym relation is only included when the direction is correct (e.g., lion implies cat but cat does not imply lion).\nThe first condition in the substitutional definition of lexical entailment (word meaning entailment) asks us to consider whether the sense of one word implies the sense of another word. We hypothesize that any such implication must depend on the semantic relation between the senses of the words. It seems to us that, if there is no semantic relation between the words, then it is not possible for one word to imply the other. If one words implies another, the implication must follow from the nature of their semantic relation. The idea of the second condition in the relational definition of lexical entailment is to make this connection between semantic relations and lexical entailment explicit.\nThe third condition of the relational definition handles ambiguous cases by erring on the side of non-entailment. Some people might feel that lion and cat suggest either the hyponym\u2013hypernym relation (assuming cat means feline) or the coordinate relation (assuming that cat means house cat). Coordinates are words with a shared hypernym. Lion and house cat share the hypernym feline. If cat means house cat, then lion and cat are coordinates. A hyponym implies its hypernym, but coordinates do not imply each other. Lion implies cat in the feline sense but not in the house cat sense. Thus these two relations (hyponym\u2013hypernym and coordinate) do not agree on whether lion implies cat. In this case, we believe that the hyponym\u2013hypernym is more natural, so we say that lion implies cat. For people who feel both semantic relations are natural, the third condition says that there is no entailment; for them, lion does not imply cat.\nThe third condition could be modified for different uses. For our dataset (Section 7.3), we chose to err on the side of non-entailment, but ideally the choice would be made based on the downstream application. For some applications, it may be better to err on the side of entailment. One possibility is to give higher weight to some relations and use the weighting to choose between entailment and nonentailment when two or more relations disagree. The weighting could be based on the corpus frequency of the relations or the contexts in which the words appear.\nTo apply the relational definition of lexical entailment, it is helpful to have a taxonomy of semantic relations, to provide options for R. In this paper, we use the taxonomy of Bejar, Chaffin, and Embretson (1991), which includes seventynine subcategories of semantic relations, grouped into ten high-level categories. The taxonomy is given in Tables 2 and 3 in Section 7.3.\nIt might seem that the relational definition redefines lexical entailment in a way that makes our results incomparable with past results, but we believe that our\nand the relational definition are operational definitions: They are tests used to determine the presence of entailment. They both require an understanding of the word implies, but implies is a synonym for entails; they are not theoretical definitions of entailment. They attempt to objectively capture the same underlying notion of implication, and hence they may be compared and contrasted in terms of how well they capture that notion.\nZhitomirsky-Geffet and Dagan\u2019s (2009) substitutional definition of lexical entailment was intended to capture only substitutional cases of entailment. They explicitly excluded non-substitutable lexical entailment. They argue that their two conditions yield good inter-annotator agreement and result in lexical entailment decisions that fit well with the needs of systems for recognizing textual entailment.\nWe believe that there is a trade-off between inter-annotator agreement and coverage. The substitutional and relational definitions differ regarding this trade-off. The substitutional definition leads to higher levels of inter-annotator agreement than the relational definition, but the substitutional definition excludes (by design) important cases of lexical entailment (see Section 7.3.2).\nConsider the following example:\nText: Jane accidentally broke a glass. Hypothesis: Jane accidentally broke something fragile.\nFor the word pair glass:fragile, the typical relation that comes to mind is item:attribute, \u2018an x has the attribute y\u2019 (ID 5a in the semantic relation taxonomy); thus the first condition of the relational definition is fulfilled. An item entails its attributes; glass entails fragile; thus the second condition is fulfilled. There are exceptions, such as bulletproof glass, but bulletproof glass is not typical glass. There is no other typical relation between glass and fragile, so the third condition is fulfilled.\nOne limitation of substitutability as defined by Zhitomirsky-Geffet and Dagan (2009) is that it does not allow lexical entailment from one part of speech to another. For example, glass entails fragile, but glass is a noun and fragile is an adjective, so we cannot substitute one for the other in a sentence. However, in spite of the difference in their parts of speech, it seems reasonable to say that glass entails fragile. In a typical situation that involves glass, the situation also involves something fragile.\nAs another example of a case where the substitutional definition excludes a lexical entailment that the relational definition captures, consider bequeath:heir, an instance of the act:recipient relation (ID 7e in the relation taxonomy):\nText: George bequeathed his estate to Jane. Hypothesis: Jane was the heir to George\u2019s estate.\nIt is reasonable to say that the act of bequeathing entails that there is an heir, although the verb bequeathed cannot be substituted for the noun heir.\nTo address this limitation of the substitutional definition, one possibility would be to relax the definition of substitutability to cope with different parts of speech. For example, given a noun x and an adjective y, we could allow \u2018an x\u2019 (a glass)\nlist of substitutional patterns could handle most part of speech substitution cases. However, we do not pursue this option here, because it does not address a fundamental limitation of the substitutional definition, which is the absence of semantic relations. We believe that semantic relations and lexical entailment are intimately connected (see Section 3).\nThe idea of substitional patterns suggests the generalization of lexical entailment to phrasal entailment. For example, the phrase \u2018x bequeathed y to z\u2019 entails the phrase \u2018z was the heir to x\u2019s y\u2019. Patterns like this have been learned from corpora (Lin and Pantel 2001) and applied successfully to RTE (Mirkin, Bar-Haim, Berant, Dagan, Shnarch, Stern, and Szpektor 2009a). However, our focus here is lexical entailment, not phrasal entailment. We believe that a good algorithm for lexical entailment should be useful as a component in an algorithm for phrasal entailment.\nIn our experiments, we use three different datasets. All three consist of word pairs that have been labeled entails or does not entail. One dataset (Section 7.1) was labeled using Zhitomirsky-Geffet and Dagan\u2019s (2009) substitutional definition. On preliminary inspection, it seems that the semantic relations in this dataset are often part\u2013whole and hyponym\u2013hypernym relations, but the word pairs have not been systematically labeled with relation categories. In another dataset (Section 7.2), all of the pairs that are labeled entails are instances of the hyponym\u2013hypernym relation. In the third dataset (Section 7.3), the pairs were generated from Bejar et al.\u2019s (1991) taxonomy. This dataset includes pairs sampled from all seventy-nine of the subcategories in the taxonomy. Each pair was labeled entails or does not entail based on the subcategory it came from. Tables 2 and 3 in Section 7.3 list all of the subcategories of relations and their entailment labels.\nLexical entailment is sometimes asymmetric (e.g., for word pairs that are instances of the hyponym\u2013hypernym relation) and sometimes symmetric (e.g., for synonyms) (Geffet and Dagan 2005; Kotlerman et al. 2010). Both the substitutional and relational definitions allow this blend of symmetry and asymmetry.\nIn the semantic relation classification literature (discussed in Section 4), supervised learning algorithms are applied to the task of classifying word pairs. In general, these algorithms are capable of classifying both symmetric and asymmetric relations. In particular, ConVecs and SimDiffs both approach lexical entailment as a problem of supervised relation classification, and both are capable of learning symmetric and asymmetric relations. They should be able to learn when lexical entailment behaves asymmetrically (e.g., with cases like glass:fragile) and when it behaves symmetrically (e.g., with cases like car:automobile).\nThe balAPinc measure is designed to capture asymmetry, but it is likely to give approximately equal scores to car:automobile and automobile:car. This can be seen by considering the details of its definition (see Section 6.1)."}, {"heading": "3 Semantic relations and lexical entailment", "text": "Some researchers have applied semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Baroni et al. 2012), but Zhitomirsky-Geffet and Dagan\n... lexical entailment is not just a superset of other known relations, but it is rather designed to select those sub-cases of other lexical relations that are needed for applied entailment inference. For example, lexical entailment does not cover all cases of meronyms (e.g., division does not entail company), but only some sub-cases of part\u2013whole relationship mentioned herein. In addition, some other relations are also covered by lexical entailment, like ocean and water and murder and death, which do not seem to directly correspond to meronymy or hyponymy relations.\nNotice also that whereas lexical entailment is a directional relation that specifies which word of the pair entails the other, the relation may hold in both directions for a pair of words, as is the case for synonyms.\nWe agree with Zhitomirsky-Geffet and Dagan (2009) that some sub-cases of part\u2013 whole involve lexical entailment and other sub-cases do not. However, this issue can be addressed by breaking the part\u2013whole category into subcategories.\nOne of the high-level categories in Bejar et al.\u2019s (1991) taxonomy is part\u2013whole (ID 2 in the taxonomy), which has ten subcategories. We claim that eight of the ten subcategories involve entailment and two do not involve entailment, which is consistent with the claim that \u2018lexical entailment does not cover all cases of meronyms\u2019 (in the above quotation).\nRegarding \u2018ocean and water and murder and death\u2019 (in the above quotation), the word pair ocean:water is an instance of Bejar et al.\u2019s (1991) object:stuff subcategory (ID 2g in the taxonomy) and murder:death is an instance of the cause:effect subcategory (ID 8a). Regarding relations for which there is lexical entailment in both directions, synonymy (ID 3a) is readily handled by marking it as entailing in both directions (see Tables 2 and 3 in Section 7.3).\nWe believe that Zhitomirsky-Geffet and Dagan\u2019s (2009) argument is correct for high-level categories but incorrect for subcategories. We offer the following hypothesis (introduced here):\nSemantic relation subcategories hypothesis: Lexical entailment is not a\nsuperset of high-level categories of semantic relations, but it is a superset of lower-level subcategories of semantic relations.\nThis hypothesis implies a tight connection between research in RLE and research in semantic relation classification.\nConVecs and SimDiffs treat RLE as a semantic relation classification problem. These algorithms do not require the semantic relation subcategories hypothesis: It is possible that it may be fruitful to use ideas from research in semantic relation classification even if the hypothesis is wrong. However, if the semantic relation subcategories hypothesis is correct, then there is even more reason to treat RLE as a semantic relation classification problem.\nWe use the semantic relation subcategories hypothesis in Section 7.3, as a new way of generating a dataset for evaluating RLE algorithms. In our experiments (Section 8), we train the algorithms using data based on Bejar et al.\u2019s (1991) taxonomy and then test them on previous lexical entailment datasets.\nWe do not claim that Bejar et al.\u2019s (1991) taxonomy handles all cases of lexical entailment, but our results suggest that it covers enough cases to be effective. Future\ntaxonomy, but we believe that the taxonomy can be expanded to handle exceptions as they are discovered."}, {"heading": "4 Related work", "text": "The first RTE Challenge took place in 2005 (Dagan, Glickman, and Magnini 2006) and it has been a regular event since then.1 Since the beginning, many RTE systems have included a module for recognizing lexical entailment (Hickl, Bensley, Williams, Roberts, Rink, and Shi 2006; Herrera, Pen\u0303as, and Verdejo 2006). The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005).\nLee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence. Weeds and Weir (2003) introduced an asymmetric similarity measure for the degree to which a specific term a is subsumed by a more general term b (see also Weeds, Weir, and McCarthy 2004). This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010). We describe balAPinc in detail in Section 6.1.\nGlickman, Dagan, and Shnarch (2006) define lexical reference, which is somewhat similar to lexical entailment, but it is defined relative to a specific text, such as a sentence. Mirkin, Dagan, and Shnarch (2009b) define entailment between lexical elements, which includes entailment between words and non-compositional elements. Their definition is not based on substitutability; they accept many kinds of lexical entailment that are excluded by substitutability. Their definition involves what can be inferred from a lexical element in the context of some natural text.\nCompared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007). Semantic relation classification has been part of several SemEval (Semantic Evaluation) exercises:2\n\u2022 SemEval-2007 Task 4: Classification of Semantic Relations between Nominals\n(Girju et al. 2007) \u2013 seven semantic relation classes\n\u2022 SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Be-\ntween Pairs of Nominals (Hendrickx, Kim, Kozareva, Nakov, Se\u0301aghdha, Pado\u0301,\n1 The RTE Challenge usually takes place once a year. See the Textual Entailment Portal at http://aclweb.org/aclwiki for more information. 2 See the SemEval Portal at http://aclweb.org/aclwiki for more information.\n\u2013 nine semantic relation classes\n\u2022 SemEval-2012 Task 2: Measuring Degrees of Relational Similarity (Jurgens\net al. 2012) \u2013 seventy-nine semantic relation classes\nOnly a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012). All of these papers emphasize the hyponym\u2013hypernym semantic relation, which is important for lexical entailment, but it is not the only relation that involves entailment.\nBaroni et al. (2012) compared their ConVecs algorithm with the balAPinc measure and found no significant difference in their performance. They also consider how quantifiers (e.g., some, all) affect entailment.\nMost algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992). One objection to supervised learning for lexical entailment is that it can require a large quantity of labeled training data.\nBaroni et al. (2012) offer an elegant solution to the training data issue, based on the observation that, in adjective\u2013noun phrases, the adjective\u2013noun pair generally entails the head noun. For example, big cat entails cat. This observation allows them to label a large quantity of training data with relatively little effort. However, their technique does not seem to be applicable to many of the relevant subcategories in Bejar et al.\u2019s (1991) taxonomy. Our solution is to use word pairs that were labeled with Bejar et al.\u2019s (1991) classes using Amazon\u2019s Mechanical Turk (Jurgens et al. 2012). (See Section 7.3.) This dataset covers a much wider range of semantic relations than Baroni et al.\u2019s (2012) dataset."}, {"heading": "5 Performance measures", "text": "One difference between an asymmetric similarity measure (such as balAPinc) and a classification model based on supervised machine learning (such as ConVecs or SimDiffs) is that the former yields a real-valued score whereas the latter gives a binary-valued classification (0 = does not entail and 1 = entails). However, this difference is superficial. Many supervised learning algorithms (including the algorithms we use here) are able to generate a real-valued probability score (the probability that the given example belongs in class 1). Likewise, it is easy to generate a binary-valued class from a real-valued score by setting a threshold on the score.\nIn our experiments (Section 8), we evaluate all three algorithms both as realvalued asymmetric similarity measures and binary-valued classifiers. We use average precision (AP) as a performance measure for real-valued scores, following Kotlerman et al. (2010). We use precision, recall, F-measure, and accuracy as performance measures for binary-valued classification, following Baroni et al. (2012). The balAPinc measure (balanced average precision for distributional inclusion) is\nprecision now, before we discuss balAPinc (in Section 6.1)."}, {"heading": "5.1 Average precision", "text": "AP was originally designed as a performance measure for information retrieval systems. Suppose we have issued a query to a search engine and it has returned a ranked list of N documents, sorted in descending order of their automatically estimated degree of relevance for our query. Assume that human judges have manually labeled all of the documents as either relevant or irrelevant for the given query. Let P(r) be the fraction of the top r highest ranked documents that have the label relevant. That is, P(r) is the precision of the ranked list if we cut the list off after the r-th document. Let rel(r) be 1 if the r-th document is labeled relevant, 0 otherwise. AP is defined as follows (Buckley and Voorhees 2000):\nAP =\n\u2211N\nr=1[P(r) \u00b7 rel(r)]\ntotal number of relevant documents (1)\nAP ranges from 0 (very poor performance) to 1 (perfect performance). Buckley and Voorhees (2000) demonstrate that AP is more stable and more discriminating than several alternative performance measures for information retrieval systems.\nThe definition of AP reflects a bias in information retrieval. For a typical query and a typical document collection, most documents are irrelevant and the emphasis is on finding the few relevant documents. In machine learning, if we have two classes, 0 and 1, they are usually considered equally important. Kotlerman et al. (2010) emphasize the class 1 (entails), but we believe class 0 (does not entail) is also important. For example, the scoring of the RTE Challenge gives an equal reward for recognizing when a text sentence entails a hypothesis sentence and when it does not. Therefore we report two variations of AP, which we call AP0 (average precision with respect to class 0) and AP1 (average precision with respect to class 1), which we define in the next paragraph.\nSuppose we have a dataset of word pairs manually labeled 0 and 1. Let N be the number of word pairs in the dataset. Let M(a, b) \u2208 \u211c be a measure that assigns a real-valued score to each word pair, a : b. Sort the pairs in descending order of their M(a, b) scores. Let P1(r) be the fraction of the top r highest ranked pairs that have the label 1. Let P0(r) be the fraction of the bottom r lowest ranked pairs that have the label 0. Let C1(r) be 1 if the r-th document from the top is labeled 1, 0 otherwise. Let C0(r) be 1 if the r-th document from the bottom is labeled 0, 0 otherwise. Let N0 be the total number of pairs labeled 0 and let N1 be the total number of pairs labeled 1. We define AP0 and AP1 as follows:\nAP0 =\n\u2211N\nr=1[P0(r) \u00b7 C0(r)]\nN0 (2)\nAP1 =\n\u2211N\nr=1[P1(r) \u00b7 C1(r)]\nN1 (3)\nto increase a system\u2019s performance according to AP1 at the cost of lower AP0 performance. The formula for AP1 is more sensitive to the labels in the top of the list. What happens at the bottom of the list has little impact on AP1, because P1(r) gives a low weight to labels at the bottom of the list. On the other hand, the formula for AP0 is more sensitive to labels at the bottom of the list. If we focus on AP1 and ignore AP0, we will prefer algorithms that get the top of the list right, even if they do poorly with the bottom of the list. Therefore it is important to report both AP0 and AP1."}, {"heading": "5.2 Precision, recall, F-measure, and accuracy", "text": "Like AP, precision and recall were originally designed as performance measures for information retrieval systems. The precision of a system is an estimate of the conditional probability that a document is truly relevant to a query, if the system says it is relevant. The recall of a system is an estimate of the conditional probability that the system will say that a document is relevant to a query, if it truly is relevant.\nThere is a tradeoff between precision and recall; one may be optimized at the cost of the other. The F-measure is the harmonic mean of precision and recall. It is designed to reward a balance of precision and recall.\nAccuracy is a natural and intuitive performance measure, but it is sensitive to the relative sizes of the classes. It is easy to interpret accuracy when we have two equal-sized classes, but it is difficult to interpret when one class is much larger than the other. The F-measure is a better measure when the classes are not balanced.\nAs with AP, there are two variations of precision, recall, and F-measure, depending on whether we focus on class 0 or class 1. Let C be a 2 \u00d7 2 confusion matrix, where cij is the number of word pairs that are actually in class i and the algorithm has predicted that they are in class j (here i, j \u2208 {0, 1}). We define precision, recall, and F-measure as follows:\nPre0 = c00/(c00 + c10) (4)\nPre1 = c11/(c11 + c01) (5)\nRec0 = c00/(c00 + c01) (6)\nRec1 = c11/(c11 + c10) (7)\nF0 = 2 \u00b7 Pre0 \u00b7 Rec0/(Pre0 +Rec0) (8)\nF1 = 2 \u00b7 Pre1 \u00b7 Rec1/(Pre1 +Rec1) (9)\nFollowing standard practice (Witten, Frank, and Hall 2011), we merge the two variations of each measure by taking their weighted averages, where the weights are determined by the class sizes:\nw0 = (c00 + c01)/(c00 + c01 + c10 + c11) (10)\nPre = w0 \u00b7 Pre0 + w1 \u00b7 Pre1 (12)\nRec = w0 \u00b7Rec0 + w1 \u00b7Rec1 (13)\nF = w0 \u00b7 F0 + w1 \u00b7 F1 (14)\nFinally, we define accuracy as usual:\nAcc = 100 \u00b7 (c00 + c11)/(c00 + c01 + c10 + c11) (15)\nThe factor of 100 converts the accuracy from a fraction to a percentage score."}, {"heading": "6 Three approaches to lexical entailment", "text": "In this section, we discuss the three approaches to RLE and describe the algorithms for each approach in detail. All three approaches are based on word\u2013context matrices. For an introduction to the concepts behind word\u2013context matrices, see the survey paper by Turney and Pantel (2010).\nIn preliminary experiments with our development datasets, Dev1 and Dev2, we tuned the three approaches to optimize their performance. We describe how Dev1 and Dev2 were generated in Section 8.1.1. For each algorithm, we selected the matrix or matrices that were most accurate with the development data. For both balAPinc and ConVecs, we chose the word\u2013context matrix from Turney, Neuman, Assaf, and Cohen (2011). For SimDiffs, we chose two word\u2013context matrices from Turney (2012).3\nConVecs and SimDiffs use support vector machines (SVMs) for supervised learning. We used the development datasets to select the best kernels for the SVMs. The best kernel for ConVecs was a second-degree polynomial kernel and the best kernel for SimDiffs was a radial basis function (RBF) kernel."}, {"heading": "6.1 The context inclusion hypothesis: balAPinc", "text": "We include balAPinc in our experiments because Kotlerman et al. (2010) experimentally compared it with a wide range of asymmetric similarity measures and found that balAPinc had the best performance. The balAPinc asymmetric similarity measure is a balanced combination of the asymmetric APinc measure (Kotlerman et al. 2010) with the symmetric LIN measure (Lin 1998). Balance is achieved by using the geometric mean:\nbalAPinc(u, v) = \u221a APinc(u, v) \u00b7 LIN(u, v) (16)\nTo define APinc and LIN, we must first introduce some terminology. Kotlerman et al. (2010) define balAPinc with terminology from set theory, whereas ConVecs\n3 Copies of all three matrices used here are available from the first author by request.\nwill use the set theoretical terminology of Kotlerman et al. (2010) and the linear algebraic terminology of Turney and Pantel (2010), so that the reader can easily see both perspectives. This leads to a small amount of redundancy, but we believe it is helpful to connect the two points of view.4\nFirst, some linear algebraic notation: Suppose that we have a word\u2013context matrix, in which each row vector corresponds to a word and each column vector corresponds to a context. Let F be the matrix of raw co-occurrence frequencies. If w is the word corresponding to the i-th row vector, fi:, and c is the context corresponding to the j-th column vector, f:j , then fij is the number of times w occurs in the context c in the given corpus.\nLet the matrix X be the result of calculating the positive pointwise mutual information (PPMI) between the word w and the context c for each element fij in F (Bullinaria and Levy 2007; Turney and Pantel 2010). PPMI takes the raw co-occurrence frequencies and transforms them to weights that represent the importance of a given context for a given word. The PPMI matrix X is typically sparse (most cells are zero) and no cells are negative.5\nThe matrix X has the same number of rows (nr) and columns (nc) as the raw frequency matrix F. The value of an element xij in X is defined as follows (Turney and Pantel 2010):\npij = fij\n\u2211nr i=1 \u2211nc j=1 fij\n(17)\npi\u2217 =\n\u2211nc j=1 fij\n\u2211nr i=1 \u2211nc j=1 fij\n(18)\np\u2217j =\n\u2211nr i=1 fij\n\u2211nr i=1 \u2211nc j=1 fij\n(19)\npmiij = log\n(\npij pi\u2217p\u2217j\n)\n(20)\nxij =\n{\npmiij if pmiij > 0\n0 otherwise (21)\nNow, some set theoretical notation: Given a word w corresponding to the i-th row in X, let Fw be the set of contexts for which xij is nonzero. That is, c \u2208 Fw if and only if xij 6= 0, where w corresponds to row i and c corresponds to column j. We may think of the contexts in the set Fw as features that characterize the word w. Let |Fw| be the number of features in Fw. If w corresponds to the i-th row in X, then |Fw| is the number of nonzero cells in the i-th row vector, xi:.\n4 ConVecs and SimDiffs are fundamentally linear algebraic in conception, whereas balAPinc is fundamentally set theoretic. We cannot readily describe all three systems with only one kind of notation. 5 Other measures of word association may be used instead of PPMI. See Chapter 5 of Manning and Schu\u0308tze (1999) for a good survey of association measures.\nin Fw in descending order of their corresponding PPMI values. Let fwr be the r-th feature in the ranking of Fw, where r ranges from 1 to |Fw|. Let rank(f, Fw) be the rank of f in Fw. Thus rank(fwr, Fw) = r. We want to normalize this rank so that it ranges between 0 and 1, where higher PPMI values are closer to 1 and lower PPMI values are closer to 0. The function rel(f, Fw) provides this normalization:\nrel(f, Fw) =\n{\n1\u2212 rank(f,Fw)|Fw|+1 if f \u2208 Fw\n0 if f /\u2208 Fw (22)\nWe may interpret rel(f, Fw) as a measure of the importance of the feature f for characterizing the word w. This function is called rel because it is somewhat analogous to relevance in information retrieval.\nRecall the context inclusion hypothesis: If a word u tends to occur in a subset of the contexts in which a word v occurs (v contextually includes u), then u (the narrower term) tends to entail v (the broader term). Suppose we test the features of u, f \u2208 Fu, in order of their rank, r, to see which features of u are contextually included in v. Let inc(r, Fu, Fv) be the set consisting of those features, among the first r features in Fu, that are included in Fv:\ninc(r, Fu, Fv) = {f | rank(f, Fu) \u2264 r and f \u2208 (Fu \u2229 Fv)} (23)\nThe size of this set, |inc(r, Fu, Fv)|, ranges from 0 to r, where r \u2264 |Fu|. The function P(r, Fu, Fv) normalizes the size to range from 0 to 1:\nP(r, Fu, Fv) = |inc(r, Fu, Fv)|\nr (24)\nWe may interpret P(r, Fu, Fv) as a measure of the density of Fv features among the top r features of Fu. This function is called P because it is somewhat analogous to precision in information retrieval.\nNow we are ready to define APinc:\nAPinc(u, v) =\n\u2211|Fu| r=1 [P(r, Fu, Fv) \u00b7 rel(fur, Fv)]\n|Fu| (25)\nAPinc is a variation of the average precision (AP) measure, originally developed for measuring the performance of information retrieval systems (see Section 5.1). Consider the first term in the sum, r = 1. If fu1, the highest-ranking feature in Fu, is included in Fv, then P(1, Fu, Fv) will be 1; otherwise it will be 0. If fu1 is in Fv, then the product P(1, Fu, Fv) \u00b7 rel(fu1, Fv) reduces to rel(fu1, Fv), the importance of the feature fu1 for the word v. APinc will have a high score when the most important features of u are also important features of v. APinc is asymmetric because it does not require that the most important features of v are important features of u.\nLet wu(f) be the weight of the feature f in the word u. The weight is given by the PPMI value in X. If u corresponds to the i-th row and f corresponds to the j-th column, then wu(f) = xij . (It may seem redundant to have both wu(f) and\nfollows (Lin 1998):\nLIN(u, v) =\n\u2211\nf\u2208Fu\u2229Fv [wu(f) + wv(f)]\n\u2211\nf\u2208Fu wu(f) +\n\u2211\nf\u2208Fv wv(f)\n(26)\nIn balAPinc (Equation 16), the LIN measure is combined with the APinc measure because the APinc measure by itself tends to be sensitive to cases where |Fu| or |Fv| are unusually small (Kotlerman et al. 2010).\nThere are two parameters, maxF and T , that can be varied to control the performance of balAPinc. The parameter maxF sets the maximum number of features for each word. For a given word w, we calculate all of the features, Fw. If |Fw | > maxF , then we remove the lowest-ranking features until |Fw| = maxF . This reduces the impact of low-ranking features on the APinc score. The parameter T is a threshold for classification. If balAPinc(u, v) < T , then the word pair u : v is classified as 0 (does not entail); otherwise, u :v is classified as 1 (entails). We describe how these parameters are tuned in Section 8.\nKotlerman et al. (2010) do not use the threshold T , since they do not evaluate balAPinc as a classifier. They also do not use the parameter maxF , although their analysis supports the utility of this parameter; see Section 5.4.4 of Kotlerman et al. (2010).\nIn the experiments with balAPinc in Section 8, the PPMI matrix X is the same matrix as used by Turney et al. (2011). The matrix has 114,501 rows and 139,246 columns. The rows correspond to single and multi-word entries (n-grams) in WordNet and the columns correspond to unigrams in WordNet, distinguished according to whether they appear in the left or right context of the given n-gram. The window size for context is four words to the left and four words to the right of the n-gram. The matrix has a density (percentage of nonzero values) of 1.22%.\nThe PPMI matrix is based on a corpus of 5\u00d71010 words, collected from university websites by a webcrawler.6 The corpus was indexed with the Wumpus search engine (Bu\u0308ttcher and Clarke 2005), which is designed for passage retrieval, rather than document retrieval.7 Suppose fij is an element in the matrix of raw co-occurrence frequencies F. The i-th row of the matrix corresponds to an n-gram w in WordNet and the j-th column of the matrix corresponds to a unigram c. The value of fij was calculated by sending the query w to Wumpus and counting the frequency of c in the retrieved passages. The matrix is described in detail in Section 2.1 of Turney et al. (2011).\nIt is common to smooth the PPMI matrix by applying a truncated singular value decomposition (SVD) (Turney and Pantel 2010). On the development datasets, we experimented with smoothing the matrix but the results were poor. The problem is that the truncated SVD yields a matrix with a density of 100%, but balAPinc is designed for highly sparse matrices.\n6 The corpus was collected by Charles Clarke at the University of Waterloo. 7 Wumpus is available at http://www.wumpus-search.org/.\nof the contexts (all of the matrix columns) are nonzero, so Fu and Fv are simply the entire set of features, and (Fu \u2229 Fv) is also the entire set of features. Likewise, in Equation 26, all of the sums, \u2211\nf , range over the entire set of features. The\nequations behind balAPinc are based on the assumption that most of the elements in the matrix are zero (i.e., the matrix is sparse), but this assumption is false if we apply a truncated SVD.\nIn the experiments in Section 8, we use the raw PPMI matrix, with no SVD smoothing. Baroni et al. (2012) also found that balAPinc works better without SVD smoothing (see their Footnote 3)."}, {"heading": "6.2 The context combination hypothesis: ConVecs", "text": "With the ConVecs algorithm, Baroni et al. (2012) were able to match the performance of balAPinc. In ConVecs, we represent a word pair a :b by the concatentation of the context vectors a for a and b for b. We apply a supervised learning algorithm to a training set of word pairs, where each word pair is represented by concatenated context vectors that are labeled entails or does not entail. The supervised learning algorithm generates a classification model, which enables us to assign labels to new word pairs, not present in the training data.\nLet X be a word\u2013context matrix, where the value of the cell xij in X is given by the PPMI between the i-th word w and the j-th context c. In our experiments, we use the word\u2013context matrix X from Turney et al. (2011), as in Section 6.1, but now we smooth X with a truncated SVD.\nSVD decomposes X into the product of three matrices U\u03a3VT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UTU = VTV = I) and \u03a3 is a diagonal matrix of singular values (Golub and Van Loan 1996). If X is of rank r, then \u03a3 is also of rank r. Let \u03a3k, where k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by selecting the corresponding columns from U and V. The matrix Uk\u03a3kV T k is the matrix of rank k that best approximates the original matrix X, in that it minimizes the approximation errors. That is, X\u0302 = Uk\u03a3kV T k minimizes \u2016X\u0302\u2212X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub and Van Loan 1996).\nWe represent a word pair a : b using row vectors from the matrix Uk\u03a3 p k. If a and b correspond to row vectors a and b in Uk\u03a3 p k, then a : b is represented by the 2k-dimensional vector that is the concatenation of a and b. We normalize a and b to unit length before we concatenate them.\nThere are two parameters in Uk\u03a3 p k that need to be set. The parameter k controls the number of latent factors and the parameter p adjusts the weights of the factors, by raising the corresponding singular values in \u03a3pk to the power p.\nThe parameter k is well-known in the literature (Landauer, McNamara, Dennis, and Kintsch 2007), but p is less familiar. Caron (2001) introduced p for improving the performance of truncated SVD with term\u2013document matrices in information retrieval. The use of p to improve the performance with word\u2013context matrices in\n(2012) and Turney (2012). In the following experiments (Section 8), we explore a range of values for p and k. Baroni et al. (2012) use k = 300 and p = 1.8\nRecall the context combination hypothesis: The tendency of a to entail b is correlated with some learnable function of the contexts in which a occurs and the contexts in which b occurs; some conjunctions of contexts tend to indicate entailment and others tend to indicate a lack of entailment. Given the context combination hypothesis, vector concatenation is a natural way to represent a : b for learning lexical entailment.\nFor their supervised learning algorithm, Baroni et al. (2012) used Weka with LIBSVM.9 They used a polynomial kernel for the support vector machine (SVM). We also use Weka and a polynomial kernel, but we use the sequential minimal optimization (SMO) SVM in Weka (Platt 1998), because it can generate real-valued probability estimates, as well as binary-valued classes. The probability estimates are based on fitting the outputs of the SVM with logistic regression models (Witten et al. 2011).\nWe tried various kernels with ConVecs on the development datasets (Dev1 and Dev2; see Section 8.1.1), and found that a second-degree polynomial kernel had the best performance. We use the default settings for the polynomial kernel SMO SVM in Weka, except we disable normalization, because the vectors are already normalized to the same length.\nIt seems to us that ConVecs is a good algorithm for a generic semantic relation, but a representation that takes advantage of some background knowledge about lexical entailment might require less training data. One thing we know about lexical entailment is a |= a, for any a. ConVecs can only reliably recognize that a |= a if a is similar to some x, such that the word pair x : x appears in the training data and has been labeled entails. To cover a broad range of possible values for a, there must be many different x :x pairs in the training data. The ConVecs representation does not make efficient use of the training data."}, {"heading": "6.3 The similarity differences hypothesis: SimDiffs", "text": "SimDiffs uses two different word\u2013context matrices, a domain matrix, D, and a function matrix, F (Turney 2012). The domain matrix is designed for measuring the domain similarity between two words (similarity of topic, subject, or field). For example, carpenter and wood have a high degree of domain similarity; they both come from the domain of carpentry. The function matrix is designed for measuring function similarity (similarity of role, relationship, or usage). For example, carpenter and mason have a high degree of function similarity; they both function as artisans.\n8 Baroni et al. (2012) mention k = 300 in their Footnote 3. In personal communication in November 2012, they said they used p = 1. 9 Weka is available at http://www.cs.waikato.ac.nz/ml/weka/ and LIBSVM is available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/.\nnouns that occur near a given word as the context for the word, whereas the function matrix uses the verbs that occur near the given word. The part-of-speech information was generated with the OpenNLP tagger.10 Our motivation for using two matrices in SimDiffs is to generate a larger and more varied set of features for the supervised learning algorithm. Turney (2012) demonstrated that domain and function matrices work together synergetically when applied to semantic relations.\nIn experiments with the development datasets (Dev1 and Dev2), we tried using the domain and function matrices with balAPinc and ConVecs, but both algorithms worked better with the word\u2013context matrix from Turney et al. (2011). For SimDiffs, the combination of the domain and function matrices from Turney (2012) had the best performance on the development datasets.\nBoth D and F use PPMI and SVD, as in Section 6.2. This results in a total of four parameters that need to be tuned, kd and pd for domain space and kf and pf for function space. In the following experiments (Section 8), to simplify the search through parameter space, we make kd = kf and pd = pf .\nThe domain and function matrices are based on the same corpus as the word\u2013 context matrix from Turney et al. (2011). Wumpus was used to index the corpus and search for passages, in the same way as described in Section 6.1. D has 114,297 rows and 50,000 columns. The PPMI matrix has a density of 2.62%. F has 114,101 rows and 50,000 columns. The PPMI matrix has a density of 1.21%. For both matrices, truncated SVD results in a density of 100%.\nThe rows for both matrices correspond to single and multi-word entries (n-grams) in WordNet. The columns are more complex; Turney (2012) provides a detailed description of the columns and other aspects of the matrices. The matrices have different numbers of rows because, before applying SVD, we removed rows that were entirely zero. The function matrix, with its lower density, had more zero-valued rows than the domain matrix.\nSuppose that the words a and b correspond to the row vectors ad and bd in the domain matrix, D. Let simd(a, b) be the similarity of a and b in domain space, as measured by the cosine of the angle between ad and bd, cos(ad,bd). Likewise, let simf(a, b) be cos(af ,bf), the cosine in function space.\nLet R be a set of reference words. Recall the similarity differences hypothesis: The tendency of a to entail b is correlated with some learnable function of the differences in their similarities, sim(a, r) \u2212 sim(b, r), to a set of reference words, r \u2208 R; some differences tend to indicate entailment and others tend to indicate a lack of entailment. In SimDiffs, we represent a word pair a :b with a feature vector composed of four sets of features, S1, S2, S3, and S4, defined as follows:\nS1 = {simd(a, r)\u2212 simd(b, r) | r \u2208 R} (27)\nS2 = {simf(a, r) \u2212 simf(b, r) | r \u2208 R} (28)\n10 OpenNLP is available at http://opennlp.apache.org/.\nS4 = {simf(a, r) \u2212 simd(b, r) | r \u2208 R} (30)\nS1 is the difference between a and b in domain space, with respect to their similarities to the reference words, R. S2 is the difference between a and b in function space. S1 and S2 are based on differences in the same spaces, whereas S3 and S4 are based on differences in different spaces.\nThe cross-spatial differences (S3 and S4) may seem counterintuitive. Consider the example murder |= death, suggested by the quotation from Zhitomirsky-Geffet and Dagan (2009) in Section 3. Murder typically involves two people, the victim and the aggressor, whereas death typically involves one person, the deceased. This suggests that there is a functional difference between the words, hence the function similarities of murder may be quite different from the function similarities of death. However, perhaps the domain similarities of murder are somewhat similar to the function similarities of death (S3) or perhaps the function similarities of murder are somewhat similar to the domain similarities of death (S4). We include these similarities here to see if the supervised learning algorithm can make use of them.\nFor R, the set of reference words, we use 2,086 words from Basic English (Ogden 1930).11 Thus a word pair a : b is represented by 2,086 \u00d7 4 = 8,344 features. The words of Basic English were selected by Ogden (1930) to form a core vocabulary, sufficient to represent most other English words by paraphrasing. We chose this set of words because it is small enough to keep the number of features manageable yet broad enough to cover a wide range of concepts. Other reference words may also be suitable; this is a topic for future work.\nWe mentioned in Section 6.2 that ConVecs may be inefficient for learning a |= a. On the other hand, consider how a |= a is represented in SimDiffs. Looking at Equations 27 and 28, we see that, given the word pair a :a, every feature in S1 and S2 will have the value zero. Therefore it should not take many examples of x :x in the training data to learn that a |= a, for any a.\nFor our supervised learning algorithm, we use the SMO SVM in Weka. Based on experiments with the development datasets (Dev1 and Dev2), we use a radial basis function (RBF) kernel. We use the default settings, except we disable normalization. We generate probability estimates for the classes."}, {"heading": "7 Three datasets for lexical entailment", "text": "This section describes the three datasets we use in our experiments. The first two datasets have been used in the past for lexical entailment research. The third dataset has been used for semantic relation research; this is the first time it has been used for lexical entailment. We refer to each dataset by the initials of the authors of the paper in which it was first reported.\n11 This word list is available at http://ogden.basic-english.org/word2000.html.\nThe KDSZ dataset was introduced by Kotlerman et al. (2010) to evaluate balAPinc. The dataset contains 3,772 word pairs, 1,068 labeled entails and 2,704 labeled does not entail. It was created by taking a dataset of 3,200 labeled word pairs from Zhitomirsky-Geffet and Dagan (2009) and adding 572 more labeled pairs.12 The labeling of the original subset of 3,200 pairs is described in detail by ZhitomirskyGeffet and Dagan (2009). The definition of lexical entailment that the judges used was the substitutional definition given in Section 2. Three judges labeled the pairs, with inter-annotator agreement between any two of the three judges varying from 90.0% to 93.5%.\nThis dataset has two properties that complicate the experiments. First, the class sizes are not balanced; 71.7% of the pairs are labeled does not entail and 28.3% are labeled entails. Second, although every word pair is unique, there are a few words that appear many times, in many different pairs. We address these points in our experiments.\nThe words in the word pairs are mainly unigrams, but there are a few bigrams (central bank, higher education, state government). Fortunately all of the bigrams appear in WordNet, so they have corresponding row vectors in our matrices."}, {"heading": "7.2 The BBDS dataset", "text": "The BBDS dataset was created by Baroni et al. (2012) and has been applied to evaluating both balAPinc and ConVecs. In their paper, Baroni et al. (2012) discuss several different datasets. We use the dataset they call N1 |= N2, described in their Section 3.3. The dataset contains 2,770 word pairs, 1,385 labeled entails and 1,385 labeled does not entail. All of the 1,385 pairs labeled entails are hyponym\u2013hypernym noun\u2013noun pairs, such as pope |= leader. The pairs were generated automatically from WordNet and then validated manually.\nAlthough the class sizes are balanced, 50% entails and 50% does not entail, the BBDS dataset is not representative of the variety of semantic relations that involve entailment, as we will see in Section 7.3. Also, although every word pair is unique, there are a few words that appear many times. All of the word pairs are composed of unigrams and all of the unigrams appear in WordNet, so they have corresponding row vectors in our matrices."}, {"heading": "7.3 The JMTH dataset", "text": "Jurgens et al. (2012) created a semantic relation dataset for SemEval-2012 Task 2: Measuring Degrees of Relational Similarity.13 This dataset contains 3,218 word pairs labeled with seventy-nine types of semantic relations. In this section, we describe"}, {"heading": "12 Personal communication with Zhitomirsky-Geffet in March 2012.", "text": "13 The dataset is available at https://sites.google.com/site/semeval2012task2/. We\nused the package called SemEval-2012-Gold-Ratings.\nType Example\n1. ID 1a 2. Category class-inclusion 3. Subcategory taxonomic 4. Paradigmatic examples (x :y) flower:tulip, emotion:rage, poem:sonnet 5. Relational schema y is a kind/type/instance of x 6. Turker examples (Phase 1) fruit:grape, song:opera, rodent:mouse, ... 7. Turker ratings (Phase 2) fruit:grape: 24.0, song:opera: -18.0, ...\nthe original SemEval-2012 dataset and the process we used to convert the dataset into 2,308 word pairs, 1,154 labeled entails and 1,154 labeled does not entail.\nThe original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories.\nFor each subcategory in Bejar et al.\u2019s (1991) relation taxonomy, we have several types of information, shown in Table 1. The first four types of information come from Bejar et al. (1991) and the rest were added by Jurgens et al. (2012).14\nThe original SemEval-2012 dataset was generated in two phases, using Amazon\u2019s Mechanical Turk (Jurgens et al. 2012).15 We refer to Mechanical Turk workers as Turkers. In the first phase, for each of the seventy-nine subcategories, Turkers were shown paradigmatic examples of word pairs in the given subcategory, and they were asked to generate more word pairs of the same semantic relation type. In the second phase, for each of the seventy-nine subcategories, Turkers were shown word pairs that were generated in the first phase, and they were asked to rate the pairs according to their degree of prototypicality for the given semantic relation type. (See Table 1 for examples of the results of the two phases.)\nWe transformed the original SemEval-2012 semantic relation dataset to the new\nlexical entailment dataset in four steps:\n1. Cleaning: To improve the quality of the dataset, we removed the ten lowest-\nrated word pairs from each subcategory. Since the original dataset has 3,218 word pairs, the average subcategory has 40.7 word pairs. Our cleaning operation reduced this to 30.7 pairs per subcategory, a total of 2,428 word pairs (3218\u2212 79\u00d7 10 = 2428). 2. Doubling: For each word pair a : b labeled with a subcategory X , we gener-\nated a new word pair b : a and labeled it X\u22121. For example, car:engine is\n14 All of this information is provided in the file SemEval-2012-Complete-Data-Package at https://sites.google.com/site/semeval2012task2/download. 15 See https://www.mturk.com/.\nobject :component\u22121. This increased the number of pairs to 4,856 and the number of subcategories to 158.\n3. Mapping: We then mapped the 158 subcategory labels to the labels 0 (does\nnot entail) and 1 (entails). The mapping is given in Tables 2 and 3. We assume all word pairs within a subcategory belong to the same class (either all entail or none entail). (This assumption is tested in Section 7.3.2.) The result of mapping was 4,856 word pairs with two labels. There were 1,154 pairs labeled 1 and 3,702 pairs labeled 0.\n4. Balancing: To make a balanced dataset, we randomly removed pairs labeled\n0 until there were 1,154 pairs labeled 0 and 1,154 pairs labeled 1, a total of 2,308 word pairs.\nHere is how to interpret Tables 2 and 3: Given the pair anesthetic:numbness with the label instrument:goal, we see from Table 3 (ID 8f) that a |= b has the value 1, so we map the label instrument:goal to the label 1 (entails). Given the pair numbness:anesthetic labeled instrument:goal\u22121, we see from the table (ID 8f) that b |= a has the value 0, so we map the label instrument:goal\u22121 to the label 0 (does not entail). In other words, anesthetic |= numbness:\nText: Due to the anesthetic, Jane felt no pain. Hypothesis: Due to the numbness, Jane felt no pain. Text |= Hypothesis\nHowever, numbness 6|= anesthetic; for example, the numbness might be caused by cold temperature, not anesthetic:\nText: George had frostbite. Due to the numbness, he felt no pain. Hypothesis: Due to the anesthetic, he felt no pain. Text 6|= Hypothesis\nOf the seventy-nine subcategories, twenty-five were labeled with class 1 for a |= b and twelve were labeled with class 1 for b |= a. Note that two of the subcategories are labeled with class 1 for both a |= b and b |= a (IDs 3a and 9e). This shows that our transformation handles both symmetric and asymmetric semantic relations.\nID Category Subcategory Example (a :b) a |= b b |= a\n1a class-inclusion taxonomic flower:tulip 0 1 1b class-inclusion functional weapon:knife 0 1 1c class-inclusion singular:collective cutlery:spoon 0 1 1d class-inclusion plural:collective dishes:saucers 0 1 1e class-inclusion class:individual mountain:Everest 0 1 2a part-whole object:component car:engine 1 0 2b part-whole collection:member forest:tree 1 0 2c part-whole mass:potion time:moment 1 0 2d part-whole event:feature banquet:food 1 0 2e part-whole stage:activity kickoff:football 0 1 2f part-whole item:topological part room:corner 1 0 2g part-whole object:stuff glacier:ice 1 0 2h part-whole creature:possession millionaire:money 1 0 2i part-whole item:nonpart horse:wings 0 0 2j part-whole item:ex-part prisoner:freedom 0 0 3a similar synonymity car:auto 1 1 3b similar dimension similarity simmer:boil 0 0 3c similar dimension excessive concerned:obsessed 0 1 3d similar dimension naughty listen:eavesdrop 0 1 3e similar conversion grape:wine 0 0 3f similar attribute similarity rake:fork 0 0 3g similar coordinates son:daughter 0 0 3h similar change crescendo:sound 1 0 4a contrast contradictory alive:dead 0 0 4b contrast contrary happy:sad 0 0 4c contrast reverse buy:sell 0 0 4d contrast directional left:right 0 0 4e contrast incompatible slow:stationary 0 0 4f contrast asymmetric contrary hot:cool 0 0 4g contrast pseudoantonym popular:shy 0 0 4h contrast defective limp:walk 1 0 5a attribute item:attribute glass:fragile 1 0 5b attribute attribute:condition edible:eaten 0 0 5c attribute object:state beggar:poverty 1 0 5d attribute attribute:state contentious:conflict 1 0 5e attribute object:typical act soldier:fight 0 0 5f attribute attribute:typical act viable:live 0 0 5g attribute act:act attribute creep:slow 1 0 5h attribute act:object attribute sterilize:infectious 0 0 5i attribute act:resultant rain:wet 1 0\nID Category Subcategory Example (a :b) a |= b b |= a\n6a non-attribute item:nonattribute harmony:discordant 0 0 6b non-attribute attr.:noncondition brittle:molded 0 0 6c non-attribute object:nonstate laureate:dishonor 0 0 6d non-attribute attr.:nonstate dull:cunning 0 0 6e non-attribute obj.:atypical act recluse:socialize 0 0 6f non-attribute attr.:atypical act reticent:talk 0 0 6g non-attribute act:act nonattr. creep:fast 0 0 6h non-attribute act:object nonattr. embellish:austere 0 0 7a case relations agent:object tailor:suit 0 0 7b case relations agent:recipient doctor:patient 0 0 7c case relations agent:instrument farmer:tractor 0 0 7d case relations act:object plow:earth 0 0 7e case relations act:recipient bequeath:heir 1 0 7f case relations object:recipient speech:audience 0 0 7g case relations object:instrument pipe:wrench 0 0 7h case relations recipient:instr. graduate:diploma 0 0 8a cause-purpose cause:effect enigma:puzzlement 1 0 8b cause-purpose cause:counteract hunger:eat 0 0 8c cause-purpose enabler:object match:candle 0 0 8d cause-purpose act:goal flee:escape 0 0 8e cause-purpose agent:goal climber:peak 0 0 8f cause-purpose instrument:goal anesthetic:numbness 1 0 8g cause-purpose instrument:use abacus:calculate 0 0 8h cause-purpose prevention pesticide:vermin 0 0 9a space-time location:item arsenal:weapon 1 0 9b space-time location:product bakery:bread 1 0 9c space-time location:activity highway:driving 0 0 9d space-time location:instr. beach:swimsuit 0 0 9e space-time contiguity coast:ocean 1 1 9f space-time time:activity childhood:play 0 0 9g space-time time:associated retirement:pension 0 0 9h space-time sequence prologue:narrative 0 0 9i space-time attachment belt:waist 0 0 10a reference sign:significant siren:danger 1 0 10b reference expression hug:affection 1 0 10c reference representation person:portrait 0 1 10d reference plan blueprint:building 0 1 10e reference knowledge psychology:minds 1 0 10f reference concealment disguise:identity 1 0\nWe (Turney and Mohammad) each independently created a mapping like Tables 2 and 3. We disagreed on twelve of the 158 (79 \u00d7 2) mappings (92.4% agreement). We compared our tables and discussed them until we arrived at a consensus. For all twelve disagreements, our consensus was to label them 0. Tables 2 and 3 are the result of our consensus.\nWe used the first five types of information in Table 1 to decide how to map relation classes to entailment classes. Before we each independently created a mapping table, we agreed to approach the task as follows:\nProcedure for annotation:\n1. The relational schemas have more weight than the paradigmatic examples when deciding whether x entails y or y entails x. 2. Consider each of the paradigm pairs as instances of the given relational schema. That is, interpret the pairs in the light of the schema. If the three paradigmatic pairs are such that x entails y, when interpreted this way, then annotate the given category as \u2018x entails y\u2019, and likewise for y entails x. If two out of three paradigmatic pairs are such that x entails y, and the pair that is the exception seems unusual in some way, make a note about the exceptional pair, for later discussion. 3. If any of the paradigmatic pairs are in the wrong order, correct their order before proceeding. Make a note of the correction.\nWe then compared our tables and combined them to form the final Tables 2 and 3."}, {"heading": "7.3.2 Inter-annotator agreement", "text": "As we mentioned above, we assume all word pairs within a subcategory belong to the same class (either all entail or none entail). To test this assumption, we randomly selected 100 word pairs, 50 labeled entails and 50 labeled does not entail. We hid the labels and then we each independently manually labeled the pairs, first using the relational definition of lexical entailment and then a second time using the substitutional definition of lexical entailment (see Section 2). Table 4 shows the percentage agreement between our manual labels and automatic labeling, generated from the SemEval-2012 dataset by the mapping in Tables 2 and 3.\nWith the relational definition of lexical entailment, we agreed on 81% of the labels. The agreement between our manual labels and the labels that were generated automatically, by applying the mapping in Tables 2 and 3 to the SemEval dataset, varied from 70% to 81%. These numbers suggest that our assumption that all word pairs within a subcategory belong to the same class is reasonable. The assumption yields levels of agreement that are comparable to the agreement in our manual labels.\nWe mentioned in Section 7.1 that Zhitomirsky-Geffet and Dagan (2009) had inter-annotator agreements in the 90% range, whereas our agreement is 81%. We hypothesize that substitutability is a relatively objective test that leads to higher levels of agreement but excludes important cases of lexical entailment. We discussed some examples of cases that are missed by the substitutional definition in Section 2.\nAs expected, the automated labeling using SemEval corresponds more closely to manual labeling with the relational definition (70-81%) than manual labeling with the substitional definition (65-68%). This confirms that the construction of the dataset is in accordance with the intention of our relational definition."}, {"heading": "8 Experiments", "text": "In this section, we evaluate the three approaches to lexical entailment (balAPinc, ConVecs, and SimDiffs) on the three datasets."}, {"heading": "8.1 Experiments with the JMTH dataset", "text": "For the first set of experiments, we used the JMTH dataset (Section 7.3). This dataset has 2,308 word pairs, 1,154 in class 0 and 1,154 in class 1."}, {"heading": "8.1.1 Experimental setup", "text": "For the experiments, we split the dataset into three (approximately) equal parts, two development sets (Dev1 and Dev2) and one test set (Test). The splits were random, except the balance of the class sizes was maintained in all three subsets. Dev1 and Dev2 both contain 768 pairs and Test contains 772 pairs.\nTable 5 shows the number of word pairs in the Test set for each of the ten highlevel categories. In Tables 2 and 3, we see that a |= b is 0 for all subcategories of\nID Category a |= b a 6|= b b |= a b 6|= a Total\n1 class-inclusion 0 12 55 0 67 2 part-whole 72 7 12 27 118 3 similar 28 15 28 15 86 4 contrast 9 25 0 26 60 5 attribute 56 13 0 26 95 6 non-attribute 0 40 0 31 71 7 case relations 6 24 0 27 57 8 cause-purpose 21 15 0 29 65 9 space-time 35 17 10 17 79 10 reference 34 8 20 12 74\n\u2014 total 261 176 125 210 772\nthe category class-inclusion, hence there are 0 pairs for a |= b in the row for classinclusion in Table 5 but there are 12 pairs for a 6|= b. On the other hand, in Tables 2 and 3, b |= a is 1 for all subcategories of the category class-inclusion, so it is not surprising to see that there are 55 pairs for b |= a in the row for class-inclusion in Table 5 and 0 pairs for b 6|= a. The number of pairs labeled entails is 261+125 = 386 and the number labeled does not entail is 176 + 210 = 386.\nThe balAPinc measure has two parameters to tune, maxF for the maximum number of features and T as a threshold for classification. On Dev1, we calculated balAPinc five times, using five different values for maxF , 1000, 2000, 3000, 4000, and 5000. For each given value of maxF , we set T to the value that optimized the F-measure on Dev1. This gave us five pairs of values for maxF and T . We tested each of these five settings on Dev2 and chose the setting that maximized the F-measure, which was maxF = 1000. The balAPinc measure is robust with respect to the parameter settings. The accuracy on Dev2 ranged from 56.5% with maxF = 1000 to 52.5% with maxF = 5000. We kept the best maxF setting, but we tuned T again on the union of Dev1 and Dev2. With these parameter settings, we then applied balAPinc to the Test set.\nConVecs has two parameters to tune, k and p for Uk\u03a3 p k. For k, we tried 100, 200, 300, 400, and 500. For p, we tried ten values, from 0.1 to 1.0 in increments of 0.1. For each of the fifty pairs of values for k and p, we ran Weka, using Dev1 as training data and Dev2 as testing data. The maximum F-measure on Dev2 was achieved with k = 100 and p = 0.4. ConVecs is robust with respect to the parameter settings. The accuracy on Dev2 ranged from a high of 70.1% to a low of 64.6%. We then ran Weka one more time, using k = 100 and p = 0.4, with the union of Dev1 and Dev2 as training data and Test as testing data.\nSimDiffs has four parameters to tune, kd and pd for domain space and kf and\nAlgorithm AP0 AP1 Pre Rec F Acc 95% C.I.\nbalAPinc 0.57 0.56 0.573 0.573 0.573 57.3 53.8\u201360.7 ConVecs 0.76 0.77 0.703 0.702 0.702 70.2 66.9\u201373.3 SimDiffs 0.80 0.79 0.724 0.724 0.724 72.4 69.1\u201375.4\npf for function space, but we reduced this to two parameters, k and p, by setting k = kd = kf and p = pd = pf . We tried the same fifty pairs of values for k and p as we did with ConVecs. The maximum F-measure on Dev2 was achieved with k = 200 and p = 0.6. SimDiffs is robust with respect to the parameter settings. The accuracy on Dev2 ranged from 74.1% to 68.2%. We then ran Weka one more time, with the union of Dev1 and Dev2 as training data and Test as testing data."}, {"heading": "8.1.2 Results", "text": "Table 6 shows the performance of all three algorithms on the Test set. The accuracy of ConVecs (70.2%) is not significantly different from the accuracy of SimDiffs (72.4%), according to Fisher\u2019s Exact Test (Agresti 1996). However, both ConVecs and SimDiffs are more accurate than balAPinc (57.3%), at the 95% confidence level. The other performance measures (AP0, AP1, Pre, Rec, and F) follow the same general pattern as accuracy, which is what we would usually expect for a balanced dataset. The final column in Table 6 shows the 95% confidence interval for accuracy, calculated using the Wilson method.\nTable 7 shows how the accuracies of the three algorithms vary over the ten high-level categories in the Test set. ConVecs and SimDiffs have roughly similar profiles but balAPinc is substantially different from the other two. This is what we would expect, given that ConVecs and SimDiffs both approach lexical entailment as a semantic relation classification problem, whereas balAPinc approaches it as a problem of designing an asymmetric similarity measure. The approach of balAPinc is near the level of the other two for some relation categories (e.g., class-inclusion, non-attribute) but substantially below for others (e.g., attribute, case relations, reference).\nIn Table 8, we explore the contribution of each set of features to the performance of SimDiffs. In the columns for S1 to S4, a value of 1 indicates that the set is included in the feature vector and 0 indicates that the set is excluded (see Section 6.3). S1 is the difference between a and b in domain space, with respect to their similarities to the reference words, R. S2 is the difference between a and b in function space. S1 and S2 are based on differences in the same spaces, whereas S3 and S4 are based on differences in different spaces. The parameters are tuned individually for each row in Table 8, the same way they are tuned for SimDiffs in Table 6. The results are based on the Test set.\nS1 S2 S3 S4 AP0 AP1 Pre Rec F Acc 95% C.I."}, {"heading": "1 1 1 1 0.80 0.79 0.724 0.724 0.724 72.4 69.1\u201375.4", "text": "1 1 0 0 0.76 0.75 0.680 0.680 0.680 68.0 64.6\u201371.2 0 0 1 1 0.79 0.79 0.717 0.716 0.716 71.6 68.3\u201374.7\n1 0 0 0 0.71 0.69 0.663 0.663 0.663 66.3 62.9\u201369.6 0 1 0 0 0.75 0.72 0.684 0.684 0.684 68.4 65.0\u201371.6 0 0 1 0 0.76 0.74 0.690 0.690 0.690 69.0 65.7\u201372.2 0 0 0 1 0.75 0.73 0.701 0.701 0.701 70.1 66.8\u201373.2\nMost of the differences in the accuracies in Table 8 are not significant, but the accuracy of all of the features together (72.4%) is significantly higher than the accuracy of S1 and S2 without the help of S3 and S4 (68.0%), according to Fisher\u2019s Exact Test at the 95% confidence level. This supports the view that working with two different spaces has a synergetic effect, since each feature in S3 and S4 is based on two different spaces, whereas each feature in S1 and S2 is based on one space. (See the discussion of this in Section 6.3.)\nLet Gen (general) refer to the matrix from Turney et al. (2011) and let Dom and Fun refer to the domain and function matrices from Turney (2012). In Section 6, we mentioned that we performed experiments on the development datasets (Dev1 and Dev2) in order to select the matrices for each algorithm. Based on these experiments, we chose the Gen matrix for both balAPinc and ConVecs, and we chose\nAlgorithm Matrices AP0 AP1 Pre Rec F Acc 95% C.I.\nbalAPinc Gen 0.57 0.56 0.573 0.573 0.573 57.3 53.8\u201360.7\nDom 0.53 0.54 0.532 0.532 0.532 53.2 49.7\u201356.7 Fun 0.54 0.53 0.530 0.530 0.530 53.0 49.5\u201356.5\nConVecs Gen 0.76 0.77 0.703 0.702 0.702 70.2 66.9\u201373.3\nDom 0.72 0.75 0.676 0.675 0.674 67.5 64.1\u201370.7 Fun 0.79 0.78 0.719 0.719 0.719 71.9 68.6\u201375.0\nSimDiffs Dom, Fun 0.80 0.79 0.724 0.724 0.724 72.4 69.1\u201375.4\nGen, Fun 0.79 0.79 0.728 0.728 0.728 72.8 69.5\u201375.9 Dom, Gen 0.77 0.77 0.702 0.702 0.702 70.2 66.9\u201373.3 Gen, Gen 0.75 0.76 0.689 0.689 0.689 68.9 65.6\u201372.1\nthe Dom and Fun matrices for SimDiffs.\nIn Table 9, we vary the matrices and evaluate the performance on the Test set, to see whether the development datasets were a reliable guide for choosing the matrices. The matrices that were chosen based on the development datasets are in bold font. For balAPinc, Gen (57.3%) is indeed the best matrix. For ConVecs, it seems that Fun (71.9%) might be a better choice than Gen (70.2%), but the difference in their accuracy is not statistically significant. For SimDiffs, Dom and Fun (72.4%) are slightly less accurate than Gen and Fun (72.8%), but again the difference is not significant. As expected, no matrices are significantly better on the Test set than the matrices that were chosen based on the development datasets."}, {"heading": "8.2 Experiments with the KDSZ dataset", "text": "The second set of experiments used the KDSZ dataset (Section 7.1). This dataset has 3,772 word pairs, 2,704 in class 0 and 1,068 in class 1."}, {"heading": "8.2.1 Experimental setup", "text": "We experimented with four different ways of splitting the dataset. The Evaluation column in Table 10 indicates the experimental setup (dataset splitting).\nThe standard evaluation is ten-fold cross-validation in which the folds are random. This evaluation yields relatively high scores, because, although every pair in the KDSZ dataset is unique, many pairs share a common term. This makes supervised learning easier, because a pair in the testing fold will often share a term with several pairs in the training folds.\nThe clustered evaluation is designed to be more challenging than the standard evaluation. The clustered evaluation is ten-fold cross-validation with non-random\nnumber of pairs with shared terms, it is not possible to construct ten folds such that there are absolutely no terms that are shared by any two folds. Therefore we gave a high priority to isolating the most common shared words to single folds, but we allowed a few less common shared words to appear in more than one fold. Thus a pair in the testing fold will only rarely share a term with pairs in the training folds.\nThe standard and clustered evaluations have more examples in class 0 (does not entail) than in class 1 (entails). The balanced dataset takes the clustered evaluation a step further, by first clustering folds and then randomly removing pairs labeled as class 0, until the folds all have an equal number of pairs in both classes.\nFor the different evaluation, instead of cross-validation, the algorithms are trained on the JMTH dataset and tested on the KDSZ dataset, after the KDSZ dataset has been balanced by randomly removing pairs labeled as class 0.\nThe balAPinc measure has two parameters, maxF for the maximum number of features and T as a threshold for classification. In all four experimental setups, we used the setting maxF = 1000, based on the tuning experiments with the JMTH dataset (Section 8.1). For T , we used the training split in each of the four experimental setups. For the standard, clustered, and balanced setups, the training split is the nine folds used for training in each step of the ten-fold cross-validation. For the different setup, the training split is the whole JMTH dataset. For all four setups, we set T to the value that optimized the F-measure on the training split.\nConVecs has two parameters to tune, k and p for Uk\u03a3 p k. In all four experimental setups, we used k = 100 and p = 0.4, based on the experiments with the JMTH dataset. The training splits were used to teach the supervised learning algorithm (the polynomial kernel SMO SVM in Weka).\nSimDiffs has four parameters to tune. We used kd = kf = 200 and pd = pf = 0.6, based on the experiments with the JMTH dataset. The training splits were used to teach the supervised learning algorithm (the RBF kernel SMO SVM in Weka)."}, {"heading": "8.2.2 Results", "text": "In Table 10, the four experimental setups (standard, clustered, balanced, and different) are given in order of increasing challenge and increasing realism. Of the four experimental setups, we believe that the different evaluation is the most challenging and most realistic. If an RLE module is part of a commercial RTE system, the module will inevitably encounter word pairs in the field that are quite different from the pairs it saw during training. The different evaluation comes closest to approximating field usage.\nOn the different evaluations, balAPinc achieves an accuracy of 58.2%, ConVecs has an accuracy of 56.1%, and SimDiffs reaches 57.4%. There is no statistically significant difference between any of these accuracies, according to Fisher\u2019s Exact Test at the 95% confidence level.\nWith ConVecs and SimDiffs, compared to balAPinc, there is a relatively large gap between the standard performance and the different performance. This is be-\nAlgorithm Evaluation AP0 AP1 Pre Rec F Acc 95% C.I.\nbalAPinc standard 0.79 0.37 0.645 0.645 0.645 64.5 63.0\u201366.0\nclustered 0.79 0.37 0.644 0.643 0.644 64.3 62.8\u201365.8 balanced 0.60 0.59 0.583 0.583 0.583 58.3 56.2\u201360.4 different 0.61 0.60 0.582 0.582 0.582 58.2 56.1\u201360.3\nConVecs standard 0.87 0.56 0.731 0.747 0.735 74.7 73.3\u201376.1\nclustered 0.78 0.36 0.636 0.690 0.645 69.0 67.5\u201370.5 balanced 0.60 0.59 0.567 0.554 0.531 55.4 53.3\u201357.5 different 0.57 0.62 0.569 0.561 0.547 56.1 54.0\u201358.2\nSimDiffs standard 0.88 0.60 0.749 0.757 0.752 75.7 74.3\u201377.0\nclustered 0.80 0.40 0.664 0.684 0.671 68.4 66.9\u201369.9 balanced 0.63 0.64 0.596 0.592 0.588 59.2 57.1\u201361.3 different 0.58 0.61 0.581 0.574 0.564 57.4 55.3\u201359.5\ncause ConVecs and SimDiffs use supervised learning and thus they benefit from the standard setup, where the training data is highly similar to the testing data. In balAPinc, the training data is used only to tune the threshold, T , which limits the benefit of the training data.\nNote that the gap between the standard performance and the different performance is not simply a question of the quantity of data. In the different setup, there is a qualitative difference between the training data and the testing data. Increasing the size of the training dataset with more data of the same type will not be helpful. The goal of the different setup is to test the ability of the algorithms to bridge the qualitative gap between the training and testing data. This qualitative gap is more challenging for supervised learning than a quantitative gap. It is a gap that learning algorithms inevitably face in real applications (Pan and Yang 2010).\nThe KDSZ dataset has been used in previous research, but the past results are not comparable with our results. Kotlerman et al. (2010) reported AP1 without AP0, but there is a trade-off between AP1 and AP0. Kotlerman et al. (2010) did not attempt to evaluate balAPinc as a classifier, so they did not report precision, recall, F-measure, or accuracy."}, {"heading": "8.3 Experiments with the BBDS dataset", "text": "The final set of experiments used the BBDS dataset (Section 7.2). The dataset has 2,770 word pairs, 1,385 in class 0 and 1,385 in class 1.\nAlgorithm Evaluation AP0 AP1 Pre Rec F Acc 95% C.I.\nbalAPinc standard 0.79 0.73 0.722 0.722 0.722 72.2 70.5\u201373.8\nclustered 0.79 0.73 0.722 0.722 0.722 72.2 70.5\u201373.8 different 0.79 0.73 0.701 0.687 0.682 68.7 67.0\u201370.4\nConVecs standard 0.95 0.95 0.876 0.876 0.876 87.6 86.3\u201388.8\nclustered 0.92 0.91 0.829 0.821 0.819 82.1 80.6\u201383.5 different 0.72 0.71 0.652 0.651 0.650 65.1 63.3\u201366.9\nSimDiffs standard 0.97 0.97 0.913 0.913 0.913 91.3 90.2\u201392.3\nclustered 0.96 0.96 0.883 0.881 0.881 88.1 86.8\u201389.3 different 0.84 0.82 0.751 0.745 0.743 74.5 72.8\u201376.1"}, {"heading": "8.3.1 Experimental setup", "text": "We experimented with three different ways of splitting the dataset. In Table 11, the evaluations follow the same setups as in Table 10. However, there is no balanced setup, since the BBDS dataset is already balanced. In the different evaluation, the algorithms are trained on the JMTH dataset and evaluated on the BBDS. This is the most realistic evaluation setup."}, {"heading": "8.3.2 Results", "text": "In Table 11, on the different evaluations, balAPinc achieves an accuracy of 68.7%, ConVecs has an accuracy of 65.1%, and SimDiffs reaches 74.5%. All of these accuracies are significantly different, according to Fisher\u2019s Exact Test at the 95% confidence level.\nThe BBDS data was used by Baroni et al. (2012) to compare balAPinc with ConVecs. They used two different evaluation setups, similar to our standard and different setups. For balAPinc using a standard setup, they obtained an accuracy of 70.1%, slighly below our result of 72.2%. The difference is likely due to minor differences in the word\u2013context matrices that we used. For balAPinc using a different setup, their accuracy was 70.4%, compared to our 68.7%. They used their own independent dataset to tune balAPinc, whereas we used the JMTH dataset. Given that our word\u2013context matrices and our training data are different from theirs, the accuracies are closer than might be expected.16\nFor ConVecs using a standard setup, Baroni et al. (2012) report an accuracy of 88.6%, whereas we achived 87.6%. Using a different setup, they obtained 69.3%, whereas our accuracy was 65.1%. It seems likely that our training data (the JMTH"}, {"heading": "16 These accuracy numbers and the numbers reported in the next paragraph are taken", "text": "from Table 2 in Baroni et al. (2012).\nAlgorithm JMTH Accuracy KDSZ Accuracy BBDS Accuracy\nbalAPinc 57.3 58.2 68.7 ConVecs 70.2 56.1 65.1 SimDiffs 72.4 57.4 74.5\ndataset) was less similar to the BBDS dataset than their own independent dataset, which made our different setup more challenging than theirs. Nonetheless, the accuracies are closer than might be expected, given the differences in the setups."}, {"heading": "9 Discussion of results", "text": "Table 12 summarizes the accuracy results from the experiments. For the KDSZ and BBDS experiments, only the different evaluation is shown. Bold font is used to mark the cases where the accuracy is significantly less than the accuracy of SimDiffs. In no case is the accuracy significantly greater than the accuracy of SimDiffs.\nThe JMTH dataset is based on seventy-nine types of semantic relations. The pairs in this dataset were labeled in accordance with the relational definition of lexical entailment (see Section 2). This explains why balAPinc, which was designed with the substitutional definition in mind, performs poorly on the JMTH dataset. ConVecs and SimDiffs were designed for semantic relation classification, so it is not surprising that they perform much better than balAPinc.\nThe KDSZ dataset was labeled using the substitutional definition of lexical entailment (see Section 2). On this dataset, there is no statistically significant difference between any of the algorithms. This is the ideal dataset for balAPinc, the dataset for which it was designed, so it is natural that balAPinc has the highest accuracy. On the other hand, we see that the two learning algorithms handle this dataset well, although they were trained on the JMTH dataset (recall that this is the different setup), which is quite different from the KDSZ dataset. It is good that they are both able to cope with the qualitative difference between the training data and the testing data.\nAll of the positive pairs in the BBDS dataset are instances of the hyponym\u2013 hypernym semantic relation. Instances of this relation are substitutable, so balAPinc is designed to handle them. ConVecs was also designed specifically for this dataset, and we see from Table 11 that ConVecs reaches an accuracy of 87.6% when the training data is similar to the testing data. However, ConVecs has trouble bridging the qualitative gap between the training data (the JMTH dataset) and the testing data with the different setup. On the other hand, SimDiffs is able to bridge this gap.\nWe have argued that the different evaluation is the most realistic scenario, but it could be argued that the entails class is more important than the does not entail\nclass, and entails is also more scarce in natural settings. Therefore Table 13 presents an alternative summary of the results. The table reports AP1 instead of accuracy; this puts the emphasis on the entails class. For the KDSZ and BBDS datasets, we report the clustered setup. This is closer to the evaluation setup of Kotlerman et al. (2010). In this table, we do not use bold font to mark significant differences, because there is no agreement on the appropriate statistical test for AP1.\nAlthough Tables 12 and 13 are based on different scores and experimental setups, both support SimDiffs and the similarity differences hypothesis. More generally, they suggest that second-order features are useful for modeling lexical entailment. They also suggest that it is beneficial to use two different spaces when constructing features for lexical entailment.\nManually designing an asymmetric similarity measure is a difficult task, as we can see from the equations in Section 6.1. We believe that lexical entailment is more tractable when it is approached as a supervised learning problem. The effort involved in manually designing feature vectors is less than that required for designing similarity measures. The performance of SimDiffs indicates that supervised learning can yield better results than manually designing measures."}, {"heading": "10 Limitations and future work", "text": "We have evaluated RLE directly, but most applications would use RLE as a module inside a larger system. Future work will be needed to demonstrate that our results with a direct evaluation can predict how an RLE module will perform as a component of a larger system.\nAlthough SimDiffs performs better than the competition, there is much room for improved performance. However, when SimDiffs is used as a component in a larger RTE system, words will be given in the contexts of sentences. With the support of this contextual information and help from the other modules in the system, SimDiffs might yield substantial improvements in RTE performance. Related to this proposed future work, Shnarch, Barak, and Dagan (2009) evaluated lexical reference rules (Glickman et al. 2006) derived from Wikipedia on the RTE-4 dataset. Used as a component in an RTE system, the rules improved the RTE-4 score by 1%.\nMost of the past work on RLE has been based on the context inclusion hypothesis, but ConVecs and SimDiffs show that other approaches, based on novel hypotheses, can achieve competitive results. We believe that progress on the problem will come\nstage of research to commit the field to a single hypothesis.\nRecall the semantic relation subcategories hypothesis: Lexical entailment is not a superset of high-level categories of semantic relations, but it is a superset of lowerlevel subcategories of semantic relations. The experiments lend some support to this hypothesis, but more research is needed. Any counterexamples for the hypothesis could be handled by revising the taxonomy. However, if the required revisions become onerous, then the hypothesis should be rejected.\nThe three algorithms here are based on three different hypotheses, but all three achieve some degree of success on the task of RLE. This suggests that it would be fruitful to combine the three approaches. One simple way to combine them would be to average their real-valued outputs or apply voting to their binary-valued outputs. This could be a useful direction for future research.\nWe have focused here on individual words, but the natural next step is to extend these ideas to phrases. Baroni et al. (2012) have achieved promising results with quantifier phrases, such as all dogs |= some dogs.\nLooking at Tables 2 and 3 in Section 7.3, we see a high density of 1\u2019s (entails) for class-inclusion and part-whole. The strong connection between these two categories and lexical entailment may explain why Morris and Hirst (2004) call hypernymy and meronymy classical relations, whereas the relation in chapel:funeral (spacetime, location:activity, ID 9c) is non-classical (this is one of their examples of a non-classical relation). For instance, WordNet contains information about hypernymy and meronymy, but not space-time relations. Particular relations might be considered classical because we find them particularly useful for making inferences. This connection is another topic for future work."}, {"heading": "11 Conclusion", "text": "In this paper, we have evaluated three different algorithms for RLE on three different datasets. Each algorithm relies on a different hypothesis about lexical entailment. We find that SimDiffs has the best performance on two of the three datasets. On the third dataset, there is no significant difference in the three algorithms. The performance of SimDiffs suggests that similarity differences make useful features for learning to recognize lexical entailment.\nWe have approached lexical entailment as a supervised learning problem of semantic relation classification. The results indicate that this is a promising approach to lexical entailment. This builds a bridge between research in lexical entailment and research in semantic relation classification. We hope that this connection will strengthen research in both fields."}, {"heading": "Acknowledgements", "text": "Thanks to Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan ZhitomirskyGeffet for providing a copy of the KDSZ dataset and answering questions. Thanks to Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan for\nNatural Language Engineering for their very helpful comments."}], "references": [{"title": "An Introduction to Categorical Data Analysis", "author": ["A. Agresti"], "venue": "Wiley, New York, NY.", "citeRegEx": "Agresti,? 1996", "shortCiteRegEx": "Agresti", "year": 1996}, {"title": "Using hypernymy acquisition to tackle (part of) textual entailment", "author": ["E. Akhmatova", "M. Dras"], "venue": "Proceedings of the 2009 Workshop on Applied Textual Inference at ACL-IJCNLP 2009, pp. 52\u201360, Suntec, Singapore.", "citeRegEx": "Akhmatova and Dras,? 2009", "shortCiteRegEx": "Akhmatova and Dras", "year": 2009}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["I. Androutsopoulos", "P. Malakasiotis"], "venue": "Journal of Artificial Intelligence Research, 38, 135\u2013187.", "citeRegEx": "Androutsopoulos and Malakasiotis,? 2010", "shortCiteRegEx": "Androutsopoulos and Malakasiotis", "year": 2010}, {"title": "Entailment above the word level in distributional semantics", "author": ["M. Baroni", "R. Bernardi", "Do", "N.-Q.", "C. Shan"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pp. 23\u201332, Avignon, France.", "citeRegEx": "Baroni et al\\.,? 2012", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Cognitive and Psychometric Analysis of Analogical Problem Solving", "author": ["I.I. Bejar", "R. Chaffin", "S.E. Embretson"], "venue": "Springer-Verlag, New York, NY.", "citeRegEx": "Bejar et al\\.,? 1991", "shortCiteRegEx": "Bejar et al\\.", "year": 1991}, {"title": "Evaluating evaluation measure stability", "author": ["C. Buckley", "E. Voorhees"], "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 33\u201340, Athens, Greece. ACM.", "citeRegEx": "Buckley and Voorhees,? 2000", "shortCiteRegEx": "Buckley and Voorhees", "year": 2000}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods, 39(3), 510\u2013526.", "citeRegEx": "Bullinaria and Levy,? 2007", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods, 44, 890\u2013907.", "citeRegEx": "Bullinaria and Levy,? 2012", "shortCiteRegEx": "Bullinaria and Levy", "year": 2012}, {"title": "Efficiency vs", "author": ["S. B\u00fcttcher", "C. Clarke"], "venue": "effectiveness in terabyte-scale information retrieval. In Proceedings of the 14th Text REtrieval Conference (TREC 2005), Gaithersburg, MD.", "citeRegEx": "B\u00fcttcher and Clarke,? 2005", "shortCiteRegEx": "B\u00fcttcher and Clarke", "year": 2005}, {"title": "Experiments with LSA scoring: Optimal rank and basis", "author": ["J. Caron"], "venue": "In Proceedings of the SIAM Computational Information Retrieval Workshop,", "citeRegEx": "Caron,? \\Q2001\\E", "shortCiteRegEx": "Caron", "year": 2001}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches", "author": ["I. Dagan", "B. Dolan", "B. Magnini", "D. Roth"], "venue": "Natural Language Engineering, 15(4), i\u2013xvii.", "citeRegEx": "Dagan et al\\.,? 2009", "shortCiteRegEx": "Dagan et al\\.", "year": 2009}, {"title": "The PASCAL Recognising Textual Entailment Challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pp. 177\u2013190, New York, NY. Springer.", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Constraints based taxonomic relation classification", "author": ["Q.X. Do", "D. Roth"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pp. 1099\u20131109, Cambridge, MA.", "citeRegEx": "Do and Roth,? 2010", "shortCiteRegEx": "Do and Roth", "year": 2010}, {"title": "Exploiting the Wikipedia structure in local and global classification of taxonomic relations", "author": ["Q.X. Do", "D. Roth"], "venue": "Natural Language Engineering, 18(2), 235\u2013262.", "citeRegEx": "Do and Roth,? 2012", "shortCiteRegEx": "Do and Roth", "year": 2012}, {"title": "A synopsis of linguistic theory 1930\u20131955", "author": ["J.R. Firth"], "venue": "Studies in Linguistic Analysis, pp. 1\u201332. Blackwell, Oxford.", "citeRegEx": "Firth,? 1957", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["M. Geffet", "I. Dagan"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005), pp. 107\u2013114, Ann Arbor, MI.", "citeRegEx": "Geffet and Dagan,? 2005", "shortCiteRegEx": "Geffet and Dagan", "year": 2005}, {"title": "SemEval-2007 Task 4: Classification of semantic relations between nominals", "author": ["R. Girju", "P. Nakov", "V. Nastase", "S. Szpakowicz", "P. Turney", "D. Yuret"], "venue": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval 2007), pp. 13\u201318, Prague, Czech Republic.", "citeRegEx": "Girju et al\\.,? 2007", "shortCiteRegEx": "Girju et al\\.", "year": 2007}, {"title": "Lexical reference: A semantic matching subtask", "author": ["O. Glickman", "I. Dagan", "E. Shnarch"], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pp. 172\u2013179, Sydney, Australia.", "citeRegEx": "Glickman et al\\.,? 2006", "shortCiteRegEx": "Glickman et al\\.", "year": 2006}, {"title": "Matrix Computations (Third edition)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press, Baltimore, MD.", "citeRegEx": "Golub and Loan,? 1996", "shortCiteRegEx": "Golub and Loan", "year": 1996}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word, 10(23), 146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M. Hearst"], "venue": "Proceedings of the 14th Conference on Computational Linguistics (COLING92), pp. 539\u2013545, Nantes, France.", "citeRegEx": "Hearst,? 1992", "shortCiteRegEx": "Hearst", "year": 1992}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["I. Hendrickx", "S.N. Kim", "Z. Kozareva", "P. Nakov", "D.O. S\u00e9aghdha", "S. Pad\u00f3", "M. Pennacchiotti", "L. Romano", "S. Szpakowicz"], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 33\u201338, Uppsala, Sweden.", "citeRegEx": "Hendrickx et al\\.,? 2010", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Textual entailment recognition based on dependency analysis and WordNet", "author": ["J. Herrera", "A. Pe\u00f1as", "F. Verdejo"], "venue": "Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, Vol. 3944 of Lecture Notes in Computer Science, pp. 231\u2013 239. Springer.", "citeRegEx": "Herrera et al\\.,? 2006", "shortCiteRegEx": "Herrera et al\\.", "year": 2006}, {"title": "Recognizing textual entailment with LCC\u2019s GROUNDHOG system", "author": ["A. Hickl", "J. Bensley", "J. Williams", "K. Roberts", "B. Rink", "Y. Shi"], "venue": "Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment, Venice, Italy.", "citeRegEx": "Hickl et al\\.,? 2006", "shortCiteRegEx": "Hickl et al\\.", "year": 2006}, {"title": "Metalogic: An Introduction to the Metatheory of Standard First Order Logic", "author": ["G. Hunter"], "venue": "University of California Press, Berkeley, CA.", "citeRegEx": "Hunter,? 1996", "shortCiteRegEx": "Hunter", "year": 1996}, {"title": "SemEval-2012 Task 2: Measuring degrees of relational similarity", "author": ["D.A. Jurgens", "S.M. Mohammad", "P.D. Turney", "K.J. Holyoak"], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pp. 356\u2013364, Montr\u00e9al, Canada.", "citeRegEx": "Jurgens et al\\.,? 2012", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Directional distributional similarity for lexical inference", "author": ["L. Kotlerman", "I. Dagan", "I. Szpektor", "M. Zhitomirsky-Geffet"], "venue": "Natural Language Engineering, 16(4), 359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Handbook of Latent Semantic Analysis", "author": ["T.K. Landauer", "D.S. McNamara", "S. Dennis", "W. Kintsch"], "venue": "Lawrence Erlbaum, Mahwah, NJ.", "citeRegEx": "Landauer et al\\.,? 2007", "shortCiteRegEx": "Landauer et al\\.", "year": 2007}, {"title": "Measures of distributional similarity", "author": ["L. Lee"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pp. 25\u201332, College Park, MD.", "citeRegEx": "Lee,? 1999", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Automatic retrieval and clustering of similar words", "author": ["D. Lin"], "venue": "Proceedings of the 17th international conference on Computational linguistics, pp. 768\u2013774, Montreal, Quebec, Canada. Association for Computational Linguistics.", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "DIRT \u2013 discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": "Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2001, pp. 323\u2013328, San Francisco, CA.", "citeRegEx": "Lin and Pantel,? 2001", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C. Manning", "H. Sch\u00fctze"], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Manning and Sch\u00fctze,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "Bar-ilan university\u2019s submission to rte-5", "author": ["S. Mirkin", "R. Bar-Haim", "J. Berant", "I. Dagan", "E. Shnarch", "A. Stern", "I. Szpektor"], "venue": "TAC 2009, Gaithersburg, MD.", "citeRegEx": "Mirkin et al\\.,? 2009a", "shortCiteRegEx": "Mirkin et al\\.", "year": 2009}, {"title": "Evaluating the inferential utility of lexical-semantic resources", "author": ["S. Mirkin", "I. Dagan", "E. Shnarch"], "venue": "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pp. 558\u2013566, Athens, Greece.", "citeRegEx": "Mirkin et al\\.,? 2009b", "shortCiteRegEx": "Mirkin et al\\.", "year": 2009}, {"title": "Non-classical lexical semantic relations", "author": ["J. Morris", "G. Hirst"], "venue": "Workshop on Computational Lexical Semantics, HLT-NAACL-04, Boston, MA.", "citeRegEx": "Morris and Hirst,? 2004", "shortCiteRegEx": "Morris and Hirst", "year": 2004}, {"title": "Exploring noun-modifier semantic relations", "author": ["V. Nastase", "S. Szpakowicz"], "venue": "Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5), pp. 285\u2013301, Tilburg, The Netherlands.", "citeRegEx": "Nastase and Szpakowicz,? 2003", "shortCiteRegEx": "Nastase and Szpakowicz", "year": 2003}, {"title": "Basic English: A General Introduction with Rules and Grammar", "author": ["C.K. Ogden"], "venue": "Kegan Paul, Trench, Trubner and Co., London.", "citeRegEx": "Ogden,? 1930", "shortCiteRegEx": "Ogden", "year": 1930}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22, 1345\u20131359.", "citeRegEx": "Pan and Yang,? 2010", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "WordNet::Similarity \u2013 Measuring the relatedness of concepts", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi"], "venue": "Demonstration Papers at HLT-NAACL 2004, pp. 38\u201341, Boston, MA.", "citeRegEx": "Pedersen et al\\.,? 2004", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods: Support Vector Learning, pp. 185\u2013208, Cambridge, MA. MIT Press.", "citeRegEx": "Platt,? 1998", "shortCiteRegEx": "Platt", "year": 1998}, {"title": "Classifying the semantic relations in nouncompounds via a domain-specific lexical hierarchy", "author": ["B. Rosario", "M. Hearst"], "venue": "Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP01), pp. 82\u201390, Pittsburgh, PA.", "citeRegEx": "Rosario and Hearst,? 2001", "shortCiteRegEx": "Rosario and Hearst", "year": 2001}, {"title": "The descent of hierarchy, and selection in relational semantics", "author": ["B. Rosario", "M. Hearst", "C. Fillmore"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pp. 247\u2013254, Philadelphia, PA.", "citeRegEx": "Rosario et al\\.,? 2002", "shortCiteRegEx": "Rosario et al\\.", "year": 2002}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M. McGill"], "venue": "McGraw-Hill, New York, NY.", "citeRegEx": "Salton and McGill,? 1983", "shortCiteRegEx": "Salton and McGill", "year": 1983}, {"title": "Extracting lexical reference rules", "author": ["E. Shnarch", "L. Barak", "I. Dagan"], "venue": null, "citeRegEx": "Shnarch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shnarch et al\\.", "year": 2009}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["R. Snow", "D. Jurafsky", "A.Y. Ng"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pp. 801\u2013808, Sydney, NSW, Australia.", "citeRegEx": "Snow et al\\.,? 2006", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Learning entailment rules for unary templates", "author": ["I. Szpektor", "I. Dagan"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), pp. 849\u2013856, Manchester, UK.", "citeRegEx": "Szpektor and Dagan,? 2008", "shortCiteRegEx": "Szpektor and Dagan", "year": 2008}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics, 32(3), 379\u2013416.", "citeRegEx": "Turney,? 2006", "shortCiteRegEx": "Turney", "year": 2006}, {"title": "Domain and function: A dual-space model of semantic relations and compositions", "author": ["P.D. Turney"], "venue": "Journal of Artificial Intelligence Research, 44, 533\u2013585.", "citeRegEx": "Turney,? 2012", "shortCiteRegEx": "Turney", "year": 2012}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["P.D. Turney", "Y. Neuman", "D. Assaf", "Y. Cohen"], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pp. 680\u2013690, Edinburgh, UK.", "citeRegEx": "Turney et al\\.,? 2011", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research, 37, 141\u2013188.", "citeRegEx": "Turney and Pantel,? 2010", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "A general framework for distributional similarity", "author": ["J. Weeds", "D. Weir"], "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2003), pp. 81\u201388, Sapporo, Japan.", "citeRegEx": "Weeds and Weir,? 2003", "shortCiteRegEx": "Weeds and Weir", "year": 2003}, {"title": "Characterising measures of lexical distributional similarity", "author": ["J. Weeds", "D. Weir", "D. McCarthy"], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING \u201904), pp. 1015\u20131021, Geneva, Switzerland.", "citeRegEx": "Weeds et al\\.,? 2004", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques, Third Edition", "author": ["I.H. Witten", "E. Frank", "M.A. Hall"], "venue": "Morgan Kaufmann, San Francisco.", "citeRegEx": "Witten et al\\.,? 2011", "shortCiteRegEx": "Witten et al\\.", "year": 2011}, {"title": "Bootstrapping distributional feature vector quality", "author": ["M. Zhitomirsky-Geffet", "I. Dagan"], "venue": "Computational Linguistics, 35(3), 435\u2013461.", "citeRegEx": "Zhitomirsky.Geffet and Dagan,? 2009", "shortCiteRegEx": "Zhitomirsky.Geffet and Dagan", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "Recognizing textual entailment (RTE) is a popular task in natural language processing research, due to its relevance for text summarization, information retrieval, information extraction, question answering, machine translation, paraphrasing, and other applications (Androutsopoulos and Malakasiotis 2010).", "startOffset": 266, "endOffset": 305}, {"referenceID": 10, "context": "The text entails the hypothesis if the meaning of the hypothesis can be inferred from the meaning of the text, according to typical human interpretations of the text and the hypothesis (Dagan et al. 2009).", "startOffset": 185, "endOffset": 204}, {"referenceID": 15, "context": "In many cases, to recognize when one sentence entails another, we must first be able to recognize when one word entails another (Geffet and Dagan 2005).", "startOffset": 128, "endOffset": 151}, {"referenceID": 49, "context": ") Vector space models (VSMs) of semantics have been particularly useful for lexical semantics (Turney and Pantel 2010), hence it is natural to apply them to RLE.", "startOffset": 94, "endOffset": 118}, {"referenceID": 19, "context": "These models are inspired by the distributional hypothesis (Harris 1954; Firth 1957):", "startOffset": 59, "endOffset": 84}, {"referenceID": 14, "context": "These models are inspired by the distributional hypothesis (Harris 1954; Firth 1957):", "startOffset": 59, "endOffset": 84}, {"referenceID": 15, "context": "The idea is to design a measure that captures the context inclusion hypothesis (Geffet and Dagan 2005):", "startOffset": 79, "endOffset": 102}, {"referenceID": 15, "context": "This is our paraphrase of what Geffet and Dagan (2005) call the distributional inclusion hypothesis.", "startOffset": 31, "endOffset": 55}, {"referenceID": 24, "context": "The context inclusion hypothesis is inspired by model theory in formal logic (Hunter 1996).", "startOffset": 77, "endOffset": 90}, {"referenceID": 40, "context": "Semantic relation classification is the task of learning to recognize when a word pair is an instance of a given semantic relation class (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju, Nakov, Nastase, Szpakowicz, Turney, and Yuret 2007).", "startOffset": 137, "endOffset": 299}, {"referenceID": 35, "context": "Semantic relation classification is the task of learning to recognize when a word pair is an instance of a given semantic relation class (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju, Nakov, Nastase, Szpakowicz, Turney, and Yuret 2007).", "startOffset": 137, "endOffset": 299}, {"referenceID": 46, "context": "Semantic relation classification is the task of learning to recognize when a word pair is an instance of a given semantic relation class (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju, Nakov, Nastase, Szpakowicz, Turney, and Yuret 2007).", "startOffset": 137, "endOffset": 299}, {"referenceID": 20, "context": "There is a relatively large body of work on semantic relation classification in general, with good results on the hyponym\u2013 hypernym relation in particular (Hearst 1992; Snow, Jurafsky, and Ng 2006).", "startOffset": 155, "endOffset": 197}, {"referenceID": 3, "context": "ConVecs is based on the context combination hypothesis (Baroni et al. 2012):", "startOffset": 55, "endOffset": 75}, {"referenceID": 3, "context": "This algorithm was not given a name by Baroni et al. (2012). For ease of reference, we will call it ConVecs (concatenated vectors).", "startOffset": 39, "endOffset": 60}, {"referenceID": 3, "context": "This hypothesis was not explicitly stated by Baroni et al. (2012) but it is implicit in their approach.", "startOffset": 45, "endOffset": 66}, {"referenceID": 40, "context": "for supervised learning with word pairs (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003).", "startOffset": 40, "endOffset": 115}, {"referenceID": 41, "context": "for supervised learning with word pairs (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003).", "startOffset": 40, "endOffset": 115}, {"referenceID": 35, "context": "for supervised learning with word pairs (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003).", "startOffset": 40, "endOffset": 115}, {"referenceID": 25, "context": "We use the datasets of Kotlerman et al. (2010), Baroni et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 3, "context": "(2010), Baroni et al. (2012), and Jurgens, Mohammad, Turney, and Holyoak (2012).", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "(2010), Baroni et al. (2012), and Jurgens, Mohammad, Turney, and Holyoak (2012). The experimental results are reported in Section 8.", "startOffset": 8, "endOffset": 80}, {"referenceID": 29, "context": "Some people might feel that lion and cat suggest either the hyponym\u2013hypernym relation (assuming cat means feline) or the coordinate relation (assuming that cat means house cat). Coordinates are words with a shared hypernym. Lion and house cat share the hypernym feline. If cat means house cat, then lion and cat are coordinates. A hyponym implies its hypernym, but coordinates do not imply each other. Lion implies cat in the feline sense but not in the house cat sense. Thus these two relations (hyponym\u2013hypernym and coordinate) do not agree on whether lion implies cat. In this case, we believe that the hyponym\u2013hypernym is more natural, so we say that lion implies cat. For people who feel both semantic relations are natural, the third condition says that there is no entailment; for them, lion does not imply cat. The third condition could be modified for different uses. For our dataset (Section 7.3), we chose to err on the side of non-entailment, but ideally the choice would be made based on the downstream application. For some applications, it may be better to err on the side of entailment. One possibility is to give higher weight to some relations and use the weighting to choose between entailment and nonentailment when two or more relations disagree. The weighting could be based on the corpus frequency of the relations or the contexts in which the words appear. To apply the relational definition of lexical entailment, it is helpful to have a taxonomy of semantic relations, to provide options for R. In this paper, we use the taxonomy of Bejar, Chaffin, and Embretson (1991), which includes seventynine subcategories of semantic relations, grouped into ten high-level categories.", "startOffset": 108, "endOffset": 1596}, {"referenceID": 15, "context": "Zhitomirsky-Geffet and Dagan\u2019s (2009) substitutional definition of lexical entailment was intended to capture only substitutional cases of entailment.", "startOffset": 12, "endOffset": 38}, {"referenceID": 15, "context": "One limitation of substitutability as defined by Zhitomirsky-Geffet and Dagan (2009) is that it does not allow lexical entailment from one part of speech to another.", "startOffset": 61, "endOffset": 85}, {"referenceID": 30, "context": "Patterns like this have been learned from corpora (Lin and Pantel 2001) and applied successfully to RTE (Mirkin, Bar-Haim, Berant, Dagan, Shnarch, Stern, and Szpektor 2009a).", "startOffset": 50, "endOffset": 71}, {"referenceID": 15, "context": ", for synonyms) (Geffet and Dagan 2005; Kotlerman et al. 2010).", "startOffset": 16, "endOffset": 62}, {"referenceID": 26, "context": ", for synonyms) (Geffet and Dagan 2005; Kotlerman et al. 2010).", "startOffset": 16, "endOffset": 62}, {"referenceID": 14, "context": "1) was labeled using Zhitomirsky-Geffet and Dagan\u2019s (2009) substitutional definition.", "startOffset": 33, "endOffset": 59}, {"referenceID": 4, "context": "3), the pairs were generated from Bejar et al.\u2019s (1991) taxonomy.", "startOffset": 34, "endOffset": 56}, {"referenceID": 1, "context": "Some researchers have applied semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Baroni et al. 2012), but Zhitomirsky-Geffet and Dagan", "startOffset": 85, "endOffset": 130}, {"referenceID": 3, "context": "Some researchers have applied semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Baroni et al. 2012), but Zhitomirsky-Geffet and Dagan", "startOffset": 85, "endOffset": 130}, {"referenceID": 14, "context": "We agree with Zhitomirsky-Geffet and Dagan (2009) that some sub-cases of part\u2013 whole involve lexical entailment and other sub-cases do not.", "startOffset": 26, "endOffset": 50}, {"referenceID": 4, "context": "One of the high-level categories in Bejar et al.\u2019s (1991) taxonomy is part\u2013whole (ID 2 in the taxonomy), which has ten subcategories.", "startOffset": 36, "endOffset": 58}, {"referenceID": 4, "context": "One of the high-level categories in Bejar et al.\u2019s (1991) taxonomy is part\u2013whole (ID 2 in the taxonomy), which has ten subcategories. We claim that eight of the ten subcategories involve entailment and two do not involve entailment, which is consistent with the claim that \u2018lexical entailment does not cover all cases of meronyms\u2019 (in the above quotation). Regarding \u2018ocean and water and murder and death\u2019 (in the above quotation), the word pair ocean:water is an instance of Bejar et al.\u2019s (1991) object:stuff subcategory (ID 2g in the taxonomy) and murder:death is an instance of the cause:effect subcategory (ID 8a).", "startOffset": 36, "endOffset": 498}, {"referenceID": 4, "context": "One of the high-level categories in Bejar et al.\u2019s (1991) taxonomy is part\u2013whole (ID 2 in the taxonomy), which has ten subcategories. We claim that eight of the ten subcategories involve entailment and two do not involve entailment, which is consistent with the claim that \u2018lexical entailment does not cover all cases of meronyms\u2019 (in the above quotation). Regarding \u2018ocean and water and murder and death\u2019 (in the above quotation), the word pair ocean:water is an instance of Bejar et al.\u2019s (1991) object:stuff subcategory (ID 2g in the taxonomy) and murder:death is an instance of the cause:effect subcategory (ID 8a). Regarding relations for which there is lexical entailment in both directions, synonymy (ID 3a) is readily handled by marking it as entailing in both directions (see Tables 2 and 3 in Section 7.3). We believe that Zhitomirsky-Geffet and Dagan\u2019s (2009) argument is correct for high-level categories but incorrect for subcategories.", "startOffset": 36, "endOffset": 871}, {"referenceID": 4, "context": "In our experiments (Section 8), we train the algorithms using data based on Bejar et al.\u2019s (1991) taxonomy and then test them on previous lexical entailment datasets.", "startOffset": 76, "endOffset": 98}, {"referenceID": 4, "context": "In our experiments (Section 8), we train the algorithms using data based on Bejar et al.\u2019s (1991) taxonomy and then test them on previous lexical entailment datasets. We do not claim that Bejar et al.\u2019s (1991) taxonomy handles all cases of lexical entailment, but our results suggest that it covers enough cases to be effective.", "startOffset": 76, "endOffset": 210}, {"referenceID": 4, "context": "work may discover lexical entailments that do not fit readily in Bejar et al.\u2019s (1991) taxonomy, but we believe that the taxonomy can be expanded to handle exceptions as they are discovered.", "startOffset": 65, "endOffset": 87}, {"referenceID": 42, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005).", "startOffset": 96, "endOffset": 120}, {"referenceID": 29, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005).", "startOffset": 138, "endOffset": 148}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005).", "startOffset": 348, "endOffset": 371}, {"referenceID": 15, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 45, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 53, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 26, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 40, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 41, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 35, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 46, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 16, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence.", "startOffset": 349, "endOffset": 384}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence. Weeds and Weir (2003) introduced an asymmetric similarity measure for the degree to which a specific term a is subsumed by a more general term b (see also Weeds, Weir, and McCarthy 2004).", "startOffset": 349, "endOffset": 584}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence. Weeds and Weir (2003) introduced an asymmetric similarity measure for the degree to which a specific term a is subsumed by a more general term b (see also Weeds, Weir, and McCarthy 2004). This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010). We describe balAPinc in detail in Section 6.1. Glickman, Dagan, and Shnarch (2006) define lexical reference, which is somewhat similar to lexical entailment, but it is defined relative to a specific text, such as a sentence.", "startOffset": 349, "endOffset": 1123}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence. Weeds and Weir (2003) introduced an asymmetric similarity measure for the degree to which a specific term a is subsumed by a more general term b (see also Weeds, Weir, and McCarthy 2004). This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010). We describe balAPinc in detail in Section 6.1. Glickman, Dagan, and Shnarch (2006) define lexical reference, which is somewhat similar to lexical entailment, but it is defined relative to a specific text, such as a sentence. Mirkin, Dagan, and Shnarch (2009b) define entailment between lexical elements, which includes entailment between words and non-compositional elements.", "startOffset": 349, "endOffset": 1300}, {"referenceID": 16, "context": "\u2022 SemEval-2007 Task 4: Classification of Semantic Relations between Nominals (Girju et al. 2007) \u2013 seven semantic relation classes \u2022 SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals (Hendrickx, Kim, Kozareva, Nakov, S\u00e9aghdha, Pad\u00f3,", "startOffset": 77, "endOffset": 96}, {"referenceID": 25, "context": "\u2022 SemEval-2012 Task 2: Measuring Degrees of Relational Similarity (Jurgens et al. 2012) \u2013 seventy-nine semantic relation classes", "startOffset": 66, "endOffset": 87}, {"referenceID": 1, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 12, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 3, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 13, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 40, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 41, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 35, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 46, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 16, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 20, "context": "2007), although some are not (Hearst 1992).", "startOffset": 29, "endOffset": 42}, {"referenceID": 25, "context": "\u2019s (1991) classes using Amazon\u2019s Mechanical Turk (Jurgens et al. 2012).", "startOffset": 49, "endOffset": 70}, {"referenceID": 25, "context": "We use average precision (AP) as a performance measure for real-valued scores, following Kotlerman et al. (2010). We use precision, recall, F-measure, and accuracy as performance measures for binary-valued classification, following Baroni et al.", "startOffset": 89, "endOffset": 113}, {"referenceID": 3, "context": "We use precision, recall, F-measure, and accuracy as performance measures for binary-valued classification, following Baroni et al. (2012). The balAPinc measure (balanced average precision for distributional inclusion) is", "startOffset": 118, "endOffset": 139}, {"referenceID": 5, "context": "AP is defined as follows (Buckley and Voorhees 2000):", "startOffset": 25, "endOffset": 52}, {"referenceID": 5, "context": "Buckley and Voorhees (2000) demonstrate that AP is more stable and more discriminating than several alternative performance measures for information retrieval systems.", "startOffset": 0, "endOffset": 28}, {"referenceID": 5, "context": "Buckley and Voorhees (2000) demonstrate that AP is more stable and more discriminating than several alternative performance measures for information retrieval systems. The definition of AP reflects a bias in information retrieval. For a typical query and a typical document collection, most documents are irrelevant and the emphasis is on finding the few relevant documents. In machine learning, if we have two classes, 0 and 1, they are usually considered equally important. Kotlerman et al. (2010) emphasize the class 1 (entails), but we believe class 0 (does not entail) is also important.", "startOffset": 0, "endOffset": 500}, {"referenceID": 26, "context": "In their experiments, Kotlerman et al. (2010) report only AP1.", "startOffset": 22, "endOffset": 46}, {"referenceID": 46, "context": "For an introduction to the concepts behind word\u2013context matrices, see the survey paper by Turney and Pantel (2010).", "startOffset": 90, "endOffset": 115}, {"referenceID": 46, "context": "For both balAPinc and ConVecs, we chose the word\u2013context matrix from Turney, Neuman, Assaf, and Cohen (2011). For SimDiffs, we chose two word\u2013context matrices from Turney (2012).", "startOffset": 69, "endOffset": 109}, {"referenceID": 46, "context": "For both balAPinc and ConVecs, we chose the word\u2013context matrix from Turney, Neuman, Assaf, and Cohen (2011). For SimDiffs, we chose two word\u2013context matrices from Turney (2012). ConVecs and SimDiffs use support vector machines (SVMs) for supervised learning.", "startOffset": 69, "endOffset": 178}, {"referenceID": 26, "context": "The balAPinc asymmetric similarity measure is a balanced combination of the asymmetric APinc measure (Kotlerman et al. 2010) with the symmetric LIN measure (Lin 1998).", "startOffset": 101, "endOffset": 124}, {"referenceID": 29, "context": "2010) with the symmetric LIN measure (Lin 1998).", "startOffset": 37, "endOffset": 47}, {"referenceID": 26, "context": "We include balAPinc in our experiments because Kotlerman et al. (2010) experimentally compared it with a wide range of asymmetric similarity measures and found that balAPinc had the best performance.", "startOffset": 47, "endOffset": 71}, {"referenceID": 26, "context": "Kotlerman et al. (2010) define balAPinc with terminology from set theory, whereas ConVecs", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "We will use the set theoretical terminology of Kotlerman et al. (2010) and the linear algebraic terminology of Turney and Pantel (2010), so that the reader can easily see both perspectives.", "startOffset": 47, "endOffset": 71}, {"referenceID": 26, "context": "We will use the set theoretical terminology of Kotlerman et al. (2010) and the linear algebraic terminology of Turney and Pantel (2010), so that the reader can easily see both perspectives.", "startOffset": 47, "endOffset": 136}, {"referenceID": 6, "context": "Let the matrix X be the result of calculating the positive pointwise mutual information (PPMI) between the word w and the context c for each element fij in F (Bullinaria and Levy 2007; Turney and Pantel 2010).", "startOffset": 158, "endOffset": 208}, {"referenceID": 49, "context": "Let the matrix X be the result of calculating the positive pointwise mutual information (PPMI) between the word w and the context c for each element fij in F (Bullinaria and Levy 2007; Turney and Pantel 2010).", "startOffset": 158, "endOffset": 208}, {"referenceID": 49, "context": "The value of an element xij in X is defined as follows (Turney and Pantel 2010):", "startOffset": 55, "endOffset": 79}, {"referenceID": 29, "context": "4 ConVecs and SimDiffs are fundamentally linear algebraic in conception, whereas balAPinc is fundamentally set theoretic. We cannot readily describe all three systems with only one kind of notation. 5 Other measures of word association may be used instead of PPMI. See Chapter 5 of Manning and Sch\u00fctze (1999) for a good survey of association measures.", "startOffset": 41, "endOffset": 309}, {"referenceID": 29, "context": ") LIN is defined as follows (Lin 1998):", "startOffset": 28, "endOffset": 38}, {"referenceID": 26, "context": "In balAPinc (Equation 16), the LIN measure is combined with the APinc measure because the APinc measure by itself tends to be sensitive to cases where |Fu| or |Fv| are unusually small (Kotlerman et al. 2010).", "startOffset": 184, "endOffset": 207}, {"referenceID": 8, "context": "The corpus was indexed with the Wumpus search engine (B\u00fcttcher and Clarke 2005), which is designed for passage retrieval, rather than document retrieval.", "startOffset": 53, "endOffset": 79}, {"referenceID": 45, "context": "In the experiments with balAPinc in Section 8, the PPMI matrix X is the same matrix as used by Turney et al. (2011). The matrix has 114,501 rows and 139,246 columns.", "startOffset": 95, "endOffset": 116}, {"referenceID": 8, "context": "The corpus was indexed with the Wumpus search engine (B\u00fcttcher and Clarke 2005), which is designed for passage retrieval, rather than document retrieval. Suppose fij is an element in the matrix of raw co-occurrence frequencies F. The i-th row of the matrix corresponds to an n-gram w in WordNet and the j-th column of the matrix corresponds to a unigram c. The value of fij was calculated by sending the query w to Wumpus and counting the frequency of c in the retrieved passages. The matrix is described in detail in Section 2.1 of Turney et al. (2011).", "startOffset": 54, "endOffset": 554}, {"referenceID": 49, "context": "It is common to smooth the PPMI matrix by applying a truncated singular value decomposition (SVD) (Turney and Pantel 2010).", "startOffset": 98, "endOffset": 122}, {"referenceID": 3, "context": "Baroni et al. (2012) also found that balAPinc works better without SVD smoothing (see their Footnote 3).", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "With the ConVecs algorithm, Baroni et al. (2012) were able to match the performance of balAPinc.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "With the ConVecs algorithm, Baroni et al. (2012) were able to match the performance of balAPinc. In ConVecs, we represent a word pair a :b by the concatentation of the context vectors a for a and b for b. We apply a supervised learning algorithm to a training set of word pairs, where each word pair is represented by concatenated context vectors that are labeled entails or does not entail. The supervised learning algorithm generates a classification model, which enables us to assign labels to new word pairs, not present in the training data. Let X be a word\u2013context matrix, where the value of the cell xij in X is given by the PPMI between the i-th word w and the j-th context c. In our experiments, we use the word\u2013context matrix X from Turney et al. (2011), as in Section 6.", "startOffset": 28, "endOffset": 764}, {"referenceID": 3, "context": "With the ConVecs algorithm, Baroni et al. (2012) were able to match the performance of balAPinc. In ConVecs, we represent a word pair a :b by the concatentation of the context vectors a for a and b for b. We apply a supervised learning algorithm to a training set of word pairs, where each word pair is represented by concatenated context vectors that are labeled entails or does not entail. The supervised learning algorithm generates a classification model, which enables us to assign labels to new word pairs, not present in the training data. Let X be a word\u2013context matrix, where the value of the cell xij in X is given by the PPMI between the i-th word w and the j-th context c. In our experiments, we use the word\u2013context matrix X from Turney et al. (2011), as in Section 6.1, but now we smooth X with a truncated SVD. SVD decomposes X into the product of three matrices U\u03a3V, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UU = VV = I) and \u03a3 is a diagonal matrix of singular values (Golub and Van Loan 1996). If X is of rank r, then \u03a3 is also of rank r. Let \u03a3k, where k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by selecting the corresponding columns from U and V. The matrix Uk\u03a3kV T k is the matrix of rank k that best approximates the original matrix X, in that it minimizes the approximation errors. That is, X\u0302 = Uk\u03a3kV T k minimizes \u2016X\u0302\u2212X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub and Van Loan 1996). We represent a word pair a : b using row vectors from the matrix Uk\u03a3 p k. If a and b correspond to row vectors a and b in Uk\u03a3 p k, then a : b is represented by the 2k-dimensional vector that is the concatenation of a and b. We normalize a and b to unit length before we concatenate them. There are two parameters in Uk\u03a3 p k that need to be set. The parameter k controls the number of latent factors and the parameter p adjusts the weights of the factors, by raising the corresponding singular values in \u03a3pk to the power p. The parameter k is well-known in the literature (Landauer, McNamara, Dennis, and Kintsch 2007), but p is less familiar. Caron (2001) introduced p for improving the performance of truncated SVD with term\u2013document matrices in information retrieval.", "startOffset": 28, "endOffset": 2233}, {"referenceID": 5, "context": "lexical semantics is supported by the empirical evaluations of Bullinaria and Levy (2012) and Turney (2012).", "startOffset": 63, "endOffset": 90}, {"referenceID": 5, "context": "lexical semantics is supported by the empirical evaluations of Bullinaria and Levy (2012) and Turney (2012). In the following experiments (Section 8), we explore a range of values for p and k.", "startOffset": 63, "endOffset": 108}, {"referenceID": 3, "context": "Baroni et al. (2012) use k = 300 and p = 1.", "startOffset": 0, "endOffset": 21}, {"referenceID": 39, "context": "We also use Weka and a polynomial kernel, but we use the sequential minimal optimization (SMO) SVM in Weka (Platt 1998), because it can generate real-valued probability estimates, as well as binary-valued classes.", "startOffset": 107, "endOffset": 119}, {"referenceID": 52, "context": "The probability estimates are based on fitting the outputs of the SVM with logistic regression models (Witten et al. 2011).", "startOffset": 102, "endOffset": 122}, {"referenceID": 3, "context": "For their supervised learning algorithm, Baroni et al. (2012) used Weka with LIBSVM.", "startOffset": 41, "endOffset": 62}, {"referenceID": 47, "context": "SimDiffs uses two different word\u2013context matrices, a domain matrix, D, and a function matrix, F (Turney 2012).", "startOffset": 96, "endOffset": 109}, {"referenceID": 3, "context": "8 Baroni et al. (2012) mention k = 300 in their Footnote 3.", "startOffset": 2, "endOffset": 23}, {"referenceID": 46, "context": "Turney (2012) demonstrated that domain and function matrices work together synergetically when applied to semantic relations.", "startOffset": 0, "endOffset": 14}, {"referenceID": 46, "context": "In experiments with the development datasets (Dev1 and Dev2), we tried using the domain and function matrices with balAPinc and ConVecs, but both algorithms worked better with the word\u2013context matrix from Turney et al. (2011). For SimDiffs, the combination of the domain and function matrices from Turney (2012) had the best performance on the development datasets.", "startOffset": 205, "endOffset": 226}, {"referenceID": 46, "context": "In experiments with the development datasets (Dev1 and Dev2), we tried using the domain and function matrices with balAPinc and ConVecs, but both algorithms worked better with the word\u2013context matrix from Turney et al. (2011). For SimDiffs, the combination of the domain and function matrices from Turney (2012) had the best performance on the development datasets.", "startOffset": 205, "endOffset": 312}, {"referenceID": 46, "context": "The domain and function matrices are based on the same corpus as the word\u2013 context matrix from Turney et al. (2011). Wumpus was used to index the corpus and search for passages, in the same way as described in Section 6.", "startOffset": 95, "endOffset": 116}, {"referenceID": 46, "context": "The columns are more complex; Turney (2012) provides a detailed description of the columns and other aspects of the matrices.", "startOffset": 30, "endOffset": 44}, {"referenceID": 15, "context": "Consider the example murder |= death, suggested by the quotation from Zhitomirsky-Geffet and Dagan (2009) in Section 3.", "startOffset": 82, "endOffset": 106}, {"referenceID": 36, "context": "For R, the set of reference words, we use 2,086 words from Basic English (Ogden 1930).", "startOffset": 73, "endOffset": 85}, {"referenceID": 36, "context": "For R, the set of reference words, we use 2,086 words from Basic English (Ogden 1930). Thus a word pair a : b is represented by 2,086 \u00d7 4 = 8,344 features. The words of Basic English were selected by Ogden (1930) to form a core vocabulary, sufficient to represent most other English words by paraphrasing.", "startOffset": 74, "endOffset": 213}, {"referenceID": 25, "context": "The KDSZ dataset was introduced by Kotlerman et al. (2010) to evaluate balAPinc.", "startOffset": 35, "endOffset": 59}, {"referenceID": 15, "context": "It was created by taking a dataset of 3,200 labeled word pairs from Zhitomirsky-Geffet and Dagan (2009) and adding 572 more labeled pairs.", "startOffset": 80, "endOffset": 104}, {"referenceID": 15, "context": "It was created by taking a dataset of 3,200 labeled word pairs from Zhitomirsky-Geffet and Dagan (2009) and adding 572 more labeled pairs. The labeling of the original subset of 3,200 pairs is described in detail by ZhitomirskyGeffet and Dagan (2009). The definition of lexical entailment that the judges used was the substitutional definition given in Section 2.", "startOffset": 80, "endOffset": 251}, {"referenceID": 3, "context": "The BBDS dataset was created by Baroni et al. (2012) and has been applied to evaluating both balAPinc and ConVecs.", "startOffset": 32, "endOffset": 53}, {"referenceID": 3, "context": "The BBDS dataset was created by Baroni et al. (2012) and has been applied to evaluating both balAPinc and ConVecs. In their paper, Baroni et al. (2012) discuss several different datasets.", "startOffset": 32, "endOffset": 152}, {"referenceID": 25, "context": "The original SemEval-2012 dataset was generated in two phases, using Amazon\u2019s Mechanical Turk (Jurgens et al. 2012).", "startOffset": 94, "endOffset": 115}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories.", "startOffset": 96, "endOffset": 116}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories. For each subcategory in Bejar et al.\u2019s (1991) relation taxonomy, we have several types of information, shown in Table 1.", "startOffset": 96, "endOffset": 346}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories. For each subcategory in Bejar et al.\u2019s (1991) relation taxonomy, we have several types of information, shown in Table 1. The first four types of information come from Bejar et al. (1991) and the rest were added by Jurgens et al.", "startOffset": 96, "endOffset": 487}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories. For each subcategory in Bejar et al.\u2019s (1991) relation taxonomy, we have several types of information, shown in Table 1. The first four types of information come from Bejar et al. (1991) and the rest were added by Jurgens et al. (2012). The original SemEval-2012 dataset was generated in two phases, using Amazon\u2019s Mechanical Turk (Jurgens et al.", "startOffset": 96, "endOffset": 536}, {"referenceID": 4, "context": "Semantic relation categories 1 to 5, based on Bejar et al. (1991)", "startOffset": 46, "endOffset": 66}, {"referenceID": 4, "context": "Semantic relation categories 6 to 10, based on Bejar et al. (1991)", "startOffset": 47, "endOffset": 67}, {"referenceID": 15, "context": "1 that Zhitomirsky-Geffet and Dagan (2009) had inter-annotator agreements in the 90% range, whereas our agreement is 81%.", "startOffset": 19, "endOffset": 43}, {"referenceID": 15, "context": "The agreement of 89% is close to the levels reported by Zhitomirsky-Geffet and Dagan (2009). On the other hand, the number of pairs labeled entails drops from 48-51% for the relational definition to 22-25% for the substitional definition.", "startOffset": 68, "endOffset": 92}, {"referenceID": 0, "context": "4%), according to Fisher\u2019s Exact Test (Agresti 1996).", "startOffset": 38, "endOffset": 52}, {"referenceID": 46, "context": ") Let Gen (general) refer to the matrix from Turney et al. (2011) and let Dom and Fun refer to the domain and function matrices from Turney (2012).", "startOffset": 45, "endOffset": 66}, {"referenceID": 46, "context": ") Let Gen (general) refer to the matrix from Turney et al. (2011) and let Dom and Fun refer to the domain and function matrices from Turney (2012). In Section 6, we mentioned that we performed experiments on the development datasets (Dev1 and Dev2) in order to select the matrices for each algorithm.", "startOffset": 45, "endOffset": 147}, {"referenceID": 37, "context": "It is a gap that learning algorithms inevitably face in real applications (Pan and Yang 2010).", "startOffset": 74, "endOffset": 93}, {"referenceID": 26, "context": "Kotlerman et al. (2010) reported AP1 without AP0, but there is a trade-off between AP1 and AP0.", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "Kotlerman et al. (2010) reported AP1 without AP0, but there is a trade-off between AP1 and AP0. Kotlerman et al. (2010) did not attempt to evaluate balAPinc as a classifier, so they did not report precision, recall, F-measure, or accuracy.", "startOffset": 0, "endOffset": 120}, {"referenceID": 3, "context": "The BBDS data was used by Baroni et al. (2012) to compare balAPinc with ConVecs.", "startOffset": 26, "endOffset": 47}, {"referenceID": 3, "context": "The BBDS data was used by Baroni et al. (2012) to compare balAPinc with ConVecs. They used two different evaluation setups, similar to our standard and different setups. For balAPinc using a standard setup, they obtained an accuracy of 70.1%, slighly below our result of 72.2%. The difference is likely due to minor differences in the word\u2013context matrices that we used. For balAPinc using a different setup, their accuracy was 70.4%, compared to our 68.7%. They used their own independent dataset to tune balAPinc, whereas we used the JMTH dataset. Given that our word\u2013context matrices and our training data are different from theirs, the accuracies are closer than might be expected. For ConVecs using a standard setup, Baroni et al. (2012) report an accuracy of 88.", "startOffset": 26, "endOffset": 743}, {"referenceID": 3, "context": "16 These accuracy numbers and the numbers reported in the next paragraph are taken from Table 2 in Baroni et al. (2012).", "startOffset": 99, "endOffset": 120}, {"referenceID": 26, "context": "This is closer to the evaluation setup of Kotlerman et al. (2010). In this table, we do not use bold font to mark significant differences, because there is no agreement on the appropriate statistical test for AP1.", "startOffset": 42, "endOffset": 66}, {"referenceID": 17, "context": "Related to this proposed future work, Shnarch, Barak, and Dagan (2009) evaluated lexical reference rules (Glickman et al. 2006) derived from Wikipedia on the RTE-4 dataset.", "startOffset": 105, "endOffset": 127}, {"referenceID": 3, "context": "Baroni et al. (2012) have achieved promising results with quantifier phrases, such as all dogs |= some dogs.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Baroni et al. (2012) have achieved promising results with quantifier phrases, such as all dogs |= some dogs. Looking at Tables 2 and 3 in Section 7.3, we see a high density of 1\u2019s (entails) for class-inclusion and part-whole. The strong connection between these two categories and lexical entailment may explain why Morris and Hirst (2004) call hypernymy and meronymy classical relations, whereas the relation in chapel:funeral (spacetime, location:activity, ID 9c) is non-classical (this is one of their examples of a non-classical relation).", "startOffset": 0, "endOffset": 340}], "year": 2014, "abstractText": "Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another. For example, buy entails own. Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification). In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies. The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b. The second approach (an instance of the relation classification strategy) represents a word pair, a : b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors. Additionally, we introduce a third approach that is a new instance of the relation classification strategy. The third approach represents a word pair, a : b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words. All three approaches use vector space models (VSMs) of semantics, based on word\u2013context matrices. We perform an extensive evaluation of the three approaches using three different datasets. The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse. Along the way, we address some of the concerns raised in past research, regarding the treatment of RLE as a problem of semantic relation classification, and we suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}