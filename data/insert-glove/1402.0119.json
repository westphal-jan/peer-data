{"id": "1402.0119", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2014", "title": "Randomized Nonlinear Component Analysis", "abstract": "wamc Classical renn techniques uninvestigated such ductility as Principal Component Analysis (serapeum PCA) shinko and vebacom Canonical jenkin Correlation 2,267 Analysis (CCA) are tout ubiquitous in statistics. 116.47 However, these influentual techniques 68.77 only reveal linear relationships in rheem data. sheepfold Although catostomus nonlinear 7-elevens variants debolt of PCA 15,100 and CCA tunng have copular been pr1 proposed, emh they are computationally rozelle prohibitive in claimed the large mosinee scale.", "histories": [["v1", "Sat, 1 Feb 2014 19:54:06 GMT  (312kb,D)", "http://arxiv.org/abs/1402.0119v1", null], ["v2", "Tue, 13 May 2014 16:41:11 GMT  (530kb,D)", "http://arxiv.org/abs/1402.0119v2", "Appearing in ICML 2014"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["david lopez-paz", "suvrit sra", "alexander j smola", "zoubin ghahramani", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1402.0119"}, "pdf": {"name": "1402.0119.pdf", "metadata": {"source": "META", "title": "Randomized Nonlinear Component Analysis", "authors": ["David Lopez-Paz", "Suvrit Sra", "Alexander J. Smola", "Zoubin Ghahramani", "Bernhard Sch\u00f6lkopf"], "emails": ["DAVID@LOPEZPAZ.ORG", "SUVRIT@TUE.MPG.DE", "ALEX@SMOLA.ORG", "ZOUBIN@ENG.CAM.AC.UK", "BS@TUE.MPG.DE"], "sections": [{"heading": null, "text": "In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving dramatic savings in computational requirements.\nIn this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas also extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. Code in R implementing our methods is provided in the Appendix."}, {"heading": "1. Introduction", "text": "Principal Component Analysis (Pearson, 1901) and Canonical Correlation Analysis (Hotelling, 1936) are two of the most popular multivariate analysis methods. They have played a crucial role in a vast array of applications since their conception a century ago.\nPrincipal Component Analysis (PCA) rotates a collection of correlated variables into their uncorrelated principal components (also known as factors or latent variables). Principal components owe their name to the following key property:\nthe first principal component captures the maximum amount of variance in the data; successive components account for the maximum amount of remaining variance in dimensions orthogonal to the preceding ones. PCA is commonly used for dimensionality reduction, assuming that core properties of a high-dimensional sample are largely captured by a small number of principal components.\nCanonical Correlation Analysis (CCA) computes linear projections of a pair of random variables such that their projections are maximally correlated. Analogous to principal components, the projections of the pair of random variables are mutually orthogonal and ordered by their amount of explained cross-correlation. CCA is widely used to learn from multiple modalities of data (Kakade & Foster, 2007), an ability particularly useful when some of the modalities are only available at training time, but keeping information about them at testing time is beneficial (Chaudhuri et al., 2009; Vapnik & Vashist, 2009).\nThe applications of PCA and CCA are ubiquitous. Some examples are feature extraction, time-series prediction, finance, medicine, meteorology, chemometrics, biology, neurology, natural language processing, speech recognition, computer vision or multimodal signal processing (Jolliffe, 2002).\nDespite their success, an impediment of PCA and CCA to modern data analysis is that both reveal only linear relationships between the variables under study. To overcome this limitation, for both PCA and CCA several nonlinear extensions have been proposed. For PCA, these include Kernel Principal Component Analysis or KPCA (Scho\u0308lkopf et al., 1999) and Autoencoder Neural Networks (Baldi & Hornik, 1989; Hinton & Salakhutdinov, 2006). For CCA, common extensions are Kernel Canonical Correlation Analysis or KCCA (Lai & Fyfe, 2000; Melzer et al., 2001; Bach & Jordan, 2002) and Deep Canonical Correlation Analysis (Andrew et al., 2013). However, these solutions tend to have\nar X\niv :1\n40 2.\n01 19\nv1 [\nst at\n.M L\n] 1\nF eb\nrather high computational complexity (often cubic in the sample size), are difficult to parallelize, and are not always accompanied by theoretical guarantees.\nIn a separate strand of recent research, randomized strategies have been introduced for constructing features that can help reveal nonlinear patterns in data when used in conjunction with linear algorithms (Rahimi & Recht, 2008; Le et al., 2013). For basic tasks such as regression or classification, using nonlinear random features incurs little or no loss in performance compared with exact kernel methods, while achieving dramatic savings in computational complexity (from cubic to linear in the sample size). Furthermore, random features are amenable to simple implementation and clean theoretical analysis.\nThe main contribution of this paper is to lay the foundations for nonlinear, randomized variants of PCA and CCA. Therefore, we dedicate key attention to studying the spectral properties of low-rank kernel matrices constructed as sums of random feature dot-products. Our analysis is powered by the recently developed matrix Bernstein inequality (Mackey et al., 2012). With little additional effort, our analysis extends to other popular multivariate analysis tools such as linear discriminant analysis, spectral clustering, or the randomized dependence coefficient.\nWe demonstrate the effectiveness of the proposed randomized methods by experimenting with several real-world data and comparing against the state-of-the-art on CCA: Deep Canonical Correlation Analysis (Andrew et al., 2013). As a novel application of the presented methods, we derive an algorithm to learn using privileged information (Vapnik & Vashist, 2009). Additional numerical simulations are provided to visualize the concentration bounds derived in our theoretical analysis. Lastly, we provide an implementation of the presented algorithms in just 15 lines of R code."}, {"heading": "1.1. Related Work", "text": "There has been a recent stream of research in kernel approximations via randomized feature maps since the seminal work of Rahimi & Recht (2008). For instance, their extensions to dot-product kernels (Kar & Karnick, 2012) and polynomial kernels (Hamid et al., 2013); the development of advanced sampling techniques using Quasi-Monte-Carlo methods (Yang et al., 2014) or their accelerated computation via fast Walsh-Hadamard transforms (Le et al., 2013).\nThe use of randomized techniques for kernelized component analysis methods dates back to (Achlioptas et al., 2002), where three kernel sub-sampling strategies were suggested to speed up KPCA. On the other hand, (Avron et al., 2013) made use of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale datasets. The use of nonlinear random features is more scarce and has only appeared\ntwice in previous literature. First, McWilliams et al. (2013) use the Nystro\u0308m method to define a randomized feature map and perform CCA to achieve fast state-of-the-art semisupervised learning. Second, Lopez-Paz et al. (2013) define the dependence statistic RDC as the largest canonical correlation between two sets of copula random projections."}, {"heading": "2. Random Nonlinear Features", "text": "Before presenting our new methods, let us first recall a few key aspects of nonlinear random features.\nConsider the class of functions Fp := { f(x) = \u222b Rd \u03b1(w)\u03c6(xTw)dw : |\u03b1(w)| \u2264 Cp(w) } , (1) over which we wish to learn. Here, \u03b1 : Rd \u2192 R is a nonlinear map of \u201cweights\u201d, while \u03c6 : R\u2192 R is a nonlinear map that satisfies |\u03c6(z)| \u2264 1; x,w are vectors in Rd, p(w) is a probability density of the parameter vectors w and C is a regularizing constant. More simply, we may consider the finite version of f :\nfm(x) := \u2211m\ni=1 \u03b1i\u03c6(w\nT i x). (2)\nKernel machines, Gaussian processes, AdaBoost, and neural networks are models that fit within this function class.\nLet D = {(xi, yi)}ni=1 \u2282 Rd \u00d7 R be a finite sample of input-output pairs drawn from a distribution Q(X,Y ). We seek to approximate a function f in class Fp by minimizing the empirical risk (over dataset D)\nRemp(f) := 1\nm \u2211m i=1 c(fm(xi), yi), (3)\nfor a suitable loss function c(y\u0302, y) that penalizes departure of fm(x) from the true label y; for us, the least-squares loss c(y\u0302, y) = (y\u0302 \u2212 y)2 will be most convenient. Jointly optimizing (3) over (\u03b1,w1, . . . ,wm) used in defining fm, is a daunting task given the nonlinear nature of \u03c6. The key insight of Rahimi & Recht (2008) is that we can instead randomly sample the parameters wi \u2208 Rd from a data-independent distribution p(w) and construct an mdimensional randomized feature map z(X) for the input dataX \u2208 Rn\u00d7d that obeys the following structure:\nw1, . . . ,wm \u223c p(w), zi := [cos(w T i x1), . . . , cos(w T i xn)] \u2208 Rn,\nz(X) := [z1 \u00b7 \u00b7 \u00b7 zm] \u2208 Rn\u00d7m. (4)\nUsing the (precomputed) nonlinear random features z(X) ultimately transforms the nonconvex optimization of (3) into a least-squares problem of the form\nmin\u03b1\u2208Rd \u2016y \u2212 z(X)\u03b1\u201622 , s.t. \u2016\u03b1\u2016\u221e \u2264 C. (5)\nThis form remarkably simplifies computation (in practice, we actually solve a regularized version of it), while incurring only a bounded error. Theorem 1 formalizes this claim. Theorem 1. (Rahimi & Recht, 2008) Let Fp be defined as in (1). Draw D \u223c Q(X, Y ). Construct z(\u00b7) as in (4). Let c : R2 \u2192 R+ be a loss-function L-Lipschitz in its first argument. Then, for any \u03b4 > 0, with probability at least 1\u2212 2\u03b4 there exist some \u03b1 = (\u03b11, . . . , \u03b1m) such that\nEQ [c (z(x)\u03b1, y)]\u2212\nmin f\u2208Fp\nEQ[c(f(x), y)] \u2264 O ((\nLC\u221a n + LC\u221a m )\u221a log 1\u03b4 ) .\nSolving (5) takes O(ndm+m2n) operations, while testing t points on the fitted model takes O(tdm) operations. Recent techniques that use subsampled Hadamard randomized transforms (Le et al., 2013) allow computation of the random features even faster, yieldingO(n log(d)m+m2n) operations to solve (5) and O(t log(d)m) to test t new points.\nIt is of special interest that randomized algorithms are in many cases more robust than their deterministic analogues (Mahoney, 2011). However, the nature and quantification of the implicit regularization induced by randomness remains an open question."}, {"heading": "2.1. Random Features and Kernel Matrices", "text": "Bochner\u2019s theorem helps connect shift-invariant kernels (Scho\u0308lkopf & Smola, 2002) and random nonlinear features. Let k(x,y) be a real valued, normalized (k(x,y) \u2264 1), shift-invariant kernel on Rd \u00d7 Rd. Then,\nk(x,y) = \u222b Rd p(w)e\u2212jw T (x\u2212y)dw\n\u2248 \u2211m\ni=1\n1 me \u2212jwTi xejw T i y = \u2211m\ni=1\n1 m cos(w T i x) cos(w T i y)\n= \u3008 1\u221a m z(x), 1\u221a m z(y)\u3009,\nwhere z(x) is a random feature map as in (4) with p(w) set to be the inverse Fourier transform of k (Rahimi & Recht, 2008)\u2014e.g., the Gaussian kernel k(x,y) = exp(\u2212s\u2016x\u2212 y\u201622) can be approximated using wi \u223c N (0, 2sI). For a data matrix X \u2208 Rn\u00d7d, let K \u2208 Rn\u00d7n be its kernel matrix, i.e., Kij = k(xi,xj) . When approximating the feature map of a kernel k using random features, we may as well approximate the kernel matrixK \u2248 K\u0302, where\nK\u0302 := 1\nm z(X)z(X)T =\n1\nm m\u2211 i=1 ziz T i = m\u2211 i=1 K\u0302(i). (6)\nThe focus of this paper is on building scalable kernel component analysis methods which not only exploit these approximations but are also accompanied by theoretical guarantees."}, {"heading": "3. Principal Component Analysis (PCA)", "text": "Principal Component Analysis or PCA (Pearson, 1901; Jolliffe, 2002) is the orthogonal transformation of a set of n observations of d correlated variablesX \u2208 Rn\u00d7d into a set of n observations of d uncorrelated principal components.\nFor a centered data matrix (zero mean columns) X , PCA requires computing the (full) singular value decomposition\nX = U\u03a3F T ,\nwhere \u03a3 is a diagonal matrix containing the singular values of X in decreasing order. The principal components are computed via the linear transformationXF .\nNonlinear variants of PCA are also known; notably,\n\u2022 Kernel PCA (Scho\u0308lkopf et al., 1999) uses the kernel trick to embed data into a high-dimension Reproducing Kernel Hilbert Space, where regular PCA is performed. Computation of the principal components reduces to an eigenvalue problem, which takes O(n3) operations. \u2022 Autoencoders (Hinton & Salakhutdinov, 2006) are artifi-\ncial neural networks configured to learn their own input. They are trained to learn compressed representations of data. The transformation computed by a linear autoencoder with a bottleneck of size r < d is the projection into the subspace spanned by the first r principal components of the training data (Baldi & Hornik, 1989)."}, {"heading": "3.1. Randomized Nonlinear PCA (RPCA)", "text": "We propose RPCA, a nonlinear randomized variant of PCA. We may view RPCA as a low-rank approximation of KPCA when the latter is equipped with a shift-invariant kernel. RPCA may be thus understood as (linear) PCA on a nonlinear mapping of the data. Schematically,\n{F , z(\u00b7)} =: RPCA(X) \u2261 PCA(z(X)) \u2248 KPCA(X),\nwhere F \u2208 Rm\u00d7m are the principal component loading vectors and z : Rn\u00d7d \u2192 Rn\u00d7m is a random feature map generated as in (4) (typically, m n). The computational complexity is O(d2n) for PCA and O(m2n) for RPCA; both are linear in the sample size n, and when m d, the latter is much cheaper. When using nonlinear features as in (4), PCA loadings are no longer linear transformations but approximations of nonlinear functions belonging to the function class Fp described in Section 2. As a consequence of Bochner\u2019s theorem (Section 2.1), RPCA converges to KPCA as the number of random features m tends to infinity. This is because random feature dot-products converge uniformly to the exact kernel evaluations in expectation (Rahimi & Recht, 2008). Since the solution of KPCA is the eigensystem of the kernel matrixK\nfor the data matrixX \u2208 Rn\u00d7d, one may study how fast the approximation K\u0302 made in (6) converges in operator norm toK as m grows.\nTo analyze this convergence we appeal to the recently proposed Matrix Bernstein Inequality. In the theorem below and henceforth \u2016X\u2016 denotes the operator norm. Theorem 2 (Matrix Bernstein, (Mackey et al., 2012)). Let Z1, . . .Zm be independent d\u00d7d random matrices. Assume that E[Zi] = 0 and that \u2016Zi\u2016 \u2264 R. Define the variance parameter \u03c32 := max {\u2225\u2225\u2211 i E[ZTi Zi]\n\u2225\u2225 ,\u2225\u2225\u2211i E[ZiZTi ]\u2225\u2225}. Then, for all t \u2265 0,\nP (\u2225\u2225\u2225\u2211\ni Zi \u2225\u2225\u2225 \u2265 t) \u2264 2d \u00b7 exp{ \u2212t2 3\u03c32 + 2Rt } .\nFurthermore, E \u2225\u2225\u2225\u2211\ni Zi \u2225\u2225\u2225 \u2264 \u03c3\u221a3 log(2d) +R log(2d). The convergence rate of RPCA to its exact kernel counterpart KPCA is expressed by the following theorem, which actually invokes the Hermitian matrix version of Theorem 3.1, and hence depends on d instead of 2d, and uses matrix squares when defining the variance \u03c32.\nTheorem 3. Assume access to the data X \u2208 Rn\u00d7d and a shift-invariant, even kernel k. Construct the kernel matrix Kij := k(xi,xj) and its approximation K\u0302 using m random features as per (6). Then,\nE\u2016K\u0302 \u2212K\u2016 \u2264 \u221a 3n2 log n\nm +\n2n log n\nm . (7)\nProof. We follow a derivation similar to (Tropp, 2012).\nDenote by\nK\u0302 := 1m \u2211m i=1 ziz T i = \u2211m i=1 K\u0302(i)\nthe n \u00d7 n sample kernel matrix, and by K its population counterpart such that E[K\u0302] =K. Note that K\u0302 is the sum of them independent matrices K\u0302(i), since our random features are sampled i.i.d. and the matrixX is defined to be constant. Consider the individual error matrices\nE = K\u0302 \u2212K = \u2211m\ni=1 Ei, Ei =\n1 m (K\u0302(i) \u2212K),\neach of which satisfies E[Ei] = 0. Since we are using cosine features\u2014see z(x) in (4)\u2014it follows that there exists a constant B such that \u2016z\u20162 \u2264 B. Thus, we see that\n\u2016Ei\u2016 = 1\nm \u2016zizTi \u2212E[zzT ]\u2016 \u2264\n1 m (\u2016zi\u20162+E[\u2016z\u20162]) \u2264 2B m ,\nbecause of the triangle inequality on the norm and Jensen\u2019s inequality on the expected value. To bound the variance\nof E, bound first the variance of each of its sumands Ei (noting that E[zizTi ] =K):\nE[E2i ] = 1 m2 E [ (ziz T i \u2212K)2 ] = 1\nm2 E [ \u2016zi\u20162zizTi \u2212 zizTi K \u2212KzizTi +K2 ] 1 m2 [ BK \u2212 2K2 +K2 ] BK m2 .\nNext, taking all summands Ei together we obtain\n\u2016E[E2]\u2016 \u2264 \u2225\u2225\u2225\u2211m\ni=1 EE2i \u2225\u2225\u2225 \u2264 1 m B\u2016K\u2016.\nWhere the first inequality follows by Jensen. We can now invoke the matrix Bernstein inequality (Theorem 3.1) on E \u2212 E[E] to obtain the bound\nE\u2016K\u0302 \u2212K\u2016 \u2264 \u221a\n3B\u2016K\u2016 log n m + 2B log n m .\nObserve that random features and kernel evaluations are upper-bounded by 1; thus both B and \u2016K\u2016 are upperbounded by n, yielding the bound (7).\nTo obtain a characterization in relative-error, we can divide both sides of (7) by \u2016K\u2016. This results in a bound that depends on n logarithmically (since \u2016K\u2016 = O(n)). Moreover, additional information may be extracted from the tail-probability version of Theorem . Please refer to Section 5.1 for additional discussion on this aspect.\nBefore closing this section, we mention a byproduct of our above analysis.\nExtension to Spectral Clustering. Spectral clustering (Luxburg, 2007) uses the spectrum of K to perform dimensionality reduction before applying k-means. Therefore, the analysis of RPCA can be easily extended to obtain a randomized and nonlinear variant of spectral clustering."}, {"heading": "4. Canonical Correlation Analysis (CCA)", "text": "Canonical Correlation Analysis or CCA (Hotelling, 1936) measures the correlation between two multidimensional random variables. Specifically, given two samples X \u2208 Rn\u00d7p and Y \u2208 Rn\u00d7q, CCA computes a pair of canonical bases F \u2208 Rp\u00d7r andG \u2208 Rq\u00d7r such that\n\u2016corr(XF ,Y G)\u2212 I\u2016F is minimized, corr(XF ,XF ) = I, corr(Y G,Y G) = I,\nwhere I stands for the identity matrix. The correlations between the canonical variables XF and Y G are referred to as canonical correlations, and up to r = max(rank(X), rank(Y )) of them can be computed.\nThe canonical correlations \u03c121, . . . , \u03c1 2 r and basis vectors f1, . . . ,fr \u2208 Rp and g1, . . . , gr \u2208 Rq form the eigensystem of the generalized eigenvalue problem (Bie et al., 2005):(\n0 CXY CY X 0 )( f g ) =\n\u03c12 ( CXX + \u03b3xI 0\n0 CY Y + \u03b3yI\n)( f g ) ,\nwhere CXY is the covariance cov(X,Y ), while the diagonal terms \u03b3I act as regularization.\nIn another words, CCA processes two different views of the same data (i.e., speech audio signals and paired speaker video frames) and returns their maximally correlated linear transformations. This is particularly useful when the two views are available at training time, but only one of them is available at test time (Kakade & Foster, 2007; Chaudhuri et al., 2009; Vapnik & Vashist, 2009).\nSeveral nonlinear extensions of CCA have been proposed:\n\u2022 Kernel Canonical Correlation Analysis or KCCA (Lai & Fyfe, 2000; Melzer et al., 2001; Bach & Jordan, 2002) uses the kernel trick to derive a nonparametric and nonlinear CCA algorithm. Its exact computation takes time O(n3). If not regularized, KCCA is ill-posed and always returns \u03c12i = 1 canonical correlations. \u2022 Deep Canonical Correlation Analysis or DCCA (Andrew et al., 2013) feeds the pair of input variables through a deep neural network. Transformation weights are learnt using gradient descent to maximize the correlation of the output mappings."}, {"heading": "4.1. Randomized Nonlinear CCA (RCCA)", "text": "We now propose RCCA, a nonlinear and randomized variant of CCA. As will be shown, RCCA is a low-rank approximation of KCCA when the latter is equipped with a pair of shift-invariant kernels. RCCA can be understood as linear CCA performed on a pair of randomized nonlinear mappings (see 4): zx : Rn\u00d7p \u2192 Rn\u00d7mx , zy : Rn\u00d7q \u2192 Rn\u00d7my of the dataX \u2208 Rn\u00d7p and Y \u2208 Rn\u00d7q . Schematically,\nRCCA(X,Y ) := CCA(zx(X), zy(Y )) \u2248 KCCA(X,Y ).\nCCA requires O((p2 + q2)n) operations while RCCA costs O(max(mx,my)\n2n). Both are linear in the sample size n; if max(mx,my) max(p, q), RCCA is much cheaper. When performing RCCA, the basis vectors f1, . . . ,fr \u2208 Rp and g1, . . . , gr \u2208 Rq become the basis functions f1, . . . ,fr : Rp \u2192 R and g1, . . . , gr : Rq \u2192 R, which approximate functions in the class Fp defined in Section 2. As with PCA, we are interested in characterizing the convergence rate of RCCA to its exact kernel counterpart KCCA as\nmx andmy grow. The solution of KCCA is the eigensystem of the matrixR\u22121L, where,\nR\u22121 :=\n( (Kx + \u03b3xI)\n\u22121 0 0 (Ky + \u03b3yI) \u22121\n) , (8)\nL := ( 0 Ky Kx 0 ) , (9)\nand (\u03b3x, \u03b3y) are positive regularizers mandatory for avoiding spurious \u00b11 correlations (Bach & Jordan, 2002). Theorem 4 characterizes the convergence rate of RCCA to KCCA. Let R\u0302\u22121 and L\u0302 be the approximations to (8) and (9) obtained by using m random features; that is\nR\u0302\u22121 :=\n( (K\u0302x + \u03b3xI)\n\u22121 0 0 (K\u0302y + \u03b3yI) \u22121\n) , (10)\nL\u0302 := ( 0 K\u0302y K\u0302x 0 ) . (11)\nTheorem 4. Assume access to the datasets X \u2208 Rn\u00d7p, Y \u2208 Rn\u00d7q and shift-invariant kernels kx, ky. Define the kernel matrices (Kx)ij := kx(xi,xj), (Ky)ij := ky(yi,yj) and their approximations K\u0302x, K\u0302y using mx, my random features as in (4), respectively. Let L, R, L\u0302, R\u0302 be as defined in (8\u201311), where \u03b3x, \u03b3y > 0 are regularization parameters. Furthermore, define \u03b3 := min(\u03b3x, \u03b3y), m := min(mx,my). Then,\nE\u2016R\u0302\u22121L\u0302\u2212R\u22121L\u2016 \u2264 1 \u03b3\n(\u221a 3n2 log 2n\nm +\n2n log 2n\nm\n) .\n(12)\nProof. As the matrices are block-diagonal, we have\nE\u2016R\u0302\u22121L\u0302\u2212R\u22121L\u2016 \u2264 max(E\u2016(K\u0302x + \u03b3xI)\u22121K\u0302y \u2212 (Kx + \u03b3xI)\u22121Ky\u2016,\nE\u2016(K\u0302y + \u03b3yI)\u22121K\u0302x \u2212 (Ky + \u03b3yI)\u22121Kx\u2016).\nWe analyze the first term of the maximum; the latter can be analyzed analogously. Let A\u0302 := (K\u0302x + \u03b3xI)\u22121 and A := (Kx + \u03b3xI) \u22121. Define the individual error terms\nEi = 1 my ( A\u0302K\u0302(i)y \u2212AKy ) , E = \u2211my i=1 Ei.\nRecall that the mx +my random features are sampled i.i.d. and that the data matricesX , Y are constant. Therefore, the random matrices K\u0302(1)x , . . . , K\u0302 (mx) x , K\u0302 (1) y , . . . , K\u0302 (my) y are i.i.d. random variables. Hence, their expectations factorize:\nE [Ei] = 1my ( E[A\u0302]Ky \u2212AKy ) ,\nwhere we used E[K\u0302(i)y ] = Ky. The deviation of the individual error matrices from their expectations is\nZi := Ei \u2212 E [Ei] = 1my ( A\u0302K\u0302(i)y \u2212 E[A\u0302]Ky ) ,\nand the norm of this deviation is bounded as\n\u2016Zi\u2016 = 1\nmy \u2016A\u0302K\u0302(i)y \u2212 E[A\u0302]Ky\u2016 \u2264\n2B\nmy\u03b3x =: R.\nThe inequality follows by applying Ho\u0308lder twice after using the triangle inequality. We now turn to the issue of computing the variance, which is defined as\n\u03c32 := max {\u2225\u2225\u2225\u2211my\ni=1 E[ZiZTi ] \u2225\u2225\u2225 ,\u2225\u2225\u2225\u2211my i=1 E[ZTi Zi] \u2225\u2225\u2225} .\nConsider first second argument of the maximum above, for which we expand an individual term in the summand:\nZTi Zi = 1\nm2y\n( K\u0302(i)y A\u0302 2K\u0302(i)y +KyE[A\u0302]2Ky\n\u2212 K\u0302(i)y A\u0302E[A\u0302]Ky \u2212 E[A\u0302]KyA\u0302K\u0302(i)y ) .\nTaking expectations we see that\nE[ZTi Zi] = 1\nm2y\n( E[K\u0302(i)y A\u03022K\u0302(i)y ]\u2212KyE[A\u0302]2Ky ) 1 m2y E[K\u0302(i)y A\u03022K\u0302(i)y ],\nwhere the inequality follows asKyE[A\u0302]2Ky 0. Taking norms and invoking Jensen\u2019s inequality we then obtain \u2225\u2225E[ZTi Zi]\u2225\u2225 \u2264 B\u2016Ky\u2016m2\u03b32 . A similar argument shows that\nE[ZiZTi ] 1\nm2y E[A\u0302(K\u0302(i)y )2A\u0302]\u21d2 \u2016E[ZiZTi ]\u2016 \u2264 B\u2016Ky\u2016 m2\u03b32 .\nAn invocation of Jensen on the definition of \u03c32 along with the two bounds above yields the worst-case estimate\n\u03c32 \u2264 B\u2016Ky\u2016 my\u03b32 .\nWe may now appeal to the matrix Bernstein inequality (Theorem 3.1) to obtain the bound\nE \u2016(K\u0302x + \u03b3xI)\u22121K\u0302y \u2212 (Kx + \u03b3xI)\u22121Ky\u2016 \u2264\n1\n\u03b3x\n(\u221a 3n2 log 2n\nmy +\n2n log 2n\nmy\n) .\nFinally, the claimed result follows by analogously bounding E \u2016(K\u0302y + \u03b3yI)\u22121K\u0302x \u2212 (Ky + \u03b3yI)\u22121Kx\u2016 and taking maxima.\nBefore concluding this section, we briefly comment on two easy extensions of our above result.\nExtension to Linear Discriminant Analysis. Linear Discriminant Analysis (LDA) seeks a linear combination of the features of the data X \u2208 Rn\u00d7d such that the samples become maximally separable with respect to a paired labeling y with yi \u2208 {1, . . . , c}. LDA can be solved by CCA(X,T ), where Tij = I{yi = j} (Bie et al., 2005). Therefore, a similar analysis to the one of RCCA could be used to obtained a randomized nonlinear variant of LDA.\nExtension to RDC. The Randomized Dependence Coefficient or RDC (Lopez-Paz et al., 2013) is defined as the largest canonical correlation of RCCA when performed on the copula transformation of the data matrices ofX and Y . Our analysis applies straightforwardly to the one of RDC."}, {"heading": "5. Experiments", "text": "We investigate the performance of RCCA in multiple experiments with real-world data against state-of-the-art algorithms. For brevity, we illustrate only RCCA as it is the more difficult of the two methods proposed in this paper. Interestingly, in Section 5.3, we provide a novel algorithm based on RCCA to perform learning using privileged information (Vapnik & Vashist, 2009).\nWe set our random features to have nonlinearity \u03c6(x) = cos(x) and sample parameters according to wi \u223c N (0, 2sI). This setup approximates the widely used spherical Gaussian kernel k(x,x\u2032) = exp(\u2212s\u2016x \u2212 x\u2032\u201622) as m\u2192\u221e. The parameter s is set using Jaakkola\u2019s heuristic for each tested dataset. RCCA requires no regularization."}, {"heading": "5.1. Empirical Validation of Bernstein Inequalities", "text": "We first turn to the issue of empirically validating the bounds obtained in Theorems 3 and 4. To do so, we perform simulations in which we separately vary the values of the sample size n, the number of random projections m, and the regularization parameter \u03b3. We use synthetic data matrices X \u2208 Rn\u00d710 and Y \u2208 Rn\u00d710 formed by i.i.d. normal entries. When not varying, the parameters are fixed to n = 1000, k = 1000 and \u03b3 = 10\u22123.\nFigure 1 depicts the value of the norms from equations (7, 12) as the parameters {n,m, \u03b3} vary, when averaged over a total of 1000 random samples {Xi,Yi}1000i=1 . The agreement with the theoretical analysis is remarkable: the sample size n and regularization parameter \u03b3 exhibit a linear effect, while increasing the number of random features m induces an O(m\u22121/2) reduction in error (the closest function\u221d m\u22121/2 is overlaid in red for comparison)."}, {"heading": "5.2. Canonical Correlation Analysis", "text": "We compare the performance of three variants of CCA on the task of learning features from two modalities of the same\ndata: linear CCA, Deep CCA (Andrew et al., 2013) and the proposed RCCA. We were unable to run exact KCCA on the proposed datasets due to its cubic computational complexity.\nWe replicate the two experiments presented in Galen et al. (2013). The task is to measure performance as the accumulated correlation between the canonical variables associated with the top canonical correlations on some unseen test data. The participating datasets are MNIST and XRMB, which are introduced in the following paragraphs.\nMNIST Handwritten Digits. Learn correlated representations between the left and right halves of the MNIST dataset of handwritten digit images (LeCun & Cortes, 1998). These images have a width and height of 28 pixels; therefore, each of the two views of CCA consists on 392 features. 54000 samples are used for training, 10000 for testing and 6000 to cross-validate the parameters of each of the CCA variants.\nX-Ray Microbeam Speech Data. Learn correlated representations of simultaneous acoustic and articulatory speech measurements (Westbury, 1994). The articulatory measurements describe the position of the speaker\u2019s lips, tongue and jaws for seven consecutive frames, yielding a 112- dimensional vector at each point in time; the acoustic measurements consist on frequency cepstral coefficients (MFCCs) for the same frames, producing a 273-dimensional vector for each point in time. 30000 samples are used for training, 10000 for testing and 10000 to cross-validate the parameters of each of the CCA variants.\nSummary of Results. Figures 2 and 3 show the sum of the largest canonical correlations obtained on the test set by each CCA variant, plotted against the number of random projections m (set equally for both X and Y ) used by RCCA. Running RCCA with 2000 random projections takes two minutes on a 1.8GHz single core laptop computer when using the R code provided in Appendix A. Running a twolayered DCCA on MNIST takes on the order of 10 hours with Intel-MKL C code. The number of weights required to be stored for test time for RCCA is a hundred times lower than for DCCA for MNIST. In terms of explained correlation in the test set, RCCA performs best.\nParameter Selection. Note that no parameters were tuned for RCCA, since the random projection widths are heuristically set, and regularization was automatically provided by the use of randomness. The number of random features m can be set to the maximum value that fits within the available computational budget. On the other hand, CCA has two parameters (regularizer for each view) and DCCA has ten (two autoencoder parameters for pretraining, number of hidden layers, number of hidden units and regularizers\nfor each view). These were tuned via cross-validation using the grids described in Galen et al. (2013).\nIf desired, further speed improvements for RCCA could be achieved by distributing the computation of covariance matrices over several computers, and by making use of truncated SVD (Baglama & Reichel, 2006)."}, {"heading": "5.3. Learning Using Privileged Information", "text": "In Vapnik\u2019s paradigm Learning Using Privileged Information or LUPI (Vapnik & Vashist, 2009) the learner has access to a set of additional featuresX? during training time. These features are understood as helpful high-level \u201cteacher explanations\u201d about each of the training samples. The challenge is to build algorithms that are able extract information from this privileged information at training time to build a better classifier for test time. We propose to use RCCA to construct a highly dependent subspace between the regular featuresX and the privileged featuresX?\nWe experiment with the Animals with Attributes dataset. In this dataset, the regular set of features X is a set of 30000 pictures of 35 different animals. The set of privileged featuresX? is a set of 85 high-level binary attributes associated with each picture (such as eats-fish or can-fly). To extract information from X? at training time, we build a feature space formed by the concatenation of the 85, fivedimensional canonical variables zx(X)F (i) 1:5 associated with each RCCA(X, [X(i)? , y]), i \u2208 {1, . . . 85}. The vector y denotes the training labels.\nWe perform 15 random training/test partitions of 1000 samples each. Each partition groups a random subset of 10 animals as class \u201c0\u201d and a second random subset of 10 animals as class \u201c1\u201d. Hence, each partition is a different, challenging binary classification problem. Figure 4 shows the test classification accuracy of a linear SVM when using as features the images\u2019 SURF features or the RCCA \u201csemi-privileged\u201d features. As a side note, using directly the high-level attributes yielded 100% accuracy. The cost\nparameter of the linear SVM is cross-validated on the grid [10\u22124, . . . , 104]. We observe an average improvement of 14% in classification when using the RCCA basis instead of the image features alone. Results are statistically significant respect to a paired Wilcoxon test on a 95% confidence interval."}, {"heading": "6. Conclusions", "text": "We presented a theoretical and empirical treatment of randomized, nonlinear variants of the classical multivariate analysis methods of Principal Component Analysis and Canonical Correlation Analysis. The use of nonlinear random features significantly reduces the computational complexity of these algorithms, whilst retaining their strong predictive performance. The proposed algorithms do not present any parameters to be tuned, except for the number of random features to be used; however, this can be understood as a convenient knob to straightforwardly trade-off between speed and accuracy.\nSeveral questions of interest for further investigation remain open. For example: (i) how can we leverage knowledge from specific datasets to better design the random feature sampling distribution p(w)? (ii) how can we incorporate sparsity assumptions on the random features? (iii) can we form a hierarchy of layers of random features to build deep, more powerful component analysis methods?"}, {"heading": "A. R Source Code", "text": "rcca_fit <- function(x,y,mx,my,sx,sy) {\nw <- matrix(sx*rnorm(ncol(x)*mx),ncol(x)) m <- matrix(sy*rnorm(ncol(y)*my),ncol(y)) C <- cancor(cos(x%*%w),cos(y%*%m)) list(a=C$xcoef,b=C$ycoef,w=w,m=m)\n}\nrcca_eval <- function(rcca,x,y) list(x=cos(x%*%rcca$w)%*%rcca$a, y=cos(y%*%rcca$m)%*%rcca$b) }\nrpca_fit <- function(x,m,s) { w <- matrix(s*rnorm(ncol(x)*m),ncol(x)) list(w=w,pca=prcomp(cos(x%*%w))) }\nrpca_eval <- function(rpca,x) predict(rpca$pca,cos(x%*%rpca$w)) }"}, {"heading": "B. Acknowledgments", "text": "DLP is thankful for the fruitful discussions had with Yarin Gal and Maxim Rabinovich. DLP is supported by Obra Social \u201cla Caixa\u201d."}], "references": [{"title": "Sampling techniques for kernel methods", "author": ["Achlioptas", "Dimitris", "McSherry", "Frank", "Sch\u00f6lkopf", "Bernhard"], "venue": "In NIPS 14,", "citeRegEx": "Achlioptas et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Achlioptas et al\\.", "year": 2002}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "In ICML,", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "In ICML,", "citeRegEx": "Avron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2013}, {"title": "Restarted block lanczos bidiagonalization methods", "author": ["J. Baglama", "L. Reichel"], "venue": "Numerical Algorithms,", "citeRegEx": "Baglama and Reichel,? \\Q2006\\E", "shortCiteRegEx": "Baglama and Reichel", "year": 2006}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks,", "citeRegEx": "Baldi and Hornik,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Eigenproblems in pattern recognition", "author": ["Bie", "T. De", "N. Cristianini", "R. Rosipal"], "venue": "Handbook of Geometric Computing,", "citeRegEx": "Bie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bie et al\\.", "year": 2005}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "In ICML,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Compact random feature maps", "author": ["Hamid", "Raffay", "Xiao", "Ying", "Gittens", "Alex", "DeCoste", "Dennis"], "venue": null, "citeRegEx": "Hamid et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hamid et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science,", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["S.M. Kakade", "D.P. Foster"], "venue": "In COLT,", "citeRegEx": "Kakade and Foster,? \\Q2007\\E", "shortCiteRegEx": "Kakade and Foster", "year": 2007}, {"title": "Random feature maps for dot product kernels", "author": ["Kar", "Purushottam", "Karnick", "Harish"], "venue": null, "citeRegEx": "Kar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2012}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P. Lai", "C. Fyfe"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Lai and Fyfe,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe", "year": 2000}, {"title": "Fastfood \u2013 Approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarlos", "A. Smola"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "The randomized dependence coefficient", "author": ["D. Lopez-Paz", "P. Hennig", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2013}, {"title": "A tutorial on spectral clustering", "author": ["U. Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "Luxburg,? \\Q2007\\E", "shortCiteRegEx": "Luxburg", "year": 2007}, {"title": "Matrix Concentration Inequalities via the Method of Exchangeable Pairs", "author": ["L. Mackey", "M.I. Jordan", "R.Y. Chen", "B. Farrell", "J.A. Tropp"], "venue": "ArXiv e-prints,", "citeRegEx": "Mackey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2012}, {"title": "Randomized algorithms for matrices and data", "author": ["M.W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney,? \\Q2011\\E", "shortCiteRegEx": "Mahoney", "year": 2011}, {"title": "Correlated random features for fast semi-supervised learning", "author": ["B. McWilliams", "D. Balduzzi", "J. Buhmann"], "venue": "In NIPS,", "citeRegEx": "McWilliams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McWilliams et al\\.", "year": 2013}, {"title": "Kernel canonical correlation analysis", "author": ["T. Melzer", "M. Reiter", "H. Bischof"], "venue": "Proceedings of the International Conference on Artificial Neural Networks,", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine,", "citeRegEx": "Pearson,? \\Q1901\\E", "shortCiteRegEx": "Pearson", "year": 1901}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": null, "citeRegEx": "Rahimi and Recht,? \\Q2008\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2008}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Kernel principal component analysis. In Advances in kernel methods Support vector learning, pp. 327\u2013352", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.R. M\u00fcller"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "User-Friendly Tools for Random Matrices: An Introduction", "author": ["J.A. Tropp"], "venue": "NIPS Tutorials,", "citeRegEx": "Tropp,? \\Q2012\\E", "shortCiteRegEx": "Tropp", "year": 2012}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["V. Vapnik", "A. Vashist"], "venue": "Neural Networks,", "citeRegEx": "Vapnik and Vashist,? \\Q2009\\E", "shortCiteRegEx": "Vapnik and Vashist", "year": 2009}, {"title": "X-Ray microbeam speech production database user\u2019s handbook version 1.0", "author": ["J.R. Westbury"], "venue": null, "citeRegEx": "Westbury,? \\Q1994\\E", "shortCiteRegEx": "Westbury", "year": 1994}, {"title": "Quasi-Monte Carlo Feature Maps for ShiftInvariant Kernels", "author": ["Yang", "Jiyan", "Sindhwani", "Vikas", "Avron", "Haim", "Mahoney", "Michael W"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Principal Component Analysis (Pearson, 1901) and Canonical Correlation Analysis (Hotelling, 1936) are two of the most popular multivariate analysis methods.", "startOffset": 29, "endOffset": 44}, {"referenceID": 6, "context": "CCA is widely used to learn from multiple modalities of data (Kakade & Foster, 2007), an ability particularly useful when some of the modalities are only available at training time, but keeping information about them at testing time is beneficial (Chaudhuri et al., 2009; Vapnik & Vashist, 2009).", "startOffset": 247, "endOffset": 295}, {"referenceID": 23, "context": "For PCA, these include Kernel Principal Component Analysis or KPCA (Sch\u00f6lkopf et al., 1999) and Autoencoder Neural Networks (Baldi & Hornik, 1989; Hinton & Salakhutdinov, 2006).", "startOffset": 67, "endOffset": 91}, {"referenceID": 19, "context": "For CCA, common extensions are Kernel Canonical Correlation Analysis or KCCA (Lai & Fyfe, 2000; Melzer et al., 2001; Bach & Jordan, 2002) and Deep Canonical Correlation Analysis (Andrew et al.", "startOffset": 77, "endOffset": 137}, {"referenceID": 1, "context": ", 2001; Bach & Jordan, 2002) and Deep Canonical Correlation Analysis (Andrew et al., 2013).", "startOffset": 69, "endOffset": 90}, {"referenceID": 12, "context": "In a separate strand of recent research, randomized strategies have been introduced for constructing features that can help reveal nonlinear patterns in data when used in conjunction with linear algorithms (Rahimi & Recht, 2008; Le et al., 2013).", "startOffset": 206, "endOffset": 245}, {"referenceID": 16, "context": "Our analysis is powered by the recently developed matrix Bernstein inequality (Mackey et al., 2012).", "startOffset": 78, "endOffset": 99}, {"referenceID": 1, "context": "We demonstrate the effectiveness of the proposed randomized methods by experimenting with several real-world data and comparing against the state-of-the-art on CCA: Deep Canonical Correlation Analysis (Andrew et al., 2013).", "startOffset": 201, "endOffset": 222}, {"referenceID": 7, "context": "For instance, their extensions to dot-product kernels (Kar & Karnick, 2012) and polynomial kernels (Hamid et al., 2013); the development of advanced sampling techniques using Quasi-Monte-Carlo methods (Yang et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 27, "context": ", 2013); the development of advanced sampling techniques using Quasi-Monte-Carlo methods (Yang et al., 2014) or their accelerated computation via fast Walsh-Hadamard transforms (Le et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 12, "context": ", 2014) or their accelerated computation via fast Walsh-Hadamard transforms (Le et al., 2013).", "startOffset": 76, "endOffset": 93}, {"referenceID": 0, "context": "The use of randomized techniques for kernelized component analysis methods dates back to (Achlioptas et al., 2002), where three kernel sub-sampling strategies were suggested to speed up KPCA.", "startOffset": 89, "endOffset": 114}, {"referenceID": 2, "context": "On the other hand, (Avron et al., 2013) made use of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale datasets.", "startOffset": 19, "endOffset": 39}, {"referenceID": 0, "context": "The use of randomized techniques for kernelized component analysis methods dates back to (Achlioptas et al., 2002), where three kernel sub-sampling strategies were suggested to speed up KPCA. On the other hand, (Avron et al., 2013) made use of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale datasets. The use of nonlinear random features is more scarce and has only appeared twice in previous literature. First, McWilliams et al. (2013) use the Nystr\u00f6m method to define a randomized feature map and perform CCA to achieve fast state-of-the-art semisupervised learning.", "startOffset": 90, "endOffset": 462}, {"referenceID": 0, "context": "The use of randomized techniques for kernelized component analysis methods dates back to (Achlioptas et al., 2002), where three kernel sub-sampling strategies were suggested to speed up KPCA. On the other hand, (Avron et al., 2013) made use of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale datasets. The use of nonlinear random features is more scarce and has only appeared twice in previous literature. First, McWilliams et al. (2013) use the Nystr\u00f6m method to define a randomized feature map and perform CCA to achieve fast state-of-the-art semisupervised learning. Second, Lopez-Paz et al. (2013) define the dependence statistic RDC as the largest canonical correlation between two sets of copula random projections.", "startOffset": 90, "endOffset": 626}, {"referenceID": 12, "context": "Recent techniques that use subsampled Hadamard randomized transforms (Le et al., 2013) allow computation of the random features even faster, yieldingO(n log(d)m+mn) operations to solve (5) and O(t log(d)m) to test t new points.", "startOffset": 69, "endOffset": 86}, {"referenceID": 17, "context": "It is of special interest that randomized algorithms are in many cases more robust than their deterministic analogues (Mahoney, 2011).", "startOffset": 118, "endOffset": 133}, {"referenceID": 20, "context": "Principal Component Analysis or PCA (Pearson, 1901; Jolliffe, 2002) is the orthogonal transformation of a set of n observations of d correlated variablesX \u2208 Rn\u00d7d into a set of n observations of d uncorrelated principal components.", "startOffset": 36, "endOffset": 67}, {"referenceID": 23, "context": "\u2022 Kernel PCA (Sch\u00f6lkopf et al., 1999) uses the kernel trick to embed data into a high-dimension Reproducing Kernel Hilbert Space, where regular PCA is performed.", "startOffset": 13, "endOffset": 37}, {"referenceID": 16, "context": "Theorem 2 (Matrix Bernstein, (Mackey et al., 2012)).", "startOffset": 29, "endOffset": 50}, {"referenceID": 24, "context": "We follow a derivation similar to (Tropp, 2012).", "startOffset": 34, "endOffset": 47}, {"referenceID": 15, "context": "Spectral clustering (Luxburg, 2007) uses the spectrum of K to perform dimensionality reduction before applying k-means.", "startOffset": 20, "endOffset": 35}, {"referenceID": 5, "context": ", gr \u2208 R form the eigensystem of the generalized eigenvalue problem (Bie et al., 2005): ( 0 CXY CY X 0 )( f g ) =", "startOffset": 68, "endOffset": 86}, {"referenceID": 6, "context": "This is particularly useful when the two views are available at training time, but only one of them is available at test time (Kakade & Foster, 2007; Chaudhuri et al., 2009; Vapnik & Vashist, 2009).", "startOffset": 126, "endOffset": 197}, {"referenceID": 19, "context": "Several nonlinear extensions of CCA have been proposed: \u2022 Kernel Canonical Correlation Analysis or KCCA (Lai & Fyfe, 2000; Melzer et al., 2001; Bach & Jordan, 2002) uses the kernel trick to derive a nonparametric and nonlinear CCA algorithm.", "startOffset": 104, "endOffset": 164}, {"referenceID": 1, "context": "\u2022 Deep Canonical Correlation Analysis or DCCA (Andrew et al., 2013) feeds the pair of input variables through a deep neural network.", "startOffset": 46, "endOffset": 67}, {"referenceID": 5, "context": "LDA can be solved by CCA(X,T ), where Tij = I{yi = j} (Bie et al., 2005).", "startOffset": 54, "endOffset": 72}, {"referenceID": 14, "context": "The Randomized Dependence Coefficient or RDC (Lopez-Paz et al., 2013) is defined as the largest canonical correlation of RCCA when performed on the copula transformation of the data matrices ofX and Y .", "startOffset": 45, "endOffset": 69}, {"referenceID": 1, "context": "data: linear CCA, Deep CCA (Andrew et al., 2013) and the proposed RCCA.", "startOffset": 27, "endOffset": 48}, {"referenceID": 1, "context": "data: linear CCA, Deep CCA (Andrew et al., 2013) and the proposed RCCA. We were unable to run exact KCCA on the proposed datasets due to its cubic computational complexity. We replicate the two experiments presented in Galen et al. (2013). The task is to measure performance as the accumulated correlation between the canonical variables associated with the top canonical correlations on some unseen test data.", "startOffset": 28, "endOffset": 239}, {"referenceID": 26, "context": "Learn correlated representations of simultaneous acoustic and articulatory speech measurements (Westbury, 1994).", "startOffset": 95, "endOffset": 111}], "year": 2017, "abstractText": "Classical techniques such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics. However, these techniques only reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, they are computationally prohibitive in the large scale. In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving dramatic savings in computational requirements. In this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas also extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. Code in R implementing our methods is provided in the Appendix.", "creator": "LaTeX with hyperref package"}}}