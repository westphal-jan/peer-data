{"id": "1402.0288", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2014", "title": "Transductive Learning with Multi-class Volume Approximation", "abstract": "jfl Given a wihdat hypothesis non-corporate space, the large praya volume magali principle vtk by warboys Vladimir whirr Vapnik dicen prioritizes superb equivalence r\u00e9duit classes according xiaozhuang to teve their enidtem volume parting in the subsidies hypothesis malnutrition space. magomadov The volume approximation chibebe has hitherto been ab\u016b successfully klatz applied modification to chari binary gral learning napa problems. In keobounphan this 2208 paper, maedhros we extend janowicz it 6.6875 naturally to peque\u00f1os a rodent-like more general definition ramaroson which can be applied andrassy to several transductive problem brookbank settings, such moxon as multi - class, quarrelled multi - label and catbird serendipitous facemasks learning. webelos Even quesadilla though faiello the mohmmad resultant learning method involves a non - delinking convex judicature optimization problem, the globally christer optimal solution is 1,565 almost mid-2001 surely 111.15 unique munakata and can lazarevic be taqiyya obtained in ossetia O (n ^ 3) time. transhumanism We theoretically all-american provide stability and mivart error eile analyses information-gathering for aspirant the proposed method, recuperate and then fashiontv experimentally cramton show beeswax that it is senegambia promising.", "histories": [["v1", "Mon, 3 Feb 2014 06:09:52 GMT  (261kb,D)", "http://arxiv.org/abs/1402.0288v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["gang niu", "bo dai", "marthinus christoffel du plessis", "masashi sugiyama"], "accepted": true, "id": "1402.0288"}, "pdf": {"name": "1402.0288.pdf", "metadata": {"source": "CRF", "title": "Transductive Learning with Multi-class Volume Approximation", "authors": ["Gang Niu", "Masashi Sugiyama"], "emails": ["gang@sg.cs.titech.ac.jp", "bohr.dai@gmail.com", "christo@sg.cs.titech.ac.jp", "sugi@cs.titech.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "The history of the large volume principle (LVP) goes back to the early age of the statistical learning theory when Vapnik (1982) introduced it for the case of hyperplanes. But it did not gain much attention until a creative approximation was proposed in El-Yaniv et al. (2008) to implement LVP for the case of soft response vectors. From then on, it has been applied to various binary learning problems successfully, such as binary transductive\nar X\niv :1\n40 2.\n02 88\nlearning (El-Yaniv et al., 2008), binary clustering (Niu et al., 2013a), and outlier detection (Li and Ng, 2013).\nLVP is a learning-theoretic principle which views learning as hypothesis selecting from a certain hypothesis space H. Despite the form of the hypothesis, H can always be partitioned into a finite number of equivalence classes after we observe certain data, where an equivalence class is a set of hypotheses that generate the same labeling of the observed data. LVP, as one of the learning-theoretic principles from the statistical learning theory, prioritizes those equivalence classes according to the volume they occupy in H. See the illustration in Figure 1: The blue ellipse represents H, and it is partitioned into C1, . . . , C4 each occupying a quadrant of the Cartesian coordinate system R2 intersected with H; LVP claims that C1 and C3 are more preferable than C2 and C4, since C1 and C3 have larger volume than C2 and C4.\nIn practice, the hypothesis space H cannot be as simple as in Figure 1. It frequently locates in very high-dimensional spaces where exact or even quantifiable volume estimation is challenging. Therefore, El-Yaniv et al. (2008) proposed a volume approximation to bypass the volume estimation. Instead of focusing on the equivalence classes of H, it directly focuses on the hypotheses in H since learning is regarded as hypothesis selecting in LVP. It defines H via an ellipsoid, measures the angles from hypotheses to the principal axes of H, and then prefers hypotheses near the long principal axes to those near the short ones. This manner is reasonable, since the long principal axes of H lie in large-volume regions. In Figure 1, h and h\u2032 are two hypotheses and v1/v2 is the long/short principal axis; LVP advocates that h is more preferable than h\u2032 as h is close to v1 and h\n\u2032 is close to v2. We can adopt this volume approximation to regularize our loss function, which has been demonstrated helpful for various binary learning problems.\nNevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn\u00d7c and linked to an ellipsoid in Rnc, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed.\nWe name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n3) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al. (2003) and Belkin et al. (2006) using USPS, MNIST, 20Newsgroups and Isolet.\nThe rest of this paper is organized as follows. In Section 2 the binary volume approximation is reviewed, and in Section 3 the multi-class volume approximation is derived. In Section 4, we develop and analyze MAVR. Finally, the experimental results are in Section 5."}, {"heading": "2 Binary Volume Approximation", "text": "The binary volume approximation in El-Yaniv et al. (2008) involves a few key concepts: The soft response vector, the hypothesis space and the equivalence class, and the power and volume of equivalence classes. We review the concepts in this section for later use in the next section.\nSuppose that X is the domain of input data, and most often but not necessarily, X \u2282 Rd where d is a natural number. Given a set of n data Xn = {x1, . . . , xn} where xi \u2208 X , a soft response vector is an n-dimensional vector\nh := (h1, . . . , hn) > \u2208 Rn, (1)\nso that hi stands for a soft or confidence-rated label of xi. For binary transductive learning problems, a soft response vector h suggests that xi is from the positive class if hi > 0, xi is from the negative class if hi < 0, and the above two cases are equally possible if hi = 0.\nA hypothesis space is a collection of hypotheses. The volume approximation requires a symmetric positive-definite matrix Q \u2208 Rn\u00d7n which contains the pairwise information about Xn. Consider the hypothesis space\nHQ := {h | h>Qh \u2264 1}, (2)\nwhere the hypotheses are soft response vectors. The set of sign vectors {sign(h) | h \u2208 HQ} contains all of N = 2n possible dichotomies of Xn, and HQ can be partitioned into a finite number of equivalence classes C1, . . . , CN , such that for fixed k, all hypotheses in Ck will generate the same labeling of Xn.\nThen, in statistical learning theory, the power of an equivalence class Ck is defined as the probability mass of all hypotheses in it (Vapnik, 1998, p. 708), i.e.,\nP(Ck) := \u222b Ck p(h)dh, k = 1, . . . , N,\nwhere p(h) is the underlying probability density of h over HQ. The hypotheses in Ck which has a large power should be preferred according to Vapnik (1998).\nWhen no specific domain knowledge is available (i.e., p(h) is unknown), it would be natural to assume the continuous uniform distribution p(h) = 1/ \u2211N k=1 V(Ck), where\nV(Ck) := \u222b Ck dh, k = 1, . . . , N,\nis the volume of Ck. That is, the volume of an equivalence class is defined as the geometric volume of all hypotheses in it. As a result, P(Ck) is proportional to V(Ck), and the larger the value V(Ck) is, the more confident we are of the hypotheses chosen from Ck.\nHowever, it is very hard to accurately compute the geometric volume of even a single convex body in Rn, let alone all 2n convex bodies, so El-Yaniv et al. (2008) introduced an efficient approximation. Let \u03bb1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn be the eigenvalues of Q, and v1, . . . ,vn be the associated orthonormal eigenvectors. Actually, the hypothesis space HQ in Eq. (2) is geometrically an origin-centered ellipsoid in Rn with vi and 1/ \u221a \u03bbi as the direction and length of its i-th principal axis. Note that a small angle from a hypothesis h in Ck to some vi with a small/large index i (i.e., a long/short principal axis) implies that V(Ck) is large/small (cf. Figure 1). Based on this crucial observation, we define\nV (h) := n\u2211 i=1 \u03bbi ( h>vi \u2016h\u20162 )2 = h>Qh \u2016h\u201622 , (3)\nwhere h>vi/\u2016h\u20162 means the cosine of the angle between h and vi. We subsequently expect V (h) to be small when h lies in a large-volume equivalence class, and conversely to be large when h lies in a small-volume equivalence class."}, {"heading": "3 Multi-class Volume Approximation", "text": "In this section, we propose a more general multi-class volume approximation that fits for several problem settings."}, {"heading": "3.1 Problem settings", "text": "Recall the setting of binary transductive problems (Vapnik, 1998, p. 341). A fixed set Xn = {x1, . . . , xn} of n points from X is observed, and the labels y1, . . . , yn \u2208 {\u22121,+1} of these points are also fixed but unknown. A subset Xl \u2282 Xn of size l is picked uniformly at random, and then yi is revealed if xi \u2208 Xl. We call Sl = {(xi, yi) | xi \u2208 Xl} the labeled data and Xu = Xn \\Xl the unlabeled data. Using Sl and Xu, the goal is to predict yi of xi \u2208 Xu (while any unobserved x \u2208 X \\Xn is currently left out of account).\nA slight modification suffices to extend the setting. Instead of y1, . . . , yn \u2208 {\u22121,+1}, we assume that y1, . . . , yn \u2208 Y where Y = {1, . . . , c} is the domain of labels and c is a natural number. Though the binary setting is popular, this multi-class setting has been studied in just a few previous works such as Szummer and Jaakkola (2001) and Zhou et al. (2003). Without loss of generality, we assume that each of the c labels possesses some labeled data.\nIn addition, it would be a multi-label setting, if y1, . . . , yn \u2286 Y with Y = {1, . . . , c} where each yi is a label set, or if y1, . . . , yn \u2208 Y with Y = {\u22121, 0, 1}c where each yi is a label vector. To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2c possible label sets and 3c possible label vectors.\nA more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi \u2208 Xl} and Yu = {yi | xi \u2208 Xu, yi 6\u2208 Yl}, then we have #Yu \u2265 1 where # measures the cardinality. It is still solvable when #Yu = 1 if a special label of outliers is allowed and when #Yu > 1 as a combination of classification and clustering problems. Zhang et al. (2011) is the unique previous work which successfully dealt with #Yu = 2 and #Yu = 3."}, {"heading": "3.2 Definitions", "text": "The multi-class volume approximation to be proposed can handle all the problem settings discussed so far in a unified manner. In order to extend the binary definitions, we need only to extend the hypothesis and the hypothesis space.\nTo begin with, we allocate a soft response vector in Eq. (1) for each of the c labels:\nh1 = (h1,1, . . . , hn,1) >, . . . ,hc = (h1,c, . . . , hn,c) >.\nThe value hi,j is a soft or confidence-rated label of xi concerning the j-th label and it suggests that\n\u2022 xi should possess the j-th label, if hi,j > 0; \u2022 xi should not possess the j-th label, if hi,j < 0; \u2022 the above two cases are equally possible, if hi,j = 0.\nFor multi-class and serendipitous problems, yi is predicted by y\u0302i = arg maxj hi,j. For multi-label problems, we need a threshold Th that is either preset or learned since usually positive and negative labels are imbalanced, and yi can be predicted by y\u0302i = {j | hi,j \u2265 Th}; or we can use the label set prediction methods proposed in Kong et al. (2013).\nThen, a soft response matrix as our transductive hypothesis is an n-by-c matrix defined by H = (h1, . . . ,hc) \u2208 Rn\u00d7c, (4) and a stacked soft response vector as an equivalent hypothesis is an nc-dimensional vector defined by\nh = vec(H) = (h>1, . . . ,h > c) > \u2208 Rnc,\nwhere vec(H) is the vectorization of H formed by stacking its columns into a single vector. As the binary definition of the hypothesis space, a symmetric positive-definite matrix Q \u2208 Rn\u00d7n which contains the pairwise information about Xn is provided, and we assume further that a symmetric positive-definite matrix P \u2208 Rc\u00d7c which contains the pairwise information about Y is available. Consider the hypothesis space\nHP,Q := {H | tr(H>QHP ) \u2264 1}, (5)\nwhere the hypotheses are soft response matrices. Let P \u2297 Q \u2208 Rnc\u00d7nc be the Kronecker product of P and Q. Due to the symmetry and the positive definiteness of P and Q, the Kronecker product P \u2297Q is also symmetric and positive definite, and HP,Q in (5) could be defined equivalently as\nHP,Q := {H | vec(H)>(P \u2297Q) vec(H) \u2264 1}. (6)\nThe equivalence of Eqs. (5) and (6) comes from the fact that tr(H>QHP ) = vec(H)>(P \u2297 Q) vec(H) following the well-known identity (see, e.g., Theorem 13.26 of Laub, 2005)\n(P>\u2297Q) vec(H) = vec(QHP ).\nAs a consequence, there is a bijection between HP,Q and\nEP,Q := {h | h>(P \u2297Q)h \u2264 1}\nwhich is geometrically an origin-centered ellipsoid in Rnc. The set of sign vectors {sign(h) | h \u2208 EP,Q} spreads over all the N = 2nc quadrants of Rnc, and thus the set of sign matrices {sign(H) | H \u2208 HP,Q} contains all of N possible dichotomies of Xn\u00d7{1, . . . , c}. In other words, HP,Q can be partitioned into N equivalence classes C1, . . . , CN , such that for fixed k, all soft response matrices in Ck will generate the same labeling of Xn \u00d7 {1, . . . , c}.\nThe definition of the power is same as before, and so is the definition of the volume: V(Ck) := \u222b Ck dH, k = 1, . . . , N.\nBecause of the bijection between HP,Q and EP,Q, V(Ck) is likewise the geometric volume of all stacked soft response vectors in the intersection of the k-th quadrant of Rnc and EP,Q. By a similar argument to the definition of V (h), we define\nV (H) := h>(P \u2297Q)h \u2016h\u201622 = tr(H>QHP ) \u2016H\u20162Fro , (7)\nwhere h = vec(H) and \u2016H\u2016Fro means the Frobenius norm of H. We subsequently expect V (H) to be small when H lies in a large-volume equivalence class, and conversely to be large when H lies in a small-volume equivalence class.\nNote that V (H) and V (h) are consistent for binary learning problem settings. We can constrain h1 + h2 = 0n if c = 2 where 0n is the all-zero vector in Rn. Let P = I2 where I2 is the identity matrix of size 2, then\nV (H) = h>1Qh1 + h > 2Qh2\n\u2016h1\u201622 + \u2016h2\u201622 = h>1Qh1 \u2016h1\u201622 = V (h1),\nwhich coincides with V (h) defined in Eq. (3). Similarly to V (h), for two soft response matrices H and H \u2032 from the same equivalence class, V (H) and V (H \u2032) may not necessarily be the same value. In addition, the domain of V (H) could be extended to Rn\u00d7c though the definition of V (H) is originally null for H outside HP,Q."}, {"heading": "4 Multi-class Approximate Volume Regularization", "text": "The proposed volume approximation motivates a family of new transductive methods taking it as a regularization. We develop and analyze an instantiation in this section whose optimization problem is non-convex but can be solved exactly and efficiently."}, {"heading": "4.1 Model", "text": "First of all, we define the label indicator matrix Y \u2208 Rn\u00d7c for convenience whose entries can be from either {0, 1} or {\u22121, 0, 1} depending on the problem settings and whether negative labels ever appear. Specifically, we can set Yi,j = 1 if xi is labeled to have the j-th label and Yi,j = 0 otherwise, or alternatively we can set Yi,j = 1 if xi is labeled to have the j-th label, Yi,j = \u22121 if xi is labeled to not have the j-th label, and Yi,j = 0 otherwise.\nLet \u2206(Y,H) be our loss function measuring the difference between Y and H. The multi-class volume approximation motivates the following family of transductive methods:\nmin H\u2208HP,Q\n\u2206(Y,H) + \u03b3 \u00b7 tr(H >QHP )\n\u2016H\u20162Fro ,\nwhere \u03b3 > 0 is a regularization parameter. The denominator \u2016H\u20162Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al. (2013a).\nWe fix a scale parameter \u03c4 > 0, constrain H to be of norm \u03c4 , replace the feasible region HP,Q with Rn\u00d7c by extending the domain of V (H) implicitly, and it becomes\nmin H\u2208Rn\u00d7c\n\u2206(Y,H) + \u03b3 tr(H>QHP )\ns.t. \u2016H\u2016Fro = \u03c4. (8)\nAlthough the optimization in (8) is done in Rn\u00d7c, the regularization is carried out relative to HP,Q, since under the constraint \u2016H\u2016Fro = \u03c4 , the regularization tr(H>QHP ) is a weighted sum of the squares of cosines between vec(H) and the principal axes of EP,Q like El-Yaniv et al. (2008).\nSubsequently, we denote by y1, . . . ,yn and r1, . . . , rn the c-dimensional vectors that satisfy Y = (y1, . . . ,yn) > and H = (r1, . . . , rn) >. Consider the following loss functions to be \u2206(Y,H) in optimization (8):\n1. Squared losses over all data \u2211\nXn \u2016yi \u2212 ri\u201622; 2. Squared losses over labeled data \u2211\nXl \u2016yi \u2212 ri\u201622; 3. Linear losses over all data \u2211\nXn \u2212y>iri; 4. Linear losses over labeled data \u2211\nXl \u2212y>iri;\nThe first loss function has been used for multi-class transductive learning (Zhou et al., 2003) and the binary counterparts of the fourth and third loss functions have been used for binary transductive learning (El-Yaniv et al., 2008) and clustering (Niu et al., 2013a). Actually, the third and fourth loss functions are identical since yi for xi \u2208 Xu is identically zero, and the first loss function is equivalent to them in (8) since \u2211 Xn \u2016yi\u201622 and\u2211\nXl \u2016yi\u201622 are constants and \u2211 Xn \u2016ri\u201622 = \u03c4 2 is also a constant. The second loss function is undesirable for (8) due to an issue of the time complexity which will be discussed later. Thus, we instantiate \u2206(Y,H) := \u2211 Xn \u2016yi \u2212 ri\u201622 = \u2016Y \u2212 H\u20162Fro, and optimization (8) becomes\nmin H\u2208Rn\u00d7c\n\u2016Y \u2212H\u20162Fro + \u03b3 tr(H>QHP )\ns.t. \u2016H\u2016Fro = \u03c4. (9)\nWe refer to constrained optimization problem (9) as multi-class approximate volume regularization (MAVR). An unconstrained version of MAVR is then\nmin H\u2208Rn\u00d7c\n\u2016Y \u2212H\u20162Fro + \u03b3 tr(H>QHP ). (10)"}, {"heading": "4.2 Algorithm", "text": "Optimization (9) is non-convex, but we can rewrite it using the stacked soft response vector h = vec(H) as\nmin h\u2208Rnc\n\u2016y \u2212 h\u201622 + \u03b3h>(P \u2297Q)h\ns.t. \u2016h\u20162 = \u03c4, (11)\nwhere y = vec(Y ) is the vectorization of Y . In this representation, the objective is a second-degree polynomial and the constraint is an origin-centered sphere, and fortunately we could solve it exactly and efficiently following Forsythe and Golub (1965). To this end, a fundamental property of the Kronecker product is necessary (see, e.g., Theorems 13.10 and 13.12 of Laub, 2005):\nTheorem 1. Let \u03bbQ,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbQ,n be the eigenvalues and vQ,1, . . . ,vQ,n be the associated orthonormal eigenvectors of Q, \u03bbP,1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbP,c and vP,1, . . . ,vP,c be those of P , and the eigen-decompositions of Q and P be Q = VQ\u039bQV > Q and P = VP\u039bPV > P . Then, the eigenvalues of P \u2297Q are \u03bbP,j\u03bbQ,i associated with orthonormal eigenvectors vP,j \u2297vQ,i for j = 1, . . . , c, i = 1, . . . , n, and the eigen-decomposition of P \u2297Q is P \u2297Q = VPQ\u039bPQV>PQ, where \u039bPQ = \u039bP \u2297 \u039bQ and VPQ = VP \u2297 VQ.\nAfter we ignore the constants \u2016y\u201622 and \u2016h\u201622 in the objective of optimization (11), the Lagrange function is\n\u03a6(h, \u03c1) = \u22122h>y + \u03b3h>(P \u2297Q)h\u2212 \u03c1(h>h\u2212 \u03c4 2), where \u03c1 \u2208 R is the Lagrangian multiplier for \u2016h\u201622 = \u03c4 2. The stationary conditions are\n\u2202\u03a6/\u2202h = \u2212y + \u03b3(P \u2297Q)h\u2212 \u03c1h = 0nc, (12) \u2202\u03a6/\u2202\u03c1 = h>h\u2212 \u03c4 2 = 0. (13)\nHence, for any locally optimal solution (h, \u03c1) where \u03c1/\u03b3 is not an eigenvalue of P \u2297 Q, we have\nh = (\u03b3P \u2297Q\u2212 \u03c1Inc)\u22121y (14) = VPQ(\u03b3\u039bPQ \u2212 \u03c1Inc)\u22121V>PQy = (VP \u2297 VQ)(\u03b3\u039bPQ \u2212 \u03c1Inc)\u22121 vec(V>QY VP ) (15)\nbased on Eq. (12) and Theorem 1. Next, we search for the feasible \u03c1 for (12) and (13) which will lead to the globally optimal h. Let z = vec(V>QY VP ), then plugging (15) into (13) gives us z>(\u03b3\u039bPQ \u2212 \u03c1Inc)\u22122z \u2212 \u03c4 2 = 0. (16) Let us sort the eigenvalues \u03bbP,1\u03bbQ,1, . . . , \u03bbP,c\u03bbQ,n into a non-descending sequence {\u03bbPQ,1, . . . , \u03bbPQ,nc}, rearrange {z1, . . . , znc} accordingly, and find the smallest k0 which satisfies zk0 6= 0. As a result, Eq. (16) implies that\ng(\u03c1) = nc\u2211 k=k0 z2k (\u03b3\u03bbPQ,k \u2212 \u03c1)2 \u2212 \u03c4 2 = 0 (17)\nfor any stationary \u03c1. By Theorem 4.1 of Forsythe and Golub (1965), the smallest root of g(\u03c1) determines a unique h so that (h, \u03c1) is the globally optimal solution to \u03a6(h, \u03c1), i.e., h minimizes the objective of (11) globally. For this \u03c1, the only exception when it cannot determine h by Eq. (14) is that \u03c1/\u03b3 is an eigenvalue of P \u2297 Q, but this happens with probability zero. Finally, the theorem below points out the location of this \u03c1 (the proof is in the appendix):\nAlgorithm 1 MAVR\nInput: P , Q, Y , \u03b3 and \u03c4 Output: H and \u03c1\n1: Eigen-decompose P and Q; 2: Construct the function g(\u03c1); 3: Find the smallest root of g(\u03c1); 4: Recover h using \u03c1 and reshape h to H.\nTheorem 2. The function g(\u03c1) defined in Eq. (17) has exactly one root in the interval [\u03c10, \u03b3\u03bbPQ,k0) and no root in the interval (\u2212\u221e, \u03c10), where \u03c10 = \u03b3\u03bbPQ,k0 \u2212 \u2016y\u20162/\u03c4 .\nThe algorithm of MAVR is summarized in Algorithm 1. It is easy to see that fixing \u03c1 = \u22121 in Algorithm 1 instead of finding the smallest root of g(\u03c1) suffices to solve optimization (10). Moreover, for a special case P = Ic where Ic is the identity matrix of size c, any stationary H is simply\nH = (\u03b3Q\u2212 \u03c1In)\u22121Y = VQ(\u03b3\u039bQ \u2212 \u03c1In)\u22121V>QY.\nLet z = V>QY 1c where 1c is the all-one vector in Rc, and k0 is the smallest number that satisfies zk0 6= 0. Then the smallest root of\ng(\u03c1) = \u2211n\nk=k0 z2k/(\u03b3\u03bbQ,k \u2212 \u03c1)2 \u2212 \u03c4 2\ngives us the feasible \u03c1 leading to the globally optimal H. The asymptotic time complexity of Algorithm 1 is O(n3). More specifically, eigendecomposing Q in the first step of Algorithm 1 costs O(n3), and this is the dominating computation time. Eigen-decomposing P just needs O(c3) and is negligible under the assumption that n c without loss of generality. In the second step, it requires O(nc log(nc)) for sorting the eigenvalues of P \u2297Q and O(n2c) for computing z. Finding the smallest root of g(\u03c1) based on a binary search algorithm uses O(log(\u2016y\u20162)) in the third step, and \u2016y\u20162 \u2264 \u221a l for multi-class problems and \u2016y\u20162 \u2264 \u221a lc for multi-label problems. In the final step, recovering h is essentially same as computing z and costs O(n2c). We would like to comment a bit more on the asymptotic time complexity of MAVR. Firstly, we employ the squared losses over all data rather than the squared losses over labeled data. If the latter loss function was plugged in optimization (8), Eq. (14) would become h = (\u03b3P \u2297Q\u2212 \u03c1Inc + Ic \u2297 J)\u22121y, where J is an n-by-n diagonal matrix such that Ji,i = 1 if xi is labeled and Ji,i = 0 if xi is unlabeled. The inverse in the expression above cannot be computed using the eigendecompositions of P and Q, and hence the computational complexity would increase from O(n3) to O(n3c3). Secondly, given fixed P and Q but different Y , \u03b3, and \u03c4 , the computational complexity is O(n2c) if we reuse the eigen-decompositions of P and Q and the sorted eigenvalues of P \u2297 Q. This property is especially advantageous for validating\nand selecting hyperparameters. It is also quite useful for picking different Xl \u2282 Xn to be labeled following transductive problem settings. Finally, the asymptotic time complexity O(n3) can hardly be improved based on existing techniques for optimizations (9) and (10). Even if \u03c1 is fixed in optimization (10), the stationary condition Eq. (12) is a discrete Sylvester equation which consumes O(n3) for solving it (Sima, 1996)."}, {"heading": "4.3 Theoretical analyses", "text": "We provide two theoretical results. Under certain assumptions, the stability analysis upper bounds the difference of two optimal H and H \u2032 trained with two different label indicator matrices Y and Y \u2032, and the error analysis bounds the difference of H from the ground truth.\nTheorem 2 guarantees that \u03c1 < \u03b3\u03bbPQ,k0 . In fact, with high probability over the choice of Y , it holds that k0 = 1 (we did not meet k0 > 1 in our experiments). For this reason, we make the following assumption:\nFix P and Q, and allow Y to change according to the partition of Xn into different Xl and Xu. There is C\u03b3,\u03c4 > 0, which just depends on \u03b3 and \u03c4 , such that for all optimal \u03c1 trained with different Y , \u03c1 \u2264 \u03b3\u03bbPQ,1 \u2212 C\u03b3,\u03c4 .\nNote that for unconstrained MAVR, there must be C\u03b3,\u03c4 > 1 since \u03b3\u03bbPQ,1 > 0 and \u03c1 = \u22121. Based on the above assumption and the lower bound of \u03c1 in Theorem 2, we can prove the theorem below.\nTheorem 3 (Stability of MAVR). Assume the existence of C\u03b3,\u03c4 . Let (H, \u03c1) and (H \u2032, \u03c1\u2032) be two globally optimal solutions trained with two different label indicator matrices Y and Y \u2032 respectively. Then,\n\u2016H \u2212H \u2032\u2016Fro \u2264 \u2016Y \u2212 Y \u2032\u2016Fro/C\u03b3,\u03c4 + |\u03c1\u2212 \u03c1\u2032|min{\u2016Y \u2016Fro, \u2016Y \u2032\u2016Fro}/C2\u03b3,\u03c4 . (18)\nConsequently, for MAVR in optimization (9) we have\n\u2016H \u2212H \u2032\u2016Fro \u2264 \u2016Y \u2212 Y \u2032\u2016Fro/C\u03b3,\u03c4 + \u2016Y \u2016Fro\u2016Y \u2032\u2016Fro/\u03c4C2\u03b3,\u03c4 ,\nand for unconstrained MAVR in optimization (10) we have\n\u2016H \u2212H \u2032\u2016Fro \u2264 \u2016Y \u2212 Y \u2032\u2016Fro/C\u03b3,\u03c4 .\nIn order to present an error analysis, we assume there is a ground-truth soft response matrix H\u2217 with two properties. Firstly, the value of V (H\u2217) should be bounded, namely,\nV (H\u2217) = tr(H\u2217>QH\u2217P )\n\u2016H\u2217\u20162Fro \u2264 Ch,\nwhere Ch > 0 is a small number. This ensures that H \u2217 lies in a large-volume region. Otherwise MAVR implementing the large volume principle can by no means learn some H close to H\u2217. Secondly, Y should contain certain information about H\u2217. MAVR makes\nuse of P , Q and Y only and the meanings of P and Q are fixed already, so MAVR may access the information about H\u2217 only through Y . To make Y and H\u2217 correlated, we assume that Y = H\u2217 + E where E \u2208 Rn\u00d7c is a noise matrix of the same size as Y and H\u2217. All entries of E are independent with zero mean, and the variance of them is \u03c3l or \u03c3u depending on its correspondence to a labeled or an unlabeled position in Y . We could expect that \u03c3l \u03c3u, such that the entries of Y in labeled positions are close to the corresponding entries of H\u2217, but the entries of Y in unlabeled positions are completely corrupted and uninformative for recovering H\u2217. Notice that we need this generating mechanism of Y even if Ch/\u03b3 is the smallest eigenvalue of P \u2297Q, since P \u2297Q may have multiple smallest eigenvalues and \u00b1H have totally different meanings. Based on these assumptions, we can prove the theorem below.\nTheorem 4 (Accuracy of MAVR). Assume the existence of C\u03b3,\u03c4 , Ch, and the generating process of Y from H\u2217 and E. Let l\u0303 and u\u0303 be the numbers of the labeled and unlabeled positions in Y and assume that EE\u2016Y \u20162Fro \u2264 l\u0303 where the expectation is with respect to the noise matrix E. For each possible Y , let H be the globally optimal solution trained with it. Then,\nEE\u2016H \u2212H\u2217\u2016Fro \u2264 ( \u221a Ch\u03b3\u03bbPQ,1/C\u03b3,\u03c4 )\u2016H\u2217\u2016Fro\n+ ( max {\u221a l\u0303/\u03c4 \u2212 \u03b3\u03bbPQ,1 \u2212 1, \u03b3\u03bbPQ,1 \u2212 C\u03b3,\u03c4 + 1 } /C\u03b3,\u03c4 ) \u2016H\u2217\u2016Fro\n+ \u221a l\u0303\u03c32l + u\u0303\u03c3 2 u/C\u03b3,\u03c4 (19)\nfor MAVR in optimization (9), and\nEE\u2016H \u2212H\u2217\u20162Fro \u2264 (Ch/4)\u2016H\u2217\u20162Fro + l\u0303\u03c32l + u\u0303\u03c32u (20)\nfor unconstrained MAVR in optimization (10).\nThe proofs of Theorems 3 and 4 are in the appendix. Considering the instability bounds in Theorem 3 and the error bounds in Theorem 4, unconstrained MAVR is superior to constrained MAVR in both cases. That being said, bounds are just bounds. We will demonstrate the potential of constrained MAVR in the next section by experiments."}, {"heading": "5 Experiments", "text": "In this section, we numerically evaluate MAVR."}, {"heading": "5.1 Serendipitous learning", "text": "We show how to handle serendipitous problems by MAVR directly without performing clustering (Hartigan and Wong, 1979; Ng et al., 2001; Sugiyama et al., 2014) or estimating the class-prior change (du Plessis and Sugiyama, 2012). The experimental results are\ndisplayed in Figure 2. There are 5 data sets, and the latter 3 data sets are from ZelnikManor and Perona (2004). The matrix Q was specified as the normalized graph Laplacian (see, e.g., von Luxburg, 2007)1 Lnor = In\u2212D\u22121/2WD\u22121/2, where W \u2208 Rn\u00d7n is a similarity matrix and D \u2208 Rn\u00d7n is the degree matrix of W . The matrix P was specified by\nP1 =  1 0 0 0 0 1 0 0 0 0 3 1 0 0 1 1 , P2 =  1 0 0 0 0 3 0 1 0 0 1 0 0 1 0 1 , P3 =  1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 3 ,\nP4 =  1 1/2 1/2 1/2\n1/2 2 0 1/2 1/2 0 2 1/2 1/2 1/2 1/2 3\n, P5 =  1 1/2 1/21/2 1 0\n1/2 0 1\n, P6 = \n1 1/2 1/2 1/2 1/2 1 0 0 1/2 0 1 0 1/2 0 0 1 . For data sets 1 and 2 we used the Gaussian similarity\nWi,j = exp(\u2212\u2016xi \u2212 xj\u201622/(2\u03c32)) 1Though the graph Laplacian matrices have zero eigenvalues, they would not cause algorithmic prob-\nlems when used as Q.\nwith the kernel width \u03c3 = 0.25, and for data sets 3 to 5 we applied the local-scaling similarity (Zelnik-Manor and Perona, 2004)\nWi,j = exp(\u2212\u2016xi \u2212 xj\u201622/(2\u03c3i\u03c3j)), \u03c3i = \u2016xi \u2212 x(k)i \u20162\nwith the number of nearest neighbors k = 7, where x (k) i is the k-th nearest neighbor of\nxi in Xn. We set \u03b3 = 99 and \u03c4 = \u221a l. Furthermore, a class balance regularization was imposed for data sets 2 to 5. The detail is omitted here due to the space limit, while the idea is to encourage balanced total responses of all c classes. For this regularization, the regularization parameter was \u03b3\u2032 = 1. We can see that in Figure 2, MAVR successfully classified the data belonging to the known classes and simultaneously clustered the data belonging to the unknown classes. By specifying different P , we could control the influence of the known classes on the unknown classes."}, {"heading": "5.2 Multi-class learning", "text": "A state-of-the-art multi-class transductive learning method named learning with local and global consistency (LGC) (Zhou et al., 2003) is closely related to MAVR. Actually, if we specify P = Ic and Q = Lnor, unconstrained MAVR will be reduced to LGC exactly. Although LGC is motivated by the label propagation viewpoint, it can be written as optimization (4) in Zhou et al. (2003). Here, we illustrate the nuance of constrained MAVR and LGC that is unconstrained MAVR using an artificial data set.\nThe artificial data set 3circles is generated as follows. We have three classes with the class ratio 1/6, 1/3, 1/2. Let yi be the ground-truth label of xi, then xi is generated by\nxi = (6yi cos(ai) + i,1, 5yi sin(ai) + i,2) > \u2208 R2,\nwhere ai is an angel drawn i.i.d. from the uniform distribution U(0, 2\u03c0), and i,1 and i,2 are noises drawn i.i.d. from the normal distribution N (0, \u03c32 ). We vary one factor and fix all other factors. The default values of these factors are \u03c3 = 0.5, \u03c3 = 0.5, l = 3, n = 300, \u03b3 = 99, and \u03c4 = \u221a l. Figure 3 shows the experimental results, where the means with the standard errors of the classification error rates are plotted. For each task that corresponds to a full specification of all factors, MAVR and LGC were repeatedly ran on 100 random samplings. We can see from Figure 3 that the performance of LGC was usually not as good as MAVR.\nOver the past decades, a huge number of transductive learning and semi-supervised learning methods have been proposed based on various motivations as graph cut (Blum and Chawla, 2001), random walk (Zhu et al., 2003), manifold regularization (Belkin et al., 2006), and information maximization (Niu et al., 2013b), just to name a few. A state-of-the-art semi-supervised learning method called Laplacian regularized least squares (LapRLS) (Belkin et al., 2006) is included to be compared with MAVR besides LGC.\nThe experimental results are reported in Figure 4. Similarly to Figure 3, the means with the standard errors of the classification error rates are shown where 4 methods were repeatedly ran on 100 random samplings for each task. We considered another specification of Q as the unnormalized graph Laplacian Lun = D \u2212 W which was also employed by LapRLS. The cosine similarity is defined by\nWi,j = x > ixj/\u2016xi\u20162\u2016xj\u20162 if xi \u223ck xj, Wi,j = 0 otherwise,\nwhere xi \u223ck xj means xi and xj are among the k-nearest neighbors of each other. We set l = n/10 for all involved n in Figure 4, and there seems no reliable model selection method given very few labeled data, so we select the best hyperparameters for each method using the labels of unlabeled data from 10 additional random samplings. Specifically, \u03c3 is the median distance \u00d7 {1/16, 1/8, 1/4, 1/2, 1}, and k is from {1, 3, 5, 7, 9} for both localscaling and cosine similarities; \u03c4 is \u221a l \u00d7 {1/16, 1/8, 1/4, 1/2, 1}. The hyperparameters are all fixed since it resulted in more stable performance. For MAVR, LGC, and \u03bbI of LapRLS, it was fixed to 99 if the Gaussian and cosine similarities were used and 1 if the local-scaling similarity was used; \u03bbA of LapRLS was 10\n\u22123 if the Gaussian and local-scaling similarities were used and 103 if the cosine similarity was used since LapRLS also needed W that was too sparse and near singular, but an exception was panel (i) where \u03bbA = 10 \u22123 gave lower error rates of LapRLS. We can see from Figure 4 that two MAVR methods often compared favorably with the state-of-the-art methods LGC and LapRLS, which implies that our proposed multi-class volume approximation is reasonable and practical."}, {"heading": "6 Conclusions", "text": "We proposed a multi-class volume approximation that can be applied to several transductive problem settings such as multi-class, multi-label and serendipitous learning. The\nresultant learning method is non-convex in nature but can be solved exactly and efficiently. It is theoretically justified by our stability and error analyses and experimentally demonstrated promising."}, {"heading": "A Proofs", "text": "A.1 Proof of Theorem 2\nThe derivative of g(\u03c1) is\ng\u2032(\u03c1) = nc\u2211 k=k0 2z2k (\u03b3\u03bbPQ,k \u2212 \u03c1)3 \u2212 \u03c4 2.\nHence, g\u2032(\u03c1) > 0 whenever \u03c1 < \u03b3\u03bbPQ,k0 , and g(\u03c1) is strictly increasing in the interval (\u2212\u221e, \u03b3\u03bbPQ,k0). Moreover,\nlim \u03c1\u2192\u2212\u221e g(\u03c1) = \u2212\u03c4 2 and lim \u03c1\u2192\u03b3\u03bbPQ,k0 g(\u03c1) = +\u221e,\nand thus g(\u03c1) has exactly one root in (\u2212\u221e, \u03b3\u03bbPQ,k0). Notice that \u2016z\u20162 = \u2016 vec(V>QY VP )\u20162 = \u2016V>PQy\u20162 = \u2016y\u20162 since VPQ is an orthonormal matrix, and then \u03c10 = \u03b3\u03bbPQ,k0 \u2212 \u2016y\u20162/\u03c4 = \u03b3\u03bbPQ,k0 \u2212 \u2016z\u20162/\u03c4 . As a result,\ng(\u03c10) = nc\u2211 k=k0 z2k (\u03b3\u03bbPQ,k \u2212 \u03c10)2 \u2212 \u03c4 2\n= nc\u2211 k=k0 z2k (\u03b3\u03bbPQ,k \u2212 \u03b3\u03bbPQ,k0 + \u2016z\u20162/\u03c4)2 \u2212 \u03c4 2\n\u2264 nc\u2211 k=k0 z2k (\u2016z\u20162/\u03c4)2 \u2212 \u03c4 2\n=\n(\u2211nc k=k0\nz2k \u2016z\u201622\n\u2212 1 ) \u03c4 2\n\u2264 0,\nwhere the first inequality is because \u03bbPQ,k \u2265 \u03bbPQ,k0 for k \u2265 k0. The fact that g(\u03c10) \u2264 0 concludes that the only root in (\u2212\u221e, \u03b3\u03bbPQ,k0) is in [\u03c10, \u03b3\u03bbPQ,k0) but not (\u2212\u221e, \u03c10).\nA.2 Proof of Theorem 3\nDenote by h = vec(H), y = vec(Y ) and M = (\u03b3P \u2297Q\u2212 \u03c1Inc), and denote by h\u2032, y\u2032 and M \u2032 similarly. Let \u03bbmin(\u00b7) and \u03bbmax(\u00b7) be two functions extracting the smallest and largest eigenvalues of a matrix. Under our assumption,\n\u03bbmin(M) = \u03b3\u03bbPQ,1 \u2212 \u03c1 \u2265 C\u03b3,\u03c4 > 0\nwhich means that M is positive definite, and so is M \u2032. By Eq. (14),\nh\u2212 h\u2032 = M\u22121y \u2212M \u2032\u22121y\u2032 = M\u22121(y \u2212 y\u2032) + (M\u22121 \u2212M \u2032\u22121)y\u2032 = M\u22121(y \u2212 y\u2032) +M\u22121(M \u2032 \u2212M)M \u2032\u22121y\u2032 = M\u22121(y \u2212 y\u2032) + (\u03c1\u2032 \u2212 \u03c1)M\u22121M \u2032\u22121y\u2032.\nNote that \u2016Av\u20162 \u2264 \u03bbmax(A)\u2016v\u20162 for any symmetric positive-definite matrix A and any vector v, as well as \u03bbmax(AB) \u2264 \u03bbmax(A)\u03bbmax(B) for any symmetric positive-definite matrices A and B. Hence,\n\u2016h\u2212 h\u2032\u20162 = \u2016M\u22121(y \u2212 y\u2032) + (\u03c1\u2032 \u2212 \u03c1)M\u22121M \u2032\u22121y\u2032\u20162 \u2264 \u2016M\u22121(y \u2212 y\u2032)\u20162 + |\u03c1\u2212 \u03c1\u2032|\u2016M\u22121M \u2032\u22121y\u2032\u20162 \u2264 \u03bbmax(M\u22121)\u2016y \u2212 y\u2032\u20162 + \u03bbmax(M\u22121)\u03bbmax(M \u2032\u22121)|\u03c1\u2212 \u03c1\u2032|\u2016y\u2032\u20162\n\u2264 \u2016y \u2212 y \u2032\u20162 C\u03b3,\u03c4 + |\u03c1\u2212 \u03c1\u2032|\u2016y\u2032\u20162 C2\u03b3,\u03c4 ,\nwhere the first inequality is the triangle inequality, the second inequality is because M\u22121 and M \u2032\u22121 are symmetric positive definite, and the third inequality follows from \u03bbmax(M \u22121) = 1/\u03bbmin(M) and \u03bbmax(M \u2032\u22121) = 1/\u03bbmin(M\n\u2032). Due to the symmetry of h and h\u2032,\n\u2016h\u2212 h\u2032\u20162 \u2264 \u2016y \u2212 y\u2032\u20162 C\u03b3,\u03c4 + |\u03c1\u2212 \u03c1\u2032|min{\u2016y\u20162, \u2016y\u2032\u20162} C2\u03b3,\u03c4 .\nThis inequality is the vectorization of (18). For MAVR in optimization (9), Theorem 2 together with our assumption indicates that\n\u03b3\u03bbPQ,1 \u2212 \u2016y\u20162/\u03c4 \u2264 \u03c1 < \u03b3\u03bbPQ,1, \u03b3\u03bbPQ,1 \u2212 \u2016y\u2032\u20162/\u03c4 \u2264 \u03c1\u2032 < \u03b3\u03bbPQ,1,\nso |\u03c1\u2032 \u2212 \u03c1| \u2264 max{\u2016y\u20162/\u03c4, \u2016y\u2032\u20162/\u03c4} and\n\u2016h\u2212 h\u2032\u20162 \u2264 \u2016y \u2212 y\u2032\u20162 C\u03b3,\u03c4 + max{\u2016y\u20162, \u2016y\u2032\u20162}min{\u2016y\u20162, \u2016y\u2032\u20162} \u03c4C2\u03b3,\u03c4\n= \u2016y \u2212 y\u2032\u20162 C\u03b3,\u03c4 + \u2016y\u20162\u2016y\u2032\u20162 \u03c4C2\u03b3,\u03c4 .\nFor unconstrained MAVR in optimization (10), we have\n\u2016h\u2212 h\u2032\u20162 \u2264 \u2016y \u2212 y\u2032\u20162 C\u03b3,\u03c4 ,\nsince \u03c1 = \u03c1\u2032 = \u22121.\nA.3 Proof of Theorem 4\nDenote by h = vec(H), y = vec(Y ), h\u2217 = vec(H\u2217), e = vec(E), and M = \u03b3P \u2297 Q. The Kronecker product P \u2297 Q is symmetric and positive definite, and then M1/2 is a well-defined symmetric and positive-definite matrix. We can know based on V (H\u2217) \u2264 Ch that\n\u2016M1/2h\u2217\u20162 = \u221a \u03b3h\u2217 > (P \u2297Q)h\u2217 \u2264 \u221a \u03b3Ch\u2016h\u2217\u201622 = \u221a \u03b3Ch\u2016h\u2217\u20162.\nLet \u03bbmin(\u00b7) and \u03bbmax(\u00b7) be two functions extracting the smallest and largest eigenvalues of a matrix. In the following, we will frequently use that \u2016Av\u20162 \u2264 \u03bbmax(A)\u2016v\u20162 for any symmetric positive-definite matrix A and any vector v.\nConsider unconstrained MAVR in optimization (10) first. Since \u03c1 = \u22121,\nh\u2212 h\u2217 = (M + Inc)\u22121y \u2212 h\u2217 = (M + Inc)\n\u22121(h\u2217 + e)\u2212 (M + Inc)\u22121(M + Inc)h\u2217 = \u2212(M + Inc)\u22121Mh\u2217 + (M + Inc)\u22121e.\nAs a consequence,\nE\u2016h\u2212 h\u2217\u201622 = \u2016(M + Inc)\u22121Mh\u2217\u201622 + E\u2016(M + Inc)\u22121e\u201622,\nsince E[(M + Inc)\u22121e] = (M + Inc)\u22121Ee = 0nc. Subsequently,\n\u2016(M + Inc)\u22121Mh\u2217\u20162 \u2264 \u03bbmax((M + Inc)\u22121M1/2) \u00b7 \u2016M1/2h\u2217\u20162 \u2264 \u03bbmax((\u03b3P \u2297Q+ Inc)\u22121(\u03b3P \u2297Q)1/2) \u00b7 \u221a \u03b3Ch\u2016h\u2217\u20162\n= \u221a \u03b3Ch\u03bbmax\n( \u221a \u03b3\n\u03b3 + 1 (\u039bPQ + Inc)\n\u22121\u039b 1/2 PQ ) \u2016h\u2217\u20162\n\u2264 \u221a Ch\u03bbmax((\u039bPQ + Inc) \u22121\u039b 1/2 PQ)\u2016h\u2217\u20162 \u2264 1 2 \u221a Ch\u2016h\u2217\u20162,\nwhere the last inequality is because the eigenvalues of (\u039bPQ + Inc) \u22121\u039b 1/2 PQ are\u221a\n\u03bbPQ,1\n\u03bbPQ,1+1 , . . . ,\n\u221a \u03bbPQ,nc \u03bbPQ,nc+1 and\nsup\u03bb\u22650\n\u221a \u03bb\n\u03bb+ 1 =\n1 2 .\nOn the other hand,\nE\u2016(M + Inc)\u22121e\u201622 \u2264 (\u03bbmax((M + Inc)\u22121))2 \u00b7 E\u2016e\u201622\n= E[e>e]\n(\u03bbmin(M + Inc))2\n\u2264 l\u0303\u03c32l + u\u0303\u03c32u.\nHence,\nE\u2016h\u2212 h\u2217\u201622 \u2264 1\n4 Ch\u2016h\u2217\u201622 + l\u0303\u03c32l + u\u0303\u03c32u,\nwhich completes the proof of inequality (20). Next, consider MAVR in optimization (9). We would have\nh\u2212 h\u2217 = (M \u2212 \u03c1Inc)\u22121y \u2212 h\u2217 = (M \u2212 \u03c1Inc)\u22121(h\u2217 + e)\u2212 (M \u2212 \u03c1Inc)\u22121(M \u2212 \u03c1Inc)h\u2217 = \u2212(M \u2212 \u03c1Inc)\u22121(M \u2212 (\u03c1+ 1)Inc)h\u2217 + (M \u2212 \u03c1Inc)\u22121e.\nIn general, E[(M\u2212\u03c1Inc)\u22121e] 6= 0nc since \u03c1 depends on e. Furthermore, M\u2212(\u03c1+1)Inc may have negative eigenvalues when \u03b3\u03bbPQ,1 \u2212 1 < \u03c1 \u2264 \u03b3\u03bbPQ,1 \u2212 C\u03b3,\u03c4 . Taking the expectation of \u2016h\u2212 h\u2217\u20162,\nE\u2016h\u2212 h\u2217\u20162 \u2264 E\u2016(M \u2212 \u03c1Inc)\u22121(M \u2212 (\u03c1+ 1)Inc)h\u2217\u20162 + E\u2016(M \u2212 \u03c1Inc)\u22121e\u20162 \u2264 E\u2016(M \u2212 \u03c1Inc)\u22121Mh\u2217\u20162 + E[|\u03c1+ 1|\u2016(M \u2212 \u03c1Inc)\u22121h\u2217\u20162] + E\u2016(M \u2212 \u03c1Inc)\u22121e\u20162.\nSubsequently, E\u2016(M \u2212 \u03c1Inc)\u22121Mh\u2217\u20162 \u2264 sup\u03c1 \u03bbmax((M \u2212 \u03c1Inc)\u22121M1/2) \u00b7 \u221a \u03b3Ch\u2016h\u2217\u20162\n= sup\u03c1 \u221a Ch\u03bbmax ( (\u039bPQ \u2212 \u03c1/\u03b3Inc)\u22121\u039b1/2PQ ) \u2016h\u2217\u20162 \u2264 \u221a Ch\u2016h\u2217\u20162 \u00b7 sup\u03c1\u2264\u03b3\u03bbPQ,1\u2212C\u03b3,\u03c4 sup\u03bb\u2265\u03bbPQ,1 ( \u221a \u03bb\n\u03bb\u2212 \u03c1/\u03b3\n)\n\u2264 \u221a Ch\u03b3\u03bbPQ,1 C\u03b3,\u03c4 \u2016h\u2217\u20162.\nOn the other hand,\nE[|\u03c1+ 1|\u2016(M \u2212 \u03c1Inc)\u22121h\u2217\u20162] \u2264 E|\u03c1+ 1| \u00b7 sup\u03c1 \u03bbmax((M \u2212 \u03c1Inc)\u22121)\u2016h\u2217\u20162\n\u2264 \u2016h \u2217\u20162\nC\u03b3,\u03c4 \u00b7 Emax{\u2212\u03c1\u2212 1, sup\u03c1 \u03c1+ 1}\n\u2264 \u2016h \u2217\u20162\nC\u03b3,\u03c4 \u00b7max{E\u2016y\u20162/\u03c4 \u2212 \u03b3\u03bbPQ,1 \u2212 1, \u03b3\u03bbPQ,1 \u2212 C\u03b3,\u03c4 + 1}\n= \u2016h\u2217\u20162 C\u03b3,\u03c4\n\u00b7max{ \u221a l\u0303/\u03c4 \u2212 \u03b3\u03bbPQ,1 \u2212 1, \u03b3\u03bbPQ,1 \u2212 C\u03b3,\u03c4 + 1}.\nwhere we used the fact that sup\u03c1 \u03c1 is independent of e, and applied Jensen\u2019s inequality to obtain that E\u2016y\u20162 \u2264 \u221a E\u2016y\u201622 \u2264 \u221a l\u0303.\nIn the end,\nE\u2016(M \u2212 \u03c1Inc)\u22121e\u20162 \u2264 sup\u03c1 \u03bbmax((M \u2212 \u03c1Inc)\u22121) \u00b7 E\u2016e\u20162\n\u2264 E \u221a e>e\nC\u03b3,\u03c4 \u2264 \u221a E[e>e] C\u03b3,\u03c4\n=\n\u221a l\u0303\u03c32l + u\u0303\u03c3 2 u\nC\u03b3,\u03c4 ,\nwhere the third inequality is due to Jensen\u2019s inequality. Therefore, inequality (19) follows by combining the three upper bounds of expectations."}], "references": [{"title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In ICML,", "citeRegEx": "Blum and Chawla.,? \\Q2001\\E", "shortCiteRegEx": "Blum and Chawla.", "year": 2001}, {"title": "Semi-supervised learning of class balance under class-prior change by distribution matching", "author": ["M.C. du Plessis", "M. Sugiyama"], "venue": "In ICML,", "citeRegEx": "Plessis and Sugiyama.,? \\Q2012\\E", "shortCiteRegEx": "Plessis and Sugiyama.", "year": 2012}, {"title": "Large margin vs. large volume in transductive learning", "author": ["R. El-Yaniv", "D. Pechyony", "V. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "El.Yaniv et al\\.,? \\Q2008\\E", "shortCiteRegEx": "El.Yaniv et al\\.", "year": 2008}, {"title": "On the stationary values of a second-degree polynomial on the unit sphere", "author": ["G. Forsythe", "G. Golub"], "venue": "Journal of the Society for Industrial and Applied Mathematics,", "citeRegEx": "Forsythe and Golub.,? \\Q1965\\E", "shortCiteRegEx": "Forsythe and Golub.", "year": 1965}, {"title": "A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied Statistics,", "citeRegEx": "Hartigan and Wong.,? \\Q1979\\E", "shortCiteRegEx": "Hartigan and Wong.", "year": 1979}, {"title": "Transductive multi-label learning via label set propagation", "author": ["X. Kong", "M. Ng", "Z.-H. Zhou"], "venue": "IEEE Transaction on Knowledge and Data Engineering,", "citeRegEx": "Kong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2013}, {"title": "Matrix Analysis for Scientists and Engineers", "author": ["A.J. Laub"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Laub.,? \\Q2005\\E", "shortCiteRegEx": "Laub.", "year": 2005}, {"title": "Maximum volume outlier detection and its applications in credit risk analysis", "author": ["S. Li", "W. Ng"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "Li and Ng.,? \\Q2013\\E", "shortCiteRegEx": "Li and Ng.", "year": 2013}, {"title": "Squared-loss mutual", "author": ["2013a. G. Niu", "W. Jitkrittum", "B. Dai", "H. Hachiya", "M. Sugiyama"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2013}, {"title": "Algorithms for Linear-Quadratic Optimization", "author": ["V. Sima"], "venue": null, "citeRegEx": "Sima.,? \\Q1996\\E", "shortCiteRegEx": "Sima.", "year": 1996}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing,", "citeRegEx": "1982", "shortCiteRegEx": "1982", "year": 1998}, {"title": "Self-tuning spectral clustering", "author": ["Zelnik-Manor", "P. Perona"], "venue": "In NIPS,", "citeRegEx": "Zelnik.Manor and Perona.,? \\Q2007\\E", "shortCiteRegEx": "Zelnik.Manor and Perona.", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "But it did not gain much attention until a creative approximation was proposed in El-Yaniv et al. (2008) to implement LVP for the case of soft response vectors.", "startOffset": 82, "endOffset": 105}, {"referenceID": 3, "context": "learning (El-Yaniv et al., 2008), binary clustering (Niu et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 8, "context": ", 2013a), and outlier detection (Li and Ng, 2013).", "startOffset": 32, "endOffset": 49}, {"referenceID": 3, "context": "learning (El-Yaniv et al., 2008), binary clustering (Niu et al., 2013a), and outlier detection (Li and Ng, 2013). LVP is a learning-theoretic principle which views learning as hypothesis selecting from a certain hypothesis space H. Despite the form of the hypothesis, H can always be partitioned into a finite number of equivalence classes after we observe certain data, where an equivalence class is a set of hypotheses that generate the same labeling of the observed data. LVP, as one of the learning-theoretic principles from the statistical learning theory, prioritizes those equivalence classes according to the volume they occupy in H. See the illustration in Figure 1: The blue ellipse represents H, and it is partitioned into C1, . . . , C4 each occupying a quadrant of the Cartesian coordinate system R intersected with H; LVP claims that C1 and C3 are more preferable than C2 and C4, since C1 and C3 have larger volume than C2 and C4. In practice, the hypothesis space H cannot be as simple as in Figure 1. It frequently locates in very high-dimensional spaces where exact or even quantifiable volume estimation is challenging. Therefore, El-Yaniv et al. (2008) proposed a volume approximation to bypass the volume estimation.", "startOffset": 10, "endOffset": 1172}, {"referenceID": 6, "context": ", 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages.", "startOffset": 42, "endOffset": 65}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn\u00d7c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly.", "startOffset": 42, "endOffset": 486}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn\u00d7c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed. We name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al.", "startOffset": 42, "endOffset": 1373}, {"referenceID": 2, "context": "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn\u00d7c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed. We name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al. (2003) and Belkin et al.", "startOffset": 42, "endOffset": 1539}, {"referenceID": 0, "context": "(2003) and Belkin et al. (2006) using USPS, MNIST, 20Newsgroups and Isolet.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "The binary volume approximation in El-Yaniv et al. (2008) involves a few key concepts: The soft response vector, the hypothesis space and the equivalence class, and the power and volume of equivalence classes.", "startOffset": 35, "endOffset": 58}, {"referenceID": 3, "context": "However, it is very hard to accurately compute the geometric volume of even a single convex body in R, let alone all 2 convex bodies, so El-Yaniv et al. (2008) introduced an efficient approximation.", "startOffset": 137, "endOffset": 160}, {"referenceID": 6, "context": "A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors.", "startOffset": 109, "endOffset": 128}, {"referenceID": 6, "context": "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet.", "startOffset": 74, "endOffset": 93}, {"referenceID": 6, "context": "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors. A more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi \u2208 Xl} and Yu = {yi | xi \u2208 Xu, yi 6\u2208 Yl}, then we have #Yu \u2265 1 where # measures the cardinality.", "startOffset": 74, "endOffset": 655}, {"referenceID": 6, "context": "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors. A more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi \u2208 Xl} and Yu = {yi | xi \u2208 Xu, yi 6\u2208 Yl}, then we have #Yu \u2265 1 where # measures the cardinality. It is still solvable when #Yu = 1 if a special label of outliers is allowed and when #Yu > 1 as a combination of classification and clustering problems. Zhang et al. (2011) is the unique previous work which successfully dealt with #Yu = 2 and #Yu = 3.", "startOffset": 74, "endOffset": 943}, {"referenceID": 6, "context": "For multi-label problems, we need a threshold Th that is either preset or learned since usually positive and negative labels are imbalanced, and yi can be predicted by \u0177i = {j | hi,j \u2265 Th}; or we can use the label set prediction methods proposed in Kong et al. (2013). Then, a soft response matrix as our transductive hypothesis is an n-by-c matrix defined by H = (h1, .", "startOffset": 249, "endOffset": 268}, {"referenceID": 3, "context": "The denominator \u2016H\u2016Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 3, "context": "The denominator \u2016H\u2016Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al. (2013a).", "startOffset": 89, "endOffset": 135}, {"referenceID": 3, "context": ", 2003) and the binary counterparts of the fourth and third loss functions have been used for binary transductive learning (El-Yaniv et al., 2008) and clustering (Niu et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 3, "context": "Although the optimization in (8) is done in Rn\u00d7c, the regularization is carried out relative to HP,Q, since under the constraint \u2016H\u2016Fro = \u03c4 , the regularization tr(H>QHP ) is a weighted sum of the squares of cosines between vec(H) and the principal axes of EP,Q like El-Yaniv et al. (2008). Subsequently, we denote by y1, .", "startOffset": 267, "endOffset": 290}, {"referenceID": 4, "context": "In this representation, the objective is a second-degree polynomial and the constraint is an origin-centered sphere, and fortunately we could solve it exactly and efficiently following Forsythe and Golub (1965). To this end, a fundamental property of the Kronecker product is necessary (see, e.", "startOffset": 185, "endOffset": 211}, {"referenceID": 4, "context": "1 of Forsythe and Golub (1965), the smallest root of g(\u03c1) determines a unique h so that (h, \u03c1) is the globally optimal solution to \u03a6(h, \u03c1), i.", "startOffset": 5, "endOffset": 31}, {"referenceID": 10, "context": "(12) is a discrete Sylvester equation which consumes O(n) for solving it (Sima, 1996).", "startOffset": 73, "endOffset": 85}, {"referenceID": 5, "context": "1 Serendipitous learning We show how to handle serendipitous problems by MAVR directly without performing clustering (Hartigan and Wong, 1979; Ng et al., 2001; Sugiyama et al., 2014) or estimating the class-prior change (du Plessis and Sugiyama, 2012).", "startOffset": 117, "endOffset": 182}, {"referenceID": 1, "context": "Over the past decades, a huge number of transductive learning and semi-supervised learning methods have been proposed based on various motivations as graph cut (Blum and Chawla, 2001), random walk (Zhu et al.", "startOffset": 160, "endOffset": 183}, {"referenceID": 0, "context": ", 2003), manifold regularization (Belkin et al., 2006), and information maximization (Niu et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 0, "context": "A state-of-the-art semi-supervised learning method called Laplacian regularized least squares (LapRLS) (Belkin et al., 2006) is included to be compared with MAVR besides LGC.", "startOffset": 103, "endOffset": 124}], "year": 2014, "abstractText": "Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we extend it naturally to a more general definition which can be applied to several transductive problem settings, such as multi-class, multi-label and serendipitous learning. Even though the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained in O(n3) time. We theoretically provide stability and error analyses for the proposed method, and then experimentally show that it is promising.", "creator": "LaTeX with hyperref package"}}}