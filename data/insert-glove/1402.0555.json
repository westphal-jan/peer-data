{"id": "1402.0555", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits", "abstract": "zandberg We 25.76 present desaturated a blagoi new algorithm for formula the contextual bandit ashburner learning problem, hemangiomas where the aubers learner poner repeatedly oly-2004-basket takes smallpox an sosso action unshorn in response to the observed context, indication observing turkmenchay the reward 11-player only for that 8-a action. Our method assumes access to pook an kerstetter oracle for 67.20 solving abhiradee cost - sensitive emule classification sugarlands problems guncotton and 2,781 achieves theban the hajat statistically optimal regret 24-yard guarantee orbis with morishige only $ \\ upcher tilde {nomadic O} (\\ thrasymachus sqrt {bandhu T} ) $ comercial oracle araneidae calls fedora across all $ T $ skedaddle rounds. 17.63 By doing so, we naraka obtain the rathfriland most practical contextual bandit learning algorithm computers amongst moisiu approaches euryanthe that http://www.nyse.com work rapprochment for general 1,500-mile policy 55.80 classes. przytyk We ventoso further conduct a proof - of - gruidae concept curitiba experiment stouter which suburgatory demonstrates batya the self-hating excellent laboratorium computational and morphism prediction fayal performance sambora of (hnorr an 4,806 online variant of) graveson our ninoslav algorithm relative spanish-english to .445 several shiu baselines.", "histories": [["v1", "Tue, 4 Feb 2014 00:48:29 GMT  (37kb)", "http://arxiv.org/abs/1402.0555v1", null], ["v2", "Tue, 14 Oct 2014 01:41:47 GMT  (39kb)", "http://arxiv.org/abs/1402.0555v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alekh agarwal", "daniel j hsu", "satyen kale", "john langford", "lihong li", "robert e schapire"], "accepted": true, "id": "1402.0555"}, "pdf": {"name": "1402.0555.pdf", "metadata": {"source": "CRF", "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits", "authors": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n05 55\nv1 [\ncs .L\nG ]\n\u221a\nT ) oracle calls across all T rounds. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines."}, {"heading": "1 Introduction", "text": "In the contextual bandit problem, an agent collects rewards for actions taken over a sequence of rounds; in each round, the agent chooses an action to take on the basis of (i) context (or features) for the current round, as well as (ii) feedback, in the form of rewards, obtained in previous rounds. The feedback is incomplete: in any given round, the agent observes the reward only for the chosen action; the agent does not observe the reward for other actions. Contextual bandit problems are found in many important applications such as online recommendation and clinical trials, and represent a natural half-way point between supervised learning and reinforcement learning. The use of features to encode context is inherited from supervised machine learning, while exploration is necessary for good performance as in reinforcement learning.\nThe choice of exploration distribution on actions is important. The strongest known results (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) provide an algorithm that carefully controls the exploration distribution to achieve an optimal regret after T rounds of\nO (\u221a KT log |\u03a0| )\nrelative to a set of policies \u03a0 \u2286 XA mapping contexts x \u2208 X to actions a \u2208 A (where K is the number of actions). The regret is the difference between the cumulative reward of the best policy in \u03a0 and the cumulative reward collected by the algorithm. Because the bound has a mild logarithmic dependence on |\u03a0|, the algorithm can compete with very large policy classes that are likely to yield high rewards, in which case the algorithm also earns high rewards. However, the computational complexity of the algorithm is linear in |\u03a0|, making it tractable for only simple policy classes.\nA sub-linear in |\u03a0| running time is possible for policy classes that can be efficiently searched. In this work, we use the abstraction of an optimization oracle to capture this property: given a set of context/reward vector pairs, the oracle returns a policy in \u03a0 with maximum total reward. Using such an oracle in an i.i.d. setting (formally defined in Section 2.1), it is possible to create \u01eb-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only a single call to the oracle per round. However, these algorithms have suboptimal regret bounds of O((K log |\u03a0|)1/3T 2/3) because the algorithms randomize uniformly over actions when they choose to explore.\nThe RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) achieves the optimal regret bound (up to logarithmic factors) in the i.i.d. setting, and runs in time poly(T, log |\u03a0|) with poly(T ) calls to the optimization oracle. This is a fascinating result because it shows that the oracle can provide an exponential speed-up over previous algorithms with optimal regret bounds. However, the algorithm is not practical because the degree of the running time polynomial is high.\nIn this work, we prove the following1:\nTheorem 1. There is an algorithm for the i.i.d. contextual bandit problem with an optimal regret bound requiring O\u0303( \u221a KT ) calls to the optimization oracle over T rounds.\nConcretely, we make O\u0303( \u221a KT ) calls to the oracle with a net running time of O\u0303(T 1.5K1/2 log |\u03a0|), vastly improving over the complexity of RandomizedUCB. The major components of the new algorithm are (i) a new coordinate descent procedure for computing a very sparse distribution over policies which can be efficiently sampled from, and (ii) a new epoch structure which allows the distribution over policies to be updated very infrequently. We consider variants of the epoch structure that make different computational trade-offs; on one extreme we concentrate the entire computational burden on O(log T ) rounds with O\u0303( \u221a KT ) oracle calls each time, while on the other we spread our computation over \u221a T rounds with O\u0303(K) oracle calls for each of these rounds. We stress that in either case, the total number of calls to the oracle is only sublinear. Finally, we develop a more efficient online variant, and conduct a proof-ofconcept experiment showing low computational complexity and high reward relative to several natural baselines.\nMotivation and related work. The EXP4-family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) solve the contextual bandit problem with optimal regret by updating weights (multiplicatively) over all policies in every round. Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies.\nIn contrast, the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) is based on a natural abstraction from supervised learning\u2014the ability to efficiently find a function in a rich function class that minimizes the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007). However, these algorithms have only suboptimal regret bounds.\nAnother class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-confidence bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated. In our experiments, we compare to a strong baseline algorithm that uses this approach (Chu et al., 2011).\nTo circumvent the \u2126(|\u03a0|) running time barrier, we restrict attention to algorithms that only access the policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key challenge is to design good supervised learning problems for querying this oracle. The RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves a certain convex program. However, the number of oracle calls in their work is prohibitively large, and the statistical analysis is also rather complex.2\n1Throughout this paper, we use the O\u0303 notation to suppress dependence on logarithmic factors. 2The paper of Dud\u0301\u0131k et al. (2011a) is colloquially referred to, by its authors, as the \u201cmonster paper\u201d (Langford, 2014).\nMain contributions. In this work, we present a new and simple algorithm for solving a similar convex program as that used by RandomizedUCB. The new algorithm is based on coordinate descent: in each iteration, the algorithm calls the optimization oracle to obtain a policy; the output is a sparse distribution over these policies. The number of iterations required to compute the distribution is small\u2014at most O\u0303( \u221a Kt) in any round t. In fact, we present a more general scheme based on epochs and warm start\nin which the total number of calls to the oracle is, with high probability, just O\u0303( \u221a KT ) over all T rounds ; we prove that this is nearly optimal for a certain class of optimization-based algorithms. The algorithm is natural and simple to implement, and we provide an arguably simpler analysis than that for RandomizedUCB. Finally, we report proof-of-concept experimental results using a variant algorithm showing strong empirical performance."}, {"heading": "2 Preliminaries", "text": "In this section, we recall the i.i.d. contextual bandit setting and some basic techniques used in previous works (Auer et al., 2002; Beygelzimer et al., 2011; Dud\u0301\u0131k et al., 2011a)."}, {"heading": "2.1 Learning Setting", "text": "Let A be a finite set of K actions, X be a space of possible contexts (e.g., a feature space), and \u03a0 \u2286 AX be a finite set of policies that map contexts x \u2208 X to actions a \u2208 A.3 Let \u2206\u03a0 := {Q \u2208 R\u03a0 : Q(\u03c0) \u2265 0 \u2200\u03c0 \u2208 \u03a0, \u2211\u03c0\u2208\u03a0 Q(\u03c0) \u2264 1} be the set of non-negative weights over policies with total weight at most one, and let RA+ := {r \u2208 RA : r(a) \u2265 0 \u2200a \u2208 A} be the set of non-negative reward vectors.\nLet D be a probability distribution over X\u00d7 [0, 1]A, the joint space of contexts and reward vectors; we assume actions\u2019 rewards from D are always in the interval [0, 1]. Let DX denote the marginal distribution of D over X .\nIn the i.i.d. contextual bandit setting, the context/reward vector pairs (xt, rt) \u2208 X \u00d7 [0, 1]A over all rounds t = 1, 2, . . . are randomly drawn independently from D. In round t, the agent first observes the context xt, then (randomly) chooses an action at \u2208 A, and finally receives the reward rt(at) \u2208 [0, 1] for the chosen action. The (observable) record of interaction resulting from round t is the quadruple (xt, at, rt(at), pt(at)) \u2208 X \u00d7 A\u00d7 [0, 1]\u00d7 [0, 1]; here, pt(at) \u2208 [0, 1] is the probability that the agent chose action at \u2208 A. We let Ht \u2286 X \u00d7 A \u00d7 [0, 1]\u00d7 [0, 1] denote the history (set) of interaction records in the first t rounds. We use the shorthand notation E\u0302x\u223cHt [\u00b7] to denote expectation when a context x is chosen from the t contexts in Ht uniformly at random.\nLet R(\u03c0) := E(x,r)\u223cD[r(\u03c0(x))] denote the expected (instantaneous) reward of a policy \u03c0 \u2208 \u03a0, and let \u03c0\u22c6 := argmax\u03c0\u2208\u03a0R(\u03c0) be a policy that maximizes the expected reward (the optimal policy). Let Reg(\u03c0) := R(\u03c0\u22c6) \u2212 R(\u03c0) denote the expected (instantaneous) regret of a policy \u03c0 \u2208 \u03a0 relative to the optimal policy. Finally, the (empirical cumulative) regret of the agent after T rounds4 is defined as\nT\u2211\nt=1\n( rt(\u03c0\u22c6(xt))\u2212 rt(at) ) ."}, {"heading": "2.2 Inverse Propensity Scoring", "text": "An unbiased estimate of a policy\u2019s reward may be obtained from a history of interaction records Ht using inverse propensity scoring (IPS; also called inverse probability weighting): the expected reward of policy \u03c0 \u2208 \u03a0 is estimated as\nR\u0302t(\u03c0) := 1\nt\nt\u2211\ni=1\nri(ai) \u00b7 1{\u03c0(xi) = ai} pi(ai) . (1)\n3Extension to VC classes is simple using standard arguments. 4We have defined empirical cumulative regret as being relative to \u03c0\u22c6, rather than to the empirical reward maximizer\nargmax\u03c0\u2208\u03a0 \u2211 T t=1 rt(\u03c0(xt)). However, in the i.i.d. setting, the two do not differ by more than O(\n\u221a\nT ln(N/\u03b4)) with probability at least 1\u2212 \u03b4.\nThis technique can be viewed as mapping Ht 7\u2192 IPS(Ht) of interaction records (x, a, r(a), p(a)) to context/reward vector pairs (x, r\u0302), where r\u0302 \u2208 RA+ is a fictitious reward vector that assigns to the chosen action a a scaled reward r(a)/p(a) (possibly greater than one), and assigns to all other actions zero rewards. This transformation IPS(Ht) is detailed in Algorithm 3 (in Appendix A); we may equivalently define R\u0302t by R\u0302t(\u03c0) := t\u22121 \u2211\n(x,r\u0302)\u2208IPS(Ht) r\u0302(\u03c0(x)). It is easy to verify that E[r\u0302(\u03c0(x))|(x, r)] = r(\u03c0(x)), as\np(a) is indeed the agent\u2019s probability (conditioned on (x, r)) of picking action a. This implies R\u0302t(\u03c0) is an unbiased estimator for any history Ht.\nLet \u03c0t := argmax\u03c0\u2208\u03a0 R\u0302t(\u03c0) denote a policy that maximizes the expected reward estimate based on inverse propensity scoring with history Ht (\u03c00 can be arbitrary), and let R\u0302egt(\u03c0) := R\u0302t(\u03c0t)\u2212 R\u0302t(\u03c0) denote estimated regret relative to \u03c0t. Note that R\u0302egt(\u03c0) is generally not an unbiased estimate of Reg(\u03c0), because \u03c0t is not always \u03c0\u22c6."}, {"heading": "2.3 Optimization Oracle", "text": "One natural mode for accessing the set of policies \u03a0 is enumeration, but this is impractical in general. In this work, we instead only acceess \u03a0 via an optimization oracle which corresponds to a cost-sensitive learner. Following (Dud\u0301\u0131k et al., 2011a), we call this oracle AMO5.\nDefinition 1. For a set of policies \u03a0, the argmax oracle (AMO) is an algorithm, which for any sequence of context and reward vectors, (x1, r1), (x2, r2), . . . , (xt, rt) \u2208 X \u00d7 RA+, returns\nargmax \u03c0\u2208\u03a0\nt\u2211\n\u03c4=1\nr\u03c4 (\u03c0(x\u03c4 ))."}, {"heading": "2.4 Projections and Smoothing", "text": "In each round, our algorithm chooses an action by randomly drawing a policy \u03c0 from a distribution over \u03a0, and then picking the action \u03c0(x) recommended by \u03c0 on the current context x. This is equivalent to drawing an action according to Q(a|x) := \u2211\u03c0\u2208\u03a0:\u03c0(x)=aQ(\u03c0), \u2200a \u2208 A . For keeping the variance of reward estimates from IPS in check, it is desirable to prevent the probability of any action from being too small. Thus, as in previous work, we also use a smoothed projection Q\u00b5(\u00b7|x) for \u00b5 \u2208 [0, 1/K], Q\u00b5(a|x) := (1 \u2212 K\u00b5)\u2211\u03c0\u2208\u03a0:\u03c0(x)=aQ(\u03c0) + \u00b5, \u2200a \u2208 A. Every action has probability at least \u00b5 under Q\u00b5(\u00b7|x).\nFor technical reasons, our algorithm maintains non-negative weights Q \u2208 \u2206\u03a0 over policies that sum to at most one, but not necessarily equal to one; hence, we put any remaining mass on a default policy \u03c0\u0304 \u2208 \u03a0 to obtain a legitimate probability distribution over policies Q\u0303 = Q + ( 1\u2212\u2211\u03c0\u2208\u03a0 Q(\u03c0) ) 1\u03c0\u0304. We then pick an action from the smoothed projection Q\u0303\u00b5(\u00b7|x) of Q\u0303 as above. This sampling procedure Sample(x,Q, \u03c0\u0304, \u00b5) is detailed in Algorithm 4 (in Appendix A)."}, {"heading": "3 Algorithm and Main Results", "text": "Our algorithm (ILOVETOCONBANDITS) is an epoch-based variant of the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) and is given in Algorithm 1. Like RandomizedUCB, ILOVETOCONBANDITS also solves an optimization problem (OP) to obtain a distribution over policies to sample from (Step 7), but does so on an epoch schedule, i.e., only on certain pre-specified rounds \u03c41, \u03c42, . . .. The only requirement of the epoch schedule is that the length of epoch m is bounded as \u03c4m+1 \u2212 \u03c4m = O(\u03c4m). For simplicity, we assume \u03c4m+1 \u2264 2\u03c4m for m \u2265 1, and \u03c41 = O(1).\nThe crucial step here is solving (OP). This problem is very similar to the one in (Dud\u0301\u0131k et al., 2011a), and our coordinate descent algorithm in Section 3.1 gives a constructive proof that the problem is feasible. As in (Dud\u0301\u0131k et al., 2011a), we have the following regret bound:\n5Cost-sensitive learners often need a cost instead of reward, in which case we use ct = 1\u2212 rt.\nTheorem 2. Suppose it is possible to solve the optimization problem (OP). With probability at least 1\u2212 \u03b4, the regret of ILOVETOCONBANDITS after T rounds is\nO (\u221a KT ln(T |\u03a0|/\u03b4) +K ln(T |\u03a0|/\u03b4) ) .\nAlgorithm 1 Importance-weighted LOw-Variance Epoch-Timed Oracleized CONtextual BANDITS algorithm (ILOVETOCONBANDITS) input Epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 , allowed failure probability \u03b4 \u2208 (0, 1). 1: Initial weights Q0 := 0 \u2208 \u2206\u03a0, initial epoch m := 1.\nDefine \u00b5m := min{1/2K, \u221a ln(16\u03c42m|\u03a0|/\u03b4)/(K\u03c4m)} for all m \u2265 0.\n2: for round t = 1, 2, . . . do 3: Observe context xt \u2208 X . 4: (at, pt(at)) := Sample(xt, Qm\u22121, \u03c0\u03c4m\u22121, \u00b5m\u22121). 5: Select action at and observe reward rt(at) \u2208 [0, 1]. 6: if t = \u03c4m then 7: Let Qm be the solution to (OP) with history Ht and minimum probability \u00b5m. 8: m := m+ 1. 9: end if\n10: end for\nOptimization Problem (OP)\nGiven a historyHt and minimum probability \u00b5m, define b\u03c0 := R\u0302egt(\u03c0) \u03c8\u00b5m for \u03c8 := 100, and find Q \u2208 \u2206\u03a0 such that\n\u2211\n\u03c0\u2208\u03a0\nQ(\u03c0)b\u03c0 \u2264 2K (2)\n\u2200\u03c0 \u2208 \u03a0 : E\u0302x\u223cHt [\n1\nQ\u00b5m(\u03c0(x)|x)\n] \u2264 2K + b\u03c0. (3)"}, {"heading": "3.1 Solving (OP) via Coordinate Descent", "text": "We now present a coordinate descent algorithm to solve (OP). The pseudocode is given in Algorithm 2. Our analysis, as well as the algorithm itself, are based on a potential function which we use to measure progress. The algorithm can be viewed as a form of coordinate descent applied to this same potential function. The main idea of our analysis is to show that this function decreases substantially on every iteration of this algorithm; since the function is nonnegative, this gives an upper bound on the total number of iterations as expressed in the following theorem.\nTheorem 3. Algorithm 2 (with Qinit := 0) halts in \u2264 4 ln(1/(K\u00b5))\u00b5 iterations, and outputs a solution Q to (OP)."}, {"heading": "3.2 Using an Optimization Oracle", "text": "We now show how to implement Algorithm 2 via AMO (c.f. Section 2.3).\nLemma 1. Algorithm 2 can be implemented using one call to AMO before the loop is started, and one call for each iteration of the loop thereafter.\nAlgorithm 2 Coordinate Descent Algorithm Require: History Ht, minimum probability \u00b5, initial weights Qinit \u2208 \u2206\u03a0. 1: Set Q := Qinit. 2: loop\n3: Define, for all \u03c0 \u2208 \u03a0,\nV\u03c0(Q) = E\u0302x\u223cHt [1/Q \u00b5(\u03c0(x)|x)] S\u03c0(Q) = E\u0302x\u223cHt [ 1/(Q\u00b5(\u03c0(x)|x))2 ]\nD\u03c0(Q) = V\u03c0(Q)\u2212 (2K + b\u03c0).\n4: if \u2211\n\u03c0 Q(\u03c0)(2K + b\u03c0) > 2K then 5: Replace Q by cQ, where\nc := 2K\u2211\n\u03c0 Q(\u03c0)(2K + b\u03c0) < 1. (4)\n6: end if 7: if there is a policy \u03c0 for which D\u03c0(Q) > 0 then 8: Add the (positive) quantity\n\u03b1\u03c0(Q) = V\u03c0(Q) +D\u03c0(Q)\n2(1\u2212K\u00b5)S\u03c0(Q) to Q(\u03c0) and leave all other weights unchanged.\n9: else\n10: Halt and output the current set of weights Q. 11: end if 12: end loop\nProof. At the very beginning, before the loop is started, we compute the best empirical policy so far, \u03c0t, by calling AMO on the sequence of historical contexts and estimated reward vectors; i.e., on (x\u03c4 , r\u0302\u03c4 ), for \u03c4 = 1, 2, . . . , t.\nNext, we show that each iteration in the loop of Algorithm 2 can be implemented via one call to AMO. Going over the pseudocode, first note that operations involving Q in step Step 4 can be performed efficiently since Q has sparse support. Note that the definitions in step Step 3 don\u2019t actually need to be computed for all policies \u03c0 \u2208 \u03a0, as long as we can identify a policy \u03c0 for which D\u03c0(Q) > 0. We can identify such a policy using one call to AMO as follows.\nFirst, note that for any policy \u03c0, we have\nV\u03c0(Q) = E\u0302x\u223cHt\n[ 1\nQ\u00b5(\u03c0(x)|x)\n] = 1\nt\nt\u2211\n\u03c4=1\n1\nQ\u00b5(\u03c0(x\u03c4 )|x\u03c4 ) ,\nand\nb\u03c0 = R\u0302egt(\u03c0)\n\u03c8\u00b5 = R\u0302t(\u03c0t) \u03c8\u00b5 \u2212 1 \u03c8\u00b5t\nt\u2211\n\u03c4=1\nr\u0302\u03c4 (\u03c0(x\u03c4 )).\nNow consider the sequence of historical contexts and reward vectors, (x\u03c4 , r\u0303\u03c4 ) for \u03c4 = 1, 2, . . . , t, where for any action a we define\nr\u0303\u03c4 (a) := 1\nt\n( \u03c8\u00b5\nQ\u00b5(a|x\u03c4 ) + r\u0302t(a)\n) . (5)\nIt is easy to check that\nD\u03c0(Q) = 1\n\u03c8\u00b5\nt\u2211\n\u03c4=1\nr\u0303\u03c4 (\u03c0(x\u03c4 ))\u2212 ( 2K +\nR\u0302t(\u03c0t) \u03c8\u00b5\n) .\nSince 2K + R\u0302t(\u03c0t)\u03c8\u00b5 is a constant independent of \u03c0, we have\nargmax \u03c0\u2208\u03a0 D\u03c0(Q) = argmax \u03c0\u2208\u03a0\nt\u2211\n\u03c4=1\nr\u0303\u03c4 (\u03c0(x\u03c4 )),\nand hence, calling AMO once on the sequence (x\u03c4 , r\u0303\u03c4 ) for \u03c4 = 1, 2, . . . , t, we obtain a policy that maximizes D\u03c0(Q) and thereby identify a policy for which D\u03c0(Q) > 0, if one exists."}, {"heading": "3.3 Epoch Schedule", "text": "Theorem 3 shows that Algorithm 2 solves (OP) with O\u0303( \u221a t) calls to AMO in round t. Thus, if we use the epoch schedule \u03c4m = m (i.e., run Algorithm 2 in every round), then we get a total of O\u0303(T 3/2) calls to AMO over all T rounds. This number can be dramatically reduced using a more carefully chosen epoch schedule.\nLemma 2. For the epoch schedule \u03c4m := 2 m\u22121, the total number of calls to AMO is O\u0303( \u221a KT ).\nProof. The epoch schedule satisfies the requirement \u03c4m+1 \u2264 2\u03c4m. With this epoch schedule, Algorithm 2 is run only O(log T ) times over T rounds, leading to O\u0303( \u221a KT ) total calls to AMO over the entire period."}, {"heading": "3.4 Warm Start", "text": "We now present a different technique to reduce the number of calls to AMO. This is based on the observation that practically speaking, it seems terribly wasteful, at the start of a new epoch, to throw out the results of all of the preceding computations and to begin yet again from nothing. Instead, intuitively, we expect computations to be more moderate if we begin again where we left off last, i.e., a \u201cwarm-start\u201d approach. Here, when Algorithm 2 is called at the end of epoch m, we use Qinit := Qm\u22121 (the previously computed weights) rather than 0.\nWe can combine warm-start with a different epoch schedule to guarantee O\u0303( \u221a KT ) total calls to AMO,\nspread across O( \u221a T ) calls to Algorithm 2.\nLemma 3. Define the epoch schedule (\u03c41, \u03c42) := (3, 5) and \u03c4m := m 2 for m \u2265 3 (this satisfies \u03c4m+1 \u2264\n2\u03c4m). With high probability, the warm-start variant of Algorithm 1 makes O\u0303( \u221a KT ) calls to AMO over\nT rounds and O( \u221a T ) calls to Algorithm 2."}, {"heading": "3.5 A Lower Bound on the Support Size", "text": "So far we have seen various ways to solve the optimization problem (OP), with corresponding bounds on the number of calls to AMO. An attractive feature of the coordinate descent algorithm, Algorithm 2, is that the number of oracle calls is directly related to the number of policies in the support of Qm. Specifically, for the doubling schedule of Section 3.3, Theorem 3 implies that we never have non-zero weights for more than 4 ln(1/(K\u00b5m))\u00b5m policies in epoch m. Similarly, the total number of oracle calls for the warm-start approach in Section 3.4 bounds the total number of policies which ever have non-zero weight over all T rounds. The support size of the distributions Qm in Algorithm 1 is crucial to the computational complexity of sampling an action (Step 4 of Algorithm 1).\nIn this section, we demonstrate a lower bound showing that it is not possible to construct substantially sparser distributions that also satisfy the low-variance constraint (3) in the optimization problem (OP). To formally define the lower bound, fix an epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 and consider the following set of non-negative vectors over policies:\nQm :={Q \u2208 \u2206\u03a0 : Q satisfies Eq. (3) in round \u03c4m}. (The distribution Qm computed by Algorithm 1 is in Qm.) Let us also define supp(Q) to be the set of policies where Q puts non-zero entries, with |supp(Q)| denoting the cardinality of this set. Then we have the following lower bound.\nTheorem 4. For any epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 and any M \u2208 N sufficiently large, there exists a distribution D over X \u00d7 [0, 1]A and a policy class \u03a0 such that, with probability at least 1\u2212 \u03b4,\ninf m\u2208N:\n\u03c4m\u2265\u03c4M/2\ninf Q\u2208Qm\n|supp(Q)| = \u2126 (\u221a\nK\u03c4M ln(|\u03a0|\u03c4M/\u03b4)\n) .\nThe proof of the theorem is deferred to Appendix E. In the context of our problem, this lower bound shows that the bounds in Lemmas 2 and 3 are unimprovable, since the number of calls to AMO is at least the size of the support, given our mode of access to \u03a0."}, {"heading": "4 Regret Analysis", "text": "In this section, we outline the regret analysis for our algorithm ILOVETOCONBANDITS, with details deferred to Appendix B and Appendix C.\nThe deviations of the policy reward estimates R\u0302t(\u03c0) are controlled by (a bound on) the variance of each term in Eq. (1): essentially the left-hand side of Eq. (3) from (OP), except with E\u0302x\u223cHt [\u00b7] replaced by Ex\u223cDX [\u00b7]. Resolving this discrepancy is handled using deviation bounds, so Eq. (3) holds with Ex\u223cDX [\u00b7], with worse right-hand side constants.\nThe rest of the analysis, which deviates from that of RandomizedUCB, compares the expected regret Reg(\u03c0) of any policy \u03c0 with the estimated regret R\u0302egt(\u03c0) using the variance constraints Eq. (3):\nLemma 4 (Informally). With high probability, for each m such that \u03c4m \u2265 O\u0303(K log |\u03a0|), each round t in epoch m, and each \u03c0 \u2208 \u03a0, Reg(\u03c0) \u2264 2R\u0302egt(\u03c0) +O(K\u00b5m).\nThis lemma can easily be combined with the constraint Eq. (2) from (OP): since the weights Qm\u22121 used in any round t in epoch m satisfy \u2211 \u03c0\u2208\u03a0 Qm\u22121R\u0302eg\u03c4m\u22121(\u03c0) \u2264 2K\u00b5\u03c4m\u22121, we obtain a bound on the (conditionally) expected regret in round t using the above lemma: with high probability,\n\u2211\n\u03c0\u2208\u03a0\nQ\u0303m\u22121 Reg(\u03c0) \u2264 O(K\u00b5m\u22121).\nSumming these terms up over all T rounds and applying martingale concentration gives the final regret bound in Theorem 2."}, {"heading": "5 Analysis of the Optimization Algorithm", "text": "In this section, we give a sketch of the analysis of our main optimization algorithm for computing weights Qm on each epoch as in Algorithm 2. As mentioned in Section 3.1, this analysis is based on a potential function.\nSince our attention for now is on a single epoch m, here and in what follows, when clear from context, we drop m from our notation and write simply \u03c4 = \u03c4m, \u00b5 = \u00b5m, etc. Let UA be the uniform distribution over the action set A. We define the following potential function for use on epoch m:\n\u03a6m(Q) = \u03c4\u00b5\n( E\u0302x[RE (UA\u2016Q\u00b5(\u00b7 | x))] 1\u2212K\u00b5 + \u2211 \u03c0\u2208\u03a0Q(\u03c0)b\u03c0 2K ) . (6)\nThe function in Eq. (6) is defined for all vectors Q \u2208 \u2206\u03a0. Also, RE (p\u2016q) denotes the unnormalized relative entropy between two nonnegative vectors p and q over the action space (or any set) A:\nRE (p\u2016q) = \u2211\na\u2208A\n(pa ln(pa/qa) + qa \u2212 pa).\nThis number is always nonnegative. Here, Q\u00b5(\u00b7|x) denotes the \u201cdistribution\u201d (which might not sum to 1) over A induced by Q\u00b5 for context x as given in Section 2.4. Thus, ignoring constants, this potential function is a combination of two terms: The first measures how far from uniform are the distributions induced by Q\u00b5, and the second is an estimate of expected regret under Q since b\u03c0 is proportional to the empirical regret of \u03c0. Making \u03a6m small thus encourages Q to choose actions as uniformly as possible while also incurring low regret \u2014 exactly the aims of our algorithm. The constants that appear in this definition are for later mathematical convenience.\nFor further intuition, note that, by straightforward calculus, the partial derivative \u2202\u03a6m/\u2202Q(\u03c0) is roughly proportional to the variance constraint for \u03c0 given in Eq. (3) (up to a slight mismatch of constants). This shows that if this constraint is not satisfied, then \u2202\u03a6m/\u2202Q(\u03c0) is likely to be negative, meaning that \u03a6m can be decreased by increasing Q(\u03c0). Thus, the weight vector Q that minimizes \u03a6m satisfies the variance constraint for every policy \u03c0. It turns out that this minimizing Q also satisfies the low regret constraint in Eq. (2), and also must sum to at most 1; in other words, it provides a complete solution to our optimization problem. Algorithm 2 does not fully minimize \u03a6m, but it is based roughly on coordinate descent. This is because in each iteration one of the weights (coordinate directions) Q(\u03c0) is increased. This weight is one whose corresponding partial derivative is large and negative.\nTo analyze the algorithm, we first argue that it is correct in the sense of satisfying the required constraints, provided that it halts.\nLemma 5. If Algorithm 2 halts and outputs a weight vector Q, then the constraints Eq. (3) and Eq. (2) must hold, and furthermore the sum of the weights Q(\u03c0) is at most 1.\nThe proof is rather straightforward: Following step 4, Eq. (2) must hold, and also the weights must sum to 1. And if the algorithm halts, then D\u03c0(Q) \u2264 0 for all \u03c0, which is equivalent to Eq. (3).\nWhat remains is the more challenging task of bounding the number of iterations until the algorithm does halt. We do this by showing that significant progress is made in reducing \u03a6m on every iteration. To begin, we show that scaling Q as in step 4 cannot cause \u03a6m to increase. Lemma 6. Let Q be a weight vector such that \u2211\n\u03c0 Q(\u03c0)(2K+ b\u03c0) > 2K, and let c be as in Eq. (4). Then \u03a6m(cQ) \u2264 \u03a6m(Q).\nProof sketch. We consider \u03a6m(cQ) as a function of c, and argue that its derivative (with respect to c) at the value of c given in the lemma statement is always nonnegative. Therefore, by convexity, it is nondecreasing for all values exceeding c. Since c < 1, this proves the lemma.\nNext, we show that substantial progress will be made in reducing \u03a6m each time that step 8 is executed.\nLemma 7. Let Q denote a set of weights and suppose, for some policy \u03c0, that D\u03c0(Q) > 0. Let Q \u2032 be a new set of weights which is an exact copy of Q except that Q\u2032(\u03c0) = Q(\u03c0) + \u03b1 where \u03b1 = \u03b1\u03c0(Q) > 0. Then\n\u03a6m(Q)\u2212 \u03a6m(Q\u2032) \u2265 \u03c4\u00b52\n4(1\u2212K\u00b5) . (7)\nProof sketch. We first compute exactly the change in potential for general \u03b1. Next, we apply a secondorder Taylor approximation, which is maximized by the \u03b1 used in the algorithm. The Taylor approximation, for this \u03b1, yields a lower bound which can be further simplified using the fact that Q\u00b5(a|x) \u2265 \u00b5 always, and our assumption that D\u03c0(Q) > 0. This gives the bound stated in the lemma.\nSo step 4 does not cause \u03a6m to increase, and step 8 causes \u03a6m to decrease by at least the amount given in Lemma 7. This immediately implies Theorem 3: for Qinit = 0, the initial potential is bounded by \u03c4\u00b5 ln(1/(K\u00b5))/(1\u2212K\u00b5), and it is never negative, so the number of times step 8 is executed is bounded by 4 ln(1/(K\u00b5))/\u00b5 as required."}, {"heading": "5.1 Epoching and Warm Start", "text": "As shown in Section 2.3, the bound on the number of iterations of the algorithm from Theorem 3 also gives a bound on the number of times the oracle is called. To reduce the number of oracle calls, one\napproach is the \u201cdoubling trick\u201d of Section 3.3, which enables us to bound the total combined number of iterations of Algorithm 2 in the first T rounds is only O\u0303( \u221a KT ). This means that the average number of\ncalls to the arg-max oracle is only O\u0303( \u221a\nK/T ) per round, meaning that the oracle is called far less than once per round, and in fact, at a vanishingly low rate.\nWe now turn to warm-start approach of Section 3.4, where in each epoch m + 1 we initialize the coordinate descent algorithm with Qinit = Qm, i.e. the weights computed in the previous epoch m. To analyze this, we bound how much the potential changes from \u03a6m(Qm) at the end of epoch m to \u03a6m+1(Qm) at the very start of epoch m + 1. This, combined with our earlier results regarding how quickly Algorithm 2 drives down the potential, we are able to get an overall bound on the total number of updates across T rounds.\nLemma 8. Let M be the largest integer for which \u03c4M+1 \u2264 T . With probability at least 1 \u2212 2\u03b4, for all (not too small) T , the total epoch-to-epoch increase in potential is\nM\u2211\nm=1\n(\u03a6m+1(Qm)\u2212 \u03a6m(Qm)) \u2264 O\u0303 (\u221a T\nK\n) ,\nwhere M is the largest integer for which \u03c4M+1 \u2264 T .\nProof sketch. The potential function, as written in Eq. (6), naturally breaks into two pieces whose epoch-to-epoch changes can be bounded separately. Changes affecting the relative entropy term on the left can be bounded, regardless of Qm, by taking advantage of the manner in which these distributions are smoothed. For the other term on the right, it turns out that these epoch-to-epoch changes are related to statistical quantities which can be bounded with high probability. Specifically, the total change in this term is related first to how the estimated reward of the empirically best policy compares to the expected reward of the optimal policy; and second, to how the reward received by our algorithm compares to that of the optimal reward. From our regret analysis, we are able to show that both of these quantities will be small with high probability.\nThus, the total amount that the potential increases across T rounds is at most O\u0303( \u221a T/K). On the other hand, Lemma 7 shows that each time Q is updated by Algorithm 2 the potential decreases by at least \u2126\u0303(1/K). Therefore, the total number of updates of the algorithm totaled over all T rounds is at most O\u0303( \u221a KT ). For instance, if we use (\u03c41, \u03c42) := (3, 5) and \u03c4m := m\n2 for m \u2265 3, then the weight vector Q is only updated about \u221a T times in T rounds, and on each of those rounds, Algorithm 2 requires\nO\u0303( \u221a K) iterations, on average. This proves Lemma 3."}, {"heading": "6 Experimental Evaluation", "text": "In this section we evaluate a variant of Algorithm 1 against several baselines. While Algorithm 1 is significantly more efficient than many previous approaches, the overall computational complexity is still at least O\u0303(T 1.5) excluding the running time of the oracle, since each invocation of the oracle at round t of Algorithm 1 needs O(t) time in order to create t cost-sensitive classification (henceforth CSC) examples through the transformation (5). After further accounting for the running time of the oracle itself, this seems markedly larger than the complexity of an ordinary supervised learning problem where it is typically\npossible to perform an O(1) complexity update upon receiving a fresh example using online learning algorithms.\nA natural solution is to use an \u201conline\u201d oracle which is stateful and accepts examples one by one. An online CSC oracle takes as input a weighted example and returns a predicted class (corresponding to one of K actions in our setting). Since the oracle is stateful, it remembers and uses examples from all previous calls in answering questions, thereby reducing the complexity of each oracle invocation to O(1) as in supervised learning. Using several of these oracles, we can efficiently track a distribution over good policies and sample from it. The detailed pseduo-code describing this high-level idea (which we call Online Cover) is provided in Algorithm 5 in Appendix F. The algorithm maintains a uniform distribution over a fixed number n of policies where n is a parameter of the algorithm. Upon receiving a fresh example, it updates all n policies with the suitable CSC examples (5). The specific CSC algorithm we use is a reduction to squared regression described in Algorithms 4 and 5 of Beygelzimer and Langford (2009), which is amenable to online updates. Our implementation is public and will be referenced in the final draft.\nDue to lack of public datasets for contextual bandit problems, we use a simple supervised-to-contextualbandit transformation (Dud\u0301\u0131k et al., 2011b) on the CCAT document classification problem in RCV1 (Lewis et al., 2004). This is a dataset of 781, 265 examples, with a total of d = 47, 152 TF-IDF features. In the data transformation, we treated the class labels as actions, and one minus 0/1-loss as the reward. Using this dataset, we provide a proof-of-concept that this approach can be empirically very effective. Our evaluation criteria is progressive validation (Blum et al., 1999) on 0/1 loss. We compare ourselves to several plausible baselines, whose results are summarized in Table 1. All baselines and our online algorithm take advantage of linear representations which are known to work well on this dataset. For each algorithm we report the result for the best parameter settings which are mentioned in Table 1.\n1. \u01eb-greedy (Sutton and Barto, 1998) explores randomly with probability \u01eb and otherwise exploits.\n2. Explore-first is a common variant where you explore uniformly over actions before switching to an exploit-only phase.\n3. A less common but powerful baseline is based on bagging. In essence, multiple algorithms can be trained with examples sampled with replacement. The different predictions of these different learned predictors yield a distribution over actions from which we can sample effectively.\n4. Another reasonable baseline is based on Thompson sampling (Chapelle and Li, 2011; Li, 2013) or linear UCB (Auer, 2002; Li et al., 2010), the latter being quite effective in past evaluations (Li et al., 2010; Chapelle and Li, 2011). It is impractical to run vanilla LinUCB on this problem due to the high-dimensionality which makes matrix inversions impractical. We report results for the algorihm run after doing a dimesionality reduction via random projections to 1000 dimensions. Even then the algorithm required 59 hours6, and simpler variants such as using diagonal matrix approximations or infrequent updating performed substantially worse.\n5. Finally, our algorithm achieves the best loss of 0.0530. Somewhat surprisingly, the minimum occurs for us with a cover set of size 1\u2014apparently for this problem the small decaying amount of uniform random sampling imposed is adequate exploration. Prediction performance is similar with a larger covering set.\nAll baselines except for LinUCB are implemented as a simple modification of Vowpal Wabbit, an open source online learning system. All reported results use default parameters where not otherwise specified. The contextual bandit learning algorithms all operate in a doubly robust mode similar to the reward estimates formed by our algorithm.\nBecause this is actually a fully supervised dataset, we can apply a fully supervised online multiclass algorithm to solve it. We use a simple one-against-all implementation to reduce this to binary classification, yielding an error rate of 0.051 which is competitive with the best previously reported results. This result is effectively a lower bound on the quality of the solution we can hope to achieve with algorithms\n6The linear algebra routines are based on Intel MKL package.\nusing only partial information. Our algorithm nearly achieves this lower bound with a running time only a factor of 2.5 slower. Hence on this dataset, very little further algorithmic improvement is possible."}, {"heading": "7 Conclusions", "text": "In this paper we have presented the first practical algorithm to our knowledge that attains the statistically optimal regret guarantee and is computationally efficient in the setting of general policy classes. A remarkable feature of the algorithm is that the total number of oracle calls over all T rounds is sublinear\u2014 a remarkable improvement over previous works in this setting. We believe that the online variant of the approach which we implemented in our experiments has the right practical flavor for a scalable solution to the contextual bandit problem. In future work, it would be interesting to directly analyze the Online Cover algorithm."}, {"heading": "A Omitted Algorithm Details", "text": "Algorithm 3 and Algorithm 4 give the details of the inverse propensity scoring transformation IPS and the action sampling procedure Sample."}, {"heading": "B Deviation Inequalities", "text": "B.1 Freedman\u2019s Inequality\nThe following form of Freedman\u2019s inequality for martingales is from Beygelzimer et al. (2011).\nLemma 9. Let X1, X2, . . . , XT be a sequence of real-valued random variables. Assume for all t \u2208 {1, 2, . . . , T }, Xt \u2264 R and E[Xt|X1, . . . , Xt\u22121] = 0. Define S := \u2211T t=1 Xt and V := \u2211T t=1 E[X 2 t |X1, . . . , Xt\u22121]. For any \u03b4 \u2208 (0, 1) and \u03bb \u2208 [0, 1/R], with probability at least 1\u2212 \u03b4,\nS \u2264 (e\u2212 2)\u03bbV + ln(1/\u03b4) \u03bb .\nB.2 Variance Bounds\nFix the epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 . Define the following for any probability distribution P over \u03a0, \u03c0 \u2208 \u03a0, and \u00b5 \u2208 [0, 1/K]:\nV (P, \u03c0, \u00b5) := Ex\u223cDX\n[ 1\nP\u00b5(\u03c0(x)|x)\n] , (8)\nV\u0302m(P, \u03c0, \u00b5) := E\u0302x\u223cH\u03c4m\n[ 1\nP\u00b5(\u03c0(x)|x)\n] . (9)\nThe proof of the following lemma is essentially the same as that of Theorem 6 from Dud\u0301\u0131k et al. (2011a).\nLemma 10. Fix any \u00b5m \u2208 [0, 1/K] for m \u2208 N. For any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,\nV (P, \u03c0, \u00b5m) \u2264 6.4V\u0302m(P, \u03c0, \u00b5m) + 75(1\u2212K\u00b5m) ln |\u03a0|\n\u00b52m\u03c4m + 6.3 ln(2|\u03a0|2m2/\u03b4) \u00b5m\u03c4m\nfor all probability distributions P over \u03a0, all \u03c0 \u2208 \u03a0, and all m \u2208 N. In particular, if\n\u00b5m \u2265 \u221a\nln(2|\u03a0|m2/\u03b4) K\u03c4m , \u03c4m \u2265 4K ln(2|\u03a0|m2/\u03b4),\nthen V (P, \u03c0, \u00b5m) \u2264 6.4V\u0302m(P, \u03c0, \u00b5m) + 81.3K.\nProof sketch. By Bernstein\u2019s (or Freedman\u2019s) inequality and union bounds, for any choice of Nm \u2208 N and \u03bbm \u2208 [0, \u00b5m] for m \u2208 N, the following holds with probability at least 1\u2212 \u03b4:\nV (P, \u03c0, \u00b5m)\u2212 V\u0302m(P, \u03c0, \u00b5m) \u2264 (e \u2212 2)\u03bbmV (P, \u03c0, \u00b5m)\n\u00b5m + ln(|\u03a0|Nm+12m2/\u03b4) \u03bbm\u03c4m\nall Nm-point distributions P over \u03a0, all \u03c0 \u2208 \u03a0, and all m \u2208 N. Here, an N -point distribution over \u03a0 is a distribution of the form 1N \u2211N i=1 1\u03c0i for \u03c01, \u03c02, . . . , \u03c0N \u2208 \u03a0. We henceforth condition on this \u2265 1 \u2212 \u03b4 probability event (for choices of Nm and \u03bbm to be determined). Using the probabilistic method, it can be shown that for any probability distribution P over \u03a0, any \u03c0 \u2208 \u03a0, any \u00b5m \u2208 [0, 1/K], and any cm > 0, there exists an Nm-point distribution P\u0303 over \u03a0 such that ( V (P, \u03c0, \u00b5m)\u2212 V (P\u0303 , \u03c0, \u00b5m) ) + cm ( V\u0302m(P\u0303 , \u03c0, \u00b5m)\u2212 V\u0302m(P, \u03c0, \u00b5m) )\n\u2264 \u03b3Nm,\u00b5m ( V (P, \u03c0, \u00b5m) + cmV\u0302m(P, \u03c0, \u00b5m) )\nwhere \u03b3N,\u00b5 := \u221a (1 \u2212K\u00b5)/(N\u00b5) + 3(1\u2212K\u00b5)/(N\u00b5).\nCombining the displayed inequalities (using cm := 1/(1\u2212 (e\u2212 2)\u03bbm/\u00b5m)) and rearranging gives\nV (P, \u03c0, \u00b5m) \u2264 1 + \u03b3Nm,\u00b5m 1\u2212 \u03b3Nm,\u00b5m \u00b7 V\u0302m(P, \u03c0, \u00b5m) 1\u2212 (e\u2212 2)\u03bbm\u00b5m + 1 1\u2212 \u03b3Nm,\u00b5m \u00b7 1 1\u2212 (e\u2212 2)\u03bbm\u00b5m \u00b7 ln(|\u03a0| Nm+12m2/\u03b4) \u03bbm\u03c4m .\nUsing Nm := \u230812(1\u2212K\u00b5m)/\u00b5m\u2309 and \u03bbm := 0.66\u00b5m for all m \u2208 N gives the claimed inequalities. If \u00b5m \u2265 \u221a ln(2|\u03a0|m2/\u03b4)/(K\u03c4m) and \u03c4m \u2265 4K ln(2|\u03a0|m2/\u03b4), then \u00b52m\u03c4m \u2265 ln(|\u03a0|)/K and \u00b5m\u03c4m \u2265 ln(2|\u03a0|2m2/\u03b4), and hence\n75(1\u2212K\u00b5m) ln |\u03a0| \u00b52m\u03c4m + 6.3 ln(2|\u03a0|2m2/\u03b4) \u00b5m\u03c4m \u2264 (75 + 6.3)K = 81.3K.\nB.3 Reward Estimates\nAgain, fix the epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 . Recall that for any epoch m \u2208 N and round t in epoch m,\n\u2022 Qm\u22121 \u2208 \u2206\u03a0 are the non-negative weights computed at the end of epoch m\u2212 1;\n\u2022 Q\u0303m\u22121 is the probability distribution over \u03a0 obtained from Qm\u22121 and the policy \u03c0m\u22121 with the highest reward estimate through epoch m\u2212 1;\n\u2022 Q\u0303\u00b5m\u22121m\u22121 (\u00b7|xt) is the probability distribution used to choose at. Let\nm(t) := min{m \u2208 N : t \u2264 \u03c4m} (10) be the index of the epoch containing round t \u2208 N, and define\nVt(\u03c0) := max 0\u2264m\u2264m(t)\u22121 {V (Q\u0303m, \u03c0, \u00b5m)} (11)\nfor all t \u2208 N and \u03c0 \u2208 \u03a0. Lemma 11. For any \u03b4 \u2208 (0, 1) and any choices of \u03bbm\u22121 \u2208 [0, \u00b5m\u22121] for m \u2208 N, with probability at least 1\u2212 \u03b4,\n|R\u0302t(\u03c0)\u2212R(\u03c0)| \u2264 Vt(\u03c0)\u03bbm\u22121 + ln(4t2|\u03a0|/\u03b4)\nt\u03bbm\u22121\nfor all policies \u03c0 \u2208 \u03a0, all epochs m \u2208 N, and all rounds t in epoch m. Proof. Fix any policy \u03c0 \u2208 \u03a0, epoch m \u2208 N, and round t in epoch m. Then\nR\u0302t(\u03c0)\u2212R(\u03c0) = 1\nt\nt\u2211\ni=1\nZi\nwhere Zi := r\u0302i(\u03c0(xi))\u2212 ri(\u03c0(xi)). Round i is in epoch m(i) \u2264 m, so\n|Zi| \u2264 1\nQ\u0303 \u00b5m(i)\u22121 m(i)\u22121 (\u03c0(xi)|xi)\n\u2264 1 \u00b5m(i)\u22121\nby the definition of the fictitious rewards. Because the sequences \u00b51 \u2265 \u00b52 \u2265 \u00b7 \u00b7 \u00b7 and m(1) \u2264 m(2) \u2264 \u00b7 \u00b7 \u00b7 are monotone, it follows that Zi \u2264 1/\u00b5m\u22121 for all 1 \u2264 i \u2264 t. Furthermore, E[Zi|Hi\u22121] = 0 and\nE[Z2i |Hi\u22121] \u2264 E[r\u0302i(\u03c0(xi))2|Hi\u22121] \u2264 V (Q\u0303m(i)\u22121, \u03c0, \u00b5m(i)\u22121) \u2264 Vt(\u03c0)\nfor all 1 \u2264 i \u2264 t. The first inequality follows because for var(X) \u2264 E(X2) for any random variable X ; and the other inequalities follow from the definitions of the fictitious rewards, V (\u00b7, \u00b7, \u00b7) in Eq. (8), and Vt(\u00b7) in Eq. (11). Applying Freedman\u2019s inequality and a union bound to the sums (1/t) \u2211t i=1 Zi and\n(1/t) \u2211t\ni=1(\u2212Zi) implies the following: for all \u03bbm\u22121 \u2208 [0, \u00b5m\u22121], with probability at least 1\u22122\u00b7\u03b4/(4t2|\u03a0|), \u2223\u2223\u2223\u2223\u2223 1 t t\u2211\ni=1\nZi \u2223\u2223\u2223\u2223\u2223 \u2264 (e \u2212 2)Vt(\u03c0)\u03bbm\u22121 + ln(4t2|\u03a0|/\u03b4) t\u03bbm\u22121 .\nThe lemma now follows by applying a union bound for all choices of \u03c0 \u2208 \u03a0 and t \u2208 N, since \u2211\n\u03c0\u2208\u03a0\n\u2211\nt\u2208N\n\u03b4\n2t2|\u03a0| \u2264 \u03b4."}, {"heading": "C Regret Analysis", "text": "Throughout this section, we fix the allowed probability of failure \u03b4 \u2208 (0, 1) provided as input to the algorithm, as well as the epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 .\nC.1 Definitions\nDefine, for all t \u2208 N,\ndt := ln(16t 2|\u03a0|/\u03b4), (12)\nand recall that,\n\u00b5m = min\n{ 1\n2K , \u221a d\u03c4m K\u03c4m } .\nObserve that dt/t is non-increasing with t \u2208 N, and \u00b5m is non-increasing with m \u2208 N. Let\nm0 := min\n{ m \u2208 N : d\u03c4m\n\u03c4m \u2264 1 4K\n} .\nObserve that \u03c4m0 \u2265 2. Define\n\u03c1 := sup m\u2265m0\n{\u221a \u03c4m\n\u03c4m\u22121\n} .\nRecall that we assume \u03c4m+1 \u2264 2\u03c4m; thus \u03c1 \u2264 \u221a 2.\nC.2 Deviation Control and Optimization Constraints\nLet E be the event in which the following statements hold:\nV (P, \u03c0, \u00b5m) \u2264 6.4V\u0302m(P, \u03c0, \u00b5m) + 81.3K (13)\nfor all probability distributions P over \u03a0, all \u03c0 \u2208 \u03a0, and all m \u2208 N such that \u00b5m \u2265 \u221a d\u03c4m/(K\u03c4m) and \u03c4m \u2265 4Kd\u03c4m ; and |R\u0302t(\u03c0)\u2212R(\u03c0)| \u2264 Vt(\u03c0)\u03bbt +\ndt t\u03bbt\n(14)\nwhere\n\u03bbt :=\n{\u221a dt 2Kt if m \u2264 m0,\n\u00b5m\u22121 if m > m0.\nfor all all policies \u03c0 \u2208 \u03a0, all epochs m \u2208 N, and all rounds t in epoch m. By Lemma 10, Lemma 11, and a union bound, Pr(E) \u2265 1\u2212 \u03b4/2.\nFor every epoch m \u2208 N, the weights Qm computed at the end of the epoch (in round \u03c4m) as the solution to (OP) satisfy the constraints Eq. (2) and Eq. (3): they are, respectively:\n\u2211\n\u03c0\u2208\u03a0\nQm(\u03c0)R\u0302eg\u03c4m(\u03c0) \u2264 \u03c8 \u00b7 2K\u00b5m (15)\nand, for all \u03c0 \u2208 \u03a0,\nV\u0302m(Qm, \u03c0, \u00b5m) \u2264 2K + R\u0302eg\u03c4m(\u03c0)\n\u03c8 \u00b7 \u00b5m . (16)\nRecall that \u03c8 = 100 (as defined in (OP), assuming \u03c1 \u2264 \u221a 2). Define \u03b81 := 94.1 and \u03b82 := \u03c8/6.4 (which come from Lemma 12); the proof of Lemma 13 requires that \u03b82 \u2265 8\u03c1, and hence \u03c8 \u2265 6.4 \u00b7 8\u03c1; this is true with our setting of \u03c8 since \u03c1 \u2264 \u221a 2.\nC.3 Proof of Theorem 2\nWe now give the proof of Theorem 2, following the outline in Section 4. The following lemma shows that if Vt(\u03c0) is large\u2014specifically, much larger thanK\u2014then the estimated regret of \u03c0 was large in some previous round.\nLemma 12. Assume event E holds. Pick any round t \u2208 N and any policy \u03c0 \u2208 \u03a0, and let m \u2208 N be the epoch achieving the max in the definition of Vt(\u03c0). Then\nVt(\u03c0) \u2264    2K if \u00b5m = 1/(2K), \u03b81K + R\u0302eg\u03c4m(\u03c0)\n\u03b82\u00b5m if \u00b5m < 1/(2K).\nProof. Fix a round t \u2208 N and policy \u03c0 \u2208 \u03a0. Let m \u2264 m(t) \u2212 1 be the epoch achieving the max in the definition of Vt(\u03c0) from Eq. (11), so Vt(\u03c0) = V (Q\u0303m, \u03c0, \u00b5m). If \u00b5m = 1/(2K), then V (Q\u0303m, \u03c0, \u00b5m) \u2264 2K. So assume instead that 1/(2K) > \u00b5m = \u221a d\u03c4m/(K\u03c4m). This implies that \u03c4m > 4Kd\u03c4m . By Eq. (13), which holds in event E , V (Q\u0303m, \u03c0, \u00b5m) \u2264 6.4V\u0302m(Q\u0303m, \u03c0, \u00b5m) + 81.3K.\nThe probability distribution Q\u0303m satisfies the inequalities\nV\u0302m(Q\u0303m, \u03c0, \u00b5m) \u2264 V\u0302m(Qm, \u03c0, \u00b5m) \u2264 2K + R\u0302eg\u03c4m(\u03c0)\n\u03c8\u00b5m .\nAbove, the first inequality follows because the value of V\u0302m(Qm, \u03c0, \u00b5m) decreases as the value of Qm(\u03c0\u03c4m) increases, as it does when going from Qm to Q\u0303m; the second inequality is the constraint Eq. (16) satisfied by Qm. Combining the displayed inequalities from above proves the claim.\nIn the next lemma, we compare Reg(\u03c0) and R\u0302egt(\u03c0) for any policy \u03c0 by using the deviation bounds for estimated rewards together with the variance bounds from Lemma 12. Define t0 := min{t \u2208 N : dt/t \u2264 1/(4K)}. Lemma 13. Assume event E holds. Let c0 := 4\u03c1(1 + \u03b81). For all epochs m \u2265 m0, all rounds t \u2265 t0 in epoch m, and all policies \u03c0 \u2208 \u03a0,\nReg(\u03c0) \u2264 2R\u0302egt(\u03c0) + c0K\u00b5m; R\u0302egt(\u03c0) \u2264 2Reg(\u03c0) + c0K\u00b5m.\nProof. The proof is by induction on m. As the base case, consider m = m0 and t \u2265 t0 in epoch m. By definition of m0, \u00b5m = 1/(2K) for all m < m0, so Vt(\u03c0) \u2264 2K for all \u03c0 \u2208 \u03a0 by Lemma 12. By Eq. (14), which holds in event E , for all \u03c0 \u2208 \u03a0,\n|R\u0302t(\u03c0)\u2212R(\u03c0)| \u2264 2K\u03bb+ dt t\u03bb\nfor all \u03c0 \u2208 \u03a0, where \u03bb = \u221a dt/(2Kt). This implies\n|R\u0302t(\u03c0) \u2212R(\u03c0)| \u2264 2 \u221a\n2Kdt t\nand therefore |R\u0302egt(\u03c0) \u2212 Reg(\u03c0)| \u2264 4 \u221a 2Kdt/t by the triangle inequality and optimality of \u03c0t and \u03c0\u22c6.\nSince t > \u03c4m0\u22121 and c0 \u2265 4 \u221a 2\u03c1, it follows that |R\u0302egt(\u03c0)\u2212 Reg(\u03c0)| \u2264 4 \u221a 2\u03c1K\u00b5m0 \u2264 c0K\u00b5m0 .\nFor the inductive step, fix some epoch m > m0. We assume as the inductive hypothesis that for all epochs m\u2032 < m, all rounds t\u2032 in epoch m\u2032, and all \u03c0 \u2208 \u03a0,\nReg(\u03c0) \u2264 2R\u0302egt\u2032(\u03c0) + c0K\u00b5m\u2032 ; R\u0302egt\u2032(\u03c0) \u2264 2Reg(\u03c0) + c0K\u00b5m\u2032 .\nWe first show that Reg(\u03c0) \u2264 2R\u0302egt(\u03c0) + c0K\u00b5m (17) for all rounds t in epoch m and all \u03c0 \u2208 \u03a0. So fix such a round t and policy \u03c0; by Eq. (14) (which holds in event E),\nReg(\u03c0) \u2212 R\u0302egt(\u03c0) = ( R(\u03c0\u22c6)\u2212R(\u03c0) ) \u2212 ( R\u0302t(\u03c0t)\u2212 R\u0302t(\u03c0) )\n\u2264 ( R(\u03c0\u22c6)\u2212R(\u03c0) ) \u2212 ( R\u0302t(\u03c0\u22c6)\u2212 R\u0302t(\u03c0) ) \u2264 ( Vt(\u03c0) + Vt(\u03c0\u22c6) ) \u00b5m\u22121 +\n2dt t\u00b5m\u22121 . (18)\nAbove, the first inequality follows from the optimality of \u03c0t. By Lemma 12, there exist epochs i, j < m such that\nVt(\u03c0) \u2264 \u03b81K + R\u0302eg\u03c4i(\u03c0)\n\u03b82\u00b5i \u00b7 1{\u00b5i < 1/(2K)},\nVt(\u03c0\u22c6) \u2264 \u03b81K + R\u0302eg\u03c4j (\u03c0\u22c6)\n\u03b82\u00b5j \u00b7 1{\u00b5j < 1/(2K)}.\nSuppose \u00b5i < 1/(2K), so m0 \u2264 i < m: in this case, the inductive hypothesis implies\nR\u0302eg\u03c4i(\u03c0) \u03b82\u00b5i \u2264 2Reg(\u03c0) + c0K\u00b5i \u03b82\u00b5i \u2264 c0K \u03b82 + 2Reg(\u03c0) \u03b82\u00b5m\u22121\nwhere the second inequality uses the fact that i \u2264 m\u2212 1. Therefore,\nVt(\u03c0)\u00b5m\u22121 \u2264 ( \u03b81 +\nc0 \u03b82\n) K\u00b5m\u22121 + 2\n\u03b82 Reg(\u03c0). (19)\nNow suppose \u00b5j < 1/(2K), so m0 \u2264 j < m: as above, the inductive hypothesis implies\nR\u0302eg\u03c4j (\u03c0\u22c6) \u03b82\u00b5j \u2264 2Reg(\u03c0\u22c6) + c0K\u00b5j \u03b82\u00b5j = c0 \u03b82 K\nsince Reg(\u03c0\u22c6) = 0. Therefore,\nVt(\u03c0\u22c6)\u00b5m\u22121 \u2264 ( \u03b81 +\nc0 \u03b82\n) K\u00b5m\u22121. (20)\nCombining Eq. (18), Eq. (19), and Eq. (20), and rearranging gives\nReg(\u03c0) \u2264 1 1\u2212 2\u03b82\n( R\u0302egt(\u03c0) + 2 ( \u03b81 +\nc0 \u03b82\n) K\u00b5m\u22121 +\n2dt t\u00b5m\u22121\n) .\nSincem \u2265 m0+1, it follows that \u00b5m\u22121 \u2264 \u03c1\u00b5m by definition of \u03c1. Moreover, since t > \u03c4m\u22121, (dt/t)/\u00b5m\u22121 \u2264 K\u00b52m\u22121/\u00b5m\u22121 \u2264 \u03c1K\u00b5m Applying these inequalities to the above display, and simplifying, yields Eq. (17) because c0 \u2265 4\u03c1(1 + \u03b81) and \u03b82 \u2265 8\u03c1.\nWe now show that R\u0302egt(\u03c0) \u2264 2Reg(\u03c0) + c0K\u00b5m (21)\nfor all \u03c0 \u2208 \u03a0. Again, fix an arbitrary \u03c0 \u2208 \u03a0, and by Eq. (14),\nR\u0302egt(\u03c0)\u2212 Reg(\u03c0) = ( R\u0302t(\u03c0t)\u2212 R\u0302t(\u03c0) ) \u2212 ( R(\u03c0\u22c6)\u2212R(\u03c0) )\n\u2264 ( R\u0302t(\u03c0t)\u2212 R\u0302t(\u03c0) ) \u2212 ( R(\u03c0t)\u2212R(\u03c0) ) \u2264 ( Vt(\u03c0) + Vt(\u03c0t) ) \u00b5m\u22121 +\n2dt t\u00b5m\u22121\n(22)\nwhere the first inequality follows from the optimality of \u03c0\u22c6. By Lemma 12, there exists an epoch j < m such\nVt(\u03c0t) \u2264 \u03b81K + R\u0302eg\u03c4j (\u03c0t)\n\u03b82\u00b5j \u00b7 1{\u00b5j < 1/(2K)}.\nSuppose \u00b5j < 1/(2K), so m0 \u2264 j < m: in this case the inductive hypothesis and Eq. (17) imply\nR\u0302eg\u03c4j (\u03c0t) \u03b82\u00b5j \u2264 2Reg(\u03c0t) + c0K\u00b5j \u03b82\u00b5j \u2264\n2 ( 2R\u0302egt(\u03c0t) + c0K\u00b5m ) + c0K\u00b5j\n\u03b82\u00b5j = 3c0 \u03b82 K\n(the last equality follows because R\u0302egt(\u03c0t) = 0). Thus\nVt(\u03c0t)\u00b5\u03c4(t)\u22121 \u2264 ( \u03b81 +\n3c0 \u03b82\n) K\u00b5m\u22121. (23)\nCombining Eq. (22), Eq. (23), and Eq. (19) gives\nR\u0302egt(\u03c0) \u2264 ( 1 + 2\n\u03b82\n) Reg(\u03c0) + ( 2\u03b81 +\n4c0 \u03b82\n) K\u00b5m\u22121 +\n2dt t\u00b5m\u22121 .\nAgain, applying the inequalities \u00b5m\u22121 \u2264 \u03c1\u00b5m and (dt/t)/\u00b5m\u22121 \u2264 K\u00b5m to the above display, and simplifying, yields Eq. (21) because c0 \u2265 4\u03c1(1 + \u03b81) and \u03b82 \u2265 8\u03c1. This completes the inductive step, and thus proves the overall claim.\nThe next lemma shows that the \u201clow estimated regret guarantee\u201d of Qt\u22121 (optimization constraint Eq. (15)) also implies a \u201clow regret guarantee\u201d, via the comparison of R\u0302egt(\u00b7) to Reg(\u00b7) from Lemma 13. Lemma 14. Assume event E holds. For every epoch m \u2208 N,\n\u2211\n\u03c0\u2208\u03a0\nQ\u0303m\u22121(\u03c0)Reg(\u03c0) \u2264 (4\u03c8 + c0)K\u00b5m\u22121\nwhere c0 is defined in Lemma 13.\nProof. Fix any epoch m \u2208 N. If m \u2264 m0, then \u00b5m\u22121 = 1/(2K), in which case the claim is trivial. Therefore assume m \u2265 m0 + 1. Then\n\u2211\n\u03c0\u2208\u03a0\nQ\u0303m\u22121(\u03c0)Reg(\u03c0) \u2264 \u2211\n\u03c0\u2208\u03a0\nQ\u0303m\u22121(\u03c0) ( 2R\u0302eg\u03c4m\u22121(\u03c0) + c0K\u00b5m\u22121 )\n= ( 2 \u2211\n\u03c0\u2208\u03a0\nQm\u22121(\u03c0)R\u0302eg\u03c4m\u22121(\u03c0)\n) + c0K\u00b5m\u22121\n\u2264 \u03c8 \u00b7 4K\u00b5m\u22121 + c0K\u00b5m\u22121.\nThe first step follows from Lemma 13, as all rounds in an epoch m \u2265 m0 + 1 satisfy t \u2265 t0; the second step follows from the fact that Q\u0303m\u22121 is a probability distribution, that Q\u0303m\u22121 = Qm\u22121 + \u03b11\u03c0\u03c4m\u22121 for some \u03b1 \u2265 0, and that R\u0302eg\u03c4m\u22121(\u03c0\u03c4m\u22121) = 0; and the last step follows from the constraint Eq. (15) satisfied by Qm\u22121.\nFinally, we straightforwardly translate the \u201clow regret guarantee\u201d from Lemma 14 to a bound on the cumulative regret of the algorithm. This involves summing the bound in Lemma 14 over all rounds t (Lemma 15 and Lemma 16) and applying a martingale concentration argument (Lemma 17).\nLemma 15. For any T \u2208 N, T\u2211\nt=1\n\u00b5m(t) \u2264 2 \u221a d\u03c4m(T )\u03c4m(T )\nK .\nProof. We break the sum over rounds into the epochs, and bound the sum within each epoch:\nT\u2211\nt=1\n\u00b5m(t) \u2264 m(T )\u2211\nm=1\n\u03c4m\u2211\nt=\u03c4m\u22121+1\n\u00b5m\n\u2264 m(T )\u2211\nm=1\n\u03c4m\u2211\nt=\u03c4m\u22121+1\n\u221a d\u03c4m K\u03c4m\n\u2264 \u221a\nd\u03c4m(T ) K\nm(T )\u2211\nm=1\n\u03c4m \u2212 \u03c4m\u22121\u221a \u03c4m\n\u2264 \u221a\nd\u03c4m(T ) K\nm(T )\u2211\nm=1\n\u222b \u03c4m \u03c4m\u22121 dx\u221a x = \u221a d\u03c4m(T ) K \u222b \u03c4m(T ) \u03c40 dx\u221a x = 2 \u221a d\u03c4m(T ) K \u221a \u03c4m(T ).\nAbove, the first step uses the fact that m(1) = 1 and \u03c4m(t)\u22121 + 1 \u2264 t \u2264 \u03c4m(t). The second step uses the definition of \u00b5m. The third step simplifies the sum over t and uses the bound d\u03c4m\u22121 \u2264 d\u03c4m(T ) . The remaining steps use an integral bound which is then directly evaluated (recalling that \u03c40 = 0).\nLemma 16. For any T \u2208 N, T\u2211\nt=1\n\u00b5m(t)\u22121 \u2264 \u03c4m0 2K +\n\u221a 8d\u03c4m(T)\u03c4m(T )\nK .\nProof. Under the epoch schedule condition \u03c4m+1 \u2264 2\u03c4m, we have \u00b5m(t)\u22121 \u2264 \u221a 2\u00b5m(t) whenever m(t) > m0; also, \u00b5m(t)\u22121 \u2264 1/(2K) whenever m(t) \u2264 m0. The conclusion follows by applying Lemma 15.\nLemma 17. For any T \u2208 N, with probability at least 1\u2212 \u03b4, the regret after T rounds is at most\nC0 ( 4Kd\u03c4m0\u22121 + \u221a 8Kd\u03c4m(T)\u03c4m(T ) ) + \u221a 8 log(2/\u03b4)\nwhere C0 := (4\u03c8 + c0) and c0 is defined in Lemma 13. Proof. Fix T \u2208 N. For each round t \u2208 N, let Zt := rt(\u03c0\u22c6(xt))\u2212 rt(at)\u2212 \u2211 \u03c0\u2208\u03a0 Q\u0303m(t)\u22121Reg(\u03c0). Since\nE[rt(\u03c0\u22c6(xt))\u2212 rt(at)|Ht\u22121] = R(\u03c0\u22c6)\u2212 \u2211\n\u03c0\u2208\u03a0\nQ\u0303m(t)\u22121(\u03c0)R(\u03c0) = \u2211\n\u03c0\u2208\u03a0\nQ\u0303m(t)\u22121Reg(\u03c0),\nit follows that E[Zt|Ht\u22121] = 0. Since |Zt| \u2264 2, it follows by Azuma\u2019s inequality that T\u2211\nt=1\nZt \u2264 2 \u221a 2T ln(2/\u03b4)\nwith probability at least 1\u2212 \u03b4/2. By Lemma 10, Lemma 11, and a union bound, the event E holds with probability at least 1 \u2212 \u03b4/2. Hence, by another union bound, with probability at least 1 \u2212 \u03b4, event E holds and the regret of the algorithm is bounded by\nT\u2211\nt=1\n\u2211\n\u03c0\u2208\u03a0\nQ\u0303m(t)\u22121(\u03c0)Reg(\u03c0) + 2 \u221a 2 ln(2/\u03b4).\nThe double summation above is bounded by Lemma 14 and Lemma 16:\nT\u2211\nt=1\n\u2211\n\u03c0\u2208\u03a0\nQ\u0303m(t)\u22121(\u03c0)Reg(\u03c0) \u2264 (4\u03c8 + c0)K T\u2211\nt=1\n\u00b5m(t)\u22121 \u2264 (4\u03c8 + c0) ( \u03c4m0 2 + \u221a 8Kd\u03c4m(T)\u03c4m(T ) ) .\nBy the definition of m0, \u03c4m0\u22121 \u2264 4Kd\u03c4m0\u22121 . Since \u03c4m0 \u2264 2\u03c4m0\u22121 by assumption, it follows that \u03c4m0 \u2264 8Kd\u03c4m0\u22121 .\nTheorem 2 follows from Lemma 17 and the fact that \u03c4m(T ) \u2264 2(T \u2212 1) whenever \u03c4m(T )\u22121 \u2265 1. There is one last result implied by Lemma 12 and Lemma 13 that is used elsewhere.\nLemma 18. Assume event E holds, and t is such that d\u03c4m(t)\u22121/\u03c4m(t)\u22121 \u2264 1/(4K). Then\nR\u0302t(\u03c0t) \u2264 R(\u03c0\u22c6) + ( \u03b81 +\nc0 \u03b82 + c0 + 1\n) K\u00b5m(t)\u22121.\nProof. Let m\u2032 < m(t) achieve the max in the definition of Vt(\u03c0\u22c6). If \u00b5m\u2032 < 1/(2K), then m\u2032 \u2265 m0, and\nVt(\u03c0\u22c6) \u2264 \u03b81K + R\u0302eg\u03c4m\u2032 (\u03c0\u22c6)\n\u03b82\u00b5m\u2032\n\u2264 \u03b81K + 2Reg(\u03c0\u22c6) + c0K\u00b5m\u2032\n\u03b82\u00b5m\u2032 = cK\nfor c := \u03b81 + c0/\u03b82. Above, the second inequality follows by Lemma 13. If \u00b5m\u2032 = 1/(2K), then the same bound also holds. Using this bound, we obtain from Eq. (14),\nR\u0302t(\u03c0\u22c6)\u2212R(\u03c0\u22c6) \u2264 cK\u00b5m(t)\u22121 + dt\nt\u00b5m(t)\u22121 .\nTo conclude,\nR\u0302t(\u03c0\u03c4m) = R(\u03c0\u22c6) + ( R\u0302t(\u03c0\u22c6)\u2212R(\u03c0\u22c6) ) + R\u0302egt(\u03c0\u22c6)\n\u2264 R(\u03c0\u22c6) + cK\u00b5m(t)\u22121 + dt\nt\u00b5m(t)\u22121 + R\u0302egt(\u03c0\u22c6)\n\u2264 R(\u03c0\u22c6) + cK\u00b5m(t)\u22121 + dt\nt\u00b5m(t)\u22121 + c0K\u00b5m(t)\nwhere the last inequality follows from Lemma 13. The claim follows because dt/t \u2264 d\u03c4m(t)\u22121/\u03c4m(t)\u22121 and \u00b5m(t) \u2264 \u00b5m(t)\u22121."}, {"heading": "D Details of Optimization Analysis", "text": "D.1 Proof of Lemma 5\nFollowing the execution of step 4, we must have\n\u2211\n\u03c0\nQ(\u03c0)(2K + b\u03c0) \u2264 2K. (24)\nThis is because, if the condition in step 7 does not hold, then Eq. (24) is already true. Otherwise, Q is replaced by Q\u2032 = cQ, and for this set of weights, Eq. (24) in fact holds with equality. Note that, since all quantities are nonnegative, Eq. (24) immediately implies both Eq. (2), and that \u2211 \u03c0 Q(\u03c0) \u2264 1.\nFurthermore, at the point where the algorithm halts at step 10, it must be that for all policies \u03c0, D\u03c0(Q) \u2264 0. However, unraveling definitions, we can see that this is exactly equivalent to Eq. (3).\nD.2 Proof of Lemma 6\nConsider the function g(c) = B0\u03a6m(cQ),\nwhere, in this proof, B0 = 2K/(\u03c4\u00b5). Let Q \u00b5 c (a|x) = (1 \u2212K\u00b5)cQ(a|x) + \u00b5. By the chain rule, the first derivative of g is:\ng\u2032(c) = B0 \u2211\n\u03c0\nQ(\u03c0) \u2202G(cQ)\n\u2202Q(\u03c0)\n= \u2211\n\u03c0\nQ(\u03c0) ( (2K + b\u03c0)\u2212 2E\u0302x\u223cHt [ 1\nQ\u00b5c (\u03c0(x)|x)\n]) (25)\nTo handle the second term, note that\n\u2211\n\u03c0\nQ(\u03c0)E\u0302x\u223cHt\n[ 1\nQ\u00b5c (\u03c0(x)|x)\n] = \u2211\n\u03c0\nQ(\u03c0)E\u0302x\u223cHt\n[\u2211\na\u2208A\n1{\u03c0(x) = a} Q\u00b5c (a|x)\n]\n= E\u0302x\u223cHt\n[\u2211\na\u2208A\n\u2211\n\u03c0\nQ(\u03c0)1{\u03c0(x) = a} Q\u00b5c (a|x)\n]\n= E\u0302x\u223cHt\n[\u2211\na\u2208A\nQ(a|x) Q\u00b5c (a|x)\n]\n= 1\nc E\u0302x\u223cHt\n[\u2211\na\u2208A\ncQ(a|x) (1\u2212K\u00b5)cQ(a|x) + \u00b5\n] \u2264 K\nc . (26)\nTo see the inequality in Eq. (26), let us fix x and define qa = cQ(a|x). Then \u2211 a qa = c \u2211\n\u03c0 Q(\u03c0) \u2264 1 by Eq. (4). Further, the expression inside the expectation in Eq. (26) is equal to\n\u2211\na\nqa (1\u2212K\u00b5)qa + \u00b5 = K \u00b7 1 K\n\u2211\na\n1\n(1\u2212K\u00b5) + \u00b5/qa\n\u2264 K \u00b7 1 (1\u2212K\u00b5) +K\u00b5/\u2211a qa\n(27)\n\u2264 K \u00b7 1 (1\u2212K\u00b5) +K\u00b5 = K. (28)\nEq. (27) uses Jensen\u2019s inequality, combined with the fact that the function 1/(1\u2212K\u00b5+ \u00b5/x) is concave (as a function of x). Eq. (28) uses the fact that the function 1/(1\u2212K\u00b5+K\u00b5/x) is nondecreasing (in x), and that the qa\u2019s sum to at most 1.\nThus, plugging Eq. (26) into Eq. (25) yields\ng\u2032(c) \u2265 \u2211\n\u03c0\nQ(\u03c0)(2K + b\u03c0)\u2212 2K\nc = 0\nby our definition of c. Since g is convex, this means that g is nondecreasing for all values exceeding c. In particular, since c < 1, this gives\nB0\u03a6m(Q) = g(1) \u2265 g(c) = B0\u03a6m(cQ),\nimplying the lemma since B0 > 0.\nD.3 Proof of Lemma 7\nWe first compute the change in potential for general \u03b1. Note that Q\u2032\u00b5(a|x) = Q\u00b5(a|x) if a 6= \u03c0(x), and otherwise\nQ\u2032 \u00b5 (\u03c0(x)|x) = Q\u00b5(\u03c0(x)|x) + (1\u2212K\u00b5)\u03b1.\nThus, most of the terms defining \u03a6m(Q) are left unchanged by the update. In particular, by a direct calculation:\n2K \u03c4\u00b5 (\u03a6m(Q)\u2212 \u03a6m(Q\u2032)) =\n2 1\u2212K\u00b5 E\u0302x\u223cHt [ ln ( 1 + \u03b1(1\u2212K\u00b5) Q\u00b5(\u03c0(x)|x) )] \u2212 \u03b1(2K + b\u03c0)\n\u2265 2 1\u2212K\u00b5 E\u0302x\u223cHt [ \u03b1(1 \u2212K\u00b5) Q\u00b5(\u03c0(x)|x) \u2212 1 2 ( \u03b1(1\u2212K\u00b5) Q\u00b5(\u03c0(x)|x) )2]\n\u2212\u03b1(2K + b\u03c0) (29) = 2\u03b1V\u03c0(Q)\u2212 (1\u2212K\u00b5)\u03b12S\u03c0(Q)\u2212 \u03b1(2K + b\u03c0) = \u03b1(V\u03c0(Q) +D\u03c0(Q))\u2212 (1\u2212K\u00b5)\u03b12S\u03c0(Q) (30)\n= (V\u03c0(Q) +D\u03c0(Q))\n2\n4(1\u2212K\u00b5)S\u03c0(Q) . (31)\nEq. (29) uses the bound ln(1+x) \u2265 x\u2212x2/2 which holds for x \u2265 0 (by Taylor\u2019s theorem). Eq. (31) holds by our choice of \u03b1 = \u03b1\u03c0(Q), which was chosen to maximize Eq. (30). By assumption, D\u03c0(Q) > 0, which implies V\u03c0(Q) > 2K. Further, since Q \u00b5(a|x) \u2265 \u00b5 always, we have\nS\u03c0(Q) = E\u0302x\u223cHt\n[ 1 Q\u00b5(\u03c0(x) | x)2 ]\n\u2264 1 \u00b5 \u00b7 E\u0302x\u223cHt\n[ 1\nQ\u00b5(\u03c0(x) | x)\n] = V\u03c0(Q)\n\u00b5 .\nThus,\n(V\u03c0(Q) +D\u03c0(Q)) 2\nS\u03c0(Q) \u2265 V\u03c0(Q)\n2\nS\u03c0(Q) = V\u03c0(Q) \u00b7\nV\u03c0(Q) S\u03c0(Q) \u2265 2K\u00b5.\nPlugging into Eq. (31) completes the lemma.\nD.4 Proof of Lemma 8\nWe break the potential of Eq. (6) into pieces and bound the total change in each separately. Specifically, by straightforward algebra, we can write\n\u03a6m(Q) = \u03c6 a m(Q) + \u03c6 b m + \u03c6 c m(Q) + \u03c6 d m(Q)\nwhere\n\u03c6am(Q) = \u03c4m\u00b5m\nK(1\u2212K\u00b5m) E\u0302x\u223cHt\n[ \u2212 \u2211\na\nlnQ\u00b5(a|x) ]\n\u03c6bm = \u03c4m\u00b5m lnK\n1\u2212K\u00b5m\n\u03c6cm(Q) = \u03c4m\u00b5m\n(\u2211\n\u03c0\nQ(\u03c0)\u2212 1 )\n\u03c6dm(Q) = \u03c4m\u00b5m 2K\n\u2211\n\u03c0\nQ(\u03c0)b\u03c0.\nWe assume throughout that \u2211\n\u03c0 Q(\u03c0) \u2264 1 as will always be the case for the vectors produced by Algorithm 2. For such a vector Q,\n\u03c6cm+1(Q)\u2212 \u03c6cm(Q) = (\u03c4m+1\u00b5m+1 \u2212 \u03c4m\u00b5m) (\u2211\n\u03c0\nQ(\u03c0)\u2212 1 ) \u2264 0\nsince \u03c4m\u00b5m is nondecreasing. This means we can essentially disregard the change in this term. Also, note that \u03c6bm does not depend on Q. Therefore, for this term, we get a telescoping sum:\nM\u2211\nm=1\n(\u03c6bm+1 \u2212 \u03c6bm) = \u03c6bM+1 \u2212 \u03c6b1 \u2264 \u03c6bM+1 \u2264 2 \u221a\nTdT K lnK\nsince K\u00b5M+1 \u2264 1/2, and where dT , used in the definition of \u00b5m, is defined in Eq. (12). Next, we tackle \u03c6am:\nLemma 19. M\u2211\nm=1\n(\u03c6am+1(Qm)\u2212 \u03c6am(Qm)) \u2264 6 \u221a\nTdT K ln(1/\u00b5M+1).\nProof. For the purposes of this proof, let\nCm = \u00b5m\n1\u2212K\u00b5m .\nThen we can write\n\u03c6am(Q) = \u2212 Cm K\n\u03c4m\u2211\nt=1\n\u2211\na\nlnQ\u00b5m(a|xt).\nNote that Cm \u2265 Cm+1 since \u00b5m \u2265 \u00b5m+1. Thus,\n\u03c6am+1(Q)\u2212 \u03c6am(Q) \u2264 Cm+1 K\n[ \u03c4m\u2211\nt=1\n\u2211\na\nlnQ\u00b5m(a|xt)\n\u2212 \u03c4m+1\u2211\nt=1\n\u2211\na\nlnQ\u00b5m+1(a|xt) ]\n= Cm+1 K\n[ \u03c4m\u2211\nt=1\n\u2211\na\nln ( Q\u00b5m(a|xt) Q\u00b5m+1(a|xt) )\n\u2212 \u03c4m+1\u2211\nt=\u03c4m+1\n\u2211\na\nlnQ\u00b5m+1(a|xt). ]\n\u2264 Cm+1[\u03c4m ln(\u00b5m/\u00b5m+1)\u2212 (\u03c4m+1 \u2212 \u03c4m) ln\u00b5m+1]. (32)\nEq. (32) uses Q\u00b5m+1(a|x) \u2265 \u00b5m+1, and also Q\u00b5m(a|x) Q\u00b5m+1(a|x) = (1 \u2212K\u00b5m)Q(a|x) + \u00b5m (1 \u2212K\u00b5m+1)Q(a|x) + \u00b5m+1 \u2264 \u00b5m \u00b5m+1 .\nA sum over the two terms appearing in Eq. (32) can now be bounded separately. Starting with the one on the left, since \u03c4m < \u03c4m+1 \u2264 T and K\u00b5m \u2264 1/2, we have\nCm+1\u03c4m \u2264 2\u03c4m\u00b5m+1 \u2264 2\u03c4m+1\u00b5m+1 \u2264 2 \u221a\nTdT K .\nThus,\nM\u2211\nm=1\nCm+1\u03c4m ln(\u00b5m/\u00b5m+1) \u2264 2 \u221a\nTdT K\nM\u2211\nm=1\nln(\u00b5m/\u00b5m+1)\n= 2 \u221a TdT K ln(\u00b51/\u00b5M+1)) \u2264 2 \u221a\nTdT K (\u2212 ln(\u00b5M+1)). (33)\nFor the second term in Eq. (32), using \u00b5m+1 \u2265 \u00b5M+1 for m \u2264 M , and definition of Cm, we have M\u2211\nm=1\n\u2212Cm+1(\u03c4m+1 \u2212 \u03c4m) ln\u00b5m+1 \u2264 \u22122(ln\u00b5M+1) M\u2211\nm=1\n(\u03c4m+1 \u2212 \u03c4m)\u00b5m+1\n\u2264 \u22122(ln\u00b5M+1) T\u2211\nt=1\n\u00b5m(t)\n\u2264 \u22124 \u221a\nTdT K (ln\u00b5M+1) (34)\nby Lemma 15. Combining Eqs. (32), (33) and (34) gives the statement of the lemma. Finally, we come to \u03c6dm(Q), which, by definition of b\u03c0, can be rewritten as\n\u03c6dm(Q) = B1\u03c4m \u2211\n\u03c0\nQ(\u03c0)R\u0302eg\u03c4m(\u03c0)\nwhereB1 = 1/(2K\u03c8) and \u03c8 is the same as appears in optimization problem (OP). Note that, conveniently,\n\u03c4mR\u0302eg\u03c4m(\u03c0) = S\u0302m(\u03c0m)\u2212 S\u0302m(\u03c0),\nwhere S\u0302m(\u03c0) is the cumulative empirical importance-weighted reward through round \u03c4m:\nS\u0302m(\u03c0) = \u03c4m\u2211\nt=1\nr\u0302t(\u03c0(xt)) = \u03c4mR\u0302\u03c4m(\u03c0).\nFrom the definition of Q\u0303, we have that\n\u03c6dm(Q\u0303) = \u03c6 d m(Q)\n+B1\n( 1\u2212 \u2211\n\u03c0\nQ(\u03c0) ) \u03c4mR\u0302eg\u03c4m(\u03c0m)\n= \u03c6dm(Q)\nsince R\u0302eg\u03c4m(\u03c0m) = 0. And by a similar computation, \u03c6 d m+1(Q\u0303) \u2265 \u03c6dm+1(Q) since R\u0302eg\u03c4m+1(\u03c0) is always nonnegative. Therefore,\n\u03c6dm+1(Qm)\u2212 \u03c6dm(Qm) \u2264 \u03c6dm+1(Q\u0303m)\u2212 \u03c6dm(Q\u0303m)\n= B1 \u2211\n\u03c0\nQ\u0303m(\u03c0) [( S\u0302m+1(\u03c0m+1)\u2212 S\u0302m+1(\u03c0) ) \u2212 ( S\u0302m(\u03c0m)\u2212 S\u0302m(\u03c0) )]\n= B1 ( S\u0302m+1(\u03c0m+1)\u2212 S\u0302m(\u03c0m) )\n\u2212B1 ( \u03c4m+1\u2211\nt=\u03c4m+1\n\u2211\n\u03c0\nQ\u0303m(\u03c0)r\u0302t(\u03c0(xt))\n) . (35)\nWe separately bound the two parenthesized expressions in Eq. (35) when summed over all epochs. Beginning with the first one, we have\nM\u2211\nm=1\n( S\u0302m+1(\u03c0m+1)\u2212 S\u0302m(\u03c0m) ) = S\u0302M+1(\u03c0M+1)\u2212 S\u03021(\u03c01) \u2264 S\u0302M+1(\u03c0M+1).\nBut by Lemma 18 (and under the same assumptions),\nS\u0302M+1(\u03c0M+1) = \u03c4M+1R\u0302\u03c4M+1(\u03c0M+1) \u2264 \u03c4M+1(R(\u03c0\u22c6) +D0K\u00b5M ) \u2264 \u03c4M+1R(\u03c0\u22c6) +D0 \u221a KTdT ,\nwhere D0 is the constant appearing in Lemma 18. For the second parenthesized expression of Eq. (35), let us define random variables\nZt = \u2211\n\u03c0\nQ\u0303\u03c4(t)(\u03c0)r\u0302t(\u03c0(xt)).\nNote that Zt is nonnegative, and if m = \u03c4(t), then\nZt = \u2211\n\u03c0\nQ\u0303m(\u03c0)r\u0302t(\u03c0(xt))\n= \u2211\na\nQ\u0303m(a|xt)r\u0302t(a)\n= \u2211\na\nQ\u0303m(a|xt) rt(a)1{a = at} Q\u0303\u00b5m(a|xt)\n\u2264 rt(at) 1\u2212K\u00b5m \u2264 2\nsince Q\u0303\u00b5m(a|x) \u2265 (1 \u2212 K\u00b5m)Q\u0303m(a|x), and since rt(at) \u2264 1 and K\u00b5m \u2264 1/2. Therefore, by Azuma\u2019s inequality, with probability at least 1\u2212 \u03b4,\n\u03c4M+1\u2211\nt=1\nZt \u2265 \u03c4M+1\u2211\nt=1\nE[Zt|Ht\u22121]\u2212 \u221a 2\u03c4M+1 ln(1/\u03b4).\nThe expectation that appears here can be computed to be\nE[Zt|Ht\u22121] = \u2211\n\u03c0\nQ\u0303m(\u03c0)R(\u03c0)\nso\nR(\u03c0\u22c6)\u2212 E[Zt|Ht\u22121] = \u2211\n\u03c0\nQ\u0303m(\u03c0)(R(\u03c0\u22c6)\u2212R(\u03c0))\n= \u2211\n\u03c0\nQ\u0303m(\u03c0)Reg(\u03c0)\n\u2264 (4\u03c8 + c0)K\u00b5m by Lemma 14 (under the same assumptions, and using the same constants). Thus, with high probability,\n\u03c4M+1\u2211\nt=1\n(R(\u03c0\u22c6)\u2212 Zt) \u2264 (4\u03c8 + c0)K \u03c4M+1\u2211\nt=1\n\u00b5m(t) + \u221a 2\u03c4M+1 ln(1/\u03b4)\n\u2264 (4\u03c8 + c0) \u221a 8KTdT + \u221a 2T ln(1/\u03b4)\nby Lemma 16. Putting these together, and applying the union bound, we find that with probability at least 1\u2212 2\u03b4, for all T (and corresponding M),\nM\u2211\nm=1\n(\u03c6dm(Qm)\u2212 \u03c6dm+1(Qm)) \u2264 O (\u221a T\nK ln(T/\u03b4)\n) .\nCombining the bounds on the separate pieces, we get the bound stated in the lemma."}, {"heading": "E Proof of Theorem 4", "text": "Recall the earlier definition of the low-variance distribution set\nQm = {Q \u2208 \u2206\u03a0 : Q satisfies Eq. (3) in round \u03c4m}.\nFix \u03b4 \u2208 (0, 1) and the epoch sequence, and assume M is large enough so \u00b5m = \u221a ln(16\u03c42m|\u03a0|/\u03b4)/\u03c4m for all m \u2208 N with \u03c4m \u2265 \u03c4M/2. The low-variance constraint Eq. (3) gives, in round t = \u03c4m,\nE\u0302x\u223cHt\n[ 1\nQ\u00b5m(\u03c0(x)|x)\n] \u2264 2K + R\u0302eg\u03c4m(\u03c0)\n\u03c8\u00b5m , \u2200\u03c0 \u2208 \u03a0.\nBelow, we use a policy class \u03a0 where every policy \u03c0 \u2208 \u03a0 has no regret (Reg(\u03c0) = 0), in which case Lemma 13 implies\nE\u0302x\u223cHt\n[ 1\nQ\u00b5m(\u03c0(x)|x)\n] \u2264 2K + c0K\u00b5m\n\u03c8\u00b5m = K\n( 2 +\nc0 \u03c8\n) , \u2200\u03c0 \u2208 \u03a0.\nApplying Lemma 10 (and using our choice of \u00b5m) gives the following constraints: with probability at least 1\u2212 \u03b4, for all m \u2208 N with \u03c4m \u2265 \u03c4M/2, for all \u03c0 \u2208 \u03a0,\nEx\u223cDX\n[ 1\nQ\u0303\u00b5m(\u03c0(x)|x)\n] \u2264 81.3K + 6.4K ( 2 +\nc0 \u03c8\n) =: cK (36)\n(to make Q into a probability distribution Q\u0303, the leftover mass can be put on any policy, say, already in the support of Q). That is, with high probability, for every relevant epoch m, every Q \u2208 Qm satisfies Eq. (36) for all \u03c0 \u2208 \u03a0.\nNext, we construct an instance with the property that these inequalities cannot be satisfied by a very sparse Q. An instance is drawn uniformly at random from N different contexts denoted as {1, 2, . . . , N} (where we set, with foresight, N := 1/(2 \u221a 2cK\u00b5M )). The reward structure in the problem will be extremely simple, with action K always obtaining a reward of 1, while all the other actions obtain a reward of 0, independent of the context. The distribution D will be uniform over the contexts (with these deterministic rewards). Our policy set \u03a0 will consist of (K\u22121)N separate policies, indexed by 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 K \u2212 1. Policy \u03c0ij has the property that\n\u03c0ij(x) =\n{ j if x = i,\nK otherwise.\nIn words, policy \u03c0ij takes action j on context i, and action K on all other contexts. Given the uniform distribution over contexts and our reward structure, each policy obtains an identical reward\nR(\u03c0) = ( 1\u2212 1\nN\n) \u00b7 1 + 1\nN \u00b7 0 = 1\u2212 1 N .\nIn particular, each policy has a zero expected regret as required. Finally, observe that on context i, \u03c0ij is the unique policy taking action j. Hence we have that Q\u0303(j|i) = Q\u0303(\u03c0ij) and Q\u0303\u00b5m(j|i) = (1\u2212K\u00b5m)Q\u0303(\u03c0ij) +\u00b5m. Now, let us consider the constraint Eq. (36) for the policy \u03c0ij . The left-hand side of this constraint can be simplified as\nEx\u223cDX\n[ 1\nQ\u0303\u00b5m(\u03c0(x)|x)\n] = 1\nN\nN\u2211\nx=1\n1\nQ\u0303\u00b5m(\u03c0ij(x)|x)\n= 1\nN\n\u2211\nx 6=i\n1\nQ\u0303\u00b5m(\u03c0ij(x)|x) +\n1 N \u00b7 1 Q\u0303\u00b5m(j|i)\n\u2265 1 N \u00b7 1 Q\u0303\u00b5m(j|i) .\nIf the distribution Q\u0303 does not put any support on the policy \u03c0ij , then Q\u0303 \u00b5m(j|i) = \u00b5m, and thus\nEx\u223cDX\n[ 1\nQ\u0303\u00b5m(\u03c0(x)|x)\n] \u2265 1\nN \u00b7 1 Q\u0303\u00b5m(j|i) = 1 N\u00b5m \u2265 1\u221a 2N\u00b5M > cK\n(since N < 1/( \u221a 2cK\u00b5M )). Such a distribution Q\u0303 violates Eq. (36), which means that every Q \u2208 Qm must have Q\u0303(\u03c0ij) > 0. Since this is true for each policy \u03c0ij , we see that every Q \u2208 Qm has\n|supp(Q)| \u2265 (K \u2212 1)N = K \u2212 1 2 \u221a 2cK\u00b5M = \u2126\n(\u221a K\u03c4M\nln(\u03c4M |\u03a0|/\u03b4)\n)\nwhich completes the proof."}, {"heading": "F Online Cover algorithm", "text": "This section describes the pseudocode of the precise algorithm use in our experiments. The minimum exploration probability \u00b5 was set as 0.05 min(1/K, 1/ \u221a tK) for our evaluation.\nAlgorithm 5 Online Cover\ninput Cover size n, minimum sampling probability \u00b5. 1: Initialize online cost-sensitive minimization oracles O1, O2, . . . , On, each of which controls a policy\n\u03c0(1), \u03c0(2), . . . , \u03c0(n); U := uniform probability distribution over these policies. 2: for round t = 1, 2, . . . do 3: Observe context xt \u2208 X . 4: (at, pt(at)) := Sample(xt, U, \u2205, \u00b5). 5: Select action at and observe reward rt(at) \u2208 [0, 1]. 6: for each i = 1, 2, . . . , n do 7: Qi := (i \u2212 1)\u22121 \u2211 j<i 1\u03c0(j) . 8: pi(a) := Q \u00b5(a|xt). 9: Create cost-sensitive example (xt, c) where c(a) = 1\u2212 rt(at)pt(at)1{a = at} \u2212 \u00b5 pi(a) .\n10: Update \u03c0(i) = Oi(x, c) 11: end for 12: end for\nTwo additional details are important in step 9:\n1. We pass a cost vector rather than a reward vector to the oracle since we have a loss minimization rather than a reward maximization oracle.\n2. We actually used a doubly robust estimate Dud\u0301\u0131k et al. (2011b) with a linear reward function that was trained in an online fashion."}], "references": [{"title": "The nonstochastic multiarmed", "author": ["Research", "2002. Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": null, "citeRegEx": "Research et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Research et al\\.", "year": 2002}, {"title": "An empirical evaluation of Thompson sampling", "author": ["cross-validation. In COLT", "1999. Olivier Chapelle", "Lihong Li"], "venue": null, "citeRegEx": "COLT et al\\.,? \\Q2011\\E", "shortCiteRegEx": "COLT et al\\.", "year": 2011}, {"title": "Predicting nearly as well as the best pruning of a decision", "author": ["2011b. David P. Helmbold", "Robert E. Schapire"], "venue": null, "citeRegEx": "Helmbold and Schapire.,? \\Q2011\\E", "shortCiteRegEx": "Helmbold and Schapire.", "year": 2011}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Generalized Thompson sampling for contextual bandits", "author": ["Lihong Li"], "venue": "CoRR, abs/1310.7163,", "citeRegEx": "Li.,? \\Q2013\\E", "shortCiteRegEx": "Li.", "year": 2013}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E. Schapire"], "venue": "In WWW,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Tighter bounds for multi-armed bandits with expert advice", "author": ["H. Brendan McMahan", "Matthew Streeter"], "venue": "In COLT,", "citeRegEx": "McMahan and Streeter.,? \\Q2009\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2009}, {"title": "Reinforcement learning, an introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R. Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}], "referenceMentions": [{"referenceID": 7, "context": "The strongest known results (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) provide an algorithm that carefully controls the exploration distribution to achieve an optimal regret after T rounds of", "startOffset": 28, "endOffset": 101}, {"referenceID": 8, "context": "1), it is possible to create \u01eb-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only a single call to the oracle per round.", "startOffset": 38, "endOffset": 62}, {"referenceID": 3, "context": "1), it is possible to create \u01eb-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only a single call to the oracle per round.", "startOffset": 79, "endOffset": 105}, {"referenceID": 3, "context": "1), it is possible to create \u01eb-greedy (Sutton and Barto, 1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only a single call to the oracle per round. However, these algorithms have suboptimal regret bounds of O((K log |\u03a0|)1/3T ) because the algorithms randomize uniformly over actions when they choose to explore. The RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) achieves the optimal regret bound (up to logarithmic factors) in the i.", "startOffset": 80, "endOffset": 411}, {"referenceID": 7, "context": "The EXP4-family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) solve the contextual bandit problem with optimal regret by updating weights (multiplicatively) over all policies in every round.", "startOffset": 30, "endOffset": 103}, {"referenceID": 8, "context": "This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007).", "startOffset": 108, "endOffset": 132}, {"referenceID": 3, "context": "This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007).", "startOffset": 150, "endOffset": 176}, {"referenceID": 9, "context": "Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011).", "startOffset": 76, "endOffset": 102}, {"referenceID": 5, "context": "Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011).", "startOffset": 76, "endOffset": 102}, {"referenceID": 2, "context": "Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies. In contrast, the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) is based on a natural abstraction from supervised learning\u2014the ability to efficiently find a function in a rich function class that minimizes the loss on a training set.", "startOffset": 32, "endOffset": 251}, {"referenceID": 2, "context": "Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies. In contrast, the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) is based on a natural abstraction from supervised learning\u2014the ability to efficiently find a function in a rich function class that minimizes the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007). However, these algorithms have only suboptimal regret bounds. Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-confidence bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated. In our experiments, we compare to a strong baseline algorithm that uses this approach (Chu et al., 2011). To circumvent the \u03a9(|\u03a0|) running time barrier, we restrict attention to algorithms that only access the policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key challenge is to design good supervised learning problems for querying this oracle. The RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves a certain convex program.", "startOffset": 32, "endOffset": 1606}, {"referenceID": 2, "context": "Except for a few special cases (Helmbold and Schapire, 1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in the number of policies. In contrast, the RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) is based on a natural abstraction from supervised learning\u2014the ability to efficiently find a function in a rich function class that minimizes the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle, which is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007). However, these algorithms have only suboptimal regret bounds. Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li, 2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-confidence bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated. In our experiments, we compare to a strong baseline algorithm that uses this approach (Chu et al., 2011). To circumvent the \u03a9(|\u03a0|) running time barrier, we restrict attention to algorithms that only access the policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key challenge is to design good supervised learning problems for querying this oracle. The RandomizedUCB algorithm of Dud\u0301\u0131k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves a certain convex program. However, the number of oracle calls in their work is prohibitively large, and the statistical analysis is also rather complex. Throughout this paper, we use the \u00d5 notation to suppress dependence on logarithmic factors. The paper of Dud\u0301\u0131k et al. (2011a) is colloquially referred to, by its authors, as the \u201cmonster paper\u201d (Langford, 2014).", "startOffset": 32, "endOffset": 1962}, {"referenceID": 4, "context": ", 2011b) on the CCAT document classification problem in RCV1 (Lewis et al., 2004).", "startOffset": 61, "endOffset": 81}, {"referenceID": 8, "context": "\u01eb-greedy (Sutton and Barto, 1998) explores randomly with probability \u01eb and otherwise exploits.", "startOffset": 9, "endOffset": 33}, {"referenceID": 5, "context": "Another reasonable baseline is based on Thompson sampling (Chapelle and Li, 2011; Li, 2013) or linear UCB (Auer, 2002; Li et al.", "startOffset": 58, "endOffset": 91}, {"referenceID": 6, "context": "Another reasonable baseline is based on Thompson sampling (Chapelle and Li, 2011; Li, 2013) or linear UCB (Auer, 2002; Li et al., 2010), the latter being quite effective in past evaluations (Li et al.", "startOffset": 106, "endOffset": 135}, {"referenceID": 6, "context": ", 2010), the latter being quite effective in past evaluations (Li et al., 2010; Chapelle and Li, 2011).", "startOffset": 62, "endOffset": 102}], "year": 2014, "abstractText": "We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes an action in response to the observed context, observing the reward only for that action. Our method assumes access to an oracle for solving cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only \u00d5( \u221a T ) oracle calls across all T rounds. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.", "creator": "LaTeX with hyperref package"}}}