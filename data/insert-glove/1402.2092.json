{"id": "1402.2092", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2014", "title": "Near-Optimally Teaching the Crowd to Classify", "abstract": "qawwal How should we present snapping training war-surplus examples to learners jarusombat to teach them classification wenxiang rules? hasan This 1503 is anthropologically a baranowo natural problem when 3-volume training kesbir workers for crowdsourcing lugares labeling fanque tasks, synonym and zilwa is also motivated by challenges chakar in toyad data - driven online pre-9 education. luksic We lampsacus propose \u00f6zge a natural stochastic model chatchawal of overturning the gulyayev learners, oystercatcher modeling inchoatia them aaronsohn as futemma randomly suras switching among hypotheses d'azur based leskinen on observed feedback. We unobjective then beguile develop STRICT, imperialists an efficient finis algorithm sampan for moamba selecting cial examples lourey to kuzmich teach razanauskas to encapsulated workers. everding Our solution greedily crees maximizes 4350 a submodular montoro surrogate immobilized objective function in pavillion order irwin to select connoted examples to arnstad show to the metamorphosen learners. admirably We zaki prove h\u014dk\u016ble\u2018a that florendo our penalva strategy vanpool is competitive laveyan with forkel the optimal 454 teaching policy. Moreover, for gringotts the special overpass case non-physical of ghada linear separators, we prove that cantinflas an exponential reduction stuhr in error probability 1267 can be 108.08 achieved. arrancar Our kazungula experiments on simulated artvin workers as well as three khavanov real image cauvery annotation tasks on ramayana Amazon Mechanical mincer Turk show the effectiveness 1520 of our multiphonics teaching walkerl algorithm.", "histories": [["v1", "Mon, 10 Feb 2014 10:36:49 GMT  (1122kb,D)", "https://arxiv.org/abs/1402.2092v1", null], ["v2", "Tue, 11 Feb 2014 11:24:42 GMT  (1122kb,D)", "http://arxiv.org/abs/1402.2092v2", null], ["v3", "Thu, 20 Feb 2014 12:25:01 GMT  (1122kb,D)", "http://arxiv.org/abs/1402.2092v3", null], ["v4", "Fri, 7 Mar 2014 19:38:37 GMT  (1123kb,D)", "http://arxiv.org/abs/1402.2092v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["adish singla", "ilija bogunovic", "g\u00e1bor bart\u00f3k", "amin karbasi", "andreas krause 0001"], "accepted": true, "id": "1402.2092"}, "pdf": {"name": "1402.2092.pdf", "metadata": {"source": "META", "title": "Near-Optimally Teaching the Crowd to Classify", "authors": ["Adish Singla", "Ilija Bogunovic", "Gabor Bartok", "Amin Karbasi", "Andreas Krause"], "emails": ["ADISH.SINGLA@INF.ETHZ.CH", "ILIJA.BOGUNOVIC@EPFL.CH", "BARTOK@INF.ETHZ.CH", "AMIN.KARBASI@INF.ETHZ.CH", "KRAUSEA@ETHZ.CH"], "sections": [{"heading": "1. Introduction", "text": "Crowdsourcing services, such as Amazon\u2019s Mechanical Turk platform1 (henceforth MTurk), are becoming vital for outsourcing information processing to large groups of workers. Machine learning, AI, and citizen science systems can hugely benefit from the use of these services as large-scale annotated data is often of crucial importance (Snow et al., 2008; Sorokin & Forsyth, 2008; Lintott et al., 2008). Data collected from such services however is often noisy, e.g., due to spamming, inexpert or careless workers (Sorokin & Forsyth, 2008). As the accuracy of the\n1MTurk: https://www.mturk.com/mturk/welcome\nCopyright 2014 by the author(s).\nannotated data is often crucial, the problem of tackling noise from crowdsourcing services has received considerable attention. Most of the work so far has focused on methods for combining labels from many annotators (Welinder et al., 2010; Gomes et al., 2011; Dalvi et al., 2013) or in designing control measures by estimating the worker\u2019s reliabilities through \u201cgold standard\u201d questions (Snow et al., 2008).\nIn this paper, we explore an orthogonal direction: can we teach workers in crowdsourcing services in order to improve their accuracy? That is, instead of designing models and methods for determining workers\u2019 reliability, can we develop intelligent systems that teach workers to be more effective? While we focus on crowdsourcing in this paper, similar challenges arise in other areas of data-driven education. As running examples, in this paper we focus on crowdsourcing image labeling. In particular, we consider the task of classifying animal species, an important component in several citizen science projects such as the eBird project (Sullivan et al., 2009).\nWe start with a high-level overview of our approach. Suppose we wish to teach the crowd to label a large set of images (e.g., distinguishing butterflies from moths). How can this be done without already having access to the labels, or a set of informative features, for all the images (in which case crowdsourcing would be useless)? We suppose we have ground truth labels only for a small \u201cteaching set\u201d of examples. Our premise is that if we can teach a worker to classify this teaching set well, she can generalize to new images. In our approach, we first elicit\u2014on the teaching set\u2014a set of candidate features as well as a collection of hypotheses (e.g., linear classifiers) that the crowd may be using. We will describe the concrete procedure used in our experimental setup in Section 5. Having access to this information we use a teaching algorithm to select training examples and steer the learner towards the target hypothesis.\nClassical work on teaching classifiers (reviewed in Section 2.2), assumes that learners are noise-free: Hypotheses are immediately eliminated from consideration upon ob-\nar X\niv :1\n40 2.\n20 92\nv4 [\ncs .L\nG ]\n7 M\nar 2\n01 4\nservation of an inconsistent training example. As we see in our experiments (Section 6), such approaches can be brittle. In contrast, we propose a noise-tolerant stochastic model of the learners, capturing our assumptions on how they incorporate training examples. We then (Section 4) propose STRICT (Submodular Teaching for cRowdsourcIng ClassificaTion), a novel teaching algorithm that selects a sequence of training examples to the workers in order to steer them towards the true hypothesis. We theoretically analyze our approach, proving strong approximation guarantees and teaching complexity results. Lastly, we demonstrate the effectiveness of our model and STRICT policy on three real image annotation tasks, carried out on the MTurk platform."}, {"heading": "2. Background and Teaching Process", "text": "We now describe our learning domain and teaching protocol. As a running example, we consider the task of teaching to classify images, e.g., to distinguish butterflies from moths (see Figure 1)."}, {"heading": "2.1. The domain and the teaching protocol", "text": "Let X denote a set of examples (e.g., images), called the teaching set. We use (x, y) to denote a labeled example where x \u2208 X and y \u2208 {\u22121, 1}. We denote by H a finite class of hypotheses. Each element of H is a function h : X 7\u2192 R. The label assigned to x by hypothesis h is sgn(h(x)). The magnitude |h(x)| indicates the confidence hypothesish has in the label ofx. For now, let us assume that X and H are known to both the teacher and the learner. In our image classification example, each image may be given by a set of features x, and each hypothesis h(x) = wTh x could be a linear function. In Section 5, we discuss the concrete hypothesis spaces used in our crowdsourcing tasks, and how we can elicit them from the crowd.\nThe teacher has access to the labels y(x) of all the examples x inX . We consider the realizable setting whereH contains a hypothesis h\u2217 (known to the teacher, but not the learner) for which sgn(h\u2217(x)) = y(x) for all x \u2208 X . The goal of the teacher is to teach the correct hypothesis h\u2217 to the learner. The basic assumption behind our approach is that if\nwe can teach the workers to classify X correctly, then they will be able to generalize to new examples drawn from the same distribution as X (for which we neither have ground truth labels nor features). We will verify this assumption experimentally in Section 6. In the following, we review existing approaches to teaching classifiers, and then present our novel teaching method."}, {"heading": "2.2. Existing teaching models", "text": "In existing methods, a broad separation can be made about assumptions that learners use to process training examples. Noise-free models assume learners immediately discard hypotheses inconsistent with observed examples. As our experiments in Section 6 show, such models can be brittle in practice. In contrast, noise-tolerant models make less strict assumptions on how workers treat inconsistent hypotheses.\nNoise-free teaching: In their seminal work, Goldman & Kearns (1992) consider the non-interactive model: The teacher reveals a sequence of labeled examples, and the learner discards any inconsistent hypotheses (i.e., for which h(x) 6= y for any example (x, y) shown). For a given hypothesis class, the Teaching Dimension is the smallest number of examples required to ensure that all inconsistent hypotheses are eliminated. More recent work (Balbach & Zeugmann, 2009; Zilles et al., 2011; Doliwa et al., 2010; Du & Ling, 2011) consider models of interactive teaching, where the teacher, after showing each example, obtains feedback about the hypothesis that the learner is currently implementing. Such feedback can be used to select future teaching examples in a more informed way. While theoretically intriguing, in this paper we focus on non-interactive models, which are typically easier to deploy in practice.\nNoise-tolerant teaching: In contrast to the noise-free setting, the practically extremely important noise-tolerant setting is theoretically much less understood. Very recently, Zhu (2013) investigates the optimization problem of generating a set of teaching examples that trades off between the expected future error of the learner and the \u201ceffort\u201d (i.e., number of examples) taken by the teacher, in the special case when the prior of the learner falls into the exponential family, and the learner performs Bayesian\ninference. Their algorithmic approach does not apply to the problem addressed in this paper. Further, the approach is based on heuristically rounding the solution of a convex program, with no bounds on the integrality gap.\nBasu & Christensen (2013) study a similar problem of teaching workers to classify images. The authors empirically investigate a variety of heuristic teaching policies on a set of human subjects for a synthetically generated data set. Lindsey et al. (2013) propose a method for evaluating and optimizing over parametrized policies with different orderings of positive and negative examples. None of these approaches offer theoretical performance guarantees of the kind provided in this paper."}, {"heading": "3. Model of the Learner", "text": "We now introduce our model of the learner, by formalizing our assumptions about how she adapts her hypothesis based on the training examples she receives from the teacher. Generally, we assume that the learner is not aware that she is being taught. We assume that she carries out a random walk in the hypothesis spaceH: She starts at some hypothesis, stays there as long as the training examples received are consistent with it, and randomly jumps to an alternative hypothesis upon an observed inconsistency. Hereby, preference will be given to hypotheses that better agree with the received training.\nMore formally, we model the learner via a stochastic process, in particular a (non-stationary) Markov chain. Before the first example, the learner randomly chooses a hypothesis h1, drawn from a prior distribution P0. Then, in every round t there are two possibilities: If the example (xt, yt) received agrees with the label implied by the learner\u2019s current hypothesis (i.e., sgn(ht(xt)) = yt), she sticks to it: ht+1 = ht. On the other hand, if the label yt disagrees with the learner\u2019s prediction sgn(ht(xt)), she draws a new hypothesis ht+1 based on a distribution Pt constructed in a way that reduces the probability of hypotheses that disagreed with the true labels in the previous steps:\nPt(h) = 1\nZt P0(h)\nt\u220f s=1\nys 6=sgn(h(xs))\nP (ys|h, xs) (1)\nwith normalization factor\nZt = \u2211 h\u2208H P0(h)\nt\u220f s=1\nys 6=sgn(h(xs))\nP (ys|h, xs).\nIn Equation (1), for some \u03b1 > 0, the term\nP (ys|h, xs) = 1\n1 + exp(\u2212\u03b1h(xs)ys)\nmodels a likelihood function, encoding the confidence that hypothesis h places in example xs. Thus, if the example (xs, ys) is \u201cstrongly inconsistent\u201d with h (i.e., h(xs)ys takes a large negative value and consequently P (ys | h, xs) is very small), then the learner will be very unlikely to jump to hypothesis h. The scaling parameter \u03b1 allows to control the effect of observing inconsistent examples. The limit \u03b1\u2192\u221e results in a behavior where inconsistent hypotheses are completely removed from consideration. This case precisely coincides with the noise-free learner models classically considered in the literature (Goldman & Kearns, 1992).\nIt can be shown (see Lemma 1 in the supplementary material), that the marginal probability that the learner implements some hypothesis h in step t is equal to Pt(h), even when the true label and the predicted label agreed in the previous step."}, {"heading": "4. Teaching Algorithm", "text": "Given the learner\u2019s prior over the hypotheses P0(h), how should the teacher choose examples to help the learner narrow down her belief to accurate hypotheses? By carefully showing examples, the teacher can control the learner\u2019s progress by steering her posterior towards h\u2217.\nWith a slight abuse of notation, if the teacher showed the set of examples A = {x1, . . . , xt} we denote the posterior distribution by Pt(\u00b7) and P (\u00b7|A) interchangeably. We use the latter notation when we want to emphasize that the examples shown are the elements ofA. With the new notation, we can write the learner\u2019s posterior after showingA as\nP (h|A) = 1 Z(A) P0(h) \u220f x\u2208A\ny(x) 6=sgn(h(x))\nP (y(x)|h, x) .\nThe ultimate goal of the teacher is to steer the learner towards a distribution with which she makes few mistakes. The expected error-rate of the learner after seeing examples A = {x1, . . . , xt} together with their labels yi = sgn(h \u2217(xi)) can be expressed as\nE[errL | A] = \u2211 h\u2208H P (h|A) err(h, h\u2217) ,where\nerr(h, h\u2217) = |{x \u2208 X : sgn(h(x)) 6= sgn(h\u2217(x))}|\n|X |\nis the fraction of examples x from the teaching set X on whichh andh\u2217 disagree about the label. We use the notation E[errL] = E[errL | {}] as shorthand to refer to the learner\u2019s error before receiving training.\nGiven an allowed tolerance for the learner\u2019s error, a natural objective for the teacher is to find the smallest set of examplesA\u2217 achieving this error, i.e.:\nA\u2217\u03b5 = arg min A\u2286X |A| s.t. E[errL | A] \u2264 . (2)\nWe will use the notation OPT( ) = |A\u2217 | to refer to the size of the optimal solution achieving error . Unfortunately, Problem (2) is a difficult combinatorial optimization problem. The following proposition, proved in the supplement, establishes hardness via a reduction from set cover.\nProposition 1. Problem (2) is NP-hard.\nGiven this hardness, in the following, we introduce an efficient approximation algorithm for Problem (2).\nThe first observation is that, in order to solve Problem (2), we can look at the objective function\nR(A) = E[errL]\u2212 E[errL | A] = \u2211 h\u2208H (P0(h)\u2212 P (h|A)) err(h, h\u2217) ,\nquantifying the expected reduction in error upon teaching A. Solving Problem (2) is equivalent to finding the smallest set A achieving error reduction E[errL] \u2212 . Thus, if we could, for each k, find a set A of size k maximizing R(A), we could solve Problem (2), contradicting the hardness.\nThe key idea is to replace the objective R(A) with the following surrogate function:\nF (A) = \u2211 h\u2208H (Q(h)\u2212Q(h|A)) err(h, h\u2217) ,where\nQ(h|A) = P0(h) \u220f x\u2208A\ny(x) 6=sgn(h(x))\nP (y(x)|h, x)\nis the unnormalized posterior of the learner. As shown in the supplementary material, this surrogate objective function satisfies submodularity, a natural diminishing returns condition. Submodular functions can be effectively optimized using a greedy algorithm, which, at every iteration, adds the example that maximally increases the surrogate function F (Nemhauser et al., 1978). We will show that maximizing F (A) gives us good results in terms of the original, normalized objective functionR(A), that is, the expected error reduction of the learner. In fact, we show that running the algorithm untilF (A) \u2265 E[errL]\u2212P0(h\u2217) is sufficient to produce a feasible solution to Problem (2), providing a natural stopping condition. We call the greedy algorithm for F (A) STRICT, and describe it in Policy 1.\nNote that in the limit \u03b1 \u2192 \u221e, F (A) quantifies the prior mass of all hypotheses h (weighted by err(h, h\u2217)) that are inconsistent with the examplesA. Thus, in this case,F (A) is simply a weighted coverage function, consistent with classical work in noise-free teaching (Goldman & Kearns, 1992)."}, {"heading": "4.1. Approximation Guarantees", "text": "The following theorem ensures that if we choose the examples in a greedy manner to maximize our surrogate objective function F (A), as done by Policy 1, we are close to being optimal in some sense.\nPolicy 1 Teaching Policy STRICT 1: Input: examplesX , hyp.H, prior P0, error . 2: Output: teaching setA 3: A\u2190 \u2205 4: while F (A) < E[errL]\u2212 P0(h\u2217) do 5: x\u2190 arg maxx\u2208X (F (A \u222a {x})) 6: A\u2190 A \u222a {x} 7: end while\nTheorem 1. Fix > 0. The STRICT Policy 1 terminates after at most OPT(P0(h\u2217) /2) log 1P0(h\u2217) steps with a set A such that E[errL | A] \u2264 .\nThus, informally, Policy 1 uses a near-minimal number of examples when compared to any policy achieving O( ) error (viewing P0(h\u2217) as a constant).\nThe main idea behind the proof of this theorem is that we first observe that F (A) is submodular and thus the greedy algorithm gives a set reasonably close toF \u2019s optimum. Then we analyze the connection between maximizing F (A) and minimizing the expected error of the learner,E[errL | A]. A detailed proof can be found in the supplementary material.\nNote that maximizing F (A) is not only sufficient, but also necessary to achieve precision. Indeed, it is immediate that P (h|A) \u2265 Q(h|A) ,which in turn leads to\nE[errL |A]= \u2211 h\u2208H P (h|A) err(h, h\u2217)\u2265 \u2211 h\u2208H Q(h|A) err(h, h\u2217)\n= E[errL]\u2212 F (A) .\nThus, if E[errL] \u2212 F (A) > , then the expected posterior error E[errL | A] of the learner is also greater than ."}, {"heading": "4.2. Teaching Complexity for Linear Separators", "text": "Theorem 1 shows that greedily optimizing F (A) leads to low error with a number of examples not far away from the optimal. Now we show that, under some additional assumptions, the optimal number of examples is not too large.\nWe consider the important case where the set of hypotheses H = {h1, h2, . . . , hn} consists of linear separators h(x) = wTh x+ bh for some weight vectorwh \u2208 Rd and offset bh \u2208 R. The label predicted byh for examplex is sgn(wTh x+bh).\nWe introduce an additional assumption, namely \u03bb-richness of (X ,H). First notice that H partitions X into polytopes (intersections of half-spaces), where within one polytope, all examples are labeled the same by every hypothesis, that is, within a polytope P , for every x, x\u2032 \u2208 P \u2286 Rd and h \u2208 H, sgn(h(x)) = sgn(h(x\u2032)). We say that X is \u03bb-rich if any P contains at least \u03bb examples. In other words, if the teacher needs to show (up to) \u03bb distinct examples to the learner from the same polytope in order to reduce her error below some level, this can be done.\nTheorem 2. Fix \u03b5 > 0. Suppose that the hypotheses are hyperplanes in Rd and that (X ,H) is (8 log2 2 )-rich. Then the STRICT policy achieves learner error less than after at mostm = 8 log2 2 teaching examples.\nThe proof of this theorem is in the supplementary material. In a nutshell, the proof works by establishing the existence (via the probabilistic method) of a teaching policy for which the number of examples needed can be bounded \u2013 hence also bounding the optimal policy \u2013 and then using Theorem 1."}, {"heading": "5. Experimental Setup", "text": "In our experiments, we consider three different image classification tasks: i) classification of synthetic insect images into two hypothetical species Vespula and Weevil (VW); ii) distinguishing butterflies and moths on real images (BM); and iii) identification of birds belonging to an endangered species of woodpeckers from real images (WP). Our teaching process requires a known feature space for image dataset X (i.e. the teaching set of images) and a hypothesis class H. While X and H can be controlled by design for the synthetic images, we illustrate different ways on how to automatically obtain a crowd-based embedding of the data for real images. We now discuss in detail the experimental setup of obtaining X with its feature space and H for the three different data sets used in our classification tasks."}, {"heading": "5.1. Vespula vs. Weevil", "text": "We first generate a classification problem using synthetic images X in order to allow controlled experimentation. As a crucial advantage, in this setting the hypothesis classH is known by design, and the task difficulty can be controlled. Furthermore, this setting ensures that workers have no prior knowledge of the image categories.\nDataset X and feature space: We generated synthetic images of insects belonging to two hypothetical species: Weevil and Vespula. The task is to classify whether a given image contains a Vespula or not. The images were generated by varying body size and color as well as head size and color. A given image xi can be distinguished based on the following two-dimensional feature vector xi = [xi,1 = f1, xi,2 = f2] \u2013 i) f1: the head/body size ratio, ii) f2: head/body color contrast. Fig. 3(a) shows the embedding of this data set in a twodimensional space based on these two features. Fig. 2 shows sample images of the two species and illustrates that Weevils have short heads with color similar to their body, whereas Vespula are distinguished by their big and contrasting heads.\nA total of 80 images per species were generated by sampling the features f1 and f2 from two bivariate Gaussian distributions: (\u00b5 = [0.10, 0.13], \u03a3 = [0.12, 0; 0, 0.12]) for Vespula and (\u00b5 = [\u22120.10,\u22120.13], \u03a3 = [0.12, 0; 0, 0.12]) for Weevil. A separate test set of 20 images per species were generated as well, for evaluating learning performance.\nHypothesis classH: Since we know the exact feature space of X , we can use any parametrized class of functionsH on X . In our experiments, we use a class of linear functions for H , and further restrictingH to eight clusters of hypotheses, centered at the origin and rotated by \u03c0/4 from each other. Specifically, we sampled the parameters of the linear hypotheses from the following multivariate Gaussian distribution: (\u00b5i = [\u03c0/4 \u00b7 i, 0], \u03a3i = [2, 0; 0, 0.005]), where i varies from 0 to 7. Each hypothesis captures a different set of cues about the features that workers could reasonably have: i) ignoring a feature, ii) using it as a positive signal for Vespula, and iii) using it as a negative signal for Vespula. Amongst the generated hypothesis, we picked target hypothesis h\u2217 as the one with minimal error on teaching setX . In order to ensure realizability, we then removed any data points x \u2208 X where sgn(h\u2217(x)) 6= y(x). Fig. 3(a) shows a subset of four of these hypothesis, with the target hypothesis h\u2217 represented in red. The prior distribution P0 is chosen as uniform."}, {"heading": "5.2. Butterflies vs. Moths", "text": "Dataset images X : As our second dataset, we used a collection of 200 real images of four species of butterflies and moths from publicly available images2: i) Peacock Butterfly, ii) Ringlet Butterfly, iii) Caterpillar Moth, iv) Tiger Moth, as shown in Fig. 2. The task is to classify whether a given image contains a butterfly or not. While Peacock Butterfly and Caterpillar Moth are clearly distinguishable as butterflies and moths, Tiger Moth and Ringlet Butterfly are visually hard to classify correctly. We used 160 of these images (40 per sub-species) as teaching setX and the remaining 40 (10 per sub-species) for testing.\nCrowd-embedding of X : A Euclidean embedding of X for such an image set is not readily available. Humanperceptible features for such real images may be difficult to compute. In fact, this challenge is one major motivation for using crowdsourcing in image annotation. However, several techniques do exist that allow estimating such an embedding from a small set of images and a limited number of crowd labels. In particular, we used the approach of Welinder et al.\n2Imagenet: http://www.image-net.org/\n(2010) as a preprocessing step. Welinder et al. propose a generative Bayesian model for the annotation process of the images by the workers and then use an inference algorithm to jointly estimate a low-dimensional embedding of the data, as well as a collection of linear hypotheses \u2013 one for each annotator \u2013 that best explain their provided labels.\nWe requested binary labels (of whether the image contains a butterfly) for our teaching set X , |X | = 160, from a set of 60 workers. By using the software CUBAM3, implementing the approach of Welinder et al., we inferred a 2-D embedding of the data, as well as linear hypotheses corresponding to each of the 60 workers who provided the labels. Fig. 3(b) shows this embedding of the data, as well as a small subset of workers\u2019 hypotheses as colored lines.\nHypothesis class H: The 60 hypothesis obtained through the crowd-embedding provide a prior distribution over linear hypotheses that the workers in the crowd may have been using. Note that these hypotheses capture various idiosyncrasies (termed \u201cschools of thought\u201d by Welinder et al.) in the workers\u2019 annotation behavior \u2013 i.e., some workers were more likely to classify certain moths as butterflies and vice versa. To create our hypothesis classH, we randomly sampled 15 hypotheses from these. Additionally, we fitted a linear classifier that best separates the classes and used it as target hypothesis h\u2217, shown in red in Fig. 3(b). The few examples in X that disagreed with h\u2217 were removed from our teaching set, to ensure realizability.\nTeaching the rest of the crowd: The teacher then uses this embedding and hypotheses in order to teach the rest of the crowd. We emphasize that \u2013 crucially \u2013 the embedding is not required for test images. Neither the workers nor the system used any information about sub-species in the images.\n3CUBAM: https://github.com/welinder/cubam"}, {"heading": "5.3. Endangered Woodpecker Bird Species", "text": "Dataset images X : Our third classification task is inspired from the eBird citizen science project (Sullivan et al., 2009) and the goal of this task is to identify birds belonging to an endangered species of woodpeckers. We used a collection of 150 real images belonging to three species of woodpeckers from a publicly available dataset (Wah et al., 2011), with one endangered species: i) Red-cockaded woodpecker and other two species belonging to the least-concerned category: ii) Red-bellied woodpecker, iii) Downy woodpecker. On this dataset, the task is to classify whether a given image contains a red-cockaded woodpecker or not. We used 80 of these images (40 per red-cockaded, and 20 each per the other two species of least-concerned category) for teaching (i.e., dataset X ). We also created a testing set of 20 images (10 for red-cockaded, and 5 each for the other two species).\nCrowd-embedding of X : We need to infer an embedding and hypothesis space of the teaching set for our teaching process. While an approach similar to the one used for the BM task is applicable here as well, we considered an alternate option of using metadata associated with these images, elicited from the crowd, as further explained below.\nEach image in this dataset is annotated with 312 binary attributes, for example, has forehead color:black, or has bill length:same as head, through workers on MTurk. The features can take values {+1, -1, 0} indicating the presence or absence of an attribute, or uncertainty (when the annotator is not sure or the answer cannot be inferred from the image given). Hence, this gives us an embedding of the data in R312. To further reduce the dimensionality of the feature space, we pruned the features which are not informative enough for the woodpecker species. We considered all the species of woodpeckers present in the dataset (total of 6), simply computed the average number of times a given species is associated positively with a feature,\nand then looked for features with maximal variance among the various species. By applying a simple cutoff of 60 on the variance, we picked the top d = 13 features as shown in Fig 3(c), also listing the average number of times the feature is associated positively with the three species.\nHypothesis class H: We considered a simple set of linear hypotheses h(x) = wTx for w \u2208 {+1, 0,\u22121}d, which place a weight of {+1, 0, -1} on any given feature and passing through the origin. The intuition behind these simple hypotheses is to capture the cues that workers could possibly use or learn for different features: ignoring a feature (0), using it as a positive signal (+1), and using it as a negative signal (\u22121). Another set of simple hypotheses that we explored are conjunctions and disjunctions of these features that can be created by setting the appropriate offset factor bh (Anthony et al., 1992). Assuming that workers focus only on a small set of features, we considered sparse hypotheses with non-zero weight on only a small set of features. To obtain the target hypothesis, we enumerated all possible hypotheses that have non-zero weight for at most three features. We then picked as h\u2217 the hypothesis with minimal error on X (shown in Fig 3(c)). Again, we pruned the few examples inX which disagreed withh\u2217 to ensure realizability. As hypothesis class H, we considered all hypotheses with a non-zero weight for at most two features along with the target h\u2217, resulting in a hypothesis class of size 339.\nTeaching the rest of the crowd: Given this embedding and hypothesis class, the teacher then uses the same approach as two previous datasets to teach the rest of the crowd. Importantly, this embedding is not required for test images."}, {"heading": "6. Experimental Results", "text": "Now we present our experimental results, consisting of simulations and actual annotation tasks on MTurk.\nMetrics and baselines: Our primary performance metric is the test error (avg. classification error of the learners), of simulated or MTurk workers on a hold-out test data set. We compare STRICT against two baseline teachers: Random (picking uniformly random examples), and SetCover (the\nclassical noise-free teaching model introduced in Section 3)."}, {"heading": "6.1. Results on Simulated Learners", "text": "We start with simulated learners and report results only on the VW dataset here for brevity. The simulations allow us to control the problem (parameters of learner, size of hypothesis space, etc.), and hence gain more insight into the teaching process. Additionally, we can observe how robust our teaching algorithm is against misspecified parameters.\nTest error. We simulated 100 learners with varying \u03b1 parameters chosen randomly from the set {2, 3, 4} and different initial hypotheses of the learners, sampled fromH. We varied the experimental setting by changing the size of the hypothesis space and the\u03b1 value used by STRICT. Fig. 4(a) reports results with \u03b1 = 2 for STRICT and size of hypothesis class 96 (2 hypotheses per each of the eight clusters, described in Section 5 for the VW dataset.\nHow robust is STRICT for a mismatched \u03b1? In realworld annotation tasks, the learner\u2019s \u03b1 parameter is not known. In this experiment, we vary the \u03b1 values used by the teaching algorithm STRICT against three learners with values of\u03b1 = 1, 2 and 3. Fig. 4(b) shows that a conservative teacher using\u03b1 bounded in the range 1 to 5 performs as good as the one knowing the true \u03b1 value.\nOn the difficulty level of teaching. Fig. 4(c) shows the difficulty of examples picked by different algorithms during the process of teaching, where difficulty is measured in terms of expected uncertainty (entropy) that a learner would face for the shown example, assuming that the expectation is taken w.r.t. the learners current posterior distribution over the hypotheses. SetCover starts with difficult examples assuming that the learner is perfect. STRICT starts with easy examples, followed by more difficult ones, as also illustrated in the experiments in Fig. 5(a). Recent results of Basu & Christensen (2013) show that such curriculum-based learning (where the difficulty level of teaching increases with time) indeed is a useful teaching mechanism. Note that our teaching process inherently incorporates this behavior, without requiring explicit heuristic choices. Also, the transition of SetCover to easier examples is just an artifact as SetCover\nrandomly starts selecting examples once it (incorrectly) infers that the learner has adopted the target hypothesis. The difficulty can be easily seen when comparing the examples picked by SetCover and STRICT in Fig. 5(a)."}, {"heading": "6.2. Results on MTurk Workers", "text": "Next, we measure the performance of our algorithms when deployed on the actual MTurk platform.\nGenerating the teaching sequence. We generate sequences of teaching examples for STRICT, as well as Random and SetCover. We used the feature spaces X and hypothesis spaces H as explained in Section 5. We chose \u03b1 = 2 for our algorithm STRICT. To better understand the execution of the algorithms, we illustrate the examples picked by our algorithm as part of teaching, shown in Fig. 5(a). We further show these examples in the 2-D embedding for the VW and BM datasets in Figs. 3(a) and 3(b).\nWorkers on MTurk and the teaching task. We recruited workers from the MTurk platform by posting the tasks on the MTurk platform. Workers were split into different control groups, depending on the algorithm and the length of teaching used (each control group corresponds to a point in the plots of Fig. 5). Fig. 1 provides a high level overview of how the teaching algorithm interacted with the worker. Teaching is followed by a phase of testing examples without providing feedback, for which we report the classification error. For the VW dataset, a total of 780 workers participated (60 workers per control group). For BM, a total of 300 workers participated, and 520 participated in the WP task. The length of the teaching phase was varied as shown in Fig. 5. The test phase was set to 10 examples for the VW and BM tasks, and 16 examples for the WP task. The workers were\ngiven a fixed payment for participation and completion, additionally a bonus payment was reserved for the top 10% performing workers within each control group. Does teaching help? Considering the worker\u2019s test set classification performance in Fig. 5, we can consistently see an accuracy improvement as workers classify unseen images. This aligns with the results from simulated learners and shows that teaching is indeed helpful in practice. Furthermore, the improvement is monotonic w.r.t. the length of teaching phase used by STRICT. In order to understand the significance of these results, we carried out Welch\u2019s t-test comparing the workers who received teaching by STRICT to the control group of workers without any teaching. The hypothesis that STRICT significantly improves the classification accuracy has two-tailed p-values of p < 0.001 for VW and WP tasks, and p = 0.01 for the BM task.\nDoes our teaching algorithm outperform baselines? Fig. 5 demonstrates that our algorithm STRICT outperforms both Random and SetCover teaching qualitatively in all studies. We check the significance by performing a paired-t test, by computing the average performance of the workers in a given control group and pairing the control groups with same length of teaching for a given task. For the VW task, STRICT is significantly better than SetCover and Random (at p = 0.05 and p = 0.05). For WP, STRICT is significantly better than SetCover (p = 0.002) whereas comparing with Random, the p-value is p = 0.07."}, {"heading": "7. Conclusions", "text": "We proposed a noise-tolerant stochastic model of the workers\u2019 learning process in crowdsourcing classification tasks. We then developed a novel teaching algorithm STRICT\nthat exploits this model to teach the workers efficiently. Our model generalizes existing models of teaching in order to increase robustness. We proved strong theoretical approximation guarantees on the convergence to a desired error rate. Our extensive experiments on simulated workers as well as on three real annotation tasks on the Mechanical Turk platform demonstrate the effectiveness of our teaching approach.\nMore generally, our approach goes beyond solving the problem of teaching workers in crowdsourcing services. With the recent growth of online education and tutoring systems 4, algorithms such as STRICT can be envisioned to aid in supporting data-driven online education (Weld et al., 2012; Dow et al., 2013)."}, {"heading": "A. Supplementary Material", "text": "A.1. Proofs\nProof of Proposition 1. We reduce from set cover. Suppose we are given a collection of finite sets S1, . . . , Sn jointly covering a set W . We reduce the problem of finding a smallest subcollection covering W to the teaching problem with the special case \u03b1 =\u221e.\nLetH = W \u222a {h\u2217}, that is, each element in W is a hypothesis that misclassifies at least one data point. We use a uniform prior p(h) = 1|W |+1 . For each set Sj , we create a teaching example xj . The label output by hypothesis h(xj) = 1 iff h \u2208 Sj , otherwise h(xj) = \u22121. We set h\u2217(x) = \u22121 for all examples. Thus, selecting Si in the set cover problem is equivalent to selecting example xi. It is easy to see that constructing the examples can be done in polynomial (in fact, linear) time.\nThe expected error after showing a set of examples is less than 1(|W |+1)n if and only if sets indexed by A cover W . Thus, if we could efficiently find the smallest setA achieving error less than 1(|W |+1)n , we could efficiently solve set cover.\nBefore proving the main theorems, we state an important lemma that will be needed throughout the analysis.\nLemma 1. Assume that the learner\u2019s current hypothesis ht is governed by the stochastic process described in Section 3. Then, the marginal distribution of ht is given by Pt\u22121(h) in every time step t.\nProof. Let the marginal distribution of ht denoted by P \u2032t\u22121(h). We will show by induction that for every t, P \u2032 t = Pt.\nObviously, P \u20320 = P0 by definition. Now, as for the induction hypothesis, let us assume that P \u2032 t\u22121 = Pt\u22121. By the definition of the stochastic process we have\nP \u2032t (h) = 1\nZ \u2032t\n( P \u2032t\u22121(h)I{yt = h(xt)|h, xt}+ Pt(h)I{yt 6= h(xt)|h, xt} ) = 1\nZ \u2032t (Pt\u22121(h)I{yt = h(xt)|h, xt}+ Pt\u22121(h)P (yt|h, xt)I{yt 6= h(xt)|h, xt})\n= 1\nZ \u2032t Pt\u22121(h) (I{yt = h(xt)|h, xt}+ P (yt|h, xt)I{yt 6= h(xt)|h, xt})\n= 1\nZ \u2032t Pt\u22121(h)P (yt|h, xt)I{yt 6=h(xt)|h,xt} = Pt(h) ,\nas stated.\nProof of Theorem 1. Clearly, F (A) can be written as\nF (A) = \u2211 h\u2208H P0(h)Gh(A) err(h, h \u2217) ,\nwhere\nGh(A) = 1\u2212 \u220f x\u2208A\ny(x) 6=sgn(h(x))\nP (y(x)|h, x) .\nIt is easy to see thatGh(A) is submodular for every h \u2208 H. Thus, F (A) is also submodular.\nLet us start to upper bound the expected error of the learner. For that, we need the following simple observation:\nP (h|A) P (h\u2217|A) = Q(h|A) Q(h\u2217|A) = Q(h|A) P0(h\u2217) .\nNow for the upper bounding: \u2211 h\u2208H P (h|A) err(h, h\u2217) \u2264 \u2211 h\u2208H P (h|A) P (h\u2217|A) err(h, h\u2217)\n= 1\nP0(h\u2217) \u2211 h\u2208H Q(h|A) err(h, h\u2217)\n= 1\nP0(h\u2217) (E \u2212 F (A)) ,\nwhere E = \u2211 h\u2208H P0(h) err(h, h\n\u2217) is an upper bound on the maximum of F (A). This means that if we choose a subset A such that F (A) \u2265 E \u2212 P0(h\u2217) , it guarantees an expected error less than . In the following, we assume that F (X ) \u2265 E \u2212 P0(h\u2217) /2. If this assumption is violated, the Theorem still holds, but the bound is meaningless, since OPT(P0(h \u2217) /2) =\u221e in this case.\nSince F (A) is submodular (and monotonic), we can achieve E \u2212 P0(h\u2217) \u201clevel\u201d with the greedy algorithm, as described below. We use the following result of the greedy algorithm for maximizing submodular functions:\nTheorem (Krause & Golovin (2014), based on Nemhauser et al. (1978)). Let f be a nonnegative monotone submodular function and let St denote the set chosen by the greedy maximization algorithm after t steps. Then we have\nf(S`) \u2265 ( 1\u2212 e\u2212`/k )\nmax S:|S|=k f(S)\nfor all integers k and l.\nLet k\u2217 be the cardinality of the smallest setA\u2217 such that F (A\u2217) \u2265 E \u2212 P0(h\u2217) /2. Thus we know that\nmax A:|A|=k\u2217\nF (A) \u2265 E \u2212 P0(h\u2217) /2 .\nNow we set ` = k\u2217 log 2EP0(h\u2217) and we denoteA` the result of the greedy algorithm after ` steps, and we get\nF (A`) \u2265 ( 1\u2212 e\u2212l/k \u2217 )( E \u2212 P0(h \u2217)\n2 ) = ( 1\u2212 P0(h \u2217)\n2E\n)( E \u2212 P0(h \u2217)\n2 ) \u2265 E \u2212 p(h\u2217) ,\nproving that running the greedy algorithm for ` steps achieves the desired result.\nProof of Theorem 2. We introduce a randomized teaching policy called Relaxed-Greedy Teaching Policy (sketched in Policy 2) and prove that with positive probability, the policy reduces the learner error exponentially. Then, we use the standard probabilistic argument: positive probability of the above event implies that there must exist a sequence of examples that reduce the learner error exponentially. We finish the proof of the theorem by using the result of Theorem 1.\nBased on our model, the way the learner updates his/her belief after showing example xt \u2208 X and receiving answer yt = sgn(h \u2217(xt)) is as follows:\nPt+1(h) = 1\nZt Pt(h)w\n(1\u2212\u03bet(h))/2 l ,\nwhere \u03bet(h) = sgn(h(xt)) \u00b7 yt, the term Zt is the normalization factor, and 0 < wl < 1 is a parameter by which the learner decreases the weight of inconsistent hypotheses. Note thatwl may very well depend on the examples shown, i.e., for hard exampleswl is typically larger than those of the easy ones as the learner is more certain about his/her answers. However, here we assume thatwl \u2264 wo < 1 and thatwo is known to the teacher. In other words, the teacher knows the minimum weight updates imposed by the learner on inconsistent hypotheses. As a result, the teacher can track Pt+1(h) conservatively as follows:\nP (t) t+1(h) =\n1\nZt P\n(t) t (h)w (1\u2212\u03bet(h))/2 o . (3)\nPolicy 2 Relaxed-Greedy Teaching Policy (RGTP) 1: Input: examplesX , hypothesisH, prior P0, error . 2: t = 0, P (t)0 (h) = P0 3: while 1\u2212 P (t)t (h\u2217) > do 4: if there exists two neighboring polytopesP andP \u2032 s.t. \u2211 h P (t) t (h)h(P) > 0 and \u2211 h P (t) h (t)h(P \u2032) < 0 then\n5: select xt uniformly at random fromP orP \u2032. 6: else 7: select xt from polytopP = arg minP\u2208\u03a0 | \u2211 h P (t) t (h)h(P)| 8: end if 9: \u2200h \u2208 H update P (t)t+1(h) according to (3) and t\u2192 t+ 1.\n10: end while\nTheorem 3. Let H be a collection of n linear separators and choose an 0 < < 1. Then, under the condition that X is m-rich, RGTP guarantees to achieve\nPr(1\u2212 Pm(h\u2217) > ) < (1\u2212 )(1\u2212 p0(h\u2217))\n\u00b7 p0(h\u2217) e\u2212m(1\u2212wo)/4,\nby showing m examples in total. In other words, to have Pr(1 \u2212 Pm(h\u2217) > ) < \u03b4, RGTP uses at most the following number of examples:\nm = 4\n1\u2212 wo log (1\u2212 )(1\u2212 p0(h\u2217)) \u03b4 \u00b7 \u00b7 p0(h\u2217) .\nThe above theorem requires that X gets a richer space for obtaining better performance. When we have a uniform prior P0 = 1/n, the the above bounds simplify to\nm = 4\n1\u2212 wo log (1\u2212 )n \u03b4 \u00b7 .\nAs at least log n queries is required to identify the correct hypothesis with probability one, the above bound is within a constant factor from log n for fixed and \u03b4.\nThe proof technique is inspired by (Burnashev & Zigangirov, 1974), (Karp & Kleinberg, 2007), and in particular beautiful insights in (Nowak, 2011). To analyze RGTP let us define the random variable\n\u03b7 (l) t = 1\u2212 Pt(h\u2217) Pt(h\u2217) .\nThis random variable log(\u03b7t) was first introduced by (Burnashev & Zigangirov, 1974) in order to analyze the classic binary search under noisy observations (for the ease of exposure we use \u03b7t instead of log(\u03b7t)). It basically captures the probability mass put on the incorrect hypothesis after t examples. Similarly, we can define\n\u03b7 (t) t = 1\u2212 P (t)t (h\u2217) p\n(t) t (h\n\u2217) .\nA simple fact to observe is the following lemma.\nLemma 2. For any sequence of examples/labels {(xt, yt)}t\u22650, and as long as 0 \u2264 wl \u2264 wo \u2264 1 we have \u03b7(l)t \u2264 \u03b7 (t) t .\nNote that RGTP is a randomized algorithm. Using Markov\u2019s inequality we obtain\nPr(1\u2212 Pm(h\u2217)) > ) \u2264 Pr(1\u2212 P (t)h\u2217 (h)) > )\n= Pr ( \u03b7t >\n1\u2212 ) \u2264 (1\u2212 )E(\u03b7 (t) t ) .\nThe above inequalities simply relates the probability we are looking for in Theorem 2 to the expected value of \u03b7(t)t . Hence, if we can show that the expected value decreases exponentially fast, we are done. To this end, let us first state the following observation.\nLemma 3. For any sequence of examples/labels {(xt, yt)}t\u22650, and for 0 \u2264 wo < 1 the corresponding random variable {\u03b7(t)t }t\u22650 are all non-negative and decreasing, i.e.,\n0 \u2264 \u03b7(t)s \u2264 \u03b7 (t) t \u2264 1\u2212 P0(h\u2217) P0(h\u2217) , s \u2265 t.\nThe above lemma simply implies that the sequence {\u03b7t}t\u22650 converges. However, it does not indicate the rate of convergence. Let us define Ft = \u03c3(P (t)0 , p (t) 1 , . . . , p (t) t ) the sigma-field generated by random variables P (t) 0 , p (t) 1 , . . . , p (t) t . Note that \u03b7 (t) t is a function of p(t)t thusFt-measurable. Now, by using the towering property of the expectation we obtain\nE(\u03b7(t)t ) = E((\u03b7 (t) t /\u03b7 (t) t\u22121)\u03b7 (t) t\u22121) = E(E((\u03b7 (t) t /\u03b7 (t) t\u22121)\u03b7 (t) t\u22121|Ft\u22121))\nSince \u03b7(t)t\u22121 isFt\u22121-measurable we get\nE(\u03b7(t)t ) = E(\u03b7 (t) t\u22121E((\u03b7 (t) t /\u03b7 (t) t\u22121)|Ft\u22121))\n\u2264 E(\u03b7(t)t\u22121) maxFt\u22121 E((\u03b7(t)t /\u03b7 (t) t\u22121)|Ft\u22121).\nThe above inequality simply implies that\nE(\u03b7(t)t ) = 1\u2212 P0(h\u2217) P0(h\u2217)\n( max\n0\u2264s\u2264t\u22121 max Fs\nE((\u03b7(t)s+1/\u03b7 (t) s )|Fs)\n)t (4)\nIn the remaining of the proof we derive a uniform upper bound (away from 1) onE((\u03b7(t)t /\u03b7 (t) t\u22121)|Ft\u22121), which readily implies exponential decay on Pr(1\u2212 Pm(h\u2217) > ) as the number of samplesm grows. For the ease of presentation, let us define the (weighted) proportion of hypothesis that agree with yt as follows:\n\u03b4t = 1\n2\n( 1 +\n\u2211 h P (t) t (h)\u03bei(h)\n) .\nAlong the same line, we define the proportion of hypothesis that predict + on polytopeP as follows\n\u03b4+P = 1\n2\n( 1 +\n\u2211 h P (t) t (h)h(P)\n) .\nNow, we can easily relate \u03b4t to the normalization factorZt:\nZt = \u2211 h P (t) t (h)w (1\u2212\u03bei(h))/2 o = (1\u2212 \u03b4t)wo + \u03b4t.\nAs a result\nP (t) t+1(h) =\nPt(h)w (1\u2212\u03bei(h))/2 o\n(1\u2212 \u03b4t)wo + \u03b4t .\nIn particular for P (t)t+1(h \u2217) we have\nP (t) t+1(h\n\u2217) = Pt(h)\n(1\u2212 \u03b4t)wo + \u03b4t .\nTo simplify the notation, we define \u03b3t = (1\u2212 \u03b4t)wo + \u03b4t.\nHence,\n\u03b7 (t) t+1 \u03b7 (t) s = \u03b3t \u2212 P (t)t (h\u2217) 1\u2212 P (t)t (h\u2217) .\nNote that since P (t)t (h \u2217) isFt-measurable the above equality entails that\nE\n( \u03b7\n(t) t+1 \u03b7 (t) t |Ft\n) =\nE(\u03b3t|Ft)\u2212 P (t)t (h\u2217) 1\u2212 P (t)t (h\u2217) .\nThus we need to show that E(\u03b3t|Ft) is bounded away from 1. To this end, we borrow the following geometric lemma from (Nowak, 2011).\nLemma 4. LetH consists of a set of linear separators where each induced polytope P \u2208 \u03a0 contains at least one example x \u2208 X . Then for any probability distribution p onH one of the following situations happens\n1. either there exists a polytopeP such that \u2211 h p(h)h(P) = 0, or 2. there exists a pair of neighboring polytopesP andP \u2032 such that \u2211 h p(h)h(P) > 0 and \u2211 h p(h)h(P \u2032) < 0.\nThe above lemma essentially characterizes Ham Sandwich Theorem (Lo et al., 1994) in discrete domain X that is 1-rich. In words, Lemma 4 guarantees that either there exists a polytope where (weighted) hypothesis greatly disagree, or there are two neighboring polytopes that are bipolar. In either case, if an example is shown randomly from these polytopes, it will be very informative. This is essentially the reason why RGTP performs well.\nNow, letPt be the polytope from which the example xt is shown. Then, based on yt we have two cases:\n\u2022 if yt = + then \u03b3+t . = \u03b3t = (1\u2212 \u03b4+Pt)wo + \u03b4 + Pt , \u2022 if yt = \u2212 then \u03b3\u2212t . = \u03b3t = \u03b4 + Ptwo + 1\u2212 \u03b4 + Pt .\nNote that for any xt picked by RGTP we have 0 < \u03b4+Pt < 1, since it never shows an example that all hypothesis agree on. As a result, both \u03b3+t and \u03b3 \u2212 t are between 0 and 1.\nBased on Lemma 4 there are only two cases. Let us define the auxiliary random variable st that simply indicate in which case we are. More precisely, st = 1 indicates that we are in case 1 and st = 2 indicates that we are in case 2. To be formal we define Gt = \u03c3(P (t)0 , p (t) 1 , . . . , p (t) t , st). Note that Ft \u2282 Gt and thus E(\u03b3t|Ft) = E(E(\u03b3t|Gt)|Ft). We need to prove the following technical lemma.\nLemma 5.\nE(\u03b3t|Gt)\n\u2264 max\n{ 3 + wo\n4 , 1 + wo 2\n, 1\u2212 (1\u2212 wo)(1\u2212 P (t) t (h \u2217))\n2\n} .\nProof. Let us first condition on st = 1. Then, RGTP chooses an xt \u2208 Pt in which case \u03b4+Pt = 1/2 and results in \u03b3+t = \u03b3 \u2212 t = (wo + 1)/2. Hence, given st = 1, we have\nE(\u03b3t|Gt) = (wo + 1)/2. (5)\nThe conditioning on st = 2 is a little bit more elaborate. Recall that in this case RGTP randomly chooses one of P and P \u2032. Note that \u03b4+P > 1/2 and \u03b4 + P\u2032 < 1/2. Now we encounter 4 possibilities:\n1. h\u2217(P) = h\u2217(P \u2032) = +: condition on st = 2 we have\nE(\u03b3t|Gt) = \u03b3+t + \u03b3 \u2212 t\n2\n\u2264 1 + (1\u2212 \u03b4+P\u2032)wo + \u03b4 + P\u2032\n2\n\u2264 3 + wo 4\n(6)\nwhere we used the fact that \u03b3+t \u2264 1 and (1\u2212 \u03b4+P\u2032)wo + \u03b4 + P\u2032 is an increasing function of \u03b4 + P\u2032 and that \u03b4 + P\u2032 < 1/2.\n2. h\u2217(P) = h\u2217(P \u2032) = \u2212: similar argument as above shows that\nE(\u03b3t|Gt) \u2264 3 + wo\n4 .\n3. h\u2217(P) = \u2212, h\u2217(P \u2032) = +: In this case we have\nE(\u03b3t|Gt) = \u03b3+t + \u03b3 \u2212 t\n2\n= \u03b4+Pwo + 1\u2212 \u03b4 + P + (1\u2212 \u03b4 + P\u2032)wo + \u03b4 + P\u2032\n2\n= 1\u2212 1\u2212 wo 2 (1 + \u03b4+P \u2212 \u03b4 + P\u2032) \u2264 1 + wo 2\n(7)\nwhere we used the fact 0 \u2264 \u03b4+P \u2212 \u03b4 + P\u2032 \u2264 1.\n4. h\u2217(P) = +, h\u2217(P \u2032) = \u2212: since P and P \u2032 are neighboring polytopes, h\u2217 should be the common face. Hence, we have \u03b4+P \u2212 \u03b4 + P\u2032 = P (t) t (h \u2217). As a result\nE(\u03b3t|Gt) = \u03b3+t + \u03b3 \u2212 t\n2\n= (1\u2212 \u03b4+P )wo + \u03b4 + P + \u03b4 + P\u2032wo + 1\u2212 \u03b4 + P\u2032\n2\n= 1 + \u03b4+P \u2212 \u03b4 + P\u2032 + wo(1\u2212 \u03b4 + P + \u03b4 + P\u2032)\n2\n\u2264 1\u2212 (1\u2212 wo)(1\u2212 P (t) t (h \u2217))\n2 . (8)\nBy combining (6), (7) and (8) we prove the lemma.\nLemma 5 readily implies that\nE\n( \u03b7\n(t) t+1 \u03b7 (t) t |Ft\n) =\nE(\u03b3t|Ft)\u2212 P (t)t (h\u2217) 1\u2212 P (t)t (h\u2217)\n\u2264 3 + wo 4 .\nHence,\nE(\u03b7(t)t ) = 1\u2212 P0(h\u2217) P0(h\u2217)\n( 1\u2212 1\u2212 wo\n4 )t \u2264 1\u2212 P0(h \u2217)\nP0(h\u2217) exp(\u2212t \u00b7 (1\u2212 wo)/4)\nThis finishes the proof of Theorem 3. Now, to finish the proof of Theorem 2, we just set \u03b4 to 1/2 and use the probabilistic argument mentioned in the beginning of the proof, resulting in an upper bound on OPT. Since we use the logistic likelihood function,wo can be bounded by 12 . Theorem 1 follows."}], "references": [{"title": "On exact specification by examples", "author": ["Anthony", "Martin", "Brightwell", "Graham", "Cohen", "Dave", "Shawe-Taylor", "John"], "venue": "In Proceedings of COLT,", "citeRegEx": "Anthony et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Anthony et al\\.", "year": 1992}, {"title": "Recent developments in algorithmic teaching", "author": ["F.J. Balbach", "T. Zeugmann"], "venue": "In Proceedings of the 3rd International Conference on Language and Automata Theory and Applications, pp", "citeRegEx": "Balbach and Zeugmann,? \\Q2009\\E", "shortCiteRegEx": "Balbach and Zeugmann", "year": 2009}, {"title": "Teaching classification boundaries to humans", "author": ["S. Basu", "J. Christensen"], "venue": "In Proceedings of the 27th Conference on Artificial Intelligence,", "citeRegEx": "Basu and Christensen,? \\Q2013\\E", "shortCiteRegEx": "Basu and Christensen", "year": 2013}, {"title": "An interval estimation problem for controlled observations", "author": ["M.V. Burnashev", "K.S. Zigangirov"], "venue": "In Proceedings of Problems of Information Transmission,", "citeRegEx": "Burnashev and Zigangirov,? \\Q1974\\E", "shortCiteRegEx": "Burnashev and Zigangirov", "year": 1974}, {"title": "Aggregating crowdsourced binary ratings", "author": ["N. Dalvi", "A. Dasgupta", "R. Kumar", "V. Rastogi"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "Dalvi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2013}, {"title": "Recursive teaching dimension, learning complexity, and maximum classes", "author": ["T. Doliwa", "H.U. Simon", "S. Zilles"], "venue": "In Proceedings of the 21st international conference on Algorithmic learning theory,", "citeRegEx": "Doliwa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Doliwa et al\\.", "year": 2010}, {"title": "A pilot study of using crowds in the classroom", "author": ["S. Dow", "E. Gerber", "A. Wong"], "venue": "In Proceedings of Human Factors in Computing Systems (CHI),", "citeRegEx": "Dow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dow et al\\.", "year": 2013}, {"title": "Active teaching for inductive learners", "author": ["Du", "Jun", "Ling", "Charles X"], "venue": "In Proceedings of the Eleventh SIAM International Conference on Data Mining,", "citeRegEx": "Du et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Du et al\\.", "year": 2011}, {"title": "On the complexity of teaching", "author": ["S.A. Goldman", "M.J. Kearns"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Goldman and Kearns,? \\Q1992\\E", "shortCiteRegEx": "Goldman and Kearns", "year": 1992}, {"title": "Noisy binary search and its applications", "author": ["Karp", "Richard M", "Kleinberg", "Robert"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Karp et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Karp et al\\.", "year": 2007}, {"title": "Submodular function maximization. In Tractability: Practical Approaches to Hard Problems (to appear)", "author": ["Krause", "Andreas", "Golovin", "Daniel"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2014}, {"title": "Optimizing instructional policies", "author": ["Lindsey", "Robert", "Mozer", "Michael", "Huggins", "William J", "Pashler", "Harold"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lindsey et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lindsey et al\\.", "year": 2013}, {"title": "Algorithms for ham-sandwich cuts", "author": ["Lo", "Chi-Yuan", "Matou\u0161ek", "Ji\u0159\u0131", "Steiger", "William"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Lo et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Lo et al\\.", "year": 1994}, {"title": "An analysis of the approximations for maximizing submodular set functions", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "Nemhauser et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "The geometry of generalized binary search", "author": ["Nowak", "Robert D"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nowak and D.,? \\Q2011\\E", "shortCiteRegEx": "Nowak and D.", "year": 2011}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Utility data annotation with amazon mechanical turk", "author": ["A. Sorokin", "D. Forsyth"], "venue": "In First IEEE Workshop on Internet Vision,", "citeRegEx": "Sorokin and Forsyth,? \\Q2008\\E", "shortCiteRegEx": "Sorokin and Forsyth", "year": 2008}, {"title": "ebird: A citizen-based bird observation network in the biological sciences", "author": ["Sullivan", "Brian L", "Wood", "Christopher L", "Iliff", "Marshall J", "Bonney", "Rick E", "Fink", "Daniel", "Kelling", "Steve"], "venue": "Biological Conservation,", "citeRegEx": "Sullivan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sullivan et al\\.", "year": 2009}, {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,", "citeRegEx": "Wah et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wah et al\\.", "year": 2011}, {"title": "Personalized online education a crowdsourcing challenge", "author": ["Weld", "D. S", "E. Adar", "L. Chilton", "R. Hoffmann", "E. Horvitz"], "venue": "Workshop on Human Computation,", "citeRegEx": "Weld et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weld et al\\.", "year": 2012}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": "In Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Machine teaching for bayesian learners in the exponential family", "author": ["Zhu", "Xiaojin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhu and Xiaojin.,? \\Q2013\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2013}, {"title": "Models of cooperative teaching and learning", "author": ["S. Zilles", "S. Lange", "R. Holte", "M. Zinkevich"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zilles et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zilles et al\\.", "year": 2011}, {"title": "level\u201d with the greedy algorithm, as described below. We use the following result of the greedy algorithm for maximizing submodular functions: Theorem (Krause", "author": ["Nemhauser"], "venue": null, "citeRegEx": "Nemhauser,? \\Q2014\\E", "shortCiteRegEx": "Nemhauser", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Machine learning, AI, and citizen science systems can hugely benefit from the use of these services as large-scale annotated data is often of crucial importance (Snow et al., 2008; Sorokin & Forsyth, 2008; Lintott et al., 2008).", "startOffset": 161, "endOffset": 227}, {"referenceID": 20, "context": "Most of the work so far has focused on methods for combining labels from many annotators (Welinder et al., 2010; Gomes et al., 2011; Dalvi et al., 2013) or in designing control measures by estimating the worker\u2019s reliabilities through \u201cgold standard\u201d questions (Snow et al.", "startOffset": 89, "endOffset": 152}, {"referenceID": 4, "context": "Most of the work so far has focused on methods for combining labels from many annotators (Welinder et al., 2010; Gomes et al., 2011; Dalvi et al., 2013) or in designing control measures by estimating the worker\u2019s reliabilities through \u201cgold standard\u201d questions (Snow et al.", "startOffset": 89, "endOffset": 152}, {"referenceID": 15, "context": ", 2013) or in designing control measures by estimating the worker\u2019s reliabilities through \u201cgold standard\u201d questions (Snow et al., 2008).", "startOffset": 116, "endOffset": 135}, {"referenceID": 17, "context": "In particular, we consider the task of classifying animal species, an important component in several citizen science projects such as the eBird project (Sullivan et al., 2009).", "startOffset": 152, "endOffset": 175}, {"referenceID": 22, "context": "More recent work (Balbach & Zeugmann, 2009; Zilles et al., 2011; Doliwa et al., 2010; Du & Ling, 2011) consider models of interactive teaching, where the teacher, after showing each example, obtains feedback about the hypothesis that the learner is currently implementing.", "startOffset": 17, "endOffset": 102}, {"referenceID": 5, "context": "More recent work (Balbach & Zeugmann, 2009; Zilles et al., 2011; Doliwa et al., 2010; Du & Ling, 2011) consider models of interactive teaching, where the teacher, after showing each example, obtains feedback about the hypothesis that the learner is currently implementing.", "startOffset": 17, "endOffset": 102}, {"referenceID": 5, "context": ", 2011; Doliwa et al., 2010; Du & Ling, 2011) consider models of interactive teaching, where the teacher, after showing each example, obtains feedback about the hypothesis that the learner is currently implementing. Such feedback can be used to select future teaching examples in a more informed way. While theoretically intriguing, in this paper we focus on non-interactive models, which are typically easier to deploy in practice. Noise-tolerant teaching: In contrast to the noise-free setting, the practically extremely important noise-tolerant setting is theoretically much less understood. Very recently, Zhu (2013) investigates the optimization problem of generating a set of teaching examples that trades off between the expected future error of the learner and the \u201ceffort\u201d (i.", "startOffset": 8, "endOffset": 621}, {"referenceID": 11, "context": "Lindsey et al. (2013) propose a method for evaluating and optimizing over parametrized policies with different orderings of positive and negative examples.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Submodular functions can be effectively optimized using a greedy algorithm, which, at every iteration, adds the example that maximally increases the surrogate function F (Nemhauser et al., 1978).", "startOffset": 170, "endOffset": 194}, {"referenceID": 20, "context": "(b) shows the 2-D embedding of images for the Moth and Butterfly data set, and the hypothesis for a small set of workers, as obtained using the approach of Welinder et al. (2010). (c) shows the 13 features used for representation of woodpecker images and thewh\u2217 vector of the target hypotheses.", "startOffset": 156, "endOffset": 179}, {"referenceID": 17, "context": "Endangered Woodpecker Bird Species Dataset images X : Our third classification task is inspired from the eBird citizen science project (Sullivan et al., 2009) and the goal of this task is to identify birds belonging to an endangered species of woodpeckers.", "startOffset": 135, "endOffset": 158}, {"referenceID": 18, "context": "We used a collection of 150 real images belonging to three species of woodpeckers from a publicly available dataset (Wah et al., 2011), with one endangered species: i) Red-cockaded woodpecker and other two species belonging to the least-concerned category: ii) Red-bellied woodpecker, iii) Downy woodpecker.", "startOffset": 116, "endOffset": 134}, {"referenceID": 0, "context": "Another set of simple hypotheses that we explored are conjunctions and disjunctions of these features that can be created by setting the appropriate offset factor bh (Anthony et al., 1992).", "startOffset": 166, "endOffset": 188}, {"referenceID": 19, "context": "With the recent growth of online education and tutoring systems 4, algorithms such as STRICT can be envisioned to aid in supporting data-driven online education (Weld et al., 2012; Dow et al., 2013).", "startOffset": 161, "endOffset": 198}, {"referenceID": 6, "context": "With the recent growth of online education and tutoring systems 4, algorithms such as STRICT can be envisioned to aid in supporting data-driven online education (Weld et al., 2012; Dow et al., 2013).", "startOffset": 161, "endOffset": 198}], "year": 2014, "abstractText": "How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners. We prove that our strategy is competitive with the optimal teaching policy. Moreover, for the special case of linear separators, we prove that an exponential reduction in error probability can be achieved. Our experiments on simulated workers as well as three real image annotation tasks on Amazon Mechanical Turk show the effectiveness of our teaching algorithm.", "creator": "LaTeX with hyperref package"}}}