{"id": "1402.3891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2014", "title": "Performance Evaluation of Machine Learning Classifiers in Sentiment Mining", "abstract": "30m In recent hansen years, sacramentals the 1947/48 use movements of pestilence machine a-390 learning classifiers is westbourne of great value 100-inch in jinsen solving a 1,746 variety of harrybnytimes.com problems in text clarinex classification. Sentiment beers mining nonvirtual is 3,518 a stankevicius kind of text classification rigour in which, messages bostridge are classified irresolute according to maalaala sentiment 817 orientation cappuccinos such finds as positive or whitecoat negative. This gegenwart paper partiz\u00e1nske extends the imrb idea of 7nb evaluating \u00e9dith the banchou performance trustee of various classifiers sudhir to show their mikhail effectiveness oleifera in robusta sentiment refugess mining of 1795 online fazzini product patroller reviews. The anna-maria product reviews cancers are opar collected zubiri from brockenhurst Amazon reviews. To evaluate manzer the performance zanzibar of classifiers pakhawaj various nielson evaluation suprapto methods like random ortsteil sampling, aloisio linear hydrophobia sampling 2.30 and bootstrap sampling are used. Our results kisutu shows that support pece vector machine with bootstrap sampling method darwaza outperforms others avoir classifiers graciously and outfoxing sampling methods in metacarpals terms dessie of misclassification rate.", "histories": [["v1", "Mon, 17 Feb 2014 05:24:42 GMT  (45kb)", "http://arxiv.org/abs/1402.3891v1", "4 pages 2 tables, International Journal of Computer Trends and Technology, volume 4, Issue 6, june 2013"]], "COMMENTS": "4 pages 2 tables, International Journal of Computer Trends and Technology, volume 4, Issue 6, june 2013", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["vinodhini g chandrasekaran rm"], "accepted": false, "id": "1402.3891"}, "pdf": {"name": "1402.3891.pdf", "metadata": {"source": "CRF", "title": "Performance Evaluation of Machine Learning Classifiers in Sentiment Mining", "authors": ["G.Vinodhini", "RM.Chandrasekaran"], "emails": [], "sections": [{"heading": null, "text": "ISSN: 2231-2803 http://www.ijcttjournal.org Page 1783\nI. INTRODUCTION E-Commerce is becoming an increasingly popular today, where a consumer can buy a product online. After buying a product, consumers can post their reviews and comments about the product in internet. These reviews can be a powerful source of finding out the consumer preferences and making recommendations of products to a new consumer who desires to buy a product. There are many product review sites like Amazon, epinions.com, etc. which provide the reviews for products. If a user who wants to buy a camera, then he goes to a product review site and read the reviews. There could be many thousands of reviews for cameras. It is practically feasible for the user to read all the reviews. So, a comprehensive system is needed to analyze the reviews and find out the quality of the product. In recent years, the use of machine learning classifiers is of great value in solving a variety of problems in sentiment classification. We focus on sentiment mining which aims to discover reviewers\u2019 attitudes, whether positive or negative, of a product.\nThe major contributions and uniqueness of the work presented in this paper are as follows: We show the robustness of four of the top classifiers in data mining in terms of their\nmisclassification rate for sentiment mining.We also analyze the effectiveness of various evaluation methods like random sampling, bootstrap sampling and linear sampling on classifier performance. The following section presents a brief description of supervised learning followed by with details of one of the top four machine learning classifiers in data mining that are also used in this paper. Section 3 is about the data source used. Section 4 explores the performance of each classifier using the data source. This section also presents results of sampling methods used. Section 5 reviews some related work to the problem of sentiment classification. We close with conclusions and directions for future research.\nII. MACHINE LEARNING ALGORITHMS\nIn supervised learning, given a set of training instances and any given prior probabilities and misclassification costs, a learning algorithm outputs a classifier. The classifier is an hypothesis about the true classification function that is learned from, or fitted to, training data. The classifier is then tested on test data. A wide range of algorithms in machine learning paradigms have been developed for the task of supervised learning classification. We now discuss the four classifiers used in this work.\nA. Decision Trees\nDecision tree induction is one of the simplest forms of supervised learning algorithm. It has been extensively used in many areas such as statistics and machine learning for the purposes of classification and prediction. Decision tree (DT) classifiers can be generalise beyond the training sample so that unseen samples could be classified with as high accuracy as possible. DTs are nonparametric and a useful means of representing the logic embodied in software routines. A decision tree takes as input a case or example described by a set of attribute values, and outputs a Boolean decision [5]. In the classification case, when the response variable takes value in a set of previously defined classes the node is assigned to the class which represents the highest proportion of observations.\nISSN: 2231-2803 http://www.ijcttjournal.org Page 1784\nB. K-Nearest Neighbour\nOne of the most venerable algorithms in machine learning is the nearest neighbour (NN). The entire training set is stored in the memory. To classify a new instance, the Euclidean distance is computed between the instance and each stored training instance and the new instance is assigned the class of the nearest neighbouring instance. More generally, the k nearest neighbours are computed, and the new instance is assigned the class that is most frequent among the k neighbours [7]. To classify an unknown pattern, the k-NN approach looks at a collection of the k nearest points and uses a \u201cvoting\u201d mechanism to select between them, instead of looking at the single nearest point and classifying according to that with ties broken at random.\nC. Na\u00efve Bayes Classifier\nBayesian learning algorithms use probability theory as an approach to concept classification. Bayesian classifiers produce probabilities for class assignments, rather than a single definite classification. Na\u00efve Bayes classifier (NBC) is perhaps the simplest and most widely studied probabilistic learning method. It learns from the training data, the conditional probability of each attribute Ai, given the class label C. The strong major assumption is that all attributes Ai are independent given the value of the class C. Classification is therefore done applying Bayes rule to compute the probability of C and then predicting the class with the highest posterior probability. The assumption of conditional independence of a collection of random attributes is very critical [8].\nD. Support Vector Machines\nSupport Vector Machines (SVMs) are pattern classifiers that can be expressed in the form of hyper-planes to discriminate positive instances from negative instances. SVMs have successfully been applied to numerical tasks, including classification. They perform structural risk minimisation and identify key \"support vectors\". Risk minimisation measures the expected error on an arbitrarily large test set with the given training set and SVMs non-linearly map their n-dimensional input space into a high dimensional feature space. In this high dimensional feature space a non-linear classifier is constructed [5]][7][8]. Given a set of points which belong to either of two classes, a linear SVM finds the hyper-plane leaving the largest possible fraction of points of the same class on the same side, while maximising the distance of either class from the hyper-plane. The hyper plane is determined by a subset of the points of the two\nclasses, named support vectors, and has a number of interesting theoretical properties [10].\nIII. DATA SOURCE\nThe reviews are collected from www.amazonreviews.com for five different products. The reviews collected are in user free format. So pre- processing of review is needed before classifier is applied. Pre-processing steps, including stop words removal and word stemming, are first applied to the review documents in order to reduce the noisy information in the following processes. These pre-processing steps can improve the performance of sentiment classification. The five different product reviews analysed are camera, mobile, i-pod, laptop and music player. The number of reviews collected for each product varies from 200 to 300. The details of the data used in the analysis are listed in Table 1."}, {"heading": "Product No.of.review", "text": "After pre-processing, the reviews are represented as unordered collections of words and the features (Unigram) are modelled as a bag of words. A word vector is created using the unigram features based on the term occurrences.\nIV. EVALUATION\nIn order to empirically evaluate the performance of the four classifiers, an experiment is used on five different product reviews in terms of misclassification error rate. To perform the experiments, each data is split randomly into ten parts of equal size with 10-fold cross validation used for this task. For each fold, nine parts were placed in the training set and the remaining onewas placed in the corresponding test set . We construct the predictive models using four classifiers from the WEKA toolkit. The WEKA is an ensemble of tools for data classification, regression, clustering, association rules and visualization. The toolkit is developed in Java and is open source software. All the four classifiers are used with their default as implemented in WEKA [5][7].\nTo measure the performance of these classifiers, the misclassification rate is used. The classifier approaches are cross validated using\nISSN: 2231-2803 http://www.ijcttjournal.org Page 1785\nseveral types of sampling for building the subsets.The linear sampling simply divides the input dataset into partitions without changing the order of the examples i.e. subsets with consecutive examples are created. The random sampling builds random subsets of the input dataset. Samples are chosen randomly for making subsets.The bootstrap sampling builds random subsets and ensures that the class distribution in the subsets is the same as in the whole input dataset. In the case of a binominal classification, bootstrap sampling builds random subsets such that each subset contains the same proportions of the two values of class labels [2].\nMisclassification rate is defined as the ratio of number of wrongly classified review to the total number of reviews classified by the prediction system.\nBased on the conducted experiments, the misclassification error rate results for Decision tree, KNN,NB and SVM for various sampling methods are presented in Table 2 for five different products . From the performance measures it is observed that SVM outperforms the other algorithms in terms of overall misclassification rate. KNN is the second better performance than the other classifiers. Among the sampling methods used boot strap sampling performs significantly better than other sampling methods for all classifiers.\nV. RELATED WORK\nThe area of sentiment mining has seen a large increase in academic interest in the last few years. Researchers in the areas of natural language processing, data mining, machine learning, and others have tested a variety of methods of automating the sentiment analysis process.\nPang et al. [4] researched sentiment mining using a binary unigram representation of patterns.In this representation, training patterns are represented by the presence/absence of words instead of by the count of total word occurrences. They tested a variety of algorithms for classification and found that a support vector machine had the highest accuracy of 82.9% using a movie reviews dataset. Whitelaw et al. [11] proposed improving sentiment mining pattern representations by using appraisal groups. They define appraisal groups as \u201ccoherent groups of words that express together a particular attitude, such as \u2018extremely boring\u2019, or \u2018not really very good\u2019.\u201d By combining a standard bag-ofwords approach with appraisal groups they report a 90.2% classification accuracy. Liu et al [1] proposed a technique based on language pattern mining to identify product features from pros and cons in reviews in the form of short sentences. They also make an effort to extract implicit features. Moreover, Kang et al [3] proposed feature extraction for capturing knowledge from product\nreviews. In their method, the output of Hu\u2019s system was used as the input to their system, and the input was mapped to the user-defined taxonomy features hierarchy thereby eliminating redundancy and providing conceptual organization.\nSnyder and Barzilay [9] describe an algorithm that breaks up reviews into multiple aspects and then provides different numerical scores for each aspect. This would be helpful for mixed reviews that explicitly describe those aspects which are good or bad. For example, a movie reviewer may like a movie\u2019s acting and special effects, but find its plot poorly conceived. For feature level, it mostly focuses on the extracting and analyzing the product features, finds the commented features, furthermore to generate opinion summaries[5][6][7][8].\nVI. CONCLUSION\nThe major contributions of the paper has been the application of one of the top four machine learning algorithms to predict sentiment orientation of the review sentences and to evaluate various sampling methods on classifier performance .Five different product reviews were utilised for this task. The results suggest that the ML algorithms can be successfully applied in sentiment mining providing significant classification performance. Based on evidence, it has been found that among all classifiers (DT, k-NN, NBC and SVM), SVM with bootstrap sampling method performs better in terms of misclassification error rate. While many research continues, practitioners and researchers may apply ensemble machine learning methods for constructing the model to predict sentiment orientation. We plan to replicate our study to predict the models based on ensemble machine learning algorithms and genetic algorithms.\nISSN: 2231-2803 http://www.ijcttjournal.org Page 1786"}], "references": [{"title": "Opinion Observer: Analyzing and Comparing Opinions on the Web,\u201dProc", "author": ["B. Liu", "M. Hu", "J. Cheng"], "venue": "International World Wide Web Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "The random subspace method for constructing decision forests", "author": ["Ho", "Tin Kam"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "NLPR at Multilingual Opinion Analysis Task in NTCIR7", "author": ["Kang Liu", "Jun Zhao"], "venue": "Proceeding of NTCIT-7 workshop Meeting", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "IEEE Circuits and Systems Magazine Third Quarter:21\u201345", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Extracting Product Features and Opinions from Reviews,", "author": ["Popescu", "O Etzioni"], "venue": "Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Multiple aspect ranking using the good grief algorithm", "author": ["B. Snyder", "R. Barzilay"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Sentiment analysis: Capturing favorability using natural language processing,", "author": ["Tetsuya Nasukawa", "Jeonghee Yi"], "venue": "In Proc. of the Second International Conferences on Knowledge Capture,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Using appraisal groups for sentiment analysis", "author": ["C. Whitelaw", "N. Garg", "S. Argamon"], "venue": "In Proceedings of the 2005 ACM CIKM International Conference on Information and Knowledge Management, Bremen,Germany, October 31 - November", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "A decision tree takes as input a case or example described by a set of attribute values, and outputs a Boolean decision [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "In this high dimensional feature space a non-linear classifier is constructed [5]][7][8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "The hyper plane is determined by a subset of the points of the two classes, named support vectors, and has a number of interesting theoretical properties [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "All the four classifiers are used with their default as implemented in WEKA [5][7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "In the case of a binominal classification, bootstrap sampling builds random subsets such that each subset contains the same proportions of the two values of class labels [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "[4] researched sentiment mining using a binary unigram representation of patterns.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[11] proposed improving sentiment mining pattern representations by using appraisal groups.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Liu et al [1] proposed a technique based on language pattern mining to identify product features from pros and cons in reviews in the form of short sentences.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "Moreover, Kang et al [3] proposed feature extraction for capturing knowledge from product reviews.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Snyder and Barzilay [9] describe an algorithm that breaks up reviews into multiple aspects and then provides different numerical scores for each aspect.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "For feature level, it mostly focuses on the extracting and analyzing the product features, finds the commented features, furthermore to generate opinion summaries[5][6][7][8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "For feature level, it mostly focuses on the extracting and analyzing the product features, finds the commented features, furthermore to generate opinion summaries[5][6][7][8].", "startOffset": 165, "endOffset": 168}], "year": 2013, "abstractText": "In recent years, the use of machine learning classifiers is of great value in solving a variety of problems in text classification. Sentiment mining is a kind of text classification in which, messages are classified according to sentiment orientation such as positive or negative. This paper extends the idea of evaluating the performance of various classifiers to show their effectiveness in sentiment mining of online product reviews. The product reviews are collected from Amazon reviews. To evaluate the performance of classifiers various evaluation methods like random sampling, linear sampling and bootstrap sampling are used. Our results shows that support vector machine with bootstrap sampling method outperforms others classifiers and sampling methods in terms of misclassification rate.", "creator": null}}}