{"id": "1403.0667", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2014", "title": "The Hidden Convexity of Spectral Clustering", "abstract": "In gijon recent years, spectral beauchief clustering has become coches a elle standard fabril method for data revolucionaria analysis yearnings used in laugher a broad jaroong range of coverture applications. invariably In fugro this dryburgh paper calamai we djellaba propose mswati a diplopia new lanne class layup of corl algorithms for alertly multiway flabbergasting spectral anti-flag clustering based sissela on optimization of odu a gnn certain \" 1510 contrast reconnoitre function \" goodna over a sphere. These algorithms twill are passeier simple to huige implement, efficient and, unlike turbostar most of the shailagh existing h.r.h. algorithms for member-states multiclass spectral lhakpa clustering, are natoli not initialization - dependent. conical Moreover, cyclopedia they seshan are wsi applicable transcripts without euro191 modification for normalized 7.7375 and un - normalized clustering, which are gehe two dobos common lanco variants 306.5 of t\u00e2mpa spectral clustering.", "histories": [["v1", "Tue, 4 Mar 2014 02:48:20 GMT  (854kb,D)", "https://arxiv.org/abs/1403.0667v1", "26 pages, 3 figures"], ["v2", "Thu, 1 Oct 2015 21:59:05 GMT  (1057kb,D)", "http://arxiv.org/abs/1403.0667v2", "37 pages"], ["v3", "Wed, 4 May 2016 18:10:13 GMT  (2769kb,D)", "http://arxiv.org/abs/1403.0667v3", "22 pages"]], "COMMENTS": "26 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james r voss", "mikhail belkin", "luis rademacher"], "accepted": true, "id": "1403.0667"}, "pdf": {"name": "1403.0667.pdf", "metadata": {"source": "CRF", "title": "The Hidden Convexity of Spectral Clustering\u2217", "authors": ["James Voss", "Mikhail Belkin", "Luis Rademacher"], "emails": ["vossj@cse.ohio-state.edu", "mbelkin@cse.ohio-state.edu", "lrademac@cse.ohio-state.edu"], "sections": [{"heading": null, "text": "Geometrically, the proposed algorithms can be interpreted as hidden basis recovery by means of function optimization. We give a complete characterization of the contrast functions admissible for provable basis recovery. We show how these conditions can be interpreted as a \u201chidden convexity\u201d of our optimization problem on the sphere; interestingly, we use efficient convex maximization rather than the more common convex minimization. We also show encouraging experimental results on real and simulated data.\nkeywords: spectral clustering, convex maximization, basis recovery"}, {"heading": "1 Introduction", "text": "Partitioning a dataset into classes based on a similarity between data points, known as cluster analysis, is one of the most basic and practically important problems in data analysis and machine learning. It has a vast array of applications from speech recognition to image analysis to bioinformatics and to data compression. There is an extensive literature on the subject, including a number of different methodologies as well as their various practical and theoretical aspects [11].\nIn recent years spectral clustering\u2014a class of methods based on the eigenvectors of a certain matrix, typically the graph Laplacian constructed from data\u2014has become a widely used method for cluster analysis. This is due to the simplicity of the algorithm, a number of desirable properties it exhibits and its amenability to theoretical analysis. In its simplest form, spectral bi-partitioning is an attractively straightforward algorithm based on thresholding the second bottom eigenvector of the Laplacian matrix of a graph. However, the more practically significant problem of multiway spectral clustering is considerably more complex. While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25]. Typical algorithms for multiway spectral clustering follow a two-step process:\n\u2217A short version of this paper previously appeared in the proceedings of the Thirtieth AAAI Conference on Artificial Intelligence [20]. Implementations of the algorithms proposed in this paper can be found at https:// github.com/vossj/HBR-Spectral-Clustering. \u2020Department of Computer Science and Engineering, the Ohio State University\nar X\niv :1\n40 3.\n06 67\nv3 [\ncs .L\nG ]\n4 M\nay 2\n01 6\n1. Spectral embedding: A similarity graph for the data is constructed based on the data\u2019s feature representation. If one is looking for k clusters, one constructs the embedding using the bottom k eigenvectors of the graph Laplacian (normalized or unnormalized) corresponding to that graph.\n2. Clustering: In the second step, the embedded data (sometimes rescaled) is clustered, typically using the conventional/spherical k-means algorithms or their variations.\nIn the first step, the spectral embedding given by the eigenvectors of Laplacian matrices has a number of interpretations. The meaning can be explained by spectral graph theory as relaxations of multiway cut problems [19]. In the extreme case of a similarity graph having k connected components, the embedded vectors reside in Rk, and vectors corresponding to the same connected component are mapped to a single point. There are also connections to other areas of machine learning and mathematics, in particular to the geometry of the underlying space from which the data is sampled [4].\nWe propose a new class of algorithms for the second step of multiway spectral clustering. The starting point is that when the k clusters are perfectly separate, the spectral embedding using the bottom k eigenvectors has a particularly simple geometric form. For the unnormalized (or asymmetric normalized) Laplacian, it is simply a (weighted) orthogonal basis in k-dimensional space, and recovering the basis vectors is sufficient for cluster identification. This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications). For the symmetric normalized Laplacian, the structure is slightly more complex, but is still suitable for our analysis. Moreover, our proposed algorithms can be used without modification.\nThe proposed approach relies on an optimization problem resembling certain Independent Component Analysis techniques, such as FastICA (see [10] for a broad overview). Specifically, the problem of identifying k clusters reduces to maximizing a certain \u201cadmissible\u201d contrast function over a (k \u2212 1)-sphere. Our main theoretical contribution is to formulate a general version of the basis recovery problem arising in spectral clustering, and to characterize the set of admissible contrast functions for guaranteed recovery1 (Section 2). Rather than the more usual convex minimization, our analysis is based on convex maximization over a (hidden) convex domain. Interestingly, while maximizing a convex function over a convex domain is generally difficult (even maximizing a positive definite quadratic form over the continuous cube [0, 1]n is NP-hard2), our setting allows for efficient optimization.\nBased on this theoretical connection between clusters and local maxima of contrast functions over the sphere, we propose practical algorithms for cluster recovery through function maximization. We discuss the choice of contrast functions and provide running time analysis. We also provide a number of encouraging experimental results on synthetic and real-world data sets.\nWe also note connections to recent work on geometric recovery. Anderson et al. [1] use the method of moments to recover a continuous simplex given samples from the uniform probability distribution. Like in our work, Anderson et al. use efficient enumeration of local maxima of a function over the sphere. Also, one of the results of Hsu and Kakade [9] shows recovery of parameters in a Gaussian Mixture Model using the moments of order three, and this result can be thought of as a case of the basis recovery problem.\nThe paper is structured as follows: In Section 2, we provide our main technical results on basis recovery and briefly outline its connection to spectral clustering. In Sections 3 and 4 we\n1Interestingly, there are no analogous recovery guarantees in the ICA setting except for the special case of cumulant functions as contrasts. In particular, typical versions of FastICA are known to have spurious maxima [22].\n2This follows from [7] together with Fact 6 below.\nintroduce spectral clustering and formulate it in terms of basis learning. In Section 5 we provide the main theoretical results for basis recovery in the spectral clustering setting, and discuss algorithic implementation details. Our experimental results are given in Section 6. Finally in Section 7, we handle the deferred proof details and discuss the admissibility of normalized graph Laplacians for our framework."}, {"heading": "2 Basis Recovery and Spectral Clustering", "text": "In this section, we provide our main technical results on hidden basis recovery. Then, we briefly discuss how our results will apply to the spectral clustering setting.\nA Note on Notation. In what follows, we will use the following notations. For a matrix B, bij indicates the element in its ith row and jth column. The ith row vector of B is denoted bi \u2022 , and the jth column vector of B is denoted b \u2022 j . For a vector v, \u2016v\u2016 denotes its standard Euclidean 2-norm. Given two vectors u and v, u \u2022 v denotes their dot product. We denote the set {1, 2, . . . , k} by [k]. We denote by 1S the indicator vector for the set S, i.e. the vector which is 1 for indices in S and 0 otherwise. The null space of a matrix M is denoted N (M). We denote the unit sphere in Rd by Sd\u22121. For points p1, . . . , pm, conv(p1, . . . , pm) will denote their convex hull. All angles are given in radians, and \u2220(u,v) denotes the angle between the vectors u and v in the domain [0, \u03c0]. We use 7\u2192 to define anonymous functions; for instance t 7\u2192 t2 is the function f : R \u2192 R defined by f(t) := t2. Finally, for X a subspace of Rd, PX denotes the square matrix corresponding to the orthogonal projection from Rd to X ."}, {"heading": "2.1 Basis Recovery via Convex Maximization", "text": "The main technical results of this section deal with reconstructing a hidden basis by simple optimization techniques. For this purpose, we introduce the following class of functions.\nDefinition 1. A function F : Rd \u2192 R is said to be an orthogonal basis encoding function (orthogonal BEF) if there exists an orthonormal basis z1, . . . , zd of Rd and functions gi : R\u2192 R such that F (u) = \u2211d i=1 gi(u \u2022 zi).\nWe will assume throughout that the functions gi (and hence F ) are continuously differentiable. In this section, we provide conditions under which recovery of the hidden basis z1, . . . , zd (up to sign) can be guaranteed for an orthogonal BEF using simple function maximization techniques. To motivate our conditions, it will be useful to first consider a classic problem which fits into the orthogonal BEF framework: the eigendecomposition of positive definite symmetric matrices.\nExample 2 (Symmetric PSD Matrix Eigendecompositions). Let A be a symmetric positive semidefinite matrix with eigendecomposition A = \u2211d i=1 \u03bbiziz T i . The function FA : Rd \u2192 R defined by FA(u) := u TAu = \u2211d i=1 \u03bbi(u \u2022 zi)\n2 is an orthogonal BEF with the functions gi : R\u2192 R defined as gi(x) := \u03bbix\n2. If the eigenvalues are ordered such that \u03bb1 > \u03bb2 > . . . > \u03bbd, then the directions \u00b1z1 are the maxima (local and global) of FA on the domain Sd\u22121. Further, after \u00b1z1 is recovered, we may maximize FA in the orthogonal complement of z1 to recover \u00b1z2. This deflationary procedure can be extended to recover all eigenvectors of FA (see Algorithm 1 for the idea).\nHowever, when A has repeated eigenvalues, then its eigendecomposition is no longer uniquely defined. For the identity matrix I, any orthonormal basis in Rd can be used to form its eigenvectors, and the function FI(u) = 1 for any choice of u \u2208 Sd\u22121. In general, the hidden basis recovery problem arising in the eigendecomposition problem is only uniquely defined when there are no repeat eigenvalues.\nAlgorithm 1 The deflationary scheme for hidden basis recovery. This is an abstract algorithm which when given access to an orthogonal BEF F (u) = \u2211d i=1 gi(u \u2022 zi) satisfying Assumption 3, it recovers and returns estimates of the hidden basis directions z1, . . . , zd up to unknown signs and potentially an unknown permutation.\n1: for i\u2190 1 to d do 2: Find z\u0303i a local maximizer of F on Sd\u22121 \u2229 span({z\u03031, z\u03032, . . . , z\u0303i\u22121})\u22a5 3: end for 4: return z\u03031, . . . , z\u0303d.\nAs pointed out by the Example 2, we will need to understand the conditions under which a deflationary approach to maximizing a BEF F on Sd\u22121 (see Algorithm 1) can be guaranteed to recover the hidden basis z1, . . . , zd. We also wish that the hidden basis z1, . . . , zd be uniquely defined by the BEF F . It turns out that the following assumption is sufficient for performing guaranteed basis recovery.\nAssumption 3 (Strict convexity). For all i \u2208 [d], t 7\u2192 gi(sign(t) \u221a |t|) is strictly convex.\nMore formally, we have the following result.\nTheorem 4. Suppose that F is an orthogonal BEF satisfying Assumption 3. Then, the set of local maxima of F on the unit sphere is non-empty and contained in the set {\u00b1z1, . . . ,\u00b1zd}.\nThe Assumption 3 is sufficient for hidden basis recovery in the sense of the following Corollary. Its proof is an exercise in induction on the number of recovered vectors z\u0303j , where the inductive step is a result of Theorem 4.\nCorollary 5. If F is an orthogonal BEF satisfying Assumption 3, then the abstract Algorithm 1 returns vectors z\u03031, . . . , z\u0303d which recover the directions z1, . . . , zd up to a choice of signs and permutation. More precisely, there exists sign si \u2208 {\u00b11} and a permutation p of [d] such that zi = siz\u0303p(i) for each i \u2208 [d].\nBefore proceding with the proof of Theorem 4, it is worth discussing the importance of strict convexity in Assumption 3. In the case of the matrix eigendecomposition Example 2 with the identity matrix I, we constructed an orthogonal BEF with contrast functions gi(t) = t2 which satisfy that each gi(sign(t) \u221a |t|) = t is convex but not strictly convex. The function FI(u) is constant on the unit sphere, and there is no uniquely defined hidden basis (or eigenvector basis) for the identity matrix. In this sense, it does not suffice for t 7\u2192 g(sign(t) \u221a |t|) to be convex.\nInterestingly, the only issue which can arise when strict convexity is relaxed to convexity in Assumption 3 is that the function F may plateau (become constant) on regions within the unit sphere Sd\u22121. Strict convexity is one way to ensure that this does not happen. Nevertheless, the problem of recovering the eigendecomposition of a positive definite symmetric matrix A (Example 2) is a limit case of our framework. Moreover, Algorithm 1 can be used to perform eigenvector recovery since one does not require uniqueness of the eigenvector basis.\nThe intuition behind Assumption 3 is captured in the proof of Theorem 4. The main idea is to introduce a change of variable and recast maximization of F over the unit sphere as a convex maximization problem defined over a (hidden) convex domain.\nProof of Theorem 4. We will use the following Fact about convex maximization (see [16, Chapter 32] for an overview of concepts related to convex maximization).\nFor a convex set K, a point x \u2208 K is said to be an extreme point if x is not equal to a strict convex combination of two other points in K.\nFact 6. Suppose that K is a closed and bounded convex set. Let f : K \u2192 R be a strictly convex function. Then, the set of local maxima of f on K is non-empty and contained in the set of extreme points of K.\nAs z1, . . . , zd form an orthonormal basis of the space, we may simplify notation and work in the coordinate system in which z1, . . . , zd are the canonical vectors e1, . . . , ed. We define \u2206\nd\u22121 := conv(e1, . . . , ed) a (hidden) simplex, and Q d\u22121 + := {u \u2208 Sd | ui \u2265 0 for all i \u2208 [d]} the restriction of the sphere onto the positive orthant. By the symmetries of the problem, it suffices to show that the set S of local maxima of F with respect to Qd\u22121+ is non-empty and that S \u2282 {e1, . . . , ed}.\nThe main idea is to use the change of variable \u03c8 : Qd\u22121+ \u2192 \u2206d\u22121 defined by \u03c8i(u) = u2i . Since\nF \u25e6 \u03c8\u22121(x) = d\u2211\ni=1\ngi(\u03c8 \u22121 i (x)) = d\u2211 i=1 gi( \u221a xi) , (1)\nthen by Assumption 3, F \u25e6 \u03c8\u22121 : \u2206d\u22121 \u2192 R is a strictly convex function defined on a closed and bounded convex domain. By Fact 6, we note that the set S\u2032 of local maxima of F \u25e6\u03c8\u22121 on \u2206d\u22121 is nonempty and contained in the set {e1, . . . , ed} of extreme points of \u2206d\u22121. Pulling back to Qd\u22121+ , we see that S = \u03c8\u22121(S\u2032) is a non-empty subset of {e1, . . . , ed}."}, {"heading": "2.2 Spectral Clustering as Basis Recovery", "text": "It turns out that orthogonal basis recovery has direct implications for spectral clustering. In particular, when an n-vertex similarity graph G has k connected components corresponding to the desired clusters, it will be seen in section 4 that the spectral embedding into Rk maps each vertex vi in the j\nth connected component onto a ray protruding from the origin in a direction zj . It happens that the directions z1, . . . , zk are orthogonal. We let xi denote the embedded points and we construct the function\nFg(u) := 1\nn n\u2211 i=1 g(|u \u2022 xi|) ,\nfrom the embedded data and the contrast function g : R\u2192 R. To see that Fg is actually an orthogonal BEF, we consider the following theoretical construction: Let S1, . . .Sk be the vertex index sets corresponding to the distinct components of the graph G, and define the functions gj : R\u2192 R for all j \u2208 [k] by gj(t) = 1n \u2211 i\u2208Sj g(t\u2016xi\u2016). Then, it may be verified\nthat Fg(u) = \u2211k\nj=1 gj(u \u2022 zj), which takes on the form of an orthogonal BEF. In particular, we will be able to recover the directions z1, . . . , zk corresponding to the desired clusters by maximizing the function Fg on the unit sphere Sk\u22121.\nDue to the special form of orthogonal BEF which arises in spectral clustering, we will have slightly stronger guarantees. In particular, it will be seen (Theorem 9 and Theorem 18) all of the directions of \u00b1z1, . . . ,\u00b1zk are strict local maximum of Fg on Sk\u22121 instead of just some."}, {"heading": "3 Spectral Clustering Problem Statement", "text": "Let G = (V,A) denote a similarity graph where V is a set of n vertices and A is an adjacency matrix with non-negative weights. Two vertices i, j \u2208 V are incident if aij > 0, and the value of aij is interpreted as a measure of the similarity between the vertices. In spectral clustering, the goal is to partition the vertices of a graph into sets S1, . . . ,Sk such that these vertex sets form natural clusters in the graph. In the most basic setting, G consists of k connected components, and the\nnatural clusters should be the components themselves. In this case, if i\u2032 \u2208 Si and j\u2032 \u2208 Sj then ai\u2032j\u2032 = 0 whenever i 6= j. For convenience, we can consider the vertices of V to be indexed such that all indices in Si precede all indices in Sj when i < j. The matrix A takes on the form:\nA =  AS1 0 \u00b7 \u00b7 \u00b7 0 0 AS2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 ASk  , a block diagonal matrix. In this setting, spectral clustering can be viewed as a technique for reorganizing a given similarity matrix A into such a block diagonal matrix.\nIn practice, G rarely consists of k truly disjoint connected components. Instead, one typically observes a matrix A\u0303 = A+E where E is a perturbation from the clean setting. The goal of spectral clustering is to permute the rows and columns of A\u0303 to form a matrix which is nearly block diagonal and to recover the corresponding clusters."}, {"heading": "4 The Spectral Embedding", "text": "Given an n-vertex similarity graph G = (V,A), let D be the diagonal degree matrix with non-zero entries dii = \u2211 j\u2208V aij . The graph Laplacian is defined as L := D \u2212 A. The following well known property of the graph Laplacian (see [19] for a review) helps shed light on its importance: Given u \u2208 Rn,\nuTLu = 1\n2 \u2211 i,j\u2208V aij(ui \u2212 uj)2 . (2)\nThe graph Laplacian L is symmetric positive semi-definite as equation (2) cannot be negative. Further, u is a 0-eigenvector of L (or equivalently, u \u2208 N (L)) if and only if uTLu = 0. When G consists of k connected components with indices in the sets S1, . . . ,Sk, inspection of equation (2) gives that u \u2208 N (L) precisely when u is piecewise constant on each Si. In particular,\n{|S1|\u22121/21S1 , . . . , |Sk| \u22121/21Sk} (3)\nis an orthonormal basis for N (L). In general, letting X \u2208 Rn\u00d7k contain an orthogonal basis of N (L), it cannot be guaranteed that the rows of X will act as indicators of the various classes, as the columns of X have only been characterized up to a rotation within the subspace N (L). However, the rows of X are contained in a scaled orthogonal basis of Rk with the basis directions corresponding to the various classes. We formulate this result as follows (see [21], [18, Proposition 5], and [15, Proposition 1] for related statements).\nProposition 7. Let the similarity graph G = (V,A) contain k connected components with indices in the sets S1, . . . ,Sk, let n = |V |, and let L be the graph Laplacian of G. Then, N (L) has dimensionality k. Let X = (x \u20221, . . . , x \u2022k) contain k scaled, orthogonal column vectors forming a basis of N (L) such that \u2016x \u2022 j\u2016 = \u221a n for each j \u2208 [k]. Then, there exist weights w1, . . . , wk with wj = |Sj | n and mutually orthogonal vectors z1, . . . , zk \u2208 R\nk such that whenever i \u2208 Sj, the row vector xi \u2022 =\n1\u221a wj zTj .\nProof. We define the matrix MSi := 1Si1 T Si . PN (L) can be constructed from any orthonormal basis of N (L). Using the two bases {|S1|\u22121/21S1 , . . . , |Sk| \u22121/21Sk} and { 1\u221anx \u20221, . . . , 1\u221a n x \u2022k} yields:\nPN (L) = k\u2211\ni=1\n|Si|\u22121MSi and PN (L) = 1\nn XXT .\nThus for i, j \u2208 V , 1nxi \u2022 \u2022 xj \u2022 = (PN (L))ij . In particular, if there exists ` \u2208 [k] such that i, j \u2208 S`, then 1nxi \u2022 \u2022 xj \u2022 = |S`|\n\u22121. When i and j belong to separate clusters, then xi \u2022 \u22a5 xj \u2022 . If i, j \u2208 Sj , then\ncos(\u2220(xi \u2022 , xj \u2022 )) = xi \u2022 \u2022 xj \u2022\n\u2016xi \u2022\u2016\u2016xj \u2022\u2016 =\n|S`|\u22121\n|S`|\u22121/2|S`|\u22121/2 = 1 ,\nimplies that xi \u2022 and xj \u2022 are in the same direction. As they also have the same magnitude, xi \u2022 and xj \u2022 coincide for any two indices i and j belonging to the same component of G.\nThus letting wi := |Si| n for i = 1, . . . , k, there are k perpendicular vectors z1, . . . , zk correspond-\ning to the k connected components of G such that xi \u2022 = 1\u221a w` zT` for all i \u2208 S`.\nProposition 7 demonstrates that using the null space of the graph Laplacian, the k connected components in G are mapped to k scaled, orthogonal basis vectors in Rk. Of course, under a perturbation of A, the interpretation of Proposition 7 must change. In particular, G will no longer consist of k connected components, and instead of using only vectors in N (L), X must be constructed using the eigenvectors corresponding to the lowest k eigenvalues of L. With the perturbation of A comes a corresponding perturbation of the eigenvectors in X. Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).\nDue to different properties of the resulting spectral embeddings, normalized graph Laplacians are often used in place of L for spectral clustering, in particular the symmetric normalized Laplacian Lsym := D \u22121/2LD\u22121/2 and the asymmetric normalized Laplacian Lrw := D \u22121L. These normalized Laplacians are often viewed as more stable to perturbations of the graph structure. Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].\nFor simplicity, we focus first on the unnormalized graph Laplacian L. However, when G consists of k connected components, N (Lrw) happens to be identical to N (L). The algorithms which we will propose for spectral clustering turn out to be equally valid when using any of L, Lsym, or Lrw, though the structure of N (Lsym) gives rise to a slightly more complicated ray-based basis structure. The discussion of N (Lsym) and its admissibility are deferred to Section 7."}, {"heading": "5 Basis Recovery for Spectral Clustering", "text": "We now focus on the second step of spectral clustering, which is clustering the points embedded by the Laplacian embedding into the desired clusters. In particular, we will now demonstrate that the embedded data (the rows of X in Proposition 7) may be used to construct a function optimization problem whereby the maxima structure of the function can be used to recover the desired clusters.\nConstruction 8. Given a graph G with n vertices and k connected components, let X; S1, . . . ,Sk; w1, . . . , wk; z1, . . . , zk; and L as in Proposition 7. We construct a function Fg : Sk\u22121 \u2192 R on the\nunit sphere using a contrast function g : [0,\u221e)\u2192 R where it is assumed that t 7\u2192 g( \u221a t) is strictly convex. We construct Fg as\nFg(u) := 1\nn n\u2211 i=1 g(|u \u2022 xi \u2022 |) . (4)\nUsing Proposition 7, this may be equivalently written as\nFg(u) = k\u2211 i=1 wig( 1\u221a wi |u \u2022 zi|) . (5)\nIn Construction 8, the vectors z1, . . . , zk form an unseen orthonormal basis of Rk, and each weight wi = |Si| n is the fraction of the rows of X indexed as x` \u2022 which are embedded from the i th component of G and which coincide with the point 1\u221awi z T i . Since each embedded point in the i th cluster lies on the line through zi and \u2212zi, it suffices to recover the basis directions z1, . . . , zk up to sign in order to cluster the points. Our idea is to show that Fg is an orthogonal BEF which satisfies Assumption 3 with the directions z1, . . . , zk corresponding to the BEF basis. As such, we will be able to use the maxima structure of Fg on Sk\u22121 in order to recover the hidden basis and thence the desired clustering.\nWe use equation (5) to see that Fg is a special form of orthogonal BEF with the functions gi (see Definition 1) defined by gi(t) := wig(\n1\u221a wi |t|). Further, since t 7\u2192 g(\n\u221a t) is strictly convex, we\nsee that t 7\u2192 gi(sign(t) \u221a |t|) is strictly convex for all i \u2208 [k], and hence Fg satisfies Assumption 3. However, due to the special form of Fg , each of the directions {\u00b1zi : i \u2208 [k]} are maxima of Fg over Sk\u22121 (as opposed to just some, cf. Theorem 4).\nTheorem 9. Let Fg : Sk\u22121 \u2192 R and z1, . . . , zk be defined as in Construction 8. Then, the set {\u00b1zi : i \u2208 [k]} is a complete enumeration of the local maxima of Fg.\nWe defer the proof of Theorem 9 to section 7.2. We also provide and prove the analogous result for when Fg is constructed using the Laplacian embedding arising from Lrw or Lsym in section 7.2.\nAs Fg is an orthogonal BEF, it follows from the discussion in section 2 that by enumerating the local maxima of Fg using a deflationary scheme, we may recover the hidden basis z1, . . . , zk corresponding to the graph clusters. By Theorem 9, we get slightly more flexibility in our algorithmic design since it is known that each of the directions z1, . . . , zk is a local maximum of Fg on Sk\u22121, and therefore we have room to relax the orthogonality constraint from the prototypical deflationary scheme (Algorithm 1) when designing algorithms for hidden basis recovery in the spectral clustering setting."}, {"heading": "5.1 Proposed Algorithms", "text": "We now design a new class of algorithms for spectral clustering. Given a similarity graph G = (V,A) containing n vertices, define a graph Laplacian L\u0303 among L, Lrw, and Lsym (reader\u2019s choice). Viewing G as a perturbation of a graph consisting of k connected components, construct X \u2208 Rn\u00d7k such that x \u2022 i gives the eigenvector corresponding to the i th smallest eigenvalue of L\u0303 with scaling \u2016x \u2022 i\u2016 = \u221a n.\nWith X in hand, choose a contrast function g satisfying the strict convexity condition from Assumption 3. From g, the function Fg(u) = 1 n \u2211n i=1 g(u \u2022 xi \u2022 ) is defined on Sk\u22121 using the rows of X. The local maxima of Fg correspond to the desired clusters of the graph vertices. Since Fg is a symmetric function, if Fg has a local maximum at u, Fg also has a local maximum at \u2212u. However, the directions u and \u2212u correspond to the same line through the origin of Rk and form an equivalence class, with each such equivalence class corresponding to a cluster.\nOur first goal is to find local maxima of Fg corresponding to distinct equivalence classes. We will use that the desired maxima of Fg should be approximately orthogonal to each other. Once we have obtained local maxima u1, . . . ,uk of Fg , we cluster the vertices of G by placing vertex i in the jth cluster using the rule j = arg max` |u` \u2022 xi \u2022 |. We sketch two algorithmic ideas in HBRopt and HBRenum (where HBR stands for hidden basis recovery).\nAlgorithm 2 Finds the local maxima of Fg defined from the embedded vertices xi \u2022 which we want to cluster. The second input \u03b7 is the learning rate (step size).\n1: function HBRopt(X, \u03b7) 2: C \u2190 {} 3: for i\u2190 1 to k do 4: Draw u uniformly from Sk\u22121 \u2229 span(C)\u22a5 5: repeat 6: u\u2190 u + \u03b7(\u2207Fg(u)\u2212 uuT\u2207Fg(u)) (= u + \u03b7Pu\u22a5\u2207Fg(u)) 7: u\u2190 Pspan(C)\u22a5u 8: u\u2190 u\u2016u\u2016 9: until Convergence\n10: Let C \u2190 C \u222a {u} 11: end for 12: return C 13: end function\nHBRopt is a form of projected gradient ascent which more fully implements the deflationary scheme of Algorithm 1. The parameter \u03b7 is the learning rate. Each iteration of the repeat-until loop moves u in the direction of steepest ascent. For gradient ascent in Rk, one would expect step 6 of HBRopt to read u \u2190 u + \u03b7\u2207Fg(u). However, gradient ascent is being performed for a function Fg defined on the unit sphere, but the gradient described by \u2207Fg is for the function Fg with domain Rk. The more expanded formula \u2207Fg(u)\u2212uuT\u2207Fg(u) is the projection of \u2207Fg onto the plane tangent to Sk\u22121 at u. This update keeps u near the sphere.\nWe may draw u uniformly at random from Sk\u22121 \u2229 span(C)\u22a5 by first drawing u from Sk\u22121 uniformly at random, projecting u onto span(C)\u22a5, and then normalizing u. It is important that u stay near the orthogonal complement of span(C) in order to converge to a new cluster rather than converging to a previously found optimum of Fg . Step 7 enforces this constraint during the update step.\nIn contrast to HBRopt, HBRenum more directly uses the point separation implied by the orthogonality of the approximate cluster centers. Since each embedded data point should be near to a cluster center, the data points themselves are used as test points. Instead of directly enforcing orthogonality between cluster means, a parameter \u03b4 > 0 specifies the minimum allowable angle between found cluster means.\nBy pre-computing the values of Fg(xi \u2022/\u2016xi \u2022\u2016) outside of the while loop, HBRenum can be run in O(kn2) time. HBRenum is likely to be slower than HBRopt which takes O(k2nt) time where t is the average number of iterations to convergence. The number of clusters k cannot exceed (and is usually much smaller than) the number of graph vertices n.\nHBRenum has a couple of nice features which may make it preferable on smaller data sets. Each center found by HBRenum will always be within a cluster of data points even when the optimization landscape is distorted under perturbation. In addition, the maxima found by HBRenum are based on a more global outlook, which may be useful in the noisy setting.\nAlgorithm 3 Finds the local maxima of Fg defined from the points xi \u2022 needed for clustering. The second input \u03b4 controls how far a point needs to be from previously found cluster centers to be a candidate future cluster center.\n1: function HBRenum(X, \u03b4) 2: C \u2190 {} 3: while |C| < k do 4: j \u2190 arg maxi{Fg( xi \u2022\u2016xi \u2022 \u2016) | \u2220( xi \u2022\u2016xi \u2022 \u2016 ,u) > \u03b4 \u2200u \u2208 C} 5: C \u2190 C \u222a { xj \u2022\u2016xj \u2022 \u2016} 6: end while 7: return C 8: end function"}, {"heading": "5.2 Choosing a Contrast Function", "text": "There are many possible choices of contrast g which are admissible for spectral clustering under Theorem 9 including the following:\ngsig(t) = \u2212 1\n1 + exp(\u2212|t|) gp(t) = |t|p where p \u2208 (2,\u221e)\nggau = e \u2212t2 gabs(t) = \u2212|t| ght(t) = log cosh(t)\nIn choosing contrasts, it is instructive to first consider the function g2(y) = y 2 (which relaxes the criterion that t 7\u2192 g( \u221a |t|) be strictly convex to plain convexity and is thus not admissible).\nNoting that Fg2(u) = \u2211k i=1wi( 1\u221a wi u \u2022 zi) 2 = 1, we see that Fg2 is constant on the unit sphere. We see that the distinguishing power of a contrast function for spectral clustering comes from our assumption that t 7\u2192 g( \u221a |t|) is strictly convex. Intuitively, \u201cmore strictly convex\u201d contrasts have better resolving power but are also more sensitive to outliers and perturbations of the data. Indeed, if g grows too rapidly, a small number of outliers far from the origin could significantly distort the maxima structure of Fg .\nDue to this tradeoff, gsig and gabs could be important practical choices for the contrast function. Both gsig( \u221a |t|) and gabs( \u221a |t|) have a strong convexity structure near the origin. As gsig is a\nbounded function, it should be very robust to perturbations. In comparison, gabs( \u221a |t|) = \u2212 \u221a |t| maintains a stronger convexity structure over a much larger region of its domain, and gabs(t) has only a linear rate of growth as t \u2192 \u221e. This is a much slower growth rate than is present for instances in gp with p > 2."}, {"heading": "6 Clustering Experiments", "text": "We now discuss our test results on our proposed spectral clustering algorithms on a variety of real and simulated data. The implementations for our spectral clustering algorithms are available on github: https://github.com/vossj/HBR-Spectral-Clustering."}, {"heading": "6.1 An Illustrating Example", "text": "Figure 1 illustrates our function optimization framework for spectral clustering. In this example, random points pi were generated from 3 concentric circles: 200 points were drawn uniformly at\nrandom from a radius 1 circle, 350 points from a radius 3 circle, and 700 points from a radius 5 circle. The points were then radially perturbed. The generated points are displayed in Figure 1 (a). The similarity matrix A was constructed as aij = exp(\u221214\u2016pi\u2212pj\u2016\n2), and the Laplacian embedding was performed using Lrw.\nFigure 1 (b) depicts the clustering process with the contrast gsig on the resulting embedded points. In this depiction, the embedded data sufficiently encodes the desired orthogonal basis structure that all local maxima of Fgsig correspond to desired clusters. The value of Fgsig is displayed by the grayscale heat map on the unit sphere in Figure 1 (b), with lighter shades of gray indicate greater values of Fgsig . The cluster labels were produced using HBRopt. The rays protruding from the sphere correspond to the basis directions recovered by HBRopt, and the recovered labels are indicated by the color and symbol used to display each data point."}, {"heading": "6.2 Image Segmentation Examples", "text": "Spectral clustering was first applied to image segmentation by Shi and Malik [17], and it has remained a popular application of spectral clustering. The goal in image segmentation is to divide an image into regions which represent distinct objects or features of the image. Figure 2 and Figure 3 show several segmentations produced by HBRopt-gabs and spherical k-means on several example images from the BSDS300 test set [13].\nFor this example application, we used a relatively simple notion of similarity based only on the color and proximity of the image\u2019s pixels. Let pi denote the i\nth pixel. Each pi has a location xi and an RGB color ci = (ri, gi, bi)\nT . We used the following similarity between any two distinct pixels pi and pj :\naij =\n{ e\u2212 1 \u03b12 \u2016xi\u2212xj\u20162e \u2212 1 \u03b22 \u2016ci\u2212cj\u20162 if \u2016xi \u2212 xj\u2016 < R\n0 if \u2016xi \u2212 xj\u2016 \u2265 R (6)\nfor some parameters \u03b1, \u03b2, and radius R. By enforcing that aij is 0 for points which are not too close, we build a sparse similarity matrix which greatly speeds up the computations. As the similarity measure decays exponentially with distance, the zeroed entries would be very small anyway.\nDetermining the number of clusters to use in spectral clustering is an unsolved problem. However, the BSDS300 data set includes hand labeled segmentations. From the hand labeled segmen-\ntations for a particular image, one human segmentation was chosen at random and the number of segments from that segmentation was used as the number of clusters k for spectral clustering to search for. No other information from the human segmentations was used in generating the image segmentations.\nIn order to reduce the effect of salt and pepper type noise, the images were preprocessed using 9\u00d79 median filtering prior to constructing the similarity matrices. The similarity from equation (6) was constructed with common fixed values of \u03b1, \u03b2, and R across all images. Spectral clustering was performed using HBRopt under the contrast function gabs and the Lrw embedding.\nQualitatively, we found that for the same embedding, k-means is more likely to over segment large regions within the image, in effect balancing the cluster sizes. In contrast, our proposed HBRopt algorithm tended to segment out additional small regions within the image more frequently."}, {"heading": "6.3 Stochastic Block Model with Imbalanced Clusters", "text": "We construct a similarity graph A = diag(A1, A2, A3) + E where each Ai is a symmetric matrix corresponding to a cluster and E is a small perturbation. We set A1 = A2 to be 10\u00d7 10 matrices with entries 0.1. We set A3 to be a 1000 \u00d7 1000 matrix which is symmetric, approximately 95% sparse with randomly chosen non-zero locations set to 0.001. When performing this experiment 50 times, HBRopt-gsig obtained a mean accuracy of 99.9%. In contrast, spherical k-means with\nrandomly chosen starting points obtained a mean accuracy of only 42.1%. It turns out that splitting the large cluster is in fact optimal in terms of the spherical k-means objective function but leads to poor classification performance. Our method does not suffer from that shortcoming."}, {"heading": "6.4 Performance Evaluation on UCI Datasets", "text": "We compare spectral clustering performance on a number of data sets with unbalanced cluster sizes. In particular, we use the E. coli, flags, glass, Iris, thyroid disease, and car evaluation data sets which are part of the UCI machine learning repository [3]. We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle. For the flags data set, we used religion as the ground truth labels, and for thyroid disease, we used the new-thyroid data.\nFor all data sets, we only used fields for which there were no missing values, we normalized the data such that every field had unit standard deviation, and we constructed the similarity matrix A using a Gaussian kernel k(yi,yj) = exp(\u2212\u03b1\u2016yi\u2212yj\u20162). The parameter \u03b1 was chosen separately for each data set in order to create a good embedding. The choices of \u03b1 were: 0.25 for E. coli, 32\nfor glass, 0.5 for Iris, 32 for thyroid disease, 128 for flags, 0.25 for car evaluation, and 0.125 for cell cycle.\nThe spectral embedding was performed using the symmetric normalized Laplacian Lsym. Then, the clustering performance of our proposed algorithms HBRopt and HBRenum (implemented with \u03b4 = 3\u03c0/8 radians) were compared with the following baselines:\n\u2022 oracle-centroids: The means \u00b5j = 1|Sj | \u2211 i\u2208Sj xi \u2022 \u2016xi \u2022 \u2016 are set using the ground truth labels for\nall j \u2208 [k]. Points are assigned to their nearest cluster mean in cosine distance.\n\u2022 k-means-cosine: Spherical k-means (standard matlab kmeans library function called using the cosine distance and using the default k-means++ mean initialization) is run with a random initialization of the means, (cf. [15]).\nWe report the clustering accuracy of each algorithm in Table 1. The accuracy is computed using the best matching between the clusters and the true labels. The reported results consist of the mean performance over a set of 25 runs for each algorithm. The number of clusters being searched for was set to the ground truth number of clusters. In most cases, our proposed algorithms show improvement in performance over spherical k-means."}, {"heading": "7 Basis Recovery With Each Laplacian Embedding", "text": "We have already argued that graph Laplacians L and Lrw can be used for spectral clustering within our BEF framework, and we have asserted that Lsym can also be used. We now discuss how orthogonal BEF recovery can be used for spectral clustering in the setting where G consists of k connected components using any of the graph Laplacians. First, in section 7.1, we show how the Laplacian embedding for the symmetric normalized Laplacian Lsym differs and generalizes upon the embedding structure arising for L and Lrw (cf. Proposition 7). Then, in section 7.2, we prove that the spectral embedding induced by any of the discussed graph Laplacians gives rise to an optimization problem on Sk\u22121 in which the local maxima enumerate the desired clusters for spectral clustering. More precisely, we prove a generalization of Theorem 9 which includes embeddings generated using L, Lrw, and Lsym.\nThe discussion in this section highlights the differences between using Lsym and using either L or Lrw for the proposed spectral algorithms. Whereas taking an orthogonal basis of N (L) or N (Lrw) produces embedded points which are orthogonal and of fixed norm within any particular class, using N (Lsym) produces embedded points along perpendicular rays but with varying intra-class norms as will be seen in Proposition 10. Despite these differences, when given a contrast function\ng meeting the strict convexity criterion from Assumption 3, the proposed algorithms HBRopt and HBRenum which worked for spectral clustering using L and Lrw also work for spectral clustering using Lsym."}, {"heading": "7.1 Null Space Structure of the Normalized Laplacians", "text": "We now investigate the null space structure of the normalized graph Laplacians. We will first describe the null space structures Lsym and Lrw for a graph G consisting of k components. Then, we will show how the null space structures of Lsym, Lrw, and L can all be viewed within a single, more generalized notion of a graph embedding.\nLet G = (V,A) be an n-vertex graph containing k connected components such that the ith\ncomponent has vertices with indices in the set Si. For any set C \u2282 V , we define \u03b4D(C) := \u2211 i\u2208C dii . (7)\nwhere D = diag(d11, d22, . . . , dnn) is a diagonal matrix with strictly positive entries. For now, we will take D to be the diagonal degree matrix D such that dii = dii = \u2211n j=1 aij . Then, \u03b4D(C) is the sum of vertex degrees for vertices in the set C. Using this definition, we are able to characterize the embedding structure of Lsym.\nProposition 10. Let G be a similarity graph consisting of k connected components for which Lsym is well defined. Let the vertex indices be partitioned into sets S1, . . . ,Sk corresponding to the k connected components. Then, dim(N (Lsym)) = k. If X = (x \u20221, . . . , x \u2022k) contains a scaled basis of N (Lsym) in its columns such that \u2016x \u2022 i\u2016 = \u221a n, then there exist k mutually orthogonal unit vectors z1, . . . , zk such that whenever i \u2208 Sj, the row vector\nxi \u2022 = \u221a ndii\u03b4D(Sj)\u22121zTj . (8)\nProof. An important property of the symmetric normalized Laplacian [19, Proposition 3] is that for all u \u2208 Rn,\nuTLsymu = 1\n2 \u2211 i,j\u2208V aij\n( ui\nd 1/2 ii\n\u2212 uj d\n1/2 jj\n)2 . (9)\nLsym is positive semi-definite, and u is a 0-eigenvector of Lsym if and only if plugging u into equation (9) yields 0. Let ySj be the vector such that\nySj =\n{ d\n1/2 ii if i \u2208 Sj .\n0 otherwise . (10)\nThen, B = (\u03b4D(S1)\u22121/2yS1 , . . . , \u03b4D(Sd) \u22121/2ySd) contains an orthonormal basis for N (Lsym) in its columns. Defining MSi = ySiy T Si , we get:\nPN (L) = BB T = k\u2211 i=1 \u03b4D(Si)\u22121MSi . (11)\nBut PN (Lsym) can be constructed from any orthonormal basis of N (Lsym). In particular, PN (Lsym) = 1 nXX T as well. Hence, 1nxi \u2022 \u2022 xj \u2022 = (PN (L))ij = \u03b4D(S`) \u22121d 1/2 ii d 1/2 jj precisely when there exists ` \u2208 [k] such that i, j \u2208 S`. Otherwise, xi \u2022 \u22a5 xj \u2022 .\nNote that for i, j \u2208 S`,\ncos(\u2220(xi \u2022 , xj, \u2022 )) = xi \u2022 \u2022 xj \u2022\n\u2016xi \u2022\u2016\u2016xj \u2022\u2016\n= n\u03b4D(S`)\u22121d 1/2 ii d 1/2 jj\n(n1/2\u03b4D(S`) \u22121/2 d 1/2 ii )(n 1/2 \u03b4D(S`) \u22121/2 d 1/2 jj )\n= 1 .\nThus, points from the same cluster lie on the same ray from the origin. It follows that there are k mutually orthogonal unit vectors, z1, . . . , zk such that xi \u2022 = \u221a ndii\u03b4D(S`)\u22121zT` for each i \u2208 S`.\nWe will make use of the close connection between the eigenvector structure of Lrw and Lsym in order to characterize the Laplacian embedding structure of Lrw. The following fact can be found in the tutorial [19, Proposition 3].\nFact 11. (\u03bb,u) is an eigenvalue-eigenvector pair of Lrw if and only if (\u03bb,D 1/2u) is an eigenvalueeigenvector pair for Lsym.\nBy using Fact 11 and Proposition 10, we obtain the embedding structure for Lrw.\nProposition 12. Let the similarity graph G = (V,A) contain k connected components with indices in the sets S1, . . . ,Sk, let n = |V |, and let Lrw be well defined for G. Then, N (Lrw) has dimensionality k. Let X = (x \u20221, . . . , x \u2022k) contain k scaled, orthogonal column vectors forming a basis of N (Lrw) such that \u2016x \u2022 j\u2016 = \u221a n for each j \u2208 [k]. Then, there exist weights w1, . . . , wk with wj = |Sj | n and mutually orthogonal vectors z1, . . . , zk \u2208 R\nk such that whenever i \u2208 Sj, the row vector xi \u2022 =\n1\u221a wj zTj .\nProof. By Fact 11, we may construct an orthogonal basis of N (Lrw) using a particular choice of orthogonal basis of N (Lsym). In particular, we define the vectors ySj the same as in the proof of Proposition 10, and we obtain that the vectors y\u0303Sj := D \u22121/2ySj are 0-eigenvectors of Lrw. Using equation (10), we see that ySj = 1Sj . In particular, it follows that {|S1| \u22121/21S1 , . . . , |Sk|\n\u22121/21Sk} is an orthonormal basis of N (Lrw). From the discussion around equation (3), it follows that N (L) and N (Lrw) are the same space in this setting where G consists of k connected components. Our desired result thus follows from Proposition 7.\nWe note that the Propositions 7, 10, and 12 are closely. From Propositions 7 and 12, we see that L and Lrw give rise to the same embedding structure when G consists of k connected components. Further, we may place the embedding structure for L (or equivalently Lrw) into the notation used for describing the ray structure of Lsym. In particular, if we let D = I, we see that \u03b4I(Sj) = |Sj |. Recalling that wj = |Sj |n , we see (by replacing D with I in equation (8)) that \u221a nIii\u03b4I(Sj)\u22121zTj = 1\u221awj z T j , which is the required replacement to recreate the statements of Proposition 7 and Proposition 12. In particular, we may create a generalized notion of a graph embedding which captures all of the Laplacian embeddings.\nDefinition 13. Let G be a similarity graph consisting of n vertices and k connected components such indices partitioned into sets S1, . . . ,Sk corresponding to the connected components. Let D = diag(d11, . . . , dnn) be a positive definite matrix. Let \u03d5 be a map which takes the i\nth vertex of G to a point xi \u2208 Rk. If there exists an orthonormal basis z1, . . . , zk of Rk such that xi = \u221a ndii\u03b4D(Sj)\u22121zj for each i \u2208 Sj , then we call \u03d5 a (G, D)-orthogonal embedding. We see by Proposition 10, the Laplacian embedding induced by Lsym is a (G,D)-orthogonal embedding; and by Propositions 7 and 12, the Laplacian embedding induced by L and Lrw are (G, I)-orthogonal embedding."}, {"heading": "7.2 Maxima Structure of the Resulting BEFs", "text": "In this section, we demonstrate that by performing function maximization over the directional projections of embedded data arising from any of the Laplacian embeddings, we are able to recover the desired clusters for spectral clustering. We will make use of the following construction.\nConstruction 14. Let G be a similarity graph consisting of n vertices and k connected components with indices partitioned into the sets S1, . . . ,Sk. We suppose that D = diag(d11, . . . , dnn) is a positive definite matrix. We suppose that x1, . . . ,xn is a (G,D)-orthogonal embedding of the vertices of G such that xi = \u221a ndii\u03b4D(Sj)\u22121zj for each i in Sj. Parallel to the text of section 5, we construct a function Fg : Sk\u22121 \u2192 R from a continuous contrast function g : [0,\u221e) \u2192 R where it is assumed that t 7\u2192 g( \u221a t) is strictly convex (cf. Assumption 3). We construct Fg as\nFg(u) := 1\nn n\u2211 i=1 g(|u \u2022 xi|)\n= 1\nn k\u2211 j=1 \u2211 i\u2208Sj g (\u2016xi\u2016 \u00b7 |u \u2022 zj |) . (12)\nFirst, we make a couple of comments about Construction 14. Using the discussion at the end of section 7.1, when D = I the embedded points xi can be obtained from the rows of X in Proposition 7, and they thus correspond to the embedded points arising from L. For this choice of D = I, Construction 14 is thus a strict generalization of Construction 8. However, Construction 14 also captures Lrw (with D = I by Proposition 12) and Lsym (with D = D by Proposition 10).\nWe now wish to generalize Theorem 9 by showing that the local maxima of Fg from Construction 14 are precisely the directions \u00b1z1, . . . ,\u00b1zk. We will first argue that Fg has no extraneous maxima, and then that the direction \u00b1z1, . . . ,\u00b1zk actually are maxima. To see that Fg has no extraneous local maxima, we need only demonstrate that Fg is an orthogonal BEF satisfying Assumption 3 and apply Theorem 4.\nLemma 15. Let Fg and z1, . . . , zk be as in Construction 14. Then, the local maxima of Fg is contained in the set {\u00b1zi | i \u2208 [k]}.\nProof. Define gi : R\u2192 R by gi(t) := 1n \u2211 j\u2208Si g(\u2016xj\u2016 \u00b7 |t|). Using equation (12), we obtain\nFg(u) = 1\nn k\u2211 i=1 \u2211 j\u2208Si g (\u2016xj\u2016 \u00b7 |u \u2022 zi|) = k\u2211 i=1 gi(u \u2022 zi) .\nSince t 7\u2192 g( \u221a t) is strictly convex, it follows that t 7\u2192 gi(sign(t) \u221a |t|) is strictly convex for all i \u2208 [k]. By Theorem 4, the local maxima of Fg are contained in {\u00b1zi : i \u2208 [k]}.\nWhat remains to be seen is that the directions {\u00b1zi | i \u2208 [k]} are local maxima of Fg . For notational simplicity, we identify z1, . . . , zk with the canonical directions e1, . . . , ek in an unknown coordinate system so that ui is shorthand for u \u2022 ei. In our proofs, we exploit the convexity structure induced by the change of variable introduced in the proof of Theorem 4, namely \u03c8 defined by \u03c8i(u) := u 2 i which maps the domain Sk\u22121 onto the simplex \u2206k\u22121 := conv(e1, . . . , ek).\nLemma 16. Let x1, . . . ,xn be as in Construction 14 with the added assumption that zi = ei for each i \u2208 [k]. Let h : [0,\u221e) \u2192 R be a strictly convex function. Let H : \u2206k\u22121 \u2192 R be given by H(u) = 1n \u2211k i=1 \u2211 j\u2208Si h(ui\u2016xj\u2016\n2). Then the set {ei | i \u2208 [k]} is contained in the set of strict local maxima of H.\nProof. By the symmetries of H, it suffices to show that e1 is a strict local maximum of H. To see this, choose u 6= e1 from a neighborhood of e1 relative to \u2206k\u22121 to be specified later. Let \u039bu = {i | i \u2208 [k] \\ {1}, ui 6= 0}. Then,\nH(e1)\u2212H(u)\n= 1\nn \u2211 j\u2208S1 h(\u2016xj\u20162) + k\u2211 i=2 \u2211 j\u2208Si h(0)\u2212 k\u2211 i=1 \u2211 j\u2208Si h(ui\u2016xj\u20162)  = 1\nn \u2211 j\u2208S1 ( h(\u2016xj\u20162)\u2212 h(u1\u2016x2j\u2016) )\n\u2212 k\u2211\ni=2 \u2211 j\u2208Si ( h(ui\u2016xj\u20162)\u2212 h(0) ) = 1\nn \u2211 j\u2208S1 \u2016xj\u20162(1\u2212 u1) h(\u2016xj\u20162)\u2212 h(u1\u2016x2j\u2016) \u2016xj\u20162(1\u2212 u1)\n\u2212 \u2211 i\u2208\u039bu \u2211 j\u2208Si ui\u2016xj\u20162 h(ui\u2016xj\u20162)\u2212 h(0) ui\u2016xj\u20162  . We have written H(e1) \u2212H(u) as a weighted sum of difference quotients (slopes). We would like to apply Lemma 20 in order to demonstrate that there is a neighborhood B of e1 relative to \u2206 k\u22121 such that u \u2208 B \\ {e1} implies H(e1)\u2212H(u) > 0. First, we notice that for each xj , u breaks the interval into left and right pieces, yielding two slopes of interest:\nm`ij(u) = h(ui\u2016xj\u20162)\u2212 h(0)\nui\u2016xj\u20162\nand\nmrij(u) = h(\u2016xj\u20162)\u2212 h(ui\u2016xj\u20162) \u2016xj\u20162(1\u2212 ui) .\nThus,\nH(e1)\u2212H(u) = 1\nn \u2211 j\u2208S1 \u2016xj\u20162(1\u2212 u1)mr1j(u)\n\u2212 \u2211 i\u2208\u039bu \u2211 j\u2208Si ui\u2016xj\u20162m`ij(u)  . Let B = {u | ui < minj\u2016xj\u2016 2\nmaxj\u2016xj\u20162 for all i 6= 1} \\ {e1}. Then, fixing u \u2208 B and i 6= 1, we have that ui\u2016xj1\u20162 < \u2016xj2\u20162 for any j1 \u2208 Si and j2 \u2208 S1. Let m`max(u) := max{m`ij(u) | i \u2208 \u039bu, j \u2208 Si} and mrmin(u) := min{mr1j(u) | j \u2208 S1}. From Lemma 20, it follows that m`max(u) < mrmin(u) for all\nu \u2208 B. Thus,\nH(e1)\u2212H(u) \u2265 1\nn \u2211 j\u2208S1 \u2016xj\u20162(1\u2212 u1)mrmin(u)\n\u2212 \u2211 i\u2208\u039bu \u2211 j\u2208Si ui\u2016xj\u20162m`max(u)  = (1\u2212 u1)mrmin(u)\u2212\nk\u2211 i=2 uim ` max(u)\n= (1\u2212 u1)[mrmin(u)\u2212m`min(u)] > 0\nwhere the first equality uses that \u2211\nj\u2208Si\u2016xj\u2016 2 = n (\u2211 j\u2208Si djj ) \u03b4D(Sj) = n for all j \u2208 [k]. It follows\nthat e1 is a local maximum of H.\nTheorem 17. In Construction 14, {\u00b1zi | i \u2208 [k]} is a complete enumeration of the local maxima of Fg.\nProof. Let \u039b denote the set of local maxima of Fg . That \u039b \u2282 {\u00b1zi | i \u2208 [k]} is immediate from Lemma 15. To see that \u039b \u2283 {\u00b1zi : i \u2208 [k]}, we note that there is a natural mapping between \u2206k\u22121 and a quadrant of Sk\u22121.\nThe set {\u00b1zi | i \u2208 [k]} gives an unknown, orthonormal basis of our space. We may without loss of generality work in the coordinate system where e1, . . . , ek coincide with z1, . . . , zk. Let Q1 = Sk\u22121 \u2229 [0,\u221e)k\u22121 give the first quadrant of the unit sphere. By the symmetries of the problem, it suffices to show that {e1, . . . , ek} are maxima of Fg . However, the map \u03c8 : Q1 \u2192 \u2206k\u22121 defined by (\u03c8(u))i = u 2 i is a homeomorphism. Defining H : \u2206\nk\u22121 \u2192 R by H(t) = Fg(\u03c8\u22121(t)), then t \u2208 \u2206k\u22121 is a local maximum of H if and only if \u03c8\u22121(t) is a local maximum of Fg relative to Q1.\nNote that H(t) = 1n \u2211k i=1 \u2211 j\u2208Si g( \u221a ti\u2016xj\u20162). As y 7\u2192 g( \u221a y) is convex, it follows by Lemma 16\nthat {ei}ki=1 are local maxima of H. Hence, using the symmetries of Fg , {\u00b1zi | i \u2208 [k]} \u2283 \u039b.\nWith Theorem 17 in hand, it is now straight forward to generalize Theorem 9 to demonstrate that the spectral embedding arising from any of the graph Laplacians is compatible with the proposed BEF function maximization framework for clustering within the embedded space.\nTheorem 18. Suppose that G is a graph consisting of n vertices and k connected components with indices in the sets S1, . . . ,Sk. Let L be a (well defined) graph Laplacian chosen among L, Lrw, or Lsym constructed from G. If X \u2208 Rn\u00d7k is such that its columns x \u2022 i form an orthogonal subspace of N (L) scaled such that \u2016x \u2022 i\u2016 = \u221a n, then there exists an orthonormal basis z1, . . . , zk of Rk such that\n1. For each j \u2208 Si, xTj \u2022 lies on the ray starting at the origin and going through zi.\n2. If we define Fg : Sk\u22121 \u2192 R by Fg(u) = 1n \u2211n\ni=1 g(u \u2022 xi \u2022 ) from a contrast g : [0,\u221e) \u2192 R satisfying that t 7\u2192 g( \u221a t) is strictly convex, then the directions {\u00b1zi | i \u2208 [k]} provide a complete enumeration of the local maxima of Fg on Sk\u22121.\nProof. Part 1 follows from the combination of Propositions 7, 10, and 12. Part 2 follows from Theorem 17 along with the observation that Construction 14 captures the given Fg irregardless of which of the 3 graph Laplacians is used to construct Fg (see Construction 14 and the surrounding discussion)."}, {"heading": "A Facts About Convex Functions", "text": "In this section, intervals can be open, half open, or closed. There is a large literature studying the properties of convex functions. As strict convexity is considered more special than convexity, results are typically stated in terms of convex functions. The following characterization of strict convexity is a version of Proposition 1.1.4 of [8] for strictly convex functions, and can be proven in a similar fashion.\nLemma 19. For an interval I, let f : I \u2192 R be a strictly convex function. Then, fixing any x0 \u2208 I, the slope function defined by m(x) := f(x)\u2212f(x0)x\u2212x0 is strictly increasing on I \\ {x0}.\nThe following result is largely a consequence of Lemma 19.\nLemma 20. Let I be an interval and let f : I \u2192 R be a convex function. Suppose that (a, b) \u2282 I and (c, d) \u2282 I are such that a \u2264 c and b \u2264 d with at least one of the inequalities being strict. Then,\nf(b)\u2212 f(a) b\u2212 a < f(d)\u2212 f(c) d\u2212 c\nProof. If c = a, then f(d)\u2212f(a)d\u2212a = f(d)\u2212f(c) d\u2212c trivially. Otherwise, a < c, and by Lemma 19, we have that f(d)\u2212f(a)d\u2212a < f(d)\u2212f(c) d\u2212c By similar reasoning, f(b)\u2212f(a) b\u2212a \u2264 f(d)\u2212f(a) d\u2212a (with equality if and only if d = b). As by assumption, a = b and c = d cannot both hold, it follows that f(b)\u2212f(a)b\u2212a \u2264 f(d)\u2212f(a)\nd\u2212a \u2264 f(d)\u2212f(c)\nd\u2212c with at least one of the inequalities being strict.\nThe following result comes from Remark 4.2.2 of Hiriart-Urruty and Lemare\u0301chal [8].\nLemma 21. Given an interval I and a function f : I \u2192 R, then the left derivative \u2202\u2212f is leftcontinuous and the right derivative \u2202+f is right-continuous respectively whenever they are defined (that is, finite)."}, {"heading": "Acknowledgements", "text": "This work was supported by NSF grants IIS 1117707, CCF 1350870, and CCF 1422830."}], "references": [{"title": "Efficient learning of simplices", "author": ["J. Anderson", "N. Goyal", "L. Rademacher"], "venue": "COLT, pages 1020\u20131045,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spectral clustering, with application to speech separation", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, 7:1963\u20132001,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., 15(6):1373\u20131396,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["C. Davis", "W.M. Kahan"], "venue": "iii. SIAM Journal on Numerical Analysis, 7(1):1\u201346,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1970}, {"title": "Identification of almost invariant aggregates in reversible nearly uncoupled markov chains", "author": ["P. Deuflhard", "W. Huisinga", "A. Fischer", "C. Sch\u00fctte"], "venue": "Linear Algebra and its Applications, 315(1):39\u201359,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "On the 0\u20131-maximization of positive definite quadratic forms", "author": ["P. Gritzmann", "V. Klee"], "venue": "Operations Research Proceedings 1988, pages 222\u2013227. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Convex Analysis and Minimization Algorithms: Part 1: Fundamentals, volume 1", "author": ["J.-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions", "author": ["D. Hsu", "S.M. Kakade"], "venue": "Proceedings of the 4th conference on Innovations in Theoretical Computer Science (ITCS), pages 11\u201320. ACM,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Independent component analysis, volume 46", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": "John Wiley & Sons,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Algorithms for clustering data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": "Prentice-Hall, Inc., Upper Saddle River, NJ, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Spectral clustering as mapping to a simplex", "author": ["P. Kumar", "N. Narasimhan", "B. Ravindran"], "venue": "2013 ICML workshop on Spectral Learning,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D.R. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "ICCV, pages 416\u2013425,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A random walks view of spectral segmentation", "author": ["M. Meil\u0103", "J. Shi"], "venue": "AI and Statistics (AISTATS),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in neural information processing systems, 2:849\u2013856,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton Landmarks in Mathematics. Princeton University Press, Princeton, NJ,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888\u2013905,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of spectral clustering algorithms", "author": ["D. Verma", "M. Meil\u0103"], "venue": "Technical report, University of Washington CSE Department, Seattle, WA 98195-2350, 2003. doi=10.1.1.57.6424, Accessed online via CiteSeerx 5 Mar", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and computing, 17(4):395\u2013416,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "The hidden convexity of spectral clustering", "author": ["J. Voss", "M. Belkin", "L. Rademacher"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, pages 2108\u20132114,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Perron cluster analysis and its connection to graph partitioning for noisy data", "author": ["M. Weber", "W. Rungsarityotin", "A. Schliep"], "venue": "Konrad-Zuse-Zentrum f\u00fcr Informationstechnik Berlin,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A study of the fixed points and spurious solutions of the deflation-based fastica algorithm", "author": ["T. Wei"], "venue": "Neural Computing and Applications, pages 1\u201312,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Model-based clustering and data transformations for gene expression data supplementary web site", "author": ["K.Y. Yeung", "C. Fraley", "A. Murua", "A.E. Raftery", "W.L. Ruzzo"], "venue": "http://faculty. washington.edu/kayee/model/, 2001. Accessed: 20 Jan", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Model-based clustering and data transformations for gene expression data", "author": ["K.Y. Yeung", "C. Fraley", "A. Murua", "A.E. Raftery", "W.L. Ruzzo"], "venue": "Bioinformatics, 17(10):977\u2013987,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on (ICCV), pages 313\u2013319,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "There is an extensive literature on the subject, including a number of different methodologies as well as their various practical and theoretical aspects [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 13, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 1, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 23, "context": "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].", "startOffset": 204, "endOffset": 219}, {"referenceID": 18, "context": "Typical algorithms for multiway spectral clustering follow a two-step process: \u2217A short version of this paper previously appeared in the proceedings of the Thirtieth AAAI Conference on Artificial Intelligence [20].", "startOffset": 209, "endOffset": 213}, {"referenceID": 17, "context": "The meaning can be explained by spectral graph theory as relaxations of multiway cut problems [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "There are also connections to other areas of machine learning and mathematics, in particular to the geometry of the underlying space from which the data is sampled [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 19, "context": "This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications).", "startOffset": 158, "endOffset": 166}, {"referenceID": 10, "context": "This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications).", "startOffset": 158, "endOffset": 166}, {"referenceID": 8, "context": "The proposed approach relies on an optimization problem resembling certain Independent Component Analysis techniques, such as FastICA (see [10] for a broad overview).", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "Interestingly, while maximizing a convex function over a convex domain is generally difficult (even maximizing a positive definite quadratic form over the continuous cube [0, 1]n is NP-hard2), our setting allows for efficient optimization.", "startOffset": 171, "endOffset": 177}, {"referenceID": 0, "context": "[1] use the method of moments to recover a continuous simplex given samples from the uniform probability distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Also, one of the results of Hsu and Kakade [9] shows recovery of parameters in a Gaussian Mixture Model using the moments of order three, and this result can be thought of as a case of the basis recovery problem.", "startOffset": 43, "endOffset": 46}, {"referenceID": 20, "context": "In particular, typical versions of FastICA are known to have spurious maxima [22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "This follows from [7] together with Fact 6 below.", "startOffset": 18, "endOffset": 21}, {"referenceID": 17, "context": "The following well known property of the graph Laplacian (see [19] for a review) helps shed light on its importance: Given u \u2208 Rn, uLu = 1 2 \u2211", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "We formulate this result as follows (see [21], [18, Proposition 5], and [15, Proposition 1] for related statements).", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).", "startOffset": 166, "endOffset": 173}, {"referenceID": 17, "context": "Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).", "startOffset": 166, "endOffset": 173}, {"referenceID": 23, "context": "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].", "startOffset": 207, "endOffset": 214}, {"referenceID": 12, "context": "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].", "startOffset": 207, "endOffset": 214}, {"referenceID": 15, "context": "Spectral clustering was first applied to image segmentation by Shi and Malik [17], and it has remained a popular application of spectral clustering.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Figure 2 and Figure 3 show several segmentations produced by HBRopt-gabs and spherical k-means on several example images from the BSDS300 test set [13].", "startOffset": 147, "endOffset": 151}, {"referenceID": 22, "context": "We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle.", "startOffset": 54, "endOffset": 62}, {"referenceID": 21, "context": "We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle.", "startOffset": 54, "endOffset": 62}, {"referenceID": 13, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "4 of [8] for strictly convex functions, and can be proven in a similar fashion.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "2 of Hiriart-Urruty and Lemar\u00e9chal [8].", "startOffset": 35, "endOffset": 38}], "year": 2016, "abstractText": "In recent years, spectral clustering has become a standard method for data analysis used in a broad range of applications. In this paper we propose a new class of algorithms for multiway spectral clustering based on optimization of a certain \u201ccontrast function\u201d over the unit sphere. These algorithms, partly inspired by certain Independent Component Analysis techniques, are simple, easy to implement and efficient. Geometrically, the proposed algorithms can be interpreted as hidden basis recovery by means of function optimization. We give a complete characterization of the contrast functions admissible for provable basis recovery. We show how these conditions can be interpreted as a \u201chidden convexity\u201d of our optimization problem on the sphere; interestingly, we use efficient convex maximization rather than the more common convex minimization. We also show encouraging experimental results on real and simulated data. keywords: spectral clustering, convex maximization, basis recovery", "creator": "LaTeX with hyperref package"}}}