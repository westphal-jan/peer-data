{"id": "1403.1024", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2014", "title": "On learning to localize objects with minimal supervision", "abstract": "Learning to localize objects with munja minimal supervision 43-car is an comparators important phytoplankton problem monooxygenase in eachnet computer vision, since golota large fully art-1stld annotated datasets are corsicana extremely costly to obtain. In scoones this paper, imabari we maheswaran propose a new method hitchner that achieves prasar this bamforth goal uralla with only sanjakbey image - sharrer level feuillet labels genre of whether the solvberg objects are present sceptic or hawalas not. Our approach anglo-australian combines egor a discriminative aveline submodular cover mannschaft problem elwha for a\u00efr automatically discovering 2,661 a set woundings of zahf positive hartsock object reworkings windows with a guardianship smoothed rexrode latent eurlings SVM graig formulation. inviolable The profil latter allows mlst us to leverage vanquishes efficient Quasi - Newton 4-18 optimization techniques. oversaw Our scholar experiments chokeslammed demonstrate seowon that the plattsburg proposed approach seventh-day provides menacing approximately 70% ballfields relative cruickshank improvement 2,069 in junagadh average precision over the 5-all current right state of yukai the art fonck on standard benchmark ambacang datasets.", "histories": [["v1", "Wed, 5 Mar 2014 07:21:20 GMT  (8077kb,D)", "https://arxiv.org/abs/1403.1024v1", "10 pages"], ["v2", "Fri, 14 Mar 2014 00:50:26 GMT  (8077kb,D)", "http://arxiv.org/abs/1403.1024v2", "10 pages"], ["v3", "Mon, 17 Mar 2014 21:04:49 GMT  (8077kb,D)", "http://arxiv.org/abs/1403.1024v3", "10 pages"], ["v4", "Thu, 15 May 2014 22:08:59 GMT  (3437kb,D)", "http://arxiv.org/abs/1403.1024v4", null]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hyun oh song", "ross b girshick", "stefanie jegelka", "julien mairal", "za\u00efd harchaoui", "trevor darrell"], "accepted": true, "id": "1403.1024"}, "pdf": {"name": "1403.1024.pdf", "metadata": {"source": "CRF", "title": "On learning to localize objects with minimal supervision", "authors": ["Hyun Oh Song", "Ross Girshick", "Stefanie Jegelka", "Julien Mairal", "Zaid Harchaoui", "Trevor Darrell"], "emails": ["SONG@EECS.BERKELEY.EDU", "RBG@EECS.BERKELEY.EDU", "STEFJE@EECS.BERKELEY.EDU", "JULIEN.MAIRAL@INRIA.FR", "ZAID.HARCHAOUI@INRIA.FR", "TREVOR@EECS.BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "The classical paradigm for learning object detection models starts by annotating each object instance, in all training images, with a bounding box. However, this exhaustive labeling approach is costly and error prone for large-scale datasets. The massive amount of textually annotated visual data available online inspires a different, more challenging, research problem. Can weakly-labeled imagery, without bounding boxes, be used to reliably train object detectors?\nIn this alternative paradigm, the goal is to learn to localize objects with minimal supervision (Weber et al., 2000a;b). We focus on the case where the learner has access to binary image labels that encode whether an image contains the target object or not, without access to any instance level annotations (i.e., bounding boxes).\nOur approach starts by reducing the set of possible image\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nlocations that contain the object of interest from millions to thousands per image, using the selective search window proposal technique introduced by Uijlings et al. (2013). Then, we formulate a discriminative submodular cover algorithm to discover an initial set of image windows that are likely to contain the target object. After training a detection model with this initial set, we refine the detector using a novel smoothed formulation of latent SVM (Andrews et al., 2003; Felzenszwalb et al., 2010). We employ recently introduced object detection features, based on deep convolutional neural networks (Donahue et al., 2014; Girshick et al., 2014), to represent the window proposals for clustering and detector training.\nCompared to prior work on weakly-supervised detector training, we show substantial improvements on the standard evaluation metric (detection average precision on PASCAL VOC). Quantitatively, our approach achieves a 50% relative improvement in mean average precision over the current state-of-the-art for weakly-supervised learning."}, {"heading": "2. Related work", "text": "Our work is related to three active research areas: (1) weakly-supervised learning, (2) unsupervised discovery of mid-level visual elements, and (3) co-segmentation.\nWe build on a number of previous approaches for training object detectors from weakly-labeled data. In nearly all cases, the task is formulated as a multiple instance learning (MIL) problem (Long & Tan, 1996). In this formulation, the learner has access to an image-level label indicating the presence or absence of the target class, but not its location (if it is present). The challenge faced by the learner is to find the sliver of signal present in the positive images, but absent from the negative images. The implicit assumption is that this signal will correspond to the positive class.\nAlthough there have been recent works on convex relaxations (Li et al., 2013; Joulin & Bach, 2012), most MIL algorithms start from an initialization and then perform some form of local optimization. Early efforts, such as (Weber\nar X\niv :1\n40 3.\n10 24\nv4 [\ncs .C\nV ]\n1 5\nM ay\n2 01\net al., 2000a;b; Galleguillos et al., 2008; Fergus et al., 2007; Crandall & Huttenlocher, 2006; Chum & Zisserman, 2007; Chen et al., 2013), focused on datasets with strong objectin-the-center biases (e.g. Caltech-101). This simplified setting enabled clarity and focus on the MIL formulation, image features, and classifier design, but masked the vexing problem of finding a good initialization in data where such helpful biases are absent.\nMore recent work, such as (Siva & Xiang, 2011; Siva et al., 2012), attempts to learn detectors, or simply automatically generate bounding box annotations from much more challenging datasets such as PASCAL VOC (Everingham et al., 2010). In this data regime, focusing on initialization is crucial and carefully designed heuristics, such as shrinking bounding boxes (Russakovsky et al., 2012), are often employed.\nRecent literature on unsupervised mid-level visual element discovery (Doersch et al., 2012; Singh et al., 2012; Endres et al., 2013; Juneja et al., 2013; Raptis et al., 2012) uses weak labels to discover visual elements that occur commonly in positive images but not in negative images. Discovered visual element representation were shown to successfully provide discriminative information in classifying images into scene types. The most recent work (Doersch et al., 2013) presents a discriminative mode seeking formulation and draws connections between discovery and meanshift algorithms (Fukunaga & Hostetler, 1975).\nThe problem of finding common structure is related to the challenging setting of co-segmentation (Rother et al., 2006; Joulin et al., 2010; Alexe et al., 2010), which is the unsupervised segmentation of an object that is present in multiple images. While in this paper we do not address pixel-level segmentation, we employ ideas from co-segmentation: the intuition behind our submodular cover framework in Section 4 is shared with CoSand (Kim et al., 2011). Finally, submodular covering ideas have recently been applied to (active) filtering of hypothesis after running a detector, and without the discriminative flavor we propose (Barinova et al., 2012; Chen et al., 2014)."}, {"heading": "3. Problem formulation", "text": "Our goal is to learn a detector for a visual category from a set of images, each with a binary label. We model an image as a set of overlapping rectangular windows and follow a standard approach to detection: reduce the problem of detection to the problem of binary classification of image windows. However, at training time we are only given image-level labels, which leads to a classic multiple instance learning (MIL) problem. We can think of each image as a \u201cbag\u201d of instances (rectangular windows) and the binary image label y = 1 specifies that the bag contains at\nleast one instance of the target category. The label y = \u22121 specifies that the image contains no instances of the category. During training, no instance labels are available.\nMIL problems are typically solved (locally) by finding a local minimum of a non-convex objective function, such as MI-SVM (Andrews et al., 2003). In practice, the quality of the local solution depends heavily on the quality of the initialization. We therefore focus extensively on finding a good initialization. In Section 4, we develop an initialization method by formulating a discriminative set multicover problem that can be solved approximately with a greedy algorithm. This initialization, without further MIL refinement, already produces good object detectors, validating our approach. However, we can further improve these detectors by optimizing the MIL objective. We explore two alternative MIL objectives in Section 5. The first is the standard Latent SVM (equivalently MI-SVM) objective function, which can be optimized by coordinate descent on an auxiliary objective that upper-bounds the LSVM objective. The second method is a novel technique that smoothes the Latent SVM objective and can be solved more directly with unconstrained smooth optimization techniques, such as L-BFGS (Nocedal & Wright, 1999). Our experimental results show modest improvements from our smoothed LSVM formulation on a variety of MIL datasets."}, {"heading": "4. Finding objects via submodular cover", "text": "Learning with LSVM is a chicken and egg problem: The model weights are needed to infer latent annotations, but the latent annotations are needed to estimate the model weights. To initialize this process, we approximately identifying jointly present objects in a weakly supervised manner. The experiments show a significant effect from this initialization. Our procedure implements two essential assumptions: (i) the correct boxes are similar, in an appropriate feature space, across positive images (or there are few modes), and (ii) the correct boxes do not occur in the negative images. In short, in the similarity graph of all boxes we seek dense subgraphs that only span the positive images. Finding such subgraphs is a nontrivial combinatorial optimization problem.\nThe problem of finding and encoding a jointly present signal in images is an old one, and has been addressed by clustering, minimum description length priors, and the concept of exemplar (Darrell et al., 1990; Leibe et al., 2004; Micolajczyk et al., 2006; Kim et al., 2011). These approaches share the idea that a small number of exemplars or clusters should well encode the shared information we are interested in. We formalize this intuition as a flexible submodular cover problem. However, we also have label information at hand that can help identify correct boxes. We therefore integrate into our covering framework the relevance\nfor positively versus negatively labeled images, generalizing ideas from (Doersch et al., 2012). This combination allows us to find multiple modes of the object appearance distribution.\nLet P be the set of all positive images. Each image contains a set BI = {b1, . . . , bm} of candidate bounding boxes generated from selective search region proposals (Uijlings et al., 2013). In practice, there are about 2000 region proposal boxes per image and about 5000 training images in the PASCAL VOC dataset. Ultimately, we will define a function F (S) on sets S of boxes that measures how well the set S represents P . For each box b, we find its nearest neighbor box in each (positive and negative) image. We sort the set N (b) of all such neighbors of b in increasing order by their distance to b. This can be done in parallel. We will define a graph using these nearest neighbors that allows us to optimize for a small set of boxes S that are (i) relevant (occur in many positive images); (ii) discriminative (dissimilar to the boxes in the negative images); and (iii) complementary (capture multiple modes).\nWe construct a bipartite graph G = (V,U , E) whose nodes V and U are all boxes occurring in P (each b occurs once in V and once in U). The nodes in U are partitioned into groups BI : BI contains all boxes from image I \u2208 P . The edges E are formed by connecting each node (box) b \u2208 V to its top k neighbors in N (b) \u2286 U from positive images. Figure 1 illustrates the graph. Connecting only to the top k neighbors (instead of all) implements discriminativeness: the neighbors must compete. If b occurs in positively and negatively labeled images equally, then many top-k closest neighbors in N (b) stem from negative images. Consequently, bwill not be connected to many nodes (boxes from P) in G. We denote the neighborhood of a set of nodes S \u2286 V by \u0393(S) = {b \u2208 U | \u2203(v, b) \u2208 E with v \u2208 S}.\nLet S \u2286 V denote a set of selected boxes. We define a\ncovering score covI,t(S) for each I that is determined by a covering threshold t and a scalar, nondecreasing concave function g : R+ \u2192 R+:\ncovI,t(S) = g(min{t, |\u0393(S) \u2229 BI |}). (1) This score measures how many boxes in BI are neighbors of S and thus \u201ccovered\u201d. We gain from covering up to t boxes from BI \u2013 anything beyond that is considered redundant. The total covering score of a set S \u2286 V is then\nF (S) = \u2211\nI\u2208P covI,t(S). (2)\nThe threshold t balances relevance and complementarity: let, for simplicity, g = id. If t = 1, then a set that maximizes covI,t(S) contains boxes from many different images, and few from a single image. The selected neighborhoods are very complementary, but some of them may not be very relevant and cover outliers. If t is large, then any additionally covered box yields a gain, and the best boxes b \u2208 V are those with the largest degree. A box has large degree if many of its closest neighbors in N (b) are from positive images. This also means b is discriminative and relevant for P . Lemma 1. The function F : 2V \u2192 R+ defined in Equation (2) is nondecreasing and submodular.\nA set function is submodular if it satisfies diminishing marginal returns: for all v and S \u2286 T \u2286 V \\ {v}, it holds that F (S \u222a {v})\u2212 F (S) \u2265 F (T \u222a {v})\u2212 F (T ).\nProof. First, the function S 7\u2192 |\u0393(S) \u2229 BI | is a covering function and thus submodular: let S \u2282 T \u2286 V \\ b. Then \u0393(S) \u2286 \u0393(T ) and therefore\n|\u0393(T \u222a {b})| \u2212 |\u0393(T )| = |\u0393(b) \\ \u0393(T )| (3) \u2264 |\u0393(b) \\ \u0393(S)| (4) = |\u0393(S \u222a {b})| \u2212 |\u0393(S)|. (5)\nThe same holds when intersecting with BI . Thus, covt,I(S) is a nondecreasing concave function of a submodular function and therefore submodular. Finally, F is a sum of submodular functions and hence also submodular. Monotonicity is obvious.\nWe aim to select a representative subset S \u2286 V with minimum cardinality:\nmin S\u2286V |S| s.t. F (S) \u2265 \u03b1F (V) (6)\nfor \u03b1 \u2208 (0, 1]. We optimize this via a greedy algorithm: let S0 = \u2205 and, in each step \u03c4 , add the node v that maximizes the marginal gain F (S\u03c4 \u222a {v})\u2212 F (S\u03c4 ). Lemma 2. The greedy algorithm solves Problem (6) within an approximation factor of 1 + log ( kg(1)\ng(t)\u2212g(t\u22121)\n) =\nO(log k).\nLemma 2 says that the algorithm returns a set S\u0302 with F (S\u0302) \u2265 \u03b1F (V) and |S\u0302| \u2264 O(log k)|S\u2217|, where S\u2217 is an optimal solution. This result follows from the analysis by Wolsey (1982) (Thm. 1) adapted to our setting. To get a better intuition for the formulation (6) we list some special cases: Min-cost cover. With t = 1 and g(a) = a being the identity, Problem 6 becomes a min-cost cover problem. Such straightforward covering formulations have been used for filtering after running a detector (Barinova et al., 2012). Maximum relevance. A minimum-cost cover merely focuses on complementarity of the selected nodes S, which may include rare outliers. At the other extreme (t large), we would merely select by the number of neighbors (Doersch et al. (2012) choose one single N (b) that way). Multi-cover. To smoothly move between the two extremes, one may choose t > 1 and g to be sub-linear. This trades off representation, relevance, and discriminativeness.\nIn Figure 2, we visualize top 5 nearest neighbors with positive labels in the first chosen cluster S1 for all 20 classes on the PASCAL VOC data. Our experiments in Section 6 show the benefits of our framework. Potentially, the results might improve even further when using the complementary mode shifts of (Doersch et al., 2013) as a pre-selection step before covering."}, {"heading": "5. Iterative refinement with latent variables", "text": "In this section, we review the latent SVM formulation, and we propose a simple smoothing technique enabling us to use classical techniques for unconstrained smooth optimization. Figure 3 illustrates our multiple instance learning analogy for object detection with one-bit labels."}, {"heading": "5.1. Review of latent SVM", "text": "For a binary classification problem, the latent SVM formulation consists of learning a decision function involving a maximization step over a discrete set of configurations Z . Given a data point x in Rp that we want to classify, and some learned model parameters w in Rd, we select a label y in {\u22121,+1} as follows:\ny = sign ( max z\u2208Z w\u1d40\u03c6(x, z) ) , (7)\nwhere z is called a \u201clatent variable\u201d chosen among the set Z . For object detection, Z is typically a set of bounding boxes, and maximizing over Z amounts to finding a bounding box containing the object. In deformable part models (Felzenszwalb et al., 2010), the set Z contains all possible part configurations, each part being associated to a position in the image. The resulting set Z has exponential size, but (7) can be solved efficiently with dynamic programming techniques for particular choices of \u03c6.\nLearning the model parameters w is more involved than solving a simple SVM problem. We are given some training data {(xi, yi)}ni=1, where the vectors xi are in Rp and the scalars yi are binary labels in {1,\u22121}. Then, the latent SVM formulation becomes\nmin w\u2208Rd\n1 2 \u2016w\u201622 + C n\u2211 i=1 ` ( yi, max z\u2208Z w\u1d40\u03c6(xi, z) ) , (8)\nwhere ` : R\u00d7R\u2192 R is the hinge loss defined as `(y, y\u0302) = max(0, 1\u2212yy\u0302), which encourages the decision function for each training example to be the same as the corresponding label. Similarly, other loss functions can be used such as the logistic or squared hinge loss.\nProblem (8) is nonconvex and nonsmooth, making it hard to tackle. A classical technique to obtain an approximate solution is to use a difference of convex (DC) programming technique, called concave-convex procedure (Yuille & Rangarajan, 2003; Yu & Joachims, 2009). We remark that the part of (8) corresponding to negative examples is convex with respect to w. It is indeed easy to show that each corresponding term can be written as a pointwise maximum of convex functions, and is thus convex (see Boyd & Vandenberghe, 2004): when yi = \u22121, ` (yi,maxz\u2208Z w \u1d40\u03c6(xi, z)) = maxz\u2208Z `(yi,w \u1d40\u03c6(xi,x)). On the other hand, the part corresponding to positive examples is concave, making the objective (8) suitable to DC programming. Even though such a procedure does not have any theoretical guarantee about the quality of the optimization, it monotonically decreases the value of the objective and performs relatively well when the problem is well initialized (Felzenszwalb et al., 2010).\nWe propose a smooth formulation of latent SVM, with two main motives. First, smoothing the objective function of latent SVM allows the use of efficient second-order optimization algorithms such as quasi-Newton (Nocedal & Wright, 1999) that can leverage curvature information to speed up convergence. Second, as we show later, smoothing the latent SVM boils down to considering the top-N configurations in the maximization step in place of the top1 configuration in the regular latent SVM. As a result, the smooth latent SVM training becomes more robust to unreliable configurations in the early stages, since a larger set of plausible configurations is considered at each maximization step."}, {"heading": "5.2. Smooth formulation of LSVM", "text": "In the objective (8), the hinge loss can be easily replaced by a smooth alternative, e.g., squared hinge, or logistic loss. However, the non-smooth points induced by the following functions are more difficult to handle\nfxi(w) := max z\u2208Z\nw\u1d40\u03c6(xi, z). (9)\nWe propose to use a smoothing technique studied by Nesterov (2005) for convex functions.\nNesterov\u2019s smoothing technique We only recall here the simpler form of Nesterov\u2019s results that is relevant for our purpose. Consider a non-smooth function that can be written in the following form:\ng(w) := max u\u2208\u2206\n\u3008Aw,u\u3009 , (10)\nwhere u \u2208 Rm, A is in Rm\u00d7d, and \u2206 denotes the probability simplex, \u2206 = {x : \u2211m i=1 xi = 1, xi \u2265 0}. Smoothing here consists of adding a strongly convex function \u03c9 in the maximization problem\ng\u00b5(w) := max u\u2208\u2206\n[ \u3008Aw,u\u3009 \u2212 \u00b5\n2 \u03c9(u)\n] . (11)\nThe resulting function g\u00b5 is differentiable for all \u00b5 > 0, and its gradient is\n\u2207g\u00b5(w) = A\u1d40u?(w), (12)\nwhere u?(w) is the unique solution of (11). The parameter \u00b5 controls the amount of smoothing. Clearly, g\u00b5(w)\u2192 g(w) for all w \u2208 W as \u00b5\u2192 0. As Nesterov (2005) shows, for a given target approximation accuracy , there is an optimal amount of smoothing \u00b5( ) that can be derived from a convex optimization perspective using the strong convexity parameter of \u03c9(\u00b7) on \u2206 and the (usually unknown) Lipschitz constant of g. In the experiments, we shall simply learn the parameter \u00b5 from data.\nSmoothing the latent SVM We now apply Nesterov\u2019s smoothing technique to the latent SVM objective function. As we shall see, the smoothed objective takes a simple form, which can be efficiently computed in the latent SVM framework. Furthermore, smoothing latent SVM implicitly models uncertainty in the selection of the best configuration z in Z , as shown by Kumar et al. (2012) for a different smoothing scheme.\nIn order to smooth the functions fxi defined in (9), we first notice that\nfxi(w) = max u\u2208\u2206 \u3008Axiw,u\u3009, (13)\nwhere Axi is a matrix of size |Z| \u00d7 d such that the j-th row of Axi is the feature vector \u03c6(xi, zj) and zj is the j-th element of Z . Considering any strongly convex function \u03c9 and parameter \u00b5 > 0, the smoothed latent SVM objective is obtained by replacing in (8) \u2022 the functions fxi by their smoothed counterparts fxi,\u00b5 obtained by applying (11) to (13); \u2022 the non-smooth hinge-loss function l by any smooth loss.\nObjective and gradient evaluations An important issue remains the computational tractability of the new formulation in terms of objective and gradient evaluations, in order to use quasi-Newton optimization techniques. The choice of the strongly convex function \u03c9 is crucial in this respect.\nThere are two functions known to be strongly convex on the simplex: i) the Euclidean norm, ii) the entropy. In the case of the Euclidean-norm \u03c9(u) = \u2016u\u201622, it turns out that the smoothed counterpart can be efficiently computed using a projection on the simplex, as shown below.\nu?(w) = arg min u\u2208\u2206 \u2225\u2225\u2225\u2225 1\u00b5Aw \u2212 u \u2225\u2225\u2225\u22252\n2\n, (14)\nwhere u?(w) is the solution of (11). Computing Aw requires a priori O(|Z|d) operations. The projection can be computed in O(|Z|) (see, e.g., Bach et al., 2012). Once u? is obtained, computing the gradient requires O(d\u2016u?\u20160) operations, where \u2016u?\u20160 is the number of non-zero entries in u?.\nWhen the set Z is large, these complexities can be improved by leveraging two properties. First, the projection on the simplex is known to produce sparse solutions, the smoothing parameter \u00b5 controlling the sparsity of u?; second, the projection preserves the order of the variables. As a result, the following heuristic can be justified. Assume that for some N < |Z|, we can obtain the top-N entries of Aw without exhaustively exploring Z . Then, performing the projection on these reduced set of N variables yields a vector u\u2032 which can be shown to be optimal for the original problem (14) whenever \u2016u\u2032\u20160 < N . In other words, whenever N is large enough and \u00b5 small enough, computing the gradient of fxi,\u00b5 can be done inO(Nd) operations. We use this heuristic in all our experiments."}, {"heading": "6. Experiments", "text": "We performed two sets of experiments, one on a multiple instance learning dataset (Andrews et al., 2003) and the other on the PASCAL VOC 2007 data (Everingham et al.). The first experiment was designed to compare the multiple instance learning bag classification performance of LSVM with Smooth LSVM (SLSVM). The second experiment evaluates detection accuracy (measured in average precision) of our framework in comparison to baselines."}, {"heading": "6.1. Multiple instance learning datasets", "text": "We evaluated our method in Section 5 on standard multiple instance learning datasets (Andrews et al., 2003). For preprocessing, we centered each feature dimension and `2 normalize the data. For fair comparison with (Andrews et al., 2003), we use the same initialization, where the initial weight vector is obtained by training an SVM with all\nthe negative instances and bag-averaged positive instances. For this experiment, we performed 10 fold cross validation on C and \u00b5. Table 1 shows the experimental results. Without the bias, our method significantly performs better than LSVM method and with the bias, our method shows modest improvement in most cases."}, {"heading": "6.2. Weakly-supervised object detection", "text": "To implement our weakly-supervised detection system we need suitable image features for computing the nearest neighbors of each image window in Section 4 and for learning object detectors. We use the recently proposed R-CNN (Girshick et al., 2014) detection framework to compute features on image windows in both cases. Specifically, we use the convolutional neural network (CNN) distributed with DeCAF (Donahue et al., 2014), which is trained on the ImageNet ILSVRC 2012 dataset (using only image-level annotations). We avoid using the better performing CNN that is fine-tuned on PASCAL data, as described in (Girshick et al., 2014), because fine-tuning requires instance-level annotations.\nWe report detection accuracy as average precision on the standard benchmark dataset for object detection, PASCAL VOC 2007 test (Everingham et al.). We compare to five different baseline methods that learn object detectors with limited annotations. Note that other baseline methods use additional information besides the one-bit image-level annotations. Deselaers et al. (2010; 2012) use a set of 799 images with bounding box annotations as meta-training data. In addition to bounding box annotations, Deselaers et al. (2010; 2012); Pandey & Lazebnik (2011) use extra instance level annotations such as pose, difficult and truncated. Siva et al. (2012); Russakovsky et al. (2012) use difficult instance annotations but not pose or truncated. First, we report the detection average precision on 6 subsets of classes in table 2 to compare with Deselaers et al. (2010; 2012); Pandey & Lazebnik (2011).\nTo evaluate the efficacy of our initialization, we compare it to the state-of-the-art algorithm recently proposed by (Siva et al., 2012). Their method constructs a set of positive windows by looping over each positive image and picking the instance that has the maximum distance to its nearest neighbor over all negative instances (and thus the name negative data mining algorithm). For a fair comparison, we used the same window proposals, the same features (Girshick et al., 2014), the same L2 distance metric, and the same PASCAL 2007 detection evaluation criteria. The class mean average precision for the mining algorithm was 11.6% compared to 29.0% obtained by our initialization procedure. Figure 4 visualizes some command failure modes in our implementation of (Siva et al., 2012). Since the negative mining method does not take into account the similarity among\npositive windows (in contrast to our method) our intuition is that the method is less robust to intra-class variations and background clutter. Therefore, it often latches onto background objects (i.e. hurdle in horse images, street signs in bus images), onto parts of the full objects (i.e. wheels of bicycles), or merges two different objects (i.e. rider and motorcycle). It is worth noting that Pandey & Lazebnik (2011); Siva et al. (2012) use the CorLoc metric1 as the evaluation metric to report results on PASCAL test set. In contrast, in our experiments, we exactly follow the PASCAL VOC evaluation protocol (and use the PASCAL VOC devkit scoring software) and report detection average precision.\nTable 3 shows the detection result on the full PASCAL 2007 dataset. There are two baseline methods (Siva & Xiang, 2011; Russakovsky et al., 2012) which report the result on the full dataset. Unfortunately, we were not able to obtain the per-class average precision data from the authors of (Russakovsky et al., 2012) except the class mean average precision (mAP) of 15.0%. As shown in Table 3, the initial detector model trained from the constructed set of positive windows already produces good object detectors but we can provide further improvement by optimizing the MIL objective."}, {"heading": "7. Conclusion", "text": "We developed a framework for learning to localize objects with one-bit object presence labels. Our results show that the proposed framework can construct a set of positive windows to train initial detection models and improve the models with the refinement optimization method. We achieve state-of-the-art performance for object detection with minimal supervision on the standard benchmark object detection dataset. Source code will be available on the author\u2019s website."}, {"heading": "Acknowledgement", "text": "We thank Yong Jae Lee for helpful insights and discussions. H. Song was supported by Samsung Scholarship Foundation. J. Mairal and Z. Harchaoui were funded by the INRIA-UC Berkeley associated team \u201cHyperion\u201d, a grant from the France-Berkeley fund, the Gargantua project under program Mastodons of CNRS, and the LabEx PERSYVAL-Lab (ANR-11-LABX-0025). This work was partially supported by ONR N00014-11-1-0688, NSF, DARPA, and Toyota."}], "references": [{"title": "Classcut for unsupervised class segmentation", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "In ECCV,", "citeRegEx": "Alexe et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Alexe et al\\.", "year": 2010}, {"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "On detection of multiple object instances using hough transforms", "author": ["O. Barinova", "V. Lempitsky", "P. Kohli"], "venue": "IEEE TPAMI,", "citeRegEx": "Barinova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Barinova et al\\.", "year": 2012}, {"title": "Extracting visual knowledge from web data", "author": ["X. Chen", "A. Shrivastava", "A. Gupta. Neil"], "venue": "In ICCV,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Active detection via adaptive submodularity", "author": ["Y. Chen", "H. Shioi", "Montesinos", "C. Fuentes", "L.P. Koh", "S. Wich", "A. Krause"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "An exemplar model for learning object classes", "author": ["O. Chum", "A. Zisserman"], "venue": "In CVPR,", "citeRegEx": "Chum and Zisserman,? \\Q2007\\E", "shortCiteRegEx": "Chum and Zisserman", "year": 2007}, {"title": "Weakly supervised learning of part-based spatial models for visual object recognition", "author": ["D. Crandall", "D. Huttenlocher"], "venue": "In ECCV", "citeRegEx": "Crandall and Huttenlocher,? \\Q2006\\E", "shortCiteRegEx": "Crandall and Huttenlocher", "year": 2006}, {"title": "Segmentation by minimal description", "author": ["T. Darrell", "S. Sclaroff", "A. Pentland"], "venue": "In ICCV,", "citeRegEx": "Darrell et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Darrell et al\\.", "year": 1990}, {"title": "Localizing objects while learning their appearance", "author": ["T. Deselaers", "B. Alex", "V. Ferrari"], "venue": "In ECCV,", "citeRegEx": "Deselaers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Deselaers et al\\.", "year": 2010}, {"title": "Weakly supervised localization and learning with generic knowledge", "author": ["T. Deselaers", "B. Alex", "V. Ferrari"], "venue": null, "citeRegEx": "Deselaers et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deselaers et al\\.", "year": 2012}, {"title": "What makes paris look like paris", "author": ["C. Doersch", "S. Singh", "A. Gupta", "J. Sivic", "A. Efros"], "venue": "In SIGGRAPH,", "citeRegEx": "Doersch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2012}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["C. Doersch", "A. Gupta", "A. Efros"], "venue": "In NIPS,", "citeRegEx": "Doersch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2013}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In ICML,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Learning collections of part models for object recognition", "author": ["I. Endres", "K. Shih", "D. Hoeim"], "venue": "In CVPR,", "citeRegEx": "Endres et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Endres et al\\.", "year": 2013}, {"title": "The PASCAL Visual Object Classes (VOC) Challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE TPAMI,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Weakly supervised scale-invariant learning of models for visual recognition", "author": ["R. Fergus", "P. Perona", "A. Zisserman"], "venue": null, "citeRegEx": "Fergus et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2007}, {"title": "The estimation of the gradient of a density function, with applications in pattern recognition", "author": ["K. Fukunaga", "L. Hostetler"], "venue": "Information Theory,", "citeRegEx": "Fukunaga and Hostetler,? \\Q1975\\E", "shortCiteRegEx": "Fukunaga and Hostetler", "year": 1975}, {"title": "Weakly supervised object localization with stable segmentations", "author": ["C. Galleguillos", "B. Babenko", "A. Rabinovich", "S. Belongie"], "venue": "In ECCV,", "citeRegEx": "Galleguillos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Galleguillos et al\\.", "year": 2008}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "A convex relaxation for weakly supervised classifiers", "author": ["A. Joulin", "F. Bach"], "venue": "In ICML,", "citeRegEx": "Joulin and Bach,? \\Q2012\\E", "shortCiteRegEx": "Joulin and Bach", "year": 2012}, {"title": "Discriminative clustering for image co-segmentation", "author": ["A. Joulin", "F. Bach", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "Joulin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2010}, {"title": "Blocks that shout: Distinctive parts for scene classification", "author": ["M. Juneja", "A. Vedaldi", "V. Jawahar", "A. Zisserman"], "venue": "In CVPR,", "citeRegEx": "Juneja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Juneja et al\\.", "year": 2013}, {"title": "Distributed cosegmentation via submodular optimization on anisotropic diffusion", "author": ["G. Kim", "E.P. Xing", "L. Fei-Fei", "T. Kanade"], "venue": "In ICCV,", "citeRegEx": "Kim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2011}, {"title": "Modeling latent variable uncertainty for loss-based learning", "author": ["P Kumar", "B Packer", "D. Koller"], "venue": "In ICML,", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Combined object categorization and segmentation with an implicit chape model", "author": ["B. Leibe", "A. Leonardis", "B. Schiele"], "venue": "In ECCVW,", "citeRegEx": "Leibe et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Leibe et al\\.", "year": 2004}, {"title": "Convex and scalable weakly labeled svms", "author": ["Y. Li", "I. Tsang", "J. Kwok", "Z. Zhou"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "PAC learning axis aligned rectangles with respect to product distributions from multiple-instance examples", "author": ["P.M. Long", "L. Tan"], "venue": "In Proc. Comp. Learning Theory,", "citeRegEx": "Long and Tan,? \\Q1996\\E", "shortCiteRegEx": "Long and Tan", "year": 1996}, {"title": "Multiple object class detection with a generative model", "author": ["K. Micolajczyk", "G. Leibe", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "Micolajczyk et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Micolajczyk et al\\.", "year": 2006}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov,? \\Q2005\\E", "shortCiteRegEx": "Nesterov", "year": 2005}, {"title": "Scene recognition and weakly supervised object localization with deformable part-based models", "author": ["M. Pandey", "S. Lazebnik"], "venue": "In ICCV,", "citeRegEx": "Pandey and Lazebnik,? \\Q2011\\E", "shortCiteRegEx": "Pandey and Lazebnik", "year": 2011}, {"title": "Discovering discriminative action parts from mid-level video representations", "author": ["M. Raptis", "I. Kokkinos", "S. Soatto"], "venue": "In CVPR,", "citeRegEx": "Raptis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raptis et al\\.", "year": 2012}, {"title": "Cosegmentation of image pairs by histogram matching incorporating a global constraint into MRFs", "author": ["C. Rother", "T. Minka", "A. Blake", "V. Kolmogorov"], "venue": "In CVPR,", "citeRegEx": "Rother et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rother et al\\.", "year": 2006}, {"title": "Object-centric spatial pooling for image classification", "author": ["O. Russakovsky", "Y. Lin", "K. Yu", "L. Fei Fei"], "venue": "In ECCV,", "citeRegEx": "Russakovsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2012}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["S. Singh", "A. Gupta", "A. Efros"], "venue": "In ECCV,", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Weakly supervised object detector learning with model drift detection", "author": ["P. Siva", "T. Xiang"], "venue": "In ICCV,", "citeRegEx": "Siva and Xiang,? \\Q2011\\E", "shortCiteRegEx": "Siva and Xiang", "year": 2011}, {"title": "In defence of negative mining for annotating weakly labelled data", "author": ["P. Siva", "C. Russell", "T. Xiang"], "venue": "In ECCV,", "citeRegEx": "Siva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Siva et al\\.", "year": 2012}, {"title": "Selective search for object recognition", "author": ["J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders"], "venue": "In IJCV,", "citeRegEx": "Uijlings et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uijlings et al\\.", "year": 2013}, {"title": "Towards automatic discovery of object categories", "author": ["M. Weber", "M. Welling", "P. Perona"], "venue": "In CVPR,", "citeRegEx": "Weber et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Weber et al\\.", "year": 2000}, {"title": "Unsupervised learning of models for recognition", "author": ["M. Weber", "M. Welling", "P. Perona"], "venue": "In ECCV,", "citeRegEx": "Weber et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Weber et al\\.", "year": 2000}, {"title": "An analysis of the greedy algorithm for the submodular set covering problem", "author": ["L. Wolsey"], "venue": null, "citeRegEx": "Wolsey,? \\Q1982\\E", "shortCiteRegEx": "Wolsey", "year": 1982}, {"title": "Learning structural svms with latent variables", "author": ["C.N. Yu", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Yu and Joachims,? \\Q2009\\E", "shortCiteRegEx": "Yu and Joachims", "year": 2009}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation,", "citeRegEx": "Yuille and Rangarajan,? \\Q2003\\E", "shortCiteRegEx": "Yuille and Rangarajan", "year": 2003}], "referenceMentions": [{"referenceID": 15, "context": "After training a detection model with this initial set, we refine the detector using a novel smoothed formulation of latent SVM (Andrews et al., 2003; Felzenszwalb et al., 2010).", "startOffset": 128, "endOffset": 177}, {"referenceID": 12, "context": "We employ recently introduced object detection features, based on deep convolutional neural networks (Donahue et al., 2014; Girshick et al., 2014), to represent the window proposals for clustering and detector training.", "startOffset": 101, "endOffset": 146}, {"referenceID": 19, "context": "We employ recently introduced object detection features, based on deep convolutional neural networks (Donahue et al., 2014; Girshick et al., 2014), to represent the window proposals for clustering and detector training.", "startOffset": 101, "endOffset": 146}, {"referenceID": 34, "context": "locations that contain the object of interest from millions to thousands per image, using the selective search window proposal technique introduced by Uijlings et al. (2013). Then, we formulate a discriminative submodular cover algorithm to discover an initial set of image windows that are likely to contain the target object.", "startOffset": 151, "endOffset": 174}, {"referenceID": 26, "context": "Although there have been recent works on convex relaxations (Li et al., 2013; Joulin & Bach, 2012), most MIL algorithms start from an initialization and then perform some form of local optimization.", "startOffset": 60, "endOffset": 98}, {"referenceID": 36, "context": "More recent work, such as (Siva & Xiang, 2011; Siva et al., 2012), attempts to learn detectors, or simply automatically generate bounding box annotations from much more challenging datasets such as PASCAL VOC (Everingham et al.", "startOffset": 26, "endOffset": 65}, {"referenceID": 14, "context": ", 2012), attempts to learn detectors, or simply automatically generate bounding box annotations from much more challenging datasets such as PASCAL VOC (Everingham et al., 2010).", "startOffset": 151, "endOffset": 176}, {"referenceID": 33, "context": "In this data regime, focusing on initialization is crucial and carefully designed heuristics, such as shrinking bounding boxes (Russakovsky et al., 2012), are often employed.", "startOffset": 127, "endOffset": 153}, {"referenceID": 10, "context": "Recent literature on unsupervised mid-level visual element discovery (Doersch et al., 2012; Singh et al., 2012; Endres et al., 2013; Juneja et al., 2013; Raptis et al., 2012) uses weak labels to discover visual elements that occur commonly in positive images but not in negative images.", "startOffset": 69, "endOffset": 174}, {"referenceID": 34, "context": "Recent literature on unsupervised mid-level visual element discovery (Doersch et al., 2012; Singh et al., 2012; Endres et al., 2013; Juneja et al., 2013; Raptis et al., 2012) uses weak labels to discover visual elements that occur commonly in positive images but not in negative images.", "startOffset": 69, "endOffset": 174}, {"referenceID": 13, "context": "Recent literature on unsupervised mid-level visual element discovery (Doersch et al., 2012; Singh et al., 2012; Endres et al., 2013; Juneja et al., 2013; Raptis et al., 2012) uses weak labels to discover visual elements that occur commonly in positive images but not in negative images.", "startOffset": 69, "endOffset": 174}, {"referenceID": 22, "context": "Recent literature on unsupervised mid-level visual element discovery (Doersch et al., 2012; Singh et al., 2012; Endres et al., 2013; Juneja et al., 2013; Raptis et al., 2012) uses weak labels to discover visual elements that occur commonly in positive images but not in negative images.", "startOffset": 69, "endOffset": 174}, {"referenceID": 31, "context": "Recent literature on unsupervised mid-level visual element discovery (Doersch et al., 2012; Singh et al., 2012; Endres et al., 2013; Juneja et al., 2013; Raptis et al., 2012) uses weak labels to discover visual elements that occur commonly in positive images but not in negative images.", "startOffset": 69, "endOffset": 174}, {"referenceID": 11, "context": "The most recent work (Doersch et al., 2013) presents a discriminative mode seeking formulation and draws connections between discovery and meanshift algorithms (Fukunaga & Hostetler, 1975).", "startOffset": 21, "endOffset": 43}, {"referenceID": 32, "context": "The problem of finding common structure is related to the challenging setting of co-segmentation (Rother et al., 2006; Joulin et al., 2010; Alexe et al., 2010), which is the unsupervised segmentation of an object that is present in multiple images.", "startOffset": 97, "endOffset": 159}, {"referenceID": 21, "context": "The problem of finding common structure is related to the challenging setting of co-segmentation (Rother et al., 2006; Joulin et al., 2010; Alexe et al., 2010), which is the unsupervised segmentation of an object that is present in multiple images.", "startOffset": 97, "endOffset": 159}, {"referenceID": 0, "context": "The problem of finding common structure is related to the challenging setting of co-segmentation (Rother et al., 2006; Joulin et al., 2010; Alexe et al., 2010), which is the unsupervised segmentation of an object that is present in multiple images.", "startOffset": 97, "endOffset": 159}, {"referenceID": 23, "context": "While in this paper we do not address pixel-level segmentation, we employ ideas from co-segmentation: the intuition behind our submodular cover framework in Section 4 is shared with CoSand (Kim et al., 2011).", "startOffset": 189, "endOffset": 207}, {"referenceID": 2, "context": "Finally, submodular covering ideas have recently been applied to (active) filtering of hypothesis after running a detector, and without the discriminative flavor we propose (Barinova et al., 2012; Chen et al., 2014).", "startOffset": 173, "endOffset": 215}, {"referenceID": 4, "context": "Finally, submodular covering ideas have recently been applied to (active) filtering of hypothesis after running a detector, and without the discriminative flavor we propose (Barinova et al., 2012; Chen et al., 2014).", "startOffset": 173, "endOffset": 215}, {"referenceID": 7, "context": "The problem of finding and encoding a jointly present signal in images is an old one, and has been addressed by clustering, minimum description length priors, and the concept of exemplar (Darrell et al., 1990; Leibe et al., 2004; Micolajczyk et al., 2006; Kim et al., 2011).", "startOffset": 187, "endOffset": 273}, {"referenceID": 25, "context": "The problem of finding and encoding a jointly present signal in images is an old one, and has been addressed by clustering, minimum description length priors, and the concept of exemplar (Darrell et al., 1990; Leibe et al., 2004; Micolajczyk et al., 2006; Kim et al., 2011).", "startOffset": 187, "endOffset": 273}, {"referenceID": 28, "context": "The problem of finding and encoding a jointly present signal in images is an old one, and has been addressed by clustering, minimum description length priors, and the concept of exemplar (Darrell et al., 1990; Leibe et al., 2004; Micolajczyk et al., 2006; Kim et al., 2011).", "startOffset": 187, "endOffset": 273}, {"referenceID": 23, "context": "The problem of finding and encoding a jointly present signal in images is an old one, and has been addressed by clustering, minimum description length priors, and the concept of exemplar (Darrell et al., 1990; Leibe et al., 2004; Micolajczyk et al., 2006; Kim et al., 2011).", "startOffset": 187, "endOffset": 273}, {"referenceID": 10, "context": "for positively versus negatively labeled images, generalizing ideas from (Doersch et al., 2012).", "startOffset": 73, "endOffset": 95}, {"referenceID": 37, "context": ", bm} of candidate bounding boxes generated from selective search region proposals (Uijlings et al., 2013).", "startOffset": 83, "endOffset": 106}, {"referenceID": 2, "context": "Such straightforward covering formulations have been used for filtering after running a detector (Barinova et al., 2012).", "startOffset": 97, "endOffset": 120}, {"referenceID": 37, "context": "This result follows from the analysis by Wolsey (1982) (Thm.", "startOffset": 41, "endOffset": 55}, {"referenceID": 2, "context": "Such straightforward covering formulations have been used for filtering after running a detector (Barinova et al., 2012). Maximum relevance. A minimum-cost cover merely focuses on complementarity of the selected nodes S, which may include rare outliers. At the other extreme (t large), we would merely select by the number of neighbors (Doersch et al. (2012) choose one single N (b) that way).", "startOffset": 98, "endOffset": 359}, {"referenceID": 11, "context": "Potentially, the results might improve even further when using the complementary mode shifts of (Doersch et al., 2013) as a pre-selection step before covering.", "startOffset": 96, "endOffset": 118}, {"referenceID": 15, "context": "In deformable part models (Felzenszwalb et al., 2010), the set Z contains all possible part configurations, each part being associated to a position in the image.", "startOffset": 26, "endOffset": 53}, {"referenceID": 15, "context": "Even though such a procedure does not have any theoretical guarantee about the quality of the optimization, it monotonically decreases the value of the objective and performs relatively well when the problem is well initialized (Felzenszwalb et al., 2010).", "startOffset": 228, "endOffset": 255}, {"referenceID": 29, "context": "We propose to use a smoothing technique studied by Nesterov (2005) for convex functions.", "startOffset": 51, "endOffset": 67}, {"referenceID": 29, "context": "As Nesterov (2005) shows, for a given target approximation accuracy , there is an optimal amount of smoothing \u03bc( ) that can be derived from a convex optimization perspective using the strong convexity parameter of \u03c9(\u00b7) on \u2206 and the (usually unknown) Lipschitz constant of g.", "startOffset": 3, "endOffset": 19}, {"referenceID": 24, "context": "Furthermore, smoothing latent SVM implicitly models uncertainty in the selection of the best configuration z in Z , as shown by Kumar et al. (2012) for a different smoothing scheme.", "startOffset": 128, "endOffset": 148}, {"referenceID": 36, "context": "Visualization of some common failure cases of constructed positive windows by(Siva et al., 2012) vs our method.", "startOffset": 77, "endOffset": 96}, {"referenceID": 36, "context": "Red bounding boxes are constructed positive windows from (Siva et al., 2012).", "startOffset": 57, "endOffset": 76}, {"referenceID": 8, "context": "(Deselaers et al., 2010) 9.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "(Deselaers et al., 2012) 5.", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "(Russakovsky et al., 2012) 30.", "startOffset": 0, "endOffset": 26}, {"referenceID": 36, "context": "(Siva et al., 2012) with our features 23.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "We use the recently proposed R-CNN (Girshick et al., 2014) detection framework to compute features on image windows in both cases.", "startOffset": 35, "endOffset": 58}, {"referenceID": 12, "context": "Specifically, we use the convolutional neural network (CNN) distributed with DeCAF (Donahue et al., 2014), which is trained on the ImageNet ILSVRC 2012 dataset (using only image-level annotations).", "startOffset": 83, "endOffset": 105}, {"referenceID": 19, "context": "We avoid using the better performing CNN that is fine-tuned on PASCAL data, as described in (Girshick et al., 2014), because fine-tuning requires instance-level annotations.", "startOffset": 92, "endOffset": 115}, {"referenceID": 8, "context": "Deselaers et al. (2010; 2012) use a set of 799 images with bounding box annotations as meta-training data. In addition to bounding box annotations, Deselaers et al. (2010; 2012); Pandey & Lazebnik (2011) use extra instance level annotations such as pose, difficult and truncated.", "startOffset": 0, "endOffset": 204}, {"referenceID": 8, "context": "Deselaers et al. (2010; 2012) use a set of 799 images with bounding box annotations as meta-training data. In addition to bounding box annotations, Deselaers et al. (2010; 2012); Pandey & Lazebnik (2011) use extra instance level annotations such as pose, difficult and truncated. Siva et al. (2012); Russakovsky et al.", "startOffset": 0, "endOffset": 299}, {"referenceID": 8, "context": "Deselaers et al. (2010; 2012) use a set of 799 images with bounding box annotations as meta-training data. In addition to bounding box annotations, Deselaers et al. (2010; 2012); Pandey & Lazebnik (2011) use extra instance level annotations such as pose, difficult and truncated. Siva et al. (2012); Russakovsky et al. (2012) use difficult instance annotations but not pose or truncated.", "startOffset": 0, "endOffset": 326}, {"referenceID": 8, "context": "Deselaers et al. (2010; 2012) use a set of 799 images with bounding box annotations as meta-training data. In addition to bounding box annotations, Deselaers et al. (2010; 2012); Pandey & Lazebnik (2011) use extra instance level annotations such as pose, difficult and truncated. Siva et al. (2012); Russakovsky et al. (2012) use difficult instance annotations but not pose or truncated. First, we report the detection average precision on 6 subsets of classes in table 2 to compare with Deselaers et al. (2010; 2012); Pandey & Lazebnik (2011).", "startOffset": 0, "endOffset": 544}, {"referenceID": 36, "context": "To evaluate the efficacy of our initialization, we compare it to the state-of-the-art algorithm recently proposed by (Siva et al., 2012).", "startOffset": 117, "endOffset": 136}, {"referenceID": 19, "context": "For a fair comparison, we used the same window proposals, the same features (Girshick et al., 2014), the same L2 distance metric, and the same PASCAL 2007 detection evaluation criteria.", "startOffset": 76, "endOffset": 99}, {"referenceID": 36, "context": "Figure 4 visualizes some command failure modes in our implementation of (Siva et al., 2012).", "startOffset": 72, "endOffset": 91}, {"referenceID": 19, "context": "For a fair comparison, we used the same window proposals, the same features (Girshick et al., 2014), the same L2 distance metric, and the same PASCAL 2007 detection evaluation criteria. The class mean average precision for the mining algorithm was 11.6% compared to 29.0% obtained by our initialization procedure. Figure 4 visualizes some command failure modes in our implementation of (Siva et al., 2012). Since the negative mining method does not take into account the similarity among positive windows (in contrast to our method) our intuition is that the method is less robust to intra-class variations and background clutter. Therefore, it often latches onto background objects (i.e. hurdle in horse images, street signs in bus images), onto parts of the full objects (i.e. wheels of bicycles), or merges two different objects (i.e. rider and motorcycle). It is worth noting that Pandey & Lazebnik (2011); Siva et al.", "startOffset": 77, "endOffset": 910}, {"referenceID": 19, "context": "For a fair comparison, we used the same window proposals, the same features (Girshick et al., 2014), the same L2 distance metric, and the same PASCAL 2007 detection evaluation criteria. The class mean average precision for the mining algorithm was 11.6% compared to 29.0% obtained by our initialization procedure. Figure 4 visualizes some command failure modes in our implementation of (Siva et al., 2012). Since the negative mining method does not take into account the similarity among positive windows (in contrast to our method) our intuition is that the method is less robust to intra-class variations and background clutter. Therefore, it often latches onto background objects (i.e. hurdle in horse images, street signs in bus images), onto parts of the full objects (i.e. wheels of bicycles), or merges two different objects (i.e. rider and motorcycle). It is worth noting that Pandey & Lazebnik (2011); Siva et al. (2012) use the CorLoc metric1 as the evaluation metric to report results on PASCAL test set.", "startOffset": 77, "endOffset": 930}, {"referenceID": 33, "context": "There are two baseline methods (Siva & Xiang, 2011; Russakovsky et al., 2012) which report the result on the full dataset.", "startOffset": 31, "endOffset": 77}, {"referenceID": 33, "context": "Unfortunately, we were not able to obtain the per-class average precision data from the authors of (Russakovsky et al., 2012) except the class mean average precision (mAP) of 15.", "startOffset": 99, "endOffset": 125}], "year": 2014, "abstractText": "Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasiNewton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.", "creator": "TeX"}}}