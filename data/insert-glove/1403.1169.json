{"id": "1403.1169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2014", "title": "A proof challenge: multiple alignment and information compression", "abstract": "biagio These notes nassar pose trabalhadores a \" cantharellus proof lookit challenge \": a proof, or raymar disproof, of blotch the proposition kenya that \" principales For shield-shaped any given body hunch of 243.2 information, I, expressed o\u2019donnell as pirmasens a scrounger one - noshiro dimensional sequence 116.61 of atomic powerful symbols, a vitkovic multiple second-lowest alignment concept, described 386.75 in the barboursville document, provides x\u00e0tiva a humanise means scorpionfish of obara encoding campione all wet-season the redundancy that mechale may wildman exist elder in I. Aspects ceramica of 106.20 the 38-25 challenge keahey are described.", "histories": [["v1", "Tue, 4 Mar 2014 17:00:19 GMT  (7kb)", "http://arxiv.org/abs/1403.1169v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["j gerard wolff"], "accepted": false, "id": "1403.1169"}, "pdf": {"name": "1403.1169.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jgw@cognitionresearch.org;"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 3.\n11 69\nv1 [\ncs .A\nI] 4\nM ar\nconcept, described in the document, provides a means of encoding all the redundancy that may exist in I. Aspects of the challenge are described."}, {"heading": "1 Introduction", "text": "For several years, I have been developing the SP theory of intelligence, designed to simplify and integrate concepts across artificial intelligence, mainstream computing, and human perception and cognition, with information compression as a unifying theme [3, 2].\nA central idea in the SP theory is a concept of multiple alignment, to be described, which achieves the effect of compressing information.\nIn this connection, it would be good to have a formal proof, or disproof, of the following proposition:\nFor any given body of information, I, expressed as a one-dimensional sequence of atomic symbols, the multiple alignnment concept pro-\nvides a means of encoding all the redundancy that may exist in I.\nThe following sections describe relevant concepts and aspects of this \u201cproof challenge\u201d.\n\u2217Dr Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP); CognitionResearch.org, Menai Bridge, UK; jgw@cognitionresearch.org; +44 (0) 1248 712962; +44 (0) 7746 290775; Skype: gerry.wolff; Web: www.cognitionresearch.org."}, {"heading": "2 The matching and unification of patterns", "text": "To cut through some of the complexities in this area, I have found it useful to focus on a rather simple idea: that we may identify repetition or \u2018redundancy\u2019 in information by searching for patterns that match each other, and that we may reduce that redundancy and thus compress information by merging or \u2018unifying\u2019 two or more copies to make one. As just described, the principle loses information about the positions of all but one of the original patterns, but this can be remedied with any of three variants of the idea:\n\u2022 Chunking-with-codes. Here, the unified pattern is given a relatively short name, identifier, or \u2018code\u2019 which is used as a shorthand for the pattern or \u2018chunk\u2019. If, for example, the words \u201cTreaty on the Functioning of the European Union\u201d appear in several different places in a document, we may save space by writing the expression once, giving it a short name such as \u201cTFEU\u201d, and then using that name as a shorthand for the expression wherever it occurs.\n\u2022 Schema-plus-correction. This is like chunking-with-codes but the unified chunk of information may have variations or \u2018corrections\u2019 on different occasions. For example, a six-course menu in a restaurant may have the general form \u2018Menu1: Appetiser (S) sorbet (M) (P) coffee-and-mints\u2019, with choices at the points marked \u2018S\u2019 (starter), \u2018M\u2019 (main course), and \u2018P\u2019 (pudding). Then a particular meal may be encoded economically as something like \u2018Menu1:(3)(5)(1)\u2019, where the digits determine the choices of starter, main course, and pudding.\n\u2022 Run-length coding. This may be used where there is a sequence two or more copies of a pattern, each one except the first following immediately after its predecessor. In this case, the multiple copies may be reduced to one, as before, with something to say how many copies there are, or when the sequence begins and ends, or, more vaguely, that the pattern is repeated. For example, a sports coach might specify exercises as something like \u201ctouch toes (\u00d715), push-ups (\u00d710), skipping (\u00d730), ...\u201d or \u201cStart running on the spot when I say \u2018start\u2019 and keep going until I say \u2018stop\u2019 \u201d.\nThe multiple alignment concept, described in the next section, combines these three techniques and also provides for the encoding of discontinuous dependencies in data, as described in Section 6.4."}, {"heading": "3 SP patterns and the multiple alignment con-", "text": "cept\nIn the SP system, all kinds of knowledge are represented with SP patterns: arrays of atomic symbols in one or two dimensions. Here, a \u2018symbol\u2019 is some kind of mark that can be compared with any other symbol to decide whether it is the \u2018same\u2019 or \u2018different\u2019. No other result is permitted.\nSo far, the main focus has been on 1D patterns but it is envisaged that, at some stage, the system will be generalised to work with 2D patterns.\nIn the SP system, all kinds of processing are done via the building of multiple alignments like the one shown in Figure 1. This and other examples in this document are created by the SP computer model, based on the SP theory.\nIn the figure, each row contains one SP pattern. By convention, row 0 contains a New pattern representing incoming information, while the remaining rows contain Old patterns representing already-stored information.\nMultiple alignments like the one shown in the figure are built in a stepwise manner, much as in bioinformatics programs that create multiple alignments from DNA sequences or amino-acid sequences. At each stage, each partiallybuilt multiple alignment is given a score in terms of its effectiveness in compressing the New pattern; and, at each stage, the best multiple alignments are selected for further processing. The overall aim is to create one or more multiple alignments that score well in compressing the New pattern."}, {"heading": "4 Unsupervised learning", "text": "In the example just shown, the Old patterns were created manually but it is envisaged that, when the system is more mature, most Old patterns would be created by the system itself, as described in [3, Section 5] and [2, Chapter 9].\nUnsupervised learning in the SP system means compressing a given body of information (I) to create a grammar (G), and an encoding (E) of I in term of G. In accordance with the principle of minimum length encoding [1], the system aims to minimise the overall size of G and E.\nFor present purposes, many of the details of how learning is achieved are not important. But a brief summary may be useful:\n\u2022 When an incoming (\u2018New\u2019) pattern is received, the system looks for good full or partial matches with pre-stored (\u2018Old\u2019) patterns, if any.\n\u2022 If there is a match between the New pattern, or part of it, with all the \u2018contents\u2019 symbols (see below) in any Old pattern, the frequency value of the Old pattern is increased by 1.\n\u2022 Whenever any New or Old pattern is not fully matched, the system creates Old patterns from the part or parts that match and also from the part or parts that do not match. If the New pattern is not matched at all, it is simply stored as an Old pattern.\n\u2022 Each newly-created Old pattern is given system-generated \u2018identification\u2019 symbols (\u2018ID-symbols\u2019) like \u2018A\u2019, \u201821\u2019, and \u2018#A\u2019 in the pattern \u2018A 21 s w e e t #A\u2019 in Figure 1. All other symbols in the pattern are \u2018contents\u2019 symbols (\u2018C-symbols\u2019).\n\u2022 Any newly-created Old pattern that is derived from patterns that match each other is assigned a frequency value of 2, while each newlycreated Old pattern that is derived from an unmatched pattern is assigned a frequency value of 1.\n\u2022 At all stages, there is a process of selecting patterns that help to minimise the overall size of G and E, and discarding the rest."}, {"heading": "5 How a New pattern may be encoded eco-", "text": "nomically via Old patterns in a multiple\nalignment\nThis section describes in outline how an encoding, E, may be derived from New and Old patterns in a multiple alignment.\nConsider the multiple alignment shown in Figure 1. From this multiple alignment, one can derive a code pattern in the following way:\n1. Scan the multiple alignment from left to right looking for columns that contain an ID-symbol by itself, not aligned with any other symbol.\n2. Copy these symbols into a code pattern in the same order that they appear in the multiple alignment.\nThe result in this case is the code pattern \u2018S PL 0a 17 6 11 21 #S\u2019. This is, in effect, a compressed representation of the sentence \u2018t h e a p p l e s a r e s w e e t\u2019. As a crude measure of compression, 17 symbols in the sentence have been reduced to 8 symbols in the encoding. If we take account of the number of bits used to represent each symbol, 160 bits in the sentence have been reduced to 54 bits in the encoding."}, {"heading": "6 Kinds of redundancy in sequential informa-", "text": "tion\nThis section considers some kinds of redundancy that may be found in a 1D sequence of atomic symbols, I, and how they may be encoded in the multiple alignment framework. The first three correspond to the three coding techniques outlined in Section 2: chunking-with-codes, schema-plus-correction, and run-length coding.\nAs a rough generalisation, any symbol or sequence of symbols that occurs 2 or more times in I may seen to represent redundancy in I. More precisely, a symbol or sequence of symbols represents redundancy if it occurs more often in I than one would expect by chance. Here, a \u201csequence of symbols\u201d may include any subsequence of I that is discontinuous within I."}, {"heading": "6.1 Chunks", "text": "Any symbol or coherent sequence of symbols that appears repeatedly in I may be seen as a \u2018chunk\u2019 of information.\nAssuming that the Old patterns in rows 1 to 8 of Figure 1 were derived, via unsupervised learning, from a relatively large body of natural language text (I), then sequences of symbols like \u2018t h e\u2019 in \u2018D 17 t h e #D\u2019, \u2018a p p l e\u2019 in \u2018N Nr 6 a p p l e #N\u2019, and \u2018s w e e t\u2019 in \u2018A 21 s w e e t #A\u2019, may be regarded as chunks of information, each one with associated IDsymbols or \u2018codes\u2019 like \u2018D\u2019, \u201817\u2019, and \u2018#D\u2019, in \u2018D 17 t h e #D\u2019, in accordance with the chunking-with-codes technique for information compression."}, {"heading": "6.2 Schemata", "text": "Any sequence of symbols containing one or more slots into which alternative subsequences may be inserted may be seen as a \u2018schema\u2019. Examples include patterns like \u2018S Num ; NP #NP V #V A #A #S\u2019 and \u2018NP 0a D #D N #N #NP\u2019 in Figure 1. In the first case, the slots are \u2018NP #NP\u2019, \u2018V #V\u2019, and \u2018A #A\u2019; while in the second case, the slots are \u2018D #D\u2019, and \u2018N #N\u2019.\nAn example of the schema-plus-correction technique for information compression is the way the sequence \u2018t h e a p p l e s a r e s w e e t\u2019 may be encoded economically as \u2018S PL 0a 17 6 11 21 #S\u2019 (Section 5). Here, \u2018S ... #S\u2019 is the schema and the symbols \u2018PL 0a 17 6 11 21\u2019 are \u2018corrections\u2019 to the schema at more than one level of abstraction."}, {"heading": "6.3 Runs", "text": "Any sequence of two or more chunks, with each one except the first following immediately after its predecessor, may be described as a \u2018run\u2019.\nIn the multiple alignment framework, a sequence like \u2018a b c a b c a b c a b c $\u2019, containing repeated instances of the chunk \u2018a b c\u2019, may be\nencoded via recursion, as shown in Figure 2. This may be seen as an example of run-length coding."}, {"heading": "6.4 Discontinous dependencies", "text": "A well-known feature of natural languages is that there may be grammatical \u2018agreements\u2019 or \u2018dependencies\u2019 between one part of a sentence and another. For example, if the subject of the sentence is singular, then the main verb must also be singular, and if the subject is plural, the main verb must be plural.\nWithin one sentence, there may be dependencies that are quite independent of each other as, for example, number dependency and gender dependency in the French sentence Les plumes sont vertes (\u201cThe feathers are green\u201d):\nP P P P Number dependencies\nLes plume s sont vert e s"}, {"heading": "F F Gender dependencies", "text": "These kinds of agreement or dependency are often described as \u2018discontinuous\u2019 because they may jump over arbitrarily large amounts of intervening structure. For example, there is a number agreement (plural) between the subject and the verb in the sentence The winds from the West are strong, even though the subject and the verb are separated by the phrase from the West. That phrase may be replaced by one or more subordinate clauses which may be arbitrarily complex.\nDependencies like these may be encoded via the multiple alignment framework as shown in Figure 1. Here, the plural dependency between the subject and the verb is marked in row 8 with the pattern \u2018Num PL ; Np Vp\u2019. The symbol \u2018Np\u2019 is aligned with the matching symbol in row 2, while the symbol \u2018Vp\u2019 is aligned with its twin in row 5.\nThe multiple alignment framework also accommodates dependencies within one sentence that are independent of each other as shown in Figure 5.8 in [2, Section 5.4.1], with the sentence Les plumes sont vert.."}, {"heading": "6.5 Mirror images", "text": "The last form of redundancy to be considered in this section is when one sequence is a mirror image of another. For example, the sequence \u2018i n f o r m a t i o n\u2019 matches the sequence \u2018n o i t a m r o f n i\u2019, provided that the process of matching reverses the order of the symbols in either of the\nsequences relative to the other, and provided that the symbols are treated as atomic, with no internal structure or left-to-right asymmetry.\nAs it stands now, the SP computer model does not do that kind of reverse matching but it could be generalised to do so. Alongside any such generalisation would be reform of the way SP patterns are represented, to facilitate the building of multiple alignments in which any one sequence may appear in its left-to-right or right-to-left ordering. For this to be possible, it would be necessary to ensure symmetry between the ID-symbols at each end. For example, the sequence \u2018i n f o r m a t i o n\u2019 would have end markers something like this: \u2018N i n f o r m a t i o n N\u2019."}, {"heading": "7 Towards a proof", "text": "The proof challenge, posed in the Introduction, is to prove, or disprove, the proposition that:\nFor any given body of information, I, expressed as a one-dimensional sequence of atomic symbols, the multiple alignnment concept provides a means of encoding all the redundancy that may exist in\nI.\nIt appears that the proposition may be proved or disproved by answering the following questions:\n\u2022 Do the kinds of redundancy described in Section 6 exhaust the possibilities? Are there any other kinds of redundancy that may be found in I?\n\u2022 For each of the kinds of redundancy described in Section 6, can the multiple alignment framework encode all such redundancies, if any, that may exist in I?\nAnother possible way to approach the problem is via the following statements (from Section 6):\nA symbol or sequence of symbols represents redundancy if it occurs more often in I than one would expect by chance. Here, a\n\u201csequence of symbols\u201d may include any subsequence of I that is discontinuous within I.\nThe target proposition may be proved (or disproved) by showing that the multiple alignment concept can (or cannot) encode all such redundancies in I."}], "references": [{"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "Parts I and II. Information and Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1964}, {"title": "Unifying Computing and Cognition: the SP Theory and Its Applications. CognitionResearch.org", "author": ["J.G. Wolff"], "venue": "Menai Bridge,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "The SP theory of intelligence: an overview", "author": ["J.G. Wolff"], "venue": "Information, 4(3):283\u2013341,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "For several years, I have been developing the SP theory of intelligence, designed to simplify and integrate concepts across artificial intelligence, mainstream computing, and human perception and cognition, with information compression as a unifying theme [3, 2].", "startOffset": 256, "endOffset": 262}, {"referenceID": 1, "context": "For several years, I have been developing the SP theory of intelligence, designed to simplify and integrate concepts across artificial intelligence, mainstream computing, and human perception and cognition, with information compression as a unifying theme [3, 2].", "startOffset": 256, "endOffset": 262}, {"referenceID": 0, "context": "In accordance with the principle of minimum length encoding [1], the system aims to minimise the overall size of G and E.", "startOffset": 60, "endOffset": 63}], "year": 2014, "abstractText": "These notes pose a \u201cproof challenge\u201d: a proof, or disproof, of the proposition that For any given body of information, I, expressed as a one-dimensional sequence of atomic symbols, a multiple alignment concept, described in the document, provides a means of encoding all the redundancy that may exist in I. Aspects of the challenge are described.", "creator": "LaTeX with hyperref package"}}}