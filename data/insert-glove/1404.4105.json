{"id": "1404.4105", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2014", "title": "Sparse Compositional Metric Learning", "abstract": "borgou We propose takshaka a weekly new meditatively approach qeshl\u0101q-e for metric samguk learning by rawad framing nonplused it alajuela as maesa learning a sparse bottled combination 24x7 of stalybridge locally discriminative metrics that avtayev are inexpensive tolerance to generate from bt30 the training data. This flexible framework allows chungu us limbong to gangwar naturally derive formulations for romanija global, e\u00dfweiler multi - task barlaam and polatsk local yelverton metric fcdu learning. The cityliner resulting renovada algorithms physics-based have several sorrowfully advantages 2,200-acre over nesha existing methods in the literature: kloer a frechaut much 801,000 smaller number of bashur parameters to 1970-71 be 260-million estimated and a principled basalmost way 10-run to generalize learned azuero metrics maslova to canavan new arvanites testing data points. laramide To lacen analyze grossa the approach theoretically, we derive a generalization splenomegaly bound galil that justifies yushchenko the sparse interacademy combination. Empirically, shahuji we elo evaluate our 48.03 algorithms on palest several 1537 datasets stsmithglobe.com against trillium state - d'amboise of - gyeongsangnam-do the - art metric mapuches learning gadek methods. flp The results algoa are prorok consistent perata with our theoretical deprogram findings and demonstrate arbogast the superiority seamed of abreu our approach in two-dollar terms non-coercive of pundravardhana classification tounsi performance and scalability.", "histories": [["v1", "Tue, 15 Apr 2014 22:55:53 GMT  (209kb,D)", "http://arxiv.org/abs/1404.4105v1", "18 pages. To be published in Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI 2014)"]], "COMMENTS": "18 pages. To be published in Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI 2014)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yuan shi", "aur\u00e9lien bellet", "fei sha"], "accepted": true, "id": "1404.4105"}, "pdf": {"name": "1404.4105.pdf", "metadata": {"source": "CRF", "title": "Sparse Compositional Metric Learning\u2217", "authors": ["Yuan Shi", "Aur\u00e9lien Bellet", "Fei Sha"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The need for measuring distance or similarity between data instances is ubiquitous in machine learning and many application domains. However, each problem has its own underlying semantic space for defining distances that standard metrics (e.g., the Euclidean distance) often fail to capture. This has led to a growing interest in metric learning for the past few years, as summarized in two recent surveys (Bellet et al., 2013; Kulis, 2012). Among these methods, learning a globally linear Mahalanobis distance is by far the most studied setting. Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012). This is equivalent to learning a linear projection of the data to a feature space where constraints on the training set (such as \u201cxi should be closer to xj than to xk\u201d) are better satisfied.\nAlthough the performance of these learned metrics is typically superior to that of standard metrics in practice, a single linear metric is often unable to accurately capture the complexity of the task, for instance when the data are multimodal or the decision boundary is complex. To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al., 2010). This line of research is motivated by the fact that locally, simple linear metrics perform well (Ramanan and Baker, 2011; Hauberg et al., 2012). The main challenge is to integrate these metrics into a meaningful global one while keeping the number of learning parameters to a reasonable level in order to avoid heavy computational burden and severe overfitting. So far, existing methods are not able to compute valid (smooth) global metrics from the local metrics they learn and do not provide a principled way of generalizing to new regions of the space at test time. Furthermore,\n\u2217This document is an extended version of a conference paper (Shi et al., 2014) that provides additional details and results. \u2020Equal contribution. \u2021Department of Computer Science, University of Southern California, {yuanshi,bellet,feisha}@usc.edu.\nar X\niv :1\n40 4.\n41 05\nv1 [\ncs .L\nG ]\n1 5\nA pr\n2 01\n4\nthey scale poorly with the dimensionality D of the data: typically, learning a Mahalanobis distance requires O(D2) parameters and the optimization involves projections onto the positive semidefinite cone that scale in O(D3). This is expensive even for a single metric when D is moderately large.\nIn this paper, we study metric learning from a new perspective to efficiently address these key challenges. We propose to learn metrics as sparse compositions of locally discriminative metrics. These \u201cbasis metrics\u201d are low-rank and extracted efficiently from the training data at different local regions, for instance using Fisher discriminant analysis. Learning higher-rank linear metrics is then formulated as learning the combining weights, using sparsity-inducing regularizers to select only the most useful basis elements. This provides a unified framework for metric learning, as illustrated in Figure 1, that we call SCML (for Sparse Compositional Metric Learning). In SCML, the number of parameters to learn is much smaller than existing approaches and projections onto the positive semidefinite cone are not needed. This gives an efficient and flexible way to learn a single global metric when D is large.\nThe proposed framework also applies to multi-task metric learning, where one wants to learn a global metric for several related tasks while exploiting commonalities between them (Caruana, 1997; Parameswaran and Weinberger, 2010). This is done in a natural way by means of a group sparsity regularizer that makes the task-specific metrics share the same basis subset. Our last and arguably most interesting contribution is a new formulation for local metric learning, where we learn a transformation T (x) that takes as input any instance x and outputs a sparse weight vector defining its metric. This can be seen as learning a smoothly varying metric tensor over the feature space (Ramanan and Baker, 2011; Hauberg et al., 2012). To the best of our knowledge, it is the first discriminative metric learning approach capable of computing, in a principled way, an instance-specific metric for any point in the feature space. All formulations can be solved using scalable optimization procedures based on stochastic subgradient descent with proximal operators (Duchi and Singer, 2009; Xiao, 2010).\nWe present both theoretical and experimental evidence supporting the proposed approach. We derive a generalization bound which provides a theoretical justification to seeking sparse combinations and suggests that the basis set B can be large without incurring overfitting. Empirically, we evaluate our algorithms against state-of-the-art global, local and multi-task metric learning methods on several datasets. The results strongly support the proposed framework.\nThe rest of this paper is organized as follows. Section 2 describes our general framework and illustrates how it can be used to derive efficient formulations for global, local and multi-task metric learning. Section 3\nprovides a theoretical analysis supporting our approach. Section 4 reviews related work. Section 5 presents an experimental evaluation of the proposed methods. We conclude in Section 6."}, {"heading": "2 Proposed Approach", "text": "In this section, we present the main idea of sparse compositional metric learning (SCML) and show how it can be used to unify several existing metric learning paradigms and lead to efficient new formulations."}, {"heading": "2.1 Main Idea", "text": "We assume the data lie in RD and focus on learning (squared) Mahalanobis distances dM (x,x\u2032) = (x \u2212 x\u2032)TM(x \u2212 x\u2032) parameterized by a positive semidefinite (PSD) D \u00d7 D matrix M . Note that M can be represented as a nonnegative weighted sum of K rank-1 PSD matrices:1\nM = K\u2211 i=1 wibib T i , with w \u2265 0, (1)\nwhere the bi\u2019s are D-dimensional column vectors. In this paper, we use the form (1) to cast metric learning as learning a sparse combination of basis elements taken from a basis set B = {bi}Ki=1. The key to our framework is the fact that such a B is made readily available to the algorithm and consists of rank-one metrics that are locally discriminative. Such basis elements can be easily generated from the training data at several local regions \u2014 in the experiments, we simply use Fisher discriminant analysis (see the corresponding section for details). They can then be combined to form a single global metric, multiple global metrics (in the multi-task setting) or a metric tensor (implicitly defining an infinite number of local metrics) that varies smoothly across the feature space, as we will show in later sections.\nWe use the notation dw(x,x\u2032) to highlight our parameterization of the Mahalanobis distance by w. Learning M in this form makes it PSD by design (as a nonnegative sum of PSD matrices) and involves K parameters (instead of D2 in most metric learning methods), enabling it to more easily deal with highdimensional problems. We also want the combination to be sparse, i.e., some wi\u2019s are zero and thus M only depends on a small subset of B. This provides some form of regularization (as shown later in Theorem 1) as well as a way to tie metrics together when learning multiple metrics. In the rest of this section, we apply the proposed framework to several metric learning paradigms (see Figure 1). We start with the simple case of global metric learning (Section 2.1.1) before considering more challenging settings: multi-task (Section 2.1.2) and local metric learning (Section 2.1.3). Finally, Section 2.2 discusses how these formulations can be solved in a scalable way using stochastic subgradient descent with proximal operators."}, {"heading": "2.1.1 Global Metric Learning", "text": "In global metric learning, one seeks to learn a single metric dw(x,x\u2032) from a set of distance constraints on the training data. Here, we use a set of triplet constraints C where each (xi,xj ,xk) \u2208 C indicates that the distance between xi and xj should be smaller than the distance between xi and xk. C may be constructed from label information, as in LMNN (Weinberger and Saul, 2009), or in an unsupervised manner based for instance on implicit users\u2019 feedback (such as clicks on search engine results). Our formulation for global\n1Such an expression exists for any PSD matrix M since the eigenvalue decomposition of M is of the form (1).\nmetric learning, SCML-Global, is simply to combine the local basis elements into a higher-rank global metric that satisfies well the constraints in C:\nmin w\n1 |C| \u2211\n(xi,xj ,xk)\u2208C\nLw(xi,xj ,xk) + \u03b2\u2016w\u20161, (2)\nwhere Lw(xi,xj ,xk) = [1 + dw(xi,xj) \u2212 dw(xi,xk)]+ with [\u00b7]+ = max(0, \u00b7), and \u03b2 \u2265 0 is a regularization parameter. The first term in (2) is the classic margin-based hinge loss function. The second term \u2016w\u20161 = \u2211K i=1wi is the `1 norm regularization which encourages sparse solutions, allowing the selection of relevant basis elements. SCML-Global is convex by the linearity of both terms and is bounded below, thus it has a global minimum."}, {"heading": "2.1.2 Multi-Task Metric Learning", "text": "Multi-task learning (Caruana, 1997) is a paradigm for learning several tasks simultaneously, exploiting their commonalities. When tasks are related, this can perform better than separately learning each task. Recently, multi-task learning methods have successfully built on the assumption that the tasks should share a common low-dimensional representation (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012). In general, it is unclear how to achieve this in metric learning. In contrast, learning metrics as sparse combinations allows a direct translation of this idea to multi-task metric learning.\nFormally, we are given T different but somehow related tasks with associated constraint setsC1, . . . , CT and we aim at learning a metric dwt(x,x\n\u2032) for each task t while sharing information across tasks. In the following, the basis set B is the union of the basis sets B1, . . . , BT extracted from each task t. Our formulation for multi-task metric learning, mt-SCML, is as follows:\nmin W T\u2211 t=1 1 |Ct| \u2211\n(xi,xj ,xk)\u2208Ct\nLwt(xi,xj ,xk) + \u03b2\u2016W \u20162,1,\nwhere W is a T \u00d7 K nonnegative matrix whose t-th row is the weight vector wt defining the metric for task t, Lwt(xi,xj ,xk) = [1 + dwt(xi,xj)\u2212 dwt(xi,xk)]+ and \u2016W \u20162,1 is the `2/`1 mixed norm used in the group lasso problem (Yuan and Lin, 2006). It corresponds to the `1 norm applied to the `2 norm of the columns of W and is known to induce group sparsity at the column level. In other words, this regularization makes most basis elements either have zero weight or nonzero weight for all tasks.\nOverall, while each metric remains task-specific (dwt is only required to satisfy well the constraints in Ct), it is composed of shared features (i.e., it potentially benefits from basis elements generated from other tasks) that are regularized to be relevant across tasks (as favored by the group sparsity). As a result, all learned metrics can be expressed as combinations of the same basis subset of B, though with different weights for each task. Since the `2/`1 norm is convex, mt-SCML is again convex."}, {"heading": "2.1.3 Local Metric Learning", "text": "Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012). For heterogeneous data, allowing the metric to vary across the feature space can capture the semantic distance much better. On the other hand, local metric learning is costly and often suffers from severe overfitting since the number of parameters to learn can be very large. In the following, we show how our framework can be used to derive an efficient local metric learning method.\nWe aim at learning a metric tensor T (x), which is a smooth function that (informally) maps any instance x to its metric matrix (Ramanan and Baker, 2011; Hauberg et al., 2012). The distance between two points should then be defined as the geodesic distance on a Riemannian manifold. However, this requires solving an intractable problem, so we use the widely-adopted simplification that distances from point x are computed based on its own metric alone (Zhan et al., 2009; Noh et al., 2010; Wang et al., 2012):\ndT (x,x \u2032) = (x\u2212 x\u2032)TT (x)(x\u2212 x\u2032)\n= (x\u2212 x\u2032)T K\u2211 i=1 wx,ibib T i (x\u2212 x\u2032),\nwhere wx is the weight vector for instance x. We could learn a weight vector for each training point. This would result in a formulation similar to mt-SCML, where each training instance is considered as a task. However, in the context of local metric learning, this is not an appealing solution. Indeed, for a training sample of size S we would need to learn SK parameters, which is computationally difficult and leads to heavy overfitting for large-scale problems. Furthermore, this gives no principled way of computing the weight vector of a test instance.\nWe instead propose a more effective solution by constraining the weight vector for an instance x to parametrically depend on some embedding of x:\nTA,c(x) = K\u2211 i=1 (aTi zx + ci) 2bib T i , (3)\nwhere zx \u2208 D\u2032 is an embedding of x,2 A = [a1 . . .aK ]T is a D\u2032 \u00d7 K real-valued matrix and c \u2208 RK . The square makes the weights nonnegative \u2200x \u2208 RD, ensuring that they define a valid (pseudo) metric. Intuitively, (3) combines the locally discriminative metrics with weights that depend on the position of the instance in the feature space.\nThere are several advantages to this formulation. First, by learning A and c we implicitly learn a different metric not only for the training data but for any point in the feature space. Second, if the embedding is smooth, TA,c(x) is a smooth function of x, therefore similar instances are assigned similar weights. This can be seen as some kind of manifold regularization. Third, the number of parameters to learn is now K(D\u2032 + 1), thus independent of both the size of the training sample and the dimensionality of x. Our formulation for local metric learning, SCML-Local, is as follows:\nmin A\u0303\n1 |C| \u2211\n(xi,xj ,xk)\u2208C\nLTA,c(xi,xj ,xk) + \u03b2\u2016A\u0303\u20162,1,\nwhere A\u0303 is a (D\u2032 + 1) \u00d7 K matrix denoting the concatenation of A and c, and LTA,c(xi,xj ,xk) =[ 1 + dTA,c(xi,xj)\u2212 dTA,c(xi,xk) ] +\n. The `2/`1 norm on A\u0303 introduces sparsity at the column level, regularizing the local metrics to use the same basis subset. Interestingly, if A is the zero matrix, we recover SCML-Global. SCML-Local is nonconvex and is thus subject to local minima."}, {"heading": "2.2 Optimization", "text": "Our formulations use (nonsmooth) sparsity-inducing regularizers and typically involve a large number of triplet constraints. We can solve them efficiently using stochastic composite optimization (Duchi and Singer,\n2In our experiments, we use kernel PCA (Scho\u0308lkopf et al., 1998) as it provides a simple way to limit the dimension and thus the number of parameters to learn. We use RBF kernel with bandwidth set to the median Euclidean distance in the data.\n2009; Xiao, 2010), which alternates between a stochastic subgradient step on the hinge loss term and a proximal operator (for `1 or `2,1 norm) that explicitly induces sparsity. We solve SCML-Global and mt-SCML using Regularized Dual Averaging (Xiao, 2010), which offers fast convergence and levels of sparsity in the solution comparable to batch algorithms. For SCML-Local, due to local minima, we ensure improvement over the optimal solution w\u2217 of SCML-Global by using a forward-backward algorithm (Duchi and Singer, 2009) which is initialized with A = 0 and ci = \u221a w\u2217i .\nRecall that unlike most existing metric learning algorithms, we do not need to perform projections onto the PSD cone, which scale in O(D3) for a D \u00d7 D matrix. Our algorithms thereby have a significant computational advantage for high-dimensional problems."}, {"heading": "3 Theoretical Analysis", "text": "In this section, we provide a theoretical analysis of our approach in the form of a generalization bound based on algorithmic robustness analysis (Xu and Mannor, 2012) and its adaptation to metric learning (Bellet and Habrard, 2012). For simplicity, we focus on SCML-Global, our global metric learning formulation in (2).\nConsider the supervised learning setting, where we are given a labeled training sample S = {zi = (xi, yi)}ni=1 drawn i.i.d. from some unknown distribution P over Z = X \u00d7 Y . We call a triplet (z, z\u2032, z\u2032\u2032) admissible if y = y\u2032 6= y\u2032\u2032. Let C be the set of admissible triplets built from S and L(w, z, z\u2032, z\u2032\u2032) = [1 + dw(x,x\n\u2032)\u2212 dw(x,x\u2032\u2032)]+ denote the loss function used in (2), with the convention that L returns 0 for non-admissible triplets.\nLet us define the empirical loss of w on S as\nRSemp(w) = 1 |C| \u2211\n(z,z\u2032,z\u2032\u2032)\u2208C\nL(w, z, z\u2032, z\u2032\u2032),\nand its expected loss over distribution P as\nR(w) = Ez,z\u2032,z\u2032\u2032\u223cPL(w, z, z\u2032, z\u2032\u2032).\nThe following theorem bounds the deviation between the empirical loss of the learned metric and its expected loss.\nTheorem 1. Let w\u2217 be the optimal solution to SCML-Global with K basis elements, \u03b2 > 0 and C constructed from S = {(xi, yi)}ni=1 as above. Let K\u2217 \u2264 K be the number of nonzero entries in w\u2217. Let us assume the norm of any instance bounded by some constant R and L uniformly upper-bounded by some constant U . Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 we have:\n\u2223\u2223R(w\u2217)\u2212RSemp(w\u2217)\u2223\u2223 \u2264 16\u03b3RK\u2217\u03b2 + 3U \u221a N ln 2 + ln 1\u03b4 0.5n ,\nwhere N is the size of an \u03b3-cover of Z .\nThis bound has a standardO(1/ \u221a n) asymptotic convergence rate.3 Its main originality is that it provides a theoretical justification to enforcing sparsity in our formulation. Indeed, notice that K\u2217 (and not K) 3In robustness bounds, the cover radius \u03b3 can be made arbitrarily close to zero at the expense of increasing N . Since N appears in the second term, the right hand side of the bound indeed goes to zero when n \u2192 \u221e. This is in accordance with other similar learning bounds, for example, the original robustness-based bounds in (Xu and Mannor, 2012).\nappears in the bound as a penalization term, which suggests that one may use a large basis set K without overfitting as long as K\u2217 remains small. This will be confirmed by our experiments (Section 5.3). A similar bound can be derived for mt-SCML, but not for SCML-Local because of its nonconvexity. The details and proofs can be found in Appendix A."}, {"heading": "4 Related Work", "text": "In this section, we review relevant work in global, multi-task and local metric learning. The interested reader should refer to the recent surveys of Kulis (2012) and Bellet et al. (2013) for more details.\nGlobal methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers. This is computationally expensive and subject to overfitting for moderate to high-dimensional problems. An exception is BoostML (Shen et al., 2012) which uses rank-one matrices as weak learners to learn a global Mahalanobis distance via a boosting procedure. However, it is not clear how BoostML can be generalized to multi-task or local metric learning.\nMulti-task methods Multi-task metric learning was proposed in (Parameswaran and Weinberger, 2010) as an extension to the popular LMNN (Weinberger and Saul, 2009). The authors define the metric for task t as dt(x,x\u2032) = (x \u2212 x\u2032)T(M0 +Mt)(x \u2212 x\u2032), where Mt is task-specific and M0 is shared by all tasks. Note that it is straightforward to incorporate their approach in our framework by defining a shared weight vector w0 and task-specific weights wt. However, this assumption of a metric that is common to all tasks can be too restrictive in cases where task relatedness is complex, as illustrated by our experiments.\nLocal methods MM-LMNN (Weinberger and Saul, 2009) is an extension of LMNN which learns only a small number of metrics (typically one per class) in an effort to alleviate overfitting. However, no additional regularization is used and a full-rank metric is learned for each class, which becomes intractable when the number of classes is large. msNCA (Hong et al., 2011) learns a function that splits the space into a small number of regions and then learns a metric per region using NCA (Goldberger et al., 2004). Again, the metrics are full-rank so msNCA does not scale well with the number of metrics. Like SCML-Local, PLML (Wang et al., 2012) is based on a combination of metrics but there are major differences with our work: (i) weights only depend on a manifold assumption: they are not sparse and use no discriminative information, (ii) the basis metrics are full-rank, thus expensive to learn, and (iii) a weight vector is learned explicitly for each training instance, which can result in a large number of parameters and prevents generalization to new instances (in practice, for a test point, they use the weight vector of its nearest neighbor in the training set). As observed by Ramanan and Baker (2011), the above methods make the implicit assumption that the metric tensor is locally constant (at the class, region or neighborhood level), while SCML-Local learns a smooth function that maps any instance to its specific metric. ISD (Zhan et al., 2009) is an attempt to learn the metrics for unlabeled points by propagation, but is limited to the transductive setting. Unlike the above discriminative approaches, GLML (Noh et al., 2010) learns a metric for each point independently in a generative way by minimizing the 1-NN expected error under some assumption for the class distributions."}, {"heading": "5 Experiments", "text": "In this section, we compare our methods to state-of-the-art algorithms on global, multi-task and local metric learning.4 We use a 3-nearest neighbor classifier in all experiments. To generate a set of locally discriminative rank-one metrics, we first divide data into regions via clustering. For each region center, we select J nearest neighbors from each class (for J = {10, 20, 50} to account for different scales), and apply Fisher discriminant analysis followed by eigenvalue decomposition to obtain the basis elements.5 Section 5.1 presents results for global metric learning, Section 5.2 for multi-task and Section 5.3 for local metric learning."}, {"heading": "5.1 Global Metric Learning", "text": "We use 6 datasets from UCI6 and BBC7 (see Table 1). The dimensionality of USPS and BBC is reduced to 100 and 200 using PCA to speed up computation. We normalize the data as in (Wang et al., 2012) and split into train/validation/test (60%/20%/20%), except for Letters and USPS where we use 3,000/1,000/1,000. Results are averaged over 20 random splits."}, {"heading": "5.1.1 Proof of Concept", "text": "Setup Global metric learning is a convenient setting to study the effect of combining basis elements. To this end, we consider a formulation with the same loss function as SCML-Global but that directly learns the metric matrix, using Frobenius norm regularization to reduce overfitting. We refer to it as Global-Frob. Both algorithms use the same training triplets, generated by identifying 3 target neighbors (nearest neighbors with same label) and 10 imposters (nearest neighbors with different label) for each instance. We tune the regularization parameter on the validation data. For SCML-Global, we use a basis set of 400 elements for Vehicle, Vowel, Segment and BBC, and 1,000 elements for Letters and USPS.\n4For all compared methods we use MATLAB code from the authors\u2019 website. The MATLAB code for our methods is available at http://www-bcf.usc.edu/\u02dcbellet/.\n5We also experimented with a basis set based on local GLML metrics. Preliminary results were comparable to those obtained with the procedure above.\n6http://archive.ics.uci.edu/ml/ 7http://mlg.ucd.ie/datasets/bbc.html\nResults Table 2 shows misclassification rates with standard errors, where Euc is the Euclidean distance. The results show that SCML-Global performs similarly as Global-Frob on low-dimensional datasets but has a clear advantage when dimensionality is high (USPS and BBC). This demonstrates that learning a sparse combination of basis elements is an effective way to reduce overfitting and improve generalization. SCMLGlobal is also faster to train than Global-Frob on these datasets (about 2x faster on USPS and 3x on BBC) because it does not require any PSD projection."}, {"heading": "5.1.2 Comparison to Other Global Algorithms", "text": "Setup We now compare SCML-Global to two state-of-the-art global metric learning algorithms: Large Margin Nearest Neighbor (LMNN, Weinberger and Saul, 2009) and BoostML (Shen et al., 2012). The datasets, preprocessing and setting for SCML-Global are the same as in Section 5.1.1. LMNN uses 3 target neighbors and all imposters, while these are set to 3 and 10 respectively for BoostML (as in SCML-Global).\nResults Table 3 shows the average misclassification rates, along with standard error and the average rank of each method across all datasets. SCML-Global clearly outperforms LMNN and BoostML, ranking first on 5 out of 6 datasets and achieving the overall highest rank. Furthermore, its training time is smaller than competing methods, especially for high-dimensional data. For instance, on the BBC dataset, SCML-Global trained in about 90 seconds, which is about 20x faster than LMNN and 35x faster than BoostML. Note also that SCML-Global is consistently more accurate than linear SVM, as shown in Appendix B.\nNumber of selected basis elements Like SCML-Global, recall that BoostML is based on combining rank-one elements (see Section 4). The main difference with SCML-Global is that our method is given a set of locally discriminative metrics and picks the relevant ones by learning sparse weights, while BoostML generates a new basis element at each iteration and adds it to the current combination. Table 4 reports the number of basis elements used in SCML-Global and BoostML solutions. Overall, SCML-Global uses fewer\nelements than BoostML (on two datasets, it uses more but this yields to significantly better performance). The results on USPS and BBC also suggest that the number of basis elements selected by SCML-Global seems to scale well with dimensionality. These nice properties come from its knowledge of the entire basis set and the sparsity-inducing regularizer. On the contrary, the number of elements (and therefore iterations) needed by BoostML to converge seems to scale poorly with dimensionality."}, {"heading": "5.2 Multi-task Metric Learning", "text": "Dataset Sentiment Analysis (Blitzer et al., 2007) is a popular dataset for multi-task learning that consists of Amazon reviews on four product types: kitchen appliances, DVDs, books and electronics. Each product type is treated as a task and has 1,000 positive and 1,000 negative reviews. To reduce computational cost, we represent each review by a 200-dimensional feature vector by selecting top 200 words of the largest mutual information with the labels. We randomly split the dataset into training (800 samples), validation (400 samples) and testing (400 samples) sets.\nSetup We compare the following metrics: st-Euc (Euclidean distance), st-LMNN and st-SCML (singletask LMNN and single-task SCML-Global, trained independently on each task), u-Euc (Euclidean trained on the union of the training data from all tasks), u-LMNN (LMNN on union), u-SCML (SCML-Global on union), multi-task LMNN (Parameswaran and Weinberger, 2010) and finally our own multi-task method mt-SCML. We tune the regularization parameters in mt-LMNN, st-SCML, u-SCML and mt-SCML on validation sets. As in the previous experiment, the number of target neighbors and imposters for our methods are set to 3 and 10 respectively. We use a basis set of 400 elements for each task for st-SCML, the union of these (1,600) for mt-SCML, and 400 for u-SCML.\nResults Table 5 shows the results averaged over 20 random splits. First, notice that u-LMNN and uSCML obtain significantly higher error rates than st-LMNN and st-SCML respectively, which suggests that the dataset may violate mt-LMNN\u2019s assumption that all tasks share a similar metric. Indeed, mt-LMNN does not outperform st-LMNN significantly. On the other hand, mt-SCML performs better than its singletask counterpart and than all other compared methods by a significant margin, demonstrating its ability to leverage some commonalities between tasks that mt-LMNN is unable to capture. It is worth noting that the solution found by mt-SCML is based on only 273 basis elements on average (out of a total of 1,600), while st-SCML makes use of significantly more elements (347 elements per task on average). Basis elements selected by mt-SCML are evenly distributed across all tasks, which indicates that it is able to exploit meaningful information across tasks to get both more accurate and more compact metrics. Finally, note that our algorithms are about an order of magnitude faster."}, {"heading": "5.3 Local Metric Learning", "text": "Setup We use the same datasets and preprocessing as for global metric learning. We compare SCMLLocal to MM-LMNN (Weinberger and Saul, 2009), GLML (Noh et al., 2010) and PLML (Wang et al., 2012). The parameters of all methods are tuned on validation sets or set by authors\u2019 recommendation. MMLMNN use 3 target neighbors and all imposters, while these are set to 3 and 10 in PLML and SCML-Local. The number of anchor points in PLML is set to 20 as done by the authors. For SCML-Local, we use the same basis set as SCML-Global, and embedding dimension D\u2032 is set to 40 for Vehicle, Vowel, Segment and BBC, and 100 for Letters and USPS.\nResults Table 6 gives the error rates along with the average rank of each method across all datasets. Note that SCML-Local significantly improves upon SCML-Global on all but one dataset and achieves the best average rank. PLML does not perform well on small datasets (Vehicle and Vowel), presumably because there are not enough points to get a good estimation of the data manifold. GLML is fast but has rather poor performance on most datasets because its Gaussian assumption is restrictive and it learns the local metrics independently. Among discriminative methods, SCML-Local offers the best training time, especially for high-dimensional data (e.g. on BBC, it trained in about 8 minutes, which is about 5x faster than MMLMNN and 15x faster than PLML). Note that on this dataset, both MM-LMNN and PLML perform worse than SCML-Global due to severe overfitting, while SCML-Local avoids it by learning significantly fewer parameters. Finally, SCML-Local achieves accuracy results that are very competitive with those of a kernel SVM, as shown in Appendix B.\nVisualization of the learned metrics To provide a better understanding of why SCML-Local works well, we apply it to digits 1, 2, and 3 of USPS projected in 2D using t-SNE (van der Maaten and Hinton, 2008),\nshown in Figure 2(a). We use 10 basis elements and D\u2032 = 5. Figure 2(b) shows the training points colored by their learned metric (based on the projection of the weight vectors in 1D using PCA). We see that the local metrics vary smoothly and are thereby robust to outliers. Unlike MM-LMNN, points within a class are allowed to have different metrics: in particular, this is useful for points that are near the decision boundary. While smooth, the variation in the weights is thus driven by discriminative information, unlike PLML where they are only based on the smoothness assumption. Finally, Figure 2(c) shows that the metrics consistently generalize to test data.\nEffect of the basis set size Figure 3 shows the number of selected basis elements and test error rate for SCML-Global and SCML-Local as a function of the size of basis set on Segment (results were consistent on other datasets). The left pane shows that the number of selected elements increases sublinearly and eventually converges, while the right pane shows that test error may be further reduced by using a larger basis set without significant overfitting, as suggested by our generalization bound (Theorem 1). Figure 3 also shows that SCML-Local generally selects more basis elements than SCML-Global, but notice that it can outperform SCML-Global even when the basis set is very small."}, {"heading": "6 Conclusion", "text": "We proposed to learn metrics as sparse combinations of rank-one basis elements. This framework unifies several paradigms in metric learning, including global, local and multi-task learning. Of particular interest is our local metric learning algorithm which can compute instance-specific metrics for both training and test points in a principled way. The soundness of our approach is supported theoretically by a generalization bound, and we showed in experimental studies that the proposed methods improve upon state-of-the-art algorithms in terms of accuracy and scalability.\nAcknowledgements This research is partially supported by the IARPA via DoD/ARL contract # W911NF12-C-0012 and DARPA via contract # D11AP00278. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, DARPA, or the U.S. Government."}, {"heading": "Appendix A Detailed Analysis", "text": "In this section, we give the details of the derivation of the generalization bounds for the global and multi-task learning formulations given in Section 3.\nA.1 Preliminaries\nWe start by introducing some notation. We are given a training sample S = {zi = (xi, yi)}ni=1 drawn i.i.d. from a distribution P over the labeled space Z = X \u00d7 Y . We assume that \u2016x\u2016 \u2264 R (for some convenient norm), \u2200x \u2208 X . We call a triplet (z, z\u2032, z\u2032\u2032) admissible if y = y\u2032 6= y\u2032\u2032. Let CS be the set of all admissible triplets built from instances in S.8\n8When the training triplets consist of only a subset of all admissible triplets (which is often the case in practice), a relaxed version of the robustness property can be used to derive similar results (Bellet and Habrard, 2012). For simplicity, we focus here on the case when all admissible triplets are used.\nLet L(h,z, z\u2032, z\u2032\u2032) be the loss suffered by some hypothesis h on triplet (z, z\u2032, z\u2032\u2032), with the convention that L returns 0 for non-admissible triplets. We assume L to be uniformly upper-bounded by a constant U . The empirical lossRCSemp(h) of h on CS is defined as\nRCSemp(h) = 1 |CS | \u2211\n(z,z\u2032,z\u2032\u2032)\u2208CS\nL(h, z, z\u2032, z\u2032\u2032),\nand its expected lossR(h) over distribution P as\nR(h) = Ez,z\u2032,z\u2032\u2032\u223cPL(h, z, z\u2032, z\u2032\u2032).\nOur goal is to bound the deviation betweenR(ACS ) andRCSemp(ACS ), whereACS is the hypothesis learned by algorithm A on CS .\nA.2 Algorithmic Robustness\nTo derive our generalization bounds, we use the recent framework of algorithmic robustness (Xu and Mannor, 2012), in particular its adaptation to pairwise and tripletwise loss functions used in metric learning (Bellet and Habrard, 2012). For the reader\u2019s convenience, we briefly review these main results below.\nAlgorithmic robustness is the ability of an algorithm to perform \u201csimilarly\u201d on a training example and on a test example that are \u201cclose\u201d. The proximity of points is based on a partitioning of the space Z: two examples are close to each other if they lie in the same region. The partition is based on the notion of covering number (Kolmogorov and Tikhomirov, 1961).\nDefinition 1 (Covering number). For a metric space (S, \u03c1) and V \u2282 S, we say that V\u0302 \u2282 V is a \u03b3-cover of V if \u2200t \u2208 V , \u2203t\u0302 \u2208 V\u0302 such that \u03c1(t, t\u0302) \u2264 \u03b3. The \u03b3-covering number of V is\nN (\u03b3,V, \u03c1) = min { |V\u0302| : V\u0302 is a \u03b3-cover of V } .\nIn particular, when X is compact, N (\u03b3,X , \u03c1) is finite, leading to a finite cover. Then, Z can be partitioned into |Y|N (\u03b3,X , \u03c1) subsets such that if two examples z = (x, y) and z\u2032 = (x\u2032, y\u2032) belong to the same subset, then y = y\u2032 and \u03c1(x,x\u2032) \u2264 \u03b3. The definition of robustness for tripletwise loss functions (adapted from Xu and Mannor, 2012) is as follows.\nDefinition 2 (Robustness for metric learning (Bellet and Habrard, 2012)). An algorithmA is (N, (\u00b7)) robust for N \u2208 N and (\u00b7) : (Z \u00d7 Z)n \u2192 R if Z can be partitioned into N disjoints sets, denoted by {Qi}Ni=1, such that the following holds for all S \u2208 Zn: \u2200(z1, z2, z3) \u2208 CS ,\u2200z, z\u2032, z\u2032\u2032 \u2208 Z, \u2200i, j \u2208 [N ] : if z1, z \u2208 Qi, z2, z\u2032 \u2208 Qj , z3, z\u2032\u2032 \u2208 Qk then\n|L(ACS , z1, z2, z3)\u2212 L(ACS , z, z \u2032, z\u2032\u2032)| \u2264 (CS),\nwhere ACS is the hypothesis learned by A on CS .\nN and (\u00b7) quantify the robustness of the algorithm and depend on the training sample. Again adapting the result from (Xu and Mannor, 2012), (Bellet and Habrard, 2012) showed that a metric learning algorithm that satisfies Definition 2 has the following generalization guarantees.\nTheorem 2. If a learning algorithmA is (N, (\u00b7))-robust and the training sample consists of the triplets CS obtained from a sample S generated by n i.i.d. draws from P , then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 we have:\n|R(ACS )\u2212R CS emp(ACS )| \u2264 (CS) + 3U\n\u221a N ln 2 + ln 1\u03b4\n0.5n .\nAs shown in (Bellet and Habrard, 2012), establishing the robustness of an algorithm is easier using the following theorem, which basically says that if a metric learning algorithm has approximately the same loss for triplets that are close to each other, then it is robust.\nTheorem 3. Fix \u03b3 > 0 and a metric \u03c1 ofZ . Suppose that \u2200z1, z2, z3, z, z\u2032, z\u2032\u2032 : (z1, z2, z3) \u2208 CS , \u03c1(z1, z) \u2264 \u03b3, \u03c1(z2, z \u2032) \u2264 \u03b3, \u03c1(z3, z\u2032\u2032) \u2264 \u03b3, A satisfies\n|L(ACS , z1, z2, z3)\u2212 L(ACS , z, z \u2032, z\u2032\u2032)| \u2264 (CS),\nand N (\u03b3/2,Z, \u03c1) <\u221e. Then the algorithm A is (N (\u03b3/2,Z, \u03c1), (CS))-robust.\nWe now have all the tools we need to prove the results of interest.\nA.3 Generalization Bounds for SCML\nA.3.1 Bound for SCML-Global\nWe first focus on SCML-Global where the loss function is defined as follows:\nL(w, z, z\u2032, z\u2032\u2032) = [ 1 + dw(x,x \u2032)\u2212 dw(x,x\u2032\u2032) ] + .\nWe obtain a generalization bound by showing that SCML-Global satisfies Definition 2 using Theorem 3. To establish the result, we need a bound on the `2 norm of the basis elements. Since they are obtained by eigenvalue decomposition, their norm is equal to (and thus bounded by) 1.\nLet w\u2217 be the optimal solution to SCML-Global. By optimality of w\u2217 we have:\nL(w\u2217, z, z\u2032, z\u2032\u2032) + \u03b2\u2016w\u2217\u20161 \u2264 L(0, z, z\u2032, z\u2032\u2032) + \u03b2\u20160\u20161 = 1,\nthus we get \u2016w\u2217\u20161 \u2264 1/\u03b2. Let M\u2217 = \u2211K i=1w \u2217 i bib T i be the learned metric. Then using Holder\u2019s inequality and the bound on w\u2217 and the b\u2019s:\n\u2016M\u2217\u20161 = \u2225\u2225\u2225\u2225\u2225 K\u2211 i=1 w\u2217i bib T i \u2225\u2225\u2225\u2225\u2225 1 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i:wi 6=0 w\u2217i bib T i \u2225\u2225\u2225\u2225\u2225\u2225 1 \u2264 \u2016w\u2217\u20161 \u2211 i:wi 6=0 \u2016bi\u2016\u221e\u2016bi\u2016\u221e \u2264 K\u2217/\u03b2,\nwhere K\u2217 \u2264 K is the number of nonzero entries in w\u2217. Using Definition 1, we can partition Z into |Y|N (\u03b3,X , \u03c1) subsets such that if two examples z = (x, y) and z\u2032 = (x\u2032, y\u2032) belong to the same subset, then y = y\u2032 and \u03c1(x,x\u2032) \u2264 \u03b3. Now, for z1, z2, z3, z\u20321, z\u20322, z\u20323 \u2208 Z, if y1 = y\u20321, \u2016x1\u2212x\u20321\u20161 \u2264 \u03b3, y2 = y\u20322, \u2016x2\u2212x\u20322\u20161 \u2264 \u03b3, y3 = y\u20323, \u2016x3\u2212x\u20323\u20161 \u2264 \u03b3, then (z1, z2, z3) and (z\u20321, z \u2032 2, z \u2032 3) are either both admissible or both non-admissible triplets.\nIn the non-admissible case, by definition their respective loss is 0 and so is the deviation between the losses. In the admissible case we have:\u2223\u2223\u2223[1 + dw\u2217(x1,x2)\u2212 dw\u2217(x1,x3)]+ \u2212 [1 + dw\u2217(x\u20321,x\u20322)\u2212 dw\u2217(x\u20321,x\u20323)]+\u2223\u2223\u2223 \u2264 |(x1 \u2212 x2)TM\u2217(x1 \u2212 x2)\u2212 (x1 \u2212 x3)TM\u2217(x1 \u2212 x3) + (x\u20321 \u2212 x\u20323)TM\u2217(x\u20321 \u2212 x\u20323)\u2212 (x\u20321 \u2212 x\u20322)TM\u2217(x\u20321 \u2212 x\u20322)| = |(x1 \u2212 x2)TM\u2217(x1 \u2212 x2)\u2212 (x1 \u2212 x2)TM\u2217(x\u20321 \u2212 x\u20322) + (x1 \u2212 x2)TM\u2217(x\u20321 \u2212 x\u20322)\u2212 (x\u20321 \u2212 x\u20322)TM\u2217(x\u20321 \u2212 x\u20322)\n+ (x\u20321 \u2212 x\u20323)TM\u2217(x\u20321 \u2212 x\u20323)\u2212 (x\u20321 \u2212 x\u20323)TM\u2217(x1 \u2212 x3) + (x\u20321 \u2212 x\u20323)TM\u2217(x1 \u2212 x3)\u2212 (x1 \u2212 x3)TM\u2217(x1 \u2212 x3)| = |(x1 \u2212 x2)TM\u2217(x1 \u2212 x2 \u2212 (x\u20321 \u2212 x\u20322)) + (x1 \u2212 x2 \u2212 (x\u20321 \u2212 x\u20322))TM\u2217(x\u20321 \u2212 x\u20322) + (x\u20321 \u2212 x\u20323)TM\u2217(x\u20321 \u2212 x\u20323 \u2212 (x1 \u2212 x3)) + (x\u20321 \u2212 x\u20323 \u2212 (x1 \u2212 x3))TM\u2217(x1 \u2212 x3)| \u2264 |(x1 \u2212 x2)TM\u2217(x1 \u2212 x\u20321)|+ |(x1 \u2212 x2)TM\u2217(x\u20322 \u2212 x2)|+ |(x1 \u2212 x\u20321)TM\u2217(x\u20321 \u2212 x\u20322)|\n+ |(x\u20322 \u2212 x2)TM\u2217(x\u20321 \u2212 x\u20322)|+ |(x\u20321 \u2212 x\u20323)TM\u2217(x\u20321 \u2212 x1)|+ |(x\u20321 \u2212 x\u20323)TM\u2217(x3 \u2212 x\u20323)| + |(x\u20321 \u2212 x1)TM\u2217(x1 \u2212 x3)|+ |(x3 \u2212 x\u20323)TM\u2217(x1 \u2212 x3)|\n\u2264 \u2016x1 \u2212 x2\u2016\u221e\u2016M\u2217\u20161\u2016x1 \u2212 x\u20321\u20161 + \u2016x1 \u2212 x2\u2016\u221e\u2016M\u2217\u20161\u2016x\u20322 \u2212 x2\u20161 + \u2016x1 \u2212 x\u20321\u20161\u2016M\u2217\u20161\u2016x\u20321 \u2212 x\u20322\u2016\u221e + \u2016x\u20322 \u2212 x2\u20161\u2016M\u2217\u20161\u2016x\u20321 \u2212 x\u20322\u2016\u221e + \u2016x\u20321 \u2212 x\u20323\u2016\u221e\u2016M\u2217\u20161\u2016x\u20321 \u2212 x1\u20161 + \u2016x\u20321 \u2212 x\u20323\u2016\u221e\u2016M\u2217\u20161\u2016x3 \u2212 x\u20323\u20161 + \u2016x\u20321 \u2212 x1\u20161\u2016M\u2217\u20161\u2016x1 \u2212 x3\u2016\u221e + \u2016x3 \u2212 x\u20323\u20161\u2016M\u2217\u20161\u2016x1 \u2212 x3\u2016\u221e \u2264 16\u03b3RK \u2217\n\u03b2 ,\nby using the property that the hinge loss is 1-Lipschitz, Holder\u2019s inequality and bounds on the involved quantities. Thus SCML-Global is (|Y |N (\u03b3,X, \u2016\u00b7\u20161), 16\u03b3RK \u2217\n\u03b2 )-robust and the generalization bound follows.\nA.3.2 Bound for mt-SCML\nIn the multi-task setting, we are given a training sample St = {zi = (xti, yti)} nt i=1. Let CSt be the set of all admissible triplets built from instances in St. Let W \u2217 be the optimal solution to mt-SCML. Using the same arguments as for SCML-Global, by\noptimality of W \u2217 we have \u2016W \u2217\u20162,1 \u2264 1/\u03b2. Let M\u2217t = \u2211K i=1W \u2217 tibib T i be the learned metric for task t and W \u2217t be the weight vector for task t, corresponding to the t-th row of W \u2217. Then using the fact that \u2016W \u2217t \u20162,1 \u2264 \u2016W \u2217\u20162,1 and \u2016b\u20162,1 = \u2016b\u20162, we have:\n\u2016M\u2217t \u20162,1 = \u2225\u2225\u2225\u2225\u2225 K\u2211 i=1 W \u2217tibib T i \u2225\u2225\u2225\u2225\u2225 2,1 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i:W \u2217ti 6=0 W \u2217tibib T i \u2225\u2225\u2225\u2225\u2225\u2225 2,1 \u2264 \u2016W \u2217t \u20162,1 \u2211 i:W \u2217ti 6=0 \u2016bi\u20162,1\u2016bi\u20162,1 \u2264 K\u2217t /\u03b2,\nwhere K\u2217t \u2264 K is the number of nonzero entries in W \u2217t . From this we can derive a generalization bound for each task using arguments similar to the global case, using a partition specific to each task defined with respect to \u2016 \u00b7 \u20162. Without loss of generality, we focus on task t and only explicitly write the last derivations as the beginning is the same as above:\u2223\u2223[1 + dw\u2217(x1,x2)\u2212 dw\u2217(x1,x3)]+ \u2212 [1 + dw\u2217(x\u20321,x\u20322)\u2212 dw\u2217(x\u20321,x\u20323)]+\u2223\u2223 \u2264 |(x1 \u2212 x2)TM\u2217(x1 \u2212 x\u20321)|+ |(x1 \u2212 x2)TM\u2217(x\u20322 \u2212 x2)|+ |(x1 \u2212 x\u20321)TM\u2217(x\u20321 \u2212 x\u20322)|\n+ |(x\u20322 \u2212 x2)TM\u2217(x\u20321 \u2212 x\u20322)|+ |(x\u20321 \u2212 x\u20323)TM\u2217(x\u20321 \u2212 x1)|+ |(x\u20321 \u2212 x\u20323)TM\u2217(x3 \u2212 x\u20323)| + |(x\u20321 \u2212 x1)TM\u2217(x1 \u2212 x3)|+ |(x3 \u2212 x\u20323)TM\u2217(x1 \u2212 x3)|\n\u2264 \u2016x1 \u2212 x2\u20162\u2016M\u2217\u2016F\u2016x1 \u2212 x\u20321\u20162 + \u2016x1 \u2212 x2\u20162\u2016M\u2217\u2016F\u2016x\u20322 \u2212 x2\u20162 + \u2016x1 \u2212 x\u20321\u20162\u2016M\u2217\u2016F\u2016x\u20321 \u2212 x\u20322\u20162 + \u2016x\u20322 \u2212 x2\u20162\u2016M\u2217\u2016F\u2016x\u20321 \u2212 x\u20322\u20162 + \u2016x\u20321 \u2212 x\u20323\u20162\u2016M\u2217\u2016F\u2016x\u20321 \u2212 x1\u20162 + \u2016x\u20321 \u2212 x\u20323\u20162\u2016M\u2217\u2016F\u2016x3 \u2212 x\u20323\u20162 + \u2016x\u20321 \u2212 x2\u20162\u2016M\u2217\u2016F\u2016x1 \u2212 x3\u20162 + \u2016x3 \u2212 x\u20323\u20162\u2016M\u2217\u2016F\u2016x1 \u2212 x3\u20162 \u2264 16\u03b3RK \u2217 t\n\u03b2 ,\nwhere we used the same arguments as above and the inequality \u2016M\u2217\u2016F \u2264 \u2016M\u2217\u20162,1. Thus mt-SCML is (|Y |N (\u03b3,X, \u2016 \u00b7 \u20162), 16\u03b3RK \u2217 t\n\u03b2 )-robust and the bound for task t follows. Note that the number of training examples in the bound is only that from task t, i.e., n = nt.\nA.3.3 Comments on SCML-Local\nIt would be interesting to be able to derive a similar bound for SCML-Local. Unfortunately, as it is nonconvex, we cannot assume optimality of the solution. If a similar formulation can be made convex, the same proof technique should apply: even though each instance has its own metric, it essentially depends on the instance itself (whose norm is bounded) and on the learned parameters shared across metrics (which could be bounded using optimality of the solution). Deriving such a convex formulation and the corresponding generalization bound is left as future work."}, {"heading": "Appendix B Experimental Comparison with Support Vector Machines", "text": "In this section, we compare SCML-Global and SCML-Local to Support Vector Machines (SVM) using a linear and a RBF kernel. We used the software LIBSVM (Chang and Lin, 2011) and tuned the parameter C as well as the bandwidth for the RBF kernel on the validation set. Table 7 shows misclassification rates averaged over 20 random splits, along with standard error and the average rank of each method across all datasets. First, we can see that SCML-Global consistently performs better than linear SVM. Second, SCMLLocal is competitive with kernel SVM. These results show that a simple k-nearest neighbor strategy with a good metric can be competitive (and even outperform) SVM classifiers."}], "references": [{"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Mach. Learn.,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Robustness and Generalization for Metric Learning", "author": ["A. Bellet", "A. Habrard"], "venue": "Technical report,", "citeRegEx": "Bellet and Habrard.,? \\Q2012\\E", "shortCiteRegEx": "Bellet and Habrard.", "year": 2012}, {"title": "A Survey on Metric Learning for Feature Vectors and Structured Data", "author": ["Aur\u00e9lien Bellet", "Amaury Habrard", "Marc Sebban"], "venue": "Technical report,", "citeRegEx": "Bellet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2013}, {"title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In ACL,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Mach. Learn.,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "LIBSVM : a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Information-theoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Efficient Online and Batch Learning Using Forward Backward Splitting", "author": ["J. Duchi", "Y. Singer"], "venue": null, "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Learning Globally-Consistent Local Distance Functions for Shape-Based Image Retrieval and Classification", "author": ["A. Frome", "Y. Singer", "F. Sha", "J. Malik"], "venue": "In ICCV,", "citeRegEx": "Frome et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2007}, {"title": "Neighbourhood Components Analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Robust multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In KDD,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "A Geometric take on Metric Learning", "author": ["S. Hauberg", "O. Freifeld", "M. Black"], "venue": "In NIPS,", "citeRegEx": "Hauberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hauberg et al\\.", "year": 2012}, {"title": "Learning a mixture of sparse distance metrics for classification and dimensionality reduction", "author": ["Y. Hong", "Q. Li", "J. Jiang", "Z. Tu"], "venue": "In CVPR,", "citeRegEx": "Hong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2011}, {"title": "Online Metric Learning and Fast Similarity Search", "author": ["P. Jain", "B. Kulis", "I. Dhillon", "K. Grauman"], "venue": "In NIPS,", "citeRegEx": "Jain et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "entropy and -capacity of sets in functional spaces", "author": ["A. Kolmogorov", "V. Tikhomirov"], "venue": "American Mathematical Society Translations,", "citeRegEx": "Kolmogorov and Tikhomirov.,? \\Q1961\\E", "shortCiteRegEx": "Kolmogorov and Tikhomirov.", "year": 1961}, {"title": "Metric Learning: A Survey", "author": ["Brian Kulis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulis.,? \\Q2012\\E", "shortCiteRegEx": "Kulis.", "year": 2012}, {"title": "Generative Local Metric Learning for Nearest Neighbor Classification", "author": ["Y.-K. Noh", "B.-T. Zhang", "D. Lee"], "venue": "In NIPS,", "citeRegEx": "Noh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2010}, {"title": "Large Margin Multi-Task Metric Learning", "author": ["S. Parameswaran", "K. Weinberger"], "venue": "In NIPS,", "citeRegEx": "Parameswaran and Weinberger.,? \\Q2010\\E", "shortCiteRegEx": "Parameswaran and Weinberger.", "year": 2010}, {"title": "Local Distance Functions: A Taxonomy, New Algorithms, and an Evaluation", "author": ["D. Ramanan", "S. Baker"], "venue": "TPAMI, 33(4):794\u2013806,", "citeRegEx": "Ramanan and Baker.,? \\Q2011\\E", "shortCiteRegEx": "Ramanan and Baker.", "year": 2011}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural Comput.,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Positive Semidefinite Metric Learning", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "Using Boostinglike Algorithms. JMLR,", "citeRegEx": "Shen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2012}, {"title": "Sparse Compositional Metric Learning", "author": ["Y. Shi", "A. Bellet", "F. Sha"], "venue": "In AAAI,", "citeRegEx": "Shi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2014}, {"title": "Visualizing Data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "JMLR, 9:2579\u20132605,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Parametric Local Metric Learning for Nearest Neighbor Classification", "author": ["J. Wang", "A. Woznica", "A. Kalousis"], "venue": "In NIPS,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Distance Metric Learning for Large Margin", "author": ["K. Weinberger", "L. Saul"], "venue": "Nearest Neighbor Classification. JMLR,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization", "author": ["L. Xiao"], "venue": "JMLR, 11:2543\u20132596,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Distance Metric Learning with Application to Clustering with Side-Information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Robustness and Generalization", "author": ["H. Xu", "S. Mannor"], "venue": "Mach. Learn.,", "citeRegEx": "Xu and Mannor.,? \\Q2012\\E", "shortCiteRegEx": "Xu and Mannor.", "year": 2012}, {"title": "Heterogeneous multitask learning with joint sparsity constraints", "author": ["X. Yang", "S. Kim", "E. Xing"], "venue": "In NIPS,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Distance Metric Learning with Eigenvalue Optimization", "author": ["Y. Ying", "P. Li"], "venue": "JMLR, 13:1\u201326,", "citeRegEx": "Ying and Li.,? \\Q2012\\E", "shortCiteRegEx": "Ying and Li.", "year": 2012}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Yuan and Lin.,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "Learning instance specific distances using metric propagation", "author": ["D.-C. Zhan", "M. Li", "Y.-F. Li", "Z.-H. Zhou"], "venue": "In ICML,", "citeRegEx": "Zhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhan et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "This has led to a growing interest in metric learning for the past few years, as summarized in two recent surveys (Bellet et al., 2013; Kulis, 2012).", "startOffset": 114, "endOffset": 148}, {"referenceID": 15, "context": "This has led to a growing interest in metric learning for the past few years, as summarized in two recent surveys (Bellet et al., 2013; Kulis, 2012).", "startOffset": 114, "endOffset": 148}, {"referenceID": 26, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 9, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 6, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 13, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 24, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 20, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 29, "context": "Representative methods include (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009; Shen et al., 2012; Ying and Li, 2012).", "startOffset": 31, "endOffset": 179}, {"referenceID": 8, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 24, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 31, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 12, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 23, "context": "To overcome this limitation, recent work has focused on learning multiple locally linear metrics at several locations of the feature space (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Hong et al., 2011; Wang et al., 2012), to the extreme of learning one metric per training instance (Noh et al.", "startOffset": 139, "endOffset": 243}, {"referenceID": 16, "context": ", 2012), to the extreme of learning one metric per training instance (Noh et al., 2010).", "startOffset": 69, "endOffset": 87}, {"referenceID": 18, "context": "This line of research is motivated by the fact that locally, simple linear metrics perform well (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 96, "endOffset": 143}, {"referenceID": 11, "context": "This line of research is motivated by the fact that locally, simple linear metrics perform well (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 96, "endOffset": 143}, {"referenceID": 21, "context": "Furthermore, \u2217This document is an extended version of a conference paper (Shi et al., 2014) that provides additional details and results.", "startOffset": 73, "endOffset": 91}, {"referenceID": 4, "context": "The proposed framework also applies to multi-task metric learning, where one wants to learn a global metric for several related tasks while exploiting commonalities between them (Caruana, 1997; Parameswaran and Weinberger, 2010).", "startOffset": 178, "endOffset": 228}, {"referenceID": 17, "context": "The proposed framework also applies to multi-task metric learning, where one wants to learn a global metric for several related tasks while exploiting commonalities between them (Caruana, 1997; Parameswaran and Weinberger, 2010).", "startOffset": 178, "endOffset": 228}, {"referenceID": 18, "context": "This can be seen as learning a smoothly varying metric tensor over the feature space (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 85, "endOffset": 132}, {"referenceID": 11, "context": "This can be seen as learning a smoothly varying metric tensor over the feature space (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 85, "endOffset": 132}, {"referenceID": 7, "context": "All formulations can be solved using scalable optimization procedures based on stochastic subgradient descent with proximal operators (Duchi and Singer, 2009; Xiao, 2010).", "startOffset": 134, "endOffset": 170}, {"referenceID": 25, "context": "All formulations can be solved using scalable optimization procedures based on stochastic subgradient descent with proximal operators (Duchi and Singer, 2009; Xiao, 2010).", "startOffset": 134, "endOffset": 170}, {"referenceID": 24, "context": "C may be constructed from label information, as in LMNN (Weinberger and Saul, 2009), or in an unsupervised manner based for instance on implicit users\u2019 feedback (such as clicks on search engine results).", "startOffset": 56, "endOffset": 83}, {"referenceID": 4, "context": "2 Multi-Task Metric Learning Multi-task learning (Caruana, 1997) is a paradigm for learning several tasks simultaneously, exploiting their commonalities.", "startOffset": 49, "endOffset": 64}, {"referenceID": 0, "context": "Recently, multi-task learning methods have successfully built on the assumption that the tasks should share a common low-dimensional representation (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012).", "startOffset": 148, "endOffset": 209}, {"referenceID": 28, "context": "Recently, multi-task learning methods have successfully built on the assumption that the tasks should share a common low-dimensional representation (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012).", "startOffset": 148, "endOffset": 209}, {"referenceID": 10, "context": "Recently, multi-task learning methods have successfully built on the assumption that the tasks should share a common low-dimensional representation (Argyriou et al., 2008; Yang et al., 2009; Gong et al., 2012).", "startOffset": 148, "endOffset": 209}, {"referenceID": 30, "context": "where W is a T \u00d7 K nonnegative matrix whose t-th row is the weight vector wt defining the metric for task t, Lwt(xi,xj ,xk) = [1 + dwt(xi,xj)\u2212 dwt(xi,xk)]+ and \u2016W \u20162,1 is the `2/`1 mixed norm used in the group lasso problem (Yuan and Lin, 2006).", "startOffset": 224, "endOffset": 244}, {"referenceID": 8, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 24, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 31, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 16, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 12, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 23, "context": "3 Local Metric Learning Local metric learning addresses the limitations of global methods in capturing complex data patterns (Frome et al., 2007; Weinberger and Saul, 2009; Zhan et al., 2009; Noh et al., 2010; Hong et al., 2011; Wang et al., 2012).", "startOffset": 125, "endOffset": 247}, {"referenceID": 18, "context": "We aim at learning a metric tensor T (x), which is a smooth function that (informally) maps any instance x to its metric matrix (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 128, "endOffset": 175}, {"referenceID": 11, "context": "We aim at learning a metric tensor T (x), which is a smooth function that (informally) maps any instance x to its metric matrix (Ramanan and Baker, 2011; Hauberg et al., 2012).", "startOffset": 128, "endOffset": 175}, {"referenceID": 31, "context": "However, this requires solving an intractable problem, so we use the widely-adopted simplification that distances from point x are computed based on its own metric alone (Zhan et al., 2009; Noh et al., 2010; Wang et al., 2012): dT (x,x \u2032) = (x\u2212 x\u2032)TT (x)(x\u2212 x\u2032) = (x\u2212 x\u2032)T K \u2211", "startOffset": 170, "endOffset": 226}, {"referenceID": 16, "context": "However, this requires solving an intractable problem, so we use the widely-adopted simplification that distances from point x are computed based on its own metric alone (Zhan et al., 2009; Noh et al., 2010; Wang et al., 2012): dT (x,x \u2032) = (x\u2212 x\u2032)TT (x)(x\u2212 x\u2032) = (x\u2212 x\u2032)T K \u2211", "startOffset": 170, "endOffset": 226}, {"referenceID": 23, "context": "However, this requires solving an intractable problem, so we use the widely-adopted simplification that distances from point x are computed based on its own metric alone (Zhan et al., 2009; Noh et al., 2010; Wang et al., 2012): dT (x,x \u2032) = (x\u2212 x\u2032)TT (x)(x\u2212 x\u2032) = (x\u2212 x\u2032)T K \u2211", "startOffset": 170, "endOffset": 226}, {"referenceID": 19, "context": "We can solve them efficiently using stochastic composite optimization (Duchi and Singer, In our experiments, we use kernel PCA (Sch\u00f6lkopf et al., 1998) as it provides a simple way to limit the dimension and thus the number of parameters to learn.", "startOffset": 127, "endOffset": 151}, {"referenceID": 25, "context": "We solve SCML-Global and mt-SCML using Regularized Dual Averaging (Xiao, 2010), which offers fast convergence and levels of sparsity in the solution comparable to batch algorithms.", "startOffset": 66, "endOffset": 78}, {"referenceID": 7, "context": "For SCML-Local, due to local minima, we ensure improvement over the optimal solution w\u2217 of SCML-Global by using a forward-backward algorithm (Duchi and Singer, 2009) which is initialized with A = 0 and ci = \u221a w\u2217 i .", "startOffset": 141, "endOffset": 165}, {"referenceID": 27, "context": "In this section, we provide a theoretical analysis of our approach in the form of a generalization bound based on algorithmic robustness analysis (Xu and Mannor, 2012) and its adaptation to metric learning (Bellet and Habrard, 2012).", "startOffset": 146, "endOffset": 167}, {"referenceID": 1, "context": "In this section, we provide a theoretical analysis of our approach in the form of a generalization bound based on algorithmic robustness analysis (Xu and Mannor, 2012) and its adaptation to metric learning (Bellet and Habrard, 2012).", "startOffset": 206, "endOffset": 232}, {"referenceID": 27, "context": "This is in accordance with other similar learning bounds, for example, the original robustness-based bounds in (Xu and Mannor, 2012).", "startOffset": 111, "endOffset": 132}, {"referenceID": 14, "context": "The interested reader should refer to the recent surveys of Kulis (2012) and Bellet et al.", "startOffset": 60, "endOffset": 73}, {"referenceID": 2, "context": "The interested reader should refer to the recent surveys of Kulis (2012) and Bellet et al. (2013) for more details.", "startOffset": 77, "endOffset": 98}, {"referenceID": 26, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 9, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 6, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 13, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 24, "context": "Global methods Most global metric learning methods learn the matrix M directly: see (Xing et al., 2002; Goldberger et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger and Saul, 2009) for representative papers.", "startOffset": 84, "endOffset": 194}, {"referenceID": 20, "context": "An exception is BoostML (Shen et al., 2012) which uses rank-one matrices as weak learners to learn a global Mahalanobis distance via a boosting procedure.", "startOffset": 24, "endOffset": 43}, {"referenceID": 17, "context": "Multi-task methods Multi-task metric learning was proposed in (Parameswaran and Weinberger, 2010) as an extension to the popular LMNN (Weinberger and Saul, 2009).", "startOffset": 62, "endOffset": 97}, {"referenceID": 24, "context": "Multi-task methods Multi-task metric learning was proposed in (Parameswaran and Weinberger, 2010) as an extension to the popular LMNN (Weinberger and Saul, 2009).", "startOffset": 134, "endOffset": 161}, {"referenceID": 24, "context": "Local methods MM-LMNN (Weinberger and Saul, 2009) is an extension of LMNN which learns only a small number of metrics (typically one per class) in an effort to alleviate overfitting.", "startOffset": 22, "endOffset": 49}, {"referenceID": 12, "context": "msNCA (Hong et al., 2011) learns a function that splits the space into a small number of regions and then learns a metric per region using NCA (Goldberger et al.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": ", 2011) learns a function that splits the space into a small number of regions and then learns a metric per region using NCA (Goldberger et al., 2004).", "startOffset": 125, "endOffset": 150}, {"referenceID": 23, "context": "Like SCML-Local, PLML (Wang et al., 2012) is based on a combination of metrics but there are major differences with our work: (i) weights only depend on a manifold assumption: they are not sparse and use no discriminative information, (ii) the basis metrics are full-rank, thus expensive to learn, and (iii) a weight vector is learned explicitly for each training instance, which can result in a large number of parameters and prevents generalization to new instances (in practice, for a test point, they use the weight vector of its nearest neighbor in the training set).", "startOffset": 22, "endOffset": 41}, {"referenceID": 31, "context": "ISD (Zhan et al., 2009) is an attempt to learn the metrics for unlabeled points by propagation, but is limited to the transductive setting.", "startOffset": 4, "endOffset": 23}, {"referenceID": 16, "context": "Unlike the above discriminative approaches, GLML (Noh et al., 2010) learns a metric for each point independently in a generative way by minimizing the 1-NN expected error under some assumption for the class distributions.", "startOffset": 49, "endOffset": 67}, {"referenceID": 9, "context": ", 2011) learns a function that splits the space into a small number of regions and then learns a metric per region using NCA (Goldberger et al., 2004). Again, the metrics are full-rank so msNCA does not scale well with the number of metrics. Like SCML-Local, PLML (Wang et al., 2012) is based on a combination of metrics but there are major differences with our work: (i) weights only depend on a manifold assumption: they are not sparse and use no discriminative information, (ii) the basis metrics are full-rank, thus expensive to learn, and (iii) a weight vector is learned explicitly for each training instance, which can result in a large number of parameters and prevents generalization to new instances (in practice, for a test point, they use the weight vector of its nearest neighbor in the training set). As observed by Ramanan and Baker (2011), the above methods make the implicit assumption that the metric tensor is locally constant (at the class, region or neighborhood level), while SCML-Local learns a smooth function that maps any instance to its specific metric.", "startOffset": 126, "endOffset": 855}, {"referenceID": 23, "context": "We normalize the data as in (Wang et al., 2012) and split into train/validation/test (60%/20%/20%), except for Letters and USPS where we use 3,000/1,000/1,000.", "startOffset": 28, "endOffset": 47}, {"referenceID": 20, "context": "2 Comparison to Other Global Algorithms Setup We now compare SCML-Global to two state-of-the-art global metric learning algorithms: Large Margin Nearest Neighbor (LMNN, Weinberger and Saul, 2009) and BoostML (Shen et al., 2012).", "startOffset": 208, "endOffset": 227}, {"referenceID": 3, "context": "2 Multi-task Metric Learning Dataset Sentiment Analysis (Blitzer et al., 2007) is a popular dataset for multi-task learning that consists of Amazon reviews on four product types: kitchen appliances, DVDs, books and electronics.", "startOffset": 56, "endOffset": 78}, {"referenceID": 17, "context": "Setup We compare the following metrics: st-Euc (Euclidean distance), st-LMNN and st-SCML (singletask LMNN and single-task SCML-Global, trained independently on each task), u-Euc (Euclidean trained on the union of the training data from all tasks), u-LMNN (LMNN on union), u-SCML (SCML-Global on union), multi-task LMNN (Parameswaran and Weinberger, 2010) and finally our own multi-task method mt-SCML.", "startOffset": 319, "endOffset": 354}, {"referenceID": 24, "context": "We compare SCMLLocal to MM-LMNN (Weinberger and Saul, 2009), GLML (Noh et al.", "startOffset": 32, "endOffset": 59}, {"referenceID": 16, "context": "We compare SCMLLocal to MM-LMNN (Weinberger and Saul, 2009), GLML (Noh et al., 2010) and PLML (Wang et al.", "startOffset": 66, "endOffset": 84}, {"referenceID": 23, "context": ", 2010) and PLML (Wang et al., 2012).", "startOffset": 17, "endOffset": 36}], "year": 2014, "abstractText": "We propose a new approach for metric learning by framing it as learning a sparse combination of locally discriminative metrics that are inexpensive to generate from the training data. This flexible framework allows us to naturally derive formulations for global, multi-task and local metric learning. The resulting algorithms have several advantages over existing methods in the literature: a much smaller number of parameters to be estimated and a principled way to generalize learned metrics to new testing data points. To analyze the approach theoretically, we derive a generalization bound that justifies the sparse combination. Empirically, we evaluate our algorithms on several datasets against state-of-the-art metric learning methods. The results are consistent with our theoretical findings and demonstrate the superiority of our approach in terms of classification performance and scalability.", "creator": "LaTeX with hyperref package"}}}