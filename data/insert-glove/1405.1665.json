{"id": "1405.1665", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2014", "title": "On Communication Cost of Distributed Statistical Estimation and Dimensionality", "abstract": "ultra-light We ratnakara explore bedouin the connection between restrained dimensionality and footrest communication accola cost in diborane distributed learning problems. Specifically 500-mhz we study the problem of maree estimating going-away the mean \\ vectheta of an faunce unknown d dimensional afsoc normal distribution kalanaur in triclinium the grundy distributed strood setting. In this kabarole problem, the croel samples from salin the unknown castelnaudary distribution electrotherapy are th\u00e9nardiers distributed processus among m griffey different sandwell machines. The teun goal is to ern\u0151 estimate nantie the mean \\ dkr vectheta at the cossato optimal 5-shot minimax afhq rate \u00e1sgeirsson while communicating as 24-gun few clardy bits godolphin as possible. hermansson We arts show that woodpile in this ravenhill simple sarafan setting, the docosahexaenoic communication cost kotlikoff scales 56.39 linearly in jammed the extra-pair number of cuffs dimensions caida i. abstract e. kiradech one needs to axelle deal with 107th different dimensions jetavana individually.", "histories": [["v1", "Wed, 7 May 2014 16:44:21 GMT  (8kb)", "http://arxiv.org/abs/1405.1665v1", null], ["v2", "Sat, 8 Nov 2014 03:06:04 GMT  (28kb)", "http://arxiv.org/abs/1405.1665v2", "to appear at NIPS'14 with oral presentation"]], "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["ankit garg", "tengyu ma", "huy l nguyen"], "accepted": true, "id": "1405.1665"}, "pdf": {"name": "1405.1665.pdf", "metadata": {"source": "CRF", "title": "Lower Bound for High-Dimensional Statistical Learning Problem via Direct-Sum Theorem", "authors": ["Ankit Garg", "Tengyu Ma", "Huy L. Nguy\u1ec5n"], "emails": ["garg@cs.princeton.edu.", "tengyu@cs.princeton.edu.", "hlnguyen@cs.princeton.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n16 65\nv1 [\ncs .L\nG ]\n7 M\nay 2"}, {"heading": "1 Introduction", "text": "The last decade has witnessed a tremendous growth in the amount data involved in machine learning tasks. In many cases, data volume has outgrown the capacity of a single machine and it is increasingly common that learning tasks are performed in a distributed fashion on many machines. Beside traditional aspects of computation such as the running time and memory usage, communication has emerged as an important resource and sometimes the bottleneck of the whole system. A lot of recent works in machine learning are devoted to understanding the amount of communication needed in distributed learning tasks [BBFM12, IPSV12b, IPSV12a, ZDJW13].\nIn this paper, we study the relation between the dimensionality and the communication complexity of statistical estimation problems. Most modern statistical problems are characterized by high dimensionality. Thus, it is natural to ask the following meta question:\nHow does the communication cost scale in the dimensionality? We study this question via the problem of estimating the mean \u03b8 of an unknown d dimensional normal distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean \u03b8 at the optimal minimax rate while communicating as few bits as possible. We show that in this simplest setting, one really needs to deal with different dimensions individually.\nTheorem 1.1. [Informal] To estimate the mean of a d-dimensional Gaussian in the distributed setting with error R, one must pay \u2126(d) times the minimum communication cost needed for estimating the mean of one dimensional Gaussian with error R/d.\n\u2217Department of Computer Science, Princeton University, email: garg@cs.princeton.edu. \u2020Department of Computer Science, Princeton University, email: tengyu@cs.princeton.edu. \u2021Department of Computer Science, Princeton University, email: hlnguyen@cs.princeton.edu.\nThe work [ZDJW13] showed a lower bound on the communication cost for this problem when d = 1. Our technique when applied to their theorem immediately yields a lower bound equal to d times the lower bound for the one dimension problem for any choice of d. See Theorem 3.3 for the precise statement.\nWe use tools from the recent development in communication complexity and information complexity. There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [CSWY01, BYJKS04, BR11, BBCR13, BEO+13]. Information complexity can be thought of as a proxy for communication complexity that is especially accurate for solving multiple copies of the same problem simultaneously [BR11]. It has become a standard tool for proving so-called \u201cdirect-sum\u201d results, namely the fact that the amount of resources required for solving d copies of a problem in parallel is equal to d times the amount required for one copy. In other words, there is no saving from solving many copies of the same problem in batch and the trivial solution of solving each of them separately is optimal. Our result can be viewed as a direct sum theorem for communication complexity for statistical estimation problems: the amount of communication needed for solving an estimation problem in d dimensions is equal to d times the amount of communication needed for the same problem in one dimension. The proof technique is directly inspired by the notion of conditional information complexity [BYJKS04], which was used to prove direct sum theorems and lower bounds for streaming algorithms. We believe this is a fruitful connection and can lead to more lower bounds in statistical machine learning.\nOur techniques. Consider the problem of estimating some parameter \u03b8 \u2208 Rd of a d-dimensional distribution from samples. To prove a lower bound for the d dimensional problem using an existing lower bound for one dimensional problem, we demonstrate a reduction that uses the (hypothetical) protocol for d dimensions to construct a better protocol for the one dimensional problem. The new protocol for the one dimensional problem works as follows: the one dimensional problem is embedded in a random coordinate of the d dimensional problem and the rest of the coordinates are filled in independently according to the prior of \u03b8. As the other coordinates are independent from the input, they are shared among the machines before the protocol starts. When the machines get the samples, they proceed to simulate the protocol for the d-dimensional problem.\nThe first question to ask is, what is this \u201csimulation\u201d protocol for the one dimensional problem good for. It has the same communication cost as the protocol for the d-dimensional problem, and that doesn\u2019t seem to help. But it turns out, while the communication cost remains the same, the information cost goes down by a factor of d. The information is somehow smeared across all the dimensions. This is consistent with a general paradigm in mathematics, when certain discrete quantities are not that well behaved, but there continuous relaxations are, and are often used as a proxy to study the discrete quantities. Here we are using information cost as a proxy for communication cost. Using the \u201csimulation\u201d protocol, we prove that the information cost of the d-dimensional problem is d times the information cost of the one dimensional problem. Now using the fact that information cost is less than the communication cost, we get a lower bound for the communication cost of the d dimensional problem in terms of the information cost of the one dimensional problem. The work [ZDJW13] already showed a lower bound on the information cost of the one dimensional problem."}, {"heading": "2 Notation and Setup", "text": "Statistical parameter estimation Let P be a family of distributions over X . Let \u03b8 : P \u2192 \u0398 denote a function defined on P. We are given samples X1, . . . ,Xn from some P \u2208 P, and are asked to estimate \u03b8(P ). Let \u03b8\u0302 : X n \u2192 \u0398 be such an estimator, and \u03b8\u0302(X1, . . . ,Xn) is the corresponding estimate.\nDefine the squared loss R of the estimator to be\nR(\u03b8\u0302, \u03b8) = E \u03b8\u0302,X,\u03b8\n[\n\u2016\u03b8\u0302(X1, . . . ,Xn)\u2212 \u03b8(P )\u2016 2 2\n]\nIn the high-dimensional case, let Pd := {~P = P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Pd : Pi \u2208 P} be the family of product distributions over X d. Let ~\u03b8 : Pd \u2192 \u0398d \u2282 Rd be the d-dimensional function obtained by applying \u03b8 point-wise ~\u03b8 (P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Pd) = (\u03b8(P1), . . . , \u03b8(Pd)).\nThroughout this paper, we consider the case when P = {N (\u03b8, \u03c32) : \u03b8 \u2208 [\u22121, 1]} for some fixed\n\u03c3. Therefore, in the high-dimensional case, Pd = {N (~\u03b8 , \u03c32Id) : ~\u03b8 \u2208 [\u22121, 1] d}. We use ~\u0302\u03b8 to denote the d-dimensional estimator. For clarity, in this paper, we always use~\u00b7 to indicate a vector in high dimensions. Multi-Machine setting: There aremmachines. Machine j receives n samples ~X(j,1), . . . , ~X(j,n) \u2208 X d from the distribution ~P . The machines communicate via a publicly shown blackboard. When a machine writes a message on the blackboard, all other machines can see the content of the message. Note that this model captures both point-to-point communication as well as broadcast communication. Therefore, our lower bounds in this model apply to both the message passing setting and the broadcast setting. We denote the transcript of the communication as Y . A deterministic function ~\u0302\u03b8 is then applied to the transcript Y to get the estimation of the mean ~\u0302\u03b8(Y ). Let letter j be reserved for index of the machine and k for the samples and letter i for the dimension. In other words, ~X (j,k) i is the ith-coordinate of kth sample of machine j. Private/public randomness: We allow the protocol to use both private and public randomness, which is crucial. The public randomness is used purely for convenience in the proof and is not counted toward the total communication because it can be shared among machines before the start of the protocol. Alternatively, because the protocol works well on average over all public randomness, there exists a fixing of the public randomness so that the protocol still works as well as the average. This particular fixing of the public randomness gives a protocol that uses no public randomness at all while performing just as well as the one with public randomness. On the other hand, the use of private randomness is extremely crucial. With a little bit of thought, one can be convinced that the machines can use private randomness to hide information from other machines in a protocol. Indeed, we will see that in the direct sum argument, the simulation protocol for one dimension, private randomness plays a very important role. Lets denote public and private randomness of the protocol by Rpub and Rpriv respectively.\nWe define the squared loss of a protocol \u03a0 by\nR\n(\n(\u03a0, ~\u0302\u03b8), ~\u03b8\n)\n= E ~\u03b8 , ~X,Y,Rpub,Rpriv\n[\u2016~\u0302\u03b8(Y )\u2212 ~\u03b8 \u20162]\nInformation cost: We define information cost IC(\u03a0) of protocol \u03a0 as follows:\nIC(\u03a0) = I( ~X ;Y | ~\u03b8 ,Rpub)\nPrivate randomness doesn\u2019t explicitly appear in the definition of information cost but if affects it. Note that the information cost is a lower bound on the communication cost:\nIC(\u03a0) = I( ~X ;Y | ~\u03b8 ) \u2264 H(Y ) \u2264 length of Y"}, {"heading": "3 Distributed Statistical Learning", "text": "We start by formally defining the our task and the mean-squared loss and information cost of a protocol.\nDefinition 1. We say a protocol and estimator pair (\u03a0, ~\u0302\u03b8) solves task T (d,m, n, \u03c32,Dd\u03b8) with information cost C and mean-squared loss R, if for ~\u03b8 randomly chosen from Dd\u03b8 , m machines, each of which takes n samples from N (~\u03b8 , \u03c32Id) as input, can run the protocol \u03a0 and get transcript Y so that the followings are true:\nE[\u2016~\u0302\u03b8(Y )\u2212 ~\u03b8 \u2016 2] = R (1)\nI( ~X ;Y | ~\u03b8 ) = C (2)\nTheorem 3.1. [Direct-sum Theorem] If (\u03a0, ~\u0302\u03b8) solves the task T (d,m, n, \u03c32,Vd) with information cost C and squared loss R, there exists (\u03a0\u2032, \u03b8\u0302) that solves the task T (1,m, n, \u03c32,V) with information cost at most 4C/d and squared loss 4R/d.\nProof. For each i \u2208 [d], we could define the following protocol \u03a0i and estimator \u03b8\u0302i induced by the d-dimensional estimator ~\u0302\u03b8. \u03a0i is described as Protocol 1.\nInputs : Machine j gets samples X(j,1), . . . ,X(j,n) distributed according to N (\u03b8, \u03c32), where \u03b8 \u223c V.\n1. All machines publicly sample \u03b8\u0306\u2212i distributed according to V d\u22121.\n2. Machine j privately samples X\u0306 (j,1) \u2212i , . . . , X\u0306 (j,n) \u2212i distributed according to N (\u03b8\u0306\u2212i, \u03c3 2Id\u22121). Let\nX\u0306(j,k) = (X\u0306 (j,k) 1 , . . . , X\u0306 (j,k) i\u22121 ,X (j,k), X\u0306 (j,k) i+1 , . . . , X\u0306 (j,k) d ).\n3. All machines run protocol \u03a0 on data X\u0306 and get transcript Yi. The estimator \u03b8\u0302i is \u03b8\u0302i(Yi) =\n~\u0302\u03b8(Yi)i i.e. the i th coordinate of the d-dimensional estimator.\nProtocol 1: \u03a0i\nThe role of private randomness can be crucially seen here. It is very important for the machines to privately get samples in coordinates other than i for the information cost to go down by a factor of d. Lets denote the private and public randomness of the protocol \u03a0i as Rpriv and Rpub respectively. We prove that \u03a0i does a good job in the average sense by the following two lemmas:\nLemma 1. If \u03b8 \u223c V and ~\u03b8 \u223c Vd, then\nd \u2211\ni=1\nR ( (\u03a0i, \u03b8\u0302i), \u03b8 ) = R ( (\u03a0, ~\u03b8 ), ~\u03b8 )\nProof. Note that\nR ( (\u03a0i, \u03b8\u0302i), \u03b8 )\n= E \u03b8,X,Yi,Rpriv,Rpub\n[(\u03b8\u0302i(Yi)\u2212 \u03b8) 2]\n= E \u03b8,X,Yi,Rpriv,Rpub\n[(~\u0302\u03b8(Yi)i \u2212 \u03b8) 2]\nHence\nd \u2211\ni=1\nR ( (\u03a0i, \u03b8\u0302i), \u03b8 ) = d \u2211\ni=1\nE \u03b8,X,Yi,Rpriv,Rpub\n[(~\u0302\u03b8(Yi)i \u2212 \u03b8) 2]\n= E ~\u03b8 , ~X,Y\n[ d \u2211\ni=1\n(~\u0302\u03b8(Y )i \u2212 ~\u03b8 i) 2]\n= E ~\u03b8 , ~X,Y\n[\u2016~\u0302\u03b8(Y )\u2212 ~\u03b8 \u20162]\nThe second equality follows from the fact that the joint distribution of \u03b8,X, Yi, Rpriv, Rpub is the same as the distribution of ~\u03b8 , ~X, Y . Also the marginal distributions of ~\u03b8 are the same as the distribution of \u03b8.\nLemma 2. If \u03b8 \u223c V and ~\u03b8 \u223c Vd, then\nd \u2211\ni=1\nIC(\u03a0i) \u2264 IC(\u03a0)\nProof. Recall under (\u03a0i, \u03b8\u0302i), machines prepare X\u0306, which has the same distribution as ~X in the problem T (d,m, n, \u03c32,Vd). Also the joint distribution of ~Xi, Y, ~\u03b8 is the same as the distribution of X,Yi, \u03b8, \u03b8\u0306\u2212i. Therefore, we have that\nI( ~Xi;Y | ~\u03b8 ) = I(X;Yi | \u03b8, \u03b8\u0306\u2212i)\nSince IC(\u03a0i) = I(X;Yi | \u03b8,Rpub) = I(X;Yi | \u03b8, \u03b8\u0306\u2212i), we have that\nd \u2211\ni=1\nIC(\u03a0i) = d \u2211\ni=1\nI(X;Yi | \u03b8, \u03b8\u0306\u2212i)\n= d \u2211\ni=1\nI( ~Xi;Y | ~\u03b8 )\nSince the distribution of ~X conditioned on ~\u03b8 isN (~\u03b8 , \u03c32Id), ~X1, . . . , ~Xd are independent conditioned on ~\u03b8 . Hence\nd \u2211\ni=1\nI( ~Xi;Y | ~\u03b8 ) \u2264 I( ~X ;Y | ~\u03b8 ) = IC(\u03a0)\nThe inequality is true because of the following:\nI( ~X ;Y | ~\u03b8 ) = d \u2211\ni=1\nI( ~Xi;Y | ~\u03b8 , ~X1, . . . , ~Xi\u22121)\n= d \u2211\ni=1\n( H( ~Xi | ~\u03b8 , ~X1, . . . , ~Xi\u22121)\u2212H( ~Xi | Y, ~\u03b8 , ~X1, . . . , ~Xi\u22121) )\n= d \u2211\ni=1\n( H( ~Xi | ~\u03b8 )\u2212H( ~Xi | Y, ~\u03b8 , ~X1, . . . , ~Xi\u22121) )\n\u2265 d \u2211\ni=1\n( H( ~Xi | ~\u03b8 )\u2212H( ~Xi | Y, ~\u03b8 ) )\n= d \u2211\ni=1\nI( ~Xi;Y | ~\u03b8 )\nThe third equality is true because ~X1, . . . , ~Xd are independent conditioned on ~\u03b8 . The inequality follows from the fact that conditioning decreases entropy.\nBy Lemma 1 and Lemma 2 and a Markov argument, there exists an i \u2208 {1, . . . , d} such that\nR ( (\u03a0i, \u03b8\u0302i), \u03b8 ) \u2264 4 d \u00b7R ( (\u03a0, ~\u03b8 ), ~\u03b8 )\nand\nIC(\u03a0i) \u2264 4\nd \u00b7 IC(\u03a0)\nThen the pair (\u03a0\u2032, \u03b8\u0302) = (\u03a0i, \u03b8\u0302i) solves the task T (1,m, n, \u03c3 2,V) with information cost at most 4C/d and squared loss 4R/d.\nWe are going to apply the theorem above to the one-dimensional lower bound by [ZDJW13]. This theorem is not explicitly stated in the paper but is implicit in the proof of Theorem 1 in their paper. Also they do not mention this, but their techniques are general enough to prove lower bounds on the information cost for protocols with private randomness. Also in their case, the definition of information cost is a bit different. They do not condition on the prior of \u03b8, but since in the one dimensional case, this prior is just over {\u00b1\u03b4}, conditioning on it can reduce the mutual information by at most 1 bit.\nI(X;Y | \u03b8,Rpub) \u2265 I(X;Y |Rpub)\u2212 1\nTheorem 3.2. [ZDJW13] Let V be the uniform distribution over {\u00b1\u03b4}, where \u03b42 \u2264 min ( 1, \u03c3 2 log(m)\nn\n)\n.\nIf (\u03a0, \u03b8\u0302) solves the task T (1,m, n, \u03c32,V) with information cost C and squared loss R, then either C \u2265 \u2126 ( \u03c32\n\u03b42n log(m)\n)\nor R \u2265 \u03b42/10.\nThe corollary below directly follows from Theorem 3.2 and Theorem 3.1.\nCorollary 3.1. Let V be the uniform distribution over {\u00b1\u03b4}, where \u03b42 \u2264 min ( 1, \u03c3 2 logm n ) . If (\u03a0, \u03b8\u0302) solves the task T (1,m, n, \u03c32,Vd) with information cost C and squared loss R, then either C \u2265 \u2126 ( d\u03c32\n\u03b42n logm\n)\nor R \u2265 d\u03b42/40.\nThis immediately proves the main theorem of the paper.\nTheorem 3.3. If (\u03a0, \u03b8\u0302) estimates the mean of N (~\u03b8 , \u03c32Id), where ~\u03b8 \u2208 [\u22121, 1] d, with mean-squared loss R, and communication cost B. Then\nR \u2265 \u2126\n(\nmin\n{\nd2\u03c32\nnB logm ,\nd\u03c32\nn logm ,d\n})\nAs a corollary, to achieve the optimal mean-squared loss R = d\u03c3 2\nmn for the case when data is on a\nsingle machine, the communication cost is at least B = \u2126 (\ndm logm\n)\nProof. Apply corollary 3.1 with the trivial bound C \u2264 B. We divide into two cases depending on whether B \u2265 1 c \u00b7 max ( d\u03c32 n logm , d log2 m ) or not, c > 1 is a constant to be specified later. If B \u2265 1 c \u00b7max ( d\u03c32 n logm , d log2 m ) , choose \u03b42 = 1 c \u00b7 d\u03c3 2 nB logm . Then we have that \u03b4 2 \u2264 min ( 1, \u03c3 2 logm n ) , and hence we can apply corollary 3.1. Also\nC \u2264 B = 1\nc \u00b7\nd\u03c32\n\u03b42n logm\nChoose c is such that this violates the lower bound on C in corrollary 3.1. Thus, we must have R \u2265 d\u03b42/40 \u2265 \u2126 ( d2\u03c32\nnB logm\n)\n. On the other hand, if B \u2264 1 c \u00b7 max\n(\nd\u03c32\nn logm , d log2 m\n)\n, choose \u03b42 =\nd\u03c32\nnmax\n(\nd\u03c32\nn log m , d\nlog2 m\n) logm . Again \u03b42 \u2264 min\n(\n1, \u03c3 2 logm n\n)\nand\nC \u2264 B \u2264 1\nc \u00b7max\n(\nd\u03c32\nn logm ,\nd\nlog2 m\n)\n= 1\nc \u00b7\nd\u03c32\n\u03b42n logm\nHence R \u2265 d\u03b42/40 \u2265 \u2126 ( min { d\u03c32 n logm , d }) . Combining the two cases, we get\nR \u2265 \u2126\n(\nmin\n{\nd2\u03c32\nnB logm ,\nd\u03c32\nn logm ,d\n})"}], "references": [{"title": "How to compress interactive communication", "author": ["Boaz Barak", "Mark Braverman", "Xi Chen", "Anup Rao"], "venue": "SIAM J. Comput.,", "citeRegEx": "Barak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2013}, {"title": "Distributed learning, communication complexity and privacy", "author": ["Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour"], "venue": "In COLT,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "A tight bound for set disjointness in the message-passing model", "author": ["Mark Braverman", "Faith Ellen", "Rotem Oshman", "Toniann Pitassi", "Vinod Vaikuntanathan"], "venue": "In FOCS,", "citeRegEx": "Braverman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2013}, {"title": "Information equals amortized communication", "author": ["Mark Braverman", "Anup Rao"], "venue": "In FOCS, pages 748\u2013757,", "citeRegEx": "Braverman and Rao.,? \\Q2011\\E", "shortCiteRegEx": "Braverman and Rao.", "year": 2011}, {"title": "An information statistics approach to data stream and communication complexity", "author": ["Ziv Bar-Yossef", "T.S. Jayram", "Ravi Kumar", "D. Sivakumar"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Bar.Yossef et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bar.Yossef et al\\.", "year": 2004}, {"title": "Informational complexity and the direct sum problem for simultaneous message complexity", "author": ["Amit Chakrabarti", "Yaoyun Shi", "Anthony Wirth", "Andrew Chi-Chih Yao"], "venue": "In FOCS,", "citeRegEx": "Chakrabarti et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 2001}, {"title": "Efficient protocols for distributed classification and optimization", "author": ["Hal Daum\u00e9 III", "Jeff M. Phillips", "Avishek Saha", "Suresh Venkatasubramanian"], "venue": "In ALT,", "citeRegEx": "III et al\\.,? \\Q2012\\E", "shortCiteRegEx": "III et al\\.", "year": 2012}, {"title": "Protocols for learning classifiers on distributed data", "author": ["Hal Daum\u00e9 III", "Jeff M. Phillips", "Avishek Saha", "Suresh Venkatasubramanian"], "venue": "In AISTATS,", "citeRegEx": "III et al\\.,? \\Q2012\\E", "shortCiteRegEx": "III et al\\.", "year": 2012}, {"title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints", "author": ["Yuchen Zhang", "John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean ~ \u03b8 of an unknown d dimensional normal distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean ~ \u03b8 at the optimal minimax rate while communicating as few bits as possible. We show that in this simple setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually.", "creator": "LaTeX with hyperref package"}}}