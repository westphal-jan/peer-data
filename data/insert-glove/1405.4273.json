{"id": "1405.4273", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2014", "title": "Compositional Morphology for Word Representations and Language Modelling", "abstract": "peace-building This paper presents benchwarmer a scalable method cnooc for oudea integrating compositional impresarios morphological representations into sfm a vector - bulent based pumila probabilistic 89.70 language model. sabeh Our cancelation approach ndirangu is labine evaluated in 53-page the 124.75 context eccrine of 4:1 log - lungo bilinear language iridaceae models, mcfoy rendered bradburn suitably then-recent efficient sna for fursey implementation maselli inside a machine pogrebin translation decoder by pissing factoring gokhan the trisong vocabulary. We perform both tdcs intrinsic krasnoye and extrinsic evaluations, presenting rehovot results on a huwei range of languages m\u00e1rcio which demonstrate that hgvs our hermagoras model farella learns morphological representations tsingy that both stanleyville perform diabo well on belu word adjustment similarity tasks trengganu and lead maldive to roomful substantial 10.72 reductions in khanpur perplexity. byob When mind-boggling used for translation into morphologically rich languages with large vocabularies, our sportingly models obtain aknowledged improvements jianguo of up diokno to magnitude-5 1. export-import 2 zuluaga BLEU 1,101 points relative 5.90 to girlhood a baseline system resales using back - ilicifolia off 52-49 n - technophobe gram models.", "histories": [["v1", "Fri, 16 May 2014 19:08:14 GMT  (756kb,D)", "http://arxiv.org/abs/1405.4273v1", "Proceedings of the 31st International Conference on Machine Learning (ICML)"]], "COMMENTS": "Proceedings of the 31st International Conference on Machine Learning (ICML)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan a botha", "phil blunsom"], "accepted": true, "id": "1405.4273"}, "pdf": {"name": "1405.4273.pdf", "metadata": {"source": "META", "title": "Compositional Morphology for Word Representations and Language Modelling", "authors": ["Jan A. Botha", "Phil Blunsom"], "emails": ["JAN.BOTHA@CS.OX.AC.UK", "PHIL.BLUNSOM@CS.OX.AC.UK"], "sections": [{"heading": "1 Introduction", "text": "The proliferation of word forms in morphologically rich languages presents challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition. Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted. Such models suffer from data sparsity arising from morphological processes and lack a coherent method of assigning probabilities or representations to unseen word forms.\nThis work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors. Such word representations have been found to capture some morphological regularity (Mikolov et al., 2013b), but we contend that there is a case for building a priori morphological awareness into\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nthe language models\u2019 inductive bias. Conversely, compositional vector-space modelling has recently been applied to morphology to good effect (Lazaridou et al., 2013; Luong et al., 2013), but lacked the probabilistic basis necessary for use with a machine translation decoder.\nThe method we propose strikes a balance between probabilistic language modelling and morphology-based representation learning. Word vectors are composed as a linear function of arbitrary sub-elements of the word, e.g. surface form, stem, affixes, or other latent information. The effect is to tie together the representations of morphologically related words, directly combating data sparsity. This is executed in the context of a log-bilinear (LBL) LM (Mnih & Hinton, 2007), which is sped up sufficiently by the use of word classing so that we can integrate the model into an open source machine translation decoder1 and evaluate its impact on translation into 6 languages, including the morphologically complex Czech, German and Russian.\nIn word similarity rating tasks, our morpheme vectors help improve correlation with human ratings in multiple languages. Fine-grained analysis is used to determine the origin of our perplexity reductions, while scaling experiments demonstrate tractability on vocabularies of 900k types using 100m+ tokens."}, {"heading": "2 Additive Word Representations", "text": "A generic CSLM associates with each word type v in the vocabulary V a d-dimensional feature vector rv \u2208 Rd. Regularities among words are captured in an opaque way through the interaction of these feature values and a set of transformation weights. This leverages linguistic intuitions only in an extremely rudimentary way, in contrast to handengineered linguistic features that target very specific phenomena, as often used in supervised-learning settings.\nWe seek a compromise that retains the unsupervised nature of CSLM feature vectors, but also incorporates a priori linguistic knowledge in a flexible and efficient manner. In particular, morphologically related words should share statistical strength in spite of differences in surface form.\n1Our source code for language model training and integration into cdec is available from http://bothameister.github.io\nar X\niv :1\n40 5.\n42 73\nv1 [\ncs .C\nL ]\n1 6\nM ay\n2 01\n4\nTo achieve this, we define a mapping \u00b5 : V 7\u2192 F+ of a surface word into a variable-length sequence of factors, i.e. \u00b5(v) = (f1, . . . , fK), where v \u2208 V and fi \u2208 F . Each factor f has an associated factor feature vector rf \u2208 Rd. We thereby factorise a word into its surface morphemes, although the approach could also incorporate other information, e.g. lemma, part of speech.\nThe vector representation r\u0303v of a word v is computed as a function \u03c9\u00b5(v) of its factor vectors. We use addition as composition function: r\u0303v = \u03c9\u00b5(v) = \u2211 f\u2208\u00b5(v) rf . The vectors of morphologically related words become linked through shared factor vectors (notation: \u2212\u2212\u2212\u2192 word, \u2212\u2212\u2212\u2192 factor ),\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 imperfection = \u2212\u2192 im + \u2212\u2212\u2212\u2212\u2192 perfect + \u2212\u2192 ion\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nperfectly = \u2212\u2212\u2212\u2212\u2192 perfect + \u2212\u2192 ly .\nFurthermore, representations for out-of-vocabulary (OOV) words can be constructed using their available morpheme vectors.\nWe include the surface form of a word as a factor itself. This accounts for noncompositional constructions ( \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 greenhouse = \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 greenhouse + \u2212\u2212\u2212\u2192 green + \u2212\u2212\u2212\u2192 house), and makes the approach more robust to noisy morphological segmentation. This strategy also overcomes the order-invariance of additive composition ( \u2212\u2212\u2212\u2212\u2212\u2212\u2192 hangover 6= \u2212\u2212\u2212\u2212\u2212\u2212\u2192overhang).\nThe number of factors per word is free to vary over the vocabulary, making the approach applicable across the spectrum of more fusional languages (e.g. Czech, Russian) to more agglutinative languages (e.g. Turkish). This is in contrast to factored language models (Alexandrescu & Kirchhoff, 2006), which assume a fixed number of factors per word. Their method of concatenating factor vectors to obtain a single representation vector for a word can be seen as enforcing a partition on the feature space. Our method of addition avoids such a partitioning and better reflects the absence of a strong intuition about what an appropriate partitioning might be. A limitation of our method compared to theirs is that the deterministic mapping \u00b5 currently enforces a single factorisation per word type, which sacrifices information obtainable from context-disambiguated morphological analyses.\nOur additive composition function can be regarded as an instantiation of the weighted addition strategy that performed well in a distributional compositional approach to derivational morphology (Lazaridou et al., 2013). Unlike the recursive neural-network method of Luong et al. (2013), we do not impose a single tree structure over a word, which would ignore the ambiguity inherent in words like un[[lock]able] vs. [un[lock]]able. In contrast to these two previous approaches to morphological modelling, our additive representations are readily implementable in a probabilistic language model suitable for use in a decoder."}, {"heading": "3 Log-Bilinear Language Models", "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models. The probability of a sentence w is decomposed over its words, each conditioned on the n\u20131 preceding words: P (w) \u2248\u220fi P (wi|wi\u22121i\u2212n+1). These distributions are modelled by a smooth scoring function \u03bd(\u00b7) over vector representations of words. In contrast, discrete n-gram models are estimated by smoothing and backing off over empirical distributions (Kneser & Ney, 1995; Chen & Goodman, 1998).\nThe LBL predicts the vector p for the next word as a function of the context vectors qj \u2208 Rd of the preceding words,\np = n\u22121\u2211 j=1 qjCj , (1)\nwhere Cj \u2208 Rd\u00d7d are position-specific transformations. \u03bd(w) expresses how well the observed word w fits that prediction and is defined as \u03bd(w) = p \u00b7 rw + bw, where bw is a bias term encoding the prior probability of a word type. Softmax then yields the word probability as\nP ( wi|wi\u22121i\u2212n+1 ) =\nexp (\u03bd(wi))\u2211 v\u2208V exp (\u03bd(v)) . (2)\nThis model is subsequently denoted as LBL with parameters \u0398LBL = (Cj , Q,R,b), where Q,R \u2208 R|V|\u00d7d contain the word representation vectors as rows, and b \u2208 R|V|. Q and R imply that separate representations are used for conditioning and output."}, {"heading": "3.1 Additive Log-Bilinear Model", "text": "We introduce a variant of the LBL that makes use of additive representations (\u00a72) by associating the composed word vectors r\u0303 and q\u0303j with the target and context words, respectively. The representation matrices Q(f), R(f) \u2208 R|F|\u00d7d thus contain a vector for each factor type. This model is designated LBL++ and has parameters \u0398LBL++ = ( Cj , Q (f), R(f),b ) . Words sharing factors are tied together, which is expected to improve performance on rare word forms.\nRepresenting the mapping \u00b5 with a sparse transformation matrix M \u2208 ZV\u00d7|F|+ , where a row vector mv has some non-zero elements to select factor vectors, establishes the relation between word and factor representation matrices as R = MR(f) and Q = MQ(f). In practice, we exploit this for test-time efficiency\u2014word vectors are compiled offline so that the computational cost of LBL++ probability lookups is the same as for the LBL.\nWe consider two obvious variations of the LBL++ to evaluate the extent to which interactions between context and\ntarget factors affect the model: LBL+o only factorises output words and retains simple word vectors for the context (i.e. Q \u2261 Q(f)), while LBL+c does the reverse, only factorising context words.2 Both reduce to the LBL when setting \u00b5 to be the identity function, such that V \u2261 F . The factorisation permits an approach to unknown context words that is less harsh than the standard method of replacing them with a global unknown symbol\u2014instead, a vector can be constructed from the known factors of the word (e.g. the observed stem of an unobserved inflected form). A similar scheme can be used for scoring unknown target words, but requires changing the event space of the probabilistic model. We use this vocabulary stretching capability in our word similarity experiments, but leave the extensions for test-time language model predictions as future work."}, {"heading": "3.2 Class-based Model Decomposition", "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary. Our approach to reducing the computational cost of normalisation is to use a class-based decomposition of the probabilistic model (Goodman, 2001; Mikolov et al., 2011). Using Brownclustering (Brown et al., 1992),3 we partition the vocabulary into |C| classes, denoting as Cc the set of vocabulary items in class c, such that V = C1 \u222a \u00b7 \u00b7 \u00b7 \u222a C|C|. In this model, the probability of a word conditioned on the history h of n\u2212 1 preceding words is decomposed as\nP (w|h) = P (c|h)P (w|h, c). (3)\nThis class-based model, CLBL, extends over the LBL by associating a representation vector sc and bias parameter tc to each class c, such that \u0398CLBL = (Cj , Q,R, S,b, t). The same prediction vector p is used to compute both class\n2The +c, +o and ++ naming suffixes denote these same distinctions when used with the CLBL model introduced later.\n3In preliminary experiments, Brown clusters gave better perplexities than frequency-binning (Mikolov et al., 2011).\nscore \u03c4(c) = p \u00b7 sc + tc and word score \u03bd(w), which are normalised separately:\nP (c|h) = exp (\u03c4(c))\u2211|C| c\u2032=1 exp (\u03c4(c \u2032)) (4)\nP (w|h, c) = exp (\u03bd(w))\u2211 v\u2032\u2208Cc exp (\u03bd(v \u2032)) . (5)\nWe favour this flat vocabulary partitioning for its computational adequacy, simplicity and robustness. Computational adequacy is obtained by using |C| \u2248 |V|0.5, thereby reducing the O(|V|) normalisation operation of the LBL to two O(|V|0.5) operations in the CLBL. Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al., 2011). We expect these approaches could have adverse effects in the rich morphology setting, where much of the vocabulary is in the long tail of the word distribution."}, {"heading": "3.3 Training & Initialisation", "text": "Model parameters \u0398 are estimated by optimising an L2regularised log likelihood objective. Training the CLBL and its additive variants directly against this objective is fast because normalisation of model scores, which is required in computing gradients, is over a small number of events.\nFor the classless LBLs we use noise-contrastive estimation (NCE) (Gutmann & Hyva\u0308rinen, 2012; Mnih & Teh, 2012) to avoid normalisation during training. This leaves the expensive test-time normalisation of LBLs unchanged, precluding their usage during decoding.\nBias terms b (resp. t) are initialised to the log unigram probabilities of words (resp. classes) in the training corpus, with Laplace smoothing, while all other parameters are initialised randomly according to sharp, zero-mean Gaussians. Representations are thus learnt from scratch and not based on publicly available embeddings, meaning our approach can easily be applied to many languages.\nOptimisation is performed by stochastic gradient descent with updates after each mini-batch of L training examples. We apply AdaGrad (Duchi et al., 2011) and tune the stepsize \u03be on development data.4 We halt training once the perplexity on the development data starts to increase."}, {"heading": "4 Experiments", "text": "The overarching aim of our evaluation is to investigate the effect of using the proposed additive representations across languages with a range of morphological complexity.\n4L=10k\u201340k, \u03be=0.05\u20130.08, dependent on |V| and data size.\nOur intrinsic language model evaluation has two parts. We first perform a model selection experiment on small data to consider the relative merits of using additive representations for context words, target words, or both, and to validate the use of the class-based decomposition.\nThen we consider class-based additive models trained on tens of millions of tokens and large vocabularies. These larger language models are applied in two extrinsic tasks: i) a word-similarity rating experiment on multiple languages, aiming to gauge the quality of the induced word and morpheme representation vectors; ii) a machine translation experiment, where we are specifically interested in testing the impact of an LBL LM feature when translating into morphologically rich languages."}, {"heading": "4.1 Data & Methods", "text": "We make use of data from the 2013 ACL Workshop on Machine Translation.5 We first describe data used for translation experiments, since the monolingual datasets used for language model training were derived from that. The language pairs are English\u2192{German, French, Spanish, Russian} and English\u2194Czech. Our parallel data comprised the Europarl-v7 and news-commentary corpora, except for English\u2013Russian where we used news-commentary and the Yandex parallel corpus.6 Pre-processing involved lowercasing, tokenising and filtering to exclude sentences of more than 80 tokens or substantially different lengths.\n4-gram language models were trained on the target data in two batches: DATA-1M consists of the first million tokens only, while DATA-MAIN is the full target-side data. Statistics are given in Table 1. newstest2011 was used as development data7 for tuning language model hyperparameters, while intrinsic LM evaluation was done on newstest2012. As metric, we use model perplexity (PPL) exp(\u2212 1N \u2211N i=1 lnP (wi)), where N is the number of test tokens. In addition to contrasting the LBL variants, we also use modified Kneser-Ney n-gram models (MKNs) (Chen & Goodman, 1998) as baselines.\n5http://www.statmt.org/wmt13/translation-task.html 6https://translate.yandex.ru/corpus?lang=en 7For Russian, some training data was held out for tuning.\nLanguage Model Vocabularies. Additive representations that link morphologically related words specifically aim to improve modelling of the long tail of the lexicon, so we do not want to prune away all rare words, as is common practice in language modelling and word embedding learning. We define a singleton pruning rate \u03ba, and randomly replace that fraction of words occurring only once in the training data with a global UNK symbol. \u03ba = 1 would imply a unigram count cut-off threshold of 1. Instead, we use low pruning rates8 and thus model large vocabularies.9\nWord Factorisation \u00b5. We obtain labelled morphological segmentations from the unsupervised segmentor Morfessor Cat-MAP (Creutz & Lagus, 2007). The mapping \u00b5 of a word is taken as its surface form and the morphemes identified by Morfessor. Keeping the morpheme labels allows the model to learn separate vectors for, say, instem the preposition and inprefix occurring as in\u00b7appropriate. By not post-processing segmentations in a more sophisticated way, we keep the overall method more language independent."}, {"heading": "4.2 Intrinsic Language Model Evaluation", "text": "Results on DATA-1M. The use of morphology-based, additive representations for both context and output words (models++) yielded perplexity reductions on all 6 languages when using 1m training tokens. Furthermore, these double-additive models consistently outperform the ones that factorise only context (+c) or only output (+o) words, indicating that context and output contribute complementary information and supporting our hypothesis that is it beneficial to model morphological dependencies across words. The results are summarised in Figure 2.\nFor lack of space we do not present numbers for individual languages, but report that the impact of CLBL++ varies by\n8 DATA-1M: \u03ba = 0.2; DATA-MAIN: \u03ba = 0.05 9 We also mapped digits to 0, and cleaned the Russian data by\nreplacing tokens having <80% Cyrillic characters with UNK.\nlanguage, correlating with vocabulary size: Russian benefited most, followed by Czech and German. Even on English, often regarded as having simple morphology, the relative improvement is 4%.\nThe relative merits of the +c and +o schemes depend on which model is used as starting point. With LBL, the output-additive scheme (LBL+o) gives larger improvements than the context-additive scheme (LBL+c). The reverse is true for CLBL, indicating the class decomposition dampens the effectiveness of using morphological information in output words.\nThe use of classes increases perplexity slightly compared to the LBLs, but this is in exchange for much faster computation of language model probabilities, allowing the CLBLs to be used in a machine translation decoder (\u00a74.4).\nResults on DATA-MAIN. Based on the outcomes of the small-scale evaluation, we focus our main language model evaluation on the additive class-based model CLBL++ in comparison to CLBL and MKN baselines, using the larger training dataset, with vocabularies of up to 500k types.\nThe overall trend that morphology-based additive representations yield lower perplexity carries over to this larger data setting, again with the biggest impact being on Czech and Russian (Table 2, top). Improvements are in the 2%\u20136% range, slightly lower than the corresponding differences on the small data.\nOur hypothesis is that the much of the improvement is due to the additive representations being especially beneficial for modelling rare words. We test this by repeating the experiment under the condition where all word types occurring only once are excluded from the vocabulary (\u03ba=1). If the additive representations were not beneficial to rare\nwords, the outcome should remain the same. Instead, we find the relative improvements become a lot smaller (Table 2, bottom) than when only excluding some singletons (\u03ba=0.05), which supports that hypothesis.\nAnalysis. Model perplexity on a whole dataset is a convenient summary of its intrinsic performance, but such a global view does not give much insight into how one model outperforms another. We now partition the test data into subsets of interest and measure PPL over these subsets.\nWe first partition on token frequency, as computed on the training data. Figure 3 provides further evidence that the additive models have most impact on rare words generally, and not only on singletons. Czech, German and Russian see relative PPL reductions of 8%\u201321% for words occurring fewer than 100 times in the training data. Reductions become negligible for the high-frequency tokens. These tend to be punctuation and closed-class words, where any putative relevance of morphology is overwhelmed by the fact that the predictive uncertainty is very low to begin with (absolute PPL<10 for the highest frequency subset). For the morphologically simpler Spanish case, PPL reductions are generally smaller across frequency scales.\nWe also break down PPL reductions by part of speech tags, focusing on German. We used the decision tree-based tagger of Schmid & Laws (2008). Aside from unseen tokens, the biggest improvements are on nouns and adjectives (Figure 4), suggesting our segmentation-based representations help abstract over German\u2019s productive compounding.\nGerman noun phrases require agreement in gender, case and number, which are marked overtly with fusional morphemes, and we see large gains on such test n-grams: 15% improvement on adjective-noun sequences, and 21% when considering the more specific case of adjective-adjectivenoun sequences. An example of the latter kind is der ehemalig\u00b7e sozial\u00b7ist\u00b7isch\u00b7e bildung\u00b7s\u00b7minister (\u2018the former socialist minister of education\u2019), where the morphological agreement surfaces in the repeated e-suffix.\nWe conducted a final scaling experiment on Czech by training models on increasing amounts of data from the monolingual news corpora. Improvements over the MKN baseline decrease, but remain substantial at 14% for the largest setting when allowing the vocabulary to grow with the data. Maintaining a constant advantage over MKN requires also increasing the dimensionality d of representations (Mikolov et al., 2013a), but this was outside the scope of our experiment. Although gains from the additive representations over the CLBL diminish down to 2%\u20133% at the scale of 128m training tokens (Figure 5), these results demonstrate the tractability of our approach on very large vocabularies of nearly 1m types."}, {"heading": "4.3 Task 1: Word Similarity Rating", "text": "In the previous section, we established the positive role that morphological awareness played in building continuousspace language models that better predict unseen text. Here we focus on the quality of the word representations learnt in the process. We evaluate on a standard word similarity rating task, where one measures the correlation between cosine-similarity scores for pairs of word vectors and a set of human similarity ratings. An important aspect of our evaluation is to measure performance on multiple languages using a single unsupervised, model-based approach.\nMorpheme vectors from the CLBL++ enable handling OOV test words in a more nuanced way than using the global unknown word vector. In general, we compose a vector u\u0303v = [q\u0303v; r\u0303v] for a word v according to a post hoc word\nTo see whether the morphological representations improve the quality of vectors for known words, we also report the correlations obtained when using the CLBL++ word vectors directly, resorting to u\u0303UNK for all OOV words v /\u2208 V (denoted \u201c\u2212compose\u201d in the results). This is also the strategy that the baseline CLBL model is forced to follow for OOVs.\nWe evaluate first using the English rare-word dataset (RW) created by Luong et al. (2013). Its 2034 word pairs contain more morphological complexity than other well-established word similarity datasets, e.g. crudeness\u2014 impoliteness. We compare against their context-sensitive morphological recursive neural network (csmRNN), using Spearman\u2019s rank correlation coefficient, \u03c1. Table 3 shows our model obtaining a \u03c1-value slightly below the best csmRNN result, but outperforming the csmRNN that used an alternative set of embeddings for initialisation.\nThis is a strong result given that our vectors come from a simple linear probabilistic model that is also suitable for integration directly into a decoder for translation (\u00a74.4) or speech recognition, which is not the case for csmRNNs. Moreover, the csmRNNs were initialised with high-quality, publicly available word embeddings trained over weeks on much larger corpora of 630\u2013990m words (Collobert & Weston, 2008; Huang et al., 2012), in contrast to ours that are trained from scratch on much less data. This renders our method directly applicable to languages which may not yet have those resources.\nRelative to the CLBL baseline, our method performs well on\ndatasets across four languages. For the English RW, which was designed with morphology in mind, the gain is 64%. But also on the standard English WS353 dataset (Finkelstein et al., 2002), we get a 26% better correlation with the human ratings. On German, the CLBL++ obtains correlations up to three times stronger than the baseline, and 39% better for French (Table 4).\nA visualisation of the English morpheme vectors (Figure 6) suggests the model captured non-trivial morphological regularities: noun suffixes relating to persons (writer, humanists) lie close together, while being separated according to number; negation prefixes share a region (un-, in-, mis-, dis-); and relational prefixes are grouped (surpa-, super-, multi-, intra-), with a potential explanation for their separation from inter- being that the latter is more strongly bound up in lexicalisations (international, intersection)."}, {"heading": "4.4 Task 2: Machine Translation", "text": "The final aspect of our evaluation focuses on the integration of class-decomposed log-bilinear models into a machine translation system. To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages. We consider 5 language pairs, translating from English into Czech, German, Russian, Spanish and French.\nAside from the choice of language pairs, this evaluation diverges from Vaswani et al. (2013) by using normalised probabilities, a process made tractable by the class-based decomposition and caching of context-specific normaliser terms. Vaswani et al. (2013) relied on unnormalised model scores for efficiency, but do not report on the performance impact of this assumption. In our preliminary experiments, there was high variance in the performance of unnormalised models. They are difficult to reason about as a feature function that must help the translation model discriminate between alternative hypotheses.\nWe use cdec (Dyer et al., 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007). Our baseline system uses a standard set of features in a log-linear translation model. This includes a baseline 4-gram MKN language model, trained with SRILM (Stolcke, 2002) and queried efficiently using KenLM (Heafield, 2011). The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012.\nTable 5 summarises our translation results. Inclusion of the CLBL++ language model feature outperforms the MKNonly baseline systems by 1.2 BLEU points for translation into Russian, and by 1 point into Czech and Spanish. The EN\u2192DE system benefits least from the additional CSLM feature, despite the perplexity reductions achieved in the intrinsic evaluation. In light of German\u2019s productive compounding, it is conceivable that the bilingual coverage of\n11Our source code for using CLBL/CLBL++ with cdec is released at http://bothameister.github.io.\nthat system is more of a limitation than the performance of the language models.\nOn the other languages, the CLBL adds 0.5 to 1 BLEU points over the baseline, whereas additional improvement from the additive representations lies within MERT variance except for EN\u2192CS. The impact of our morphology-aware language model is limited by the translation system\u2019s inability to generate unseen inflections. A future task is thus to combine it with a system that can do so (Chahuneau et al., 2013)."}, {"heading": "5 Related Work", "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors. Alexandrescu & Kirchhoff (2006) demonstrated how factorising the representations of context-words can help deal with out-of-vocabulary words, but they did not evaluate the effect of factorising output words and did not conduct an extrinsic evaluation.\nA variety of strategies have been explored for bringing CSLMs to bear on machine translation. Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012). For efficiency, this line of work relied heavily on small \u201cshortlists\u201d of common words, by-passing the CSLM and using a back-off n-gram model for the remainder of the vocabulary. Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013).\nRecent work has moved beyond monolingual vector-space modelling, incorporating phrase similarity ratings based on bilingual word embeddings as a translation model feature (Zou et al., 2013), or formulating translation purely in terms of continuous-space models (Kalchbrenner & Blunsom, 2013). Accounting for linguistically derived infor-\nmation such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words. Our contribution is to create morphological awareness in a probabilistic language model."}, {"heading": "6 Conclusion", "text": "We introduced a method for integrating morphology into probabilistic continuous-space language models. Our method has the flexibility to be used for morphologically rich languages (MRLs) across a range of linguistic typologies. Our empirical evaluation focused on multiple MRLs and different tasks. The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech. By demonstrating that the class decomposition enables full integration of a normalised CSLM into a decoder, we open up many other possibilities in this active modelling space."}], "references": [{"title": "Factored Neural Language Models", "author": ["A. Alexandrescu", "K. Kirchhoff"], "venue": "In Proc. HLT-NAACL: short papers. ACL,", "citeRegEx": "Alexandrescu and Kirchhoff,? \\Q2006\\E", "shortCiteRegEx": "Alexandrescu and Kirchhoff", "year": 2006}, {"title": "Factored Language Models and Generalized Parallel Backoff", "author": ["J.A. Bilmes", "K. Kirchhoff"], "venue": "In Proc. NAACL-HLT: short papers. ACL,", "citeRegEx": "Bilmes and Kirchhoff,? \\Q2003\\E", "shortCiteRegEx": "Bilmes and Kirchhoff", "year": 2003}, {"title": "Class-Based n-gram Models of Natural Language", "author": ["P.F. Brown", "P.V. DeSouza", "R.L. Mercer", "V.J. Della Pietra", "J.C. Lai"], "venue": "Comp. Ling.,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Translating into Morphologically Rich Languages with Synthetic Phrases", "author": ["V. Chahuneau", "E. Schlinger", "N.A. Smith", "C. Dyer"], "venue": "In Proc. EMNLP,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "An Empirical Study of Smoothing Techniques for Language Modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Technical report,", "citeRegEx": "Chen and Goodman,? \\Q1998\\E", "shortCiteRegEx": "Chen and Goodman", "year": 1998}, {"title": "Hierarchical Phrase-Based Translation", "author": ["D. Chiang"], "venue": "Comp. Ling.,", "citeRegEx": "Chiang,? \\Q2007\\E", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proc. ICML. ACM,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Unsupervised Models for Morpheme Segmentation and Morphology Learning", "author": ["M. Creutz", "K. Lagus"], "venue": "ACM Trans. on Speech and Language Processing,", "citeRegEx": "Creutz and Lagus,? \\Q2007\\E", "shortCiteRegEx": "Creutz and Lagus", "year": 2007}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models", "author": ["C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik"], "venue": "In Proc. ACL: demonstration session,", "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "In Proc. NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Trans. on Information Systems,", "citeRegEx": "Finkelstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Classes for Fast Maximum Entropy Training", "author": ["J. Goodman"], "venue": "In Proc. ICASSP, pp. 561\u2013564", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Using the Structure of a Conceptual Network in Computing Semantic Relatedness", "author": ["I. Gurevych"], "venue": "In Proc. IJCNLP,", "citeRegEx": "Gurevych,? \\Q2005\\E", "shortCiteRegEx": "Gurevych", "year": 2005}, {"title": "Noise-Contrastive Estimation of Unnormalized Statistical Models", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "with Applications to Natural Image Statistics. JMLR,", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? \\Q2012\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2012}, {"title": "Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge", "author": ["S. Hassan", "R. Mihalcea"], "venue": "In Proc. EMNLP,", "citeRegEx": "Hassan and Mihalcea,? \\Q2009\\E", "shortCiteRegEx": "Hassan and Mihalcea", "year": 2009}, {"title": "KenLM: Faster and Smaller Language Model Queries", "author": ["K. Heafield"], "venue": "In Proc. Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield,? \\Q2011\\E", "shortCiteRegEx": "Heafield", "year": 2011}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proc. ACL,", "citeRegEx": "Hermann and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2013}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Proc. ACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Comparison of Semantic Similarity for Different Languages Using the Google N-gram Corpus and Second- Order Co-occurrence Measures", "author": ["C. Joubarne", "D. Inkpen"], "venue": "In Proc. Canadian Conference on Advances in AI,", "citeRegEx": "Joubarne and Inkpen,? \\Q2011\\E", "shortCiteRegEx": "Joubarne and Inkpen", "year": 2011}, {"title": "Recurrent Continuous Translation Models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Improved Backing-off for m-gram Language Modelling", "author": ["R. Kneser", "H. Ney"], "venue": "In Proc. ICASSP,", "citeRegEx": "Kneser and Ney,? \\Q1995\\E", "shortCiteRegEx": "Kneser and Ney", "year": 1995}, {"title": "Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics", "author": ["A. Lazaridou", "M. Marelli", "R. Zamparelli", "M. Baroni"], "venue": "In Proc. ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Structured Output Layer Neural Network Language Model", "author": ["Le", "H.-S", "I. Oparin", "A. Allauzen", "Gauvain", "J.-L", "F. Yvon"], "venue": "In Proc. ICASSP,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Luong", "M.-T", "R. Socher", "C.D. Manning"], "venue": "In Proc. of CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur"], "venue": "In Proc. Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of Recurrent Neural Network Language Model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur"], "venue": "In Proc. ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proc. ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "Yih", "W.-t", "G. Zweig"], "venue": "In Proc. HLTNAACL. ACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three New Graphical Models for Statistical Language Modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "In Proc. ICML,", "citeRegEx": "Mnih and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["A. Mnih", "G. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Mnih and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2008}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "In Proc. ICML,", "citeRegEx": "Mnih and Teh,? \\Q2012\\E", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Minimum Error Rate Training in Statistical Machine Translation", "author": ["F.J. Och"], "venue": "In Proc. ACL, pp", "citeRegEx": "Och,? \\Q2003\\E", "shortCiteRegEx": "Och", "year": 2003}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein and Goodenough,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough", "year": 1965}, {"title": "Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging", "author": ["H. Schmid", "F. Laws"], "venue": "In Proc. COLING,", "citeRegEx": "Schmid and Laws,? \\Q2008\\E", "shortCiteRegEx": "Schmid and Laws", "year": 2008}, {"title": "Efficient Training of Large Neural Networks for Language Modeling", "author": ["H. Schwenk"], "venue": "In Proc. IEEE Joint Conference on Neural Networks,", "citeRegEx": "Schwenk,? \\Q2004\\E", "shortCiteRegEx": "Schwenk", "year": 2004}, {"title": "Large and Diverse Language Models for Statistical Machine Translation", "author": ["H. Schwenk", "P. Koehn"], "venue": "In Proc. IJCNLP,", "citeRegEx": "Schwenk and Koehn,? \\Q2008\\E", "shortCiteRegEx": "Schwenk and Koehn", "year": 2008}, {"title": "Continuous Space Language Models for Statistical Machine Translation", "author": ["H. Schwenk", "D. Dchelotte", "Gauvain", "J.-L"], "venue": "In Proc. COLING/ACL,", "citeRegEx": "Schwenk et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2006}, {"title": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation", "author": ["H. Schwenk", "A. Rousseau", "M. Attik"], "venue": "In In Proc. NAACL-HLT Workshop: On the Future of Language Modeling for HLT,", "citeRegEx": "Schwenk et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2012}, {"title": "SRILM \u2013 An extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proc. ICSLP, pp", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Visualizing Data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "JMLR, 9:2579\u20132605,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Decoding with Large-Scale Neural Language Models Improves Translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "In Proc. EMNLP,", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Automatically creating datasets for measures of semantic relatedness", "author": ["T. Zesch", "I. Gurevych"], "venue": "In Proc. Workshop on Linguistic Distances,", "citeRegEx": "Zesch and Gurevych,? \\Q2006\\E", "shortCiteRegEx": "Zesch and Gurevych", "year": 2006}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 37, "context": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted.", "startOffset": 102, "endOffset": 167}, {"referenceID": 25, "context": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted.", "startOffset": 102, "endOffset": 167}, {"referenceID": 22, "context": "Conversely, compositional vector-space modelling has recently been applied to morphology to good effect (Lazaridou et al., 2013; Luong et al., 2013), but lacked the probabilistic basis necessary for use with a machine translation decoder.", "startOffset": 104, "endOffset": 148}, {"referenceID": 24, "context": "Conversely, compositional vector-space modelling has recently been applied to morphology to good effect (Lazaridou et al., 2013; Luong et al., 2013), but lacked the probabilistic basis necessary for use with a machine translation decoder.", "startOffset": 104, "endOffset": 148}, {"referenceID": 22, "context": "Our additive composition function can be regarded as an instantiation of the weighted addition strategy that performed well in a distributional compositional approach to derivational morphology (Lazaridou et al., 2013).", "startOffset": 194, "endOffset": 218}, {"referenceID": 22, "context": "Our additive composition function can be regarded as an instantiation of the weighted addition strategy that performed well in a distributional compositional approach to derivational morphology (Lazaridou et al., 2013). Unlike the recursive neural-network method of Luong et al. (2013), we do not impose a single tree structure over a word, which would ignore the ambiguity inherent in words like un[[lock]able] vs.", "startOffset": 195, "endOffset": 286}, {"referenceID": 12, "context": "Our approach to reducing the computational cost of normalisation is to use a class-based decomposition of the probabilistic model (Goodman, 2001; Mikolov et al., 2011).", "startOffset": 130, "endOffset": 167}, {"referenceID": 26, "context": "Our approach to reducing the computational cost of normalisation is to use a class-based decomposition of the probabilistic model (Goodman, 2001; Mikolov et al., 2011).", "startOffset": 130, "endOffset": 167}, {"referenceID": 2, "context": "Using Brownclustering (Brown et al., 1992),3 we partition the vocabulary into |C| classes, denoting as Cc the set of vocabulary items in class c, such that V = C1 \u222a \u00b7 \u00b7 \u00b7 \u222a C|C|.", "startOffset": 22, "endOffset": 42}, {"referenceID": 26, "context": "In preliminary experiments, Brown clusters gave better perplexities than frequency-binning (Mikolov et al., 2011).", "startOffset": 91, "endOffset": 113}, {"referenceID": 35, "context": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al.", "startOffset": 123, "endOffset": 138}, {"referenceID": 23, "context": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al., 2011).", "startOffset": 229, "endOffset": 246}, {"referenceID": 8, "context": "We apply AdaGrad (Duchi et al., 2011) and tune the stepsize \u03be on development data.", "startOffset": 17, "endOffset": 37}, {"referenceID": 24, "context": "We evaluate first using the English rare-word dataset (RW) created by Luong et al. (2013). Its 2034 word pairs contain more morphological complexity than other well-established word similarity datasets, e.", "startOffset": 70, "endOffset": 90}, {"referenceID": 18, "context": "Moreover, the csmRNNs were initialised with high-quality, publicly available word embeddings trained over weeks on much larger corpora of 630\u2013990m words (Collobert & Weston, 2008; Huang et al., 2012), in contrast to ours that are trained from scratch on much less data.", "startOffset": 153, "endOffset": 199}, {"referenceID": 24, "context": "(Luong et al., 2013) Our models", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "But also on the standard English WS353 dataset (Finkelstein et al., 2002), we get a 26% better correlation with the human ratings.", "startOffset": 47, "endOffset": 73}, {"referenceID": 13, "context": "10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).", "startOffset": 46, "endOffset": 62}, {"referenceID": 13, "context": "10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006). Table 4. Word-pair similarity task (multi-language), showing Spearman\u2019s \u03c1\u00d7100 and the number of word pairs in each dataset. As benchmarks, we include the best results from Luong et al. (2013), who relied on more training data and pre-existing embeddings not available in all languages.", "startOffset": 47, "endOffset": 360}, {"referenceID": 41, "context": "Aside from the choice of language pairs, this evaluation diverges from Vaswani et al. (2013) by using normalised probabilities, a process made tractable by the class-based decomposition and caching of context-specific normaliser terms.", "startOffset": 71, "endOffset": 93}, {"referenceID": 41, "context": "Aside from the choice of language pairs, this evaluation diverges from Vaswani et al. (2013) by using normalised probabilities, a process made tractable by the class-based decomposition and caching of context-specific normaliser terms. Vaswani et al. (2013) relied on unnormalised model scores for efficiency, but do not report on the performance impact of this assumption.", "startOffset": 71, "endOffset": 258}, {"referenceID": 9, "context": "We use cdec (Dyer et al., 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007).", "startOffset": 12, "endOffset": 37}, {"referenceID": 5, "context": ", 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007).", "startOffset": 108, "endOffset": 122}, {"referenceID": 39, "context": "This includes a baseline 4-gram MKN language model, trained with SRILM (Stolcke, 2002) and queried efficiently using KenLM (Heafield, 2011).", "startOffset": 71, "endOffset": 86}, {"referenceID": 16, "context": "This includes a baseline 4-gram MKN language model, trained with SRILM (Stolcke, 2002) and queried efficiently using KenLM (Heafield, 2011).", "startOffset": 123, "endOffset": 139}, {"referenceID": 32, "context": "11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012.", "startOffset": 57, "endOffset": 68}, {"referenceID": 3, "context": "A future task is thus to combine it with a system that can do so (Chahuneau et al., 2013).", "startOffset": 65, "endOffset": 89}, {"referenceID": 35, "context": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al.", "startOffset": 63, "endOffset": 78}, {"referenceID": 37, "context": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al.", "startOffset": 123, "endOffset": 168}, {"referenceID": 38, "context": ", 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012).", "startOffset": 76, "endOffset": 98}, {"referenceID": 41, "context": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013).", "startOffset": 122, "endOffset": 144}, {"referenceID": 43, "context": "Recent work has moved beyond monolingual vector-space modelling, incorporating phrase similarity ratings based on bilingual word embeddings as a translation model feature (Zou et al., 2013), or formulating translation purely in terms of continuous-space models (Kalchbrenner & Blunsom, 2013).", "startOffset": 171, "endOffset": 189}, {"referenceID": 24, "context": "Accounting for linguistically derived information such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words.", "startOffset": 69, "endOffset": 113}, {"referenceID": 22, "context": "Accounting for linguistically derived information such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words.", "startOffset": 69, "endOffset": 113}], "year": 2014, "abstractText": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.", "creator": "LaTeX with hyperref package"}}}