{"id": "1405.4423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2014", "title": "A two-step learning approach for solving full and almost full cold start problems in dyadic prediction", "abstract": "mellano Dyadic prediction methods scatology operate 1995-1997 on pairs helplines of objects (drunkard dyads ), aiming to lagorce infer labels wak for etruscan out - of - 0.000 sample rastapopoulos dyads. kaci We folkish consider tamina the 21.38 full grotberg and lussi almost full cold start problem in cross-strait dyadic prediction, jj. a canedy setting v.p. that marostica occurs when both 30/30 objects in an out - 121.4 of - 49.86 sample manufacture-on-demand dyad wilrijk have italeli not been antebi observed during training, or retracement if one bdc of them has huangguoshu been sitoe observed, phindile but aluminij very few npca times. A popular mamed approach for convicting addressing hmr this punters problem is to vrnja\u010dka train technomart a model kindliness that makes avail predictions caba\u00f1a based aeolians on a pairwise feature marzouki representation of the shoddily dyads, azores or, in admire case girly of kernel methods, based hoath on raymont a tensor product pairwise leuven kernel. As zhuang an alternative to kandji such explorer a unsalaried kernel approach, we aldeburgh introduce a 3,048 novel nezhat two - h\u014d step uong learning algorithm sgarbi that shenxin borrows hortencia ideas famosos from boonjong the 800-billion fields of pairwise learning and yadier spectral anagarika filtering. We show levens theoretically rajnagar that mothballing the kirgizstan two - p27 step .2001 method nightgown is keheliya very closely related to ss14 the tensor 75-25 product tr\u00e1nsito kernel epoxi approach, hamwi and sansho experimentally alexopoulos that daimlerbenz it lincroft yields silcock a slightly ghali better predictive yuzhin performance. webcomic Moreover, unlike krautrock existing wales tensor macaco product ostracism kernel protohistory methods, jannie the kayishema two - step method allows small-group closed - form 61-21 solutions wartelle for training keble and njegus parameter 2645 selection via cross - validation estimates c-3 both in the mahaweli full almqvist and almost hanzi full cold outrank start klimentyev settings, making the approach bipole much more efficient and morrocan straightforward eyedea to moravske implement.", "histories": [["v1", "Sat, 17 May 2014 18:20:13 GMT  (70kb,D)", "http://arxiv.org/abs/1405.4423v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tapio pahikkala", "michiel stock", "antti airola", "tero aittokallio", "bernard de baets", "willem waegeman"], "accepted": false, "id": "1405.4423"}, "pdf": {"name": "1405.4423.pdf", "metadata": {"source": "CRF", "title": "A two-step learning approach for solving full and almost full cold start problems in dyadic prediction", "authors": ["Tapio Pahikkala", "Michiel Stock", "Antti Airola", "Tero Aittokallio", "Bernard De Baets", "Willem Waegeman"], "emails": ["firstname.surname@utu.fi", "firstname.surname@UGent.be", "firstname.surname@fimm.fi"], "sections": [{"heading": null, "text": "ar X\niv :1\ntheoretically that the two-step method is very closely related to the tensor product kernel approach, and experimentally that it yields a slightly better predictive performance. Moreover, unlike existing tensor product kernel methods, the two-step method allows closed-form solutions for training and parameter selection via cross-validation estimates both in the full and almost full cold start settings, making the approach much more efficient and straightforward to implement."}, {"heading": "1 A subdivision of dyadic prediction methods", "text": "Many real-world machine learning problems can be naturally represented as pairwise learning or dyadic prediction problems, for which feature representations of two different types of objects (aka a dyad) are jointly used to predict a relationship between those objects. Amongst others, applications of that kind emerge in biology (e.g. predicting protein-RNA interactions), medicine (e.g. design of personalized drugs), chemistry (e.g. prediction of binding between two types of molecules), social network analysis (e.g. link prediction) and recommender systems (e.g. personalized product recommendation).\nFor many dyadic prediction problems it is extremely important to implement appropriate training and evaluation procedures. [28] make in a recent Nature-review on dyadic prediction an important distinction between four main settings. Given t and d as the feature representations of the two types of objects, those four settings can be summarized as follows:\n\u2022 Setting A: Both t and d are observed during training, as parts of separate dyads, but the label of the dyad (t,d) must be predicted.\n\u2022 Setting B: Only t is known during training, while d is not observed in any dyad, and the label of the dyad (t,d) must be predicted.\n\u2022 Setting C: Only d is known during training, while t is not observed in any dyad, and the label of the dyad (t,d) must be predicted.\n\u2022 Setting D: Neither t nor d occur in any training dyad, but the label of the dyad (t,d) must be predicted (referred to as the full cold start problem).\nSetting A is of all four settings by far the most studied setting in the machine literature. Motivated by applications in collaborative filtering and\nlink prediction, matrix factorization and related techniques are often applied to complete partially observed matrices, where missing values represent (t,d) combinations that are not observed during training - see e.g. [15] for a review.\nSettings B and C are very similar, and a variety of machine learning methods can be applied for these settings. From a recommender systems viewpoint, those settings resemble the cold start problem (new user or new item), for which hybrid and content-based filtering techniques are often applied \u2013 see e.g. [1, 10, 20, 34, 38] for a not at all exhaustive list. From a bioinformatics viewpoint, Settings B and C are often analyzed using graphbased methods that take the structure of a biological network into account \u2013 see e.g. [32] for a recent review. When the features of t are negligible or unavailable, while those of d are informative, Setting B can be interpreted as a multi-label classification problem (binary labels), a multivariate regression problems (continuous labels) or a specific multi-task learning problem. Here as well, a large number of applicable methods exists in the literature."}, {"heading": "1.1 The problem setting considered in this article", "text": "Matrix factorization and hybrid filtering strategies are not applicable to Setting D. We will refer to this setting as the full cold start problem, which finds important applications in domains such as bioinformatics and chemistry \u2013 see experiments. Compared to the other three settings, Setting D has received less attention in the literature (with some exceptions, see e.g. [20, 23, 24, 27]), and it will be our main focus in this article. Furthermore, we will also investigate the transition phase between Settings C and D, when t occurs very few times in the training dataset, while d of the dyad (d, t) is only observed in the prediction phase. We refer to this setting as the almost full cold start problem.\nFull and almost full cold start problems can only be solved by considering feature representations of dyads (aka side information in the recommender systems literature). Similar to several existing papers dealing with Setting D, we will consider tensor product feature representations and their kernel duals. Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23]. For classification and regression problems a standard recipe exists of plugging pairwise kernels in support vector machines, kernel ridge regression (KRR), or any other kernel method.\nEfficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced. In our theoretical and experimental analysis we will compare KRR with a tensor product pairwise kernel to the two-step approach that we introduce in this paper."}, {"heading": "1.2 Formulation as a transfer learning problem", "text": "As discussed above, dyadic prediction is closely related to several subfields of machine learning. Further on in this article we decide to adopt a multi-task learning or transfer learning terminology, using d and t to denote the feature representations of instances and tasks, respectively. From this viewpoint, Setting C corresponds to a specific instantiation of a traditional transfer learning scenario, in which the aim is to transfer knowledge obtained from already learned auxiliary tasks to the target task of interest [26]. Stretching the concept of transfer learning even further, in the case of so-called zerodata learning, one arrives at Setting D, which is characterized by no available labeled training data for the target task [16]. If the target task is unknown during the training time, the learning method must be able to generalize to it \u201con the fly\u201d at prediction time. The only available data here is coming from auxiliary training tasks.\nWe present a simple but elegant two-step approach to tackle these settings. First, a KRR model trained on auxiliary tasks is used to predict labels for the related target task. Next, a second model is constructed, using KRR on the target data, augmented by the predictions of the first phase. We show via spectral filtering that this approach is closely related to learning a pairwise model using a tensor product pairwise kernel. However, the two-step approach is much simpler to implement and it allows more heterogeneous transfer learning settings than the ordinary pairwise kernel ridge regression, as well as a more flexible model selection. Furthermore, it allows for a more efficient generalization to new tasks not known during training time, since the model built on auxiliary tasks does not need to be re-trained in such settings. In the experiments we consider three distinct dyadic prediction problems, concerning drug-target, newsgroup document similarity and protein functional similarity predictions. Our results show that the two-step transfer learning approach can be highly beneficial when there is no labeled data at all, or only a small amount of labeled data available for the target task, while in settings where there is a significant amount of labeled data available for the target task a single-task model suffices. In related work,\n[33] have recently proposed a similar two-step approach based on tree-based ensemble methods for biological network inference."}, {"heading": "2 Solving full and almost full cold start prob-", "text": "lems via transfer learning\nAdopting a multi-task learning methodology, the training set is assumed to consist of a set {xh}nh=1 of object-task pairs and a vector y \u2208 Rn of their real-valued labels. We assume that each training input can be represented as x = (d, t), where d \u2208 D and t \u2208 T are the objects and tasks, respectively, and D and T are the corresponding spaces of objects and tasks. Moreover, let D = {di}mi=1 and T = {tj} q j=1 denote, respectively, the sets of distinct objects and tasks encountered in the training set with m = |D| and q = |T |. We say that the training set is complete if it contains every object-task pair with object in D and task in T exactly once. For complete training sets, we introduce a further notation for the matrix of labels Y \u2208 Rm\u00d7q, so that its rows are indexed by the objects in D and the columns by the tasks in T . In full and almost full cold start prediction problems, this matrix will not contain any target task info."}, {"heading": "2.1 Kernel ridge regression with tensor product ker-", "text": "nels\n[4] and several other authors (see [2] and references therein) have extended KRR to involve task correlations via matrix-valued kernels. However, most of the literature concerns kernels for which the tasks are fixed at training time. An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs\n\u0393(x,x) = \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t )\n(1)\nas a product of the data kernel k and task kernel g. Given that K \u2208 Rm\u00d7m and G \u2208 Rq\u00d7q are the kernel matrices for the data points and tasks, respectively, the kernel matrix for the object-task pairs is, for a complete training set, the tensor product \u0393 = K \u2297 G. If the training set is not complete,\nthe kernel matrix is a principal sub-matrix of \u0393. Pairwise KRR seeks for a prediction function of type\nf(x) = n\u2211 i=1 \u03b1i\u0393(x,xi) ,\nwhere \u03b1i are parameters that minimize the following objective function:\nJ(\u03b1) = (\u0393\u03b1\u2212 y)T(\u0393\u03b1\u2212 y) + \u03bb\u03b1T\u0393\u03b1, (2)\nwhose minimizer can be found by solving the following system of linear equations:\n(\u0393 + \u03bbI)\u03b1 = y. (3)\nSeveral authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36]. Namely, the complexity scales roughly of order O(|D|3 + |T |3) which is required by computing the singular value decomposition (SVD) of both the object and task kernel matrices, but the complexities can be scaled down even further by using sparse kernel matrix approximations.\nHowever, the above computational short-cut only concerns the case in which the training set is complete. If some of the pairs are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient descent based training approaches. While these approaches can also be accelerated via tensor algebraic optimization, they still remain considerably slower than the SVD-based approach. A serious short-coming of the approach is that when generalizing to new tasks, the whole training procedure needs to be re-done with the new training set that contains the union of the auxiliary data and the target data. If the amount of auxiliary data is large, as one would hope in order to expect any positive transfer to happen, this makes generalization to new tasks on-the-fly computationally impractical."}, {"heading": "2.2 Two-step kernel ridge regression", "text": "Next, we present a two-step procedure for performing transfer learning. In the following, we assume that we are provided a training set in which every auxiliary task has the same labeled training objects. This assumption is\nAlgorithm 1 Two-step kernel ridge regression 1: C\u2190 argminC\u2208Rm\u00d7q { \u2016CG\u2212Y\u20162F + \u03bbttr(CGCT) } 2: z\u2190 ( zTL, (CUg) T )T\n3: a\u2190 argmina\u2208Rm { (Ka\u2212 z)T(Ka\u2212 z) + \u03bbdaTKa }\n4: return ft(\u00b7) = \u2211m i=1 aik(di, \u00b7)\nfairly realistic in many practical settings, since one can carry out, for example, a preliminary completion step by using the extensive toolkit of missing value imputation or matrix completion algorithms. A newly given target task, in contrast, is assumed to have only a subset of the training objects labeled. That is, the training set consisting of both the auxiliary and the target tasks is incomplete, because of the missing object labels of the target task, ruling out the direct application of the SVD-based training. To cope with this incompleteness, we consider an approach of performing the learning in two steps, of which the first step is used for completing the training set for the target tasks part (almost full cold start) and the second step for building a model for the target task (full cold start). A particular benefit of the approach is that the first phase where a model is trained on auxiliary data needs to be performed only once, and the resulting model may be subsequently re-used when new target tasks appear.\nLet L \u2286 D and U \u2286 D be the set of objects that are, respectively, labeled and unlabeled for the target task. Moreover, let Y now denote the matrix of labels for the auxiliary tasks and zL \u2208 R|L| the vector of known labels for the target task. Furthermore, let g \u2208 Rq denote the vector of task kernel evaluations between the target task and the auxiliary tasks, e.g. g = (g(t, t1), . . . , g(t, tq))\nT, where t is the target task and ti the auxiliary tasks. Finally, let \u03bbt and \u03bbd be the regularization parameters for the first and the second learning steps, respectively. The two-step approach is summarized in Algorithm 1. The first training step (line 1) can be carried out by training a multi-label KRR model, in which a matrix C of parameters is estimated. The second step (lines 2-4) employs a single-label KRR, in which a vector a of parameters is fitted to the data."}, {"heading": "2.3 Computational considerations and model selection", "text": "Let d and r denote the feature space dimensionalities of the object and task kernels, respectively. These dimensions can be reduced, for example, by the\nAlgorithm 2 Two-step with LOOCV-based automatic model selection Require: Y \u2208 Rm\u00d7q,\u03a6 \u2208 Rm\u00d7d,\u03a8 \u2208 Rq\u00d7r,g \u2208 Rq, zL \u2208 R|L| with d \u2264 m and r \u2264 q.\n1: U, \u221a\n\u03a3,V\u2190 SVD(\u03a6), with U \u2208 Rm\u00d7d, V \u2208 Rd\u00d7d . O(qr2) 2: P, \u221a S,Q\u2190 SVD(\u03a8), with P \u2208 Rq\u00d7r,Q \u2208 Rr\u00d7r . O(md2) 3: e\u2190\u221e 4: for \u03bbt \u2208 {Grid of parameter values} do 5: for j = 1, . . . , q do G\u0303j,j \u2190 Pj(diag((S + \u03bbtI)\u22121) PTj ) . O(qr) 6: C\u2190 YP(S + \u03bbtI)\u22121PT . O(mqr) 7: for i = 1, . . . ,m and j = 1, . . . , q do Ri,j \u2190 Yi,j \u2212 ( G\u0303j,j )\u22121 Ci,j . O(mq)\n8: e\u2190 E(R,Y) . Error between labels and LOO predictions 9: if e < e then \u03bbt, e,R,C\u2190 \u03bbt, e,R,C 10: e\u2190\u221e 11: for \u03bbd \u2208 {Grid of parameter values} do 12: for i = 1, . . . ,m do K\u0303i,i \u2190 Ui(diag((\u03a3 + \u03bbdI)\u22121) UTi ) . O(md) 13: A\u2190 U(\u03a3 + \u03bbdI)\u22121UTR . O(mqd) 14: for i = 1, . . . ,m and j = 1, . . . , q do T i,j \u2190 Yi,j \u2212 ( K\u0303i,i )\u22121 Ai,j . O(mq)\n15: e\u2190 E(T,Y) . Error between labels and LOO predictions 16: if e < e then \u03bbd, e,T,A\u2190 \u03bbd, e,T,A 17: z\u2190 ( zTL, (CUg) T )T 18: a\u2190 U(\u03a3 + \u03bbdI)\u22121UTz . O(md) 19: return ft(\u00b7) = \u2211m i=1 aik(di, \u00b7)\nNystro\u0308m method in order to lower both the time and space complexities of kernel methods [31], and hence in the following we assume that d \u2264 m and r \u2264 q. Let \u03a6 \u2208 Rm\u00d7d and \u03a8 \u2208 Rq\u00d7r be the matrices containing the feature representations of the training objects and tasks in D and T , respectively, so that \u03a6\u03a6T = K and \u03a8\u03a8T = G. Let \u03a6 = U \u221a \u03a3VT and \u03a8 = P \u221a SQT be the SVDs of \u03a6 and \u03a8, respectively. Since the ranks of the feature matrices are at most the dimensions of the feature spaces, we can save both space and time by only computing the singular vectors that correspond to the nonzero singular values. That is, we compute the matrices U \u2208 Rm\u00d7d, V \u2208 Rd\u00d7d, P \u2208 Rq\u00d7r, and Q \u2208 Rr\u00d7r via the economy sized SVD, requiring O(md2+qr2) time. The outcomes of the first and second steps of the two-step KRR (e.g. the first and third lines of Algorithm 1) can be, respectively, written as\nC = YG\u0303 and a = K\u0303z, where G\u0303 = (G + \u03bbtI) \u22121 = U(\u03a3 + \u03bbdI) \u22121UT and K\u0303 = (K + \u03bbdI) \u22121 = U(\u03a3 + \u03bbdI)\n\u22121UT. Given that the above described SVD components are available, the computational complexity is dominated by the multiplication of the eigenvectors with the label matrix, which requires O(mqr) time if the matrix multiplications are performed in the optimal order.\nWe next present an automatic model selection and training approach for the two-step KRR that uses leave-one-out cross-validation (LOOCV) for selecting the values of both \u03bbt and \u03bbd. This is illustrated in Algorithm 2. It is well known that, for KRR, the LOOCV performance can be efficiently computed without training the model from scratch during each CV round (we refer to [30] for details). Adapting this to the first step of the twostep KRR, the \u201cleave-column-out\u201d performance for the ith datum on the jth task (e.g. a CV in which each of the columns of Y are held out at a time to measure the generalization ability to new columns) can be obtained\nin constant time from Yi,j \u2212 ( G\u0303j,j )\u22121 Ci,j, given that the diagonal entries of\nG\u0303 and the dual variables Ci,j are computed and stored in memory. Using the SVD components, both G\u0303j,j and Ci,j can be computed in O(r) time, which enables the efficient selection of the regularization parameter value with LOOCV. If the value is selected from a set of t candidates and LOOCV is computed for all data points and tasks, the overall complexity is O(mqrt). This is depicted in lines 4-9 of Algorithm 2, where the overline symbols denote temporary variables used in the search of the optimal candidate value and E denotes a prediction performance.\nBy the definition of the two-step KRR, the second step consists of training a model using the predictions made during the first step as training labels, while the aim is to make good predictions of the true labels. Therefore, we select the regularization parameter value for the second step using LOOCV on a multi-label KRR model trained using the LOO prediction matrix R obtained from the first step as a label matrix. The second regularization parameter value is thus selected so that the error E(T,Y) between the LOO predictions made during the second step and the original labels Y is as small as possible. In contrast to the first step, the aim of the second step is to generalize to new data points, and hence the CV is done in the leave-row-out\nsense, which can again be efficiently computed as Yi,j \u2212 ( K\u0303i,i )\u22121 Ai,j, where\nAi,j are the model parameters of the multi-label KRR trained row-wise. This is done in lines 11-16 of Algorithm 2.\nThe overall computational complexity of the two-step KRR with automatic model selection is O(md2 + qr2 + mqrt + mqdt), where the first two terms denote the time required by SVD computations and the two latter the time spent for CV and grid search for the regularization parameter. The two-step KRR, in addition to enabling non-zero training sets for the target task, provides a very flexible machinery for CV and model selection. This is in contrast to the ordinary KRR with pairwise tensor product kernels for which such short-cuts are not available to our knowledge, and there is no efficient closed form solution available for the almost full cold start settings. Note also that, while the above described method separately selects the regularization parameter values for the tasks and the data, the method is easy to modify so that it would select a separate regularization parameter value for each task and for each datum (e.g. altogether m + q parameters), thus allowing considerably more degrees of freedom. However, the consideration of this variation is omitted due to the lack of space."}, {"heading": "3 Theoretical considerations", "text": "Here, we analyze the two-step learning approach by studying its connections to learning with pairwise tensor product kernels of type (1). These two approaches coincide in an interesting way for full cold start problems, a special case in which the target task has no labeled data at all. This, in turn, allows us to show the consistency of the two-step KRR via its universal approximation and spectral regularization properties.\nThe connection between the two-step and pairwise KRR is characterized by the following result.\nProposition 1. Let us consider a full cold start setting with a complete training set. Let ft(\u00b7) be a model trained with two-step KRR for the target task t and f(\u00b7, \u00b7) be a model trained with an ordinary least-squares regression on the object-task pairs with the following pairwise kernel function on D\u00d7T :\n\u0393 (( d, t), (d, t )) = ( k ( d,d ) + \u03bbd\u03b4 ( d,d )) ( g ( t, t) + \u03bbt\u03b4 ( t, t )))\n(4)\nwhere \u03b4 is the delta kernel whose value is 1 if the arguments are equal and 0 otherwise. Then, ft(d) = f(t,d) for any d \u2208 D.\nProof. Writing the steps of the algorithm together and denoting G\u0303 = (G + \u03bbI)\u22121 and K\u0303 = (K + \u03bbI)\u22121, we observe that the model parameters\na of the target task can also be obtained from the following closed form:\na = K\u0303YG\u0303g . (5)\nThe prediction for a datum d is ft(d) = k Ta, where k \u2208 Rm is the vector containing all kernel evaluations between d and the training data points. The kernel matrix corresponding to the complete training set of auxiliary tasks can be expressed as the following tensor product: \u0393 = (G + \u03bbtI) \u2297 (K + \u03bbdI) . The regression problem being\n\u03b1 = argmin \u03b1\u2208Rmq\n{ (vec(Y)\u2212 \u0393\u03b1)T (vec(Y)\u2212 \u0393\u03b1) } ,\nits minimizer can be expressed as \u03b1 = \u0393\u22121vec(Y) = ( (G + \u03bbtI) \u22121 \u2297 (K + \u03bbdI)\u22121 ) vec(Y)\n= vec ( (K + \u03bbdI) \u22121 Y (G + \u03bbtI) \u22121) = vec(K\u0303YG\u0303) . (6)\nThe prediction for the datum d is (g\u2297k)Tvec ( K\u0303YG\u0303 ) = kTK\u0303YG\u0303g.\nThe kernel point of view allows us to consider the universal approximation properties of the learned knowledge transfer models. Recall the concept of universal kernel functions:\nDefinition 1. [35] A continuous kernel k on a compact metric space X (i.e. X is closed and bounded) is called universal if the reproducing kernel Hilbert space (RKHS) induced by k is dense in C(X ), where C(X ) is the space of all continuous functions f : X \u2192 R.\nThe universality property indicates that the hypothesis space induced by an universal kernel can approximate any continuous function to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find the approximation [35].\nProposition 2. The kernel \u0393 on D \u00d7 T defined in (4) is universal if the kernels k on D and g on T are both universal.\nProof. We provide here a high-level sketch of the proof. The details are omitted due to lack of space but they can be easily verified from the existing literature. The RKHS of sums of reproducing kernels was characterized by [3] as follows: Let H(k1) and H(k2) be RKHSs over X with reproducing kernels k1 and k2, respectively. If k = k1 + k2 and H(k) denotes the corresponding RKHS, then H(k) = {f1 + f2 : fi \u2208 H(ki), i = 1, 2}. Thus, if the object kernel is universal, the sum of the object and delta kernels is also universal and the same concerns the task kernel. The product of two universal kernels is also universal, as considered in our previous work [37].\nThe full cold start setting with complete auxiliary training set allows us to consider the two-step approach from the spectral filtering regularization point of view [18], an approach that has recently gained some attention due to its ability to study various types of regularization approaches under the same framework. Continuing from (3), we observe that\n\u03b1 = \u03d5\u03bb(\u0393)vec(Y) = W\u03d5\u03bb(\u039b)W Tvec(Y),\nwhere \u0393 = W\u039bWT is the eigen decomposition of the kernel matrix \u0393 and \u03d5\u03bb is a filter function, parameterized by \u03bb, such that if v is an eigenvector of \u0393 and \u03c3 is its corresponding eigenvalue, then \u0393v = \u03d5\u03bb(\u03c3)v. The filter function corresponding to the Tikhonov regularization being \u03d5\u03bb(\u03c3) = 1 \u03c3+\u03bb\n, and the ordinary least-squares approach corresponding to the \u03bb = 0 case, several other learning approaches, such as spectral cut-off and gradient descent, can also be expressed as filter functions, but which cannot be expressed as a penalized empirical error minimization problem analogous to (2).\nThe eigenvalues of the kernel matrix obtained with the tensor product kernel on a complete training set can be expressed as the tensor product \u039b = \u03a3\u2297S of the eigenvalues \u03a3 and S of the object and task kernel matrices. Now, instead of considering the two-step learning approach from the kernel point of view, one can also cast it into the spectral filtering regularization framework, resulting to the following filter function:\n\u03d5\u03bb(\u03c3) = 1\n(\u03c31 + \u03bbt)(\u03c32 + \u03bbd) =\n1\n\u03c31\u03c32 + \u03bbd\u03c31 + \u03bbt\u03c32 + \u03bbt\u03bbd , (7)\nwhere \u03c31, \u03c32 are the factors of \u03c3, namely eigenvalues of K and G. This differs from the Tikhonov regularization only by the two middle terms in the denominator if one sets \u03bb = \u03bbt\u03bbd. In the experiments, we observe that this\ndifference is rather small also in practical cases, making the two-step learning approach a viable alternative for pairwise KRR with ordinary tensor product kernels.\nIn the following, we assume that the kernel is bounded, that is, there exists \u03ba > 0 such that supx\u2208X \u221a \u0393(x,x) \u2264 \u03ba, indicating that the eigenvalues of kernel matrices are in [0, \u03ba2]. To further analyze the above filter functions, we follow [4, 6, 18] and say that a function \u03d5\u03bb : [0, \u03ba\n2] \u2192 R, 0 < \u03bb \u2264 \u03ba2, parameterized by 0 < \u03bb \u2264 \u03ba2, is an admissible regularizer if there exists constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that\nsup 0<\u03c3\u2264\u03ba2 |\u03c3\u03d5\u03bb(\u03c3)| \u2264 D, sup 0<\u03c3\u2264\u03ba2\n|\u03d5\u03bb(\u03c3)| \u2264 B\n\u03bb , sup 0<\u03c3\u2264\u03ba2 |1\u2212 \u03c3\u03d5\u03bb(\u03c3)| \u2264 \u03b3 ,\nand sup 0<\u03c3\u2264\u03ba2\n|1\u2212 \u03c3\u03d5\u03bb(\u03c3)|\u03c3\u03bd \u2264 \u03b3\u03bd\u03bb\u03bd , \u2200\u03bd \u2208 (0, \u03bd\u0304].\nThe admissibility, in turn, ensures that R(f\u0302\u03bb)\u2212 inff\u2208HR(f) = O ( n\u2212 \u03bd\u0304 2\u03bd\u0304+1 ) (8)\nholds with high probability, where R denotes the expected prediction error with respect to some unknown probability measure \u03c1(x, y) on the joint space X \u00d7 R of inputs and labels that is, R(f) = \u222b X\u00d7R(f(x) \u2212 y)\n2d\u03c1(x, y) . We refer to [4, 6, 18] for a detailed consideration and further results. It is straightforward to see that, analogously to the Tikhonov regularization, the admissibility of the function (7) is confirmed by D,B, \u03b3, \u03b3\u03bd , \u03bd\u0304 = 1 for arbitrary factorizations of \u03bb = \u03bbt\u03bbd and \u03c3 = \u03c31\u03c32 such that \u03bbt, \u03bbd > 0 and \u03c31, \u03c32 \u2265 0. Thus, function (7) can be considered under the spectral filtering regularization framework with separate regularization parameter values for objects and tasks. The universality of the kernel ensures that inff\u2208HR(f) in (8) is the error of the underlying regression function to be learned, and the admissibility of the regularizer ensures that R(f\u0302\u03bb) converges to it when the size of the training set approaches infinity, guaranteeing the consistency of the two-step KRR method."}, {"heading": "4 Experiments", "text": "In the experiments, we compare different types of transfer learning settings in solving three dyadic prediction problems: drug-target, document similarity and protein similarity prediction. We simulate the full and almost full\ncold start problem as follows. In each experiment, one drug, document or protein is considered to be the target task in question, where the task is to predict the interactions of drugs or similarities of documents or proteins with respect to the target. Further, other tasks formed in the same way are provided as auxiliary information, leading to a full cold start or almost full cold start setting. The experiments are performed 100 times with different training/test set splits, the performances are averages over all repetitions and over all target tasks. The performance is measured using the concordance index [11] (C-index), also known as the pairwise ranking accuracy 1 |{(i,j)|yi>yj}| \u2211 yi>yj\nH(y\u0302i \u2212 y\u0302j), where yi denote the true and y\u0302i the predicted labels, and H is the Heaviside step function. The regularization parameter selection is performed using LOOCV on the training data. For the two-step approach, we select the first regularization parameter via LOOCV on the auxiliary tasks, and the second one via LOOCV on the target task data augmented with predictions from the first step. The implementation of the algorithms used in the experiments will be made available in the RLScore open source machine learning library1.\nThe drug-target interaction prediction data2 [9, 22] consists of 68 drug compounds and 442 protein targets. The kernel between the drugs is based on the 3D Tanimoto coefficient similarity, and the sequence similarity between the protein targets was computed using the normalized version of the Smith-Waterman score. Further, for each drug-protein pair we have a realvalued label, negative logarithm of the kinase disassociation constant Kd, that characterizes the interaction affinity between the drug and target in question. In each experiment, the task of interest corresponds to one of the drugs in the data set. The goal is to learn to predict for the given drug the Kd values for proteins unseen during the training phase. The performances are always computed over a testing set of 192 protein targets for a given task, i.e. we assess whether for a given target we can discriminate between proteins with more or less affinity for this drug.\nFor each task, we vary the number of available training proteins, from 5 to 250. In addition, we have available the training data for the 250 training proteins for the 67 auxiliary tasks. As summarized in Figure 1, we evaluate a number of different approaches:\n\u2022 Single-task: use only training data from target task (traditional regres1 Available at https://github.com/aatapa/RLScore 2http://users.utu.fi/aatapa/data/DrugTarget\nsion setting, tackled with KRR)\n\u2022 Multi-task: both target task and auxiliary tasks have same amount of training data available (multi-output learning leveraging task correlations, tackled with pairwise tensor product KRR)\n\u2022 Full cold start: no data available for the target task (tackled with pairwise tensor product KRR and two-step KRR)\n\u2022 Almost full cold start: use a varying amount of data from the target task, and all the available data from auxiliary tasks (tackled with twostep KRR)\nWe do not consider the pairwise KRR in the almost full cold start experiment due to computational considerations, as unlike for the two-step approach no closed-form solution exists for the method in this setting, and the iterative conjugate gradient based method has rather poor scalability.\nIn Figure 2, we present the results for the drug-target experiments. In Figure 2 (a) we present an experiment, where all the 67 auxiliary tasks have\navailable the data for all 250 training proteins, and the amount of data available for the target task is varied. It can be seen that learning is possible even in the full cold start setting, where both two-step KRR and pairwise KRR perform much better than randomly. The single-task approach begins to outperform the full cold start setting after the point when one has access to a bit more than 50 training proteins. Combining these two sources of information leads to the best performance up until 150 training proteins. However, once there is enough data available for the target task, there is no longer any positive transfer from the auxiliary tasks.\nIn Figure 2 (b) we consider the setting, where there is the same amount of data available for both the auxiliary tasks and the target tasks. This setting corresponds closely to the traditional multi-output regression problem, the exception being that only the label for the target task is of interest during testing. Here we can see that the multi-task method that uses the task correlation information fails to outperform the simple single-task approach, suggesting that on this type of data one requires significantly more data in the auxiliary tasks compared to the target tasks in order for it to be helpful for learning.\nIn Figure 2 (c) we consider the full cold start learning setting, while increasing the amount of data available for the auxiliary tasks. Here we observe that the simple two-step approach slightly outperforms pairwise KRR, possibly due to the property that it allows regularizing the drugs and the targets separately. Both approaches generalize to the unknown target task, though the results are still much worse than when having significant amount of data for the target task.\nFurther, we compare all the considered learning settings on the 20 News-\ngroups data3. Here, given any target document, the goal is to predict the similarity of other documents with respect to it. This constitutes a threelevel ordinal regression task, where documents from the same newsgroup as the target receive the highest rating, documents from similar newsgroups the second highest, and documents from dissimilar newsgroups the lowest rating. These similarities are assigned according to the taxonomy available at the data set web site. The documents are represented using bag-of-words features together with a linear kernel. In the experiments the number of target domain data ranges from 50 to 1500 documents (transfer learning, single-task, multi-task methods), and the number of auxiliary tasks and data available for each either ranges from 50 to 1500 documents (multi-task, full cold start learning), or stays fixed at 2000 documents (transfer learning).\nThe results are presented in Figure 3. For the transfer learning approaches, already the starting point of 50 target domain documents suffices to reach a performance that is as good as the single-task method with at least 1500 documents. The multi-task learning setting does not outperform the single-task setting, and while learning is possible in the full cold start setting, some target task data is still required to reach a high predictive performance. Two-step learning slightly outperforms pairwise KRR.\nThe UniProt data was generated by downloading all the protein amino acid sequences with all the gene ontology (GO) annotations of the Universal Protein Resource (UniProt) database. For the amino acid sequences we used the normalized spectrum kernel [17]. This kernel is a popular tool for com-\n3http://qwone.com/~jason/20Newsgroups/\nparing biological sequences without alignments. The normalized spectrum kernel is based on the number of k -mers two sequences have in common. In our experiments, k was set to three. Two proteins were labeled as \u2019similar in function\u2019 when they had at least one GO term in common. The problem of protein function prediction was thus transformed to a binary classification problem. The experimental setup is the same as for the Newsgroup data, and the results, presented in Figure 3, are very similar, though at 1500 proteins the performance of the two-step method actually falls below that of the single-task approach.\nIn all experiments the two-step approach shows itself to be competitive compared to the pairwise learning approach. Previously, [32] have in their overview article on dyadic prediction in the biological domain made the observation that in terms of predictive accuracy experimentally there does not seem to be a clear winner between the single-task and multi-task type of learning approaches. Based on our experimental results, a deciding factor on whether one may expect positive transfer from related tasks seems to be based on the amount of data available for the target task. The two-step method performs well in the almost full cold start settings with availability of a significant amount of auxiliary data and only very little data for the target task. But when there is enough data available for the target task, auxiliary data is no longer helpful."}], "references": [{"title": "Incorporating side information into probabilistic matrix factorization using Gaussian processes", "author": ["R.P. Adams", "G.E. Dahl", "I. Murray"], "venue": "Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, pp", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Kernels for vector-valued functions: a review", "author": ["M. \u00c1lvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundation and Trends in Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Multi-output learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Unifying collaborative and content-based filtering. Proceedings of the twenty-first international conference on Machine learning (ICML\u201904)", "author": ["J. Basilico", "T. Hofmann"], "venue": "ACM International Conference Proceeding Series, vol", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "On regularization algorithms in learning theory", "author": ["F. Bauer", "S. Pereverzev", "L. Rosasco"], "venue": "Journal of Complexity", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Kernel methods for predicting protein-protein interactions", "author": ["A. Ben-Hur", "W. Noble"], "venue": "Bioinformatics 21 Suppl", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Kernel multi-task learning using task-specific features", "author": ["E.V. Bonilla", "F. Agakov", "C. Williams"], "venue": "In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Comprehensive analysis of kinase inhibitor selectivity", "author": ["M.I. Davis", "J.P. Hunt", "S. Herrgard", "P. Ciceri", "L.M. Wodicka", "G. Pallares", "M. Hocker", "D.K. Treiber", "P.P. Zarrinkar"], "venue": "Nature biotechnology 29(11),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Matrix co-factorization for recommendation with rich side information and implicit feedback", "author": ["Y. Fang", "L. Si"], "venue": "Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Concordance probability and discriminatory power in proportional hazards regression", "author": ["M. G\u00f6nen", "G. Heller"], "venue": "Biometrika 92(4),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Self-measuring similarity for multi-task gaussian process", "author": ["K. Hayashi", "T. Takenouchi", "R. Tomioka", "H. Kashima"], "venue": "ICML Workshop on Unsupervised and Transfer Learning, JMLR Proceedings,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Protein-ligand interaction prediction: an improved chemogenomics", "author": ["L. Jacob", "J. Vert"], "venue": "approach\u201d, bioinformatics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Link propagation: A fast semi-supervised learning algorithm for link prediction", "author": ["H. Kashima", "T. Kato", "Y. Yamanishi", "M. Sugiyama", "K. Tsuda"], "venue": "Proceedings of the SIAM International Conference on Data Mining (SDM", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Zero-data learning of new tasks", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "Proceedings of the 23rd national conference on Artificial intelligence (AAAI\u201908),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "W.S.S.: The spectrum kernel: a string kernel for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "Noble"], "venue": "Proceedings of the Pacific Symposium on Biocomputing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Spectral algorithms for supervised learning", "author": ["L. Lo Gerfo", "L. Rosasco", "F. Odone", "E. De Vito", "A. Verri"], "venue": "Neural Computation 20(7),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Shifted Kronecker product systems", "author": ["C.D. Martin", "C.F. Van Loan"], "venue": "SIAM Journal on Matrix Analysis and Applications 29(1),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "A log-linear model with latent features for dyadic prediction", "author": ["A. Menon", "C. Elkan"], "venue": "ICDM, pp", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Using feature conjunctions across examples for learning pairwise classifiers", "author": ["S. Oyama", "C. Manning"], "venue": "Proceedings of the European conference on Machine learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Toward more realistic drug-target interaction predictions", "author": ["T. Pahikkala", "A. Airola", "S. Pietil\u00e4", "S. Shakyawar", "A. Szwajda", "J. Tang", "T. Aittokallio"], "venue": "Briefings in Bioinformatics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Efficient regularized least-squares algorithms for conditional ranking on relational data", "author": ["T. Pahikkala", "A. Airola", "M. Stock", "B.D. Baets", "W. Waegeman"], "venue": "Machine Learning 93(2-3),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Conditional ranking on relational data", "author": ["T. Pahikkala", "W. Waegeman", "A. Airola", "T. Salakoski", "B. De Baets"], "venue": "Proceedings of the European conference on Machine learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Learning intransitive reciprocal relations with kernel methods", "author": ["T. Pahikkala", "W. Waegeman", "E. Tsivtsivadze", "T. Salakoski", "B. De Baets"], "venue": "European Journal of Operational Research", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["S.T. Park", "W. Chu"], "venue": "Proceedings of the Third ACM Conference on Recommender Systems, pp", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Flaws in evaluation schemes for pair-input computational predictions", "author": ["Y. Park", "E.M. Marcotte"], "venue": "Nature Methods", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Fast and scalable algorithms for semisupervised link prediction on static and dynamic graphs", "author": ["R. Raymond", "H. Kashima"], "venue": "Proceedings of the European conference on Machine learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Notes on regularized least squares", "author": ["R. Rifkin", "R. Lippert"], "venue": "Tech. Rep. MIT-CSAIL-TR-2007-025, Massachusetts Institute of Technology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Input space versus feature space in kernel-based methods", "author": ["B. Sch\u00f6lkopf", "S. Mika", "C. Burges", "P. Knirsch", "K.R. M\u00fcller", "G. R\u00e4tsch", "A. Smola"], "venue": "IEEE Transactions On Neural Networks", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "On protocols and measures for the validation of supervised methods for the inference of biological networks", "author": ["M. Schrynemackers", "R. K\u00fcffner", "P. Geurts"], "venue": "Front Genet", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Classifying pairs with trees for supervised biological network inference", "author": ["M. Schrynemackers", "L. Wehenkel", "M.M. Babu", "P. Geurts"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Generalized probabilistic matrix factorizations for collaborative filtering", "author": ["H. Shan", "A. Banerjee"], "venue": "ICDM, pp", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "Journal of Machine Learning Research", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "The ubiquitous kronecker product", "author": ["C.F. Van Loan"], "venue": "Journal of Computational and Applied Mathematics 123(1\u20132),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "A kernel-based framework for learning graded relations from data", "author": ["W. Waegeman", "T. Pahikkala", "A. Airola", "T. Salakoski", "M. Stock", "B. De Baets"], "venue": "IEEE Transactions on Fuzzy Systems", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Kernelized probabilistic matrix factorization: Exploiting graphs and side information", "author": ["T. Zhou", "H. Shan", "A. Banerjee", "G. Sapiro"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}], "referenceMentions": [{"referenceID": 26, "context": "[28] make in a recent Nature-review on dyadic prediction an important distinction between four main settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] for a review.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 32, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 36, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "[32] for a recent review.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 21, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 22, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 25, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 118, "endOffset": 125}, {"referenceID": 25, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 118, "endOffset": 125}, {"referenceID": 5, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 170, "endOffset": 177}, {"referenceID": 12, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 170, "endOffset": 177}, {"referenceID": 11, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 21, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 253, "endOffset": 257}, {"referenceID": 12, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 60, "endOffset": 72}, {"referenceID": 21, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 60, "endOffset": 72}, {"referenceID": 25, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 60, "endOffset": 72}, {"referenceID": 21, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "From this viewpoint, Setting C corresponds to a specific instantiation of a traditional transfer learning scenario, in which the aim is to transfer knowledge obtained from already learned auxiliary tasks to the target task of interest [26].", "startOffset": 235, "endOffset": 239}, {"referenceID": 14, "context": "Stretching the concept of transfer learning even further, in the case of so-called zerodata learning, one arrives at Setting D, which is characterized by no available labeled training data for the target task [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 31, "context": "[33] have recently proposed a similar two-step approach based on tree-based ensemble methods for biological network inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[4] and several other authors (see [2] and references therein) have extended KRR to involve task correlations via matrix-valued kernels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[4] and several other authors (see [2] and references therein) have extended KRR to involve task correlations via matrix-valued kernels.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 5, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 6, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 10, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 19, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 21, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 25, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 1, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 12, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 17, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 22, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 27, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 34, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 29, "context": "Nystr\u00f6m method in order to lower both the time and space complexities of kernel methods [31], and hence in the following we assume that d \u2264 m and r \u2264 q.", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "It is well known that, for KRR, the LOOCV performance can be efficiently computed without training the model from scratch during each CV round (we refer to [30] for details).", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "[35] A continuous kernel k on a compact metric space X (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "The universality property indicates that the hypothesis space induced by an universal kernel can approximate any continuous function to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find the approximation [35].", "startOffset": 313, "endOffset": 317}, {"referenceID": 35, "context": "The product of two universal kernels is also universal, as considered in our previous work [37].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "The full cold start setting with complete auxiliary training set allows us to consider the two-step approach from the spectral filtering regularization point of view [18], an approach that has recently gained some attention due to its ability to study various types of regularization approaches under the same framework.", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "To further analyze the above filter functions, we follow [4, 6, 18] and say that a function \u03c6\u03bb : [0, \u03ba ] \u2192 R, 0 < \u03bb \u2264 \u03ba, parameterized by 0 < \u03bb \u2264 \u03ba, is an admissible regularizer if there exists constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that", "startOffset": 57, "endOffset": 67}, {"referenceID": 4, "context": "To further analyze the above filter functions, we follow [4, 6, 18] and say that a function \u03c6\u03bb : [0, \u03ba ] \u2192 R, 0 < \u03bb \u2264 \u03ba, parameterized by 0 < \u03bb \u2264 \u03ba, is an admissible regularizer if there exists constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that", "startOffset": 57, "endOffset": 67}, {"referenceID": 16, "context": "To further analyze the above filter functions, we follow [4, 6, 18] and say that a function \u03c6\u03bb : [0, \u03ba ] \u2192 R, 0 < \u03bb \u2264 \u03ba, parameterized by 0 < \u03bb \u2264 \u03ba, is an admissible regularizer if there exists constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that", "startOffset": 57, "endOffset": 67}, {"referenceID": 2, "context": "We refer to [4, 6, 18] for a detailed consideration and further results.", "startOffset": 12, "endOffset": 22}, {"referenceID": 4, "context": "We refer to [4, 6, 18] for a detailed consideration and further results.", "startOffset": 12, "endOffset": 22}, {"referenceID": 16, "context": "We refer to [4, 6, 18] for a detailed consideration and further results.", "startOffset": 12, "endOffset": 22}, {"referenceID": 9, "context": "The performance is measured using the concordance index [11] (C-index), also known as the pairwise ranking accuracy 1 |{(i,j)|yi>yj}| \u2211 yi>yj H(\u0177i \u2212 \u0177j), where yi denote the true and \u0177i the predicted labels, and H is the Heaviside step function.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "The drug-target interaction prediction data [9, 22] consists of 68 drug compounds and 442 protein targets.", "startOffset": 44, "endOffset": 51}, {"referenceID": 20, "context": "The drug-target interaction prediction data [9, 22] consists of 68 drug compounds and 442 protein targets.", "startOffset": 44, "endOffset": 51}, {"referenceID": 15, "context": "For the amino acid sequences we used the normalized spectrum kernel [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 30, "context": "Previously, [32] have in their overview article on dyadic prediction in the biological domain made the observation that in terms of predictive accuracy experimentally there does not seem to be a clear winner between the single-task and multi-task type of learning approaches.", "startOffset": 12, "endOffset": 16}], "year": 2014, "abstractText": "Dyadic prediction methods operate on pairs of objects (dyads), aiming to infer labels for out-of-sample dyads. We consider the full and almost full cold start problem in dyadic prediction, a setting that occurs when both objects in an out-of-sample dyad have not been observed during training, or if one of them has been observed, but very few times. A popular approach for addressing this problem is to train a model that makes predictions based on a pairwise feature representation of the dyads, or, in case of kernel methods, based on a tensor product pairwise kernel. As an alternative to such a kernel approach, we introduce a novel two-step learning algorithm that borrows ideas from the fields of pairwise learning and spectral filtering. We show 1 ar X iv :1 40 5. 44 23 v1 [ cs .L G ] 1 7 M ay 2 01 4 theoretically that the two-step method is very closely related to the tensor product kernel approach, and experimentally that it yields a slightly better predictive performance. Moreover, unlike existing tensor product kernel methods, the two-step method allows closed-form solutions for training and parameter selection via cross-validation estimates both in the full and almost full cold start settings, making the approach much more efficient and straightforward to implement. 1 A subdivision of dyadic prediction methods Many real-world machine learning problems can be naturally represented as pairwise learning or dyadic prediction problems, for which feature representations of two different types of objects (aka a dyad) are jointly used to predict a relationship between those objects. Amongst others, applications of that kind emerge in biology (e.g. predicting protein-RNA interactions), medicine (e.g. design of personalized drugs), chemistry (e.g. prediction of binding between two types of molecules), social network analysis (e.g. link prediction) and recommender systems (e.g. personalized product recommendation). For many dyadic prediction problems it is extremely important to implement appropriate training and evaluation procedures. [28] make in a recent Nature-review on dyadic prediction an important distinction between four main settings. Given t and d as the feature representations of the two types of objects, those four settings can be summarized as follows: \u2022 Setting A: Both t and d are observed during training, as parts of separate dyads, but the label of the dyad (t,d) must be predicted. \u2022 Setting B: Only t is known during training, while d is not observed in any dyad, and the label of the dyad (t,d) must be predicted. \u2022 Setting C: Only d is known during training, while t is not observed in any dyad, and the label of the dyad (t,d) must be predicted. \u2022 Setting D: Neither t nor d occur in any training dyad, but the label of the dyad (t,d) must be predicted (referred to as the full cold start problem). Setting A is of all four settings by far the most studied setting in the machine literature. Motivated by applications in collaborative filtering and", "creator": "LaTeX with hyperref package"}}}