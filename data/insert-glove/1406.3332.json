{"id": "1406.3332", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2014", "title": "Convolutional Kernel Networks", "abstract": "azizul An important goal henrician in visual recognition cleat is benvenisti to stemware devise odisha image representations andjelko that hisar are fermina invariant to phalacrocoracidae particular ligitan transformations. vegeta In colori this paper, acclamation we address blaufr\u00e4nkisch this goal with slichter a boie new 42-meter type of convolutional waterson neural network (mipham CNN) stenmark whose invariance balcer is encoded olso by 4.22 a reproducing kernel. l'ann\u00e9e Unlike traditional lupoe approaches stagnate where neural nakane networks are learned searle either innumerable to represent data leimert or for solving 34-foot a classification 1846-47 task, passionless our magnificat network look-up learns ilter to afsoc approximate then-wife the kernel queasily feature onigbinde map slags on acucar training seismicity data. Such an approach zainur enjoys several benefits jarba over classical 114.40 ones. ductus First, by buchori teaching atiq CNNs to be quwo invariant, we far-right obtain simple network architectures 26-10 that achieve evite a thimphu similar dashingly accuracy to more 40million complex catoe ones, pisek while being aung easy to train zhucheng and robust to pawlowicz overfitting. irritant Second, we revzin bridge clemencia a gap abidance between the neural rosnes network hanshu literature and kernels, which revise are kaitlyn natural geatish tools demean to rainford model lafargue invariance. We evaluate our gimmick methodology on visual pres. recognition pushor tasks where gondii CNNs have animatic proven simun to perform well, nhsltot e. g. , digit v.d. recognition with acidify the tinkering MNIST dataset, turbomeca and supersingular the subnational more eberbach challenging legit CIFAR - bit-part 10 and kathleen STL - 10 dor datasets, teletypewriter where nanosecond our 80.9 accuracy is competitive flanker with the state of 62.10 the shrewdness art.", "histories": [["v1", "Thu, 12 Jun 2014 19:41:03 GMT  (49kb)", "http://arxiv.org/abs/1406.3332v1", null], ["v2", "Fri, 14 Nov 2014 16:58:48 GMT  (52kb)", "http://arxiv.org/abs/1406.3332v2", "appears in Advances in Neural Information Processing Systems (NIPS), Dec 2014, Montreal, Canada,this http URL"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["julien mairal", "piotr koniusz", "za\u00efd harchaoui", "cordelia schmid"], "accepted": true, "id": "1406.3332"}, "pdf": {"name": "1406.3332.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["firstname.lastname@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n33 32\nv1 [\ncs .C\nV ]\n1 2\nJu n\n20 14"}, {"heading": "1 Introduction", "text": "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29]. The architecture of CNNs is relatively simple and consists of successive layers organized in a hierarchical fashion. Each layer involves convolutions with learned filters followed by a non-linearity, and downsampling operations called \u201cfeature pooling\u201d. The resulting image representation seems to achieve some invariance to image perturbations and to encode complex visual patterns [32]. Training CNNs remains however difficult since high-capacity networks may involve billions of parameters to learn, which requires both high computational power, e.g., GPUs, and appropriate regularization techniques [18, 21, 29].\nThe exact nature of invariance achieved by CNNs is also not well understood. Only recently, the invariance of some related architectures has been characterized; this is the case for the wavelet scattering transform [8], or the hierarchical models of [7]. Our work revisits convolutional neural networks, but adopts a significantly different approach than the traditional one. We use indeed kernels [26], which are natural tools to model invariance [14]. Inspired by the hierarchical kernel descriptors of [2], we propose a reproducing kernel that produces invariant multi-layer image representations.\nOur main contribution is an approximation scheme called convolutional kernel network (CKN) to make the kernel approach computationally feasible. Such a scheme turns out to be a new type of CNN, which differs from classical ones in its objective function. The network is trained to linearly approximate the kernel, and thus the procedure involves no supervision. Another difference is in the non-linear functions that we use. Interestingly, they resemble rectified linear units [1, 29], even though they naturally emerge from the approximation scheme and were not handcrafted.\nAs a result, we bridge a gap between kernel methods and neural networks, and we believe that such a direction is fruitful for the future. Our network is learned without supervision since the label\n\u2217LEAR team, Inria Grenoble, Laboratoire Jean Kuntzmann, CNRS, Univ. Grenoble Alpes, France.\ninformation is only used subsequently in a support vector machine (SVM). Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation. Open-source code for learning our convolutional kernel networks will be provided upon publication of the paper."}, {"heading": "1.1 Related Work", "text": "There were several attempts in the past to build kernel-based methods that mimic deep neural networks; we only review here the most related to our approach.\nArc-cosine kernels. Kernels for building deep large-margin classifiers have been introduced in [10]. For any pair of vectors, the single-layer arc-cosine kernel relies on an integral representation, and multilayer extensions are built by multiple kernel compositions. Similarly, our kernels rely on a integral representation, and enjoy a multilayer construction. However, in contrast to arccosine kernels: (i) we build our sequence of kernels by convolutions, using local information over spatial neighborhoods (as opposed to compositions, using global information); (ii) we propose a new training procedure for learning a compact representation of the kernel in a data-dependent manner.\nMultilayer derived kernels. Kernels that enjoy invariance properties for visual recognition were proposed in [7]. Such kernels are built with a parameterized \u201cneural response\u201d function, which consists in computing the maximal response of a base kernel over a local neighborhood. Multiple layers are then built by iteratively renormalizing the response kernels and pooling using neural response functions. Learning is performed by plugging the obtained kernel in a SVM. In contrast to [7], we propagate information up, from lower to upper layers, by using sequences of convolutions. Furthermore, we propose a simple and effective data-dependent way to learn a compact representation of our kernels and show that we obtain near state-of-the-art performance on several benchmarks.\nHierarchical kernel descriptors. The kernels proposed in [2, 3] produce multilayer image representations for visual recognition tasks. We discuss in details these kernels in the next section: our paper generalizes them and establishes a strong link with convolutional neural networks."}, {"heading": "2 Convolutional Multilayer Kernels", "text": "The convolutional multilayer kernel is a generalization of the hierarchical kernel descriptors introduced in computer vision [2, 3]. The kernel produces a sequence of image representations that are built one on top of each other in a multilayer fashion. Each layer can be interpreted as a non-linear transformation of the previous one with additional invariance. We call these layers image feature maps1, and formally define them as follows:\nDefinition 1. An image feature map \u03d5 is a function \u03d5 : \u2126 \u2192 H, where \u2126 is a (usually discrete) subset of [0, 1]d representing \u201ccoordinates\u201d in the image and H is a Hilbert space.\nFor all practical examples in this paper, \u2126 is a two-dimensional grid and corresponds to different locations in a two-dimensional image. In other words, \u2126 is a set of pixel coordinates. Given z in \u2126, the point \u03d5(z) corresponds to some characteristics of the image at location z, or in a neighborhood of z. For instance, a color image of size m \u00d7 n with three channels, red, green, and blue, may be represented by an initial feature map \u03d50 : \u21260 \u2192 H0, where \u21260 is an m \u00d7 n regular grid, H0 is the Euclidean space R3, and \u03d50 provides the color pixel values. With the multilayer scheme, non-trivial feature maps will be obtained subsequently, which will encode more complex image characteristics. With this terminology in hand, we now introduce the convolutional kernel, first, for a single layer.\nDefinition 2 (Convolutional Kernel with Single Layer). Let us consider two images represented by two image feature maps, respectively \u03d5 and \u03d5\u2032 : \u2126 \u2192 H, where \u2126 is a set of pixel locations, and H is a Hilbert space. The one-layer convolutional kernel between \u03d5 and \u03d5\u2032 is defined as\nK(\u03d5, \u03d5\u2032) := \u2211\nz\u2208\u2126\n\u2211\nz\u2032\u2208\u2126\n\u2016\u03d5(z)\u2016H \u2016\u03d5\u2032(z\u2032)\u2016H e \u2212 1\n2\u03b22 \u2016z\u2212z\u2032\u20162\n2e\u2212 1 2\u03c32 \u2016\u03d5\u0303(z)\u2212\u03d5\u0303\u2032(z\u2032)\u20162 H , (1)\n1In the kernel literature, \u201cfeature map\u201d denotes the mapping between data points and their representation in a RKHS [26]. Here, feature maps refer to spatial maps, as usual in the neural network literature [22].\nwhere \u03b2 and \u03c3 are smoothing parameters of Gaussian kernels, and \u03d5\u0303(z) and \u03d5\u0303(z\u2032) are normalized versions of \u03d5(z) and \u03d5\u2032(z\u2032), respectively. More precisely, \u03d5\u0303(z) :=(1/max(\u2016\u03d5(z)\u2016H , \u03b5))\u03d5(z), and the definition is similar for \u03d5\u2032(z\u2032).2\nIt is easy to see that K is a positive definite kernel since it only involves sums and products of positive definite kernels [26]. It compares normalized features \u03d5\u0303(z) and \u03d5\u0303\u2032(z\u2032) extracted at all locations z and z\u2032 with a Gaussian kernel. Another Gaussian function compares the locations z and z\u2032 providing invariance to local deformations. Indeed, when \u03b2 goes to infinity, the kernel becomes invariant to the positions z and z\u2032 of the features \u03d5(z) and \u03d5\u2032(z\u2032). When \u03b2 is small, only features placed at the same location z = z\u2032 are compared to each other, and the kernel has no shift-invariance. Before moving to a model with additional layers, let us present a few concrete examples when applying the convolutional kernel to simple input feature maps \u03d50 : \u21260 \u2192 H0. Gradient map. Assume that H0=R2 and that \u03d50(z) provides the two-dimensional gradient of the image at pixel z, which is often computed with first-order differences along each dimension. Then, the quantity \u2016\u03d50(z)\u2016H0 is the gradient intensity, and \u03d5\u03030(z) is its orientation, which can be characterized by a particular angle\u2014that is, there exists \u03b8 in [0; 2\u03c0] such that \u03d5\u03030(z) = [cos(\u03b8), sin(\u03b8)]. The resulting kernel K is exactly the kernel descriptor introduced in [2, 3] for natural image patches.\nPatch map. In that setting, \u03d50 associates to a location z an image patch of size m \u00d7m centered at z. Then, the space H0 is simply Rm\u00d7m, and \u03d5\u03030(z) is a contrast-normalized version of the patch, which is a useful transformation for visual recognition according to classical findings in computer vision [19]. When the image is encoded with three color channels, patches are of size m\u00d7m\u00d7 3. We now define the multilayer convolutional kernel, generalizing some ideas of [2]. Definition 3 (Multilayer Convolutional Kernel). Let us consider a set \u2126k\u22121 \u2286 Rd and a Hilbert space Hk\u22121. We build a new set \u2126k and a new Hilbert space Hk as follows: (i) choose a patch shape Pk defined as a bounded symmetric subset of [0, 1]d, and a set of coordinates \u2126k such that for all zk in \u2126k, the patch {zk}+ Pk is a subset of \u2126k\u22121;3\n(ii) define the convolutional kernel Kk on the \u201cpatch\u201d feature maps Pk \u2192 Hk\u22121, by replacing in (1), \u2126 by Pk, H by Hk\u22121, and \u03c3, \u03b2 by appropriate smoothing parameters \u03c3k, \u03b2k. We denote by Hk the Hilbert space for which Kk is a reproducing kernel. An image represented by a feature map \u03d5k\u22121 : \u2126k\u22121 \u2192 Hk\u22121 at layer k\u22121 is now encoded in the k-th layer as \u03d5k : \u2126k \u2192 Hk, where for all zk in \u2126k, \u03d5k(zk) is the representation in Hk of the patch feature map z 7\u2192 \u03d5k\u22121(zk + z) for z in Pk. Concretely, the kernel Kk between two patches of \u03d5k and \u03d5\u2032k at respective locations zk and z \u2032 k is\n\u2211\nz\u2208Pk\n\u2211\nz\u2032\u2208Pk\n\u2016\u03d5k(zk + z)\u2016Hk \u2016\u03d5 \u2032 k(z \u2032 k + z \u2032)\u2016Hk e \u2212 1 2\u03b22 k \u2016z\u2212z\u2032\u20162 2e \u2212 1 2\u03c32 k \u2016\u03d5\u0303k(zk+z)\u2212\u03d5\u0303\u2032k(z\u2032k+z\u2032)\u20162Hk . (2)\nIn Figure 1(a), we illustrate the interactions between the different sets of coordinates\u2126k, patches Pk, and feature spaces Hk across layers. For two-dimensional grids, a typical patch shape is a square, for example P := {\u22121/n, 0, 1/n} \u00d7 {\u22121/n, 0, 1/n} for a 3 \u00d7 3 patch in an image of size n \u00d7 n. Information encoded in the k-th layer differs from the (k\u2212 1)-th one in two aspects: first, each point \u03d5k(zk) in layer k contains information about several points from the (k\u22121)-th layer and can possibly represent more complex patterns; second, the new feature map is more shift-invariant than the previous one due to the term involving the parameter \u03b2k in Eq. (2).\nThe multilayer convolutional kernel slightly differs from the hierarchical kernel descriptors of [2] but exploits similar ideas. Bo et al. [2] define indeed several ad hoc kernels for representing local information in images, such as gradient, color, or shape. These kernels are close to the one defined in (1) but with a few variations. Some of them do not use normalized features \u03d5\u0303(z), and these kernels use different weighting strategies for the summands of (1) that are specialized to the image modality, e.g., color, or gradient, whereas we use the same weight \u2016\u03d5(z)\u2016H \u2016\u03d5\u2032(z\u2032)\u2016H for all kernels. We propose instead the generic formulation (1), which appears to perform well in practice. We believe that such a generalization is useful per se, but our main contribution comes in the next section, where we use the kernel as a new tool for learning invariant convolutional neural networks.\n2When \u2126 is not discrete, the notation \u2211 in (1) should be replaced by the Lebesgue integral \u222b\nin the paper. 3For two sets A and B, the Minkowski sum A+B is defined as {a + b : a \u2208 A, b \u2208 B}."}, {"heading": "3 Training Invariant Convolutional Kernel Networks", "text": "We now show that a natural approximation scheme for the multilayer convolutional kernel gives rise to a convolutional neural network. In other words, the approximation can be achieved with a sequence of spatial convolutions with learned filters, non-linearities, and pooling operations.\nSeveral schemes have been proposed for approximating a non-linear kernel with a linear one, such as the Nystro\u0308m method and its variants [5, 30], which consist of projecting the data onto a finitedimensional subspace of the Hilbert space for which the kernel is reproducing. Random sampling techniques in the Fourier domain for shift-invariant kernels are also popular [24]. Being able to linearly approximate convolutional kernels is critical because computing the full kernel matrix on a database of images is computationally infeasible, even for a moderate number of images (\u2248 10 000) and moderate number of layers. For this reason, Bo et al. [2] use the Nystro\u0308m method for their hierarchical kernel descriptors. In our paper, we show that a particular convolutional neural network fits well the structure of the convolutional kernel, leading to an approach enjoying classical benefits of CNNs such as efficient prediction at test time."}, {"heading": "3.1 Fast Approximation of the Gaussian Kernel", "text": "A recurrent component of our formulation is the Gaussian kernel. In this section, we show that an approximation scheme involves a linear operation with learned filters followed by a pointwise non-linearity. Our starting point is the next lemma, which can be obtained after a simple calculation.\nLemma 1 (Expansion of the Gaussian Kernel). For all x and x\u2032 in Rm, and \u03c3 > 0,\ne\u2212 1 2\u03c32 \u2016x\u2212x\u2032\u201622 =\n(\n2\n\u03c0\u03c32\n) m 2 \u222b\nw\u2208Rm e\u2212\n1 \u03c32 \u2016x\u2212w\u201622e\u2212 1 \u03c32 \u2016x\u2032\u2212w\u201622dw. (3)\nThe lemma gives us an infinite-dimensional linear representation [ \u221a Ce\u2212(1/\u03c3\n2)\u2016x\u2212w\u201622 ]w\u2208Rm for all x in Rm, where C is the constant in front of the integral. To obtain a finite-dimensional representation, we need to approximate the integral with a weighted finite sum, which is a classical problem in numerical analysis and statistics (see [28] and chapter 8 of [6] for a review). We choose instead a data-driven approach, and consider two different scenarios:\nSmall dimension, m \u2264 2. When the data lives in a compact set of Rm, the integral in (3) can be approximated by uniform sampling over a large enough set. We choose such a strategy for two types of kernels from Eq. (1): (i) the spatial kernels e\u2212(1/2\u03b2 2)\u2016z\u2212z\u2032\u201622 ; (ii) the term e\u2212(1/2\u03c3 2)\u2016\u03d5\u0303(z)\u2212\u03d5\u0303(z)\u2032\u20162 H when \u03d5 is the \u201cgradient map\u201d presented in Section 2. In the latter case, H = R2 and \u03d5\u0303(z) is the gradient orientation. We typically sample a few orientations as explained in Section 3.4.\nHigher dimensions. When the dimension is high, uniform sampling suffers from the curse of dimensionality. Therefore, we choose to leverage the intrinsic low-dimensionality of the data. We learn importance weights \u03b7 = [\u03b7l] p l=1 in R p + and sampling points W = [wl] m l=1 in R\nm\u00d7p on training data x1, . . . ,xn in Rm with the following data-driven non-convex formulation\nmin \u03b7\u2208Rp\n+ ,W\u2208Rm\u00d7p\n\n\n1\nn2\n\u2211\n(i,j)\n(\ne\u2212 1 2\u03c32 \u2016xi\u2212xj\u2016 2 2 \u2212\np \u2211\nl=1\n\u03b7le \u2212 1 \u03c32 \u2016xi\u2212wl\u2016 2 2e\u2212 1 \u03c32 \u2016xj\u2212wl\u2016 2 2\n)2 \n . (4)\nThe resulting representation of a new data point x is simply the vector [ \u221a \u03b7le \u2212(1/\u03c32)\u2016x\u2212wl\u2016 2 2 ]pl=1 in Rp. We use the method (4) for approximating the Gaussian kernels e\u2212(1/2\u03c3 2)\u2016\u03d5\u0303(z)\u2212\u03d5\u0303\u2032(z\u2032)\u20162\nH in (1), by proceeding as follows: first, we assume that we already have a finite-dimensional approximation of \u03d5\u0303(z)\u2014say, of dimension m\u2014which we denote by \u03c8\u0303(z); second, we apply (4) to the kernel e\u2212(1/2\u03c3 2)\u2016\u03c8\u0303(z)\u2212\u03c8\u0303\u2032(z\u2032)\u201622 by using a database of features \u03c8\u0303(z) in Rm obtained from training data.\nWe can now start relating our approach to neural networks. After learning the parameters W and \u03b7, computing the finite-dimensional approximation in H of a unit-norm vector x only involves a linear operation followed by a non-linearity. Indeed, the quantities e\u2212(1/\u03c3 2)\u2016x\u2212wl\u2016 2 2 can be written as fl(w \u22a4 l x), where fl is the real-valued function u 7\u2192 e\u2212(1+\u2016wl\u2016 2 2)/\u03c3\n2+2u/\u03c32 . Since we apply (4) to normalized data \u03c8\u0303(z), we expect the \u21132-norm of the sampling points wl to to be close to one, and the functions fl to have the form: u 7\u2192 e(2/\u03c3\n2)(u\u22121) for u = w\u22a4l x in [\u22121, 1]. In Figure 2, we show that we obtain a shape resembling the \u201crectified linear unit\u201d function used in neural networks [29]."}, {"heading": "3.2 Approximating the Single-Layer Convolutional Kernel", "text": "With the methodology presented in the previous section, we now introduce an approximation scheme for the convolutional kernel K(\u03d5, \u03d5\u2032) from Definition 2. We proceed as follows:\n(i) we assume that for all \u03d5(z) and \u03d5\u2032(z\u2032), we already know some finite-dimensional approximations \u03c8(z) and \u03c8\u2032(z\u2032) in Rm;\n(ii) the kernels e\u2212(1/2\u03b2 2)\u2016z\u2212z\u2032\u201622 are approximated by uniform sampling: we define a set of equally\nspaced points N , such that e\u2212(1/2\u03b22)\u2016z\u2212z\u2032\u201622 \u2248 C\u2032 \u2211 c\u2208N e \u2212(1/\u03b22)\u2016z\u2212c\u201622e\u2212(1/\u03b2 2)\u2016z\u2032\u2212c\u201622 , and C\u2032 is a constant independent of z and z\u2032.\n(iii) we learn some weights \u03b7 in Rp+ and sampling points W = [w1, . . . ,wp] in R m\u00d7p to approxi-\nmate the kernels e\u2212(1/2\u03c3 2)\u2016\u03d5\u0303(z)\u2212\u03d5\u0303\u2032(z\u2032)\u20162 H following the approach of the previous section.\nNote that all parameter choices, such as N , \u03b2, and \u03c3, are discussed in Section 3.4. By plugging the above approximations in Eq. (1) and interchanging the sums, we get\nK(\u03d5, \u03d5\u2032) \u2248 C\u2032 \u2211\nc\u2208N\np \u2211\nl=1\n[\n\u2211\nz\u2208\u2126\n\u03b6l(z)e\u2212(1/\u03b2 2)\u2016c\u2212z\u201622\n][\n\u2211\nz\u2032\u2208\u2126\n\u03b6l(z\u2032)e\u2212(1/\u03b2 2)\u2016c\u2212z\u2032\u201622\n]\n, (5)\nwhere we have introduced the quantity\n\u03b6l(z) := \u2016\u03c8(z)\u20162 \u221a \u03b7le \u2212(1/\u03c32)\u2016\u03c8\u0303(z)\u2212wl\u2016 2 2 ,\nand also, by analogy, the quantity \u03b6l\u2032(z\u2032) for \u03d5\u2032. Each function \u03b6l : \u2126 \u2192 R can be interpreted as a spatial map, where \u03b6l(z) represents a non-linear filter response involving the non-linear functions\npresented in Figure 2. According to (5), we need to subsample these maps \u03b6l at some points N after convolving them with the two-dimensional filter z 7\u2192 \u221a C\u2032e\u2212(1/\u03b2\n2)\u2016z\u201622 , leading to a new set of p maps denoted by \u03bel : N \u2192 R. In the same way, a set of p maps \u03bel\u2032 are obtained for \u03d5\u2032, and\nK(\u03d5, \u03d5\u2032) \u2248 \u2211\nc\u2208N\np \u2211\nl=1\n\u03bel(c)\u03bel\u2032(c\u2032). (6)\nThe maps \u03b6l and \u03bel coincide with the terminology of \u201cfeature maps\u201d from neural networks. Building the maps \u03bel from \u03b6l is achieved by Gaussian filtering and subsampling at predefined points c in N , which is also called a \u201clinear pooling step\u201d in neural networks, and \u201cdownsampling\u201d with a Gaussian anti-aliasing filter in signal processing. In the next section, we show that approximating the multilayer convolutional kernel can be achieved similarly, and leads to a particular CNN."}, {"heading": "3.3 Convolutional Kernel Networks", "text": "We have seen that for approximating the single-layer convolutional kernel on a feature map \u03d50 : \u21260 \u2192 H0 where H0 is the Euclidean space Rm0 , we need to (i) compute the linear responses \u03d5\u03030(z) \u22a4 wl; (ii) apply a pointwise non-linearity to obtain the maps \u03b6l; (iii) perform linear pooling with Gaussian filtering to obtain new maps \u03bel. When in addition \u03d50 is the \u201cpatch map\u201d defined in Section 2, the vectors wl can be interpreted as spatial filters, and the dot products \u03d5\u03030(z)\u22a4wl as convolutions. As a result, we have just described a one-layer convolutional neural network.\nWhen the kernel involves several layers \u03d50, . . . , \u03d5k, as in Definition 3, the methodology of the previous section allows us to build a linear approximation of the kernel K(\u03d5k, \u03d5\u2032k) under one condition: we need to know in advance finite-dimensional approximations \u03c8k(zk) of \u03d5k(zk) for all zk in \u2126k. We assume that such a condition is always satisfied for H0, which we choose finite-dimensional. Therefore, it remains to show how to build \u03c8k(zk) when the condition holds for k \u2212 1. From Definition 3, \u03d5k(zk) in Hk is the representation derived from the kernel Kk presented in Eq. (2) of the patch feature map z 7\u2192 \u03d5k\u22121(zk + z). To achieve our goal, we apply the methodology of the previous section to Kk, by replacing in (5) \u2126 by Pk, \u03b2 by \u03b2k, \u03c3 by \u03c3k, p by pk, and N by Nk. Then, we obtain an approximation \u03c8k(zk) of dimension mk = |Nk|pk. Note that since the patches may overlap, computing the vectors \u03c8k(zk) can be done efficiently for a given image represented by \u03d5k\u22121 : \u2126k\u22121 \u2192 Hk\u22121. As in the previous section, let us define the maps \u03b6lk : \u2126k\u22121 \u2192 R as \u03b6lk(z) := \u2016\u03c8k\u22121(z)\u20162 \u221a \u03b7le \u2212(1/\u03c32k)\u2016\u03c8\u0303k\u22121(z)\u2212wl\u2016 2 2 for all z in \u2126k\u22121 and l = 1, . . . , pk. Given the maps \u03b6lk, all vectors \u03c8k(zk) can be computed in O(|\u2126k||Nk||Pk|pk) operations, whereas the maps \u03b6lk require storing O(|\u2126k\u22121|pk) real values. At this point, we have obtained a computationally feasible approximation scheme that is fairly general regarding the choice of domains \u2126k and patches Pk. However, a last approximation allows us to fill in the gap between our approach and multilayer CNNs under a few conditions: (i) \u2126k\u22121 and \u2126k are grids; (ii) Pk is a square with same spacing as \u2126k\u22121; (iii) Nk is a square containing 0 with same spacing as \u2126k; Then, we can build the maps \u03bekl as in the previous section, by convolving the maps \u03b6lk with the Gaussian filter z 7\u2192 \u221a C\u2032e\u2212(1/\u03b2\n2)\u2016z\u201622 , followed by subsampling with the same spacing as \u2126k. Such a construction is illustrated in Figure 1(b). As a result, a |Nk|pk-dimensional approximation of \u03d5k(zk) is simply the pk \u00d7Nk patch [\u03belk(zk + z)]l,z\u2208Nk . Consequently, obtaining the desired approximation\u03c8k(zk) requires extracting a patch from pk maps of size O(|\u2126k|), and the dot products w\u22a4l \u03c8k(zk) are spatial convolutions on the \u201cpooled\u201d maps \u03be l k . We have now obtained the convolutional kernel network displayed in Figure 1(b)."}, {"heading": "3.4 Practical Implementation: Parameter Setting and Optimization", "text": "The parameters that our convolutional kernel network requires are the following quantities: N (number of layers), \u03b2k, \u03c3k (smoothing parameters); pk (number of filters per layer), Nk (to obtain \u03c8k(zk) from the maps \u03belk), the downsampling factor between \u03be l k and \u03b6 l k, and the initial maps \u03b6 l 0. Other parameters appearing in the original kernel formulation of Section 2, such as Pk, \u2126k, do not need to be chosen since they are implicitly linked with the ones we have just listed.\nFirst, N , pk, Nk, and the downsampling factors are classical parameters of CNNs, which are left to the discretion of the user. The sets Nk represent patches on the maps \u03belk, as shown in Figure 1(b),\nand are typically small. We tried the sizes m\u00d7m with m = 3, 4, 5 for the first layer, and m = 2, 3 for the upper ones. The number of filters pk in our experiments is in the set {50, 100, 200, 400, 800}. The downsampling factor is always chosen to be 2 between two consecutive layers, whereas the last layer is downsampled to produce final maps \u03belN of a small size\u2014say, 5\u00d7 5 or 4\u00d7 4. The initial map \u03b6l0 also needs to be set up. When using the \u201cpatch map\u201d of Section 2, the maps \u03b6 l 0 are simply the input image and N1 defines the patch size. For the \u201cgradient map\u201d, we have p0 = 2, the maps \u03b6l0 carry the image gradient, and we set N1 = {0}, representing a 1\u00d7 1 patch. Finally, the parameters \u03b2k are chosen according to signal processing principles. The purpose of the filter z \u21927\u2192 e\u2212(1/\u03b22)\u2016z\u201622 is indeed to remove high frequencies from the maps \u03b6lk before subsampling. When downsampling by a factor \u03ba, we simply choose \u03b2 to be \u03ba times the spacing of two pixels on \u03b6lk. Similarly, the parameter \u03c31 when sampling p1 orientations for the \u201cpatch maps\u201d is set to 2\u03c0/p1, where p1 = 12 in all our experiments. The parameter \u03c3k in (4) is chosen according to a heuristic rule. We set \u03c3k to the 0.1 quantile of the training data \u2016\u03c8k\u22121(z)\u2212 \u03c8\u2032k\u22121(z\u2032)\u20162. Regarding the optimization problem (4), we learn the parameters W,\u03b7 of the layers from bottom to top. Stochastic gradient descent (SGD) is a natural candidate for this task since an enormous amount of training data is available. For the purpose of this paper, we have preferred to use L-BFGS-B [9] on 300 000 pairs of randomly selected training data points, and initializing the weights W with the K-means algorithm. L-BFGS-B is a parameter-free state-of-the-art batch method, which is probably not as fast as SGD, but which is much easier to use. Our goal was to demonstrate the performance of a new type of convolutional network, and we leave as future work any speed improvement."}, {"heading": "4 Experiments", "text": "In this section, we present experiments on visual recognition tasks, and start by visualizing the filters we learn on natural image patches. All experiments were performed using a Matlab implementation, which will be made available upon publication of the paper, and an L-BFGS-B solver [9], interfaced in Matlab by Stephen Becker. Since our work is not focused on speed, we run the L-BFGS-B algorithm for 4 000 iterations, which seems to ensure convergence of the objective function to a stationary point. The image representations obtained with our convolutional kernel network, represented by the last map \u03bek, are used in a linear support vector machine. We use the software package LibLinear [16] modified to handle dense matrices. The image representations are centered, rescaled to have unit \u21132-norm on average, and the parameter C of the SVM is always selected on a validation set or by 5-fold cross-validation in the range 2i, i = \u221215 . . . , 15."}, {"heading": "4.1 Discovering the Structure of Natural Image Patches", "text": "Unsupervised learning was first used for discovering the underlying structure of natural image patches by Olshausen and Field [23]. Without making any a priori assumption about the data except a parsimony principle, the method is able to produce small prototypes that resemble Gabor wavelets\u2014that is, spatially localized oriented basis functions. The results were found impressive by the scientific community and their work received substantial attention. It is also known that such results can also be achieved with CNNs [25]. We show in this section that this is also the case for convolutional kernel networks, even though they are not explicitly trained to reconstruct data.\nFollowing [23], we randomly select a database of 300 000 whitened natural image patches of size 12\u00d7 12 and learn p = 256 filters W using the formulation (4). We initialize W with Gaussian random noise without performing the K-means step, in order to ensure that the output we obtain is not an artifact of the initialization. In Figure 3, we display the filters associated to the top-128 largest weights \u03b7l. Among the 256 filters, 197 exhibit interpretable Gabor-like structures and the rest was less interpretable. To the best of our knowledge, this is the first time that the explicit kernel map of the Gaussian kernel for whitened natural image patches is shown to be related to Gabor wavelets."}, {"heading": "4.2 Digit Classification on MNIST", "text": "The MNIST dataset [22] consists of 60 000 images of handwritten digits for training and 10 000 for testing. We use two types of initial maps in our networks: the \u201cpatch map\u201d, denoted by CNK-PM and the \u201cgradient map\u201d, denoted by CNK-GM. We follow the evaluation methodology of [25] for comparison when varying the training set size. We follow Section 3.4 for building the networks. We select the SVM parameter by 5-fold cross validation when the training size is smaller than 20 000, or otherwise, we keep 10 0000 examples from the training set for validation. We report in Table 1 the results obtained for four simple architectures. CKN-GM1 is the simplest one: its second layer uses 3\u00d7 3 patches and only p2 = 50 filters, resulting in a network with 5 400 parameters. Yet, it achieves an outstanding performance of 0.58% error on the full dataset. The best performing, CKN-GM2, is similar to CKN-GM1 but uses p2 = 400 filters. When working with raw patches, two layers (CKN-PM2) gives better results than one layer. More details about the network architectures are provided in the supplementary material. In general, our method achieves a state-of-the-art accuracy for this task since lower error rates have only been reported by using data augmentation [11]."}, {"heading": "4.3 Visual Recognition on CIFAR-10 and STL-10", "text": "We now move to the more challenging datasets CIFAR-10 [20] and STL-10 [13]. We again follow Section 3.4 for choosing all parameters. We select the best architectures on a validation set of 10 000 examples from the training set for CIFAR-10, and by 5-fold cross-validation on STL-10 since the latter only contains 100 training examples per class. We report the results for CKN-GM, defined in the previous section, without exploiting color information, and CKN-PM when working on raw RGB patches whose mean color is subtracted. The best selected models have always two layers. with 800 filters for the top layer. Since CKN-PM and CKN-GM exploit a different information, we also report a combination of such two models, CKN-CO, by concatenating normalized image representations together. The standard deviations for STL-10, obtained using the 10 training folds, was always below 0.7% for our results. Our approach appears to be competitive with the state of the art, especially on STL-10 where only one method does better than ours, despite the fact that our models only use 2 layers and require learning few parameters (see supplementary material)."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a new methodology for combining kernels and convolutional neural networks. We show that mixing the ideas of these two concepts is fruitful, since we achieve near state-of-the-art performance on several datasets such as MNIST, CIFAR-10, and STL10, with simple architectures and no data augmentation. Some challenges regarding our work are left open for the future. The first one is the use of supervision to better approximate the kernel for the prediction task. The second consists in leveraging the kernel interpretation of our convolutional neural networks to better understand the theoretical properties of the feature spaces that these networks produce."}, {"heading": "A List of Architectures Reported in the Experiments", "text": "We present in details the architectures used in the paper in Table 3."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Object recognition with hierarchical kernel descriptors", "author": ["L. Bo", "K. Lai", "X. Ren", "D. Fox"], "venue": "Proc. CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel descriptors for visual recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "Adv. NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised feature learning for RGB-D based object recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "Experimental Robotics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient match kernel between sets of features for visual recognition", "author": ["L. Bo", "C. Sminchisescu"], "venue": "Adv. NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-Scale Kernel Machines (Neural Information Processing)", "author": ["L. Bottou", "O. Chapelle", "D. DeCoste", "J. Weston"], "venue": "The MIT Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "On invariance in hierarchical models", "author": ["J.V. Bouvrie", "L. Rosasco", "T. Poggio"], "venue": "Adv. NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE T. Pattern Anal., 35(8):1872\u2013 1886", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM J. Sci. Comput., 16(5):1190\u20131208", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Large-margin classification in infinite neural networks", "author": ["Y. Cho", "L.K. Saul"], "venue": "Neural Comput., 22(10)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Proc. CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "Adv. NIPS", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "Proc. AISTATS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Training invariant support vector machines", "author": ["D. Decoste", "B. Sch\u00f6lkopf"], "venue": "Mach. Learn., 46(1-3):161\u2013 190", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "preprint arXiv:1310.1531", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Mach. Learn. Res., 9:1871\u20131874", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "Adv. NIPS", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "Proc. ICML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "What is the best multi-stage architecture for object recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "ICCV", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Tech. Rep.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Adv. NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "P. IEEE, 86(11):2278\u20132324", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Nature, 381(6583):607\u2013609", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Adv. NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.-J. Huang", "Y-L. Boureau", "Y. LeCun"], "venue": "Proc. CVPR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "Proc. ICML", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Spline models for observational data", "author": ["G. Wahba"], "venue": "SIAM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1990}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M.D. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "Proc. ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "Adv. NIPS", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "preprint arXiv:1301.3557", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "preprint arXiv:1311.2901v3", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].", "startOffset": 158, "endOffset": 170}, {"referenceID": 20, "context": "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].", "startOffset": 158, "endOffset": 170}, {"referenceID": 28, "context": "We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22], due to their success in large-scale visual recognition tasks [15, 21, 29].", "startOffset": 158, "endOffset": 170}, {"referenceID": 31, "context": "The resulting image representation seems to achieve some invariance to image perturbations and to encode complex visual patterns [32].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": ", GPUs, and appropriate regularization techniques [18, 21, 29].", "startOffset": 50, "endOffset": 62}, {"referenceID": 20, "context": ", GPUs, and appropriate regularization techniques [18, 21, 29].", "startOffset": 50, "endOffset": 62}, {"referenceID": 28, "context": ", GPUs, and appropriate regularization techniques [18, 21, 29].", "startOffset": 50, "endOffset": 62}, {"referenceID": 7, "context": "Only recently, the invariance of some related architectures has been characterized; this is the case for the wavelet scattering transform [8], or the hierarchical models of [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 6, "context": "Only recently, the invariance of some related architectures has been characterized; this is the case for the wavelet scattering transform [8], or the hierarchical models of [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 25, "context": "We use indeed kernels [26], which are natural tools to model invariance [14].", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "We use indeed kernels [26], which are natural tools to model invariance [14].", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "Inspired by the hierarchical kernel descriptors of [2], we propose a reproducing kernel that produces invariant multi-layer image representations.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Interestingly, they resemble rectified linear units [1, 29], even though they naturally emerge from the approximation scheme and were not handcrafted.", "startOffset": 52, "endOffset": 59}, {"referenceID": 28, "context": "Interestingly, they resemble rectified linear units [1, 29], even though they naturally emerge from the approximation scheme and were not handcrafted.", "startOffset": 52, "endOffset": 59}, {"referenceID": 21, "context": "Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation.", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation.", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "Yet, we achieve near state-of-the-art results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with simple architectures, few parameters to learn, and no data augmentation.", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "Kernels for building deep large-margin classifiers have been introduced in [10].", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Kernels that enjoy invariance properties for visual recognition were proposed in [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "In contrast to [7], we propagate information up, from lower to upper layers, by using sequences of convolutions.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "The kernels proposed in [2, 3] produce multilayer image representations for visual recognition tasks.", "startOffset": 24, "endOffset": 30}, {"referenceID": 2, "context": "The kernels proposed in [2, 3] produce multilayer image representations for visual recognition tasks.", "startOffset": 24, "endOffset": 30}, {"referenceID": 1, "context": "The convolutional multilayer kernel is a generalization of the hierarchical kernel descriptors introduced in computer vision [2, 3].", "startOffset": 125, "endOffset": 131}, {"referenceID": 2, "context": "The convolutional multilayer kernel is a generalization of the hierarchical kernel descriptors introduced in computer vision [2, 3].", "startOffset": 125, "endOffset": 131}, {"referenceID": 0, "context": "An image feature map \u03c6 is a function \u03c6 : \u03a9 \u2192 H, where \u03a9 is a (usually discrete) subset of [0, 1] representing \u201ccoordinates\u201d in the image and H is a Hilbert space.", "startOffset": 90, "endOffset": 96}, {"referenceID": 25, "context": "In the kernel literature, \u201cfeature map\u201d denotes the mapping between data points and their representation in a RKHS [26].", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "Here, feature maps refer to spatial maps, as usual in the neural network literature [22].", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "It is easy to see that K is a positive definite kernel since it only involves sums and products of positive definite kernels [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "The resulting kernel K is exactly the kernel descriptor introduced in [2, 3] for natural image patches.", "startOffset": 70, "endOffset": 76}, {"referenceID": 2, "context": "The resulting kernel K is exactly the kernel descriptor introduced in [2, 3] for natural image patches.", "startOffset": 70, "endOffset": 76}, {"referenceID": 18, "context": "Then, the space H0 is simply R, and \u03c6\u03030(z) is a contrast-normalized version of the patch, which is a useful transformation for visual recognition according to classical findings in computer vision [19].", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "We now define the multilayer convolutional kernel, generalizing some ideas of [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "We build a new set \u03a9k and a new Hilbert space Hk as follows: (i) choose a patch shape Pk defined as a bounded symmetric subset of [0, 1], and a set of coordinates \u03a9k such that for all zk in \u03a9k, the patch {zk}+ Pk is a subset of \u03a9k\u22121; (ii) define the convolutional kernel Kk on the \u201cpatch\u201d feature maps Pk \u2192 Hk\u22121, by replacing in (1), \u03a9 by Pk, H by Hk\u22121, and \u03c3, \u03b2 by appropriate smoothing parameters \u03c3k, \u03b2k.", "startOffset": 130, "endOffset": 136}, {"referenceID": 1, "context": "The multilayer convolutional kernel slightly differs from the hierarchical kernel descriptors of [2] but exploits similar ideas.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "[2] define indeed several ad hoc kernels for representing local information in images, such as gradient, color, or shape.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Several schemes have been proposed for approximating a non-linear kernel with a linear one, such as the Nystr\u00f6m method and its variants [5, 30], which consist of projecting the data onto a finitedimensional subspace of the Hilbert space for which the kernel is reproducing.", "startOffset": 136, "endOffset": 143}, {"referenceID": 29, "context": "Several schemes have been proposed for approximating a non-linear kernel with a linear one, such as the Nystr\u00f6m method and its variants [5, 30], which consist of projecting the data onto a finitedimensional subspace of the Hilbert space for which the kernel is reproducing.", "startOffset": 136, "endOffset": 143}, {"referenceID": 23, "context": "Random sampling techniques in the Fourier domain for shift-invariant kernels are also popular [24].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "[2] use the Nystr\u00f6m method for their hierarchical kernel descriptors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "To obtain a finite-dimensional representation, we need to approximate the integral with a weighted finite sum, which is a classical problem in numerical analysis and statistics (see [28] and chapter 8 of [6] for a review).", "startOffset": 182, "endOffset": 186}, {"referenceID": 5, "context": "To obtain a finite-dimensional representation, we need to approximate the integral with a weighted finite sum, which is a classical problem in numerical analysis and statistics (see [28] and chapter 8 of [6] for a review).", "startOffset": 204, "endOffset": 207}, {"referenceID": 28, "context": "In Figure 2, we show that we obtain a shape resembling the \u201crectified linear unit\u201d function used in neural networks [29].", "startOffset": 116, "endOffset": 120}, {"referenceID": 28, "context": "Figure 2: In red, we plot the function u 7\u2192 max(u, 0) often called \u201crectified linear unit\u201d [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "For the purpose of this paper, we have preferred to use L-BFGS-B [9] on 300 000 pairs of randomly selected training data points, and initializing the weights W with the K-means algorithm.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "All experiments were performed using a Matlab implementation, which will be made available upon publication of the paper, and an L-BFGS-B solver [9], interfaced in Matlab by Stephen Becker.", "startOffset": 145, "endOffset": 148}, {"referenceID": 15, "context": "We use the software package LibLinear [16] modified to handle dense matrices.", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "Unsupervised learning was first used for discovering the underlying structure of natural image patches by Olshausen and Field [23].", "startOffset": 126, "endOffset": 130}, {"referenceID": 24, "context": "It is also known that such results can also be achieved with CNNs [25].", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "Following [23], we randomly select a database of 300 000 whitened natural image patches of size 12\u00d7 12 and learn p = 256 filters W using the formulation (4).", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "CNN Scat-1 Scat-2 CKN-GM1 CKN-GM2 CKN-PM1 CKN-PM2 [31] [18] [19] size [25] [8] [8] (12/50) (12/400) (200) (50/200) 300 7.", "startOffset": 79, "endOffset": 82}, {"referenceID": 21, "context": "The MNIST dataset [22] consists of 60 000 images of handwritten digits for training and 10 000 for testing.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "We follow the evaluation methodology of [25] for comparison when varying the training set size.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "In general, our method achieves a state-of-the-art accuracy for this task since lower error rates have only been reported by using data augmentation [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "We now move to the more challenging datasets CIFAR-10 [20] and STL-10 [13].", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "We now move to the more challenging datasets CIFAR-10 [20] and STL-10 [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.", "startOffset": 7, "endOffset": 11}, {"referenceID": 26, "context": "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.", "startOffset": 27, "endOffset": 30}, {"referenceID": 16, "context": "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "Method [12] [27] [18] [13] [4] [17] [31] CKN-GM CKN-PM CKN-CO CIFAR-10 82.", "startOffset": 36, "endOffset": 40}], "year": 2017, "abstractText": "An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.", "creator": "LaTeX with hyperref package"}}}