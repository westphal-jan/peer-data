{"id": "1406.3407", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2014", "title": "Restricted Boltzmann Machine for Classification with Hierarchical Correlated Prior", "abstract": "Restricted ballardini Boltzmann legionella machines (RBM) turfi and adelsheim its variants masci have hatami become catchall hot tasmanian research hurac\u00e1n topics lokhandwala recently, rohrabacher and widely applied to many machover classification problems, such 150-meter as 255,000 character threepwood recognition tsukai and document sailboat categorization. Often, crofters classification romao RBM alexandru ignores the vnpt interclass relationship or aneirin prior knowledge frese of 42,500 sharing information plowman among euro482 classes. In gap.com this brainwaves paper, huneker we are ve interested in RBM hassanpour with the hierarchical prior over classes. sub-humid We assume parameters aloofness for corallo nearby fakery nodes scarpe are hydranautics correlated in hah the hierarchical rafidha tree, raouhi and further the cromford parameters at 0.100 each senoi node loevy of razavi the 500-foot tree be affluents orthogonal 10b5-1 to those olton at bouwerie its hurford ancestors. We lansford propose w/w a hierarchical ccra correlated 4,656 RBM for selectmen classification audace problem, mentoring which lkaplow@coxnews.com generalizes the chocolates classification kexin RBM with nanu sharing information among advancements different classes. adelin In order reculver to reduce the redundancy between fieldworkers node parameters in mayon the 3.10 hierarchy, borbon we duim also paszkowski introduce 55.49 orthogonal restrictions to our objective function. We lunette test our specifiers method smartmedia on challenge datasets, and show promising 2-1/2 results songer compared eateries to competitive l-tyrosine baselines.", "histories": [["v1", "Fri, 13 Jun 2014 02:19:26 GMT  (760kb,D)", "https://arxiv.org/abs/1406.3407v1", "13 pages, 5 figures"], ["v2", "Mon, 20 Apr 2015 18:39:18 GMT  (583kb,D)", "http://arxiv.org/abs/1406.3407v2", "13 pages, 5 figures"]], "COMMENTS": "13 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen", "sargur h srihari"], "accepted": true, "id": "1406.3407"}, "pdf": {"name": "1406.3407.pdf", "metadata": {"source": "CRF", "title": "CATION WITH HIERARCHICAL CORRELATED PRIOR", "authors": ["Gang Chen", "Sargur N. Srihari"], "emails": ["gangchen@buffalo.edu,", "srihari@cedar.buffalo.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Restricted Boltzmann machines (RBM) Hinton (2002) are a specific neural network with no hiddenhidden and visible-visible connections. They have attracted significant attention recently on many machine learning problems, such as dimension reduction Hinton & Salakhutdinov (2006), text categorization Larochelle et al. (2012), collaborative filtering Salakhutdinov et al. (2007) and object recognition Krizhevsky et al. (2012). A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as flat and little work has been done to explore the interclass relationship.\nIn this paper, we generalize RBM with hierarchical prior for classification problems. Basically, we divide the classification RBM into traditional RBM for representation learning and multinomial logit model for classification, see Fig. 1(a) for intuitive understanding. For the traditional RBM (red in Fig. 1(a)), we can extend it into deep belief network (DBN), while for the multinomial logit model (green in Fig. 1(a)), we can incorporate the interclass relationship to it. In this work, we focus on the hierarchical prior over the classification RBM, and we take a similar strategy as corrMNL, that means we use sums of parameters along paths from root to a specific leaf in the tree as model parameters for hierarchical classification. However, we consider it in a rather different way from the previous work. We can think our method is a kind of mixture of corrMNL Shahbaba & Neal (2007) and the orthogonal SVM model Xiao et al. (2011). However, our model inherits the advantage of\nar X\niv :1\n40 6.\n34 07\nv2 [\ncs .L\nG ]\n2 0\nA pr\n2 01\n5\nRBM, which can learn the hidden representation for better classification Hinton & Salakhutdinov (2006); Larochelle et al. (2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al. (2011). Moreover, we only have a single RBM in our model, while there are multiple SVMs in the orthogonal hierarchical SVM Xiao et al. (2011).\nOur contributions are: (1) we introduce the hierarchical semantic prior over labels into restricted Boltzmann machine; (2) we add orthogonal constraints over adjacent layers in the hierarchy, which makes our model more robust for classification problems. We test our method in the experiments, and show comparative results over competitive baselines."}, {"heading": "2 CLASSIFICATION RESTRICTED BOLTZMANN MACHINE WITH HIERARCHICAL CORRELATED PRIOR", "text": "We will revisit the classification RBM, then we will introduce our model. Throughout the paper, matrix variables are denoted with bold uppercases, and vector quantities are written in bold lowercase. For matrix W, we indicate its i-th row and j-th column element as Wij , its i-th row vector Wi. and j-th column vector W.j . For different matrixes, we use different subscripts to discern them. For example, A12 and A21 are different matrixes, which are indicated by different subscripts."}, {"heading": "2.1 CLASSIFICATION RESTRICTED BOLTZMANN MACHINE", "text": "Denote X \u2208 Rd be an instance domain and Y be a set of labels. Assume that we have a training set D = {(xi, yi)}, comprising for the i-th pair: an input vector xi \u2208 X and a target class yi \u2208 Y , where xi \u2208 Rd and yi \u2208 {1, ...,K}. An RBM with n hidden units is a parametric model of the joint distribution between a layer of hidden variables h = (h1, ..., hn) and the observations x = (x1, ..., xd) and y.\nThe classification RBM was first proposed in Hinton (2007) and was further developed in Larochelle & Bengio (2008); Larochelle et al. (2012) with discriminative training model. The joint likelihood of the classification RBM takes the following form:\np(y,x,h) \u221d e\u2212E(y,x,h) (1)\nwhere the energy function is\nE(y,x,h) = \u2212hTWx\u2212 bTx\u2212 cTh\u2212 dTy \u2212 hTUy (2)\nwith parameters \u0398 = {W,b, c,d,U} and y = (1y=i)Ki=1 for K classes, where matrix W \u2208 Rn\u00d7d, and U \u2208 Rn\u00d7K . For classification problem, we need to compute the conditional probability for p(y|x). As shown in Salakhutdinov et al. (2007), this conditional distribution has explicit formula and can be calculated\nexactly, by writing it as follows: p(y|x) = edy \u220fn j=1 ( 1 + ecj+Ujy+ \u2211 iWijxi )\u2211 y\u2217 e dy\u2217 \u220fn j=1 ( 1 + ecj+Ujy\u2217+ \u2211 iWijxi\n) (3) To learn RBM parameters, we need to optimize the joint likelihood p(y,x) on training data D. Note that it is intractable to compute p(y,x), because it needs to model p(x). Fortunately, Hinton proposed an efficient stochastic descent method, namely contrastive divergence (CD) Hinton (2002) to maximize the joint likelihood. Thus, we get the following stochastic gradient updates for W and U from CD respectively\n\u2202logp(x, y)\n\u2202Wij = \u3008vihj\u3009data \u2212 \u3008vihj\u3009model\n\u2202logp(x, y)\n\u2202Ujk = \u3008hjyk\u3009data \u2212 \u3008hjyk\u3009model (4)\nAnd update \u0398 until convergence with gradient descent\n\u0398 = \u0398 + \u03b7 \u2202logp(x, y)\n\u2202\u0398 (5)\nwhere \u03b7 is the learning rate for the classification RBM."}, {"heading": "2.2 RESTRICTED BOLTZMANN MACHINE WITH HIERARCHICAL PRIOR", "text": "Our model introduces hierarchical prior over label sets for logistic regression classifier in the classification RBM. Note that we divide the classification RBM into two parts: RBM (feature learning) and multinomial logit model (classifier), corresponding to red and green regions shown in Fig. 1(a) respectively. Our model introduces the hierarchical prior over multinomial logit regression classifier, which is vital for classification problems under RBM framework.\nDefine the hierarchical tree T = (V, E), the number of node N = |V| and the number of edge M = |E|. Furthermore, we assume all parameters along edges are A = {A1, ..,Am}, where {Aj}mj=1 describes the parameter for each edge in the hierarchy respectively and Aj has the same size as U in the above subsection 2.1. For any node \u03bd in the tree, we denoteA(\u03bd) as its direct parent (vertex adjacent to v), and A(i)(\u03bd) to be its i-th ancestor of \u03bd. As in Dekel et al. (2004), we also define the path for each node \u03bd \u2208 T , define P (\u03bd) to be the set of nodes along the path from root to v, P (\u03bd) = {\u00b5 \u2208 T : \u2203i \u00b5 = A(i)(\u03bd)} (6) Now we can define the coefficient parameters for each leaf node \u03bd as\nA(\u03bd) = \u2211\n\u00b5\u2208P (\u03bd)\nA\u00b5 (7)\nwhere the classification coefficient for each class in Eq. (7) is decomposed into contributions along paths from root to the leaf associated to that class. For our model, each leaf node is associated to one class, which takes the same methodology as in Salakhutdinov et al. (2007). Fig. 1(b) is an example with total five classes, where the sums of parameters along the path to the leaf node are coefficient parameters used for classification. In Fig. 1(b), A12 and A13 are parameters along branches in the first level, and A21, A22, A31, A32 and A33 are parameters in the second level. For example, the coefficient parameter of class 1 is A12 + A21 according to Eq. (7); similarly, for class 4, its coefficient parameter is A13 + A32. For example, we can see class 1 and class 2 sharing the common term A12, which can be thought as the prior correlation between the parameters of nearby classes in the hierarchy.\nFor K classes, we have U \u2208 Rn\u00d7K and Aj \u2208 Rn\u00d7K for j = {1, ..,m}. Thus we can factorize\nU = VA (8)\nwhere A = {A1, ..,Am} \u2208 Rmn\u00d7K is the concatenation of parameters {Aj}mj=1 of all edges in the hierarchy, while V \u2208 Rn\u00d7mn implies the hierarchical prior over labels, refer Eq. (7) for construction of the correlated matrix V. Note that V (just) encodes the given hierarchical structures with 0 or 1 and is fixed during training the models. In addition, we introduce orthogonal restrictions\njust as in Xiao et al. (2011) to reduce redundancy between adjacent layers. Given a training set D = {(xi, yi)}, we propose the following objective function:\nL(D; \u0398) = \u2212 |D|\u2211 i=1 logp(yi,xi) + C \u2211 \u03bd,\u00b5\u2208P (\u03bd) trace(AT\u00b5A\u03bd) (9)\nwhere C is the weight to balance the two terms. The first term is from the negative log likelihood as in RBM and the second term forces parameters at children to be orthogonal to those at its ancestor as much as possible.\nThe differences between our model and RBM lie: (1) hierarchical prior over labels, which can induce correlation between the parameters of nearby nodes in the tree; (2) we have orthogonal regularization which can make our model more robust, and also reduce redundancy in model parameters. For parameters updating, we have the same equations as in the classification RBM, except for U which introduces hierarchical prior and orthogonal restrictions among children-parent pairs.\nAccording to chain rule, we can differenciate L(D; \u0398) r.w.t A\u03bd and get the following derivative\n\u2202L(D; \u0398) \u2202A\u03bd\n= \u2212 \u2202 \u2211|D| i=1 logp(yi,xi)\n\u2202U \u00b7 \u2202U \u2202A\u03bd\n+ C \u2211\n\u00b5\u2208P (\u03bd)\nA\u00b5 (10)\nNote that the derivative of \u2211|D| i=1 logp(yi,xi) w.r.t. U can be computed via Eq. (4). Thus, we can use Eq. (10) to calculate derivative w.r.t. A\u03bd , and then update A\u03bd with stochastic gradient descent. Given A\u03bd , we can use Eq. (8) to update U."}, {"heading": "2.3 ALGORITHM", "text": "Note that our model incorporates the hierarchical prior and orthogonal constraints through U. In other words, we can update all parameters with CD, except U. Because U is the function of A, we can compute the derivative of U w.r.t. A and update A with gradient descent. After we get A, we can calculate U, which can be used in the next iteration. We list the pseudo code below in Alg. 1.\nAlgorithm 1 Learning RBM with hierarchical correlated prior Input: training data D = {(xi, yi)}, the number of hidden nodes n, learning rate \u03b7, C and maximum epoch T Output: \u0398 = {W,b, c,d,U} 1: Initialize parameters W,b, c,d,U; 2: Divide the training data into batches; 3: for t = 1 to T do 4: for each batch do 5: Use 1-step Gibbs sampling to update the gradient according to Eq. (4); 6: Update all other parameters except U with CD; 7: Compute gradient w.r.t. A\u03bd according to Eq. (10); 8: Update A with gradient descent with Eq. (5); 9: Update U according to Eq. (8); 10: end for 11: end for 12: Output W,b, c,d,U; 13: End"}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "We analyze our model with experiments on two classification problems: character recognition and document classification, and compare our results to those from competitive baselines below. RBM or RBM for classification was first proposed in Hinton & Salakhutdinov (2006) and later was further developed in Larochelle et al. (2012). Its mathematical formula is shown in Eq. (2). Hierarchical classification RBM with soft assignment (HRBMs) is a nested hierarchical classifier in a top-down way, shown in Fig. 2(b). In the training stage, for each internal node (including root\nnode) in the current level, HRBM will split training data according to its children and learn a classification RBM for multiple classes (decided by the number of its children). In the inference stage, the likelihood for certain classes in the current layer depends both on the output probability of this layer classifier and also the conditional likelihood on the upper levels. For example, the probability to assign label 2 to a given instance in Fig. 2(b) depends on the output probabilities from RBM1, RBM21 and RBM31. For each data instance, its probability belongs to each class is the probability production along path from root to the leaf of that class, and finally we assign the data instance to the label with largest probability. Hierarchical classification RBM with hard assignment (HRBMh) has the similar hierarchical structure as HRBMs in Fig. 2(b). The difference between HRBMs and HRBMh is that HRBMs assign classification probability to each node, while HRBMh assign labels. Hidden hierarchical classification RBM (HHRBM) is similar as the hierarchical classification RBM (HRBM) in a top-down manner. For any current node, HHRBM learns a classification RBM and projects the training data into hidden space for its children (Note that RBM can map any input instance into its hidden space). Then, all its children recursively learn classification RBMs with projected hidden states as input from its parent node until to leaf level. In a sense, HHRBM works similar to the deep believe network (DBN) in Hinton (2007). Hence, the only difference between HHRBM and HRBM is that HRBM computes the classification probability with the visual data as input for all levels, while HHRBM calculates the classification probability with hidden states as input in a top-down manner. Multinomial logit model (MNL), a.k.a multiclass logistic regression, has no class correlated hierarchical structure. Correlated Multinomial logit regression (corrMNL) 1 extends MNL with hierarchical prior over classes, refer to Shahbaba & Neal (2007) for more details.\nIn all the above baselines, HRBMs, HRBMh, HHRBM and corrMNL leverage the hierarchical prior over label sets for classification, while RBM and MNL have no such prior information available. As for the difference in the number of RBMs used, (H)HRBMs belong to the tow-down classification approaches where multiple RBMs are constructed and each of which is trained to classify training\n1http://www.ics.uci.edu/\u02dcbabaks/Site/Codes.html\nexamples into one of its children in a hierarchical tree while our approach maintains only a single RBM.\nCharacter Recognition MNIST digits2 consists of 28\u00d7 28-size images of handwriting digits from 0 through 9, and has been widely used to test character recognition methods. In the experiment, we use Fig. 2(a) as our hierarchical prior over label sets. To test our method and other baselines, we sample 5000 images from the training sets as our training examples and 1000 examples from the testing sets as our testing data. The reason that we use a subset of MNIST is to answer whether the correlation between different classes is valuable for classification problem when the number of training examples for individual classes may be relatively small. In order to make our method comparable to other baselines, we have the same parameter setting for RBM related methods (including RBM, HRBMs, HRBMh and our method). We set the number of hidden states n = 100 and the learning rate \u03b7 = 0.1 for RBM related methods, and the extra parameter in our model C = 0.1. Both HRBMs and HRBMh learn a RBM for each node and recursively to leafs, shown in Fig. 3. For the HHRBM with 4 layers decided by the hierarchical prior in Fig. 2(a), we set its number of hidden states 100, 50, 25 and 20 for each layer respectively.\nThe comparison between our method and the baselines is shown in Table (1). Our method incorporates the hierarchical prior structure over labels, and the experimental results show that our method outperform other RBM related methods, and also demonstrates that the hierarchical prior in our method is helpful to improve the recognition accuracy.\nTo further indicate whether our method is helpful or not with few training samples and how it performs on rare classes, we tested our model on the balanced and unbalanced cases. For the balanced case (each class was sampled equally), we random sampled from 1000 to 5000 examples respectively and tested on the 1000 samples from testing set. The results in Fig. 4(a) demonstrates that our model works better than RBM. We also tested our method on the rare classes. Basically, we sampled a few examples for each rare class, while keep the other classes with 500 samples respectively. For example, we sample the 10 examples for the class \u20180\u2019, while the rest 9 classes have 500\n2http://yann.lecun.com/exdb/mnist/\ntraining examples respectively. Then, we training our model with these samples, and test it on the test cases. Similarly, we did the same testing on classes \u20181\u2019 to \u20189\u2019 respectively. We tested our method for each rare class on the testing set, and show the over average error rate in Fig. 4(b), which clearly demonstrates that our method is much better than RBM on rare classes.\nDocument Classification We also evaluated our model on 20 news group dataset for document classification. The 20 news group dataset3 has 18,846 articles with with 61188 vocabularies, which has been widely used in text categorization and document classification. In the experiment, we tested our model on the version of the 20 news group dataset4, in order to make our results comparable to the current state of the art results. In the experiment, we used the hierarchical prior structure over label shown in Fig. 3 for HHRBM, HRBMh, HRBMs and our model. As for parameter setting, we use CD-1, and set the number of hidden states n = 2000, learning rate \u03b7 = 0.1 and the maximum epoch equals to 100 for RBM related methods. For HHRBM, we set the number of hidden states to be 1000, 500, 200 and 200 respectively for each layer. As for our method, we set n = 1500, \u03b7 = 0.01, C = 0.1 and maximum epoch 200.\nThe results of different methods are shown in Table (2). Once again, our model outperforms the other RBM models, also get better results than SVM and neural network classifiers. HRBMs and corrMNL have bad performance in this dataset. The reason we guess is that HRBMs calculates the classification probability for each class by multiplying the output probabilities along the path from root to the leaf associated to that class. Thus, HRBMs will prefer the high level class for unbalanced hierarchical structure. Note that the hierarchical tree in Fig. 3 is unbalanced structure. For HRBMs, \u2018alt.Atheism\u2019, \u2018misc.forsale\u2019 and \u2018soc.religon.christian\u2019 will have higher probability to be labeled compared to leafs (or classes) in the level 4. corrMNL may have the same problem as HRBMs. Another reason for the low performance is that corrMNL does not consider the parameter redundancy problem between adjacent layers as in our model.\nWe also evaluate how the regularization term influences the performance. We set C = 0 to remove the orthogonal restriction, and get accuracy 30.1% in Table (2), which is significant lower than the result with orthogonal restriction. Hence, it demonstrates that it is useful to introduce orthogonal restriction to the correlated hierarchical prior."}, {"heading": "4 RELATED WORK", "text": "The hierarchical structure is organized according to the similarity of classes. Two classes are considered similar if it is difficult to distinguish one from the other on the basis of their representation. The similarity of classes increases as we descend the hierarchy. Thus, the hierarchical prior over\n3http://people.csail.mit.edu/jrennie/20Newsgroups/ 4http://www.cs.toronto.edu/\u02dclarocheh/public/datasets/20newsgroups/\n20newsgroups_{train,valid,test}_binary_5000_voc.txt\ncategories provides semantic meaning and valuable information among different classes; and thus to some extent it can assist classification problems in hand Shahbaba & Neal (2007); Xiao et al. (2011); Rohit et al. (2013). Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below. The first approach classifies each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the final assignment probability for each class. Xiao et al. introduced a hierarchical classification method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible. Another example is the nested multinomial logit model Shahbaba & Neal (2007), in which the nested classification model for each node is statistically independent, conditioned on its parent in the upper levels. One weakness of this strategy for hierarchical classification is that errors will propagate from parents to children, if any misclassification happened in the top level. The other methodology for hierarchical classification prefers to use the sum of parameters along the tree for classifying cases ended at leaf nodes. Cai and Hoffmann Cai & Hofmann (2004) proposed a hierarchical larger margin multi-class SVM with tree-induced loss functions. Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification. Recently, Shahbaba et al. proposed a correlated multinomial logit model (corrMNL) Shahbaba & Neal (2007), whose regression coefficients for each leaf node are represented by the sum of parameters on all the branches leading to that class.\nApart from the two approaches mentioned above, there are also other methods proposed in the past. Dumais and Chen trained different classifiers kind of layer by layer by exploring the hierarchical structure Dumais & Chen (2000). Cesa-Bianchi et al. combined Bayesian inference with the probabilities output from SVM classifiers in Cesa-Bianchi et al. (2006) for hierarchical classification. Similarly, Gopal et al. Gopal et al. (2012) used Bayesian approach (with variational inference) with hierarchical prior for classification problems."}, {"heading": "5 CONCLUSION", "text": "We consider restricted Boltzmann machines (RBM) for classification problems, with prior knowledge of sharing information among classes in a hierarchy. Basically, our model decompose classification RBM into traditional RBM for representation learning and multi-class logistic model for classification, and then introduce hierarchical prior over multi-class logistic model. In order to reduce the redundancy between node parameters, we also introduce orthogonal restrictions in our objective function. To the best of our knowledge, this is the first paper that incorporates hierarchical prior over RBM framework for classification. We test our method on challenge datasets, and show promising results compared to benchmarks."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Hierarchical document categorization with support vector machines", "author": ["Cai", "Lijuan", "Hofmann", "Thomas"], "venue": "In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Cai et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2004}, {"title": "Hierarchical classification: Combining bayes with svm", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Gentile", "Claudio", "Zaniboni", "Luca"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Large margin hierarchical classification", "author": ["Dekel", "Ofer", "Keshet", "Joseph", "Singer", "Yoram"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2004}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "jia Li", "Li", "Kai", "Fei-fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Hierarchical classification of web content", "author": ["Dumais", "Susan", "Chen", "Hao"], "venue": "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Dumais et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 2000}, {"title": "ed.). WordNet An Electronic Lexical Database", "author": ["Fellbaum", "Christiane"], "venue": null, "citeRegEx": "Fellbaum and Christiane,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and Christiane", "year": 1998}, {"title": "Classes for fast maximum entropy training", "author": ["Goodman", "Joshua"], "venue": null, "citeRegEx": "Goodman and Joshua.,? \\Q2001\\E", "shortCiteRegEx": "Goodman and Joshua.", "year": 2001}, {"title": "Bayesian models for large-scale hierarchical classification", "author": ["Gopal", "Siddharth", "Yang", "Yiming", "Bai", "Bing", "Niculescu-Mizil", "Alexandru"], "venue": null, "citeRegEx": "Gopal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gopal et al\\.", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G E Hinton", "Salakhutdinov", "R R"], "venue": "Science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Comput.,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "Learning multiple layers of representation", "author": ["Hinton", "Geoffrey E"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Hinton and E.,? \\Q2007\\E", "shortCiteRegEx": "Hinton and E.", "year": 2007}, {"title": "Hierarchically classifying documents using very few words", "author": ["Koller", "Daphne", "Sahami", "Mehran"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Koller et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["Larochelle", "Hugo", "Mandel", "Michael", "Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Semantic hierarchies for visual object recognition", "author": ["Marszalek", "Marcin", "Schmid", "Cordelia"], "venue": "In CVPR. IEEE Computer Society,", "citeRegEx": "Marszalek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Marszalek et al\\.", "year": 2007}, {"title": "Improving text classification by shrinkage in a hierarchy of classes", "author": ["McCallum", "Andrew", "Rosenfeld", "Ronald", "Mitchell", "Tom M", "Ng", "Andrew Y"], "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning,", "citeRegEx": "McCallum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 1998}, {"title": "On flat versus hierarchical classification in large-scale taxonomies", "author": ["Rohit", "Babbar", "Ioannis", "Partalas", "Eric", "Gaussier", "Massih-Reza", "Amini"], "venue": "In Neural Information Processsing Systems (NIPS),", "citeRegEx": "Rohit et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohit et al\\.", "year": 2013}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Improving classification when a class hierarchy is available using a hierarchy-based prior", "author": ["Shahbaba", "Babak", "Neal", "Radford M"], "venue": "Bayesian Analysis,", "citeRegEx": "Shahbaba et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shahbaba et al\\.", "year": 2007}, {"title": "Exploiting hierarchy in text categorization", "author": ["Weigend", "Andreas S", "Wiener", "Erik D", "Pedersen", "Jan O"], "venue": "Inf. Retr.,", "citeRegEx": "Weigend et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Weigend et al\\.", "year": 1999}, {"title": "Hierarchical classification via orthogonal transfer", "author": ["Xiao", "Lin", "Zhou", "Dengyong", "Wu", "Mingrui"], "venue": "In ICML, pp", "citeRegEx": "Xiao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "They have attracted significant attention recently on many machine learning problems, such as dimension reduction Hinton & Salakhutdinov (2006), text categorization Larochelle et al. (2012), collaborative filtering Salakhutdinov et al.", "startOffset": 165, "endOffset": 190}, {"referenceID": 10, "context": "They have attracted significant attention recently on many machine learning problems, such as dimension reduction Hinton & Salakhutdinov (2006), text categorization Larochelle et al. (2012), collaborative filtering Salakhutdinov et al. (2007) and object recognition Krizhevsky et al.", "startOffset": 165, "endOffset": 243}, {"referenceID": 10, "context": "(2007) and object recognition Krizhevsky et al. (2012). A recent survey Bengio et al.", "startOffset": 30, "endOffset": 55}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us.", "startOffset": 16, "endOffset": 37}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al.", "startOffset": 16, "endOffset": 323}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al.", "startOffset": 16, "endOffset": 346}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al.", "startOffset": 16, "endOffset": 362}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization.", "startOffset": 16, "endOffset": 383}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al.", "startOffset": 16, "endOffset": 874}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively.", "startOffset": 16, "endOffset": 906}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al.", "startOffset": 16, "endOffset": 1068}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as flat and little work has been done to explore the interclass relationship.", "startOffset": 16, "endOffset": 1094}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as flat and little work has been done to explore the interclass relationship. In this paper, we generalize RBM with hierarchical prior for classification problems. Basically, we divide the classification RBM into traditional RBM for representation learning and multinomial logit model for classification, see Fig. 1(a) for intuitive understanding. For the traditional RBM (red in Fig. 1(a)), we can extend it into deep belief network (DBN), while for the multinomial logit model (green in Fig. 1(a)), we can incorporate the interclass relationship to it. In this work, we focus on the hierarchical prior over the classification RBM, and we take a similar strategy as corrMNL, that means we use sums of parameters along paths from root to a specific leaf in the tree as model parameters for hierarchical classification. However, we consider it in a rather different way from the previous work. We can think our method is a kind of mixture of corrMNL Shahbaba & Neal (2007) and the orthogonal SVM model Xiao et al.", "startOffset": 16, "endOffset": 2096}, {"referenceID": 0, "context": "A recent survey Bengio et al. (2012) shows how to improve classification accuracy by exploiting prior knowledge about the world around us. The purpose of this paper is to answer whether we can leverage the hierarchical structure over categories to improve the classification accuracy. The hierarchical tree Fellbaum (1998); Weigend et al. (1999); Goodman (2001); Dekel et al. (2004) over different classes is an efficient and effective way for knowledge representation and categorization. The top level of the taxonomy hierarchies starts with a general or abstract description of common properties for all objects, while the low levers add more specific characteristics. In other words, the semantic relationship among classes is constructed from generalization to specification as depth increasing in the hierarchical tree or taxonomy. For example, WordNet Fellbaum (1998) and ImageNet Deng et al. (2009) use this semantic hierarchy to model human psycholinguistic knowledge and object taxonomy respectively. Unfortunately, traditional RBM Larochelle & Bengio (2008); Larochelle et al. (2012) treats the category structure as flat and little work has been done to explore the interclass relationship. In this paper, we generalize RBM with hierarchical prior for classification problems. Basically, we divide the classification RBM into traditional RBM for representation learning and multinomial logit model for classification, see Fig. 1(a) for intuitive understanding. For the traditional RBM (red in Fig. 1(a)), we can extend it into deep belief network (DBN), while for the multinomial logit model (green in Fig. 1(a)), we can incorporate the interclass relationship to it. In this work, we focus on the hierarchical prior over the classification RBM, and we take a similar strategy as corrMNL, that means we use sums of parameters along paths from root to a specific leaf in the tree as model parameters for hierarchical classification. However, we consider it in a rather different way from the previous work. We can think our method is a kind of mixture of corrMNL Shahbaba & Neal (2007) and the orthogonal SVM model Xiao et al. (2011). However, our model inherits the advantage of", "startOffset": 16, "endOffset": 2144}, {"referenceID": 13, "context": "RBM, which can learn the hidden representation for better classification Hinton & Salakhutdinov (2006); Larochelle et al. (2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al.", "startOffset": 104, "endOffset": 129}, {"referenceID": 13, "context": "RBM, which can learn the hidden representation for better classification Hinton & Salakhutdinov (2006); Larochelle et al. (2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al.", "startOffset": 104, "endOffset": 187}, {"referenceID": 3, "context": "(2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 3, "context": "(2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al. (2011). Moreover, we only have a single RBM in our model, while there are multiple SVMs in the orthogonal hierarchical SVM Xiao et al.", "startOffset": 86, "endOffset": 126}, {"referenceID": 3, "context": "(2012), compared to the multinomial logit Shahbaba & Neal (2007) and hierarchical SVM Dekel et al. (2004); Xiao et al. (2011). Moreover, we only have a single RBM in our model, while there are multiple SVMs in the orthogonal hierarchical SVM Xiao et al. (2011). Our contributions are: (1) we introduce the hierarchical semantic prior over labels into restricted Boltzmann machine; (2) we add orthogonal constraints over adjacent layers in the hierarchy, which makes our model more robust for classification problems.", "startOffset": 86, "endOffset": 261}, {"referenceID": 14, "context": "The classification RBM was first proposed in Hinton (2007) and was further developed in Larochelle & Bengio (2008); Larochelle et al. (2012) with discriminative training model.", "startOffset": 116, "endOffset": 141}, {"referenceID": 19, "context": "As shown in Salakhutdinov et al. (2007), this conditional distribution has explicit formula and can be calculated", "startOffset": 12, "endOffset": 40}, {"referenceID": 3, "context": "As in Dekel et al. (2004), we also define the path for each node \u03bd \u2208 T , define P (\u03bd) to be the set of nodes along the path from root to v, P (\u03bd) = {\u03bc \u2208 T : \u2203i \u03bc = A(\u03bd)} (6) Now we can define the coefficient parameters for each leaf node \u03bd as", "startOffset": 6, "endOffset": 26}, {"referenceID": 19, "context": "For our model, each leaf node is associated to one class, which takes the same methodology as in Salakhutdinov et al. (2007). Fig.", "startOffset": 97, "endOffset": 125}, {"referenceID": 22, "context": "just as in Xiao et al. (2011) to reduce redundancy between adjacent layers.", "startOffset": 11, "endOffset": 30}, {"referenceID": 14, "context": "RBM or RBM for classification was first proposed in Hinton & Salakhutdinov (2006) and later was further developed in Larochelle et al. (2012). Its mathematical formula is shown in Eq.", "startOffset": 117, "endOffset": 142}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.", "startOffset": 7, "endOffset": 32}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.", "startOffset": 7, "endOffset": 73}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.", "startOffset": 7, "endOffset": 109}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.8 HRBMh (\u03b7 = 0.1, n = 2000) 30.6 HRBMs (\u03b7 = 0.1, n = 2000) 63.7 HHRBM (\u03b7 = 0.1, n = 1000, 500, 200 and 200) 32.0 Ours (\u03b7 = 0.01, n = 1500 and C = 0) 30.1 Ours (\u03b7 = 0.01, n = 1500 and C = 0.1) 23.6 MNL 30.8 corrMNL Shahbaba & Neal (2007) 79.", "startOffset": 7, "endOffset": 349}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.8 HRBMh (\u03b7 = 0.1, n = 2000) 30.6 HRBMs (\u03b7 = 0.1, n = 2000) 63.7 HHRBM (\u03b7 = 0.1, n = 1000, 500, 200 and 200) 32.0 Ours (\u03b7 = 0.01, n = 1500 and C = 0) 30.1 Ours (\u03b7 = 0.01, n = 1500 and C = 0.1) 23.6 MNL 30.8 corrMNL Shahbaba & Neal (2007) 79.3 SVM Larochelle et al. (2012) 32.", "startOffset": 7, "endOffset": 383}, {"referenceID": 14, "context": "9 DRBM Larochelle et al. (2012) 27.6 RBM + NNet Larochelle et al. (2012) 26.8 HDRBM Larochelle et al. (2012) 23.8 HRBMh (\u03b7 = 0.1, n = 2000) 30.6 HRBMs (\u03b7 = 0.1, n = 2000) 63.7 HHRBM (\u03b7 = 0.1, n = 1000, 500, 200 and 200) 32.0 Ours (\u03b7 = 0.01, n = 1500 and C = 0) 30.1 Ours (\u03b7 = 0.01, n = 1500 and C = 0.1) 23.6 MNL 30.8 corrMNL Shahbaba & Neal (2007) 79.3 SVM Larochelle et al. (2012) 32.8 NNet Larochelle et al. (2012) 28.", "startOffset": 7, "endOffset": 418}, {"referenceID": 15, "context": "categories provides semantic meaning and valuable information among different classes; and thus to some extent it can assist classification problems in hand Shahbaba & Neal (2007); Xiao et al. (2011); Rohit et al.", "startOffset": 181, "endOffset": 200}, {"referenceID": 14, "context": "(2011); Rohit et al. (2013). Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 14, "context": "(2011); Rohit et al. (2013). Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al.", "startOffset": 8, "endOffset": 208}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al.", "startOffset": 180, "endOffset": 203}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007).", "startOffset": 180, "endOffset": 226}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007).", "startOffset": 180, "endOffset": 248}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007).", "startOffset": 180, "endOffset": 270}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below.", "startOffset": 180, "endOffset": 319}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below. The first approach classifies each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the final assignment probability for each class. Xiao et al. introduced a hierarchical classification method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible.", "startOffset": 180, "endOffset": 888}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below. The first approach classifies each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the final assignment probability for each class. Xiao et al. introduced a hierarchical classification method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible. Another example is the nested multinomial logit model Shahbaba & Neal (2007), in which the nested classification model for each node is statistically independent, conditioned on its parent in the upper levels.", "startOffset": 180, "endOffset": 1074}, {"referenceID": 14, "context": "Much work has extensively been done in the past years to exploit hierarchical prior over labels for classification problem, such as document categorization Koller & Sahami (1997); McCallum et al. (1998); Weigend et al. (1999); Dumais & Chen (2000); Cai & Hofmann (2004) and object recognition Marszalek & Schmid (2007). Two most popular approaches to leverage hierarchical prior can be categorized below. The first approach classifies each node recursively, by choosing the label of which the associated vector has the largest output score among its siblings till to a leaf node. An variant way is to compute the conditional probability for each class at each level, and then multiply these probabilities along every branch to compute the final assignment probability for each class. Xiao et al. introduced a hierarchical classification method with orthogonal transfer Xiao et al. (2011), which requires the parameters of children nodes are orthogonal to those of its parents as much as possible. Another example is the nested multinomial logit model Shahbaba & Neal (2007), in which the nested classification model for each node is statistically independent, conditioned on its parent in the upper levels. One weakness of this strategy for hierarchical classification is that errors will propagate from parents to children, if any misclassification happened in the top level. The other methodology for hierarchical classification prefers to use the sum of parameters along the tree for classifying cases ended at leaf nodes. Cai and Hoffmann Cai & Hofmann (2004) proposed a hierarchical larger margin multi-class SVM with tree-induced loss functions.", "startOffset": 180, "endOffset": 1564}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification.", "startOffset": 11, "endOffset": 47}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification.", "startOffset": 11, "endOffset": 77}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification. Recently, Shahbaba et al. proposed a correlated multinomial logit model (corrMNL) Shahbaba & Neal (2007), whose regression coefficients for each leaf node are represented by the sum of parameters on all the branches leading to that class.", "startOffset": 11, "endOffset": 238}, {"referenceID": 2, "context": "Similarly, Dekel et al. in Dekel et al. (2004) improved Cai & Hofmann (2004) into an online version for hierarchical classification. Recently, Shahbaba et al. proposed a correlated multinomial logit model (corrMNL) Shahbaba & Neal (2007), whose regression coefficients for each leaf node are represented by the sum of parameters on all the branches leading to that class. Apart from the two approaches mentioned above, there are also other methods proposed in the past. Dumais and Chen trained different classifiers kind of layer by layer by exploring the hierarchical structure Dumais & Chen (2000). Cesa-Bianchi et al.", "startOffset": 11, "endOffset": 600}, {"referenceID": 2, "context": "Cesa-Bianchi et al. combined Bayesian inference with the probabilities output from SVM classifiers in Cesa-Bianchi et al. (2006) for hierarchical classification.", "startOffset": 0, "endOffset": 129}, {"referenceID": 2, "context": "Cesa-Bianchi et al. combined Bayesian inference with the probabilities output from SVM classifiers in Cesa-Bianchi et al. (2006) for hierarchical classification. Similarly, Gopal et al. Gopal et al. (2012) used Bayesian approach (with variational inference) with hierarchical prior for classification problems.", "startOffset": 0, "endOffset": 206}], "year": 2015, "abstractText": "Restricted Boltzmann machines (RBM) and its variants have been widely used on classification problems. In a sense, its success of RBM should be attributed to its strong representation power with hidden variables. Often, classification RBM ignores the interclass relationship or prior knowledge of sharing information among classes. In this paper, we propose a RBM with hierarchical prior for classification problem, by generalizing the classification RBM with sharing information among different classes. Basically, we assume the hierarchical prior over classes, where parameters for nearby nodes are correlated in the hierarchical tree, and further the parameters at each node of the tree to be orthogonal to those at its ancestors. Through the hierarchical prior, our model improves the information sharing between different classes and reduce the redundancy for robust classification. We test our method on several datasets, and show promising results compared to competitive baselines.", "creator": "LaTeX with hyperref package"}}}