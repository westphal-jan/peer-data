{"id": "1406.4802", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2014", "title": "Homotopy based algorithms for $\\ell_0$-regularized least-squares", "abstract": "delmee Sparse kijac signal approximation investindo can douglassville be formulated montefiore as hermiston the nevel mixed $ \\ a-point ell_2 $ - $ \\ biofilm ell_0 $ minimization shotover problem $ \\ nucleocapsid min_x yorg J (innocent x; \\ 4,000-word lambda) = \\ | mismas y - Ax \\ | 11.17 _2 ^ syringes 2 + \\ lambda \\ | x \\ | selber _0 $. 197th We bronnitsy propose gmo two heuristic cartesianism search jealousies algorithms formula to mistura minimize colrain J peptidyl for tiwintza a australia/new continuum collaborated of $ \\ schoomaker lambda $ - values, yielding a sequence orum of raptures coarse to fine adauto approximations. bistri\u0163a Continuation cur\u00e9 Single miankuh Best formula Replacement o&a is torma a bidirectional pullbacks greedy polymath algorithm nicholas adapted l'aurore from the wangdi Single sold-out Best ringim Replacement juive algorithm carboxy-lyases previously proposed 13.0 for delijan minimizing pelluer J for makes fixed $ \\ raushan lambda $. $ \\ ell_0 $ regularization path vma track 69.93 is vampyre a more williamsbridge complex recinos algorithm leduff exploiting 25.9 that 61.93 the $ \\ leksands ell_2 $ - $ \\ ell_0 $ kololo regularization path contrail is piecewise chert constant dzong with respect cash-in to $ \\ lambda $. wenbin Tracking the $ \\ 0500 ell_0 $ balika regularization gencorp path radiantly is btw done in al-mihdhar a 59.52 sub - aacm optimal aviel manner by euthria maintaining (epitomises i) a 1823 list of bergdahl subsets qahwaji that not-guilty are magnus candidates markevich to be wildfell solution diapsids supports laurium for high-priority decreasing $ \\ fulk lambda $ ' kahlenberg s freckles and (ii) henny the zoos list 12,500 of ozymandias critical $ \\ appendage lambda $ - 2,691 values hannukah around nottawasaga which khalifan the 26-22 solution lutzes changes. jaba Both far-off algorithms gradually construct particularism the $ \\ meel ell_0 $ tengri regularization umbral path by peachey performing 1941 single rissmiller replacements, miscasting i. welayta e. , huajin adding deltacom or removing npn a dictionary pakistanis atom vairelles from a olperez subset. jungmann A straightforward schleyer adaptation escorial of neumarkt these salsola algorithms shrubs yields lunge sub - benfleet optimal mwr solutions warm-blooded to $ \\ dinham min_x \\ | steinbeck y - canini Ax \\ | _2 ^ redistributes 2 $ billington subject to $ \\ | chantilly x \\ | _0 \\ leq k $ many-body for euro271 contiguous values bakoyiannis of $ five-seater k \\ geq vaani 0 $ horologium and myelodysplasia to $ \\ nhp min_x \\ | 220-million x \\ | kyzyl _0 $ silvanesti subject amour to $ \\ | y - buzzy Ax \\ | humbuckers _2 ^ 13.71 2 \\ sitapur leq \\ sireniki varepsilon $ wieringen for barde continuous values representative-elect of $ \\ saadia varepsilon $. zibi Numerical simulations jesli show metaphysically the effectiveness of college-bound the hfd algorithms bracondale on cecelia a nayagarh difficult sparse ramaya deconvolution sihp problem netters inducing cosmote a infects highly guttormsen correlated dictionary catnap A.", "histories": [["v1", "Fri, 31 Jan 2014 22:26:17 GMT  (723kb)", "https://arxiv.org/abs/1406.4802v1", "28 pages"], ["v2", "Wed, 18 Mar 2015 16:37:16 GMT  (183kb)", "http://arxiv.org/abs/1406.4802v2", "38 pages"]], "COMMENTS": "28 pages", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["charles soussen", "j\\'er\\^ome idier", "junbo duan", "david brie"], "accepted": false, "id": "1406.4802"}, "pdf": {"name": "1406.4802.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Charles Soussen", "J\u00e9r\u00f4me Idier", "Junbo Duan"], "emails": ["charles.soussen@univ-lorraine.fr,", "david.brie@univ-lorraine.fr.", "jerome.idier@irccyn.ec-nantes.fr.", "junbo.duan@mail.xjtu.edu.cn."], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n48 02\nv2 [\ncs .N\nA ]\n1 8\nM ar\nSparse signal restoration is usually formulated as the minimization of a quadratic cost function \u2016y\u2212Ax\u201622, where A is a dictionary and x is an unknown sparse vector. It is well-known that imposing an \u21130 constraint leads to an NP-hard minimization problem. The convex relaxation approach has received considerable attention, where the \u21130-norm is replaced by the \u21131-norm. Among the many efficient \u21131 solvers, the homotopy algorithm minimizes \u2016y \u2212Ax\u20162 2 + \u03bb\u2016x\u20161 with respect to x for a continuum of \u03bb\u2019s. It is inspired by the piecewise regularity of the \u21131-regularization path, also referred to as the homotopy path. In this paper, we address the minimization problem \u2016y \u2212Ax\u20162 2 + \u03bb\u2016x\u20160 for a continuum of \u03bb\u2019s and propose two heuristic search algorithms for \u21130-homotopy. Continuation Single Best Replacement is a forward-backward greedy strategy extending the Single Best Replacement algorithm, previously proposed for \u21130-minimization at a given \u03bb. The adaptive search of the \u03bb-values is inspired by \u21131-homotopy. \u21130 Regularization Path Descent is a more complex algorithm exploiting the structural properties of the \u21130-regularization path, which is piecewise constant with respect to \u03bb. Both algorithms are empirically evaluated for difficult inverse problems involving ill-conditioned dictionaries. Finally, we show that they can be easily coupled with usual methods of model order selection.\nThis work was carried out in part while C. Soussen was visiting IRCCyN during the academic year 2010-2011 with the\nfinancial support of CNRS.\nC. Soussen and D. Brie are with the Universite\u0301 de Lorraine and CNRS at the Centre de Recherche en Automatique de Nancy (UMR 7039).Campus Sciences, B.P. 70239, F-54506 Vand\u0153uvre-le\u0300s-Nancy, France. Tel: (+33)-3 83 59 56 43, Fax: (+33)-3 83 68 44 62. E-mail: charles.soussen@univ-lorraine.fr, david.brie@univ-lorraine.fr.\nJ. Idier is with L\u2019UNAM Universite\u0301, Ecole Centrale Nantes and CNRS at the Institut de Recherche en Communications et Cyberne\u0301tique de Nantes (UMR 6597), 1 rue de la Noe\u0308, BP 92101, F-44321 Nantes Cedex 3, France. Tel: (+33)-2 40 37 69 09, Fax: (+33)-2 40 37 69 30. E-mail: jerome.idier@irccyn.ec-nantes.fr.\nJ. Duan was with CRAN. He is now with the Department of Biomedical Engineering, Xi\u2019an Jiaotong University. No. 28, Xianning West Road, Xi\u2019an 710049, Shaanxi Province, China. Tel: (+86)-29-82 66 86 68, Fax: (+86)-29 82 66 76 67. E-mail: junbo.duan@mail.xjtu.edu.cn.\nMarch 19, 2015 DRAFT\nIndex Terms\nSparse signal estimation; \u21130-regularized least-squares; \u21130-homotopy; \u21131-homotopy; stepwise algo-\nrithms; orthogonal least squares; model order selection.\nI. INTRODUCTION\nSparse approximation from noisy data is traditionally addressed as the constrained least-square problems\nmin x\n\u2016y \u2212Ax\u201622 subject to \u2016x\u20160 \u2264 k (1)\nor\nmin x\n\u2016x\u20160 subject to \u2016y \u2212Ax\u2016 2 2 \u2264 \u03b5 (2)\nwhere \u2016x\u20160 is the \u21130-\u201cnorm\u201d counting the number of nonzero entries in x, and the quadratic fidelity-todata term \u2016y \u2212Ax\u201622 measures the quality of approximation. Formulation (1) is well adapted when one has a knowledge of the maximum number k of atoms to be selected in the dictionary A. On the contrary,\nthe choice of (2) is more appropriate when k is unknown but one has a knowledge of the variance of\nthe observation noise. The value of \u03b5 may then be chosen relative to the noise variance. Since both (1)\nand (2) are subset selection problems, they are discrete optimization problems. They are known to be\nNP-hard except for specific cases [1].\nWhen no knowledge is available on either k or \u03b5, the unconstrained formulation\nmin x\n{J (x;\u03bb) = \u2016y \u2212Ax\u201622 + \u03bb\u2016x\u20160} (3)\nis worth being considered, where \u03bb expresses the trade-off between the quality of approximation and the\nsparsity level [2]. In a Bayesian viewpoint, (3) can be seen as a (limit) maximum a posteriori formulation where \u2016y\u2212Ax\u201622 and the penalty \u2016x\u20160 are respectively related to a Gaussian noise distribution and a prior distribution for sparse signals (a limit Bernoulli-Gaussian distribution with infinite Gaussian variance) [3]."}, {"heading": "A. Classification of methods", "text": "1) \u21130-constrained least-squares: The discrete algorithms dedicated to problems (1)-(2) can be categorized into two classes. First, the forward greedy algorithms explore subsets of increasing cardinalities\nstarting from the empty set. At each iteration, a new atom is appended to the current subset, therefore\ngradually improving the quality of approximation [4]. Greedy algorithms include, by increasing order of\ncomplexity: Matching Pursuit (MP) [5], Orthogonal Matching Pursuit (OMP) [6], and Orthogonal Least\nMarch 19, 2015 DRAFT\nSquares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order\nRecursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10]. The\nsecond category are thresholding algorithms, where each iteration delivers a subset of same cardinality\nk. Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and\nCoSaMP [13].\nAmong these two categories, greedy algorithms are well-adapted to the resolution of (1) and (2) for\nvariable sparsity levels. Indeed, they yield a series of subsets for consecutive k (i.e., for decreasing\napproximation errors \u03b5) since at each iteration, the current subset is increased by one element.\n2) \u21130-penalized least-squares: In [3], we evidenced that the minimization of J (x;\u03bb) using a descent algorithm leads to bidirectional extensions of forward (orthogonal) greedy algorithms. To be more specific,\nconsider a candidate subset S corresponding to the support of x. Including a new element into S yields a decrease of the square error, defined as the minimum of \u2016y\u2212Ax\u201622 for x supported by S. On the other hand, the penalty term \u03bb\u2016x\u20160 is increased by \u03bb. Overall, the cost function J (x;\u03bb) decreases as soon as the square error variation exceeds \u03bb. Similarly, a decrease of J (x;\u03bb) occurs when an element is removed\nfrom S provided that the squared error increment is lower than \u03bb. Because both inclusion and removal\noperations can induce a decrease of J , the formulation (3) allows one to design descent schemes allowing\na \u201cforward-backward\u201d search strategy, where each iteration either selects a new atom (forward selection)\nor de-selects an atom that was previously selected (backward elimination). The Bayesian OMP [14] and\nSingle Best Replacement (SBR) [3] algorithms have been proposed in this spirit. They are extensions of\nOMP and OLS, respectively. Their advantage over forward greedy algorithms is that an early wrong atom\nselection may be later cancelled. Forward-backward algorithms include the so-called stepwise regression\nalgorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14],\n[17].\n3) Connection with the continuous relaxation of the \u21130 norm: The algorithms described so far are discrete search strategies dedicated to \u21130-regularized least-squares. A classical alternative consists in relaxing the \u21130-norm by a continuous function that is nondifferentiable at 0, and optimizing the resulting cost function. See, e.g., [18], [19] and [20]\u2013[27] for convex (\u21131) and nonconvex relaxation, respectively. The convex problem minx \u2016y \u2212 Ax\u201622 s.t. \u2016x\u20161 \u2264 t is referred to as both Basis Pursuit Denoising (BPDN) and the LASSO. It is noticeable that BPDN leads to stepwise algorithms [18], [28] including\nthe popular \u21131-homotopy [28]\u2013[30], a forward-backward greedy search whose complexity is close to that of OMP. \u21131-homotopy is closely connected to the Least Angle Regression (LARS), a simpler forward strategy allowing only atom selections. It is referred to as \u201cLARS with the LASSO modification\u201d in [30].\nMarch 19, 2015 DRAFT\nImportantly, \u21131-homotopy solves the BPDN for a continuum of values of t."}, {"heading": "B. Main idea", "text": "Our approach is dedicated to \u21130-penalized least-squares. It is based on the following geometrical\ninterpretation.\nFirst, for any subset S, we can define a linear function \u03bb 7\u2192 E(S) + \u03bb|S|, where E(S) = \u2016y \u2212Ax\u201622 is the corresponding least-square error and |S| stands for the cardinality of S. For each subset S, this\nfunction yields a line in the 2D domain (\u03bb,J ), as shown on Fig. 1.\nSecond, the set of solutions to (3) is piecewise constant with respect to \u03bb (see Appendix A for a proof).\nGeometrically, this result can be easily understood by noticing that the minimum of J (x;\u03bb) with respect\nto x is obtained for all \u03bb-values by considering the concave envelope of the set of lines \u03bb 7\u2192 E(S)+\u03bb|S|\nfor all subsets S. The resulting piecewise affine curve is referred to as the \u21130-curve (see Fig. 1). Its edges are related to the supports of the sparse solutions for all \u03bb, and its vertices yield the breakpoints \u03bb\u22c6i around which the set of optimal solutions argmin x J (x;\u03bb) is changing.\nWe take advantage of this interpretation to propose two suboptimal greedy algorithms that address (3)\nfor a continuum of \u03bb-values. Continuation Single Best Replacement (CSBR) repeatedly minimizes J (x;\u03bb)\nwith respect to x for decreasing \u03bb\u2019s. \u21130 Regularization Path Descent (\u21130-PD) is a more complex algorithm maintaining a list of subsets so as to improve (decrease) the current approximation of the \u21130 curve.\nMarch 19, 2015 DRAFT"}, {"heading": "C. Related works", "text": "1) Bi-objective optimization: The formulations (1), (2) and (3) can be interpreted as the same biobjective problem because they all intend to minimize both the approximation error \u2016y \u2212Ax\u201622 and the sparsity measure \u2016x\u20160. Although x is continuous, the bi-objective optimization problem should rather be considered as a discrete one where both objectives reread E(S) and |S|. Indeed, the continuous solutions\ndeduce from the discrete solutions, x reading as a least-square minimizer among all vectors supported\nby S.\nFig. 2 is a classical bi-objective representation where each axis is related to a single objective [31], namely |S| and E(S). In bi-objective optimization, a point S is called Pareto optimal when no other point S\u2032 can decrease both objectives [32]. In the present context, |S| takes integer values, thus the Pareto solutions are the minimizers of E(S) subject to |S| \u2264 k for consecutive values of k. Equivalently, they minimize |S| subject to E(S) \u2264 \u03b5 for some \u03b5. They are usually classified as supported or non-supported. The former lay on the convex envelope of the Pareto frontier (the bullet points in Fig. 2) whereas the latter lay in the nonconvex areas (the square point). It is well known that a supported solution can be reached when minimizing the weighted sum of both objectives, i.e., when minimizing E(S) + \u03bb|S| with respect to S for some weight \u03bb. On the contrary, the non-supported solutions cannot [32]. Choosing between the weighting sum method and a more complex method is a nontrivial question. The answer depends on the problem at-hand and specifically, on the size of the nonconvex areas in the Pareto frontier.\nMarch 19, 2015 DRAFT\n2) \u21131 and \u21130-homotopy seen as a weighted sum method: It is important to notice that for convex objectives, the Pareto solutions are all supported. Consider the BPDN; because \u2016y \u2212 Ax\u201622 and \u2016x\u20161 are convex functions of x, the set of minimizers of \u2016y \u2212 Ax\u201622 + \u03bb\u2016x\u20161 for all \u03bb coincides with the set of minimizers of \u2016y \u2212 Ax\u201622 s.t. \u2016x\u20161 \u2264 t for all t [33]. Both sets are referred to as the (unique) \u201c\u21131-regularization path\u201d. The situation is different with \u21130-regularization. Now, the weighted sum formulation (3) may not yield the same solutions as the constrained formulations (1) and (2) because the\n\u21130-norm is nonconvex [2]. This will lead us to define two \u21130-regularization paths, namely the \u201c\u21130-penalized path\u201d and the \u201c\u21130-constrained path\u201d (Section II).\nOn the algorithmic side, the \u21130 problems are acknowledged to be difficult. Many authors actually discourage the direct optimization of J because there are a very large number of local minimizers [20],\n[23]. In [3], however, we showed that forward-backward extensions of OLS are able to escape from some\nlocal minimizers of J (x;\u03bb) for a given \u03bb. This motivates us to propose efficient OLS-based strategies\nfor minimizing J for variable \u03bb-values.\n3) Positioning with respect to other stepwise algorithms: In statistical regression, the word \u201cstepwise\u201d\noriginally refers to Efroymson\u2019s algorithm [15], proposed in 1960 as an empirical extension of forward\nselection (i.e., OLS). Other stepwise algorithms were proposed in the 1980\u2019s [8, Chapter 3] among\nwhich Berk\u2019s and Broersen\u2019s algorithms [16], [34]. All these algorithms perform a single replacement\nper iteration, i.e., a forward selection or a backward elimination. They were originally applied to over-\ndetermined problems in which the number of columns of A is lower than the number of rows. Recent\nstepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36]. They all aim to\nfind subsets of cardinality k yielding a low approximation error E(S) for all k. Although our algorithms\nshare the same objective, they are inspired by (i) the \u21131-homotopy algorithm; and (ii) the structural properties of the \u21130-regularization paths. To the best of our knowledge, the idea of reconstructing an \u21130-regularization path using \u21130-homotopy procedures is novel.\nCSBR and \u21130-PD both read as descent algorithms in different senses: CSBR, first sketched in [37], repeatedly minimizes J (x;\u03bb) for decreasing \u03bb\u2019s. On the contrary, \u21130-PD minimizes J (x;\u03bb) for any \u03bbvalue simultaneously by maintaining a list of candidate subsets. The idea of maintaining a list of support\ncandidates was recently developed within the framework of forward selection [38], [39]. Our approach\nis different, because a family of optimization problems are being addressed together. In contrast, the\nsupports in the list are all candidate solutions to solve the same problem in [38], [39].\n4) Positioning with respect to continuation algorithms: The principle of continuation is to handle a\ndifficult problem by solving a sequence of simpler problems with warm start initialization, and gradually\nMarch 19, 2015 DRAFT\ntuning some continuous hyperparameter [40]. In sparse approximation, the word continuation is used in\ntwo opposite contexts.\nFirst, the BDPN problem involving the \u21131-norm. BPDN is solved for decreasing hyperparameter values using the solution for each value as a warm starting point for the next value [4]. \u21131-homotopy [28], [30], [41] exploits that the \u21131 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces. CSBR is designed in a similar spirit and can be interpreted as an \u201c\u21130-homotopy\u201d procedure (although the \u21130 minimization steps are solved in a sub-optimal way) working for decreasing \u03bb-values.\nSecond, the continuous approximation of the (discrete) \u21130 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the reso-\nlution of continuous optimization problems with warm start initialization. Although the full reconstruction\nof the \u21130-regularization path has been rarely addressed, it is noticeable that a GNC-like approach, called SparseNet, aims to gradually update some estimation of the regularization path induced by increasingly\nnon-convex sparsity measures [44]. This strategy relies on the choice of a grid of \u03bb-values. Because\nthe influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is\nmodified [44]. On the contrary, our approach does not rely on a grid definition. The \u03bb-values are rather\nadaptively computed similar to the \u21131-homotopy principle [28], [30].\nThe paper is organized as follows. In Section II, we define the \u21130-regularization paths and establish their main properties. The CSBR and \u21130-PD algorithms are respectively proposed in Sections III and IV. In Section V, both algorithms are analyzed and compared with the state-of-art algorithms based on\nnonconvex penalties for difficult inverse problems. Additionally, we investigate the automatic choice of\nthe cardinality k using classical order selection rules.\nII. \u21130-REGULARIZATION PATHS"}, {"heading": "A. Definitions, terminology and working assumptions", "text": "Let m\u00d7n denote the size of the dictionary A (usually, m \u2264 n in sparse approximation). The observation\nsignal y and the weight vector x are of size m\u00d71 and n\u00d71, respectively. We assume that any min(m,n)\ncolumns of A are linearly independent so that for any subset S \u2282 {1, . . . , n}, the submatrix of A gathering\nthe columns indexed by S is full rank, and the least-square error E(S) can be numerically computed.\nThis assumption is however not necessary for the theoretical results provided hereafter.\nWe denote by |S| the cardinality of a subset S. We use the alternative notations \u201cS+{i}\u201d and \u201cS\u2212{i}\u201d\nfor the forward selection S \u222a {i} and backward elimination S \\ {i}. We can then introduce the generic\nMarch 19, 2015 DRAFT\nnotation S \u00b1{i} for single replacements: S \u00b1 {i} stands for S + {i} if i /\u2208 S, and S \u2212{i} if i \u2208 S. We\nwill frequently resort to the geometrical interpretation of Fig. 1. With a slight abuse of terminology, the\nline \u03bb 7\u2192 E(S) + \u03bb|S| will be simply referred to as \u201cthe line S\u201d.\nHereafter, we start by defining the \u21130-regularized paths as the set of supports of the solutions to problems (1), (2) and (3) for varying hyperparameters. As seen in Section I, the solutions may differ whether\nthe \u21130-regularization takes the form of a bound constraint or a penalty. This will lead us to distinguish the \u201c\u21130-constrained path\u201d and the \u201c\u21130-penalized path\u201d. We will keep the generic terminology \u201c\u21130-regularization paths\u201d for statements that apply to both. The solutions delivered by our greedy algorithms will be referred\nto as the \u201capproximate \u21130-penalized path\u201d since they are suboptimal algorithms."}, {"heading": "B. Definition and properties of the \u21130-regularized paths", "text": "The continuous problems (1), (2) and (3) can be converted as the discrete problems:\nmin S E(S) subject to |S| \u2264 k, (4)\nmin S |S| subject to E(S) \u2264 \u03b5, (5)\nmin S\n{ J\u0302 (S;\u03bb) , E(S) + \u03bb|S| } , (6)\nwhere S stands for the support of x. The optimal solutions x to problems (1), (2) and (3) can indeed\nbe simply deduced from those of (4), (5) and (6), respectively, x reading as the least-square minimizers\namong all vectors supported by S. In the following, the formulation (5) will be omitted because it leads\nto the same \u21130-regularization path as formulation (4) [2].\nLet us first define the set of solutions to (4) and (6) and the \u21130-curve, related to the minimum value\nin (6) for all \u03bb > 0.\nDefinition 1 For k \u2264 min(m,n), let S\u22c6C(k) be the set of minimizers of the constrained problem (4).\nFor \u03bb > 0, let S\u22c6P(\u03bb) be the set of minimizers of the penalized problem (6). Additionally, we define the \u21130-curve as the function \u03bb 7\u2192 minS{J\u0302 (S;\u03bb)}. It is the concave envelope of a finite number of linear functions. Thus, it is concave and piecewise affine. Let \u03bb\u22c6I+1 , 0 < \u03bb \u22c6 I < . . . < \u03bb \u22c6 1 < \u03bb \u22c6 0 , +\u221e delimit the affine intervals (I + 1 contiguous intervals; see Fig. 1 in the case where I = 2).\nEach set S\u22c6C(k) or S \u22c6 P(\u03bb) can be thought of as a single support (e.g., S \u22c6 C(k) is reduced to the support S\u22c6a in the example of Fig. 2). They are defined as sets of supports because the minimizers of (4) and (6) might not be always unique. Let us now provide a key property of the set S\u22c6P(\u03bb).\nMarch 19, 2015 DRAFT\nTheorem 1 S\u22c6P(\u03bb) is a piecewise constant function of \u03bb, being constant on each interval \u03bb \u2208 (\u03bb \u22c6 i+1, \u03bb \u22c6 i ).\nProof: See Appendix A.\nThis property allows us to define the \u21130-regularization paths in a simple way.\nDefinition 2 The \u21130-constrained path is the set (of sets) S\u22c6C = {S \u22c6 C(k), k = 0, . . . ,min(m,n)}.\nThe \u21130-penalized path is defined as S\u22c6P = {S \u22c6 P(\u03bb), \u03bb > 0}. According to Theorem 1, S \u22c6 P is composed\nof (I + 1) distinct sets S\u22c6P(\u03bb), one for each interval \u03bb \u2208 (\u03bb \u22c6 i+1, \u03bb \u22c6 i ).\nS\u22c6C gathers the solutions to (4) for all k. As illustrated on Fig. 2, the elements of S \u22c6 C are the Pareto solutions whereas the elements of S\u22c6P correspond to the convex envelope of the Pareto frontier. Therefore, both \u21130-regularization paths may not coincide [2], [31]. As stated in Theorem 2, S\u22c6P \u2282 S \u22c6 C, but the reverse inclusion is not guaranteed.\nTheorem 2 S\u22c6P \u2282 S \u22c6 C. Moreover, for any \u03bb /\u2208 {\u03bb \u22c6 I , . . . , \u03bb \u22c6 0}, there exists k such that S \u22c6 P(\u03bb) = S \u22c6 C(k).\nProof: See Appendix A."}, {"heading": "C. Approximate \u21130-penalized regularization path", "text": "Let us introduce notations for the approximate \u21130-penalized path delivered by our heuristic search algorithms. Throughout the paper, the \u22c6 notation is reserved for optimal solutions (e.g., S\u22c6P). It is removed when dealing with numerical solutions. The outputs of our algorithms will be composed of a list \u03bb =\n{\u03bb1, . . . , \u03bbJ+1} of decreasing \u03bb-values, and a list S = {S0, . . . , SJ} of candidate supports, with S0 = \u2205. Sj is a suboptimal solution to (6) for \u03bb \u2208 (\u03bbj+1, \u03bbj). In the first interval \u03bb > \u03bb1, the solution is S0 = \u2205. The reader shall keep in mind that each output Sj induces a suboptimal solution xj to (3) for \u03bb \u2208 (\u03bbj+1, \u03bbj). This vector is the least-square solution supported by Sj . It can be computed using the pseudo-inverse of the subdictionary indexed by the set of atoms in Sj .\nGeometrically, each support Sj yields a line segment. Appending these segments yields an approximate\n\u21130-curve covering the domain (\u03bbJ+1,+\u221e), as illustrated on Fig. 3."}, {"heading": "III. GREEDY CONTINUATION ALGORITHM (CSBR)", "text": "Our starting point is the Single Best Replacement algorithm [3] dedicated to the minimization of\nJ (x;\u03bb) with respect to x, or equivalently to J\u0302 (S;\u03bb) = E(S)+\u03bb|S| with respect to S. We first describe\nSBR for a given \u03bb. Then, the CSBR extension is presented for decreasing and adaptive \u03bb\u2019s.\nMarch 19, 2015 DRAFT"}, {"heading": "A. Single Best Replacement", "text": "SBR is a deterministic descent algorithm dedicated to the minimization of J\u0302 (S;\u03bb) with the initial\nsolution S = \u2205. An SBR iteration consists of three steps:\n1) Compute J\u0302 (S \u00b1 {i};\u03bb) for all possible single replacements S \u00b1 {i} (n insertion and removal\ntrials);\n2) Select the best replacement Sbest = S \u00b1 {\u2113}, with\n\u2113 \u2208 argmin i\u2208{1,...,n} J\u0302 (S \u00b1 {i};\u03bb); (7)\n3) Update S \u2190 Sbest.\nSBR terminates when J\u0302 (Sbest;\u03bb) \u2265 J\u0302 (S;\u03bb), i.e., when no single replacement can decrease the cost function. This occurs after a finite number of iterations because SBR is a descent algorithm and there are\na finite number of possible subsets S \u2282 {1, . . . , n}. In the limit case \u03bb = 0, we have J\u0302 (S; 0) = E(S).\nOnly insertions can be performed since any removal increases the squared error E(S). SBR coincides with\nthe well-known OLS algorithm [7]. Generally, the n replacement trials necessitate to compute E(S+{i})\nfor all insertion trials and E(S \u2212 {i}) for all removals. In [3], we proposed a fast and stable recursive implementation based on the Cholesky factorization of the Gram matrix ATSAS when S is modified by one element (where AS stands for the submatrix of A gathering the active columns). SBR is summarized in Tab. I. The optional output parameters \u2113add and \u03b4Eadd are unnecessary in the standard version. Their knowledge will be useful to implement the extended CSBR algorithm.\nLet us illustrate the behavior of SBR on a simple example using the geometrical interpretation of Fig. 4,\nwhere a single replacement is represented by a vertical displacement (from top to bottom) between the two lines S and S \u00b1 {\u2113}. Sinit = \u2205 yields an horizontal line since J\u0302 (\u2205;\u03bb) = \u2016y\u201622 does not depend on\nMarch 19, 2015 DRAFT\n\u03bb. At the first SBR iteration, a new dictionary atom \u2113 = a is selected. The line related to the updated\nsupport S \u2190 {a} is of slope |S| = 1. Similarly, some new dictionary atoms b and c are being selected\nin the next iterations, yielding the supports S \u2190 {a, b} and S \u2190 {a, b, c}. On Fig. 4, the dotted lines\nrelated to the latter supports have slopes equal to 2 and 3. At iteration 4, the single best replacement is\nthe removal \u2113 = a. The resulting support S \u2190 {b, c} is of cardinality 2, and the related line is parallel to\nthe line {a, b} found at iteration 2. During the fifth iteration, none of the n single replacements decreases\nJ\u0302 ({b, c};\u03bb). SBR stops with output S = {b, c}."}, {"heading": "B. Principle of the continuation search", "text": "Our continuation strategy is inspired by \u21131-homotopy which recursively computes the minimizers of \u2016y\u2212Ax\u201622 +\u03bb\u2016x\u20161 when \u03bb is continuously decreasing [28]\u2013[30]. An iteration of \u21131-homotopy consists in two steps:\n\u2022 Find the next value \u03bbnew < \u03bbcur for which the \u21131 optimality conditions are violated with the current\nactive set S (\u03bbcur denotes the current value);\n\u2022 Compute the single replacement S \u2190 S \u00b1 {i} allowing to fulfill the \u21131 optimality conditions at\n\u03bb = \u03bbnew.\nMarch 19, 2015 DRAFT\nCSBR follows the same principle. The first step is now related to some local \u21130-optimality conditions, and the second step consists in calling SBR at \u03bbnew with the current active set as initial solution; see Fig. 5 for a sketch. A main difference with \u21131-homotopy is that the \u21130 solutions are suboptimal, i.e., they are local minimizers of J (x;\u03bb) with respect to x.\n1) Local optimality conditions: Let us first reformulate the stopping conditions of SBR at a given \u03bb.\nSBR terminates when a local minimum of J\u0302 (S;\u03bb) has been found:\n\u2200i \u2208 {1, . . . , n}, J\u0302 (S \u00b1 {i};\u03bb) \u2265 J\u0302 (S;\u03bb). (8)\nThis condition is illustrated on Fig. 6(a): all lines related to single replacements S \u00b1 {i} lay above the\nblack point representing the value of J\u0302 (S;\u03bb) for the current \u03bb. By separating the conditions related to\ninsertions S + {i} and removals S \u2212 {i}, (8) rereads as the interval condition:\n\u03bb \u2208 [\u03b4Eadd(S), \u03b4Ermv(S)], (9)\nwhere\n\u03b4Eadd(S) , max i/\u2208S\n{ E(S)\u2212 E(S + {i}) }\n(10a)\n\u03b4Ermv(S) , min i\u2208S\n{ E(S \u2212 {i}) \u2212 E(S) }\n(10b)\nrefer to the maximum variation of the squared error when an atom is added in the support S (respectively,\nremoved from S).\nMarch 19, 2015 DRAFT\n2) Violation of the local optimality conditions: Consider the current output S = SBR(Sinit;\u03bbcur). The local optimality condition (9) is then met for \u03bb = \u03bbcur, but also for any \u03bb \u2208 [\u03b4Eadd(S), \u03bbcur]. The new value for which (9) is violated is \u03bbnew = \u03b4Eadd(S) \u2212 c where c > 0 is arbitrarily small. The violation occurs for i = \u2113add, with\n\u2113add \u2208 argmax i/\u2208S {E(S) \u2212 E(S + {i})}. (11)\nMarch 19, 2015 DRAFT\nIn practice, \u03bbnew can be set to the limit value\n\u03bbnew = \u03b4Eadd(S) (12)\nprovided that S is replaced with S + {\u2113add}.\nAs illustrated on Fig. 6(b), the line S+ {\u2113add} lays below all other parallel lines S+ {i}. It intersects line S at \u03bbnew. The vertical arrow represents the new call to SBR with inputs S + {\u2113add} and \u03bbnew. Because S and S+{\u2113add} both lead to the same value of J\u0302 ( . ;\u03bbnew), the de-selection of \u2113add is forbidden in the first iteration of SBR."}, {"heading": "C. CSBR algorithm", "text": "CSBR is summarized in Tab. II. The repeated calls to SBR deliver subsets Sj for decreasing \u03bbj . As shown on Fig. 5, the solution Sj covers the interval (\u03bbj+1, \u03bbj ]. At the very first iteration, we have S0 = \u2205, and (11)-(12) reread:\n\u2113add \u2208 argmax i\u2208{1,...,n}\n|\u3008y,ai\u3009|\n\u2016ai\u20162 and \u03bb1 =\n\u3008y,a\u2113add\u3009 2\n\u2016a\u2113add\u2016 2 2\n. (13)\nAccording to Tab. II, CSBR stops when \u03bbj = 0, i.e., the whole domain \u03bb \u2208 R+ has been scanned. However, this choice may not be appropriate when dealing with noisy data and overcomplete dictionaries.\nIn such cases, ad hoc early stopping rules can be considered [28], [45]. A natural rule takes the form\nMarch 19, 2015 DRAFT\n\u03bbj \u2264 \u03bbstop with \u03bbstop > 0. Alternative rules involve a maximum cardinality (|Sj | \u2265 kstop) and/or a minimum squared error (E(Sj) \u2264 \u03b5stop).\nFig. 5 shows a step-by-step illustration with the early stop \u03bbj \u2264 \u03bbstop. The initial support Sinit = {\u2113add} and \u03bb1 are precomputed in (13). In the first call S1 = SBR(Sinit;\u03bb1), a number of single replacements updates S \u2190 S\u00b1{\u2113} are carried out leading to S1 = S. This process is represented by the plain vertical arrow at \u03bb1 linking both lines S0 and S1 (the line Sinit is not shown for readability reasons). Once S1 is obtained, the next value \u03bb2 is computed. This process is represented by an oblique, dashed arrow joining \u03bb1 and \u03bb2. These two processes are being repeated alternatively at the second and third iterations of CSBR. Finally, CSBR terminates after \u03bb4 has been computed because \u03bb4 \u2264 \u03bbstop.\nIV. \u21130-REGULARIZATION PATH DESCENT (\u21130-PD)\nOn the theoretical side, the \u21130-penalized regularization path is piecewise constant (Theorem 1). It yields the \u21130 curve which is piecewise affine, continuous and concave (Fig. 1). The curve related to the CSBR outputs does not fulfill this property since: (i) there might be jumps in this curve; and (ii) the slope of the\nline Sj is not necessarily increasing with j (see Fig. 5). This motivates us to propose another algorithm whose outputs are consistent with the structural properties of the \u21130-curve.\nWe propose to gradually update a list S of candidate subsets Sj while imposing that the related curve is a concave polygon, obtained as the concave envelope of the set of lines Sj (see Fig. 7(a)). The subsets in S are updated so as to decrease at most the concave polygonal curve. In particular, we impose that\nthe least value is \u03bbJ+1 = 0, so that the concave envelope is computed over the whole domain \u03bb \u2208 R+."}, {"heading": "A. Descent of the concave polygon", "text": "The principle of \u21130-PD is to perform a series of descent steps, where a new candidate subset Snew is considered and included in the list S only if the resulting concave polygon can be decreased. This\ndescent test is illustrated on Fig. 7 for two examples (top and bottom subfigures). For each example, the\ninitial polygon is represented in (a). It is updated when its intersection with the line Snew is non-empty (b). The new concave polygon (c) is obtained as the concave envelope of the former polygon and the\nline Snew. All subsets in S whose edges lay above the line Snew are removed from S .\nThis procedure is formally presented in Tab. III. Let us now specify how the new candidate subsets\nSnew are built.\nMarch 19, 2015 DRAFT"}, {"heading": "B. Selection of the new candidate support", "text": "We first need to assign a Boolean label Sj .expl to each subset Sj . It equals 1 if Sj has already been \u201cexplored\u201d and 0 otherwise. The following exploration process is being carried out given a subset\nS = Sj: all the possible single replacements S\u00b1{i} are tested. The best insertion \u2113add and removal \u2113rmv are both kept in memory, with \u2113add defined in (11) and similarly,\n\u2113rmv \u2208 argmin i\u2208S {E(S \u2212 {i}) \u2212 E(S)}. (14)\nAt any \u21130-PD iteration, the unexplored subset Sj of lowest cardinality (i.e., of lowest index j) is selected. \u21130-PD attempts to include Sadd = Sj + {\u2113add} and Srmv = Sj \u2212 {\u2113rmv} into S , so that the concave polygon can be decreased at most. The CCV Descent procedure (Tab. III) is first called with\nSnew \u2190 Sadd leading to possible updates of S and \u03bb. It is called again with Snew \u2190 Srmv. Fig. 7 illustrates each of these calls: the slope of Snew is |Sj | + 1 and |Sj| \u2212 1, respectively. When a support\nMarch 19, 2015 DRAFT\nSj has been explored, the new supports that have been included in S (if any) are tagged as unexplored."}, {"heading": "C. \u21130-PD algorithm", "text": "\u21130-PD is stated in Tab. IV. Initially, S is formed of the empty support S0 = \u2205. The resulting concave polygon is reduced to a single horizontal edge. The corresponding endpoints are \u03bb1 = 0 and (by extension) \u03bb0 , +\u221e. In the first iteration, S0 is explored: the best insertion Sadd = {\u2113add} is computed in (13), and included in S during the call to CCV Descent. The updated set S is now composed of S0 = \u2205 (explored) and S1 = Sadd (unexplored). The new concave polygon has two edges delimited by \u03bb2 = 0, \u03bb1 and \u03bb0 = +\u221e, with \u03bb1 given in (13). Generally, either 0, 1, or 2 new unexplored supports Sadd and Srmv may be included in S at a given iteration while a variable number of supports may be removed from S .\n\u21130-PD terminates when all supports in S have been explored. When this occurs, the concave polygon cannot decrease anymore with any single replacement Sj\u00b1{i}, with Sj \u2208 S . Practically, the early stopping rule \u03bbj \u2264 \u03bbstop can be adopted, where j denotes the unexplored subset having the least cardinality. This rule ensures that all candidate subsets Sj corresponding to the interval (\u03bbstop,+\u221e) have been explored. Similar to CSBR, alternative stopping conditions of the form |Sj | \u2265 kstop or E(Sj) \u2264 \u03b5stop can be adopted.\nMarch 19, 2015 DRAFT"}, {"heading": "D. Fast implementation", "text": "concave polygon S and a line Snew. Lemma 1 states that this intersection is empty in two simple situations. Hence, the call to intersect is not needed in these situations. This implementation detail is\nomitted in Tab. III for brevity reasons.\nLemma 1 Let S = {Sj , j = 0, . . . , J} be a list of supports associated to a continuous, concave polygon \u03bb 7\u2192 minj J\u0302 (Sj ;\u03bb) with J + 1 edges, delimited by \u03bb = {\u03bb0, . . . , \u03bbJ+1}. The following properties hold for all j:\n\u2022 If \u03b4Eadd(Sj) < \u03bbj+1, then the line Sadd = Sj + {\u2113add} lays above the current concave polygon. \u2022 If \u03b4Ermv(Sj) > \u03bbj , then the line Srmv = Sj \u2212 {\u2113rmv} lays above the current concave polygon.\nMarch 19, 2015 DRAFT\nProof: We give a sketch of proof using geometrical arguments. Firstly, \u03b4Eadd(Sj) is the \u03bb-value of\nthe intersection point between lines Sj and Snew = Sj + {\u2113add}; see Fig. 7(b). Secondly, we notice that |Sj | \u2264 |Sadd| \u2264 |Sj+1| because the concave polygon is concave and |Sadd| = |Sj| + 1. It follows from these two facts that if \u03b4Eadd(Sj) < \u03bbj+1, the line Sadd lays above Sj+1 for \u03bb \u2264 \u03bbj+1, and above Sj for \u03bb \u2265 \u03bbj+1.\nThis proves the first result. A similar sketch applies to the second result."}, {"heading": "E. Main differences between CSBR and \u21130-PD", "text": "First, we stress that contrary to CSBR, the index j in \u03bbj does not identify with the iteration number anymore for \u21130-PD. Actually, the current iteration of \u21130-PD is related to an edge of the concave polygon, i.e., a whole interval (\u03bbj+1, \u03bbj), whereas the current iteration of CSBR is dedicated to a single value \u03bbj which is decreasing when the iteration number j increases.\nSecond, the computation of the next value \u03bbj+1 \u2264 \u03bbj in CSBR is only based on the violation of the lower bound of (9), corresponding to atom selections. In \u21130-PD, the upper bound is considered as well. This is the reason why the \u03bb-values are not scanned in a decreasing order anymore. This may improve\nthe very sparse solutions found in the early iterations within an increased computation time, as we will\nsee hereafter."}, {"heading": "V. NUMERICAL RESULTS", "text": "The algorithms are evaluated on two kinds of problems involving ill-conditioned dictionaries. The be-\nhavior of CSBR and \u21130-PD is first analyzed for simple examples. Then, we provide a detailed comparison with other nonconvex algorithms for many scenarii."}, {"heading": "A. Two generic problems", "text": "The sparse deconvolution problem takes the form y = h \u2217 x\u22c6 + n where the impulse response h is\na Gaussian filter of standard deviation \u03c3, and the noise n is assumed i.i.d. and Gaussian. The problem rereads y = Ax\u22c6+n where A is a convolution matrix. In the default setting, y and x are sampled at the\nsame frequency. h is approximated by a finite impulse response of length 6\u03c3 by thresholding the smallest values. A is a Toeplitz matrix of dimensions chosen so that any Gaussian feature h\u2217x\u22c6 if fully contained\nwithin the observation window {1, . . . ,m}. This implies that A is slightly undercomplete: m > n with m \u2248 n. Two simulated data vectors y are represented in Fig. 8(a,b) where x\u22c6 are k-sparse vectors with\nMarch 19, 2015 DRAFT\nk = 10 and 30, and the signal-to-noise ratio (SNR) is equal to 25 and 10 dB, respectively. It is defined by SNR = 10 log(\u2016Ax\u22c6\u201622/(m\u03c3 2 n)) where \u03c3 2 n is the variance of the noise process n.\nThe jump detection problem is illustrated on Fig. 8(c,d). Here, A is the squared dictionary (m = n) defined by Ai,j = 1 if i \u2265 j, and 0 otherwise. The atom aj codes for a jump at location j, and x\u22c6j matches the height of the jump. When x\u22c6 is k-sparse, Ax\u22c6 yields a piecewise constant signal with k pieces, x\u22c6 being the first-order derivative of the signal Ax\u22c6.\nBoth generic problems involve either square or slightly undercomplete dictionaries. The case of over-\ncomplete dictionaries will be discussed as well, e.g., by considering the deconvolution problem with\nundersampled observations y. The generic problems are already difficult because neighboring columns of\nA are highly correlated, and a number of fast algorithms that are efficient for well-conditioned dictionaries may fail to recover the support of x\u22c6. The degree of difficulty of the deconvolution problem is controlled\nby the width \u03c3 of the Gaussian impulse response and the sparsity k: for large values of k and/or \u03c3, the Gaussian features resulting from the convolution h\u2217x\u22c6 strongly overlap. For the jump detection problem,\nMarch 19, 2015 DRAFT\nall the step signals related to the atoms aj have overlapping supports."}, {"heading": "B. Empirical behavior of CSBR and \u21130-PD", "text": "1) Example: Consider the problem shown on Fig. 8(c). Because CSBR and \u21130-PD provide very similar results, we only show the CSBR results. CSBR delivers sparse solutions xj for decreasing \u03bbj , xj being the least-square solution supported by the j-th output of CSBR (Sj). Three sparse solutions xj are represented on Fig. 9. For the first solution (lowest value of |Sj |, largest \u03bbj), only the seven main jumps are being detected (Fig. 9(a)). The cardinality of Sj increases with j, and some other jumps are obtained together with possible false detections (Figs. 9(b,c)).\n2) Model order selection: It may often be useful to select a single solution xj . The proposed algorithms are compatible with most classical methods of model order selection [46], [47] because they are greedy\nalgorithms. Assuming that the variance of the observation noise is unknown, we distinguish two categories\nof cost functions for estimation of the order \u2016xj\u20160 = |Sj |. The first take the form minj{m log E(Sj) + \u03b1|Sj |} where \u03b1 equals 2, logm, and 2 log logm for the Akaike, Minimum Description Length (MDL) and Hannan and Quinn criteria, respectively [46]. The second are cross-validation criteria [48], [49]. The\nsparse approximation framework allows one to derive simplified expressions of the latter up to the storage\nof intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].\nFor the sparse deconvolution and jump detection problems, we found that the Akaike and cross\nvalidation criteria severely over-estimate the expected number of spikes. On the contrary, the MDL\nMarch 19, 2015 DRAFT\ncriterion yields quite accurate results. We found that the modified MDLc version dedicated to short data\nrecords (i.e., when the number of observations is moderately larger than the model order) [51] yields the\nbest results for all the scenarii we have tested. It reads:\nMarch 19, 2015 DRAFT\nfor noisy data, the spikes of smallest amplitudes are drowned in the noise. One cannot expect to detect\nthem.\n3) Further empirical observations: Fig. 11 is a typical display of the approximate \u21130-curves yielded by CSBR and \u21130-PD. The \u21130-PD curve is structurally continuous and concave whereas for the CSBR curve, there are two kinds of breakpoints depicted with black and white circles. The former are \u201ccontinuous\u201d\nbreakpoints. This occurs when no single replacement is done during the call to SBR (SBR(Sinit;\u03bbj) returns Sj = Sinit; see Tab. II). Otherwise, a discontinuity breakpoint (white circle) appears. In Fig. 11, the CSBR and \u21130-PD curves almost coincide for large \u03bb\u2019s, where only continuous breakpoints can be observed. For low \u03bb\u2019s, the \u21130-PD curve lays below the CSBR curve, and discontinuity breakpoints appear in the latter curve.\nFig. 12 provides some insight on the CSBR and \u21130-PD iterations for a sparse deconvolution problem with \u2016x\u22c6\u20160 = 17 and SNR = 20 dB. In the CSBR subfigures, the horizontal axis represents the number of single replacements: 60 replacements are being performed from the initial empty support during the\nsuccessive calls to SBR. For \u21130-PD, the horizontal axis shows the iteration number. At most two new supports are being included in the list of candidate subsets at each iteration. The number of effective\nsingle replacements is therefore increased by 0, 1 or 2. During the first 25 iterations, \u21130-PD mainly operates atom selections similar to CSBR. The explored subsets are thus of increasing cardinality and\n\u03bb is decreasing (Figs. 12(c,d)). From iterations 25 to 40, the very sparse solutions previously found\n(k \u2264 20) are improved as a series of atom de-selections is performed. They are being improved again\naround iteration 80. On the contrary, the sparsest solutions are never improved with CSBR, which works\nfor decreasing \u03bb\u2019s (Figs. 12(a,b)). For \u21130-PD, the early stopping parameter \u03bbstop may have a strong influence on the improvement of the sparsest solutions and the overall computation time. This point will\nbe further discussed below."}, {"heading": "C. Extensive comparisons", "text": "The proposed algorithms are compared with popular nonconvex algorithms for both problems intro-\nduced in subsection V-A with various parameter settings: problem dimension (m,n), ratio m/n, signal-tonoise ratio, cardinality of x\u22c6, and width \u03c3 of the Gaussian impulse response for the deconvolution problem.\nThe settings are listed on Table V for 10 scenarii. Because the proposed algorithms are orthogonal greedy\nalgorithms, they are better suited to problems in which the level of sparsity is moderate to high. We therefore restrict ourselves to the case where k = \u2016x\u22c6\u20160 \u2264 30.\nMarch 19, 2015 DRAFT\n1) Competing algorithms: We focus on the comparison with algorithms based on nonconvex penalties.\nIt is indeed increasingly acknowledged that the BPDN estimates are less accurate than sparse approxima-\ntion estimates based on nonconvex penalties. We do not consider forward greedy algorithms either; we\nalready showed that SBR is (unsurprisingly) more efficient than the simpler OMP and OLS algorithms [3].\nAmong the popular nonconvex algorithms, we consider:\n1) Iterative Reweighted Least Squares (IRLS) for \u2113q minimization, q < 1 [52]; 2) Iterative Reweighted \u21131 (IR\u21131) coupled with the penalty log(|xi|+ \u03b5) [20], [23], [53];\nMarch 19, 2015 DRAFT\nWe resort to a penalized least-square implementation for all algorithms, the only algorithm directly\nworking with the \u21130 penalty being L0LS-CD. We do not consider simpler thresholding algorithms (Iterative Hard Thresholding, CoSaMP, Subspace Pursuit) proposed in the context of compressive sensing since we\nfound that SBR behaves much better than these algorithms for ill-conditioned dictionaries [3]. We found\nthat L0LS-CD is more efficient than thresholding algorithms. Moreover, the cyclic descent approach is\nbecoming very popular in the recent sparse approximation literature [44], [56] although its speed of\nconvergence is sensitive to the quality of the initial solution. Here, we use the BPDN initial solution argmin x {\u2016y\u2212Ax\u201622+\u00b5\u2016x\u20161} where \u00b5 is set to half of the maximum tested \u03bb-value (more details will be given hereafter). This simple ad hoc setting allows us to get a rough initial solution that is nonzero\nand very sparse within a fast computation time.\nThe three other considered algorithms work with sparsity measures depending on an arbitrary parameter.\nRegarding IRLS, we set q = 0.5 or 0.1 as suggested in [52]. We chose to run IRLS twice, with q = 0.5\nand then q = 0.1 (with the previous output at q = 0.5 as initial solution) so that IRLS is less sensitive to\nlocal solutions at q = 0.1. SL0 is a GNC-like algorithm working for increasingly non-convex penalties\nMarch 19, 2015 DRAFT\n(i.e., Gaussian functions of decreasing widths). For simplicity reasons, we set the lowest width relative to the knowledge of the smallest nonzero amplitude of the ground truth solution x\u22c6. The basic SL0\nimplementation is dedicated to noise-free problems [43]. There exist several adaptations in the noisy\nsetting [55], [57] including the precursory work [58]. We chose the efficient implementation of [57] in\nwhich the original pseudo-inverse calculations are replaced by a quasi-Newton strategy using limited\nmemory BFGS updates. Finally, the IR\u21131 implementation depends on both the choice of parameter \u03b5 (which controls the degree of nonconvexity) and the \u21131 solver. We have tested two \u21131 solvers: the incrowd algorithm [59] together with an empirical setting of \u03b5 > 0, and \u21131 homotopy in the limit case \u03b5 \u2192 0, following [53]. We found that \u21131 homotopy is faster than in-crowd, mainly because the Matlab implementation of in-crowd (provided by the authors) makes calls to the quadprog built-in function,\nwhich is computationally expensive for large dimension problems.\n2) Numerical protocol: Because the competing algorithms work for a single \u03bb value, we need to define a grid, denoted by {\u03bbGi , i = 1, . . . , N\u03bb}, for comparison purposes. Such grid is defined in logscale for each of the 10 scenarii (k,A,SNR) defined in Table V. The number of grid points is N\u03bb = 11. For a given scenario, T = 30 trials are being performed in which k-sparse vectors x\u22c6(t) and noise vector n(t) are randomly drawn. This leads us to simulate T observation vectors y(t) = Ax\u22c6(t) + n(t) with t \u2208 {1, . . . , T}. Specifically, the location of the nonzero amplitudes in x\u22c6(t) are uniformly distributed and\nthe amplitude values are drawn according to an i.i.d. Gaussian distribution. For each trial t, all competing algorithms need to be run N\u03bb times with y(t) and \u03bbGi as inputs whereas CSBR and \u21130-PD are run only once since they deliver estimates for a continuum of values of \u03bb. Their solution for each \u03bbGi directly deduces from their set of output supports and the knowledge of both breakpoints surrounding \u03bbGi .\nThe algorithms are first evaluated in the optimization viewpoint: the related criteria are their capacity to\nreach a low value of J (x;\u03bb) and the corresponding CPU time. In this viewpoint, the proposed methods\nmight be somehow favored since they are more directly designed with the criterion J (x;\u03bb) in mind.\nOn the other hand, J (x;\u03bb) appears to be a natural indicator because solving either \u21130-minimization problem (1), (2) or (3) is the ultimate goal of any sparse approximation method. As detailed below,\nsome post-processing will be applied to the outputs of algorithms that do not rely on the \u21130-norm so that they are not strongly disadvantaged. Practically, we store the value of J (x;\u03bbGi ) found for each trial and each \u03bbGi . Averaging this value over the trials t yields a table TabJ(a, \u03bb G i ) where a denotes a candidate algorithm. Similarly, the CPU time is averaged over the trials t, leading to another table TabCPU(a, \u03bbGi ). Each table is represented separately as a 2D plot with a specific color for each algorithm: see, e.g.,\nFig. 13. CSBR and \u21130-PD are represented with continuous curves because J (x;\u03bb) is computed for a\nMarch 19, 2015 DRAFT\nminx J (x;\u03bb) CPU Time (seconds) minx J (x;\u03bb) CPU Time (seconds)\ncontinuum of \u03bb\u2019s, and the CPU time is computed only once.\nThe algorithms are also evaluated in terms of support recovery accuracy. For this purpose, let us first\ndefine the \u201csupport error\u201d as the minimum over i of the distance\n|S\u22c6(t)\\S(t, a, \u03bbGi )|+ |S(t, a, \u03bb G i )\\S \u22c6(t)| (16)\nbetween the support S\u22c6(t) of the unknown sparse vector x\u22c6(t) and the support S(t, a, \u03bbGi ) of the sparse reconstruction at \u03bbGi with algorithm a. (16) takes into account both numbers of false negatives |S\u22c6(t)\\S(t, a, \u03bbGi )| and of false positives |S(t, a, \u03bb G i )\\S \u22c6(t)|. Denoting by S(t, a, \u03bbGopt) \u2190 S(t, a, \u03bb G i ) the solution support that is the closest to S\u22c6(t) according to (16), we further consider the number of true positives in S(t, a, \u03bbGopt), defined as |S \u22c6(t) \u2229 S(t, a, \u03bbGopt)|. We will thus report:\n\u2022 the support error;\n\u2022 the corresponding number of true positives; \u2022 the corresponding model order |S(t, a, \u03bbGopt)|.\nMarch 19, 2015 DRAFT\nAveraging these measures over T trials yields the support error score SE(a), the true positive score TP(a)\nand the model order, denoted by Order(a). The numbers of false positives (FP) and of true/false negatives\ncan be directly deduced, e.g., FP(a) = Order(a) \u2212 TP(a).\nThe underlying idea in this analysis is that when SE is small (respectively, TP is high), the algorithms\nare likely to perform well provided that \u03bb is appropriately chosen. However, in practical applications,\nonly one estimate is selected using a suitable model selection criterion. We therefore provide additional\nevaluations of the MDLc estimate accuracy. For CSBR and \u21130-PD, all output supports are considered to compute the MDLc estimate as described in subsection V-B. For other algorithms, it is equal to one of the sparse reconstructions obtained at \u03bbGi for i \u2208 {1, . . . , N\u03bb}. The same three measures as above are computed for the MDLc estimate and averaged over T trials. They are denoted by MDLc-SE(a),\nMDLc-TP(a) and MDLc-Order(a).\n3) Technical adaptations for comparison purposes: Because IRLS and SL0 do not deliver sparse\nvectors in the strict sense, it is necessary to sparsify their outputs before computing their SE(a) score.\nThis is done by running one iteration of cyclic descent (L0LS-CD): most small nonzero amplitudes are\nthen thresholded to 0. Regarding the values of J (x;\u03bb), a post-processing is performed for algorithms\nthat do not rely on the \u21130-norm. This post-processing can be interpreted as a local descent of J (x;\u03bb). It consists in: (i) running one iteration of cyclic descent (L0LS-CD); (ii) computing the squared error\nrelated to the output support. L0LS-CD is indeed a local descent algorithm dedicated to J (x;\u03bb) but the\nconvergence towards a least-square minimizer is not reached in one iteration.\n4) Analysis in the optimization viewpoint: CSBR and \u21130-PD are always among the most accurate to minimize the cost function, as illustrated on Figs. 13, 14 and 15. We can clearly distinguish two\ngroups of algorithms on these figures: IRLS, L0LS-CD and SL0 one the one hand, and the OLS-based\nalgorithms (SBR, CSBR, \u21130-PD) and IR\u21131 on the other hand, which are the most accurate. We cannot clearly discriminate the accuracy of SBR and CSBR: one may behave slightly better than the other\ndepending on the scenarii. On the contrary, SBR and CSBR are often outperformed by \u21130-PD. The obvious advantage of CSBR and \u21130-PD over SBR and IR\u21131 is that they are \u21130-homotopy algorithms, i.e., a set of solutions are delivered for many sparsity levels, and the corresponding \u03bb-values are adaptively\nfound. On the contrary, the SBR output is related to a single \u03bb whose tuning may be tricky. Another\nadvantage over IR\u21131 is that the structure of forward-backward algorithms is simpler, as no call to any \u21131 solver is required. Moreover, the number of parameters to tune is lower: there is a single (early) stopping\nparameter \u03bbstop.\nThe price to pay for a better performance is an increase of the computation burden. On Figs. 13, 14\nMarch 19, 2015 DRAFT\nminx J (x;\u03bb) CPU Time (seconds) minx J (x;\u03bb) CPU Time (seconds)\nand 15, two lines are drawn for CSBR (respectively, for \u21130-PD). They are horizontal because the algorithm is run only once per trial, so there is a single computation time measurement. The first line corresponds\nto the overall computation time, i.e., from the start to the termination of CSBR / \u21130-PD. This time is often more expensive than for other algorithms. However, the latter times refer to a single execution for some \u03bbGi value. If one wants to recover sparse solutions for many \u03bb G i \u2019s, they must be cumulated. This is the reason why we have drawn a second line for CSBR and \u21130-PD corresponding to a normalization (by N\u03bb = 11) of the overall computation time. In this viewpoint, the CPU time of CSBR and \u21130-PD are very reasonable.\nThe computation time depends on many factors among which the implementation of algorithms\n(including the memory storage) and the chosen stopping rules. We have followed an homogeneous\nimplementation of algorithms to make the CPU time comparisons meaningful. We have defined two\nsets of stopping rules depending on the problem dimension. The default parameters apply to medium\nsize problems (m = 300). They are relaxed for problems of larger dimension (m > 500) to avoid huge computational costs. The stopping rule of CSBR and \u21130-PD is always \u03bb \u2264 \u03bbstop = \u03b1\u03bbG1 with \u03b1 = 1\nMarch 19, 2015 DRAFT\nfor CSBR and 0.5 (medium size) or 0.8 (large size) for \u21130-PD. For L0LS-CD, the maximum number of cyclic descents (update of every amplitude xi) is set to 60 or 10 depending on the dimension. For SL0, we have followed the default setting of [43] for the rate of deformation of the nonconvex penalty.\nThe number of BFGS iterations done in the local minimization steps for each penalty is set to L = 40\nor 5. It is set to 5L for the last penalty which is the most nonconvex. Regarding IRLS and IRL\u21131, we keep the same settings whatever the dimension since the computation times remain reasonable for large\ndimensions. Finally, SBR does not require any arbitrary stopping rule. The problems of large dimensions\ncorrespond to scenarii C and D. We observe on Fig. 13 that the comparison (trade-off performance vs\ncomputation time) is now clearly in favor of CSBR and \u21130-PD. IR\u21131 remains very competitive although\nMarch 19, 2015 DRAFT\nthe average numerical cost becomes larger.\n5) Analysis in the support recovery viewpoint: The support recovery performance is only shown for\nthe scenarii E to J (Tabs. VI and VII). For noisy deconvolution problems, these results are omitted\nbecause the support error is often quite large and the true positive scores are low whatever the algorithm,\nespecially for scenarii B to D. Specifically, the least support error always exceeds 20, 10, 10 and 32 for\nthe scenarii A to D (k = 30, 10, 10 and 30, respectively). For such difficult problems, one can hardly\ndiscriminate algorithms based on simple binary tests such as the true positive rate. More sophisticated\nlocalization tests are non binary and would take into account the distance between the location of the true\nspikes and their wrong estimates [60]. It is noticeable, though, that the MDLc estimator delivers subsets\nof realistic cardinality for scenarii A to D (e.g., the subsets found with CSBR are of cardinalities 33,\n9, 15 and 38, the true cardinalities being 30, 10, 10 and 30). The model orders are also quite accurate\nfor the noisy jump detection problem (Tab. VI) whereas the true support is often partially detected by\nseveral of the considered algorithms. Here, CSBR and \u21130-PD are among the best algorithms in terms of\nMarch 19, 2015 DRAFT\nsupport error.\nThe results of Tab. VII and Fig. 15 correspond to the deconvolution problem in noise-free case. The\ndata y are undersampled so that the dictionary A is overcomplete. The undersampling rate \u2206 \u2248 m/n is\nset to 2 in scenarii H and I and 4 in scenario J. Again, CSBR and \u21130-PD are among the best (SE, TP, MDLc-order) especially for the most difficult problem J.\n6) Overcomplete dictionaries with noise: We now provide arguments indicating that the proposed\nalgorithms are competitive as well for noisy problems with overcomplete dictionaries. The detailed\nexperiments commented below are not reported for space reasons.\nWe have first considered the noisy deconvolution problem with \u2206 = 2 or 4 leading to overcomplete\ndictionaries, the other parameters being set as in scenarii A to D. Although the data approximation is\nqualitatively good for CSBR and \u21130-PD, the SE and TP scores are very weak. It is hard to discriminate the\nMarch 19, 2015 DRAFT\nperformance of algorithms because these measures are very weak for all considered algorithms. Moreover,\nthe values of J (\u03bb) found for most algorithms are often similar.\nWe have also considered an adaptive spline approximation problem generalizing the jump detection\nproblem to the approximation of a signal using piecewise polynomials of degree P = 1 or 2 [3]. The jump\ndetection problem can indeed be thought of as the approximation with a piecewise constant signal (P = 0).\nThe generalized version [3] is inspired from the regression spline modeling in [61]. Now, the dictionary\natoms are related to the detection of the locations of jumps, changes of slopes and changes of curvatures in the signal y (subdictionaries A0, A1 and A2). The dictionary then takes the form A \u2190 [A0,A1] or A \u2190 [A0,A1,A2] where each sub-dictionary Ap (p \u2264 P ) is formed of shifted versions of the one-sided power function i 7\u2192 [max(i, 0)]p. The size of the full dictionary A is approximately m \u00d7 (P + 1)m.\nHence, it becomes overcomplete as soon as P \u2265 1. We have shown [3] that SBR is competitive when\nP = 1 or 2. We have carried out new tests confirming that CSBR and \u21130-PD are more efficient than their competitors in terms of values of J (\u03bb). However, the rate of true positives is low for P \u2265 1 since the\nlocation of the change of slopes and of curvatures can hardly be exactly recovered from noisy data."}, {"heading": "VI. SOFTWARE", "text": "The Matlab implementation of the proposed CSBR and \u21130-PD algorithms is available at\nwww.cran.univ-lorraine.fr/perso/charles.soussen/software.html including programs showing how to call\nthese functions."}, {"heading": "VII. CONCLUSION", "text": "The choice of a relevant sparse approximation algorithm relies on a trade-off between the desired\nperformance and the computation time one is ready to spend. The proposed algorithms are relatively\nexpensive but very well suited to inverse problems inducing highly correlated dictionaries. A reason is that they have the capacity to escape from local minimizers of J (x;\u03bb) = \u2016y\u2212Ax\u201622 +\u03bb\u2016x\u20160 [3]. This behavior is in contrast with other classical sparse algorithms.\nWe have shown the usefulness and efficiency of the two SBR extensions when the level of sparsity\nis moderate to high, i.e., k/min(m,n) is lower than 0.1. They remain competitive when k/min(m,n)\nranges between 0.1 and 0.2, and their performance gradually degrade for weaker levels of sparsity, which\nis an expected behavior for such greedy type algorithms. For a single \u03bb, CSBR is as efficient as SBR,\nand \u21130-PD improves the SBR and CSBR performance within a larger computation cost. The main benefit over SBR is that sparse solutions are provided for a continuum of \u03bb-values, enabling the utilization of\nMarch 19, 2015 DRAFT\nany classical order selection method. We found that the MDL criterion yields very accurate estimates of\nthe cardinality \u2016x\u20160.\nOur perspectives include the proposal of forward-backward search algorithms that will be faster than\nSBR and potentially more efficient. In the standard version of SBR, CSBR and \u21130-PD, a single replacement refers to the insertion or removal of a dictionary element. The cost of an iteration is essentially related to\nthe n linear system resolutions done to test single replacements for all dictionary atoms. The proposed\nalgorithms obviously remain valid when working with a larger neighborhood, e.g., when testing the\nreplacement of two atoms simultaneously, but their complexity becomes huge. To avoid such numerical\nexplosion, one may rather choose not to carry out all the replacement tests, but only some tests that are\nlikely to be effective. Extensions of OMP and OLS were recently proposed in this spirit [36] and deserve\nconsideration for proposing efficient forward-backward algorithms."}, {"heading": "APPENDIX A", "text": "PROPERTIES OF THE \u21130 REGULARIZATION PATHS\nIn this appendix, we prove that the \u21130-penalized path S\u22c6P (see Definition 2) is piecewise constant (Theorem 1) and is a subset of the \u21130-constrained regularization path S\u22c6C (Theorem 2). We will denote the \u21130-curve by \u03bb 7\u2192 J \u22c6(\u03bb) = minS{J\u0302 (S;\u03bb)}. Let us recall that this function is concave and affine on each interval (\u03bb\u22c6i+1, \u03bb \u22c6 i ), with i \u2208 {0, . . . , I} (Definition 1). Moreover, \u03bb \u22c6 I+1 = 0 and \u03bb \u22c6 0 = +\u221e."}, {"heading": "A. Proof of Theorem 1", "text": "We prove Theorem 1 together with the following lemma, which is informative about the content of\nS\u22c6P(\u03bb) for the breakpoints \u03bb = \u03bb \u22c6 i .\nLemma 2 Let i \u2208 {1, . . . , I \u2212 1}. Then, for all \u03bb \u2208 (\u03bb\u22c6i+1, \u03bb \u22c6 i ), S \u22c6 P(\u03bb) \u2282 S \u22c6 P(\u03bb \u22c6 i+1) \u2229 S \u22c6 P(\u03bb \u22c6 i ).\nFor the first and last intervals, we have:\n\u2022 For all \u03bb \u2208 (0, \u03bb\u22c6I), S \u22c6 P(\u03bb) \u2282 S \u22c6 P(\u03bb \u22c6 I). \u2022 For all \u03bb \u2208 (\u03bb\u22c61,+\u221e), S \u22c6 P(\u03bb) = {\u2205} \u2282 S \u22c6 P(\u03bb \u22c6 1).\nProof of Theorem 1: By definition, the \u21130-curve is the concave envelope of the (finite) set of lines\nS for all possible subsets S. Because it is affine on the i-th interval (\u03bb\u22c6i+1, \u03bb \u22c6 i ), J \u22c6(\u03bb) coincides with J\u0302 (Si;\u03bb) = E(Si) + \u03bb|Si|, where Si is some optimal subset for all \u03bb \u2208 (\u03bb\u22c6i+1, \u03bb \u22c6 i ).\nLet \u03bb \u2208 (\u03bb\u22c6i+1, \u03bb \u22c6 i ) and S \u2208 S \u22c6 P(\u03bb). Then, J\u0302 (S;\u03bb) = J\u0302 (Si;\u03bb). It follows that both lines S and Si necessarily coincide; otherwise, they would intersect at \u03bb, and line S would lay below Si on either\nMarch 19, 2015 DRAFT\ninterval (\u03bb\u22c6i+1, \u03bb) or (\u03bb, \u03bb \u22c6 i ), which contradicts the definition of Si. We conclude that S \u2208 S \u22c6 P(\u03bb \u2032) for all \u03bb\u2032 \u2208 (\u03bb\u22c6i+1, \u03bb \u22c6 i ).\nWe have shown that the content of S\u22c6P(\u03bb) does not depend on \u03bb when \u03bb \u2208 (\u03bb \u22c6 i+1, \u03bb \u22c6 i ).\nProof of Lemma 2: The first result S\u22c6P(\u03bb) \u2282 S \u22c6 P(\u03bb \u22c6 i+1) \u2229 S \u22c6 P(\u03bb \u22c6 i ) is obtained by slightly adapting\nthe proof of Theorem 1: replace (\u03bb\u22c6i+1, \u03bb \u22c6 i ) by the closed interval [\u03bb \u22c6 i+1, \u03bb \u22c6 i ], and set \u03bb \u2032 to both endpoints of this interval.\nThe second and third results are obtained similarly, by considering the intervals (0, \u03bb\u22c6I ] and [\u03bb \u22c6 1,+\u221e),\nand setting \u03bb\u2032 \u2190 \u03bb\u22c6I and \u03bb \u2032 \u2190 \u03bb\u22c61, respectively. It is obvious that S \u22c6 P(\u03bb) reduces to the empty support for \u03bb > \u03bb\u22c61 since the \u21130-curve is constant for \u03bb > \u03bb \u22c6 1."}, {"heading": "B. Proof of Theorem 2", "text": "The first result is straightforward: for any \u03bb and for S \u2208 S\u22c6P(\u03bb), we have S \u2208 S \u22c6 C(|S|). Otherwise, there would exist S\u2032 with |S\u2032| \u2264 |S| and E(S\u2032) < E(S). Then, J\u0302 (S\u2032;\u03bb) < J\u0302 (S;\u03bb) would contradict S \u2208 S\u22c6P(\u03bb).\nTo prove the second result, let us first show that for any i, \u2203ki : \u2200\u03bb \u2208 (\u03bb\u22c6i+1, \u03bb \u22c6 i ), S \u22c6 P(\u03bb) \u2282 S \u22c6 C(ki). Let S \u2208 S\u22c6P(\u03bb) for some \u03bb \u2208 (\u03bb \u22c6 i+1, \u03bb \u22c6 i ). Theorem 1 implies that S \u2208 S \u22c6 P(\u03bb) for any \u03bb \u2208 (\u03bb \u22c6 i+1, \u03bb \u22c6 i ). Therefore, J \u22c6(\u03bb) = J\u0302 (S;\u03bb) for \u03bb \u2208 (\u03bb\u22c6i+1, \u03bb \u22c6 i ), and the slope of line S, i.e., |S|, is constant whatever S \u2208 S\u22c6P(\u03bb) and \u03bb \u2208 (\u03bb \u22c6 i+1, \u03bb \u22c6 i ). Let us denote this constant by ki = |S|. According to the first paragraph of the proof, S \u2208 S\u22c6P(\u03bb) implies that S \u2208 S \u22c6 C(ki).\nLet us prove the reverse inclusion S\u22c6C(ki) \u2282 S \u22c6 P(\u03bb). Let \u03bb \u2208 (\u03bb \u22c6 i+1, \u03bb \u22c6 i ) and S \u2208 S \u22c6 C(ki). First, we have |S| \u2264 ki. Second, for any S\u2032 \u2208 S\u22c6P(\u03bb), we have |S \u2032| = ki by definition of ki. We also have that E(S\u2032) = E(S) because S\u22c6P(\u03bb) \u2282 S \u22c6 C(ki). Finally, J\u0302 (S \u2032;\u03bb) \u2265 J\u0302 (S;\u03bb). S\u2032 \u2208 S\u22c6P(\u03bb) implies that S \u2208 S\u22c6P(\u03bb). This concludes the proof of the second result."}], "references": [{"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Description of the minimizers of least squares regularized with l0 norm. Uniqueness of the global minimizer", "author": ["M. Nikolova"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Computational methods for sparse solution of linear inverse problems\u201d, Proc. IEEE, invited paper (Special Issue \u201cApplications of sparse representation and compressive sensing\u201d)", "author": ["J.A. Tropp", "S.J. Wright"], "venue": "vol. 98,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "in Proc. 27th Asilomar Conf. on Signals, Systems and Computers,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Orthogonal least squares methods and their application to non-linear system identification", "author": ["S. Chen", "S.A. Billings", "W. Luo"], "venue": "Int. J. Control,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Subset selection in regression", "author": ["A.J. Miller"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Forward sequential algorithms for best basis selection", "author": ["S.F. Cotter", "J. Adler", "B.D. Rao", "K. Kreutz-Delgado"], "venue": "IEE Proc. Vision, Image and Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Optimized orthogonal matching pursuit approach", "author": ["L. Rebollo-Neira", "D. Lowe"], "venue": "IEEE Signal Process. Lett., vol. 9,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Iterative thresholding for sparse approximations", "author": ["T. Blumensath", "M.E. Davies"], "venue": "J. Fourier Anal. Appl.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Appl. Comp. Harmonic Anal., vol. 26,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Multiple regression analysis", "author": ["M.A. Efroymson"], "venue": "Mathematical Methods for Digital Computers, A. Ralston and H. S. Wilf, Eds., vol. 1, pp. 191\u2013203. Wiley, New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1960}, {"title": "Forward and backward stepping in variable selection", "author": ["K.N. Berk"], "venue": "J. Statist. Comput. Simul.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1980}, {"title": "Adaptive forward-backward greedy algorithm for learning sparse representations", "author": ["T. Zhang"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems", "author": ["M.A.T. Figueiredo", "R.D. Nowak", "S.J. Wright"], "venue": "IEEE J. Sel. Top. Signal Process.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "l1 \u2212 l2 optimization in signal and image processing", "author": ["M. Zibulevsky", "M. Elad"], "venue": "IEEE Sig. Proc. Mag., vol. 27,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Minimizing nonconvex functions for sparse vector reconstruction", "author": ["N. Mourad", "J.P. Reilly"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Iterative reweighted l1 and l2 methods for finding sparse solutions", "author": ["D.P. Wipf", "S. Nagarajan"], "venue": "IEEE J. Sel. Top. Signal Process. (Special issue on Compressive Sensing),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A general framework for sparsity-based denoising and inversion", "author": ["A. Gholami", "S.M. Hosseini"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Universal regularizers for robust sparse coding and modeling", "author": ["I. Ram\u0131\u0301rez", "G. Sapiro"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Sparse signal recovery by difference of convex functions algorithms", "author": ["H.A. Le Thi", "B.T. Nguyen Thi", "H.M. Le"], "venue": "Intelligent Information and Database Systems, A. Selamat, N. T. Nguyen, and H. Haron, Eds., Berlin", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse signal estimation by maximally sparse convex optimization", "author": ["I. Selesnick", "I. Bayram"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A new approach to variable selection in least squares problems", "author": ["M.R. Osborne", "B. Presnell", "B.A. Turlach"], "venue": "IMA Journal of Numerical Analysis, vol. 20, no. 3, pp. 389\u2013403", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Ann. Statist., vol. 32,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems", "author": ["I. Das", "J.E. Dennis"], "venue": "Structural optimization,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Survey of multi-objective optimization methods for engineering", "author": ["R.T. Marler", "J.S. Arora"], "venue": "Structural and Multidisciplinary Optimization,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Probing the Pareto frontier for basis pursuit solutions", "author": ["E. van den Berg", "M.P. Friedlander"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Subset regression with stepwise directed search", "author": ["P.M.T. Broersen"], "venue": "J. R. Statist. Soc. C, vol. 35, no. 2, pp. 168\u2013177", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1986}, {"title": "A bidirectional greedy heuristic for the subspace selection problem\u201d, in Engineering stochastic local search algorithms. Designing, implementing and analyzing effective heuristics", "author": ["D. Haugland"], "venue": "vol. 4638 of Lect. Notes Comput. Sci.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Projection-based and look-ahead strategies for atom selection", "author": ["S. Chatterjee", "D. Sundman", "M. Vehkaper\u00e4", "M. Skoglund"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "A continuation approach to estimate a solution path of mixed L2-L0 minimization problems\u201d, in Signal Processing with Adaptive Sparse Structured Representations (SPARS workshop)", "author": ["J. Duan", "C. Soussen", "D. Brie", "J. Idier"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Multipath matching pursuit", "author": ["S. Kwon", "J. Wang", "B. Shim"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "The Viterbi algorithm for subset selection", "author": ["S. Maymon", "Y. Eldar"], "venue": "IEEE Signal Process. Lett., vol. 22,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Numerical solutions by the continuation method", "author": ["E. Wasserstrom"], "venue": "SIAM Rev., vol. 15,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1973}, {"title": "Homotopy continuation for sparse signal representation", "author": ["D.M. Malioutov", "M. \u00c7etin", "A.S. Willsky"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic  l0minimization", "author": ["J. Trzasko", "A. Manduca"], "venue": "IEEE Trans. Medical Imaging,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "A fast approach for overcomplete sparse decomposition based on smoothed l norm", "author": ["G.H. Mohimani", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "SparseNet: Coordinate descent with nonconvex penalties", "author": ["R. Mazumder", "J.H. Friedman", "T. Hastie"], "venue": "J. Acoust. Soc. Amer.,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Practical approximate solutions to linear operator equations when the data are noisy", "author": ["G. Wahba"], "venue": "SIAM J. Num. Anal., vol. 14, no. 4, pp. 651\u2013667", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1977}, {"title": "Generalized cross-validation as a method for choosing a good ridge parameter", "author": ["G.H. Golub", "M. Heath", "G. Wahba"], "venue": "Technometrics, vol. 21,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1979}, {"title": "On the relation between sparse reconstruction and parameter estimation with model order selection", "author": ["C.D. Austin", "R.L. Moses", "J.N. Ash", "E. Ertin"], "venue": "IEEE J. Sel. Top. Signal Process.,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Improved iteratively reweighted least squares for unconstrained smoothed lq minimization", "author": ["M.-J. Lai", "Y. Xu", "W. Yin"], "venue": "SIAM J. Num. Anal.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "The adaptive Lasso and its oracle properties", "author": ["H. Zou"], "venue": "J. Acoust. Soc. Amer., vol. 101,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2006}, {"title": "Sparse coloured system identification with guaranteed stability", "author": ["A.J. Seneviratne", "V. Solo"], "venue": "IEEE Conference on Decision and Control,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Robust-sl0 for stable sparse representation in noisy settings", "author": ["A. Eftekhari", "M. Babaie-Zadeh", "C. Jutten", "H.A. Moghaddam"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "lq sparsity penalized linear regression with cyclic descent", "author": ["G. Marjanovic", "V. Solo"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Sparse channel estimation of mimo-ofdm systems with unconstrained smoothed l0-norm-regularized least squares compressed sensing", "author": ["X. Ye", "W.-P. Zhu", "A. Zhang", "J. Yan"], "venue": "EURASIP J. Wireless Comm. and Networking,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "Superresolution of noisy band-limited data by data adaptive regularization and its application to seismic trace inversion", "author": ["N. Saito"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1990}, {"title": "A novel spike distance", "author": ["M.C. van Rossum"], "venue": "Neural Computation,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2001}, {"title": "Multivariate adaptive regression splines", "author": ["J.H. Friedman"], "venue": "Ann. Statist.,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "They are known to be NP-hard except for specific cases [1].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "is worth being considered, where \u03bb expresses the trade-off between the quality of approximation and the sparsity level [2].", "startOffset": 119, "endOffset": 122}, {"referenceID": 2, "context": "At each iteration, a new atom is appended to the current subset, therefore gradually improving the quality of approximation [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Greedy algorithms include, by increasing order of complexity: Matching Pursuit (MP) [5], Orthogonal Matching Pursuit (OMP) [6], and Orthogonal Least", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 8, "context": "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": ", [18], [19] and [20]\u2013[27] for convex (l1) and nonconvex relaxation, respectively.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [18], [19] and [20]\u2013[27] for convex (l1) and nonconvex relaxation, respectively.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": ", [18], [19] and [20]\u2013[27] for convex (l1) and nonconvex relaxation, respectively.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "It is noticeable that BPDN leads to stepwise algorithms [18], [28] including the popular l1-homotopy [28]\u2013[30], a forward-backward greedy search whose complexity is close to that of OMP.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "It is noticeable that BPDN leads to stepwise algorithms [18], [28] including the popular l1-homotopy [28]\u2013[30], a forward-backward greedy search whose complexity is close to that of OMP.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "It is referred to as \u201cLARS with the LASSO modification\u201d in [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "2 is a classical bi-objective representation where each axis is related to a single objective [31], namely |S| and E(S).", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "In bi-objective optimization, a point S is called Pareto optimal when no other point S\u2032 can decrease both objectives [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "On the contrary, the non-supported solutions cannot [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "\u2016x\u20161 \u2264 t for all t [33].", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "Now, the weighted sum formulation (3) may not yield the same solutions as the constrained formulations (1) and (2) because the l0-norm is nonconvex [2].", "startOffset": 148, "endOffset": 151}, {"referenceID": 17, "context": "Many authors actually discourage the direct optimization of J because there are a very large number of local minimizers [20], [23].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "3) Positioning with respect to other stepwise algorithms: In statistical regression, the word \u201cstepwise\u201d originally refers to Efroymson\u2019s algorithm [15], proposed in 1960 as an empirical extension of forward selection (i.", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "Other stepwise algorithms were proposed in the 1980\u2019s [8, Chapter 3] among which Berk\u2019s and Broersen\u2019s algorithms [16], [34].", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "Other stepwise algorithms were proposed in the 1980\u2019s [8, Chapter 3] among which Berk\u2019s and Broersen\u2019s algorithms [16], [34].", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "CSBR and l0-PD both read as descent algorithms in different senses: CSBR, first sketched in [37], repeatedly minimizes J (x;\u03bb) for decreasing \u03bb\u2019s.", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "The idea of maintaining a list of support candidates was recently developed within the framework of forward selection [38], [39].", "startOffset": 118, "endOffset": 122}, {"referenceID": 32, "context": "The idea of maintaining a list of support candidates was recently developed within the framework of forward selection [38], [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 31, "context": "In contrast, the supports in the list are all candidate solutions to solve the same problem in [38], [39].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "In contrast, the supports in the list are all candidate solutions to solve the same problem in [38], [39].", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "tuning some continuous hyperparameter [40].", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "BPDN is solved for decreasing hyperparameter values using the solution for each value as a warm starting point for the next value [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 23, "context": "l1-homotopy [28], [30], [41] exploits that the l1 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces.", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "l1-homotopy [28], [30], [41] exploits that the l1 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces.", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "Second, the continuous approximation of the (discrete) l0 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the resolution of continuous optimization problems with warm start initialization.", "startOffset": 70, "endOffset": 74}, {"referenceID": 36, "context": "Second, the continuous approximation of the (discrete) l0 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the resolution of continuous optimization problems with warm start initialization.", "startOffset": 122, "endOffset": 126}, {"referenceID": 37, "context": "Although the full reconstruction of the l0-regularization path has been rarely addressed, it is noticeable that a GNC-like approach, called SparseNet, aims to gradually update some estimation of the regularization path induced by increasingly non-convex sparsity measures [44].", "startOffset": 272, "endOffset": 276}, {"referenceID": 26, "context": "Because the influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is modified [44].", "startOffset": 46, "endOffset": 50}, {"referenceID": 37, "context": "Because the influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is modified [44].", "startOffset": 123, "endOffset": 127}, {"referenceID": 23, "context": "The \u03bb-values are rather adaptively computed similar to the l1-homotopy principle [28], [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "In the following, the formulation (5) will be omitted because it leads to the same l0-regularization path as formulation (4) [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "Therefore, both l0-regularization paths may not coincide [2], [31].", "startOffset": 57, "endOffset": 60}, {"referenceID": 24, "context": "Therefore, both l0-regularization paths may not coincide [2], [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "SBR coincides with the well-known OLS algorithm [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 23, "context": "Our continuation strategy is inspired by l1-homotopy which recursively computes the minimizers of \u2016y\u2212Ax\u20162 +\u03bb\u2016x\u20161 when \u03bb is continuously decreasing [28]\u2013[30].", "startOffset": 152, "endOffset": 156}, {"referenceID": 38, "context": "The second are cross-validation criteria [48], [49].", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "The second are cross-validation criteria [48], [49].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "The sparse approximation framework allows one to derive simplified expressions of the latter up to the storage of intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].", "startOffset": 188, "endOffset": 191}, {"referenceID": 40, "context": "The sparse approximation framework allows one to derive simplified expressions of the latter up to the storage of intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].", "startOffset": 199, "endOffset": 203}, {"referenceID": 41, "context": "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ \u03b5) [20], [23], [53];", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ \u03b5) [20], [23], [53];", "startOffset": 209, "endOffset": 213}, {"referenceID": 42, "context": "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ \u03b5) [20], [23], [53];", "startOffset": 215, "endOffset": 219}, {"referenceID": 43, "context": "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "Moreover, the cyclic descent approach is becoming very popular in the recent sparse approximation literature [44], [56] although its speed of convergence is sensitive to the quality of the initial solution.", "startOffset": 109, "endOffset": 113}, {"referenceID": 45, "context": "Moreover, the cyclic descent approach is becoming very popular in the recent sparse approximation literature [44], [56] although its speed of convergence is sensitive to the quality of the initial solution.", "startOffset": 115, "endOffset": 119}, {"referenceID": 41, "context": "1 as suggested in [52].", "startOffset": 18, "endOffset": 22}, {"referenceID": 36, "context": "The basic SL0 implementation is dedicated to noise-free problems [43].", "startOffset": 65, "endOffset": 69}, {"referenceID": 44, "context": "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].", "startOffset": 53, "endOffset": 57}, {"referenceID": 46, "context": "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].", "startOffset": 59, "endOffset": 63}, {"referenceID": 47, "context": "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].", "startOffset": 94, "endOffset": 98}, {"referenceID": 46, "context": "We chose the efficient implementation of [57] in which the original pseudo-inverse calculations are replaced by a quasi-Newton strategy using limited memory BFGS updates.", "startOffset": 41, "endOffset": 45}, {"referenceID": 42, "context": "We have tested two l1 solvers: the incrowd algorithm [59] together with an empirical setting of \u03b5 > 0, and l1 homotopy in the limit case \u03b5 \u2192 0, following [53].", "startOffset": 154, "endOffset": 158}, {"referenceID": 36, "context": "For SL0, we have followed the default setting of [43] for the rate of deformation of the nonconvex penalty.", "startOffset": 49, "endOffset": 53}, {"referenceID": 48, "context": "More sophisticated localization tests are non binary and would take into account the distance between the location of the true spikes and their wrong estimates [60].", "startOffset": 160, "endOffset": 164}, {"referenceID": 49, "context": "The generalized version [3] is inspired from the regression spline modeling in [61].", "startOffset": 79, "endOffset": 83}, {"referenceID": 29, "context": "Extensions of OMP and OLS were recently proposed in this spirit [36] and deserve consideration for proposing efficient forward-backward algorithms.", "startOffset": 64, "endOffset": 68}], "year": 2015, "abstractText": "Sparse signal restoration is usually formulated as the minimization of a quadratic cost function \u2016y\u2212Ax\u201622, where A is a dictionary and x is an unknown sparse vector. It is well-known that imposing an l0 constraint leads to an NP-hard minimization problem. The convex relaxation approach has received considerable attention, where the l0-norm is replaced by the l1-norm. Among the many efficient l1 solvers, the homotopy algorithm minimizes \u2016y \u2212Ax\u2016 2 + \u03bb\u2016x\u20161 with respect to x for a continuum of \u03bb\u2019s. It is inspired by the piecewise regularity of the l1-regularization path, also referred to as the homotopy path. In this paper, we address the minimization problem \u2016y \u2212Ax\u2016 2 + \u03bb\u2016x\u20160 for a continuum of \u03bb\u2019s and propose two heuristic search algorithms for l0-homotopy. Continuation Single Best Replacement is a forward-backward greedy strategy extending the Single Best Replacement algorithm, previously proposed for l0-minimization at a given \u03bb. The adaptive search of the \u03bb-values is inspired by l1-homotopy. l0 Regularization Path Descent is a more complex algorithm exploiting the structural properties of the l0-regularization path, which is piecewise constant with respect to \u03bb. Both algorithms are empirically evaluated for difficult inverse problems involving ill-conditioned dictionaries. Finally, we show that they can be easily coupled with usual methods of model order selection. This work was carried out in part while C. Soussen was visiting IRCCyN during the academic year 2010-2011 with the financial support of CNRS. C. Soussen and D. Brie are with the Universit\u00e9 de Lorraine and CNRS at the Centre de Recherche en Automatique de Nancy (UMR 7039).Campus Sciences, B.P. 70239, F-54506 Vand\u0153uvre-l\u00e8s-Nancy, France. Tel: (+33)-3 83 59 56 43, Fax: (+33)-3 83 68 44 62. E-mail: charles.soussen@univ-lorraine.fr, david.brie@univ-lorraine.fr. J. Idier is with L\u2019UNAM Universit\u00e9, Ecole Centrale Nantes and CNRS at the Institut de Recherche en Communications et Cybern\u00e9tique de Nantes (UMR 6597), 1 rue de la No\u00eb, BP 92101, F-44321 Nantes Cedex 3, France. Tel: (+33)-2 40 37 69 09, Fax: (+33)-2 40 37 69 30. E-mail: jerome.idier@irccyn.ec-nantes.fr. J. Duan was with CRAN. He is now with the Department of Biomedical Engineering, Xi\u2019an Jiaotong University. No. 28, Xianning West Road, Xi\u2019an 710049, Shaanxi Province, China. Tel: (+86)-29-82 66 86 68, Fax: (+86)-29 82 66 76 67. E-mail: junbo.duan@mail.xjtu.edu.cn. March 19, 2015 DRAFT", "creator": "LaTeX with hyperref package"}}}