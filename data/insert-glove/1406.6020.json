{"id": "1406.6020", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2014", "title": "Stationary Mixing Bandits", "abstract": "eastin We study the bandit problem where 13.1-mile arms stoppini are associated critchell with stationary horyn phi - vadodara mixing ferreria processes 496 and where blagoi rewards kalecik are therefore dependent: the cemetry question that steamrolled arises from ustari this ohad setting is ashkar that laneway of katsaros recovering ailor some unawares independence by uwharrie ignoring championing the value eastcote of pacquiao some vicious rewards. takfir As disneys we schneerson shall see, the poonam bandit 50mm problem nafi we khussa tackle requires adovasio us to address hawkin the exploration / stayner exploitation / dhimmis independence gyude trade - hypoid off. free-flowing To cinematheque do himitsu so, we kirkley provide a bmx UCB strategy together wief with plop a general regret analysis for the palghar case abakaliki where khordadian the 1.4880 size of hurdlers the grabovski independence musson blocks (zijn the black-throated ignored rewards) kanniyakumari is fixed co-sponsors and mld we go schenker a step shaded beyond by providing galatas an meia algorithm duaner that is bezirkshauptmannschaften able to compute unprincipled the d'arch\u00e9ologie size of the independence blocks paal from the data. Finally, we doughboy give euro422 an repulsive analysis destructs of our bandit maribor problem in the 673,000 restless 1910-1911 case, i. poup\u00e9e e. , 66.01 in the situation iceman where the backlund time counters kelheim for cablevision all mixing processes chop simultaneously evolve.", "histories": [["v1", "Mon, 23 Jun 2014 18:48:59 GMT  (19kb)", "http://arxiv.org/abs/1406.6020v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["julien audiffren", "liva ralaivola"], "accepted": false, "id": "1406.6020"}, "pdf": {"name": "1406.6020.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n60 20\nv1 [\ncs .L\nG ]\n2 3\nJu n\n20 14"}, {"heading": "1 Introduction", "text": "Bandit with mixing arms. The bandit problem consists in an agent who has to choose at each step between K arms. A stochastic process is associated to each arm, and pulling an arm produces a reward which is the realization of the corresponding stochastic process. The objective of the agent is to maximize its long term reward. It is classically assumed that the stochastic process associated to each arm is a sequence of independently and identically distributed (i.i.d) random variables (see, e.g. [1]). In that case, the challenge the agent has to face is the well-known exploration/exploitation problem: she has to simultaneously make sure that she collects information from all arms to try to identify the most rewarding ones \u2014this is exploration\u2014 and to maximize the rewards along the sequence of pulls she performs \u2014this is exploitation. Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1]. We propose to go a step further than the i.i.d setting and to work in the situation where the process associated with each arm is a stationary \u03d5-mixing process and the rewards are thus dependent from one another, but with a strength of dependence that weakens over time. From an application point of view, this is a reasonable dependence structure: if a user clicks on some ad (a typical use of bandit algorithms) at some point in time, it is very unlikely that she will click again on this ad in the near future. As it shall appear in the sequel, working with such dependent observations poses the question of how informative are some of the rewards with respect to the value of an arm since, because of the dependencies and the high correlation between close-by (in time) rewards, they might not reflect the true \u2019value\u2019 of the arms. However, as the dependencies weaken over time, some kind of independence might be recovered if rewards are ignored. This actually requires to deal with a new trade-off exploration/exploitation/independence that need be precisely handled.\nRested and Restless case. A closely related setup that addresses the bandit problem with dependent rewards is when they are distributed according to Markov processes, such as Markov chains and Markov decision process (MDP) [5, 6], where the dependences between rewards are of bounded range, which is what distinguishes those works with ours. Contributions in this area study two settings, that we will analyze as well: the rested case, where the process attached to an arm evolves only when the arm is pulled, and the restless case, where all processes simultaneously evolve at each time step.\nContributions and structure of the paper. We define the notion of a \u03d5-mixing bandit and its regret (Section 2), we provide a general analysis and an algorithm to solve the rested case where the size of independence blocks is fixed (Section 3). We provide another approach where these sizes are computed from the data by introducing another algorithm (Section 4). Finally, in Section 5, we provide an algorithm and a regret analysis to deal with the restless case."}, {"heading": "2 Overview of the problem", "text": "Let (\u03c9,F ,P) be a probability space. We recall the definitions of stationary and of \u03d5-mixing processes:\nDefinition 1 (Stationarity). A sequence of random variables X = {Xt}+\u221et=\u2212\u221e is stationary if, for any t and nonnegative integerm and s, the random subsequences (Xt, . . . , Xt+m) and (Xt+s, . . . , Xt+m+s) are identically distributed.\nDefinition 2 (\u03d5-mixing process). Let X = {Xt}+\u221et=\u2212\u221e be a stationary sequence of random variables. For any i, j \u2208 Z\u222a{\u2212\u221e,+\u221e}, let \u03c3ji denote the \u03c3-algebra generated by the random variables Xt, i \u2264 t \u2264 j. Then, for any positive integer n, the \u03d5-mixing coefficient \u03d5(n) of the stochastic process X is defined as\n\u03d5(n) = sup t,A\u2208\u03c3+\u221e\nt+k ,B\u2208\u03c3t \u2212\u221e\n|P [A|B]\u2212 P [A]| . (1)\nX is said to be \u03d5-mixing if \u03d5(n) \u2192 0 as n\u2192 \u221e.\nWe are interested in the problem of sampling from a K-armed \u03d5-mixing bandit. In our setting, pulling arm k at time t provides the agent with a realization of the random variable Xk\n\u03c4k(t) , where\n\u03c4k(t) = t in the restless case and \u03c4k(t) is the number of times arm k was pulled in the rested case, and where the family (Xkt )t\u22651 satisfies the following hypotheses :\n1. \u2200k, (Xkt )t\u2208Z is stationary;\n2. the sequences (Xkt )t\u2208Z are \u03d5-mixing;\n3. each Xk1 takes values in a discrete finite set.\nThis setting assumes the possibility of long-term dependencies between the rewards output by the arms. It is important to note that by definition of the \u03d5-mixing processes, the amount of dependence decreases with time. Hence, as evoked earlier, in order to choose which arm to pull, the agent is forced to address the exploration/exploitation/independence trade-off where independence may be partially recovered by ignoring some rewards so as to make computations on data that are distant in time, i.e. data that are not too correlated (thanks to the mixing property).\nIt is critical to note that unlike in the i.i.d. framework, Hoeffding inequality cannot be applied in this setting, thus the widely used upper confidence bound (UCB) algorithms cannot be used here. In the case of stationary \u03d5-mixing distributions, we have the following theorem from [7].\nTheorem 1 ([7, 8]). Let \u03c8 : Um \u2192 R be a function defined over a countable space U , and X be a stationary \u03d5 mixing process. If \u03c8 is l-Lipschitz with respect to the Hamming metric for some l > 0, then the following holds for all t > 0:\nPX [|\u03c8(X)\u2212 E\u03c8(X)| > t] \u2264 2 exp [ \u2212 t 2\n2ml2\u2016\u039bm\u20162\u221e\n] , (2)\nwhere \u2016\u039bm\u2016\u221e \u2264 1 + 2 \u2211m k=1 \u03d5(k).\nIn the following, we consider a more general framework than the one usually encountered in the bandit literature. Instead of looking at cumulative gains, we look at rewards computed according to Lipschitz functions meeting the requirements of the concentration inequality stated in Theorem 1.\nMore precisely, we suppose that we have K known families (\u03c8kt )t\u22651 k = 1, . . . ,K of functions such that for every non-negative integer m:\n1. \u03c8km : Um \u2192 R accounts for the reward associated with m consecutive outcomes of arm k;\n2. \u03c8km is 1-Lipchitz with respect to the Hamming metric.\nIn the following sections, we use m to identify the reward functions (\u03c8km)1\u2264k\u2264K we want to optimize, b to refer to the size of the independence blocks and we consider blocs of s trials. We study three different scenarios. In the next section, we will present a general algorithm and regret analysis in the rested case where m is fixed and s . = m + b. In Section 4, with an additional hypothesis on the \u03d5-mixing processes, we will take another approach to the rested case when m + b | s (i.e. s is a multiple of m + b) by including the notion of dependency into the regret. Finally, in Section 5, we present a general algorithm and regret analysis for the restless case."}, {"heading": "3 Mixing Bandits in the Rested Case", "text": "Regret analysis. Here, we are going to analyze the situation where m and b are fixed. Our goal is to show that a simple algorithm derived from UCB that works by making blocks s . = m + b consecutive trials on each arm has low regret, with a notion of regret that we define in the sequel. The running time index is therefore of the form of st, with t the number of times arm selection has been performed, and the sequence accessed to are such as (Xkst, . . . , X k (s+1)t\u22121). The reward that is accessed to at time t (with a slight abuse of notation that makes us use t as the time index) when pulling arm k does not make use of the full information provided by this sequence but instead is \u03c8km(X k t,s,b) where\nXkt,s,b . = (Xkst, . . . , X k (s+1)t\u2212b\u22121); (3)\nthis means that only the first m points from (Xst, . . . , X(s+1)t\u22121) are taken advantage of. Given \u03c4 , the total number of trials of s-blocks, the regret that we are going to work with is\nR .= \u03c4\u00b5\u2217\u03c8,m \u2212 K\u2211\nk=1\nE\u03c4k(\u03c4)\u00b5 k \u03c8,m (4)\nwhere: \u00b5\u2217\u03c8,m . = maxk=1,...,K \u00b5 k \u03c8,m, \u00b5 k \u03c8,m . = EXk1 ,...,Xkm\u03c8 k m(X k 1 , . . . , X k m) and \u03c4k(t) is the number of times arm k has been chosen given that a total (i.e. over all arms) of t pulls of s-blocks have been performed.\nThe arm selection procedure of the algorithm that we propose and dub Block-UCB, is depicted in Algorithm 1, where the function \u039bk is defined as\n\u039bk(t) . = 1 + 2\nt\u2211\nr=1\n\u03d5k(rb + (r \u2212 1)m). (5)\nIt is possible to show that Block-UCB has the following regret.\nTheorem 2 (Regret of Block-UCB). The regret of Block-UCB is bounded by\n\u2211\nk:\u00b5k \u03c8,m <\u00b5\u2217 \u03c8,m\n( uk\u2206k + 1\n\u03b1\u2212 2\n) ,\nwhere the uk\u2019s are the solutions of the problems\nuk\u2206 2 k \u2212 8\u03b1\u039b2k(uk) log \u03c4 = 0, k = 1, . . . ,K. (6)\nThe result of Theorem 2 hinges on the derivation of a concentration inequality for each arm\nk that relates the random variable 1 \u03c4 \u2211\u03c4\u22121 r=0 \u03c8 k m ( Xkr,s,b ) to \u00b5k\u03c8,m. To establish this concentration inequality, we study the random variables \u0393kb (X k 0,s,b, . . . , X k \u03c4\u22121,s,b), defined for b \u2265 0 as\n\u0393kb (X k 0,s,b, . . . , X k \u03c4\u22121,s,b)\n. =\n1\n\u03c4\n\u03c4\u22121\u2211\nr=0\n\u03c8km(X k rs, . . . , X k (r+1)s\u2212b\u22121)\u2212 \u00b5k\u03c8,m. (7)\nThe concentration inequality that we are going to use to prove our regret bound is the following:\nAlgorithm 1 Main iteration of Block-UCB\nChoose arm It as\nIt \u2208 argmax k\n1\nt\nt\u22121\u2211\nr=0\n\u03c8km ( Xkr,s,b ) + \u039bk(\u03c4k(t\u2212 1))\n\u221a 2\u03b1 log t\n\u03c4k(t\u2212 1) .\nTheorem 3. For all \u03c4, k, b, and assuming that \u03c8km takes value in [0; 1]:\nP (\u2223\u2223\u2223\u0393kb (Xk0,s,b, . . . , Xk\u03c4\u22121,s,b) \u2223\u2223\u2223 \u2265 \u03b5 ) \u2264 exp ( \u2212 \u03c4\u03b5 2\n2\u039b2k(\u03c4)\n) , (8)\nwhere \u039bk is defined in (5).\nProof. We make the proof for some arm k. We also assume that s and b are fixed and to lighten the notation, we drop the dependence on these variables when no confusion is possible: we use Xr (resp. \u0393) for Xkr,s,b (resp. \u0393b). The proof hinges on the fact that since (Xt)t\u22650 is a stationary mixing sequence with mixing coefficients (\u03d5(t))t\u22651, (Xr,)r\u22650 is a stationary mixing sequence with mixing coefficients (\u03d5(q))q\u22651 such that \u03d5(q) . = \u03d5(qb + (q \u2212 1)m) (see Proposition 5, Appendix). To obtain the targeted result, we make use of the concentration inequality of Theorem 1 with the function \u0393 to get (8) and we exploit the observation that E\u0393kb = 0, which results from the stationarity of (Xt)t\u22650.\nLet q be an integer in {0, . . . , r \u2212 1}, x0, . . . , xr\u22121 and x\u2032q blocks from Us\u2212b. Then:\n\u2223\u2223\u0393(x0, . . . , xq, . . . , xr\u22121)\u2212 \u0393(x0, . . . , x\u2032q, . . . , xr\u22121) \u2223\u2223 = \u2223\u2223\u2223\u2223 1\n\u03c4 (\u03c8s\u2212b(xq)\u2212 \u03c8s\u2212b(x\u2032q))\n\u2223\u2223\u2223\u2223 \u2264 1\n\u03c4 ,\nwhich comes from the range of \u03c8s\u2212b being [0; 1]. \u0393 is therefore 1/\u03c4 -Lipschitz with respect to the Hamming metric, which, combined with (Xr)r\u22650 being a \u03d5-mixing sequence, gives (8).\nThe proof of the previous theorem uses the following more general result, that is of independent interest.\nTheorem 4 (General Regret). Suppose that the arms we work with are such that\n\u2200k, P( \u2223\u2223\u00b5\u0302k\u03c4 \u2212 \u00b5k \u2223\u2223 \u2265 \u03b5) exp (\u2212\u03b8k(\u03c4)\u03b3k(\u03b5)) , (9)\nwhere \u03c4 is the number of data the empirical mean \u00b5\u0302k\u03c4 is computed on, and \u03b8k and \u03b5k are increasing functions defined on (0;+\u221e].\nConsider the regret defined by\nR . = \u03c4\u00b5\u2217 \u2212\nK\u2211\nk=1\nE\u03c4k(\u03c4)\u00b5k\nwhere \u03c4k(t) is the number of times a (suboptimal) arm k has been chosen up to time t. The (\u03b1, \u03b8, \u03b3)-UCB that chooses at iteration t an arm It according to\nIt \u2208 argmax \u00b5\u0302k\u03c4k(t\u22121) + \u03b3 \u22121 k\n( \u03b1\n\u03b8k(\u03c4k(t\u2212 1)) log t\n)\nhas regret bounded by: \u2211\nk:\u00b5i<\u00b5\u2217\n(\u2308 \u03b8\u22121k ( \u03b1 log \u03c4\n\u03b3k(\u2206k/2)\n)\u2309 \u2206k + 1\n\u03b1\u2212 2\n) . (10)\nProof. Note that Theorem 2 is a consequence of this theorem with \u03b8k(s) = s/\u039b 2 k(s) and \u03b3k(\u03b5) = \u03b52/2.\nThe proof use the standard technique to prove the regret of UCB-like algorithms. Namely, at iteration t, if It = i for i not optimal, then one of the following events E\u22171 (t), E2(i, t), E3(i, t) must occur\nE\u22171 (t) . = { \u00b5\u0302\u2217 < \u00b5\u2217 \u2212 \u03b3\u22121\u2217 ( \u03b1\n\u03b8\u2217(\u03c4\u2217(t\u2212 1)) log t\n)} , (11)\nE2(i, t) .= { \u00b5i \u2264 \u00b5\u0302i \u2212 \u03b3\u22121i ( \u03b1\n\u03b8i(\u03c4i(t\u2212 1)) log t\n)} , (12)\nE3(i, t) .= { \u03c4i(t\u2212 1) \u2264 \u03b8\u22121i ( \u03b1 log \u03c4\n\u03b3i(\u2206i/2)\n)} . (13)\nIndeed, if none of the events occurs then (using \u2206i = \u00b5 \u2217 \u2212 \u00b5i)\n\u00b5\u0302 \u2217+\u03b3\u22121\n\u2217\n(\n\u03b1\n\u03b8\u2217(\u03c4\u2217(t\u2212 1)) log t\n)\n(11)\n\u2265 \u00b5i+\u2206i (12) > \u00b5i+2\u03b3 \u22121 i\n(\n\u03b1\n\u03b8i(\u03c4i(t\u2212 1)) log \u03c4\n)\n(13) > \u00b5\u0302i+\u03b3 \u22121 i\n(\n\u03b1\n\u03b8i(\u03c4i(t\u2212 1)) log t\n)\nwhere we have used that t 7\u2192 \u03b3\u22121i (a log t) is an increasing function of t on [1;\u221e) whenever a > 0. This implies that It 6= i, which contradicts our working hypothesis.\nIf we let u be defined as:\nu . = \u2308 \u03b8\u22121i ( \u03b1 log \u03c4\n\u03b3i(\u2206i/2)\n)\u2309 ,\nthen, for i suboptimal, we have the following\nE\u03c4i(\u03c4) =\n\u03c4\u2211\nt=1\nE1[It=i] \u2264 u+ \u03c4\u2211\nt=u+1\nE1[It=i \u2227 \u00acE3(i,t)] \u2264 u+ \u03c4\u2211\nt=u+1\nE1[E\u22171 (t) \u2228 E2(i,t)]\n\u2264 u+ \u03c4\u2211\nt=u+1\n[P(\u2203t : E\u22171 (t)) + P(\u2203t : E2(i, t))]\nUsing the union bound and Equation (9), both probabilities P(\u2203t : E\u22171 (t)), P(\u2203t : E2(i, t)) can be bounded from above by 1/t\u03b1. Standard calculations allow us to get desired result (10).\nDiscussion. Some observations must be made regarding the result of Theorem 2. First, as we used Theorem 4 to prove our regret bound, it is necessary for the result to hold for the functions \u03b8k . = \u03c4/\u039b2k(\u03c4) to be increasing. In addition, the regret only makes sense if it is bounded, i.e. if the uk\u2206k are bounded. Finally, if these conditions hold, it might be interesting to find, for some fixed horizon \u03c4 , the value of b that minimizes the regret. We now depicts common settings for the \u03d5k that yield instructive results and that build upon the previous remarks. For the sake of conciseness, we will assume that all \u03d51 = . . . = \u03d5K and we use \u03d5 to refer to the mixing coefficients.\nIndependent case. if \u03d5 = 0, i.e. we are in the independent case, and the \u03b8k\u2019s are naturally increasing. In addition, it is straightforward to observe that the best use of the data is achieved for b = 0, i.e. each and every reward is used to estimate the quality of an arm.\nCase \u039b < +\u221e. In that case, we are again back to a situation almost similar to the usual independent case. The \u03b8k\u2019s are increasing, the uk are well-defined and the regret as the usual O( \u2211\nk log \u03c4/\u2206k) form.\nAlgebraically mixing case. Here, \u03d5(t) = \u03d50t \u2212p for p > 1, and a few calculations give\n\u039b(\u03c4 ) = 1 + 2\u03d50\n\u03c4 \u2211\nr=1\n1\n(rs\u2212m)p \u2264 1 + 2\u03d50\n(\n1 +\n\u222b \u03c4\n1\n1\n(rs\u2212m)p dr\n)\n= 1 + 2\u03d50 + 2\u03d50\ns(p\u2212 1)\n(\n1\nbp\u22121 \u2212\n1\n(\u03c4s\u2212m)p\u22121\n)\nUsing this upper bound to find the uks as in (6) and to solve for b so that this bound is minimized provides a way to find a data-dependent b. Another (coarser) way to look at the algebraically mixing situation is not to optimize for b and to consider that it is a particular case of the previous case, since \u2211 t \u03d5(t) <\u221e, i.e. \u039b < +\u221e. This assumption is made in the rest of the paper."}, {"heading": "4 m + b | s: Expressing the Independence Trade-Off in the", "text": "Regret\nThis section introduces another way of encoding the trade-off between exploration, exploitation and independence. As before, we consider sequences of s trials, but among the s results, we seek to optimize the number m of results we use to update the empirical value of the arms and the number b of results we ignore in order to improve the independence between the considered realization of our random variables.\nIn addition to the case s = m+ b, we also consider the situation where \u03b2(m+ b) = s with \u03b2 \u2208 N and \u03b2 > 1. The sequence of s trials can then be interpreted as \u03b2 successive sequences of m + b trials, and the value of this particular (m, b) distribution is thus multiplied by \u03b2."}, {"heading": "4.1 Hypotheses and Regrets", "text": "In this section we make the following additional assumption on the \u03d5-mixing processes (Xkt )t\u22650:\n\u22001 \u2264 k \u2264 K, \u2200b \u2208 N, Mk(b) .= 1 + \u2211\ni\u22651\n\u03d5k(b(i+ 1)) < +\u221e.\nNote that this is equivalent to the widely used assumption that the \u03d5k(i) are summable over i (see for instance the case of algebraically mixing sequence mentioned before). Also, note that Mk(1) is an upper bound of (\u039bkm)m which appears in Theorem 1, and Mk(\u00b7) is a decreasing function such that Mk(b) \u2265 1.\nThe setting is the following: at each step, the agent pulls an arm s times, and has to choose how to split those s elements between a meaningful part of m elements, used to update empirical values of the arm, and the non-significant part of b element, used to strengthen the independence between the variables. For each such combination m+ b | s and for each arm k, we define the value of the combination (m, b, k) as:\n\u03bdkm,b . = \u03b2m,b Mk(b) \u00b5km, (14)\nwhere \u00b5km . = EXk1 ,...,Xkm\u03c8 k m(X k 1 , . . . , X k m) and \u03b2m,b = s/(m + b). This value explicitly shows the trade-off between independence (through Mk(b)) and exploitation (through the \u00b5 k m,b). The value of an arm k is then defined as the maximum value of the possible combination (m, b, k), for m+b|s. With this in mind, we define the regret R at time t as :\nR .= K\u2211\nk=1\nE(T (k)) ( \u03bdk \u2217\nm\u2217,b\u2217 \u2212 \u03bdkm\u2217 k ,b\u2217 k\n) (15)\nwhere (k\u2217,m\u2217, b\u2217) = argmax(k,m,b) \u03bd k mk,bk , (m\u2217k, b \u2217 k) = argmax(m,b) \u03bd k mk,bk\n, and T (k) denotes the number of times arm k was pulled.\nIt is important to note that one of the main difference between (15) and the classical formulation of regret from a multi arm bandit in the i.i.d. case is that in our setting, we are comparing the value of the best combination of the best arm with the value of the best combination of the pulled arm."}, {"heading": "4.2 Concentration inequality and algorithm", "text": "We now introduce a concentration inequality tailored for the \u03bdkm,b of (14).\nProposition 1. Let 1 \u2264 k \u2264 K, 1 \u2264 m \u2264 s, b = m \u2212 s, n \u2208 N\u2217, \u03c8km : Um \u2192 R be a 1-Lipschitz with respect to the Hamming metric function defined over a countable space U . Suppose that the \u03d5k(i) are summable over i, and let us define Mk(b) = 1 + \u2211 i\u2208N\u2217 \u03d5\nk(i(b + 1)), and \u03b6sm : N 7\u2192 N, \u03b6sm(t) = t+ b\u230a(t\u2212 1)/m\u230b. Then the following holds for all t > 0:\nP\n[ 1\nMk(b) \u2223\u2223\u2223\u2223\u2223 1 n n\u22121\u2211\ni=0\n\u03c8km ( Xk\u03b6sm(im+1), \u00b7 \u00b7 \u00b7 , X k \u03b6sm((i+1)m) ) \u2212 \u00b5km \u2223\u2223\u2223\u2223\u2223 > t ] \u2264 2 exp [ \u2212nt 2 2 ] . (16)\nAlgorithm 2 Block-UCB with parameters s, \u03b1 fixed\nt\u2190 0, \u03c8\u0302km,b,0 \u2190 0, k = 1, . . . ,K,m+ b | s, \u03c4k,0 \u2190 0, k = 1, . . . ,K for t = 1, . . . , \u03c4 do\nSelect arm k\u0302 with k\u0302 \u2208 argmax k max m+b|s\ns\nm+ b\n( \u03c8\u0302km,b,t\u22121 + \u221a 2\u03b1(m+ b) log(t)\ns\u03c4k,t\u22121\n)\nUpdate the block counts \u03c4k,t \u2190 \u03c4k,t\u22121 + \u03b4kk\u0302, \u2200k Compute the values of the \u03c8\u0302km,b,t, \u2200m+ b | s\n\u03c8\u0302k\u0302m,b,t \u2190 1\nMk(b)\u03b2\u03c4k\u0302,t\n\u03b2(\u03c4 k\u0302,t \u22121)\u2211\nr=0\n\u03c8k\u0302m ( X k\u0302r(m+b), . . . , X k\u0302 (m+b)r+m\u22121 )\n\u03c8\u0302km,t \u2190 \u03c8\u0302km,t\u22121, for k 6= k\u0302\nend for\nProof. This proposition naturally follows from the proof of Theorem 1 with the function \u03c6kn = 1 n \u2211n\u22121 i=0 \u03c8 k m ( X\u03b6sm(im+1), \u00b7 \u00b7 \u00b7 , X\u03b6sm((i+1)m) ) which is 1/n-Lipschitz with respect to the Hamming metric, and Proposition 5 ( see Appendix).\nNote that the upper bound on the probability that appears in (16) is uniform over (k,m, b). This is crucial to define our algorithm and to analyze its regret (more details in the next subsection). We now introduce Algorithm 2 that is designed for the particular setting of \u03d5-mixing bandit problem just described. First, note that since the pair (m, b) (with m + b | s) which gives the best result for each arm is unknown, the algorithm needs to compute an empirical estimator for each of these combinations for each arm. In other words, the algorithm needs to efficiently and simultaneously learn both the best combination and the best arm."}, {"heading": "4.3 Regret Analysis", "text": "In this subsection we provide an upper bound for the regret of Algorithm 2. Proposition 2. Let R be the regret as defined in (15) and let \u03b7 : N 7\u2192 N, \u03b7(s) .= \u2211s\ni=1 i1i|s Then,\nwith \u2206i = \u03bd k\u2217 m\u2217,b\u2217 \u2212 \u03bdim\u2217 i ,b\u2217 i\nR \u2264 \u2211\n1\u2264i\u2264K\n( (1 + \u03b7(s))\u2206i + 8\u03b1s log(t)\n\u2206i\n) .\nProof. The main difference with the standard technique to proving regret bounds comes from the fact that the value of each arm is the maximum of its coordinate: as suchs we have to consider the following event\n\u03c8k \u2217 t \u2264 \u03bdk \u2217 m\u2217,b\u2217\n\u2203m+ b | s, \u03bdkm\u2217 k ,b\u2217 k \u2264 s m+ b\n( \u03c8\u0302km,b,t\u22121 \u2212 \u221a 2\u03b1(m+ b) log(t)\ns\u03c4k,t\u22121\n)\n\u2203m+ b | s, 2 \u221a 2\u03b1(m+ b) log(t)\ns\u03c4k,t\u22121 \u2265 \u03bdk\u2217m\u2217,b\u2217 \u2212 \u03bdkm\u2217 k ,b\u2217 k\nBy carefully using the property of the maximum, the result can be recovered. All the details of the proof can be found in the supplementary material.\nWe have just seen another approach to the rested mixing bandit. By assuming the summability of the \u03d5k, and by properly defining the value of an arm, we were able using Algorithm 2 to address the case where m and b are no longer fixed, but must be computed from the data by the agent.\nIt is interesting to note that the upper bound in Proposition 2 differs from the usual bound in the classical dependence-free setting by two multiplicative constants: \u03b7(s), which encodes the total number of combination of pair (m, b) such that m+ b | s, and s, which is in fact used as an upper bound for s/(m+ b)."}, {"heading": "5 Restless \u03d5 mixing bandits", "text": "In this section, we provide an analysis for the restless \u03d5-mixing bandit. Recall that contrarily to the rested case studied previously, the stochastic processes associated to each arm evolves regardless of the actions of the agent. This difference is of paramount importance in the \u03d5-mixing setting. Indeed, in the rested case, the agent was bound to ignore some realizations obtained from an arm to enforce the independence and therefore the accuracy of its predictor. In the restless case, instead of pulling an arm to no avail, an agent willing to increase the independence of the realization of an arm k can pull another arm k\u2032 6= k to gather information about k\u2032 while enforcing the independence of k. This idea is central to this section.\nLike in Section 4, we assume that \u2200k = 1, \u00b7 \u00b7 \u00b7 ,K, the stochastic process Xk is a \u03d5-mixing process, and its mixing coefficients \u03d5k(i) are summable, and we define the following upper bound function Mk, which differs for the one defined in the previous section:\n\u22001 \u2264 k \u2264 K, \u2200b \u2208 N, Mk(b) .= 1 + \u2211\ni\u2265b\n\u03d5k(i) < +\u221e.\nIn the restless \u03d5-mixing bandit, the agent pulls the arm k in sequences of mk trials, where the mk are fixed parameters and may differ for each arm. The mean value of this sequence is defined as follows:\n\u00b5k . = EXk1 ,...,Xkmk \u03c8km(X k 1 , . . . , X k mk )\nand we use the same definition of regret as defined in Section 3. In the restless setting, an interesting way of dealing with the trade-off exploration/exploitation/independence appears: in addition to the usual exploration, it might be interesting for the agent to pull an apparently sub-optimal arm to get an increased independence on the result of the other arms \u2013since the time between two consecutive sequences of pull decrease their dependency. In order to study this trade-off, we introduce a suitable concentration inequality.\nProposition 3. Let 1 \u2264 k \u2264 K, 1 \u2264 m \u2264 s, b = m \u2212 s, n \u2208 N\u2217, \u03c8km : Um \u2192 R be a 1-Lipschitz with respect to the Hamming metric function defined over a countable space U . Let \u03b6sm : N 7\u2192 N, be such that\n\u03b6sm(t) = t+ b1t\u2265m+1.\nThen the following holds for all t > 0:\nP [\u2223\u2223\u2223\u2223\u2223 1 n n\u22121\u2211\ni=0\n\u03c8km ( Xk\u03b6sm(im+1), \u00b7 \u00b7 \u00b7 , X k \u03b6sm((i+1)m) ) \u2212 \u00b5k \u2223\u2223\u2223\u2223\u2223 > t ] \u2264 2 exp [ \u2212 nt 2\n2M2k(b)\n] . (17)\nProof. This proposition follows from the proof of Theorem 1 using the same technique as Proposition 1.\nIt is interesting to note that the independence trade-off naturally appears in the right term of inequality 17 within the Mk, and will modify the upper confidence bound. We introduce algorithm 3 to solve this particular setting of \u03d5 mixing bandit problem, and we provide a regret analysis for this algorithm.\nProposition 4 (Regret analysis). Let R(t) be the regret at time t for Algorithm 3. Then, with \u2206i = \u00b5\nk\u2217 \u2212 \u00b5i, Rt \u2264 \u2211\n1\u2264i\u2264K\n( \u2206i + 8\u03b1 log(t)\n\u2206i\n) .\nAlgorithm 3 Restless Block-UCB with parameters mk, \u03b1 fixed\nt\u2190 0, \u03c8\u0302k0 \u2190 0, \u03c4k,0 \u2190 0, , \u03b7k,0 \u2190 0, k = 1, . . . ,K for t = 1, . . . , \u03c4 do\nSelect arm k\u0302 \u2208 argmax k \u03c8\u0302kt\u22121 +\n\u221a 2\u03b1 log(t)\n\u03c4k,t\u22121M2k(\u03b7k,t) Update counters and timers :\n\u03b7 k\u0302,t \u2190 0, \u03c4 k\u0302,t \u2190 \u03c4 k\u0302,t\u22121 + 1,\n\u2200k 6= k\u0302, \u03c4k,t \u2190 \u03c4k,t\u22121, and \u03b7k,t \u2190 \u03b7k,t\u22121 +mk\u0302 Compute the values of the \u03c8\u0302kt , m+ b | s\n\u03c8\u0302k\u0302t \u2190 1\n\u03c4 k\u0302,t\n\u03c4 k\u0302,t \u22121\u2211\nr=0\n\u03c8k\u0302m ( X k\u0302rm\nk\u0302 , . . . , X k\u0302(r+1)m k\u0302 \u22121\n)\n\u03c8\u0302kt \u2190 \u03c8\u0302kt\u22121, for k 6= k\u0302\nend for\nProof. The proof uses the same ideas as the previous regret analysis from Section 3 and 4, and naturally follows from Proposition 3. The proof can be found in the supplementary materials.\nIn this section, we have provided an algorithm for the restless \u03d5-mixing framework. We have seen that the restless setting is a natural framework to use with \u03d5-mixing bandit, as it naturally makes it possible to decrease the dependence of the variables for the arms not chosen. Our algorithm takes advantage of this observation and we were able to show that is has low regret. In order to do so, in addition to the usual \u03c4k, the number of time a given arm has been pulled, the Algorithm 3 computes the \u03b7k, the time spent since the last time the arm k was sampled. Indeed, as seen in (17), as \u03b7k increases, M2k(\u03b7k) decreases and the optimistic value of the arm increases."}, {"heading": "6 Conclusion", "text": "We have studied an extension of the multi-armed bandit problem to the stationary \u03d5-mixing framework, both in the rested and in the restless case. We have provided both a theoretical analysis in a general framework, and a more practical study of the problem in the case of fast mixing sequences (with \u2211 \u03d5(i) < +\u221e). For each of theses cases, we provided algorithms and accompanying regret analyses, which are strict extensions of the methods that exist for the i.i.d situation, as usual results might be recovered from our bounds when the mixing coefficients are all 0. Future works might include a study of the restless case where the mk has to be computed from the data, as well as a study in the more difficult case of \u03b2-mixing processes."}, {"heading": "A Appendix", "text": "Proposition 5. Let m, b \u2208 N and s = m + b, Xt be a \u03d5-mixing process on \u2126 taking value in R, with mixing coefficient \u03d5X(\u00b7) , and and \u03c8 : Rm 7\u2192 R be a measurable function. Then the stochastic process Zt defined by\nZt = \u03c8(Xst+1, . . . , Xst+m)\nis also a \u03d5-mixing process with mixing coefficient \u03d5Z = \u03d5X \u25e6 \u03ba, where \u03ba(t) = bt+m(t\u2212 1). Proof. In the following we use \u03c3(A) to denote the \u03c3-algebra generated by A. First note that since \u03c8 is measurable \u03c3(\u03c8\u22121(R)) \u2282 \u03c3(Rm), and as a consequence\n\u03c3(Zt) \u2282 \u03c3(Xst+1, . . . , Xst+m)\n(since \u03c3-algebra are closed under countable intersection) . Now for any i, j \u2208 Z \u222a {\u2212\u221e,+\u221e}, let \u03c3ji (Z) denote the \u03c3-algebra generated by the random variables Zk, i \u2264 k \u2264 j. Then, for any positive integer k, the \u03d5-mixing coefficient \u03d5Z(t) of the stochastic process Z is defined as\n\u03d5Z(t) = sup n,A\u2208\u03c3+\u221e\nn+t(Z),B\u2208\u03c3 n \u2212\u221e (Z)\n|P [A|B]\u2212 P [A]|\n\u2264 sup n,A\u2208\u03c3+\u221e\ns(n+t) (X),B\u2208\u03c3ns+m \u2212\u221e (X)\n|P [A|B]\u2212 P [A]|\n= \u03d5X (s(n+ t)\u2212 ns\u2212m) = \u03d5X(tb + (t\u2212 1)m)"}, {"heading": "B Proof of Proposition 2", "text": "Proof : If at time t, the arm chosen is k instead of k\u2217, then one of the following must be true :\n\u03c8k \u2217 t \u2264 \u03bdk \u2217 m\u2217,b\u2217 (18)\n\u2203m+ b | s, \u03bdkm\u2217 k ,b\u2217 k \u2264 \u03b2m,b ( \u03c8\u0302km,b,t\u22121 \u2212 \u221a \u03b1Jm,b(k, t\u2212 1) ) (19)\n\u2203m+ b | s, 2 \u221a 2\u03b1\u03b2m,b log(t)\n\u03c4k,t\u22121 \u2265 \u03bdk\u2217m\u2217,b\u2217 \u2212 \u03bdkm\u2217 k ,b\u2217 k\n(20)\nOtherwise, \u2200m+ b | s,\n\u03c8k \u2217 t \u2265 \u03bdk \u2217 m\u2217,b\u2217 \u2265 \u03bdkm\u2217 k ,b\u2217 k + 2\n\u221a 2\u03b1\u03b2m,b log(t)\n\u03c4k,t\u22121\n\u2265 \u03bdkm\u2217 k ,b\u2217 k + 2\n\u221a 2\u03b1\u03b2m,b log(t)\n\u03c4k,t\u22121\n\u2265 \u03b2m,b ( \u03c8\u0302km,b,t\u22121 + \u221a \u03b1Jm,b(k, t\u2212 1) )\nSince the last line is true \u2200m+b | s,, we deduce that \u03c8k\u2217t \u2265 \u03c8kt hence T (t) 6= k, which is absurd. Then, we need to bound the probability of the events defined by (18), (19) and (20). Because \u03c8k \u2217\nt is defined as a maximum, we have\nP(\u03c8k \u2217 t \u2264 \u03bdk \u2217 m\u2217,b\u2217) \u2264 P ( \u03b2m\u2217,b\u2217(\u03c8\u0302 k m,b,t\u22121 \u2212 \u221a \u03b1Jm\u2217,b\u2217(k \u2217, t\u2212 1)) \u2264 \u03bdk\u2217m\u2217,b\u2217 )\n\u2264 1 t\u03b1 ,\nusing Proposition 1. Now, by definition of \u03bdkm\u2217\nk ,b\u2217 k ,\nP ( \u2203m+ b | s, \u03bdkm\u2217\nk ,b\u2217 k \u2264 \u03b2m,b(\u03c8\u0302km,b,t\u22121 \u2212\n\u221a \u03b1Jm,b(k, t\u2212 1))\n)\n\u2264 P ( \u2203m+ b | s, \u03bdkm,b \u2264 \u03b2m,b(\u03c8\u0302km,b,t\u22121 \u2212 \u221a \u03b1Jm,b(k, t\u2212 1)) )\n\u2264 \u03c3(s) max m+b|s P\n( \u03bdkm,b \u2264 \u03b2m,b(\u03c8\u0302km,b,t\u22121 \u2212 \u221a \u03b1Jm,b(k, t\u2212 1)) )\n\u2264 \u03c3(s) t\u03b1\nwhere we used Proposition 1 again at the last line. Finally,\n{ \u2203m+ b | s, 2 \u221a 2\u03b1\u03b2m,b log(t)\n\u03c4k,t\u22121 \u2265 \u03bdk\u2217m\u2217,b\u2217 \u2212 \u03bdkm\u2217 k ,b\u2217 k\n} \u2282 { 2 \u221a 2\u03b1s log(t)\n\u03c4k,t\u22121 \u2265 \u03bdk\u2217m\u2217,b\u2217 \u2212 \u03bdkm\u2217 k ,b\u2217 k\n}\n=\n{ 8\u03b1s log(t)\n(\u03bdk \u2217 m\u2217,b\u2217 \u2212 \u03bdkm\u2217 k ,b\u2217 k )2\n\u2265 \u03c4k,t\u22121 }\ni.e. the event defined by (20) happens at most u =\n\u2308 8\u03b1s log(t)\n(\u03bdk \u2217 m\u2217,b\u2217 \u2212 \u03bdkm\u2217 k ,b\u2217 k )2\n\u2309 times.\nhence the conclusion"}, {"heading": "C Proof of Proposition 5", "text": "Proof. If at time t, the arm chosen is k instead of k\u2217, then one of the following must be true :\n\u03c8k \u2217 t \u2264 \u00b5k \u2217\n(21)\n\u00b5k \u2264 ( \u03c8\u0302kt\u22121 \u2212 \u221a \u03b1J(k, t\u2212 1) ) (22)\n2\n\u221a 2\u03b1 log(t)\nM2k(\u03b7k,t)\u03c4k,t\u22121 \u2265 \u00b5k\u2217 \u2212 \u00b5k (23)\nFrom the last inequality, we deduce that :\n\u03c4k,t\u22121 \u2264 8\u03b1 log(t) M2k(\u03b7k,t)\u22062k \u2264 8\u03b1 log(t) \u22062k (24)\nsince M2k(\u03b7k,t) \u2264 1."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6:422", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multi- armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning Journal, 47(23):235\u2013256", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Ucb revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["P. Auer", "R. Ortner"], "venue": "Periodica Mathematica Hungarica, 61:5565", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "volume 5 of Foundation and Trends in Machine Learning. NOW", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Online learning of rested and restless bandits", "author": ["C. Tekin", "M. Liu"], "venue": "IEEE Transactions on Information Theory, 58(8):5588\u20135611", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Concentration inequalities for dependent random variables via the martingale method", "author": ["L. Kontorovich", "K. Ramanan"], "venue": "The Annals of Probability, 36(6):2126\u20132158", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Stability bounds for stationary -mixing and -mixing processes", "author": ["M. Mohri", "A. Rostamizadeh"], "venue": "Journal of Machine Learning Research, 11:789\u2013814", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 2, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 3, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 0, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 4, "context": "A closely related setup that addresses the bandit problem with dependent rewards is when they are distributed according to Markov processes, such as Markov chains and Markov decision process (MDP) [5, 6], where the dependences between rewards are of bounded range, which is what distinguishes those works with ours.", "startOffset": 197, "endOffset": 203}, {"referenceID": 5, "context": "In the case of stationary \u03c6-mixing distributions, we have the following theorem from [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "Theorem 1 ([7, 8]).", "startOffset": 11, "endOffset": 17}, {"referenceID": 6, "context": "Theorem 1 ([7, 8]).", "startOffset": 11, "endOffset": 17}], "year": 2014, "abstractText": "We study the bandit problem where arms are associated with stationary \u03c6-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of recovering some independence by ignoring the value of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off. To do so, we provide a UCB strategy together with a general regret analysis for the case where the size of the independence blocks (the ignored rewards) is fixed and we go a step beyond by providing an algorithm that is able to compute the size of the independence blocks from the data. Finally, we give an analysis of our bandit problem in the restless case, i.e., in the situation where the time counters for all mixing processes simultaneously evolve.", "creator": "LaTeX with hyperref package"}}}