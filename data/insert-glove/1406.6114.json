{"id": "1406.6114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2014", "title": "Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform", "abstract": "In this research clmv we diminuendo address the problem psychodrama of idlib capturing recurring sv1 concepts in adrain a mauri data stream ranci\u00e8re environment. underboss Recurrence capture 2995 enables the re - use of sultan previously learned istrabadi classifiers circumambulation without lemelson the need for 64.38 re - tongxiang learning mbola while buend\u00eda providing bertolaso for better accuracy 200k during the concept recurrence goldenfleece interval. We salutatorian capture concepts by 10:53 applying 1,181 the bitten Discrete Fourier etude Transform (lenska DFT) draws to finback Decision Tree classifiers to 2,341 obtain rabbo highly compressed versions infinite of the sargus trees depardon at transphobia concept ravenglass drift sanzar points in bleachers the stream in-depth and elastic store such orl trees in a gadaffi repository tarsia for airbags future teegardin use. Our empirical results inter-colonial on real world b'elanna and synthetic data exhibiting varying lamination degrees of zhiping recurrence jorkens show 1420s that the castrating Fourier compressed trees dorel are contradanza more robust chandramohan to noise beauvilliers and cathartic are khari able to gyoergy capture recurring extensiveness concepts with benavente higher precision than shawn a meta learning approach similan that weidenbaum chooses sickel to 53.3 re - 2.89 use classifiers destron in their torhout originally mirabad occurring form.", "histories": [["v1", "Tue, 24 Jun 2014 00:48:23 GMT  (487kb,D)", "http://arxiv.org/abs/1406.6114v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sakthithasan sripirakas", "russel pears"], "accepted": false, "id": "1406.6114"}, "pdf": {"name": "1406.6114.pdf", "metadata": {"source": "CRF", "title": "Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform", "authors": ["Sakthithasan Sripirakas"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Data stream mining has been the subject of extensive research over the last decade or so. One of the major issues with data stream mining is dealing with concept drift that causes models built by classifiers to degrade in accuracy over a period of time.\nWhile data steam environments require that models are updated to reflect current concepts, the capture and storage of recurrent concepts allows a classifier to use an older version of the model that provides a better fit with newly arriving data in place of the current model. This approach removes the need to explicitly re-learn the model, thus improving both accuracy and computational cost. A number of methods have been proposed that deal with the capture and exploitation of recurring concepts [4], [5], [7], [1] and [12]. Although achieving higher accuracy as expected during phases of concept recurrence in the stream, a major issue with existing approaches is the setting of user defined parameters to determine whether a current concept matches with one from the past.\nSuch parameters are difficult to set, particularly due to the drifting nature of real world data streams. Our approach avoids this problem by applying the Discrete Fourier Transform (DFT) as a meta learner. The DFT, when applied on a concept (Decision Tree model) results in a spectral representation that captures the classification power of the original models. One very attractive property of the Fourier representation of Decision Tree is that most of the energy and classification power is contained within\nar X\niv :1\n40 6.\n61 14\nv1 [\ncs .L\nG ]\n2 4\nJu n\nthe low order coefficients [9]. The implication of this is that that when a concept C recurs as concept C* with relatively small differences caused by noise or concept drift, then such differences are likely to manifest in the high order coefficients of spectra S and S* (derived from C and C* respectively), thus increasing the likelihood of C* being recognized as a recurrence of C.\nThe DFT, apart from its use in meta learning, has a number of other desirable properties that make it attractive for mining high speed data streams. This includes the ability to classify directly from the spectra generated, thus eliminating the need for expensive traversal of a tree structure.\nOur experimental results in section 5 clearly show the accuracy, processing speed and memory advantages of applying the DFT as opposed to the meta learning approach proposed by Gama and Kosina in [4].\nThe rest of the paper is as follows. In section 2 we review work done in the area of capturing recurrences. We describe the basics of applying the DFT to decision trees in section 3. In section 4 we discuss a novel approach to optimizing the computation of the Fourier spectrum from a Decision Tree. Our experimental results are presented in section 5 and we conclude the paper in section 6 where we draw conclusions on the research and discuss some directions for future research."}, {"heading": "2 Related Research", "text": "While a vast literature on concept drift detection exists [14] only a small body of work exists so far on exploitation of recurrent concepts. The methods that exist fall into two broad categories. Firstly, methods that store past concepts as models and then use a meta learning mechanism to find the best match when a concept drift is triggered [4], [5]. Secondly, methods that store past concepts as an ensemble of classifiers.\nLazarescu in [10] proposes an evidence forgetting mechanism for data instances based on a multiple window approach and a prediction module to adapt classifiers based on an estimation of the future rate of change. Whenever the difference between the observed and estimated rates of change are above a user defined threshold a classifier that best represents the current concept is stored in a repository. Experimentation on the STAGGER dataset showed that the proposed approach outperformed the FLORA method on classification accuracy with re-emergence of previous concepts in the stream.\nRamamurthy and Bhatnagar [15] use an ensemble approach based on a set of classifiers in a global set G. An ensemble of classifiers is built dynamically from a collection of classifiers in G if none of the existing individual classifiers are able to meet a minimum accuracy threshold based on a user defined acceptance factor. Whenever the ensemble accuracy falls below the accuracy threshold, then the global set G is updated with a new classifier trained on the current chunk of data.\nAnother ensemble based approach by Katakis et al. is proposed in [8]. A mapping function is applied on data stream instances to form conceptual vectors which are then grouped together into a set of clusters. A classifier is incrementally built on each cluster and an ensemble is formed based on the set of classifiers. Experimentation on the Usenet dataset showed that the ensemble approach produced better accuracy than a simple incremental version of the Naive Bayes classifier.\nGomes et al [5] used a two layer approach with the first layer consisting of a set of classifiers trained on the current concept while the second contains classifiers created from past concepts. A concept drift detector is used to flag changes in concept and\nwhen a warning state is triggered incoming data instances are buffered in a window to prepare a new classifier. If the number of instances in the warning window is below a user defined threshold then the classifier in layer 1 is used instead of re-using classifiers in layer 2. One major issue with this method is validity of the assumption that explicit contextual information is available in the data stream.\nGama and Kosina also proposed a two layered system in [4] designed for delayed labeling, similar in some respects to the Gomes et al. [5] approach. In their approach Gama and Kosina pair a base classifier in the first layer with a referee in second layer. Referees learn regions of feature space which its corresponding base classifier predicts accurately and is thus able to express a level of confidence on its base classifier with respect to a newly generated concept. The base classifier which receives the highest confidence score is selected, provided that it is above a user defined hit ratio parameter; if not, a new classifier is learned."}, {"heading": "3 Application of the Discrete Fourier Transform on Decision Trees", "text": "The Discrete Fourier Transform (DFT) has a vast area of application in very diverse domains such as time series analysis, signal processing, image processing and so on. It turns out as Park [13] and Kargupta [9] show that the DFT is very effective in terms of classification when applied on a decision tree model. Kargupta and Park in [9] explored the use of the DFT in a distributed environment but did not explore its usage in a data stream environment as this research sets out to do.\nKargupta and Park in [9] showed that the Fourier spectrum consisting of a set of Fourier coefficients fully captures a decision tree in algebraic form, meaning that the Fourier representation preserves the same classification power as the original decision tree.\nA decision tree can be represented in compact algebraic form by applying the DFT to the paths of the tree. We illustrate the process by considering a binary tree for simplicity but in practice the DFT can be applied to non binary trees as well [9]. For trees with a total of d binary valued features the jth Fourier coefficient \u03c9j is given by:\n\u03c9j = 1\n2d \u2211 x f(x)\u03c8j(x) (1)\nwhere f(x) is the classification outcome of path vector x and \u03c8j(x), the Fourier basis function is given by:\n\u03c8j(x) = (\u22121)(j.x) (2)\nFigure 1 shows a simple example with 3 binary valued features x1, x2 and x3, out of which only x1 and x3 are actually used in the classification.\nAs shown in [13] only coefficients for paths that are defined by attributes that actually appear in the tree need to be computed as all other coefficients are guaranteed to be zero in value. Thus any coefficient of the form \u03c9\u22171\u2217 will be zero since attribute x2 does not appear in the tree.\nWith the wild card operator * in place we can use equations (1) and (2) to calculate\nnon zero coefficients. Thus for example we can compute:\n\u03c9000 = 4\n8 f(\u2217 \u2217 0)\u03c8000(\u2217 \u2217 0) +\n2 8 f(0 \u2217 1)\u03c8000(0 \u2217 1) + 2 8 f(1 \u2217 1)\u03c8000f(1 \u2217 1) = 3 4\n\u03c9001 = 4\n8 f(\u2217 \u2217 0)\u03c8001(\u2217 \u2217 0) +\n2 8 f(0 \u2217 1)\u03c8001(0 \u2217 1) + 2 8 f(1 \u2217 1)\u03c8001f(1 \u2217 1) = 1 4\nand so on. In addition to the properties discussed above, the Fourier spectrum of a given decision tree has two very useful properties that make it attractive as a tree compression technique [9]:\n1. All coefficients corresponding to partitions not defined in the tree are zero.\n2. The magnitudes of the Fourier coefficients decrease exponentially with their order, where the order is taken as the number of defining attributes in the partition.\nTaken together these properties mean that the spectrum of a decision tree can be approximated by computing only a small number of low order coefficients, thus reducing storage overhead. With a suitable thresholding scheme in place, the Fourier spectrum consisting of the set of low order coefficients is thus an ideal mechanism for capturing past concepts.\nFurthermore, classification of unlabeled data instances can be done directly in the Fourier domain as it is well known that the inverse of the DFT defined in expression (3) can be used to recover the classification vector, instead of the use of a tree traversal which can be expensive in the case of deep trees. Expression 3 uses the complex conjugate \u03c8j(x) function for the inverse operation in place of the original basis function of \u03c8j(x).\nf(x) = \u2211 j \u03c9j\u03c8j(x) (3)\nDue to thresholding and loss of some high order coefficient values the classification value f(x) for a given data instance x may need to be rounded to the nearest integer in order to assign the class value. For example, with binary classes a value for f is rounded up to 1 if it is in the range [0.5, 1) and rounded down to 0 in the range (0, 0.5)."}, {"heading": "4 Exploitation of the Fourier Transform for Recurrent Concept Capture", "text": "We first present the basic algorithm used in section 5.1 and then go on to discuss an optimization that we used for energy thresholding in section 5.2."}, {"heading": "4.1 The FCT algorithm", "text": "We use CBDT [6] as the base algorithm which maintains a forest of trees. This forest of trees is dynamic in the sense that it can adapt to changing concepts at drift detection points. We thus define the memory consumed by this forest as active.\nWe integrate the basic CBDT algorithm with the ADWIN [2] drift detector to signal concept drift. At the first concept drift point the best performing tree (in terms of accuracy) is identified and the DFT is applied after energy thresholding after which the resulting spectrum is stored in the repository for future use if the current concept recurs.\nThe spectra stored in the repository are fixed in nature as the intention is to capture past concepts. At each subsequent drift point a winner model is chosen by polling both the active memory and the repository. If the winner emerges from the active memory, two checks are made before the DFT is applied. First of all, we check whether the difference in accuracy between the winner tree in active memory (T) and the best performing model in the repository is greater than a tie threshold \u03c4 . If this check is passed then the DFT is applied to T and a further check is made to ensure that its Fourier representation is not already in the Repository. If the winner model at a drift point emerges from an already existing spectrum in the Repository then no Fourier conversion is applied on any of the trees in active memory. Whichever model is chosen as the winner it is applied to classify all unlabeled data instances until a new winner emerges at a subsequent drift point. The least performing model M having the lowest weighted accuracy function is deleted if the repository has no room for new models.The weighted accuracy of M is defined by: weight(M) = winner tally(M) \u2217 accuracy(M), where winner tally is the number of times that M was declared a winner since it was inserted into the repository.\nAlgorithm FCT Input: Energy Threshold ET , Accuracy Tie Threshold \u03c4 Output: Best Performing model M that suits current concept 1. read an instance I from the data stream 2. repeat 3. if best model M is in repository call Classify to classify I 4. append 0 to ADWIN\u2019s window for M if classification is correct, else append 1 5. until drift is detected by ADWIN 6. if M is from active memory 7. identify best performing model F in repository 8. if (accuracy(M)-accuracy(F))> \u03c4 9. apply DFT on model M to produce F* using energy threshold ET 10. if F* is not already in repository 11. insert F* into repository 12. Identify best performing model M by polling active memory and repository 13. GoTo 1\nAlgorithm Classify Input: Instance I , Classifier M Output: class value 1. if M is a Decision Tree, route I to a leaf and return the class label of the leaf 2. else using all coefficients (j) of M , Calculate f(x) using f(x) = \u2211 j \u03c9j\u03c8j(x)\nwhere \u03c8j(x) is the the complex conjugate function of \u03c8j(x) and x is the instance I\n3. If f(x) is greater than 0.5, return class1, otherwise class2"}, {"heading": "4.2 Optimizing the Energy Thresholding Process", "text": "In order to avoid unnecessary computation of higher order coefficients which yield increasingly low returns on classification accuracy, energy threshold is highly desirable. To threshold on energy a subset S of the (lower order) coefficients needs to be determined such that E(S)E(T ) > , where E(T ) denotes the total energy across the spectrum and is the desired energy threshold value.\nIn our optimized thresholding, we first compute the cumulative energyCEi at order i given by: CEi = \u2211i j=0 \u2211 k(wk\n2|order(k) = j). Given an order i, an upper bound estimate for the cumulative energy across the rest of the spectrum is given by: (d+1\u2212(i+1)+1)CEi, as the exponential decay property ensures that the energy at each of the orders i + 1, i + 2, \u00b7 \u00b7 \u00b7 , d is less than energy Ei at order i, where d is number of attributes in the dataset. Thus a lower bound estimate for the fraction of the cumulative energy CEFi at order i to the total energy across all orders can then be expressed as:\nCEFi = CEi\nCEi + (d\u2212 i+ 1)Ei (4)\nwhere Ei is actual (computed) energy at order i. The lower bound estimate allows the specification of a threshold based on the energy captured by a given order i which is more meaningful to set rather than an arbitrary threshold.\nThe scheme expressed by equation (4) enables the thresholding process to be done algorithmically. If the cumulative energy CEFi \u2265 , then we can guarantee that the actual energy captured is at least , since CEFi is a lower bound estimate. On the other hand if CEFi < , then CEFi+1 can be expressed as:\nCEFi+1 = CEi+1\nCEi+1 + (d\u2212 i)Ei+1 = CEi + Ei+1 CEi + dEi+1\n(5)\nThus equation (5) enables the cumulative fraction to be easily updated incrementally for the next higher order (i+1) by simply computing the energy at that order while exploiting the exponential decay property of Fourier spectrum. The thresholding method guarantees that no early termination will take place. This is because CEFi is a lower bound estimate and hence the order that it returns will never be less than the true order that captures a given fraction of the total actual energy in the spectrum."}, {"heading": "5 Experimental Study", "text": "This section elaborates on our empirical study involving the following learning systems: CBDT, FCT (Fourier Concept Trees) and MetaCT. The FCT incorporates the Fourier compressed trees in a repository in addition to the forest of trees that standard CBDT maintains. We implement Gama\u2019s meta learning approach with CBDT as the base learner, namely MetaCT. The main focus of the study is to assess the extent to which recurrences are recognized using old models preserved in classifier pools."}, {"heading": "5.1 Parameter Values", "text": "All experimentation was done with the following parameter values:\n\u2022 Hoeffding Tree Parameters The desired probability of choosing the correct split attribute=0.99, Tie Threshold=0.01, Growth check interval=32\n\u2022 Tree Forest Parameters Maximum Node Count=5000, Maximum Number of Fourier Trees=50, Accuracy Tie Threshold \u03c4=0.01\n\u2022 ADWIN Parameters drift significance value=0.01, warning significance value=0.3 (MetaCT only)\nAll experiments were done on the same software with C# .net runtime and hardware with Intel i5 CPU and 8GB RAM, clearning the memory in each run to have a fair comparison."}, {"heading": "5.2 Datasets used for the experimental study", "text": "We experimented with data generated from 3 data generators commonly used in drift detection and recurrent concept mining, namely SEA concept [16], RBF and Rotating hyperplane generators. In addition we used 2 real-world datasets, Spam and the NSW electricity which have also been commonly used in previous research.\nFor the synthetic datasets, each of the 4 concepts spanned 5,000 instances and reappeared 25 times in a data set, yielding a total of 500,000 instances with 100 true concept drift points.\nIn order to challenge the concept recognition process, we added a 10% noise level for all synthetic data sets to ensure that concepts recur in similar, but not exact form."}, {"heading": "5.2.1 Synthetic Data Sets", "text": "We used MOA [3] as the tool to generate these datasets.\n1. SEA: The concepts are defined by the function feature1+feature2 > threshold. We ordered the concepts as concept1, concept2, concept3 and concept4 generated using threshold values 8,7,9 and 9.5 respectively on the first data segment of size 20,000. We generated 96 recurrences of a modified form of these concepts by using different seed values in MOA for each sequence of recurrence. Thus, our version of this dataset differed from the one used by Gama and Kosina [4]. who simply used 3 concepts with the third being an exact copy of the first.\n2. RBF: The number of centroids parameter was adjusted to generate different concepts for the RBF dataset. Concept1, concept2, concept3 and concept4 were produced with the number of centroids set to 5,15, 25 and 35 respectively. Similar to the SEA dataset, the seed parameter helped in producing similar concepts for a given centroid count value. This dataset had 10 attributes.\n3. Rotating hyperplane: The number of drifting attributes was adjusted to 2,4,6, and 8 in a 10 dimensional data set to create the four concepts. The concept ordering, generation of similar concepts and concatenation were exactly the same as in the other data sets mentioned above."}, {"heading": "5.2.2 Real World datasets", "text": "1. Spam Data Set: The Spam dataset was used in it original form 1 which encapsulates an evolution of Spam messages. There are 9,324 instances and 499 informative attributes, which was different from the one version used by Gama that had 850 attributes.\n1from http://www.liaad.up.pt/kdus/products/datasets-for-concept-drift\n2. Electricity Data Set: NSW Electricity dataset is also used in its original form 2. There are two classes Up and Down that indicate the change of price with respect to the moving average of the prices in last 24 hours."}, {"heading": "5.3 Tuning MetaCT Key Parameters", "text": "In our preliminary experiments, we found optimal values for the two parameters, delay in receiving labels for the instances in short term memory, and hit percentage threshold value as 200 and 80%, respectively. The latter parameter reflects the estimated similarity of the current concept with one from the past and thus controls the degree of usage of classifiers from the pool."}, {"heading": "5.4 Comparative study: CBDT vs FCT vs MetaCT", "text": "Our focus in this series of experiments was to assess the models in terms of accuracy, memory consumption and processing times. None of the previous studies reported in the recurrent concept mining literature undertook a comparative study against other approaches and so we believe our empirical study to be the first such effort. Furthermore, all of the previous studies focused exclusively on accuracy without tracking memory and execution time overheads and so this study would also be the first of its kind."}, {"heading": "5.4.1 Accuracy", "text": "A delay period of 200 was used with all three approaches in order to perform a fair comparison. Figure 2 clearly shows that overall, FCT significantly outperforms its two rivals with respect to classification accuracy. The major reason for FCT\u2019s superior performance was its ability to re-use previous classifiers as shown in the segment 20k-25k on the RBF dataset where the concept is similar to concept1 that occurred in interval 1-5K. This is in contrast to MetaCT which was unable to recognize the recurrence of concept1. A similar situation occurs in the interval 25k-35k where the concept is similar to the previously occurring concepts, which are concept2 and 3. As expected CBDT, operating on its own without support for concept recurrence had a relatively flat trajectory throughout the stream segment.\nA similar trend to the RBF dataset was observed in Rotating Hyperplane and SEA datasets as well. It can be clearly seen that FCT was successful in reusing the models learned before on data segments from 20k to 25k and from 30k to 35k. Though a preserved model was reused on the data segment from 25k to 30k (corresponding to concept3), the accuracy was not as high as in the above two segments. On the segment from 35k to 40k, concept recurrence was not picked up by either FCT or MetaCT resulting in a new classifier being used.\nWe omit the figure for the SEA dataset due to space constraints. In summary, FCT outperformed MetaCT over 90 recurring concepts whereas MetaCT did better in 6 occurrences, thus maintaining the same trend as with the other 2 synthetic datasets that we experimented with.\nThe next experiment was on the NSW Electricity data set. Figure 2 shows that overall, FCT was the winner here as well, outperforming MetaCT at 25 segments out of 35 that we tracked. Sudden fall in accuracy of MetaCT is occational but due to incorrect selection of winner which was a decision stump.\n2from http://moa.cms.waikato.ac.nz/datasets/"}, {"heading": "5.4.2 Memory", "text": "Our experimentation on accuracy has revealed, especially in the case of FCT, the key role that concept capture and re-use has played in improving accuracy. The question is, what price has to be paid in terms of memory overhead in storing these recurrent concepts? Table1 clearly shows that the Fourier transformed trees consume a small fraction of the memory used by the pool of trees kept in FCT\u2019s active memory, despite the fact that collectively these models outperform their decision tree counterparts at a greater number of points in the stream progression.\nComparison of overall memory consumption across FCT and MetaCT is complicated by the fact that the latter tended to have immature trees in its classifier pool that under fits concepts. Despite this, Table 1 reveals that FCT\u2019s memory consumption is competitive to that of MetaCT. The only case where MetaCT had a substantially lower consumption was with the Spam dataset with a lower overhead for active memory."}, {"heading": "5.4.3 Processing Speed", "text": "FCT and MetaCT have two very contrasting methods of classification. The former routes each unlabeled instance to a single tree, which is the best performing tree selected at the last concept drift point. In contrast MetaCT classifies by routing an unlabeled instance to all referees to obtain their assessment of their corresponding models and in general will have more processing overhead on a per instance basis. However, FCT has potentially more overhead at concept drift points if the winner tree is one that is selected from the active forest as this tree needs to be converted into its Fourier representation. Thus it is interesting to contrast the run time performances of the two approaches.\nTable 1 shows that in general FCT has a higher processing speed (measured in instances processed per second); the only exception was with the Electricity dataset where MetaCT was faster. The electricity data contains a relatively larger number of drift points in comparison to the other datasets and this in turn required a greater number of DFT operations to be performed, thus slowing down the processing. In our future research we will investigate methods of optimizing the DFT process.\nFinally, we close this section with two general observations on FCT which hold\nacross all experiments reported above. Firstly, we note that the Discrete Fourier Transform (DFT), as expected, was able to capture the essence of a concept to the extent that when it reappeared in a modified form in the presence of noise it was still recognizable and was able to classify it accurately. Secondly, not only was the DFT robust to noise, it actually performed better than the original decision trees at concept recurrence points due to its better generalization capability."}, {"heading": "5.5 Sensitivity Analysis on FCT", "text": "Having established the superiority of FCT we were interested in exploring the sensitivity of FCT\u2019s accuracy on two key factors.\n5.5.1 Energy Threshold\nFCT\u2019s energy threshold parameter controls the extent to which it captures recurring contexts. We ran experiments with all datasets we experimented with and tracked accuracy across four different thresholds: 95%, 80%, 40% and 20%. The trends observed for all datasets were very similar and hence we display results for the SEA concepts dataset due to space constraints. Figure 3 clearly shows that very little difference in accu-\nracy exists between the trajectories for 40% and 95%, showing the resilience of the DFT in capturing the classification power of concepts at low energy levels such as 40%. Thus the low order Fourier coefficients that survive the 40% threshold hold almost the same classification power of spectra at the 80% or 90% levels which contain more coefficients. Such higher energy spectra would represent larger decision trees in which some of the decision nodes would be split into leaf nodes, thus enabling them to reach a slightly higher level of accuracy."}, {"heading": "5.5.2 Noise Level", "text": "In section 5.4 we observed that FCT outperformed MetaCT by recognizing concepts from the past even though the concepts did not recur exactly in their original form due partly to noise and partly due to different data instances being produced as a result of re-seeding of the concept generation functions. In this experiment we explicitly test the resilience of FCT to noise level by subjecting it to three different levels of noise\n- 10%, 20% and 30%. For reasons of completeness we also included MetaCT in he experimentation to aid in the interpretation of results.\nFigure 4 reveals three interesting pieces of information. Firstly, FCT is still able to recognize recurring concepts at the 20% noise level even though the models it re-uses do not have quite the same classification power (when compared to the 10% noise level) on the current concept due to data instances being corrupted by a relatively higher level of noise.\nSecondly, FCT\u2019s concept recurrence recognition is essentially disabled at the 30% noise level as shown by its flat trajectory, thus essentially performing at the level of the base CBDT system. It is able to avoid drops in accuracy on account of the forest of trees that is maintained and is able to switch quickly and seamlessly from one tree to another when concepts change occurs.\nThirdly, although MetaCT is not the focus of this experiment we see that MetaCT\u2019s ability to recognize recurring concepts is disabled at the 20% level, showing once gain the resilience of the DFT to noise. At the 30% level its accuracy drops quite sharply at certain points in the stream.This is due to the fact that it learns a single new classifier and relies on it to classify instances in the current concept. In contrast, FCT exploits the entire forest of trees and switches from one tree to another tree in its active forest in response to drift."}, {"heading": "6 Conclusions and Future Work", "text": "In this research we proposed a novel mechanism for mining data streams by capturing and exploiting recurring concepts. Our experimentation showed that the Discrete Fourier Transform when applied on Decision Trees captures concepts very effectively, both in terms of information content and conciseness. The Fourier transformed trees were robust to noise and were thus able to recognize concepts that reappeared in modified form, thus contributing significantly to improving accuracy. Overall our proposed approach significantly outperformed the meta learning approach by Gama and Kosina [4] in terms of classification accuracy while being competitive in terms of memory and processing speed.\nWe were able to optimize the derivation of the Fourier spectrum by an efficient thresholding process but there is further scope for optimization in the computation of low order coefficients in streams exhibiting frequent drifts, as our experimentation with the NSW Electricity dataset reveals. Our future work will concentrate on two areas. Firstly we plan to investigate the use of multi-threading on a parallel processor platform to optimize the DFT operation. Allocating the DFT process to a thread while another thread processes the incoming stream will greatly speed up processing for FCT as the two processes are independent of each other and can be executed in parallel. Secondly, the computation of the Fourier basis function that requires a vector dot product computation can be optimized by using patterns in the two vectors involved."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In this research we address the problem of capturing recurring concepts in a data stream environment. Recurrence capture enables the re-use of previously learned classifiers without the need for re-learning while providing for better accuracy during the concept recurrence interval. We capture concepts by applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to obtain highly compressed versions of the trees at concept drift points in the stream and store such trees in a repository for future use. Our empirical results on real world and synthetic data exhibiting varying degrees of recurrence show that the Fourier compressed trees are more robust to noise and are able to capture recurring concepts with higher precision than a meta learning approach that chooses to re-use classifiers in their originally occurring form.", "creator": "LaTeX with hyperref package"}}}