{"id": "1409.1458", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2014", "title": "Communication-Efficient Distributed Dual Coordinate Ascent", "abstract": "modulations Communication remains the most significant bottleneck rolexes in rearwards the performance tuguegarao of heineken distributed kage optimization cardamine algorithms unlikable for large - scale gorham machine bo\u017eo learning. In 150-day this paper, late-model we propose sse a cgt communication - champasak efficient framework, landreaux CoCoA, denouncing that omak uses crescent local computation in dl&w a primal - dual setting to dauth dramatically reduce the panionios amount uab of necessary wolzien communication. We provide 5.05 a strong convergence rate analysis commisison for this carols class of soejima algorithms, as ofterschwang well crotched as experiments stories on damanhour real - ct2 world harbottle distributed sub-fields datasets with holtzbrinck implementations 9:56 in c-5s Spark. In our moc experiments, zilla we find that shawm as cucchi compared ferrite to voltaire state - of - cognomina the - art mini - oxenbridge batch versions bohne of selina SGD factually and abv SDCA algorithms, dehydrogenation CoCoA khomeyni converges buttresses to canners the rendsburg same. 001 - terengganu accurate solution peafowls quality execrable on b\u00f6rse average awhile 25x spent as lippes quickly.", "histories": [["v1", "Thu, 4 Sep 2014 14:59:35 GMT  (552kb)", "http://arxiv.org/abs/1409.1458v1", null], ["v2", "Mon, 29 Sep 2014 16:07:32 GMT  (187kb)", "http://arxiv.org/abs/1409.1458v2", "NIPS 2014 version, including proofs. Published in Advances in Neural Information Processing Systems 27 (NIPS 2014)"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["martin jaggi", "virginia smith", "martin tak\u00e1c", "jonathan terhorst", "sanjay krishnan", "thomas hofmann", "michael i jordan"], "accepted": true, "id": "1409.1458"}, "pdf": {"name": "1409.1458.pdf", "metadata": {"source": "META", "title": "Communication-Efficient Distributed Dual Coordinate Ascent", "authors": ["Martin Jaggi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n14 58\nv1 [\ncs .L\nG ]\n4 S"}, {"heading": "1 Introduction", "text": "With the immense growth of available data, developing distributed algorithms for machine learning is increasingly important, and yet remains a challenging topic both theoretically and in practice. On typical real-world systems, communicating data between machines is vastly more expensive than reading data from main memory, e.g. by a factor of several orders of magnitude when leveraging commodity hardware.1 Yet, despite this reality, most existing distributed optimization methods for machine learning require significant communication between workers, often equalling the amount of local computation (or reading of local data). This includes for example popular mini-batch versions of online methods, such as stochastic subgradient (SGD) and coordinate descent (SDCA).\nIn this work, we target this bottleneck. We propose a distributed optimization framework that allows one to freely steer the trade-off between communication and local computation. In doing so, the framework can be easily adapted to the diverse spectrum of available large-scale computing systems, ranging from high-latency commodity clusters to low-latency communication supercomputers or the multi-core setting.\nOur new framework, called COCOA (Communication-efficient distributed dual Coordinate Ascent), supports objectives for linear reguarlized loss minimization, encompassing a broad class of machine learning models. By leveraging the primal-dual structure of these optimization problems, COCOA can effectively combine partial results from local computation while avoiding conflict with updates simultaneously computed on other machines. In each round, COCOA employs steps of an arbitrary dual optimization method on the local data on each machine, in parallel. After that, a single update vector is communicated to the master node. For example, when choosing to perform H iterations (usually order of the data size n) of an online optimization method locally per round, our scheme\n\u2217Both authors contributed equally. 1On typical computers, the latency for accessing data in main memory is in the order of 100 nanoseconds.\nIn contrast, the latency for sending data over a standard network connection is around 250,000 nanoseconds.\nsaves a factor of H in terms of communication compared to the corresponding naive distributed update scheme (i.e., updating a single point before communication). When processing the same number of datapoints, this is clearly a dramatic savings.\nOur theoretical analysis in Section 4 shows that this significant reduction in communication cost comes with only a very moderate increase in the amount of total computation, in order to reach the same optimization accuracy. The theory shows in general that the distributed COCOA framework will inherit the convergence rate of the internally-used local optimization method. When using SDCA (randomized dual coordinate ascent) as the local optimizer and assuming smooth losses, this convergence rate is geometric.\nIn practice, our experiments with the method implemented on the fault-tolerant Spark platform2 [ZCD+12] confirm both the clock time performance and huge communication savings of the proposed method on a variety distributed datasets. Our experiments consistently show order of magnitude gains over traditional mini-batch methods of both SGD and SDCA, and significant gains over the faster but theoretically less justified local SGD methods.\nRelated Work. As we discuss below (Section 5), our approach is distinguished from recent work on parallel and distributed optimization [Yan13, YZJL13, TBRS13, NRRW11, ZWSL10, BKBG11, MRT14, NC13] in that we provide a general framework for improving the communication efficiency of any dual optimization method. Further, to the best of our knowledge, our work is the first to analyze the convergence rate for an algorithm with this level of communication efficiency, without making data-dependent assumptions. The presented analysis covers the case of smooth losses, but should also be extendable to the non-smooth case. Existing methods using mini-batches [TBRS13, Yan13, TRS14] are closely related, though our algorithm makes significant improvements by immediately applying all updates locally while they are processed, a scheme that is not considered in the classic mini-batch setting. This intuitive modification results in dramatically improved empirical results and also strengthens our theoretical convergence rate. More precisely, the convergence rate shown here only degrades with the number of workers K , instead of with the significantly larger mini-batch-size (typically order n) in the case of mini-batch methods.\nOur method builds on a closely related recent line of work of [Yan13, YZJL13, YHCL10, YHCL12]. We generalize the algorithm of [Yan13, YZJL13] by allowing the use of arbitrary (dual) optimization methods as the local subroutine within our framework. In the special case of using coordinate ascent as the local optimizer, the resulting algorithm is very similar, though with a different computation of the coordinate updates. Moreover, we provide the first theoretical convergence rate analysis for such methods, without making strong assumptions on the data.\nThe proposed COCOA framework in its basic variant is entirely free of tuning parameters or learning rates, in contrast to SGD-based methods. The only choice to make is the selection of the internal local optimization procedure, steering the desired trade-off between communication and computation. When choosing a primal-dual optimizer as the internal procedure, the duality gap readily provides a fair stopping criterion and efficient accuracy certificates during optimization.\nPaper Outline. The rest of the paper is organized as follows. In Section 2 we describe the problem setting of interest. Section 3 outlines the proposed framework, COCOA, and the convergence analysis of this method is presented in Section 4. We discuss related work in Section 5, and compare against several other state-of-the-art methods empirically in Section 6."}, {"heading": "2 Setup", "text": "A large class of methods in machine learning and signal processing can be posed as the minimization of a convex loss function of linear predictors with a convex regularization term:\nmin w\u2208Rd\n[\nP (w) := \u03bb\n2 \u2016w\u20162 +\n1\nn\nn\u2211\ni=1\n\u2113i(w T xi)\n]\n, (1)\n2 We plan to release our code to be used with the open-source machine-learning library of Spark, MLlib.\nHere the data training examples are real-valued vectors xi \u2208 Rd; the loss functions \u2113i, i = 1, . . . , n are convex and depend possibly on labels yi \u2208 R; and \u03bb > 0 is the regularization parameter. Using the setup of [SSZ13], we assume the regularizer is the \u21132-norm for convenience. Examples of this class of problems include support vector machines, as well as regularized linear and logistic regression, ordinal regression, and others.\nThe most popular method to solve problems of the form (1) is the stochastic subgradient method (SGD) [RM51, Bot10, SSSSC10]. In this setting, SGD becomes an online method where every iteration only requires access to a single data example (xi, yi), and the convergence rate is wellunderstood.\nThe associated conjugate dual problem of (1) takes the following form, and is defined over one dual variable per each example in the training set.\nmax \u03b1\u2208Rn\n[\nD(\u03b1) := \u2212 \u03bb\n2 \u2016A\u03b1\u20162 \u2212\n1\nn\nn\u2211\ni=1\n\u2113\u2217i (\u2212\u03b1i) ] , (2)\nwhere \u2113\u2217i is the conjugate (Fenchel dual) of the loss function \u2113i, and the data matrix A \u2208 R d\u00d7n collects the (normalized) data examples Ai := 1\u03bbnxi in its columns. The duality comes with the convenient mapping from dual to primal variables w(\u03b1) := A\u03b1 as given by the optimality conditions [SSZ13]. For any configuration of the dual variables \u03b1, we have the duality gap defined as P (w(\u03b1))\u2212D(\u03b1). This gap is a computable certificate of the approximation quality to the unknown true optimum P (w\u2217) = D(\u03b1\u2217), and therefore serves as a useful stopping criteria for algorithms.\nFor problems of the form (2), coordinate descent methods have proven to be very efficient, and come with several benefits over primal methods. In randomized dual coordinate ascent (SDCA), updates are made to the dual objective (2) by solving for one coordinate completely while keeping all others fixed. This algorithm has been implemented in a number of software packages (e.g. LibLinear [HCL+08]), and has proven very suitable for use in large-scale problems, while giving stronger convergence results than the primal-only methods (such as SGD), at the same iteration cost [SSZ13]. In addition to superior performance, this method also benefits from requiring no stepsize, and having a well-defined stopping criterion given by the duality gap."}, {"heading": "3 Method Description", "text": "The COCOA framework, as presented in Algorithm 1, assumes that the data {(xi, yi)}ni=1 for a regularized loss minimization problem of the form (1) is distributed over K worker machines. We associate with the datapoints their corresponding dual variables {\u03b1i}ni=1, being partitioned between the workers in the same way. The core idea is to use the dual variables to efficiently merge the parallel updates from the different workers without much conflict, by exploiting the fact that they all work on disjoint sets of dual variables.\nAlgorithm 1: COCOA: Communication-Efficient Distributed Dual Coordinate Ascent Input: T \u2265 1, scaling parameter 1 \u2264 \u03b2K \u2264 K (default: \u03b2K := 1). Data: {(xi, yi)}ni=1 distributed over K machines Initialize: \u03b1(0)[k] \u2190 0 for all machines k, and w (0) \u2190 0 for t = 1, 2, . . . , T for all machines k = 1, 2, . . . ,K in parallel\n(\u2206\u03b1[k],\u2206wk) \u2190 LOCALDUALMETHOD(\u03b1 (t\u22121) [k] ,w (t\u22121)) \u03b1 (t) [k] \u2190 \u03b1 (t\u22121) [k] + \u03b2K K\n\u2206\u03b1[k] end reduce w(t) \u2190 w(t\u22121) + \u03b2K\nK \u2211K k=1 \u2206wk\nend\nIn each round, the K workers in parallel perform some steps of an arbitrary optimization method, applied to their local data. This internal procedure tries to maximize the dual formulation (2), only with respect to their own local dual variables. We call this local procedure LOCALDUALMETHOD,\nas specified in the template Procedure A. Our core observation is that the necessary information each worker requires about the state of the other dual variables can be very compactly represented by a single primal vector w \u2208 Rd, without ever sending around data or dual variables between the machines.\nProcedure A: LOCALDUALMETHOD: Dual algorithm for prob. (2) on a single coordinate block k\nInput: Local \u03b1[k] \u2208 Rnk , and w \u2208 Rd consistent with other coordinate blocks of \u03b1 s.t. w = A\u03b1 Data: Local {(xi, yi)} nk i=1 Output: \u2206\u03b1[k] and \u2206w := A[k]\u2206\u03b1[k]\nProcedure B: LOCALSDCA: SDCA iterations for problem (2) on a single coordinate block k\nInput: H \u2265 1, \u03b1[k] \u2208 Rnk , and w \u2208 Rd consistent with other coordinate blocks of \u03b1 s.t. w = A\u03b1 Data: Local {(xi, yi)} nk i=1 Initialize: w(0) \u2190 w, \u2206\u03b1[k] \u2190 0 \u2208 Rnk for h = 1, 2, . . . , H choose i \u2208 {1, 2, . . . , nk} uniformly at random\nfind \u2206\u03b1 maximizing \u2212\u03bbn2 \u2016w (h\u22121) + 1 \u03bbn \u2206\u03b1xi\u2016 2 \u2212 \u2113\u2217i ( \u2212 (\u03b1 (h\u22121) i +\u2206\u03b1) ) \u03b1 (h) i \u2190 \u03b1 (h\u22121) i +\u2206\u03b1 (\u2206\u03b1[k])i \u2190 (\u2206\u03b1[k])i +\u2206\u03b1 w (h) \u2190 w(h\u22121) + 1\n\u03bbn \u2206\u03b1xi\nend Output: \u2206\u03b1[k] and \u2206w := A[k]\u2206\u03b1[k]\nAllowing the subroutine to process more than one local data example per round dramatically reduces the amount of communication between the workers. By definition, COCOA in each outer iteration only requires communication of a single vector for each worker, that is \u2206wk \u2208 Rd. Further, as we will show in Section 4, COCOA inherits the convergence guarantee of any algorithm run locally on each node in the inner loop of Algorithm 1. We suggest to use randomized dual coordinate ascent (SDCA) [SSZ13] as the internal optimizer in practice, as implemented in Procedure B, and also used in our experiments.\nNotation. In the same way the data is partitioned across the K worker machines, we write the dual variable vector as \u03b1 = (\u03b1[1], . . . ,\u03b1[K]) \u2208 Rn with the corresponding coordinate blocks\u03b1[k] \u2208 Rnk such that \u2211\nk nk = n. The submatrix A[k] collects the columns of A (i.e. rescaled data examples) which are available locally on the k-th worker. The parameter T determines the number of outer iterations of the algorithm, while when using an online internal method such as LOCALSDCA, then the number of inner iterations H determines the computation-communication trade-off factor."}, {"heading": "4 Convergence Analysis", "text": "Considering the dual problem (2), we define the local suboptimality on each coordinate block as:\n\u03b5D,k(\u03b1) := max \u03b1\u0302[k]\u2208R n k D((\u03b1[1], . . . , \u03b1\u0302[k], . . . ,\u03b1[K]))\u2212D((\u03b1[1], . . . ,\u03b1[k], . . . ,\u03b1[K])), (3)\nthat is how far we are from the optimum on block k with all other blocks fixed. Note that this differs from the global suboptimality max\u03b1\u0302 D(\u03b1\u0302)\u2212D((\u03b1[1], . . . ,\u03b1[K])).\nAssumption 1 (Local Geometric Improvement of LOCALDUALMETHOD). We assume that there exists \u0398 \u2208 [0, 1) such that for any given \u03b1, LOCALDUALMETHOD when run on block k alone returns a (possibly random) update \u2206\u03b1[k] such that\nE[\u01ebD,k((\u03b1[1], . . . ,\u03b1[k\u22121],\u03b1[k] +\u2206\u03b1[k],\u03b1[k+1], . . . ,\u03b1[K]))] \u2264 \u0398 \u00b7 \u01ebD,k(\u03b1). (4)\nNote that this assumption is satisfied for several available implementations of the inner procedure LOCALDUALMETHOD, in particular for LOCALSDCA, as shown in the following Proposition.\nFrom here on, we assume that the input data is scaled such that \u2016xi\u2016 \u2264 1 for all datapoints.\nProposition 1. Assume the loss functions \u2113i are (1/\u03b3)-smooth. Then for using LOCALSDCA, Assumption 1 holds with\n\u0398 =\n(\n1\u2212 \u03bbn\u03b3\n1 + \u03bbn\u03b3\n1\nn\u0303\n)H\n. (5)\nwhere n\u0303 := maxk nk is the size of the largest block of coordinates.\nTheorem 2. Assume that Algorithm 1 is run for T outer iterations on K worker machines, with the procedure LOCALDUALMETHOD having local geometric improvement \u0398, and let \u03b2K := 1. Further, assume the loss functions \u2113i are (1/\u03b3)-smooth. Then the following geometric convergence rate holds for the global (dual) objective:\nE[D(\u03b1\u2217)\u2212D(\u03b1(T ))] \u2264\n(\n1\u2212 (1\u2212\u0398) 1\nK\n\u03bbn\u03b3\n\u03c3 + \u03bbn\u03b3\n)T (\nD(\u03b1\u2217)\u2212D(\u03b1(0)) ) . (6)\nHere \u03c3 is any real number satisfying\n\u03c3 \u2265 \u03c3min := max \u03b1\u2208Rn\n\u03bb2n2 \u2211K k=1\u2016A[k]\u03b1[k]\u2016 2 \u2212 \u2016A\u03b1\u20162\n\u2016\u03b1\u20162 \u2265 0. (7)\nLemma 3. If K = 1 then \u03c3min = 0. For any K \u2265 1, when assuming \u2016xi\u2016 \u2264 1 \u2200i, we have\n0 \u2264 \u03c3min \u2264 n\u0303.\nMoreover, if datapoints between different workers are orthogonal, i.e. (ATA)i,j = 0 \u2200i, j such that i and j do not belong to the same part, then \u03c3min = 0.\nIf we choose K = 1 then, Theorem 2 together with Lemma 3 implies that\nE[D(\u03b1\u2217)\u2212D(\u03b1(T ))] \u2264 \u0398T ( D(\u03b1\u2217)\u2212D(\u03b1(0)) ) ,\nas expected, showing that the analysis is tight in the special case K = 1. More interestingly, we observe that for any K , in the extreme case when the subproblems are solved to optimality (i.e. letting H \u2192 \u221e in LOCALSDCA), then the algorithm as well as the convergence rate match that of serial/parallel block-coordinate descent [RT14, RT12].\nNote: If choosing the starting point as \u03b1(0) := 0 as in the main algorithm, then it is known that D(\u03b1\u2217)\u2212D(\u03b1(0)) \u2264 1 (see e.g. Lemma 20 in [SSZ13])."}, {"heading": "5 Related Work", "text": "Distributed Primal-Dual Methods. Our approach is most closely related to recent work by [Yan13, YZJL13], which generalizes the distributed optimization method for linear SVMs as in [YHCL10] to the primal-dual setting considered here (which was introduced by [SSZ13]). The difference between our approach and the \u2018practical\u2019 method of [Yan13] is that our internal steps directly correspond to coordinate descent iterations on the global dual objective (2), for coordinates in the current block, while in [YZJL13, Equation 8] and [Yan13], the inner iterations apply to a slightly different notion of the sub-dual problem defined on the local data. In terms of convergence results, the analysis of [Yan13] only addresses the mini-batch case without local updates, while the more recent paper [YZJL13] shows a convergence rate for a variant of COCOA with inner coordinate steps, but under the unrealistic assumption that the data is orthogonal between the different workers. In this case, the optimization problems become independent, so that an even simpler single-round communication scheme summing the individual resulting models w would give an exact solution. Instead, we show a linear convergence rate for the full problem class of smooth losses, without any assumptions on the data, in the same generality as the non-distributed setting of [SSZ13].\nWhile the experimental results in all papers [YHCL10, Yan13, YZJL13] are encouraging for this type of method, they do not yet provide a quantitative comparison of the gains in communication efficiency, or compare to the analogous SGD schemes that use the same distribution and communication patterns, which is the main goal or our experiments in Section 6. For the special case of linear\nSVMs, the first paper to propose the same algorithmic idea was [YHCL10], which used LibLinear in the inner iterations. However, the proposed algorithm [YHCL10] processes the blocks sequentially (not in the parallel or distributed setting). Also, it is assumed that the subproblems are solved to near optimality on each block before selecting the next, making the method essentially standard block-coordinate descent. While no convergence rate was given, the empirical results in the journal paper [YHCL12] suggest that running LibLinear for just one pass through the local data performs well in practice. Here, we prove this, quantify the communication efficiency, and show that fewer local steps can improve the overall performance. For the LASSO case, [BKBG11] has proposed a parallel coordinate descent method converging to the true optimum, which could potentially also be interpreted in our framework here.\nMini-Batches. Another closely related avenue of research includes methods that use mini-batches to distribute updates. In these methods, a mini-batch, or sample, of the data examples is selected for processing at each iteration. All updates within the mini-batch are computed based on the same fixed parameter vector w, and then these updates are either added or averaged in a reduce step and communicated back to the worker machines. This concept has been studied for both SGD and SDCA, see e.g. [TBRS13, TRS14] for the SVM case. The so called naive variant of [Yan13] is also identical to mini-batch coordinate descent, with a slight scaling difference.\nAs is shown in [Yan13] and below in Section 6, the performance of these algorithms suffers when processing large batch sizes, as they do not take local updates immediately into account. Furthermore, they are very sensitive to the choice of the parameter \u03b2b, which controls the magnitude of all updates between \u03b2b := 1 for (conservatively) averaging, and \u03b2b := b for (aggressively) adding the updates (here we denote b as the size of the selected mini-batch, which can be of size up to n). This instability is illustrated by the fact that even the change of \u03b2b := 2 instead of \u03b2b := 1 can lead to divergence of coordinate descent (SDCA) in the simple case of just two coordinates [RT13]. In practice it can be very difficult to choose the correct data-dependent parameter \u03b2b especially for large mini-batch sizes b \u2248 n, as the parameter range spans many orders of magnitude, and directly controls the step size of the resulting algorithm, and therefore the convergence rate [RT13, FQR14]. For sparse data, the work of [RT13, FQR14] gives some data dependent choices of \u03b2b which are safe. Known convergence rates for the mini-batch methods degrade with larger batch size b \u2248 \u0398(n). More precisely, the improvement in objective function per example processed degrades with a factor of \u03b2b in [TBRS13, RT13, FQR14]. In contrast, our convergence rate as shown in Theorem 2 only degrades with the much smaller number of worker machines K , which can be a several order of magnitudes smaller than the mini-batch size b.\nSingle Round of Communication. One extreme is to consider methods with only a single round of communication (e.g. one map-reduce operation), as in [ZDW13, ZWSL10, MMM+09]. The output of these methods is the average of K individual models, trained only on the local data from each machine. In [ZDW13], the authors give conditions on the data and computing environment under which these type of one-communication algorithms may be sufficient. In general, however, the true optimum of the original problem (1) is not the average of these K models, no matter how accurately the subproblems are solved [SSZ14].\nNaive Distributed Online Methods, Delayed Gradients, and Multi-Core. On the other extreme, a natural way to distribute updates is to let every machine send updates to the master node (sometimes called the \u201cparameter server\u201d) as soon as they are performed. This is what we call the naive distributed SGD / CD in our experiments. The amount of communication for such naive distributed online methods is the same as the number of data examples processed. In contrast to this, the number of communicated vectors in our method is divided by H , that is the number of inner local steps performed per outer iteration, which can be \u0398(n).\nThe early work of [TBA86] introduced the nice framework of gradient updates where the gradients come with some delays, i.e. are based on outdated iterates, and shows some robust convergence rates. In the machine learning setting, [DGBSX12] and the later work of [AD11] have provided more insights into these type of methods. However, these papers study the case of smooth objective functions of a sum structure, and so do not directly apply to general case we consider here. In the same spirit, [NRRW11] implements SGD with communication-intense updates after each example processed, allowing asynchronous updates again within some delay. For coordinate descent, the\nanalogous approach was studied in [LWR+14]. Both methods [NRRW11, LWR+14] are H times less efficient in terms of communication when compared to COCOA, and are designed for multicore shared memory machines (where communication is as fast as memory access). They require the same communication cost as naive distributed SGD / CD, which we include in our experiments in Section 6, and a slightly larger number of iterations due to the asynchronicity. The 1/t convergence rate shown in [NRRW11] only holds under strong sparsity assumptions on the data. A more recent paper [DJM13] deepens the understanding of such methods, but still only applies to very sparse data. For general data, [BBFM12] theoretically shows that 1/\u03b52 communications rounds of single vectors are enough to obtain \u03b5-quality for linear classifiers, with the rate also growing with K2 in the number of workers. Our new analysis here makes the dependence on 1/\u03b5 logarithmic."}, {"heading": "6 Experiments", "text": "In this section, we compare COCOA to traditional mini-batch versions of stochastic dual coordinate ascent and stochastic gradient descent, as well as the locally-updating version of stochastic gradient descent. We implement mini-batch SDCA (denoted mini-batch-CD) as described in [TBRS13, Yan13]. The SGD-based methods are mini-batch and locally-updating versions of Pegasos [SSSSC10], differing only in whether the primal vector is updated locally on each inner iteration or not, and whether the resulting combination/communication of the updates is by an average over the total size KH of the mini-batch (mini-batch-SGD) or just over the number of machines K (local-SGD). For each algorithm, we additionally study the effect of scaling the average by a parameter \u03b2K , as first described in [TBRS13], while noting that it is a benefit to avoid having to tune this data-dependent parameter.\nWe apply these algorithms to standard hinge loss \u21132-regularized support vector machines, using implementations written in Spark on m1.large Amazon EC2 instances [ZCD+12]. Though this nonsmooth case is not yet covered in our theoretical analysis, we still see remarkable empirical performance. Our results indicate that COCOA is able to converge to .001-accurate solutions nearly 25\u00d7 as fast compared the other algorithms, when all use \u03b2K = 1. The datasets used in these analyses are summarized in Table 1, and were distributed among K = 4, 8, and 32 nodes, respectively. We use the same regularization parameters as specified in [SSSSC10, HCL+08].\nIn comparing each algorithm and dataset, we analyze the progress in primal objective value as a function of both time (Figure 1) and communication (Figure 2). For all competing methods, we present the result for the batch size (H) that yields the best performance in terms of reduction in objective value over time. For the locally-updating methods (COCOA and local-SGD), these tend to be larger batch sizes corresponding to processing almost all of the local data at each outer step. For the non-locally updating mini-batch methods, (mini-batch SDCA [TBRS13] and mini-batch SGD [SSSSC10]), these typically correspond to smaller values of H , as averaging the solutions to guarantee safe convergence becomes less of an impediment for smaller batch sizes.\nFirst, we note that there is a clear correlation between the wall-time spent processing each dataset and the number of vectors communicated, indicating that communication has a significant effect on convergence speed. We see clearly that COCOA is able to converge to a more accurate solution in all datasets much faster than the other methods. On average, COCOA reaches a .001-accurate solution for these datasets 25x faster than the best competitor. This is a testament to the algorithm\u2019s ability to avoid communication while still making significant global progress by efficiently combining the local updates of each iteration. The improvements are robust for both regimes n \u226b d and n \u226a d.\nIn Figure 3 we explore the effect of H , the computation-communication trade-off factor, on the convergence of COCOA for the cov dataset on a cluster of 4 nodes. As described above, increasing H decreases communication but also affects the convergence properties of the algorithm. In Figure 4,\nwe attempt to scale the averaging step of each algorithm by using various \u03b2K values, for two different batch sizes on the Cov dataset (H = 1e5 and H = 100). We see that though \u03b2K has a larger impact on the smaller batch size, it is still not enough to improve the mini-batch algorithms beyond what is achieved by COCOA and local-SGD.\n\u22126\n10 \u22124\n10 \u22122\n10 0\n10 2\nTime (s)\nCOCOA (\u03b2 k =1) mini\u2212batch\u2212CD (\u03b2 k =10) local\u2212SGD (\u03b2 k =1) mini\u2212batch\u2212sgd (\u03b2 k =10)"}, {"heading": "7 Conclusion", "text": "We have presented a communication-efficient framework for distributed dual coordinate ascent algorithms that can be used to solve large-scale regularized loss minimization problems. This is crucial in settings where datasets must be distributed across multiple machines, and where communication amongst nodes is costly. We have shown that the proposed algorithm performs competitively on real-world, large-scale distributed datasets, and have presented the first theoretical analysis of this algorithm that achieves competitive convergence rates without making additional assumptions on the data itself.\nIt remains open to obtain improved convergence rates for more aggressive updates corresponding to \u03b2K > 1, which might be suitable for using the \u2018safe\u2019 updates techniques of [TBRS13] and the related expected separable over-approximations of [RT14, RT12], here applied to K instead of n blocks. Furthermore, it remains open to show convergence rates for local SGD in the same communication efficient setting as described here.\nAcknowledgments. We particularly want to thank Sanjay Krishnan for his help, as well as Shivaram Venkataraman, Ameet Talwalkar, and Peter Richta\u0301rik for fruitful discussions. MJ acknowledges support by the Simons Institute for the Theory of Computing."}, {"heading": "A Proof of Theorem 2", "text": "In the following, for a given vector \u03b1[k] \u2208 Rnk , we write \u03b1\u3008[k]\u3009 \u2208 Rn for its zero-padded version which coincides with \u03b1[k] on the k-th coordinate block, and is zero everywhere else.\nTheorem\u2019 2. Assume that Algorithm 1 is run for T outer iterations on K worker machines, with the procedure LOCALDUALMETHOD having local geometric improvement \u0398, and let \u03b2K := 1. Further, assume the loss functions \u2113i are (1/\u03b3)-smooth. Then the following geometric convergence rate holds for the global (dual) objective:\nE[D(\u03b1\u2217)\u2212D(\u03b1(T ))] \u2264\n(\n1\u2212 (1\u2212\u0398) 1\nK\n\u03bbn\u03b3\n\u03c3 + \u03bbn\u03b3\n)T (\nD(\u03b1\u2217)\u2212D(\u03b1(0)) ) .\nHere \u03c3 is any real number satisfying\n\u03c3 \u2265 \u03c3min := max \u03b1\u2208Rn\n\u03bb2n2 \u2211K k=1\u2016A[k]\u03b1[k]\u2016 2 \u2212 \u2016A\u03b1\u20162\n\u2016\u03b1\u20162 \u2265 0.\nProof. From the definition of the update performed by Algorithm 1 (for the setting of \u03b2K := 1), we have \u03b1(t+1) = \u03b1(t) + 1\nK \u2211K k=1 \u2206\u03b1\u3008[k]\u3009. Let us estimate the change of objective function after one\nouter iteration. Then using concavity of D we have\nD(\u03b1(t+1)) = D ( \u03b1 (t) + 1\nK \u2211K k=1\u2206\u03b1\u3008[k]\u3009\n) = D (\n1 K \u2211K k=1(\u03b1 (t)) + \u2206\u03b1\u3008[k]\u3009\n)\n\u2265 1 K \u2211K k=1D(\u03b1 (t)) + \u2206\u03b1\u3008[k]\u3009).\nSubtracting D(\u03b1(t)) from both sides and denoting by \u03b1\u0302\u2217[k] the local maximizer as in (3) we obtain\nD(\u03b1(t+1))\u2212D(\u03b1(t)) \u2265 1 K \u2211K k=1 [ D(\u03b1(t) +\u2206\u03b1\u3008[k]\u3009)\u2212D(\u03b1 (t)) ]\n= 1 K \u2211K k=1\n[\nD(\u03b1(t) +\u2206\u03b1\u3008[k]\u3009)\u2212D((\u03b1 (1) [t] , . . . , \u03b1\u0302 \u2217 [k], . . . ,\u03b1 (K) [t] ))\n+D((\u03b1 (1) [t] , . . . , \u03b1\u0302 \u2217 [k], . . . ,\u03b1 (K) [t] ))\u2212D(\u03b1\n(t)) ]\n(3) = 1\nK \u2211K k=1 [ \u03b5D,k(\u03b1 (t))\u2212 \u03b5D,k(\u03b1 (t) +\u2206\u03b1\u3008[k]\u3009) ] .\nConsidering the expectation of this quantity, we are now ready to use Assumption 1 on the local geometric improvement of the inner procedure. We have\nE[D(\u03b1(t+1))\u2212D(\u03b1(t)) |\u03b1(t)] \u2265 1 K \u2211K k=1 E[\u03b5D,k(\u03b1 (t))\u2212 \u03b5D,k(\u03b1 (t) +\u2206\u03b1\u3008[k]\u3009) |\u03b1 (t)]\n(4) \u2265 1\nK (1\u2212\u0398) \u2211K k=1\u03b5D,k(\u03b1 (t)).\nIt remains to bound \u2211K\nk=1\u03b5D,k(\u03b1 (t)).\n\u2211K k=1\u03b5D,k(\u03b1 (t)) (3) = max\n\u03b1\u0302\u2208Rn\n{ \u2211K\nk=1\n[ D((\u03b1[1], . . . , \u03b1\u0302[k], . . . ,\u03b1[K]))\u2212D((\u03b1[1], . . . ,\u03b1[k], . . . ,\u03b1[K]))\n]}\n(2) = max\n\u03b1\u0302\u2208Rn\n{ 1 n \u2211n i=1(\u2212\u2113 \u2217 i (\u2212\u03b1\u0302i) + \u2113 \u2217 i (\u2212\u03b1 (t) i ))\n+\u03bb2 \u2211K k=1\n[\n\u2212\u2016A\u03b1(t) + A[k](\u03b1\u0302[k] \u2212\u03b1 (t) [k])\u2016\n2 + \u2016A\u03b1(t)\u20162 ]}\n= max \u03b1\u0302\u2208Rn\n{\nD(\u03b1\u0302)\u2212D(\u03b1(t)) + \u03bb2 \u2016A\u03b1\u0302\u2016 2 \u2212 \u03bb2 \u2016A\u03b1 (t)\u20162\n+\u03bb2 \u2211K k=1\n[\n\u2212\u2016A\u03b1(t) + A[k](\u03b1\u0302[k] \u2212\u03b1 (t) [k])\u2016\n2 + \u2016A\u03b1(t)\u20162 ]}\n= max \u03b1\u0302\u2208Rn\n{\nD(\u03b1\u0302)\u2212D(\u03b1(t)) + \u03bb2\n[ \u2016A\u03b1\u0302\u20162 \u2212 \u2016A\u03b1(t)\u20162 ]\n+\u03bb2\n[\n\u22122(A\u03b1(t))TA(\u03b1\u0302 \u2212\u03b1(t))\u2212 \u2211K k=1\u2016A[k](\u03b1\u0302[k] \u2212\u03b1 (t) [k])\u2016\n2 ]}\n= max \u03b1\u0302\u2208Rn\n{\nD(\u03b1\u0302)\u2212D(\u03b1(t)) + \u03bb2\n[ \u2016A\u03b1\u0302\u20162 + \u2016A\u03b1(t)\u20162 \u2212 2(A\u03b1(t))TA\u03b1\u0302 ]\n+\u03bb2\n[\n\u2212 \u2211K k=1\u2016A[k](\u03b1\u0302[k] \u2212\u03b1 (t) [k])\u2016\n2 ]}\n= max \u03b1\u0302\u2208Rn\n{\nD(\u03b1\u0302)\u2212D(\u03b1(t)) + \u03bb2\n[ \u2016A(\u03b1(t) \u2212 \u03b1\u0302)\u20162 ]\n+\u03bb2\n[\n\u2212 \u2211K k=1\u2016A[k](\u03b1\u0302[k] \u2212\u03b1 (t) [k])\u2016\n2 ]}\n\u2265 max \u03b1\u0302\u2208Rn\n{\nD(\u03b1\u0302)\u2212D(\u03b1(t))\u2212 \u03c32\u03bbn2 \u2016\u03b1\u0302\u2212\u03b1 (t)\u20162\n}\n,\nby the definition (7) of the complexity parameter \u03c3. Now, we can conclude the bound as follows: \u2211K\nk=1\u03b5D,k(\u03b1 (t)) \u2265 max\n\u03b1\u0302\u2208Rn\n{\nD(\u03b1\u0302)\u2212D(\u03b1(t))\u2212 \u03c32\u03bbn2 \u2016\u03b1\u0302\u2212\u03b1 (t)\u20162\n}\n\u2265 max \u03b7\u2208[0,1]\n{\nD(\u03b7\u03b1\u2217 + (1\u2212 \u03b7)\u03b1(t))\u2212D(\u03b1(t))\u2212 1\n2\n\u03c3\u03b72 \u03bbn2 \u2016\u03b1\u2217 \u2212\u03b1(t)\u20162\n}\n\u2265 max \u03b7\u2208[0,1]\n{\n\u03b7D(\u03b1\u2217) + (1\u2212 \u03b7)D(\u03b1(t))\u2212D(\u03b1(t))\n+ \u03b3\u03b7(1\u2212 \u03b7)\n2n \u2016\u03b1\u2217 \u2212\u03b1(t)\u20162 \u2212\n1\n2\n\u03c3\u03b72 \u03bbn2 \u2016\u03b1\u2217 \u2212\u03b1(t)\u20162 }\n\u2265 max \u03b7\u2208[0,1]\n{\n\u03b7(D(\u03b1\u2217)\u2212D(\u03b1(t))) + \u03b7\n2n\n(\n\u03b3(1\u2212 \u03b7)\u2212 \u03c3\u03b7\n\u03bbn\n) \u2016\u03b1\u2217 \u2212\u03b1(t)\u20162 } .\nChoosing \u03b7\u2217 := \u03bbn\u03b3 \u03c3+\u03bbn\u03b3 \u2208 [0, 1] gives\n\u2211K k=1\u03b5D,k(\u03b1 (t)) \u2265 \u03bbn\u03b3 \u03c3+\u03bbn\u03b3 (D(\u03b1 \u2217)\u2212D(\u03b1(t))).\nTherefore, we have\nE[D(\u03b1(t+1))\u2212D(\u03b1\u2217) +D(\u03b1\u2217)\u2212D(\u03b1(t)) |\u03b1(t)] \u2265 (1\u2212\u0398) 1\nK\n\u03bbn\u03b3\n\u03c3 + \u03bbn\u03b3 (D(\u03b1\u2217)\u2212D(\u03b1(t))),\nE[D(\u03b1\u2217)\u2212D(\u03b1(t+1)) |\u03b1(t)] \u2264\n[\n1\u2212 (1 \u2212\u0398) 1\nK\n\u03bbn\u03b3\n\u03c3 + \u03bbn\u03b3\n]\n(D(\u03b1\u2217)\u2212D(\u03b1(t))).\nThis implies the claim (6) of the theorem."}, {"heading": "B Proof of Proposition 1", "text": "B.1 Decomposition of the Duality Structure over the Blocks of Coordinates\nThe core concept in our new analysis technique is the following primal-dual structure on each local coordinate block. Using this, we will show below that all steps of LOCALDUALMETHOD can be interpreted as performing coordinate ascent steps on the global dual objective function D(\u03b1), with the coordinates changes restricted to the k-th block, as follows.\nFor concise notation, we write {Ik}Kk=1 for the partition of data indices {1, 2, . . . , n} between the workers, such that |Ik| = nk. In other words, each set Ik consists of the indices of the k-th block of coordinates (those with the dual variables \u03b1[k] and datapoints A[k], as available on the k-th worker).\nLet us define the local dual problem as\nmax \u03b1[k]\u2208R n k\n[\nDk(\u03b1[k];w) := \u2212 \u03bb\n2 \u2016w +A[k]\u03b1[k]\u2016\n2 \u2212 1\nn\n\u2211\ni\u2208Ik\n\u2113\u2217i (\u2212\u03b1i) + \u03bb\n2 \u2016w\u20162\n]\n. (8)\nThe definition is valid for any fixed vector w. Here the reader should think of w as representing the status of those dual variables \u03b1i which are not part of the active block k. The idea is the following: For the choice of w := \u2211\nk\u2032 6=k A[k\u2032]\u03b1 (0) [k\u2032], it turns out that the local dual objective, as a function of\nthe local variables \u03b1[k], is identical the global (dual) objective, up to a constant independent of \u03b1[k], or formally\nDk(\u03b1[k];w) = D ( \u03b1 (0) [1] , . . . ,\u03b1[k], . . . ,\u03b1 (0) [K] ) + C\u2032 \u2200 \u03b1[k] \u2208 R nk .\nThe following proposition observes that the defined local problem has a very similar duality structure as the original problem (1) with its dual (2).\nProposition 4. For w \u2208 Rd, let us define the \u201clocal\u201d primal problem on the k-th block as\nmin wk\u2208Rd\n[\nPk(wk;w) := 1\nn\n\u2211\ni\u2208Ik\n\u2113i((w +wk) T xi) +\n\u03bb 2 \u2016wk\u2016\n2 ]\n. (9)\nThen the dual of this formulation (with respect to the variable wk) is given by the local dual problem (8) for the k-th coordinate block.\nProof. The dual of this problem is derived by plugging in the definition of the conjugate function \u2113i((w +wk) T xi) = max\u03b1i \u2212\u03b1i(w +wk) T xi \u2212 \u2113 \u2217 i (\u2212\u03b1i), which gives\nmin wk\u2208Rd\n1\nn\n\u2211\ni\u2208Ik\nmax \u03b1i\n( \u2212 \u03b1i(w +wk) T xi \u2212 \u2113 \u2217 i (\u2212\u03b1i) ) + \u03bb\n2 \u2016wk\u2016\n2\n= 1\nn\n\u2211\ni\u2208Ik\nmax \u03b1i \u2212\u2113\u2217i (\u2212\u03b1i) + min wk\u2208Rd\n[\n\u2212\u03b1i(w +wk) T xi +\n\u03bb 2 \u2016wk\u2016 2\n]\n= max \u03b1\u2208Rnk\n1\nn\n\u2211\ni\u2208Ik\n\u2212\u2113\u2217i (\u2212\u03b1i) + min wk\u2208Rd\n[\n\u2212 1\nn\n\u2211\ni\u2208Ik\n\u03b1i(w +wk) T xi +\n\u03bb 2 \u2016wk\u2016 2\n]\n.\nThe first-order optimality condition forwk, by setting its derivative to zero in the inner minimization, can be written as\nw \u2217 k =\n1\n\u03bbn\n\u2211\ni\u2208Ik\n\u03b1ixi = A[k]\u03b1[k]\nplugging this back in, we have that the inner minimization becomes\n\u2212 \u03bb 1\n\u03bbn\n\u2211\ni\u2208Ik\n\u03b1i(w +w \u2217 k) T xi +\n\u03bb 2 \u2016w\u2217k\u2016 2\n=\u2212 \u03bb(w +w\u2217k) T w \u2217 k +\n\u03bb 2 \u2016w\u2217k\u2016 2\nWriting the resulting full problem, we obtain precisely the local dual problem (8) for the k-th coordinate block.\nUsing these local subproblems, we have the following nice structure of local and global duality gaps:\ngk(\u03b1) := Pk(wk;w)\u2212Dk(\u03b1[k];w) and g(\u03b1) := P (w(\u03b1))\u2212D(\u03b1)\nHere the contributions of all blocks except the active one are collected in w := w\u2212wk. Recall that we defined wk := A[k]\u03b1[k] and w := A\u03b1, so that w = \u2211K k=1 wk.\nB.2 Local Convergence of LOCALSDCA\nThis section is just a small modification of results obtained in [SSZ13].\nObserve that the coordinate step performed by an iteration of LOCALSDCA can equivalently be written as\n\u2206\u03b1\u2217i = \u2206\u03b1 \u2217 i\n( \u03b1\n(h\u22121) [k] ,w\n) := argmax\n\u2206\u03b1i\n(\nDk(\u03b1 (h\u22121) [k] + ei\u2206\u03b1i;w)\n)\n. (10)\nFor a vector w \u2208 Rd. In other words, the step will optimize one of the local coordinates with respect to the local dual objective, which is identical to the global dual objective when the coordinates of the other blocks are kept fixed, as we have seen in the previous subsection.\nLemma 5. Assume that \u2113\u2217i is \u03b3-strongly convex (where \u03b3 \u2265 0). Then for all iterations h of LOCALSDCA and any s \u2208 [0, 1] we have\nE[Dk(\u03b1 (h) [k] ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w)] \u2265\ns\nnk\n( Pk(w (h\u22121) k ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w) ) \u2212\ns2\n2\u03bbn2 G(h), (11)\nwhere G(h) := 1 nk\n\u2211\ni\u2208Ik\n(\n\u2016xi\u2016 2 \u2212 \u03bbn\u03b3(1\u2212s)\ns\n)\n(u (h) i \u2212\u03b1 (h\u22121) i ) 2, \u2212u (h\u22121) i \u2208 \u2202\u2113i(x T i w (h\u22121)) and\nw (h\u22121) = w +A[k]\u03b1 (h\u22121) [k] .\nProof.\nn [\nDk(\u03b1 (h) [k] ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w)\n]\n=\u2212\u2113\u2217i (\u2212\u03b1 (h) i )\u2212\n\u03bbn\n2 \u2016w(h)\u20162\n\ufe38 \ufe37\ufe37 \ufe38\nA\n\u2212\n(\n\u2212\u2113\u2217i (\u2212\u03b1 (h\u22121) i )\u2212\n\u03bbn\n2 \u2016w(h\u22121)\u20162\n)\n\ufe38 \ufe37\ufe37 \ufe38\nB\n.\nBy the definition of the update (10) we have for all s \u2208 [0, 1] that\nA = max \u2206\u03b1i\n\u2212\u2113\u2217i (\u2212\u03b1 (h\u22121) i \u2212\u2206\u03b1i)\u2212\n\u03bbn\n2 \u2016w(h\u22121) +\n1\n\u03bbn \u2206\u03b1ixi\u2016\n2\n\u2265 \u2212\u2113\u2217i (\u2212\u03b1 (h\u22121) i \u2212 s(u (h) i \u2212 \u03b1 (h\u22121) i ))\u2212\n\u03bbn\n2 \u2016w(h\u22121) +\n1\n\u03bbn s(u\n(h) i \u2212 \u03b1 (h\u22121) i )xi\u2016 2.\nFrom strong convexity we have\n\u2113\u2217i\n(\n\u2212\u03b1 (h\u22121) i \u2212 s(u (h) i \u2212 \u03b1 (h\u22121) i )\n)\n\u2264 s\u2113\u2217i (\u2212u (h) i ) + (1\u2212 s)\u2113 \u2217 i (\u2212\u03b1 (h\u22121) i )\u2212\n\u03b3 2 s(1\u2212 s)(u (h) i \u2212 \u03b1 (h\u22121) i ) 2. (12)\nHence\nA (12) \u2265 \u2212s\u2113\u2217i (\u2212u (h) i )\u2212 (1 \u2212 s)\u2113 \u2217 i (\u2212\u03b1 (h\u22121) i ) + \u03b3\n2 s(1\u2212 s)(u\n(h) i \u2212 \u03b1 (h\u22121) i ) 2\n\u2212 \u03bbn\n2 \u2016w(h\u22121) +\n1\n\u03bbn s(u\n(h) i \u2212 \u03b1 (h\u22121) i )xi\u2016 2\n= \u2212s(\u2113\u2217i (\u2212u (h) i ) + u (h) i x T i w (h\u22121)) \ufe38 \ufe37\ufe37 \ufe38\ns\u2113(xT i w (h\u22121))\n+(\u2212\u2113\u2217i (\u2212\u03b1 (h\u22121) i )\u2212\n\u03bbn\n2 \u2016w(h\u22121)\u20162)\n\ufe38 \ufe37\ufe37 \ufe38\nB\n+ s\n2\n(\n\u03b3(1\u2212 s)\u2212 1\n\u03bbn s\u2016xi\u2016\n2\n)\n(u (h) i \u2212 \u03b1 (h\u22121) i ) 2 + s(\u2113\u2217i (\u2212\u03b1 (h\u22121) i ) + \u03b1 (h\u22121) i x T i w (h\u22121).\nTherefore\nA\u2212B \u2265 s [\n\u2113(xTi w (h\u22121)) + \u2113\u2217i (\u2212\u03b1 (h\u22121) i ) + \u03b1 (h\u22121) i x T i w (h\u22121)\n+ 1\n2\n(\n\u03b3(1\u2212 s)\u2212 1\n\u03bbn s\u2016xi\u2016\n2\n)\n(u (h) i \u2212 \u03b1 (h\u22121) i )\n2 ]\n. (13)\nRecall that our above definition of the local pair of primal and dual problems gives\nPk(wk;w)\u2212Dk(\u03b1[k];w) = 1\nn\n\u2211\ni\u2208Ik\n( \u2113i((w +wk) T xi) + \u2113 \u2217 i (\u2212\u03b1i) + \u03b1i(w +wk) T xi ) .\nwhere wk := A[k]\u03b1[k].\nIf we take the expectation of (13) we obtain\n1 s E[A\u2212B] \u2265 n\nnk\n1\nn\n\u2211\ni\u2208Ik\n[\n\u2113(xTi w (h\u22121)) + \u2113\u2217i (\u2212\u03b1 (h\u22121) i ) + \u03b1 (h\u22121) i x T i w\n(h\u22121) ]\n\ufe38 \ufe37\ufe37 \ufe38\nPk(w (h\u22121) k ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w)\n\u2212 s\n2\u03bbn\n1\nnk\n\u2211\ni\u2208Ik\n(\n\u2016xi\u2016 2 \u2212\n\u03bbn\u03b3(1\u2212 s)\ns\n)\n(u (h) i \u2212 \u03b1 (h\u22121) i ) 2\n\ufe38 \ufe37\ufe37 \ufe38\nG(h)\n.\nTherefore, we have obtained the claimed improvement bound\nn s E[Dk(\u03b1 (h) [k] ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w)] \u2265 n nk (Pk(w (h\u22121) k ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w))\u2212 s 2\u03bbn G(h).\nProposition\u2019 1. Assume the loss functions \u2113i are (1/\u03b3)-smooth. Then for using LOCALSDCA, Assumption 1 holds with\n\u0398 =\n(\n1\u2212 \u03bbn\u03b3\n1 + \u03bbn\u03b3\n1\nn\u0303\n)H\n. (14)\nwhere n\u0303 := maxk nk is the size of the largest block of coordinates.\nLemma 6 (Local Convergence on the Subproblem). For any \u03b1(0)[k] \u2208 R nk and w \u2208 Rd let us define\n\u03b1 (\u2217) [k] := argmax\n\u03b1[k]\u2208R n k\nDk(\u03b1[k];w). (15)\nIf LOCALSDCA is used for H iterations on block k, then\nE [ Dk(\u03b1 (\u2217) [k] ;w)\u2212Dk(\u03b1 (H) [k] ;w) ] \u2264\n(\n1\u2212 s\nnk\n)H (\nDk(\u03b1 (\u2217) [k] ;w)\u2212Dk(\u03b1 (0) [k] ;w)\n)\n. (16)\nProof. We will use Lemma 5 with s := \u03bbn\u03b31+\u03bbn\u03b3 \u2208 [0, 1]. Because \u2016xi\u2016 \u2264 1, we have G (h) \u2264 0. Therefore\nE [ Dk(\u03b1 (h) [k] ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w) ] \u2265 s\nnk\n(\nPk(w (h\u22121) k ;w)\u2212Dk(\u03b1 (h\u22121) [k] ;w)\n)\n. (17)\nFollowing the proof of Theorem 5 in [SSZ13] we obtain the claimed bound.\nNow, to prove Proposition 1 it is enough to observe that for fixed k and any \u03b1 = (\u03b1[1], . . . ,\u03b1[K]) \u2208 R n, assuming we define w = \u2211\nk\u2032 6=k A[k\u2032]\u03b1[k\u2032], it holds that\n\u03b5D,k(\u03b1) (3) = max\n\u03b1\u0302[k]\u2208R n k\nD((\u03b1[1], . . . , \u03b1\u0302[k], . . . ,\u03b1[K]))\u2212D((\u03b1[1], . . . ,\u03b1[k], . . . ,\u03b1[K]))\n= D((\u03b1[1], . . . ,\u03b1 (\u2217) [k] , . . . ,\u03b1[K]))\u2212D((\u03b1[1], . . . ,\u03b1[k], . . . ,\u03b1[K]))\n= Dk(\u03b1 (\u2217) [k] ;w)\u2212Dk(\u03b1[k];w),\nwhere \u03b1(\u2217)[k] is defined by (15)."}, {"heading": "C Proof of Lemma 3", "text": "Lemma\u2019 3. If K = 1 then \u03c3min = 0. For any K \u2265 1, when assuming \u2016xi\u2016 \u2264 1 \u2200i, we have\n0 \u2264 \u03c3min \u2264 n\u0303.\nMoreover, if datapoints between different workers are orthogonal, i.e. (ATA)i,j = 0 \u2200i, j such that i and j do not belong to the same part, then \u03c3min = 0.\nProof. If K = 1 then \u03b1[K] \u2261 \u03b1 and hence \u03c3min = 0. For a non-trivial case, when K > 1 we have\n\u03c3min (7) = max\n\u03b1\u2208Rn \u03bb2n2\n\u2211K k=1\u2016A[k]\u03b1[k]\u2016 2 \u2212 \u2016A\u03b1\u20162\n\u2016\u03b1\u20162\n= max \u03b1\u2208Rn\n\u2211K k=1\u2016X[k]\u03b1[k]\u2016 2 \u2212 \u2016X\u03b1\u20162\n\u2016\u03b1\u20162\n\u2264 max \u03b1\u2208Rn\n\u2211K k=1\u2016X[k]\u03b1[k]\u2016 2\n\u2211K k=1 \u2016\u03b1[k]\u2016\n2 \u2264 max \u03b1\u2208Rn\n\u2211K k=1n\u0303\u2016\u03b1[k]\u2016 2\n\u2211K k=1 \u2016\u03b1[k]\u2016\n2 \u2264 n\u0303,\nwhere we have used the definition of the rescaled data matrix A = 1 \u03bbn X . The last inequality follows from the fact that since the datapoints have bounded norm \u2016xi\u2016 \u2264 1, so that for all parts k \u2208 {1, . . . ,K} we have\nmax\u03b1[k]\u2208Rnk \u2016X[k]\u03b1[k]\u2016\n2\n\u2016\u03b1[k]\u20162 = \u2016X[k]\u2016\n2 op \u2264 \u2016X[k]\u2016 2 frob \u2264 nk \u2264 n\u0303, where n\u0303 := maxk nk.\nThe case when ATA is block-diagonal (with respect to the partition) is trivial."}], "references": [{"title": "Distributed Delayed Stochastic Optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "In NIPS,", "citeRegEx": "Agarwal and Duchi.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi.", "year": 2011}, {"title": "Distributed Learning, Communication Complexity and Privacy", "author": ["Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour"], "venue": null, "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Parallel Coordinate Descent for L1-Regularized Loss Minimization", "author": ["Joseph K Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin"], "venue": "In ICML,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Large-Scale Machine Learning with Stochastic Gradient Descent", "author": ["L\u00e9on Bottou"], "venue": "editors, COMPSTAT\u20192010 - Proceedings of the 19th International Conference on Computational Statistics,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Optimal Distributed Online Prediction", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "Using Mini-Batches. JMLR,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Estimation, Optimization, and Parallelism when Data is Sparse", "author": ["John C Duchi", "Michael I Jordan", "H Brendan Mcmahan"], "venue": "In NIPS,", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Fast Distributed Coordinate Descent for NonStrongly Convex Losses", "author": ["Olivier Fercoq", "Zheng Qu", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "Fercoq et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2014}, {"title": "A Dual Coordinate Descent Method for Large-scale Linear SVM", "author": ["Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin", "S Sathiya Keerthi", "S Sundararajan"], "venue": "In the 25th International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models", "author": ["Gideon Mann", "Ryan McDonald", "Mehryar Mohri", "Nathan Silberman", "Daniel D Walker"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Distributed Block Coordinate Descent for Minimizing Partially Separable Functions", "author": ["Jakub Marecek", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Marecek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marecek et al\\.", "year": 2014}, {"title": "Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints: Application to distributed MPC", "author": ["Ion Necoara", "Dragos Clipici"], "venue": "Journal of Process Control,", "citeRegEx": "Necoara and Clipici.,? \\Q2013\\E", "shortCiteRegEx": "Necoara and Clipici.", "year": 2013}, {"title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright"], "venue": "In NIPS,", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "A Stochastic Approximation Method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Parallel Coordinate Descent Methods for Big Data Optimization", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2012}, {"title": "Distributed Coordinate Descent Method for Learning with Big Data", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "maths.ed.ac.uk,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2013}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2014\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2014}, {"title": "Pegasos: Primal Estimated Sub-Gradient Solver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": null, "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2013}, {"title": "Communication-Efficient Distributed Optimization using an Approximate Newton-type Method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": null, "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J Tsitsiklis", "D Bertsekas", "M Athans"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1986}, {"title": "Mini-Batch Primal and Dual Methods for SVMs", "author": ["Martin Tak\u00e1\u010d", "Avleen Bijral", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2013}, {"title": "Primal-Dual Parallel Coordinate Descent for Machine Learning Optimization", "author": ["Martin Tak\u00e1\u010d", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": null, "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2014}, {"title": "Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent", "author": ["Tianbao Yang"], "venue": "In NIPS,", "citeRegEx": "Yang.,? \\Q2013\\E", "shortCiteRegEx": "Yang.", "year": 2013}, {"title": "Large linear classification when data cannot fit in memory", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin"], "venue": "In the 16th ACM SIGKDD international conference,", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "Large Linear Classification When Data Cannot Fit in Memory", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "On Theoretical Analysis of Distributed Stochastic Dual Coordinate Ascent", "author": ["Tianbao Yang", "Shenghuo Zhu", "Rong Jin", "Yuanqing Lin"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Tathagata Das", "Ankur Dave", "Murphy McCauley", "Michael J Franklin", "Scott Shenker", "Ion Stoica"], "venue": null, "citeRegEx": "Zaharia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2012}, {"title": "Communication-Efficient Algorithms for Statistical Optimization", "author": ["Yuchen Zhang", "John C Duchi", "Martin J Wainwright"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Parallelized Stochastic Gradient Descent", "author": ["Martin A Zinkevich", "Markus Weimer", "Alex J Smola", "Lihong Li"], "venue": "NIPS 2010: Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [], "year": 2017, "abstractText": "Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, COCOA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to stateof-the-art mini-batch versions of SGD and SDCA algorithms, COCOA converges to the same .001-accurate solution quality on average 25\u00d7 as quickly.", "creator": "LaTeX with hyperref package"}}}