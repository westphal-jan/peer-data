{"id": "1409.4504", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2014", "title": "Voting for Deceptive Opinion Spam Detection", "abstract": "enigmatic Consumers ' purchase manmatha decisions 33-day are increasingly influenced by belu user - articulately generated urbanism online syrianska reviews. dharris Accordingly, sparring there aim-120 has been zwickel growing concern jawbones about nzs the potential for kathrin posting deceptive balmier opinion delphi spam braben fictitious reviews that accidentally have 1205 been creel deliberately kunce written 99.14 to sound camo authentic, to ragini deceive the wisdoms readers. Existing approaches brabner mainly focus usda on iigep developing automatic supervised learning based methods reductions to tropoje help users identify deceptive zamano opinion well-received spams.", "histories": [["v1", "Tue, 16 Sep 2014 05:12:50 GMT  (15kb)", "http://arxiv.org/abs/1409.4504v1", "arXiv admin note: text overlap witharXiv:1204.2804by other authors"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1204.2804by other authors", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["tao wang", "hua zhu"], "accepted": false, "id": "1409.4504"}, "pdf": {"name": "1409.4504.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["pwangtao@gmail.com,", "huaz@whu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n45 04\nv1 [\ncs .C\nL ]\n1 6\nSe p\n20 14"}, {"heading": "1 Introduction", "text": "Consumers increasingly rely on user-generated online reviews when making purchase decision (Cone, 2011; Ipsos, 2012). Unfortunately, the ease of posting content to the Web, potentially anonymously, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader (Ott et al., 2011). For example, a hotel manager may post fake positive reviews to promote their own hotel, or fake negative reviews to demote a competitor\u2018s hotel. Accordingly, there appears to be widespread and growing concern among both businesses and the public (Meyer, 2009; Miller, 2009; Streitfeld, 2012; Topping, 2010). The followings are two reviews, one is deceptive and the other one is truthful.\n1. My husband and I arrived for a 3 night stay. for our 10th wedding anniversary. We had booked an Executive Guest room, upon ar- rival we were informed that they would be upgrading us to a beautiful Junior Suite. This was just a wonderful unexpected plus to our beautifully planned weekend. The front desk manager was professional and made us feel warmly welcomed. The Chicago Affinia was just a gorgeous hotel, friendly staff, lovely food and great atmosphere. Not the mention the feather pillows and bedding that was just fantastic. Also we were allowed to bring out beloved Shi-Tzu and he experienced the Jet Set Pets stay. The grooming was perfect, the daycare service we felt completely comfortable with. This was a beautiful weekend, thank you Affinia Hotels! We would visit this hotel again!\n2. As others have said, once all the construction works are completed I suspect that the prices this hotel can (legitimately) charge will put it out of our price range. Which will be a pity because it is an excellent hotel, the location couldn\u2019t be better. The room was very spacious, with separate sitting study areas a nice bathroom. Only 3 minor points: downstairs they were serving a free basic breakfast (coffee and pastries), but we only knew of it on our last morning nobody had mentioned this; the cost of internet is a bit dear especially when lots of motels now offer it free; there was only powered milk in the rooms, which wasn\u2019t that nice. But none of these really spolied a really enjoyable stay..\nAs can be seen, as the distinction is too subtle, it is hard to manually tell truthful reviews from deceptive one (the first one is deceptive). Existing approaches for spam detection usually focus on developing supervised learning-based algo-\nrithms to help users identify deceptive opinion spam (Jindal and Liu, 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2010; Ott et al., 2011; Wang et al., 2011; Wu et al., 2010). These supervised approaches suffer one main disadvantage: they are highly dependent upon high-quality gold-standard labelled data. One option for producing gold-standard labels, for example, would be to rely on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). Recent studies, however, shows that unlike other kinds of spam, such as Web (Castillo et al., 2006; Martinez-Romo and Araujo, 2009) and e-mail spam (Chirita et al., 2005), deceptive opinion spam is neither easily ignored nor easily identified by human readers (Ott et al., 2011). This is especially the case when considering the overtrusting nature of most human judges, a phenomenon referred to in the psychological deception literature as a truth bias (Vrij, 2008).\nDue to the difficulty in manually labeling deceptive opinion, Ott et al. (2011) (Ott et al., 2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers1. According to their findings, truthful hotel opinions are more specific about spatial configurations(e.g. small, bathroom, location), which can be easily explained by the fact that the Turkers have never been to that hotels.\nOur work in this paper started from the dataset, 400 truthful and 400 deceptive reviews, from (Ott et al., 2011; Li et al., 2013a), a large-scale, publicly available dataset for deceptive opinion spam research. Then we use three ways to preprocessing dataset. (a) Use N-gram language model to build a probabilistic language model (b) Use POS-tag to delve the linguistic reason why a review would likely to be considered fake (c) Use TF-IDF to reflect how important a word is in given text. After pre-processing, the data has high dimensional features which is quite computational expensive and easily raises the over fitting problem. Thus we are motivated to reduce dimension using Latent Semantic Indexing (LSI), one of most popular indexing and retrieval method that uses singular value decomposition (SVD) to map the documents into vector in the latent concept-space.\n1Truthful opinions are selected from 5-star reviews from TripAdvisor. http://www.tripadvisor.com/\nLSI extracts useful information in latent concepts through learning a low-dimensional representation of this text, which helps us to know more detailed difference in fake and real reviews. We also use supervised latent semantic analysis (Sprinkle) to overcome the shortcoming of LSI and results show performance improves a lot. To classify data, we use Support Vector Machine (SVM) and Naive Bayes, two popular and mature classifier, on our dataset after pre-processing and dimension reduction. Although these classifiers are quite powerful, it may still misclassify correct points. So we use a voting scheme or a weighted combination of the multiple hypothesis to achieve final conclusion. The remainder of this paper is organized as follows: Section 2 talks about related work. Section 3 presents details of our fake reviews detection framework, including three approaches to preprocessing data, two ways of dimension reduction, and two classification method. In section 4 shows the experimental results and discussion of performance; Finally, Section 5 concludes our work."}, {"heading": "2 Related Work", "text": "Jindal and Liu (2008) first studied the deceptive opinion problem and trained models using features based on the review text, reviewer, and product to identify duplicate opinions, i.e., opinions that appear more than once in the corpus with similar contexts. Wu et al. (2010) propose an alternative strategy to detect deceptive opinion spam in the absence of a gold standard. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the linguistic differences between them. Ott et al. created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013; Li et al., 2014). For example, Song et al. (2012) looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features.\nIn addition to exploring text or linguistic features in deception, some existing work looks into customers\u2019 behavior to identify deception (Mukherjee et al., 2013). For example, Mukherjee\net al. (2011; 2012) delved into group behavior to identify group of reviewers who work collaboratively to write fake reviews."}, {"heading": "3 Data Processing", "text": "Before building n-gram model, we tokenized the text of each review and used porter stemming algorithms to eliminate the influence of different forms of one word. An n-gram model is a type of 2 probabilistic language model for predicting the next item in such a sequence in the form of n-1 order Markov model. We built unigram and bigram model in our experiment.\ntf-idf TF-IDF, short for term frequencyinverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to control for the fact that some words are generally more common than others. For example, stop words \u2013 common words that appears every where but do not have much meaning, such as the, a, and that, are of little significance. So tf-idf will assign small weight to these words.\nThe formulae of tf-idf is:\nw = tf \u00b7 idfi,j = tf \u00b7 log N\ndfi,j\nPOS Part-of-speech tagging, also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text as corresponding to a particular part of speech. Our project use POS-tag to delve the linguistic reason why a review would likely to be considered fake. We use the Stanford Parser to obtain the relative POS frequencies as feature vector to separate data."}, {"heading": "4 Model", "text": "The power of dimension reduction lies in the fact that it can ease later processing, improve computational performance improvement, filter useless noise and recover underlying causes. In our project, we are using LSI for text classification. LSI is based upon the assumption that there is an underlying semantic structure in textual data, and that the relationship between terms and documents can be re-described in this semantic structure form. Textual documents are represented as\nvectors in a vector space. It is fundamentally based on SVD that breaks original relationship of the data into linearly independent components, where the original term vectors are represented by left singular vectors and document vectors by right singular vectors. That is\nUd\u00d7lsl\u00d7lVl\u00d7d (1)\nThe column of Ud\u00d7l defines the lower dimensional coordinate system. Sl is a diagonal matrix with the l largest singular values in non-increasing order along its diagonal. The l columns are the new coordinates of each document after dimensionality reduction.\nOne major key of LSI is that it overcomes two of the most problematic constraints synonymy and polysemy. It is very tolerant of noise (i.e, misspelled words, typographical errors, etc.) and has been proven to capture key relationship information, including causal, goal-oriented, and taxonomic information.Another usage of LSI is in data visualization. Let l=2 or 3. Each xi is approximated by a l-dimensional vector now suitable for plotting in 2D or 3D spaces.\nLSI has limitations in classification because it is an unsupervised method which doesn\u2019t take class information into account. So we decide to use the process Sprinkling in which LSI is performed on a term-document matrix augmented with sprinkled terms, namely class labels."}, {"heading": "4.1 Classification", "text": ""}, {"heading": "4.2 Naive Bayes", "text": "Naive Bayes classifiers are among the most successful known algorithms for learning to classify text documents. As we know, text classifiers often don\u2019t use kind of deep representation about language: often a document is represented as a bag of words. This is a very simple representation of document : it only knows which words are included in the document and throws away the word order. NB classifier just relies on this representation. For a document D and a class C. In our project, we only have two classes: fake or real. We classify d as the class which has the highest posterior probability P(C\u2014D), which can be re-expressed using Bayes Theorem:\np(c|d) = p(d|c)p(c)\np(d) (2)\nCMAP = argmaxcP (c|d)\n= argmaxc P (d|c)P (c)\np(d)\n= argmaxcP (d|c)P (c)\n(3)"}, {"heading": "4.3 Support Vector Machine", "text": "Support Vector Machines(SVM) (Joachims, 1999) are supervised learning algorithms that used to recognize patterns and classify data. Given a set of binary training data, an SVM training algorithm builds a model to calculate a hyper-plane that separate data into one category or the other. The goal of SVM is to find a hyper-plane that clearly classify data and maximize the distance of the support vectors to hyper-plane at the same time .\n1 2 ||w||2 \u2212\n\u2211\ni\n\u03b1i[yi(w \u00b7 xi \u2212 b)\u2212 1] (4)\nWe choose SVM as our classifier since in the area of text classification, SVM performs more robust than other techniques. When implementing SVM, we first pre-process each original document to a vector of features. Such text feature vectors often have high dimension, however, SVM supports quite well. SVMs use overfitting protection, so the performance does not affect much when the number of features increases. Joachims shows most text categorization problems are linearly separable, so we use linear SVMs, which is also faster to learn, to solve our problem. In our project, we use SVMlight to train our linear SVM models on datasets that processed by UNIGRAMS, BIGRAMS, POS-Tag, and TF-IDF. We also train dataset after dimension reduction. Results show that SVMs perform well on classifying our data."}, {"heading": "4.4 Voting", "text": "Voting Scheme, also called weighted combination of the multiple hypothesis, is used to improve performance when having different approaches to a certain problem. Although some classifier is quite powerful, it may still misclassify correct points. On the other hand, we desire simple models that free users from troublesome algorithm design. The uncertainty of dimension selection and goodness evaluation makes classification difficult. This method is more robust to difficult classified text and achieves better performance comparing to separate models."}, {"heading": "5 Experimental Results", "text": "To better understand the models learned by these automated approaches, we decide to demonstrate what SVD in LSI analysis is capturing. We find the group of words that have the most significant influence in latent concepts. we can see, the second concept contributes most for separating the fake reviews. We list in Table with the highest scores that loads on the positive polar (truthful) and negative polar (fake) of second concept.\nWe are using Sprinkled LSI and LSI to reduce the term-document matrix in different dimensions from 50 to 700. We use SVM to classify the data. The results show that with the dimension increasing, the training accuracy of two methods is also increasing because we have higher features which give us more information. Besides, the testing accuracy and precision is increasing when dimension is from 0 to 500. When dimension continues to increase, the training accuracy increases while testing accuracy goes down. It implies to us that there is an overfitting problem when the dimension of the training data is high. Results in bold responds to best accuracy. We can see that when we use Sprinkle + SVM and the dimension is around 500, the accuracy is 90% which is quite good.\nIn this paper, we combine 5 different algorithms above to automatically obtain the final classification by voting for the test results. If over 3 methods vote for the same class, we will determine the document to belong to that class. The five algorithms we choose as follows: Sprinkle+SVM (dimension 500), Sprinkle+SVM(dimension 300), Unigram+SVM, TF.IDF+SVM, Unigram+NB. The voting algorithm gets 0.95 accuracy."}, {"heading": "6 Conclusion", "text": "In this work we use different methods to analyse and improve fake reviews classification result based on Myle Ott and Yejin Choi work. It shows that the detection of fake reviews by using automatically approaches is well beyond using human judges. We first build n-gram models and POS model combined with SVM and Naive Bayes to detect the deceptive opinion. Similar to Myle Ott and Yejin Chois result, n-gram based test categorization have the best performance suggesting that key-word based approaches might be necessary in detecting fake review. Besides, we used LSI to exert dimension reduction and some theoretical analysis. Contrast to Myle Ott and Yejin Choi, our result shows that 2nd person pronouns, rather than 1st person form are relatively tend to be used in deceptive review. The fake reviews also have the character of lacking of the concrete noun and adjectives. Furthermore, we compare the experimental results of Sprinkled LSI and LSI . We find Sprinkled LSI+ SVM outperforms LSI + SVM when dimension is not high. Finally, we proposed a voting scheme which combines a few algorithms we used above to achieve better classification performance."}], "references": [{"title": "A reference collection for web spam", "author": ["Debora Donato", "Luca Becchetti", "Paolo Boldi", "Stefano Leonardi", "Massimo Santini", "Sebastiano Vigna"], "venue": "In ACM Sigir Forum,", "citeRegEx": "Castillo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Castillo et al\\.", "year": 2006}, {"title": "Mailrank: using ranking for spam detection", "author": ["J\u00f6rg Diederich", "Wolfgang Nejdl"], "venue": "In Proceedings of the 14th ACM international conference on Information and knowledge management,", "citeRegEx": "Chirita et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chirita et al\\.", "year": 2005}, {"title": "Detecting deceptive opinions with profile compatibility", "author": ["Feng", "Hirst2013] Vanessa Wei Feng", "Graeme Hirst"], "venue": "In Proceedings of the 6th International Joint Conference on Natural Language Processing,", "citeRegEx": "Feng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Syntactic stylometry for deception detection", "author": ["Feng et al.2012] Song Feng", "Ritwik Banerjee", "Yejin Choi"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Feng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2012}, {"title": "Opinion spam and analysis", "author": ["Jindal", "Liu2008] Nitin Jindal", "Bing Liu"], "venue": "In Proceedings of the international conference on Web search and web data mining,", "citeRegEx": "Jindal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jindal et al\\.", "year": 2008}, {"title": "Finding unusual review patterns using unexpected rules", "author": ["Jindal et al.2010] Nitin Jindal", "Bing Liu", "Ee-Peng Lim"], "venue": "In Proceedings of the 19th ACM international conference on Information and knowledge management,", "citeRegEx": "Jindal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jindal et al\\.", "year": 2010}, {"title": "Making large scale svm learning practical", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Learning to identify review spam", "author": ["Li et al.2011] Fangtao Li", "Minlie Huang", "Yi Yang", "Xiaoyan Zhu"], "venue": "In Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume Three,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Topicspam: a topic-model based approach for spam detection", "author": ["Li et al.2013a] Jiwei Li", "Claire Cardie", "Sujian Li"], "venue": "ACL", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Identifying manipulated offerings on review portals", "author": ["Li et al.2013b] Jiwei Li", "Myle Ott", "Claire Cardie"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Towards a general rule for identifying deceptive opinion spam", "author": ["Li et al.2014] Jiwei Li", "Myle Ott", "Claire Cardie", "Eduard Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Detecting product review spammers using rating behaviors", "author": ["Lim et al.2010] Ee-Peng Lim", "Viet-An Nguyen", "Nitin Jindal", "Bing Liu", "Hady Wirawan Lauw"], "venue": "In Proceedings of the 19th ACM international conference on Information and knowledge", "citeRegEx": "Lim et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2010}, {"title": "Web spam identification through language model analysis", "author": ["Martinez-Romo", "Araujo2009] Juan MartinezRomo", "Lourdes Araujo"], "venue": null, "citeRegEx": "Martinez.Romo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martinez.Romo et al\\.", "year": 2009}, {"title": "Fake reviews prompt belkin apology", "author": ["David Meyer"], "venue": null, "citeRegEx": "Meyer.,? \\Q2009\\E", "shortCiteRegEx": "Meyer.", "year": 2009}, {"title": "Company settles case of reviews it faked", "author": ["C Miller"], "venue": null, "citeRegEx": "Miller.,? \\Q2009\\E", "shortCiteRegEx": "Miller.", "year": 2009}, {"title": "Detecting group review spam", "author": ["Bing Liu", "Junhui Wang", "Natalie Glance", "Nitin Jindal"], "venue": "In Proceedings of the 20th international conference companion on World wide web,", "citeRegEx": "Mukherjee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2011}, {"title": "Spotting fake reviewer groups in consumer reviews", "author": ["Bing Liu", "Natalie Glance"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "Mukherjee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2012}, {"title": "Spotting opinion spammers using behavioral footprints", "author": ["Abhinav Kumar", "Bing Liu", "Junhui Wang", "Meichun Hsu", "Malu Castellanos", "Riddhiman Ghosh"], "venue": "In Proceedings of the 19th ACM SIGKDD interna-", "citeRegEx": "Mukherjee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2013}, {"title": "Finding deceptive opinion spam by any stretch of the imagination", "author": ["Ott et al.2011] Myle Ott", "Yejin Choi", "Claire Cardie", "Jeffrey T. Hancock"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Ott et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ott et al\\.", "year": 2011}, {"title": "Estimating the prevalence of deception in online review communities", "author": ["Ott et al.2012] Myle Ott", "Claire Cardie", "Jeff Hancock"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "Ott et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ott et al\\.", "year": 2012}, {"title": "Negative deceptive opinion spam", "author": ["Ott et al.2013] Myle Ott", "Claire Cardie", "Jeffrey T. Hancock"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Ott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ott et al\\.", "year": 2013}, {"title": "Historian orlando figes agrees to pay damages for fake reviews", "author": ["A Topping"], "venue": "The Guardian.,", "citeRegEx": "Topping.,? \\Q2010\\E", "shortCiteRegEx": "Topping.", "year": 2010}, {"title": "Review graph based online store review spammer detection", "author": ["Wang et al.2011] Guan Wang", "Sihong Xie", "Bing Liu", "Philip S Yu"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Distortion as a validation criterion in the identification of suspicious reviews", "author": ["Wu et al.2010] Guangyu Wu", "Derek Greene", "Barry Smyth", "P\u00e1draig Cunningham"], "venue": "In Proceedings of the First Workshop on Social Media Analytics,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Comparison of deceptive and truthful travel reviews. In Information and communication technologies in tourism", "author": ["Yoo", "Gretzel2009] Kyung-Hyan Yoo", "Ulrike Gretzel"], "venue": null, "citeRegEx": "Yoo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yoo et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": "Unfortunately, the ease of posting content to the Web, potentially anonymously, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader (Ott et al., 2011).", "startOffset": 289, "endOffset": 307}, {"referenceID": 13, "context": "Accordingly, there appears to be widespread and growing concern among both businesses and the public (Meyer, 2009; Miller, 2009; Streitfeld, 2012; Topping, 2010).", "startOffset": 101, "endOffset": 161}, {"referenceID": 14, "context": "Accordingly, there appears to be widespread and growing concern among both businesses and the public (Meyer, 2009; Miller, 2009; Streitfeld, 2012; Topping, 2010).", "startOffset": 101, "endOffset": 161}, {"referenceID": 21, "context": "Accordingly, there appears to be widespread and growing concern among both businesses and the public (Meyer, 2009; Miller, 2009; Streitfeld, 2012; Topping, 2010).", "startOffset": 101, "endOffset": 161}, {"referenceID": 5, "context": "rithms to help users identify deceptive opinion spam (Jindal and Liu, 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2010; Ott et al., 2011; Wang et al., 2011; Wu et al., 2010).", "startOffset": 53, "endOffset": 185}, {"referenceID": 7, "context": "rithms to help users identify deceptive opinion spam (Jindal and Liu, 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2010; Ott et al., 2011; Wang et al., 2011; Wu et al., 2010).", "startOffset": 53, "endOffset": 185}, {"referenceID": 11, "context": "rithms to help users identify deceptive opinion spam (Jindal and Liu, 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2010; Ott et al., 2011; Wang et al., 2011; Wu et al., 2010).", "startOffset": 53, "endOffset": 185}, {"referenceID": 18, "context": "rithms to help users identify deceptive opinion spam (Jindal and Liu, 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2010; Ott et al., 2011; Wang et al., 2011; Wu et al., 2010).", "startOffset": 53, "endOffset": 185}, {"referenceID": 22, "context": "rithms to help users identify deceptive opinion spam (Jindal and Liu, 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2010; Ott et al., 2011; Wang et al., 2011; Wu et al., 2010).", "startOffset": 53, "endOffset": 185}, {"referenceID": 23, "context": "rithms to help users identify deceptive opinion spam (Jindal and Liu, 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2010; Ott et al., 2011; Wang et al., 2011; Wu et al., 2010).", "startOffset": 53, "endOffset": 185}, {"referenceID": 5, "context": "One option for producing gold-standard labels, for example, would be to rely on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012).", "startOffset": 115, "endOffset": 160}, {"referenceID": 16, "context": "One option for producing gold-standard labels, for example, would be to rely on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012).", "startOffset": 115, "endOffset": 160}, {"referenceID": 0, "context": "Recent studies, however, shows that unlike other kinds of spam, such as Web (Castillo et al., 2006; Martinez-Romo and Araujo, 2009) and e-mail spam (Chirita et al.", "startOffset": 76, "endOffset": 131}, {"referenceID": 1, "context": ", 2006; Martinez-Romo and Araujo, 2009) and e-mail spam (Chirita et al., 2005), deceptive opinion spam is neither easily ignored nor easily identified by human readers (Ott et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 18, "context": ", 2005), deceptive opinion spam is neither easily ignored nor easily identified by human readers (Ott et al., 2011).", "startOffset": 97, "endOffset": 115}, {"referenceID": 18, "context": "(2011) (Ott et al., 2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers1.", "startOffset": 7, "endOffset": 25}, {"referenceID": 18, "context": "Due to the difficulty in manually labeling deceptive opinion, Ott et al. (2011) (Ott et al.", "startOffset": 62, "endOffset": 80}, {"referenceID": 18, "context": "Our work in this paper started from the dataset, 400 truthful and 400 deceptive reviews, from (Ott et al., 2011; Li et al., 2013a), a large-scale, publicly available dataset for deceptive opinion spam research.", "startOffset": 94, "endOffset": 130}, {"referenceID": 19, "context": "created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013; Li et al., 2014).", "startOffset": 126, "endOffset": 219}, {"referenceID": 20, "context": "created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013; Li et al., 2014).", "startOffset": 126, "endOffset": 219}, {"referenceID": 10, "context": "created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013; Li et al., 2014).", "startOffset": 126, "endOffset": 219}, {"referenceID": 16, "context": "Wu et al. (2010) propose an alternative strategy to detect deceptive opinion spam in the absence of a gold standard.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "Wu et al. (2010) propose an alternative strategy to detect deceptive opinion spam in the absence of a gold standard. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the linguistic differences between them.", "startOffset": 0, "endOffset": 140}, {"referenceID": 7, "context": ", 2013; Li et al., 2013b; Feng and Hirst, 2013; Li et al., 2014). For example, Song et al. (2012) looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance.", "startOffset": 8, "endOffset": 98}, {"referenceID": 7, "context": ", 2013; Li et al., 2013b; Feng and Hirst, 2013; Li et al., 2014). For example, Song et al. (2012) looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features.", "startOffset": 8, "endOffset": 244}, {"referenceID": 17, "context": "In addition to exploring text or linguistic features in deception, some existing work looks into customers\u2019 behavior to identify deception (Mukherjee et al., 2013).", "startOffset": 139, "endOffset": 163}, {"referenceID": 6, "context": "Support Vector Machines(SVM) (Joachims, 1999) are supervised learning algorithms that used to recognize patterns and classify data.", "startOffset": 29, "endOffset": 45}], "year": 2014, "abstractText": "Consumers\u2019 purchase decisions are increasingly influenced by user-generated online reviews. Accordingly, there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic, to deceive the readers. Existing approaches mainly focus on developing automatic supervised learning based methods to help users identify deceptive opinion spams. this work, we used the LSI and Sprinkled LSI technique to reduce the dimension for deception detection. We make our contribution to demonstrate what LSI is capturing in latent semantic space and reveal how deceptive opinions can be recognized automatically from truthful opinions. Finally, we proposed a voting scheme which integrates different approaches to further improve the classification performance.", "creator": "LaTeX with hyperref package"}}}