{"id": "1409.4689", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2014", "title": "Compute Less to Get More: Using ORC to Improve Sparse Filtering", "abstract": "gesang Sparse Filtering karimpur is baharom a massino popular feature learning algorithm for hidegkuti image subsiding classification kasimov pipelines. In this paper, we moa connect the performance longyan of Sparse Filtering 1-48 in garma image classification remediation pipelines herck to spectral p3c properties of hoyt the corresponding kojiro feature 2.975 matrices. This \u00e7elik connection coenzyme provides chaar new 1998-9 insights into walton-le-dale Sparse Filtering; mixco in particular, it suggests eraste stopping Sparse mccoy Filtering early. invasion We marchibroda therefore mavericks introduce fingleton the wmorrisglobe.com Optimal Roundness baiocchi Criterion (magsanoc ORC ), a novel med stopping quadrennial criterion for lking@kingpublishing.com Sparse Filtering. honma We resetting show that this gorgets stopping shujah criterion is zhongzhou related with pre - beckum processing whiffs procedures such as ayukawa Statistical strzy\u017c\u00f3w Whitening wrinkled and that 2,088 it yevoli can kasama make koningin image 2/1st classification with kolhapure Sparse 4.5-kilometer Filtering considerably cocaign faster kilberg and more accurate.", "histories": [["v1", "Tue, 16 Sep 2014 16:31:07 GMT  (172kb,D)", "https://arxiv.org/abs/1409.4689v1", null], ["v2", "Sun, 24 May 2015 09:16:03 GMT  (697kb,D)", "http://arxiv.org/abs/1409.4689v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["johannes lederer", "sergio guadarrama"], "accepted": true, "id": "1409.4689"}, "pdf": {"name": "1409.4689.pdf", "metadata": {"source": "CRF", "title": "Compute Less to Get More: Using ORC to Improve Sparse Filtering", "authors": ["Johannes Lederer", "Sergio Guadarrama"], "emails": ["johanneslederer@cornell.edu", "sergio.guadarrama@berkeley.edu"], "sections": [{"heading": "Introduction", "text": "Standard ways to improve image classification are to collect more samples or to change the representation and the processing of the data. In practice, the number of samples is typically limited, so that the second approach becomes relevant. An important tool for this second approach are feature learning algorithms, which aim at easing the classification task by transforming the data. Recently proposed deep learning methods intend to jointly learn learn a feature transformation and the classification (Krizhevsky, Sutskever, and Hinton 2012). In this work, however, we focus on unsupervised feature learning, especially on Sparse Filtering, because of their simplicity and scalability.\nFeature learning algorithms for image classification pipelines typically consists of three steps: pre-processing, (un)supervised dictionary learning, and encoding. An abundance of procedures is available for each of these steps, but for accurate image classification, we need procedures that are effective and interact beneficially with each other (Agarwal and Triggs 2006; Coates and Ng 2011; Coates, Ng, and Lee 2011; Jia, Huang, and Darrell 2012; Le 2013; LeCun, Huang, and Bottou 2004). Therefore, a profound understanding of these procedures is crucial to ensure accurate results and efficient computations.\nIn this paper, we study the performance of Sparse Filtering (Ngiam et al. 2011) for image classification. Our main contributions are: Copyright \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\u2022 we show that Sparse Filtering can strongly benefit from early stopping;\n\u2022 we show that the performance of Sparse Filtering is correlated with spectral properties of feature matrices on tests sets;\n\u2022 we introduce the Optimal Roundness Criterion (ORC), a stopping criterion for Sparse Filtering based on the above correlation, and demonstrate that the ORC can considerably improve image classification."}, {"heading": "Feature Learning for Image Classification", "text": "Feature learning algorithms often consist of two steps: In a first step, a dictionary is learned, and in a second step, the samples are encoded based on this dictionary. A typical dictionary learning step for image classification is sketched in Figure 1: First, random patches (samples) are extracted from the training images. These patches are then pre-processed using, for example, Statistical Whitening or Contrast Normalization. Finally, an unsupervised learning algorithm is applied to learn a dictionary from the pre-processed patches. Once a dictionary is learnt, several further steps need to be applied to finally train an image classifier, see, for example, (Coates and Ng 2011; Coates, Ng, and Lee 2011; Jia, Huang, and Darrell 2012; Le 2013). Our pipeline is similar to the one in (Coates and Ng 2011): We extract square patches comprising 9 \u00d7 9 pixels, pre-process them with Contrast Normalization1 and/or Statistical Whitening, and finally pass them to Random Patches or Sparse Filtering. (Note that our outcomes differ slightly from those in (Coates and Ng 2011) because we use square patches comprising 9\u00d79 pixels instead of 6\u00d76 pixels.) Subsequently, we apply soft-thresholding for encoding, 4\u00d74 spatial max pooling for extracting features from the training data images, and finally L2 SVM classification (cf. (Coates and Ng 2011)).\nNumerous examples show that feature learning can considerably improve classification. Therefore, insight in the underlying principles of feature learning algorithms such as Statistical Whitening and Sparse Filtering is of great interest.\n1Contrast normalization consists of subtracting the mean and dividing by the standard deviation of the pixel values.\nar X\niv :1\n40 9.\n46 89\nv2 [\ncs .C\nV ]\n2 4\nM ay\n2 01\nIn mathematical terms, a feature learning algorithm provides a transformation\nF : Rl\u00d7p \u2192 Rn\u00d7p\nX 7\u2192 F(X) (1)\nof an original feature matrix X \u2208 Rl\u00d7p to a new feature matrix F(X) \u2208 Rn\u00d7p. We adopt the convention that the rows of the matrices correspond to the features, the columns to the samples; this convention implies in particular that l \u2208 N is the number of original features, p \u2208 N the number of samples, and n \u2208 N the number of new features."}, {"heading": "The Optimal Roundness Criterion", "text": ""}, {"heading": "Roundness of Feature Matrices", "text": "Feature learning can be seen as trade-off between reducing the correlations of the feature representation and preservation of relevant information. This trade-off can be readily understood looking at Statistical Whitening. For this, recall that pre-processing with Statistical Whitening transforms a set of image patches into a new set of patches by changing the local correlation structure. More precisely, Statistical Whitening transforms patches XPatch \u2208 Rn\n\u2032\u00d7p (n\u2032 < n), that is, subsets of the entire feature matrix, into new patches FPatch(XPatch) such that\nFPatch(XPatch)TFPatch(XPatch) = n\u2032 In\u2032 .\nStatistical Whitening therefore acts locally: while the correlation structures of the single patches are directly and radically changed, the structure of the entire matrix is affected only indirectly. However, these indirect effects on the entire matrix are important for the following. To capture these effects, we therefore introduce the roundness of a feature matrix F := F(X) given an original feature matrix X . On a high level, we say that the new feature matrix F is round if the spectrum of the associated Gram matrixFFT \u2208 Rn\u00d7n is narrow. To specify this notion, we denote the ordered eigenvalues of FFT by \u03c31(F ) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3n(F ) \u2265 0 and their mean by \u03c3(F ) := 1n \u2211n i=1 \u03c3i(F ) and define roundness as follows:\nDefinition 1. For any matrix F 6= 0, we define its roundness as\nr(F ) := \u03c3(F )\n\u03c31(F ) \u2208 [0, 1].\nThe largest eigenvalue \u03c31 measures the width of the spectrum of the Gram matrix; alternative measures of the width such as the standard deviation of the eigenvalues would\nserve the same purpose. The mean of the eigenvalues \u03c3, on the other hand, is basically a normalization as the following result illustrates (the proof is found in the supplementary material):\nTheorem 1. Denote the columns of F by F 1, . . . , F p. Then, the mean \u03c3(F ) of the eigenvalues of the Gram matrix FFT is constant on S := {F \u2208 Rn\u00d7p : \u2016F 1\u20162 = \u00b7 \u00b7 \u00b7 = \u2016F p\u20162 = 1} :\n\u03c3(F ) := p\nn for all F \u2208 S.\nDefinition 1 therefore states that the larger is r, the narrower is the spectrum of the eigenvalues of the Gram matrix of F , and therefore, the rounder is the matrix F . With this notion of roundness at hand, we can now understand the effects of Statistical Whitening: On the one hand, Definition 1 indicates that Statistical Whitening renders single patches perfectly round, that is, r(FPatch) = 1. On the other hand, Statistical Whitening preserves global structures in the feature matrix. In particular, the entire feature matrix is made rounder but not rendered perfectly round, that is, r(F(X)) < 1. In this sense, Statistical Whitening can be seen as trade-off between increasing of roundness and preservation of global structures. It therefore remains to connect roundness and randomization."}, {"heading": "Roundness and Randomness", "text": "A connection between roundness and randomization is provided by random matrix theory. To illustrate this connection, we first recall Gordon\u2019s theorem for Gaussian random matrices (see (Eldar and Kutyniok 2012, Chapter 5) for a recent introduction to random matrix theory):\nTheorem 2 (Gordon). Let F \u2208 Rn\u00d7p be a random matrix with independent standard normal entries. Then,\n1\u2212 \u221a n/p \u2264 E [\u221a \u03c3n(F )/p ] \u2264 E [\u221a \u03c31(F )/p ] \u2264 1 + \u221a n/p.\nSuch exact bounds are available only for matrices with independent standard normal entries, but sharp bounds in probability are available also for other random matrices. For our purposes, the common message of all these bounds is that random matrices with sufficiently many columns (number of samples) have a small spectrum. This means in particular that such matrices are round as the following asymptotic result illustrates (the proof is based on well-known results from random matrix theory and therefore omitted):\nLemma 1. Let the number of features n \u2261 n(p) be a function of the number of samples p such that n/p \u2192 0. Moreover, for all p \u2208 {1, 2, . . . }, let F \u2261 F (p) be a random matrix with independent standard normal entries. Then, for all > 0,\nP (|r(F )\u2212 1| > )\u2192 0 for p\u2192\u221e,\nthat is, r(F ) converges in probability to 1.\nSimilar results can be derived for non-Gaussian or correlated entries, indicating that random matrices are typically round.\nBesides the connection between roundness and randomization, the above results for random matrices also provide a link between roundness and sample sizes. Indeed, we observe that the above results indicate that large samples sizes lead to round matrices. To make this link more tangible, we conduct simulations with Toeplitz matrices, which can model local correlations that are typical for nearby pixels in natural images (Girshick and Malik 2013). To this end, we first recall that for any fixed parameter \u03c1 \u2208 [0, 1), the entries of a Toeplitz matrix T\u03c1 are defined as2 (T\u03c1)ij := \u03c1|i\u2212j|. We now construct a feature matrix U\u03c1 by drawing each of its columns, that is, the samples, from the normal distribution with mean zero and covariance matrix T\u03c1 \u2208 Rn\u00d7n. Toeplitz matrices with \u03c1 = 0 lead to feature matrices with independent entries; Toeplitz matrices with \u03c1 = 0.8 lead to feature matrices with dependence structures that are more similar to dependence structures found in natural images. In Figure 2, we report the roundness of U\u03c1 for \u03c1 = 0 (plot on the left) and \u03c1 = 0.8 (plot on the right) as a function of the numbers of samples p for different numbers of features n. The results are commensurate with the theoretical findings above: First, both plots illustrate that the roundness of matrices increases if the number of samples is increased but decreases if the number of features is increased (cf. Theorem 2 and Lemma 1). Second, a comparison of the two plots illustrate that the roundness is larger for \u03c1 = 0 than for \u03c1 = 0.8 (since T0 is perfectly round while T0.8 is not)."}, {"heading": "Optimal Roundness Criterion (ORC)", "text": "The above discussion suggest that optimal feature learning is the result of a trade-off between increasing the roundness of the feature matrix and preserving global structures in the data. In this part, we want to exploit this insight to understand and improve iterative feature learning algorithms. Common feature learning algorithms consist of transformations that are defined as minimizers of a functional. These functionals are then often computed iteratively via a se-\n2We set 0k := 1 for k = 0 and 0k := 0 for k 6= 0.\nquence of gradient based operations. In this paper, we therefore focus on feature learning algorithms where the transformation F as in (1) is the limit of a sequence of transformations (Gk)k\u2208N, that is,\nF = lim k\u2192\u221e Gk,\nwhere for all k \u2208 N,\nGk : Rl\u00d7p \u2192 Rn\u00d7p\nX 7\u2192 Gk(X).\nA prominent representative of such iterative algorithms is Sparse Filtering. Sparse Filtering consists of normalizations and the minimization of an `1-criterion (see next Section). It is reasonable to assume that these operations - similar to the local changes by Statistical Whitening - preserve certain global structures of the feature matrix. In view of a tradeoff between roundness and preservation of global structures, we are therefore interested in stopping the iterations as soon as the roundness is maximized. More formally, we introduce the ORC, which serves as stopping criterion to maximize the roundness:\nDefinition 2. Let r be the roundness introduced in Definition 1. The Optimal Roundness Criterion (ORC) replaces the transformation F by\nG\u0302 := Gk\u0302 for\nk\u0302 := argmax k\u2208N\n{r(Gk\u2032) < r(Gk) for all k\u2032 < k}\nif the arg-maximum is finite and G\u0302 := F otherwise.\nThe ORC assures that the computations continue only as long as the roundness increases. Assuming that certain global structures are preserved by the transformations, the ORC provides an optimization scheme for the performance of iterative feature learning algorithms. One could also think of modifications of the ORC that include an additive constant or a factor to force larger increases or to allow for temporary decreases of the roundness."}, {"heading": "Image Classification on CIFAR-10", "text": "For our all experiments, we use the CIFAR-10 dataset (Krizhevsky and Hinton 2009)3. This dataset consists of 60 000 color images partitioned into 10 classes, each containing 6 000 images. Each of the images comprises 32\u00d732 pixels. The dataset is split into a training set with 50 000 images and a test set with 10 000 images. From the training set, we randomly select 10 000 patches for the unsupervised feature learning. These patches are also used to determine the parameters of Contrast Normalization and Statistical Whitening (if applied).\n3http://www.cs.toronto.edu/\u02dckriz/cifar. html"}, {"heading": "Random Patches", "text": "For the dictionary learning step, it was shown that simple randomized procedures combined with Statistical Whitening work surprisingly well (Coates and Ng 2011; Jarrett et al. 2009; Saxe et al. 2011). A popular example is Random Patches, which creates a dictionary matrix by simply stacking up randomly selected samples. In Table 1, we report the influence of Contrast Normalization and Statistical Whitening on Random Patches (cf. (Coates and Ng 2011)). We see that Statistical Whitening is very beneficial for Random Patches and increases the roundness of the transformed feature matrix. This suggests that the roundness can be used as an indicator for the performance of feature learning. (Note that the roundness is on different scales for different numbers of features and can therefore not be compared for different numbers of features.)"}, {"heading": "Sparse Filtering", "text": "Sparse Filtering (Ngiam et al. 2011) is an unsupervised feature learning algorithm that computationally scales particularly well with the dimensions. To recall the definition of Sparse Filtering, we denote by N : Rn\u00d7p \u2192 Rn\u00d7p the function that first normalizes4 the rows of a matrix in Rn\u00d7p to unit Euclidean norm and then normalizes the columns of the resulting matrix to unit Euclidean norm. For any fixed matrix X \u2208 Rl\u00d7p, we then define a matrix WX \u2208 Rn\u00d7l\n4We set 0/0 := 0 \u2217 \u221e := \u221e in the corresponding operations ensure that (2) is well defined.\nsuch that WX \u2208 argmin\nW\u2208Rn\u00d7l \u2016N (WX)\u20161 (2)\nif the minimum is finite and WX := 0 otherwise. Sparse Filtering is then the transformation\nFSF : Rl\u00d7p \u2192 Rn\u00d7p\nX 7\u2192 FSF (X) :=WXX. (Sparse Filtering)\nHowever, we now show by making the normalizations explicit that these normalizations make Sparse Filtering intricate. For this, we define the rank one matricesE1, . . . , En \u2208 Rn\u00d7n via\n(Ei)kl := \u03b4kl\u03b4ik \u2200i, k, l \u2208 {1, . . . , n}\nand G1, . . . , Gp \u2208 Rp\u00d7p via\n(Gi)kl := \u03b4kl\u03b4ik \u2200i, k, l \u2208 {1, . . . , p}\nwhere \u03b4 is the usual Kronecker delta. This then yields the following form of Definition (2). Theorem 3. The matrix WX in (2) is the minimizer of\u2225\u2225\u2225\u2225\u2225 [ n\u2211 i=1 EiWX(WX) TEi ]- 1 2 WX \u00d7\n\u00d7\n[ p\u2211\ni=1\nGi(WX) T\n[ n\u2211\ni=1\nEiWX(WX) TEi ]-1 WXGi ]- 1 2 \u2225\u2225\u2225\u2225\u2225 1\nover all matrices W \u2208 Rl\u00d7n. Although Sparse Filtering is sometimes claimed to have sparsity properties due to the involvement of the `1-norm (similar as the Lasso (Tibshirani 1996), for example), the above reformulation demonstrates that this is far from obvious and needs further clarification.\nIt is apparent that the choice of the number of features n influences the performance of Sparse Filtering. As can be seen in Figure /refWe will see below, however, that the choice of the number of iterations surprisingly can have an even larger influence. We are therefore interested in choosing an appropriate number of iterations. A standard approach would involve l-fold cross-validation schemes, but this requires training of l models and is therefore computationally costly. The ORC, on the other hand, can be a computationally feasible alternative to cross-validation. To illustrate this, we compare in Table 2 the outcomes of Sparse Filtering on the CIFAR-10 dataset with and without application of the ORC. We have also computed the intermediate outcomes of Sparse Filtering at every 20 iterations and report in Figure 3 the corresponding test accuracy, training accuracy, roundness on the training set, and correlations with the test accuracy. The roundness on the test set is basically indistinguishable from the roundness on the training set and is therefore not shown. We make three crucial observations: (i) the test accuracy of Sparse Filtering peaks at around 20 iterations and then decreases monotonically; (ii) the roundness on the training set is highly correlated with the test accuracy; in particular, the locations of the peaks of these curves coincide; (iii) the roundness on the training set is highly correlated with the roundness on the test set. These observations suggest that (i) Sparse Filtering should be stopped early; (ii) the ORC can optimize the performance of Sparse Filtering; (iii) it is sufficient to compute the roundness on the training set. To further support these claims, we have also computed the intermediate outcomes of Sparse Filtering at every 2 iterations in the region around the peaks, that is, we have computed a zoomed-in version of Figure 3. We report the results in Figure 4. We observe that training accuracy, test accuracy, and roundness are highly correlated, which corroborates the above claims and therefore confirms the potential\nof the ORC. We finally note that the curves in the zoomedin version are wiggly not only because of the randomness involved but also because computations of gradients over a small number of iterations involve numerical imprecisions."}, {"heading": "Conclusions and Outlook", "text": "The spectral analysis of feature matrices is a novel and promising approach to feature learning. In particular, our results show that this \u201cgeometric\u201d approach can provide new interpretations and substantial improvements of wide-spread feature learning tools such as Statistical Whitening, Random Patches, and Sparse Filtering. For example, we have revealed that Sparse Filtering can, quite surprisingly, deteriorate with increasing number of iterations and can be made considerably faster and more accurate by early stopping according to the spectrum of the intermediate feature matrices.\nRegarding the theory, it would be of interest to obtain, for specific procedures, predictions on how the roundness changes with the iterations and to what it converges in the limit.\nIn an extended version of this paper, we are planning to include an analysis of Roundness in Convolutional Neural Networks (CNNs) (Fukushima 1980). After being neglected for many years, CNNs have received an enormous deal of attention recently, see (Krizhevsky, Sutskever, and Hinton 2012; Girshick et al. 2013) and many others. We therefore expect that the application of our approach to CNNs can be of substantial interest."}, {"heading": "Acknowledgments", "text": "We thank the reviewers for their insightful comments."}, {"heading": "Appendix: Proofs", "text": "We denote in the following the columns and rows of any matrix M by M1, . . . ,Mp and M1, . . . ,Mn, respectively.\nProof of Theorem 1. The matrix FFT is symmetric and can therefore be diagonalized. This implies that there is an orthogonal orthogonal matrix A \u2208 Rn\u00d7n such that the diagonal entries of AFFTAT \u2208 Rn\u00d7n are \u03c31(F ), . . . , \u03c3n(F ). For this matrix A, it then holds\n\u03c3(F ) = 1\nn n\u2211 i=1 \u03c3i(F )\n= 1\nn n\u2211 i=1 (AFFTAT )ii\n= 1\nn trace(AFFTAT ).\nNext, we invoke the cyclic property of the trace and the orthogonality of the matrix A to obtain\ntrace(AFFTAT ) = trace(FTATAF )\n= trace(FTF )\n= p\u2211 i=1 (FTF )ii.\nFinally, we note that the normalization of the columns of F yields\n(FTF )ii = (F i)TF i = \u2016F i\u201622 = 1\nfor all i \u2208 {1, . . . , p}. The desired result\n\u03c3(F ) = 1\nn p\u2211 i=1 1 = p n\ncan now derived combining the three displays.\nProof of Theorem 3. We first show that for a matrix A \u2208 Rn\u00d7p, the corresponding matrix AR \u2208 Rn\u00d7p with normalized rows can be written as\nAR = [ n\u2211 i=1 EiAA TEi ]-1/2 A. (Claim 1)\nTo this end, we observe that the normalization of the rows of the matrix A corresponds to the matrix multiplication\nAR = DRA,\nwhere DR \u2208 Rn\u00d7n is the diagonal matrix with nonzero entries\n(DR)ii = 1/ \u221a\u221a\u221a\u221a p\u2211 j=1 (Aij)2 \u2200i \u2208 {1, . . . , n}.\nNext, we note that p\u2211 j=1 (Aij) 2 = (AAT )ii \u2200i \u2208 {1, . . . , n}\nand therefore\n((DR)-1)ii = 1/(D R)ii = \u221a (AAT )ii \u2200i \u2208 {1, . . . , n}.\nThis yields the matrix equation\n(DR)-2 = n\u2211 i=1 EiAA TEi\nand therefore\nDR = [ n\u2211 i=1 EiAA TEi ]-1/2 .\nThis proves the first claim. We now show that for a matrix B \u2208 Rn\u00d7p, the corresponding matrix BC \u2208 Rn\u00d7p with normalized columns is given by\nBC = B [ p\u2211 i=1 GiB TBGi ]-1/2 . (Claim 2)\nWe first note that we can write the normalization step - this time for the columns - as the matrix multiplication\nBC = BDC ,\nwhere DC \u2208 Rp\u00d7p is the diagonal matrix with entries\n(DC)ii = 1/ \u221a\u221a\u221a\u221a n\u2211 j=1 (Bji)2 \u2200i \u2208 {1, . . . , p}.\nNext, we note that n\u2211 j=1 (Bji) 2 = (BTB)ii \u2200i \u2208 {1, . . . , p},\nand therefore for the inverse diagonal matrix\n((DC)-1)ii = 1/(D C)ii = \u221a (BTB)ii \u2200i \u2208 {1, . . . , p}.\nThis yields the matrix equation\n(DC)-2 = p\u2211 i=1 GiB TBGi\nand therefore\nDC = [ p\u2211 i=1 GiB TBGi ]-1/2 .\nThis proves the second claim. We now consider FW := WX \u2208 Rn\u00d7p for an arbitrary matrix W \u2208 Rn\u00d7p and apply Claim 1 and Claim 2: Setting A = FW , we obtain from Claim 1 that normalizing the rows of the matrix FW yields the matrix FRW \u2208 Rn\u00d7p given by\nFRW := [ n\u2211 i=1 EiFWF T WEi ]-1/2 FW .\nThis implies in particular\n(FRW ) TFRW =F T W [ n\u2211 i=1 EiFWF T WEi ]-1 FW .\nSetting then B = FRW , we obtain from Claim 2 and the two previous displays that the matrix FW becomes after normalizing its rows and then its columns the matrix[\nn\u2211 i=1 EiFWF T WEi\n]-1/2 FW \u00d7\n\u00d7  p\u2211 i=1 GiF T W [ n\u2211 i=1 EiFWF T WEi ]-1 FWGi -1/2 . The desired result can then be deduced from the definition of WX in (2)."}, {"heading": "Appendix", "text": "We also present numerical outcomes for a different random splitting of the CIFAR-10 dataset. In particular, we recompute Figures 3 and 4 for a different splitting and give the results in Figures 5 and 6 below. The conclusions are virtually the same as above, which further corroborates our findings."}], "references": [{"title": "and Triggs", "author": ["A. Agarwal"], "venue": "B.", "citeRegEx": "Agarwal and Triggs 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Ng", "author": ["A. Coates"], "venue": "A.", "citeRegEx": "Coates and Ng 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks", "author": ["Ng Coates", "A. Lee 2011] Coates", "A. Ng", "H. Lee"], "venue": null, "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "and Kutyniok", "author": ["Y. Eldar"], "venue": "G., eds.", "citeRegEx": "Eldar and Kutyniok 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Malik", "author": ["R. Girshick"], "venue": "J.", "citeRegEx": "Girshick and Malik 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation. preprint arxiv:1311.2524", "author": ["Girshick"], "venue": null, "citeRegEx": "Girshick,? \\Q2013\\E", "shortCiteRegEx": "Girshick", "year": 2013}, {"title": "What is the Best Multi-Stage Architecture for Object Recognition", "author": ["Jarrett"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Jarrett,? \\Q2009\\E", "shortCiteRegEx": "Jarrett", "year": 2009}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Huang Jia", "Y. Darrell 2012] Jia", "C. Huang", "T. Darrell"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Jia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2012}, {"title": "and Hinton", "author": ["A. Krizhevsky"], "venue": "G.", "citeRegEx": "Krizhevsky and Hinton 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Huang LeCun", "Y. Bottou 2004] LeCun", "F. Huang", "L. Bottou"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "P", "author": ["Ngiam, J.", "Koh"], "venue": "W.; Chen, Z.; Bhaskar, S.; and Ng, A.", "citeRegEx": "Ngiam et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "On random weights and unsupervised feature learning", "author": ["Saxe"], "venue": "In 28th International Conference on Machine Learning", "citeRegEx": "Saxe,? \\Q2011\\E", "shortCiteRegEx": "Saxe", "year": 2011}], "referenceMentions": [], "year": 2015, "abstractText": "Sparse Filtering is a popular feature learning algorithm for image classification pipelines. In this paper, we connect the performance of Sparse Filtering with spectral properties of the corresponding feature matrices. This connection provides new insights into Sparse Filtering; in particular, it suggests early stopping of Sparse Filtering. We therefore introduce the Optimal Roundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We show that this stopping criterion is related with pre-processing procedures such as Statistical Whitening and demonstrate that it can make image classification with Sparse Filtering considerably faster and more accurate. Introduction Standard ways to improve image classification are to collect more samples or to change the representation and the processing of the data. In practice, the number of samples is typically limited, so that the second approach becomes relevant. An important tool for this second approach are feature learning algorithms, which aim at easing the classification task by transforming the data. Recently proposed deep learning methods intend to jointly learn learn a feature transformation and the classification (Krizhevsky, Sutskever, and Hinton 2012). In this work, however, we focus on unsupervised feature learning, especially on Sparse Filtering, because of their simplicity and scalability. Feature learning algorithms for image classification pipelines typically consists of three steps: pre-processing, (un)supervised dictionary learning, and encoding. An abundance of procedures is available for each of these steps, but for accurate image classification, we need procedures that are effective and interact beneficially with each other (Agarwal and Triggs 2006; Coates and Ng 2011; Coates, Ng, and Lee 2011; Jia, Huang, and Darrell 2012; Le 2013; LeCun, Huang, and Bottou 2004). Therefore, a profound understanding of these procedures is crucial to ensure accurate results and efficient computations. In this paper, we study the performance of Sparse Filtering (Ngiam et al. 2011) for image classification. Our main contributions are: Copyright \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. \u2022 we show that Sparse Filtering can strongly benefit from early stopping; \u2022 we show that the performance of Sparse Filtering is correlated with spectral properties of feature matrices on tests sets; \u2022 we introduce the Optimal Roundness Criterion (ORC), a stopping criterion for Sparse Filtering based on the above correlation, and demonstrate that the ORC can considerably improve image classification. Feature Learning for Image Classification Feature learning algorithms often consist of two steps: In a first step, a dictionary is learned, and in a second step, the samples are encoded based on this dictionary. A typical dictionary learning step for image classification is sketched in Figure 1: First, random patches (samples) are extracted from the training images. These patches are then pre-processed using, for example, Statistical Whitening or Contrast Normalization. Finally, an unsupervised learning algorithm is applied to learn a dictionary from the pre-processed patches. Once a dictionary is learnt, several further steps need to be applied to finally train an image classifier, see, for example, (Coates and Ng 2011; Coates, Ng, and Lee 2011; Jia, Huang, and Darrell 2012; Le 2013). Our pipeline is similar to the one in (Coates and Ng 2011): We extract square patches comprising 9 \u00d7 9 pixels, pre-process them with Contrast Normalization1 and/or Statistical Whitening, and finally pass them to Random Patches or Sparse Filtering. (Note that our outcomes differ slightly from those in (Coates and Ng 2011) because we use square patches comprising 9\u00d79 pixels instead of 6\u00d76 pixels.) Subsequently, we apply soft-thresholding for encoding, 4\u00d74 spatial max pooling for extracting features from the training data images, and finally L2 SVM classification (cf. (Coates and Ng 2011)). Numerous examples show that feature learning can considerably improve classification. Therefore, insight in the underlying principles of feature learning algorithms such as Statistical Whitening and Sparse Filtering is of great interest. Contrast normalization consists of subtracting the mean and dividing by the standard deviation of the pixel values. ar X iv :1 40 9. 46 89 v2 [ cs .C V ] 2 4 M ay 2 01 5 Training images Extraction of random patches Pre-processing Unsupervised learning Dictionary Figure 1: A typical dictionary learning step. Statistical Whitening and Contrast Normalization are examples for preprocessing procedures; Random Patches and Sparse Filtering are examples for unsupervised learning procedures. In mathematical terms, a feature learning algorithm provides a transformation F : Rl\u00d7p \u2192 Rn\u00d7p X 7\u2192 F(X) (1) of an original feature matrix X \u2208 Rl\u00d7p to a new feature matrix F(X) \u2208 Rn\u00d7p. We adopt the convention that the rows of the matrices correspond to the features, the columns to the samples; this convention implies in particular that l \u2208 N is the number of original features, p \u2208 N the number of samples, and n \u2208 N the number of new features. The Optimal Roundness Criterion Roundness of Feature Matrices Feature learning can be seen as trade-off between reducing the correlations of the feature representation and preservation of relevant information. This trade-off can be readily understood looking at Statistical Whitening. For this, recall that pre-processing with Statistical Whitening transforms a set of image patches into a new set of patches by changing the local correlation structure. More precisely, Statistical Whitening transforms patches XPatch \u2208 R \u2032\u00d7p (n\u2032 < n), that is, subsets of the entire feature matrix, into new patches FPatch(XPatch) such that FPatch(XPatch)FPatch(XPatch) = n\u2032 In\u2032 . Statistical Whitening therefore acts locally: while the correlation structures of the single patches are directly and radically changed, the structure of the entire matrix is affected only indirectly. However, these indirect effects on the entire matrix are important for the following. To capture these effects, we therefore introduce the roundness of a feature matrix F := F(X) given an original feature matrix X . On a high level, we say that the new feature matrix F is round if the spectrum of the associated Gram matrixFF \u2208 Rn\u00d7n is narrow. To specify this notion, we denote the ordered eigenvalues of FF by \u03c31(F ) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3n(F ) \u2265 0 and their mean by \u03c3(F ) := 1 n \u2211n i=1 \u03c3i(F ) and define roundness as follows: Definition 1. For any matrix F 6= 0, we define its roundness as", "creator": "LaTeX with hyperref package"}}}