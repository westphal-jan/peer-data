{"id": "1409.6448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2014", "title": "HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection", "abstract": "belloch In calcified this formula paper, we propose hrudey a andrzejewski novel method for ghesquiere fast downcourt face bagnolet recognition re-structured called L1 / 2 Regularized Sparse beat-up Representation misplacing using k\u0101lid\u0101sa Hierarchical usability Feature shaila Selection (HSR ). lopardo By wellville employing misidentifications hierarchical hectareas feature resupply selection, we dreekmann can comparing compress the scale soliola and mueller dimension computability of nayef global gqozo dictionary, caged which directly contributes to railroaded the g9 decrease ghafur of computational cost volvarina in sparse rauhofer representation jarquin that beschreibung our approach is southeast-northwest strongly ostp rooted snb in. It ruffini consists of Gabor 31.60 wavelets uebber and Extreme 175,000-dollar Learning Machine Auto - Encoder (ELM - recuperation AE) trolle hierarchically. 2-for-18 For Gabor mcsa wavelets part, local features can be nift extracted koru at multiple beween scales schoeneweis and orientations reinder to form Gabor - aitape feature hadramaut based tessy image, mondragone which in turn anbarabad improves pelossof the mutha recognition rate. puszcza Besides, dinnerware in chin the inmaculada presence of firebee occluded gogel face adlouni image, the ofgem scale konstfack of i-215 Gabor - 767-200 feature wara based quarter-wave global variable-frequency dictionary can be compressed catfights accordingly quyi because redundancies adv20 exist foreseeable in Gabor - clariden feature based occlusion dictionary. tingstrom For ELM - AE part, the zamel dimension of 50-38 Gabor - locarno feature based leh\u00e1r global dictionary hadebe can be compressed kops because grotzinger high - baturina dimensional face images tugging can be rapidly represented krocsko by squirtle low - waives dimensional fog feature. By introducing tailpiece L1 / bt19 2 regularization, our hlp approach grito can doesburg produce warehousemen sparser deoband and succeded more haizhou robust franco-belgian representation softer compared to riviera regularized jettisoned Sparse rocket-assisted Representation based Classification (anbaa SRC ), which duwayne also contributes to kozluk the enjoyment decrease of nizkor the efty computational immature cost psychostimulant in sparse superman representation. In sweringen comparison boppers with mahoso related work such eyesight as SRC and Gabor - apiary feature unilateral based rodionova SRC (lobiondo GSRC ), 2.2-percent experimental results on a hagedorn variety jetix of smashup face databases glovers demonstrate hips the great phillies advantage of our ljr method zayad for computational frangelico cost. firecrest Moreover, serchhip we also cut-throat achieve approximate or even romero better recognition camerarius rate.", "histories": [["v1", "Tue, 23 Sep 2014 08:36:05 GMT  (899kb)", "http://arxiv.org/abs/1409.6448v1", "Submitted to IEEE Computational Intelligence Magazine in 09/2014"]], "COMMENTS": "Submitted to IEEE Computational Intelligence Magazine in 09/2014", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["bo han", "bo he", "tingting sun", "mengmeng ma", "amaury lendasse"], "accepted": false, "id": "1409.6448"}, "pdf": {"name": "1409.6448.pdf", "metadata": {"source": "CRF", "title": "HSR: \u2044 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection", "authors": ["Bo Han", "Bo He", "Tingting Sun", "Mengmeng Ma", "Amaury Lendasse"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose a novel method for fast face recognition called \u2044 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features can be extracted at multiple scales and orientations to form Gabor-feature based image, which in turn improves the recognition rate. Besides, in the presence of occluded face image, the scale of Gabor-feature based global dictionary can be compressed accordingly because redundancies exist in Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of Gabor-feature based global dictionary can be compressed because high-dimensional face images can be rapidly represented by low-dimensional feature. By introducing \u2044 regularization, our approach can produce sparser and more robust representation compared to regularized Sparse Representation based Classification (SRC), which also contributes to the decrease of the computational cost in sparse representation. In comparison with related work such as SRC and Gaborfeature based SRC (GSRC), experimental results on a variety of face databases demonstrate the great advantage of our method for computational cost. Moreover, we also achieve approximate or even better recognition rate.\nKeywords: Fast Face Recognition, Hierarchical Feature Selection, Gabor wavelets, ELM-AE,\nSparse Representation, \u2044 Regularization, HSR"}, {"heading": "1 Introduction", "text": "The technique of face recognition plays an important role in people\u2019s life ranging from commercial to law enforcement applications, such as real-time surveillance, biometric personal identification, and information security[1]. It is one of the most challenging topics in the interface of computer vision and cognitive science. Over past years, extensive research on face recognition has been conducted by many psychophysicists, neuroscientists and engineers. In general views, the definition of face recognition can be formulated as follows. Different faces in a static image can be identified using a database of stored faces. Available collateral information like facial expression may enhance the recognition rate. Generally speaking, if the face images are sufficiently provided, the quality of face recognition will be mainly related to feature extraction and recognition modeling.\nFor feature extraction, more specifically, there are roughly two kinds of popular face features including holistic features and local features. However, the classical methods using holistic features such as Eigenface[2], Fisherface[3] and Randomface are hardly to reveal the essential structures of high-dimensional faces[4]. Therefore, researchers recently prefer local-feature based methods like subspace learning[5] or manifold representation[6]. On one hand, high-dimensional images can be effectively projected into low-dimensional subspace or sub-manifold. On the other hand, compared to holistic-feature based approaches, local-feature based approaches are always less sensitive to variations of illumination, viewpoint and expression, which in turn improves the recognition rate.\nFor recognition modeling, lots of researchers usually evaluate the performance of model by recognition rate instead of computational cost. Recently, Wright and Ma[7] reported their work called the sparse representation based classification (SRC). To be more specific, it can represent the testing image sparsely using training samples via\n-norm minimization, which can be solved by balancing the minimum reconstructed error and the sparsest coefficients. Experimental results showed that the recognition rate of SRC is much higher than that of classical algorithms such as Nearest Neighbor, Nearest Subspace and Linear Support Vector Machine (SVM). However, there are three drawbacks behind the SRC. First, SRC is based on the holistic features, which cannot exactly capture the partial deformation of the face images. Second, regularized SRC usually runs slowly for high-dimensional face images. Third, in the presence of occluded face images, Wright et al. introduce an occlusion dictionary to sparsely code the occluded components in face images. However, the computational cost of SRC increase drastically because of large number of elements in the occlusion dictionary. Therefore, the computational cost of SRC limits its application in real-time area, which increasingly attracts researchers\u2019 attention to solve this issue.\nRecently, Yang and Zhang\u2019s work (Gabor-feature based SRC (GSRC)) [8] claimed that if Gabor wavelets[9] can be employed in feature extraction, it is possible to obtain a much more compact occlusion dictionary in the presence of occluded faces, which not only speeds up the computation but also improves the recognition rate. Although the GSRC provides us a good insight about how to reduce the computational cost of SRC in the presence of occluded faces, to our best knowledge, there is still an essence issue to be addressed. Namely, the computational cost of sparse representation is highly related to three aspects including the dimension of face images, the scale of occlusion dictionary and the speed of regularized optimization. Without any occlusion, Gabor wavelets mainly play an important role in local features\nextraction. So the computational cost of SRC can be determined by the dimension of face images and -norm minimization instead of the scale of occlusion dictionary. If we want to reduce computational cost of SRC in a general condition, on the one hand, we should effectively project high-dimensional faces into low-dimensional features. On the other hand, we should find a sparser representation than regularized SRC.\nInspired by these observations, in this paper, we propose a novel method for fast face recognition called \u2044 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). In the feature extraction, we employ hierarchical feature selection because it contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE)[10] hierarchically. To be more specific, Gabor wavelets could effectively extract local features at multiple scales and orientations[11] forming Gabor-feature based images, which can greatly improve the recognition rate. Moreover, in the presence of occluded faces, we can obtain a compact occlusion dictionary via sparse coding because of redundancies in Gabor-feature based occlusion dictionary, thus the scale of global dictionary can be decreased accordingly. In addition, high-dimensional face images can be effectively represented by low-dimensional features via ELM-AE, thus the dimension of global dictionary can be decreased accordingly. So for Gabor-feature based global dictionary, the compression of scale and dimension contributes to the decrease of computational cost in sparse representation. Finally, the Gabor-feature based methods have been applied into face recognition leading to state-of-the-art recognition rate[12]. Also the computational cost of ELM-AE is much less than Principal Component Analysis (PCA) used in SRC. In the recognition modeling, the main difference between our method HSR and SRC is that -norm minimization is replaced by \u2044 -norm minimization[13] because \u2044 -norm minimization can produce sparser representation, which directly decreases the computational cost of sparse representation. Although \u2044 -norm minimization belongs to non-convex optimization problems, it can be easily transformed into a series of weighted -norm minimization, which is convenient for us to solve by existing methods. Moreover,\n\u2044 -norm minimization is more robust than -norm minimization, which is more suitable to process occluded face images. In our experiments, the new method has been verified on representative face databases (Extended Yale B, AR and FERET) with different conditions like lighting, pose, expression, and occlusion. In comparison with related work such as SRC[7] and GSRC[8], experimental results demonstrated that our method is slightly complicated in structure, but it shows the great advantage for computational cost. And we also achieve approximate or even better recognition rate. Therefore, our method has a great potential for the application of fast face recognition like real-time surveillance.\nThe rest of paper is organized as follows. In section 2, we briefly discuss previous work on ELM and sparse representation based on regularization. In section 3, we describe our new method including hierarchical feature selection and \u2044 regularized sparse representation. In section 4, we report experimental results on SRC, GSRC and our method HSR under representative face databases with different conditions. Also we present discussions on the performance of new method. Finally, in section 5, we show conclusions on our current research and indicate two important directions for future work."}, {"heading": "2 Previous works", "text": ""}, {"heading": "2.1 The structure of the original ELM", "text": "Extreme Learning Machine (ELM) was proposed by Huang et.al for faster learning speed and higher generalization performance[14][15]. The essence of ELM is that the parameters of the hidden nodes can be generated randomly without manually tuning[16]. Specifically speaking, the input data is mapped to L-dimensional hidden layer and the network output is given by Eq(1).\n( ) \u2211 ( )\n(1)\nWhere [ ] are the output weights between the hidden nodes and\nthe output nodes, ( ) ( ) is the output of hidden layer, is the\ninput weight, is the input bias and ( ) is the activation function, they all\ncorrespond with the output of the hidden node. The ELM algorithm can be summarized as follows. Given training samples { } , where input data\n[ ] and the target labels [ ] . The input data is\nmapped to -dimensional hidden layer initially. The structure of ELM can be determined if the output weights can be calculated, so the following learning problems can be formulated by Eq(2).\n(2)\nWhere [ ] is the target matrix, [ ( ) ( ) ( )] is the hidden layer output matrix, and ( ) [ ( ) ( ) ( )] . So the output weights can be calculated by Eq(3).\n(3)\nWhere [15][17] denotes the Moore-Penrose generalized inverse of matrix .\nTo make the resultant solution more stable and have better generalization performance[10], a positive value \u2044 as a regularization term can be added to the diagonal of shown Eq(4).\n(\n)\n(4)"}, {"heading": "2.2 Sparse representation based on \ud835\udc73\ud835\udfcf regularization", "text": "Given training samples [ ] from all the training\nsamples, where ( ) is an -dimensional vector, which belongs to\nthe sample of the class. Denote by , a test sample from the same\nclass. Intuitively, can be approximately represented by the linear combination of the training samples within .\n\u2211\n(5)\nSuppose that the test sample is initially unknown of the exact class, a new matrix is defined to concatenate the entire training samples of all classes:\n[ ] [ ] (6)\nThen the linear representation of can be naturally written as Eq(7).\n(7)\nAccording to sparse coding via -norm minimization, the sparse coefficients can be calculated as Eq(8).\n \u0302 {\u2016 \u2016\n\u2016 \u2016 } (8)\nIn the case of occluded data, we should express the test sample as a sum of sparse representation and error. Then the previous model[7] can be modified as Eq(9).\n[ ] [ ] (9)\nWhere ( ) , and the is a noise term with bounded energy\u2016 \u2016 . According to sparse coding via -norm minimization, the sparse coefficients can be calculated as Eq(10).\n \u0302 {\u2016 \u2016\n\u2016 \u2016 } (10)\nTherefore, we can represent test sample ( ) as sparse coefficients ( \u0302  \u0302 ),\nwhich can be employed to identify the class of test sample."}, {"heading": "3 \ud835\udc73\ud835\udfcf \ud835\udfd0\u2044 Regularized Sparse Representation using Hierarchical Feature Selection", "text": ""}, {"heading": "3.1 Framework", "text": "In this section, we briefly introduce the new method called \u2044 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). Our method will tackle with two critical issues in face recognition. First, how can we reduce computational cost of recognition modeling while keeping the recognition rate? Second, how can we ensure the robustness of our method to occluded faces? Our approach roughly consists of feature extraction and recognition modeling, which solves above problems accordingly. And also we provide the convincing reasons for the choice of recognition model and parameters. The structure of HSR is shown in Fig1.\nFor feature extraction, by employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. To be more specific, it consists of Gabor wavelets and ELM-AE hierarchically. For Gabor wavelets part, according to theories of visual neuroscience, the mechanism of retina cells in human eyes can be simply simulated by Gabor wavelets, which could effectively extract local features at multiple scales and orientations. And the local-feature based methods are always less sensitive to variations of illumination, viewpoint and expression. Therefore, Gabor-feature based\n0 100 200 300 400 500 600 700 -0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u00d7\nCoefficients Dictionary\nSparse\nRepresentation\n1\ni\nN\n1\nL\nInput Space Feature Space\nGabor Filters\nELM-AE\nor\nHierarchical Feature Selection\n(a) (b) (c)\nFig1. The framework of HSR\nimages can improve recognition rate to some extent. Moreover, for the occluded images, the enormous scale of occlusion dictionary is the principle factor to affect the computational cost of sparse representation. Because of redundancies exist in Gabor-feature based occlusion dictionary, the scale of Gabor-feature based global dictionary can be compressed. Besides, the Gabor-feature based methods have been applied into face recognition leading to state-of-the-art recognition rate like Liu and Wechsler\u2019s work. For ELM-AE part, we hope to modify the basic ELM to represent input training and testing images meaningfully. Namely, the output weight of ELM-AE is responsible of learning the features from the input data via singular values. According to ELM theory, ELM-AE is a universal approximator that has a strong ability to achieve compressed, sparse, and equal dimension representation. So it is reasonable to believe that images in a higher dimensional input space can be effectively projected into a lower dimensional feature space via ELM-AE. Thus the dimension of Gabor-feature based global dictionary can be compressed. Moreover, the computational cost of ELM-AE is much less than that of PCA used in SRC because of its random weights and biases of the hidden nodes.\nFor recognition modeling, more specifically, our approach is strongly rooted in the framework of sparse representation, which has showed its excellent performance on recognition rate especially for occluded faces. The testing image can be sparsely represented by the linear combination of the training samples and our target is to balance the reconstructed error and the sparsest coefficients via different kinds of regularization. In our approach, we choose \u2044 -norm minimization instead of\n-norm minimization used in SRC or another regularized parameters because of two reasons. First, regularization locates between regularization and regularization, so regularization has sparse property and it can be solved easily. Naturally thinking, \u2044 regularization locates between regularization and\nregularization, so we expect that \u2044 regularization has sparser property than\nregularization. Actually, the geometry property of \u2044 and regularization has obviously proved our expectation. Second, Xu\u2019s experiments[13] demonstrated that the performance of sparse representation using \u2044 regularization is stronger than that using other regularization (0<p<1/2 or 1/2<p<1). One might argue that\n\u2044 -norm minimization belongs to non-convex optimization problems, which means it is hard to solve. However, we can transform it into a series of weighted -norm minimization, which is convenient for us to solve by existing methods. Moreover,\naccording to Xu\u2019s experiments, \u2044 -norm minimization is more robust than\n-norm minimization, which is more suitable to process occluded faces."}, {"heading": "3.2 Hierarchical Feature Selection", "text": "In sparse representation, the compression of global dictionary normally comes from the reduction of dimension and scale (the number of elements), which directly contributes to the decrease of computational cost. For hierarchical feature selection, we employ Gabor wavelets and ELM-AE hierarchically. By employing Gabor wavelets, we can initially represent original images by Gabor-feature based images, which can improve the recognition rate. In addition, when the face images are partially occluded, we can compress the scale of Gabor-feature based occlusion dictionary via sparse coding, thus the scale of Gabor-feature based global dictionary can be compressed accordingly. By using ELM-AE, high-dimensional images can be rapidly represented by low-dimensional features, thus we can compress the dimension of Gabor-feature based global dictionary and testing images."}, {"heading": "3.2.1 Gabor-Feature based Image Representation and Occlusion Dictionary", "text": "The motivation that we choose Gabor wavelets for image representation is mainly due to their biological relevance and computational properties. In this section, we will mainly formulate how to represent original image via Gabor wavelets below. Then, in the presence of occluded images, we briefly introduce how to compress the scale of Gabor-feature based occlusion dictionary via sparse coding.\nMore specifically, Gabor wavelets[12] usually demonstrate good characteristics of spatial locality and orientation selectivity. Moreover, in the space and frequency domains, they are optimally localized. It can also be defined with the orientation and scale as follows.\n( ) \u2016 \u2016 ( \u2016 \u2016 \u2016 \u2016 \u2044 )[\n\u2044 ] (11)\nWhere the pixel of an image is ( ), the wave vector is defined as\nwith\n\u2044 and \u2044 . is the maximum frequency,\nand is the spacing factor between kernels in the frequency domain. Besides, determines the ratio of the Gaussian window width to wavelength. In most cases, Gabor wavelets have five different scales and eight orientations. As Liu and Wechsler\u2019s work, the real part of Gabor wavelets can be shown in Fig2.(a).\nHere we should also note that when the parameters of Gabor wavelets are as\n, \u2044 , \u221a , the Gabor wavelets demonstrate the excellent characteristics of spatial frequency, orientation selectivity and spatial locality.\nAccording to above discussion, we can naturally represent high-dimensional images via Gabor wavelets. The Gabor-feature based local representation is equal to the convolution of the input images with each Gabor wavelet. For example, the convolution of image ( ) with a Gabor wavelet is defined as below.\n( ) ( ) ( ) (12)\nGabor wavelets\n(a) (b)\nGabor-feature image\nFig2 (a)The real part of the Gabor wavelets at five scales and eight orientations,\n(b)Gabor-feature based image\nThese convolution results show different scales, localities and orientations corresponding to the Gabor wavelets. As Liu and Wechsler\u2019s work, the convolution results are all complex number. To contain a Gabor-feature based image, we should first normalize all convolution results, and then concatenate them to form an augmented feature vector .\n(\n)\n(13)\nWhere is an image based on Gabor-feature, which not only improves recognition rate but also bears to image local deformation to some degree. can be shown in Fig2.(b).\nTo make a further step, we can derive Gabor-feature based Image Representationin two situations. First, without any occlusion, the linear representation of can be rewritten as . By employing Gabor wavelets, we can derive the Eq(14).\n( ) ( ) ( ) ( ) ( ) (14)\nWhere ( ) [ ( ) ( ) ( )] and ( ) [ ( ) ( )].\nIn the presence of occluded testing image , the Eq(14) should be modified as Ma\u2019s work indicated as Eq(15).\n( ) [ ( ) ( )] [ ] ( ) (15)\nWhere ( ) is the Gabor-feature based occlusion dictionary, and is the representation coefficient vector of the input Gabor feature vector ( ) over ( ).\nThe occlusion dictionary in SRC is normally selected as the identity matrix [7], so SRC has a large number of elements in occlusion dictionary, which definitely increases the computational cost of optimization. For example, If the dimension of original images is ( ), then the dimension of occlusion dictionary in SRC is of . By using Gabor wavelets, the occlusion dictionary will be represented into Gabor-feature based occlusion dictionary ( ) , which is obviously redundant. So we should compress it from two aspects including\nthe dimension and scale. Now, let\u2019s briefly discuss the scale compression. We hope to compress the scale of Gabor-feature based occlusion dictionary because redundancies\nexist in it. For example, suppose ( ) [ ] the original\nGabor-feature based occlusion dictionary, then the scale-compressed occlusion\ndictionary is denoted by [ ] ( ), and we can represent\nby via sparse coding. So our objective function is defined as Eq(16).\n{\u2016 \u2016 \u2016 \u2016 }\n(16)\nIt is easy to solve this optimization problem by optimizing and alternatively. Therefore, the compression of scale is easily achieved. The Eq(15) can be modified by Eq(17)\n( ) [ ( ) ] [ ] (17)\nIn the next section, we will introduce a new method called ELM-AE, which can effectively compress the dimension of global dictionary."}, {"heading": "3.2.2 ELM-AE for High-Dimensional Images Representation", "text": "The motivation that we choose ELM-AE for image representation is due to its representation ability and computational cost. We will mainly introduce ELM-AE for high-dimensional images representation below. And then we will briefly verify the performance of ELM-AE.\nFor Auto-Encoder[18], the output data  \u0302 is similar to the input data . Some interesting structure can be obtained when constraints are placing on the networks. For example, suppose that the input image is , and the number of the input nodes is , but only hidden nodes, then the network must try to reconstruct -dimension output nodes with the hidden nodes, which is forced to learn a compressed representation a compressed representation of the input. Based on the above concept, the ELM-AE was first proposed by Huang et.al[10], and the main objective of ELM-AE is to represent the input data meaningfully and rapidly. In term of the number of the input nodes and hidden nodes, there are three different representation including compressed representation, sparse representation and equal dimension representation. For face recognition task, we hope to represent input training and testing images by compressed representation. The training structure of ELM-AE can be seen as Fig3.(a).\nTo be more specific, we first modify the basic ELM[15][19] to conduct unsupervised learning ( ), and random weights and biases of the hidden nodes are chosen to be orthogonal because orthogonalization will make the generalization of ELM-AE better.\nThe orthogonal random weight and bias can be calculated by Eq(18).\n(18)\nWhere [ ] is the orthogonal random weight and [ ] is the orthogonal random bias between the input nodes and hidden nodes. Then we calculate the hidden layer output matrix as the original ELM does.\n( ) (19)\nAfter training process, we first hypothesize that the output weights of ELM can be treated as coding parameters of Auto-Encoder, which is responsible of learning the low-dimensional features from the high-dimensional data. And the output weight as Eq(20).\n(\n)\n(20)\nWhere [ ] is the output of hidden layer and [ ] is the input data.\nTherefore, the trained ELM-AE will be employed to conduct high-dimensional images representation, for example, the original image is initially represented via Gabor wavelets, and the dimension of Gabor-feature based image will naturally increase. Then we represent Gabor-feature based image into low-dimensional features via ELM-AE by Eq(21).\n(21)\nAfter hierarchical feature selection, the image representation can be visualized in Fig3.(b), which will be sent into the framework of sparse representation for further processing.\n1\ni\nN\n1\nL\nInput Space Feature Space\n1\nL\n1\ni\nN\nx x\nInput Nodes Output Nodes\n(a1,b1)\n(aL,bL)\ng1\ngL\n1\ni\nN\nGabor-feature image \nr \n(b)\n(a)\nFig3. (a)The training of ELM-AE. (When the number of hidden nodes is less than that\nof input nodes, it can be used as the compressed representation), (b) Image\nrepresentation after hierarchical feature selection\nWe can clearly see that the computational cost of ELM-AE is much lower than that of PCA. It shows that ELM-AE itself can speed up the process of feature extraction. Moreover, in HSR, the ELM-AE can effectively reduce the dimension of global dictionary and testing images, which greatly relieve the computational burden undertaken by optimization methods."}, {"heading": "3.3 \ud835\udc73\ud835\udfcf \ud835\udfd0\u2044 Regularized Sparse Representation", "text": "For recognition modeling, our approach is strongly rooted in the framework of sparse representation because of good recognition rate and robustness to occluded faces. In this section, we first introduce generic framework of sparse representation, and then compare different regularized parameters such as \u2044 , and . Finally, we decide to employ \u2044 because it can produce sparser and more robust representation compared to , which speeds up the face recognition."}, {"heading": "3.3.1 Generic Framework of Sparse Representation for Face Recognition", "text": "Specifically speaking, supposing that well-aligned training face images of each class are sufficiently provided. We collect training images together forming a large training dictionary and each column is normalized via -norm. One classical assumption is that a new image of class can be well represented as a linear combination of all the training samples. However, if the identity of the test image is unknown, the problem becomes more complex because we will represent the test image using training dictionary . So the linear representation of the test image can be written as Eq(22).\n(22)\nWhere is a vector of sparse coefficients. In practical situations, the coefficient vector is often complicated because of the presence of partial occluded faces . The linear model should be modified as Eq(23).\n[ ] [ ] (23)\nWhere is a vector of error, [ ] and [ ] , thus face recognition in the presence of occlusion can be represented as the sparest coefficients. We hope to introduce more general framework of sparse representation, so we will choose general loss function for there constructed error and an uncertain norm for regularized parameters. Therefore, the sparest coefficients can be represented as Eq(24).\n \u0302 { ( ) \u2016 \u2016 } (24)\nWhere \u2016 \u2016 denotes -norm that represents the uncertain parameter. Therefore, we can represent test sample using sparse coefficients \u0302 . The general framework can almost explain all special cases. For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when . In our approach, we choose traditional square loss function, and the next section will discuss the choice of regularized parameters."}, {"heading": "3.3.2 Regularized Parameters: \ud835\udc73 , \ud835\udc73\ud835\udfcf \ud835\udfd0\u2044 and \ud835\udc73\ud835\udfcf", "text": "The motivation why we choose \u2044 -norm minimization is due to two aspects.\nFirst, although the sparsest coefficients can be obtained via -norm minimization, the procedure of solving -norm minimization is a NP-hard problem. Therefore, Tibshirani introduced the Lasso algorithm ( -norm minimization) to obtain relative sparse coefficients, which is much easier for us to solve because -norm minimization belongs to the convex problem. What\u2019s more, they proved that regularization is equal to regularization on the certain constraint condition.\nHowever, in the practical application, we find that -norm minimization usually cannot produce the sparest solution, so a question is raised that whether we can introduce a new regularized parameter, which provides a sparser solution than regularization. Fortunately, Xu\u2019s experimental results proved that \u2044 regularization can produce sparser representation compared with regularization, which is also proved by their geometry property. From the Fig4, the bound of regularization has\ndifferent shape, and the solution of -norm minimization is equal to the intersection of the bound and the loss function. For example, we can clearly see that solution via\nis not sparse at all and the solution via \u2044 is sparser than that via because\nthe bound of \u2044 is easier to intersect with loss function at coordinates.\n(a) (b) (c) \u2044\nFig4. The possibility of sparse solution via , and \u2044\nAlthough \u2044 -norm minimization belongs to non-convex optimization problems, we can transform it into a series of weighted -norm minimization, which is also convenient for us to solve by existing methods. Moreover, according to Xu\u2019s experiments, \u2044 -norm minimization is more robust than -norm minimization, which is more suitable to process occlusion in face images.\nSecond, although we initially want to explore other possibilities like -norm minimization (0<p<1/2 or 1/2<p<1), Xu\u2019s experiments[13] clearly demonstrated that\nthe performance of sparse representation using \u2044 regularization is stronger than that using other regularization (0<p<1/2 or 1/2<p<1). Therefore \u2044 regularization can completely replace regularization (0<p<1).\nOverall, we naturally introduce \u2044 regularized sparse representation for fast face recognition using hierarchical feature selection (HSR). Our method has solved two critical issues raised in the section3.1. First, we employ Gabor wavelets and ELM-AE hierarchically in order to reduce the dimension and scale of global dictionary, which\naccordingly reduces the computational cost of sparse representation. Second,\n\u2044 -norm minimization can produce sparser and more robust representation than\n-norm minimization, which not only reduces the computational cost of optimization but also is more suitable to process occlusion in face images. The algorithm of HSR and its explanation are released below.\nAlgorithm 1 The HSR algorithm 1.Images representation: is an occluded face, which is a special case of normal face. The columns of and are normalized to have unit -norm.\n[ ] [ ]\nWhere , ( ).\n2. Gabor wavelets: extract local feature to enhance recognition rate and compress the scale of Gabor feature based occlusion dictionary via sparse coding.\n( ) [ ( ) ( )] [ ]\n( ) [ ( ) ] [ ]\nWhere ( ) ,\n( ), ( ), and .\n3. ELM-EA: compress the dimension of global dictionary and testing image\n( ) [ ( ) ] [ ]\nWhere ( ) ,\n( ), and .\n4. Solve the \u2044 -norm minimization problem: get a sparser and more robust representation.\n \u0302 \u2044 {\u2016 ( )\n\u2016 \u2016 \u2016 \u2044\n\u2044 }\nWhere\n \u0302 \u2044 [ \u0302 \u2044  \u0302 \u2044 ]\n[ ( ) ]\nAnd is a positive scalar number that balances the reconstructed error and sparse coefficients.\n5. Compute the residuals\n( ( )) \u2016 ( )  \u0302 \u2044 ( ) ( \u0302 \u2044 )\u2016\nWhere ( ) is the characteristic function which selects the coefficients associated with the class.\n6. Output that ( ) ( ( )) ( ( ))"}, {"heading": "4 Experimental results", "text": "In this section, we present some experimental results on available benchmark databases to compare the performance of the proposed algorithm HSR with GSRC and SRC. The reason why HSR does not compare with deep learning algorithms[24] is due to the common fact that their computational cost is very expensive. To evaluate the performance of HSR comprehensively, this section is divided into two detailed sections. In section 4.1 we first tested our method on the face datasets without occlusion. And then in section 4.2 we tested the new method on the face datasets against occlusion using two different frameworks (no partition and partition). All the simulations for the HSR, GSRC and SRC algorithms are carried out in Matlab 7.8 environment running in an Intel Xeon E5-1650 CPU. In the experiments of\nGabor wavelets, the parameters are set as \u2044 , \u221a , , eight orientations { } and five different scales { }by our experience. And the parameters are fixed for all the experiments below. The activation function of ELM is set to \u2018sig\u2019 representing the sigmoidal function, the parameter is set to and the number of the hidden neurons is equal to the compressive feature space dimension. In addition, all face images provided in the databases are cropped and aligned by the location of eyes. The face images from the databases are further normalized to zero mean and unit variance."}, {"heading": "4.1 Face recognition without occlusion", "text": "We compared the performance of the HSR with two classical algorithms SRC and GSRC on three typical facial image databases: Extended Yale B[25], AR[26] and FERET[27]. For the Extended Yale B and AR databases, we compared the performance of HSR, SRC and GSRC versus feature dimension. Moreover, we compared the performance of different compression methods (ELM-AE and PCA) and different regularized parameters ( and \u2044 ) on two databases. For the FERET, we compared the performance of HSR, SRC and GSRC versus pose angle.\n1) Extended Yale B Database: The database consists of frontal-face images of individuals. The images are normalized to  under various laboratory-controlled lighting conditions. We randomly selected half of the images for training (i.e., images per subject), and the other half for testing. Choosing the training set randomly assures that our results will be independent of any special choice. Fig5 shows some samples from the same object class, and it is obvious that only illumination is added to these images. The dimension of the Gabor-feature based image is (   ) through a set of Gabor wavelets, which includes five different scales and eight orientations. They can capture abundant local features to form Gabor-feature based image, which will take a lot of time to process this high-dimensional image. To compress the feature space, we applied ELM-AE (a part of HSR) and PCA (a part of SRC and GSRC) respectively with the feature dimensions and on the Gabor-feature based images. Then we computed the recognition rate and the computational cost. In addition, the computational cost of sparse representation is equal to the testing time because there is no training process. In our experiments, we set [7] in HSR, GSRC and SRC by our experience. It shows the recognition rates in Fig6.(a) and computational cost in Fig6.(b) of HSR comparing with GSRC and SRC versus the feature dimension. It is turned out that with the increase of feature dimension, the recognition rate becomes higher and the computational cost becomes more. HSR achieves a maximum recognition rate of\n% with D feature space. In contrast, the maximum recognition rate of GSRC is % and SRC is %. The computational cost of SRC and new method is similar, which much less than that of GSRC. According to a specific dimension \ud835\udc37, the computational cost of compression by PCA is s while ELM-AE is .\nFig5.Samples from the same object of Extended Yale B dataset\n(a) (b)\nFig6. Recognition rates (a) and time (b) by SRC, GSRC and HSR versus feature\ndimension across Extended Yale B database\n2) AR Database: The AR database consists of frontal images from individuals. We chose a subset consisting of male subjects and female subjects. For each subject, images are selected, which includes only illumination changes and expressions. Fig7 shows several samples from the same object class with the variation of expression and illumination. We selected seven images from Session 1 for training and seven images from Session 2 for testing. The images were cropped and converted to gray scale with the size is  . The dimension of the Gabor-feature vector is after a set of Gabor wavelets. Then we continued to reduce the feature space with five dimensions: and .We also set in HSR, GSRC and SRC, like on the Extended Yale B database. it shows the recognition rates in Fig8.(a) and the computational cost in Fig8.(b) of HSR comparing with GSRC and SRC versus the feature dimension. On this database, the maximum recognition rate of HSR, GSRC and SRC are % % and %respectively. The computational cost of HSR is much less than the other two methods on different dimensions. To be more specific, the total computational time of HSR is about % less than that of SRC, and about % less than that of GSRC.\n0 200 400 600 70\n75\n80\n85\n90\n95\n100\nFeature Dimension\nR e c o g n it io\nn R\na te\n(% )\n0 200 400 600 0\n2000\n4000\n6000\n8000\nFeature Dimension\nT im\ne (s\n)\nSRC GSRC HSR\nSRC GSRC HSR\nFig7.samples from the same object of AR dataset\n(a) (b)\nFig8. Recognition rates (a) and time (b) by SRC, GSRC and HSR versus feature\ndimension across AR database\nOn the whole, the AR database is more challenging than the Extended Yale B database, thus the total recognition rate of the AR database is declining slightly. This is due to the AR database includes subjects, but the training samples (dictionary atoms) are only seven images. With more stringent conditions, different lighting conditions are added into four neutral faces and different expressions are added into three faces per subject for AR database. In contrast, for the Extended Yale B database, the number of each subject is larger, and the scale of global dictionary is bigger. Only illumination variations exist on the images. The above two experiments illustrate the performance of HSR is much better than that of the SRC and GSRC versus feature dimension especially for computational cost.\nFor briefly verifying the compression performance of ELM-AE, we selected PCA as a control group for high-dimensional images representation. Only the computational cost of the compression component was taken into consideration in our experiments. We compared the computational cost of ELM-AE and PCA on the Extended Yale B and the AR database before the process of recognition modeling. The testing sets of the Extended Yale B and the AR are compressed to a certain dimension (405D for the Extended Yale B and 450D for the AR), whose computational costs are list on the follow Table 1.\nTable1 : Comparation on computational cost of ELM-AE and PCA\nDatabases Yale AR\nTime(s) ELM-AE 1.3011 0.4596\nPCA 36.2022 9.8460\nWe also conducted a quantitative experiment using different compression methods (ELM-AE and PCA) and different regularized parameters ( -norm and \u2044 -norm) on two databases (table 2). For one thing, when testing sets are compressed to a certain dimension ( D for the Extended Yale B and D for the AR), we demonstrated that \u2044 -norm minimization is superior to the -norm minimization for computational cost while keeping the approximate recognition rate. For another thing, ELM-AE and PCA are used to compress the Gabor-feature based images in different mechanisms. So under the same regularized parameter, the computational cost of methods using ELM-AE is less.\n0 200 400 600 40\n50\n60\n70\n80\n90\n100\nFeature Dimension\nR e c o g n it io\nn R\na te\n(% )\n0 200 400 600 0\n500\n1000\n1500\n2000\n2500\nFeature Dimension\nT im\ne (s\n)\nSRC GSRC HSR\nSRC GSRC HSR\n(3) FERET pose database: this database includes images from subjects ( images per subject). Among the images, images are the frontal face with illumination and facial expressions and the others are the face variation with different pose angles. The images marked with \u2018ba\u2019,\u2019bd\u2019, \u2018be\u2019, \u2018bf\u2019, \u2018bg\u2019, \u2018bj\u2019 and \u2018bk\u2019 stand for the different illumination, facial expressions and pose angles (Fig9). In our experiments, the images of this database were already cropped to the size of  . In order to examine the robustness of HSR comparing with the other original algorithms, we tested the recognition rates and computational cost with respect to the variable pose angle. Then in the first test, we used images marked with \u2018ba\u2019 and \u2018bj\u2019 for training, images marked with \u2018bk\u2019 for testing. In another four tests, images marked with \u2018ba\u2019, \u2018bj\u2019 and \u2018bk\u2019 were used as training set, and the rest of images were respectively used as testing set. After feature extraction, the dimension was fixed on 350D in above three methods. We set the parameters for HSR and GSRC and for SRC, which will conduct the best results. The results showed a growing trend of the recognition rate with less pose angle variability in Fig10.(a). When the pose angle becomes larger, the recognition rate of HSR is almost 40% higher than the nearest competitor but still poor. Besides, the computational cost of HSR and GSRC is much less than that of SRC in Fig10 (b). The above experiment illustrated the performance of HSR is also much better than that of the SRC and GSRC versus pose angle.\nFig9. Samples from the same object class of FERET database\nba:gallery bd:+25 be:+15 bf:-15 bg:-25 bj:expression bk:illumination\n(a) (b) Fig10. Recognition rates (a) and time (b) by SRC, GSRC and HSR versus pose\nangle across FERET database"}, {"heading": "4.2 Face recognition with occlusion", "text": "In this section, we also compared the performance of the HSR with SRC, GSRC on a subset of AR dataset, which includes occluded images. The chosen subset consists of images from subjects ( male and female). In this subset, images of unoccluded frontal face with expression and illumination variation were used for training set. Besides, the rest data were split into two separate test sets of equal size. The first test set contains images, on which all the subjects are wearing sunglasses. The second test set also contains images, and all the subjects wear scarves instead. Sunglass occludes about % of the image and scarf occludes about % of the image intuitively (Fig11).\nFig11: samples of occluded faces with sunglasses and scarves on AR database\nThe parameters of HSR and GSRC were set to and SRC used , which will conduct the best results. The images of this dataset were resized to 8360, then the size of global dictionary is  in the original SRC. In the case of the proposed HSR and GSRC, the dimension of Gabor-feature based image is\n  , and then decreases to D by ELM-AE. Meanwhile, the scale of Gabor-feature based occlusion dictionary is compressed to by sparse coding. As a result, after hierarchical feature selection, the size of global dictionary is  . Table 3 has shown the experimental results on two testing sets implemented by SRC, GSRC and HSR. Apparently, SRC performs the worst recognition rate and the highest computational cost, in other words, the holistic features used in SRC are not suitable for the occluded images and the scale of global dictionary decides the computational\n-25 -15 0 0\n20\n40\n60\n80\n100\nPose Angle(degree)\nR e c o g n it io\nn R\na te\n(% )\n-25 -15 0 0\n100\n200\n300\n400\n500\nPose Angle(degree)\nT im\ne (s\n)\nSRC GSRC HSR\nSRC GSRC HSR\ncost of sparse representation to some degree. Besides, it is clearly seen that the computational cost of HSR is much less than that of GSRC in two datasets while the recognition rate of HSR is higher than that of GSRC in AR scarves.\nWe quoted the approach in [Wright][7] to partition the whole image into blocks and processed each block independently, assuming the occlusion part is contiguous. In these blocks, some of them are assumed to be completely occluded and some of them may be partially occluded. We calculated the performance of each block using the HSR, which naturally determined the performance of the whole image by voting. In our experiments, the image is divided into (  ) blocks, and rescaled to the size of a small (  for AR database) pixel patch. In each block, After hierarchical feature selection, the dimension of Gabor-feature based image is , and the scale of Gabor-feature based occlusion dictionary is fixed on . Thus, the global dictionary in SRC is  , while the global dictionary of HSR and GSRC are  . Table 4 illustrates the recognition rate and computational cost with the partition approach. The HSR with the partition achieves % in the case of sunglasses testing set and % in the case of scarves testing set with the least computational cost.\nFor comparing difference between partitioned and no-partitioned approaches, we have visualized all results from Table3 and Table4 into Fig12 and Fig13.\nFig12. Recognition rates and time of SRC, GSRC and HSR with no partitioning and\npartitioning on the sunglasses testing set\nFig13. Recognition rates and time of SRC, GSRC and HSR with no partitioning and\npartitioning on the scarves testing set\nWe can clearly see that using partitioning method, the recognition rates generally increase while the computational costs generally decrease except for GSRC. We believe that the number of sub-blocks which make wrong classifications is normally less than the number of sub-blocks that are correctly classified, which can ensure the final recognition rate. What\u2019s more, because of partitioning, the dimension and scale of occlusion dictionary are accordingly decreased, which in turn reduce the computational cost of sparse representation."}, {"heading": "5 Conclusions", "text": "In this paper, we proposed a novel method for fast face recognition called \u2044 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can extract the local features from image, which improves recognition rate because local features are less sensitive to the facial variation. More importantly, the global dictionary can be easily compressed in the dimension and scale by hierarchical feature selection, which speeds up the computation of sparse representation. To be more specific, it is feasible to compress the scale of Gabor-feature based occlusion dictionary via sparse coding. And high-dimensional images and global dictionary can be rapidly compressed into low-dimensional feature space via ELM-AE. By introducing \u2044 regularized sparse representation, our method can produce sparser representation than regularized SRC, which in turn speeds up the face recognition. Besides, our method can also produce more robust representation than regularized SRC, which is more suitable to identify occluded faces such as AR sunglasses and scarves. We evaluated our method on a variety of face databases. Experimental results have demonstrated the great advantage of our method for computational cost in comparison with SRC and GSRC. Besides, we also achieve approximate or even better recognition rate. Therefore, our method has a great potential for the application of fast face recognition like real-time surveillance. Our future work will focus on two aspects. First, we will extend ELM-AE into Multi-Layer ELM-AE, which may extract more representative features in order to improve the recognition rate. Second, we will optimize the \u2044 regularization algorithm in order to reduce the computational cost further."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Dr. Jun Zhou at Griffith University for helpful and excellent discussions and comments. This work is partially supported by Natural Science Foundation of China (41176076, 31202036, 51075377).\nReference\n[1] W.Y. Zhao, R. Chellppa, P.J. Phillips, A. Rosenfeld, Face recognition: A liter-\nature survey. ACM Computing Survey 35 (2003) 399\u2013459.\n[2] M. Turk, A. Pentland, Eigenfaces for recognition. J. Cognitive Neuroscience 3\n(1991) 71\u201386.\n[3] P.N. Belhumeur, J.P. Hespanha, D.J. Kriengman, Eigenfaces vs. fisherfaces:\nRecognition using class specific linear projection. IEEE PAMI 19 (1997) 711\u2013720.\n[4] Barkan, Oren, et al. Fast high dimensional vector multiplication face recognition.\nComputer Vision (ICCV), 2013 IEEE International Conference on. IEEE, 2013.\n[5] Jiang, Xudong. Linear subspace learning-based dimensionality reduction. Signal\nProcessing Magazine, IEEE 28.2 (2011) 16-26.\n[6] O. Freifeld, M.J. Black, Lie bodies: A manifold representation of 3D human shape,\nComputer Vision\u2013ECCV 2012. Springer Berlin Heidelberg (2012) 1-14.\n[7] J. Wright, A.Y. Yang, A. Ganesh, et al., Robust face recognition via sparse\nrepresentation, Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(2) (2009) 210-227.\n[8] M. Yang, L. Zhang, Gabor feature based sparse representation for face recognition\nwith gabor occlusion dictionary, Computer Vision\u2013ECCV 2010. Springer Berlin Heidelberg, (2010) 448-461.\n[9] Sharma, Buddhi Prakash, Rajesh Rana, and Rajesh Mehra. Face Recognition\nUsing Gabor Wavelet for Image Processing Applications. (2013).\n[10] Cambria, Erik, et al. Extreme Learning Machines. IEEE Intelligent Systems 28.6\n(2013) 30-59.\n[11] Gupta, Hitesh A., Anirudh Raju, and Abeer Alwan. The effect of non-linear\ndimension reduction on Gabor filter bank feature space. The Journal of the Acoustical Society of America 134.5 (2013) 4069-4069.\n[12] C. Liu, H. Wechsler, Gabor feature based classification using the enhanced fisher\nlinear discriminant model for face recognition. IEEE IP 11 (2002) 467\u2013476.\n[13] Z B. Xu, Data modeling: Visual psychology approach and L1/2 regularization\ntheory, Proc. Int. Congress of Mathmaticians. (2010) 3151-3184.\n[14] G.B. Huang, Q.Y. Zhu, and C.K. Siew, Extreme learning machine: theory and\napplications, Neurocomputing 70(1) (2006) 489-501.\n[15] G.B. Huang, Q.Y. Zhu, and C.K. Siew, Extreme learning machine: a new\nlearning scheme of feedforward neural networks, Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on. IEEE, 2 (2004) 985-990.\n[16] G.B. Huang, H. Zhou, X. Ding, Extreme learning machine for regression and\nmulticlass classification, Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 42(2) (2012) 513-529.\n[17] B. Noble, Methods for computing the Moore-Penrose generalized inverse, and\nrelated matters. Generalized Inverses and Applications, (1976) 245-301.\n[18] Baldi, Pierre. Autoencoders, Unsupervised Learning, and Deep Architectures.\nICML Unsupervised and Transfer Learning. 2012.\n[19] G.B. Huang, L. Chen, Convex incremental extreme learning machine.\nNeurocomputing, 70(16) (2007) 3056-3062.\n[20] H. Akaike, Factor analysis and AIC, Psychometrika, 52(3) (1987) 317-332.\n[21] K.P. Burnham, D.R. Anderson, Multimodel inference understanding AIC and\nBIC in model selection, Sociological methods & research, 33(2) (2004) 261-304.\n[22] P. Zhao, B. Yu, On model selection consistency of Lasso, The Journal of\nMachine Learning Research, 7 (2006) 2541-2563.\n[23] Ogutu, Joseph O., Torben Schulz-Streeck, and Hans-Peter Piepho. Genomic\nselection using regularized linear regression models: ridge regression, lasso, elastic net and their extensions. BMC proceedings. Vol. 6. No. Suppl 2. BioMed Central Ltd, 2012. [24] J. Kim, K. Koh, M. Kusting, et al., A method for large-scale l1-regularized least squares. IEEE J Se Top Signal Process, 1 (2007) 606-617.\n[24] Bengio, Yoshua. Learning deep architectures for AI. Foundations and trends\u00ae in\nMachine Learning 2.1 (2009) 1-127.\n[25] K. Lee, J. Ho, D. Kriegman, Acquring linear subspaces for face recognition\nunder variable lighting. IEEE PAMI 27 (2005) 684\u2013698.\n[26] A. Martinez, R. benavente, The AR face database (1998).\n[27] P.J. Phillips, H. Moon, S.A. Rizvi, P. Rauss, The FERET evaluation method-\nology for face recognition algorithms. IEEE PAMI 22 (2000) 1090\u20131104."}], "references": [{"title": "Face recognition: A literature survey", "author": ["W.Y. Zhao", "R. Chellppa", "P.J. Phillips", "A. Rosenfeld"], "venue": "ACM Computing Survey 35 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "J. Cognitive Neuroscience 3 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Eigenfaces vs", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriengman"], "venue": "fisherfaces: Recognition using class specific linear projection. IEEE PAMI 19 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Fast high dimensional vector multiplication face recognition", "author": ["Barkan", "Oren"], "venue": "Computer Vision (ICCV),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Linear subspace learning-based dimensionality reduction", "author": ["Jiang", "Xudong"], "venue": "Signal Processing Magazine, IEEE", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Lie bodies: A manifold representation of 3D human shape", "author": ["O. Freifeld", "M.J. Black"], "venue": "Computer Vision\u2013ECCV 2012. Springer Berlin Heidelberg ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["J. Wright", "A.Y. Yang"], "venue": "Ganesh, et al., Robust face recognition via sparse representation, Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(2) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Gabor feature based sparse representation for face recognition with gabor occlusion dictionary", "author": ["M. Yang", "L. Zhang"], "venue": "Computer Vision\u2013ECCV 2010. Springer Berlin Heidelberg, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Face Recognition Using Gabor Wavelet for Image Processing Applications", "author": ["Sharma", "Buddhi Prakash", "Rajesh Rana", "Rajesh Mehra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Extreme Learning Machines", "author": ["Cambria", "Erik"], "venue": "IEEE Intelligent Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "The effect of non-linear dimension reduction on Gabor filter bank feature space", "author": ["Gupta", "Hitesh A", "Anirudh Raju", "Abeer Alwan"], "venue": "The Journal of the Acoustical Society of America", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition", "author": ["C. Liu", "H. Wechsler"], "venue": "IEEE IP 11 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Data modeling: Visual psychology approach and L1/2 regularization theory", "author": ["Z B. Xu"], "venue": "Proc. Int. Congress of Mathmaticians. ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Extreme learning machine: theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing 70(1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on. IEEE, 2 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.B. Huang", "H. Zhou", "X. Ding"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 42(2) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Methods for computing the Moore-Penrose generalized inverse", "author": ["B. Noble"], "venue": "and related matters. Generalized Inverses and Applications, ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1976}, {"title": "Autoencoders, Unsupervised Learning, and Deep Architectures", "author": ["Baldi", "Pierre"], "venue": "ICML Unsupervised and Transfer Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Convex incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing, 70(16) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Factor analysis and AIC", "author": ["H. Akaike"], "venue": "Psychometrika, 52(3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1987}, {"title": "Multimodel inference understanding AIC and BIC in model selection", "author": ["K.P. Burnham", "D.R. Anderson"], "venue": "Sociological methods & research, 33(2) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "The Journal of Machine Learning Research, 7 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Genomic selection using regularized linear regression models: ridge regression, lasso, elastic net and their extensions", "author": ["Ogutu", "Joseph O", "Torben Schulz-Streeck", "Hans-Peter Piepho"], "venue": "BMC proceedings. Vol. 6. No. Suppl 2. BioMed Central Ltd,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning deep architectures for AI. Foundations and trends\u00ae in Machine Learning", "author": ["Bengio", "Yoshua"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Acquring linear subspaces for face recognition under variable lighting", "author": ["K. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE PAMI 27 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "R", "author": ["A. Martinez"], "venue": "benavente, The AR face database ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "The FERET evaluation methodology for face recognition algorithms", "author": ["P.J. Phillips", "H. Moon", "S.A. Rizvi", "P. Rauss"], "venue": "IEEE PAMI 22 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The technique of face recognition plays an important role in people\u2019s life ranging from commercial to law enforcement applications, such as real-time surveillance, biometric personal identification, and information security[1].", "startOffset": 238, "endOffset": 241}, {"referenceID": 1, "context": "However, the classical methods using holistic features such as Eigenface[2], Fisherface[3] and Randomface are hardly to reveal the essential structures of high-dimensional faces[4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "However, the classical methods using holistic features such as Eigenface[2], Fisherface[3] and Randomface are hardly to reveal the essential structures of high-dimensional faces[4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "However, the classical methods using holistic features such as Eigenface[2], Fisherface[3] and Randomface are hardly to reveal the essential structures of high-dimensional faces[4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "Therefore, researchers recently prefer local-feature based methods like subspace learning[5] or manifold representation[6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Therefore, researchers recently prefer local-feature based methods like subspace learning[5] or manifold representation[6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 6, "context": "Recently, Wright and Ma[7] reported their work called the sparse representation based classification (SRC).", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Recently, Yang and Zhang\u2019s work (Gabor-feature based SRC (GSRC)) [8] claimed that if Gabor wavelets[9] can be employed in feature extraction, it is possible to obtain a much more compact occlusion dictionary in the presence of occluded faces, which not only speeds up the computation but also improves the recognition rate.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "Recently, Yang and Zhang\u2019s work (Gabor-feature based SRC (GSRC)) [8] claimed that if Gabor wavelets[9] can be employed in feature extraction, it is possible to obtain a much more compact occlusion dictionary in the presence of occluded faces, which not only speeds up the computation but also improves the recognition rate.", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE)[10] hierarchically.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "To be more specific, Gabor wavelets could effectively extract local features at multiple scales and orientations[11] forming Gabor-feature based images, which can greatly improve the recognition rate.", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Finally, the Gabor-feature based methods have been applied into face recognition leading to state-of-the-art recognition rate[12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "In the recognition modeling, the main difference between our method HSR and SRC is that -norm minimization is replaced by \u2044 -norm minimization[13] because \u2044 -norm minimization can produce sparser representation, which directly decreases the computational cost of sparse representation.", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "In comparison with related work such as SRC[7] and GSRC[8], experimental results demonstrated that our method is slightly complicated in structure, but it shows the great advantage for computational cost.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "In comparison with related work such as SRC[7] and GSRC[8], experimental results demonstrated that our method is slightly complicated in structure, but it shows the great advantage for computational cost.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "al for faster learning speed and higher generalization performance[14][15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "al for faster learning speed and higher generalization performance[14][15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The essence of ELM is that the parameters of the hidden nodes can be generated randomly without manually tuning[16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "Where [15][17] denotes the Moore-Penrose generalized inverse of matrix .", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "Where [15][17] denotes the Moore-Penrose generalized inverse of matrix .", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "To make the resultant solution more stable and have better generalization performance[10], a positive value \u2044 as a regularization term can be added to the diagonal of shown Eq(4).", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Then the previous model[7] can be modified as Eq(9).", "startOffset": 23, "endOffset": 26}, {"referenceID": 12, "context": "Second, Xu\u2019s experiments[13] demonstrated that the performance of sparse representation using \u2044 regularization is stronger than that using other regularization (0<p<1/2 or 1/2<p<1).", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "More specifically, Gabor wavelets[12] usually demonstrate good characteristics of spatial locality and orientation selectivity.", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "The occlusion dictionary in SRC is normally selected as the identity matrix [7], so SRC has a large number of elements in occlusion dictionary, which definitely increases the computational cost of optimization.", "startOffset": 76, "endOffset": 79}, {"referenceID": 17, "context": "For Auto-Encoder[18], the output data  \u0302 is similar to the input data .", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "al[10], and the main objective of ELM-AE is to represent the input data meaningfully and rapidly.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "To be more specific, we first modify the basic ELM[15][19] to conduct unsupervised learning ( ), and random weights and biases of the hidden nodes are chosen to be orthogonal because orthogonalization will make the generalization of ELM-AE better.", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "To be more specific, we first modify the basic ELM[15][19] to conduct unsupervised learning ( ), and random weights and biases of the hidden nodes are chosen to be orthogonal because orthogonalization will make the generalization of ELM-AE better.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 261, "endOffset": 265}, {"referenceID": 12, "context": "Second, although we initially want to explore other possibilities like -norm minimization (0<p<1/2 or 1/2<p<1), Xu\u2019s experiments[13] clearly demonstrated that the performance of sparse representation using \u2044 regularization is stronger than that using other regularization (0<p<1/2 or 1/2<p<1).", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "The reason why HSR does not compare with deep learning algorithms[24] is due to the common fact that their computational cost is very expensive.", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "We compared the performance of the HSR with two classical algorithms SRC and GSRC on three typical facial image databases: Extended Yale B[25], AR[26] and FERET[27].", "startOffset": 138, "endOffset": 142}, {"referenceID": 25, "context": "We compared the performance of the HSR with two classical algorithms SRC and GSRC on three typical facial image databases: Extended Yale B[25], AR[26] and FERET[27].", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "We compared the performance of the HSR with two classical algorithms SRC and GSRC on three typical facial image databases: Extended Yale B[25], AR[26] and FERET[27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 6, "context": "In our experiments, we set [7] in HSR, GSRC and SRC by our experience.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "We quoted the approach in [Wright][7] to partition the whole image into blocks and processed each block independently, assuming the occlusion part is contiguous.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "[1] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Barkan, Oren, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jiang, Xudong.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] O.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Sharma, Buddhi Prakash, Rajesh Rana, and Rajesh Mehra.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Cambria, Erik, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Gupta, Hitesh A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Z B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Baldi, Pierre.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Ogutu, Joseph O.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Bengio, Yoshua.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] P.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "In this paper, we propose a novel method for fast face recognition called \u2044 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features can be extracted at multiple scales and orientations to form Gabor-feature based image, which in turn improves the recognition rate. Besides, in the presence of occluded face image, the scale of Gabor-feature based global dictionary can be compressed accordingly because redundancies exist in Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of Gabor-feature based global dictionary can be compressed because high-dimensional face images can be rapidly represented by low-dimensional feature. By introducing \u2044 regularization, our approach can produce sparser and more robust representation compared to regularized Sparse Representation based Classification (SRC), which also contributes to the decrease of the computational cost in sparse representation. In comparison with related work such as SRC and Gaborfeature based SRC (GSRC), experimental results on a variety of face databases demonstrate the great advantage of our method for computational cost. Moreover, we also achieve approximate or even better recognition rate.", "creator": "Microsoft\u00ae Word 2010"}}}