{"id": "1409.7384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2014", "title": "A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure", "abstract": "chipmaker Feature subset levies selection, remotest as telepathic a asteras special case sadakazu of the general gamecocks subset selection trouve problem, methods has been dispell the topic of a considerable number berthollet of orebody studies gulliksen due to the growing importance bokeelia of g-music data - numbskulls mining applications. girlschool In gumshoes the feature perl subset selection tadahiro problem there zeidenberg are millimeters two jadwiga main issues that parama need osirian to affirming be argentatus addressed: (verhovek i) capilano Finding an superboy-prime appropriate reifert measure taille function dadush than kuchinsky can be nonworking fairly fast mekbel and robustly 23-22 computed 49.86 for lalanne high - dimensional data. (kunishige ii) najafgarh A 393d search strategy to finglass optimize the measure over chernov the refocused subset blackwells space in newlyweds a reasonable amount of time. hoult In this centrals article mutual 67,200 information between fitlinxx features toss and class labels is 200-square considered pullman to be the ornellas measure function. tows Two qmail series expansions for mutual information are jarvik proposed, and herkie it federal is bangor shown 5,140 that most heuristic criteria standley suggested in milliman the fissure literature navios are ceratopsian truncated approximations arecaceae of these sudama expansions. 52,400 It child-sized is well - cicarelli known that moralized searching icewind the whole subset thhe space is fischhoff an khryapa NP - rolla hard problem. 4000m Here, 97.99 instead of 20:25 the conventional sequential search algorithms, mcginniss we morain suggest a parallel ethiopia search strategy parlacen based on woodchopper semidefinite carbonate programming (SDP) epoch that serial can 29-17 search dfler through 55.16 the pottier subset space 82.16 in polynomial time. autopista By mohannad exploiting the similarities between 91.00 the proposed algorithm and gunshots an hubristic instance of the ndou maximum - cut dynamically problem rehearing in graph theory, vouched the m-47 approximation tialata ratio of this toughbook algorithm reykjanes is derived and 10-for-15 is warhawk compared with preadolescent the burga approximation soyuz ratio of the backward elimination vervaeke method. The garreau experiments kongs show pahlen that it l0 can kwang be misleading to sci-tech judge the jewsbury quality xiaowen of a measure caff solely cresta based 3,521 on foxwoods the confer classification yukteswar accuracy, ggf without 3.8000 taking the stairway effect of kakavia the harwick non - rockridge optimum fulbright-hays search uncomfortably strategy into 1997-2001 account.", "histories": [["v1", "Thu, 25 Sep 2014 11:57:48 GMT  (2712kb)", "https://arxiv.org/abs/1409.7384v1", "Submitted to IEEETrans On Pattern Analysis and Machine Intelligence"], ["v2", "Wed, 12 Nov 2014 13:56:54 GMT  (2712kb)", "http://arxiv.org/abs/1409.7384v2", "IEEETrans On Pattern Analysis and Machine Intelligence"]], "COMMENTS": "Submitted to IEEETrans On Pattern Analysis and Machine Intelligence", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tofigh naghibi", "sarah hoffmann", "beat pfister"], "accepted": false, "id": "1409.7384"}, "pdf": {"name": "1409.7384.pdf", "metadata": {"source": "CRF", "title": "A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure", "authors": ["Tofigh Naghibi"], "emails": ["naghibi@tik.ee.ethz.ch", "hoffmann@tik.ee.ethz.ch", "pfister@tik.ee.ethz.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n73 84\nv2 [\ncs .L\nG ]\n1 2\nN ov\nIndex Terms\u2014Feature Selection, Mutual information, Convex objective, Approximation ratio."}, {"heading": "1 INTRODUCTION", "text": "From a purely theoretical point of view, given the underlying conditional probability distribution of a dependent variable C and a set of features X, the Bayes decision rule can be applied to construct the optimum induction algorithm. However, in practice learning machines are not given access to this distribution, Pr(C|X). Therefore, given a feature vector or variables X \u2208 RN , the aim of most machine learning algorithms is to approximate this underlying distribution or estimate some of its characteristics. Unfortunately, in most practically relevant data mining applications, the dimensionality of the feature vector is\nThe authors are with the Computer Engineering and Networks Lab., ETH Zurich, Switzerland.\nE-mail: {naghibi,hoffmann,pfister}@tik.ee.ethz.ch\nquite high making it prohibitive to learn the underlying distribution. For instance, gene expression data or images may easily have more than tens of thousands of features. While, at least in theory, having more features should result in a more discriminative classifier, it is not the case in practice because of the computational burden and curse of dimensionality.\nHigh-dimensional data poses different challenges on induction and prediction algorithms. Essentially, the amount of data to sustain the spatial density of the underlying distribution increases exponentially with the dimensionality of the feature vector, or alternatively, the sparsity increases exponentially given a constant amount of data. Normally in real-world applications, a limited amount of data is available and obtaining a sufficiently good estimate of the underlying high-dimensional probability distribution is almost impossible unless for some special data structures or under some assumptions (independent features, etc).\nThus, dimensionality reduction techniques, particularly feature extraction and feature selection methods, have to be employed to reconcile idealistic learning algorithms with real-world applications.\nIn the context of feature selection, two main issues can be distinguished. The first one is to define an appropriate measure function to assign a score to a set of features. The second issue is to develop a search strategy that can find the optimal (in a sense of optimizing the value of the measure function) subset of features among all feasible subsets in a reasonable amount of time.\nDifferent approaches to address these two problems can roughly be categorized into three groups: Wrapper methods, embedded methods and filter methods.\nWrapper methods [27] use the performance of an induction algorithm (for instance a classifier) as the measure function. Given an inducer I, wrapper approaches search through the space of all possible feature subsets and select the one that maximizes the induction accuracy. Most of the methods of this type require to check all the possible 2N subsets of features and thus, may rapidly become prohibitive due\n2 to the so-called combinatorial explosion. Since the measure function is a machine learning (ML) algorithm, the selected feature subset is only optimal with respect to that particular algorithm, and may show poor generalization performance over other inducers.\nThe second group of feature selection methods are called embedded methods [34] and are based on some internal parameters of the ML algorithm. Embedded approaches rank features during the training process and thus simultaneously determine both the optimal features and the parameters of the ML algorithm. Since using (accessing) the internal parameters may not be applicable in all ML algorithms, this approach cannot be seen as a general solution to the feature selection problem. In contrast to wrapper methods, embedded strategies do not require to run the exhaustive search over all subsets since they mostly evaluate each feature individually based on the score calculated from the internal parameters. However, similar to wrapper methods, embedded methods are dependent on the induction model and thus the selected subset is somehow tuned to a particular induction algorithm.\nFilter methods, as the third group of selection algorithms, focus on filtering out irrelevant and redundant features in which irrelevancy is defined according to a predetermined measure function. Unlike the first two groups, filter methods do not incorporate the learning part and thus show better generalization power over a wider range of induction algorithms. They rely on finding an optimal feature subset through the optimization of a suitable measure function. Since the measure function is selected independently of the induction algorithm, this approach decouples the feature selection problem from the following ML algorithm.\nThe first contribution of this work is to analyze the popular mutual information measure in the context of the feature selection problem. We will expand the mutual information function in two different series and show that most of the previously suggested information-theoretic criteria are the first or second order truncation-approximations of these expansions. The first expansion is based on generalization of mutual information and has already appeared in literature while the second one is new, to the best of our knowledge. The well-known minimal Redundancy Maximal Relevance (mRMR) score function can be immediately concluded from the second expansion. We will discuss the consistency and accuracy of these approximations and experimentally investigate the conditions in which these truncationapproximations may lead to high estimation errors.\nAlternatively, feature selection methods can be categorized based on the search strategies they employ. Popular search approaches can be divided into four categories: Exhaustive search, greedy search, projection and heuristic. A trivial approach is to exhaustively search in the subset space as it is done in wrapper methods. However, as the number of features increases, it can rapidly become infeasible. Hence, many popular search approaches use greedy hill climbing, as an approximation to this NP-hard combinatorial problem.\nGreedy algorithms iteratively evaluate a candidate subset of features, then modify the subset and evaluate if the new subset is an improvement over the old one. This can be done in a forward selection setup which starts with an empty set and adds one feature at a time or with a backward elimination process which starts with the full set of features and removes one feature at each step. The third group of the search algorithms are based on targeted projection pursuit which is a linear mapping algorithm to pursue an optimum projection of data onto a low dimensional manifold that scores highly with respect to a measure function [15]. In heuristic methods, for instance genetic algorithms, the search is started with an initial subset of features which gradually evolves toward better solutions.\nRecently, two convex quadratic programing based methods, QPFS in [39] and SOSS in [32] have been suggested to address the search problem. QPFS is a deterministic algorithm and utilizes the Nystro\u0308m method to approximate large matrices for efficiency purposes. SOSS on the other hand, has a randomized rounding step which injects a degree of randomness into the algorithm in order to generate more diverse feature sets.\nDeveloping a new search strategy is another contribution of this paper. Here, we introduce a new class of search algorithms based on Semi-Definite Programming (SDP) relaxation. We reformulate the feature selection problem as a (0-1)-quadratic integer programming and will show that it can be relaxed to an SDP problem, which is convex and hence can be solved with efficient algorithms [7]. Moreover, there is a discussion about the approximation ratio of the proposed algorithm in subsection 3.2. We show that it usually gives better solutions than greedy algorithms in the sense that its approximate solution is more probable to be closer to the optimal point of the criterion."}, {"heading": "2 MUTUAL INFORMATION PROS AND CONS", "text": "Let us consider an N dimensional feature vector X = [X1, X2, ..., XN ] and a dependent variable C which can be either a class label in case of classification or a target variable in case of regression. The mutual information function is defined as a distance from independence between X and C measured by the Kullback-Leibler divergence [11]. Basically, mutual information measures the amount of information shared between X and C by measuring their dependency level. Denote the joint pdf of X and C and its marginal distributions by Pr(X, C), Pr(X) and Pr(C), respectively. The mutual information between the feature vector and the class label can be defined as follows:\nI(X1, X2, . . . ,XN ;C)= I(X;C) = \u222b\nPr(X, C)log Pr(X, C)\nPr(X)Pr(C) dX dC (1)\nIt reaches its maximum value when the dependent variable is perfectly described by the feature set. In this case mutual information is equal to H(C), where H(C) is the Shannon entropy of C.\n3 Mutual information can also be considered a measure of set intersection [38]. Namely, let A and B be event sets corresponding to random variables A and B, respectively. It is not difficult to verify that a function \u00b5 defined as:\n\u00b5(A \u2229 B) = I(A;B) (2)\nsatisfies all three properties of a formal measure over sets [42] [6], i.e., non-negativity, assigning zero to empty set and countable additivity. However, as we see later, the generalization of the mutual information measure to more than two sets will no longer satisfy the non-negativity property and thus can be seen as a signed measure which is the generalization of the concept of measure by allowing it to have negative values.\nThere are at least three reasons for the popularity of the use of mutual information in feature selection algorithms.\n1. Most of the suggested non information-theoretic score functions are not formal set measures (for instance correlation function). Therefore, they cannot assign a score to a set of features but rather to individual features. However, mutual information as a formal set measure is able to evaluate all possible informative interactions and complex functional relations between features and as a result, fully extract the information contained in a set of features.\n2. The relevance of the mutual information measure to misclassification error is supported by the existence of bounds relating the probability of misclassification of the Bayes classifier, Pe, to the mutual information. More specifically, Fano\u2019s weak lower bound [13] on Pe,\n1 + Pelog2(ny\u22121) \u2265 H(C)\u2212 I(X;C) (3)\nwhere ny is the number of classes and the Hellman-Raviv [22] upper bound,\nPe \u2264 1\n2 (H(C)\u2212 I(X;C)) (4)\non Pe, provide somewhat a performance guarantee.\nAs it can be seen in (3) and (4), maximizing the mutual information between X and C decreases both upper and lower bounds on misclassification error and guarantees the goodness of the selected feature set. However, there is somewhat of a misunderstanding of this fact in the literature. It is sometimes wrongly claimed that maximizing the mutual information results in minimizing the Pe of the optimal Bayes classifier. This is an unfounded claim since Pe is not a monotonic function of the mutual information. Namely, it is possible that a feature vector A with less relevant information-content about the class label C than a feature vector B yields a lower classification error rate than B. The following example may clarify this point.\nExample 1: Consider a binary classification problem with equal number of positive and negative training samples and two binary features X1 and X2. The goal is to select the optimum feature for the classification task. Suppose the first feature X1 is positive if the outcome is positive. However, when the outcome is negative, X1 can take both positive\nand negative values with the equal probability. Namely, Pr(X1=1|C=1) = 1 and Pr(X1= \u2212 1|C= \u2212 1) = 0.5. In the same manner, the likelihood of X2 is defined as Pr(X2=1|C=1) = 0.9 and Pr(X2= \u2212 1|C=\u2212 1) = 0.7. Then, the Bayes classifier with feature X1 yields the classification error:\nPe1 =Pr(C=\u22121)Pr(X1=1|C=\u22121)\n+ Pr(C=1)Pr(X=\u22121|C=1) = 0.25 (5)\nSimilarly, the Bayes classifier with X2 yields Pe1 = 0.2 meaning that, X2 is a better feature than X1 in the sense of minimizing the probability of misclassification. However, unlike their error probabilities, I(X1;C) = 0.31, is greater than I(X2;C) = 0.29. That is, X1 conveys more information about the class label in the sense of Shannon mutual information than X2.\nA more detailed discussion can be found in [17]. However, it is worthwhile to mention that although using mutual information may not necessarily result in the highest classification accuracy, it guarantees to reveal a salient feature subset by reducing the upper and lower bounds of Pe.\n3- By adapting classification error as a criterion, most standard classification algorithms fail to correctly classify the instances from minority classes in imbalanced datasets. Common approaches to address this issue are to either assign higher misclassification costs to minority classes or replace the classification accuracy criterion with the area under the ROC curve which is a more relevant criterion when dealing with imbalanced datasets. Either way, the features should also be selected by an algorithm which is insensitive (robust) with respect to class distributions (otherwise the selected features may not be informative about minority classes, in the first place). Interestingly, by internally applying unequal class dependent costs, mutual information provides some robustness with respect to class distributions. Thus, even in an imbalanced case, a mutual information based feature selection algorithm is likely (though not guaranteed) to not overlook the features that represent the minority classes. In citebao:11, the concept of the mutual information classifier is investigated. Specifically, the internal cost matrix of the mutual information classifier is derived to show that it applies unequal misclassification costs when dealing with imbalanced data and showed that the mutual information classifier is an optimal classifier in the sense of maximizing a weighted classification accuracy rate. The following example shows this robustness.\nExample 2: Assume an imbalanced binary classification task where Pr(C=1) = 0.9. As in Example 1, there are two binary features X1 and X2 and the goal is to select the optimum feature. Suppose Pr(X1=1|C=1) = 1 and Pr(X1= \u2212 1|C= \u2212 1) = 0.5. Unlike the first feature, X2 can much better classify the minority class Pr(X2=\u22121|C=\u22121) = 1 and Pr(X2=1|C=1) = 0.8. It can be seen that the Bayes classifier with X1 results in 100% classification rate for the majority class while only\n4 50% correct classification for the minority. On the other hand, using X2 leads to 100% correct classification for the minority class and 80% for the majority. Based on the probability of error, X1 should be preferred since its probability of error is Pe1 = 0.05 while Pe2 = 0.18. However, by using X1 the classifier can not learn the rare event (50% classification rate) and thus randomly classifies the minority class which is the class of interest in many applications. Interestingly, unlike the Bayesian error probabilities, mutual information prefers X2 over X1, since I(X2;C) = 0.20 is greater than I(X1;C) = 0.18. That is, mutual information is to some extent robust against\nimbalanced data.\nUnfortunately, despite the theoretical appeal of the mutual information measure, given a limited amount of data, an accurate estimate of the mutual information would be impossible. Because to calculate mutual information, estimating the high-dimensional joint probability Pr(X, C) is inevitable which is, in turn, known to be an NP hard problem [25].\nAs mutual information is hard to evaluate, several alternatives have been suggested [5], [35], [28]. For instance, the Max-Relevance criterion approximates (1) with the sum of the mutual information values between individual features Xi and C:\nMax-Relevance = N \u2211\ni=1\nI(Xi;C) (6)\nSince it implicitly assumes that features are independent, it is likely that selected features are highly redundant. To overcome this problem, several heuristic corrective terms have been introduced to remove the redundant information and select mutually exclusive features. Here, it is shown that most of these heuristics are derived from the following expansions of mutual information with respect to Xi."}, {"heading": "2.1 First Expansion: Multi-way Mutual Information Expansion", "text": "The first expansion of mutual information that is used here, relies on the natural extension of mutual information to more than two random variables proposed by McGill [30] and Abramson [1]. According to their proposal, the threeway mutual information between random variables Yi is defined by:\nI(Y1;Y2;Y3) =I(Y1;Y3) + I(Y2;Y3)\u2212 I(Y1, Y2;Y3)\n=I(Y1;Y2)\u2212 I(Y1;Y2|Y3) (7)\nwhere \u201c,\u201d between variables denotes the joint variables. Note that, similar to two-way mutual information, it is symmetric with respect to Yi variables, i.e., I(Y1;Y2;Y3) = I(Y2;Y3;Y1). Generalizing over N variables:\nI(Y1;Y2; . . . ;YN ) = I(Y1; . . . ;YN\u22121)\n\u2212 I(Y1; . . . ;YN\u22121|YN ) (8)\n\u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 \u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\nx values\ny va\nlu es\nEllipse class Line class\nFig. 1: Synergy between x and y features. While information of each individual feature about the class label (ellipse or line) is almost zero, their joint information can almost completely remove the class label ambiguity.\nUnlike 2-way mutual information, the generalized mutual information is not necessarily nonnegative and hence, can be interpreted as a signed measure of set intersection [21]. Consider (7) and assume Y3 is class label C, then positive I(Y1;Y2;C) implies that Y1 and Y2 are redundant with respect to C since I(Y1, Y2;C) \u2264 I(Y1;C) + I(Y2;C). However, the more interesting case is when I(Y1;Y2;C) is negative, i.e., I(Y1, Y2;C) \u2265 I(Y1;C) + I(Y2;C). This means, the information contained in the interactions of the variables is greater than the sum of the information of the individual variables [20].\nAn artificial example for this situation is the binary classification problem depicted in Figure 1, where the classification task is to discriminate between the ellipse class (class samples depicted by circles) and the line class (star samples) by using two features: values of x axis and values of y axis. As can be seen, since I(x;C)\u22480 and I(y;C)\u22480, there is no way to distinguish between these two classes by just using one of the features. However, it is obvious that employing both features results in almost perfect classification, i.e., I(x, y;C)\u2248H(C). The mutual information in (1) can be expanded out in terms of generalized mutual information between the features and the class label as:\nI(X;C) =\nN \u2211\ni1=1\nI(Xi1 ;C)\u2212 N\u22121 \u2211\ni1=1\nN \u2211\ni2=i1+1\nI(Xi1 ;Xi2 ;C)\n+ \u00b7 \u00b7 \u00b7+ (\u22121)N\u22121I(X1; . . . ;XN ;C) (9)\nFrom the definition in (8) it is straightforward to infer this expansion. However, the more intuitive proof is to use the fact that mutual information is a measure of set intersection, i.e., I(Y1;Y2;Y3) = \u00b5(Y1 \u2229 Y2 \u2229 Y3), where Yi is the corresponding event set of the Yi variable. Now, expanding the N -variable measure function results in:\nI(X;C) = \u00b5((\nN \u22c3\ni=1\nXi) \u2229 C) = \u00b5( N \u22c3\ni=1\n(Xi \u2229C)) (10)\n=\nN \u2211\ni=1\n\u00b5(Xi \u2229 C)\u2212 N\u22121 \u2211\ni1=1\nN \u2211\ni2=i1+1\n\u00b5(Xi1 \u2229 Xi2 \u2229 C)\n+ \u00b7 \u00b7 \u00b7+ (\u22121)N\u22121\u00b5(X1 \u2229 X2 \u00b7 \u00b7 \u00b7 \u2229 XN \u2229 C)\n5 where the last equation follows directly from the addition law or sum rule in set theory. The proof is complete by recalling that all measure functions with the set intersection arguments in the last equation can be replaced by the mutual information functions according to the definition of mutual information in (2)."}, {"heading": "2.2 Second Expansion: Chain Rule of Information", "text": "The second expansion for mutual information is based on the chain rule of information [11]:\nI(X;C) =\nN \u2211\ni=1\nI(Xi;C|Xi\u22121, . . . , X1) (11)\nThe chain rule of information leaves the choice of ordering quite flexible. For example, the right side can be written in the order (X1, X2, . . . , XN ) or (XN , XN\u22121, . . . , X1). In general, it can be expanded over N ! different permutations of the feature set {X1, . . . , XN}. Taking the sum over all possible expansions yields,\n(N !)I(X;C) = (N\u22121)! N \u2211\ni=1\nI(Xi;C) (12)\n+ (N\u22122)! N \u2211\ni1=1\n\u2211\ni2\u2208{1,...,N}/i1\nI(Xi2 ;C|Xi1)\n+ \u00b7 \u00b7 \u00b7+ (N\u22121)! N \u2211\ni=1\nI(Xi;C|{X1, . . . , XN}\\Xi)\nDividing both sides by (N\u22121)!/2, and using the following equation I(Xi1 ;C|Xi2 ) = I(Xi1 ;C) \u2212 I(Xi1 ;Xi2 ;C) to replace I(Xi1 ;C|Xi2) terms, our second expansion can be expressed as\nN\n2 I(X;C) =\nN \u2211\ni=1\nI(Xi;C) (13)\n\u2212 1\nN \u2212 1\nN\u22121 \u2211\ni1=1\nN \u2211\ni2=i1+1\nI(Xi1 ;Xi2 ;C)\n+ \u00b7 \u00b7 \u00b7+ 1\n2\nN \u2211\ni=1\nI(Xi;C|{X1, . . . , XN}\\Xi)\nIgnoring the unimportant multiplicative constant N/2 on the left side of equation (13), the right side can be seen as a series expansion form of mutual information (up to a known constant factor)."}, {"heading": "2.3 Truncation of the Expansions", "text": "In the both proposed expansions (9) and (13), mutual information terms with more than two features represent higher-order interaction properties. Neglecting the higher order terms yields the so-called truncated approximation of the mutual information function. If we ignore the constant\ncoefficient in (13), the truncated forms of suggested expansions can be written as:\nD1 =\nN \u2211\ni=1\nI(Xi;C)\u2212 N\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi;Xj ;C)\nD2 =\nN \u2211\ni=1\nI(Xi;C)\u2212 1\nN \u2212 1\nN\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi;Xj;C)\n(14)\nwhere D1 is the truncated approximation of (9) and D2 is for (13). Interestingly, despite the very similar structure of the expressions in (14), they have intrinsically different behaviors. This difference seems to be rooted in different functional forms they employ to approximate the underlying high-order pdf with lower order distributions ( i.e., how they combine these lower order terms). For instance, the functional form that MIFS employs to approximate Pr(x) is shown in (18). While D1 is not necessarily a positive value, D2 is guaranteed to be a positive approximation since all terms in (12) are positive. However, D2 may highly underestimate the mutual information values since it may violate the fact that (1) is always greater than or equal to maxi I(Xi;C)."}, {"heading": "2.3.1 JMI, mRMR & MIFS Criteria", "text": "Several known criteria including Joint Mutual Information (JMI) [31], minimal Redundancy Maximal Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can immediately be derived from D1 and D2.\nUsing the identity: I(Xi;Xj ;C) = I(Xi;C)+ I(Xj ;C)\u2212 I(Xi, Xj ;C) in D2 reveals that D2 is equivalent to JMI.\nJMI= D2 = N\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi, Xj ;C) (15)\nUsing I(Xi;Xj ;C) = I(Xi;Xj) \u2212 I(Xi;Xj |C) and ignoring the terms containing more than two variables, i.e., I(Xi;Xj |C), in the second approximation D2, one may immediately recognize the popular score function\nmRMR= N \u2211\ni=1\nI(Xi;C)\u2212 1\nN \u2212 1\nN\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi;Xj) (16)\nintroduced by Peng et al. in [35]. That is, mRMR is a truncated approximation of mutual information and not a heuristic approximation as suggested in [9].\nThe same line of reasoning as for mRMR can be applied to D1 to achieve MIFS with \u03b2 equal to 1.\nMIFS= N \u2211\ni=1\nI(Xi;C)\u2212 N\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi;Xj) (17)\nObservation: A constant feature is a potential danger for the above measures. While adding an informative but correlated feature may reduce the score value (since\n6 I(Xi;Xj|C) \u2212 I(Xi;Xj) can be negative), adding a noninformative constant feature Z to a feature set does not reduce its score value since both I(Z;C) and I(Z;Xi;C) terms are zero, that is, constant features may be preferred over informative but correlated features. Therefore, it is essential to remove constant features by some preprocessing before using the above criteria for feature selection."}, {"heading": "2.3.2 Implicitly Assumed Distribution", "text": "A natural question arising in this context with respect to the proposed truncated approximations is: Under what probabilistic assumptions do the proposed approximations become valid mutual information functions? That is, which structure should a joint pdf admit, to yield mutual information in the forms of D1 or D2?\nFor instance, if we assume features are mutually and class conditionally independent, i.e., Pr(X) =\n\u220fN i=1 Pr(Xi)\nand Pr(X, C) = Pr(C) \u220fN\ni=1 Pr(Xi|C), then it is easy to verify that mutual information has the form of MaxRelevance introduced in (6). These two assumptions, define the adapted independence-map of Pr(X, C) where the independence-map of a joint probability distribution is defined as follows.\nDefinition 1: An independence-map (i-map) is a look up table or a set of rules that denote all the conditional and unconditional independence between random variables. Moreover, an i-map is consistent if it leads to a valid factorized probability distribution.\nThat is, given a consistent i-map, a high-order joint probability distribution is approximated with product of loworder pdfs and the obtained approximation is a valid pdf itself (e.g.,\n\u220fN i=1 Pr(Xi) is an approximation of the\nhigh-order pdf Pr(X) and it is also a valid probability distribution).\nThe question regarding the implicit consistent i-map that MIFS adopts has been investigated in [4]. However, the assumption set (i-map) suggested in their work is inconsistent and leads to the incorrect conclusion that MIFS upper bounds the Bayesian classification error via the inequality (4). As we show in the following theorem, unlike the MaxRelevance case, there is no i-map that can produce mutual information in the forms of mRMR of MIFS (ignoring the trivial solution that reduces mRMR or MIFS to MaxRelevance).\nTheorem 1. Ignoring the trivial solution, i.e., the i-map indicating that random variables are mutually and class conditionally independent, there is no consistent i-map that can produce mutual information functions in the forms of mRMR (16) or MIFS (17) for arbitrary number of features.\nProof: The proof is by contradiction. Suppose there is a consistent i-map, where its corresponding joint pdf P\u0302 r(X, C) (which is the approximation of Pr(X, C)) can generate mutual information in the forms of (16) or (17). That is, if this i-map is adopted, by replacing P\u0302 r(X, C)\nin (1) we get mRMR or MIFS. This implies that mRMR and MIFS are always valid set measures for all datasets regardless of their true underlying joint probability distributions. Now, if we show (by any example) that they are not valid mutual information measures, i.e., they are not always positive and monotonic, then we have contradicted our assumption that P\u0302 r(X, C) exists and is a valid pdf. It is not so difficult to construct an example in which mRMR or MIFS can get negative values. Consider the case where features are independent of class label, I(Xi;C) = 0, while they have nonzero dependencies among themselves, I(Xi;Xj) 6= 0. In this case, both mRMR and MIFS generate negative values which is not allowed by a valid set measure. This contradicts our assumption that they are generated by a valid distribution, so we are forced to conclude that there is no consistent i-map that results in mutual information in the mRMR or MIFS forms.\nThe same line of reasoning can be used to show that D1 and D2 are also not valid measures.\nHowever, despite the fact that no valid pdf can produce mutual information of those forms, it is still valid to ask for which low-order approximations of the underlying high-order pdfs, mutual information reduces to a truncated approximation form. That is, we do not restrict an approximation to be a valid distribution anymore. Any functional form of low-order pdfs may be seen as an approximation of the high-order pdfs and may give rise to MIFS or mRMR. In the next subsection we reveal these assumptions for the MIFS criterion."}, {"heading": "2.3.3 MIFS Derivation from Kirkwood Approximation", "text": "It is shown in [26] that truncation of the joint entropy H(X) at the rth-order is equivalent to approximating the full-dimensional pdf Pr(X) using joint pdfs with dimensionality of r or smaller. This approximation is called rth order Kirkwood approximation. The truncation order that we choose, partially determines our belief about the structure of the function that we are going to estimate the exact Pr(X) with.\nThe 2nd order Kirkwood approximation of Pr(X), can be denoted as follows [26]:\nP\u0302 r(X) =\n\u220fN\u22121 i=1 \u220fN j=i+1 Pr(Xi, Xj)\n[ \u220fN i=1 Pr(Xi) ]N\u22122\n(18)\nNow, assume the following two assumptions hold:\nAssumption 1: Features are class conditionally independent, that is: Pr(X|C) =\n\u220fN i=1 Pr(Xi|C)\nAssumption 2: Pr(X) is well approximated by a 2nd order Kirkwood superposition approximation in (18).\nThen, writing the definition of mutual information and\n7 applying the above assumptions yields the MIFS criterion\nI(X;C) = H(X)\u2212H(X|C) (19)\n(a) \u2248\nN \u2211\ni=1\nH(Xi)\u2212 N\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi;Xj)\u2212H(X|C)\n(b) =\nN \u2211\ni=1\nI(Xi;C)\u2212 N\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi;Xj)\nIn the above equation, (a) follows the second assumption by substituting the 2nd order Kirkwood approximation (18) inside the logarithm of the entropy integral and (b) is an immediate consequence of the first assumption.\nThe first assumption has already appeared in previous works [9] [4]. However, the second assumption is novel and, to the best of our knowledge, the connection between the Kirkwood approximation and the MIFS criterion has not been explored before.\nIt is worth to mention that, in reality, both assumptions can be violated. Specifically, the Kirkwood approximation may not precisely reproduce dependencies we might observe in real-world datasets. Moreover, it is important to remember that the Kirkwood approximation is not a valid probability distribution."}, {"heading": "2.4 D2 Approximation", "text": "From our experiments, which we omit because of space constraints, D2 tends to underestimate the mutual information while D1 shows a large overestimation for independent features and a large underestimation (even becoming negative) in the presence of dependent features. In general, D2 shows more robustness than D1. The same results can be observed for mRMR which is derived from D2 and MIFS derived from D1. Previous work also arrived to the same results and reported that mRMR performs better and more robustly than MIFS especially when the feature set is large. Therefore, in the following sections we use D2 as the truncated approximation. For simplicity, its subscript is dropped and it is rewritten as follows:\nD({X1, . . . , XN}) = N \u2211\ni=1\nI(Xi;C) (20)\n\u2212 1\nN \u2212 1\nN\u22121 \u2211\ni=1\nN \u2211\nj=i+1\nI(Xi;Xj ;C)\nNote that although D in (20) is not a formal set measure any more, it still can be seen as a score function for sets. However, it is noteworthy that unlike formal measures, the suggested approximations are no longer monotonic where the monotonicity merely means that a subset of features should not be better than any larger set that contains the very same subset. Therefore, as explained in [33] the branch and bound based search strategies can not be applied to them.\nA very similar approach has been applied [8] (by using D1 approximation) to derive several known criteria like MIFS [5] and mRMR [35]. However, in [8] and most of other previous works, the set score function in (20) is immediately reduced to an individual-feature score function by fixing N\u22121 features in the feature set. This will let them to run a greedy selection search method over the feature set which essentially is a one-feature-at-a-time selection strategy. It is clearly a naive approximation of the optimal NP-hard search algorithm and may perform poorly under some conditions. In the following, we investigate a convex approximation of the binary objective function appearing in feature selection inspired by the Goemans-Williamson maximum cut approximation approach [18]."}, {"heading": "3 SEARCH STRATEGIES", "text": "Given a measure function1 D, the Subset Selection Problem (SSP) can be defined as follows:\nDefinition 2: Given N features Xi and a dependent variable C, select a subset of P \u226aN features that maximizes the measure function. Here it is assumed that the cardinality P of the optimal feature subset is known.\nIn practice, the exact value of P can be obtained by evaluating subsets for different values of cardinality P with the final induction algorithm. Note that it is intrinsically different than wrapper methods. While in wrapper methods 2N subsets have to be tested, here at most N runs of the learning algorithm are needed to evaluate all possible values of P .\nA search strategy is an algorithm trying to find a feature subset in the feature subset space with 2N members2 that optimizes the measure function. The wide range of proposed search strategies in the literature can be divided into three categories: 1- Exponential complexity methods including exhaustive search [27], branch and bound based algorithms [33]. 2- Sequential selection strategies with two very popular members, forward selection and backward elimination methods. 3- Stochastic methods like simulated annealing and genetic algorithms [41], [12].\nHere, we introduce a fourth class of search strategies which is based on the convex relaxation of the 0-1 integer programming and explore its approximation ratio by establishing a link between SSP and an instance of the maximum-cut problem in graph theory. In the following, we briefly discuss the two popular sequential search methods and continue with the proposed solution: a close to optimal polynomial-time complexity search algorithm and its evaluation on different datasets.\n1By some abuse of terminology, we refer to any set function in this section as a measure, no matter whether they satisfy the formal measure properties.\n2Given a P , the size of the feature subset space reduces to ( N\nP\n)\n.\n8"}, {"heading": "3.1 Convex Based Search", "text": "The forward selection (FS) algorithm selects a set S of size P iteratively as follows:\n1) Initialize S0 = \u2205. 2) In each iteration i, select the feature Xm maximizing\nD(Si\u22121 \u222aXm), and set Si = Si\u22121 \u222aXm. 3) Output SP .\nSimilarly, backward elimination (BE) can be described as:\n1) Start with the full set of feature SN . 2) Iteratively remove a variable Xm maximizing\nD(Si\\Xm), and set Si\u22121 = Si\\Xm, where removing X from S is denoted by S\\X .\n3) Output SP .\nAn experimentally comparative evaluation of several variants of these two algorithms has been conducted in [2]. From an information theoretical standpoint, the main disadvantage of the forward selection method is that it only can evaluate the utility of a single feature in the limited context of the previously selected features. The artificial binary classifier in Figure 1 may illustrate this issue. Since the information content of each feature (x and y) is almost zero, it is highly probable that the forward selection method fails to select them in the presence of some other more informative features.\nContrary to forward selection, backward elimination can evaluate the contribution of a given feature in the context of all other features. Perhaps this is why it has been frequently reported to show superior performance than forward selection. However, its overemphasis on feature interactions is a double-edged sword and may lead to a sub-optimal solution.\nExample 3: Imagine a four dimensional feature selection problem where X1 and X2 are class conditionally and mutually independent of X3 and X4, i.e., Pr(X1, X2, X3, X4) = Pr(X1, X2)Pr(X3, X4) and Pr(X1, X2, X3, X4|C) = Pr(X1, X2|C)Pr(X3, X4|C). Consider I(X1;C) and I(X2;C) are equal to zero, while their interaction is informative. That is, I(X1, X2;C) = 0.4. Moreover, assume I(X3;C) = 0.2, I(X4;C) = 0.25 and I(X3, X4;C) = 0.45. The goal is to select only two features out of four. Here, backward elimination will select {X1, X2} rather than the optimal subset {X3, X4} because, removing any of X1 or X2 features will result in 0.4 reduction of the mutual information value I(X1, . . . , X4;C), while eliminating X3 or X4 deducts at most 0.25. One may draw the conclusion that backward elimination tends to sacrifice the individually-informative features in favor of the merely cooperatively-informative features. As a remedy, several hybrid forward-backward sequential search methods have been proposed. However, they all fail in one way or another and more importantly cannot guarantee the goodness of the solution.\nAlternatively, a sequential search method can be seen as an approximation of the combinatorial subset selection problem. To propose a new approximation method, the underlying combinatorial problem has to be studied. To this end, we may formulate the SSP defined in the beginning of this section as:\nmax x\nxTQx\nN \u2211\ni=1\nxi = P (21)\nxi \u2208 {0, 1} for i = 1, . . . , N\nwhere Q is a symmetric mutual information matrix constructed from the mutual information terms in (20):\nQ =\n\n   \nI(X1;C) \u00b7 \u00b7 \u00b7 \u2212 \u03bb 2 I(X1;XN ;C) \u2212\u03bb2 I(X1;X2;C) \u00b7 \u00b7 \u00b7 \u2212 \u03bb 2 I(X2;XN ;C)\n... . . . ... \u2212\u03bb2 I(X1;XN ;C) \u00b7 \u00b7 \u00b7 I(XN ;C)\n\n   \n(22) where \u03bb = 1P\u22121 and x = [x1, . . . , xN ] is a binary vector where the variables xi are set-membership binary variables indicating the presence of the corresponding features Xi in the feature subset. It is straightforward to verify that for any binary vector x, the objective function in (21) is equal to the score function D(Xnz) where Xnz = {Xi|xi = 1; i = 1, . . . , N}. Note that, for mRMR I(Xi;Xj;C) terms have to be replaced with I(Xi;Xj).\nThe (0,1)-quadratic programming problem (21) has attracted a great deal of theoretical study because of its importance in combinatorial problems [36, and references therein]. This problem can simply be transformed to a (-1,1)-quadratic programming problem,\nmax y\n1 4 yTQy + 1 2 yTQe+ c\nN \u2211\ni=1\nyi = 2P \u2212N (23)\nyi \u2208 {\u22121, 1} for i = 1, . . . , N\nvia y = 2x \u2212 e transformation, where e is an all ones vector. Additionally c in the above formulation is a constant equal to 14e\nTQe and it can be ignored because of its independence of y. In order to homogenize the objective function in (23), define an (N+1)\u00d7(N+1) matrix Qu by adding a 0-th row and column to Q so that:\nQu =\n(\n0 eTQ QTe Q\n)\n(24)\nIgnoring the constant factor 14 in (23), the equivalent homogeneous form of (21) can be written as:\nSSSP = max y\nyTQuy\n\u3008SSP\u3009 N \u2211\ni=1\nyiy0 = 2P \u2212N (25)\nyi \u2208 {\u22121, 1} for i = 0, . . . , N\n9 Note that y is now an N + 1 dimensional vector with the first element y0 = \u00b11 as a reference variable. Given the solution of the problem above, i.e., y, the optimal feature subset is obtained by Xop = {Xi|yi = y0}.\nThe optimization problem in (25) can be seen as an instance of the maximum-cut problem [18] with an additional cardinality constraint, also known as the k-heaviest subgraph or maximum partitioning graph problem. The two main approaches to solve this combinatorial problem are either to use the linear programming relaxation by linearizing the product of two binary variables [16], or the semidefinite programming (SDP) relaxation suggested in [18]. The SDP relaxation has been proved to have exceptionally high performance and achieves the approximation ratio of 0.878 for the original maximum-cut problem. The SDP relaxation of (25) is:\nSSDP = max Y\ntr{QuY}\nN \u2211\ni,j=1\nYij = (2P \u2212N) 2\n\u3008SDP\u3009 N \u2211\ni=1\nYi0 = (2P \u2212N) (26)\ndiag(Y) = e\nY 0\nwhere Y is an unknown (N + 1) \u00d7 (N + 1) positive semidefinite matrix and tr{Y} denotes its trace. Obviously, any feasible solution y for \u3008SSP\u3009is also feasible for its SDP relaxation by Y = yyT . Furthermore, it is not difficult to see that any rank one solution, rank(Y) = 1, of \u3008SDP\u3009is a solution of \u3008SSP\u3009.\nThe \u3008SDP\u3009problem can be solved within an additive error \u03b3 of the optimum by for example interior point methods [7] whose computational complexity are polynomial in the size of the input and log( 1\u03b3 ). However, since its solution is not necessarily a rank one matrix, we need some more steps to obtain a feasible solution for \u3008SSP\u3009. The following three steps summarize the approximation algorithm for \u3008SSP\u3009which in the following will be referred to as convex based relaxation approximation (COBRA) algorithm.\nCOBRA Algorithm:\n1) SDP: Solve \u3008SDP\u3009and obtain Ysdp. Repeat the following steps many times and output the best solution. 2) Randomized rounding: Using the multivariate normal distribution with a zero mean and a covariance matrix R = Ysdp to sample u from distribution N (0,R) and construct x\u0302 = sign(u). Select X = {Xi|x\u0302i = x\u03020}. 3) Size adjusting: By using the greedy forward or backward algorithm, resize the cardinality of X to P .\nThe randomized rounding step is a standard procedure to produce a binary solution from the real-valued solution\nof \u3008SDP\u3009and is widely used for designing and analyzing approximation algorithms [37]. The third step is to construct a feasible solution that satisfies the cardinality constraint. Generally, it can be skipped since in feature selection problems the exact satisfaction of the cardinality constraint is not required.\nWe use the SDP-NAL solver [43] with the Yalmip interface [29] to implement this algorithm in Matlab. SDP-NAL uses the Newton-CG augmented Lagrangian method to efficiently solve SDP problems. It can solve large scale problems (N up to a few thousand) in an hour on a PC with an Intel Core i7 CPU. Even more efficient algorithms for low-rank SDP have been suggested claiming that they can solve problems with the size up to N=30000 in a reasonable amount of time [19]. Here we only use the SDPNAL for our experiments."}, {"heading": "3.2 Approximation Analysis", "text": "In order to gain more insight into the quality of a measure function, it is essential to be able to directly examine it. However, since estimating the exact mutual information value in real data is not feasible, it is not possible to directly evaluate the measure function. Its quality can only be indirectly examined through the final classification performance (or other measurable criteria). However, the quality of a measure function is not the only contributor to the classification rate. Since SSP is an NP-hard problem, the search strategy can only find a local optimal solution. That is, besides the quality of a measure function, the inaccuracy of the search strategy also contributes to the final classification error. Thus, in order to draw a conclusion concerning the quality of a measure function, it is essential to have an insight about the accuracy of the search strategy in use. In this section, we compare the accuracy of the proposed method with the traditional backward elimination approach.\nA standard approach to investigate the accuracy of an optimization algorithm is by analyzing how close it gets to the optimal solution. Unfortunately, feature selection is an NP-hard problem and thus achieving the optimal solution to use as reference is only feasible for small-sized problems. In such cases, one wants a provable solution\u2019s quality and certain properties about the algorithm, such as its approximation ratio. Given a maximization problem, an algorithm is called \u03c1-approximation algorithm if the approximate solution is at least \u03c1 times the optimal value. That is, in our case \u03c1SSSP \u2264 SCOBRA, where SCOBRA = D(XCOBRA). The factor \u03c1 is usually referred to as the approximation ratio in the literature.\nThe approximation ratios of BE and COBRA can be found by linking the SSP to the k-heaviest subgraph problem (kHSP) in graph theory. k-HSP is an instance of the maxcut problem with a cardinality constraint on the selected subset, that is, to determine a subset S of k vertices such\n10\nthat the weight of the subgraph induced by S is maximized [40]. From the definition of k-HSP, it is clear that SSP with the criterion (20) is equivalent to the P -heaviest subgraph problem since it selects the heaviest subset of features with the cardinality P , where heaviness of a set is the score assigned to it by D.\nAn SDP based algorithm for k-HSP has been suggested in [40] and its approximation ratio has been analyzed. Their results are directly applicable to COBRA since both algorithms use the same randomization method (step 2 of COBRA) and the randomization is the main ingredient of their approximation analysis. The approximation ratio of BE for k-HSP has been investigated in [3]. It is a deterministic analysis and their results are also valid for our case, i.e., using BE for maximizing D.\nThe approximation ratios of both algorithms for different values of P , as a function of N (total number of features), have been listed in Table 1 (values are calculated from the formulas in [3]). As can be seen, as P becomes smaller, the approximation ratio approaches zero yielding the trivial lower bound 0 on the approximate solution. However, for larger values of P , the approximation ratio is nontrivial since it is bounded away from zero. For all cases shown in the table except the last one, COBRA gives better guarantee bound than BE. Thus, we may conclude that COBRA is more likely to achieve better approximate solution than BE.\nIn the following section, we will focus on comparing our search algorithm with sequential search methods in conjunction with different measure functions and over different classifiers and datasets."}, {"heading": "4 EXPERIMENTS", "text": "The evaluation of a feature selection algorithm is an intrinsically difficult task since there is no direct way to evaluate the goodness of a selection process in general. Thus, usually a selection algorithm is scored based on the performance of its output, i.e., the selected feature subset, in some specific classification (regression) system. This kind of evaluation can be referred to as the goal-dependent evaluation. However, this method obviously cannot evaluate the generalization power of the selection process on different induction algorithms. To evaluate the generalization strength of a feature selection algorithm, we need a goal-independent evaluation. Thus, for evaluation of the feature selection algorithms, we propose to compare the algorithms over different datasets with multiple classifiers. This method leads to a more classifier-independent evaluation process.\nSome properties of the eight datasets used in the experiments are listed in Table 2. All datasets are available on\nthe UCI machine learning archive [14], except the NCI data which can be found in the website of Peng et al. [35]. These datasets have been widely used in previous feature selection studies [35], [10]. The goodness of each feature set is evaluated with five classifiers including Support Vector Machine (SVM), Random Forest (RF), Classification and Regression Tree (CART), Neural Network (NN) and Linear Discriminant Analysis (LDA). To derive the classification accuracies, 10-fold cross-validation is performed except for the NCI, DBW and LNG datasets where leave-one-out cross-validation is used.\nAs explained before, filter-based methods consist of two components: A measure function and a search strategy. The measure functions we use for our experiments are mRMR and JMI defined in (16) and (15), respectively. To unambiguously refer to an algorithm, it is denoted by measure function + search method used in that algorithm, eg., mRMR+FS.\nA simple algorithm listed in Table 3 is employed to search for the optimal value of the subset cardinality P , where P ranges over a set P of admissible values. In the worst case, P = {1, . . . , N}.\nTable 4 shows the results obtained for the 8 datasets and 5 classifiers. Friedman test with the corresponding WilcoxonNemenyi post-hoc analysis was used to compare the different algorithms. However, looking at the classification rates even before running the Friedman tests on them reveals a few interesting points which are marked in bold font.\nFirst, on the small size datasets (NCI, DBW and LNG), mRMR+COBRA consistently shows higher performance than other algorithms. The reason lies in the fact that the similarity ratio of the feature sets selected by COBRA is lower than BE or FS feature sets. The similarity ratio Si is defined as the number of features in the intersection of ith and i+1th feature sets divided by the cardinality of the ith feature set. From its definition it is clear that for BE and FS this ratio is always equal to 1. However, because of the\n11\nrandomization step this ratio may widely vary for COBRA. That is, COBRA generates quite diverse feature sets. Some of these feature sets have relatively low scores as compared with BE or FS sets. However, since for small datasets the estimated mutual information terms are highly inaccurate, features that rank low with our noisy measure function may in fact be better for classification. The average of the similarity ratios of 50 subsequent feature sets ( 150 \u221155 i=5 Si) have been reported for 4 datasets in Table 5. As seen, for NCI the averaged similarity ratio is significantly smaller than 1 while for CNA which is a relatively larger dataset, it is almost constant and equal to 1.\nThe second interesting point is with respect to the Madelon dataset. As can be seen, mRMR with greedy search algorithms perform poorly on this dataset. Several authors have already utilized this dataset to compare their proposed criterion with mRMR and arrived at the conclusion that mRMR cannot handle highly correlated features, as in Madelon dataset. However, surprisingly the performance of the mRMR+COBRA is as good as JMI on this dataset meaning that it is not the criterion but the search method that has difficulty to deal with highly correlated features. Thus, any conclusion with respect to the quality of a measure has to be drawn carefully since, as in this case, the effect of the non optimum search method can be decisive.\nTo discover the statistically meaningful differences between the algorithms, we applied the Friedman test following with\nWilcoxon-Nemenyi post-hoc analysis, as suggested in [23], on the average accuracies (the last column of Table 4). Note that since we have 8 datasets, there are 8 independent measurements available for each algorithm. The results of this test for mRMR based algorithms have been depicted in Figure 2. In all box plots, CO stands for COBRA algorithm. Each box plot compares a pair of the algorithms. The green box plots represent the existence of a significant difference between the corresponding algorithms. The adjusted pvalues for each pair of algorithms have also been reported in Figure 2. The smaller the p-value, the stronger the evidence against the null hypothesis. As can be seen, COBRA shows meaningful superiority over both greedy algorithms. However, if we set the significance level at p = 0.05, only FS rejects the null hypothesis and shows a meaningful difference with COBRA.\nThe same test was run for each classifier and its results can be found in Figure 3. While three of the classifiers show some differences between FS and COBRA, neither of them reveal any meaningful difference between BE and COBRA. At this point, the least we can conclude is that independent of the classification algorithm we choose, it is a good chance that FS performs worse than COBRA.\nFor JMI, however, the performances of all algorithms are comparable and with only 8 datasets it is difficult to draw any conclusion. Thus, the Wilcoxon-Nemenyi test results for JMI is not shown here because of the lack of space.\nIn the next experiment COBRA is compared with two other convex programming based feature selection algorithms, SOSS [32] and QPFS [39]. Both SOSS and QPFS employ quadratic programing techniques to maximize a score function. SOSS, however, uses an instance of ran-\n13\ndomized rounding to generate the set-membership binary values while QPFS ranks the features based on their scores (achieved from solving the convex problem) and therefore, sidesteps the difficulties of generating binary values. Note that both COBRA and SOSS first need to calculate the mutual information matrix Q. Once it is calculated, they can solve their corresponding convex optimization problems for different values of P . The first 3 rows of Table 6 report the average (over 5 classifiers) classification accuracies of these three algorithms and the standard deviation of these mean accuracies (calculated over the cross-validation folds). In the next three rows of the table, the computational times of each algorithm for a single run (in second) are shown, i.e., the amount of time needed to select a feature set with (given) P features. The reported times for COBRA and SOSS consist of two values. The first value is the time needed to calculate the mutual information matrix Q and the second value is the amount of time needed to solve the corresponding convex optimization problem. All the values were measured on a PC with an Intel Core i7 CPU. As seen, QPFS is significantly faster than COBRA and SOSS. This computational superiority, however, seems to come at the expense of lower classification accuracy. For large datasets such as IAD, CNA and MAD, the Nystro\u0308m approximation used in QPFS to cast the problem into a lower dimensional subspace does not yield a precise enough approximation and results in lower classification accuracies. An important remark to interpret these results is that, for NCI dataset (in all the experiments) we first filtered out the features with the low mutual information values with the class label and only kept 2000 informative features (similarly for DEX and DBW datasets). Thus, the dimension is 2000 and not 9703 as mentioned in Table 2.\nThe generalization power of the COBRA algorithm over different classifiers is another important issue to test. As can be observed in Table 4, the number of selected features varies quite markedly from one classifier to another. However, based on our experiments, the optimum feature set of any of the classifiers, usually (for large enough datasets) achieves a near-optimal accuracy in conjunction with other classifiers as well. This is shown in Table 7 for 4 classifiers and 3 datasets. The COBRA features of the LDA classifier in Table 4 is used here to train other classifiers. Table 7 lists the accuracies obtained by using the LDA features and the optimal features, repeated from Table 4. Unlike the CNA\nand IAD datasets, a significant accuracy reduction can be observed in the case of ARR data which has substantially less training data than CNA and IAD. It suggests that for small size datasets, a feature selection scheme should take the induction algorithm into account since the learning algorithm is sensitive to small changes of the feature set."}, {"heading": "5 CONCLUSION", "text": "A convex based parallel search strategy for feature selection, COBRA, was suggested in this work. Its approximation ratio was derived and compared with the approximation ratio of the backward elimination method. It was experimentally shown that COBRA outperforms sequential search methods especially in the case of sparse data. Moreover, we presented two series expansions for mutual information, and showed that most mutual information based score functions in the literature including mRMR and MIFS are truncated approximations of these expansions. Furthermore, the underlying connection between MIFS and the Kirwood approximation was explored, and it was shown that by adopting the class conditional independence assumption and the Kirkwood approximation for Pr(X), mutual information reduces to the MIFS criterion."}, {"heading": "6 ACKNOWLEDGMENTS", "text": "This work has partly been supported by Swiss National Science Foundation (SNSF)."}], "references": [{"title": "Information theory and coding", "author": ["N. Abramson"], "venue": "McGraw-Hill, New York", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1963}, {"title": "A comparative evaluation of sequential feature selection algorithms", "author": ["D.W. Aha", "R.L. Bankert"], "venue": "Learning from Data: Artificial Intelligence and Statistics V. Springer-Verlag", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Greedily finding a dense subgraph", "author": ["Y. Asahiro", "K. Iwama", "H. Tamaki", "T. Tokuyama"], "venue": "Journal of Algorithms", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "On the feature selection criterion based on an approximation of multidimensional mutual information", "author": ["K.S. Balagani", "V.V. Phoha"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Trans. on Neural Networks, 5:537\u2013550", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Measure Theory", "author": ["V.I. Bogachev"], "venue": "Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A new perspective for information theoretic feature selection", "author": ["G. Brown"], "venue": "Proceedings of Artificial Intelligence and Statistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection", "author": ["G. Brown", "A. Pocock", "M. Zhao", "M. Luj\u00e1n"], "venue": "Journal of Machine Learning", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "An evolving system based on probabilistic neural network", "author": ["P.M. Ciarelli", "E.O.T. Salles", "E. Oliveira"], "venue": "Proceedings of BSNN", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "An evaluation of feature selection methods and their application to computer security", "author": ["J. Doak"], "venue": "Technical Report CSE-92-18, University of California at Davis", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Transmission of Information: A Statistical Theory of Communications", "author": ["R. Fano"], "venue": "The MIT Press, Cambridge, MA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1961}, {"title": "A projection pursuit algorithm for exploratory data analysis", "author": ["J.H. Friedman", "J.W. Tukey"], "venue": "IEEE Trans. on Computers", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1974}, {"title": "On the quadratic assignment problem", "author": ["A.M. Frieze", "J. Yadegar"], "venue": "Discrete Applied Mathematics", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "On the potential inadequacy of mutual information for feature selection", "author": ["B. Fr\u00e9nay", "G. Doquire", "M. Verleysen"], "venue": "Proceedings of ESANN", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "SpeeDP: an algorithm to compute SDP bounds for very large maxcut instances", "author": ["L. Grippo", "L. Palagi", "M. Piacentini", "V. Piccialli", "G. Rinaldi"], "venue": "Mathematical Programming", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal feature extraction and fusion for audiovisual speech recognition", "author": ["M. Gurban"], "venue": "PhD thesis, 4292, STI, EPF Lausanne", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiple mutual informations and multiple interactions in frequency data", "author": ["T.S. Han"], "venue": "Information and Control", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1980}, {"title": "Probability of error", "author": ["M. Hellman", "J. Raviv"], "venue": "equivocation and the Chernoff bound. IEEE Trans. on Information Theory", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1970}, {"title": "Nonparametric Statistical Methods", "author": ["M. Hollander", "D.A. Wolfe"], "venue": "2nd Edition. Wiley-Interscience", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "What are the differences between Bayesian classifiers and mutual-information classifiers? IEEE Trans", "author": ["B.G. Hu"], "venue": "on Neural Networks and Learning Systems", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Markov networks: Maximum bounded tree-width graphs", "author": ["D. Karger", "N. Srebro"], "venue": "Proceedings of the 12th Annual Symposium on Discrete Algorithms, pages 392\u2013401", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Extraction of configurational entropy from molecular simulations via an expansion approximation", "author": ["B.J. Killian", "J.Y. Kravitz", "M.K. Gilson"], "venue": "Journal of Chemical Physics", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Wrappers for performance enhancement and oblivious decision graphs", "author": ["R. Kohavi"], "venue": "PhD thesis, Stanford, CA, USA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Input feature selection for classification problems", "author": ["N. Kwak", "C. Choi"], "venue": "IEEE Trans. on Neural Networks", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Yalmip: a toolbox for modeling and optimization in matlab", "author": ["J. Lofberg"], "venue": "Proceedings of Computer Aided Control Systems Design", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Multivariate information transmission", "author": ["W. McGill"], "venue": "IRE Professional Group on Information Theory", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1954}, {"title": "On the Use of Variable Complementarity for Feature Selection in Cancer Classification", "author": ["P. Meyer", "G. Bontempi"], "venue": "Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex approximation of the NP-hard search problem in feature subset selection", "author": ["T. Naghibi", "S. Hoffmann", "B. Pfister"], "venue": "Proceedings of ICASSP", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A branch and bound algorithm for feature subset selection", "author": ["P.M. Narendra", "K. Fukunaga"], "venue": "IEEE Trans. on Computers", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1977}, {"title": "SVM-based feature selection by direct objective minimisation", "author": ["J. Neumann", "C. Schn\u00f6rr", "G. Steidl"], "venue": "Proceedings of DAGM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Feature selection based on mutual information: criteria of max-dependency", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "max-relevance, and min-redundancy. IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "and H", "author": ["S. Poljak", "F. Rendl"], "venue": "Wolkowicz. A recipe for semidefinite relaxation for (0,1)-quadratic programming. Journal of Global Optimization", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1995}, {"title": "Probabilistic construction of deterministic algorithms: approximating packing integer programs", "author": ["P. Raghavan"], "venue": "Journal of Computer and System. Sciences", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1988}, {"title": "An Introduction to Information Theory", "author": ["F.M. Reza"], "venue": "Dover Publications, Inc., New York", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1961}, {"title": "Ch", "author": ["I. Rodriguez-Lujan", "R. Huerta"], "venue": "Elkan, and C. S. Cruz. Quadratic programming feature selection. Journal of Machine Learning Research", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding dense subgraphs with semidefinite programming", "author": ["A. Srivastav", "K. Wolf"], "venue": "Approximation Algorithms for Combinatiorial Optimization. Springer", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Robust feature selection algorithms", "author": ["H. Vafaie", "K. De Jong"], "venue": "Proceedings of TAI", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1993}, {"title": "A new outlook on Shannon\u2019s information measures", "author": ["R.W. Yeung"], "venue": "IEEE Trans. on Information Theory", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1991}, {"title": "A Newton-CG augmented Lagrangian method for semidefinite programming", "author": ["X. Zhao", "D. Sun", "K. Toh"], "venue": "SIAM J. on Optimization", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Wrapper methods [27] use the performance of an induction algorithm (for instance a classifier) as the measure function.", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "The second group of feature selection methods are called embedded methods [34] and are based on some internal parameters of the ML algorithm.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "The third group of the search algorithms are based on targeted projection pursuit which is a linear mapping algorithm to pursue an optimum projection of data onto a low dimensional manifold that scores highly with respect to a measure function [15].", "startOffset": 244, "endOffset": 248}, {"referenceID": 37, "context": "Recently, two convex quadratic programing based methods, QPFS in [39] and SOSS in [32] have been suggested to address the search problem.", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "Recently, two convex quadratic programing based methods, QPFS in [39] and SOSS in [32] have been suggested to address the search problem.", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "We reformulate the feature selection problem as a (0-1)-quadratic integer programming and will show that it can be relaxed to an SDP problem, which is convex and hence can be solved with efficient algorithms [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 10, "context": "The mutual information function is defined as a distance from independence between X and C measured by the Kullback-Leibler divergence [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 36, "context": "Mutual information can also be considered a measure of set intersection [38].", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": "satisfies all three properties of a formal measure over sets [42] [6], i.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "satisfies all three properties of a formal measure over sets [42] [6], i.", "startOffset": 66, "endOffset": 69}, {"referenceID": 12, "context": "More specifically, Fano\u2019s weak lower bound [13] on Pe,", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "1 + Pelog2(ny\u22121) \u2265 H(C)\u2212 I(X;C) (3) where ny is the number of classes and the Hellman-Raviv [22] upper bound,", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "A more detailed discussion can be found in [17].", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "Because to calculate mutual information, estimating the high-dimensional joint probability Pr(X, C) is inevitable which is, in turn, known to be an NP hard problem [25].", "startOffset": 164, "endOffset": 168}, {"referenceID": 4, "context": "As mutual information is hard to evaluate, several alternatives have been suggested [5], [35], [28].", "startOffset": 84, "endOffset": 87}, {"referenceID": 33, "context": "As mutual information is hard to evaluate, several alternatives have been suggested [5], [35], [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 26, "context": "As mutual information is hard to evaluate, several alternatives have been suggested [5], [35], [28].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "The first expansion of mutual information that is used here, relies on the natural extension of mutual information to more than two random variables proposed by McGill [30] and Abramson [1].", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "The first expansion of mutual information that is used here, relies on the natural extension of mutual information to more than two random variables proposed by McGill [30] and Abramson [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 19, "context": "Unlike 2-way mutual information, the generalized mutual information is not necessarily nonnegative and hence, can be interpreted as a signed measure of set intersection [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "This means, the information contained in the interactions of the variables is greater than the sum of the information of the individual variables [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "The second expansion for mutual information is based on the chain rule of information [11]:", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "Several known criteria including Joint Mutual Information (JMI) [31], minimal Redundancy Maximal Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can immediately be derived from D1 and D2.", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "Several known criteria including Joint Mutual Information (JMI) [31], minimal Redundancy Maximal Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can immediately be derived from D1 and D2.", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "Several known criteria including Joint Mutual Information (JMI) [31], minimal Redundancy Maximal Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can immediately be derived from D1 and D2.", "startOffset": 167, "endOffset": 170}, {"referenceID": 33, "context": "in [35].", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "That is, mRMR is a truncated approximation of mutual information and not a heuristic approximation as suggested in [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "The question regarding the implicit consistent i-map that MIFS adopts has been investigated in [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 24, "context": "It is shown in [26] that truncation of the joint entropy H(X) at the rth-order is equivalent to approximating the full-dimensional pdf Pr(X) using joint pdfs with dimensionality of r or smaller.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "The 2nd order Kirkwood approximation of Pr(X), can be denoted as follows [26]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "The first assumption has already appeared in previous works [9] [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "The first assumption has already appeared in previous works [9] [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 31, "context": "Therefore, as explained in [33] the branch and bound based search strategies can not be applied to them.", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "A very similar approach has been applied [8] (by using D1 approximation) to derive several known criteria like MIFS [5] and mRMR [35].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "A very similar approach has been applied [8] (by using D1 approximation) to derive several known criteria like MIFS [5] and mRMR [35].", "startOffset": 116, "endOffset": 119}, {"referenceID": 33, "context": "A very similar approach has been applied [8] (by using D1 approximation) to derive several known criteria like MIFS [5] and mRMR [35].", "startOffset": 129, "endOffset": 133}, {"referenceID": 7, "context": "However, in [8] and most of other previous works, the set score function in (20) is immediately reduced to an individual-feature score function by fixing N\u22121 features in the feature set.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "In the following, we investigate a convex approximation of the binary objective function appearing in feature selection inspired by the Goemans-Williamson maximum cut approximation approach [18].", "startOffset": 190, "endOffset": 194}, {"referenceID": 25, "context": "The wide range of proposed search strategies in the literature can be divided into three categories: 1- Exponential complexity methods including exhaustive search [27], branch and bound based algorithms [33].", "startOffset": 163, "endOffset": 167}, {"referenceID": 31, "context": "The wide range of proposed search strategies in the literature can be divided into three categories: 1- Exponential complexity methods including exhaustive search [27], branch and bound based algorithms [33].", "startOffset": 203, "endOffset": 207}, {"referenceID": 39, "context": "3- Stochastic methods like simulated annealing and genetic algorithms [41], [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "3- Stochastic methods like simulated annealing and genetic algorithms [41], [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "An experimentally comparative evaluation of several variants of these two algorithms has been conducted in [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 16, "context": "The optimization problem in (25) can be seen as an instance of the maximum-cut problem [18] with an additional cardinality constraint, also known as the k-heaviest subgraph or maximum partitioning graph problem.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "The two main approaches to solve this combinatorial problem are either to use the linear programming relaxation by linearizing the product of two binary variables [16], or the semidefinite programming (SDP) relaxation suggested in [18].", "startOffset": 163, "endOffset": 167}, {"referenceID": 16, "context": "The two main approaches to solve this combinatorial problem are either to use the linear programming relaxation by linearizing the product of two binary variables [16], or the semidefinite programming (SDP) relaxation suggested in [18].", "startOffset": 231, "endOffset": 235}, {"referenceID": 6, "context": "The \u3008SDP\u3009problem can be solved within an additive error \u03b3 of the optimum by for example interior point methods [7] whose computational complexity are polynomial in the size of the input and log( 1 \u03b3 ).", "startOffset": 111, "endOffset": 114}, {"referenceID": 35, "context": "The randomized rounding step is a standard procedure to produce a binary solution from the real-valued solution of \u3008SDP\u3009and is widely used for designing and analyzing approximation algorithms [37].", "startOffset": 192, "endOffset": 196}, {"referenceID": 41, "context": "We use the SDP-NAL solver [43] with the Yalmip interface [29] to implement this algorithm in Matlab.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "We use the SDP-NAL solver [43] with the Yalmip interface [29] to implement this algorithm in Matlab.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Even more efficient algorithms for low-rank SDP have been suggested claiming that they can solve problems with the size up to N=30000 in a reasonable amount of time [19].", "startOffset": 165, "endOffset": 169}, {"referenceID": 38, "context": "that the weight of the subgraph induced by S is maximized [40].", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "An SDP based algorithm for k-HSP has been suggested in [40] and its approximation ratio has been analyzed.", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "The approximation ratio of BE for k-HSP has been investigated in [3].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "The approximation ratios of both algorithms for different values of P , as a function of N (total number of features), have been listed in Table 1 (values are calculated from the formulas in [3]).", "startOffset": 191, "endOffset": 194}, {"referenceID": 33, "context": "[35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "These datasets have been widely used in previous feature selection studies [35], [10].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "These datasets have been widely used in previous feature selection studies [35], [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Wilcoxon-Nemenyi post-hoc analysis, as suggested in [23], on the average accuracies (the last column of Table 4).", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "In the next experiment COBRA is compared with two other convex programming based feature selection algorithms, SOSS [32] and QPFS [39].", "startOffset": 116, "endOffset": 120}, {"referenceID": 37, "context": "In the next experiment COBRA is compared with two other convex programming based feature selection algorithms, SOSS [32] and QPFS [39].", "startOffset": 130, "endOffset": 134}], "year": 2014, "abstractText": "Feature subset selection, as a special case of the general subset selection problem, has been the topic of a considerable number of studies due to the growing importance of data-mining applications. In the feature subset selection problem there are two main issues that need to be addressed: (i) Finding an appropriate measure function than can be fairly fast and robustly computed for high-dimensional data. (ii) A search strategy to optimize the measure over the subset space in a reasonable amount of time. In this article mutual information between features and class labels is considered to be the measure function. Two series expansions for mutual information are proposed, and it is shown that most heuristic criteria suggested in the literature are truncated approximations of these expansions. It is well-known that searching the whole subset space is an NP-hard problem. Here, instead of the conventional sequential search algorithms, we suggest a parallel search strategy based on semidefinite programming (SDP) that can search through the subset space in polynomial time. By exploiting the similarities between the proposed algorithm and an instance of the maximumcut problem in graph theory, the approximation ratio of this algorithm is derived and is compared with the approximation ratio of the backward elimination method. The experiments show that it can be misleading to judge the quality of a measure solely based on the classification accuracy, without taking the effect of the non-optimum search strategy into account.", "creator": "LaTeX with hyperref package"}}}