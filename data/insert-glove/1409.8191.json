{"id": "1409.8191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2014", "title": "A Neural Networks Committee for the Contextual Bandit Problem", "abstract": "implacably This pipi paper tinissa presents a new 4001 contextual bandit algorithm, NeuralBandit, thrapston which 64a does not donnalley need 180s hypothesis commercialism on andr\u00e9a stationarity of contexts steppe and kecil rewards. Several moakley neural phadungsil networks kinbote are commoners trained 45.97 to barmby modelize the synchronously value shal of rewards rosse knowing the snap context. bastareaud Two variants, elizaga based mehedin\u0163i on multi - hardworking experts 15-18 approach, soglin are costes proposed to choose online the parameters of multi - layer perceptrons. emil The proposed shonku algorithms are successfully klinikum tested on gummidipoondi a large megafaunal dataset passionless with hyper and chupke without gadonneix stationarity of colorization rewards.", "histories": [["v1", "Mon, 29 Sep 2014 17:08:21 GMT  (36kb,D)", "http://arxiv.org/abs/1409.8191v1", "21st International Conference on Neural Information Processing"]], "COMMENTS": "21st International Conference on Neural Information Processing", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["robin allesiardo", "raphael feraud", "djallel bouneffouf"], "accepted": false, "id": "1409.8191"}, "pdf": {"name": "1409.8191.pdf", "metadata": {"source": "CRF", "title": "A Neural Networks Committee for the Contextual Bandit Problem", "authors": ["Robin Allesiardo", "Rapha\u00ebl F\u00e9raud", "Djallel Bouneffouf"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In online decision problems such as online advertising or marketing optimization, a decision algorithm must select amoung several actions. Each of these options is associated with side information (profile or context) and the reward feedback is limited to the chosen option. For example, in online advertising, a visitor queries a web page; a request with the context (web page address, cookies, customer profile, etc.) is send to the server; the server sends an ad which is displayed on the page. If the visitor clicks on the ad the server receives a reward. The server must trade-off between the explorations of new ads and the exploitation of known ads. Moreover, in an actual applications, both rewards and data distributions can change with time. For instance, the display of a new ad can change the probability of clicks of all ads, the content of a web page can change over time. Robustness to non-stationarity is thus strongly recommended."}, {"heading": "2 Previous work", "text": "The multi-armed bandit problem is a model of exploration and exploitation where one player gets to pick within a finite set of decisions the one which maximizes the cumulated reward. This problem has been extensively studied. Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6]. Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]). However these approaches do not take into account the context while the arm\u2019s performance may be correlated therewith.\nA naive solution to the contextual bandit problem is to allocate one bandit problem for each context. A tree structured bandit such as X-armed bandits [11] or a UCT variant [12] can be used to explore and exploit a tree structure of contextual variables to\nar X\niv :1\n40 9.\n81 91\nv1 [\ncs .N\nE ]\n2 9\nSe p\n20 14\nfind the leaves which provide the highest rewards [13]. The combinatorial aspect of these approaches limits their use to small context size. Seldin et al [14] modelize the contexts by state sets, which are associated with bandit problems. Without prior knowledge of the contexts, it is necessary to use a state per context, which is equivalent to the naive approach. Dud\u0131\u0301k et al [15] propose an algorithm of policies elimination. The performance of this algorithm depends on the presence of a good policy in the set. The epoch-greedy algorithm [16] alternates exploration then exploitation. During exploration, the arms to play are randomly drawn to collect an unbiased training set. Then, this set is used to train a classifier which will be used for exploitation during the next cycle. The nature of the classifier remains to be defined for concrete use. In LINUCB [17,18] and in Contextual Thompson Sampling (CTS) [19], the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors. Banditron [20] uses a perceptron per action to recognize rewarded contexts. Furthermore, these algorithms assume that the data and the rewards are drawn from stationary distribution, which limits their practical use. The EXP4 algorithm [6] selects the best arm from N experts advices (probability vectors). Exploring the link between the rewards of each arm and the context is delegated to experts. Unlike the previous algorithms, the rewards are assumed to be chosen in advance by an adversary. Thus, this algorithm can be applied to non-stationnary data.\nAt first we will formalize the contextual bandit problem and will propose a first algorithm: NeuralBandit1. Inspired by Banditron [20] it estimates the probabilities of rewards by using neural networks in order to be free of the hypothesis of linear separability of the data. Neural networks are universal approximators [21]. They are used in reinforcement learning [22,23] and can estimate accurately the probabilities of rewards within actual and complex problems. In addition, the stochastic gradient achieves good performances in terms of convergence to the point of best generalization [24] and has the advantage of learning online. In seeking to reach a local minimum, the stochastic gradient can deal with non-stationarity. This will result in a change of the cost function landscape over the time. If this landscape changes at a reasonable speed, the algorithm will continue the descent to a new local minimum. The main issue raised by the use of multilayer perceptrons remains the online setting of various parameters such as the number of hidden neurons, the value of the learning step or the initalization of the weight seed. We propose two advanced versions of the algorithm NeuralBandit1 to adjust these settings using adversarial bandits that seek, among several models initialized with different parameters, the best one. We conclude by comparing these different approaches to the state-of-the-art on stationnary and on non-stationnary data."}, {"heading": "3 Our algorithm: NeuralBandit1", "text": "Definition 1 (Contextual bandit). Let xt \u2208 X be a contextual vector and (yt,1, ..., yt,K) a vector of rewards associated with the arm k \u2208 [K] = {1, ...,K} and ((x1, y1), ..., (xT , yT )) the sequence of contexts and rewards. The sequence can be drawn from a stochastic process or chosen in advance by an adversary. At each round t < T , the context xt is announced. The player, who aims to maximize his cumulated rewards, chooses an arm kt. The reward yt,k of the played arm, and only this one, is revealed.\nDefinition 2 (Cumulated regret). Let H : X \u2192 [K] be a set of hypothesis , ht \u2208 H a hypothesis computed by the algorithm A at the round t and h\u2217t = argmax\nht\u2208H yt,ht(xt)\nthe optimal hypothesis at the same round. The cumulated regret is:\nR(A) = T\u2211 t=1 yt,h\u2217t (xt) \u2212 yt,ht(xt)\nThe purpose of a contextual bandit algorithm is to minimize the cumulative regret.\nEach action k is associated with a neural network with one hidden layer which learns the probability of reward for an action knowing its context. We choose this modelization rather than one neural network with as many outputs as actions to be able to add or remove actions easier.\nLetK be the number of actions, C the number of neurons of each hidden layers and Nkt : X \u2192 Y the function associating a context xt to the output of the neural network corresponding to the action k at the round t. N denotes the number of connections of each network with N = dim(X)C + C. To simplify the notations, we place the set of connections in the matrix Wt of size K \u00d7N . Thus, each row of the matrix Wt contains the weight of a network. \u2206t is the matrix of size K \u00d7N containing the update of each weight between rounds t and t+ 1. The update equation is:\nWt+1 =Wt +\u2206t\nThe backpropagation algorithm [25] allows calculating the gradient of the error for each neuron from the last to the first layer by minimizing a cost function. Here, we use the quadratic error function and a sigmoid activation function.\nLet \u03bb be the learning step, x\u0302n,kt the input associated with the connection n in the network k, \u03b4n,kt the gradient of the cost function at round t for the neuron having as input the connection n in the network k and \u2206n,kt the value corresponding to the index (n, k) of the matrix \u2206t. When the reward of an arm is known, we can compute:\n\u2206n,kt = \u03bbx\u0302 n,k t \u03b4 n,k t\nIn the case of partial information, only the reward of the arm kt is available. To learn the best action to play, an approach consists of a first exploration phase, where each action is played the same number of times in order to train a model, and then an exploitation phase where the obtained model is used. Thus, the estimator would not be biased on the most played action. However, this approach would have abysmal performances in case of non-stationary data. We choose to use an exploration factor \u03b3, constant over time, allowing continuing the update of the model in the case of nonstationary data. The probability of playing the action k at round t knowing that k\u0302t is the arm with the highest reward prediction is:\nPt(k) = (1\u2212 \u03b3)1[k = k\u0302t] + \u03b3\nK\nWe propose a new update rule taking into account the exploration factor:\nWt+1 =Wt + \u2206\u0303t , (1)\nwith \u2206\u0303n,kt = \u03bbx\u0302n,kt \u03b4 n,k t 1[k\u0302t = k]\nPt(k)\nTheorem 1. The expected value of \u2206\u0303n,kt is \u2206 n,k t .\nThe proof is straightforward: E[\u2206\u0303n,kt ] = \u2211K k=1 Pt(k)( \u03bbx\u0302n,kt \u03b4 n,k t 1[k\u0302t=k] Pt(k) )\n= \u03bbx\u0302n,kt \u03b4 n,k t = \u2206n,kt\nut The proposed algorithm, NeuralBandit1, can adapt to non-stationarity by continuing to learn over time, while achieving the same expected result (in the case of stationary data) as a model trained in a first phase of exploration.\nAlgorithm 1: NeuralBandit1 Data: \u03b3 \u2208 [0, 0.5] et \u03bb \u2208]0, 1] begin\nInitialize W1 \u2208]\u2212 0.5, 0.5[N\u00d7K for t = 1, 2, ..., T do\nContext xt is revealed k\u0302t = arg max\nk\u2208[K] Nkt (xt)\n\u2200k \u2208 [K] on a Pt(k) = (1\u2212 \u03b3)1[k = k\u0302t] + \u03b3K k\u0303t is drawn from Pt k\u0303t is predicted and yt,k\u0303t is revealed Compute \u2206\u0303t such as \u2206\u0303n,kt = \u03bbx\u0302 n,k t \u03b4 n,k t 1[kt=k]\nPt(k)\nWt+1 =Wt + \u2206\u0303t"}, {"heading": "4 Models selection with adversarial bandit", "text": "Performances of neural networks are influenced by several parameters such as the learning step, the number of hidden layers, their size, and the initalization of weights. The multi-layer perceptron corresponding to a set of parameters is called model. Using batch learning, the models selection is done with a validation set. Using online learning, we propose to train the models in parallel and to use the adversarial bandit algorithm EXP3 [5,6] to choose the best model. The choice of an adversarial bandit algorithm is justified by the fact that the performance of each model changes overtime due to the learning itself or due to the non-stationarity of the data.\nExp3 Let \u03b3model \u2208 [0, 1] be an exploration parameter, wt = (w1t , ..., wMt ) a weight vector, where each of its coordinate is initialized to 1, and M the number of models. Let m be the model chosen at time t, and yt,m be the obtained reward. The probability to choose m at round t is:\nPmodelt(m) = (1\u2212 \u03b3model) wmt\u2211M i=1 w i t + \u03b3model M\n(2)\nThe weight update equation is:\nwit+1 = w i texp\n( \u03b3model1[i = m]yt,m\nPmodelt(i)M\n) (3)\nNeuralBandit2 (see Algorithm 2) If we consider that a model is an arm, a run of this model can be considered as a sequence of rewards. The algorithm takes as inputs a list of M models and a model exploration parameter \u03b3model. For each element of the list, one NeuralBandit1 instance is initialized. Each instance corresponds to an arm. EXP3 algorithm is used to choose the arms over time. After receiving a reward, each neural network corresponding to the played arm is updated, and the weights of EXP3 are updated.\nNeuralBandit3 (see Algorithm 3) The use of the algorithm NeuralBandit2 corresponds to the assumption that there is a model NeuralBandit1 which is the best for all actions. The algorithm NeuralBandit3 lifts this limitation by associating one EXP3 per action.\nNeuralBandit3 has greater capacity of expression than NeuralBandit2 as each action can be associated with different models. However if the best model in NeuralBandit3 exists in NeuralBandit2, then NeuralBandit2 should find this model faster than NeuralBandit3 because it has only one instance of EXP3 with less possibility to update."}, {"heading": "5 Experiments", "text": "The Forest Cover Type dataset from the UCI Machine Learning Repository is used. It contains 581.000 instances and it is shuffled. We have recoded each continuous variable using equal frequencies into five binary variables and we have recoded each categorical variable into binary variables. We have obtained 94 binary variables for the context, and we have used the 7 target classes as the set of actions. In order to simulate a datastream, the dataset is played in loop. At each round, if the algorithm chooses the right class the reward is 1 or else 0. The cumulated regret is computed from the rewards of an offline algorithm fitting the data with 93% of classification. The plot of the curves (Figure 1) are produced by averaging 10 runs of each algorithms with \u03b3 = 0.005, \u03b3model = 0.1. Each run began at a random position in the dataset. The parameters of each model (NeuralBandit1) are the combination of different sizes of hidden layer (1, 5, 25, 50, 100) and different values of \u03bb (0.01, 0.1, 1).\nAlgorithm 2: NeuralBandit2 Data: \u03b3model \u2208 [0, 0.5] and a list of M models parameters begin\nInitialize M NeuralBandit1 ; Initialize the EXP3 weight vector w0 with \u2200m \u2208 [M ] wm0 = 1; for t = 1, 2, ..., T do\nContext xt is revealed; mt is drawn from Pmodelt (2); The model mt choose action k\u0303t; k\u0303t is predicted and yt,k\u0303t is revealed; Update of each network corresponding to the action k\u0303t for each model with (1); Update of the EXP3 weight vector wt with (3);\nAlgorithm 3: NeuralBandit3 Data: \u03b3 \u2208 [0, 0.5], \u03b3model \u2208 [0, 0.5]and a list of M model parameters begin\nInitialize K neural networks per model m ; Initialize K instance of EXP3; for t = 1, 2, ..., T do\nContext xt is revealed; for k = 1, 2, ...,K do\nmkt is drawn from Pmodelkt (2); Action k is scored skt = N mkt ,k t (xt);\nk\u0302t = arg max k\u2208[K]\nskt ;\n\u2200k \u2208 [K] on a Pt(k) = (1\u2212 \u03b3)1[k = k\u0302t] + \u03b3K ; k\u0303t is drawn from Pt; k\u0303t is predicted and yt,k\u0303t is revealed; Update of each network corresponding to the action k\u0303t for each model with (1); Update of each EXP3 weight vectors with (3);\nOn stationary data (left part of Figure 1) Banditron achieves a high cumulated regret (57% of classification computed on the 100.000 last predictions) and is outperformed by all other contextual bandit algorithms on this dataset. The cumulated regret and classification rate of LinUCB (72%), CTS (73%) and NeuralBandit3 (73%) are similar and their curves tend to be the same. NeuralBandit2 has the fastest convergence rate and achieves a smaller cumulated regret with 76% of classification.\nOn non-stationary data (right part of Figure 1) Non-stationarity is simulated by swapping classes with a circular cycle (1 \u2192 2, 2 \u2192 3 ,...,7 \u2192 1) every 500.000 iterations. At each concept drift, curves increase then stabilize. On stationary data LinUCB and CTS achieve a lowest regret than Banditron but can\u2019t deal with non-stationarity thus are outperformed by it after the first drift. Banditron converges again near instantanly.\nModels selection algorithms need between 75.000 and 350.000 in the worst case to stabilize at each drift. NeuralBandit2 and NeuralBandit3 are more complex models than Banditron but achieve better performances on this non-stationary datastream. NeuralBandit3 seem to be more robust to nonstationarity than NeuralBandit2 on this dataset. This can be explained by the fact that sometime, one neural network can stay on a bad local minimum. If this append in NeuralBandit2, the entire model is penalized while in NeuralBandit3 the algorithm can still use all the other networks."}, {"heading": "6 Conclusion", "text": "We introduced a new contextual bandit algorithm NeuralBandit1. Two variants with models selection NeuralBandit2 and NeuralBandit3 used an adversarial bandit algorithm to find the best parameters of neural networks. We confronted them to stationary and non-stationary datastream. They achieve a smaller cumulated regret than Banditron, LinUCB and CTS. This approach is successful and has the advantage of being trivially parallelizable. Models differentiation show a significant gain on the Forest Covert Type dataset with nonstationarity. We also showed empirically that our two models selection algorithms are robust to concept drift. These experimental results suggest that neural networks are serious candidates for addressing the issue of contextual bandit. However, they are freed from the constraint of linearity at the expense of the bound on the regret, the cost function being not convex."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics 6(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning 47(2-3)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": "Biometrika 25", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1933}, {"title": "Thompson Sampling: An Asymptotically Optimal Finite Time Analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "Algorithmic Learning Theory, Proc. of the 23rd International Conference (ALT). Volume LNCS 7568., Lyon, France, Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "On-line learning with malicious noise and the closure algorithm", "author": ["P. Auer", "N. Cesa-Bianchi"], "venue": "Ann. Math. Artif. Intell. 23(1-2)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput. 32(1)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R.D. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma"], "venue": "COLT.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Mortal multi-armed bandits", "author": ["D. Chakrabarti", "R. Kumar", "F. Radlinski", "E. Upfal"], "venue": "NIPS.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A stochastic bandit algorithm for scratch games", "author": ["R. Feraud", "T. Urvoy"], "venue": "In Hoi, S.C.H., Buntine, W.L., eds.: ACML. Volume 25 of JMLR Proceedings., JMLR.org", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploration and exploitation of scratch games", "author": ["R. Feraud", "T. Urvoy"], "venue": "Machine Learning 92(2-3)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Online optimization in x-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "In Koller, D., Schuurmans, D., Bengio, Y., Bottou, L., eds.: NIPS, Curran Associates, Inc.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In F\u00fcrnkranz, J., Scheffer, T., Spiliopoulou, M., eds.: ECML. Volume 4212 of Lecture Notes in Computer Science., Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Feature selection as a one-player game", "author": ["R. Gaudel", "M. Sebag"], "venue": "In F\u00fcrnkranz, J., Joachims, T., eds.: ICML, Omnipress", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Pac-bayesian analysis of contextual bandits", "author": ["Y. Seldin", "P. Auer", "F. Laviolette", "J. Shawe-Taylor", "R. Ortner"], "venue": "In Shawe-Taylor, J., Zemel, R.S., Bartlett, P.L., Pereira, F.C.N., Weinberger, K.Q., eds.: NIPS.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient optimal learning for contextual bandits", "author": ["M. Dud\u0131\u0301k", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang"], "venue": "CoRR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In Platt, J.C., Koller, D., Singer, Y., Roweis, S.T., eds.: NIPS, Curran Associates, Inc.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "CoRR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Contextual bandits with linear payoff functions", "author": ["W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "In Gordon, G.J., Dunson, D.B., Dudk, M., eds.: AISTATS. Volume 15 of JMLR Proceedings., JMLR.org", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "CoRR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908, New York, NY, USA, ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Netw. 2(5)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Programming backgammon using self-teaching neural nets", "author": ["G. Tesauro"], "venue": "Artificial Intelligence 134", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "CoRR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On-line learning for very large datasets", "author": ["L. Bottou", "Y. LeCun"], "venue": "Applied Stochastic Models in Business and Industry 21(2)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "1. MIT Press, Cambridge, MA, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 0, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 68, "endOffset": 73}, {"referenceID": 1, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 68, "endOffset": 73}, {"referenceID": 2, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 98, "endOffset": 103}, {"referenceID": 3, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 98, "endOffset": 103}, {"referenceID": 4, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 141, "endOffset": 146}, {"referenceID": 5, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 141, "endOffset": 146}, {"referenceID": 6, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 143, "endOffset": 148}, {"referenceID": 7, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 143, "endOffset": 148}, {"referenceID": 8, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 189, "endOffset": 195}, {"referenceID": 9, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 189, "endOffset": 195}, {"referenceID": 10, "context": "A tree structured bandit such as X-armed bandits [11] or a UCT variant [12] can be used to explore and exploit a tree structure of contextual variables to ar X iv :1 40 9.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "A tree structured bandit such as X-armed bandits [11] or a UCT variant [12] can be used to explore and exploit a tree structure of contextual variables to ar X iv :1 40 9.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "find the leaves which provide the highest rewards [13].", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "Seldin et al [14] modelize the contexts by state sets, which are associated with bandit problems.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "Dud\u0131\u0301k et al [15] propose an algorithm of policies elimination.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "The epoch-greedy algorithm [16] alternates exploration then exploitation.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "In LINUCB [17,18] and in Contextual Thompson Sampling (CTS) [19], the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.", "startOffset": 10, "endOffset": 17}, {"referenceID": 17, "context": "In LINUCB [17,18] and in Contextual Thompson Sampling (CTS) [19], the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.", "startOffset": 10, "endOffset": 17}, {"referenceID": 18, "context": "In LINUCB [17,18] and in Contextual Thompson Sampling (CTS) [19], the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Banditron [20] uses a perceptron per action to recognize rewarded contexts.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "The EXP4 algorithm [6] selects the best arm from N experts advices (probability vectors).", "startOffset": 19, "endOffset": 22}, {"referenceID": 19, "context": "Inspired by Banditron [20] it estimates the probabilities of rewards by using neural networks in order to be free of the hypothesis of linear separability of the data.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "Neural networks are universal approximators [21].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "They are used in reinforcement learning [22,23] and can estimate accurately the probabilities of rewards within actual and complex problems.", "startOffset": 40, "endOffset": 47}, {"referenceID": 22, "context": "They are used in reinforcement learning [22,23] and can estimate accurately the probabilities of rewards within actual and complex problems.", "startOffset": 40, "endOffset": 47}, {"referenceID": 23, "context": "In addition, the stochastic gradient achieves good performances in terms of convergence to the point of best generalization [24] and has the advantage of learning online.", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "The backpropagation algorithm [25] allows calculating the gradient of the error for each neuron from the last to the first layer by minimizing a cost function.", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "Using online learning, we propose to train the models in parallel and to use the adversarial bandit algorithm EXP3 [5,6] to choose the best model.", "startOffset": 115, "endOffset": 120}, {"referenceID": 5, "context": "Using online learning, we propose to train the models in parallel and to use the adversarial bandit algorithm EXP3 [5,6] to choose the best model.", "startOffset": 115, "endOffset": 120}, {"referenceID": 0, "context": "Exp3 Let \u03b3model \u2208 [0, 1] be an exploration parameter, wt = (w t , .", "startOffset": 18, "endOffset": 24}], "year": 2014, "abstractText": "This paper presents a new contextual bandit algorithm, NeuralBandit, which does not need hypothesis on stationarity of contexts and rewards. Several neural networks are trained to modelize the value of rewards knowing the context. Two variants, based on multi-experts approach, are proposed to choose online the parameters of multi-layer perceptrons. The proposed algorithms are successfully tested on a large dataset with and without stationarity of rewards.", "creator": "LaTeX with hyperref package"}}}