{"id": "1410.5137", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2014", "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "abstract": "The use of M - estimators schiro in generalized linear regression metopes models in thimbleful high dimensional settings requires struhs risk fultonville minimization heraclius with kumaresan hard $ L_0 $ ramapough constraints. Of scow the gajewski known 136.00 methods, the class digest of loudhailers projected gradient mihok descent (noon-4 also known as mcconnel iterative hd-2 hard thresholding (jtwc IHT) ) ullyot methods cides is moritz known to offer buzzers the muzhi fastest magendie and 677 most scalable roboto solutions. ahhs However, readaptation the anticancer current state - calcraft of - the - art fibbed is sonne only monssen able to analyze pono these methods in hearstdc.com extremely bartnoff restrictive settings which pyrrolizidine do 637 not musleh hold in chus high intransigence dimensional statistical hermida models. In cyanohydrin this shaposhnikov work combated we bridge this vinograd gap by nightclothes providing rh1 the first qubad analysis for shehada IHT - perun style methods 66.46 in warsop the high cardholders dimensional statistical setting. Our al-houthi bounds are tight 6-of-11 and boiro match ayato known minimax osswald lower bounds. ridler Our results rely on ericks a heeded general uddin analysis framework craine that 55.86 enables holmstead us mitiukov to \u00e1ngel analyze kauf several jyh popular nrk hard giovanna thresholding michaelis style jlr algorithms (drydocked such tytonidae as 2:5 HTP, obstructive CoSaMP, SP) in the prachai high dimensional regression pan-arabism setting. We also dozo extend franzia our analysis tempa to 855,000 large family use of \" fully gessen corrective zinzendorf methods \" bacque that 122.72 includes yongsan two - stage and toma\u0161evi\u0107 partial newsedge hard - thresholding 795,000 algorithms. durrant We show that mangels our bothnia results hold odama for 15,625 the matamoros problem of zanla sparse regression, 11.02 as well faleomavaega as low - vf-2 rank 14.78 matrix recovery.", "histories": [["v1", "Mon, 20 Oct 2014 02:29:27 GMT  (158kb,D)", "https://arxiv.org/abs/1410.5137v1", "20 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014"], ["v2", "Tue, 21 Oct 2014 08:45:56 GMT  (79kb,D)", "http://arxiv.org/abs/1410.5137v2", "20 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014"]], "COMMENTS": "20 pages, 3 figures, To appear in the proceedings of the 28th Annual Conference on Neural Information Processing Systems, NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["prateek jain 0002", "ambuj tewari", "purushottam kar"], "accepted": true, "id": "1410.5137"}, "pdf": {"name": "1410.5137.pdf", "metadata": {"source": "CRF", "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "authors": ["Prateek Jain", "Ambuj Tewari", "Purushottam Kar"], "emails": ["prajain@microsoft.com,", "t-purkar@microsoft.com,", "tewaria@umich.edu"], "sections": [{"heading": "1 Introduction", "text": "Modern statistical estimation is routinely faced with real world problems where the number of parameters p handily outnumbers the number of observations n. In general, consistent estimation of parameters is not possible in such a situation. Consequently, a rich line of work has focused on models that satisfy special structural assumptions such as sparsity or low-rank structure. Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.\nThe question of efficient estimation, however, is faced with feasibility issues since consistent estimation routines often end-up solving NP-hard problems. Examples include sparse regression which requires loss minimization with sparsity constraints and low-rank regression which requires dealing with rank constraints which are not efficiently solvable in general [6].\nInterestingly, recent works have demonstrated that these hardness results can be avoided by assuming certain natural conditions over the loss function being minimized such as restricted strong convexity (RSC) and restricted strong smoothness (RSS). The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.\nar X\niv :1\n41 0.\n51 37\nv2 [\ncs .L\nG ]\n2 1\nO ct\nDespite this, certain limitations have precluded widespread use of these techniques. Convex relaxation-based methods typically suffer from slow rates as they solve non-smooth optimization problems apart from being hard to analyze in terms of global guarantees. Greedy methods, on the other hand, are slow in situations with non-negligible sparsity or relatively high rank, owing to their incremental approach of adding/removing individual support elements.\nInstead, the methods of choice for practical applications are actually projected gradient (PGD) methods, also referred to as iterative hard thresholding (IHT) methods. These methods directly project the gradient descent update onto the underlying (non-convex) feasible set. This projection can be performed efficiently for several interesting structures such as sparsity and low rank. However, traditional PGD analyses for convex problems viz. [10] do not apply to these techniques due to the non-convex structure of the problem.\nAn exception to this is the recent work [11] that demonstrates that PGD with non-convex regularization can offer consistent estimates for certain high-dimensional problems. However, the work in [11] is only able to analyze penalties such as SCAD, MCP and capped L1. Moreover, their framework cannot handle commonly used penalties such as L0 or low-rank constraints."}, {"heading": "1.1 Insufficiency of RIP based Guarantees for M-estimation", "text": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed. However, the analysis of these algorithms has traditionally been restricted to settings that satisfy the Restricted Isometry property (RIP) or incoherence property. As the discussion below demonstrates, this renders these analyses inaccessible to high-dimensional statistical estimation problems.\nAll existing results analyzing these methods require the condition number of the loss function, restricted to sparse vectors, to be smaller than a universal constant. The best known such constant is due to the work of [17] that requires a bound on the RIP constant \u03b42k \u2264 0.5 (or equivalently a bound 1+\u03b42k 1\u2212\u03b42k \u2264 3 on the condition number). In contrast, real-life high dimensional statistical settings, wherein pairs of variables can be arbitrarily correlated, routinely require estimation methods to perform under arbitrarily large condition numbers. In particular if two variates have a covariance\nmatrix like\n[ 1 1\u2212\n1\u2212 1\n] , then the restricted condition number (on a support set of size just 2)\nof the sample matrix cannot be brought down below 1/ even with infinitely many samples. In particular when < 1/6, none of the existing results for hard thresholding methods offer any guarantees. Moreover, most of these analyses consider only the least squares objective. Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting."}, {"heading": "1.2 Overview of Results", "text": "Our main contribution in this work is an analysis of PGD/IHT-style methods in statistical settings. Our bounds are tight, achieve known minmax lower bounds [20], and hold for arbitrary differentiable, possibly even non-convex functions. Our results hold even when the underlying condition number is arbitrarily large and only require the function to satisfy RSC/RSS conditions. In partic-\nular, this reveals that these iterative methods are indeed applicable to statistical settings, a result that escaped all previous works.\nOur first result shows that the PGD/IHT methods achieve global convergence if used with a relaxed projection step. More formally, if the optimal parameter is s\u2217-sparse and the problem satisfies RSC and RSS constraints \u03b1 and L respectively (see Section 2), then PGD methods offer global convergence so long as they employ projection to an s-sparse set where s \u2265 4(L/\u03b1)2s\u2217. This gives convergence rates that are identical to those of convex relaxation and greedy methods for the Gaussian sparse linear model. We then move to a family of efficient \u201cfully corrective\u201d methods and show as before, that for arbitrary functions satisfying the RSC/RSS properties, these methods offer global convergence.\nNext, we show that these results allow PGD-style methods to offer global convergence in a variety of statistical estimation problems such as sparse linear regression and low rank matrix regression. Our results effortlessly extend to the noisy setting as a corollary and give bounds similar to those of [21] that relies on solving an L1 regularized problem.\nOur proofs are able to exploit that even though hard-thresholding is not the prox-operator for any convex prox function, it still provides strong contraction when projection is performed onto sets of sparsity s s\u2217. This crucial observation allows us to provide the first unified analysis for hard thresholding based gradient descent algorithms. Our empirical results confirm our predictions with respect to the recovery properties of IHT-style algorithms on badly-conditioned sparse recovery problems, as well as demonstrate that these methods can be orders of magnitudes faster than their L1 and greedy counterparts."}, {"heading": "1.3 Organization of the Paper", "text": "Section 2 sets the notation and the problem statement. Section 3 introduces the PGD/IHT algorithm that we study and proves that the method guarantees recovery assuming the RSC/RSS property. We also generalize our guarantees to the problem of low-rank matrix regression. Section 4 then provides crisp sample complexity bounds and statistical guarantees for the PGD/IHT estimators. Section 5 extends our analysis to a broad family of \u201cfully-corrective\u201d hard thresholding methods compressive sensing algorithms that include the so-called two-stage hard thresholding and partial hard thresholding algorithms and provide similar results for them as well. We present some empirical results in Section 6 and conclude in Section 7."}, {"heading": "2 Problem Setup and Notations", "text": "High-dimensional Sparse Estimation. Given data points X = [X1, . . . , Xn] T , where Xi \u2208 Rp, and the target Y = [Y1, . . . , Yn] T , where Yi \u2208 R, the goal is to compute an s\u2217-sparse \u03b8\u2217 s.t.,\n\u03b8\u2217 = arg min \u03b8,\u2016\u03b8\u20160\u2264s\u2217 f(\u03b8). (1)\nTypically, f can be thought of as an empirical risk function i.e. f(\u03b8) = 1n \u2211\ni `(\u3008Xi,\u03b8\u3009, Yi) for some loss function ` (see examples in Section 4). However, for our analysis of PGD and other algorithms, we need not assume any other property of f other than differentiability and the following two RSC and RSS properties.\nDefinition 1 (RSC Property). A differentiable function f : Rp \u2192 R is said to satisfy restricted strong convexity (RSC) at sparsity level s = s1 + s2 with strong convexity constraint \u03b1s if the following holds for all \u03b81,\u03b82 s.t. \u2016\u03b81\u20160 \u2264 s1 and \u2016\u03b82\u20160 \u2264 s2:\nf(\u03b81)\u2212 f(\u03b82) \u2265 \u3008\u03b81 \u2212 \u03b82,\u2207\u03b8f(\u03b82)\u3009+ \u03b1s 2 \u2016\u03b81 \u2212 \u03b82\u201622.\nDefinition 2 (RSS Property). A differentiable function f : Rp \u2192 R is said to satisfy restricted strong smoothness (RSS) at sparsity level s = s1 + s2 with strong convexity constraint Ls if the following holds for all \u03b81,\u03b82 s.t. \u2016\u03b81\u20160 \u2264 s1 and \u2016\u03b82\u20160 \u2264 s2:\nf(\u03b81)\u2212 f(\u03b82) \u2264 \u3008\u03b81 \u2212 \u03b82,\u2207\u03b8f(\u03b82)\u3009+ Ls 2 \u2016\u03b81 \u2212 \u03b82\u201622.\nLow-rank Matrix Regression. Low-rank matrix regression is similar to sparse estimation as presented above except that each data point is now a matrix i.e. Xi \u2208 Rp1\u00d7p2 , the goal being to estimate a low-rank matrix W \u2208 Rp1\u00d7p2 that minimizes the empirical loss function on the given data.\nW \u2217 = arg min W,rank(W )\u2264r f(W ). (2)\nFor this problem the RSC and RSS properties for f are defined similarly as in Definition 1, 2 except that the L0 norm is replaced by the rank function."}, {"heading": "3 Iterative Hard-thresholding Method", "text": "In this section we study the popular projected gradient descent (a.k.a iterative hard thresholding) method for the case of the feasible set being the set of sparse vectors (see Algorithm 1 for pseudocode). The projection operator Ps(z), can be implemented efficiently in this case by projecting z onto the set of s-sparse vectors by selecting the s largest elements (in magnitude) of z. The standard projection property implies that \u2016Ps(z) \u2212 z\u201622 \u2264 \u2016\u03b8\u2032 \u2212 z\u201622 for all \u2016\u03b8\u2032\u20160 \u2264 s. However, it turns out that we can prove a significantly stronger property of hard thresholding for the case when \u2016\u03b8\u2032\u20160 \u2264 s\u2217 and s\u2217 s. This property is key to analysing IHT and is formalized below.\nLemma 1. For any index set I, any z \u2208 RI , let \u03b8 = Ps(z). Then for any \u03b8\u2217 \u2208 RI such that \u2016\u03b8\u2217\u20160 \u2264 s\u2217, we have\n\u2016\u03b8 \u2212 z\u201622 \u2264 |I| \u2212 s |I| \u2212 s\u2217 \u2016\u03b8\u2217 \u2212 z\u201622.\nSee Appendix A for a detailed proof. Our analysis combines the above observation with the RSC/RSS properties of f to provide\ngeometric convergence rates for the IHT procedure below.\nTheorem 1. Let f have RSC and RSS parameters given by L2s+s\u2217(f) = L and \u03b12s+s\u2217(f) = \u03b1 respectively. Let Algorithm 1 be invoked with f , s \u2265 32 ( L \u03b1 )2 s\u2217 and \u03b7 = 23L . Also let \u03b8 \u2217 = arg min\u03b8,\u2016\u03b8\u20160\u2264s\u2217 f(\u03b8). Then, the \u03c4 -th iterate of Algorithm 1, for \u03c4 = O( L \u03b1 \u00b7 log( f(\u03b80) )) satisfies:\nf(\u03b8\u03c4 )\u2212 f(\u03b8\u2217) \u2264 .\nAlgorithm 1 Iterative Hard-thresholding\n1: Input: Function f with gradient oracle, sparsity level s, step-size \u03b7 2: \u03b81 = 0, t = 1 3: while not converged do 4: \u03b8t+1 = Ps(\u03b8\nt \u2212 \u03b7\u2207\u03b8f(\u03b8t)) 5: t = t+ 1 6: end while 7: Output: \u03b8t\nProof. (Sketch) Let St = supp(\u03b8t), S\u2217 = supp(\u03b8\u2217), St+1 = supp(\u03b8t+1) and It = S\u2217 \u222a St \u222a St+1. Using the RSS property and the fact that supp(\u03b8t) \u2286 It and supp(\u03b8t+1) \u2286 It, we have:\nf(\u03b8t+1)\u2212 f(\u03b8t) \u2264 \u3008\u03b8t+1 \u2212 \u03b8t, gt\u3009+ L 2 \u2016\u03b8t+1 \u2212 \u03b8t\u201622,\n= L\n2 \u2016\u03b8t+1It \u2212 \u03b8 t It +\n2\n3L \u00b7 gtIt\u2016 2 2 \u2212\n1\n2L \u2016gtIt\u2016 2 2,\n\u03b61 \u2264 L 2 \u00b7 |I t| \u2212 s |It| \u2212 s\u2217 \u00b7 \u2016\u03b8\u2217It \u2212 \u03b8 t It + 1 L \u00b7 gtIt\u2016 2 2 \u2212 1 2L (\u2016gtIt\\(St\u222aS\u2217)\u2016 2 2 + \u2016gtSt\u222aS\u2217\u2016 2 2), (3)\nwhere \u03b61 follows from an application of Lemma 1 with I = I t and the Pythagoras theorem. The above equation has three critical terms. The first term can be bounded using the RSS condition. Using f(\u03b8t) \u2212 f(\u03b8\u2217) \u2264 \u3008gtSt\u222aS\u2217 ,\u03b8 t \u2212 \u03b8\u2217\u3009 \u2212 \u03b12 \u2016\u03b8 t \u2212 \u03b8\u2217\u201622 \u2264 12\u03b1\u2016g t St\u222aS\u2217\u2016 2 2 bounds the third term in (3). The second term is more interesting as in general elements of gt S\u2217 can be arbitrarily small. However, elements of gtIt\\(St\u222aS\u2217) should be at least as large as g t S\u2217\\St+1 as they are selected by hard-thresholding. Combining this insight with bounds for gtS\u2217\\St+1 and with (3), we obtain the theorem. See Appendix A for a detailed proof."}, {"heading": "3.1 Low-rank Matrix Regression", "text": "We now generalize our previous analysis to a projected gradient descent (PGD) method for low-rank matrix regression. Formally, we study the following problem:\nmin W\nf(W ), s.t., rank(W ) \u2264 s. (4)\nThe hard-thresholding projection step for low-rank matrices can be solved using SVD i.e.\nPMs(W ) = Us\u03a3sV T s ,\nwhere W = U\u03a3V T is the singular value decomposition of W . Us, Vs are the top-s singular vectors (left and right, respectively) of W and \u03a3s is the diagonal matrix of the top-s singular values of W . To proceed, we first note a property of the above projection similar to Lemma 1.\nLemma 2. Let W \u2208 Rp1\u00d7p2 be a rank-|It| matrix and let p1 \u2265 p2. Then for any rank-s\u2217 matrix W \u2217 \u2208 Rp1\u00d7p2 we have\n\u2016PMs(W )\u2212W\u20162F \u2264 |It| \u2212 s |It| \u2212 s\u2217 \u2016W \u2217 \u2212W\u20162F . (5)\nProof. Let W = U\u03a3V T be the singular value decomposition of W . Now, \u2016PMs(W ) \u2212 W\u20162F =\u2211|It| i=s+1 \u03c3 2 i = \u2016Ps(diag(\u03a3)) \u2212 diag(\u03a3)\u201622, where \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3|It| \u2265 0 are the singular values of W . Using Lemma 1, we get:\n\u2016PMs(W )\u2212W\u20162F \u2264 |It| \u2212 s |It| \u2212 s\u2217 \u2016\u03a3\u2217 \u2212 diag(\u03a3)\u201622 \u2264 |It| \u2212 s |It| \u2212 s\u2217 \u2016W \u2217 \u2212W\u20162F , (6)\nwhere the last step uses the von Neumann\u2019s trace inequality (Tr(A \u00b7B) \u2264 \u2211\ni \u03c3i(A)\u03c3i(B)).\nThe following result for low-rank matrix regression immediately follows from Lemma 4.\nTheorem 2. Let f have RSC and RSS parameters given by L2s+s\u2217(f) = L and \u03b12s+s\u2217(f) = \u03b1. Replace the projection operator Ps in Algorithm 1 with its matrix counterpart PMs as defined in (5).\nSuppose we invoke it with f, s \u2265 32 ( L \u03b1 )2 s\u2217, \u03b7 = 23L . Also let W \u2217 = arg minW,rank(W )\u2264s\u2217 f(W ). Then the \u03c4 -th iterate of Algorithm 1, for \u03c4 = O(L\u03b1 \u00b7 log( f(W 0) ) satisfies:\nf(W \u03c4 )\u2212 f(W \u2217) \u2264 .\nProof. A proof progression similar to that of Theorem 1 suffices. The only changes that need to be made are: firstly Lemma 2 has to be invoked in place of Lemma 1. Secondly, in place of considering vectors restricted to a subset of coordinates viz. \u03b8S , g t I , we would need to consider matrices restricted to subspaces i.e. WS = USU T SW where US is a set of singular vectors spanning the range-space of S."}, {"heading": "4 High Dimensional Statistical Estimation", "text": "This section elaborates on how the results of the previous section can be used to give guarantees for IHT-style techniques in a variety of statistical estimation problems. We will first present a generic convergence result and then specialize it to various settings. Suppose we have a sample of data points Z1:n and a loss function L(\u03b8;Z1:n) that depends on a parameter \u03b8 and the sample. Then we can show the following result. (See Appendix B for a proof.)\nTheorem 3. Let \u03b8\u0304 be any s\u2217-sparse vector. Suppose L(\u03b8;Z1:n) is differentiable and satisfies RSC and RSS at sparsity level s + s\u2217 with parameters \u03b1s+s\u2217 and Ls+s\u2217 respectively, for s \u2265 32 ( L2s+s\u2217 \u03b12s+s\u2217 )2 s\u2217. Let \u03b8\u03c4 be the \u03c4 -th iterate of Algorithm 1 for \u03c4 chosen as in Theorem 1 and \u03b5 be the function value error incurred by Algorithm 1. Then we have\n\u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u20162 \u2264 2 \u221a s+ s\u2217\u2016\u2207L(\u03b8\u0304;Z1:n)\u2016\u221e\n\u03b1s+s\u2217 +\n\u221a 2\n\u03b1s+s\u2217 .\nNote that the result does not require the loss function to be convex. This fact will be crucially used later. We now apply the above result to several statistical estimation scenarios.\nSparse Linear Regression. Here Zi = (Xi, Yi) \u2208 Rp\u00d7R and Yi = \u3008\u03b8\u0304, Xi\u3009+\u03bei where \u03bei \u223c N (0, \u03c32) is label noise. The empirical loss is the usual least squares loss i.e. L(\u03b8;Z1:n) = 1n\u2016Y \u2212 X\u03b8\u2016 2 2. Suppose X1:n are drawn i.i.d. from a sub-Gaussian distribution with covariance \u03a3 with \u03a3jj \u2264 1 for\nall j. Then [22, Lemma 6] immediately implies that RSC and RSS at sparsity level k hold, with probability at least 1 \u2212 e\u2212c0n, with \u03b1k = 12\u03c3min(\u03a3) \u2212 c1 k log p n and Lk = 2\u03c3max(\u03a3) + c1 k log p n (c0, c1 are universal constants). So we can set k = 2s + s\u2217 and if n > 4c1k log p/\u03c3min(\u03a3) then we have \u03b1k \u2265 14\u03c3min(\u03a3) and Lk \u2264 2.25\u03c3max(\u03a3) which means that Lk/9\u03b1k \u2264 \u03ba(\u03a3) := \u03c3max(\u03a3)/\u03c3min(\u03a3). Thus it is enough to choose s = 2592\u03ba(\u03a3)2s\u2217 and apply Theorem 3. Note that \u2016\u2207L(\u03b8\u0304;Z1:n)\u2016\u221e = \u2016XT \u03be/n\u2016\u221e \u2264 2\u03c3 \u221a log p n with probability at least 1\u2212c2p \u2212c3 (c2, c3 are universal constants). Putting everything together, we have the following bound with high probability:\n\u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u20162 \u2264 145 \u03ba(\u03a3)\n\u03c3min(\u03a3) \u03c3\n\u221a s\u2217 log p\nn + 2\n\u221a\n\u03c3min(\u03a3) ,\nwhere is the function value error incurred by Algorithm 1.\nNoisy and Missing Data. We now look at cases with feature noise as well. More specifically, assume that we only have access to X\u0303i\u2019s that are corrupted versions of Xi\u2019s. Two models of noise are popular in literature [21]: a) (additive noise) X\u0303i = Xi+Wi where Wi \u223c N (0,\u03a3W ), and b) (missing data) X\u0303 is an R\u222a{?}-valued matrix obtained by independently, with probability \u03bd \u2208 [0, 1), replacing each entry in X with ?. For the case of additive noise (missing data can be handled similarly), Zi = (X\u0303i, Yi) and L(\u03b8;Z1:n) = 12\u03b8\nT \u0393\u0302\u03b8\u2212\u03b3\u0302T\u03b8 where \u0393\u0302 = X\u0303T X\u0303/n\u2212\u03a3W and \u03b3\u0302 = X\u0303TY/n are unbiased estimators of \u03a3 and \u03a3T \u03b8\u0304 respectively. [21, Appendix A, Lemma 1] implies that RSC, RSS at sparsity level k hold, with failure probability exponentially small in n, with \u03b1k = 1 2\u03c3min(\u03a3)\u2212 k\u03c4(p)/n and Lk = 1.5\u03c3max(\u03a3)+k\u03c4(p)/n for \u03c4(p) = c0\u03c3min(\u03a3) max( (\u2016\u03a3\u20162op+\u2016\u03a3W \u20162op)2\n\u03c32min(\u03a3) , 1) log p. Thus for k = 2s+s\u2217\nand n \u2265 4k\u03c4(p)/\u03c3min(\u03a3) we have Lk/\u03b1k \u2264 7\u03ba(\u03a3). Note that L(\u00b7;Z1:n) is non-convex but we can still apply Theorem 3 with s = 1568\u03ba(\u03a3)2s\u2217 because RSC, RSS hold. Using the high probability upper bound (see [21, Appendix A, Lemma 2]) \u2016\u2207L(\u03b8\u0304;Z1:n)\u2016\u221e \u2264 c1\u03c3\u0303\u2016\u03b8\u0304\u20162 \u221a log p/n gives us the following\n\u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u20162 \u2264 c2 \u03ba(\u03a3)\n\u03c3min(\u03a3) \u03c3\u0303\u2016\u03b8\u0304\u20162\n\u221a s\u2217 log p\nn + 2\n\u221a\n\u03c3min(\u03a3) where \u03c3\u0303 = \u221a \u2016\u03a3W \u20162op + \u2016\u03a3\u20162op(\u2016\u03a3W \u2016op + \u03c3) and is the function value error in Algorithm 1."}, {"heading": "5 Fully-corrective Methods", "text": "In this section, we study a variety of \u201cfully-corrective\u201d methods. These methods keep the optimization objective fully minimized over the support of the current iterate. To this end, we first prove a fundamental theorem for fully-corrective methods that formalizes the intuition that for such methods, a large function value should imply a large gradient at any sparse \u03b8 as well. This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.\nLemma 3. Consider a function f with RSC parameter given by L2s+s\u2217(f) = L and RSS parameter given by \u03b12s+s\u2217(f) = \u03b1. Let \u03b8 \u2217 = arg min\u03b8,\u2016\u03b8\u20160\u2264s\u2217 f(\u03b8) with S \u2217 = supp(\u03b8\u2217). Let St \u2286 [p] be any subset of co-ordinates s.t. |St| \u2264 s. Let \u03b8t = arg min\u03b8,supp(\u03b8)\u2286St f(\u03b8). Then, we have:\n2\u03b1(f(\u03b8t)\u2212 f(\u03b8\u2217)) \u2264 \u2016gtSt\u222aS\u2217\u2016 2 2 \u2212 \u03b12\u2016\u03b8tSt\\S\u2217\u2016 2 2\nSee Appendix C for a detailed proof.\nAlgorithm 2 Two-stage Hard-thresholding\n1: Input: function f with gradient oracle, sparsity level s, sparsity expansion level ` 2: \u03b81 = 0, t = 1 3: while not converged do 4: gt = \u2207\u03b8f(\u03b8t), St = supp(\u03b8t) 5: Zt = St \u222a (largest ` elements of |gt\nSt |)\n6: \u03b2t = arg min\u03b2,supp(\u03b2)\u2286Zt f(\u03b2) // fully corrective step 7: \u03b8\u0303t = Ps(\u03b2 t) 8: \u03b8t+1 = arg min \u03b8,supp(\u03b8)\u2286supp(\u03b8\u0303t) f(\u03b8) // fully corrective step\n9: t = t+ 1 10: end while 11: Output: \u03b8t"}, {"heading": "5.1 Two-stage Hard Thresholding Methods", "text": "Here we will concentrate on a family of two-stage fully corrective methods that contains popular compressive sensing algorithms like CoSaMP and Subspace Pursuit (see Algorithm 2 for pseudocode). These algorithms have thus far been analyzed only under RIP conditions for the least squares objective. Using our analysis framework developed in the previous sections, we present a generic RSC/RSS-based analysis for general two-stage methods for arbitrary loss functions. Our analysis shall use the following key observation that the the hard thresholding step in two stage methods does not increase the objective function a lot.\nLemma 4. Let Zt \u2286 [n] and |Zt| \u2264 q. Let \u03b2t = arg min\u03b2,supp(\u03b2)\u2286Zt f(\u03b2) and \u03b8\u0302t = Pq(\u03b2t). Then, the following holds:\nf(\u03b8\u0302t)\u2212 f(\u03b2t) \u2264 L \u03b1 \u00b7 ` s+ `\u2212 s\u2217 \u00b7 (f(\u03b2t)\u2212 f(\u03b8\u2217)).\nProof. Let vt = \u2207\u03b8f(\u03b2t). Then, using the RSS property we get:\nf(\u03b8\u0302t)\u2212 f(\u03b2t) \u2264 \u3008\u03b8\u0302t \u2212 \u03b2t,vt\u3009+ L 2 \u2016\u03b8\u0302t \u2212 \u03b2t\u201622 \u03b61 = L 2 \u2016\u03b8\u0302t \u2212 \u03b2t\u201622 \u03b62 \u2264 L 2 |`| |s+ `\u2212 s\u2217| \u2016w \u2212 \u03b2t\u201622, (7)\nwhere w is any vector such that wZt = 0 and \u2016w\u20160 \u2264 s \u2217. \u03b61 follows by observing v t Zt = 0 and by noting that supp(\u03b8\u0302t) \u2286 Zt. \u03b62 follows by Lemma 1 and the fact that \u2016w\u20160 \u2264 s\u2217. Now, using RSS property and the fact that \u2207\u03b8f(\u03b2t) = 0, we have:\n\u03b1 2 \u2016w \u2212 \u03b2t\u201622 \u2264 f(\u03b2t)\u2212 f(w) \u2264 f(\u03b2t)\u2212 f(\u03b8\u2217). (8)\nThe result now follows by combining (7) and (8).\nTheorem 4. Let f has RSC and RSS parameters given by \u03b12s+s\u2217(f) = \u03b1 and L2s+`(f) = L resp. Call Algorithm 2 with f , ` \u2265 s\u2217 and s \u2265 4L2 \u03b12 `+s\u2217\u2212` \u2265 4L2 \u03b12 s\u2217. Also let \u03b8\u2217 = arg min\u03b8,\u2016\u03b8\u20160\u2264s\u2217 f(\u03b8). Then, the \u03c4 -th iterate of Algorithm 2, for \u03c4 = O(L\u03b1 \u00b7 log( f(\u03b80) ) satisfies:\nf(\u03b8\u03c4 )\u2212 f(\u03b8\u2217) \u2264 .\nSee Appendix C for a detailed proof."}, {"heading": "5.2 Partial Hard Thresholding Methods", "text": "Algorithm 3 Iterative Partial Hard-thresholding\n1: Input: function f with gradient oracle, sparsity level s, step size \u03b7, partial thresholding level ` 2: \u03b81 = 0, t = 1 3: while not converged do 4: zt = \u03b8t \u2212 \u03b7\u2207\u03b8tf(\u03b8t), St = supp(\u03b8t) 5: vt = PHTs(z\nt;St, `) 6: \u03b8t+1 = arg min\u03b8,supp(\u03b8)\u2286supp(vt) f(\u03b8) // fully corrective step 7: t = t+ 1 8: end while 9: Output: \u03b8t\nWe now study Partial Hard Thresholding methods (PHT), a family of fully-corrective iterative methods introduced by [17]. This family is known to provide the best known RIP guarantees in the compressive sensing setting, but the proof is restricted to the RIP setting, and for the leastsquares objective. An interesting member of this family is Orthogonal Matching Pursuit with Replacement (OMPR), which is also a Forward-Backward Greedy Selection method but performs one forward-backward step per iteration.\nThe pseudo code of the general IPHT(`) algorithm is given in Algorithm 3. The algorithm is similar to the fully-corrective projected gradient descent (PGD) method, in fact, PHT(0) is indeed exactly same as the fully-corrected PGD method. But, the partial hard-thresholding projection is used instead of hard-thresholding projection.\nThe partial hard thresholding operator vt = PHTs(z t;St, `) projects zt onto the non-convex set of s-sparse vectors s.t. |supp(vt)\\St| \u2264 `. Although, the operator projects onto a non-convex set, still the projection can be performed efficiently by performing hard thresholding only over zt\nSt\nand the ` smallest elements of ztSt . That is, let bot t = smallest ` elements of |ztSt |. Then,\nvtSt\\bott = z t St\\bott , and, v t St\u222abott = H`(z t St\u222abott).\nWe first show that at least a new element is added during each iteration of IPHT(`).\nLemma 5. Let f , s be supplied to Algorithm 3 and let the RSC and RSS parameters of f be given by L2s+s\u2217(f) = L and \u03b12s+s\u2217(f) = \u03b1 respectively. Let s \u2265 4 ( L \u03b1 )2 s\u2217. Then, either f(St) = f(S\u2217) or |St+1\\St| \u2265 1. That is, at least one new element is added at each iteration of Algorithm 3.\nProof. Suppose no new element is added i.e. |St+1\\St| = 0. Since ` elements of St+1 are selected by hard thresholding zt\nSt\u222abott , hence each element of ztSt should be larger (in magnitude) than\nmaxi\u2208St |z t i |. Note that, ztSt = \u2212\u03b7g t St . Hence,\n\u2016ztSt\\S\u2217\u2016 2 2 |St\\S\u2217| = \u2016\u03b8tSt\\S\u2217 \u2212 \u03b7g t St\\S\u2217\u2016 2 2 |St\\S\u2217| = \u2016\u03b8tSt\\S\u2217\u2016 2 2 |St\\S\u2217| \u2265 \u2016ztS\u2217\\St\u2016 2 2 |S\u2217\\St| = \u2016\u03b8tS\u2217\\St \u2212 \u03b7g t S\u2217\\St\u2016 2 2 |S\u2217\\St|\n= \u03b72 \u2016gtS\u2217\\St\u2016 2 2\n|S\u2217\\St| = \u03b72\n\u2016gtS\u2217\u222aSt\u2016 2 2\n|S\u2217\\St| ,\nwhere we have used the fact that gtSt = 0 and \u03b8 t St = 0. Using Lemma 3 and the above equation, we have:\n0 \u2264 2\u03b3 ( f(\u03b8t)\u2212 f(\u03b8\u2217)\u2212 \u03b1 2 \u00b7 ( 1 \u03b1\u03b3 \u2212 1 ) \u2016\u03b8t \u2212 \u03b8\u2217\u201622 ) \u2264 ( \u03b32 \u2212 \u03b72 |S t\\S\u2217| |S\u2217\\St| ) \u2016gtSt\u222aS\u2217\u2016 2. (9)\nThe lemma now follows by observing that |S t\\S\u2217| |S\u2217\\St| \u2265 s\u2212s\u2217 s\u2217 \u2265 \u03b32 \u03b72 \u2265 1 \u03b72\u03b12 , by the choice of s.\nWe now provide the proof of convergence for IPHT(`) method in the general RSC-RSS setting:\nTheorem 5. Let f, s be supplied to Algorithm 3. Also, let the RSC and RSS parameters of f be given by \u03b12s+s\u2217(f) = \u03b1 and L2s+s\u2217(f) = L respectively. Let s \u2265 4 ( L \u03b1 )2 s\u2217 and let \u03b7 = 12L . Then, the \u03c4 -th iterate of Algorithm 3 satisfies:\nf(\u03b8\u03c4 )\u2212 f(\u03b8\u2217) \u2264 (\n1\u2212 \u03b1 4L \u00b7 1 `+ 1\n)\u03c4 \u00b7 ( f(\u03b80)\u2212 f(\u03b8\u2217) ) ,\nwhere \u03b8\u2217 = arg min\u03b8,\u2016\u03b8\u20160\u2264s\u2217 f(\u03b8).\nThis implies that for \u03c4 = O ( L` \u03b1 \u00b7 log( f(\u03b80) ) ) , we have f(\u03b8\u03c4 ) \u2212 f(\u03b8\u2217) \u2264 . See Appendix C for\na detailed proof."}, {"heading": "6 Experiments", "text": "We conducted simulations on high dimensional sparse linear regression problems to verify our predictions. Our experiments demonstrate that hard thresholding and projected gradient techniques can not only offer recovery in stochastic setting, but offer much more scalable routines for the same.\nData: Our problem setting is identical to the one described in the previous section. We fixed a parameter vector \u03b8\u0304 by choosing s\u2217 random coordinates and setting them randomly to \u00b11 values.\nData samples were generated as Zi = (Xi, Yi) where Xi \u223c N (0, Ip) and Yi = \u3008\u03b8\u0304, Xi\u3009 + \u03bei where \u03bei \u223c N (0, \u03c32). We studied the effect of varying dimensionality p, sparsity s\u2217, sample size n and label noise level \u03c3 on the recovery properties of the various algorithms as well as their run times. We chose baseline values of p = 20000, s\u2217 = 100, \u03c3 = 0.1, n = fo \u00b7 s\u2217 log p where fo is the oversampling factor with default value fo = 2. Keeping all other quantities fixed, we varied one of the quantities and generated independent data samples for the experiments.\nAlgorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16]. We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.\nEvaluation Metrics: For the baseline noise level \u03c3 = 0.1, we found that all the algorithms were able to recover the support set within an error of 2%. Consequently, our focus shifted to running times for these experiments. In the experiments where noise levels were varied, we recorded, for each method, the number of undiscovered support set elements.\nResults: Figure1 describes the results of our experiments in graphical form. For sake of clarity we included only HTP, GraDeS, L1 and FoBa results in these graphs. Graphs for the other algorithms CoSaMP, SP and OMPR can be seen in the supplementary material. The graphs indicate that whereas hard thresholding techniques are equally effective as L1 and greedy techniques for recovery in noisy settings, as indicated by Figure1(a), the former can be much more efficient and scalable than the latter. For instance, as Figure1(b), for the base level of p = 20000, HTP was 150\u00d7 faster than the L1 method. For higher values of p, the runtime gap widened to more than 350\u00d7. We also note that in both these cases, HTP actually offered exact support recovery whereas L1 was unable to recover 2 and 4 support elements respectively.\nAlthough FoBa was faster than L1 on Figure1(b) experiments, it was still slower than HTP by 50\u00d7 and 90\u00d7 for p = 20000 and 25000 respectively. Moreover, due to its greedy and incremental nature, FoBa was found to suffer badly in settings with larger true sparsity levels. As Figure 1(c) indicates, for even moderate sparsity levels of s\u2217 = 300 and 500, FoBa is 60 \u2212 75\u00d7 slower than HTP. As mentioned before, the reason for this slowdown is the greedy approach followed by FoBa: whereas HTP took less than 5 iterations to converge for these two problems, FoBa spend 300 and 500 iterations respectively. GraDeS was found to offer much lesser run times in comparison being slower than HTP by 30\u2212 40\u00d7 for larger values of p and 2\u2212 5\u00d7 slower for larger values of s\u2217.\nExperiments on badly conditioned problems. We also ran experiments to verify the performance of IHT algorithms in high condition number setting. Values of p, s\u2217 and \u03c3 were kept at baseline levels. After selecting the optimal parameter vector \u03b8\u0304, we selected s\u2217/2 random coordinates from its support and s\u2217/2 random coordinates outside its support and constructed a covariance matrix with heavy correlations between these chosen coordinates. The condition number of the resulting matrix was close to 50. Samples were drawn from this distribution and the recovery properties of the different IHT-style algorithms was observed as the projected sparsity levels s were increased. Our results (see Figure 1(d)) corroborate our theoretical observation that these algorithms show a remarkable improvement in recovery properties for ill-conditioned problems with an enlarged projection size."}, {"heading": "7 Discussion and Conclusions", "text": "In our work we studied iterative hard thresholding algorithms and showed that these techniques can offer global convergence guarantees for arbitrary, possibly non-convex, differentiable objective functions, which nevertheless satisfy Restricted Strong Convexity/Smoothness (RSC/RSM) conditions. Our results apply to a large family of algorithms that includes existing algorithms such as IHT, GraDeS, CoSaMP, SP and OMPR. Previously the analyses of these algorithms required stringent RIP conditions that did not allow the (restricted) condition number to be larger than universal constants specific to these algorithms.\nOur basic insight was to relax this stringent requirement by running these iterative algorithms with an enlarged support size. We showed that guarantees for high-dimensional M-estimation follow seamlessly from our results by invoking results on RSC/RSM conditions that have already been established in the literature for a variety of statistical settings. Our theoretical results put hard thresholding methods on par with those based on convex relaxation or greedy algorithms. Our experimental results demonstrate that hard thresholding methods outperform convex relaxation and greedy methods in terms of running time, sometime by orders of magnitude, all the while offering competitive or better recovery properties.\nOur results apply to sparsity and low rank structure, arguably two of the most commonly used structures in high dimensional statistical learning problems. In future work, it would be interesting to generalize our algorithms and their analyses to more general structures. A unified analysis for general structures will probably create interesting connections with existing unified frameworks such as those based on decomposability [5] and atomic norms [25]."}, {"heading": "A Proofs for Section 3", "text": "Proof of Lemma 1. Without loss of generality, assume that we have reordered coordinates such that |z1| \u2265 |z2| \u2265 . . . \u2265 |zI |. Since the projection operator Ps(\u00b7) operates by selecting the largest elements by magnitude, we have \u03b81 = z1, . . . ,\u03b8s = zs and \u03b8s+1 = \u03b8s+2 = . . . = \u03b8|I| = 0.\nAlso define \u03b8z = Ps\u2217(z). By the above argument, we have \u03b8 z 1 = z1, . . . ,\u03b8 z s\u2217 = zs\u2217 and \u03b8 z s\u2217+1 =\n\u03b8zs\u2217+2 = . . . = \u03b8 z |I| = 0. Now we have\n\u2016\u03b8z \u2212 z\u2016 |I| \u2212 s\u2217 \u2212 \u2016\u03b8 \u2212 z\u2016 |I| \u2212 s = 1 |I| \u2212 s\u2217 s\u2211\ni=s\u2217+1\nz2i +\n( 1\n|I| \u2212 s\u2217 \u2212 1 |I| \u2212 s ) |I|\u2211 i=s+1 z2i\n\u2265 s\u2212 s \u2217\n|I| \u2212 s\u2217 z2s + s\u2217 \u2212 s (|I| \u2212 s\u2217)(|I| \u2212 s) (|I| \u2212 s)z2s+1 \u2265 0, (10)\nsince the coordinates of z are arranged in decreasing order of magnitude. Combining the above with the observation that, due to the projection property \u2016\u03b8\u2217\u2212z\u2016 \u2265 \u2016\u03b8z\u2212z\u2016, proves the result.\nProof of Theorem 1. Recall that \u03b8t+1 = Ps(\u03b8 t \u2212 \u03b7\n\u2032\nL g t) where \u03b7\u2032 = 23 < 1. Let S t = supp(\u03b8t), S\u2217 = supp(\u03b8\u2217), and St+1 = supp(\u03b8t+1). Also, let It = S\u2217 \u222a St \u222a St+1.\nNow, using the RSS property and the fact that supp(\u03b8t) \u2286 It and supp(\u03b8t+1) \u2286 It, we have:\nf(\u03b8t+1)\u2212 f(\u03b8t) \u2264 \u3008\u03b8t+1 \u2212 \u03b8t, gt\u3009+ L 2 \u2016\u03b8t+1 \u2212 \u03b8t\u201622,\n= L\n2 \u2016\u03b8t+1It \u2212 \u03b8 t It +\n\u03b7\u2032 L \u00b7 gtIt\u2016 2 2 \u2212 (\u03b7\u2032)2 2L \u2016gtIt\u2016 2 2 + (1\u2212 \u03b7\u2032)\u3008\u03b8t+1 \u2212 \u03b8t, gt\u3009. (11)\nAs supp(\u03b8t) = St, supp(\u03b8t+1) = St+1 and St\\St+1, St+1 are disjoint, we have:\n\u3008\u03b8t+1 \u2212 \u03b8t, gt\u3009 = \u2212\u3008\u03b8tSt\\St+1 , g t St\\St+1\u3009+ \u3008\u03b8 t+1 St+1 \u2212 \u03b8tSt+1 , g t St+1\u3009,\n\u03b61 = \u2212\u3008\u03b8tSt\\St+1 , g t St\\St+1\u3009 \u2212\n\u03b7\u2032 L \u2016gtSt+1\u2016 2 2,\n\u03b62 \u2264 \u03b7\n\u2032\n2L \u2016gtSt+1\\St\u2016 2 2 \u2212\n\u03b7\u2032\n2L \u2016gtSt\\St+1\u2016 2 2 \u2212\n\u03b7\u2032 L \u2016gtSt+1\u2016 2 2,\n\u03b63 = \u2212 \u03b7\n\u2032\n2L \u2016gtSt+1\\St\u2016 2 2 \u2212\n\u03b7\u2032\n2L \u2016gtSt\\St+1\u2016 2 2 \u2212\n\u03b7\u2032 L \u2016gtSt\u2229St+1\u2016 2 2\n\u2264 \u2212 \u03b7 \u2032\n2L \u2016gtSt\u222aSt+1\u2016 2 2, (12)\nwhere the equality \u03b61 follows from the gradient step, i.e., \u03b8 t+1 St+1 = \u03b8tSt+1\u2212 \u03b7\u2032 L g t St+1 . The inequality \u03b62 follows using the fact that \u03b8t+1 is obtained using hard thresholding and the fact that |St\\St+1| =\n|St+1\\St|, as follows:\n\u2016\u03b8tSt\\St+1 \u2212 \u03b7\u2032\nL gtSt\\St+1\u2016 2 2 \u2264 \u2016\u03b8t+1St+1\\St\u2016 2 2 =\n(\u03b7\u2032)2\nL2 \u2016gtSt+1\\St\u2016 2 2. (13)\nThe equality \u03b63 follows from \u2016gtSt+1\u2016 2 2 = \u2016gtSt+1\\St\u2016 2 2 + \u2016gtSt\u2229St+1\u2016 2 2.\nHence, using (11) and (12), we have:\nf(\u03b8t+1)\u2212 f(\u03b8t) \u2264 L 2 \u2016\u03b8t+1It \u2212 \u03b8 t It +\n\u03b7\u2032 L \u00b7 gtIt\u2016 2 2 \u2212 (\u03b7\u2032)2 2L \u2016gtIt\u2016 2 2 \u2212 \u03b7\u2032(1\u2212 \u03b7\u2032) 2L \u2016gtSt\u222aSt+1\u2016 2 2,\n= L\n2 \u2016\u03b8t+1It \u2212 \u03b8 t It +\n\u03b7\u2032 L \u00b7 gtIt\u2016 2 2 \u2212 (\u03b7\u2032)2 2L \u2016gtIt\\(St\u222aS\u2217)\u2016 2 2 \u2212 (\u03b7\u2032)2 2L \u2016gtSt\u222aS\u2217\u2016 2 2\n\u2212 \u03b7 \u2032(1\u2212 \u03b7\u2032)\n2L \u2016gtSt\u222aSt+1\u2016 2 2. (14)\nNext, let us try to upper bound the first two terms on the right hand side above. Since It\\(St \u222a S\u2217) = St+1\\(St \u222a S\u2217) \u2286 St+1, we have \u03b8t+1It\\(St\u222aS\u2217) = \u03b8 t It\\(St\u222aS\u2217) \u2212 \u03b7\u2032 L g t It\\(St\u222aS\u2217). However, as \u03b8tIt\\St = 0, we actually have \u03b8 t+1 It\\(St\u222aS\u2217) = \u2212 \u03b7\u2032 L g t It\\(St\u222aS\u2217). Now let us choose a set R \u2286 S t\\St+1 such that |R| = |St+1\\(St \u222a S\u2217)|. Such a choice is possible since |St+1\\(St \u222a S\u2217)| = |St\\St+1| \u2212 |(St+1 \u2229 S\u2217)\\St| (which itself is a consequence of the fact that |St+1| = |St|). Moreover, since \u03b8t+1\nis obtained by hard-thresholding ( \u03b8t \u2212 \u03b7 \u2032 L g t ) , for any choice of R made above, we have:\n(\u03b7\u2032)2\nL2 \u2016gtIt\\(St\u222aS\u2217)\u2016 2 2 = \u2016\u03b8t+1It\\(St\u222aS\u2217)\u2016 2 2 \u2265 \u2016\u03b8tR \u2212\n\u03b7\u2032 L gtR\u201622. (15)\nUsing above equation, and the fact that \u03b8t+1R = 0 (since R \u2286 St+1), we have:\nL 2 \u2016\u03b8t+1It \u2212 \u03b8 t It +\n\u03b7\u2032 L \u00b7 gtIt\u2016 2 2 \u2212 (\u03b7\u2032)2 2L \u2016gtIt\\(St\u222aS\u2217)\u2016 2 2\n\u2264 L 2 \u2016\u03b8t+1It \u2212 \u03b8 t It +\n\u03b7\u2032 L \u00b7 gtIt\u2016 2 2 \u2212 L 2 \u2016\u03b8t+1R \u2212 \u03b8 t R + \u03b7\u2032 L gtR\u201622\n= L\n2 \u2016\u03b8t+1It\\R \u2212 \u03b8 t It\\R +\n\u03b7\u2032 L \u00b7 gtIt\\R\u2016 2 2. (16)\nWe can bound the size of It\\R as |It\\R| \u2264 |St+1|+|(St\\St+1)\\R|+|S\u2217| \u2264 s+|(St+1\u2229S\u2217)\\St|+s\u2217 \u2264 s+ 2s\u2217. Also, since St+1 \u2286 (It\\R), we have \u03b8t+1It\\R = Ps(\u03b8 t It\\R \u2212 \u03b7\u2032 L g t It\\R).\nUsing the above observation with (16) and Lemma 1, we get:\nL 2 \u2016\u03b8t+1It \u2212 \u03b8 t It +\n\u03b7\u2032 L \u00b7 gtIt\u2016 2 2 \u2212 (\u03b7\u2032)2 2L \u2016gtIt\\(St\u222aS\u2217)\u2016 2 2\n\u2264 L 2 \u00b7 |I t\\R| \u2212 s |It\\R| \u2212 s\u2217 \u2016\u03b8\u2217It\\R \u2212 \u03b8 t It\\R + \u03b7\u2032 L \u00b7 gtIt\\R\u2016 2 2,\n\u03b61 \u2264 L 2 \u00b7 2s\n\u2217\ns+ s\u2217 \u2016\u03b8\u2217It \u2212 \u03b8 t It +\n\u03b7\u2032 L \u00b7 gtIt\u2016 2 2,\n= 2s\u2217 s+ s\u2217 \u00b7 ( \u03b7\u2032\u3008\u03b8\u2217 \u2212 \u03b8t, gt\u3009+ L 2 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622 + (\u03b7\u2032)2 2L \u2016gtIt\u2016 2 2 ) ,\n\u03b62 \u2264 2s\n\u2217 s+ s\u2217 \u00b7 ( \u03b7\u2032f(\u03b8\u2217)\u2212 \u03b7\u2032f(\u03b8t) + L\u2212 \u03b7 \u2032\u03b1 2 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622 + (\u03b7\u2032)2 2L \u2016gtIt\u2016 2 2 ) , (17)\nwhere the inequality \u03b61 follows by |It\\R| \u2264 s+ 2s\u2217 as shown earlier and the observation that x\u2212ax\u2212b is a positive and increasing function on the interval x \u2265 a if a \u2265 b \u2265 0. Note that since we have St+1 \u2286 (It\\R), we get |It\\R| \u2265 s. The inequality \u03b62 follows by using RSC.\nUsing (14), (17), and using St+1\\(St \u222a S\u2217) \u2286 (St+1 \u222a St), we get:\nf(\u03b8t+1)\u2212 f(\u03b8t) \u2264 2s \u2217 s+ s\u2217 \u00b7 ( \u03b7\u2032f(\u03b8\u2217)\u2212 \u03b7\u2032f(\u03b8t) + L\u2212 \u03b7 \u2032\u03b1 2 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622 + (\u03b7\u2032)2 2L \u2016gtIt\u2016 2 2 ) \u2212 (\u03b7 \u2032)2\n2L \u2016gtSt\u222aS\u2217\u2016 2 2 \u2212 \u03b7\u2032(1\u2212 \u03b7\u2032) 2L \u2016gtSt+1\\(St\u222aS\u2217)\u2016 2 2. (18)\nWe now set \u03b7\u2032 = 2/3 as per our earlier choice and set s = 32 ( L \u03b1 )2 s\u2217, so that we have 2s \u2217\ns+s\u2217 \u2264 \u03b12\n16L(L\u2212\u03b7\u2032\u03b1) . Since L \u2265 \u03b1, we also have \u03b12 16L(L\u2212\u03b7\u2032\u03b1) \u2264 3 16 . Using these inequalities, we now rearrange the terms in (18) above.\nf(\u03b8t+1)\u2212 f(\u03b8t) \u2264 2s \u2217\ns+ s\u2217 \u00b7 \u03b7\u2032 \u00b7\n( f(\u03b8\u2217)\u2212 f(\u03b8t) ) + \u03b12\n32L \u2016\u03b8\u2217 \u2212 \u03b8t\u201622 +\n1\n24L \u2016gtIt\u2016 2 2\n\u2212 2 9L \u2016gtSt\u222aS\u2217\u2016 2 2 \u2212 1 9L \u2016gtSt+1\\(St\u222aS\u2217)\u2016 2 2. (19)\nSplitting \u2016gtIt\u2016 2 2 = \u2016gtSt\u222aS\u2217\u2016 2 2 + \u2016gtSt+1\\(St\u222aS\u2217)\u2016 2 2 gives us\nf(\u03b8t+1)\u2212 f(\u03b8t) \u2264 2s \u2217\ns+ s\u2217 \u00b7 \u03b7\u2032 \u00b7\n( f(\u03b8\u2217)\u2212 f(\u03b8t) ) \u2212 1\n2L\n( 13\n36 \u2016gtSt\u222aS\u2217\u2016 2 2 \u2212\n\u03b12 16 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622 ) \u2212 1 2L \u00b7 ( 4 9 \u2212 1 12 ) \u2016gtSt+1\\(St\u222aS\u2217)\u2016 2 2,\n\u2264 2s \u2217\ns+ s\u2217 \u00b7 \u03b7\u2032 \u00b7\n( f(\u03b8\u2217)\u2212 f(\u03b8t) ) \u2212 13\n72L\n( \u2016gtSt\u222aS\u2217\u2016 2 2 \u2212 \u03b12\n4 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622 ) \u2264 2s \u2217\ns+ s\u2217 \u00b7 \u03b7\u2032 \u00b7\n( f(\u03b8\u2217)\u2212 f(\u03b8t) ) \u2212 \u03b1\n12L\n( f(\u03b8t)\u2212 f(\u03b8\u2217) ) , (20)\nwhere the last inequality above follows using Lemma 6. The result now follows by observing that 2s\u2217 s+s\u2217 \u2265 0. Lemma 6. ( \u2016gtSt\u222aS\u2217\u2016 2 2 \u2212 \u03b12\n4 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622\n) \u2265 \u03b1 2 \u00b7 ( f(\u03b8t)\u2212 f(\u03b8\u2217) ) .\nProof. Using the RSC property, we have:\nf(\u03b8t)\u2212 f(\u03b8\u2217) \u2264 \u3008gt,\u03b8t \u2212 \u03b8\u2217\u3009 \u2212 \u03b1 2 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622\n= \u3008gtSt\u222aS\u2217 ,\u03b8 t St\u222aS\u2217 \u2212 \u03b8 \u2217 St\u222aS\u2217\u3009 \u2212\n\u03b1 2 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622,\n\u2264 \u2016gtSt\u222aS\u2217\u20162\u2016\u03b8 t \u2212 \u03b8\u2217\u20162 \u2212\n\u03b1 2 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622. (21)\nNow,\n\u2016gtSt\u222aS\u2217\u2016 2 2 \u2212\n\u03b12\n4 \u2016\u03b8\u2217 \u2212 \u03b8t\u201622 =\n( \u2016gtSt\u222aS\u2217\u20162 \u2212 \u03b1\n2 \u2016\u03b8\u2217 \u2212 \u03b8t\u20162\n)( \u2016gtSt\u222aS\u2217\u20162 + \u03b1\n2 \u2016\u03b8\u2217 \u2212 \u03b8t\u20162\n) ,\n\u2265 ( f(\u03b8t)\u2212 f(\u03b8\u2217) ) \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u00b7 ( \u2016gtSt\u222aS\u2217\u20162 + \u03b1 2 \u2016\u03b8\u2217 \u2212 \u03b8t\u20162 ) \u2265 \u03b1\n2 \u00b7 ( f(\u03b8t)\u2212 f(\u03b8\u2217) ) , (22)\nwhere the first inequality above follows from (21)."}, {"heading": "B Proofs for Section 4", "text": "Proof of Theorem 3. Let \u03b8\u2217 be the empirical loss minimizer over the set of s-sparse vectors. Then invoking Theorem 1 with f = L(\u00b7;Z1:n), we get\nL(\u03b8\u03c4 , Z1:n)\u2212 \u2264 L(\u03b8\u2217, Z1:n) \u2264 L(\u03b8\u0304, Z1:n)\n\u2264 L(\u03b8\u03c4 ;Z1:n) + \u3008\u2207L(\u03b8\u0304;Z1:n), (\u03b8\u0304 \u2212 \u03b8\u03c4 )\u3009 \u2212 \u03b1s+s\u2217\n2 \u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u201622\nwhere the 2nd inequality is by definition of \u03b8\u2217 and 3rd is by RSC (since \u03b8\u2217,\u03b8\u03c4 are s\u2217, s sparse). Duality gives us the upper bound\n\u3008\u2207L(\u03b8\u0304;Z1:n), (\u03b8\u0304 \u2212 \u03b8\u03c4 )\u3009 \u2264 \u2016\u2207L(\u03b8\u0304;Z1:n)\u2016\u221e\u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u20161 \u2264 \u221a s+ s\u2217\u2016\u2207L(\u03b8\u0304;Z1:n)\u2016\u221e\u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u20162\nCombining the last two inequalities and rearranging gives a quadratic inequality in \u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u20162:\n\u03b1s+s\u2217\n2 \u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u201622 \u2212\n\u221a s+ s\u2217\u2016\u2207L(\u03b8\u0304;Z1:n)\u2016\u221e\u2016\u03b8\u0304 \u2212 \u03b8\u03c4\u20162 \u2212 \u2264 0\nthat immediately yields the result."}, {"heading": "C Proofs for Section 5", "text": "Proof of Lemma 3. We will start by proving a more general result of which the claimed result will be a corollary. More specifically, we shall prove that for any \u03b3 \u2265 1\u03b1 , we have\n2\u03b3(f(\u03b8t)\u2212 f(\u03b8\u2217)) \u2264 2\u03b3 ( f(\u03b8t)\u2212 f(\u03b8\u2217) + \u03b1 2 \u00b7 ( 1\u2212 1 \u03b1\u03b3 ) \u2016\u03b8t \u2212 \u03b8\u2217\u201622 ) \u2264 \u03b32\u2016gtSt\u222aS\u2217\u2016 2 2 \u2212 \u2016\u03b8tSt\\S\u2217\u2016 2 2,\nSetting \u03b3 = 1\u03b1 will yield the claimed result. It is easy to see that the following inequality holds trivially since \u03b3 \u2265 1\u03b1\n2\u03b3(f(\u03b8t)\u2212 f(\u03b8\u2217)) \u2264 2\u03b3 ( f(\u03b8t)\u2212 f(\u03b8\u2217) + \u03b1 2 \u00b7 ( 1\u2212 1 \u03b1\u03b3 ) \u2016\u03b8t \u2212 \u03b8\u2217\u201622 ) .\nFor the second inequality, we first use the RSC condition to obtain:\nf(\u03b8\u2217)\u2212 f(\u03b8t) \u2265 \u3008\u03b8\u2217 \u2212 \u03b8t, gt\u3009+ \u03b1 2 \u2016\u03b8t \u2212 \u03b8\u2217\u201622.\nNow let MDt = S \u2217\\St be the set of true support elements missing from \u03b8t and FAt = St\\S\u2217 be the set of incorrect elements included in the support of \u03b8t. Since \u03b8t is obtained by a \u201cfully corrective\u201d process (recall \u03b8t = arg min\u03b8,supp(\u03b8)\u2286St f(\u03b8)), we have g t St = 0. Thus \u3008\u03b8 \u2217\u2212\u03b8t, gt\u3009 = \u3008\u03b8\u2217MDt , g t MDt \u3009.\nPutting this into the above expansion gives\n\u3008\u03b8\u2217MDt , g t MDt\u3009 \u2264 f(\u03b8 \u2217)\u2212 f(\u03b8t)\u2212 \u03b1 2 \u2016\u03b8t \u2212 \u03b8\u2217\u201622 (23)\nWe now present some simple inequalities that will help us get our desired bounds. Firstly, we have\n\u2016\u03b8\u2217MDt + \u03b3g t MDt\u2016 2 2 = \u2016\u03b8\u2217MDt\u2016 2 2 + \u03b3 2\u2016gtMDt\u2016 2 2 + 2\u03b3\u3008\u03b8\u2217MDt , g t MDt\u3009 \u2265 0, (24)\nsince the first expression is a norm. Next, since MDt \u2229 FAt = \u2205, we have\n\u2016\u03b8\u2217 \u2212 \u03b8t\u201622 \u2265 \u2016\u03b8\u2217MDt\u2016 2 2 + \u2016\u03b8tFAt\u2016 2 2. (25)\nPutting equations 23 and 24, we have: 2\u03b3 ( f(\u03b8t)\u2212 f(\u03b8\u2217) + \u03b1\n2 \u2016\u03b8t \u2212 \u03b8\u2217\u201622\n) \u2264 \u2016\u03b8\u2217MDt\u2016 2 2 + \u03b3 2\u2016gtMDt\u2016 2 2. (26)\nNow, using (25), we get:\n2\u03b3 ( f(\u03b8t)\u2212 f(\u03b8\u2217) + \u03b1\n2\n( 1\u2212 1\n\u03b1\u03b3\n) \u2016\u03b8t \u2212 \u03b8\u2217\u201622 ) \u2264 \u03b32\u2016gtMDt\u2016 2 2 \u2212 \u2016\u03b8tFAt\u2016 2 2\nWe finish off the proof by noticing that since gtSt = 0, we have \u2016g t MDt \u201622 = \u2016gtSt\u222aS\u2217\u2016 2 2\nProof of Theorem 4. Let ztSt = \u03b8 t St , z t Zt\\St = \u2212 1 Lg t Zt\\St , and z t Zt\n= 0. Then, using the RSS property, we have:\nf(zt)\u2212 f(\u03b8t) \u2264 \u3008zt \u2212 \u03b8t, gt\u3009+ L 2 \u2016zt \u2212 \u03b8t\u201622,\n\u03b61 \u2264 \u2212 1\nL \u2016gtZt\\St\u2016 2 2 +\nL 2 \u2016ztZt\\St\u2016 2 2,\n\u03b62 = \u2212 1\n2L \u00b7 \u2016gtZt\\St\u2016 2 2,\n\u03b63 \u2264 \u2212 1\n2L \u00b7 \u2016gtS\u2217\\St\u2016 2 2,\n\u03b64 \u2264 \u2212\u03b1 L \u00b7 ( f(\u03b8t)\u2212 f(\u03b8\u2217) ) , (27)\nwhere \u03b61 follows by observing g t St = 0, and S t \u2286 Zt. \u03b62 follows by ztZt\\St = \u2212 1 Lg t Zt\\St . \u03b63 follows by ` \u2265 s\u2217, and Zt\\St are the ` largest elements of |gtZt\\St |. Now, using Lemma 4 and (27) along with f(\u03b8t+1) \u2264 f(\u03b8\u0303t) and f(\u03b2t) \u2264 f(zt), we have:\nf(\u03b8t+1)\u2212 f(\u03b8\u2217) \u2264 (\n1\u2212 \u03b1 L\n) \u00b7 ( 1 + L\n\u03b1 \u00b7 ` s+ `\u2212 s\u2217\n) \u00b7 ( f(\u03b8t)\u2212 f(\u03b8\u2217) ) . (28)\nTheorem now follows by using the above equation with the assumption that s+ `\u2212 s\u2217 \u2265 4L2\u00b7` \u03b12 .\nProof of Theorem 5. Using RSS property:\nf(vt)\u2212 f(\u03b8t) \u2264 \u3008vt \u2212 \u03b8t, gt\u3009+ L 2 \u2016vt \u2212 \u03b8t\u201622,\n\u03b61 \u2264 \u2212\u03b7\u2016gtSt+1\\St\u2016 2 2 + L\n2 (\u2016vtSt+1\\St\u2016 2 2 + \u2016\u03b8tSt\\St+1\u2016 2),\n\u03b62 \u2264 \u2212\u03b7\u2016gtSt+1\\St\u2016 2 2 + L\u2016vtSt+1\\St\u2016 2 2,\n\u03b63 = \u2212 (1\u2212 \u03b7 \u00b7 L) \u00b7 \u03b7 \u00b7 \u2016gtSt+1\\St\u2016 2 2, (29)\nwhere \u03b61 follows by observing that g t St = 0 and v t St+1\\St = \u2212\u03b7g t St+1\\St . \u03b62 follows by the property of PHT operator which ensures that each element of vtSt+1\\St is bigger than \u03b8 t St\\St+1 and by using |St+1\\St| = |St\\St+1|. \u03b63 follows by using vtSt+1\\St = \u2212\u03b7g t St+1\\St .\nSimilar to the analysis given in [17], we divide the analysis in three mutually exclusive cases. The lemma then follows by combining (29) with the case-by-case analyses below and observing that f(\u03b8t+1) \u2264 f(vt) because of the fully corrective step.\nCase 1: |St+1\\St| = ` < |S\u2217\\St|. In this case, As vtSt+1\\St is obtained by selecting |S t+1\\St|\nlargest elements of zt St\u222abott . Hence,\n\u03b72\u2016gtSt+1\\St\u2016 2 2 \u2265 min ( 1, |St+1\\St| |S\u2217\\St| ) \u2016ztS\u2217\\St\u2016 2 2\n= \u03b72 min ( 1, |St+1\\St| |S\u2217\\St| ) \u2016gtS\u2217\\St\u2016 2 2\n= \u03b72 min ( 1, |St+1\\St| |S\u2217\\St| ) \u2016gtS\u2217\u222aSt\u2016 2 2, (30)\nsince gtSt = 0. Now, using the fact that |S t+1\\St| = `, |S\u2217\\St| \u2264 s\u2217, and using Lemma 3, we have:\n\u2016gtSt+1\\St\u2016 2 2 \u2265 2\u03b1 \u00b7min\n( 1, `\ns\u2217\n)( f(\u03b8t)\u2212 f(\u03b8\u2217) ) . (31)\nCase 2: |St+1\\St| < `, |St+1\\St| \u2264 |S\u2217\\St|. In this case, each element of ztSt+1\u2229St is larger than each element of ztS\u2217\\(St+1\u222aSt), else that element of S\n\u2217\\(St+1 \u222a St) would have been selected. That is,\n\u2016ztS\u2217\\(St+1\u222aSt)\u2016 2 2 |S\u2217\\(St+1 \u222a St)| \u2264 \u2016zt(St+1\u2229St)\\S\u2217\u2016 2 2 |(St+1 \u2229 St)\\S\u2217| .\nUsing ztS\u2217\\St = \u2212\u03b7g t S\u2217\\St , z t St = \u03b8 t St and (S t+1 \u2229 St)\\S\u2217 \u2286 St\\S\u2217, we have:\n\u03b72\u2016gtS\u2217\\(St\u222aSt+1)\u2016 2 2 \u2264\ns\u2217\ns\u2212 `\u2212 s\u2217 \u2016\u03b8tSt\\S\u2217\u2016 2 2, (32)\nwhere the bound on |S \u2217\\(St+1\u222aSt)| |(St+1\u2229St)\\S\u2217| follows by observing |S \u2217\\(St+1 \u222a St)| \u2264 s\u2217 and |(St+1 \u2229 St)\\S\u2217| \u2265 s \u2212 ` \u2212 s\u2217. Using (32) and the fact that each element of vt(St+1\\St)\u2229S\u2217 is selected from the largest |(St+1\\St) \u2229 S\u2217| elements of \u2212\u03b7 \u00b7 gt(St+1\\St)\u2229S\u2217 we have:\n\u03b72\u2016gtS\u2217\\St\u2016 2 2 \u2264 \u03b72 ( \u2016gt(St+1\\St)\u2229S\u2217\u2016 2 2 + \u2016gtS\u2217\\(St+1\u222aSt)\u2016 2 2 ) \u2264 ( \u03b72\u2016gt(St+1\\St)\u2229S\u2217\u2016 2 2 +\ns\u2217\ns\u2212 `\u2212 s\u2217 \u2016\u03b8tSt\\S\u2217\u2016 2 2\n) .\n(33)\nUsing the above equation and Lemma 3, we have:\n2\n\u03b1\n( f(\u03b8t)\u2212 f(\u03b8\u2217) ) \u2264 1 \u03b12 \u2016gt(St+1\\St)\u2229S\u2217\u2016 2 2 +\n( 1\n\u03b12\u03b72 \u00b7 s\n\u2217 s\u2212 `\u2212 s\u2217 \u2212 1 ) \u2016\u03b8tSt\\S\u2217\u2016 2 2, (34)\nUsing s \u2265 4 ( L \u03b1 )2 s\u2217, we have:\n\u2016gtSt+1\\St\u2016 2 2 \u2265 2\u03b1\n( f(\u03b8t)\u2212 f(\u03b8\u2217) ) . (35)\nCase 3: |St+1\\St| \u2265 |S\u2217\\St|. Now, as vtSt+1\\St is obtained by selecting |S t+1\\St| largest\nelements of zt St\u222abott . Hence, using Lemma 3, we have:\n\u2016vtSt+1\\St\u2016 2 2 \u2265 \u2016ztS\u2217\\St\u2016 2 2 = \u03b7 2\u2016gtS\u2217\\St\u2016 2 2 \u2265 2\u03b72 \u00b7 \u03b1 \u00b7\n( f(\u03b8t)\u2212 f(\u03b8\u2217) ) . (36)\nThe lemma now follows by combining (29), (31), (35), and (36)"}, {"heading": "D Supplementary Experimental Results", "text": "Below we present plots that were not included in the main text.\n0 0.1 0.2 0.3 0.4 0\n20\n40\n60\n80\nNoise level (sigma)\nS up\npo rt\nR ec\nov er\ny E\nrr or\nOMPR CoSaMP L1 SP\n(a)\n0.5 1 1.5 2 2.5\nx 104\n0\n50\n100\n150\n200\nDimensionality (p)\nR un\ntim e\n(s ec\n)\nOMPR CoSaMP L1 SP\n(b)\n0 100 200 300 400 500 10\u22123 10\u22122\n100\n102\n104\nSparsity (s*)\nR un\ntim e\n(s ec\n)\nOMPR CoSaMP L1 SP\n(c)"}], "references": [{"title": "Statistics for high-dimensional data: methods, theory and applications", "author": ["Peter B\u00fchlmann", "Sara Van De Geer"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Minimax rates of estimation for high-dimensional linear regression over `q-balls", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Estimation of high-dimensional low-rank matrices", "author": ["Angelika Rohde", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Sparse approximate solutions to linear systems", "author": ["Balas Kausik Natarajan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Forward-backward greedy algorithms for general convex smooth functions over a cardinality constraint", "author": ["Ji Liu", "Ryohei Fujimaki", "Jieping Ye"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "On learning discrete graphical models using greedy methods", "author": ["Ali Jalali", "Christopher C Johnson", "Pradeep D Ravikumar"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1935}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["Shai Shalev-Shwartz", "Nathan Srebro", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87 of Applied Optimization", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima", "author": ["P. Loh", "M.J. Wainwright"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Thomas Blumensath", "Mike E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Rahul Garg", "Rohit Khandekar"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Hard thresholding pursuit: an algorithm for compressive sensing", "author": ["Simon Foucart"], "venue": "SIAM J. on Num. Anal.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "CoSaMP: Iterative Signal Recovery from Incomplete and Inaccurate Samples", "author": ["Deanna Needell", "Joel A. Tropp"], "venue": "Appl. Comput. Harmon. Anal.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["Wei Dai", "Olgica Milenkovic"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Orthogonal matching pursuit with replacement", "author": ["Prateek Jain", "Ambuj Tewari", "Inderjit S. Dhillon"], "venue": "In Annual Conference on Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Greedy sparsity-constrained optimization", "author": ["Sohail Bahmani", "Bhiksha Raj", "Petros T Boufounos"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Gradient hard thresholding pursuit for sparsityconstrained optimization", "author": ["Xiaotong Yuan", "Ping Li", "Tong Zhang"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Lower bounds on the performance of polynomial-time algorithms for sparse linear regression", "author": ["Yuchen Zhang", "Martin J. Wainwright", "Michael I. Jordan"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1918}, {"title": "High-dimension regression with noisy and missing data: Provable guarantees with non-convexity", "author": ["P. Loh", "M.J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Fast global convergence of gradient methods for high-dimensional statistical recovery", "author": ["Alekh Agarwal", "Sahand N. Negahban", "Martin J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Graphical Model Structure Learning with L1-Regularization", "author": ["Mark Schmidt"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations", "author": ["Tong Zhang"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 1, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 2, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 3, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the \u201cn p\u201d regime as well.", "startOffset": 57, "endOffset": 72}, {"referenceID": 5, "context": "Examples include sparse regression which requires loss minimization with sparsity constraints and low-rank regression which requires dealing with rank constraints which are not efficiently solvable in general [6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 111, "endOffset": 120}, {"referenceID": 7, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 111, "endOffset": 120}, {"referenceID": 8, "context": "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.", "startOffset": 111, "endOffset": 120}, {"referenceID": 9, "context": "[10] do not apply to these techniques due to the non-convex structure of the problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "An exception to this is the recent work [11] that demonstrates that PGD with non-convex regularization can offer consistent estimates for certain high-dimensional problems.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "However, the work in [11] is only able to analyze penalties such as SCAD, MCP and capped L1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 162, "endOffset": 166}, {"referenceID": 12, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 177, "endOffset": 181}, {"referenceID": 13, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 215, "endOffset": 219}, {"referenceID": 14, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 228, "endOffset": 232}, {"referenceID": 15, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 256, "endOffset": 260}, {"referenceID": 16, "context": "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.", "startOffset": 274, "endOffset": 278}, {"referenceID": 16, "context": "The best known such constant is due to the work of [17] that requires a bound on the RIP constant \u03b42k \u2264 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting.", "startOffset": 92, "endOffset": 100}, {"referenceID": 18, "context": "Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting.", "startOffset": 92, "endOffset": 100}, {"referenceID": 19, "context": "Our bounds are tight, achieve known minmax lower bounds [20], and hold for arbitrary differentiable, possibly even non-convex functions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Our results effortlessly extend to the noisy setting as a corollary and give bounds similar to those of [21] that relies on solving an L1 regularized problem.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "Two models of noise are popular in literature [21]: a) (additive noise) X\u0303i = Xi+Wi where Wi \u223c N (0,\u03a3W ), and b) (missing data) X\u0303 is an R\u222a{?}-valued matrix obtained by independently, with probability \u03bd \u2208 [0, 1), replacing each entry in X with ?.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": "We now study Partial Hard Thresholding methods (PHT), a family of fully-corrective iterative methods introduced by [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 22, "context": "We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.", "startOffset": 150, "endOffset": 154}, {"referenceID": 4, "context": "A unified analysis for general structures will probably create interesting connections with existing unified frameworks such as those based on decomposability [5] and atomic norms [25].", "startOffset": 159, "endOffset": 162}], "year": 2014, "abstractText": "The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. We also extend our analysis to a large family of \u201cfully corrective methods\u201d that includes two-stage and partial hard-thresholding algorithms. We show that our results hold for the problem of sparse regression, as well as low-rank matrix recovery.", "creator": "LaTeX with hyperref package"}}}