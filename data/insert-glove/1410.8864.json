{"id": "1410.8864", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2014", "title": "Greedy Subspace Clustering", "abstract": "semi-empirical We consider the k\u014db\u014d problem imd of vatos subspace apoplexy clustering: haunch given tausug points that lie on or near the union dmo of many low - dimensional linear zafran subspaces, haili recover the shom\u0101reh-ye subspaces. To this suasion end, atx one lulus first cyrenaean identifies sets of points orobanche close to hugg the same nictd subspace attas and timofey uses mavrocordatos the sets to estimate the subspaces. As gengold the geometric kurten structure kondratiev of r33 the clusters (first-rate linear torlonia subspaces) rohmer forbids proper performance of ilsfeld general recapped distance based lerici approaches such 20min as K - means, bigley many model - specific calipso methods khufu have comediennes been piute proposed. heppenheim In intravenously this paper, hungle we provide trahlt new simple appan and efficient algorithms 90.48 for this biologics problem. Our refurbishes statistical 30,700 analysis shows that helmig the 9-16 algorithms ivory are guaranteed exact (perfect) caption clustering damiri performance 1,743 under thrustmaster certain lift-to-drag conditions on the number of points kultury and ganzi the saarbrucken affinity 7x between subspaces. These railmotor conditions montebelluna are weaker sequoia than those 88-84 considered deramus in ------------------------------------------------------ the printers standard asmi statistical hairi literature. maryland-based Experimental 180.1 results ys on synthetic data slow-wave generated from draves the 183-run standard nov\u00e1\u010dek unions of subspaces consilium model demonstrate 16:26 our brabin theory. skjerv\u00f8y We lawman also krasicki show deathmatches that misgav our lionnet algorithm satirizing performs competitively against powerscourt state - rubel of - bruinsma the - art slobo algorithms heav on pinafore real - clubhead world dimitroff applications such st.mary as wittig motion segmentation toback and face dellucci clustering, with much inward simpler implementation 458 and 434th lower debts computational cost.", "histories": [["v1", "Fri, 31 Oct 2014 19:50:42 GMT  (380kb,D)", "http://arxiv.org/abs/1410.8864v1", "To appear in NIPS 2014"]], "COMMENTS": "To appear in NIPS 2014", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["dohyung park", "constantine caramanis", "sujay sanghavi"], "accepted": true, "id": "1410.8864"}, "pdf": {"name": "1410.8864.pdf", "metadata": {"source": "CRF", "title": "Greedy Subspace Clustering", "authors": ["Dohyung Park", "Constantine Caramanis", "Sujay Sanghavi"], "emails": ["dhpark@utexas.edu", "constantine@utexas.edu", "sanghavi@mail.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "Subspace clustering is a classic problem where one is given points in a high-dimensional ambient space and would like to approximate them by a union of lower-dimensional linear subspaces. In particular, each subspace contains a subset of the points. This problem is hard because one needs to jointly find the subspaces, and the points corresponding to each; the data we are given are unlabeled. The unions of subspaces model naturally arises in settings where data from multiple latent phenomena are mixed together and need to be separated. Applications of subspace clustering include motion segmentation [23], face clustering [8], gene expression analysis [10], and system identification [22]. In these applications, data points with the same label (e.g., face images of a person under varying illumination conditions, feature points of a moving rigid object in a video sequence) lie on a low-dimensional subspace, and the mixed dataset can be modeled by unions of subspaces. For detailed description of the applications, we refer the readers to the reviews [10, 20] and references therein.\nThere is now a sizable literature on empirical methods for this particular problem and some statistical analysis as well. Many recently proposed methods, which perform remarkably well and\nar X\niv :1\n41 0.\n88 64\nv1 [\nst at\n.M L\n] 3\n1 O\nhave theoretical guarantees on their performances, can be characterized as involving two steps: (a) finding a \u201cneighborhood\u201d for each data point, and (b) finding the subspaces and/or clustering the points given these neighborhoods. Here, neighbors of a point are other points that the algorithm estimates to lie on the same subspace as the point (and not necessarily just closest in Euclidean distance).\nOur contributions: In this paper we devise new algorithms for each of the two steps above; (a) we develop a new method, Nearest Subspace Neighbor (NSN), to determine a neighborhood set for each point, and (b) a new method, Greedy Subspace Recovery (GSR), to recover subspaces from given neighborhoods. Each of these two methods can be used in conjunction with other methods for the corresponding other step; however, in this paper we focus on two algorithms that use NSN followed by GSR and Spectral clustering, respectively. Our main result is establishing statistical guarantees for exact clustering with general subspace conditions, in the standard models considered in recent analytical literature on subspace clustering. Our condition for exact recovery is weaker than the conditions of other existing algorithms that only guarantee correct neighborhoods1, which do not always lead to correct clustering. We provide numerical results which demonstrate our theory. We also show that for the real-world applications our algorithm performs competitively against those of state-of-the-art algorithms, but the computational cost is much lower than them. Moreover, our algorithms are much simpler to implement."}, {"heading": "1.1 Related work", "text": "The problem was first formulated in the data mining community [10]. Most of the related work in this field assumes that an underlying subspace is parallel to some canonical axes. Subspace clustering for unions of arbitrary subspaces is considered mostly in the machine learning and the computer vision communities [20]. Most of the results from those communities are based on empirical justification. They provided algorithms derived from theoretical intuition and showed that they perform empirically well with practical dataset. To name a few, GPCA [21], Spectral curvature clustering (SCC) [2], and many iterative methods [1, 19, 26] show their good empirical performance for subspace clustering. However, they lack theoretical analysis that guarantees exact clustering.\n1By correct neighborhood, we mean that for each point every neighbor point lies on the same subspace.\nAs described above, several algorithms with a common structure are recently proposed with both theoretical guarantees and remarkable empirical performance. Elhamifar and Vidal [4] proposed an algorithm called Sparse Subspace Clustering (SSC), which uses `1-minimization for neighborhood construction. They proved that if the subspaces have no intersection2, SSC always finds a correct neighborhood matrix. Later, Soltanolkotabi and Candes [16] provided a statistical guarantee of the algorithm for subspaces with intersection. Dyer et al. [3] proposed another algorithm called SSC-OMP, which uses Orthogonal Matching Pursuit (OMP) instead of `1-minimization in SSC. Another algorithm called Low-Rank Representation (LRR) which uses nuclear norm minimization is proposed by Liu et al. [14]. Wang et al. [24] proposed an hybrid algorithm, Low-Rank and Sparse Subspace Clustering (LRSSC), which involves both `1-norm and nuclear norm. Heckel and Bo\u0308lcskei [6] presented Thresholding based Subspace Clustering (TSC), which constructs neighborhoods based on the inner products between data points. All of these algorithms use spectral clustering for the clustering step.\nThe analysis in those papers focuses on neither exact recovery of the subspaces nor exact clustering in general subspace conditions. SSC, SSC-OMP, and LRSSC only guarantee correct neighborhoods which do not always lead to exact clustering. LRR guarantees exact clustering only when the subspaces have no intersections. In this paper, we provide novel algorithms that guarantee exact clustering in general subspace conditions. When we were preparing this manuscript, it is proved that TSC guarantees exact clustering under certain conditions [7], but the conditions are stricter than ours. (See Table 1)"}, {"heading": "1.2 Notation", "text": "There is a set of N data points in Rp, denoted by Y = {y1, . . . , yN}. The data points are lying on or near a union of L subspaces D = \u222aLi=1Di. Each subspace Di is of dimension di which is smaller than p. For each point yj , wj denotes the index of the nearest subspace. Let Ni denote the number\nof points whose nearest subspace is Di, i.e., Ni = \u2211N\nj=1 Iwj=i. Throughout this paper, sets and subspaces are denoted by calligraphic letters. Matrices and key parameters are denoted by letters in upper case, and vectors and scalars are denoted by letters in lower case. We frequently denote the set of n indices by [n] = {1, 2, . . . , n}. As usual, span{\u00b7} denotes a subspace spanned by a set of vectors. For example, span{v1, . . . , vn} = {v : v = \u2211n i=1 \u03b1ivi, \u03b11, . . . , \u03b1n \u2208 R}. ProjUy is defined as the projection of y onto subspace U . That is, ProjUy = arg minu\u2208U \u2016y \u2212 u\u20162. I{\u00b7} denotes the indicator function which is one if the statement is true and zero otherwise. Finally, \u2295 denotes the direct sum."}, {"heading": "2 Algorithms", "text": "We propose two algorithms for subspace clustering as follows.\n\u2022 NSN+GSR : Run Nearest Subspace Neighbor (NSN) to construct a neighborhood matrix W \u2208 {0, 1}N\u00d7N , and then run Greedy Subspace Recovery (GSR) for W .\n\u2022 NSN+Spectral : Run Nearest Subspace Neighbor (NSN) to construct a neighborhood matrix W \u2208 {0, 1}N\u00d7N , and then run spectral clustering for Z = W +W>.\n2By no intersection between subspaces, we mean that they share only the null point."}, {"heading": "2.1 Nearest Subspace Neighbor (NSN)", "text": "NSN approaches the problem of finding neighbor points most likely to be on the same subspace in a greedy fashion. At first, given a point y without any other knowledge, the one single point that is most likely to be a neighbor of y is the nearest point of the line span{y}. In the following steps, if we have found a few correct neighbor points (lying on the same true subspace) and have no other knowledge about the true subspace and the rest of the points, then the most potentially correct point is the one closest to the subspace spanned by the correct neighbors we have. This motivates us to propose NSN described in the following.\nAlgorithm 1 Nearest Subspace Neighbor (NSN)\nInput: A set of N samples Y = {y1, . . . , yN}, The number of required neighbors K, Maximum subspace dimension kmax. Output: A neighborhood matrix W \u2208 {0, 1}N\u00d7N yi \u2190 yi/\u2016yi\u20162, \u2200i \u2208 [N ] . Normalize magnitudes for i = 1, . . . , N do . Run NSN for each data point Ii \u2190 {i} for k = 1, . . . ,K do . Iteratively add the closest point to the current subspace\nif k \u2264 kmax then U \u2190 span{yj : j \u2208 Ii} end if j\u2217 \u2190 arg maxj\u2208[N ]\\Ii \u2016ProjUyj\u20162 Ii \u2190 Ii \u222a {j\u2217}\nend for Wij \u2190 Ij\u2208Ii or yj\u2208U , \u2200j \u2208 [N ] . Construct the neighborhood matrix\nend for\nNSN collects K neighbors sequentially for each point. At each step k, a k-dimensional subspace U spanned by the point and its k\u22121 neighbors is constructed, and the point closest to the subspace is newly collected. After k \u2265 kmax, the subspace U constructed at the kmaxth step is used for collecting neighbors. At last, if there are more points lying on U , they are also counted as neighbors. The subspace U can be stored in the form of a matrix U \u2208 Rp\u00d7dim(U) whose columns form an orthonormal basis of U . Then \u2016ProjUyj\u20162 can be computed easily because it is equal to \u2016U>yj\u20162. While a naive implementation requires O(K2pN2) computational cost, this can be reduced to O(KpN2), and the faster implementation is described in Section A.1. We note that this computational cost is much lower than that of the convex optimization based methods (e.g., SSC [4] and LRR [14]) which solve a convex program with N2 variables and pN constraints.\nNSN for subspace clustering shares the same philosophy with Orthogonal Matching Pursuit (OMP) for sparse recovery in the sense that it incrementally picks the point (dictionary element) that is the most likely to be correct, assuming that the algorithms have found the correct ones. In subspace clustering, that point is the one closest to the subspace spanned by the currently selected points, while in sparse recovery it is the one closest to the residual of linear regression by the selected points. In the sparse recovery literature, the performance of OMP is shown to be comparable to that of Basis Pursuit (`1-minimization) both theoretically and empirically [18, 11]. One of the contributions of this work is to show that this high-level intuition is indeed born out, provable, as we show that NSN also performs well in collecting neighbors lying on the same subspace."}, {"heading": "2.2 Greedy Subspace Recovery (GSR)", "text": "Suppose that NSN has found correct neighbors for a data point. How can we check if they are indeed correct, that is, lying on the same true subspace? One natural way is to count the number of points close to the subspace spanned by the neighbors. If they span one of the true subspaces, then many other points will be lying on the span. If they do not span any true subspaces, few points will be close to it. This fact motivates us to use a greedy algorithm to recover the subspaces. Using the neighborhood constructed by NSN (or some other algorithm), we recover the L subspaces. If there is a neighborhood set containing only the points on the same subspace for each subspace, the algorithm successfully recovers the unions of the true subspaces exactly.\nAlgorithm 2 Greedy Subspace Recovery (GSR)\nInput: N points Y = {y1, . . . , yN}, A neighborhood matrix W \u2208 {0, 1}N\u00d7N , Error bound Output: Estimated subspaces D\u0302 = \u222aLl=1D\u0302l. Estimated labels w\u03021, . . . , w\u0302N yi \u2190 yi/\u2016yi\u20162, \u2200i \u2208 [N ] . Normalize magnitudes Wi \u2190 Top-d{yj : Wij = 1}, \u2200i \u2208 [N ] . Estimate a subspace using the neighbors for each point I \u2190 [N ] while I 6= \u2205 do . Iteratively pick the best subspace estimates\ni\u2217 \u2190 arg maxi\u2208I \u2211N\nj=1 I{\u2016ProjWiyj\u20162 \u2265 1\u2212 } D\u0302l \u2190 W\u0302i\u2217 I \u2190 I \\ {j : \u2016ProjWi\u2217yj\u20162 \u2265 1\u2212 }\nend while w\u0302i \u2190 arg maxl\u2208[L] \u2016ProjD\u0302lyi\u20162, \u2200i \u2208 [N ] . Label the points using the subspace estimates\nRecall that the matrix W contains the labelings of the points, so that Wij = 1 if point i is assigned to subspace j. Top-d{yj : Wij = 1} denotes the d-dimensional principal subspace of the set of vectors {yj : Wij = 1}. This can be obtained by taking the first d left singular vectors of the matrix whose columns are the vector in the set. If there are only d vectors in the set, Gram-Schmidt orthogonalization will give us the subspace. As in NSN, it is efficient to store a subspace Wi in the form of its orthogonal basis because we can easily compute the norm of a projection onto the subspace.\nTesting a candidate subspace by counting the number of near points has already been considered in the subspace clustering literature. In [25], the authors proposed to run RANdom SAmple Consensus (RANSAC) iteratively. RANSAC randomly selects a few points and checks if there are many other points near the subspace spanned by the collected points. Instead of randomly choosing sample points, GSR receives some candidate subspaces (in the form of sets of points) from NSN (or possibly some other algorithm) and selects subspaces in a greedy way as specified in the algorithm above."}, {"heading": "3 Theoretical results", "text": "We analyze our algorithms in two standard noiseless models. The main theorems present sufficient conditions under which the algorithms cluster the points exactly with high probability. For simplicity of analysis, we assume that every subspace is of the same dimension, and the number of data\npoints on each subspace is the same, i.e., d , d1 = \u00b7 \u00b7 \u00b7 = dL, n , N1 = \u00b7 \u00b7 \u00b7 = NL. We assume that d is known to the algorithm. Nonetheless, our analysis can extend to the general case."}, {"heading": "3.1 Statistical models", "text": "We consider two models which have been used in the subspace clustering literature:\n\u2022 Fully random model: The subspaces are drawn iid uniformly at random, and the points are also iid randomly generated.\n\u2022 Semi-random model: The subspaces are arbitrarily determined, but the points are iid randomly generated.\nLet Di \u2208 Rp\u00d7d, i \u2208 [L] be a matrix whose columns form an orthonormal basis of Di. An important measure that we use in the analysis is the affinity between two subspaces, defined as\naff(i, j) , \u2016D>i Dj\u2016F\u221a\nd =\n\u221a\u2211d k=1 cos\n2 \u03b8i,jk d \u2208 [0, 1],\nwhere \u03b8i,jk is the kth principal angle between Di and Dj . Two subspaces Di and Dj are identical if and only if aff(i, j) = 1. If aff(i, j) = 0, every vector on Di is orthogonal to any vectors on Dj . We also define the maximum affinity as\nmax aff , max i,j\u2208[L],i 6=j aff(i, j) \u2208 [0, 1].\nThere are N = nL points, and there are n points exactly lying on each subspace. We assume that each data point yi is drawn iid uniformly at random from Sp\u22121 \u2229 Dwi where Sp\u22121 is the unit sphere in Rp. Equivalently,\nyi = Dwixi, xi \u223c Unif(Sd\u22121), \u2200i \u2208 [N ].\nAs the points are generated randomly on their corresponding subspaces, there are no points lying on an intersection of two subspaces, almost surely. This implies that with probability one the points are clustered correctly provided that the true subspaces are recovered exactly."}, {"heading": "3.2 Main theorems", "text": "The first theorem gives a statistical guarantee for the fully random model.\nTheorem 3.1. Suppose L d-dimensional subspaces and n points on each subspace are generated in the fully random model with n polynomial in d. There are constants C1, C2 > 0 such that if\nn d > C1\n( log ne\nd\u03b4\n)2 , d\np <\nC2 log n\nlog(ndL\u03b4\u22121) , (1)\nthen with probability at least 1 \u2212 3L\u03b41\u2212\u03b4 , NSN+GSR 3 clusters the points exactly. Also, there are other constants C \u20321, C \u2032 2 > 0 such that if (1) with C1 and C2 replaced by C \u2032 1 and C \u2032 2 holds then NSN+Spectral4 clusters the points exactly with probability at least 1 \u2212 3L\u03b41\u2212\u03b4 . e is the exponential constant.\n3NSN with K = kmax = d followed by GSR with arbitrarily small . 4NSN with K = kmax = d.\nOur sufficient conditions for exact clustering explain when subspace clustering becomes easy or difficult, and they are consistent with our intuition. For NSN to find correct neighbors, the points on the same subspace should be many enough so that they look like lying on a subspace. This condition is spelled out in the first inequality of (1). We note that the condition holds even when n/d is a constant, i.e., n is linear in d. The second inequality implies that the dimension of the subspaces should not be too high for subspaces to be distinguishable. If d is high, the random subspaces are more likely to be close to each other, and hence they become more difficult to be distinguished. However, as n increases, the points become dense on the subspaces, and hence it becomes easier to identify different subspaces.\nLet us compare our result with the conditions required for success in the fully random model in the existing literature. In [16], it is required for SSC to have correct neighborhoods that n should be superlinear in d when d/p fixed. In [6, 24], the conditions on d/p becomes worse as we have more points. On the other hand, our algorithms are guaranteed exact clustering of the points, and the sufficient condition is order-wise at least as good as the conditions for correct neighborhoods by the existing algorithms (See Table 1). Moreover, exact clustering is guaranteed even when n is linear in d, and d/p fixed.\nFor the semi-random model, we have the following general theorem.\nTheorem 3.2. Suppose L d-dimensional subspaces are arbitrarily chosen, and n points on each subspace are generated in the semi-random model with n polynomial in d. There are constants C1, C2 > 0 such that if\nn d > C1\n( log ne\nd\u03b4\n)2 , max aff < \u221a C2 log n\nlog(dL\u03b4\u22121) \u00b7 log(ndL\u03b4\u22121) . (2)\nthen with probability at least 1\u2212 3L\u03b41\u2212\u03b4 , NSN+GSR 5 clusters the points exactly.\nIn the semi-random model, the sufficient condition does not depend on the ambient dimension p. When the affinities between subspaces are fixed, and the points are exactly lying on the subspaces, the difficulty of the problem does not depend on the ambient dimension. It rather depends on max aff, which measures how close the subspaces are. As they become closer to each other, it becomes more difficult to distinguish the subspaces. The second inequality of (2) explains this intuition. The inequality also shows that if we have more data points, the problem becomes easier to identify different subspaces.\nCompared with other algorithms, NSN+GSR is guaranteed exact clustering, and more importantly, the condition on max aff improves as n grows. This remark is consistent with the practical performance of the algorithm which improves as the number of data points increases, while the existing guarantees of other algorithms are not. In [16], correct neighborhoods in SSC are guaranteed if max aff = O( \u221a log(n/d)/ log(nL)). In [6], exact clustering of TSC is guaranteed if max aff = O(1/ log(nL)). However, these algorithms perform empirically better as the number of data points increases."}, {"heading": "4 Experimental results", "text": "In this section, we empirically compare our algorithms with the existing algorithms in terms of clustering performance and computational time (on a single desktop). For NSN, we used the fast\n5NSN with K = d\u2212 1 and kmax = d2 log de followed by GSR with arbitrarily small .\nimplementation described in Section A.1. The compared algorithms are K-means, K-flats6, SSC, LRR, SCC, TSC7, and SSC-OMP8. The numbers of replicates in K-means, K-flats, and the Kmeans used in the spectral clustering are all fixed to 10. The algorithms are compared in terms of Clustering error (CE) and Neighborhood selection error (NSE), defined as\n(CE) = min \u03c0\u2208\u03a0L\n1\nN N\u2211 i=1 I(wi 6= \u03c0(w\u0302i)), (NSE) = 1 N N\u2211 i=1 I(\u2203j : Wij 6= 0, wi 6= wj)\nwhere \u03a0L is the permutation space of [L]. CE is the proportion of incorrectly labeled data points. Since clustering is invariant up to permutation of label indices, the error is equal to the minimum disagreement over the permutation of label indices. NSE measures the proportion of the points which do not have all correct neighbors.9"}, {"heading": "4.1 Synthetic data", "text": "We compare the performances on synthetic data generated from the fully random model. In Rp, five d-dimensional subspaces are generated uniformly at random. Then for each subspace n unitnorm points are generated iid uniformly at random on the subspace. To see the agreement with the theoretical result, we ran the algorithms under fixed d/p and varied n and d. We set d/p = 3/5 so that each pair of subspaces has intersection. Figures 1 and 2 show CE and NSE, respectively. Each error value is averaged over 100 trials. Figure 1 indicates that our algorithm clusters the data points better than the other algorithms. As predicted in the theorems, the clustering performance improves as the number of points increases. However, it also improves as the dimension of subspaces\n6K-flats is similar to K-means. At each iteration, it computes top-d principal subspaces of the points with the same label, and then labels every point based on its distances to those subspaces.\n7The MATLAB codes for SSC, LRR, SCC, and TSC are obtained from http://www.cis.jhu.edu/~ehsan/code. htm, https://sites.google.com/site/guangcanliu, and http://www.math.duke.edu/~glchen/scc.html, http:// www.nari.ee.ethz.ch/commth/research/downloads/sc.html, respectively.\n8For each data point, OMP constructs a neighborhood for each point by regressing the point on the other points up to 10\u22124 accuracy.\n9For the neighborhood matrices from SSC, LRR, and SSC-OMP, the d points with the maximum weights are regarded as neighbors for each point. For TSC, the d nearest neighbors are collected for each point.\ngrows in contrast to the theoretical analysis. We believe that this is because our analysis on GSR is not tight. In Figure 2, we can see that more data points obtain correct neighbors as n increases or d decreases, which conforms the theoretical analysis.\nWe also compare the computational time of the neighborhood selection algorithms for different numbers of subspaces and data points. As shown in Figure 3, the greedy algorithms (OMP, Thresholding, and NSN) are significantly more scalable than the convex optimization based algorithms (`1-minimization and nuclear norm minimization)."}, {"heading": "4.2 Real-world data : motion segmentation and face clustering", "text": "We compare our algorithm with the existing ones in the applications of motion segmentation and face clustering. For the motion segmentation, we used Hopkins155 dataset [17], which contains 155 video sequences of 2 or 3 motions. For the face clustering, we used Extended Yale B dataset with cropped images from [5, 13]. The dataset contains 64 images for each of 38 individuals in frontal view and different illumination conditions. To compare with the existing algorithms, we used the set of 48 \u00d7 42 resized raw images provided by the authors of [4]. The parameters of the existing\nalgorithms were set as provided in their source codes.10 Tables 2 and 3 show CE and average computational time.11 We can see that NSN+Spectral performs competitively with the methods with the lowest errors, but much faster. Compared to the other greedy neighborhood construction based algorithms, SSC-OMP and TSC, our algorithm performs significantly better.\n10As SSC-OMP and TSC do not have proposed number of parameters for motion segmentation, we found the numbers minimizing the mean CE. The numbers are given in the table.\n11The LRR code provided by the author did not perform properly with the face clustering dataset that we used. We did not run NSN+GSR since the data points are not well distributed in its corresponding subspaces."}, {"heading": "A Discussion on implementation issues", "text": ""}, {"heading": "A.1 A faster implementation for NSN", "text": "At each step of NSN, the algorithm computes the projections of all points onto a subspace and find one with the largest norm. A naive implementation of the algorithm requires O(pK2N2) time complexity.\nIn fact, we can reduce the complexity to O(pKN2). Instead of finding the maximum norm of the projections, we can find the maximum squared norm of the projections. Let Uk be the subspace U at step k. For any data point y, we have\n\u2016ProjUky\u2016 2 2 = \u2016ProjUk\u22121y\u2016 2 2 + |u>k y|2\nwhere uk is the new orthogonal axis added from Uk\u22121 to make Uk. That is, Uk\u22121 \u22a5 uk and Uk = Uk\u22121 \u2295 uk. As \u2016ProjUk\u22121y\u2016 2 2 is already computed in the (k \u2212 1)\u2019th step, we do not need to compute it again at step k. Based on this fact, we have a faster implementation as described in the following. Note that Pj at the kth step is equal to \u2016ProjUkyj\u2016 2 2 in the original NSN algorithm.\nAlgorithm 3 Fast Nearest Subspace Neighbor (F-NSN)\nInput: A set of N samples Y = {y1, . . . , yN}, The number of required neighbors K, Maximum subspace dimension kmax. Output: A neighborhood matrix W \u2208 {0, 1}N\u00d7N yi \u2190 yi/\u2016yi\u20162, \u2200i \u2208 [N ] for i = 1, . . . , N do Ii \u2190 {i}, u1 \u2190 yi Pj \u2190 0,\u2200j \u2208 [N ] for k = 1, . . . ,K do\nif k \u2264 kmax then Pj \u2190 Pj + \u2016u>k yj\u20162, \u2200j \u2208 [N ] end if j\u2217 \u2190 arg maxj\u2208[N ],j /\u2208Ii Pj Ii \u2190 Ii \u222a {j\u2217} if k < kmax then\nuk+1 \u2190 yj\u2217\u2212\n\u2211k l=1(u > l yj\u2217 )ul\n\u2016yj\u2217\u2212 \u2211k l=1(u > l yj\u2217 )ul\u20162\nend if end for Wij \u2190 Ij\u2208Ii or Pj=1, \u2200j \u2208 [N ]\nend for"}, {"heading": "A.2 Estimation of the number of clusters", "text": "When L is unknown, it can be estimated at the clustering step. For Spectral clustering, a wellknown approach to estimate L is to find a knee point in the singular values of the neighborhood matrix. It is the point where the difference between two consecutive singular values are the largest. For GSR, we do not need to estimate the number of clusters a priori. Once the algorithms finishes, the number of the resulting groups will be our estimate of L."}, {"heading": "A.3 Parameter setting", "text": "The choices of K and kmax depend on the dimension of the subspaces d. If data points are lying exactly on the model subspaces, K = kmax = d is enough for GSR to recover a subspace. In practical situations where the points are near the subspaces, it is better to set K to be larger than d. kmax can also be larger than d because the kmax\u2212d additional dimensions, which may be induced from the noise, do no intersect with the other subspaces in practice. For Extended Yale B dataset and Hopkins155 dataset, we found that NSN+Spectral performs well if K is set to be around 2d."}, {"heading": "B Proofs", "text": ""}, {"heading": "B.1 Proof outline", "text": "We describe the first few high-level steps in the proofs of our main theorems. Exact clustering of our algorithms depends on whether NSN can find all correct neighbors for the data points so that the following algorithm (GSR or Spectral clustering) can cluster the points exactly. For NSN+GSR, exact clustering is guaranteed when there is a point on each subspace that have all correct neighbors which are at least d\u2212 1. For NSN+Spectral, exact clustering is guaranteed when each data point has only the n \u2212 1 other points on the same subspace as neighbors. In the following, we explain why these are true.\nStep 1-1: Exact clustering condition for GSR\nThe two statistical models have a property that for any d-dimensional subspace in Rp other than the true subspaces D1, . . . ,DL the probability of any points lying on the subspace is zero. Hence, we claim the following.\nFact B.1 (Best d-dimensional fit). With probability one, the true subspaces D1, . . . ,DL are the L subspaces containing the most points among the set of all possible d-dimensional subspaces.\nThen it suffices for each subspace to have one point whose neighbors are d\u2212 1 all correct points on the same subspace. This is because the subspace spanned by those d points is almost surely identical to the true subspace they are lying on, and that subspace will be picked by GSR.\nFact B.2. If NSN with K \u2265 d\u22121 finds all correct neighbors for at least one point on each subspace, GSR recovers all the true subspaces and clusters the data points exactly with probability one.\nIn the following steps, we consider one data point for each subspace. We show that NSN with K = kmax = d finds all correct neighbors for the point with probability at least 1\u2212 3\u03b41\u2212\u03b4 . Then the union bound and Fact B.2 establish exact clustering with probability at least 1\u2212 3L\u03b41\u2212\u03b4 .\nStep 1-2: Exact clustering condition for spectral clustering\nIt is difficult to analyze spectral clustering for the resulting neighborhood matrix of NSN. A trivial case for a neighborhood matrix to result in exact clustering is when the points on the same subspace form a single fully connected component. If NSN with K = kmax = d finds all correct neighbors for every data point, the subspace U at the last step (k = K) is almost surely identical to the true subspace that the points lie on. Hence, the resulting neighborhood matrix W form L fully connected components each of which contains all of the points on the same subspace.\nIn the rest of the proof, we show that if (1) holds, NSN finds all correct neighbors for a fixed point with probability 1\u2212 3\u03b41\u2212\u03b4 . Let us assume that this is true. If (1) with C1 and C2 replaced by C1 4 and C2 2 holds, we have\nn > C1d\n( log ne\nd(\u03b4/n)\n)2 , d\np <\nC2 log n\nlog(ndL(\u03b4/n)\u22121) .\nThen it follows from the union bound that NSN finds all correct neighbors for all of the n points on each subspace with probability at least 1 \u2212 3L\u03b41\u2212\u03b4 , and hence we obtain Wij = Iwi=wj for every (i, j) \u2208 [N ]2. Exact clustering is guaranteed.\nStep 2: Success condition for NSN\nNow the only proposition that we need to prove is that for each subspace Di NSN finds all correct neighbors for a data point (which is a uniformly random unit vector on the subspace) with probability at least 1\u2212 3\u03b41\u2212\u03b4 . As our analysis is independent of the subspaces, we only consider D1. Without loss of generality, we assume that y1 lies on D1 (w1 = 1) and focus on this data point.\nWhen NSN finds neighbors of y1, the algorithm constructs kmax subspaces incrementally. At each step k = 1, . . . ,K, if the largest projection onto U of the uncollected points on the same true subspace D1 is greater than the largest projection among the points on different subspaces, then NSN collects a correct neighbor. In a mathematical expression, we want to satisfy\nmax j:wj=1,j /\u2208I1 \u2016ProjUyj\u20162 > max j:wj 6=1,j /\u2208I1 \u2016ProjUyj\u20162 (3)\nfor each step of k = 1, . . . ,K. The rest of the proof is to show (1) and (2) lead to (3) with probability 1 \u2212 3\u03b41\u2212\u03b4 in their corresponding models. It is difficult to prove (3) itself because the subspaces, the data points, and the index set I1 are all dependent of each other. Instead, we introduce an Oracle algorithm whose success is equivalent to the success of NSN, but the analysis is easier. Then the Oracle algorithm is analyzed using stochastic ordering, bounds on order statistics of random projections, and the measure concentration inequalities for random subspaces. The rest of the proof is provided in Sections B.3 and B.4."}, {"heading": "B.2 Preliminary lemmas", "text": "Before we step into the technical parts of the proof, we introduce the main ingredients which will be used. The following lemma is about upper and lower bounds on the order statistics for the projections of iid uniformly random unit vectors.\nLemma B.3. Let x1, . . . , xn be drawn iid uniformly at random from the d-dimensional unit ball Sd\u22121. Let z(n\u2212m+1) denote the m\u2019th largest value of {zi , \u2016Axi\u20162, 1 \u2264 i \u2264 n} where A \u2208 Rk\u00d7d.\na. Suppose that the rows of A are orthonormal to each other. For any \u03b1 \u2208 (0, 1), there exists a constant C > 0 such that for n,m, d, k \u2208 N where\nn\u2212m+ 1 \u2265 Cm ( log ne\nm\u03b4\n)2 (4)\nwe have\nz2(n\u2212m+1) > k\nd +\n1 d \u00b7min\n{ 2 log ( n\u2212m+ 1\nCm ( log nem\u03b4\n)2 ) , \u03b1 \u221a d\u2212 k } (5)\nwith probability at least 1\u2212 \u03b4m.\nb. For any k \u00d7 d matrix A,\nz(n\u2212m+1) < \u2016A\u2016F\u221a d + \u2016A\u20162\u221a d \u00b7 (\u221a 2\u03c0 + \u221a 2 log ne m\u03b4 ) (6)\nwith probability at least 1\u2212 \u03b4m.\nLemma B.3b can be proved by using the measure concentration inequalities [12]. Not only can they provide inequalities for random unit vectors, they also give us inequalities for random subspaces.\nLemma B.4. Let the columns of X \u2208 Rd\u00d7k be the orthonormal basis of a k-dimensional random subspace drawn uniformly at random in d-dimensional space.\na. For any matrix A \u2208 Rp\u00d7d. E[\u2016AX\u20162F ] = k\nd \u2016A\u20162F\nb. [15, 12] If \u2016A\u20162 is bounded, then we have\nPr { \u2016AX\u2016F > \u221a k\nd \u2016A\u2016F + \u2016A\u20162 \u00b7\n(\u221a 8\u03c0\nd\u2212 1 + t\n)} \u2264 e\u2212 (d\u22121)t2 8 ."}, {"heading": "B.3 Proof of Theorem 3.2", "text": "Following Section B.1, we show in this section that if (2) holds then NSN finds all correct neighbors for y1 (which is assumed to be on D1) with probability at least 1\u2212 3\u03b41\u2212\u03b4 .\nStep 3: NSN Oracle algorithm\nConsider the Oracle algorithm in the following. Unlike NSN, this algorithm knows the true label of each data point. It picks the point closest to the current subspace among the points with the same label. Since we assume w1 = 1, the Oracle algorithm for y1 picks a point in {yj : wj = 1} at every step.\nNote that the Oracle algorithm returns failure if and only if the original algorithm picks an incorrect neighbor for y1. The reason is as follows. Suppose that NSN for y1 picks the first incorrect point at step k. By the step k \u2212 1, correct points have been chosen because they are the nearest points for the subspaces in the corresponding steps. The Oracle algorithm will also pick those points because they are the nearest points among the correct points. Hence U \u2261 Vk. At step k, NSN picks an incorrect point as it is the closest to U . The Oracle algorithm will declare failure because that incorrect point is closer than the closest point among the correct points. In\nAlgorithm 4 NSN Oracle algorithm for y1 (assuming w1 = 1) Input: A set of N samples Y = {y1, . . . , yN}, The number of required neighbors K = d \u2212 1, Maximum subspace dimension kmax = d2 log de I(1)1 \u2190 {1} for k = 1, . . . ,K do\nif k \u2264 kmax then Vk \u2190 span{yj : j \u2208 I (k) 1 }\nj\u2217k \u2190 arg maxj\u2208[N ]:wj=1,j /\u2208I(k)1 \u2016ProjVkyj\u20162\nelse j\u2217k \u2190 arg maxj\u2208[N ]:wj=1,j /\u2208I(k)1\n\u2016ProjVkmaxyj\u20162 end if if max\nj\u2208[N ]:wj=1,j /\u2208I (k) i \u2016ProjVkyj\u20162 \u2264 maxj\u2208[N ]:wj 6=1 \u2016ProjVky\u20162 then Return FAILURE\nend if I(k+1)1 \u2190 I (k) 1 \u222a {j\u2217k}\nend for Return SUCCESS\nthe same manner, we see that NSN fails if the Oracle NSN fails. Therefore, we can instead analyze the success of the Oracle algorithm. The success condition is written as\n\u2016ProjVkyj\u2217k\u20162 > maxj\u2208[N ]:wj 6=1 \u2016ProjVky\u20162, \u2200k = 1, . . . , kmax,\n\u2016ProjVkmaxyj\u2217k\u20162 > maxj\u2208[N ]:wj 6=1 \u2016ProjVkmaxy\u20162, \u2200k = kmax + 1, . . . ,K. (7)\nNote that Vk\u2019s are independent of the points {yj : j \u2208 [N ], wj 6= 1}. We will use this fact in the following steps.\nStep 4: Lower bounds on the projection of correct points (the LHS of (7))\nLet Vk \u2208 Rd\u00d7k be such that the columns of D1Vk form an orthogonal basis of Vk. Such a Vk exists because Vk is a k-dimensional subspace of D1. Then we have\n\u2016ProjVkyj\u2217k\u20162 = \u2016V > k D > 1 D1xj\u2217k\u20162 = \u2016V > k xj\u2217k\u20162\nIn this step, we obtain lower bounds on \u2016V >k xj\u2217k\u20162 for k \u2264 kmax and \u2016V > kmax xj\u2217k\u20162 for k > kmax. It is difficult to analyze \u2016V >k xj\u2217k\u20162 because Vk and xj\u2217k are dependent. We instead analyze another random variable that is stochastically dominated by \u2016V >k xj\u2217k\u2016 2 2. Then we use a high-probability lower bound on that variable which also lower bounds \u2016V >k xj\u2217k\u2016 2 2 with high probability.\nDefine Pk,(m) as the m\u2019th largest norm of the projections of n \u2212 1 iid uniformly random unit vector on Sd\u22121 onto a k-dimensional subspace. Since the distribution of the random unit vector is isotropic, the distribution of Pk,(m) is identical for any k-dimensional subspaces independent of the random unit vectors. We have the following key lemma.\nLemma B.5. \u2016V >k xj\u2217k\u20162 stochastically dominates Pk,(k), i.e.,\nPr{\u2016V >k xj\u2217k\u20162 \u2265 t} \u2265 Pr{Pk,(k) \u2265 t}\nfor any t \u2265 0. Moreover, Pk,(k) \u2265 Pk\u0302,(k) for any k\u0302 \u2264 k.\nThe proof of the lemma is provided in Appendix B.5. Now we can use the lower bound on Pk,(k) given in Lemma B.3a to bound \u2016V >k xj\u2217k\u20162. Let us pick \u03b1 and C for which the lemma holds. The first inequality of (2) with C1 = C + 1 leads to n\u2212 d > Cd ( log ned\u03b4 )2 , and also\nn\u2212 k > Ck ( log ne\nk\u03b4\n)2 , \u2200k = 1, . . . , d\u2212 1. (8)\nHence, it follow from Lemma B.3a that for each k = 1, . . . , kmax, we have\n\u2016V >k xj\u2217k\u20162 \u2265 k\nd +\n1 d min\n{ 2 log ( n\u2212 k + 1 Ck ( log nek\u03b4 )2 ) , \u03b1 \u221a d\u2212 k }\n\u2265 k d + 1 d min\n{ 2 log ( n\u2212 d\nCd ( log ne\u03b4\n)2 ) , \u03b1 \u221a d\u2212 2 log d } (9)\nwith probability at least 1\u2212 \u03b4k. For k > kmax, we want to bound \u2016ProjVkmaxyj\u2217k\u20162. We again use Lemma B.5 to obtain the bound. Since the condition for the lemma holds as shown in (8), we have\n\u2016V >kmaxxj\u2217k\u20162 \u2265 2 log d\nd +\n1 d min\n{ 2 log ( n\u2212 k + 1 Ck ( log nek\u03b4 )2 ) , \u03b1 \u221a d\u2212 2 log d }\n\u2265 2 log d d + 1 d min\n{ 2 log ( n\u2212 d\nCd ( log ne\u03b4\n)2 ) , \u03b1 \u221a d\u2212 2 log d } (10)\nwith probability at least 1\u2212 \u03b4k, for every k = kmax + 1, . . . , d\u2212 1. The union bound gives that (9) and (10) hold for all k = 1, . . . , d \u2212 1 simultaneously with probability at least 1\u2212 \u03b41\u2212\u03b4 .\nStep 5: Upper bounds on the projection of incorrect points (the RHS of (7))\nSince we have \u2016ProjVkyj\u20162 = \u2016V > k D > 1 Dwjxj\u20162, the RHS of (7) can be written as\nmax j:j\u2208[N ],wj 6=1\n\u2016V >k D>1 Dwjxj\u20162 (11)\nIn this step, we want to bound (11) for every k = 1, . . . , d\u22121 by using the concentration inequalities for Vk and xj . Since Vk and xj are independent, the inequality for xj holds for any fixed Vk.\nIt follows from Lemma B.3b and the union bound that with probability at least 1\u2212 \u03b4,\nmax j:j\u2208[N ],wj 6=1\n\u2016V >k D>1 Dwjxj\u20162\n\u2264 maxl 6=1 \u2016V >k D>1 Dl\u2016F\u221a\nd + maxl 6=1 \u2016V >k D>1 Dl\u20162\u221a d \u00b7\n( \u221a\n2\u03c0 + \u221a 2 log\nn(L\u2212 1)e \u03b4/d\n)\n\u2264 maxl 6=1 \u2016V >k D>1 Dl\u2016F\u221a\nd \u00b7\n( 5 + \u221a 2 log ndL\n\u03b4\n)\nfor all k = 1, . . . , d \u2212 1. The last inequality follows from the fact \u2016V >k D>1 Dl\u20162 \u2264 \u2016V >k D>1 Dl\u2016F . Since \u2016V >k D>1 Dwjxj\u20162 \u2264 \u2016V >k D>1 Dwj\u2016F \u2264 maxl 6=1 \u2016V >k D>1 Dl\u2016F for every j such that wj 6= 1, we have\nmax j:j\u2208[N ],wj 6=1\n\u2016V >k D>1 Dwjxj\u20162 \u2264 maxl 6=1 \u2016V >k D>1 Dl\u2016F\u221a\nd \u00b7min\n{ 5 + \u221a 2 log ndL\n\u03b4 , \u221a d\n} . (12)\nNow let us consider maxl 6=1 \u2016V >k D>1 Dl\u2016F . In our statistical model, the new axis added to Vk at the kth step (uk+1 in Algorithm 3) is chosen uniformly at random from the subspace in D1 orthogonal to Vk. Therefore, Vk is a random matrix drawn uniformly from the d \u00d7 k Stiefel manifold, and the probability measure is the normalized Haar (rotation-invariant) measure. From Lemma B.4b and the union bound, we obtain that with probability at least 1\u2212 \u03b4/dL,\n\u2016V >k D>1 Dl\u2016F \u2264 \u221a k\nd \u2016D>1 Dl\u2016F + \u2016D>1 Dl\u20162 \u00b7\n(\u221a 8\u03c0\nd\u2212 1 +\n\u221a 8\nd\u2212 1 log\ndL\n\u03b4\n)\n\u2264 \u2016D>1 Dl\u2016F \u00b7\n(\u221a k\nd +\n\u221a 8\u03c0\nd\u2212 1 +\n\u221a 8\nd\u2212 1 log\ndL\n\u03b4\n)\n\u2264 max aff \u00b7 \u221a d \u00b7\n(\u221a k\nd +\n\u221a 8\u03c0\nd\u2212 1 +\n\u221a 8\nd\u2212 1 log\ndL\n\u03b4\n) . (13)\nThe union bound gives that with probability at least 1 \u2212 \u03b4, maxl 6=1 \u2016V >k D>1 Dl\u2016F is also bounded by (13) for every k = 1, . . . , kmax.\nPutting (13) and (12) together, we obtain\nmax j:j\u2208[N ],wj 6=1\n\u2016V >k D>1 Dwjxj\u20162\n\u2264 max aff \u00b7\n(\u221a k\nd +\n\u221a 8\u03c0\nd\u2212 1 +\n\u221a 8\nd\u2212 1 log\ndL\n\u03b4\n) \u00b7min { 5 + \u221a 2 log ndL\n\u03b4 , \u221a d\n} (14)\nfor all k = 1, . . . , d\u2212 1 with probability at least 1\u2212 2\u03b4.\nFinal Step: Proof of the main theorem\nPutting (9), (10), and (14) together, we obtain that if\nmax aff < min 1\u2264k\u2264d\u22121\n\u221a min{k, 2 log d}+ min { 2 log ( n\u2212d Cd ) \u2212 4 log log ne\u03b4 , \u03b1 \u221a d\u2212 2 log d } (\u221a\nmin{k, 2 log d}+ \u221a\n8\u03c0d d\u22121 + \u221a 8d d\u22121 log dL \u03b4 ) \u00b7min { 5 + \u221a 2 log ndL\u03b4 , \u221a d } , (15) then (7) holds, and hence NSN finds all correct neighbors for y1 with probability at least 1\u2212 3\u03b41\u2212\u03b4 . The RHS of (15) is minimized when k \u2265 2 log d, and consequently the condition (15) is equivalent to\nmax aff <\n\u221a 2 log d+ min { 2 log ( n\u2212d Cd ) \u2212 4 log log ne\u03b4 , \u03b1 \u221a d\u2212 2 log d } (\u221a\n2 log d+ \u221a\n8\u03c0d d\u22121 + \u221a 8d d\u22121 log dL \u03b4 ) \u00b7min { 5 + \u221a 2 log ndL\u03b4 , \u221a d } . (16)\nAs n is polynomial in d, there is a constant C3 > 0 such that (RHS of (16)) > C3 \u221a\nlog (n\u2212 d)\u2212 log log ne\u03b4\u221a log dL\u03b4 \u00b7 log ndL \u03b4\nThis completes the proof."}, {"heading": "B.4 Proof of Theorem 3.1", "text": "As we did in Section B.3, we prove in this section that if (1) holds then NSN finds all correct neighbors for y1 with probability at least 1\u2212 3\u03b41\u2212\u03b4 .\nThe only difference between the semi-random model and the fully random model is the statistical dependence between subspaces. We can follow Step 3 in Section B.3 because they do not take any statistical dependence between subspaces into account. We assert that (7) is the success condition also for the fully random model. However, as K = kmax = d, there is no case where k > kmax in this proof.\nNow we provide a new proof of the last three steps for the fully random model.\nStep 4: Lower bounds on the projection of correct points (the LHS of (7))\nWe again use Lemma B.5. For k > d/2, we use the fact that \u2016V >k xj\u2217k\u20162 stochastically dominates Pbd/2c,(k). Then it follows from Lemma B.3a that\n\u2016V >k xj\u2217k\u20162 \u2265 k\n2d +\n1 d min\n{ 2 log ( n\u2212 k + 1 Ck ( log nek\u03b4 )2 ) , \u03b1 \u221a d/2 } (17)\nfor all k = 1, . . . , d\u2212 1 simultaneously with probability at least 1\u2212 \u03b41\u2212\u03b4 .\nStep 5: Upper bounds on the projection of incorrect points (the RHS of (7))\nWe again use the notion of Xk \u2208 Rd\u00d7k which is defined in the proof of Theorem 3.2. Since \u2016ProjVkyj\u20162 = \u2016V > k D > 1 yj\u20162, the RHS of (7) can be written as\nmax j:j\u2208[N ],wj 6=1\n\u2016V >k D>1 yj\u20162 (18)\nSince the true subspaces are independent of each other, yj with wj 6= 1 is also independent of D1 and Vk, and its marginal distribution is uniform over Sp\u22121. It follows from Lemma B.3b that with probability at least 1\u2212 \u03b4/d,\n(18) \u2264 \u2016V >k D>1 \u2016F\u221a p + \u2016V >k D>1 \u20162\u221a p \u00b7\n\u221a 2 log\nn(L\u2212 1)e \u03b4/d\n\u2264\n\u221a k\np +\n\u221a 2\np log\nndLe\n\u03b4 . (19)\nThe last inequality is obtained using the facts \u2016D1Vk\u2016F = \u221a k and \u2016D1Vk\u20162 \u2264 1. The union bound provides that (19) holds for every k = 1, . . . , d\u2212 1 with probability at least 1\u2212 \u03b4.\nFinal Step: Proof of the main theorem\nNow it suffices to show that (17) > (19) for every k = 1, 2, . . . , d\u2212 1, i.e.,\u221a\u221a\u221a\u221a k 2d + 1 d min { 2 log ( n\u2212 k + 1 Ck ( log nek\u03b4 )2 ) , \u03b1 \u221a d 2 } > \u221a k p + \u221a 2 p log ndLe \u03b4 , \u2200k = 1, 2, . . . , d\u2212 1.\n(20)\nwhere \u03b1,C are the constants described in Lemma B.3a. (20) is equivalent to\nd p < min 1\u2264k\u2264d\u22121\nk/2 + min { 2 log ( n\u2212k+1 Ck ) \u2212 4 log ( log nek\u03b4 ) , \u03b1 \u221a d/2 }\n(\u221a k + \u221a 2 log(ndL\u03b4\u22121e) )2 . (21) As n is polynomial in d, the numerator can be replaced by O(k + log(n \u2212 k + 1)). The RHS is minimized when k = O(log(ndL\u03b4\u22121)). Hence, the above condition is satisfied if (1) holds."}, {"heading": "B.5 Proof of Lemma B.5", "text": "We construct a generative model for two random variables that are equal in distribution to \u2016V >k xj\u2217k\u2016 2 2 and P 2k,(k). Then we show that the one corresponding to \u2016V > k xj\u2217k\u2016 2 2 is greater than the other corresponding to P 2k,(k). This generative model uses the fact that for any isotropic distributions the marginal distributions of the components along any orthogonal axes are invariant.\nThe generative model is given as follows.\n1. For k = 1, . . . , kmax, repeat 2.\n2. Draw n\u2212 1 iid random variables Y (k)1 , . . . , Y (k) n\u22121 as follows.\nY (k) j =\n( 1\u2212\nk\u22121\u2211 i=1 Y (i) j ) \u00b7 (X(k)j1 ) 2, X (k) j \u223c Unif(S d\u2212k), \u2200j = 1, . . . , n\u2212 1.\nwhere X (k) j1 is the first coordinate of X (k) j . Define\n\u03c0k , arg max j:j 6=\u03c01,...,\u03c0k\u22121 ( k\u2211 i=1 Y (i) j ) .\n3. For k = kmax + 1, . . . , d\u2212 1, repeat\n\u03c0k , arg max j:j 6=\u03c01,...,\u03c0k\u22121 ( kmax\u2211 i=1 Y (i) j ) .\nWe first claim that ( \u2211k\ni=1 Y (i) \u03c0k ) is equal in distribution to \u2016V >k xj\u2217k\u2016 2 2. Consider the following\ntwo sets of random variables.\nAk , ( k\u2211 i=1 Y (i) j : j \u2208 [n\u2212 1], j 6= \u03c01, . . . , \u03c0k\u22121 ) ,\nBk , ( \u2016V >k xj\u201622 : wj = 1, j 6= 1, j\u22171 , . . . , j\u2217k\u22121 ) .\nEach set contains (n \u2212 k) random variables. We prove by induction that the joint distribution of the random variables of Ak is equal to those of Bk. Then the claim follows because ( \u2211k i=1 Y (i) \u03c0k ) and \u2016V >k xj\u2217k\u2016 2 2 are the maximums of Ak and Bk, respectively.\n\u2022 Base case : As V1 = x1, B1 = (\u2016V >1 xj\u201622 : wj = 1, j 6= 1) is the set of squared inner products with x1 for the n \u2212 1 other points. Since the n \u2212 1 points are iid uniformly random unit vectors independent of x1, the squared inner products with x1 are equal in distribution to\nY (1) j = (X (1) j1 ) 2. Therefore, the joint distribution of B1 = (\u2016V >1 xj\u201622 : wj = 1, j 6= 1) is equal to the joint distribution of A1 = (Y (1) j : j = 1, . . . , n\u2212 1).\n\u2022 Induction : Assume that the joint distribution of Ak is equal to the joint distribution of Bk. It is sufficient to show that given Ak \u2261 Bk the conditional joint distribution of Ak+1 = ( \u2211k+1\ni=1 Y (i) j : j \u2208 [n \u2212 1], j 6= \u03c01, . . . , \u03c0k) is equal to the conditional joint distribution of\nBk+1 = (\u2016V >k+1xj\u201622 : wj = 1, j 6= 1, j\u22171 , . . . , j\u2217k). Define\nvk = xj\u2217k \u2212 VkV > k xj\u2217k\n\u2016xj\u2217k \u2212 VkV > k xj\u2217k\u20162\n.\nvk is the unit vector along the new orthogonal axis added on Vk for Vk+1. Since we have\n\u2016V >k+1xj\u201622 = \u2016V >k xj\u201622 + (v>k xj)2, \u2200j : wj = 1,\nThe two terms are independent of each other because Vk \u22a5 vk, and xj is isotropically distributed. Hence, we only need to show that ((v>k xj)\n2 : wj = 1, j 6= 1, j\u22171 , . . . , j\u2217k) is equal in distribution to (Y\n(k+1) j : j \u2208 [n\u2212 1], j 6= \u03c01, . . . , \u03c0k).\nSince vk is a normalized vector on the subspace V\u22a5k \u2229D1, and xj\u2217k is drawn iid from an isotropic distribution, vk is independent of V\n> k xj\u2217k . Hence, the marginal distribution of vk given Vk is\nuniform over (V\u22a5k \u2229 D1) \u2229 Sp\u22121. Also, vk is also independent of the points {xj : wj = 1, j 6= 1, j\u22171 , . . . , j \u2217 k}. Therefore, the random variables (v>k xj)2 for j with wj = 1, j 6= 1, j\u22171 , . . . , j\u2217k are iid equal in distribution to Y (k+1) j for any j.\nSecond, we can see that the k\u2019th maximum of { \u2211k\ni=1 Y (i) j : j \u2208 [n\u2212 1]} is equal in distribution to P 2k,(k). This is because each \u2211k i=1 Y (i) j can be seen as the norm of the projection of a uniformly random unit vector in Rd onto a k-dimensional subspace. Now we are ready to complete the proof. Since (\u2211k i=1 Y (i) \u03c0k ) is the maximum of the n \u2212 k\nvariables of Ak, it is greater than or equal to the k\u2019th maximum of (\u2211k i=1 Y (i) j : j \u2208 [n\u2212 1] ) . Therefore, \u2016V >k xj\u2217k\u2016 2 2 stochastically dominates P 2 k,(k).\nThe second claim is clear because Vk\u0302 \u2286 Vk, and hence the norm of the projection onto Vk is always larger than the norm of the projection onto Vk\u0302."}, {"heading": "B.6 Proof of Lemma B.3a", "text": "Let x be an unit vector drawn uniformly at random from Sd\u22121. Equivalently, x can be drawn from\nx = w\n\u2016w\u20162 , w \u223c N (0, Id\u00d7d).\nDefine A\u22a5 \u2208 R(d\u2212k)\u00d7d as a matrix with orthonormal rows such that \u2016w\u201622 = \u2016Aw\u201622 + \u2016A\u22a5w\u201622 for any w \u2208 Rd. We have\nPr { \u2016Ax\u201622 > k\nd (1 + )\n} = Pr { \u2016Aw\u201622 \u2016w\u201622 > k d (1 + ) } = Pr { \u2016Aw\u201622\n\u2016Aw\u201622 + \u2016A\u22a5w\u201622 > k d (1 + ) } \u2265 Pr { \u2016Aw\u201622 > k(1 + ), \u2016A\u22a5w\u201622 < (d\u2212 k)\u2212 k\n} = Pr { \u2016Aw\u201622 > k(1 + ) } \u00b7 Pr { \u2016A\u22a5w\u201622 < (d\u2212 k)\u2212 k } , (22)\nwhere the last equality follows from that \u2016Aw\u20162 and \u2016A\u22a5w\u20162 are independent of each other because w \u223c N (0, Id\u00d7d). Note that \u2016Aw\u201622 and \u2016A\u22a5w\u201622 are Chi-square random variables with degrees of freedom k and d\u2212 k, respectively.\nNow we use the following lemma.\nLemma B.6 (Chi-square upper-tail lower-bound). For any k \u2208 N and any \u2265 0, we have\nPr{\u03c72k \u2265 k(1 + )} \u2265 1\n3 \u221a k + 6 exp\n( \u2212k\n2\n) .\nwhere \u03c72k is the chi-square random variable with k degrees of freedom.\nSuppose 0 \u2264 \u2264 \u03b1 (d\u2212k) 1 2\nk for some \u03b1 \u2208 (0, 1). It follows from Lemma B.6 and the central limit theorem that\n(22) \u2265 Pr { \u2016Aw\u201622 > k(1 + ) } \u00b7 Pr { \u2016A\u22a5w\u201622 \u2212 (d\u2212 k) < \u2212\u03b1(d\u2212 k) 1 2 } \u2265 f(\u03b1)\n3k + 6 exp\n( \u2212k\n2 ) where f(\u03b1) \u2208 (0, 1) is some constant depending only on \u03b1.\nThen it follows that\nPr { z2(n\u2212m+1) < k\nd (1 + )\n} = Pr { \u2203I \u2282 [n], |I| = n\u2212m+ 1 : z2i < k\nd (1 + ), \u2200i \u2208 I } \u2264 ( n\nm\u2212 1\n) \u00b7 Pr { z21 < k\nd (1 + ) }n\u2212m+1 \u2264 (ne m )m \u00b7 ( 1\u2212 f(\u03b1) 3k + 6 exp ( \u2212k 2\n))n\u2212m+1 \u2264 exp { m log ne\nm \u2212 f(\u03b1) \u00b7 (n\u2212m+ 1) 3k + 6 exp\n( \u2212k\n2\n)} (23)\nwhere we use the facts ( n m ) \u2264 ( ne m )m and 1 + x \u2264 ex,\u2200x.\nSet C = 6f(\u03b1) , and choose such that\n= 1\nk min\n{ 2 log ( n\u2212m+ 1\nCm ( log nem\u03b4\n)2 ) , \u03b1 \u221a d\u2212 k } .\nThis is valid because 0 \u2264 \u2264 \u03b1 (d\u2212k) 1 2\nk . Then we obtain\n(23) \u2264 exp m log ne m \u2212 f(\u03b1) \u00b7 (n\u2212m+ 1) 6 log ( n\u2212m+1\nCm(log nem\u03b4 ) 2\n) + 6 \u00b7 Cm\n( log nem\u03b4 )2 n\u2212m+ 1  = exp\nm log nem \u2212 6 log nem\u03b46(1 + log (f(\u03b1)6 \u00b7 n\u2212m+1m \u00b7 (log nem\u03b4 )\u22122)) \u00b7m log ne m\u03b4  \u2264 exp { m log ne\nm \u2212\n6(1 + log nm)\n6(1 + log f(\u03b1)6 + log n m) m log\nne\nm\u03b4 } \u2264 exp { m log ne\nm \u2212m log ne m\u03b4 } \u2264 \u03b4m.\nThis completes the proof."}, {"heading": "B.7 Proof of Lemma B.3b", "text": "We use a special case of Levy\u2019s lemma for this proof.\nLemma B.7 ([12]). For x \u223c Unif(Sd\u22121),\nPr{\u2016Ax\u20162 > m\u2016Ax\u20162 + t} \u2264 exp ( \u2212 dt 2\n2\u2016A\u201622\n) ,\nPr{\u2016Ax\u20162 < m\u2016Ax\u20162 \u2212 t} \u2264 exp ( \u2212 dt 2\n2\u2016A\u201622\n) .\nfor any matrix A \u2208 Rp\u00d7d and t > 0. m\u2016Ax\u20162 is the median of \u2016Ax\u20162.\nIt follows from the lemma that |E\u2016Ax\u20162 \u2212m\u2016Ax\u20162| \u2264 E [|\u2016Ax\u20162 \u2212m\u2016Ax\u20162|] \u2264 \u222b \u221e\n0 2e \u2212 dt\n2\n2\u2016A\u201622 dt =\n\u221a 2\u03c0\nd \u2016A\u20162.\nThen we have\nPr { \u2016Axi\u20162 > \u221a \u2016A\u20162F d + \u221a 2\u03c0 d \u2016A\u20162 + t } = Pr { \u2016Axi\u20162 > \u221a E\u2016Axi\u201622 + \u221a 2\u03c0 d \u2016A\u20162 + t }\n\u2264 Pr { \u2016Axi\u20162 > E\u2016Axi\u20162 + \u221a 2\u03c0\nd \u2016A\u20162 + t } \u2264 Pr {\u2016Axi\u20162 > m\u2016Axi\u20162 + t}\n\u2264 exp ( \u2212 dt 2\n2\u2016A\u201622\n) .\nIt follows that\nPr { z(n\u2212m+1) > \u221a \u2016A\u20162F d + \u221a 2\u03c0 d \u2016A\u20162 + t }\n\u2264 Pr { \u2203I \u2282 [n], |I| = m : \u2016Axi\u20162 > \u221a \u2016A\u20162F d + \u221a 2\u03c0 d \u2016A\u20162 + t,\u2200i \u2208 I }\n\u2264 ( n\nm\n) \u00b7 Pr { \u2016Ax1\u20162 > \u221a \u2016A\u20162F d + \u221a 2\u03c0 d \u2016A\u20162 + t }m \u2264 (ne m )m \u00b7 exp ( \u2212 mdt 2\n2\u2016A\u201622 ) = exp { m log ne\nm \u2212 mdt\n2\n2\u2016A\u201622\n} .\nReplacing t with \u221a 2\u2016A\u201622 d log ne m\u03b4 , we obtain the desired result."}, {"heading": "B.8 Proof of Lemma B.4a", "text": "Let A = U\u03a3V > be the singular value decomposition of A. Then we have E[\u2016AX\u20162F ] = E[\u2016U\u03a3V >X\u20162F ] = E[\u2016\u03a3X\u20162F ] = min(p,d)\u2211 i=1 \u03c32i \u00b7  k\u2211 j=1 E[X2ij ]  = min(p,d)\u2211 i=1 \u03c32i \u00b7 k d = k d \u2016A\u20162F .\nwhere the second last equality follows from that Xij is a coordinate of a uniformly random unit vector, and thus\nE[X2ij ] = 1\nd , \u2200i, j."}, {"heading": "B.9 Proof of Lemma B.4b", "text": "Consider the Stiefel manifold Vk(Rd) equipped with the Euclidean metric. We see that X is drawn from Vk(Rd) with the normalized Harr probability measure. We have\n\u2016AX\u2016F \u2212 \u2016AY \u2016F \u2264 \u2016AX \u2212AY \u2016F = \u2016A(X \u2212 Y )\u2016F \u2264 \u2016A\u20162\u2016X \u2212 Y \u2016F\nfor any X,Y \u2208 Rd\u00d7k. Since \u2016A\u20162 \u2264 1, \u2016AX\u2016F is a 1-Lipschitz function of X. Then it follows from [15, 12] that\nPr{\u2016AX\u2016F > m\u2016AX\u2016F + t} \u2264 e\u2212 (d\u22121)t2 8 ,\nwhere m\u2016AX\u2016F is the median of \u2016AX\u2016F . Also, we have\nPr{|\u2016AX\u2016F \u2212m\u2016AX\u2016F | > t} \u2264 2e\u2212 (d\u22121)t2 8 ,\nand then it follows that |E\u2016AX\u2016F \u2212m\u2016AX\u2016F | \u2264 E [|\u2016AX\u2016F \u2212m\u2016AX\u2016F |] \u2264 \u222b \u221e\n0 2e\u2212\n(d\u22121)t2 8 dt =\n\u221a 8\u03c0\nd\u2212 1 .\nIt follows from Jensen\u2019s inequality and Lemma B.4a that\nE\u2016AX\u2016F \u2264 \u221a E\u2016AX\u20162F = \u221a k\nd \u2016A\u2016F\nPutting the above inequalities together using the triangle inequality, we obtain the desired result."}, {"heading": "B.10 Proof of Lemma B.6", "text": "For k \u2265 2, it follows from [9, Proposition 3.1] that\nPr{\u03c72k \u2265 k(1 + )} \u2265 1\u2212 e\u22122\n2\nk(1 + )\nk + 2 \u221a k exp\n( \u22121\n2 (k \u2212 (k \u2212 2) log(1 + ) + log k) ) \u2265 1\n3 \u221a k + 6 exp\n( \u2212k\n2 ( \u2212 log(1 + )) ) \u2265 1\n3 \u221a k + 6 exp\n( \u2212k\n2\n) .\nFor k = 1, we can see numerically that the inequality holds."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>We consider the problem of subspace clustering: given points that lie on or near the union of<lb>many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies<lb>sets of points close to the same subspace and uses the sets to estimate the subspaces. As the<lb>geometric structure of the clusters (linear subspaces) forbids proper performance of general<lb>distance based approaches such as K-means, many model-specific methods have been proposed.<lb>In this paper, we provide new simple and efficient algorithms for this problem. Our statistical<lb>analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under<lb>certain conditions on the number of points and the affinity between subspaces. These conditions<lb>are weaker than those considered in the standard statistical literature. Experimental results on<lb>synthetic data generated from the standard unions of subspaces model demonstrate our theory.<lb>We also show that our algorithm performs competitively against state-of-the-art algorithms on<lb>real-world applications such as motion segmentation and face clustering, with much simpler<lb>implementation and lower computational cost.", "creator": "LaTeX with hyperref package"}}}