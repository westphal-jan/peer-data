{"id": "1411.0169", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2014", "title": "Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms", "abstract": "mitteilungen Let $ 0.4 p $ ridgeley be 104.66 an unknown khanjar and lacorte arbitrary gotcher probability distribution stein over $ [kfog 0, sabbe 1) $. We duty-free consider the 11.57 problem 14:06 of {\\ trimtabs em zizhong density fractionalized estimation }, endplate in doch which a learning algorithm is foscari given megevand i. tjp i. d. mozes draws sabriel from $ p $ and laming must (pendeen with high overweening probability) dont\u00e9 output a ch\u00e2teau-thierry hypothesis distribution seminal that mellisa is shf close to $ mashaie p $. jazira The main mithra contribution thiab of samaritano this paper barones is a three-body highly rikki efficient sodegaura density 56-bit estimation algorithm sanctity for cosafa learning using kloeckner a bessinger variable - width psychosis histogram, kemeys i. e. , a 574,000 hypothesis distribution with a 417.8 piecewise shelman@globe.com constant sannyasi probability nishimizu density ninel function.", "histories": [["v1", "Sat, 1 Nov 2014 21:03:59 GMT  (25kb)", "http://arxiv.org/abs/1411.0169v1", "conference version appears in NIPS 2014"]], "COMMENTS": "conference version appears in NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG cs.DS math.ST stat.TH", "authors": ["siu-on chan", "ilias diakonikolas", "rocco a servedio", "xiaorui sun"], "accepted": true, "id": "1411.0169"}, "pdf": {"name": "1411.0169.pdf", "metadata": {"source": "CRF", "title": "Near\u2013Optimal Density Estimation in Near\u2013Linear Time Using Variable\u2013Width Histograms", "authors": ["Siu-On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "emails": ["sochan@gmail.com.", "ilias.d@ed.ac.uk.", "rocco@cs.columbia.edu.", "xiaoruisun@cs.columbia.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n01 69\nv1 [\ncs .L\nIn more detail, for any k and \u03b5, we give an algorithm that makes O\u0303(k/\u03b52) draws from p, runs in O\u0303(k/\u03b52) time, and outputs a hypothesis distribution h that is piecewise constant with O(k log2(1/\u03b5)) pieces. With high probability the hypothesis h satisfies dTV(p, h) \u2264 C \u00b7optk(p)+\u03b5, where dTV denotes the total variation distance (statistical distance), C is a universal constant, and opt\nk (p) is the smallest total variation distance between p and any k-piecewise constant\ndistribution. The sample size and running time of our algorithm are optimal up to logarithmic factors. The \u201capproximation factor\u201d C in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of k and \u03b5 can achieve C < 2 regardless of what kind of hypothesis distribution it uses."}, {"heading": "1 Introduction", "text": "Consider the following fundamental statistical task: Given independent draws from an unknown probability distribution, what is the minimum sample size needed to obtain an accurate estimate of the distribution? This is the question of density estimation, a classical problem in statistics with a rich history and an extensive literature (see e.g., [BBBB72, DG85, Sil86, Sco92, DL01]). While this broad question has mostly been studied from an information\u2013theoretic perspective, it is an inherently algorithmic question as well, since the ultimate goal is to describe and understand algorithms that are both computationally and information-theoretically efficient. The need for computationally efficient learning algorithms is only becoming more acute with the recent flood\n\u2217Supported by EPSRC grant EP/L021749/1, and a Marie Curie Career Integration Grant. \u2020Supported by NSF grants CCF-0915929 and CCF-1115703. \u2021Supported by NSF grant CCF-1149257.\nof data across the sciences; the \u201cgold standard\u201d in this \u201cbig data\u201d context is an algorithm with information-theoretically (near-) optimal sample size and running time (near-) linear in its sample size.\nIn this paper we consider learning scenarios in which an algorithm is given an input data set which is a sample of i.i.d. draws from an unknown probability distribution. It is natural to expect (and can be easily formalized) that, if the underlying distribution of the data is inherently \u201ccomplex\u201d, it may be hard to even approximately reconstruct the distribution. But what if the underlying distribution is \u201csimple\u201d or \u201csuccinct\u201d \u2013 can we then reconstruct the distribution to high accuracy in a computationally and sample-efficient way? In this paper we answer this question in the affirmative for the problem of learning \u201cnoisy\u201d histograms, arguably one of the most basic density estimation problems in the literature.\nTo motivate our results, we begin by briefly recalling the role of histograms in density estimation. Histograms constitute \u201cthe oldest and most widely used method for density estimation\u201d [Sil86], first introduced by Karl Pearson in [Pea95]. Given a sample from a probability density function (pdf) p, the method partitions the domain into a number of intervals (bins) B1, . . . , Bk, and outputs the \u201cempirical\u201d pdf which is constant within each bin. A k-histogram is a piecewise constant distribution over bins B1, . . . , Bk, where the probability mass of each interval Bj , j \u2208 [k], equals the fraction of observations in the interval. Thus, the goal of the \u201chistogram method\u201d is to approximate an unknown pdf p by an appropriate k-histogram. It should be emphasized that the number k of bins to be used and the \u201cwidth\u201d and location of each bin are unspecified; they are parameters of the estimation problem and are typically selected in an ad hoc manner.\nWe study the following distribution learning question:\nSuppose that there exists a k-histogram that provides an accurate approximation to the unknown target distribution. Can we efficiently find such an approximation?\nIn this paper, we provide a fairly complete affirmative answer to this basic question. Given a bound k on the number of intervals, we give an algorithm that uses a near-optimal sample size, runs in near-linear time (in its sample size), and approximates the target distribution nearly as accurately as the best k-histogram.\nTo formally state our main result, we will need a few definitions. We work in a standard model of learning an unknown probability distribution from samples, essentially that of [KMR+94], which is a natural analogue of Valiant\u2019s well-known PAC model for learning Boolean functions [Val84] to the unsupervised setting of learning an unknown probability distribution.1 A distribution learning problem is defined by a class C of distributions over a domain \u2126. The algorithm has access to independent draws from an unknown pdf p, and its goal is to output a hypothesis distribution h that is \u201cclose\u201d to the target distribution p. We measure the closeness between distributions using the statistical distance or total variation distance. In the \u201cnoiseless\u201d setting, we are promised that p \u2208 C and the goal is to construct a hypothesis h such that (with high probability) the total variation distance dTV(h, p) between h and p is at most \u03b5, where \u03b5 > 0 is the accuracy parameter.\nThe more challenging \u201cnoisy\u201d or agnostic model captures the situation of having arbitrary (or even adversarial) noise in the data. In this setting, we do not make any assumptions about the target density p and the goal is to find a hypothesis h that is almost as accurate as the \u201cbest\u201d approximation of p by any distribution in C. Formally, given sample access to a (potentially arbitrary) target distribution p and \u03b5 > 0, the goal of an agnostic learning algorithm for C is to compute a hypothesis distribution h such that dTV(h, p) \u2264 \u03b1 \u00b7 optC(p) + \u03b5, where optC(p) :=\n1We remark that our model is essentially equivalent to the \u201cminimax rate of convergence under the L1 distance\u201d in statistics [DL01], and our results carry over to this setting as well.\ninfq\u2208C dTV(q, p) \u2013 i.e., optC(p) is the statistical distance between p and the closest distribution to it in C \u2013 and \u03b1 \u2265 1 is a constant (that may depend on the class C). We will call such a learning algorithm an \u03b1-agnostic learning algorithm for C; when \u03b1 > 1 we sometimes refer to this as a semi-agnostic learning algorithm.\nA distribution f over a finite interval I \u2286 R is called k-flat if there exists a partition of I into k intervals I1, . . . , Ik such that the pdf f is constant within each such interval. We henceforth (without loss of generality for densities with bounded support) restrict ourselves to the case I = [0, 1). Let Ck be the class of all k-flat distributions over [0, 1). For a (potentially arbitrary) distribution p over [0, 1) we will denote by optk(p) := inff\u2208Ck dTV(f, p).\nIn this terminology, our learning problem is exactly the problem of agnostically learning the class of k-flat distributions. Our main positive result is a near-optimal algorithm for this problem, i.e., a semi-agnostic learning algorithm that has near-optimal sample size and near-linear running time. More precisely, we prove the following:\nTheorem 1 (Main). There is an algorithm A with the following property: Given k \u2265 1, \u03b5 > 0, and sample access to a target distribution p, algorithm A uses O\u0303(k/\u03b52) independent draws from p, runs in time O\u0303(k/\u03b52), and outputs a O(k log2(1/\u03b5))-flat hypothesis distribution h that satisfies dTV(h, p) \u2264 O(optk(p)) + \u03b5 with probability at least 9/10.\nUsing standard techniques, the confidence probability can be boosted to 1 \u2212 \u03b4, for any \u03b4 > 0, with a (necessary) overhead of O(log(1/\u03b4)) in the sample size and the running time.\nWe emphasize that the difficulty of our result lies in the fact that the \u201coptimal\u201d piecewise constant decomposition of the domain is both unknown and approximate (in the sense that optk(p) > 0); and that our algorithm is both sample-optimal and runs in (near-) linear time. Even in the (significantly easier) case that the target p \u2208 Ck (i.e., optk(p) = 0), and the optimal partition is explicitly given to the algorithm, it is known that a sample of size \u2126(k/\u03b52) is informationtheoretically necessary. (This lower bound can, e.g., be deduced from the standard fact that learning an unknown discrete distribution over a k-element set to statistical distance \u03b5 requires an \u2126(k/\u03b52) size sample.) Hence, our algorithm has provably optimal sample complexity (up to a logarithmic factor), runs in essentially sample linear time, and is \u03b1-agnostic for a universal constant \u03b1 > 1.\nIt should be noted that the sample size required for our problem is well-understood; it follows from the VC theorem (Theorem 3) that O(k/\u03b52) draws from p are information-theoretically sufficient. However, the theorem is non-constructive, and the \u201cobvious\u201d algorithm following from it has running time exponential in k and 1/\u03b5. In recent work, Chan et al [CDSS14] presented an approach employing an intricate combination of dynamic programming and linear programming which yields a poly(k/\u03b5) time algorithm for the above problem. However, the running time of the [CDSS14] algorithm is \u2126(k3) even for constant values of \u03b5, making it impractical for applications. As discussed below our algorithmic approach is significantly different from that of [CDSS14], using neither dynamic nor linear programming. Applications. Nonparametric density estimation for shape restricted classes has been a subject of study in statistics since the 1950\u2019s (see [BBBB72] for an early book on the topic and [Gre56, Bru58, Rao69, Weg70, HP76, Gro85, Bir87] for some of the early literature), and has applications to a range of areas including reliability theory (see [Reb05] and references therein). By using the structural approximation results of Chan et al [CDSS13], as an immediate corollary of Theorem 1 we obtain sample optimal and near-linear time estimators for various well-studied classes of shape restricted densities including monotone, unimodal, and multimodal densities (with unknown mode locations), monotone hazard rate (MHR) distributions, and others (because of space constraints we do not enumerate the exact descriptions of these classes or statements of these results here,\nbut instead refer the interested reader to [CDSS13]). Birge\u0301 [Bir87] obtained a sample optimal and linear time estimator for monotone densities, but prior to our work, no linear time and sample optimal estimator was known for any of the other classes.\nOur algorithm from Theorem 1 is \u03b1-agnostic for a constant \u03b1 > 1. It is natural to ask whether a significantly stronger accuracy guarantee is efficiently achievable; in particular, is there an agnostic algorithm with similar running time and sample complexity and \u03b1 = 1? Perhaps surprisingly, we provide a negative answer to this question. Even in the simplest nontrivial case that k = 2, and the target distribution is defined over a discrete domain [N ] = {1, . . . , N}, any \u03b1-agnostic algorithm with \u03b1 < 2 requires large sample size:\nTheorem 2 (Lower bound, Informal statement). Any 1.99-agnostic learning algorithm for 2-flat distributions over [N ] requires a sample of size \u2126( \u221a N).\nSee Theorem 7 in Section 4 for a precise statement. Note that there is an exact correspondence between distributions over the discrete domain [N ] and pdf\u2019s over [0, 1) which are piecewise constant on each interval of the form [k/N, (k + 1)/N) for k \u2208 {0, 1, . . . , N \u2212 1}. Thus, Theorem 2 implies that no finite sample algorithm can 1.99-agnostically learn even 2-flat distributions over [0, 1). (See Corollary 4.3 in Section 4 for a detailed statement.) Related work. A number of techniques for density estimation have been developed in the mathematical statistics literature, including kernels and variants thereof, nearest neighbor estimators, orthogonal series estimators, maximum likelihood estimators (MLE), and others (see Chapter 2 of [Sil86] for a survey of existing methods). The main focus of these methods has been on the statistical rate of convergence, as opposed to the running time of the corresponding estimators. We remark that the MLE does not exist for very simple classes of distributions (e.g., unimodal distributions with an unknown mode, see e.g, [Bir97]). We note that the notion of agnostic learning is related to the literature on model selection and oracle inequalities [MP007], however this work is of a different flavor and is not technically related to our results.\nHistograms have also been studied extensively in various areas of computer science, including databases and streaming [JKM+98, GKS06, CMN98, GGI+02] under various assumptions about the input data and the precise objective. Recently, Indyk et al [ILR12] studied the problem of learning a k-flat distribution over [N ] under the L2 norm and gave an efficient algorithm with sample complexity O(k2 log(N)/\u03b54). Since the L1 distance is a stronger metric, Theorem 1 implies an improved sample and time bound of O\u0303(k/\u03b52) for their setting."}, {"heading": "2 Preliminaries", "text": "Throughout the paper we assume that the underlying distributions have Lebesgue measurable densities. For a pdf p : [0, 1) \u2192 R+ and a Lebesgue measurable subset A \u2286 [0, 1), i.e., A \u2208 L([0, 1)), we use p(A) to denote \u222b z\u2208A p(z). The statistical distance or total variation distance between two densities p, q : [0, 1) \u2192 R+ is dTV(p, q) := supA\u2208L([0,1)) |p(A) \u2212 q(A)|. The statistical distance satisfies the identity dTV(p, q) =\n1 2\u2016p \u2212 q\u20161 where \u2016p \u2212 q\u20161, the L1 distance between p and q, is\u222b\n[0,1) |p(x) \u2212 q(x)|dx; for convenience in the rest of the paper we work with L1 distance. We refer to a nonnegative function p over an interval (which need not necessarily integrate to one over the interval) as a \u201csub-distribution.\u201d Given a value \u03ba > 0, we say that a (sub-)distribution p over [0, 1) is \u03ba-well-behaved if supx\u2208[0,1)Prx\u223cp[x] \u2264 \u03ba, i.e., no individual real value is assigned more than \u03ba probability under p. Any probability distribution with no atoms is \u03ba-well-behaved for all \u03ba > 0. Our results apply for general distributions over [0, 1) which may have an atomic part as well as a non-atomic part. Given m independent draws s1, . . . , sm from a distribution p over [0, 1), the\nempirical distribution p\u0302m over [0, 1) is the discrete distribution supported on {s1, . . . , sm} defined as follows: for all z \u2208 [0, 1), Prx\u223cp\u0302m [x = z] = |{j \u2208 [m] | sj = z}|/m. The VC inequality. Let p : [0, 1) \u2192 R be a Lebesgue measurable function. Given a family of subsets A \u2286 L([0, 1)) over [0, 1), define \u2016p\u2016A = supA\u2208A |p(A)|. The VC dimension of A is the maximum size of a subset X \u2286 [0, 1) that is shattered by A (a set X is shattered by A if for every Y \u2286 X, some A \u2208 A satisfies A\u2229X = Y ). If there is a shattered subset of size s for all s \u2208 Z+, then we say that the VC dimension of A is \u221e. The well-known Vapnik-Chervonenkis (VC) inequality states the following:\nTheorem 3 (VC inequality, [DL01, p.31]). Let p : I \u2192 R+ be a probability density function over I \u2286 R and p\u0302m be the empirical distribution obtained after drawing m points from p. Let A \u2286 2I be a family of subsets with VC dimension d. Then E[\u2016p\u2212 p\u0302m\u2016A] \u2264 O( \u221a d/m).\nPartitioning into intervals of approximately equal mass. As a basic primitive, given access to a sample drawn from a \u03ba-well-behaved target distribution p over [0, 1), we will need to partition [0, 1) into \u0398(1/\u03ba) intervals each of which has probability \u0398(\u03ba) under p. There is a simple algorithm, based on order statistics, which does this and has the following performance guarantee (see Appendix A.2 of [CDSS14]):\nLemma 2.1. Given \u03ba \u2208 (0, 1) and access to points drawn from a \u03ba/64-well-behaved distribution p over [0, 1), the procedure Approximately-Equal-Partition draws O((1/\u03ba) log(1/\u03ba)) points from p, runs in time O\u0303(1/\u03ba), and with probability at least 99/100 outputs a partition of [0, 1) into \u2113 = \u0398(1/\u03ba) intervals such that p(Ij) \u2208 [\u03ba/2, 3\u03ba] for all 1 \u2264 j \u2264 \u2113."}, {"heading": "3 The algorithm and its analysis", "text": "In this section we prove our main algorithmic result, Theorem 1. Our approach has the following high-level structure: In Section 3.1 we give an algorithm for agnostically learning a target distribution p that is \u201cnice\u201d in two senses: (i) p is well-behaved (i.e., it does not have any heavy atomic elements), and (ii) optk(p) is bounded from above by the error parameter \u03b5. In Section 3.2 we give a general efficient reduction showing how the second assumption can be removed, and in Section 3.3 we briefly explain how the first assumption can be removed, thus yielding Theorem 1."}, {"heading": "3.1 The main algorithm", "text": "In this section we give our main algorithmic result, which handles well-behaved distributions p for which optk(p) is not too large:\nTheorem 4. There is an algorithm Learn-WB-small-opt-k-histogram that given as input O\u0303(k/\u03b52) i.i.d. draws from a target distribution p and a parameter \u03b5 > 0, runs in time O\u0303(k/\u03b52), and has the following performance guarantee: If (i) p is \u03b5/ log(1/\u03b5)384k -well-behaved, and (ii) optk(p) \u2264 \u03b5, then with probability at least 19/20, it outputs an O(k \u00b7 log2(1/\u03b5))-flat distribution h such that dTV(p, h) \u2264 2 \u00b7 optk(p) + 3\u03b5.\nWe require some notation and terminology. Let r be a distribution over [0, 1), and let P be a set of disjoint intervals that are contained in [0, 1). We say that the P-flattening of r, denoted (r)P , is the sub-distribution defined as\nr(v) = { r(I)/|I| if v \u2208 I, I \u2208 P 0 if v does not belong to any I \u2208 P\nObserve that if P is a partition of [0, 1), then (since r is a distribution) (r)P is a distribution. We say that two intervals I, I \u2032 are consecutive if I = [a, b) and I \u2032 = [b, c). Given two consecutive intervals I, I \u2032 contained in [0, 1) and a sub-distribution r, we use \u03b1r(I, I \u2032) to denote the L1 distance between (r){I,I \u2032} and (r){I\u222aI \u2032}, i.e., \u03b1r(I, I \u2032) = \u222b I\u222aI\u2032 |(r){I,I\n\u2032}(x) \u2212 (r){I\u222aI\u2032}(x)|dx. Note here that {I \u222a I \u2032} is a set that contains one element, the interval [a, c)."}, {"heading": "3.1.1 Intuition for the algorithm", "text": "We begin with a high-level intuitive explanation of the Learn-WB-small-opt-k-histogram algorithm. It starts in Step 1 by constructing a partition of [0, 1) into z = \u0398(k/\u03b5\u2032) intervals I1, . . . , Iz (where \u03b5\u2032 = \u0398\u0303(\u03b5)) such that p has weight \u0398(\u03b5\u2032/k) on each subinterval. In Step 2 the algorithm draws a sample of O\u0303(k/\u03b52) points from p and uses them to define an empirical distribution p\u0302m. This is the only step in which points are drawn from p. For the rest of this intuitive explanation we pretend that the weight p\u0302(I) that the empirical distribution p\u0302m assigns to each interval I is actually the same as the true weight p(I) (Lemma 3.1 below shows that this is not too far from the truth).\nBefore continuing with our explanation of the algorithm, let us digress briefly by imagining for a moment that the target distribution p actually is a k-flat distribution (i.e., that optk(p) = 0). In this case there are at most k \u201cbreakpoints\u201d, and hence at most k intervals Ij for which \u03b1p\u0302m(Ij , Ij+1) > 0, so computing the \u03b1p\u0302m(Ij , Ij+1) values would be an easy way to identify the true breakpoints (and given these it is not difficult to construct a high-accuracy hypothesis).\nIn reality, we may of course have optk(p) > 0; this means that if we try to use the \u03b1p\u0302m(Ij , Ij+1) criterion to identify \u201cbreakpoints\u201d of the optimal k-flat distribution that is closest to p (call this k-flat distribution q), we may sometimes be \u201cfooled\u201d into thinking that q has a breakpoint in an interval Ij where it does not (but rather the value \u03b1p\u0302m(Ij , Ij+1) is large because of the difference between q and p). However, recall that by assumption we have optk(p) \u2264 \u03b5; this bound can be used to show that there cannot be too many intervals Ij for which a large value of \u03b1p\u0302m(Ij, Ij+1) suggests a \u201cspurious breakpoint\u201d (see the proof of Lemma 3.3). This is helpful, but in and of itself not enough; since our partition I1, . . . , Iz divides [0, 1) into k/\u03b5\n\u2032 intervals, a naive approach based on this would result in a (k/\u03b5\u2032)-flat hypothesis distribution, which in turn would necessitate a sample complexity of O\u0303(k/\u03b5\u20323), which is unacceptably high. Instead, our algorithm performs a careful process of iteratively merging consecutive intervals for which the \u03b1p\u0302m(Ij , Ij+1) criterion indicates that a merge will not adversely affect the final accuracy by too much. As a result of this process we end up with k \u00b7 polylog(1/\u03b5) intervals for the final hypothesis, which enables us to output a (k \u00b7 polylog(1/\u03b5\u2032))-flat final hypothesis using O\u0303(k/\u03b5\u20322) draws from p.\nIn more detail, this iterative merging is carried out by the main loop of the algorithm in Step 4. Going into the t-th iteration of the loop, the algorithm has a partition Pt\u22121 of [0, 1) into disjoint sub-intervals, and a set Ft\u22121 \u2286 Pt\u22121 (i.e., every interval belonging to Ft\u22121 also belongs to Pt\u22121). Initially P0 contains all the intervals I1, . . . , Iz and F0 is empty. Intuitively, the intervals in Pt\u22121 \\Ft\u22121 are still being \u201cprocessed\u201d; such an interval may possibly be merged with a consecutive interval from Pt\u22121 \\ Ft\u22121 if doing so would only incur a small \u201ccost\u201d (see condition (iii) of Step 4(b) of the algorithm).The intervals in Ft\u22121 have been \u201cfrozen\u201d and will not be altered or used subsequently in the algorithm."}, {"heading": "3.1.2 The algorithm", "text": "Algorithm Learn-WB-small-opt-k-histogram: Input: parameters k \u2265 1, \u03b5 > 0; access to i.i.d. draws from target distribution p over [0, 1) Output: If (i) p is \u03b5/ log(1/\u03b5)384k -well-behaved and (ii) optk(p) \u2264 \u03b5, then with probability at least 99/100 the output is a distribution q such that dTV(p, q) \u2264 2optk(p) + 3\u03b5.\n1. Let \u03b5\u2032 = \u03b5/ log(1/\u03b5). Run Algorithm Approximately-Equal-Partition on input parameter \u03b5 \u2032\n6k to partition [0, 1) into z = \u0398(k/\u03b5 \u2032) intervals I1 = [i0, i1), . . . , Iz = [iz\u22121, iz), where\ni0 = 0 and iz = 1, such that with probability at least 99/100, for each j \u2208 {1, . . . , z} we have p([ij\u22121, ij)) \u2208 [\u03b5\u2032/12k, \u03b5\u2032/2k] (assuming p is \u03b5\u2032/(384k)-well-behaved).\n2. Draw m = O\u0303(k/\u03b5\u20322) points from p and let p\u0302m be the resulting empirical distribution.\n3. Set P0 = {I1, I2, . . . Iz}, and F0 = \u2205.\n4. Let s = log2 1 \u03b5\u2032 . Repeat for t = 1, . . . until t = s:\n(a) Initialize Pt to \u2205 and Ft to Ft\u22121. (b) Without loss of generality, assume Pt\u22121 = {It\u22121,1, . . . , It\u22121,zt\u22121} where interval It\u22121,i\nis to the left of It\u22121,i+1 for all i. Scan left to right across the intervals in Pt\u22121 (i.e., iterate over i = 1, . . . , zt\u22121 \u2212 1). If intervals It\u22121,i, It\u22121,i+1 are (i) both not in Ft\u22121, and (ii) \u03b1p\u0302m(It\u22121,i, It\u22121,i+1) > \u03b5\n\u2032/(2k), then add both It\u22121,i and It\u22121,i+1 into Ft. (c) Initialize i to 1, and repeatedly execute one of the following four (mutually exclusive\nand exhaustive) cases until i > zt\u22121: [Case 1] i \u2264 zt\u22121\u22121 and It\u22121,i = [a, b), It\u22121,i+1 = [b, c) are consecutive intervals both not in Ft. Add the merged interval It\u22121,i \u222a It\u22121,i+1 = [a, c) into Pt. Set i \u2190 i+ 2. [Case 2] i \u2264 zt\u22121 \u2212 1 and It\u22121,i \u2208 Ft. Set i \u2190 i+ 1. [Case 3] i \u2264 zt\u22121 \u2212 1, It\u22121,i /\u2208 Ft and It\u22121,i+1 \u2208 Ft. Add It\u22121,i into Ft and set i \u2190 i+ 2. [Case 4] i = zt\u22121. Add It\u22121,zt\u22121 into Ft if It\u22121,zt\u22121 is not in Ft and set i \u2190 i+ 1.\n(d) Set Pt \u2190 Pt \u222a Ft.\n5. Output the |Ps|-flat hypothesis distribution (p\u0302m)Ps ."}, {"heading": "3.1.3 Analysis of the algorithm and proof of Theorem 4", "text": "It is straightforward to verify the claimed running time given Lemma 2.1, which bounds the running time of Approximately-Equal-Partition. Indeed, we note that Step 2, which simply draws O\u0303(k/\u03b5\u20322) points and constructs the resulting empirical distribution, dominates the overall running time. In the rest of this subsection we prove correctness.\nWe first observe that with high probability the empirical distribution p\u0302m defined in Step 2 gives a high-accuracy estimate of the true probability of any union of consecutive intervals from I1, . . . , Iz. The following lemma from [CDSS14] follows from the standard multiplicative Chernoff bound:\nLemma 3.1 (Lemma 12, [CDSS14]). With probability 99/100 over the sample drawn in Step 2, for every 0 \u2264 a < b \u2264 z we have that |p\u0302m([ia, ib))\u2212 p([ia, ib))| \u2264 \u221a \u03b5\u2032(b\u2212 a) \u00b7 \u03b5\u2032/(10k).\nWe henceforth assume that this 99/100-likely event indeed takes place, so the above inequality holds for all 0 \u2264 a < b \u2264 z. We use this to show that the \u03b1p\u0302m(It\u22121,i, It\u22121,i+1) value that the algorithm uses in Step 4(b) is a good proxy for the actual value \u03b1p(It\u22121,i, It\u22121,i+1) (which of course is not accessible to the algorithm):\nLemma 3.2. Fix 1 \u2264 t \u2264 s. Then we have |\u03b1p\u0302m(It\u22121,i, It\u22121,i+1)\u2212 \u03b1p(It\u22121,i, It\u22121,i+1)| \u2264 2\u03b5\u2032/(5k).\nProof. Observe that in iteration t, two consecutive intervals It\u22121,i and It\u22121,i+1 correspond to two unions of consecutive intervals Ia\u222a\u00b7 \u00b7 \u00b7\u222aIb and Ib+1\u222a\u00b7 \u00b7 \u00b7\u222aIc respectively from the original partition P0. Moreover, since each interval in Pt\u22121 \\ Ft\u22121, t > 1, is formed by merging two consecutive intervals from Pt\u22122 \\ Ft\u22122, it must be the case that b \u2212 a + 1, c \u2212 b + 1 \u2264 2t\u22121 < 2s\u22121 \u2264 1/(2\u03b5\u2032). Hence, by Lemma 3.1, we have\n|p(It\u22121,i)\u2212 p\u0302m(It\u22121,i))| \u2264 \u221a \u03b5\u2032 \u00b7 2s\u22121 \u00b7 \u03b5 \u2032\n10k \u2264 \u03b5\n\u2032\n10 \u221a 2k\nand similarly,\n|p(It\u22121,i+1)\u2212 p\u0302m(It\u22121,i+1))| \u2264 \u03b5\u2032\n10 \u221a 2k .\nTo simplify notation, let I = It\u22121,i and J = It\u22121,i+1. By definition of \u03b1,\n\u03b1p(I, J) = \u2223\u2223\u2223\u2223 p(I) |I| \u2212 p(I) + p(J) |I|+ |J | \u2223\u2223\u2223\u2223 |I|+ \u2223\u2223\u2223\u2223 p(J) |J | \u2212 p(I) + p(J) |I|+ |J | \u2223\u2223\u2223\u2223 |J |\n= 2 |I|+ |J | \u2223\u2223p(I)|J | \u2212 p(J)|I| \u2223\u2223. (1)\nA straightforward calculation now gives that\n|\u03b1p(I, J)\u2212 \u03b1p\u0302m(I, J)| = 2 |I|+ |J | \u2223\u2223\u2223 \u2223\u2223p(I)|J | \u2212 p(J)|I| \u2223\u2223 \u2212 \u2223\u2223p\u0302m(I)|J | \u2212 p\u0302m(J)|I| \u2223\u2223 \u2223\u2223\u2223\n\u2264 2|I|+ |J | (\u2223\u2223p(I)\u2212 p\u0302m(I) \u2223\u2223|J |+ \u2223\u2223p(J)\u2212 p\u0302m(J) \u2223\u2223|I| ) \u2264 2\u03b5\u2032/(5k).\nFor the rest of the analysis, let q denote a fixed k-flat distribution that is closest to p, so \u2016p \u2212 q\u20161 = optk(p). (We note that while optk(p) is defined as infq\u2208C \u2016p \u2212 q\u20161, standard closure arguments can be used to show that the infimum is actually achieved by some k-flat distribution q.) Let Q be the partition of [0, 1) corresponding to the intervals on which q is piecewise constant. We say that a breakpoint of Q is a value in [0, 1] that is an endpoint of one of the (at most) k intervals in Q.\nThe following important lemma bounds the number of intervals in the final partition Ps:\nLemma 3.3. Ps contains at most O(k log2(1/\u03b5)) intervals.\nProof. We start by recording a basic fact that will be useful in the proof of the lemma. Let p be a distribution over an interval I and let q be any sub-distribution over I. Perhaps contrary to initial intuition, the optimal scaling c \u00b7 q, c > 0, of q to approximate p (with respect to the L1-distance) is not necessarily obtained by scaling q so that c \u00b7 q is a distribution over I. However, a simple argument (see e.g., Appendix A.1 of [CDSS14]) shows that scaling so that c \u00b7 q is a distribution cannot result in L1-error more than twice that of the optimal scaling: Claim 3.4. Let p, g : I \u2192 R\u22650 be probability distributions over I (so \u222b I p(x)dx = \u222b I g(x)dx = 1).\nThen, writing \u2016f\u20161 to denote \u222b I |f(x)|dx, for every a > 0 we have that \u2016p \u2212 g\u20161 \u2264 2\u2016p\u2212 ag\u20161.\nWe now proceed with the proof of Lemma 3.3. We first show that a total of at most O(k log(1/\u03b5\u2032)) intervals are ever added into Ft across all executions of Step 4(b). Suppose that intervals It\u22121,i, It\u22121,i+1 are added into Ft in some execution of Step 4(b). We consider the following two cases:\nCase 1: It\u22121,i \u222a It\u22121,i+1 contains at least one breakpoint of Q. Since Q has at most k breakpoints, this can happen at most k times in total.\nCase 2: It\u22121,i \u222a It\u22121,i+1 does not contain any breakpoint of Q. Then It\u22121,i \u222a It\u22121,i+1 is a subset of an interval in Q. Recalling that intervals It\u22121,i, It\u22121,i+1 were added into Ft in an execution of Step 4(b), we have that \u03b1p\u0302m(It\u22121,i, It\u22121,i+1) > \u03b5 \u2032/(2k), and hence by Lemma 3.2, we have that\n\u03b1p(It\u22121,i, It\u22121,i+1) \u2265 15 \u00b7 \u03b5 \u2032 k . Claim 3.4 now implies that the contribution to the L1 distance\nbetween p and q from It\u22121,i \u222a It\u22121,i+1, i.e., \u222b It\u22121,i\u222aIt\u22121,i+1 |p(x)\u2212 q(x)|dx, is at least 110 \u03b5 \u2032 k . Since \u2016p\u2212 q\u20161 = optk(p), there can be at most\nk +O\n( optk(p) \u00b7 k\n\u03b5\u2032\n) = O ( k \u00b7 log 1\n\u03b5\n)\nintervals ever added into Ft across all executions of Step 4(b) (note that for the last equality we have used the assumption that optk(p) \u2264 \u03b5).\nNext, we argue that each Ft satisfies |Ft| \u2264 O(k log2(1/\u03b5)). We have bounded the number of intervals added into Ft in Step 4(b) by O(k log(1/\u03b5\u2032)), so it remains to bound the number of intervals added in Step 4(c)(Case 3) and 4(c)(Case 4). It is clear that a total of at most O(log(1/\u03b5\u2032)) intervals are ever added in 4(c)(Case 4). Inspection of Step 4(c)(Case 3) shows that for a given value of t, the number of intervals that this step adds to Ft is at most the number of \u201cblocks\u201d of consecutive Ft-intervals. Since each interval added in Step 4(c)(Case 3) extends some blocks of consecutive Ft-intervals but does not create a new one (and hence does not increase their number), across the s = log(1/\u03b5\u2032) stages, the total number of intervals that can be added in executions of Step 4(c)(Case 3) is at most O(k log2(1/\u03b5\u2032)). It follows that we have |Fs| = O(k log2(1/\u03b5)) as claimed.\nTo bound |Pt\\Ft|, we observe that by inspection of the algorithm, for each t we have |Pt \\Ft| \u2264 1 2 |Pt\u22121 \\Ft\u22121|. Since |P0| = \u0398(k/\u03b5\u2032), it follows that |Ps \\Fs| = O(k), and the lemma is proved.\nThe following definition will be useful:\nDefinition 5. Let P denote any partition of [0, 1). We say that partition P is \u03b5\u2032-good for (p, q) if for every breakpoint v of Q, the interval I in P containing v satisfies p(I) \u2264 \u03b5\u2032/(2k).\nThe above definition is justified by the following lemma:\nLemma 3.5. If P is \u03b5\u2032-good for (p, q), then \u2016p \u2212 (p)P\u20161 \u2264 2optk(p) + \u03b5\u2032.\nProof. Fix an interval I in P. If there does not exist an interval J in Q such that I \u2286 J , then I must contain a breakpoint of Q, and hence since P is \u03b5\u2032-good for (p, q), we have p(I) \u2264 \u03b5\u2032/(2k). This implies that the contribution to \u2016(p)P \u2212 q\u20161 that comes from I, namely \u222b I |(p)P(x)\u2212 q(x)|dx, satisfies \u222b\nI |(p)P (x)\u2212 q(x)|dx \u2264\n\u222b\nI |(p)P (x)\u2212 p(x)|dx+\n\u222b\nI |p(x)\u2212 q(x)|dx\n\u2264 \u222b\nI |p(x)\u2212 q(x)|dx+ 2p(I)\n\u2264 \u222b\nI |p(x)\u2212 q(x)|dx+ \u03b5\n\u2032\nk .\nThe other possibility is that there exists an interval J in Q such that I \u2286 J . In this case, we have that \u222b\nI |(p)P (x)\u2212 q(x)|dx \u2264\n\u222b\nI |p(x)\u2212 q(x)|dx.\nSince there are at most k intervals in P containing breakpoints of Q, summing the above inequalities over all intervals I in P, we get that\n\u2016(p)P \u2212 q\u20161 \u2264 \u2016p \u2212 q\u20161 + \u03b5\u2032 = optk(p) + \u03b5\u2032,\nand hence \u2016(p)P \u2212 p\u20161 \u2264 \u2016(p)P \u2212 q\u20161 + \u2016p\u2212 q\u20161 \u2264 2optk(p) + \u03b5\u2032.\nWe are now in a position to prove the following:\nLemma 3.6. There exists a partition R of [0, 1) that is \u03b5\u2032-good for (p, q) and satisfies\n\u2016(p)Ps \u2212 (p)R\u20161 \u2264 \u03b5.\nProof. We construct the claimed R based on Ps,Ps\u22121, . . . ,P0 as follows:\n(i) If I is an interval in Ps not containing a breakpoint of Q, then I is also in R.\n(ii) If I is an interval in Ps that does contain a breakpoint of Q, then we further partition I into a set of intervals S by calling procedure Refine-partition(s, I). This recursive procedure exploits the local structure of the earlier, finer partitions Ps\u22121,Ps\u22122, . . . as described below.\nProcedure Refine-partition:\nInput: Integer t, Interval J\nOutput: S, a partition of interval J\n1. If t = 0, then output {J}. 2. If J is an interval in Pt, then\n(a) If J contains a breakpoint of Q, then output Refine-partition(t\u2212 1, J).\n(b) Otherwise output {J}. 3. Otherwise, J is a union of two intervals in Pt. Let J1 and J2 denote the two\nintervals in Pt such that J1 \u222a J2 = J . Output Refine-partition(t, J1) \u222a Refine-partition(t, J2).\nWe claim that |R| (the number of intervals in R) is at most |Ps| + O(k \u00b7 log 1\u03b5 ). To see this, note that each interval I \u2208 Ps not containing a breakpoint of Q (corresponding to (i) above) translates directly to a single interval of R. For each interval of type (ii) in Ps, inspection of the Refine-Partition procedure shows that that these intervals are partitioned into at most O(k log(1/\u03b5)) intervals in R.\nIn the rest of the proof, we show that for any interval J in Ps containing at least one breakpoint of Q, the contribution to the L1 distance between (p)Ps and (p)R coming from interval J is at most |bJ | \u00b7 \u03b5 \u2032 log 1 \u03b5\nk , where bJ is the set of breakpoints of Q in J . Consider a fixed breakpoint v of Q. Let It,v denote the interval containing v in the partition Pt. If It,v merges with another interval in Pt in Case 1 of Step 4(c), we denote that other interval as I \u2032t,v. Since It,v merges with I \u2032 t,v in Case 1 of Step 4(c), these intervals are both not in Ft and hence were both not in Ft\u22121 in Step 4(b). Consequently when t > 1 it must be the case that condition (ii) of Step 4(b) does not hold for these intervals, i.e., \u03b1p\u0302m(It,v, I \u2032 t,v) \u2264 \u03b5\u2032/(2k). It follows that by Lemma 3.2, we have that \u03b1p(It,v, I \u2032 t,v) is at most 4\u03b5\u2032\n5k . When t = 1, we have a similar bound \u03b1p(It,v, I \u2032 t,v) \u2264 \u03b5\u2032/k, by using (1) and the fact that p(It,v), p(I \u2032t,v) \u2264 \u03b5\u2032/2k when It,v, I \u2032t,v \u2208 P0.\nOn the other hand, inspection of the procedure Refine-Partition gives that if two intervals in Pt are unions of some intervals in Refine-partition(s, I), and their union is an interval in Pt+1, then there exists v which is a breakpoint of Q such that the two intervals are It,v and I \u2032t,v.\nThus, the contribution to the L1 distance between (p) Ps and (p)R coming from interval J is at\nmost \u03b5 \u2032 k \u00b7 log 1\u03b5\u2032 \u00b7 |bJ |. Summing over all intervals J that contain at least one breakpoint and recalling that the total number of breakpoints is at most k, we get that the overall L1 distance between (p)Ps and (p)R is at most \u03b5.\nFinally, by putting everything together we can prove Theorem 4:\nProof of Theorem 4. By Lemma 3.5 applied to R, we have that \u2016p\u2212 (p)R\u20161 \u2264 2optk(p)+ \u03b5\u2032. By Lemma 3.6, we have that \u2016(p)Ps \u2212(p)R\u20161 \u2264 \u03b5; thus the triangle inequality gives that \u2016p\u2212(p)Ps\u20161 \u2264 2optk(p)+2\u03b5. By Lemma 3.3 the partition Ps contains at mostO(k log2(1/\u03b5)) intervals, so both (p)Ps and (p\u0302m)\nPs are O(k log2(1/\u03b5))-flat distributions. Thus, \u2016(p)Ps \u2212 (p\u0302m)Ps\u20161 = \u2016(p)Ps \u2212 (p\u0302m)Ps\u2016A\u2113 , where \u2113 = O(k log2(1/\u03b5)) and A\u2113 is the family of all subsets of [0, 1) that consist of unions of up to \u2113 intervals (which has VC dimension 2\u2113). Consequently by the VC inequality (Theorem 3, for a suitable choice of m = O\u0303(k/\u03b5\u20322), we have that E[\u2016(p)Ps \u2212 (p\u0302m)Ps\u20161] \u2264 4\u03b5\u2032/100. Markov\u2019s inequality now gives that with probability at least 96/100, we have \u2016(p)Ps \u2212(p\u0302m)Ps\u20161 \u2264 \u03b5\u2032. Hence, with overall probability at least 19/20 (recall the 1/100 error probability incurred in Lemma 3.1), we have that \u2016p\u2212 (p\u0302m)Ps\u20161 \u2264 2optk(p) + 3\u03b5, and the theorem is proved.\n3.2 A general reduction to the case of small opt for semi-agnostic learning\nIn this section we show that under mild conditions, the general problem of agnostic distribution learning for a class C can be efficiently reduced to the special case when optC is not too large compared with \u03b5. While the reduction is simple and generic, we have not previously encountered it\nin the literature on density estimation, so we provide a proof in the following. A precise statement of the reduction follows:\nTheorem 6. Let A be an algorithm with the following behavior: A is given as input i.i.d. points drawn from p and a parameter \u03b5 > 0. A uses m(\u03b5) = \u2126(1/\u03b5) draws from p, runs in time t(\u03b5) = \u2126(1/\u03b5), and satisfies the following: if optC(p) \u2264 10\u03b5, then with probability at least 19/20 it outputs a hypothesis distribution q such that (i) \u2016p\u2212 q\u20161 \u2264 \u03b1 \u00b7 optC(p)+ \u03b5, where \u03b1 is an absolute constant, and (ii) given any r \u2208 [0, 1), the value q(r) of the pdf of q at r can be efficiently computed in T time steps.\nThen there is an algorithm A\u2032 with the following performance guarantee: A\u2032 is given as input i.i.d. draws from p and a parameter \u03b5 > 0.2 Algorithm A\u2032 uses O(m(\u03b5/10)+ log log(1/\u03b5)/\u03b52) draws from p, runs in time O(t(\u03b5/10)) + T \u00b7 O\u0303(1/\u03b52), and outputs a hypothesis distribution q\u2032 such that with probability at least 39/40 we have \u2016p\u2212 q\u2032\u20161 \u2264 10(\u03b1+ 2) \u00b7 optC(p) + \u03b5.\nProof. The algorithm A\u2032 works in two stages, which we describe and analyze below. In the first stage, A\u2032 iterates over \u2308log(20/\u03b5)\u2309 \u201cguesses\u201d for the value of optC(p), where the i-th guess gi is \u03b5 10 \u00b7 2i\u22121 (so g1 = \u03b510 and g\u2308log(20/\u03b5)\u2309 \u2265 1). For each value of gi, it performs r = O(1) runs of Algorithm A (using a fresh sample from p for each run) using parameter gi as the \u201c\u03b5\u201d parameter for each run; let h1,i, . . . , hr,i be the r hypotheses thus obtained for the i-th guess. It is clear that this stage uses O(m(\u03b5/10)+m(2\u03b5/10)+ \u00b7 \u00b7 \u00b7 ) = O(m(\u03b5)) draws from p, and similarly that it runs in time O(t(\u03b5)). If optC(p) \u2264 \u03b5, then (for a suitable choice of r = O(1)) we get that with probability at least 39/40, some hypothesis h1,\u2113 satisfies \u2016p\u2212h1,\u2113\u2016 \u2264 \u03b1 \u00b7 optC(p)+ \u03b5/10. Otherwise, there must be some i \u2208 {2, . . . , \u2308log(20/\u03b5)\u2309} such that gi/2 < optC(p) \u2264 gi; in this case, for a suitable choice of r = O(1) we get that with probability at least 39/40, there is some hypothesis hi,\u2113 that satisfies \u2016p \u2212 hi,\u2113\u20161 \u2264 \u03b1 \u00b7 optC(p) + gi \u2264 (\u03b1 + 2) \u00b7 optC(p). Thus in either event, with probability at least 39/40 some hi,\u2113 satisfies \u2016p \u2212 hi,\u2113\u20161 \u2264 (\u03b1+ 2) \u00b7 optC(p) + \u03b5/10.\nIn the second stage, A\u2032 runs a hypothesis selection procedure to choose one of the candidate hypotheses hi,\u2113. A number of such procedures are known (see e.g., Section 6.6 of [DL01] or [DDS12, DK14, AJOS14]); all of them work by running some sort of \u201ctournament\u201d over the hypotheses, and all have the guarantee that with high probability they will output a hypothesis from the pool of candidates which has L1 error (with respect to the target distribution p) not much worse than that of the best candidate in the pool. We use the classic Scheffe\u0301 algorithm (see [DL01]) as described and analyzed in [AJOS14] (see Algorithm SCHEFFE\u2217 in Appendix B of that paper). Adapted to our context, this algorithm has the following performance guarantee:\nProposition 3.7. Let p be a target distribution over [0, 1) and let D\u03c4 = {pj}Nj=1 be a collection of N distributions over [0, 1) with the property that there exists i \u2208 [N ] such that \u2016p\u2212pi\u20161 \u2264 \u03c4 . There is a procedure SCHEFFE which is given as input a parameter \u03b5 > 0 and a confidence parameter \u03b4 > 0, and is provided with access to\n(i) i.i.d. draws from p and from pi for all i \u2208 [N ], and\n(ii) an evaluation oracle evalpi for each \u2208 [N ]. This is a procedure which, on input r \u2208 [0, 1), outputs the value pi(r) of the pdf of pi at the point r.\nThe procedure SCHEFFE has the following behavior: It makes s = O ( (1/\u03b52) \u00b7 (logN + log(1/\u03b4)) )\ndraws from p and from each pi, i \u2208 [N ], and O(s) calls to each oracle evalpi , i \u2208 [N ], and performs O(sN2) arithmetic operations. With probability at least 1 \u2212 \u03b4 it outputs an index i\u22c6 \u2208 [N ] that satisfies \u2016p\u2212 pi\u22c6\u20161 \u2264 10max{\u03c4, \u03b5}.\n2 Note that now there is no guarantee that optC(p) \u2264 \u03b5; indeed, the point here is that optC(p) may be arbitrary.\nThe algorithm A\u2032 runs the procedure SCHEFFE using the N = O(log(1/\u03b5)) hypotheses hi,\u2113, with its \u201c\u03b5\u201d parameter set to 110 \u00b7(the input parameter \u03b5 that is given to A\u2032) and its \u201c\u03b4\u201d parameter set to 1/40. By Proposition 3.7, with overall probability at least 19/20 the output is a hypothesis hi,\u2113 satisfying \u2016p\u2212 hi,\u2113\u20161 \u2264 10(\u03b1+2)optC(p) + \u03b5. The overall running time and sample complexity are easily seen to be as claimed, and the theorem is proved."}, {"heading": "3.3 Dealing with distributions that are not well behaved", "text": "The assumption that the target distribution p is \u0398\u0303(\u03b5/k)-well-behaved can be straightforwardly removed by following the approach in Section 3.6 of [CDSS14]. That paper presents a simple linear-time sampling-based procedure, using O\u0303(k/\u03b5) samples, that with high probability identifies all the \u201cheavy\u201d elements (atoms which cause p to not be well-behaved, if any such points exist).\nOur overall algorithm first runs this procedure to find the set S of \u201cheavy\u201d elements, and then runs the algorithm presented above (which succeeds for well-behaved distributions, i.e., distributions that have no \u201cheavy\u201d elements) using as its target distribution the conditional distribution of p over [0, 1) \\ S (let us denote this conditional distribution by p\u2032). A straightforward analysis given in [CDSS14] shows that (i) optk(p) \u2265 optk(p\u2032), and moreover (ii) dTV(p, p\u2032) \u2264 optk(p). Thus, by the triangle inequality, any hypothesis h satisfying dTV(h, p\n\u2032) \u2264 Coptk(p\u2032) + \u03b5 will also satisfy dTV(h, p) \u2264 (C + 1)optk(p) + \u03b5 as desired."}, {"heading": "4 Lower bounds on agnostic learning", "text": "In this section we establish that \u03b1-agnostic learning with \u03b1 < 2 is information theoretically impossible, thus establishing Theorem 2.\nFix any 0 < t < 1/2. We define a probability distribution Dt over a finite set of discrete distributions over the domain [2N ] = {1, . . . , 2N} as follows. (We assume without loss of generality below that t is rational and that tN is an integer.) A draw of pS1,S2,t from Dt is obtained as follows.\n1. A set S1 \u2282 [N ] is chosen uniformly at random from all subsets of [N ] that contain precisely tN elements. For i \u2208 [N ], the distribution pS1,S2,t assigns probability weight as follows:\npS1,S2,t(i) = 1\n4N if i \u2208 S1, pS1,S2,t(i) =\n1\n2N\n( 1 +\nt\n2(1 \u2212 t)\n) if i \u2208 [N ] \\ S1.\n2. A set S2 \u2282 [N +1, . . . , 2N ] is chosen uniformly at random from all subsets of [N +1, . . . , 2N ] that contain precisely tN elements. For i \u2208 [N + 1, . . . , 2N ], the distribution pS1,S2,t assigns probability weight as follows:\npS1,S2,t(i) = 3\n4N if i \u2208 S2,\n1\n2N\n( 1\u2212 t\n2(1\u2212 t)\n) if i \u2208 [N ] \\ S1.\nUsing a birthday paradox type argument, we show that no o( \u221a N)-sample algorithm can successfully distinguish between a distribution pS1,S2,t \u223c Dt and the uniform distribution over [2N ]. We then leverage this indistinguishability to show that any (2\u2212\u03b4)-semi-agnostic learning algorithm, even for 2-flat distributions, must use a sample of size \u2126( \u221a N):\nTheorem 7. Fix any \u03b4 > 0 and any function f(\u00b7). There is no algorithm A with the following property: given \u03b5 > 0 and access to independent points drawn from an unknown distribution p over\n[2N ], algorithm A makes o( \u221a N) \u00b7 f(\u03b5) draws from p and with probability at least 51/100 outputs a hypothesis distribution h over [2N ] satisfying \u2016h\u2212 p\u20161 \u2264 (2\u2212 \u03b4)opt2(p) + \u03b5.\nProof. We write U2N to denote the uniform distribution over [2N ]. The following proposition shows that U2N has L1 distance from pS1,S2,t almost twice that of the optimal 2-flat distribution: Proposition 4.1. Fix any 0 < t < 1/2.\n1. For any distribution pS1,S2,t in the support of Dt, we have\n\u2016U2N \u2212 pS1,S2,t\u20161 = t.\n2. For any distribution pS1,S2,t in the support of Dt, we have\nopt2(pS1,S2,t) \u2264 t\n2\n( 1 + t\n1\u2212 t\n) .\nProof. Part (1.) is a simple calculation. For part (2.), consider the 2-flat distribution\nq(i) =    1 2N ( 1 + t2(1\u2212t) ) if i \u2208 [N ]\n1 2N ( 1\u2212 t2(1\u2212t) ) if i \u2208 [N + 1, . . . , 2N ]\nIt is straightforward to verify that \u2016pS1,S2,t \u2212 q\u20161 = t2 ( 1 + t1\u2212t ) as claimed.\nFor a distribution p we write Ap to indicate that algorithm A is given access to i.i.d. points drawn from p.\nThe following simple proposition states that no algorithm can successfully distinguish between a distribution pS1,S2,t \u223c Dt and U2N using fewer than (essentially) \u221a N draws:\nProposition 4.2. There is an absolute constant c > 0 such that the following holds: Fix any 0 < t < 1/2, and let B be any \u201cdistinguishing algorithm\u201d which receives c \u221a N i.i.d. draws from a distribution over [2N ] and outputs either \u201cuniform\u201d or \u201cnon-uniform\u201d. Then\n\u2223\u2223\u2223Pr[BU[2N] outputs \u201cuniform\u201d]\u2212PrpS1,S2,t\u223cDt [B pS1,S2,t outputs \u201cuniform\u201d] \u2223\u2223\u2223 \u2264 0.01. (2)\nThe proof is an easy consequence of the fact that in both cases (the distribution is U[2N ], or the distribution is pS1,S2,t \u223c Dt), with probability at least 0.99 the c \u221a N draws received by A are a\nuniform random set of c \u221a N distinct elements from [2N ] (this can be shown straighforwardly using a birthday paradox type argument). Now we use Proposition 4.2 to show that any (2\u2212 \u03b4)-semi-agnostic learning algorithm even for\n2-flat distributions must use a sample of size \u2126( \u221a N), and thereby prove Theorem 7.\nFix a value of \u03b4 > 0 and suppose, for the sake of contradiction, that there exists such an algorithm A. We describe how the existence of such an algorithm A yields a distinguishing algorithm B that violates Proposition 4.2.\nThe algorithm B works as follows, given access to i.i.d. draws from an unknown distribution p. It first runs algorithm A with its \u201c\u03b5\u201d parameter set to \u03b5 := \u03b4 3\n12(2+\u03b4) , obtaining (with probability\nat least 51/100) a hypothesis distribution h over [2N ] such that \u2016h \u2212 p\u20161 \u2264 (2 \u2212 \u03b4)opt2(p) + \u03b5. It then computes the value \u2016h \u2212 U2N\u20161 of the L1-distance between h and the uniform distribution\n(note that this step uses no draws from the distribution). If \u2016h \u2212 U2N\u20161 < 3\u03b5/2 then it outputs \u201cuniform\u201d and otherwise it outputs \u201cnon-uniform.\u201d\nSince \u03b4 (and hence \u03b5) is independent of N , the algorithm B makes fewer than c \u221a N draws from p (for N sufficiently large). To see that the above-described algorithm B violates (2), consider first the case that p is U[2N ]. In this case opt2(p) = 0 and so with probability at least 51/100 the hypothesis h satisfies \u2016h\u2212 U2N\u20161 \u2264 \u03b5, and hence algorithm B outputs \u201cuniform\u201d with probability at least 51/100.\nOn the other hand, suppose that p = pS1,S2,t is drawn from Dt, where t = \u03b42+\u03b4 . In this case, with probability at least 51/100 the hypothesis h satisfies\n\u2016h\u2212 pS1,S2,t\u20161 \u2264 (2\u2212 \u03b4)opt2(pS1,S2,t) + \u03b5 \u2264 (2\u2212 \u03b4) \u00b7 t 2 \u00b7 ( 1 + t 1\u2212 t ) + \u03b5,\nby part (2.) of Proposition 4.1. Since by part (1.) of Proposition 4.1 we have \u2016U2N \u2212 pS1,S2,t\u20161 = t, the triangle inequality gives that\n\u2016h\u2212 U2N\u20161 \u2265 t\u2212 (2\u2212 \u03b4) \u00b7 t 2 \u00b7 ( 1 + t 1\u2212 t ) \u2212 \u03b5 = 2\u03b5,\nwhere to obtain the final equality we recalled the settings \u03b5 = \u03b4 3 12(2+\u03b4) , t = \u03b4 2+\u03b4 . Hence algorithm B outputs \u201cuniform\u201d with probability at most 49/100. Thus we have\n\u2223\u2223\u2223Pr[BU[2N] outputs \u201cuniform\u201d]\u2212PrpS1,S2,t\u223cDt [B pS1,S2,t outputs \u201cuniform\u201d] \u2223\u2223\u2223 \u2265 0.02\nwhich contradicts (2) and proves the theorem.\nAs described in the Introduction, via the obvious correspondence that maps distributions over [N ] to distributions over [0, 1), we get the following:\nCorollary 4.3. Fix any \u03b4 > 0 and any function f(\u00b7). There is no algorithm A with the following property: given \u03b5 > 0 and access to independent draws from an unknown distribution p over [0, 1), algorithm A makes f(\u03b5) draws from p and with probability at least 51/100 outputs a hypothesis distribution h over [0, 1) satisfying \u2016h\u2212 p\u20161 \u2264 (2\u2212 \u03b4)opt2(p) + \u03b5."}], "references": [{"title": "Near-optimal-sample estimators for spherical gaussian mixtures", "author": ["J. Acharya", "A. Jafarpour", "A. Orlitsky", "A.T. Suresh"], "venue": "Technical Report http://arxiv.org/abs/1402.4746,", "citeRegEx": "Acharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2014}, {"title": "Statistical Inference under Order Restrictions", "author": ["R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk"], "venue": null, "citeRegEx": "Barlow et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Barlow et al\\.", "year": 1972}, {"title": "Estimating a density under order restrictions: Nonasymptotic minimax risk", "author": ["L. Birg\u00e9"], "venue": "Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1987\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1987}, {"title": "Estimation of unimodal densities without smoothness assumptions", "author": ["L. Birg\u00e9"], "venue": "Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1997\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1997}, {"title": "On the estimation of parameters restricted by inequalities", "author": ["H.D. Brunk"], "venue": "Ann. Math. Statist.,", "citeRegEx": "Brunk.,? \\Q1958\\E", "shortCiteRegEx": "Brunk.", "year": 1958}, {"title": "Learning mixtures of structured distributions over discrete domains", "author": ["S. Chan", "I. Diakonikolas", "R. Servedio", "X. Sun"], "venue": "In SODA,", "citeRegEx": "Chan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2013}, {"title": "Efficient density estimation via piecewise polynomial approximation", "author": ["S. Chan", "I. Diakonikolas", "R. Servedio", "X. Sun"], "venue": "Technical Report http://arxiv.org/abs/1305.3207, conference version in STOC,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Random sampling for histogram construction: How much is enough", "author": ["S. Chaudhuri", "R. Motwani", "V. Narasayya"], "venue": "In SIGMOD Conference,", "citeRegEx": "Chaudhuri et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 1998}, {"title": "Inverse problems in approximate uniform generation", "author": ["A. De", "I. Diakonikolas", "R. Servedio"], "venue": "Available at http://arxiv.org/pdf/1211.1722v1.pdf,", "citeRegEx": "De et al\\.,? \\Q2012\\E", "shortCiteRegEx": "De et al\\.", "year": 2012}, {"title": "Nonparametric Density Estimation: The L1 View", "author": ["L. Devroye", "L. Gy\u00f6rfi"], "venue": null, "citeRegEx": "Devroye and Gy\u00f6rfi.,? \\Q1985\\E", "shortCiteRegEx": "Devroye and Gy\u00f6rfi.", "year": 1985}, {"title": "Faster and sample near-optimal algorithms for proper learning mixtures of gaussians", "author": ["C. Daskalakis", "G. Kamath"], "venue": "In COLT,", "citeRegEx": "Daskalakis and Kamath.,? \\Q2014\\E", "shortCiteRegEx": "Daskalakis and Kamath.", "year": 2014}, {"title": "Combinatorial methods in density estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye and Lugosi.,? \\Q2001\\E", "shortCiteRegEx": "Devroye and Lugosi.", "year": 2001}, {"title": "Fast, small-space algorithms for approximate histogram maintenance", "author": ["A. Gilbert", "S. Guha", "P. Indyk", "Y. Kotidis", "S. Muthukrishnan", "M. Strauss"], "venue": "In STOC,", "citeRegEx": "Gilbert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gilbert et al\\.", "year": 2002}, {"title": "Approximation and streaming algorithms for histogram construction problems", "author": ["S. Guha", "N. Koudas", "K. Shim"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "Guha et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2006}, {"title": "On the theory of mortality", "author": ["U. Grenander"], "venue": "measurement. Skand. Aktuarietidskr.,", "citeRegEx": "Grenander.,? \\Q1956\\E", "shortCiteRegEx": "Grenander.", "year": 1956}, {"title": "Estimating a monotone density", "author": ["P. Groeneboom"], "venue": "In Proc. of the Berkeley Conference in Honor of Jerzy Neyman and Jack Kiefer,", "citeRegEx": "Groeneboom.,? \\Q1985\\E", "shortCiteRegEx": "Groeneboom.", "year": 1985}, {"title": "Consistency in concave regression", "author": ["D.L. Hanson", "G. Pledger"], "venue": "The Annals of Statistics,", "citeRegEx": "Hanson and Pledger.,? \\Q1976\\E", "shortCiteRegEx": "Hanson and Pledger.", "year": 1976}, {"title": "Approximating and Testing k-Histogram Distributions in Sub-linear Time", "author": ["P. Indyk", "R. Levi", "R. Rubinfeld"], "venue": "In PODS,", "citeRegEx": "Indyk et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 2012}, {"title": "Optimal histograms with quality guarantees", "author": ["H.V. Jagadish", "N. Koudas", "S. Muthukrishnan", "V. Poosala", "K. Sevcik", "T. Suel"], "venue": "In VLDB,", "citeRegEx": "Jagadish et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jagadish et al\\.", "year": 1998}, {"title": "On the learnability of discrete distributions", "author": ["M. Kearns", "Y. Mansour", "D. Ron", "R. Rubinfeld", "R. Schapire", "L. Sellie"], "venue": "In Proc. 26th STOC,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "Estimation of a unimodal density", "author": ["B.L.S. Prakasa Rao"], "venue": "Sankhya Ser. A,", "citeRegEx": "Rao.,? \\Q1969\\E", "shortCiteRegEx": "Rao.", "year": 1969}, {"title": "Estimation of a function under shape restrictions", "author": ["L. Reboul"], "venue": "Applications to reliability. Ann. Statist.,", "citeRegEx": "Reboul.,? \\Q2005\\E", "shortCiteRegEx": "Reboul.", "year": 2005}, {"title": "Multivariate Density Estimation: Theory, Practice and Visualization", "author": ["D.W. Scott"], "venue": null, "citeRegEx": "Scott.,? \\Q1992\\E", "shortCiteRegEx": "Scott.", "year": 1992}, {"title": "Density Estimation", "author": ["B.W. Silverman"], "venue": null, "citeRegEx": "Silverman.,? \\Q1986\\E", "shortCiteRegEx": "Silverman.", "year": 1986}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "In Proc. 16th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "Maximum likelihood estimation of a unimodal density", "author": ["E.J. Wegman"], "venue": "I. and II. Ann. Math. Statist.,", "citeRegEx": "Wegman.,? \\Q1970\\E", "shortCiteRegEx": "Wegman.", "year": 1970}], "referenceMentions": [], "year": 2014, "abstractText": "Let p be an unknown and arbitrary probability distribution over [0, 1). We consider the problem of density estimation, in which a learning algorithm is given i.i.d. draws from p and must (with high probability) output a hypothesis distribution that is close to p. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function. In more detail, for any k and \u03b5, we give an algorithm that makes \u00d5(k/\u03b5) draws from p, runs in \u00d5(k/\u03b5) time, and outputs a hypothesis distribution h that is piecewise constant with O(k log(1/\u03b5)) pieces. With high probability the hypothesis h satisfies dTV(p, h) \u2264 C \u00b7optk(p)+\u03b5, where dTV denotes the total variation distance (statistical distance), C is a universal constant, and opt k (p) is the smallest total variation distance between p and any k-piecewise constant distribution. The sample size and running time of our algorithm are optimal up to logarithmic factors. The \u201capproximation factor\u201d C in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of k and \u03b5 can achieve C < 2 regardless of what kind of hypothesis distribution it uses.", "creator": "LaTeX with hyperref package"}}}