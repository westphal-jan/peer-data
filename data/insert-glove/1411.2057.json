{"id": "1411.2057", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2014", "title": "Online Collaborative-Filtering on Graphs", "abstract": "A common aniko phenomena quadriceps in 1,551 modern lats recommendation fated systems is the gray use of feedback carmello from iptf one user to infer kelty the ` m\u0101ori value ' of fulkerson an playtime item carbocations to other users. This naft results glenelg in an daule exploration vs. exploitation trojan\u00f3w trade - caimans off, in which items hunzvi of possibly felids low newmyer value doll-like have to crisscross be presented guven to smitt users end-1997 in order ecn to cumia ascertain their value. aucuparia Existing hidipo approaches a.r.rahman to solving boulkiemd\u00e9 this eri problem focus on the case 1927 where lulzsec the 14.20 number of items nafs are \u03c31 small, hachey or karbon admit some underlying mukhopadhyay structure - - it sono is litle unclear, 37.85 however, if good recommendation spaht is possible neutrinos when enguerrand dealing dendi with cyberattacks content - alarc\u00f3n rich settings thunderdome with over-run unstructured 126.1 content.", "histories": [["v1", "Fri, 7 Nov 2014 22:52:19 GMT  (185kb,D)", "http://arxiv.org/abs/1411.2057v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siddhartha banerjee", "sujay sanghavi", "sanjay shakkottai"], "accepted": false, "id": "1411.2057"}, "pdf": {"name": "1411.2057.pdf", "metadata": {"source": "CRF", "title": "Online Collaborative Filtering on Graphs", "authors": ["Siddhartha Banerjee", "Sujay Sanghavi", "Sanjay Shakkottai"], "emails": ["sidb@stanford.edu", "sanghavi@mail.utexas.edu,", "shakkott@mail.utexas.edu"], "sections": [{"heading": null, "text": "Online Collaborative Filtering on Graphs Siddhartha Banerjee\nDepartment of Management Science and Engineering, Stanford University, Stanford, CA 94025 sidb@stanford.edu\nSujay Sanghavi, Sanjay Shakkottai Department of ECE, The University of Texas at Austin, Austin, TX 78705\nsanghavi@mail.utexas.edu, shakkott@mail.utexas.edu\nA common phenomena in modern recommendation systems is the use of feedback from one user to infer the \u2018value\u2019 of\nan item to other users. This results in an exploration vs. exploitation trade-off, in which items of possibly low value have to be presented to users in order to ascertain their value. Existing approaches to solving this problem focus on the case where the number of items are small, or admit some underlying structure \u2013 it is unclear, however, if good recommendation is possible when dealing with content-rich settings with unstructured content.\nWe consider this problem under a simple natural model, wherein the number of items and the number of item-views are of the same order, and an \u2018access-graph\u2019 constrains which user is allowed to see which item. Our main insight is that the presence of the access-graph in fact makes good recommendation possible \u2013 however this requires the exploration policy to be designed to take advantage of the access-graph. Our results demonstrate the importance of \u2018serendipity\u2019 in exploration, and how higher graph-expansion translates to a higher quality of recommendations; it also suggests a reason why in some settings, simple policies like Twitter\u2019s \u2018Latest-First\u2019 policy achieve a good performance.\nFrom a technical perspective, our model presents a way to study exploration-exploitation tradeoffs in settings where the number of \u2018trials\u2019 and \u2018strategies\u2019 are large (potentially infinite), and more importantly, of the same order. Our algorithms admit competitive-ratio guarantees which hold for the worst-case user, under both finite-population and infinite-horizon settings, and are parametrized in terms of properties of the underlying graph. Conversely, we also demonstrate that improperly-designed policies can be highly sub-optimal, and that in many settings, our results are order-wise optimal.\nKey words: online recommendation, social networks, competitive analysis"}, {"heading": "1. Introduction", "text": "The modern internet experience hinges on the ability of content providers to effectively recommend con-\ntent to users. In such online recommendation settings, user feedback often provides the best guide to the\n\u2018value\u2019 of a piece of content. In content-curation websites like Digg and Reddit, article recommendation\nis often done in terms of \u2018popular stories\u2019, i.e. content other users found interesting on viewing. In social\nnetworks like Twitter and Facebook, each user is shown a (often small) subset of all content generated by\nher friends/contacts; the selection is based, among other things, on feedback (\u2018likes\u2019) from other users. In\n1\nar X\niv :1\n41 1.\n20 57\nv1 [\ncs .L\nG ]\n7 N\nov 2\n01 4\nonline advertising, ads that have been shown to a lot of users without much uptake are less likely to work\nthan others with good uptake.\nTwo features are common across these settings: (i) content-richness, wherein the amount of available\ncontent grows far in excess of what users can consume, and (ii) unstructured content, wherein the value\nof one item of content need not be predictive of the value of other items. For example, in social networks,\nevery user is both a consumer and also a creator of content, at comparable rates; thus, the available content\nis of the same order as the total content-views across all users (and far exceeding what a single user can be\nshown). Further, content is often unstructured, exhibiting high variability due to periodic trends, one time\nevents, etc. In particular, knowing one piece of content is good need not imply that all content uploaded\nby the same user is of uniformly high quality. These features make algorithms for recommending relevant\ncontent become more critical, but also harder to design.\nAny system that both recommends items to users, and then leverages their feedback to improve the rec-\nommendations, faces an exploration-exploitation trade-off: should a user be shown a new item of unknown\nvalue, in the hope of benefiting future users? Or should she be shown an item that is already known to\ngive her good value? This trade-off has been extensively studied in settings wherein the number of items is\nsmall, or admit some underlying structure (See Section 6 for a discussion of this prior work) \u2013 however it is\nunclear if these techniques carry over to the applications we describe above.\nAnother crucial feature of the settings described above is the presence of an underlying access graph\nbetween users and items, which constrains what items a user can be presented with. For example, in a\nsocial network, users only want to view content uploaded by their friends \u2013 the access graph here is the\nfriendship/follower graph. In content-curation, users may \u2018subscribe\u2019 to a set of topics, indicating that they\nare only interested in content related to these topics. The focus of this paper is to study the effect of such\nan access graph on recommendation algorithm design \u2013 in particular, we suggest that if properly used, the\npresence of this access-graph may in fact improve the quality of recommendation algorithms.\nWe consider the following stylized model: we are given a bipartite access graph between users and items,\nwhich specifies which items each user can potentially be shown. Both users and items arrive to the system\naccording to some random process with similar rates \u2013 this captures content-richness. For each visiting user,\nthe algorithm selects a subset of \u2018neighboring items\u2019 to present to the user. Each item has an associated\nvalue, which can be arbitrary \u2013 this captures the unstructured nature of the content. Furthermore, item\nvalues are a priori unknown to the algorithm \u2013 to learn them, the algorithm depends on feedback from\nusers. We capture this dependence via the following condition: for any user, the algorithm can identify the\ncorresponding highest valued items from the set of pre-explored items \u2013 where an item is said to be pre-\nexplored if the algorithm has presented it to at least one user (or more generally, some finite number of\nusers). The performance of an algorithm is measured in terms of the competitive-ratio \u2013 the ratio of the\nreward that the algorithm earns for an arbitrary user, to the best available reward for that user (i.e., the reward earned by a \u2018genie-aided\u2019 algorithm, with complete knowledge of the item-values).\nIn content-rich settings, it is not possible that all items be presented more than some constant number of times to users. Thus, the act of presenting a popular item to many users may result in other items never being presented to anyone. In a sense, the critical distinction in content-rich settings is between items for which there are no ratings, and those for which there are some \u2013 this is precisely what our model captures.\nGiven that the algorithm knows the value of pre-explored items, a sufficient condition for guaranteeing a good per-user competitive-ratio is as follows: the algorithm should explore items in a manner such that for any user, her most relevant items are explored before she arrives in the system. It is not hard to see that this is a desirable property, but it may appear too strong a requirement; surprisingly however, we show a milder condition \u2013 the above property holding with a non-vanishing probability \u2013 is in fact achievable in many settings, and using very simple algorithms. On the other hand, competitive performance is by no means guaranteed for all algorithms in this scenario \u2013 we show that certain \u2018natural\u2019 algorithms turn out to have vanishing competitive ratio. Furthermore, we derive minimax upper bounds on the competitive ratio which show our results are orderwise optimal in many settings.\nOur results point to three interesting qualitative observations: \u2022 The role of the access-graph: We show how the presence of the graph can in fact improve the quality of\nrecommendations; more precisely, we quantify how this improvement depends on certain expansionlike parameters of the graph. \u2022 The importance of serendipity: Our exploration schemes depend on biasing recommendations towards\ncontent from less popular users. Moreover, we show that this is necessary in a very strong sense.\n\u2022 The efficacy of simple algorithms: In particular, our results in the infinite-horizon setting suggest a\nreason behind the effectiveness of Twitter\u2019s \u2018latest-first\u2019 recommendation policy.\nExploration-exploitation trade-offs have been extensively studied in online recommendation literature; in particular, a popular model is the stochastic bandit model and its variants. The main assumption in bandit settings is that each item (or arm), upon being displayed, gives an i.i.d reward from some distribution with unknown mean (Auer et al. (2002); see also Bubeck and Cesa-Bianchi (2008) for a survey of the field). Algorithms for these settings are closely tied to this assumption \u2013 they focus on detecting suboptimal items via repeated plays, where the number of plays scale with the number of items. This is not feasible in content-rich settings with a very large, possibly infinite, number of arms, and arbitrary values. Indeed using bandit algorithms implies a 0 competitive-ratio in our setting, which is not surprising as traditional bandit algorithms are designed for a different setting. We discuss this in more detail in Section 6.\nWe present our results for the case where an item needs to be explored once to know its value; however, our algorithms extend to settings where each item needs a finite number of showings to estimate its value to within a multiplicative factor (as discussed in Section 5). Empirical observations (e.g., by Szabo and\nHuberman (2010) and Yang and Leskovec (2011)) indicate that this model is reasonable: in large social\nnetworks/content-curation sites, the popularity of an item can be reliably inferred by showing it to a small\nnumber of users. Furthermore, work on static recommendation (Keshavan et al. (2010), Jagabathula and\nShah (2008)) also provide guarantees for learning item-value from a few ratings under alternate structural\nassumptions; these observations tie in well with our model."}, {"heading": "1.1. Summary of Our Contributions", "text": "We consider two settings \u2013 a finite-population setting and an infinite-horizon setting. The first is a good\nmodel for ad-placement and content-curation, wherein items arrive in batches; the infinite-horizon model is\nmore natural for applications like social-network updates, which have continuous arrivals and departures. Model: In the finite-population model (Section 2.1) we assume there is a bipartite access graph between a\n(fixed) set of nU users and nI items \u2013 a user can view an item if and only if she is connected to it. Users arrive in a random order, and are presented with r item-recommendations. Each item i has an intrinsic value\nV (i) \u2013 the total reward earned by a user is the sum of rewards of presented items. The item values are a\npriori unknown to the algorithm but become known after an item is recommended for the first time. Thus,\nfor any user, the algorithm can always identify the top r pre-explored items.\nIn the infinite-horizon model (Section 3.1), the underlying access-graph G is between a finite set of users\nNu and a finite set of item-classes NC . The system evolves in time, with user/item arrivals and departures. Each user makes multiple visits to the system, according to an independent Poisson process; similarly, for\neach item-class, individual items arrive according to an independent Poisson process. Items have arbitrary\nvalues, which are a priori unknown; again, we assume that the algorithm can identify the top r pre-explored\nitems for each user. Furthermore, each item is available only for a fixed lifetime. To the best of our knowl-\nedge, ours is the first work which provides guarantees for online recommendation under Markovian dynam-\nics but arbitrary item-values.\nOur algorithms are as follows: given r slots to present items to an arriving user, we split them between\nexplore and exploit slots uniformly at random. In the exploit slots, we present the highest-valued pre-\nexplored items (which by our assumption can be identified). For the explore slots, we present previously\nunexplored items \u2013 the crucial ingredient is the policy for choosing these items. Our results are as follows:\n1. Exploration via Balanced Partitions: In the finite setting, we present an algorithm based on picking\nunexplored items via balanced semi-matchings (or balanced item-partitions). We show this achieves a competitive-ratio guarantee of \u2126(r/d\u2217(G)) (Theorem 1), where r is the number of recommendations per user, and d\u2217(G) is the minimum makespan of the graph G.\n2. Exploration via Inverse-Degree Sampling: We also present an alternate algorithm that does not use pre-\nprocessing, and further only requires node-degree information. For each user, the algorithm chooses\nitems for exploration by randomly picking neighborhood items with a probability inversely proportional to their degree. This policy achieves a competitive-ratio guarantee of \u2126(r/Zmax(G)) (Theorem 2), where Zmax(G) is a measure of the non-regularity of the graph \u2013 it is greater than the makespan d\u2217(G), but the two are close when the graph is near-regular. 3. In the case of regular graphs, both the above algorithms have competitive-ratio guarantees of\n\u2126(rnI/nU). Conversely, in the finite setting, we show that for all graphs, no algorithm can achieve a competitive ratio better than O(rnI/nU) (Theorem 5). 4. Exploration via Uniform Latest-Item Sampling: In the infinite-horizon setting, we propose a compet-\nitive algorithm based on discarding items if not explored by their first neighboring user. Each user is presented items drawn uniformly and without replacement from the set of latest-items \u2013 those which have not had the chance to be presented to any prior user. When all arrival processes (of users/items) have rate 1, we prove that this policy achieves a competitive-ratio of \u2126(r/Zmax(G)) (Theorem 3). 5. Finally, we show that some intuitive algorithms \u2013 those which always exploit if sufficiently high-valued\nitems are available, or sample nodes uniformly or proportional to degree (or in fact, proportional to any polynomial function other than inverse-degree) \u2013 have 0 competitive-ratio. In both models, our algorithms and results generalize to the setting where an item needs to be viewed by f users to approximately determine the value \u2013 within a multiplicative (1\u00b1 \u03b4(f)) factor for some \u03b4(f) \u2208 [0,0.5). Further, we do not require for our results that the value be known, but rather, that the top r preexplored items for a user be identified by the algorithm. This allows for various extensions \u2013 in particular, the value can depend on the user identity, i.e., V (i) is replaced by V (u, i) where i corresponds to the item and u to the user identity. We refer to Section 5 for a more detailed discussion."}, {"heading": "2. The Finite-Population Setting", "text": "We first consider a finite-population setting, where the number of users and items is fixed, and users arrive uniformly at random. This is a good model for certain content-curation problems like news-aggregators (e.g., Google News), where a large number of articles appear together (at the beginning of a day), and expire at the end of the day \u2013 in the meantime, throughout the day, users appear uniformly at random. Furthermore, it also lets us present our main ideas in a more succinct form, avoiding the technical aspects of the infinite-setting while still conveying the main ideas and challenges."}, {"heading": "2.1. System Model", "text": "Access Graph: G(NU ,NI ,E) represents the (given) bipartite access graph between users NU and items NI (with |NU | = nU and |NI | = nI). For a user u \u2208 NU , we define its neighborhood as N (u) := {i \u2208 NI |(u, i) \u2208 E}, and degree du = |N (u)|; similarly for item i \u2208NI , we can define N (i) and di. Items are always present in the system, while users arrive to the system according to a uniform random permutation.\nItem Exploration: Each item has an associated non-negative value V (i), which is a priori unknown; how-\never, presenting it to even one user reveals V (i) exactly. Upon arrival, a user is presented a set of r items from N (u). We define the N explI at any instant to be the set of pre-explored items, i.e., which have been presented to at least 1 user in the past. We assume that N explI = \u03c6 at the start; however, all our results hold for any initial N explI .\nObjective: For any user u, upon arrival, algorithm A presents r items {iA1 (u) . . . iAr (u)}. Thus, for given item rewards V , the total reward earned by u under algorithm A is RAr (u) = \u2211r k=1 V (i A k (u)). Further, suppose the r highest-valued neighboring items for u be {i\u22171(u) . . . i\u2217r(u)} \u2013 then we define the optimal\nreward R\u2217r(u) = \u2211r k=1 V (i \u2217 k(u)). Finally, we define the competitive-ratio \u03b3 A r (G) for algorithmA (for graph G, r-recommendations) as:\n\u03b3A(G,r) = inf V \u2208RnI+ inf u\u2208NU E [RAr (u)] R\u2217r(u) .\nThe expectation here is both over random user-arrivals as well as randomness in algorithm A; however, note that R\u2217r(u) is uniquely determined \u2200u given G and item-values V . The competitive-ratio thus captures a worst case guarantee for individual users and all non-negative item values. Note that taking an infimum\nover user-rewards, rather than considering the cumulative reward (i.e., the sum over all users), results in a\nmore stringent objective. However, this is more appropriate in a recommendation setting, as it corresponds\nto a natural notion of fairness \u2013 it is a guarantee on the quality of experience for any user on the platform."}, {"heading": "2.2. Exploration via Balanced Item-Partitions:", "text": "For each user, the algorithm splits the r recommendations between explore and exploit uniformly at random.\nThe exploration step is based on the following pre-processing step: we partition the item-set NI into nU sets by associating each item with exactly one of its neighboring users \u2013 we do so in a manner such that the\npartitions are balanced, i.e., we try to minimize the cardinality of the largest set.\nDEFINITION 1. (Balanced Partition) Given graph G, a semi-matching M = {M(u)}u\u2208NU is a partition of the item-set such thatM(u)\u2286N (u)\u2200u\u2208NU (i.e., each setM(u) is a subset of the neighbors of user u). Given a semi-matchingM , we define the load of user u as dM(u) = |M(u)|. Then a balanced item-partition M is a solution to the optimization problem:\nd\u2217(G) = Minimize {M :semi-matching} [ max u\u2208NU dM(u) ] .\nThe above problem is known in different communities as the minimum makespan problem (Graham (1966)), or optimal semi-matching problem (Harvey et al. (2003)) \u2013 we henceforth refer to d\u2217(G) as the makespan\nof graph G. Efficient algorithms are known for finding a balanced item-partition, with a complexity of O(m \u221a n logn) (Fakcharoenphol et al. (2010)), where m= |E|, n= nU +nI .\nGiven a balanced item-partition generation routine, we define the Balanced Partition Exploration Algo-\nrithm, or BPExp, which can be summarized as follows: we pre-select a balanced item-partition as an\nexploration schedule; for each arriving user, we independently allocate each \u2018recommendation slot\u2019 to be an\nexplore or exploit slot with probability 1/2; for exploration, we display items picked uniformly at random\n(without replacement) from the user\u2019s items in the balanced item-partition; for exploitation, we display the\nmost valuable available pre-explored items. Formally, the algorithm is given in Algorithm 1.\nAlgorithm 1 BPExp: Exploration via Balanced Item-Partitions 1: Generate a balanced item-partition M of G. Initialize set of explored items N explI = \u03c6.\n2: for arriving user u\u2208NU do 3: Choose R1(u) \u223c Binomial(r, 12) slots for exploration, and the rest R2(u) = r \u2212 R1(u) slots for\nexploitation.\n4: {Exploration}: Choose R1(u) items from the set M(u) uniformly at random, without replacement. 5: {Exploitation}: Recommend the R2(u) highest-valued items from N (u)\u2229N explI . 6: Update N explI by adding the R1(u) items explored by u. 7: end for\nTHEOREM 1. Given graph G, reward-function V (i) and uniformly random user-arrival pattern, using the BPExp algorithm (Algorithm 1) we get:\n\u03b3BPExp(G,r)\u2265min {\nr 8d\u2217(G) , 1 4\n} .\nRemarks:\n\u2022 An immediate corollary of this result is as follows: given any graphG that contains a perfect matching, then the competitive-ratio guaranteed by the BPExp algorithm on this graph is min { r 8 , 1 4 } . More\ngenerally, if G is a bi-regular graph, with nI \u2265 nU (i.e., if all nodes in NU have the same degree, and similarly all nodes in NI), then \u03b3BPExp(G,r)\u2265 rnU8nI . \u2022 BPExp guarantees a linear scaling with r. However, note that we compare the reward earned by BPExp\nto the optimal reward for r recommendations. In settings where there are \u2126(r) high-valued items, the optimal reward scales linearly with r \u2013 in such cases, BPExp\u2019s reward scales quadratically. In Section 4, we show that linear scaling of \u03b3(G,r) with r is in fact the best achievable by any algorithm. \u2022 Consider a graph, where each user is connected to d\u2217(G) items of degree 1 \u2013 in this case, it is clear\nthat the best possible competitive-ratio is r d\u2217(G) . This example is somewhat trivial as it offers no scope for using feedback \u2013 at the other extreme, in Section 4, we show that no algorithm can have a better competitive-ratio than rnU 2nI in the complete bipartite graph, where d\u2217(G) = nI nU . The above theorem\nshows that on the other hand \u2126 (\nr d\u2217(G)\n) is achievable in all graphs.\nProof Outline: Consider the r= 1 case. The proof now rests on the following observation \u2013 any user u is guaranteed to be presented its corresponding highest-valued item i\u2217(u) if either: 1. the user u\u2032 responsible for exploring i\u2217(u) comes to the system before u, or 2. u\u2032 chose to explore, and explored i\u2217(u); u chose to exploit. The former happens with probability 1 2 due to randomness in user arrivals. Further, the way we define the BPExp algorithm allows the probability of the latter to be bounded. Combining the two we get the result. We provide the complete proof in Section 7."}, {"heading": "2.3. Exploration via Inverse-Degree Sampling:", "text": "Although it has a good competitive-ratio, the BPExp has several drawbacks:\n1. Pre-processing to generate a balanced item-partition is computationally expensive for large graphs. 2. The pre-processing step is inherently centralized and requires extensive coordination between the\nusers. This may be infeasible (due to complexity, privacy concerns, etc.).\n3. The exploration policy is static. If the underlying graph changes, the item-partition has to be updated. We now present an alternate approach which overcomes these problems by using a distributed and dynamic exploration policy. The main idea is that a user, upon arrival, picks a neighboring item for exploration with a probability inversely proportional to the degree of the item. This can be done with minimal local knowledge of the graph (in fact, the degree information is often publicly available, e.g., followers on\nTwitter, friends on Facebook/Google+). The resulting competitive-ratio bounds are weaker \u2013 in particular, the makespan d\u2217(G) is now replaced a quantity Zmax(G), defined as follows:\nZmax(G) := max u\u2208NU\nZ(u), where Z(u) := \u2211\ni\u2208N (u)\nd\u22121i .\nNote that Z(u) is the normalization in inverse-degree sampling, i.e., when all neighboring items are unexplored, then user u samples item i with pui = d\u22121i /Z(u). To avoid problems with conditioning in the proof, we perform an additional step \u2013 for each user, we partition the neighboring items as follows:\nDEFINITION 2. (Greedy Neighborhood Partitioning) For user u, given neighboring-items set N (u) with degrees {di}, we sort the items in descending order of d\u22121i and then generate partition Pu = {P 1u , P 2u , . . . , P ru} by iteratively assigning each item to the set P ku with smallest sum-weight.\nNote that the item-partitioning is performed separately for each user \u2013 it is not a centralized operation. Given this pre-processing routine, we define the Inverse Degree Exploration Algorithm, or IDExp, as follows:\nAlgorithm 2 IDExp: Exploration via Inverse-Degree Sampling 1: for arriving user u\u2208NU do\n2: Generate item-set partition Pu = {P 1u , P 2u , . . . , P ru} using Greedy Neighborhood Partitioning. 3: Choose R1(u) \u223c Binomial(r, 12) slots for exploration, and the rest R2(u) = r \u2212 R1(u) slots for\nexploitation.\n4: {Exploration} Pick R1 sets without replacement from Pu, and from each, pick one item i with\nprobability proportional to d\u22121i .\n5: {Exploitation}: Recommend the R2(u) highest-valued items from N (u)\u2229N explI . 6: Update N explI by adding the R1(u) items explored by u. 7: end for\nTHEOREM 2. Given graph G, reward-function V (u, i) and uniformly random user-arrivals, the IDExp algorithm (Algorithm 2) for recommending r items guarantees:\n\u03b3 IDExp(G,r)\u2265min {\nr\n8eZmax(G) ,\n1\n2e\n} .\nRemarks:\n\u2022 Compared to Theorem 1, the above guarantee is weaker by a factor of d \u2217(G)\nZmax(G) (ignoring constants).\nIn the two extreme cases we considered before (complete bipartite graph, and disjoint item-sets), it is easy to check Zmax(G) has the same value as d\u2217(G); thus we again have that no algorithm can be orderwise uniformly better over all graphs. Further, in case of bi-regular graphs, the two quantities are almost equal (in particular, Zmax(G) = nI nU , and d\u2217(G) = d nI nU e).\n\u2022 In general, we have d\u2217(G) \u2264 bZmax(G)c. However there is no O(1)-bound in the other direction,\nand one can construct graphs where Zmax(G)/d\u2217(G) is \u2126(nI). This shows that the IDEXP algorithm performs best when the graph is close to regular, but may deteriorate with increasing non-regularity. \u2022 The fact that Zmax(G) is large due to non-regularity can result in the above bound being weak; how-\never, in real-world social network graphs, the performance of the IDEXP algorithm is often much better than the bound. This is because the above bound is for the worst-case node; in real-world graphs, removing a few nodes often improves Zmax(G) by a large amount. \u2022 It is somewhat non-intuitive to explore items with a probability inversely proportional to its degree \u2013\nfor example, if an item has the same value for all neighboring users (i.e., V (u, i) = V (i)), then not exploring a high-degree item with a high value may seem costly. However, in Section 4, we show that inverse-degree randomization is the only competitive approach in the following strong sense: any algorithm that explores item i with a probability proportional to d\u22121\u00b1 i has 0 competitive-ratio. Proof Outline: To see the intuition behind the inverse-degree sampling scheme, note that for any item with degree d, each of its neighboring users try to explore it with probability d\u22121 \u2013 thus in a sense, every item is explored with near-constant probability. From the point of view of any user u, its top item(s) are explored with some constant probability \u2013 further, due to random dynamics, there is a constant probability that the user arrives after the items are explored. We provide the complete proof in Section 7.2."}, {"heading": "3. The Infinite-Horizon Setting", "text": ""}, {"heading": "3.1. System Model", "text": "We now consider a setting where the system evolves in time with user/item arrivals and departures \u2013 this is a more natural model for social-network news feeds, and some content-curation sites like Digg/Reddit, where content is posted in a more continuous manner. Access graph: We are given an underlying access-graphG(NU ,NC ,E) between usersNU and item-classes NC (with |NU | = nU , |NC | = nC). For example, for the problem of generating news-feeds in social networks, the access to user-generated content is restricted by the \u2018follower\u2019 graph \u2013 a user can only see updates from people whom she follows. On the other hand, a content-curation website can be viewed as a graph between users and article-topics, with edges incident on a user encoding the personalized set of topics that she is interested in. Each user visits the website periodically to view articles from her topics of interest; correspondingly, for each topic, new articles arrive from time to time. User/Item Dynamics: We assume the system evolves in continuous time. Each user generates a series of visit events according to a Poisson process of rate 1. Equivalently, by the aggregation property of Poisson processes, all user-visits together constitute a marked Poisson process of rate nU \u2013 each visit is denoted by a unique index s \u2208 N+ (i.e., a running count of user visits), and has an associated random mark U(s) corresponding to the identity of the visiting user. A user in each visit is presented r items, chosen from\navailable items in her neighboring item-classes; she accrues rewards from these, provides feedback and leaves instantaneously.\nIn parallel, we have an infinite stream of items, where for each item-class, individual items arrive according to independent Poisson(1) processes. As with the users, the item-streams together constitute a marked Poisson process of rate nC \u2013 each arriving item is denoted by a unique index i \u2208 N+, and has three associated parameters: item-class C(i), reward-function V (i), arrival time T (i). Also each item expires after a fixed lifetime \u03c4 , which we assume is the same for all items. Reward Function: Each item has an arbitrary value \u2013 this can depend on the item class, but not on the specific sample path. One way to visualize this is as follows \u2013 we allow an adversary to pick a sequence of item-values for each item class \u2013 however the adversary must pick this sequence before the user/item arrivals and item recommendation process, and not dynamically as the system evolves (i.e., the adversary is unaware of the sample path of the system when picking the value sequences). Formally, each item-class c has an associated (infinite) sequence of positive values Vc, and the kth item of class c arriving in the system has associated value Vc(k). Note that in any given sample-path \u03c9, the kth item of class cwill have associated index I\u03c9 \u2265 k, depending on when it arrives in the system \u2013 by our previous notation, we have C(I\u03c9) = c and V (I\u03c9) = Vc(k).\nWe say an item-sequence I = {C(i), V (i), T (i), \u03c4(i)}i\u2208N+ is valid if it satisfies the above assumptions. At any time t, we define N explI (t) to be the set of pre-explored items (i.e., presented during at least one prior visit) currently in the system \u2013 for brevity, we suppress the dependance on t. As before, we assume the highest-valued pre-explored items can be identified by the algorithm (see Section 5 for approximate identifiablity from a finite number of user-views).\nObjective: Given valid item-sequence I, and a visit s, we define R\u2217r(s) as the optimal offline reward for visit s; note that this is a random variable which depends on which user U(s) corresponds to visit s, which\nitems are in the system, etc. Similarly, for a given algorithm A, we can define RAr (s). Combining these, we can define the competitive-ratio of algorithm A (given graph G, r recommendations) as:\n\u03b3A(G,r) = inf Valid item-sequence I inf s\u2208N+\nE [ RAr (s)\nR\u2217r(s)\n] ."}, {"heading": "3.2. Uniform Latest-Item Exploration", "text": "Given our results for the finite-population setting, a first idea for the infinite-horizon setting would be to\napply the IDExp algorithm on the set of available items. This however does not guarantee a competitive\nratio, as the number of unexplored items only decreases by at most r after each user-visit. The main idea\nin designing an exploration policy in this setting is that exploration should be biased towards more recent\nitems, while discarding older unexplored items. Let Ts and Ti be the arrival times of visit s and item i respectively. Then, for item i \u2208 N, we can define its first neighbor S1(i) as the first visit after Ti by a neighboring user (i.e., S1(i) = min{s|Ts \u2265 Ti, i \u2208N (s)}). Correspondingly, for visit s, we can define the set of latest-items L(s) as the set of available items, for which it is the first neighbor (i.e., L(s) = {i|s = S1(i), Ts <Ti + \u03c4}). Now we have the Uniform Latest-Item Exploration Algorithm (ULExp):\nAlgorithm 3 ULExp: Uniform Latest-Item Exploration 1: for session s\u2208N+ do\n2: Determine L(s), the set of latest items. 3: Choose R1(s) \u223c Binomial(r, 12) slots for exploration, and the rest R2(s) = r \u2212 R1(s) slots for\nexploitation.\n4: {Exploration} Pick R1 items from L(s) uniformly at random, and without replacement. 5: {Exploitation}: Recommend the R2(s) highest-valued neighboring items in N explI . 6: Update N explI by adding the R1(s) items explored by u. 7: end for 8: Remove items from N explI when they leave the system.\nRecall in Section 2, we defined Zmax(G) := maxu\u2208NU Z(u), where Z(u) := \u2211 i\u2208N (u) d \u22121 i . We now\nhave the following theorem for the competitive-ratio of the ULExp algorithm:\nTHEOREM 3. Given graph G, with both users and items arriving according to independent Poisson(1)\nprocesses, using the ULExp Algorithm, we have:\n\u03b3ULExp(G,r)\u2265 r 4(5Zmax + 2) .\nRemarks:\n\u2022 We do not need to assume that all the Poisson processes have the same rate \u2013 in fact, in Section 7.3, we\nprove the result for general {\u03bbu, \u03bbc}. Note that we do not need to know these rates for the algorithm.\n\u2022 On the practical side, recommendation via showing the latest items, as done on Twitter, can be thought\nto be a form of uniform latest exploration \u2013 our result suggests that for good recommendation, this\nshould be equally mixed with items which are popular (\u2018trending\u2019).\nProof Outline: Using the reversibility of the Poisson processes, we can show that for any visit s, the\naverage number of items in its latest-item set is bounded by Zmax \u2013 to see this, note that for visit s and any neighboring item-class c, the number of items in L(s) of class c is one less than a Geometric (\ndi di+1 ) random variable. This suggests that uniform latest-item exploration ensures that any item is explored with\nhigh enough probability.\nThe technical difficulty arises in the fact that for any visit, we want such a guarantee for the corresponding\nhighest-valued item for that visit \u2013 this item is selected based on the sample-path and the sequence of\nrewards, which is arbitrary. Note that the rewards can affect which item is the highest-valued \u2013 for example,\nit the sequence of item rewards is strictly decreasing, then the most valuable item is the oldest available\nitem. Thus we can not argue that the probability of the highest-valued item being explored is the same as\nthat of a typical item. We present a more refined counting argument that accounts for this conditioning \u2013 the\ncomplete proof is given in Section 7."}, {"heading": "4. Converse Results", "text": "We now present some converse results, which put in perspective the performance of our algorithms. We\npresent two types of results \u2013 upper bounds on the competitive-ratio over all possible online algorithms,\nand negative results (0 competitive-ratio) for specific algorithms. All results in this sections are for the\nfinite-population setting.\nUpper Bounds: For our upper bounds, we consider a complete bipartite access-graph, and binary rewards \u2013 wherein each item has a value V (i) \u2208 {0,1}. In this setting, we show that no algorithm can achieve a competitive-ratio better than nU/2nI . Note that for these graphs Zmax(G) = nU nI , and d\u2217(G) = dnU nI e.\nTHEOREM 4. Given any > 0 and nU , there exists a sufficiently large nI such that for a nU \u00d7nI complete bipartite access-graph, no algorithm can achieve \u03b3(G,1)> nU 2nI + .\nMoreover, for r item recommendations, no algorithm can achieve better than linear scaling in r:\nTHEOREM 5. Given any > 0, nU and r, there exists a sufficiently large nI such that for a nU \u00d7 nI complete bipartite access-graph, no algorithm which is allowed to show at most r recommendations per user can achieve \u03b3(G,r)> rnU 2nI + .\nTogether, these results show that our competitive-ratio bounds are the best possible up to constant factors. Negative Results: In the IDExp algorithm, items were chosen for exploration with a probability inversely proportional to their degree. This choice is somewhat non-intuitive \u2013 a more natural choice would seem to be to bias towards higher degree items (as they can reward more users in the universal rewards setting). However, it turns out that a sampling distribution which is proportional to any other polynomial in the degree in fact has vanishing competitive-ratio.\nTHEOREM 6. Given any algorithm A which choses item i for exploration with probability proportional to d\u22121\u00b1 i for any > 0, then \u2203 a sequence of graphs Gn (with Zmax(Gn) = 2), and a corresponding collection of item values V \u2208RnI+ , such that for r= 1,the competitive-ratio \u03b3A(Gn,1) goes to 0.\nThe formal construction of Gn and proof is presented in Appendix A. We note that for the graph family Gn used the above result, IDExp achieves a competitive-ratio of r 8e .\nFinally, in all our algorithms, we split the recommendation slots between explore and exploit recommendations uniformly at random. It is not clear if we need this randomization \u2013 however we can show that some simple intuitive schemes for deciding between explore and exploit are non-competitive.\nTHEOREM 7. Suppose we are given a recommendation algorithm A where the exploit/explore decision based on one of the following rules:\n\u2022 Exploit-when-possible: exploit whenever there is a non-0 valued available item, else explore. \u2022 Exploit-above-threshold-t: Exploit when the best available item gives a reward > t for some fixed\nthreshold t > 0.\nThen, independent of the choice of exploration policy, \u2203 a sequence of graphsGn (with Zmax(Gn) = 2), and corresponding collection of item values V \u2208RnI+ , such that for r= 1, the competitive-ratio \u03b3A(Gn,1) goes to 0."}, {"heading": "4.1. Proof Outlines of Converse Results:", "text": "The main technique we use to obtain converse results is Yao\u2019s minimax principle (see Motwani and Raghavan (1997)): essentially it states that the competitive-ratio of the optimal deterministic algorithm for a given randomized input (where the measure over inputs is known to the algorithm) is an upper bound for the competitive-ratio.\nIn case of Theorems 4, 5, the underlying graph is the complete bipartite graph on nU \u00d7nI nodes: now for an i chosen uniformly at random fromNI , we set V (u, i) = 1\u2200u\u2208N (i), and V (u, i) = 0 for all other (u, i) pairs. Note that the above choice implies that the reward-function V is a binary uniform reward-function. Theorem 5 is more involved \u2013 essentially we show that the competitive-ratio is bounded above by that of an easier \u2018search\u2019 problem, where an adversary chooses an item-node, and the users sample r nodes each to try and discover this chosen node.\nFinally, the negative results in Theorem 6 require constructing a sequence of graphs Gn with associated reward-functions Vn for which Zmax(G) is constant, but the competitive-ratio for non-inverse-degree sampling rules goes to 0. For the full proofs, refer to Appendix A."}, {"heading": "5. Discussion and Extensions", "text": ""}, {"heading": "5.1. Inferring Item Values from Multiple Ratings", "text": "First, we have assumed that the algorithm knows the value of an item once it has been explored by at least one user. However, as we mentioned in the Section 1, a more general condition would be that once an item is viewed by at least f users, its value is known to the algorithm within a multiplicative factor of (1\u00b1\u03b4(f)), for some \u03b4(f) \u2208 [0,0.5). This generalizes the case considered so far, which corresponds to f = 1 and \u03b4(1) = 0 \u2013 we now show how we can modify our algorithms to handle this more general setting.\nWe now define an item to be pre-explored if it has been presented to at least f users \u2013 the set of preexplored items is still denoted N explI . To provide competitive-ratio guarantees for this setting, we modify the algorithms as follows:\n\u2022 For every user u (or visit s in the infinite-horizon setting), the algorithm chooses R1(u) \u223c Binomial(r, f\nf+1 ) slots for exploration, and the rest for exploitation.\n\u2022 The exploration policies are modified in a natural way so as to allow for items to get explored up to f times (see below). \u2022 For the exploitation, the algorithm still picks the top items from the pre-explored items, based on the noisy estimates of item-value. Now suppose for a user u, its top item i\u22171(u), has been explored by at least f users before u arrives. Further, suppose the user decides to exploit at least one item (i.e., R2(u) \u2265 1) then either item i\u22171(u) is chosen, or another item i\u2032 6= i\u22171(u), but such that V (i\u2032)\u2265 (1\u2212 2\u03b4(f))V \u2217(i\u2217k(u)). The last statement follows since only then i\u2032 has higher value than i\u22171(u). Thus the competitive-ratio is reduced at most by a factor (1\u2212 2\u03b4(f)).\nWe now briefly discuss how the exploration step can be done in the infinite-horizon setting for the ULExp algorithm (Algorithm 3) \u2013 the arguments for the finite-population setting are similar. Recall that in the algorithm, during each visit s, the visiting-user was presented R1(s) items chosen uniformly from the set of latest-items \u2013 those for which it was the first neighbor. We now modify it as follows: each item has a counter, initialized to 0, which is incremented whenever a neighboring user makes a visit (note: the item may or may not be presented during the visit). Once the counter reaches f , the item is declared to be preexplored if it had been presented to all its f visiting neighbors, else it is discarded. Finally, during each visit s, given R1(s) slots for exploration, the algorithm first chooses R1(s) numbers {l1, l2, . . . , lR1(s)} from the set {0,1, . . . , f\u22121} uniformly at random with replacement, and then, for each lj , chooses an item uniformly at random from amongst neighboring items whose counter equals lj . It is easy to see that for f = 1, this is precisely uniform latest-item exploration. We call this the ULExp-f algorithm, and we have the following theorem:\nTHEOREM 8. In the infinite-horizon setting, given graph G, users and items arriving according to Poisson(1) processes, and given that for any item, its value is known to within (1\u00b1\u03b4(f)) after it is explored f times; then the ULExp-f algorithm guarantees:\n\u03b3ULExp-f(G,r)\u2265 1 (f + 1)f+1\n\u00b7 (\nr\n5Zmax + 2\n)f \u00b7 (1\u2212 2\u03b4(f))\nNote that substituting f = 1 and \u03b4(f) = 0 gives us back the result from Theorem 1. An analogous theorem\nholds for the finite population case; we skip details for brevity.\nProof Outline: By expanding the latest-item set to items which have seen < f neighboring-user visits, we show that the (expected) size of the latest item set (for the top item corresponding to visit s) can now be bounded by f \u00b7 (5Zmax + 2) (where (5Zmax + 2) is the bound on the latest-item set for f = 1 which we derive in the proof of Theorem 3). Thus for any visit, its item is explored by all of the first f neighboring\nusers with probability at least (\nfr f+1 \u00b7 1 f(5Zmax+2)\n)f . Furthermore, the user corresponding to s exploits with\nprobability 1 f+1 , and if her top item I\u22171 (s) is pre-explored, it is either presented, or substituted by another pre-explored item with a true value greater than (1\u2212 2\u03b4(f))V (I\u22171 (s)). Combining these, we get the result. The formal proof is given in Appendix B."}, {"heading": "5.2. More General Reward Models", "text": "In the paper till now, we have mostly focused on the universal rewards scenario, wherein the reward given by item i to all neighboring user is V (i). This model is studied for ease of exposition and notation; however our proofs allow for more general reward-functions: Personalization: Item i has intrinsic value V (i), but gives neighboring user u a reward of V (u, i) = fui(V (i)), where fui(\u00b7) are (non-negative, invertible) functions known to the algorithm. This can capture different preferences a user may have vis-a\u0300-vis different items. Collaborative Ranking: In several setting, the reward earned due to recommending an item may not be possible to quantify \u2013 however, the algorithm can still succeed if it is able to infer a ranking of the explored items from user-feedback. This is reminiscent of the Secretary problem (Babaioff et al. (2008)), and also allows for techniques such as in (Jagabathula and Shah (2008)). Probabilistic Predictability: In many cases, we may be only able to identify the top item for a user with some probability Ppred; for example, in collaborative filtering algorithms such as matrix completion (Keshavan et al. (2010)). In this case, all our competitive-ratio bounds get scaled by Ppred."}, {"heading": "6. Related Work", "text": "Static Recommendation: Learning from feedback in large-scale settings is far from being a new idea; however most of the work in this space does not capture user-item dynamics and the explore-exploit tradeoff. Instead, the dominant view is one of taking the user feedback data as a static given (Keshavan et al. (2010),\nJagabathula and Shah (2008)); in contrast, our model captures the fact that there is a selection to be made of what user data can be collected, and this selection affects the performance. Bandit Algorithms: Bandit models refer to settings where choosing an action (or arm) from a set of actions both yields a reward as well as some feedback about the system, which then affects future control decisions. There are two broad classes of problems that go under the title of bandit problems \u2013 finite-time bandits and infinite-horizon (or Markovian) bandits.\nMarkovian bandits (Eg. Gittins (1979)) focus on settings where each arm has an underlying state, and playing an arm results in a reward that depends on the state, as well as a possible transition to another state (the other arms remaining unaffected). Both rewards and state transition matrices are assumed to be known, and the aim is to maximize the discounted sum reward. Our work differs in that we want to avoid assuming an underlying stochastic model for item-values.\nFinite-time bandit problems were originally proposed by Lai and Robbins (1985) \u2013 subsequent works have greatly generalized the setting by considering different reward-generation processes. Algorithms for these settings control the additive loss (or regret) w.r.t. the best policy by bounding the number of times a suboptimal action is chosen. These bounds are in terms of some increasing function of the number of users (plays); however, in content-rich settings where the number of content pieces is of the same order as the number of content-views, it is infeasible that all arms get shown more than a constant number of times. Thus using existing bandit algorithms for our problem leads to a 0 competitive ratio. For example, consider a setting with n users, n items and a complete bipartite access graph. Suppose one item has a value of 1 and the rest 0, and each user is presented with 1 item (i.e., r = 1) \u2013 then a bandit algorithm will sample all items at-least once (in particular, the standard UCB algorithm of Auer et al. (2002) will sample each arm once just during initialization), thereby getting a competitive-ratio of \u03b3 =O(1/n)\u2192 0. On the other hand, the algorithms we present in this work achieve a competitive-ratio of 1 8 .\nA notion of an access graph is incorporated in some bandit models such as the Contextual Bandits (Dud\u0131\u0301k et al. (2011)) or Sleeping Bandits (Kleinberg et al. (2010)) models, the graph and user dynamics are assumed arbitrary. The graph is not used to inform the algorithm design except in that it constrains what items can be shown \u2013 essentially this corresponds to having arbitrary access-constraints, which leads to the results being pessimistic. In our setup, on the other hand, imposing natural stochastic assumptions on user/item dynamics leads to much stronger competitive-ratio guarantees. Online Matching and its Variants: Although having the appearance of a bandit problem, our setting is in fact much closer in spirit to certain online optimization problems on graphs. Online auction design problems (Mehta et al. (2005)) incorporate the fact that an item can be displayed to multiple users, constrained by an underlying graph. However, in such problems the node weights (bids) are known, which often allows greedy algorithms to be constant-factor competitive. Related problems include the generalized secretary problem (Babaioff et al. (2008)) and online transversal-matroid selection (Dimitrov and Plaxton (2008)); both are\nbased on a bipartite graph between a \u2018static set\u2019 and an \u2018online set of nodes\u2019, where node weights (of online\nnodes) are automatically revealed upon arrival. In contrast, in our problem, the reward-function is unknown,\nand becomes known only via exploration \u2013 this may affect many future users in a non-trivial manner."}, {"heading": "7. Proofs of Competitive-Ratio Guarantees", "text": "We now give the proofs for our results. First, we briefly recall some definitions from before. In the finitepopulation model, we are given graph G(NU ,NI ,E) users NU and items NI (|NU |= nU , |NI |= nI). For a user u \u2208NU , we define N (u) , {i \u2208NI |(u, i) \u2208 E}, du = |N (u)| (similarly for items). In the infinitehorizon model, the definitions essentially remain the same except that instead of the set of items we have the set of item-classes NC (where |NC |= nC). Further, we can define the neighborhood of a user-visit s as items of neighboring classes currently in the system (and similarly for items).\nIn the finite-population model, when a user arrives, the recommender algorithm A presents r items {iA1 (u) . . . iAr (u)} \u2286N (u) \u2013 given reward-function V , such that the user u earns a reward of V (u, i) from item i (or V (i) if the reward is the same for all users; see Section 5) , the total reward earned by u is\u2211r k=1 V (u, i A k (u)). Further, for a given user u, we define the ordering {i\u2217k(u)}duk=1 of its neighboring items sorted in decreasing order of their values. Then the algorithm\u2019s competitive-ratio (for graph G, and for r-recommendations per user) is \u03b3A(G,r) = infV infu\u2208NU E[RAr (u)] R\u2217r(u) . Note that the expectation here is over randomness in user arrival-pattern and the algorithm A; note also that R\u2217r(u) is not random given G and reward-function V . For the infinite-horizon setting, again the definitions are similar, but instead of users,\nwe consider visits \u2013 we will then have \u03b3A(G,r) = infV infs\u2208N+ E [ RAr (s) R\u2217r(s) ] \u2013 here even the optimal reward is a random variable.\nFinally, we use R+ for the sets of non-negative reals (x\u2265 0), and N+ for natural numbers (x\u2208 {1,2, . . .}). For any n\u2208N+, we define [n] = {1,2, . . . , n}. We use the shorthand a\u2228 b= max{a, b}, a\u2227 b= min{a, b}. We use 1E to denote an indicator random-variable for an event E, taking value 1 when E occurs, else returning 0 \u2013 similarly we use 1AE to be an indicator r.v. for event E under algorithm A."}, {"heading": "7.1. A Preliminary Lemma", "text": "We first state and prove a lemma which we use in all our proofs \u2013 it encapsulates the idea that in order\nto be competitive, it is sufficient to ensure that for every user, with a near-equal probability, the algorithm\nshould recommend its corresponding highest-valued item. For ease of exposition, we state the lemma for the\nfinite-population setting. For the infinite-horizon setting, we can get an identical result with user u replaced\nwith user-visit s, and conditioned on the items currently in the system during visit s.\nGiven algorithm A and reward-function V , for any pair (u, i) where i \u2208N (u) we define 1Au\u2192i to be an\nindicator random variable that is 1 if user u is shown item i by algorithm A, and else 0. Then we have:\nLEMMA 1. Given a graphG and reward-function V , then for any algorithmA displaying r items, we have: E[RAr (u)]\u2265 (\ninf u\u2208NU min 1\u2264k\u2264r\nE [ 1Au\u2192i\u2217\nk (u)\n]) R\u2217r(u)\nProof. For non-negative rewards (i.e., V (i)\u2265 0\u2200 i) , we can bound the reward earned by user u under\nalgorithm A as:\nE[RAr (u)]\u2265E \u2211 k\u2208[r] V (i\u2217k(u))1 A u\u2192i\u2217 k (u) = \u2211 k\u2208[r] V (i\u2217k(u))E [ 1Au\u2192i\u2217 k (u) ] \u2265 (\nmin k\u2208[r]\nE [ 1Au\u2192i\u2217\nk (u) ])\u2211 k\u2208[r] V (i\u2217k(u)) = ( min k\u2208[r] E [ 1Au\u2192i\u2217 k (u) ]) R\u2217r(u)\nTaking infimum over u\u2208NU (or s\u2208N+ in the infinite-horizon case), we get the result."}, {"heading": "7.2. Performance Analysis: Finite-Population Setting", "text": "Before presenting our proofs, we recall that our algorithms share the following structure:\n\u2022 Divide the r recommendations uniformly at random between explore and exploit recommendations\n(i.e., the number of exploration slots R1 \u223cBinomial(r, 12), rest are for exploitation).\n\u2022 For the exploitation step, the algorithm leverages our assumption that for any user, the highest-valued\npre-explored items can always be identified.\n\u2022 For the exploration step, we proposed 3 different exploration policies (in Algorithms 1,2 and 3).\nThese are designed to leverage the graph topology and randomness in user-arrivals to ensure balanced exploration: for any neighboring user-item pair, we can lower-bound the probability that the item is explored before the user arrives to the system.\nWe also need one additional definition: in the finite-population setting, we define a user-arrival pattern to be a permutation \u03c0 \u2208 SnU (where SnU is the set of permutations of users NU ) \u2013 we assume that \u03c0 is chosen uniformly at random. Performance Analysis for BPExp Algorithm:\nProof of Theorem 1. Suppose we are given a reward-function V , and a user-arrival pattern \u03c0 \u2208 SnU chosen uniformly at random. For any user u, recall R1(u) is the number of items explored by u; further, we assume that exploration occurs according to chosen balanced item-partition M (note that M is not random \u2013 c.f. Algorithm 1). Now let pui denote the probability that u explores i \u2013 for this to happen, we need that (u, i) \u2208M (i.e., the edge (u, i) is present in the balanced item-partition which we choose), and further, that i is one of the R1(u) items explored by u. From the definition of the BPExp algorithm and the makespan d\u2217(G), conditioned on the events (u, i) \u2208M and R1(u) = k, a standard picking-without-\nreplacement argument gives that u explores i with probability at least (\nk d\u2217(G) \u2227 1\n) . Thus, we have that for\nany neighboring user-item pair (u, i), pui \u2265 [ R1(u) d\u2217(G) \u2227 1 ] \u00b71{(u,i)\u2208M} \u2013 note this is a r.v., depending onR1(u).\nFor any item i \u2208NI , let uM(i) be the (unique) user connected to it in the item-partition M . Recall we define 1BPExpu\u2192i to be the indicator that user u is shown (neighboring) item i by algorithm BPExp. Then for any user u\u2208NU , and any item i\u2217k(u), k \u2208 [r] (i.e., one of the top r items for u), we have that 1 BPExp u\u2192i\u2217\nk (u) = 1 iff:\ni. u= uM(i\u2217k(u)) AND u explores i \u2217 k(u),\nOR\nii. uM(i\u2217k(u)) = u \u2032 6= u AND u\u2032 arrives before u in arrival-pattern \u03c0 AND u\u2032 explores i\u2217k(u).\nThe first condition captures the case where u explores i\u2217k(u) (via the R1(u) slots used for exploration). The second condition captures the case where i\u22171(u) is explored by the time u arrives, and hence u can exploit it via its R2(u) exploration slots. Note that the above options are mutually exclusive; which of the conditions holds is uniquely determined given values V and the item-partition M . Now under condition i, using our\nprevious characterization of pui and Jensen\u2019s inequality, we have that:\nE [ 1\nBPExp u\u2192i\u2217\nk (u)\n] \u2265E [ E [ k d\u2217(G) \u2227 1 \u2223\u2223\u2223\u2223R1(u) = k]]\u2265 r2d\u2217(G) \u2227 1\nUnder condition ii, by a similar calculation, we have that the probability of u\u2032 exploring i\u2217k(u) is \u2265( r 2d\u2217(G) \u2227 1 ) . Further, since \u03c0 is chosen uniformly at random from SnU , we have that u \u2032 arrives before u with probability 1/2 (more generally, for any two users u, v \u2208NU , u arrives before v with probability 1/2). Note that the expected reward under the second condition is lower than that in the first case \u2013 since the two\nare mutually exclusive, a lower bound on the performance under the second condition translates to a lower\nbound for the BPExp algorithm.\nFinally, to bound the performance of BPExp under condition ii, we observe that it can be stochastically under-dominated via the following modified algorithm: First, note that choosing R1 \u223c Binomial(r, 12) is equivalent to sequentially allocating slots {1,2, . . . , r} to either exploration with probability 1/2, else exploitation. Now when user u arrives, suppose we allocate R2(u) slots {k1, k2, . . . , kR2} \u2286 [r] for exploitation \u2013 then instead of showing the top R2(u) pre-explored items, we show items {i\u2217k1(u), i \u2217 k2 (u), . . . , i\u2217kR2 (u)} if they have been pre-explored, and fill any remaining exploitation slot with the top remaining pre-explored items. A coupling argument shows that this modified algorithm is stochas-\ntically dominated by BPExp (since it may recommend a less valuable item). However, under the modified policy, it is easy to see that \u2200k \u2208 [r], whenever item i\u2217k(u) is in the set of pre-explored items, then user u exploits it with probability 1/2 \u2013 thus, under condition ii using the modified policy, we have:\nE [ 1u\u2192i\u2217\nk (u)\n] =E [ 1{u\u2032 arrives before u}1{u\u2032 explores i\u2217\nk (u)}1{u exploits i\u2217 k (u)} ] \u2265 1 2 \u00b7 1 2 \u00b7 ( r 2d\u2217(G) \u2227 1 ) = ( r 8d\u2217(G) \u2227 1 4\n) Finally, using Lemma 1, and taking infimum over users u\u2208NU , we get the result,\nPerformance Analysis for IDExp Algorithm: Next, we prove Theorem 2; refer Section 2.3 for details and theorem statement. We first need a lemma characterizing Greedy Neighborhood Partitioning (Definition 2):\nLEMMA 2. If {Pu}u\u2208NU is generated by independently applying Greedy Neighborhood Partitioning for each user u\u2208NU , then Zmax(G,r,{P}),maxu\u2208NU maxk\u2208[r] \u2211 i\u2208Pku d\u22121i obeys:\nZmax(G,r,{P})\u2264 2Zmax(G)\nr .\nProof. First, note that by definition, \u2211\ni\u2208N (u) d \u22121 i is bounded by Zmax(G) for all u. Further, d \u22121 i \u2264 1 for\nall i. Together, this implies that the optimal balanced neighborhood partition {P\u0303 ku }k\u2208[r] for any user u has the property that maxk\u2208[r] \u2211\ni\u2208P\u0303ku d\u22121i \u2264 Zmax(G) r . The above claim now follows from the existing result of\nGraham (1966), which shows that greedy set partitioning has an approximation ratio of 1 2 .\nUsing this bound, we can now prove the stated result. Proof of Theorem 2. Consider any user u. First, from Lemma 1, we get:\nE[RIDExpr (u)]\u2265 (\nmin k\u2208[r]\nE [ 1\nIDExp u\u2192i\u2217\nk (u)\n]) \u00b7R\u2217r(u)\nAs before, we drop the superscript indicating that relevant quantities are conditional on using the IDExp algorithm. We now show that the algorithm results in a uniform lower bound over all users u \u2208NU , and\n\u2200k \u2208 [r], of E [ 1u\u2192i\u2217\nk (u)\n] \u2265 1\n4e\n( r 2Zmax(G) \u2227 1 ) ; substituting this in the above equation, we get our result.\nThe difficulty in analyzing IDExp is that the item-explorations are no longer independent, but rather, depend on the decisions made by all previous users. However, we can stochastically dominate the IDExp algorithm by a fictitious algorithm wherein for each user, all its neighboring items are eligible for exploration, irrespective of whether they have been explored before. Clearly this can only make the performance worse, as under our assumption that an item\u2019s value is known once explored. Further, we define Z = 2Zmax(G)\nr \u2228 1. Now, for any user u and neighboring item i, we claim that the probability that u explores i,\ngiven by pui, obeys:\npui \u2265 d\u22121i 2Z\nTo see this, first note that from Lemma 2, we have that for every user u, and every neighborhood partition P ku , k \u2208 [r], we have that \u2211 i\u2208Pku d\u22121i is bounded by Z. Now for user u, the number of explore slots is\nR1(u)\u223cBinomial(r,1/2) \u2013 thus pui \u2265E [ R1(u)\nrZ\n] =\nd\u22121i 2Z .\nNow given reward-function V and user arrival-pattern \u03c0 chosen uniformly at random from SnU , consider any user u \u2208NU with associated highest-valued r items i\u2217k(u), k \u2208 [r]. Consider item i\u2217k(u) \u2013 let At be the event that there are t \u2208 {0,1, . . . , di \u2212 1} neighbors of i\u2217k(u) who arrive in the system before u in arrivalpattern \u03c0; we denote these users as {ak}tk=1. Conditioned on At, under the IDExp algorithm, 1u\u2192i\u2217k(u) = 1 iff i\u2217k(u) is explored by u OR by explored by one of the t neighbors of i \u2217 k(u) who arrived before u, and\nexploited by u. As in the proof of Theorem 1, we have that the probability that i\u2217k(u) is exploited by u when it is pre-explored is at least 1 2 . Thus we have (using i as shorthand for i\u2217k(u)):\nE [ 1u\u2192i\u2217\nk (u) \u2223\u2223\u2223At]\u2265 1 2 [ 1\u2212 (1\u2212 pui)\u03a0tk=1(1\u2212 paki) ] \u2265 1\n2 [pa1i + (1\u2212 pa1i)pa2i + (1\u2212 pa1i)(1\u2212 pa2i) . . . (1\u2212 pati)pui] .\nNote that d \u22121 i 2Z \u2264 pui \u2264 1di . Now we have:\nE [ 1u\u2192i\u2217\nk (u) \u2223\u2223\u2223At]\u2265 1 2 t\u2211 k=0 ( 1\u2212 1 di )k d\u22121i 2Z = 1 2Z ( 1\u2212 ( 1\u2212 1 di )t+1)\nSince \u03c0 is drawn uniformly at random from SnU , we have that P[At] = 1 di . Thus:\nE[1u\u2192i1(u)]\u2265 1 di \u00b7 1 4Z di\u2211 t=1\n( 1\u2212 ( 1\u2212 1\ndi\n)t)\n= 1\n4Z\n( 1\ndi +\n( 1\u2212 1\ndi\n)di+1)\n\u2265 1 4Z\n( 1\ndi +\n1 e\n( 1\u2212 1\ndi\n)2) ,\nsince ( 1\u2212 1\nd )d \u2265 1 e \u2212 1 ed \u2200d\u2208N+. Thus we have:\nE[1u\u2192i\u22171(u)]\u2265 1\n4Z\n( 1\ne +\n1 di\n( 1\u2212 2\ne\n) + 1\ned2i\n) \u2265 1\n4eZ ,\nwhere Z = 2Zmax(G) r \u2228 1. This completes the proof."}, {"heading": "7.3. Performance Analysis: Infinite-Horizon Setting", "text": "Finally, we turn to the infinite-horizon setting. Recall that we now have a graph between users NU and item-classes NC , with user-visits x \u2208N+ and items i \u2208N+. Each item i has an item-class C(i) (according to the underlying independent Poisson processes), a lifetime \u03c4 (same for all items) and a reward-function V (i). More specifically, for each item-class c, we define Vc to be an arbitrary, infinite sequence of rewardfunctions, such that the kth item of class c has the kth reward function in the sequence (i.e., Vc(k)).\nWe define S1(i) to be the first visit by a neighboring user u \u2208N (C(i)) after item i arrives to the system (and before it expires). Complementary to this, for visit s, we defined the latest-items set L(s). Finally the Uniform Latest-Item Exploration strategy is based on randomly picking R1(s)\u223cBinomial(r,1/2) items from L(s) without replacement, for exploration during visit s.\nThe main idea behind latest-item exploration is that it can be shown that for any typical item, the expected size of the latest-items set is bounded by 2Zmax(G) + 1. Coupled with Jensen\u2019s inequality, this result suggests that the probability that any typical item is explored is greater than 1 2Zmax(G)+1 \u2013 however this is not\nsufficient to obtain our result because for a given user, we are interested in the latest-item set as seen by its corresponding highest-valued items. This however is determined by the reward-function V , and it is not clear how the dependence can be quantified. This is the main technical challenge in the proof.\nThe main idea of our proof is as follows \u2013 for any visit s arriving at time Ts, we consider the arrivals in the time-interval [Ts\u2212 \u03c4,Ts] \u2013 since each item has a lifetime of \u03c4 , the items in this interval are the only ones which matter. We argue that the statistics of these arrivals are unaffected by the reward-function V , and further, using the ULExp scheme, we can control the probability with which a particular item is explored. For this, we first require the following combinatorial lemma:\nLEMMA 3. Given a uniform permutation of R red and B blue balls, let Ncons(i), i\u2208 [R] be the number of consecutive red balls (i.e., bounded on either side by either a blue ball or a boundary) containing the ith red ball. Then we have:\nmax i\nE[Ncons(i)]\u2264 4R\nB+ 1 + 2\nRemarks: For ease of notation, we define N(R,B) = maxiE[Ncons(i)|R red,B blue balls]. N(R,B) is clearly greater than the expected number of consecutive balls (which, by symmetry is R/(B+ 1)); a more subtle fact is thatN(R,B) is greater than the expected number of consecutive red balls as seen by a random red ball (unlike, for example, the PASTA property for queueing processes). This is due to the presence of boundary conditions \u2013 for example, for B = 1, we can compute the expected consecutive sequence seen by a random ball to be (2R + 1)/3, while we show in the proof below that N(R,B) in this case is (3R + 1)/4. Crucially however, our bound on N(R,B) is much less than the expected value of the maximum number of consecutive balls \u2013 in the case whereB =R= n, it is known that the longest sequence is \u0398(logn/ log logn), while our bound on N(n,n) is 6.\nProof. First, note that when B \u2264 3, the bound given in the lemma evaluates to a value \u2265 R, which is a trivial upper bound on the length of a consecutive subsequence. Hence, we essentially need to prove it for B > 3 and general R \u2013 further, in this range, we can lower bound the RHS by 4(R+1) B+1 + 1, which is the bound we will prove. The proof outline is as follows \u2013 first we exactly evaluate the quantity N(R,1) (i.e., for the case B = 1) \u2013 subsequently we use an induction argument, wherein we bound N(R,B) by a function of N(R,B \u2212 1). We show that this function is increasing as long as B > 3, and then verify the inequality inductively.\nFirst we explicitly compute N(R,1). Here, it is easy to see that the index of the red ball which sees the longest expected consecutive sequence is i\u2217 = dR/2e \u2013 for any other index i, the number of consecutive red balls is either the same (if the lone blue ball falls on the same side of i and i\u2217) or less (if it falls in between). Now in case R is odd, we have:\nN(R,1) = 1\nR+ 1\n( 2. R+ 1\n2 + 2.\nR+ 3\n2 + . . .+ 2.R\n) = 3R+ 1\n4\nSimilarly, in case R is even, we have:\nN(R,1) = 1\nR+ 1\n( R\n2 + 2.\nR+ 2\n2 + . . .+ 2.R\n) \u2264 3R+ 1\n4\nThus, combining the two cases, we get N(R,1)\u2264 3R+1 4 .\nNow to obtain the bound on N(R,B) for larger values of B, we bootstrap the above result as follows: Suppose we define X(R,B \u2212 1, i) to be the length of the consecutive sequence as seen by the ith red ball after we drop B\u2212 1 blue balls uniformly at random. Then from the above argument, we have:\nE[X(R,B, i)|X(R,B\u2212 1, i) = k]\u2264 k+ 1 R+ 1 . 3k+ 1 4 +\n( 1\u2212 k+ 1\nR+ 1\n) .k\n= \u2212k2 + 4(R+ 1)k+ 1\n4(R+ 1) ,\nand taking expectations, via Jensen\u2019s inequality, we get:\nE[X(R,B, i)]\u2264 \u2212E[X(R,B\u2212 1, i)] 2 + 4(R+ 1)E[X(R,B\u2212 1, i)] + 1\n4(R+ 1) ,\nNow note that f(k) =\u2212k2 + 4(R+ 1)k+ 1 is increasing for k \u2264 2(R+ 1) \u2013 hence we can further upper bound the RHS by replacing E[X(R,B \u2212 1, i)] by N(R,B \u2212 1). Finally, since the resulting expression is independent of i, we can replace E[X(R,B, i)] with N(R,B). Rearranging the above inequality, we have:\nN(R,B)\u2264N(R,B\u2212 1) + 1\u2212N(R,B\u2212 1) 2\n4(R+ 1) ,\nFinally, suppose we have that N(R,B\u22121) satisfies the required bound. Again, since the RHS is increasing as long as N(R,B\u2212 1)\u2264 2R+ 2, thus for B > 3, we have:\nN(R,B)\u2264 4(R+ 1) B + 1\u2212 16(R+ 1) 2/B2 + 8(R+ 1)/B 4(R+ 1)\n\u2264 4(R+ 1) B + 1\u2212 4(R+ 1)/B+ 2 B \u2264 1 + 4(R+ 1)B\u2212 1 B2 \u2264 1 + 4(R+ 1) B+ 1\nThis completes the proof.\nProof of Theorem 3. From Lemma 1, we have that for any visit s: E [RULExpr (s)]\u2265E [ min k\u2208[r] E [ 1 ULExp s\u2192I\u2217 k (s) ] R\u2217r(s) ] Here the inner expectation is over the randomness in the algorithm, and the outer expectation is over randomness in the sample path. We henceforth suppress the superscript.\nTo complete the proof, for any user-visit s, and for all k \u2208 [r], we need to show that the corresponding kth-top item, denoted I\u2217k(s) satisfies E [ 1 ULExp s\u2192i\u2217\nk (s)\n] \u2265 r\n4(5Zmax(G)+2) . We will in fact prove a more general\nresult \u2013 suppose items of any item-class c arrive at rate \u03bbc, and each user u visits at rate \u03bbu. We now obtain that E [ 1s\u2192I\u2217\nk (s)\n] \u2265 r\n4(5Z+2) , where:\nZ = max u\u2208NU \u2211 c\u2208N (u) \u03bbc\u2211 u\u2032\u2208N (c) \u03bbu\u2032\nWhen \u03bbu = \u03bbc = 1\u2200u, c, we get the claimed result.\nNow suppose we denote L(I\u2217k(s)) to be the size of the latest-item set of the first-neighbor of item I \u2217 k(s) (formally, in our notation, L(S1(I\u2217k(s))) \u2013 we use L(I \u2217 k(s)) as a shorthand for this). Then we have that the probability of I\u2217k(s) being explored by S1(I \u2217 k(s)) under the ULExp policy is given by:\nP[I\u2217k(s) is explored by S1(I\u2217k(s))] =E [ R1(S1(I \u2217 k(s)))\n|L(I\u2217(s))|\n] \u2265 r\n2E [|L(I\u2217(s))|] ,\nwhere for the last inequality, we have used that R1(S1(I\u2217k(s))) is independent of L(I \u2217 k(s)), and further bounded it via Jensen\u2019s inequality. Further, via similar arguments as in Theorems 1 and 2, we have that:\nE [ 1s\u2192i\u2217\nk (s)\n] \u2265 1\n2 P[I\u2217k(s)] is explored by S1(I\u2217k(s))]\u2265\nr 4\n1\nE [|L(I\u2217k(s))|] . (1)\nThus a lower bound on the competitive-ratio essentially involves upper bounding E [|L(I\u2217k(s))|] = E [|L(s\u2032)||i= I\u2217k(s), s\u2032 = S1(i)\u2264 s]. Note that the conditioning depends on the bipartite-graph G, and also, on the reward-function sequence V ; it can not be removed in a trivial manner (i.e., we can not argue that E [|L(I\u2217k(s))|] = E [|L(s\u2032)|] for some \u2018typical\u2019 visit s\u2032). Instead, we need to exactly characterize and bound the dependence on the graph and reward-function.\nWe do so as follows: given user s arrives at time Ts, we consider all sample-paths of the process\nparametrized by two sets of random variables:\n\u2022 Il(s) = {Il(s, c)}c\u2208NC are the indices of the most recent items for each item-class. \u2022 Rs = {Rc}c\u2208NC are the number of items of each class that arrived in the interval [Ts \u2212 \u03c4,Ts], and\nsimilarly Bs = {Bu}u\u2208NU are the number of visits by each user in the same time interval.\nSince all items have a lifetime of \u03c4 , it is clear that I\u2217k(s) must have arrived in the interval [Ts \u2212 \u03c4,Ts]. Further, given Il(s),Rs and Bs, I\u2217k(s) is deterministic \u2013 we can now define c\u2217 =C(I\u2217k(s)) = c\u2217 \u2208N (U(s)), and further i\u2217 to be the index (or position) of I\u2217k(s) among all the items of class c \u2217 arriving in the interval (i.e., i\u2217 \u2208 {1,2, . . . ,Rc\u2217}). The crucial observation is that conditioning on {Rs,Bs} item/user-visits arriving in the interval implies that any ordering of these {Rs,Bs} events is equally likely \u2013 further, this remains unchanged given Il(s). This now puts us in a position where we can use Lemma 3.\nRecall that we want an upper bound on E[|L(I\u2217k(s))|] \u2013 as we argued, given the conditioning presented above, this corresponds to the item of class c\u2217 with index i\u2217 among all items of that class in the interval [Ts \u2212 \u03c4,Ts]. Now we define L(I\u2217k(s), c) to be the number of latest items of item-class c encountered by\nS1(I \u2217 k(s)) \u2013 thus L(I \u2217 k(s)) = \u2211 c\u2208N (u(s))L(I \u2217 k(s), c). Note that the first-visit of I \u2217 k(s) could correspond to any neighboring user; from Lemma 3, we thus have:\nE [|L(I\u2217k(s), c\u2217)|]\u2264E\n[ 4Rc\u2217\n1 + \u2211\nu\u2208N (c\u2217)Bu + 2\n] +E[Lprior],\nwhere Lprior is the number of additional items which arrived before Ts \u2212 \u03c4 , but were potentially in L(I\u2217k(s)). Now note that Rc\u2217 \u223c Poisson(\u03bbc\u2217\u03c4), and further, for its neighboring users, \u2211 u\u2208N (c\u2217)Bu \u223c\nPoisson( \u2211\nu\u2208N (c\u2217) \u03bbu\u03c4) (since they are a sum of independent Poisson processes). Thus we have:\nE [|L(I\u2217k(s), c\u2217)|]\u2264E [4Rc\u2217 ]E\n[ 1\u2211\nu\u2208N (c\u2217)Bu + 1\n] + 2 +\n\u03bbc\u2217\u2211 u\u2208N (c\u2217) \u03bbu\n\u2264 4\u03bbc\u2217\u03c4\n( 1\u2212 exp(\u2212\u03c4 \u2211 u\u2208N (c\u2217) \u03bbu) ) \u2211\nu\u2208N (c\u2217) \u03bbu\u03c4 + 2 + \u03bbc\u2217\u2211 u\u2208N (c\u2217) \u03bbu\n\u2264 5\u03bbc \u2217\u2211\nu\u2208N (c\u2217) \u03bbu + 2\nTo complete the proof, we need to get a bound on E [|L(I\u2217k(s), c)|] \u2200 c 6= c\u2217. Consider any visit s\u2032 in the interval [Ts\u2212 \u03c4,Ts] \u2013 conditioning only on {Rs,Bs}, it follows from symmetry that\nE [|L(s\u2032)||{Rs,Bs}]\u2264 \u2211\nc\u2208N (U(s\u2032))\nRc 1 + \u2211 u\u2208N (c)Bu + \u03bbc\u2211 u\u2208N (c) \u03bbu ,\nwhere the second term accounts for the arrivals prior to Ts \u2212 \u03c4 . In our case, we are interested in L(S1(I \u2217 k(s)), c) \u2013 so we need to take into account the condition that visit s \u2032 saw I\u2217k(s) in its latest-item set. However, in case of items of class c\u2217, we have that the number of items in the latest-item set of S1(I\u2217k(s)) can at most increase by a factor of 4. Thus, via similar arguments as above, we can show that:\nE [|L(I\u2217k(s), c)|]\u2264E\n[ 4Rc\n1 + \u2211\nu\u2208N (c)Bu\n] +\n\u03bbc\u2211 u\u2208N (c) \u03bbu ,\nand thus we have, for all user-visits s and for all k \u2208 [r]:\nE [|L(I\u2217k(s))|]\u2264E [|L(I\u2217k(s), c\u2217)|] +E  \u2211 c\u2208N (S1(I\u2217k(s))),c6=c \u2217 |L(I\u2217k(s), c\u2217)|  \u2264 2 +E\n \u2211 c\u2208N (S1(I\u2217k(s))) 4Rc 1 + \u2211 u\u2208N (c)Bu + \u03bbc\u2211 u\u2208N (c) \u03bbu  \u2264 2 + max\nu\u2208NU \u2211 c\u2208N (u)\n[ 5\u03bbc\u2211\nu\u2032\u2208N (c) \u03bbu\u2032\n] = 2 + 5Z\nNow can substitute this in Equation 1, to get the result."}, {"heading": "Appendix A: Converse Results", "text": "The main technique we use to show upper bounds on \u03b3 over any online algorithms is Yao\u2019s minimax principle (refer Motwani and Raghavan (1997)): the competitive ratio of the optimal deterministic algorithm for a given randomized input is an upper bound for the competitive ratio. Note that the algorithms are aware of the input distribution.\nA.1. Upper Bound: Competitive-ratio for the Complete Bipartite Graph:\nProof of Theorem 4. Consider the complete bipartite graph G(NU ,NI ,E), i.e., E = {(u, i)\u2200u \u2208 NU , i \u2208 NI}, with |NU | = nU and |NI | = nI . We choose a single item i\u2217 uniformly at random from NI and set V (i\u2217) = 1; the remaining items have V (i) = 0 \u2013 thus R\u2217(u) = 1 for any user u. We denote the competitive ratio of the best deterministic algorithm in this setting to be \u03b3det(G,1). Then by Yao\u2019s minimax principle, we have that for any randomized online algorithm that makes a single recommendation: \u03b3(G,1)\u2264 \u03b3det(G,1). For a user u, let the expected reward achieved by the best deterministic algorithm be denoted Rdet(u); further, let the expected sum of rewards over all users be Rdet. From symmetry, we it is clear that Rdet(u) = Rdet nU \u2200u. We now claim that the optimal deterministic algorithm achieves:\nRdet = min{nU , nI} (2nU + 1\u2212min{nU , nI})\n2nI (2)\nAlso, note that for any user u, R\u2217(u) = 1, and thus \u03b3det(G,1) = (2nU+1\u2212min{nU ,nI}) (2nI )max{1,nU/nI} . On the other hand, we have Zmax(G) = nU nI \u2013 thus, given > 0, for large enough nI we have\n\u03b3(G,1)\u2264 1 2 \u00b7max{1,Zmax(G)} + .\nTo complete the proof, we establish equation 2 via a 2-dimensional induction argument on (nU , nI). We denote the LHS of equation 2 as Rdet(nU , nI). The base cases are easy to establish \u2013 for (nU ,1), equation 2 gives Rdet(nU ,1) = nU , and for (1, nI) we have Rdet(1, nI) = 1/nI ; both these hold trivially for any deterministic algorithm. Now suppose equation 2 holds for all (n\u2032U , n \u2032 I) such that either n \u2032 U < nU , n \u2032 I \u2264 nI or n\u2032U \u2264 nU , n\u2032I < nI . Now to compute Rdet(nU , nI), we observe that any deterministic algorithm either uncovers item i\u2217 in the first exploration, else it reduces to a complete bipartite graph with (nU \u2212 1, nI \u2212 1) nodes. Since i\u2217 is picked uniformly at random by the adversary, we have:\nRdet(nU , nI) = 1\nnI \u00b7nU + nI \u2212 1 nI Rdet(nU \u2212 1, nI \u2212 1),\nand using the induction hypothesis, we get:\nRdet(nU , nI) = 1\nnI \u00b7nU + ( nI \u2212 1 nI )( min{nU \u2212 1, nI \u2212 1} (2(nU \u2212 1) + 1\u2212min{nU \u2212 1, nI \u2212 1}) 2(nI \u2212 1) ) =\n2nU + (min{nU , nI}\u2212 1) (2nU \u2212min{nU , nI}) 2nI\n= min{nU , nI} (2nU \u2212min{nU , nI}+ 1)\n2nI\nThis completes the proof.\nA.2. Upper Bound: Scaling with Number of Recommendations:\nProof of Corollary 5. Consider the complete bipartite graph G(NU ,NI ,E) with binary rewards, as in the proof of Theorem 4. V \u2217 = {i \u2208NI |V (i) = 1} is now chosen to be a uniformly-random set of r items fromNI ; thus the sum of optimal offline rewardsR\u2217r(G) = rnU . LetRdet,r(G) be the sum reward earned by the optimal deterministic algorithm showing r items on graph G \u2013 by symmetry, we have that the per-user competitive ratio is the same as the ratio of the total reward earned by the algorithm Rdet,r(G) to the total optimal offline reward R\u2217.\nNow consider an alternate problem where we have a complete bipartite graph G \u2032 (N \u2032U ,N \u2032 I ,E \u2032), where\nN \u2032I =NI and V \u2032\u2217 = V \u2217, but |NU |= rnU \u2013 essentially, G \u2032 is derived from graph G by making r copies of each user. We henceforth use G and G \u2032 as shorthand to refer to these two problems.\nSuppose now in problem G \u2032 , we can recommend only a single item. Then clearly R\u22171(G) \u2032 (the optimal\noffline reward in G \u2032 ) is rnU . Let Rdet,1(G \u2032 ) denote the expected reward earned by the optimal deterministic algorithm showing 1 recommendation on graph G \u2032 . Then we have that Rdet,1(G \u2032 )\u2265Rdet,r(G); this follows from the fact that any deterministic algorithm that recommends r items on graph G can be converted to a deterministic algorithm for graph G \u2032 (by recommending to the first r users of G \u2032 the same r items as recommended to the first user in G, and so on for each block of r users in G \u2032 ) such that they have the same rewards. However the class of all deterministic algorithms for G \u2032 is larger (in particular, it includes algorithms that recommend the same item to multiple users in a block of r consecutive users, which in the aforementioned mapping would correspond to recommending the same item multiple times to the same user in G). Now using Theorem 4, we have that:\n\u03b3det(G,r)\u2264 \u03b3det(G \u2032 ,1) =  rnU 2nI + 1 2nI : rnU \u2264 nI 1\n2 + rnU \u2212nI + 1 2rnU : rnU >nI\n,\nHence by Yao\u2019s minimax principle, for given \u2265 0, r and nU and for any randomized online algorithm that recommends \u2264 r items, for sufficiently large nI , we have \u03b3r \u2264 rnUnI + . A.3. Negative Result: Non Inverse-Degree Dynamic Exploration\nProof of Theorem 6. We first define a family of graphs: for n\u2208Z+, we define G\u0302n(NU ,NI ,E): \u2022 |NU |= n and |NI |= 2n; each user has an index in [n], and similarly each item an index in [2n]. \u2022 Each user is connected to the item with the same index, i.e., (j, j)\u2208E \u2200 j \u2208 [n]. \u2022 The remaining items are connected to all users, i.e., (u, i)\u2208E \u2200u\u2208 [n], i\u2208 {n+ 1, n+ 1, . . . ,2n}\nOne can show that Zmax(G\u0302n) = 2\u2200n; thus recommendation via the IDExp algorithm (with r = 1) guarantees a competitive ratio of 1 8e for any predictable reward-function. For the subsequent examples, we use the more restrictive binary rewards setting, i.e., V (i)\u2208 {0,1}\u2200 i; we also define V \u2217 = {i\u2208NI |V (i) = 1}.\nNow we show that given > 0, picking item i for exploration with a probability proportional to d\u22121\u00b1 i gives a vanishing competitive ratio. Note that due to the symmetry of the graph, all user arrival patterns are equivalent.\n\u2022 Consider an algorithm that picks items to explore with probability proportional to d\u22121+ i . Choose rewards such that V \u2217 = {1,2, . . . , n}, i.e., the first n items. Since each of these items is connected to the user with same index, therefore R\u2217(u) = 1. Now for user 1, the probability of picking item 1 is\ngiven by p11 = 11+n\u00b7n \u22121 = 1 1+n , and for user k+ 1, irrespective of the choices made by the previous users, we can show that:\np(k+1)(k+1) \u2264 1\n(n\u2212 k) \u00b7 (n) \u22121\nThus we have that the total reward summed across all users obeys:\nE[Ralg]\u2264 n\u2211 k=1\n1\n(n\u2212 k) \u00b7 (n) \u22121 =O(n1\u2212 logn)\nFinally, by symmetry we have that the per-user competitive ratio is the same as the ratio of sum reward to sum of optimal rewards for all users. Hence \u03b3(G,1) =O(n\u2212 logn) = o(n). \u2022 Next consider an algorithm that picks items to explore with probability proportional to d\u22121\u2212 i . Let V \u2217 = {n + 1}, i.e., the (n + 1)st item, which is now connected to all users, and hence the sum of optimal rewards over all users is R\u2217 = n. Now the probability that item n+ 1 is first explored by user\nk+ 1 is given by:\npk+1 \u2264 (\n1\n1 + (n\u2212 k) \u00b7n\u22121\u2212\n)k( n\u22121\u2212\n1 + (n\u2212 k) \u00b7n\u22121\u2212 ) = nk+k\n(n\u2212 k+n1+ )k+1 \u2264 n\u22121\u2212\nThus we have:\nE[Ralg] = n\u22121\u2211 k=0 pk+1 \u00b7 (n\u2212 k)\u2264 n\u2211 j=1 jn\u22121\u2212 =O(n1\u2212 )\nand hence (again via symmetry arguments) \u03b3(G,1) =O(n\u2212 ) = o(n).\nA.4. Negative Result: Deterministic Policies For Explore Vs. Exploit\nProof of Theorem 7. Exploit-when-possible is not competitive: In Exploit-when-possible, a user u exploits whenever a non-zero valued item is available in N (u)\u2229N explI , and explores otherwise (via some arbitrary policy). Given any \u2208 (0,1), we consider the complete bipartite graph on n \u00d7 n nodes (i.e., nI = nU = n). We consider the item values to be generated as follows: an item i\u2217 \u2208NI is picked uniformly at random, and V is defined to be:\nV (i) =\n{ 1 : i= i\u2217\n\u03b4 : i 6= i\u2217\nAgain, via symmetry arguments, we focus on the ratio of the sum of rewards earned by the algorithm to\nthe sum of optimal rewards across users. For an arbitrary user-arrival pattern \u03c0, we have that an algorithm\nwhich exploits whenever a user has a neighboring item with value > 0 will earn an expected reward of Ralg1 = 1 + n\u03b4(1\u2212 1n), and hence \u03b3 alg 1 = \u03b4+ 1\u2212\u03b4 n . Now given > 0, we can choose n\u2265 1 and \u03b4 < n \u22121 n\u22121 to get \u03b3alg1 < . Exploit-above-fixed-threshold is not competitive: A possible fix for the above problem could be to assume that a user u exploits whenever a the highest-valued item in N (u) \u2229N explI has a value greater than some threshold t \u2208 R+ (and explores otherwise, again via some arbitrary policy); note that such a deterministic threshold is part of the algorithm specification and hence assumed to be known to the adversary. Similar to\nthe previous case, we now show that such a strategy is also non-competitive.\nConsider again the complete bipartite graph on n\u00d7n nodes (i.e., nI = nU = n). Given a threshold t\u2208R+,\nwe pick an item i\u2217 \u2208NI uniformly at random and define V as:\nV (i) =\n{ \u03b4 : i= i\u2217\n0 : i 6= i\u2217\nThen clearlyR\u22171 = n\u03b4. Now suppose \u03b4 < t; for any arbitrary user-arrival pattern \u03c0 we have that an algorithm which exploits above threshold t will give an expected reward ofRalg1 = \u03b4 and hence \u03b3 alg 1 = 1 n . Again, given\n> 0, we choose n\u2265 1 to get \u03b3alg1 < ."}, {"heading": "Appendix B: Inferring Item-Values from Multiple Ratings", "text": "Proof of Theorem 8 As before, for k \u2208 [r], we define 1ULExp-fs\u2192I\u2217 k (s) to be 1 if the user corresponding to visit s is presented with the kth highest valued available item. Suppose \u03b4(f) = 0, i.e., an item\u2019s value is exactly known after f explorations. Then, from Lemma 1, we have that for any visit s, E [Rr(s)] \u2265\nE [ mink\u2208[r]E [ 1 ULExp-f s\u2192I\u2217\nk (s)\n] R\u2217r(s) ] , where the inner expectation is over the randomness in the algorithm, and\nthe outer expectation is over randomness in the sample path. Now since we assume that algorithms know\nthe value of pre-explored items (defined now as those which have been explored at least f times) have their value known to within a multiplicative factor of (1 \u00b1 \u03b4(f)), then we have that when I\u2217k(s) is in N expl I , then with probability 1 f+1 , either it is explored or a lower valued item is explored, with a value at least (1\u00b1 \u03b4(f))V (I\u2217k(s)). In the worst case, this affects all r top items; via linearity of expectation, we get:\nE [Rr(s)]\u2265 (1\u2212 2\u03b4(f))E [ min k\u2208[r] E [ 1 ULExp-f s\u2192I\u2217 k (s) ] R\u2217r(s) ] To complete the proof, for any user-visit s, and for all k \u2208 [r], we need to show that the corresponding\nkth-top item, denoted I\u2217k(s) satisfies E [ 1 ULExp s\u2192i\u2217\nk (s)\n] \u2265 1\n(f+1)f+1\n( r\n(5Zmax(G)+2)\n)f . Note that for I\u2217k(s), visit s\nmay correspond to one the first f neighboring users, or may come after I\u2217k(s) has already had f chances to be viewed \u2013 in the latter case, in order to be presented to s, we need that the item I\u2217k(s) have been seen by\nall of its first f neighboring users. Since this is the least likely of the above events, we need to lower bound this probability to complete the proof.\nIn the proof of Theorem 3, we had shown that for any visit s and any k \u2208 [r], the latest-items set L(I\u2217k(s)) satisfied E[|L(I\u2217k(s))|] \u2264 5Zmax + 2. Now for the same user, and for any of its first f neighboring visiting-users, we have that under the ULExp algorithm, it is picked independently with probability at least rf f+1 \u00b7 1 f \u00b7 1E[|(I\u2217\nk (s))|] . Thus the probability that it is explored by all f of its first visiting users is at least(\nr (f+1)(5Zmax+2)\n)f . Furthermore, the user corresponding to s exploits the kth highest-valued available item\nwith probability at least 1 f+1 . Combining these, we get the result."}, {"heading": "Acknowledgments", "text": "This work was supported by NSF Grants CNS-1017525 and CNS-1320175, and an Army Research Office Grant W911NF-11-1-0265."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer."], "venue": "Machine Learning 47(2-3) 235\u2013256. 3, 17", "citeRegEx": "Auer et al\\.,? 2002", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Online auctions and generalized secretary problems", "author": ["Babaioff", "Moshe", "Nicole Immorlica", "David Kempe", "Robert Kleinberg."], "venue": "SIGecom Exchanges 7(2). 16, 17", "citeRegEx": "Babaioff et al\\.,? 2008", "shortCiteRegEx": "Babaioff et al\\.", "year": 2008}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["Bubeck", "S\u00e9bastien", "Nicol\u00f2 Cesa-Bianchi."], "venue": "Foundations and Trends R", "citeRegEx": "Bubeck et al\\.,? 2008", "shortCiteRegEx": "Bubeck et al\\.", "year": 2008}, {"title": "Competitive weighted matching in transversal matroids", "author": ["Dimitrov", "Nedialko B.", "C. Greg Plaxton."], "venue": "ICALP (1). 397\u2013408. 17", "citeRegEx": "Dimitrov et al\\.,? 2008", "shortCiteRegEx": "Dimitrov et al\\.", "year": 2008}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Dud\u0131\u0301k", "Miroslav", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "UAI. 169\u2013178", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Faster algorithms for semi-matching problems", "author": ["J. Fakcharoenphol", "B. Laekhanukit", "D. Nanongkai."], "venue": "Automata, Languages and Programming 176\u2013187. 7", "citeRegEx": "Fakcharoenphol et al\\.,? 2010", "shortCiteRegEx": "Fakcharoenphol et al\\.", "year": 2010}, {"title": "Bandit processes and dynamic allocation indices", "author": ["Gittins", "John C."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 148\u2013177. 17", "citeRegEx": "Gittins and C.,? 1979", "shortCiteRegEx": "Gittins and C.", "year": 1979}, {"title": "Bounds for certain multiprocessing anomalies", "author": ["Graham", "Ronald L."], "venue": "Bell System Tech. J. 45. 1563\u20131581. 7, 21", "citeRegEx": "Graham and L.,? 1966", "shortCiteRegEx": "Graham and L.", "year": 1966}, {"title": "Semi-matchings for bipartite graphs and load balancing", "author": ["N. Harvey", "R. Ladner", "L. Lov\u00e1sz", "T. Tamir."], "venue": "Algorithms and data structures 294\u2013306. 7", "citeRegEx": "Harvey et al\\.,? 2003", "shortCiteRegEx": "Harvey et al\\.", "year": 2003}, {"title": "Inferring rankings under constrained sensing", "author": ["S. Jagabathula", "D. Shah."], "venue": "Advances in Neural Information Processing Systems 21 753\u2013760. 4, 16, 17", "citeRegEx": "Jagabathula and Shah.,? 2008", "shortCiteRegEx": "Jagabathula and Shah.", "year": 2008}, {"title": "Matrix completion from noisy entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh."], "venue": "The Journal of Machine Learning Research 11 2057\u20132078. 4, 16", "citeRegEx": "Keshavan et al\\.,? 2010", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma."], "venue": "Machine learning 80(2-3) 245\u2013272. 17", "citeRegEx": "Kleinberg et al\\.,? 2010", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2010}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Herbert Robbins."], "venue": "Advances in applied mathematics 6(1) 4\u201322. 17", "citeRegEx": "Lai et al\\.,? 1985", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Adwords and generalized on-line matching", "author": ["Mehta", "Aranyak", "Amin Saberi", "Umesh Vazirani", "Vijay Vazirani."], "venue": "FOCS \u201905: Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science. 264\u2013273. 17", "citeRegEx": "Mehta et al\\.,? 2005", "shortCiteRegEx": "Mehta et al\\.", "year": 2005}, {"title": "Randomized Algorithms", "author": ["Motwani", "Rajeev", "Prabhakar Raghavan."], "venue": "Cambridge University Press. 14, 28", "citeRegEx": "Motwani et al\\.,? 1997", "shortCiteRegEx": "Motwani et al\\.", "year": 1997}, {"title": "Predicting the popularity of online content", "author": ["Szabo", "Gabor", "Bernardo A Huberman."], "venue": "Communications of the ACM 53(8) 80\u201388. 3", "citeRegEx": "Szabo et al\\.,? 2010", "shortCiteRegEx": "Szabo et al\\.", "year": 2010}, {"title": "Patterns of temporal variation in online media", "author": ["Yang", "Jaewon", "Jure Leskovec."], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining. ACM, 177\u2013186. 4", "citeRegEx": "Yang et al\\.,? 2011", "shortCiteRegEx": "Yang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "d reward from some distribution with unknown mean (Auer et al. (2002); see also Bubeck and Cesa-Bianchi (2008) for a survey of the field).", "startOffset": 51, "endOffset": 70}, {"referenceID": 0, "context": "d reward from some distribution with unknown mean (Auer et al. (2002); see also Bubeck and Cesa-Bianchi (2008) for a survey of the field).", "startOffset": 51, "endOffset": 111}, {"referenceID": 9, "context": "Furthermore, work on static recommendation (Keshavan et al. (2010), Jagabathula and Shah (2008)) also provide guarantees for learning item-value from a few ratings under alternate structural assumptions; these observations tie in well with our model.", "startOffset": 44, "endOffset": 67}, {"referenceID": 9, "context": "(2010), Jagabathula and Shah (2008)) also provide guarantees for learning item-value from a few ratings under alternate structural assumptions; these observations tie in well with our model.", "startOffset": 8, "endOffset": 36}, {"referenceID": 8, "context": "The above problem is known in different communities as the minimum makespan problem (Graham (1966)), or optimal semi-matching problem (Harvey et al. (2003)) \u2013 we henceforth refer to d\u2217(G) as the makespan", "startOffset": 135, "endOffset": 156}, {"referenceID": 5, "context": "O(m \u221a n logn) (Fakcharoenphol et al. (2010)), where m= |E|, n= nU +nI .", "startOffset": 15, "endOffset": 44}, {"referenceID": 1, "context": "This is reminiscent of the Secretary problem (Babaioff et al. (2008)), and also allows for techniques such as in (Jagabathula and Shah (2008)).", "startOffset": 46, "endOffset": 69}, {"referenceID": 1, "context": "This is reminiscent of the Secretary problem (Babaioff et al. (2008)), and also allows for techniques such as in (Jagabathula and Shah (2008)).", "startOffset": 46, "endOffset": 142}, {"referenceID": 1, "context": "This is reminiscent of the Secretary problem (Babaioff et al. (2008)), and also allows for techniques such as in (Jagabathula and Shah (2008)). Probabilistic Predictability: In many cases, we may be only able to identify the top item for a user with some probability Ppred; for example, in collaborative filtering algorithms such as matrix completion (Keshavan et al. (2010)).", "startOffset": 46, "endOffset": 375}, {"referenceID": 10, "context": "Instead, the dominant view is one of taking the user feedback data as a static given (Keshavan et al. (2010),", "startOffset": 86, "endOffset": 109}, {"referenceID": 0, "context": ", r = 1) \u2013 then a bandit algorithm will sample all items at-least once (in particular, the standard UCB algorithm of Auer et al. (2002) will sample each arm once just during initialization), thereby getting a competitive-ratio of \u03b3 =O(1/n)\u2192 0.", "startOffset": 117, "endOffset": 136}, {"referenceID": 0, "context": ", r = 1) \u2013 then a bandit algorithm will sample all items at-least once (in particular, the standard UCB algorithm of Auer et al. (2002) will sample each arm once just during initialization), thereby getting a competitive-ratio of \u03b3 =O(1/n)\u2192 0. On the other hand, the algorithms we present in this work achieve a competitive-ratio of 1 8 . A notion of an access graph is incorporated in some bandit models such as the Contextual Bandits (Dud\u0131\u0301k et al. (2011)) or Sleeping Bandits (Kleinberg et al.", "startOffset": 117, "endOffset": 458}, {"referenceID": 0, "context": ", r = 1) \u2013 then a bandit algorithm will sample all items at-least once (in particular, the standard UCB algorithm of Auer et al. (2002) will sample each arm once just during initialization), thereby getting a competitive-ratio of \u03b3 =O(1/n)\u2192 0. On the other hand, the algorithms we present in this work achieve a competitive-ratio of 1 8 . A notion of an access graph is incorporated in some bandit models such as the Contextual Bandits (Dud\u0131\u0301k et al. (2011)) or Sleeping Bandits (Kleinberg et al. (2010)) models, the graph and user dynamics are assumed arbitrary.", "startOffset": 117, "endOffset": 504}, {"referenceID": 0, "context": ", r = 1) \u2013 then a bandit algorithm will sample all items at-least once (in particular, the standard UCB algorithm of Auer et al. (2002) will sample each arm once just during initialization), thereby getting a competitive-ratio of \u03b3 =O(1/n)\u2192 0. On the other hand, the algorithms we present in this work achieve a competitive-ratio of 1 8 . A notion of an access graph is incorporated in some bandit models such as the Contextual Bandits (Dud\u0131\u0301k et al. (2011)) or Sleeping Bandits (Kleinberg et al. (2010)) models, the graph and user dynamics are assumed arbitrary. The graph is not used to inform the algorithm design except in that it constrains what items can be shown \u2013 essentially this corresponds to having arbitrary access-constraints, which leads to the results being pessimistic. In our setup, on the other hand, imposing natural stochastic assumptions on user/item dynamics leads to much stronger competitive-ratio guarantees. Online Matching and its Variants: Although having the appearance of a bandit problem, our setting is in fact much closer in spirit to certain online optimization problems on graphs. Online auction design problems (Mehta et al. (2005)) incorporate the fact that an item can be displayed to multiple users, constrained by an underlying graph.", "startOffset": 117, "endOffset": 1169}, {"referenceID": 0, "context": ", r = 1) \u2013 then a bandit algorithm will sample all items at-least once (in particular, the standard UCB algorithm of Auer et al. (2002) will sample each arm once just during initialization), thereby getting a competitive-ratio of \u03b3 =O(1/n)\u2192 0. On the other hand, the algorithms we present in this work achieve a competitive-ratio of 1 8 . A notion of an access graph is incorporated in some bandit models such as the Contextual Bandits (Dud\u0131\u0301k et al. (2011)) or Sleeping Bandits (Kleinberg et al. (2010)) models, the graph and user dynamics are assumed arbitrary. The graph is not used to inform the algorithm design except in that it constrains what items can be shown \u2013 essentially this corresponds to having arbitrary access-constraints, which leads to the results being pessimistic. In our setup, on the other hand, imposing natural stochastic assumptions on user/item dynamics leads to much stronger competitive-ratio guarantees. Online Matching and its Variants: Although having the appearance of a bandit problem, our setting is in fact much closer in spirit to certain online optimization problems on graphs. Online auction design problems (Mehta et al. (2005)) incorporate the fact that an item can be displayed to multiple users, constrained by an underlying graph. However, in such problems the node weights (bids) are known, which often allows greedy algorithms to be constant-factor competitive. Related problems include the generalized secretary problem (Babaioff et al. (2008)) and online transversal-matroid selection (Dimitrov and Plaxton (2008)); both are", "startOffset": 117, "endOffset": 1492}, {"referenceID": 0, "context": ", r = 1) \u2013 then a bandit algorithm will sample all items at-least once (in particular, the standard UCB algorithm of Auer et al. (2002) will sample each arm once just during initialization), thereby getting a competitive-ratio of \u03b3 =O(1/n)\u2192 0. On the other hand, the algorithms we present in this work achieve a competitive-ratio of 1 8 . A notion of an access graph is incorporated in some bandit models such as the Contextual Bandits (Dud\u0131\u0301k et al. (2011)) or Sleeping Bandits (Kleinberg et al. (2010)) models, the graph and user dynamics are assumed arbitrary. The graph is not used to inform the algorithm design except in that it constrains what items can be shown \u2013 essentially this corresponds to having arbitrary access-constraints, which leads to the results being pessimistic. In our setup, on the other hand, imposing natural stochastic assumptions on user/item dynamics leads to much stronger competitive-ratio guarantees. Online Matching and its Variants: Although having the appearance of a bandit problem, our setting is in fact much closer in spirit to certain online optimization problems on graphs. Online auction design problems (Mehta et al. (2005)) incorporate the fact that an item can be displayed to multiple users, constrained by an underlying graph. However, in such problems the node weights (bids) are known, which often allows greedy algorithms to be constant-factor competitive. Related problems include the generalized secretary problem (Babaioff et al. (2008)) and online transversal-matroid selection (Dimitrov and Plaxton (2008)); both are", "startOffset": 117, "endOffset": 1563}], "year": 2014, "abstractText": "A common phenomena in modern recommendation systems is the use of feedback from one user to infer the \u2018value\u2019 of an item to other users. This results in an exploration vs. exploitation trade-off, in which items of possibly low value have to be presented to users in order to ascertain their value. Existing approaches to solving this problem focus on the case where the number of items are small, or admit some underlying structure \u2013 it is unclear, however, if good recommendation is possible when dealing with content-rich settings with unstructured content. We consider this problem under a simple natural model, wherein the number of items and the number of item-views are of the same order, and an \u2018access-graph\u2019 constrains which user is allowed to see which item. Our main insight is that the presence of the access-graph in fact makes good recommendation possible \u2013 however this requires the exploration policy to be designed to take advantage of the access-graph. Our results demonstrate the importance of \u2018serendipity\u2019 in exploration, and how higher graph-expansion translates to a higher quality of recommendations; it also suggests a reason why in some settings, simple policies like Twitter\u2019s \u2018Latest-First\u2019 policy achieve a good performance. From a technical perspective, our model presents a way to study exploration-exploitation tradeoffs in settings where the number of \u2018trials\u2019 and \u2018strategies\u2019 are large (potentially infinite), and more importantly, of the same order. Our algorithms admit competitive-ratio guarantees which hold for the worst-case user, under both finite-population and infinite-horizon settings, and are parametrized in terms of properties of the underlying graph. Conversely, we also demonstrate that improperly-designed policies can be highly sub-optimal, and that in many settings, our results are order-wise optimal.", "creator": "LaTeX with hyperref package"}}}