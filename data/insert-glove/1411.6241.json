{"id": "1411.6241", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Improved Spectral Clustering via Embedded Label Propagation", "abstract": "1924-1925 Spectral laghu clustering 62.60 is a key thurah research topic in gau the field of 46m machine cucamonga learning and hellmund data 62.60 mining. fishing Most representante of demichelis the existing washn spectral clustering scapular algorithms rh7 are stereolithography built schrempp upon grainne Gaussian kostolac Laplacian matrices, dbd which jirapan are sensitive to parameters. We hammadi propose a formula novel carpinello parameter free, distance quizzically consistent Locally Linear badalona Embedding. 23.19 The distillates proposed alveolo-palatal distance consistent LLE promises sidelocks that edges 32-county between samarqand closer jow data points rockhopper have greater weight. receded Furthermore, whip-like we sufferance propose 50-metres a novel improved spectral aknowledged clustering via embedded label zeri propagation. chengchi Our rengifo algorithm is pompom built j. upon two advancements of satyrus the dumpster state nbs of vocalizing the art: 1) label propagation, citadeed which colmes propagates a p\u00e1draig node \\ ' wuyuan s tommie labels cybernauts to neighboring 9.75 nodes lestrade according to aguateca their 5,499 proximity; chomp and norske 2) 15.84 manifold learning, which has sews been widely kunsthistorisches used 90-percent in scrunches its villefranche-sur-mer capacity to leverage p&w the manifold hungrier structure of data turzii points. curious First we perform standard 22.81 spectral clustering kwin on original krzynowek data baseketball and assign each zicklin cluster to k nearest guardroom data layaway points. Next, todd we bse-500 propagate labels cantores through dense, unlabeled seing data galella regions. harshness Extensive 49.17 experiments modelers with panaman various makasa datasets kolnik validate the superiority dlewis of the bo\u010dek proposed algorithm anat compared to current achmat state of lusaka the 418,000 art spectral katsumi algorithms.", "histories": [["v1", "Sun, 23 Nov 2014 13:35:29 GMT  (1997kb)", "http://arxiv.org/abs/1411.6241v1", null], ["v2", "Tue, 6 Oct 2015 16:49:39 GMT  (0kb,I)", "http://arxiv.org/abs/1411.6241v2", "Withdraw for a wrong formulation"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "yi yang", "heng huang"], "accepted": false, "id": "1411.6241"}, "pdf": {"name": "1411.6241.pdf", "metadata": {"source": "CRF", "title": "Improved Spectral Clustering via Embedded Label Propagation", "authors": ["Xiaojun Chang", "Feiping Nie", "Yi Yang", "Heng Huang"], "emails": ["cxj273@gmail.com,", "feipingnie@gmail.com,", "yi.yang@uq.edu.au,", "heng@uta.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n62 41\nv1 [\ncs .L\nG ]\n2 3\nN ov\n2 01"}, {"heading": "Introduction", "text": "Data clustering is a fundamental research topic that is widely used for many applications in the fields of artificial intelligence, statistics and the social sciences (Jain, Murty, and Flynn 1999)(Jain and Dubes 1988a)(Girolami 2002)(Ye, Zhao, and Liu 2007a). The purpose of clustering is to partition the original data points into various groups so that data points within the same cluster are dense while those in different clusters are far apart (Jain and Dubes 1988b)(Filippone et al. 2008a) (Chang et al. 2015).\nAmong various implementations of clustering, k-means is one of the most popular in reality because of its simplicity and effectiveness (Wu et al. 2012). The general procedure of traditional k-means (TKM) is to randomly initialize c clustering centers, assign each data point to its nearest cluster and compute a new clustering center. Current researchers claim the curse of dimensionality may deteriorate the performance of TKM (Ding and Li 2007). A straight-forward solution to this problem is to project original datasets to\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\na low-dimensional subspace by dimensionality reduction, i.e., PCA, before performing TKM. Discriminative analysis has proven effective in enhancing clustering performance (Ding and Li 2007) (la Torre, Fernando, and Kanade 2006) (Ye, Zhao, and Liu 2007b). Thus, discriminative k-means (DKM) (Ye, Zhao, and Wu 2007) is proposed as a way to incorporate discriminative analysis and clustering into a single framework to formalize clustering as a trace maximization problem. However, TKM and DKM fail to take lowdimensional manifold structure of data into consideration.\nOn a further note, spectral clustering (SC) (Yu and Shi 2003) (Filippone et al. 2008b) (Shi and Malik 2000) has gradually attracted increasing research attention for its capacity to mine intrinsic data geometric structures, which facilitates partitioning data with more complicated structures (Belkin and Niyogi 2003) (Yang et al. 2011) (Nie et al. 2009) (Wu and Schlkopf 2006) (Yang et al. 2010). The basic idea of SC is to find a cluster assignment of data points by adopting a spectrum of the similarity matrix that leverages the nonlinear and lowdimensional manifold structure of original data. Inspired by the benefits of spectral clustering, different variants of the SC method have been proposed to demonstrate its effectiveness. For example, local learning-based clustering (LLC) (Wu and Schlkopf 2006) utilizes a kernel regression model for label prediction based on the assumption that the class label of a data point can be determined by its neighbors. Similarly, self-tuning SC (Zelnik-Manor and Perona 2004) is able to tune parameters automatically in an unsupervised scenario. Normalized cuts are capable of balancing the volume of clusters for the usage of data density information (Shi and Malik 2000).\nLabel propagation has been shown effective in propagating labels through the dataset along high density areas defined by unlabeled data in (Zhu and Ghahramani 2002) (Wang and Zhang 2008). Central to label propagation is the following cluster assumption (Chapelle, Weston, and Schlkopf 2002): (1) nearby data points are likely to belong to the same cluster; and (2) data points on the same structures are likely to have the same label. Motivated by the benefits inherent to label propagation, we intend to introduce label propagation into the field of spectral clustering(Kang, Jin, and Sukthankar 2006)\n(Cao, Luo, and Huang 2008) (Cheng, Liu, and Yang 2009).\nOur proposed spectral clustering algorithm therefore combines the strengths of spectral clustering and label propagation. The main process of our algorithm is shown in Fig. 1. We first perform standard spectral clustering on our original dataset and obtain c clusters. Then, we select k data points that are respectively close to each cluster center and form label matrix Y . By way of manifold learning, we propagate labels through dense unlabeled data regions. We call the proposed method improved Spectral Clustering via embedded Label Propagation (SCLP).\nThe main contributions of this paper can be summarized as follows:\n1. To the best of our knowledge, this is the first time spectral clustering and embedded label propagation have been incorporated into a single framework. We propagate the labels obtained by spectral clustering to other unlabeled data points.\n2. We integrate the advantage of manifold learning, which is capable of leveraging manifold structure among data points, into the proposed framework.\n3. A novel distance-consistent Locally Linear Embedding is proposed herein as well. Unlike a traditional Guassian graph approach, the proposed graph is parameter-free.\n4. Extensive experiments on seven real-world datasets demonstrate that the proposed spectral clustering framework (SCLP) outperforms state-of-the-art clustering algorithms.\nAfter revisiting related work on Locally Linear Embedding and spectral clustering in Section 2, we detail our SCLP algorithm in Section 3. Extensive experiments are given in Section 4 and Section 5 concludes this paper.\nRelated Work"}, {"heading": "Locally Linear Embedding", "text": "Locally Linear Embedding (LLE) (Roweis and Saul 2000) aims to identify low-dimensional global coordinates that lie on, or very near to, a manifold embedded in a highdimensional space. The purpose is to combine the data points with minimal discrepancy after completing a different linear dimensionality reduction at each point.\nThe main procedure of LLE can be summarized in three steps: (1) build a neighborhood for each data point; (2) find the weights in order to linearly approximate the data in said neighborhood; and (3) find the low-dimensional coordinates best reconstructed by those weights.\nBy way of example, given a dataset matrix X = {x1, x2, . . . , xn}, the main steps of LLE are as follows:\n1. For each data point xi, find its k nearest neighbors.\n2. Compute the weight matrix A by minimizing the residual sum of squares to reconstruct each xi from its neighbours as such:\nmin A\nn\u2211\ni=1\n\u2016xi \u2212 k\u2211\nj=1\nxjaij\u201622, (1)\nwhere aij = 0 if xj is not one of xi\u2019s k-nearest neighbours, and for each data point xi, \u2211 j aij = 1.\n3. Obtain the coordinates S by minimizing the following reconstruction error using the weights:\nargmin S\nn\u2211\ni=1\n\u2016si \u2212 k\u2211\nj=1\nyjaij\u201622,\nwhere yj is the cluster indicator vector for the datum xj ,\u2211 i Sij = 0 for each j and S TS = I ."}, {"heading": "Spectral Clustering", "text": "Consider a dataset X = {x1, x2, . . . , xn} \u2208 Rd\u00d7n, where d is the dimension of the data point and n is the total number of data points. The objective of clustering is to partition X into c clusters Ci|ci=1 so as to keep data points within the same cluster close to one another, while data points from different clusters remain apart. Let us denote Y = [y1, y2, . . . , yn]\nT \u2208 Rn\u00d7c as the cluster indicator matrix, where yi is the cluster indicator vector for the datum xi. The j-th element of yi is 1 if xi belongs to the j-th cluster and 0 otherwise. Following the work in (Ye, Zhao, and Wu 2007),\nwe denote the scaled cluster indicator matrix F as follows:\nF = [F1, F2, . . . , Fn] T = Y (Y TY )\u22121/2, (2)\nwhereFi is the scaled cluster indicator of xi. The j-th column of F is defined as follows by (Ye, Zhao, and Wu 2007):\nfj =  0, . . . , 0,\ufe38 \ufe37\ufe37 \ufe38\n\u2211j\u22121 i=1 ni\n1 \u221a nj , . . . , 1 \u221a nj , \ufe38 \ufe37\ufe37 \ufe38 nj 0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38\u2211 c i=j+1 nk\n  , (3)\nand indicates which data points are partitioned into the jth cluster Cj . Meanwhile, nj is the number of data points in cluster Cj .\nAccording to (Dhillon, Guan, and Kulis 2004), the overall function of spectral clustering can be defined as follows:\nmin F\nTr(FTLF )\ns.t. F = Y (Y TY )\u22121/2, (4)\nwhere Tr(\u00b7) denotes the trace operator and L is a graph Laplacian matrix computed in accordance with local data structure. Among different strategies, a common way to compute the weight matrix is thus:\naij =\n \n exp(\u2212 \u2016xi\u2212xj\u2016\n2\n\u03b42 ), if xi and xj are k nearest neighbours.\n0, otherwise\n(5)\nwhere Nk (xj) denotes k nearest neighbors of xj and \u03b4 is utilized to control the spread of neighbors. The Laplacian graph L is then computed by L = D \u2212W , where D is a diagonal matrix with its diagonal elements as Dii = \u2211 j Aij .\nBy replacing L in Eq. (4) by the normalized Laplacian matrix Ln,\nLn = D \u22121/2 LD \u22121/2 = I \u2212D \u22121/2 AD \u22121/2 (6)\nthe objective function becomes the well-known SC algorithm normalized cut (Shi and Malik 2000). In the same manner, if we replace L in Eq. (4) with Ll, which is a Laplacian matrix obtain(Yang et al. 2010)ed by local learning (Wu and Schlkopf 2006), the objective function is then modified to Local Learning Clustering (LLC)."}, {"heading": "The Proposed Framework", "text": "In this section, we illustrate the detailed framework of our algorithm.\nWe aim to cluster the dataset into c clusters. Suppose X \u2208 R\nd\u00d7n indicates the dataset; d is the dimension of data points and n is the total number of data points."}, {"heading": "Distance-Consistent Similarity Learning", "text": "Following the work in (Karasuyama and Mamitsuka 2013), we propose leveraging manifold regularization built upon the Laplacian graph for label propagation. To begin, we first present a novel distance-consistent Local Linear Embedding (LLE).\nIntuitively, we expect close data points to have similar labels. We create a graph in which all data points are considered nodes. If xi (xj) is in k-Nearest-Neighbor of xj (xi), then the two nodes are connected. The edge between them is weighted so that the closer the nodes are in Euclidean distance, the larger the weight aij . Because we have\u2211\nj aij = 1, the objective function of LLE in Eq. (1) can be safely rewritten as follows:\nmin A\nn\u2211\ni=1\n\u2016[xi, . . . , xi]aij \u2212 k\u2211\nj=1\nxijaij\u201622, (7)\nBy way of simple mathematical deduction, we can rewrite the above objective function in this manner:\nmin A\nn\u2211\ni=1\n\u2016[xi \u2212 xi1 , . . . , xi \u2212 xik ]ai\u201622 (8)\n\u21d2 min A\nn\u2211\ni=1\nTr(aTi [xi\u2212xi1 , . . . , xi\u2212xik ]T [xi\u2212xi1 , . . . , xi\u2212xik ]ai)\n(9) For simplicity\u2019s sake, we assign all the non-diagonal elements of [xi\u2212xi1 , . . . , xi\u2212xik ]T [xi\u2212xi1 , . . . , xi\u2212xik ] to zero. The above objective function is equivalent to the following:\nmin A\nn\u2211\ni=1\nk\u2211\nj=1\n\u2016xi \u2212 xij \u201622a2ij (10)\ns.t. k\u2211\nj=1\naij = 1, aij \u2265 0\nFrom the above function, we can observe that the proposed distance-consistent LLE suggests that the edge between closer nodes has a greater weight.\nThe Lagrangian function of problem of Eq. (10) can be written as\nk\u2211\nj=1\n\u2016xi \u2212 xij\u201622a2ij \u2212 \u03b3( k\u2211\nj=1\naij \u2212 1), (11)\nwhere \u03b3 is a Lagrange multiplier. By setting the derivative of Eq. (11) w.r.t. aij to zero, we have\naij = \u03b3\n2\u2016xi \u2212 xij\u201622 . (12)\nBy substituting the resultant aij in Eq. (12) into the constraint \u2211k j=1 aij = 1, we arrive at\n\u03b3 = 1\n\u2211k j=1\n1\n2\u2016xi\u2212xij \u2016 2 2\n. (13)\nBy integrating Eq. (12) and Eq. (13), we obtain the final solution for aij . By denotingD as a diagonal matrix with its diagonal di =\u2211 j aij , the graph Laplacian can be calculated as\nL = D \u2212A. (14)"}, {"heading": "Refined Spectral Clustering", "text": "After initially clustering the dataset into c clusters through traditional spectral clustering, we select k data points per cluster, which are nearest to each clustering center, and mark them as labeled data points. The remaining points are marked as unlabeled data points. Note that we assume these k data points are grouped into the proper clusters. Hence, we obtain the label matrix Y \u2208 Rn\u00d7c and diagonal selection matrix U \u2208 Rn\u00d7n, where Yi,j = 1 if Xi is labeled, and Xi belongs to the j-th cluster. Yi,j = 0 otherwise. Uii = \u221e if\nXi is labeled and Uii = 0 otherwise. In the experiment, we use 1010 to approximate \u221e. We propose propagating labels of labeled data points to unlabeled data points.\nMoreover, we denote a predicted label matrix F \u2208 Rn\u00d7c for the data points in X . According to (Nie et al. 2010), F should satisfy the smoothness on both the obtained label matrix Y and the manifold structure. Hence, F can be obtained as follows (Zhu 2006):\nmin F\nTr(FTLF ) + Tr((F \u2212 Y )TU(F \u2212 Y )), (15)\nwhere Tr(\u00b7) denotes the trace operator. The purpose of this definition is to keep the predicted labels F consistent with the ground truth labels Y .\nWe further incorporate a regularization term into the objective function to correlate the features with the predicted labels. Consequently, the objective function arrives at\nmin F\nTr(FTLF ) + Tr((F \u2212 Y )TU(F \u2212 Y ))\n+ \u03b1\u2016XTW \u2212 F\u20162F + \u03b2\u2016W\u20162F (16)\nwhere \u2016 \u00b7 \u2016F denotes the Frobenius norm of a matrx. Since the least square loss function is very sensitive to outliers, we employ l2,1-norm on the regularization term to handle this issue. Hence, we can rewrite the objective function as follows:\nmin F\nTr(FTLF ) + Tr((F \u2212 Y )TU(F \u2212 Y ))\n+ \u03b1\u2016XTW \u2212 F\u20162F + \u03b2\u2016W\u20162,1 (17)\nIt is worth noting that the proposed framework can be readily applied to out-of-sample clustering. By calculating XTW , we obtain the label predictor matrix for outside samples."}, {"heading": "Optimization", "text": "The proposed function involves the l2,1-norm, which is difficult to solve in a closed form. We propose to solve this problem in the following steps. By setting the derivative of Eq. (17) w.r.t. W to zero, we have\nW = \u03b1(\u03b1XXT + \u03b2D)\u22121XF, (18)\nwhere I is an identity matrix and D is a diagonal matrix which is defined as:\nD =   1 2\u2016w1\u20162 . . .\n1\n2\u2016wd\u20162\n  . (19)\nLetting H represent (\u03b1XXT + \u03b2D)\u22121, the objective becomes\nmin F\nTr(FTLF ) + Tr((F \u2212 Y )TU(F \u2212 Y ))\n+ \u03b1\u2016\u03b1XTHXF \u2212 F\u20162F + \u03b2\u2016\u03b1HXF\u20162F (20)\nBy denoting P as P = \u03b1XTHX \u2212 I , the objective becomes\nmin F\nTr(FTLF ) + Tr((F \u2212 Y )TU(F \u2212 Y ))\n+ \u03b1\u2016PF\u20162F + \u03b2\u2016\u03b1HXF\u20162F . (21)\nBy setting the derivative of Eq. (21) w.r.t. F to zero, we have\nF = (L+ U + \u03b1PP + \u03b12\u03b2XTHHX)\u22121UY (22)\nBased on the above mathematical deduction, we propose an efficient iterative algorithm to optimize the objective function Eq. (17), which is summarized in Algorithm 1.\nAlgorithm 1: Optimization Algorithm for SCLP\nData: Data X \u2208 Rd\u00d7n The number of clusters c Parameters \u03b1 and \u03b2\nResult: The cluster indicator matrix F 1 Construct the Laplacian matrix L according to Eq. (14) ; 2 Compute the selecting matrix U \u2208 Rn\u00d7n ; 3 Cluster data X into c clusters through Spectral\nClustering ; 4 Pick out nL data points that are close to each cluster\ncenter and construct Y ; 5 repeat 6 Compute D according to\nD =   1 2\u2016w1\u20162 . . .\n1\n2\u2016wd\u20162\n  .\n7 Compute H according to H = (\u03b1XXT + \u03b2D)\u22121 ; 8 Compute P according to P = \u03b1XTHX \u2212 I ; 9 Compute F according to\nF = (L+U +\u03b1PP +\u03b12\u03b2XTHHX)\u22121(U \u2217 Y ) ; 10 until Convergence; 11 Return F\nExperiments In this section, we conduct extensive experiments to validate the proposed SCLP\u2019s performance and compare it with related state-of-the-art spectral clustering algorithms, following a study of parameter sensitivity."}, {"heading": "Dataset Description", "text": "We use seven trademark datasets to validate the performance of the proposed algorithm. The USPS dataset has 9298 gray-scale handwritten digital images with an image size of 256 scanned from envelopers with the U.S. Postal Service. The Yale-B dataset (Georghiades, Belhumeur, and Kriegman 2001)"}, {"heading": "Dataset Matrix Size Dataset Size Class #", "text": "consists of 2414 near frontal images from 38 persons under different illuminations. The AR dataset (Martinez and Benavente 1998) has 840 images with a dimension of 768. The FRGC dataset (Phillips et al. 2005) was collected at the University of Notre Dame and contains 50,000 images. All the images were taken across 13 different poses, under 43 different illumination conditions and with four different expressions per person. The MSRA50 dataset (He et al. 2004) consists of 1799 images and 12 classes. The PALM dataset consists 700 right-hand images, seven samples per person across 100 users, taken via digital camera. The images are resized to the same dimension of 100 \u00d7 100. The human lung carcinomas (LUNG) dataset (Singh et al. 2002) contains 203 samples and 3312 genes."}, {"heading": "Experiment Setup", "text": "We compare the proposed SCLP with traditional kmeans (TKM) (Wu et al. 2012), discriminative k-means (DKM) (Ye, Zhao, and Wu 2007), Local Learning Clustering (LLC) (Wu and Schlkopf 2006), Non-negative Normalized Cut (NNC) (Shi and Malik 2000), Spectral Clustering (SC), LLC (Wu and Schlkopf 2006), CLGR (Wang, Zhang, and Li 2009) and Spectral Embedding Clustering (SEC) (Nie et al. 2009).\nThe size of neighborhood k is set to 5 for all spectral clustering algorithms. For the parameter, \u03b4, in NNC, we perform a self-tuning algorithm (Zelnik-Manor and Perona 2004) to determine the best parameter. For parameters in DKM, LLC, CLGR and SEC, we tune them in the range of {10\u22126, 10\u22124, 10\u22122, 100, 102, 104, 106} and report the best results. Note that the results of all clustering algorithms vary based on initialization. To reduce the influence of statistical variation, we repeat each clustering 50 times with random initialization and report the results according to the best objective function values. For SCLP, we select 2 data points per cluster nearest to the clustering center."}, {"heading": "Evaluation Metrics", "text": "Following related clustering studies, we utilize clustering accuracy (ACC) and normalized mutual information (NMI) as our experiments\u2019 evaluation metrics.\nLet qi represent the clustering label result from a clustering algorithm; pi represent the corresponding ground truth\nlabel of arbitrary data point xi. From there, ACC is defined as follows:\nACC =\n\u2211n i=1 \u03b4(pi,map(qi))\nn , (23)\nwhere \u03b4(x, y) = 1 if x = y and \u03b4(x, y) = 0 otherwise. map(qi) is the best mapping function that permutes clustering labels to match the ground truth labels using the KuhnMunkres algorithm. A larger ACC indicates a better clustering performance.\nFor any two arbitrary variables P and Q, NMI is defined as follows (Strehl and Ghosh 2003)\nNMI = I(P,Q)\u221a H(P )H(Q) , (24)\nwhere I(P,Q) computes mutual information between P and Q, and H(P ) and H(Q) are the entropies of P and Q. Let tl represent the number of data points in the cluster Cl(1 \u2264 l \u2264 c) generated by a clustering algorithm and t\u0303h represent the number of data points from the h-th ground truth class. The NMI metric is then computed as follows (Strehl and Ghosh 2003)\nNMI =\n\u2211c l=1 \u2211c h=1 tl,hlog( n\u00d7tl,h ?tl t\u0303h\n) \u221a ( \u2211c\nl=1 tl log tl n )( \u2211c h=1 t\u0303h log t\u0303h n )\n, (25)\nwhere tl,h is the number of data samples that lies in the intersection between Cl and hth ground truth class. Similarly, a larger NMI indicates a better clustering performance."}, {"heading": "Experimental Results", "text": "We show the clustering results of different algorithms in terms of ACC and NMI over seven benchmark datasets in Tables 2 and 3. Based on the results of our experiment, we can make the following observations:\n1. When comparing the k-means based algorithms (i.e., TKM and DKM), DKM generally outperforms TKM because discriminative dimension reduction is integrated into a single framework. Thus each cluster is more identifiable and facilitates clustering performance. We can therefore safely conclude that discriminative information is beneficial for clustering.\n2. SC outperforms LLC on the YaleB and USPS datasets, while LLC outperforms SC on all those remaining. That is, CLGR achieves better performance on all datasets than both algorithms combined.\n3. SEC obtains the second-best performance over the seven datasets, which indicates that linearity regularization can also facilitate clustering performance. Similar to our algorithm, SEC is capable of dealing with out-of-sample data.\n4. The proposed algorithm SCLP generally outperforms the compared clustering algorithms on the seven benchmark datasets, which demonstrates that manifold regularization-based label propagation is beneficial for spectral clustering.\nTable 3: Performance Comparison(NMI %) of KM, DKM, NNC, SC, LLC, CLGR, SEC and SCLP. The proposed algorithm, SCLP, generally outperforms the compared algorithms, which indicates that manifold regularization-based label propagation is beneficial for spectral clustering.\nKM DKM NNC SC LLC CLGR SEC SCLP\nLUNG 59.1\u00b1 1.8 60.4\u00b1 1.6 59.7\u00b1 1.4 60.0\u00b1 1.7 63.8\u00b1 1.9 64.9\u00b1 2.1 68.2\u00b1 1.5 73.3\u00b1 1.5 PALM 81.8\u00b1 2.3 83.4\u00b1 1.9 82.1\u00b1 2.1 85.6\u00b1 1.8 87.2\u00b1 2.0 87.5\u00b1 1.8 89.3\u00b1 1.6 91.4\u00b1 1.9\nMSRA50 61.7\u00b1 2.4 63.4\u00b1 2.2 62.5\u00b1 1.9 62.8\u00b1 2.5 69.4\u00b1 2.1 68.5\u00b1 1.8 70.9\u00b1 1.7 73.3\u00b1 1.6 FRGC 66.3\u00b1 1.8 68.5\u00b1 1.4 67.8\u00b1 1.7 66.7\u00b1 1.5 68.9\u00b1 1.9 69.2\u00b1 2.1 71.2\u00b1 1.8 73.9\u00b1 1.4\nAR 69.3\u00b1 2.5 73.8\u00b1 2.1 72.1\u00b1 2.4 73.7\u00b1 1.9 74.2\u00b1 2.1 74.6\u00b1 1.7 75.3\u00b1 2.2 77.4\u00b1 1.8 YaleB 17.3\u00b1 1.8 37.2\u00b1 2.2 36.5\u00b1 2.5 35.2\u00b1 1.9 32.8\u00b1 2.4 37.3\u00b1 1.8 67.6\u00b1 1.6 67.1\u00b1 2.3 USPS 60.2\u00b1 1.2 62.4\u00b1 1.7 61.8\u00b1 3.5 63.2\u00b1 2.8 63.9\u00b1 2.4 65.2\u00b1 1.8 69.6\u00b1 2.4 74.4\u00b1 1.9"}, {"heading": "Parameter Sensitivity", "text": "In this section, we study performance variance w.r.t. on the regularization parameters \u03b1 and \u03b2.\nWe use the MSRA50 dataset for these experiments. Fig. 2 shows how clustering performance varies w.r.t. different combinations of\u03b1 and \u03b2. We can see that better performance occurs when \u03b1 and \u03b2 are comparable."}, {"heading": "Conclusion", "text": "In this paper, we have proposed a novel improved spectral clustering algorithm SCLP. Most of the existing spectral clustering algorithms are based on Gaussian matrices or LLE, each of which are extremely sensitive to parameters. Moreover, the parameters are difficult to tune. We have therefore presented a novel distance-consistent LLE that is parameter-free. The distance-consistent LLE can promise that the edge between closer data points has a greater weight. Utilizing this distance-consistent LLE, we have proposed an improved means of spectral clustering via label propagation. The proposed algorithm takes advantage of label propagation and manifold learning. With label propagation, we can propagate the labels obtained through spectral clustering to other unlabeled data points. By adopting manifold learning, we leverage the manifold structure among data points. Note that our framework can also be readily applied to out-ofsample data. Finally, we have evaluated the clustering per-\n10^\u22126 10^\u22124\n10^\u22122 1\n10^2 10^4\n10^6\n10^\u22126 10^\u22124\n10^\u22122 1 10^2\n10^4 10^6\n0\n0.2\n0.4\n0.6\n0.8\n\u03b1\u03b2\nM A\nP\nFigure 2: Performance variance w.r.t. \u03b1 and \u03b2.\nformance of the proposed algorithm over seven datasets. The experimental results demonstrate that the proposed algorithm consistently outperforms other algorithms to which it is compared."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation 15(6):1373\u20131396", "author": ["Belkin", "M. Niyogi 2003] Belkin", "P. Niyogi"], "venue": null, "citeRegEx": "Belkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2003}, {"title": "Annotating photo collections by label propagation according to multiple similarity cues", "author": ["Luo Cao", "L. Huang 2008] Cao", "J. Luo", "T.S. Huang"], "venue": "In ACM Multimedia,", "citeRegEx": "Cao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2008}, {"title": "A convex formulation for shrunk spectral clustering", "author": ["Chang"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, January", "citeRegEx": "Chang,? \\Q2015\\E", "shortCiteRegEx": "Chang", "year": 2015}, {"title": "Cluster kernels for semisupervised learning", "author": ["Weston Chapelle", "O. Schlkopf 2002] Chapelle", "J. Weston", "B. Schlkopf"], "venue": "In Proc. NIPS,", "citeRegEx": "Chapelle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2002}, {"title": "Sparsity induced similarity measure for label propagation", "author": ["Liu Cheng", "H. Yang 2009] Cheng", "Z. Liu", "J. Yang"], "venue": "In Proc. CVPR,", "citeRegEx": "Cheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2009}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["Guan Dhillon", "I.S. Kulis 2004] Dhillon", "Y. Guan", "B. Kulis"], "venue": "In Proc. ACM SIGKDD,", "citeRegEx": "Dhillon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2004}, {"title": "Adaptive dimension reduction using discriminant analysis and k-means clustering", "author": ["Ding", "C. Li 2007] Ding", "T. Li"], "venue": "In Proc. ICML,", "citeRegEx": "Ding et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2007}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["Filippone"], "venue": "Pattern recognition", "citeRegEx": "Filippone,? \\Q2008\\E", "shortCiteRegEx": "Filippone", "year": 2008}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["Filippone"], "venue": "Pattern Recognition", "citeRegEx": "Filippone,? \\Q2008\\E", "shortCiteRegEx": "Filippone", "year": 2008}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["Belhumeur Georghiades", "A.S. Kriegman 2001] Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. PAMI 23(6):643\u2013660", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "Face recognition using laplacianfaces", "author": ["He"], "venue": "IEEE Trans", "citeRegEx": "He,? \\Q2004\\E", "shortCiteRegEx": "He", "year": 2004}, {"title": "Algorithms for clustering data", "author": ["Jain", "A.K. Dubes 1988a] Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1988}, {"title": "Algorithms for clustering data", "author": ["Jain", "A.K. Dubes 1988b] Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1988}, {"title": "Data clustering: a review. ACM computing surveys 31(3):264\u2013323", "author": ["Murty Jain", "A.K. Flynn 1999] Jain", "M.N. Murty", "P.J. Flynn"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1999}, {"title": "Correlated label propagation with application to multi-label learning", "author": ["Jin Kang", "F. Sukthankar 2006] Kang", "R. Jin", "R. Sukthankar"], "venue": "In Proc. CVPR,", "citeRegEx": "Kang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2006}, {"title": "Manifold-based similarity adaptation for label propagation", "author": ["Karasuyama", "M. Mamitsuka 2013] Karasuyama", "H. Mamitsuka"], "venue": "In Proc. NIPS,", "citeRegEx": "Karasuyama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karasuyama et al\\.", "year": 2013}, {"title": "Discriminative cluster analysis", "author": ["Fernando la Torre", "D. Kanade 2006] la Torre", "Fernando", "T. Kanade"], "venue": "In Proc. ICML,", "citeRegEx": "Torre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Torre et al\\.", "year": 2006}, {"title": "The ar face database", "author": ["Martinez", "A.M. Benavente 1998] Martinez", "R. Benavente"], "venue": "Technical report,", "citeRegEx": "Martinez et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Martinez et al\\.", "year": 1998}, {"title": "Spectral embedded clustering", "author": ["Nie"], "venue": "In Proc. IJCAI,", "citeRegEx": "Nie,? \\Q2009\\E", "shortCiteRegEx": "Nie", "year": 2009}, {"title": "Flexible manifold embedding: A framework for semisupervised and unsupervised dimension reduction", "author": ["Nie"], "venue": "IEEE Trans. Image Process", "citeRegEx": "Nie,? \\Q2010\\E", "shortCiteRegEx": "Nie", "year": 2010}, {"title": "Overview of the face recognition grand challenge", "author": ["Phillips"], "venue": null, "citeRegEx": "Phillips,? \\Q2005\\E", "shortCiteRegEx": "Phillips", "year": 2005}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "S.T. Saul 2000] Roweis", "L.K. Saul"], "venue": "Science", "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["Shi", "J. Malik 2000] Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence", "citeRegEx": "Shi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2000}, {"title": "Gene expression correlates of clinical prostate cancer behavior. Cancer cell 1(2):203\u2013209", "author": ["Singh"], "venue": null, "citeRegEx": "Singh,? \\Q2002\\E", "shortCiteRegEx": "Singh", "year": 2002}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions. Machine Learning Research 3:583\u2013617", "author": ["Strehl", "A. Ghosh 2003] Strehl", "J. Ghosh"], "venue": null, "citeRegEx": "Strehl et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2003}, {"title": "Label propagation through linear neighborhoods", "author": ["Wang", "F. Zhang 2008] Wang", "C. Zhang"], "venue": "IEEE Trans. Know. Data Engin", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Clustering with local and global regularization", "author": ["Zhang Wang", "F. Li 2009] Wang", "C. Zhang", "T. Li"], "venue": "IEEE Trans. Know. Data Engin", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "A local learning approach for clustering", "author": ["Wu", "M. Schlkopf 2006] Wu", "B. Schlkopf"], "venue": "In Proc. NIPS,", "citeRegEx": "Wu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2006}, {"title": "Learning bregman distance functions for semisupervised clustering", "author": ["Wu"], "venue": "IEEE Trans. Knowledge and Data Eng", "citeRegEx": "Wu,? \\Q2012\\E", "shortCiteRegEx": "Wu", "year": 2012}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Yang"], "venue": "IEEE Trans. Image Process", "citeRegEx": "Yang,? \\Q2010\\E", "shortCiteRegEx": "Yang", "year": 2010}, {"title": "Nonnegative spectral clustering with discriminative regularization", "author": ["Yang"], "venue": "In Proc. AAAI", "citeRegEx": "Yang,? \\Q2011\\E", "shortCiteRegEx": "Yang", "year": 2011}, {"title": "Adaptive distance metric learning for clustering", "author": ["Zhao Ye", "J. Liu 2007a] Ye", "Z. Zhao", "H. Liu"], "venue": "In Proc. CVPR,", "citeRegEx": "Ye et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2007}, {"title": "Adaptive distance metric learning for clustering", "author": ["Zhao Ye", "J. Liu 2007b] Ye", "Z. Zhao", "H. Liu"], "venue": "In Proc. CVPR,", "citeRegEx": "Ye et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2007}, {"title": "Discriminative k-means for clustering", "author": ["Zhao Ye", "J. Wu 2007] Ye", "Z. Zhao", "M. Wu"], "venue": "In Proc. NIPS,", "citeRegEx": "Ye et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2007}, {"title": "Multiclass spectral clustering", "author": ["Yu", "S.X. Shi 2003] Yu", "J. Shi"], "venue": "In Proc. ICCV,", "citeRegEx": "Yu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2003}, {"title": "Self-tuning spectral clustering", "author": ["Zelnik-Manor", "L. Perona 2004] Zelnik-Manor", "P. Perona"], "venue": "In Proc. NIPS,", "citeRegEx": "Zelnik.Manor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zelnik.Manor et al\\.", "year": 2004}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Zhu", "X. Ghahramani 2002] Zhu", "Z. Ghahramani"], "venue": "Technical report,", "citeRegEx": "Zhu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2002}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 3, Computer Science, University of Wisconsin-Madison", "citeRegEx": "Zhu,? \\Q2006\\E", "shortCiteRegEx": "Zhu", "year": 2006}], "referenceMentions": [{"referenceID": 37, "context": "Hence, F can be obtained as follows (Zhu 2006):", "startOffset": 36, "endOffset": 46}], "year": 2017, "abstractText": "Spectral clustering is a key research topic in the field of machine learning and data mining. Most of the existing spectral clustering algorithms are built upon Gaussian Laplacian matrices, which are sensitive to parameters. We propose a novel parameter-free, distanceconsistent Locally Linear Embedding (LLE). The proposed distance-consistent LLE promises that edges between closer data points have greater weight. Furthermore, we propose a novel improved spectral clustering via embedded label propagation. Our algorithm is built upon two advancements of the state of the art: 1) label propagation, which propagates a node\u2019s labels to neighboring nodes according to their proximity; and 2) manifold learning, which has been widely used in its capacity to leverage the manifold structure of data points. First we perform standard spectral clustering on original data and assign each cluster to k-nearest data points. Next, we propagate labels through dense, unlabeled data regions. Extensive experiments with various datasets validate the superiority of the proposed algorithm compared to current state-of-the-art spectral algorithms.", "creator": "LaTeX with hyperref package"}}}