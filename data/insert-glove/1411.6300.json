{"id": "1411.6300", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions", "abstract": "In a Bayesian internationalism network, weve we perkes wish 43-20 to 18:48 evaluate the tianwei marginal layup probability of jakey a query thailands variable, which may manatuto be conditioned on jingle the observed carcaterra values zamanbek of some potted evidence elizabet variables. allodynia Here phile we gebeng first 13-track present andriano our \" border snezana algorithm, \" open-pit which 4,555 converts boek a BN into connectedness a ktvi directed chain. zygomorphic For the marceau polytrees, we then crumble present in line-by-line details, with swyche some 4pts modifications and saudargas within pulite the lahlou border algorithm framework, filmographies the \" 56kg revised mortadella polytree algorithm \" by rnk Peot & euro383 amp; Shachter (gaile 1991 ). Finally, minus-10 we earth-two present our \" coursework parentless polytree gtmo method, \" which, coupled meteors with terrible the hathcock border algorithm, converts any kneipp Bayesian rag network imagistic into a 2.705 polytree, rendering krister the paellmann complexity waterskiing of our rogerio inferences cobol independent of 2,382 the size of 793 network, gemarkung and linear kersting with nano the number hiphopdx of cottrell its 120.60 evidence tanno and query variables. ghinva All gratification quantities dongjin in stringbean this unics paper f2004 have probabilistic 38.11 interpretations.", "histories": [["v1", "Sun, 23 Nov 2014 21:19:44 GMT  (1591kb,D)", "http://arxiv.org/abs/1411.6300v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["do le paul minh"], "accepted": false, "id": "1411.6300"}, "pdf": {"name": "1411.6300.pdf", "metadata": {"source": "CRF", "title": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions", "authors": [], "emails": ["dminh@fullerton.edu"], "sections": [{"heading": null, "text": "Keywords: Bayesian networks; Exact inference; Border algorithm; Revised polytree algorithm; Parentless polytree method"}, {"heading": "1 The Bayesian Networks (BNs)", "text": "Consider a directed graph G defined over a set of ` nodes V = {V1, V2, ..., V`}, in which each node represents a variable. (We denote both a variable and its corresponding node by the same notation, and use the two terms interchangeably.) The pairs of nodes (Vi, Vj) may be connected by either the\nar X\niv :1\n41 1.\n63 00\nv1 [\ncs .A\ndirected edge Vi \u2192 Vj or Vj \u2192 Vi, but not both. It is not necessary that all pairs be connected in this manner. In this paper, we will first use the graph in Figure 1 as an example.\nFor node V \u2208 V , we call\n1. the nodes sending the directed edges to V the \u201cparents\u201d of V . We denote the set of the parents of V by HV . In Figure 1, HH = {C,D}. A node is said to be a \u201croot\u201d if it has no parents. (For example, nodes A, B and G.)\n2. the nodes receiving the directed edges from V the \u201cchildren\u201d of V . We denote the set of the children of V by LV . In Figure 1, LD = {H, I}. A node is said to be a \u201cleaf\u201d if it has no children. (For example, nodes J , K and L.) We also call the parents and children of V its \u201cneighbors.\u201d\n3. the parents of the children of V , except V , the \u201cco-parents\u201d of V . We denote the set of the co-parents of V by KV = {\u222a\u03b7\u2208LVH\u03b7} \\V . (We denote by X\\Y the set {X : X \u2208 X , X /\u2208 Y}. X\\Y = \u2205 iff X \u2286 Y .) In our example, KD = {C,F}.\nThe set of edges connecting nodes Vi and Vj either directly or via other nodes Vk, ..., Vm in the form of Vi \u2192 Vk \u2192 ... \u2192 Vm \u2192 Vj is called a \u201c directed path\u201d from Vi to Vj. We restrict ourselves to the \u201cdirected acyclic\ngraph\u201d (DAG) in which there is no directed path that starts and ends at the same node. If there is a directed path from Vi to Vj, we say Vi is an \u201cancestor\u201d of Vj and Vj a \u201cdescendant\u201d of Vi. Let NV andMV be the set of all ancestors and descendants of V , respectively. In Figure 1 , NI = {A,B,D, F},MC = {H, J,K}.\nThe \u201cMarkovian assumption\u201d of a DAG is that every variable is conditionally independent of its non-descendants given its parents. Attached to each node V \u2208 V is a conditional probability distribution Pr {V |HV }. If a node has no parent, its distribution is unconditional. We assume in this paper that all V \u2208 V are discrete, and all conditional probability distributions are in the form of the conditional probability tables (CPTs), taking strictly positive values. We assume that the \u201csize\u201d of Pr {V |HV } (that is, the number of possible values of V and HV ) is finite for all V \u2208 V .\nA \u201cBayesian network\u201d (BN) is a pair (G,\u0398), where G is a DAG over a set of variables V = {V1, V2, ..., V`} (called the \u201cnetwork structure\u201d) and \u0398 a set of all CPTs (called the \u201cnetwork parametrization\u201d). We will refer to the DAG in Figure 1 and its parametrization the Bayesian network A, or the BN A.\nIt has been shown that the dependence constraints imposed by G and the numeric constraints imposed by \u0398 result in the unique joint probability distribution,\nPr {V} = Pr {V1, V2, ..., V`} = \u220f V \u2208V Pr {V |HV } . (1)\nThis equation is known as the \u201cchain rule for Bayesian networks\u201d (Pearl, 1987, Equation 3). In our example,\nPr {A = a,B = b, C = c, ..., L = `} = Pr {A = a}Pr {B = b}Pr {C = c|A = a,B = b} ...Pr {L = `|I = i} ."}, {"heading": "1.1 The Marginal Distribution", "text": "We wish to evaluate the marginal probability Pr {Q}, in which Q \u2208 V is known as a \u201cquery variable.\u201d This probability may be conditioned on the fact that some other variables in V are observed to take certain values.\nSuppose f is a function defined over a set of variables X \u2286 V . We say the \u201cscope\u201d of f is X . We list out the scope if necessary, such as f (X ); if not, we simply write f (\u00b7).\nIn this paper, suppose X = {Y ,Z} \u2286 V where Y\u2229Z = \u2205 and Y = {Y1, ..., Yn}. We express Pr {X} as Pr {Y ,Z}. Given Pr {Y ,Z}, \u201csumming out\u201d (or \u201celiminating\u201d) Y from Pr {Y ,Z} means obtaining Pr {Z} as follows: For every fixed Z = z,\u2211\nY\nPr {z,Y}\n= \u2211 Y1 ... \u2211 Yn\u22121 (\u2211 Yn Pr {z, Y1 = y1, ..., Yn\u22121 = yn\u22121, Yn = yn} ) = \u2211 Y1 ... \u2211 Yn\u22121 Pr {z, Y1 = y1, ..., Yn\u22121 = yn\u22121}\n = Pr {z} . We write, \u2211\nY\nPr {Z,Y} = Pr {Z} . (2)\nOne way to evaluate the marginal probability Pr {Vj} is to use Equation (1) to calculate the joint probability Pr {V1, ..., V`}, then sum out all variables in {V1, ..., Vj\u22121, Vj+1, ..., V`}. This brute-force method is known to be NPhard; that is, there is often an exponential relationship between the number of variables ` and the complexity of computations (Cooper, 1990). Thus it may be infeasible for large networks.\nThere have been many attempts in the literature to find the most efficient methods to calculate Pr {Q}. They can be divided into two broad categories: the approximate and the exact methods. One example of the approximate methods is using Gibbs samplings to generate \u201cvariates\u201d (or \u201cinstantiations\u201d) for V , then using statistical techniques to find an estimate for Pr {Q}. (See Pearl, 1987.) In this paper, we present a method to compute Pr {Q} exactly, apart from precision or rounding errors.\nGuo & Hsu (2002) did a survey of the exact algorithms for the BNs, including the two most well-known ones, namely the variable eliminations (Zhang & Poole, 1996; Dechter, 1999) and the clique-tree propagations (Lauritzen & Spiegelhalter, 1988; Lepar & Shenoy, 1999). Other methods reviewed were the message propagations in polytrees (Kim & Pearl, 1983; Pearl 1986a, 1986b), loop cutset conditioning (Pearl, 1986b; D\u0131\u0301ez, 1996), arc reversal/node reduction (Shachter, 1990), symbolic probabilistic inference (Shachter et al., 1990) and differential approach (Darwiche, 2003). We\nalso want to mention the more recent LAZY propagation algorithm (Madsen & Jensen, 1999).\nIn this paper, we first present the border algorithm. Like the clique-tree propagation, instead of obtaining the joint probability Pr {V1, ..., V`}, the border algorithm breaks a Bayesian network into smaller parts and calculate the marginal probabilities of these parts, avoiding the exponential blow-ups associated with large networks. In the next section, we first show how a BN can be so divided, in such a way that its independency structure can be exploited. In Section 3, we explain how to calculate the marginal probability of each part when there is no observed evidence. In Section 4, we show how to calculate them, conditional on some observed evidences.\nIn Section 5, we focus on a special kind of BN called the \u201cpolytrees,\u201d and present in details, with some modifications and within the border algorithm framework, the \u201crevised polytree algorithm\u201d by Peot & Shachter (1991).\nIn Section 6, we present our parentless polytree method, which, coupled with the border algorithm, can convert any BN into a polytree. This part is static, in that they need to be done only once, off-line, prior to any dialogue with a user. Then we show the dynamic, on-line part of our method, in which the conditional marginal probabilities can be calculated whenever new evidences are entered or queries posed.\nFinally, our discussions and summary are presented in Section 7."}, {"heading": "2 Partitioning a DAG", "text": "In this section, we will show how a BN can be partitioned into smaller parts."}, {"heading": "2.1 The Set Relationships", "text": "Consider a non-empty set of nodes X \u2286 V . We also call\n1. HX = {\u222aV \u2208XHV } \\X the \u201cparent\u201d of X . If HX = \u2205, we say X is \u201cparentless\u201d (or \u201cancestral\u201d). For the BN A, H{A,H} = {C,D}.\n2. LX = {\u222aV \u2208XLV } \\ {X ,HX} the \u201cchild\u201d of X . If LX = \u2205, we say X is \u201cchildless.\u201d For the BN A, L{A,H} = {J,K}. (Although D is a child of A, it is also a parent of H; so it is a member of H{A,H}, not of L{A,H}.)\n3. KX = {\u222aV \u2208XKV } \\ {X ,HX ,LX} the \u201cco-parent\u201d of X . If a child of V \u2208 X is also in X , then all its parents are in {X ,HX}. Thus we are only concerned with the children of V in LX . KX therefore can also be defined as {\u222aV \u2208LXHV } \\ {X ,LX}. For BN A, K{A,H} = {B,G, I}. If KX = \u2205, we say X is \u201cco-parentless.\u201d"}, {"heading": "2.2 The Growing Parentless Set", "text": "Consider a parentless set P \u2286 V . It is \u201cgrowing\u201d when it \u201crecruits\u201d new members. There are simple algorithms in the literature that allow P to recruit a member in a \u201ctopological order\u201d (that is, after all its parents), so that it is always parentless. (For example, Koller & Friedman, 2009, p. 1146.) Let us call D = V\\P the \u201cbottom part\u201d of the BN. We present here an algorithm that not only constructs a growing parentless P , but also divides P into two parts: P = {A,B}, where A is called the \u201ctop part,\u201d and B the \u201cborder\u201d that separates A from D. It will become clear later why we wish to keep the size of border B (that is, the number of possible values of B and HB) as small as possible.\nWe call the members of A, B and D the \u201ctop variables,\u201d the \u201cborder variables,\u201d and the \u201cbottom variables\u201d, respectively.\nFor the initial top part A, we start with A = \u2205. We use a co-parentless set of roots as the initial border B. There is at least one such set of roots in a BN. (Suppose a set of roots has a non-root co-parent. Then if we trace through the ancestors of this co-parent, we must encounter another set of roots. Again, if this set is not co-parentless, we trace up further. Eventually, we must see a co-parentless set of roots in a finite BN.) In our example, none of the roots is co-parentless, but the set {A,B} is.\nAll bottom variables will eventually join P . However, we do not choose which bottom variable to join next. This method does not give us control over the membership of the top part A. Instead, we first decide which variable in B is to be \u201cpromoted\u201d to A. The promotion of a variable B \u2208 B may leave a \u201chole\u201d in the border B; thus B no longer separates A from D. This necessitates recruiting some bottom variables into B to fill that hole, allowing P to grow. We call the set of the bottom variables that are recruited into border B upon the promotion of node B the \u201ccohort\u201d of B and denote it by C. To fill the hole, C must include at least the part of the \u201cMarkov blanket\u201d of B (that is, all its children and co-parents) in D. C may be empty, or may be more than what we need for B to separate A and D.\nFor P to remain parentless, it is necessary that HC \u2286 P . Because all members of C are separated from A by B, C cannot have any parent in A. So we only need HC \u2286 B. Below are the many ways by which we can choose the next variable B \u2208 B to promote to A, approximately in order of preference to keep the size of B small:\n1. B has no bottom children, hence no bottom co-parent. Then C = \u2205.\n2. B has bottom children, but no bottom co-parent. Then C = LB \u2229 D. (This is why we start B with a co-parentless set of roots.)\n3. B has bottom co-parents, which are roots or have no bottom parents. Then C = {LB \u222a KB} \u2229 D. In Figure 2, variable J (having co-parent N , with HN = K /\u2208 D) can be promoted with cohort {N,O}. Variable H can also be promoted with cohort {M,L}, because its co-parent L is a root.\n4. B is a fictitious variable \u2205, the cohort of which is a bottom variable having all parents in P . In Figure 2, we can recruit variable V (resulting in new border {K, J, I,H, V }).\n5. B is a fictitious variable \u2205, the cohort of which is a bottom root. In Figure 2, we can recruit root P (resulting in new border {K, J, I,H, P}) or root S (resulting in new border {K, J, I,H, S}).\n6. B is any variable in B, the cohort of which includes not only its bottom children, but also all their bottom ancestors (hence the bottom coparents of B and perhaps some roots). In Figure 2, I can be promoted with cohort {V,W,U, S}.\n7. B is a fictitious variable \u2205, the cohort of which includes any bottom variable, together with all its bottom ancestors. Unless it is necessary, the worst (but legal) strategy is to bring all bottom variables simultaneously into B.\n8. B is a fictitious variable \u2205, the cohort of which is the whole parentless set P\u2217 \u2286 D. This is equivalent to \u201cmerging\u201d P and P\u2217. In Figure 2, we can merge P = {A,B} with the parentless P\u2217 = {P,Q,R, S, T, U}. If P\u2217 has already been divided into the top part A\u2217 and the border B\u2217, then A\u2217 can be merged with A and B\u2217 with B. (This is equivalent to\nsimultaneously promoting all members of A\u2217 after merging P and P\u2217.) In Figure 2, if B\u2217 = {U, T}, then the new border is {K, J, I,H, U, T}.\nWe continue to use the notations such as P , A, B and D. However, we also define the initial top part as A0 = \u2205, and denote the initial border, comprising of a co-parentless set of roots, by B0.\nAt \u201ctime\u201d j \u2265 1, the variable promoted to Aj\u22121 (which may be a fictitious variable \u2205) is re-named as Vj; the resulting top part becomes Aj. Thus, for all j \u2265 1,\nAj = {Aj\u22121, Vj} = {V1, ..., Vj} . (3)\nLet C0 = B0. For all j \u2265 1, the cohort associated with Vj is re-named as Cj. After promoting Vj \u2208 Bj\u22121 and recruiting Cj, the resulting border is, for all j \u2265 1,\nBj = {Bj\u22121\\Vj, Cj} , (4)\nwith HCj \u2286 Bj\u22121. Let P0 = B0. The parentless set P grows cohort-by-cohort as, for all j \u2265 1,\nPj = {Aj,Bj} = {Aj\u22121, Vj} \u222a {Bj\u22121\\Vj, Cj} = {Pj\u22121, Cj} = \u222ajk=0Ck. (5)\nEventually, all variables in V will join P . Let \u03b3 be the time that this happens. We call B\u03b3 the \u201clast\u201d border. Then V is partitioned into disjoint sets as V = P\u03b3 = \u222a\u03b3k=0Ck.\nLet the bottom part at time j (0 \u2264 j \u2264 \u03b3) be\nDj = V\\Pj = P\u03b3\\Pj = \u222a\u03b3k=j+1Ck = {Cj+1,Dj+1} . (6)\nThe above promotion rules do not result in a unique promotion order; and we do not attempt to optimize here, so that the maximum size of all borders Bi (i = 1, 2, ..., \u03b3) is as small as possible. We can heuristically search among all members of Bj to identify the node whose promotion leads to the smallest next border Bj+1, but this does not guarantee a global minimum. The above order of preference may help.\nWe show the results obtained by one particular promotion order for the BN A in Table 1. The last column shows the rule we use to promote Vi. The function \u03a6 (Ci) will be introduced later.\nTo keep {Pi, i = 1, 2, ..., \u03b3} parentless, some borders Bi may have more members than what required to separate Ai and Di. Rule 1 is useful in this case, to reduce the membership of Bi to its minimum. For example, it was used to reduce B4 = {F,H, I} to B5 = {H, I}.\nHere we construct a directed chain of possibly overlapping borders {Bi, i = 1, 2, ..., \u03b3}, called the \u201cborder chain.\u201d A border chain is Markovian, in the sense that the knowledge of Bj is sufficient for the study of Bj+1. Figure 3 shows the corresponding border chain for the BN A."}, {"heading": "3 Inferences without Evidences", "text": "In this section, we explain how the \u201cprior marginal probability\u201d Pr {Bi} can be calculated, assuming that no variable is observed taking any value."}, {"heading": "3.1 The Parentless Set Probabilities", "text": "We first present the following important lemma, which is based on a simple observation that in a BN, a parentless set of nodes and its parametrization is a Bayesian sub-network :\nLemma 1 If P \u2286 V is parentless, then Pr {P} = \u220f V \u2208P Pr {V |HV } .\nProof. The lemma follows from Equation (1) For the BN A, as {A,B,D} is parentless,\nPr {A,B,D} = Pr {A}Pr {B}Pr {D|A,B} .\nWe do not use {A,B,D} however, because D alone does not separate {A,B} from the rest of the network. So for a general BN, we start with the parentless P0 = B0 = C0 and define\n\u03a6 (C0) = Pr {P0} = Pr {B0} = Pr {C0} = \u220f V \u2208B0 Pr {V } . (7)\nRecall that, when Vj is promoted, it brings a cohort Cj into Bj\u22121. By the Markovian assumption, we do not need the whole Pr {Cj|Pj\u22121}, but only Pr { Cj|HCj } with HCj \u2286 Bj\u22121 \u2286 Pj\u22121. For all 0 \u2264 j \u2264 \u03b3, let us denote\nthe \u201ccohort probability tables\u201d Pr { Cj|HCj } by \u03a6 (Cj). If Cj = \u2205, we set \u03a6 (Cj) = 1. Column 5 of Table 1 shows the cohort probability tables for the BN A.\nTheorem 2 For all 0 \u2264 j \u2264 \u03b3,\nPr {Pj} = \u03a6 (Cj) Pr {Pj\u22121} = j\u220f\nk=0\n\u03a6 (Ck) .\nProof. From Equation (5),\nPr {Pj} = Pr {Pj\u22121, Cj} = Pr {Cj|Pj\u22121}Pr {Pj\u22121} = Pr { Cj|HCj } Pr {Pj\u22121} = \u03a6 (Cj) Pr {Pj\u22121} .\nThe theorem follows because Pr {P0} = \u03a6 (C0). Theorem 2 can be used to obtain the joint probability of Pj when j is small. For example,\nPr {P1} = Pr {A,B,C,D, F} = \u03a6 (C0) \u03a6 (C1) .\nHowever, as P grows, eventually we return to Equation (1): Pr {V} = Pr {P\u03b3} = \u220f\u03b3 k=0 \u03a6 (Ck), which is what we did not want to use in the first place. Fortunately, as we will see in the next section, what we have here is a very \u201ccruel\u201d parentless set of nodes that, after promoting and extracting information from a member, immediately \u201celiminates\u201d that member!"}, {"heading": "3.2 The Border Probabilities", "text": "We now show how Pr {Bj} can be recursively calculated from Pr {Bj\u22121}:\nTheorem 3 For all 1 \u2264 j \u2264 \u03b3, Pr {Bj} = \u2211 Vj \u03a6 (Cj) Pr {Bj\u22121} .\nProof. For all Pj (1 \u2264 j \u2264 \u03b3), our strategy is not to eliminate all members of Aj at the same time, in the form of\nPr {Bj} = \u2211 Aj Pr {Aj,Bj} = \u2211 Aj Pr {Pj} = \u2211 Aj \u03a6 (Cj) Pr {Pj\u22121} .\nRather, we eliminate the variables in Aj one-by-one: After variable Vj is promoted into Aj = {Aj\u22121, Vj}, it is immediately eliminated. In other words,\nbecause the scope of \u03a6 (Cj) (which is { Cj,HCj } \u2286 {Cj,Bj\u22121}) does not include any member of Aj\u22121, as far as the summing out of Aj\u22121 is concerned, \u03a6 (Cj) can be treated as a constant:\nPr {Bj} = \u2211\n{Aj\u22121,Vj} \u03a6 (Cj) Pr {Pj\u22121} = \u2211 Vj \u03a6 (Cj) \u2211 Aj\u22121 Pr {Aj\u22121,Bj\u22121}  , hence the theorem.\nThere must be one value of \u03c4 (0 \u2264 \u03c4 \u2264 \u03b3) such that Pr {B\u03c4} can be calculated. At least, from Equation (7), we know Pr {B0}. Starting with Pr {B\u03c4}, we can calculate Pr {Bj} for all \u03c4 < j \u2264 \u03b3 recursively by the above theorem.\nWe call our algorithm the \u201cborder algorithm\u201d because it breaks the large joint probability Pr {V} down into many smaller border probabilities Pr {Bj}, thus avoiding the exponential blow-ups associated with large networks. That is why we want the size of the largest border to be as small as possible.\nWe now show how the marginal probabilities can be obtained given some evidences."}, {"heading": "4 Inferences with Evidences", "text": "For a variable V \u2208 V , let Va (V ) be the set of possible values of V such that Pr {V = v|HV } > 0. A variable E is said to be an \u201cevidence variable\u201d if it is observed taking value only in a subset Vae (E) \u2282 Va (E). Variable V is nonevidential if Vae (V ) = Va (V ). For example, suppose Va (X) = {1, 2, 3}. If X is observed not taking value 3, then it is evidential with Vae (X) = {1, 2}. Let E be the set of all evidence variables.\nConsider set Y = {Y1, ..., Yn} \u2286 V . We denote the event that Y occurs by\n[Y ] = {Y \u2208 Vae (Y1)\u00d7 ...\u00d7 Vae (Yn)} .\nIf Y \u2229 E = \u2205, [Y ] is a sure event. Thus [Y ] = [Y \u2229 E ]. One of the most important tasks in analyzing a BN is to calculate Pr {Q| [E ]}, which is known as the \u201cposterior marginal distribution\u201d (or the \u201cconditional marginal distribution\u201d) of a \u201cquery variable\u201d Q.\nFor the rest of this paper, we will show how we can first calculate the joint distribution table Pr {Q, [E ]} for all possible values of Q. This allows\nus to calculate Pr {[E ]} = \u2211\nQ Pr {Q, [E ]} and then\nPr {Q| [E ]} = Pr {Q, [E ]}\u2211 Q Pr {Q, [E ]} = Pr {Q, [E ]} Pr {[E ]} ."}, {"heading": "4.1 The Evidence Indicator Columns", "text": "Consider a table having t rows, each row corresponding to an instantiation of a set of variables X \u2286 V . If an evidence variable E is in X , we define an \u201cevidence indicator column\u201d IE having size t, such that it takes value 1 if E \u2208 Vae (E), and 0 otherwise. For a non-evidence variable V \u2208 X , we also define the column IV having size t, all members of which are 1.\nWe will use the following notation for a set of nodes X = Y \u222a Z \u2286 V :\nIX = \u220f V \u2208X IV = IY\u222aZ = IYIZ .\nMultiplying a table having scope X with column IX is equivalent to zeroing out the rows inconsistent with the evidences in X .\nFor the CPT Pr {V |HV }, we define its \u201creduced CPT\u201d as:\nPrr {V |HV } = Pr {V |HV } IV \u222aHV . (8)\nPreviously we defined the cohort probability tables \u03a6 (Cj) = Pr { Cj|HCj } for all 0 \u2264 j \u2264 \u03b3. We now define the \u201creduced cohort probability tables\u201d as\n\u03c6 (Cj) = \u03a6 (Cj) ICj\u222aHCj .\nThe following lemma is the evidential version of Equation (2):\nLemma 4 Given Y ,Z \u2286 V, Y \u2229 Z = \u2205, then with scope Z,\nPr {Z, [Y ]} = \u2211 Y Pr {Z,Y} IY .\nProof. If Y \u2229 E = \u2205, we have Equation (2) because IY = 1 and Pr {Z, [Y ]} = Pr {Z}. Suppose Y = {Y1, ..., Yn} is observed taking value in Vae (Y1)\u00d7 ...\u00d7\nVae (Yn). For every fixed Z = z, summing out Y yields:\u2211 Y Pr {z,Y} IY\n= \u2211 Y1 ... \u2211 Yn\u22121 (\u2211 Yn Pr {z, Y1 = y1, ..., Yn\u22121 = yn\u22121, Yn = yn} n\u220f i=1 IYi ) = \u2211 Y1 ... \u2211 Yn\u22121 Pr {z, Y1 = y1, ..., Yn\u22121 = yn\u22121, Yn \u2208 Vae (Yn)} n\u22121\u220f i=1 IYi\n = \u2211 Y1 Pr {z, Y1 = y1, Y2 \u2208 Vae (Y2) , ..., Yn \u2208 Vae (Yn)} IY1 = Pr {z, [Y ]} .\nWe are now ready to obtain the necessary information for the calculations of Pr {Q, [E ]}."}, {"heading": "4.2 The Downward Pass for the Top Evidences", "text": "We first consider the \u201ctop evidences\u201d within the top part Aj, and define the following notation: For all 0 \u2264 j \u2264 \u03b3, by Lemma 4,\n\u03a0 (Bj) = Pr {Bj, [Aj]} IBj = \u2211 Aj Pr {Bj,Aj} IAj\u222aBj = \u2211 Aj Pr {Pj} IPj . (9)\nThe following theorem is the evidential version of Theorem 3:\nTheorem 5 For all 1 \u2264 j \u2264 \u03b3, \u03a0 (Bj) = \u2211 Vj \u03c6 (Cj) \u03a0 (Bj\u22121) .\nProof. Because HCj \u2286 Pj\u22121, we have Pj = Cj \u222a Pj\u22121 = Cj \u222a HCj \u222a Pj\u22121. From Definition (9) and Theorem 2,\n\u03a0 (Bj) = \u2211 Aj Pr {Pj} IPj = \u2211 Aj \u03a6 (Cj) ICj\u222aHCj Pr {Pj\u22121} IPj\u22121\n= \u2211 Aj \u03c6 (Cj) Pr {Pj\u22121} IPj\u22121 .\nFrom Equation (3), and as the scope of \u03c6 (Cj) (which is { Cj,HCj } ) is not in Aj\u22121,\n\u03a0 (Bj) = \u2211 Vj \u03c6 (Cj) \u2211 Aj\u22121 Pr {Pj\u22121} IPj\u22121 = \u2211 Vj \u03c6 (Cj) \u03a0 (Bj\u22121) .\nThere must be one value of \u03bd (0 \u2264 \u03bd \u2264 \u03b3) such that \u03a0 (B\u03bd) can be calculated. Let \u03b1 be the first time an evidence variable is recruited into P . For all 0 \u2264 j < \u03b1, Aj has no evidence and IBj = 1; thus \u03a0 (Bj) = Pr {Bj}. A\u03b1 also has no evidence and thus,\n\u03a0 (B\u03b1) = Pr {B\u03b1} IB\u03b1 . (10)\nStarting with \u03a0 (B\u03bd), we can calculate \u03a0 (Bj) recursively for all \u03bd < j \u2264 \u03b3 by the above theorem.\nFor the BN A, assume E = {H = h,K = k}. Since H \u2208 C3, \u03b1 = 3. Thus\n\u03a0 (B0) = Pr {B0} = Pr {A,B} ; \u03a0 (B1) = Pr {B1} = Pr {B,C,D, F} ; \u03a0 (B2) = Pr {B2} = Pr {C,D, F} ; \u03a0 (B3) = Pr {D,F,H} IH = Pr {D,F, h} .\n1. Border B4 = {F,H, I} has V4 = D and C4 = I: \u03a0 (B4) = \u2211 D \u03c6 (C4) \u03a0 (B3) = \u2211 D Pr {I|D,F}Pr {D,F, h}\n= \u2211 D Pr {D,F, I, h} = Pr {F, I, h} .\n2. Border B5 = {H, I} has V5 = F and C5 = \u2205: \u03a0 (B5) = \u2211 F \u03c6 (C5) \u03a0 (B4) = \u2211 F Pr {F, I, h} = Pr {I, h} .\n3. Border B6 = {I, J,K,G} has V6 = H = h and C6 = {J,K,G}:\n\u03a0 (B6) = \u03c6 (C6) \u03a0 (B5) = Pr {J |G, h}Pr {k|I, h}Pr {G}Pr {I, h} = Pr {I, J,G, k, h} .\n4. Border B7 = {I, J,K} has V7 = G and C7 = \u2205:\n\u03a0 (B7) = \u2211 G \u03c6 (C7) \u03a0 (B6) = \u2211 G Pr {I, J,G, k, h} = Pr {I, J, k, h} .\n5. Border B8 = {J,K, L} has V8 = I and C8 = L:\n\u03a0 (B8) = \u2211 I \u03c6 (C8) \u03a0 (B7) = \u2211 I Pr {L|I}Pr {I, J, k, h} = Pr {J, L, k, h} ."}, {"heading": "4.3 The Upward Pass for the Bottom Evidences", "text": "Moving downward border-by-border from B0 to Bj, we can only collect information about the top evidences inside Pj. To collect information about the bottom evidences inside Dj, we move upward from the last border B\u03b3 to Bj.\nIn the downward passes, we make use of the parentless property of Pj; in the upward passes we need the fact that the border Bj separates Aj and Dj = V\\Pj. Thus, to study Dj, we do not need the information of the whole Pj, but only of Bj.\nWe first present the following lemma:\nLemma 6 For all 1 \u2264 j \u2264 \u03b3,\nPr {Dj\u22121|Bj\u22121} = \u03b3\u220f k=j \u03a6 (Ck) = \u03a6 (Cj) Pr {Dj|Bj} .\nProof. For all 1 \u2264 j \u2264 \u03b3, as both V = P\u03b3 = {Dj\u22121,Pj\u22121} and Pj\u22121 are parentless, from Theorem 2,\nPr {P\u03b3} = \u03b3\u220f k=0 \u03a6 (Ck) = Pr {Dj\u22121,Pj\u22121} = Pr {Dj\u22121|Pj\u22121}Pr {Pj\u22121}\n= Pr {Dj\u22121|Bj\u22121} j\u22121\u220f k=0 \u03a6 (Ck) .\nAssuming all Pr {V |HV } > 0, Pr {Dj\u22121|Bj\u22121} = \u03b3\u220f k=j \u03a6 (Ck) = \u03a6 (Cj) \u03b3\u220f k=j+1 \u03a6 (Ck) = \u03a6 (Cj) Pr {Dj|Bj} .\nWe define the following notation: For all 0 \u2264 j \u2264 \u03b3 \u2212 1, by Lemma 4, \u039b (Bj) = Pr {[Dj] |Bj} IBj = \u2211 Dj Pr {Dj|Bj} IDj\u222aBj . (11)\nSince D\u03b3 = \u2205, we also define \u039b (B\u03b3) = IB\u03b3 . Although we write \u039b (Bj), the scope of \u039b (Bj) may not be the whole Bj, because Bj may have more variables than the minimal set needed to separate Aj and Dj. For example, in the BN A, while B4 = {F,H, I}, we only need {H, I} for the study of D4 = {G, J,K, L}.\nTheorem 7 For all 1 \u2264 j \u2264 \u03b3, \u039b (Bj\u22121) = \u2211 Cj \u03c6 (Cj) \u039b (Bj) .\nProof. From Equation (6), Dj\u22121 = {Cj,Dj}. From Equation (4), Bj\u22121\u222aCj = Vj \u222a Bj. Also, if Vj 6= \u2205, then its cohort Cj must include all its bottom children, or Vj \u2208 HCj \u2286 Bj\u22121. Thus,\nBj\u22121 \u222a Dj\u22121 = Bj\u22121 \u222a { Cj \u222aHCj } \u222a {Cj \u222a Dj}\n= Vj \u222a Bj \u222aHCj \u222a {Cj \u222a Dj} = Bj \u222aHCj \u222a Cj \u222a Dj.\nFrom Lemma 6, \u039b (Bj\u22121) = \u2211 Dj\u22121 Pr {Dj\u22121|Bj\u22121} IBj\u22121\u222aDj\u22121 = \u2211 {Dj ,Cj} \u03a6 (Cj) ICj\u222aHCj Pr {Dj|Bj} IDj\u222aBj .\nBecause the scope of \u03c6 (Cj) (which is { Cj,HCj } \u2286 {Cj,Pj\u22121} = Pj) is not in Dj, from Equation (6),\n\u039b (Bj\u22121) = \u2211 Cj \u03c6 (Cj) \u2211 Dj Pr {Dj|Bj} IDj\u222aBj = \u2211 Cj \u03c6 (Cj) \u039b (Bj) .\nSuppose there is a value of \u03c9 (1 < \u03c9 \u2264 \u03b3) such that \u039b (B\u03c9) can be calculated. Especially, let \u03b2 be the last time an evidence variable is recruited into P . Then for all \u03b2 \u2264 j \u2264 \u03b3, Dj has no evidence. Hence\n\u039b (Bj) = IBj for all \u03b2 \u2264 j \u2264 \u03b3. (12)\nStarting with \u039b (B\u03c9), we can calculate \u039b (Bj) for all 0 \u2264 j < \u03c9 recursively by the above lemma.\nFor the BN A, with E = {H = h,K = k}. Thus \u03b2 = 6 and \u039b (B8) = \u039b (B7) = \u039b (B6) = IK .\n1. Because B6 has cohort C6 = {J,K,G}: \u039b (B5) = \u2211 C6 \u03c6 (C6) \u039b (B6)\n= Pr {k|h, I} \u2211 G\n( Pr {G}\n\u2211 J Pr {J |G, h}\n) IK = Pr {k|I, h} .\n2. Because B5 has cohort C5 = \u2205:\n\u039b (B4) = \u039b (B5) = Pr {k|I, h} .\n3. Because B4 has cohort C4 = I: \u039b (B3) = \u2211 C4 \u03c6 (C4) \u039b (B4) = \u2211 I Pr {I|D,F}Pr {k|I, h}\n= \u2211 I Pr {k, I|D,F, h} = Pr {k|D,F, h} .\n4. Because B3 has cohort C3 = H = h:\n\u039b (B2) = \u03c6 (C3) \u039b (B3) = Pr {h|C,D}Pr {k|D,F, h} = Pr {h, k|C,D, F} .\n5. Because B2 has cohort C2 = \u2205:\n\u039b (B1) = \u039b (B2) = Pr {h, k|C,D, F} .\n6. Because B1 has cohort C1 = {C,D, F}: \u039b (B0) = \u2211 C1 \u03c6 (C1) \u039b (B1)\n= \u2211 {C,D,F} Pr {C|A,B}Pr {D|A,B}Pr {F |A,B}Pr {h, k|C,D, F}\n= Pr {h, k|A,B} ."}, {"heading": "4.4 The Posterior Marginal Distributions", "text": "Combining the downward and upward passes yields:\nTheorem 8 For all 0 \u2264 j \u2264 \u03b3,\nPr {Bj, [E\\Bj]} IBj = \u03a0 (Bj) \u039b (Bj) .\nProof. By Lemma 4, because the event [E\\Bj] is the same as the event [V\\Bj] = [Aj \u222a Dj],\nPr {Bj, [E\\Bj]} IBj = Pr {Bj, [Aj \u222a Dj]} IBj = \u2211 {Aj ,Dj} Pr {Bj,Aj,Dj} IBj\u222aAj\u222aDj\n= \u2211 {Aj ,Dj} Pr {Aj,Bj}Pr {Dj|Bj} IBj\u222aAj\u222aDj .\nAs Dj \u2229 {Aj,Bj} = \u2205,\nPr {Bj, [E\\Bj]} IBj = \u2211 Aj Pr {Aj,Bj} IAj\u222aBj \u2211 Dj Pr {Dj|Bj} IDj\u222aBj  = \u2211 Aj Pr {Aj,Bj} IAj\u222aBj\u039b (Bj) .\nAs Bj \u2229 Aj = \u2205,\nPr {Bj, [E\\Bj]} IBj = \u039b (Bj) \u2211 Aj Pr {Aj,Bj} IAj\u222aBj = \u03a0 (Bj) \u039b (Bj) .\nCorollary 9 For node Q \u2208 Bj where 0 \u2264 j \u2264 \u03b3,\nPr {Q, [E\\Q]} IQ = \u2211 Bj\\Q \u03a0 (Bj) \u039b (Bj) .\nIf Q /\u2208 E , Pr {Q, [E ]} = \u2211 Bj\\Q \u03a0 (Bj) \u039b (Bj) .\nProof. For node Q \u2208 Bj, Pr {Q, [E\\Q]} IQ = \u2211 E\\Q Pr {E} IE = \u2211 Bj\\Q IBj \u2211 E\\Bj Pr {Bj, E\\Bj} IE\\Bj\n= \u2211 Bj\\Q IBj Pr {Bj, [E\\Bj]} = \u2211 Bj\\Q \u03a0 (Bj) \u039b (Bj) .\nRecall that \u03b2 is the last time an evidence is recruited into P , if we are looking for the \u201cpost-evidence\u201d Pr {Q, [E\\Q]} IQ where Q \u2208 Bj and \u03b2 \u2264 j \u2264 \u03b3, then due to Equation (12), we can find them by the downward pass alone as \u2211 Bj\\Q \u03a0 (Bj) IBj = \u2211 Bj\\Q \u03a0 (Bj).\nFor the BN A with E = {H = h,K = k}, by the downward pass alone we already have\nPr {B8, [E\\B8]} IB8 = \u03a0 (B8) = Pr {J, L, k, h} ; Pr {B7, [E\\B7]} IB7 = \u03a0 (B7) = Pr {I, J, k, h} ; Pr {B6, [E\\B6]} IB6 = \u03a0 (B6) = Pr {I, J,G, k, h} .\nNow with Theorem 8,\n1. \u03a0 (B5) \u039b (B5) = Pr {I, h}Pr {k|I, h} = Pr {I, h, k} .\n2. \u03a0 (B4) \u039b (B4) = Pr {F, I, h}Pr {k|I, h} = Pr {F, I, h, k} .\n3. \u03a0 (B3) \u039b (B3) = Pr {D,F, h}Pr {k|D,F, h} = Pr {D,F, h, k} .\n4. \u03a0 (B2) \u039b (B2) = Pr {C,D, F}Pr {h, k|C,D, F} = Pr {C,D, F, h, k} .\n5. \u03a0 (B1) \u039b (B1) = Pr {B,C,D, F}Pr {h, k|C,D, F} = Pr {B,C,D, F, h, k} .\n6. \u03a0 (B0) \u039b (B0) = Pr {A,B}Pr {h, k|A,B} = Pr {A,B, h, k} .\nA variable V \u2208 V may appear in more than one borders. For example, variable I appears in B4, B5, B6 and B7. We obtain the same result regardless which of these borders we choose to marginalize.\nBorder algorithm is applicable to all BNs. In the next section, we study a special kind of BNs called the polytrees, and present in details, with some modifications and within the border algorithm framework, the \u201crevised polytree algorithm\u201d by Peot & Shachter (1991). This is an important section, because we will show later that, with the help of the border algorithm, any BN can be modified to become a polytree."}, {"heading": "5 The Revised Polytree Algorithm", "text": "A polytree is a BN which is \u201csingly connected;\u201d that is, there is only one undirected path connecting any two nodes. (From now on, \u201cpath\u201d means \u201cundirected path.\u201d) The BN A in Figure 1 is not a polytree because there are 2 paths from A to H, namely A\u2212 C \u2212H and A\u2212D \u2212H.\nIn other words, while we assume all BNs are acyclic (that is, they have no directed cycles), a polytree also does not have any undirected cycle (or \u201cloop\u201d). The BN A has loop A\u2212 C \u2212H \u2212D \u2212 A.\nAs an illustration, we will use the polytree as shown in Figure 4, which we will refer to as the Polytree B."}, {"heading": "5.1 To Find the Path Connecting Two Nodes", "text": "Here we present a method to identify the unique path connecting any two nodes in a polytree.\nWe strategically designate some nodes as \u201chubs,\u201d and pre-load the unique\npath connecting each pair of hubs, excluding the hubs themselves. The bigger the network, the more hubs we need. For the Polytree B, let us pick nodes J and H as hubs, connected by path I \u2212M \u2212D \u2212 C.\nFor each node, we also pre-load the path from it to its nearest hub. For node P the path is P \u2212 I \u2212 J ; for node A the path is A\u2212D \u2212 C \u2212H.\nTo find the path from node X to node Y :\n1. Form the (possibly cyclic) path from X to the hub nearest to X, then to the hub nearest to Y , then to Y . For nodes P and A, this path is (P \u2212 I \u2212 J)\u2212 (I \u2212M \u2212D \u2212 C)\u2212 (H \u2212 C \u2212D \u2212 A).\n2. Replace the largest loop around each hub with its furthest node. With the above path, replace loop I \u2212 J \u2212 I with node I, and loop D \u2212 C \u2212H \u2212 C \u2212D with node D, resulting in path P \u2212 I \u2212M \u2212D \u2212 A connecting nodes P and A."}, {"heading": "5.2 The Decompositions by Nodes", "text": "Node V in a polytree decomposes the polytree into two parts:\n1. PV , the parentless set including V and all the nodes that are connected to V \u201cfrom above,\u201d via its parentsHV . (In Figure 4, PD is in the shaded region.) PV has border V and the top part AV = PV \\V . Consistent with Definition (9), we define\n\u03a0 (V ) = Pr {V, [AV ]} IV = \u2211 AV Pr {PV } IPV . (13)\n2. DV , the bottom set of PV , in which all nodes are connected to V \u201cfrom below,\u201d via its children LV . Consistent with Definition (11), we define\n\u039b (V ) = Pr {[DV ] |V } IV = \u2211 DV Pr {DV |V } IV \u222aDV . (14)"}, {"heading": "5.3 The Decompositions by Edges", "text": "So far, we focused on the nodes in a BN. Let us now consider a typical edge X \u2192 Y . While node X decomposes the polytree into PX and DX , edge X \u2192 Y also decomposes it into two parts:\n1. TX\u2192Y , the parentless set of nodes on the parent side of edge X \u2192 Y , having border X. Not only does TX\u2192Y include PX , but also all the nodes that connect to X from below, except those via Y . In the Polytree B, in addition to PD, TD\u2192M also includes {N,R12, L9, L10}. Consistent with Definition (9), we define the downward message about the evidences in TX\u2192Y that node X can send to its child Y as\n\u03a0Y (X) = Pr {X, [TX\u2192Y \\X]} IX = \u2211\nTX\u2192Y \\X\nPr {TX\u2192Y } ITX\u2192Y . (15)\n2. The bottom set UX\u2192Y = V\\TX\u2192Y , on the child side of edge X \u2192 Y , separated from TX\u2192Y \\X by X. Not only does UX\u2192Y include {Y,DY }, but also all the nodes that connect to Y from above, except those via X. Hence,\nUX\u2192Y = { Y,DY ,\u222aV \u2208HY \\XTV\u2192Y } . (16)\nOn the other hand, TX\u2192Y = { PX ,\u222aV \u2208LX\\Y UX\u2192V } . (17)\nConsistent with Definition (11), we define the upward message about the evidences in UX\u2192Y that node Y can send to its parent X as\n\u039bY (X) = Pr {[UX\u2192Y ] |X} IX = \u2211 UX\u2192Y Pr {UX\u2192Y |X} IX\u222aUX\u2192Y . (18)\nWe will often use the following two properties related to edge X \u2192 Y in a polytree:\n1. Because two distinct parents Z and T of X have X as a common child, the two parentless sets TZ\u2192X and TT\u2192X must be disjoint and independent (otherwise, there are two paths from their common member to Y , via the two parents). Thus,\nPr {\u222aV \u2208HXTV\u2192X} = \u220f\nV \u2208HX\nPr {TV\u2192X} .\n2. Because two distinct children Y and W of X have X as a common parent, the two sets UX\u2192Y and UX\u2192W must be disjoint and independent given X (otherwise, there are two paths from X to their common member, via the two children). Thus,\nPr {\u222aV \u2208LXUX\u2192V |X} = \u220f V \u2208LX Pr {UX\u2192V |X} ."}, {"heading": "5.4 The Message Propagations in the Polytrees", "text": "We now present the lemmas about the relationships among \u03a0 (X), \u039b (X), \u03a0Y (X) and \u039bY (X), which are known in the literature, but are now proven within the border algorithm framework:\nLemma 10 For edge X \u2192 Y in a polytree, \u03a0Y (X) = \u03a0 (X) \u220f\nV \u2208LX\\Y\n\u039bV (X) .\nProof. Consider the parentless PX , having border X. As in Equation (17), recruiting the childless \u222aV \u2208LX\\Y UX\u2192V without promotion results in the parentless TX\u2192Y with border X \u222aV \u2208LX\\Y UX\u2192V . From Theorem 5,\n\u03a0 ( X \u222aV \u2208LX\\Y UX\u2192V ) = \u03a0 (X) Pr { \u222aV \u2208LX\\Y UX\u2192V |X } IX\u222aV \u2208LX\\Y UX\u2192V\n= \u03a0 (X) \u220f\nV \u2208LX\\Y\nPr {UX\u2192V |X} IX\u222aUX\u2192V .\nNow use Rule 1 to promote the childless \u222aV \u2208LX\\Y UX\u2192V without cohort, resulting in the parentless TX\u2192Y having border X. From Theorem 5,\n\u03a0Y (X) = \u2211\n\u222aV \u2208LX\\Y UX\u2192V\n\u03a0 ( X \u222aV \u2208LX\\Y UX\u2192V ) =\n\u2211 \u222aV \u2208LX\\Y UX\u2192V \u03a0 (X) \u220f V \u2208LX\\Y Pr {UX\u2192V |X} IX\u222aUX\u2192V .\nAs the scope of \u03a0 (X) (which is X) is not in \u222aV \u2208LX\\Y UX\u2192V ,\n\u03a0Y (X) = \u03a0 (X) \u220f\nV \u2208LX\\Y ( \u2211 UX\u2192V Pr {UX\u2192V |X} IX\u222aUX\u2192V ) ,\nwhich is the lemma by Definition (18). This is similar to Equation (4.45) in Pearl, 1988.\nLemma 11 For edge X \u2192 Y in a polytree,\n\u039bY (X) = \u2211 Y \u039b (Y ) \u2211 HY \\X Prr {Y |HY } \u220f V \u2208HY \\X \u03a0Y (V ) .\nProof. Consider the parentless TX\u2192Y , having border X and the bottom set UX\u2192Y . As shown in Equation (16), recruiting Y \u222aV \u2208HY \\X TV\u2192Y without promotion results in the parentless PY with bottom part DY . The reduced cohort table is\nPr { Y \u222aV \u2208HY \\X TV\u2192Y |X } IX\u222aY \u222aV \u2208HY \\XTV\u2192Y\n= Pr { Y | \u222aV \u2208HY \\X TV\u2192Y , X } Pr { \u222aV \u2208HY \\XTV\u2192Y |X } IX\u222aY \u222aV \u2208HY \\XTV\u2192Y\nBecause HY \u2286 X \u222aV \u2208HY \\X TV\u2192Y , the reduced cohort table becomes\nPr {Y |HY } IY \u222aHY Pr { \u222aV \u2208HY \\XTV\u2192Y } I\u222aV \u2208HY \\XTV\u2192Y\n= Prr {Y |HY } \u220f\nV \u2208HY \\X\nPr {TV\u2192Y } ITV\u2192Y .\nFrom Theorem 7, as Y /\u2208 \u222aV \u2208HY \\XTV\u2192Y , \u039bY (X) = \u2211\nY \u222aV \u2208HY \\XTV\u2192Y\nPrr {Y |HY } \u220f V \u2208HY \\X Pr {TV\u2192Y } ITV\u2192Y \u039b (Y ) = \u2211 Y \u039b (Y ) \u2211 HY \\X Prr {Y |HY } \u220f V \u2208HY \\X  \u2211 TV\u2192Y \\V Pr {TV\u2192Y } ITV\u2192Y\n , which is the lemma by Definition (15). This is similar to Equation (4.44) in Pearl (1988).\nThe values of \u03a0 (\u00b7) and \u039b (\u00b7) can be calculated from the messages as in the following lemmas:\nLemma 12 For node X in a polytree, \u03a0 (X) = \u2211 HX Prr {X|HX} \u220f V \u2208HX \u03a0X (V ) .\nProof. Consider the parentless \u222aV \u2208HXTV\u2192X having border HX and the top part \u222aV \u2208HX {TV\u2192X\\V }. By Definition (9),\n\u03a0 (HX) = \u2211\n\u222aV \u2208HX {TV\u2192X\\V }\nPr {\u222aV \u2208HXTV\u2192X} I\u222aV \u2208HX TV\u2192X\n= \u220f\nV \u2208HX  \u2211 TV\u2192X\\V Pr {TV\u2192X} ITV\u2192X  = \u220f V \u2208HX \u03a0X (V ) .\nNow recruit X, resulting in the parentless PX = X \u222aV \u2208HX TV\u2192X . Then use Rule 1 to promote HX without cohort, leaving X as the border of PX . From Theorem 5,\n\u03a0 (X) = \u2211 HX Prr {X|HX}\u03a0 (HX) = \u2211 HX Prr {X|HX} \u220f V \u2208HX \u03a0X (V ) ,\nhence the lemma, which is similar to Equation (4.38) in Pearl (1988).\nLemma 13 For node X in a polytree, \u039b (X) = \u220f V \u2208LX \u039bV (X) .\nProof. Consider the parentless PX , having border X. Recruit the rest of the network DX = \u222aV \u2208LXUX\u2192V without promotion, resulting in the bottom part \u2205 with \u039b (\u2205) = 1. From Theorem 7,\n\u039b (X) = \u2211\n\u222aV \u2208LXUX\u2192V\nPr {\u222aV \u2208LXUX\u2192V |X} IX\u222aV \u2208LXUX\u2192V \u039b (\u2205)\n= \u220f V \u2208LX \u2211 UX\u2192V Pr {UX\u2192V |X} IX\u222aUX\u2192V ,\nhence the lemma by Definition (18), which is similar to Equation (4.35) in Pearl (1988).\nOur dealing with the evidences here is slightly different to that in Peot & Shachter (1991, pp. 308-309). While we attach the indicator column IV to all \u03a0 (V ), \u03a0Y (V ), \u039b (V ) and \u039bY (V ), they call it the \u201clocal evidence\u201d \u039bV (V ) and attach it to \u039b (V ) only.\nCombining Lemmas 10 and 12 yields:\nTheorem 14 If node X has received the messages from all members of HX\u222a {LX\\Y }, then it can send a downward message to its child Y as\n\u03a0Y (X) =  \u220f V \u2208LX\\Y \u039bV (X) \u2211 HX ( Prr {X|HX} \u220f V \u2208HX \u03a0X (V ) ) .\nCombining Lemmas 11 and 13 yields:\nTheorem 15 If node X has received the messages from all members of {HX\\H} \u222a LX , then it can send an upward message to is parent H as\n\u039bX (H) = \u2211 X ( \u220f V \u2208LX \u039bV (X) ) \u2211 HX\\H Prr {X|HX} \u220f V \u2208HX\\H \u03a0X (V )  ."}, {"heading": "5.5 The Collection Phase", "text": "In the first phase of the revised polytree algorithm, which is known as the \u201ccollection phase,\u201d a randomly selected node P is designated as a \u201cpivot,\u201d and the messages are passed (or \u201cpropagated\u201d) to P ."}, {"heading": "5.5.1 The Evidential Cores", "text": "The \u201cevidential core\u201d (or EC) of a polytree is the smallest sub-polytree which contains all the evidences. In other words, it comprises of the evidence set E and all the nodes and edges on the path connecting every pair of the evidence nodes. Corresponding to a particular evidence set, the EC is a unique.\nFigure 5 shows the EC in the Polytree B, corresponding to E = {B,C,K,L4}, not including the nodes or edges in dash, such as node L1. We call it the EC B.\nA node in a polytree is said to be \u201cinside\u201d if it is in the EC; otherwise it is \u201coutside.\u201d Likewise, an edge X \u2192 Y is \u201cinside\u201d if both X and Y are inside; otherwise it is \u201coutside.\u201d A path is \u201cinside\u201d if all its component edges are.\nIt is important to observe that, for an outside edge X \u2192 Y , the whole EC can only be either on its parent side TX\u2192Y , or its child side UX\u2192Y , not both. Otherwise, being connected, the EC must include edge X \u2192 Y on the path connecting the evidences on both sides."}, {"heading": "5.5.2 The Boundary Conditions", "text": "We want to use Theorems 14 and 15 for calculating the messages along the inside edges only. However, this requires the knowledge of the boundary conditions; that is, of the messages from the outside neighbors to the inside nodes, such as from those in dash in Figure 5. So we need the following theorem:\nTheorem 16 Consider an inside node X of an EC. (a) If V \u2208 HX is outside, then \u03a0X (V ) = Pr {V }. (b) If V \u2208 LX is outside, then \u039bV (X) = IX .\nProof. (a) Because the inside X is on the child side of edge V \u2192 X, so is the whole EC, and thus TV\u2192X has no evidence, or ITV\u2192X = 1. Hence, by Definition (15), \u03a0X (V ) = Pr {V } .\n(b) If V \u2208 LX is outside, then because the inside X is on the parent side of edge X \u2192 V , so is the whole EC; hence UX\u2192V has no evidence. By Definition (18), \u039bV (X) = IX .\nSo, assuming that all prior probabilities Pr {V } are pre-loaded, Theorems 14 and 15 can be used without the need to calculate the messages from the outside neighbors."}, {"heading": "5.5.3 The Message Initializations", "text": "Like all polytrees, an EC has its own roots and leaves. For example, the sets of roots and leaves of the EC B are {K,A,C} and {B,L4,M}, respectively.\nBy definition of the EC, every path in it must end with an evidence node, although there are evidence nodes not at the end of a path (such as nodes C in the EC B). Also, an evidence node at the end of a path can only have one inside child or parent, otherwise the path does not end with it. If it has one child, then it is a root of the EC (such as nodes K in the EC B); if one parent, it is a leaf (such as nodes B and L4 in the EC B).\nWe initialize the message propagations in an EC as follows:\nTheorem 17 Consider an EC. (a) For an evidence root R having one inside child C, \u03a0C (R) = \u03a0 (R) = Pr {R} IR. (b) For an evidence leaf L, \u039b (L) = IL.\nProof. (a) Because AR = PR\\R is non-evidential, \u03a0 (R) = Pr {R} IR by Definition (13). Also, because R has only one inside child C, \u039bV (R) = IR for all V \u2208 LR\\C. Thus \u03a0C (R) = \u03a0 (R) from Lemma 10.\n(b) For an evidence leaf L, DL has no evidence and thus \u039b (L) = IL by Definition (14)."}, {"heading": "5.5.4 To the Pivot", "text": "In the collection phase, starting with an evidence root or leaf in the EC, we send the messages from all the inside nodes to a randomly selected inside pivot node P . At any time, if P has not received a message along an edge to or from it, we trace along the possibly many inside paths leading to this edge, and must see either an inside node that is ready to do so, or an evidence node at the end of an inside path that can send its message to P by Theorem 17.\nWe may need both Theorems 14 and 15 to send the messages along a path, because the directions may change (for example, along path B \u2190 A \u2192 D). In the EC B in Figure 5, if we use node D as the pivot, then the collection phase includes the messages sent along paths B \u2190 A \u2192 D, L4 \u2190 H \u2190 C \u2192 D and K \u2192 M \u2190 D."}, {"heading": "5.6 The Distribution Phase", "text": "In the second phase of the revised polytree algorithm, which is known as the \u201cdistribution phase,\u201d the messages are passed from the pivot node P to a query variable."}, {"heading": "5.6.1 The Informed Nodes", "text": "Once a node Q has received the messages from all its neighbors, we say it is \u201cinformed.\u201d At the end of the collection phase, the pivot node is the first informed node.\nThe posterior marginal probability of an informed node can now be obtained: With the messages from all its parents, we can use Lemma 12 to calculate \u03a0 (Q); with the messages from all its children, we can use Lemma 13 to calculate \u039b (Q). Pr {Q, [E\\Q]} IQ then can be calculated by Theorem 8 as \u03a0 (Q) \u039b (Q). Alternatively, we can use the following theorem:\nTheorem 18 With edge Q\u2192 Y ,\nPr {Q, [E\\Q]} IQ = \u03a0Y (Q) \u039bY (Q) .\nProof. By Theorem 8, Lemmas 13 and 10, Pr {Q, [E\\Q]} IQ = \u03a0 (Q) \u039b (Q) = \u03a0 (Q) \u220f V \u2208LQ \u039bV (Q)\n= \u03a0 (Q) \u220f V \u2208LQ\\Y \u039bV (Q) \u039bY (Q) = \u03a0Y (Q) \u039bY (Q) ."}, {"heading": "5.6.2 The Inside Query Variables", "text": "Let p be the number of paths connecting an inside node V with the rest of the network (that is, the number of its neighbors). If it is not the pivot, then only one of these paths leads to the pivot node P . For us to use Theorem 14 or 15 to send a message from V to P in the collection phase, V must have received the messages along all p \u2212 1 paths, except the one leading to P . Now, in the distribution phase, once it receives a message from the informed P , it becomes informed.\nLet J be the set of all the informed nodes, which we call the \u201cinformed set.\u201d When we sent the messages from an informed node V to an uninformed node Q, not only Q, but also all the nodes along the path from V to Q become informed. In the EC B in Figure 5, the message propagations from the informed node D to node H also make node C informed. Thus, \u201cspreading out\u201d from a single pivot node, the informed set forms a connected sub-polytree, in that there is a single path connecting any two informed nodes and all nodes along that path are informed."}, {"heading": "5.6.3 The Outside Query Variables", "text": "Starting from an inside pivot node, the informed set J does not have to cover the EC and can spread beyond it.\nConsider now an outside uninformed node Q. The paths starting from Q to all nodes in J must go through a unique \u201cgate\u201d T \u2208 J ; otherwise, because J is connected, there are more than one paths connecting Q with\nan informed node via the different gates. Thus Q only needs the messages sent from T to it (in this direction) to become informed, as this has all the evidential information. All other messages sent to it along other paths are from the outside neighbors. In this manner, the informed set J spreads to Q.\nFor the Polytree B in Figure 5, suppose J = {D,C,H} and the outside node R8 is a query variable. The gate from R8 to J is C and the messages along path C \u2190 L3 \u2192 R8 are all that are needed to make R8 informed.\nPeot & Shachter\u2019s revised polytree algorithm has been very much neglected in the literature because not many practical BNs have this form. While they further suggested that their algorithm be applied to a general BN via \u201ccutset conditioning,\u201d we continue this paper by presenting a novel method to convert any BN into a polytree."}, {"heading": "6 The Border Polytrees (BPs)", "text": "In Section 2, we showed that the border algorithm \u201cstretches\u201d a BN into a border chain, which is a special form of the polytrees, in which each node has at most one parent and one child. For example, we stretched the BN A in Figure 1 into a border chain in Figure 3. We will now show how to convert any BN into a polytree."}, {"heading": "6.1 Stage I: The Macro-node Polytrees", "text": "Our method involves two stages: In Stage I, we partition the BN to form a polytree.\nA set of nodes is said to be \u201ccombinable\u201d into a \u201cmacro-node\u201d if the network remains acyclic after the combination of its members. For example, we cannot combine nodes A and H in the BN A, as this results in the directed loop {A,H} \u2192 D \u2192 {A,H}. According to Chang & Fung\u2019s (1989) Node Aggregation Theorem, if all directed paths connecting any two nodes in a set do not contain a node outside it, then this set is combinable. Hence, set {A,C,D,H} in the BN A is.\nIf two loops in a DAG share a common node, they belong to the same \u201cindependent loop set\u201d (ILS). If we convert all ILSs in a DAG into polytrees, then the DAG itself becomes a polytree. Because each ILS is combinable, we can combine the nodes in each for this purpose. However this may yield some\nunnecessarily large macro-nodes. Chang & Fung suggest two methods of converting an ILS into a polytree; one is a heuristic search through the space of its \u201cfeasible partitions\u201d for the optimal node combinations, the other is what they call the \u201cgraph-directed algorithm.\u201d Here, we propose an algorithm which is somewhat along the line of Ng and Levitt\u2019s method for incrementally extending what they called a \u201clayered\u201d polytree (1994)."}, {"heading": "6.1.1 The Isolated Loops", "text": "In a DAG, let us isolate one of its loops (that is, delete all nodes outside it) and call it an \u201cisolated loop.\u201d Some non-root and non-leaf nodes of the DAG may now become the roots or leaves of the isolated loop. On the other hand, some roots or leaves of the former may no longer be in the latter.\nLet us label the r roots in a loop consecutively as \u03c11, ..., \u03c1r, with \u03c1r+1 = \u03c11. (See Figure 6(a).)\nLemma 19 An isolated loop has the same number of roots and leaves, and can be organized into a star shape.\nProof. Two neighboring roots cannot be in a parent-child relationship, thus must be connected from below via a leaf.\nIf there are r = 2 roots, then we need two leaves to have a loop connecting all four. However, if r > 2 and there are more than one leaves in the direct path connecting any two consecutive roots, we have a loop connecting all four, in addition to the loop connecting all r roots, which violates our isolated loop assumption. Thus with r roots, we have r leaves. Figure 6(a) shows a typical loop which is organized into a star shape.\nLet \u03bbi be the single leaf between \u03c1i and \u03c1i+1. Let \u03bai,k (k = 1, ..., ni) be the ni nodes in the directed path from \u03c1i to \u03bbi and \u03b3i,k (k = 1, ...,mi) be the mi nodes in the directed path from \u03c1i+1 to \u03bbi.\nThere are more than one ways we can combine the nodes in an isolated loop in order to \u201copen\u201d it; that is, to make it a polytree. For example, we can combine all its roots after some appropriate node combinations to make path \u03c1i \u2192 ... \u2192 \u03bbi and path \u03c1i+1 \u2192 ... \u2192 \u03bbi having the same \u201clength;\u201d that is, have the same number of macro-nodes. For example, to have the polytree in Figure 6(b), we form path \u03c12 \u2192 \u03b32,1 \u2192 {\u03bb1, \u03b32,2}, so that it has the same length as path \u03c11 \u2192 \u03ba1,1 \u2192 \u03bb1, before combining them into path {\u03c11, \u03c12} \u2192 {\u03b32,1, \u03ba1,1} \u2192 {\u03bb1, \u03b32,2}. Similarly, we can combine all its leaves.\nAnother novel method is to \u201cfold\u201d the loop along one of its axes, bringing together the pairs of opposite paths in order to combine them, creating an acyclic semi-star. The star must be made \u201csymmetrical\u201d around the chosen axis first, in that each pair of opposite paths must have the same length. The loop in Figure 6(a) is not symmetrical around axis \u03bb2 \u2212 \u03c15. To make it symmetrical, among other combinations, we may form path {\u03c12, \u03b32,1} \u2192 \u03b32,2 \u2192 \u03bb1, so that it can be combined with path \u03c13 \u2192 \u03ba3,1 \u2192 \u03bb3. The polytree in Figure 6(c) is one possible result of folding the loop in Figure 6(a) along its axis \u03bb2 \u2212 \u03c15.\nSo it seems that we can covert a DAG into a polytree by open all loops in isolation, one-by-one in this manner. Unfortunately, this procedure generally is not correct, as this may make other loops in the same ILS cyclic. For example, if there is an additional directed path \u03c12 \u2192 \u03b7 \u2192 \u03c13 in Figure 6(a), then combining \u03c12 and \u03c13 as in Figure 6(c) violates the Node Aggregation Theorem. Furthermore, these cyclic loops may not be easily identifiable.\nWe now suggest a correct and systematic way to identify and open all loops in a DAG."}, {"heading": "6.1.2 The Parentless Polytree Method", "text": "In our \u201cparentless polytree method,\u201d we construct a growing parentless polytree from a DAG as follows: We start with a root in the DAG and execute the following steps:\n1. While there is an un-recruited node\n(a) Recruit node \u03c4 in a topological order (that is, after all its parents {\u03c01, ..., \u03c0p}), together with edge \u03c4 \u2190 \u03c01\n(b) While there is an un-recruited edge \u03c4 \u2190 \u03c0i i. recruit edge \u03c4 \u2190 \u03c0i\nii. isolate and open any resulted loop\n(c) End while\n(d) Go to Step 1\n2. End while\nThe growing polytree must be kept parentless, so that no directed path can return to it, and thus no directed cycle via a node outside the polytree can occur as a result of any node combinations inside the polytree.\nWhen we recruit node \u03c4 having p parents into the polytree P , we bring with it p edges \u03c4 \u2190 \u03c0i (i = 1, ..., p), where each pair {\u03c0i, \u03c0j} is connected by at most one path. (We have no path if \u03c0i and \u03c0j are in two unconnected subsets of P .) Recruiting \u03c4 with all its parents creates at most Cp2 loops.\n\u201cRecruiting edge \u03c4 \u2190 \u03c0i\u201d means adding it in the polytree. Following Ng and Levitt (1994), not only do we recruit the nodes one-at-a-time, we also recruit the edges one-at-a-time. If \u03c4 has only one parent, then with edge \u03c4 \u2190 \u03c01 we still have a polytree. Otherwise, after recruiting edge \u03c4 \u2190 \u03c02, we have at most one loop \u03c4 \u2190 \u03c01 \u2212 ... \u2212 \u03c02 \u2192 \u03c4 . After opening this loop, we again have a polytree. Then edge \u03c4 \u2190 \u03c03 yields at most another loop via \u03c03.... In this way, we only have to open at most p\u2212 1 easily identifiable loops.\nIf two loops created by \u03c4 have different pairs of parents {\u03c0i, \u03c0j} and {\u03c0k, \u03c0m}, then opening one cannot make the other cyclic. So let us consider two loops having parents {\u03c0i, \u03c0j} and {\u03c0j, \u03c0k} and recruit edges \u03c4 \u2190 \u03c0i and \u03c4 \u2190 \u03c0j first, resulting in a loop having path \u03c0i \u2192 \u03c4 \u2190 \u03c0j. (If P is not parentless, we may have a loop with path \u03c0i \u2192 \u03c4 \u2192 \u03c8 instead.) If we open this loop without combining \u03c4 with any node, then when we recruit edge \u03c4 \u2190 \u03c0k later, the resulting loop is acyclic, because path \u03c0j \u2192 \u03c4 \u2190 \u03c0k is still in it. It is feasible not to combine leaf \u03c4 in the folding method, as we do not need the macro-node {\u03c4, \u03c0i} to reduce the length of path \u03c4 \u2190 \u03c0i \u2190 ...\u2190 \u03c1k and we can choose the symmetrical axis going through \u03c4 , so that it is not combined with any other leaf.\nBut suppose we wish to combine leaf \u03c4 with node \u03b7 in the loop with {\u03c0i, \u03c0j}, and only to find out later when recruiting edge \u03c4 \u2190 \u03c0k that loop {\u03b7, \u03c4} \u2192 ... \u2192 \u03c0k \u2192 {\u03b7, \u03c4} is cyclic because of the directed path \u03b7 \u2192 ... \u2192 \u03c0k \u2192 \u03c4 . In this case, we \u201copen\u201d this loop by expand the macro-node {\u03b7, \u03c4} to {\u03b7, ..., \u03c0k, \u03c4}. If there is any loop in this macro-node, it has the form \u03c4 \u2190 ... \u2212 \u03b7 \u2192 ... \u2192 \u03c0k \u2192 \u03c4 and therefore is not cyclic. After recruiting all p edges, we have a polytree (without any cycle, hence is acyclic), and can return to Step 1.\nAfter recruiting all nodes and their edges in a DAG in this manner, we will have partitioned the DAG into what we call a \u201cmacro-node polytree.\u201d\nFor illustration, let us use the BN in Figure 7, which we will refer to as the Bayesian network C, or BN C. Assume that, after forming the macro-nodes {R, T} and {B,C}, we arrive at the parentless polytree above the curved line in Figure 7(a).\n1. Recruiting node X results in loop X \u2212U \u2212{R, T}\u2212V \u2212X, which can be opened by forming the macro-node {U, V } .\n2. Recruiting node Y and then node Z results in loop Z \u2212 Y \u2212 {U, V } \u2212 X \u2212 Z, which can be opened by forming the macro-node {X, Y } .\n3. Recruiting node Q results in loop Q\u2212 {U, V } \u2212 {R, T} \u2212 S \u2212M \u2212Q, which can be opened by forming the macro-node {M,S, U, V } .\n4. Recruiting edges P \u2190 {M,S, U, P} and P \u2190 Q results in loop P \u2212 Q\u2212{M,S, U, V }\u2212P , which can be opened by forming the macro-node {M,S, U, V,Q} .\n5. Recruiting edge P \u2190 N results in loop P\u2212N\u2212L\u2212{M,S, U, V,Q}\u2212P , which can be opened by forming the macro-node {N,M,S, U, V,Q} .\n6. Recruiting edges J \u2190 P and J \u2190 O results in loop J\u2212P\u2212{N,M,S, U, V,Q}\u2212 O \u2212 J , which can be opened by forming the macro-node {O,P} .\n7. Recruiting edge J \u2190 F results in loop J \u2212 {O,P} \u2212 {B,C} \u2212 F \u2212 J , which can be opened by forming the macro-node {F,O, P} .\n8. Recruiting node H results in loop H \u2212 {F,O, P} \u2212 {B,C} \u2212 D \u2212 H, which can be opened by forming the macro-node {D,F,O, P} .\n9. Recruiting node I results in loop I \u2212H \u2212 {D,F,O, P} \u2212 I, which can be opened by forming the macro-node {I,H} .\nWe now can apply the revised polytree algorithm to the final macro-node polytree corresponding to the BN C, as shown in Figure 7(b). However, the size of some CPTs can be large, such as\nPr {D,F,O, P |N,M,S, U, V,Q,G,B,C} = Pr {O|S, G,B,C} , (19)\nwhere O = {D,F,O, P} and S = {N,M,S, U, V,Q}. This is why we do not stop after Stage I, but continue to Stage II."}, {"heading": "6.2 Stage II: The Border Polytrees", "text": "In Stage II, we use the border algorithm, which explores the independence relationships between the individual nodes within the macro-nodes, to stretch each macro-node into a border chain, if it is not already in this form.\nA result obtained for the BN C after Stage II is shown in Figure 8. Inside a macro-node, we follow the eight rules in \u00a72.2 for promoting a node from a border and recruiting its cohort, so that each border separates the top part of the macro-node with its bottom part. However, because not all macro-notes are parentless, we need to introduce the following additional rules:\n9. We cannot stretch a macro-node before its parents. In the BN C, we must start with the macro-node {K}, {R, T}, {G} or {A}.\n10. Macro-node \u0393 cannot recruit any member of its macro-node child \u2206. In the macro-node S of the BN C, border {N,M,Q} is obtained from border {N,M,Q, V } by promoting V without recruiting its child Y in macro-node {X, Y }.\n11. If macro-node \u2206 has parents in macro-node \u0393, the entire set \u0393 \u2229 H\u2206 must be together in at least one border in \u0393. In the BN C, because S \u2229H{X,Y } = {U, V } we form border {N,M,U, V } \u2286 S, because S \u2229HO = {N,M,Q} we form border {N,M,Q} \u2286 S. Also, because O \u2229H{H,I} = {D,F} and O \u2229HJ = {F,O, P}, we form border {D,F,O, P} \u2286 O.\n12. Suppose we wish to promote node B \u2208 Bi\u22121 \u2286 \u2206. By Rule 10, we do not recruit LB * \u2206. However, we need to recruit all un-recruited members of LB \u2229\u2206 (that is, FB = LB \u2229 DBi\u22121 \u2229\u2206) and all members of HFB \u2286 V (that is, the co-parents of B), even if they are not in \u2206. Assume there are r macro-nodes \u0393k 6= \u2206 (k = 1, ..., r) such that \u0393k \u2229 HFB 6= \u2205. By Rules 9 and 11, we must have already constructed in \u0393k a border Bi\u22121,k such that {\u0393k \u2229HFB} \u2286 B\u2217i\u22121,k = {\u0393k \u2229H\u2206} \u2286 Bi\u22121,k. For notational advantages, we also denote Bi\u22121 \u2286 \u2206 by Bi\u22121,0 or B\u2217i\u22121,0.\nWe break our procedure into two steps: (i) We first recruit to \u2206 all \u0393k \u2229H\u2206 (k = 1, ..., r) (which have not been recruited) to form border\nBi = Bi\u22121 \u222arj=1 {\u0393k \u2229H\u2206} = \u222arj=0B\u2217i\u22121,j \u2287 HFB , (20)\nwhere Bi has all Bi\u22121,k as parents. (ii) Then we promote node B from border Bi with cohort FB \u2286 \u2206 by Rule 2. Note that the entire set \u0393k \u2229 H\u2206 constructed by Rule 11 must be recruited into \u2206 together. This is to ensure that there is at most one edge connecting any two stretched macro-nodes. Otherwise, the main benefit of the macro-node polytrees is destroyed.\nIn the BN C, suppose we wish to promote nodeN from border {N,P} \u2282 O. Because FN = LN \u2229 D{N,P} \u2229 O = O, we need to recruit nodes C \u2208 {B,C} \u2229 HO and G \u2208 {G} \u2229 HO. However, Rule 11 puts {B,C} \u2229 HO = {B,C} in one border and instead of C alone, we recruit the whole set {B,C}. We thus form border \u222a2j=0B\u2217i\u22121,j with B\u2217i\u22121,0 = {N,P}, B\u2217i\u22121,1 = {B,C}, B\u2217i\u22121,2 = {G} before promoting N with cohort O.\n13. Rule 12 also applies when we start stretching a non-root macro-node \u2206. We do not start with the nodes having parents in it, but with node V having parents only in other macro-nodes \u0393k 6= \u2206 (k = 1, ..., r); that is, \u2206 \u2229 HV = \u2205, and \u0393k \u2229 HV 6= \u2205. In this case we first form border Bi as in Equation (20), with Bi\u22121,0 = B\u2217i\u22121,0 = \u2205 and with B\u2217i\u22121,k such that {\u0393k \u2229HV } \u2286 B\u2217i\u22121,k = {\u0393k \u2229H\u2206} \u2286 Bi\u22121,k. Then we can recruit V .\nIn the BN C, to start stretching macro-node S with node S having parents in macro-nodes {L} and {R, T}, we first form border \u222a2j=1B\u2217i\u22121,j with B\u2217i\u22121,1 = HS \u2229 {L} = L and B\u2217i\u22121,2 = HS \u2229 {R, T} = {R, T}. We then can promote R with cohort {S, U}. If r = 1 and B\u2217i\u22121,1 = Bi\u22121,1, then there is no need to repeat Bi = Bi\u22121,1. Suppose we wish to start stretching macro-node O in the BN C by recruiting node P . Because P only has parents in S, there is no need to repeat border B\u2217i\u22121,1 = Bi\u22121,1 = S \u2229HO = {N,M,Q}. We simply promote M to recruit P .\nAfter stretching all macro-nodes, we obtain a \u201cborder polytree\u201d or a BP, made up of the \u201cborders.\u201d The variables in each border are called its \u201ccomponent variables.\u201d The BP in Figure 8 is called the BP C.\nFor comparison with a undirected junction tree of the well-known \u201cDyspnoea\u201d example by Lauritzen & Spiegelhalter (1988, pp. 164 and 209), we include in Figure 9(a) its macro-node and border polytrees. Figure 9(b) shows the macro-node and border polytrees of the BN A.\nLike a clique tree (or a junction tree), a BP has the \u201crunning intersection property;\u201d that is, after a variable is recruited into border Bj, it stays in all future consecutive borders Bj+1, Bj+2,... until its promotion; then it will never be recruited again into any later borders. However, a BP is not \u201cfamily preserving,\u201d in that it is not necessary that a variable must be with all its parents in one border. Most importantly, a BP is directional, while a clique tree is not.\nThe borders in a BP generally have smaller CPTs than the macro-nodes in macro-node polytree. In the BN C, instead of the CPT as in Equation (19) involving thirteen variables, we now have Pr {D,F,O, P |B,F,O, P}, with five variables {D,F,O, P,B}. Despite their smaller CPTs, their poste-\nrior marginal probabilities still can be calculated by adapting the results in Section 4 as we will show next."}, {"heading": "6.3 The Message Propagations in a Border Polytree", "text": "There are two types of non-root borders in a BP:\n1. Bi = {Bi\u22121, Ci} \\Vi, obtained by following Rules 1-8 in \u00a72.2, having Bi\u22121 as a single parent. (If Bi = X and Bi\u22121 = Y , then Ci = X, Vi = Y .)\n2. Bi = \u222arj=0B\u2217i\u22121,j, obtained by Rule 12 or 13, withHBi = {Bi\u22121,j, j = 0, ..., r}.\nReturning to the proofs of Lemmas 10 and 13, we see that they are still valid for the BPs. However, we need to modify Lemmas 11 and 12 according to the type of border in a BP."}, {"heading": "6.3.1 The Downward Propagations", "text": "We first modify Lemma 12:\nLemma 20 For border Bi = {Bi\u22121, Ci} \\Vi,\n\u03a0 (Bi) = \u2211 Vi \u03c6 (Ci) \u03a0Bi (Bi\u22121) ."}, {"heading": "If Bi is the only child of Bi\u22121,", "text": "\u03a0 (Bi) = \u2211 Vi \u03c6 (Ci) \u03a0 (Bi\u22121) .\nProof. Consider the parentless TBi\u22121\u2192Bi with border Bi\u22121. Promoting Vi \u2208 Bi\u22121 with cohort Ci yields the parentless PBi with border Bi. This because, with {Bi\u22121\\Vi} \u2286 TBi\u22121\u2192Bi ,\nTBi\u22121\u2192Bi \u222a Ci = TBi\u22121\u2192Bi \u222a {Bi\u22121\\Vi} \u222a Ci = TBi\u22121\u2192Bi \u222a Bi = PBi .\nThe lemma follows from Theorem 5. If Bi is the only child of Bi\u22121, by Lemma 10, \u03a0Bi (Bi\u22121) = \u03a0 (Bi\u22121).\nLemma 21 For border Bi = \u222arj=0B\u2217i\u22121,j,\n\u03a0 (Bi) = r\u220f j=0 \u2211 Bi\u22121,j\\B\u2217i\u22121,j \u03a0Bi (Bi\u22121,j) .\nProof. Consider the parentless T i\u22121 = \u222arj=0TBi\u22121,j\u2192Bi having border Bi\u22121 = \u222arj=0Bi\u22121,j. Because all TBi\u22121,j\u2192Bi are parentless and independent, from Definitions (9) and (15),\n\u03a0 ( Bi\u22121 ) = Pr { Bi\u22121, [ T i\u22121\\Bi\u22121 ]} IBi\u22121\n= Pr { \u222arj=0 { Bi\u22121,j, [ TBi\u22121,j\u2192Bi\\Bi\u22121,j ]}} I\u222arj=0Bi\u22121,j\n= r\u220f j=0 Pr { Bi\u22121,j, [ TBi\u22121,j\u2192Bi\\Bi\u22121,j ]} IBi\u22121,j = r\u220f j=0 \u03a0Bi (Bi\u22121,j) .\nBecause Bi\u22121,j\\B\u2217i\u22121,j does not have children in the bottom set {Bi,DBi}, we use Rule 1 to promote \u222arj=0 { Bi\u22121,j\\B\u2217i\u22121,j } without cohort to obtain the same parentless T i\u22121 with border Bi = \u222arj=0B\u2217i\u22121,j. From Theorem 5,\n\u03a0 (Bi) = \u2211 \u222arj=0{Bi\u22121,j\\B\u2217i\u22121,j} \u03a0 ( Bi\u22121 ) = \u2211 \u222arj=0{Bi\u22121,j\\B\u2217i\u22121,j} r\u220f j=0 \u03a0Bi (Bi\u22121,j) ,\nhence the lemma. Combining Lemma 10 with above lemmas yields the following BP version of Theorem 14.\nTheorem 22 If border Bi has received the messages from all members of HBi \u222a {LBi\\Bi+1,j}, then it can send a downward message to its child Bi+1,j as\n\u03a0Bi+1,j (Bi) = \u03a0 (Bi)  \u220f W\u2208LBi\\Bi+1,j \u039bW (Bi)  , where \u03a0 (Bi) is calculated by Lemma 20 or 21."}, {"heading": "6.3.2 The Upward Propagations", "text": "We now present the following BP versions of Lemma 11:\nLemma 23 For border Bi = {Bi\u22121, Ci} \\Vi,\n\u039bBi (Bi\u22121) = \u2211 Ci \u03c6 (Ci) \u039b (Bi) ."}, {"heading": "If Bi is the only child of Bi\u22121,", "text": "\u039b (Bi\u22121) = \u2211 Ci \u03c6 (Ci) \u039b (Bi) .\nProof. Consider the parentless TBi\u22121\u2192Bi with border Bi\u22121. As shown in Lemma 20, promoting Vi \u2208 Bi\u22121 with cohort Ci yields the parentless PBi with border Bi. The lemma follows from Theorem 7.\nIf Bi is the only child of Bi\u22121, by Lemma 13, \u039bBi (Bi\u22121) = \u039b (Bi\u22121).\nLemma 24 If border Bi = \u222arj=0B\u2217i\u22121,j, then for all 0 \u2264 k \u2264 r,\n\u039bBi (Bi\u22121,k) = \u2211\nHBi\\Bi\u22121,k\n\u039b (Bi) \u220f\nW\u2208HBi\\Bi\u22121,k\n\u03a0Bi (W) .\nProof. Consider the parentless TBi\u22121,k\u2192Bi (0 \u2264 k \u2264 r) having border Bi\u22121,k. Let Ii\u22121,k = HBi\\Bi\u22121,k. We recruit \u222aW\u2208Ii\u22121,kTW\u2192Bi without promotion, resulting in the parentless T i\u22121 = \u222arj=0TBi\u22121,j\u2192Bi having border Bi\u22121 = \u222arj=0Bi\u22121,j. From Theorem 7,\n\u039bBi (Bi\u22121,k)\n= \u2211\n\u222aW\u2208Ii\u22121,kTW\u2192Bi\n \u220f W\u2208Ii\u22121,k Pr {TW\u2192Bi} ITW\u2192Bi \u039b (Bi\u22121)\n= \u2211 Ii\u22121,k  \u2211\n\u222aW\u2208Ii\u22121,k{TW\u2192Bi\\W}\n \u220f W\u2208Ii\u22121,k Pr {TW\u2192Bi} ITW\u2192Bi \u039b (Bi\u22121)  .\nBecause Bi\u22121 \u2229 { \u222aW\u2208Ii\u22121,kTW\u2192Bi\\W } = \u2205,\n\u039bBi (Bi\u22121,k)\n= \u2211 Ii\u22121,k \u039b ( Bi\u22121 ) \u2211\n\u222aW\u2208Ii\u22121,k{TW\u2192Bi\\W}\n \u220f W\u2208Ii\u22121,k Pr {TW\u2192Bi} ITW\u2192Bi  \n= \u2211 Ii\u22121,k \u039b ( Bi\u22121 ) \u220f W\u2208Ii\u22121,k  \u2211 TW\u2192Bi\\W Pr {TW\u2192Bi} ITW\u2192Bi  . By Definition (15),\n\u039bBi (Bi\u22121,k) = \u2211 Ii\u22121,k \u039b ( Bi\u22121 ) \u220f W\u2208Ii\u22121,k \u03a0Bi (W) .\nNow use Rule 1 to promote \u222arj=0 { Bi\u22121,j\\B\u2217i\u22121,j } without cohort, leaving\nborder Bi = \u222arj=0B\u2217i\u22121,j. From Theorem 7, \u039b ( Bi\u22121 ) = \u039b (Bi), hence the lemma. Lemma 24 can be carried out more efficiently as\n\u039bBi (Bi\u22121,k) = \u2211 Bi\u22121,r \u03a0 Bi\u22121,r Bi ... \u2211 Bi\u22121,k+1 \u03a0 Bi\u22121,k+1 Bi \u2211 Bi\u22121,k\u22121 \u03a0 Bi\u22121,k\u22121 Bi ... \u2211 Bi\u22121,0 \u03a0 Bi\u22121,0 Bi \u039b (Bi) ,\nwhere \u03a0 Bi\u22121,j Bi = \u03a0Bi (Bi\u22121,j) for all 0 \u2264 j \u2264 r.\nCombining Lemma 13 with the above lemmas yields the BP version of Theorem 15:\nTheorem 25 If border Bi has received the messages from all members of {HBi\\Bi\u22121,k} \u222a LBi, then it can send an upward message \u039bBi (Bi\u22121,k) to its parent Bi\u22121,k as in Lemma 23 or 24, in which\n\u039b (Bi) = \u220f W\u2208LBi \u039bW (Bi) .\nBoth the downward and upward passes use the reduced cohort probability tables \u03c6 (\u00b7), rather than the individual CPTs. So all \u03a6 (\u00b7) should be calculated and pre-loaded."}, {"heading": "6.3.3 The Border Evidential Cores", "text": "A border is evidential if one of its component variables is. Let the \u201cborder evidential core\u201d (or \u201cborder EC\u201d) be the smallest sub-polytree which contains all the evidence variables in its borders. For the BN C, let us assume E = {B,O,Q}. Then the largest sub-polytree in Figure 8 containing all the evidence variables includes path {N,M,Q, V } \u2192 {N,M,Q} \u2192 {N,P,Q} \u2192 ... \u2192 {D,F,O, P} and path {B,C} \u2192 {B,C,G,N, P}. However, its border EC is smaller, including only path {N,P,Q} \u2192 {N,P} \u2192 {B,C,G,N, P} \u2192 {B,C,G,O, P}. For some evidence sets E , the border EC is not unique.\nA border is \u201cinside\u201d if it is in the border EC; otherwise it is \u201coutside;\u201d edge Bi \u2212 Bj is \u201cinside\u201d if both borders are inside; otherwise it is \u201coutside;\u201d a path is \u201cinside\u201d if all its component edges are."}, {"heading": "6.3.4 The Boundary Conditions", "text": "As in the polytrees, the calculations of the messages along an inside edge of a border EC may require the knowledge of the boundary conditions; that is, the messages to an inside border from its outside neighboring borders. However, unlike the polytree, our definition of the border ECs allows the outside borders to be evidential. (For example, borders {N,M,Q, V } or {B,F,O, P} in Figure 8 with E = {B,O,Q}.) So we need the following BP version of Theorem 16:\nTheorem 26 Consider an inside border Bi. (a) If Bi\u22121 \u2208 HBi is outside, then \u03a0Bi (Bi\u22121) = Pr {Bi\u22121} IBi\u22121. (b) If Bi+1 \u2208 LBi is outside, then \u039bBi+1 (Bi) = IBi.\nProof. (a) In a border polytree, assume Bi is inside, Bi\u22121 \u2208 HBi is outside, and the outside parentless TBi\u22121\u2192Bi has an evidence variable E \u2208 B\u03031 such that E /\u2208 Bi. By its definition, the border EC must include another evidence border B\u03032 3 E. By the running intersection property, the path connecting B\u03031 and B\u03032 cannot go through Bi. Thus there are two paths connecting B\u03031 and B\u03032, including the one via Bi. Because this is contradictory to the polytree assumption, we must have E \u2208 Bi. By the running intersection property, E \u2208 Bi\u22121. In other words, ITBi\u22121\u2192Bi = IBi\u22121 . By Definition (15),\n\u03a0Bi (Bi\u22121) = \u2211\nTBi\u22121\u2192Bi\\Bi\u22121\nPr { TBi\u22121\u2192Bi } ITBi\u22121\u2192Bi = Pr {Bi\u22121} IBi\u22121 .\n(b) In a border polytree, assume Bi is inside, Bi+1 \u2208 LBi is outside, and the bottom part UBi\u2192Bi+1 has an evidence variable E \u2208 B\u03033 such that E /\u2208 Bi. The border EC must include another evidence border B\u03034 3 E. By the running intersection property, the path connecting B\u03033 and B\u03034 cannot go through Bi. Thus there are two paths connecting B\u03033 and B\u03034, including the one via Bi. As this is a contradiction to the polytree assumption, all evidence variables in UBi\u2192Bi+1 must be in Bi. In other words, IUBi\u2192Bi+1 = IBi . By Definition (18),\n\u039bBi+1 (Bi) = \u2211\nUBi\u2192Bi+1\nPr { UBi\u2192Bi+1 |Bi } IBi\u222aUBi\u2192Bi+1\n= \u2211\nUBi\u2192Bi+1\nPr { UBi\u2192Bi+1|Bi } IBi = IBi ."}, {"heading": "6.3.5 The Message Initializations", "text": "Following is the BP version of Theorem 17:\nTheorem 27 Consider a border EC. (a) If BR is a root with inside child BC, then \u03a0 (BR) = \u03a0BC (BR) = Pr {BR} IBR. (b) If BL is a leaf, then \u039b (BL) = IBL in Lemma 23 or 24.\nProof. (a) Let BR be a root of the border EC. As shown in the proof of Theorem 26(a), for all W \u2208 HBR , all evidence variables in TW\u2192BR are in the inside BR; so are all evidence variables in PBR = BR \u222aW\u2208HBR TW\u2192BR . Hence IPBR = IBR . From Definition (13),\n\u03a0 (BR) = \u2211 ABR Pr {PBR} IPBR = \u2211 ABR Pr {ABR ,BR} IBR = Pr {BR} IBR .\nAlso, because BR has only one inside child BC , \u039bW (BR) = IBR for all W \u2208 LBR\\BC . Thus \u03a0 (BR) = \u03a0BC (BR) from Lemma 10.\n(b) Let BL be a leaf of the border EC. As shown in the proof of Theorem 26(b), for allW \u2208 LBL , all evidence variables in UBL\u2192W must be in BL; so are all evidence variables in DBL = \u222aW\u2208LBLUBL\u2192W . Hence IDBL = IBL . From Definition (14),\n\u039b (BL) = \u2211 DBL Pr {DBL|BL} IBL\u222aDBL = \u2211 DBL Pr {DBL|BL} IBL = IBL .\nFor the border chain obtained in Section 2, recall that we denote the first time and the last time an evidence is recruited into P by \u03b1 and \u03b2, respectively. We now see that the part of the border chain from \u03b1 to \u03b2 is its border EC, and Theorem 27 is consistent with Equations (10) and (12)."}, {"heading": "6.3.6 The Collection Phase", "text": "The inferences in a border polytree are carried out in the same manner as in a polytree: In the collection phase, we start with Theorem 27 and propagate the messages inside the border EC to an arbitrarily chosen inside pivot border.\nFor the border EC in Figure 8 with E = {B,O,Q}, suppose we pick its leaf {B,C,G,O, P} as the pivot, then the collection phase includes:\n1. By Theorem 27(a),\n\u03a0 (N,P,Q) = Pr {N,P,Q} IQ.\n2. By Lemma 20(b) and Theorem 22, \u03a0{B,C,G,N,P} (N,P ) = \u03a0 (N,P ) = \u2211 Q Pr {N,P,Q} IQ.\n3. By Lemma 21, with the boundary conditions \u03a0{B,C,G,N,P} (B,C) = Pr {B,C} IB and \u03a0{B,C,G,N,P} (G) = Pr {G},\n\u03a0 (B,C,G,N, P )\n= \u03a0{B,C,G,N,P} (B,C) \u03a0{B,C,G,N,P} (G) \u03a0{B,C,G,N,P} (N,P ) = Pr {B,C} IB Pr {G} \u2211 Q Pr {N,P,Q} IQ.\n4. By Lemma 20(b), at the pivot border, \u03a0 (B,C,G,O, P ) = \u2211 N Pr {O|C,G,N} IO\u03a0 (B,C,G,N, P )\n= \u2211 N Pr {O|C,G,N} IO Pr {B,C} IB Pr {G} \u2211 Q Pr {N,P,Q} IQ.\n(21)\nWith the initial condition \u039b (B,C,G,O, P ) = IBIO, the pivot border is now informed. By Corollary 9, Pr {P, [B,O,Q]} can be calculated as\u2211 {B,C,G,O} \u03a0 (B,C,G,O, P ) \u039b (B,C,G,O, P )\n= \u2211\n{B,C,G,O} \u2211 N Pr {O|C,G,N} IO Pr {B,C} IB Pr {G} \u2211 Q Pr {N,P,Q} IQ.\n(22)\nIt is easy to verify that the RHS is indeed Pr {P, [B,O,Q]}. On the other hand, suppose we pick root {N,P,Q} as the pivot border, then the collection phase includes:\n1. By Theorem 27(b),\n\u039b (B,C,G,O, P ) = IBIO.\n2. By Lemma 23(b), \u039b (B,C,G,N, P ) = \u2211 O Pr {O|C,G,N} IO\u039b (B,C,G,O, P )\n= \u2211 O Pr {O|C,G,N} IOIB.\n3. By Theorem 25 and Lemma 24, with the boundary conditions \u03a0{B,C,G,N,P} (B,C) = Pr {B,C} IB and \u03a0{B,C,G,N,P} (G) = Pr {G},\n\u039b (N,P ) = \u039b{B,C,G,N,P} (N,P ) = \u2211 {B,C,G} \u03a0{B,C,G,N,P} (B,C) \u03a0{B,C,G,N,P} (G) \u039b (B,C,G,N, P )\n= \u2211 {B,C,G} Pr {B,C} IB Pr {G} \u2211 O Pr {O|C,G,N} IO.\n4. By Theorem 25 and Lemma 23(b), at the pivot border,\n\u039b (N,P,Q) = \u039b (N,P ) . (23)\nWith the initial condition \u03a0 (N,P,Q) = Pr {N,P,Q} IQ, the pivot border is now informed. By Corollary 9, Pr {P, [B,O,Q]} can be found by\u2211 {N,Q} \u03a0 (N,P,Q) \u039b (N,P,Q)\n= \u2211 {N,Q} Pr {N,P,Q} IQ \u2211 {B,C,G} Pr {B,C} IB Pr {G} \u2211 O Pr {O|C,G,N} IO,\nwhich is the same as Equation (22)."}, {"heading": "6.3.7 The Distribution Phase", "text": "In the collection phase, the messages converge to the pivot border; so it helps to know all the evidence variables and construct the border EC before the message propagations. Introducing a new evidence may require re-calculating some messages already sent to the pivot border. On the other hand, as discussed in \u00a75.6, the distribution phase starts when the pivot border becomes the single member of the informed set J . Introducing a new query border Q, which includes a query variable, only requires calculating the messages from the corresponding gate in J to Q, making Q informed and thus allow Pr {Q, [E\\Q]} IQ to be calculated as \u03a0 (Q) \u039b (Q) by Theorem 8. There is no message re-calculation when the query variables are considered one-at-a-time.\nFor the BN C with E = {B,O,Q}, assume we wish to obtain the posterior marginal of the outside variable M . With \u039b (N,P,Q) obtained in Equation (23), we can use Lemma 23 to send an upward message to border {N,M,Q} as\n\u039b (N,M,Q) = \u2211 P Pr {P |N,M,Q} IQ\u039b (N,P,Q)\nFinally, with \u03a0 (N,M,Q) = Pr {N,M,Q} IQ, Pr {M, [B,O,Q]} = \u2211 {N,Q} \u03a0 (N,M,Q) \u039b (N,M,Q) = \u2211 {N,Q} Pr {N,M,Q} IQ\u039b (N,M,Q) .\nOn the other hand, if we wish to obtain the posterior marginal of the outside variable F , then we can start with \u03a0 (B,C,G,O, P ) calculated in Equation (21):\n1. Using Lemma 20, \u03a0 (B,C,O, P ) = \u2211 G \u03a0 (B,C,G,O, P ) .\n2. Using Lemma 20, \u03a0 (B,F,O, P ) = \u2211\nC Pr {F |B,C} IB\u03a0 (B,C,O, P ) .\nFinally, with \u039b (B,F,O, P ) = IBIO, Pr {F, [B,O,Q]} = \u2211 {B,O,P} \u03a0 (B,F,O, P ) IBIO.\nIf all non-evidence variables are the query variables, the messages may be sent \u201casynchronously,\u201d without a goal; that is, once a border becomes informed, it may send the messages to all of its neighbors that have not received a message from it. (See D\u0131\u0301ez & Mira, 1994.) Especially in this case,\n1. If the number of children of Bi is large, then it may be advantageous to calculate \u039b (Bi) = \u220f W\u2208LBi \u039bW (Bi) first, then, for all Bi+1,j \u2208 LBi in The-\norem 22, we calculate \u039b (Bi) /\u039bBi+1,j (Bi) instead of \u220f\nW\u2208LBi\\Bi+1,j \u039bW (Bi);\n2. If the number of parents of Bi is large, then it may be advantageous to calculate \u03a0 (HBi) = \u220f W\u2208HBi \u03a0Bi (W) first, then, for all Bi\u22121,k \u2208 HBi in\nLemma 24, we calculate \u03a0 (HBi) /\u03a0Bi (Bi\u22121,k) instead of \u220f\nW\u2208HBi\\Bi\u22121,k \u03a0Bi (W)."}, {"heading": "7 Discussions", "text": "1. Our algorithm can handle what are known as the \u201csoft evidences.\u201d An evidence variable E is soft if Vae (E), its set of observed values, may have more than one members. In other words, the evidence indicator column IE may have more than one non-zero values. This is less restrictive than the \u201chard evidence\u201d assumption normally found in other inference algorithms, which requires Vae (E) to have only one value. (See Langevin & Valtorta, 2008.) 2. All junction-tree based inferences share a worst-case complexity, which is exponential with respect to the largest clique size of the underlying undirected graph. According to Wu and Butz (2005), \u201cLauritzen and Spiegelhalter... were concerned with the size of the clique in the junction tree (transformed from the DAG of a BN), and they realized that their method would not be computational feasible if a large clique is present in the junction tree. The Hugin architecture has the same concern as the Lauritzen-Spiegelhalter\narchitecture, namely, the size of the clique in a junction tree. The ShaferShenoy architecture... used hypertree and Markov tree (junction tree) to describe the architecture. In [9], it was repeatedly emphasized that the efficiency and feasibility of their architecture depends on the size of the clique in a junction tree.\u201d ([9] referred to Shafer, 1996.) Wu and Butz (2005) then showed that \u201cthe presence of a node with a large number of parents can occur in both singly connected and multiply connected BNs. Therefore, in both singly and multiply connected BNs, the computation for exact inference will be exponential in the worst case.\u201d\nIn our algorithm, let us similarly assume that the largest border size in a border polytree is not too large. This imposes some limitations on the sizes of all cohort probability tables \u03a6 (\u00b7), which is dependent to the numbers of parents and children, and the number of possible values of each variable V \u2208 V . 3. The collection phase collects information about the evidences in the entire border polytree. It is intuitive that this process should be reduced to within the sub-polytree border EC: We only need to pass messages within it, toward its inside pivot border, starting from its evidential roots and leaves as in Theorem 27. In other words, in the collection phase, the BP is \u201cpruned\u201d to its border EC. As illustrated above with the BN C having E = {B,O,Q}, the collection phase to the pivot border {N,P,Q} only requires four message propagations within its border EC.\nThis message passing reduction is possible in our algorithm because we know the boundary conditions: Having a directed border polytree, we can use Theorem 26 because we can determine whether a border\u2019s outside neighbor is its parent or child. Also, it is important that all prior marginals Pr {Bi} are pre-calculated (only once) and pre-loaded for this theorem. For example, to use Equation (21), besides the CPTs Pr {O|C,G,N} and Pr {G}, we need the prior marginals Pr {N,P,Q} and Pr {B,C}.\nIn many applications, the difference between the prior and the posterior marginals of a variable is more telling than the posterior itself, so precalculating the prior marginals should be done anyhow. Off-line, we calculate Pr {Bi} as \u03a0 (Bi) without evidence, in a topological order of the macro-nodes and of the borders inside each macro-node, using Lemma 20 (with the complete cohort probability tables \u03a6 (\u00b7)) and Lemma 21 (with \u03a0Bi (Bi\u22121,j) = \u03a0 (Bi\u22121,j) by Theorem 22).\nIn the collection phase, each border in a EC sends only one message toward the pivot border. Thus the time complexity of the collection phase is lin-\near with respect to the number of borders within the sub-polytree border EC. With one additional evidence, this number increases by the number of borders between it and the pivot border. If N is the sole evidence in the BN C, then the collection phase is not needed, as the pivot border {N,P,Q} is automatically informed with \u039b (N,P,Q) = IN and \u03a0 (N,P,Q) = Pr {N,P,Q} IN , where Pr {N,P,Q} is pre-loaded. Thus the time complexity in the collection phase is linear with respect to the number of evidence variables. 4. Similarly, in the distribution phase, we pass messages from the growing informed set J to a query border Q. As discussed above, the propagation starts from the informed gate connecting J with Q; hence there is no need to re-visit any node inside J . In other words, in the distribution phase, the BP is \u201cpruned\u201d to its \u201cquery core,\u201d which is the smallest sub-polytree that contains all the query borders. The computational complexity in the distribution phase is linear with respect to the number of borders inside the query core, which in turn is linear with the number of query variables. Again, the pre-loaded prior marginals Pr {Bi} and the directed border polytrees are essential for the boundary conditions.\nConsider the case where all non-evidence variables are the query variables. As far as we know, all junction-tree based inference architectures (including the LAZY propagation algorithm, Madsen & Jensen, 1999) require two passes through the entire network, one in each phase. Our algorithm requires one pass through the smaller border EC in the collection phase, and one pass through the entire network in the distribution phase. On the other hand, consider the case in which the single evidence variable and the single query variable are in the same border; our algorithm requires no message propagation in both phases. 5. In summary, the novel features in this paper are:\ni. The parentless polytree method (\u00a76.1) to partition a BN into a macronode polytree, by opening the loops in an otherwise growing parentless polytree.\nii. The border algorithm to construct a directed chain from a BN (\u00a72), or from a macro-node (\u00a76.2).\nCombining the above two algorithms, we can convert any Bayesian network into a border polytree. The border algorithm then provides the means to propagate the downward and upward messages in a border polytree, allowing us to calculate its posterior marginal probabilities (\u00a76.3).\nAlso, the message propagations in the distribution phase is carried out one query border at a time, within the query core sub-polytree only (\u00a75.5).\nWith the above novel features, the time complexity of our inferences in a Bayesian network is linear with respect to the number of its evidence and query variables, regardless of the number of borders in its corresponding border polytree, or the number of its variables."}], "references": [{"title": "Node Aggregation for Distributed Inference in Bayesian Networks", "author": ["K. Chang", "R. Fung"], "venue": "In Proceedings of the 11th International Joint Conference on Artificial Intelligence, Detroit,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks", "author": ["G.F. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Local conditioning in Bayesian networks", "author": ["F. D\u0131\u0301ez"], "venue": "Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Distributed inference in Bayesian networks", "author": ["F. D\u0131\u0301ez", "J. Mira"], "venue": "Cybernetics and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "A survey of algorithms for real-time Bayesian network inference", "author": ["H. Guo", "W. Hsu"], "venue": "In the joint AAAI-02/KDD-02/UAI-02 workshop on Real-Time Decision Support and Diagnosis Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "A computational model for combined causal and diagnostic reasoning in inference engines", "author": ["J.H. Kim", "Pearl J"], "venue": "Proceedings of the 8th International Joint Conference on Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Probabilistic Graphical Models: Principles and Techniques, Massachusetts", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Performance evaluation of algorithms for soft evidential update in Bayesian networks: First results", "author": ["S. Langevin", "M. Valtorta"], "venue": "Scalable Uncertainty Management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Local computations with probabilities on graphical structures and their applications to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "A comparison of Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer architectures for computing marginals of probability distributions", "author": ["V. Lepar", "P. Shenoy"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "LAZY propagation: A junction tree inference algorithm based on lazy evaluation", "author": ["A.L. Madsen", "F.V. Jensen"], "venue": "Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Incremental Dynamic Construction of Layered Polytree Networks", "author": ["K. Ng", "T.S. Levitt"], "venue": "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "A constraint-propagation approach to probabilistic reasoning", "author": ["J. Pearl"], "venue": "Proceedings of the 2nd Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1986}, {"title": "Fusion, propagation and structuring in belief networks", "author": ["J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Evidential reasoning using stochastic simulation of causal models", "author": ["J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1987}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "Fusion and propagation with multiple observations in belief networks.", "author": ["M.A. Peot", "R.D. Shachter"], "venue": "Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1991}, {"title": "Evidence absorption and propagation through evidence reversals", "author": ["R.D. Shachter"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1990}, {"title": "Symbolic probabilistic inference in belief networks", "author": ["R.D. Shachter", "B. D\u2019Ambrosio", "B.D. Del Favero"], "venue": "Proceedings of the 8th National Conference on Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1990}, {"title": "Probabilistic Expert Systems, Society for Industrial and Applied Mathematics", "author": ["G. Shafer"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Exploiting causal independence in Bayesian network inference", "author": ["N.L. Zhang", "D. Poole"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}], "referenceMentions": [{"referenceID": 8, "context": "In [9], it was repeatedly emphasized that the efficiency and feasibility of their architecture depends on the size of the clique in a junction tree.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "\u201d ([9] referred to Shafer, 1996.", "startOffset": 3, "endOffset": 6}], "year": 2014, "abstractText": "In a Bayesian network, we wish to evaluate the marginal probability of a query variable, which may be conditioned on the observed values of some evidence variables. Here we first present our \u201cborder algorithm,\u201d which converts a BN into a directed chain. For the polytrees, we then present in details, with some modifications and within the border algorithm framework, the \u201crevised polytree algorithm\u201d by Peot & Shachter (1991). Finally, we present our \u201cparentless polytree method,\u201d which, coupled with the border algorithm, converts any Bayesian network into a polytree, rendering the complexity of our inferences independent of the size of network, and linear with the number of its evidence and query variables. All quantities in this paper have probabilistic interpretations.", "creator": "LaTeX with hyperref package"}}}