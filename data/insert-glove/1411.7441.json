{"id": "1411.7441", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2014", "title": "Pattern Decomposition with Complex Combinatorial Constraints: Application to Materials Discovery", "abstract": "Identifying dossett important components 2,516 or osano factors in large amounts of noisy guzar data is a boming key problem in machine learning hertog and data 3,060 mining. jalili Motivated decisiones by caecidae a strehlow pattern galdos decomposition problem 2987 in yongmin materials discovery, dakhil aimed at homegrown discovering centro new materials for 13-under renewable wehda energy, waterhouses e. g. hassassian for ullevalseter fuel 1,523 and solar bhan cells, we introduce riach CombiFD, wicha a ultimatetv framework 17.80 for 185.8 factor unchecked based pattern verweij decomposition panini that rawly allows the incorporation jalouse of a - phertzberg priori hochfilzen knowledge phineus as coloured constraints, including cuartel complex feets combinatorial headquartered constraints. In sainte-marie addition, chuj we rokaf propose sedalia a fangzhou new kudat pattern decomposition algorithm, called tyisha AMIQO, based alshafei on solving a sequence truncatus of (spielbergian mixed - integer) quadratic dant\u00e8s programs. Our approach abut considerably outperforms bibbins the state fractionated of the art on livengood the perlecan materials 1billion discovery aree problem, prestonsburg scaling hurdling to ganancia larger datasets euro230 and 64-game recovering trocheck more precise 3,830 and physically meaningful decompositions. sellout We cuadernos also show the unbranded effectiveness purina of legislature our shorthorns approach for xiaoguang enforcing prinsen background knowledge on nhf other application semitones domains.", "histories": [["v1", "Thu, 27 Nov 2014 02:31:41 GMT  (674kb,D)", "http://arxiv.org/abs/1411.7441v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["stefano ermon", "ronan le bras", "santosh k suram", "john m gregoire", "carla p gomes", "bart selman", "robert bruce van dover"], "accepted": true, "id": "1411.7441"}, "pdf": {"name": "1411.7441.pdf", "metadata": {"source": "CRF", "title": "Pattern Decomposition with Complex Combinatorial Constraints: Application to Materials Discovery", "authors": ["Stefano Ermon", "Ronan Le Bras", "Santosh K. Suram", "John M. Gregoire", "Carla P. Gomes", "Bart Selman", "Robert B. van Dover"], "emails": ["ermon@cs.stanford.edu", "lebras@cs.cornell.edu", "gregoire}@caltech.edu", "gomes@cs.cornell.edu", "selman@cs.cornell.edu", "rbv2@cornell.edu"], "sections": [{"heading": "Introduction", "text": "In recent years, we have seen an enormous growth in data generation rates in many fields of science (Halevy, Norvig, and Pereira 2009). For instance, in combinatorial materials discovery, scientists search for new materials with desirable properties by obtaining measurements on hundreds of samples in a single batch experiment using a composition spread technique (Ginley et al. 2005; Narasimhan, Mallapragada, and Porter 2007). Providing computational tools for automatically analyzing and for determining the structure of the materials formed in a composition spread is an important and exciting direction in the emerging field of Computational Sustainability (Gomes 2009). For example, this approach has been successfully applied to speed up the discovery of new materials with improved catalytic activity for fuel cell applications (Gregoire et al. 2010; van Dover, Schneemeyer, and Fleming 1998) and of oxygen evolution catalysts with applications to solar fuel generation (Haber et al. 2014). Long-term solutions to several sustainability issues in energy and transportation are likely to come\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nfrom break-through innovations in materials (White 2012), and computer science can play a role and provide support to accelerate new materials discovery (Le Bras et al. 2011).\nTo accelerate the discovery of new materials, materials scientists have developed high-throughput experimental procedures to create composition-spread libraries of new materials, a process that can be intuitively understood as generating an enormous number of compounds in a single experiment by mixing different amounts of a small number of basic elements (Takeuchi, van Dover, and Koinuma 2002). The promising libraries are then characterized using X-ray diffraction to determine the underlying crystal structure and composition. Specifically, a set of X-ray diffraction signals are sampled at n different material compositions, each one corresponding to a different mixture of some basic elements. A key problem in materials discovery is called the phase map identification problem, defined as finding k n basic phases (basis functions that change gradually with composition, in terms of structure and intensity), such that all the n X-ray measurements can be explained as a mixture of the k basic phases. The decomposition is subject to physical constraints that govern the underlying crystallographic process.\nThe phase map identification is effectively a source separation or spectral unmixing problem (Berry et al. 2007) where the sources are the k non-negative, basic x-ray diffraction signals and each observation is a non-negative combination of these k sources. Therefore, a standard approach from the literature is non-negative matrix factorization (NMF) (Long et al. 2009). Nevertheless, this approach overlooks the physical constraints from the crystal formation. For example, it does not guarantee connectivity of the \u201cphase regions\u201d in the phase map, nor can it handle basis patterns that are slightly changing (\u201cshifting\u201d) as the crystal lattice constants change. Recent development (Kusne et al. 2014), while not enforcing these constraints per se, have shown to be resilient to peak shifting for example. To obtain physically meaningful decompositions, researchers (Le Bras et al. 2011; Ermon et al. 2012) have looked at constraint programming formulations that can incorporate all the necessary constraints. The down side of these approaches is that they are fully discrete (they require a discretization of the data through peak detection) and they cannot directly deal ar X iv :1\n41 1.\n74 41\nv1 [\ncs .A\nI] 2\n7 N\nov 2\n01 4\nwith continuous measurement data. On the other side of the spectrum with respect to NMF, the unsupervised nature is lost, and scalability becomes an issue as, for example, in a fully discrete problem there is no notion of gradient anymore. In addition, these approaches are not robust to the presence of noise in the data, as noise considerably impacts the efficiency of their filtering and propagation mechanisms.\nIn this work, we aim to achieve the best of both worlds by bridging the previous approaches and providing a new hybrid formulation, where we integrate additional domain knowledge as additional constraints into the basic NMF approach. We introduce CombiFD, a novel pattern decomposition framework that allows the specification of very general constraints. These include constraints used with some matrix factorization and clustering approaches (such as non-negativity or partial labeling information) as well as more general ones that require a richer representation language, powerful enough to capture more complex, combinatorial dependencies. For example, we show how to encode complex a-priori scientific domain knowledge by specifying combinatorial dependencies on the variables imposed by physical laws. We also propose a novel solution technique called AMIQO (Alternating Mixed Integer Quadratic Optimization), that involves the solution of a sequence of (mixed-integer) quadratic programs. Overall, our constrained factorization algorithm clearly outperforms previous approaches: it scales to large real world datasets, and recovers physically meaningful and significantly more accurate interpretations of the data when prior knowledge is taken into account."}, {"heading": "Framework", "text": "Given n data points ai \u2208 Rm, each one represented by anmdimensional vector of real-valued features, we represent the input data compactly as a matrix A \u2208 Rm\u00d7n, each column corresponding to a data point and each row to a feature. In the context of the phase map identification, A corresponds to the n observed X-ray diffraction patterns, each of them as a vector of m scattering intensity values.\nWe are interested in low-dimensional representations which can approximate the input data A by identifying its essential components or factors. Namely, for a given number k of basic phases, we want to approximate A as A \u2248 WH , where W \u2208 Rm\u00d7k represents k basic phases (or phase patterns) and H \u2208 Rk\u00d7n the combination coefficients at each data point.\nThis problem belongs to the family of low-rank approximation problems, an important research theme in machine learning and data mining with numerous applications, including source separation, denoising, compression, and dimensionality reduction (Berry et al. 2007)."}, {"heading": "Low-Rank Approximation", "text": "Given a non-negative matrix A \u2208 Rm\u00d7n and a desired rank k < min(n,m), we seek matrices W \u2208 Rm\u00d7k and H \u2208 Rk\u00d7n, H,W \u2265 0 that give the best low rank approximation of A, i.e. A \u2248 WH . Typically this is formulated as\nthe following optimization problem:\nmin W,H ||A\u2212WH||2 (1)\nwhere W \u2208 Rm\u00d7k is a matrix of basis vectors or patterns and H \u2208 Rk\u00d7n the coefficient matrix. The symbol || \u00b7 ||2 indicates (entry-wise) Frobenius norm.\nThis basic problem can be solved by the so-called truncated Singular Value Decomposition (SVD) approach, which produces the best approximation in terms of squared distance. It can be computed efficiently and robustly, obtaining a representation where data points can be interpreted as linear combinations of a small number of basis vectors.\nIn many applications input data are non-negative, for instance representing color intensities, word counts (Tjioe et al. 2008) or x-ray scattering intensities in our motivating application. The basis vectors computed with an SVD, however, are generally not guaranteed to be non-negative, and this leads to an undesirable lack of interpretability. For example, it is not possible to interpret an image as the superposition of simpler patches, or an X-ray diffraction pattern as the composition of several basic compounds when obtaining negative values for some of the basis vectors or the coefficients. To overcome this limitation, researchers have introduced the NMF approach, which explicitly enforces nonnegativity constraints on the basis vectors and coefficients.\nWhile non-negativity is a very common constraint in many domains, in some applications we have additional valuable a priori information on the features (each feature corresponds to a row of A and W ). For instance, we might also know a priori an upper bound on the value of some features, or that two Boolean features are incompatible, or that non-negativity only holds for a subset of the features. In particular, in our materials science domain basis, vectors (patterns) correspond to chemical compounds and their underlying crystal structures. For chemical systems in thermodynamic equilibrium, the compositional variation of the concentration and lattice parameters of each compound follow well-defined rules, from which constraints on the coexistence and variation of basis patterns can be defined (Le Bras et al. 2011; Ermon et al. 2012). While some constraints such as non-negativity can be individually enforced in some approaches, others such as connectivity and the complex rules defining shifting basis patterns (see below for details) have not been considered before. To the best of our knowledge, there is no general factor analysis framework that can handle the combination of all these constraints. This motivates the definition of the following general pattern decomposition subject to combinatorial constraints problem."}, {"heading": "CombiFD: Pattern Decomposition with Combinatorial Constraints", "text": "Given a (general) matrix A \u2208 Rm\u00d7n and a desired rank k < min(n,m) we seek matrices W \u2208 Rm\u00d7k and H \u2208 Rk\u00d7n that minimize ||A \u2212 WH||p where p \u2208 {1, 2} and || \u00b7 ||p is an entry-wise norm (e.g. p = 2 corresponds to the Frobenius norm). Moreover, the factors need to satisfy an additional set of J linear inequality constraints, possibly requiring binary or integer variables. This is formalized as\nthe following optimization problem:\nminimize W,H,x,b ||A\u2212WH||p = f(W,H)\nsubject to C[vec(W ), vec(H), x, b]T \u2264 d bi \u2208 {0, 1}, i \u2208 [1, N ].\n(2)\nwhere x \u2208 RM is a vector of additional real-valued variables, b is a vector of N binary variables, d \u2208 RJ , C \u2208 RJ\u00d7(mk+nk+M+N) and vec(\u00b7) denotes vectorization (stacking a matrix into a vector). That is, we have J linear inequalities involving the entries of W , H , x and b, with coefficients given by C and right-hand side given by d. As Integer Linear Programming is well known to be NP-complete, very general constraints can be encoded by appropriately choosing C and d. These additional (combinatorial) constrains are extremely useful to encode prior knowledge we might have about the domain. These include non-negativity (W,H \u2265 0), upper bounds (Wi,j \u2264 u), sparsity (see Example 1), semi-supervised clustering (see Example 2) as well as many others. Other examples of intricate constraints can be found in the experimental section below.\nExample 1: L0 sparsity Suppose we want to explicitly formulate a sparsity constraint on the coefficient matrix. That is, we want to find W,H such that A \u2248WH and each column ofH has at most S non-zero entries, i.e. ||Hi||0 \u2264 S where Hi is the i-th column of H . For instance, in a topic modeling application where the basis vectors correspond to topics, this constraint ensures that each document can have at most S topics. This can be encoded as follows:\nmin W,H,b ||A\u2212WH||2 s.t. bi,j \u2265 hi,j , \u2211 j hi,j = 1, \u2211 i bi,j \u2264 S\nbi,j \u2208 {0, 1} i \u2208 [1, k], j \u2208 [1, n]\n(3)\nwhich can be easily rewritten more compactly in the form (2) by selecting appropriate C and d (in this case, M = 0 and N = kn).\nExample 2: Semi-Supervised Clustering As an example, in a semi-supervised clustering problem, we can easily include partial labeling information as constraints on H , e.g. enforcing Must-Link or Cannot-Link constraints (Liu and Wu 2010; Basu, Davidson, and Wagstaff 2008; Choo et al. 2013). Suppose we have prior information on PML pairs of data points ML = {(i1, j1), \u00b7 \u00b7 \u00b7 , (iPML , jPML)} that are known to belong to the same cluster (Must-Link) , and pCL pairs of data points CL = {(i1, j1), \u00b7 \u00b7 \u00b7 , (ipCL , jpCL)} that are known not to belong to the same cluster (CannotLink).We encode this prior knowledge into our CombiFD framework as follows:\nmin W,H,b ||A\u2212WH||2 s.t. W,H \u2265 0, \u2211\nj hi,j = 1, \u2211 i bi,j \u2264 S\nbi,j \u2265 hi,j , bi,j \u2208 {0, 1} i \u2208 [1, k], j \u2208 [1, n] bi,is = bi,js i \u2208 [1, k], (is, js) \u2208ML bi,is + bi,js \u2264 1 i \u2208 [1, k], (is, js) \u2208 CL\n(4)"}, {"heading": "Related work", "text": "Many low rank approximation schemes are available, including QR decomposition, Independent Component Analysis, truncated Singular Value Decomposition, and Nonnegative Matrix Factorization (Berry et al. 2007).\nWhile these basic methods are unsupervised, there is a growing interest in incorporating prior knowledge or user guidance into these frameworks (Zhi et al. 2013). For example, in semi-supervised clustering applications, user guidance is often given by partial labeling information, which can be incorporated using hard constraints (Liu and Wu 2010; Basu, Davidson, and Wagstaff 2008; Choo et al. 2013). Typical constraints used in this case are Must-Link and Cannot-Link, enforcing that two data points must or cannot be in the same cluster, respectively. For example, (Hossain et al. 2010) present an integrated framework for clustering non-homogenous data, and show how to turn Must-Link and Cannot-Link constraints into dependent and disparate clustering problems. Recently, researchers have also considered interactive matrix factorization schemes for topic modeling that can take into account user feedback on the quality of the decomposition (Choo et al. 2013) (topic refinement, merging or splitting) and semi-supervised NMF with label information as constraints (Liu and Wu 2010). Constraint clustering (Basu, Davidson, and Wagstaff 2008) is another example of this approach. Alternatively, regularizations or penalty terms are also used to obtain solutions with certain desired properties such as sparsity (Hoyer 2004; Cai et al. 2011), convexity (Ding, Li, and Jordan 2010), temporal structural properties and shift invariances (Smaragdis 2004; Smaragdis, Raj, and Shashanka 2008).\nMost of the work in the literature is however confined to a single type of constraints or simple conjunctions, which limits their usability. With CombiFD, we propose a new approach for finding a low dimensional approximation of some data (decomposition into basic patterns) that is able to incorporate not only existing types of constraints but also more complex logical combinations."}, {"heading": "Constrained Factorization Algorithm", "text": "Solving the general CombiFD optimization problem (2) is challenging for two reasons. First, the objective function is not convex, hence minimization is difficult even in the presence of simple non-negativity constraints (Berry et al. 2007). Second, we are allowing a very expressive class of constraints, which can potentially specify very complex, intricate dependencies among the variables. Unfortunately, general nonconvex mixed-integer non-linear programming has not seen as much progress as their linear (MILP) and quadratic (MIQP) counterparts, and most approaches are either application specific or do not scale well. Even in the presence of simple constraints (as it is the case for NMF), the problem is rarely solved to optimality and in practice heuristic approaches are used. Yet, simple heuristic approaches such as multiplicative update rules (Lee and Seung 1999), which is one of the most widely used algorithms, do not apply to our case due to the integer variables. Similarly, projected gradient techniques cannot be directly applied\nhere (Lin 2007). We thereofore introduce a new approximate technique called AMIQO which exploits the special structure of the problem and takes full advantage of advanced MIQP optimization techniques from the OR literature.\nTo solve the general CombiFD optimization problem (2), we introduce AMIQO (Alternating Mixed Integer Quadratic Optimization) with pseudocode reported as Algorithm 1. AMIQO is an iterative two-block coordinate descent procedure enhanced with sophisticated combinatorial optimization techniques beyond the standard convex optimization methods. In fact, the key advantage of our CombiFD framework is that for eitherH orW fixed, problem (2) is a mixedinteger quadratic program.1 Mixed-integer quadratic programs have been widely studied in the operations research literature, and we can leverage a wide range of techniques that have been developed and are implemented in stateof-the-art mixed-integer quadratic programming (MIQP) solvers such as IBM CPLEX. These integer programs do not have to be solved to optimality, and it is sufficient to improve the objective function with respect to the factorization found at the previous step (which is used to warm-start the search). Notice that when there are no binary variables (N = 0), the optimization problems in the inner loop of AMIQO correspond to standard quadratic programs that can be solved efficiently, even in the presence of (linear) constraints in the form (2), which are more general than non-negativity. AMIQO is inspired by the seminal work of Paatero and Tapper who initially proposed the use of a block coordinate descent procedure for NMF (Paatero and Tapper 1994), and was later followed upon by a number of researchers, including an unconstrained least squares version (Berry et al. 2007), and solution techniques based on projected gradient descent (Lin 2007), Quasi-Newton (Kim, Sra, and Dhillon 2007), and Active-set (Kim and Park 2008). However, our approach is novel in that it uses a mixed integer solver in each coordinate descent step, and is the only one that can take into account combinatorial constraints, guaranteeing feasibility of the solution at every iteration even in the presence of intricate combinatorial constraints.\nAlgorithm 1 AMIQO Find feasible W 0, H0, x0, b0 for (2) . Use MIP solver for j = 0, \u00b7 \u00b7 \u00b7 , t\u2212 1 do\nW j+1, x\u0303j+1, b\u0303j+1 \u2190 argminW,x,b f(W,Hj) s.t. (2) and H = Hj . Use MIQP solver\nHj+1, xj+1, bj+1 \u2190 argminH,x,b f(W j+1, H) s.t. (2) and W =W j+1 . Use MIQP solver end for return W t, Ht\nWe summarize the properties of AMIQO with the following proposition: Proposition 1. Let W j , Hj be as in Algorithm 1. If (2) is feasible, the following two properties hold: 1) For all j, 0 \u2264 j \u2264 t, the optimization problems in the inner loop of the algorithm are feasible and (W j , Hj , xj , bj) is feasible for (2).\n1For p = 1, it simplifies to a mixed-integer linear program.\n2) The objective function ||A \u2212W jHj ||p is monotonically non-increasing, i.e. ||A\u2212W jHj ||p \u2265 ||A\u2212W j+1Hj+1||p Theorem 1. AMIQO run on CombiFD problem (4) with S = 1,ML = CL = \u2205 is equivalent to k-means.\nProof. See Appendix for both proofs.\nAlthough the objective function is monotonically nonincreasing, AMIQO is not guaranteed to converge to a global minimum. This is consistent with the hardness of problem (2). More specifically, the quality of the final solution found might depend on the initialization ofW 0 andH0. This issue is common to other standard matrix factorization algorithms, and several heuristic initialization schemes have been proposed to mitigate the issue (Albright et al. 2006). Nevertheless, in our experimental evaluation, the initialization did not play a major role, and we typically converged to the same solution, regardless of the initial conditions.\nExperiments \u2013 Encoding domain knowledge as additional constraints\nIn order to show the generality of our approach, we provide experimental results on three application domains, with increasingly more complex constraints capturing a-priori domain knowledge. We start with semi-supervised clustering with partial labeling information, i.e. simple Must-Link and Cannot-Link constraints. We then consider another clustering problem, where we include more complex logical constraints on the features, describing higher-level biological knowledge. Finally we consider our motivating application: the phase map identification problem.\nClustering with partial labeling information NMF has become a popular approach for clustering, with application domains ranging from document clustering (Shahnaz et al. 2006) and graph clustering (Kuang, Park, and Ding 2012) to gene expression data clustering (Tjioe et al. 2008). Cluster membership is determined by the coefficient matrix H , which reflects how each data point decomposes into the basis vectors.\nThere are several ways to obtain hard clustering assignments (binary indicators) from the coefficient matrixH (real valued). We follow (Ding, Li, and Peng 2008) and normalize the matrix H so that the entries can be interpreted as the posterior probability p(cs|dj) that a data point j belongs to cluster s. Specifically, we let DW = diag(1TW ) and estimate the cluster membership probability as p(cs|dj) \u221d [DWH]sj . As a result, a data point j is assigned a cluster s\u2217(j) such that s\u2217(j) = argmaxs[DWH]sj .\nWe consider a semi-supervised clustering task where we assume to have some prior information on the labels (equivalently, on the cluster assignment) of a subset of datapoints. Specifically, we assume to have information about pairs of data points, which should either belong to the same cluster (Must-Link) or not (Cannot-Link). This information is obtained using standard labeled datasets from the UCI repository (Bache and Lichman 2013) for which a ground truth clustering is known. To generate various amounts of prior\nknowledge, we randomly select P pairs of data points, using their labels to specify a MustLink or CannotLink constraint.\nWe compare our CombiFD formulation of the problem (4) with two previous approaches from the literature: CNMF (Constrained NMF) (Liu and Wu 2010) and NMFS (NMFbased semi supervised clustering) (Li, Ding, and Jordan 2007). The first approach is based on enforcing non combinatorial constraints on H , while the second approach captures the ML and CL constraints using penalty terms in a modified objective function which is then approximately minimized using multiplicative updates.\nWe report in Figure 1 the accuracy obtained using these methods as a function of the amount of supervision, i.e. the number of Must-Link or Cannot-Link constraints used. Accuracy is defined as in (Liu and Wu 2010) and corresponds toAC = 1/n \u00b7max\u03c3 \u2211k i=1 |ri\u2229c\u03c3i |, where \u03c3 : 1..k \u2192 1..k is a bijection mapping clusters ri to ground-truth classes cj . Namely, each cluster is assigned a label such that the labeling best matches the ground truth labels. Note that the accuracy can be computed efficiently using, for example, the Hungarian algorithm.\nResults are averaged over 100 runs. We see that CombiFD significantly outperforms the competing techniques across all levels of prior knowledge. Intuitively, this is because by properly taking into account the combinatorial nature of the problem, CombiFD can automatically make logically sound inferences about the data: for example, we can take into account transitive closure (i.e., if a and b must link, and b and c must link, then a and c must link as well) and other logical implications. The deeper reasoning power and greater accuracy provided by AMIQO however involves a small computational overhead, with a typical runtime in the order of a couple of minutes for AMIQO versus a few tens of seconds for the competing techniques.\nClustering with more complex prior knowledge In this experiment, we consider the Zoo dataset from UCI (Bache and Lichman 2013). This is a dataset of 101 animals, each one represented as a vector of 17 non-negative features (e.g. whether it has hair or not, whether it has feathers or not, or its number of legs). There are 7 class labels.\nWe solve the clustering problem using our CombiFD approach (2), where we enforce non-negativity as well as additional constraints capturing some well known biological facts. Specifically, we enforce the following constraints on the basis vectors using our CombiFD framework: \u00ac(HasMilk \u2227 HasEggs), HasFeathers \u2192 HasEggs, and \u00ac(HasFeathers\u2227HasHair). Since we are not aware of any other matrix factorization technique that can take into account this kind of complex, logically structured prior knowledge, we compare with standard NMF (problem (1)), which is a totally unsupervised technique.\nTable 1 reports the accuracy (defined as before) and runtime of the two approaches averaged over 100 runs. The results show that the CombiFD approach, which incorporates a limited amount of logically structured prior knowledge, significantly improves the accuracy of standard NMF, while still running within seconds using AMIQO."}, {"heading": "Spectral Unmixing for Materials Discovery", "text": "We first present how to incorporate complex constraints capturing some of the key physical laws that govern the datagenerating process for the phase map identification problem.\nSparsity: According to the so-called Gibbs phase rule, under equilibrium conditions at fixed temperature and pressure, there can be mixtures of at mostM phases occurring at each data point (chemical composition) in a library involving M basic elements. This is encoded as ||Hs||0 \u2264 M , for s \u2208 [1, k] as in (3).\nShifting: Each basis pattern may be slightly stretched by a different amount for each composition sample. Indeed, chemical alloying within a given compound may alter the crystal lattice constants, leading to a systematic shift (as a function of composition) of peak positions in the measured X-ray patterns. For isotropic lattice expansion and signals measured versus the scattering vector magnitude, the peak shifts are proportional to the peak positions, corresponding to a linear stretch of the signal. Therefore, we use Qk basis vectors, where k are free and (Q \u2212 1)k are constrained to be shifted versions of the free basis vectors. This is encoded as follows: Ai,j = interpolate(Abi/(1+`\u03b3)c,zQ, Adi/(1+`\u03b3)e,zQ) for j = zQ + `, z \u2208 [0, k\u22121], ` \u2208 [1, Q\u22121], where \u03b3 is a constant for the shifting granularity and interpolate(x, y) denotes linear interpolation between x and y. By choosing Q to be no smaller than the ratio of the maximum peak shift and the minimum peak width (a known value for a given experiment), a linear sum of the Q mutually-constrained basis patterns can accurately model each instance of the shifting phase pattern.\nConnectivity: The compositions at which a phase (or basis) is observed should form a connected region in composition space, and its lattice parameters should vary smoothly across the region. Hence we build a graph G = (V,E) with n vertices (one vertex per data point) and edges between sample points that have similar compositions. Let G(H`) be the subgraph induced by the vertex set V (H`) = {i : H`,i > 0} \u2286 V (the set of vertices where phase ` is used). We want to enforce G(H`) is connected for ` = 1, \u00b7 \u00b7 \u00b7 , k. The first formulation we consider is flow based. Intuitively, this flow-based encoding defines a flow for each phase ` that can only pass through vertices where the phase is present. There is a source node that injects positive flow in the network, and there is some outgoing flow at every vertex where a phase is used. In order to satisfy flow conservation, there has to be a path from the source to any other node belonging to the same phase in the network. This constraint enforces connectivity but it can be expensive to include in the MIQP formulation when there is a large number of points.\nWe therefore also consider the following variation. For all triples of points v1, v2, v3 that lie on a straight line (in this order) in the composition space, and for every phase `, we enforce the constraint that h`,v2 \u2265 min{h`,v1 , h`,v3}. Although these constraints do not strictly enforce connectivity, they are simpler and often strong enough that the solution obtained is actually connected.\nWe consider synthetic data from (Le Bras et al. 2014), generated from the Aluminium(Al)-Lithium(Li)-Iron(Fe) oxide system and for which the ground truth is known. This dataset has 219 points and 6 underlying phases, where each phase has up to 42 diffraction peaks. We report in Fig. 2 the data interpretation obtained using regular NMF, CombiFD and the ground truth. On average, the number of iterations of AMIQO is about 8, with runtimes ranging from minutes to a few hours. We can see that although we cannot recover exactly the ground truth solution, our interpretation obtained considering prior domain knowledge is much more accurate. For example, using NMF the sparsity constraint is violated for many sample points. In terms of evaluation metric we adapt the previous definition of accuracy to the case of soft cluster assignment, to reflect the fact that a sample point might be assigned to multiple clusters. Namely, we define AC = 1/n \u00b7max\u03c3 \u2211k i=1 |ri \u2229 c\u03c3i |/|ri \u222a c\u03c3i |. Note that this metric does not explicitly reflect the amount of violated combinatorial constraints, which would be interesting to evaluate in future work. We report the results in Table 2. Overall, CombiFD outperforms NMF, confirming the vi-\nsual that incorporating prior knowledge indeed improves the accuracy of the interpretation. Compared with the previous constraint programming formulation (SMT) (Ermon et al. 2012), the CombiFD algorithm exhibits much better scalability, allowing us to quickly analyze more realistic sized datasets with hundreds of sample points in a few hours. In fact, SMT could not find solutions after 20 hours on all instances but one.\nFinally, we show results obtained with CombiFD on a real dataset (Fe-Bi-V oxide system, Fig. 3). While the phase map is unknown for this system, Fig. 3 shows the excellent match between the phase 1 basis pattern from the CombiFD solution and the pattern of the known Bi4V2O11 phase (Bergerhoff and Brown 1987). Chemical alloying of Fe into this compound, of the form Bi4V2\u2212xFexO11\u2212x, has also been observed, which may be related to the identification of this phase 1 basis pattern over a wide composition range in the library (Vannier et al. 2003). The results of Fig. 3 demonstrate the ability of CombiFD to identify well-connected phase regions from experimental data, and further investigations are underway to evaluate the presence of minority phases with weak signals in this composition library."}, {"heading": "Conclusions", "text": "The ability to integrate complex prior knowledge into unsupervised data analysis approaches is a rich and challenging research problem, pervasive in a variety of domains, such as scientific discovery. In particular, the motivating appli-\ncation of this work is the discovery of new fuel cell and solar fuel materials, thereby addressing pressing issues in sustainability such as the need for renewable and clean energy. We introduced CombiFD, a novel factor-based pattern decomposition framework that significantly generalizes and extends prior approaches. Our framework allows the specification of general constraints (including combinatorial ones), which are used to specify a-priori domain knowledge on the factorization that is being sought. These include traditional constraints such as non-negativity as well as more intricate dependencies, such as the ones coming from known phase behavior of chemical systems. We introduced a general factorization algorithm called AMIQO, based on solving a sequence of (mixed-integer) quadratic programs. We showed that AMIQO outperforms state-of-the-art approaches on a key problem in materials discovery: it scales to large real world datasets, it can handle complex, logically structured prior knowledge and by including prior knowledge into the model, we obtain significantly more accurate interpretations of the data. There are many directions to further extend our work, namely concerning representation formalism to capture other combinatorial constraints, with good performance/runtime trade-offs, as well as other algorithms to solve the combinatorial optimization problem."}, {"heading": "Acknowledgments", "text": "Work supported by the National Science Foundation (NSF Expeditions in Computing award for Computational Sustainability, grant 0832782, NSF Inspire grant 1344201 and NSF Eager grant 1258330). Experiments were run\non an infrastructure supported by the NSF Computing research infrastructure for Computational Sustainability grant 1059284. In collaboration with the Energy Materials Center at Cornell (emc2), an Energy Frontier Research Center funded by the U.S. Department of Energy (DOE), Office of Science, Office of Basic Energy Sciences (Award No. de-sc0001086). Research conducted at the Cornell High Energy Synchrotron Source (CHESS) is supported by the NSF and the National Institutes of Health/National Institute of General Medical Sciences under NSF award DMR0936384. Based upon work performed by the Joint Center for Artificial Photosynthesis (JCAP), a DOE Energy Innovation Hub, supported through the Office of Science of the U.S. Department of Energy (Award No. de-sc0004993). Use of the Stanford Synchrotron Radiation Lightsource, SLAC National Accelerator Laboratory, is supported by the U.S. DOE, Office of Science, Office of Basic Energy Sciences under Contract No. DE-AC02-76SF00515."}, {"heading": "Appendix: Proofs", "text": "Proof of Proposition 1 The proof is by induction. Suppose (W j , Hj , xj , bj) is feasible for (2) (the base case j = 0 holds by construction). It follows that (2) augmented with the additional constraint H = Hj is still feasible, and therefore minW,x,b f(W,H0) subject to (2) and H = Hj admits an optimal solution W j+1, x\u0303j+1, b\u0303j+1. Since W j+1, Hj , x\u0303j+1, b\u0303j+1 is feasible for (2), it follows that (2) augmented with the additional constraint W = W j+1 is also feasible. Therefore, minH,x,b f(W j+1, H) subject to (2) and W = W j+1 admits an optimal solution Hj+1, xj+1, bj+1. It also follows that W j+1, Hj+1, xj+1, bj+1 is feasible for (2). Finally, since we are optimizing at every step, it follows that ||A\u2212W jHj ||p \u2265 ||A\u2212W j+1Hj ||p \u2265 ||A\u2212W j+1Hj+1||p.\nProof of Theorem 1 The initial (feasible) values of H0, b0 can be seen as an initial (hard) assignment of data points to clusters, where data point i belongs to cluster s if b0s,i = 1. The optimal solution for minW,b f(W,Hj) subject to (2) and H = Hj is to choose each column W to be the centroid of the data points assigned to the corresponding cluster by bj at iteration j, i.e. for each s set the s-th column of W to be ws = 1/( \u2211 i bs,i) \u2211 aibs,i (non-negative because the data points are assumed to be non-negative). An optimal solution for minH,b f(W j+1, H) subject to (2) and W = W j+1 can be found by assigning each data point i to the cluster\nwhose centroid ws is closest to data point ai, i.e. setting hs\u2217(i),i = bs\u2217(i),i = 1 where s\u2217(i) = argmins ||ws \u2212 ai||2. These operations exactly correspond to k-means clustering initialized with the hard cluster assignment given by b0."}], "references": [{"title": "Algorithms, initializations, and convergence for the nonnegative matrix factorization", "author": ["Albright"], "venue": "Technical report, NCSU Tech Report", "citeRegEx": "Albright,? \\Q2006\\E", "shortCiteRegEx": "Albright", "year": 2006}, {"title": "and Lichman", "author": ["K. Bache"], "venue": "M.", "citeRegEx": "Bache and Lichman 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Constrained clustering: Advances in algorithms, theory, and applications", "author": ["Davidson Basu", "S. Wagstaff 2008] Basu", "I. Davidson", "K. Wagstaff"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2008}, {"title": "and Brown", "author": ["G. Bergerhoff"], "venue": "I.", "citeRegEx": "Bergerhoff and Brown 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "R", "author": ["M.W. Berry", "M. Browne", "A.N. Langville", "V.P. Pauca", "Plemmons"], "venue": "J.", "citeRegEx": "Berry et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "T", "author": ["D. Cai", "X. He", "J. Han", "Huang"], "venue": "S.", "citeRegEx": "Cai et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "C", "author": ["J. Choo", "C. Lee", "Reddy"], "venue": "K.; and Park, H.", "citeRegEx": "Choo et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "M", "author": ["C.H. Ding", "T. Li", "Jordan"], "venue": "I.", "citeRegEx": "Ding. Li. and Jordan 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing", "author": ["Li Ding", "C. Peng 2008] Ding", "T. Li", "W. Peng"], "venue": "Computational Stat. & Data Analysis 52(8):3913\u20133927", "citeRegEx": "Ding et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2008}, {"title": "C", "author": ["S. Ermon", "R. Le Bras", "Gomes"], "venue": "P.; Selman, B.; and van Dover, R. B.", "citeRegEx": "Ermon et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Combinatorial materials science", "author": ["Ginley"], "venue": "In AccessScience. McGraw-Hill Companies", "citeRegEx": "Ginley,? \\Q2005\\E", "shortCiteRegEx": "Ginley", "year": 2005}, {"title": "C", "author": ["Gomes"], "venue": "P.", "citeRegEx": "Gomes 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "F", "author": ["J.M. Gregoire", "M.E. Tague", "S. Cahen", "S. Khan", "H.D. Abruna", "DiSalvo"], "venue": "J.; and van Dover, R. B.", "citeRegEx": "Gregoire et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "J", "author": ["J.A. Haber", "Y. Cai", "S. Jung", "C. Xiang", "S. Mitrovic", "J. Jin", "A.T. Bell", "Gregoire"], "venue": "M.", "citeRegEx": "Haber et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The unreasonable effectiveness of data", "author": ["Norvig Halevy", "A. Pereira 2009] Halevy", "P. Norvig", "F. Pereira"], "venue": "Intelligent Systems,", "citeRegEx": "Halevy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halevy et al\\.", "year": 2009}, {"title": "R", "author": ["M.S. Hossain", "S. Tadepalli", "L.T. Watson", "I. Davidson", "Helm"], "venue": "F.; and Ramakrishnan, N.", "citeRegEx": "Hossain et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "P", "author": ["Hoyer"], "venue": "O.", "citeRegEx": "Hoyer 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Park", "author": ["H. Kim"], "venue": "H.", "citeRegEx": "Kim and Park 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "I", "author": ["D. Kim", "S. Sra", "Dhillon"], "venue": "S.", "citeRegEx": "Kim. Sra. and Dhillon 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "C", "author": ["D. Kuang", "H. Park", "Ding"], "venue": "H.", "citeRegEx": "Kuang. Park. and Ding 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["A.G. Kusne", "T. Gao", "A. Mehta", "L. Ke", "M.C. Nguyen", "K.-M. Ho", "V. Antropov", "C.-Z. Wang", "Kramer"], "venue": "J.; Long, C.; et al.", "citeRegEx": "Kusne et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["R. Le Bras", "T. Damoulas", "J.M. Gregoire", "A. Sabharwal", "C.P. Gomes", "Van Dover"], "venue": "B.", "citeRegEx": "Le Bras et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "C", "author": ["R. Le Bras", "R. Bernstein", "J.M. Gregoire", "S.K. Suram", "Gomes"], "venue": "P.; Selman, B.; and van Dover, R. B.", "citeRegEx": "Le Bras et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "H", "author": ["D.D. Lee", "Seung"], "venue": "S.", "citeRegEx": "Lee and Seung 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "2007", "author": ["T. Li", "C. Ding", "M. I Jordan"], "venue": "Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization. In Data Mining, 2007. ICDM", "citeRegEx": "Li. Ding. and Jordan 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Lin", "C.-J"], "venue": "Neural comp", "citeRegEx": "Lin and C..J.,? \\Q2007\\E", "shortCiteRegEx": "Lin and C..J.", "year": 2007}, {"title": "and Wu", "author": ["H. Liu"], "venue": "Z.", "citeRegEx": "Liu and Wu 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Rapid identification of structural phases in combinatorial thin-film libraries using x-ray diffraction and nonnegative matrix factorization", "author": ["Long"], "venue": "Rev. Sci. Instruments", "citeRegEx": "Long,? \\Q2009\\E", "shortCiteRegEx": "Long", "year": 2009}, {"title": "Combinatorial materials science", "author": ["Mallapragada Narasimhan", "B. Porter 2007] Narasimhan", "S. Mallapragada", "M. Porter"], "venue": null, "citeRegEx": "Narasimhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2007}, {"title": "and Tapper", "author": ["P. Paatero"], "venue": "U.", "citeRegEx": "Paatero and Tapper 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "R", "author": ["F. Shahnaz", "M.W. Berry", "V.P. Pauca", "Plemmons"], "venue": "J.", "citeRegEx": "Shahnaz et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "M", "author": ["P. Smaragdis", "B. Raj", "Shashanka"], "venue": "V.", "citeRegEx": "Smaragdis. Raj. and Shashanka 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Combinatorial synthesis and evalu", "author": ["van Dover Takeuchi", "I. Koinuma 2002] Takeuchi", "R.B. van Dover", "H. Koinuma"], "venue": null, "citeRegEx": "Takeuchi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Takeuchi et al\\.", "year": 2002}, {"title": "Using a literature-based nmf model for discovering gene functional relationships", "author": ["Tjioe"], "venue": "BMC Bioinformatics", "citeRegEx": "Tjioe,? \\Q2008\\E", "shortCiteRegEx": "Tjioe", "year": 2008}, {"title": "Discovery of a useful thin-film dielectric using a composition-spread approach", "author": ["Schneemeyer van Dover", "R.B. Fleming 1998] van Dover", "L. Schneemeyer", "R. Fleming"], "venue": "Nature", "citeRegEx": "Dover et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dover et al\\.", "year": 1998}, {"title": "Bi4V2O11 polymorph crystal structures related to their electrical properties", "author": ["Vannier"], "venue": "Solid State Ionics 157(1):147\u2013153", "citeRegEx": "Vannier,? \\Q2003\\E", "shortCiteRegEx": "Vannier", "year": 2003}, {"title": "Clustering with complex constraints-algorithms and applications", "author": ["Zhi"], "venue": null, "citeRegEx": "Zhi,? \\Q2013\\E", "shortCiteRegEx": "Zhi", "year": 2013}], "referenceMentions": [], "year": 2014, "abstractText": "Identifying important components or factors in large amounts of noisy data is a key problem in machine learning and data mining. Motivated by a pattern decomposition problem in materials discovery, aimed at discovering new materials for renewable energy, e.g. for fuel and solar cells, we introduce CombiFD, a framework for factor based pattern decomposition that allows the incorporation of a-priori knowledge as constraints, including complex combinatorial constraints. In addition, we propose a new pattern decomposition algorithm, called AMIQO, based on solving a sequence of (mixed-integer) quadratic programs. Our approach considerably outperforms the state of the art on the materials discovery problem, scaling to larger datasets and recovering more precise and physically meaningful decompositions. We also show the effectiveness of our approach for enforcing background knowledge on other application domains.", "creator": "LaTeX with hyperref package"}}}