{"id": "1411.7450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2014", "title": "Worst-Case Linear Discriminant Analysis as Scalable Semidefinite Feasibility Problems", "abstract": "countershading In cross-strait this ex-prime paper, we kokcha propose nelia an efficient garabed semidefinite vivah programming (rossman SDP) approach to 441 worst - case 37.88 linear discriminant flamm analysis (WLDA ). arago Compared delijan with allegely the langwell traditional LDA, barican WLDA ndlea considers the dimensionality ajaokuta reduction dhiab problem geq from petrila the aif worst - case brigands viewpoint, formula which is in sutel general more 149.4 robust for classification. However, the dovedale original problem hagenbeck of wuxing WLDA is pleiotropic non - comedia convex and pessanha difficult to optimize. europe-wide In marquard this paper, 1,013 we aeromarine reformulate the eliezer optimization problem muskogee of kreutzer WLDA skittles into a junquera sequence durrance of reichsf\u00fchrer-ss semidefinite orthogoniinae feasibility napierville problems. To efficiently telecomunicacoes solve mesmerize the tiltrotor semidefinite feasibility clwydian problems, rayne we design a new negras scalable nidderdale optimization method with rhapsodically quasi - profligate Newton formia methods failures and felicidade eigen - supandi decomposition torain being debutants the core components. The and-3 proposed method wmiller is orders 16-bit of magnitude prosody faster than tsering standard whispered interior - point based SDP bombed solvers.", "histories": [["v1", "Thu, 27 Nov 2014 02:52:56 GMT  (205kb,D)", "http://arxiv.org/abs/1411.7450v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hui li", "chunhua shen", "anton van den hengel", "qinfeng shi"], "accepted": false, "id": "1411.7450"}, "pdf": {"name": "1411.7450.pdf", "metadata": {"source": "CRF", "title": "Worst-Case Linear Discriminant Analysis as Scalable Semidefinite Feasibility Problems", "authors": ["Hui Li", "Chunhua Shen", "Anton van den Hengel", "Qinfeng Shi"], "emails": ["(chunhua.shen@adelaide.edu.au)."], "sections": [{"heading": null, "text": "Experiments on a variety of classification problems demonstrate that our approach achieves better performance than standard LDA. Our method is also much faster and more scalable than standard interior-point SDP solvers based WLDA. The computational complexity for an SDP with m constraints and matrices of size d by d is roughly reduced from O(m3+md3+m2d2) to O(d3) (m > d in our case).\nIndex Terms\u2014Dimensionality Reduction, Worst-Case Linear Discriminant Analysis, Semidefinite Programming\nI. INTRODUCTION\nDimensionality reduction is a critical problem in machine learning, pattern recognition and computer vision, which for linear case learns a transformation matrix W\u2208 Rd\u00d7r (r \u2264 d) to project the input data x\u2208Rd to a lower-dimensional space y = W>x \u2208 Rr such that the important structure or geometry of input data is preserved. It can help us to eliminate the inherent noise of data, and improve the classification performance. It can also decrease the computational complexities of subsequent machine learning tasks. There are two classical dimensionality reduction methods used widely in many applications, principal component analysis (PCA) and linear discriminant analysis (LDA). PCA is an unsupervised linear dimensionality reduction method, which seeks a subspace of the data that have the maximum variance and subsequently projects the input data onto it. PCA may not give good classification performance due to its unsupervised nature. LDA is in supervised fashion, which aims to maximize the average distance between each two class means and minimize the average distance between each two samples within the same class. However, it has some limitations: 1) For c-class data, the target dimension of the projected subspace is restricted to be at most (c \u2212 1). In this sense, LDA is suboptimal and\nThe authors are with School of Computer Science, The University of Adelaide, Australia. C. Shen and A. van den Hengel are also with Australian Centre for Robotic Vision. Correspondence should be addressed to C. Shen (chunhua.shen@adelaide.edu.au).\nmay cause so called class separation problem that LDA tends to merge classes which are close in the original space; 2) It assumes that conditional probability density functions are normally distributed, which does not hold in many cases; 3) The scatter matrices are required to be nonsingular.\nThere are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]. Among these methods, our focus in this paper is on the discrimination criterion of worst-case linear discriminant analysis (WLDA), which was proposed by Zhang et al. [5]. Instead of using average between-class and within-class distances as LDA, WLDA considers scatter measures from the worst-case view, which arguably is more suitable for applications like classification. Specifically, WLDA tries to maximize the minimum of pairwise distances between class means, and minimize the maximum of within-class pairwise distances over all classes. Due to the complex formulation of its criterion, the problem of WLDA is difficult to optimize.\nIn [5], the problem of WLDA was firstly relaxed to a metric learning problem on Z=WW>, which can be solved by a sequence of SDP optimization procedures, where SDP problems are solved by standard interior-point methods (We denote it as Zhang et al. (SDP)). However, standard interior-point SDP solvers scale poorly to problems with high dimensionality, as the computational complexity is O(m3 +md3 +m2d2), where d is the problem size, and m is the number of constraints in SDP. An alternative optimization procedure was then proposed in [5] for large-scale WLDA problems, in which one column of the transformation matrix was found at each iteration while fixing the other directions. We denote this method as Zhang et al. (SOCP) since it was reformulated as a series of secondorder cone programming (SOCP) problems lastly. Typically, this greedy method does not guarantee to find a globally optimal solution.\nIn this paper we propose a fast SDP approach to solve WLDA problem. The problem is converted to a sequence of SDP feasibility problems using bisection search strategy, which can find a globally optimal solution to the relaxed problem. More importantly, we adopt a novel approach to solve SDP feasibility problem at each iteration. Motivated by [11], a Frobenius-norm regularized SDP formulation is used, and its Lagrangian dual can be solved effectively by quasi-Newton methods. The computational complexity of this optimization method is dominated by eigen-decomposition at each iteration, which is O(d3). The proposed method is denoted as SDWLDA. The main contributions of this work are: 1) By introducing an auxiliary variable, the original WLDA problem is reformulated and can be solved via a sequence of convex\nar X\niv :1\n41 1.\n74 50\nv1 [\ncs .L\nG ]\n2 7\nN ov\n2 01\n4\nfeasibility problems, by which the global optimum can be obtained for the relaxed metric learning problem. 2) By virtue of the use of Frobenius norm regularization, the optimization problem can be addressed by solving its Lagrange dual, where first-order methods such as quasi-Newton can be used. This approach is much faster than solving the corresponding primal problem using standard interior-point methods, and can be applied to large-scale problems. Next, we briefly review some relevant work.\nDimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9]. Assuming dimensions with large within-class covariance are not relevant to subsequent classification tasks, RCA [1] assigns large weights to \u201crelevant dimensions\u201d and small weights to \u201cirrelevant dimensions\u201d, where the relevance is estimated using equivalence constraints. NCA [2] learns the transformation matrix W directly by minimizing the expected leave-one-out classification error of k-nearest neighbours on the transformed space. Because the objective function to be optimized is not convex, NCA tends to converge to a local optimum. NLDA [3], OLDA [4] and EFDC [6] were proposed to address the problem that standard LDA fails when scatter matrices are singular. NLDA maximizes the between-class distance in the null space of the within-class scatter matrix, while OLDA calculates a set of orthogonal discriminant vectors by diagonalizing the scatter matrices simultaneously. The resulting transformation matrices are both orthogonal for NLDA and OLDA, and they are equivalent to each other under a mild condition [12]. EFDC incorporates the intra-class variation into the Fisher discriminant criterion, so that data from the same class can be mapped to a subspace where both the intraclass compactness and intraclass variation are well preserved. In this way, this method is robust to the intraclass variation and results in a good generalization capability. To avoid the class separation problem of LDA, Tao et al. [13] proposed a general averaged divergence analysis (GADA) framework, which presented a general mean function in place of the arithmetic mean used in LDA. By choosing different mean functions, several subspace selection algorithms have been developed. GMSS [7] investigates the effectiveness of the geometric mean-based subspace selection, which maximizes the geometric mean of Kullback-Leibler (KL) divergences between different class pairs. HMSS [8] maximizes the harmonic mean of the symmetric KL divergences between all class pairs. They adaptively give large weights to class pairs that are close to each other, and result in better class separation performance than LDA. Instead of assigning weights to class pairs, MMDA [9] directly maximizes the minimum pairwise distance of all class pairs in the low-dimensional subspace, which guarantees the separation of all class pairs. However, MMDA does not take into account of the within-class pairwise\ndistances over all classes. Recently, Bian et al. [10] presented an asymptotic generalization analysis of LDA, which enriched the existing theory of LDA further. They showed that the generalization ability of LDA is mainly determined by the ratio of dimensionality to training sample size, where both feature dimensionality and training data size can be proportionally large.\nMany dimensionality reduction algorithm such as PCA and LDA can be formulated into a trace ratio optimization problem [14]. Guo et al. [15] presented a generalized Fisher discriminant criterion, which is essentially a trace ratio. They proposed a heuristic bisection way, which was proven to converge to the precise solution. Wang et al. [16] tackled the trace ratio problem directly by an efficient iterative procedure, where a trace difference problem was solved via the eigendecomposition method in each step. Shen et al. [17] provided a geometric revisit to the trace ratio problem in the framework of optimization on the Grassmann manifold. Different from [16], they proposed another efficient algorithm, which employed only one step of the parallel Rayleigh quotient iteration at each iteration. Kokiopoulou et al. [18] also treated the dimensionality reduction problem as trace optimization problems, and gave an overview of the eigenvalue problems encountered in dimensionality reduction area. They made a comparition between nonlinear and linear methods for dimensionality reduction, including Locally Linear Embedding (LLE), Laplacean Eigenmaps, PCA, Locality Preserving Projections (LPP), LDA, etc., and showed that all the eigenvalue problems in explicit linear projections can be regarded as projected analogues of the socalled nonlinear projections.\nDifferent from the aforementioned methods, WLDA considers the dimensionality reduction problem from a worst-case viewpoint. It maximizes the worst-case between-class scatter matrix and minimizes the worst-case within-class scatter matrix simultaneously, which can lead to more robust classification performance. The inner maximization and minimization over discrete variables make it different from the general trace ratio problem, and difficult to solve. The method of solving the general trace ratio problem cannot be extended here directly. Furthermore, different from the iterative algorithm for trace ratio optimization problem [16], we formulate the WLDA problem as a sequence of SDP problems, and propose an efficient SDP solving method. The eigen-decomposition we used is to solve the Lagrange dual gradient, which differs from that employed in solving the trace ratio optimization problem.\nSolving large-scale SDP problems Instead of learning the transformation matrix W, quadratic Mahalanobis distance metric learning methods (which are highly related to dimensionality reduction methods) optimize over Z = WW>, in order to obtain a convex relaxation. The transformation matrix W can be recovered from the eigen-decomposition of Z. Because Z is positive semidefinite (p.s.d.) by definition, quadratic Mahalanobis metric learning methods optimizing on Z usually need to solve an SDP problem.\nXing et al. [19] formulated metric learning as a convex (SDP) optimization problem, and a globally optimal solution can be obtained. Weinberger et al. [20] presented a distance metric learning method, which optimizes a Mahalanobis metric\nsuch that the k-nearest neighbours always belong to the same class while samples from different classes are separated by a large margin. In terms of SDP solver, they proposed an alternate projection method, where the learned metric Z is projected back onto the p.s.d. cone by eigen-decomposition at each iteration. MMDA [9] was solved approximately by a sequence of SDP problems using standard interior-point methods. Shen et al. [21] proposed a novel SDP based method for directly solving trace quotient problems for dimensionality reduction. With this method, globally-optimal solutions can be obtained for trace quotient problems.\nAs we can see, many aforementioned methods used standard interior-point SDP solvers, which are unfortunately computationally expensive (computational complexity is O(m3 + md3 +m2d2)) and scale poorly to large-scale problems. Thus an efficient SDP optimization approach is critical for largescale metric learning problems.\nThere are many recent work to address large-scale SDP problems arising from distance metric learning and other computer vision tasks. Shen et al. [11] proposed a fast SDP approach for solving Mahalanobis metric learning problem. They introduced a Frobenius-norm regularization in the objective function of SDP problems, which leads to a much simpler Lagrangian dual problem: the objective function is continuously differentiable and p.s.d. constraints in the dual can be eliminated. L-BFGS-B was used to solve the dual, where a partial eigen-decomposition needed to be calculated at each iteration. Wang et al. [22] also employed a similar dual approach to solve binary quadratic problems for computer vision tasks, such as image segmentation, co-segmentation, image registration. SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems. The key motivation of [23], [24] is that the objective function of the corresponding dual problem is continuously differentiable but not twice differentiable, therefore first-order methods can be applied. Malick [24] and Boyd and Xiao [23] proposed to use quasi-Newton methods and projected gradient methods respectively, to solve the Lagrangian dual of semidefinite leastsquares problems. Semismooth Newton-CG methods [25] and smoothing Newton methods [26] are also exploited for semidefinite least-squares problems, which require much less number of iterations at the cost of higher computational complexity per iteration (full eigen-decomposition plus conjugate gradient).\nAlternatively, stochastic (sub)gradient descent (SGD) methods [27] were also employed to solve SDP problems. Combining with alternating direction methods [28], [29], SGD can be used for SDP problems with inequality and equality constraints. The computational bottleneck of typical SGD is the projection of one infeasible point onto the p.s.d. cone at each iteration, which leads to the eigen-decomposition of a d \u00d7 d matrix. A number of methods have been proposed to speed up the projection operation at each iteration. Chen et al. [30] proposed a low-rank SGD method, in which rankk stochastic gradient is constructed and then the projection operation is simplified to compute at most k eigenpairs. In the works of [31], [32], [33], [34], the distance metric is updated\nby rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required. Note that SGD methods usually need more iterations to converge than the dual approaches based on quasi-Newton methods [11].\nThe most related work to ours may be Shen et al.\u2019s [11]. We use similar SDP optimization technique as that in [11]. However, SDP feasibility problems are considered in our paper while the work in [11] focuses on standard SDP problems with linear objective functions.\nNotation We use a bold lower-case letter x to denote a column vector, and a bold capital letter X to denote a matrix. X> is the transposition of X. Rm\u00d7n indicates the set of m\u00d7n matrices. In represents an n \u00d7 n identity matrix. X < Y indicates that the matrix X \u2212 Y is positive semidefinite. \u3008\u00b7, \u00b7\u3009 denotes the inner product of two matrices or vectors. Tr(\u00b7) indicates the trace of a matrix. \u2016 \u00b7 \u2016F is the Frobenius norm of a matrix. diag(\u00b7) returns a diagonal matrix with the input elements on its main diagonal. Suppose that the eigen-decomposition of a symmetric matrix X \u2208 Rn\u00d7n is X = Udiag(\u03bb1, \u03bb2, . . . , \u03bbn)U\n>, where U is the orthonormal matrix of eigenvectors of X, and \u03bb1, . . . , \u03bbn are the corresponding eigenvalues, we define the positive and negative parts of X respectively as\n(X)+ = Udiag(max(\u03bb1, 0), . . .max(\u03bbn, 0))U >, (1) (X)\u2212 = Udiag(min(\u03bb1, 0), . . .min(\u03bbn, 0))U > (2)\nIt is clear that X = (X)+ + (X)\u2212 holds."}, {"heading": "II. WORST-CASE LINEAR DISCRIMINANT ANALYSIS", "text": "We briefly review WLDA problem proposed by [5] firstly. Given a training set of n samples D = {x1, . . . ,xn} (xn \u2208 Rd), which consists of c \u2265 2 classes \u03a0i, i = 1, . . . , c, where class \u03a0i contains ni samples. As we mentioned before, the aim of linear dimensionality reduction is to find a transformation matrix W \u2208 Rd\u00d7r with r \u2264 d.\nWe define the within-class scatter matrix of the kth class \u03a0k as\nSk = 1\nnk \u2211 xi\u2208\u03a0k (xi \u2212mk)(xi \u2212mk)>, (3)\nwhich is also the covariance matrix for the kth class, and mk =\n1 nk \u2211 xi\u2208\u03a0k xi is the class mean of the kth class \u03a0k.\nThe between-class scatter matrix of the ith and jth classes is defined as\nSij = (mi \u2212mj)(mi \u2212mj)>. (4)\nUnlike LDA which seeks to minimize the average withinclass pairwise distance, the within-class scatter measure used in WLDA is defined as\n\u03c1w = max 1\u2264k\u2264c\n{ Tr(W>SkW) } , (5)\nwhich is the maximum of the within-class pairwise distances over all classes.\nOn the other hand, the between-class scatter measure used in WLDA is defined as\n\u03c1b = min 1\u2264i<j\u2264c\n{ Tr(W>SijW) } . (6)\n\u03c1b uses the minimum of the pairwise distances between class means, instead of the average of distances between each class mean and the sample mean employed in LDA.\nThe optimality criterion of WLDA is defined as to maximize the ratio of the between-class scatter measure to the withinclass scatter measure:\nmax W \u03c1b \u03c1w , (7a)\ns.t. W>W = Ir. (7b)\nAs stated in [5], this problem (7) is not easy to optimize with respect to W, so a new variable Z = WW> \u2208 Rd\u00d7d is introduced, and the problem (7) is formulated as a metric learning problem.\nTheorem 2.1. Define sets \u21261 = {WW> : W>W = Ir,W \u2208 Rd\u00d7r}, and \u21262 = {Z : Z = Z>,Tr(Z) = r,0 4 Z 4 Id}. Then \u21261 is the set of extreme points of \u21262, and \u21262 is the convex hull of \u21261.\nThis theorem has been widely used and its proof can be found in [35]. According to Theorem 2.1, the orthonormal constraint on W can be relaxed to convex constraints on Z = WW>, and the problem (7) can be relaxed to the following problem defined on a p.s.d. variable Z [5]:\nmax Z min1\u2264i<j\u2264c{Tr(SijZ)} max1\u2264k\u2264c{Tr(SkZ)} , (8a)\ns.t. Tr(Z) = r, (8b) 0 4 Z 4 Id. (8c)\nOnce the optimal solution Z? is obtained, the optimal W? for problem (7) can be recovered using the top r eigenvectors of Z?.\nIn [5], Zhang et al. proposed two methods to solve (7), as we stated in Section I. In the first one, an iterative algorithm was presented to solve the relaxed problem (8), where an SDP problem needs to be solved at each step by standard SDP solver. This method is not scalable to problems with high dimensionality or large training data points. The second one is based on a greedy approach, which cannot guarantee to find a globally optimal solution.\nHence, in the next section, we will describe our algorithm (so called SD-WLDA) of finding the transformation matrix Z = WW> that maximizes (8), and demonstrate how to solve it using an efficient approach."}, {"heading": "III. A FAST SDP APPROACH TO WLDA", "text": "In this section, problem (8) is firstly reformulated into a sequence of SDP optimization problems based on bisection search. Then, a Frobenius norm regularization is introduced and the SDP problem in each step is solved through Lagrangian dual formulation. With this SD-WLDA method, the global optimum can be acquired for the relaxed problem (8). The computational complexity can be reduced as well by solving the dual problem using quasi-Newton methods, compared with solving the primal problem directly using interior-point based algorithm.\nAlgorithm 1 Solving problem (9) by bisection search. Input: \u03b4l: the lower bound of \u03b4; \u03b4u: the upper bound of \u03b4;\nand the tolerance \u03c3 > 0. while |\u03b4u\u2212\u03b4l|\u03b4l > \u03c3 do\n1). \u03b4\u25e6 = \u03b4l+\u03b4u2 . 2). Solve SDP feasibility problem (10) if (10) is feasible then\nGet the feasible solution Z, \u03b4l = \u03b4\u25e6; else \u03b4u = \u03b4\n\u25e6. end if\nend while 3). \u03b4? = \u03b4\u25e6, and save the corresponding Z?.\nOutput: Z?,\u03b4?."}, {"heading": "A. Problem Reformulation", "text": "By introducing an auxiliary variable \u03b4, problem (8) can be rewritten as\nmax \u03b4,Z \u03b4, (9a)\ns.t. Tr(SijZ) \u2265 \u03b4Tr(SkZ), \u22001 \u2264 i < j \u2264 c, 1 \u2264 k \u2264 c, (9b)\nTr(Z) = r, (9c) 0 4 Z 4 Id. (9d)\nThere are two variables \u03b4,Z to be optimized in problem (9), but we are interested in finding Z that can maximize \u03b4. Problem (9) is clearly non-convex with respect to \u03b4 and Z since the constraint (9b) is not convex. However, noting that (9b) will become linear if \u03b4 is given, we employ the bisection search strategy and convert the optimization problem (9) into a set of convex feasibility problems, by which the global optimum can be computed effectively.\nLet \u03b4? denote the optimal value of (9a). Given \u03b4\u25e6 \u2208 R, if the convex feasibility problem\nfind Z, (10a) s.t. Tr(SijZ) \u2265 \u03b4\u25e6Tr(SkZ), \u22001 \u2264 i < j \u2264 c, 1 \u2264 k \u2264 c,\n(10b) Tr(Z) = r, (10c) 0 4 Z 4 Id, (10d)\nis feasible, then \u03b4? \u2265 \u03b4\u25e6. Otherwise, if the above problem is infeasible, then \u03b4? < \u03b4\u25e6.\nAlgorithm 1 shows the bisection search based optimization process. Once a feasible solution Z? is obtained which maximize \u03b4, Z? will be the globally optimal solution to the relaxed problem (8). The optimal W? for problem (7) can be acquired using the top r eigenvectors of Z?."}, {"heading": "B. Lagrangian Dual Formulation", "text": "Algorithm 1 shows that an SDP feasibility problem needs to be solved at each step during the bisection search process. Considering that standard interior-point SDP solvers have a computational complexity of O(m3 + md3 + m2d2), where d is the dimension of input data, and m is the number of constraints in SDP, it becomes quite expensive for processing\nhigh-dimensional data. In this subsection, we reformulate the feasibility problem (10) into a Frobenius norm regularized SDP problem, which can be efficiently solved via its Lagrangian dual using first-order methods like quasi-Newton. The computational complexity will be reduced to O(d3). The primal solution Z? can then be calculated from the dual solution based on Karush-Kuhn-Tucker (KKT) [36, p. 243] conditions.\nThe problem (10) can be expressed equivalently in the following form:\nfind X = [ Z 0 0 Q ] , (11a)\ns.t. Tr(S\u0304ijkX) \u2265 0, \u22001 \u2264 i < j \u2264 c, 1 \u2264 k \u2264 c, (11b) Tr(I\u0304dX) = r, (11c)\nTr(H>stX) = Id(s, t), \u22001 \u2264 t \u2264 s \u2264 d, (11d) X < 0, (11e)\nwhere S\u0304ijk = [\nSij \u2212 \u03b4\u25e6Sk 0 0 0\n] , I\u0304d = [ Id 0 0 0 ] , and\nHst(p, q) =  1 p = s, q = t; or p = t, q = s; or p = s+ d, q = t+ d; or p = t+ d, q = s+ d;\n0 Otherwise.\n.\nIn the above formulation, the variable 0 4 Z 4 Id is replaced by X = [\nZ 0 0 Q\n] < 0, where Q = Id \u2212 Z. The\nconstraints (11b) and (11c) correspond to (10b) and (10c), respectively. The constraints (11d) stem from the fact that Q + Z = Id.\nProposition 3.1. Given \u03b4\u25e6 \u2208 R, if the problem (10) and equivalently (11) is feasible, one feasible solution exists for the following semidefinite least-squares problem:\nmin X\n1 2 \u2016X\u20162F , (12a)\ns.t. Tr(S\u0304ijkX) \u2265 0, \u22001 \u2264 i < j \u2264 c, 1 \u2264 k \u2264 c, (12b) Tr(I\u0304dX) = r, (12c)\nTr(H>stX) = Id(s, t), \u22001 \u2264 t \u2264 s \u2264 d, (12d) X < 0, (12e)\nIf the problem (12) is feasible, its optimal solution X? can be used as a feasible solution to (11), and one solution Z? to problem (10) can be acquired as well.\nThe problem (12) is a standard semidifinite least-square problem and can be solved readily by off-the-shelf SDP solvers. However, as we mentioned before, the computational complexity is really high if we solve the primal problem directly by standard interior-point SDP solvers. It will greatly hamper the use of WLDA in large-scale problems. Thanks to the Frobenius norm regularization in the objective function of (12), we can use Lagrangian dual approach to solve the problem easily.\nIntroducing the Lagrangian multipliers u \u2208 R 12 (c3\u2212c2), v \u2208 R, p \u2208 R 12 (d2+d) corresponding to the constraints (12b)-(12d), and a symmetric matrix Y \u2208 Rd\u00d7d corresponding to the p.s.d. constraint (12e), the following result can be acquired.\nProposition 3.2. The Lagrangian dual problem of (12) can be simplified in the following form:\nmin u,v,p\n1 2 \u2016(A\u0304)+\u20162F \u2212 vr \u2212 d\u2211 s=1 pss, s.t. u \u2265 0, (13)\nwhere A\u0304 = \u2211 i,j,k uijkS\u0304ijk + vI\u0304d + \u2211 s,t pstH > st. (14)\nFurthermore, the optimal solution to problem (12) is X? = (A\u0304?)+, where A\u0304? = \u2211 i,j,k u ? ijkS\u0304ijk + v ?I\u0304d + \u2211 s,t p ? stH > st, which is calculated based on the optimal dual variables u?, v?, and p?.\nFrom the definition of A\u0304? and the operator (\u00b7)+, X? is forced to be p.s.d. and block-diagonal, so the optimal solution Z? to problem (10) can be acquired easily. In addition, it is noticed that the objective function of (13) is differentiable (but not necessarily to be twice differentiable), it allows us to solve the dual problem efficiently using first-order methods, such as quasi-Newton methods.\nThe gradient of the objective function in problem (13)\nis g(u, v,p) =  \u3008A\u0304+, Sijk\u3009, \u22001 \u2264 i < j \u2264 c, 1 \u2264 k \u2264 c\u3008A\u0304+, Id\u3009 \u2212 r \u3008A\u0304+, H>st\u3009 \u2212 \u03b8, \u22001 \u2264 t \u2264 s \u2264 d , where \u03b8 = { 1 s = t;\n0 s 6= t."}, {"heading": "C. Feasibility Condition", "text": "As stated in Proposition 3.1, if (10) is feasible, a solution can be found by solving the problem (12). During running quasi-Newton algorithms to solve the problem (12), an infeasibility condition of the problem (10) needs to be checked iteratively, which is presented here:\nProposition 3.3. If the following conditions are satisfied{ \u2016(A\u0304)+\u2016F |vr+ \u2211d s=1 pss| < ,\nvr + \u2211d s=1 pss > 0,\n(15)\nthen the problem (10) is considered as infeasible.\nThis infeasibility condition can be deduced from a general conic feasibility problem presented in [37]. Explanations are presented in detail in the appendix. We check this condition at each iteration of quasi-Newton algorithms. A\u0304+ is evaluated during calculating the dual objective function, so it will not bring extra computational cost. Once the condition (15) is satisfied, problem (10) (equivalently (11)) is not feasible and quasi-Newton algorithms will be stopped. Otherwise, quasiNewton algorithms keep running until convergence, and then a feasible solution X? = (A\u0304?)+ to the problem (11) is found."}, {"heading": "D. Solving the Feasibility Problem", "text": "In this subsection, we summarize the procedure of solving the problem (10) by our fast SDP optimization algorithm. It has been domenstrated that we can find the feasible solution to (10) by solving the dual problem (13) with quasi-Newton\nAlgorithm 2 Optimization procedure for solving problem (10). Input: \u03b4\u25e6, flag = 1. Initialize dual variables u, v, p.\n1). Solve the dual (13) using L-BFGS-B. repeat\n1.1). Calculate the objective and gradient of the objective function in (13). 1.2). Check the feasibility condition (15): if the condition (15) is satisfied then\nflag = 0 and break. end if 1.3). Update dual variables u, v, p.\nuntil L-BFGS-B is converged. 2). Compute X? by eigen-decomposition. 3). Decompose Z? from X?.\nOutput: Z?, flag (1: feasible, 0: infeasible).\nmethods. In this work, we use L-BFGS-B [38], [39], a limitedmemory quasi-Newton algorithm package, which can handle the problem with box constraints. Here we only need to provide the callback function to L-BFGS-B, which calculates the objective function of (13) and its gradient. The procedure of finding the feasible solution Z is described in Algorithm 2."}, {"heading": "E. Computational Complexity Analysis", "text": "The computational complexity of L-BFGS-B is O(Km), where K is a moderate number between 3 to 20, m = 1 2 (c 3 \u2212 c2) + 1 + 12 (d 2 + d) is the problem size to be solved by L-BFGS-B, which is equal to the number of constraints in the primal SDP problem (12). At each iteration of L-BFGSB, the eigen-decomposition of a 2d \u00d7 2d matrix is carried out to compute A\u0304+, which is used to evaluate all the dual objective values, gradients, as well as the feasibility conditions (15). The computational complexity is O(d3). Hence, the overall computational complexity of our algorithm SD-WLDA is O(Km + d3). Since Km d3, eigen-decomposition dominates the most computational time of SD-WLDA, which is O(d3). On the other hand, solving an SDP problem using standard interior-point methods needs a computational complexity of O(m3 + md3 + m2d2). Since m > d in our case, our algorithm is much faster than interior-point methods, and can be used to large-scale problems."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, experiments are performed to verify the performance of SD-WLDA. We conduct comparisons between SD-WLDA and other methods on both classification performance and computational complexity. The classification performance is contrasted between SD-WLDA and LDA, LMNN, OLDA. We also compare the performance of our SDWLDA with both optimization methods proposed by Zhang et al. in [5] (Zhang et al. (SDP) and Zhang et al. (SOCP) receptively). This can be used to verify the correctness of our algorithm. The computational complexity is compared between SD-WLDA, standard interior-point algorithms to solve our SDP formulation (SDPT3 [40] and SeDuMi [41]),\nZhang et al. (SDP) which uses SDPT3 as well, and Zhang et al. (SOCP).\nAll algorithms are tested on a 2.7GHz Intel CPU with 20G memory. The SD-WLDA algorithm is implemented in Matlab, where the Fortran code of L-BFGS-B is employed to solve the dual problem (13). The Matlab routine \u201ceig\u201d is used to compute eigen-decomposition. The tolerance setting of LBFGS-B is set to default. The tolerance \u03c3 in Algorithm 1 is set to 1e\u22123, and the parameter in the feasibility condition (15) is set to 1e\u22123."}, {"heading": "A. Experiments on UCI Datasets", "text": "Some UCI datasets [42] are used here firstly. We perform 30 random splits for each dataset, with 70% as training samples and 30% as test samples. The classification performance is evaluated based on 5 nearest neighbour (5-NN) classifier. For fair comparison with LDA, the final dimension is set to (c\u22121).\nThe experimental results are presented in Table I, where the baseline results are obtained by applying 5-NN classifier on the original feature space directly. For each dataset, the experiment runs 30 times, and the error rate is reported by the mean error as well as the standard deviation. The smallest classification error is shown in bold. The results illustrate that WLDA gives smaller classification error rates compared to other algorithms in most datasets. The classification results by our fast SDP solving algorithm SD-WLDA and Zhang et al. (SDP) are quite similar, with small difference coming from numerical error during computation. The error rates calculated by Zhang et al. (SOCP) are sometimes quite different from that by Zhang et al. (SDP) and SD-WLDA, e.g., on \u201cHeart\u201d and \u201cWaveform\u201d datasets, which results from different relaxation methods and optimization procedures employed.\nIn terms of computational speed, SD-WLDA approach is much faster than other methods. Zhang et al. (SDP), which is also solved by standard interior-point algorithm SDPT3, is faster than Ours (SDPT3), because of different SDP problem formulations. The merit of SD-WLDA on computation is even more dramatic for high dimensional problems. For example, we compare the computational speeds of SD-WLDA and SDPT3 on the datasets of \u201cIris\u201d and \u201cWaveform\u201d, which have the same number of classes. SD-WLDA is about 5 times faster than Zhang et al. (SDP), and 20 times faster than Ours (SDPT3) on \u201cIris\u201d which has 105 training samples with the input dimension as 4, whereas it becomes 12 times quicker than Zhang et al. (SDP), and 300 times quicker than Ours (SDPT3) on \u201cWaveform\u201d which has 3500 training samples with the input dimension as 40. The computational time increases more significantly for SeDuMi with respect to input dimension. Zhang et al. (SOCP) has no computational advantage on solving problems with few training samples and low dimensionality. The computational superiority appears when dimensionality increases, referring to the results on \u201cWaveform\u201d dataset, which is quicker than Zhang et al. (SDP). Because of the column-wise iteration solving method of Zhang et al. (SOCP), the computational complexity of Zhang et al. (SOCP) relates closely with the final dimension. That is why Zhang et al. (SOCP) is quite\nTABLE I TEST ERRORS AND COMPUTATIONAL TIME OF DIFFERENT METHODS ON UCI DATASETS WITH 5-NN CLASSIFIER. THE TEST ERROR IS THE AVERAGE OVER 30 RANDOM SPLITS, WITH STANDARD DEVIATION SHOWN IN THE BRACKET. THE COMPUTATIONAL TIME IS ALSO THE AVERAGE OVER 30 RUNS. SD-WLDA IS EFFICIENT IN COMPUTATION, AND GIVES COMPARABLE CLASSIFICATION PERFORMANCE COMPARED TO OTHER METHODS.\nHeart Waveform Iris Balance Sonar Ionosphere # Train 206 3500 105 438 146 246 # Test 88 1500 45 187 62 105 # Classes 5 3 3 3 2 2 Input Dim. 13 40 4 4 60 34 Final Dim. 4 2 2 2 1 1 Error Rates (%) Euclidean 45.58 (3.66) 18.44 (0.94) 3.26 (1.82) 15.03 (1.94) 25.00 (4.76) 16.19 (2.26) LDA 36.10 (3.14) 15.60 (0.65) 3.19 (1.62) 10.88 (1.85) 27.10 (3.94) 15.43 (2.34) LMNN 41.33 (3.43) 14.27 (0.92) 3.26 (1.62) 11.50 (6.25) 49.73 (6.17) 20.86 (3.40) OLDA 36.36 (3.52) 15.58 (0.63) 3.19 (1.72) 10.90 (2.20) 26.61 (3.25) 16.14 (3.23) Zhang et al. (SDP) 35.38 (4.09) 15.49 (0.96) 2.89 (1.67) 10.86 (2.49) 27.10 (3.47) 15.70 (3.21) Zhang et al. (SOCP) 37.12 (3.68) 17.34 (2.10) 2.96 (1.78) 10.43 (2.19) 26.83 (2.87) 15.70 (3.39) SD-WLDA 35.57 (4.64) 15.47 (1.01) 2.89 (1.67) 10.80 (2.23) 26.94 (3.32) 15.43 (2.34) Computation Time Ours (SDPT3) 57.3s 16m40s 8.7s 11.1s 53m2s 5m20s Ours (SeDuMi) 23.5s 1h31m 3.7s 5.2s 12h30m 30m10s Zhang et al. (SDP) 16.5s 37.0s 2.3s 2.0s 21.6s 58.2s Zhang et al. (SOCP) 71.8s 36.0s 26.2s 27.9s 14.1s 13.9s SD-WLDA 9.9s 3.0s 0.4s 0.9s 3.9s 1.2s\nfast on \u201cSonar\u201d and \u201cIonosphere\u201d datasets, which set the final dimension to 1. However, it still slower than SD-WLDA.\nTo prove the robust classification performance of SDWLDA, we change the ratio of training samples \u03b1 from 20% to 80% on datasets \u201cSonar\u201d and \u201cIonosphere\u201d. For each value of \u03b1, we calculate the average test error as well as the standard deviation across 10 trials by SD-WLDA and LDA respectively. The results in Fig. 1 demonstrate that SD-WLDA is more superior than LDA when there is small number of training samples. This phenomenon illustrates that WLDA alleviates the dependence of classification performance on large number of training samples.\nLDA requires the data to map to at most (c\u22121) dimension, while SD-WLDA, which is based on an SDP optimization method, does not have such a restriction. Here we perform another experiment by SD-WLDA on \u201cHeart\u201d dataset, with different final dimensions. The result in Table II shows that (c \u2212 1) is not the best final dimension for \u201cHeart\u201d. So SDWLDA algorithm is more flexible."}, {"heading": "B. Experiments on Face, Object and Letter Datasets", "text": "As shown before, SD-WLDA algorithm can be used to solve large-scale problems due to its efficiency on computation. In this section, experiments are carried out on face, object and letter image datasets, which have high input dimension and more classes. The images are resized to different resolution before experiments, as shown in Table III. PCA has been applied beforehand to reduce the original dimension and also to remove noises. The final dimension is still set to be (c\u22121) for fair comparison to LDA.\n1) Face recognition: four face databases are employed here. ORL [43] consists of 400 face images of 40 individuals, each with 10 images. We randomly choose 70% of the samples for training and the remaining 30% for test. The Yale dataset [44] contains 165 grey-scale images of 15 individuals, 11 images per subject. We split them into training and test sets by 7/3 sampling as well. PIE dataset (Pose C29) [45] has 40 subjects, and 24 images for each individual. 80% of the samples are chosen randomly for training. UMist dataset [46]\ncontains 564 grayscale images of 20 different people. We only use 30% of the samples for training to test the performance of SD-WLDA.\nExperimental results in Table III show that SD-WLDA gives better classification performance for all datasets. The classification error rates by SD-WLDA and Zhang et al. (SDP) are identical with each other, as PCA used before has already removed the noises, which proves the correctness of our algorithm. However, SD-WLDA is much faster than Zhang et\nTABLE II IN CONTRAST TO LDA, SD-WLDA CAN BE USED TO PROJECT DATA TO FINAL DIMENSION LARGER THAN (c\u2212 1). THE TEST ERROR IS BY SD-WLDA WITH 5-NN CLASSIFIER ON \u201cHEART\u201d DATASET USING DIFFERENT FINAL DIMENSION. THE TEST ERROR IS THE AVERAGE ERROR OF 10 RUNS, WITH STANDARD DEVIATION IN THE BRACKET.\nFinal Dim. 4 6 7 8 9 SD-WLDA (%) 32.84 (3.95) 28.41 (2.94) 28.41 (2.29) 31.82 (3.88) 34.09 (4.84)\nal. (SDP) method. For example, SD-WLDA runs almost 14 times faster than Zhang et al. (SDP) on Yale dataset. The error rates calculated by Zhang et al. (SOCP) are rather larger, which result from the non-globally optimal solution the algorithm reached. The computational superiority of Zhang et al. (SOCP) does not show up as well.\nIn order to illustrate the computational speed of SD-WLDA and both methods in [5] with respect to the number of classes and the input data dimension (here it refers to the dimension after PCA) respectively, more experiments are performed on Yale dataset. Firstly, we set the dimension after PCA to be 50, and change the number of classes from 9 to 15. Experimental results in Fig. 2(1) demonstrate that compared with the other methods, the speed of SD-WLDA is less sensitive to the increase of the amount of classes. Since the final dimension r is set to be (c\u2212 1), the computational complexity of Zhang et al. (SOCP) jumps up obviously with the increase of classes. Secondly, we use all classes, and let the dimension after PCA change from 30 to 115. Experimental results in Fig. 2(2) show that the computational cost of SD-WLDA rises up pretty slower with the growth of input dimension, in contrast to Zhang et al. (SDP). Zhang et al. (SOCP) becomes faster than Zhang et al. (SDP) when input dimension is larger than 90. This phenomenon certifies that Zhang et al. (SOCP) is more suitable for processing high dimensional datasets than Zhang et al. (SDP), as [5] presented. However, this method cannot guarantee to find a global optimal solution. Finally, we test on Yale dataset with an even high input dimension as 800. Experimental results in Table IV demonstrate that SD-WLDA is absolutely faster than Zhang et al. (SDP), both of which lead to similar classification error rates. Although Zhang et al. (SOCP) is comparable with SD-WLDA on computational time, the error rate it obtained is relatively bigger.\nIn addition, to show the superiority of SD-WLDA on classification, another experiment is conducted using SD-WLDA and LDA on Yale dataset, with the dimension after PCA as 50 and 15 classes. We reduce the final dimension from (c\u22121) to 1. The test results shown in Fig. 3 demonstrate that SDWLDA always gives lower test error than LDA, which further proves the good classification performance of SD-WLDA.\n2) Object recognition: Three datasets are used here: Coil20 [47], Coil30 [48], and ALOI [49]. Coil20 dataset contains 1440 grey-scale images with black background for 20 objects, with each containing 72 different images. Coil30 dataset consists of 750 RGB images for 100 objects. We choose the first 30 objects and convert them into greyscale images in our experiment. ALOI dataset consists of 1000 objects taken at varied viewing angles, illumination angles, etc.. We use the first 25 objects here, with 24 images for every object. Different training and test splitting ratios are\nadopted for different datasets in order to test the performance under different situations, which can be found in Table III. The experimental results demonstrate that SD-WLDA is better in computational speed. Although OLDA produces the smallest classification error on Coil20 (2.91%), SD-WLDA gives a comparable result (3.14%).\n3) Letter recognition: The Binary Alphadigits dateset [50] is employed here. The dataset BA1 contains 10 digits of 0 to 9, and BA2 contains 15 capital letters A through O. Experimental results in Table III present that WLDA produces better classification performance on both databases. Zhang et al. (SDP) and SD-WLDA give similar classification results, while Zhang et al. (SOCP) lead to much larger error rates. In terms of computational speed, SD-WLDA runs 5 times faster than Zhang et al. (SDP) on BA1, and more than 9 times faster than Zhang et al. (SDP) on BA2, which has more training samples and number of classes. This experiment demonstrates again that SD-WLDA is more efficient in processing largescale datasets.\n4) Classification performance regarding to number of classes: It has been validated that SD-WLDA can give smaller classification errors than LDA using a small size of training set. In this section, we evaluate the classification performance of SD-WLDA and LDA with respect to the number of classes c. Take the datasets ORL and Coil20 as examples. The experimental settings for each dataset are shown in the captions of Table V and VI respectively. We choose different numbers of classes from each dataset, and compare the test errors of SDWLDA and LDA. The results in Tables V and VI demonstrate that when the number of classes is small, SD-WLDA and LDA have comparable classification results, whereas when the number of classes increases, SD-WLDA shows better classification performance."}, {"heading": "V. CONCLUSION", "text": "In this work, an efficient SDP optimization algorithm has been proposed to solve the problem of worst-case linear discriminant analysis. WLDA takes into account the worstcase pairwise distance between and within classes, which achieves better classification performance than conventional LDA. In order to reduce the computational complexity so that it can be applied to large-scale problems, a fast algorithm has been presented by introducing the Frobenius norm regularization, and its Lagrangian dual can be simplified. Using our algorithm, the global optimum can be obtained in O(d3) time. The algorithm is simple to implement and much faster than conventional SDP solvers. Experimental results on some UCI databases as well as face and object recognition tasks show the effectiveness on classification performance and the efficiency on computation of SD-WLDA."}], "references": [{"title": "Adjustment learning and relevant component analysis", "author": ["N. Shental", "T. Hertz", "D. Weinshall", "M. Pavel"], "venue": "Proc. Eur. Conf. Comp. Vis., 2002, pp. 776\u2013790.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L. Saul"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 18, 2006, pp. 1473\u20131481.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A new lda-based face recognition system which can solve the small sample size problem", "author": ["L.-F. Chen", "H.-Y.M. Liao", "M.-T. Ko", "J.-C. Lin", "G.-J. Yu"], "venue": "Pattern Recogn., vol. 33, no. 10, pp. 1713\u20131726, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems", "author": ["J. Ye", "B. Yu"], "venue": "J. Mach. Learn. Res., vol. 6, no. 4, pp. 483\u2013502, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Worst-case linear discriminant analysis", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 23, 2010, pp. 2568\u20132576.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhanced fisher discriminant criterion for image recognition", "author": ["Q. Gao", "J. Liu", "J. Zhang", "J. Hou", "X. Yang"], "venue": "Pattern Recogn., vol. 45, no. 10, pp. 3717\u20133724, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric mean for subspace selection", "author": ["D. Tao", "X. Li", "X. Wu", "S.J. Maybank"], "venue": "IEEE Trans. Anal. Mach. Intell., vol. 31, no. 2, pp. 260\u2013274, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Harmonic mean for subspace selection", "author": ["W. Bian", "D. Tao"], "venue": "in Proceedings of the 19th International Conference on Pattern Recognition, 2008, pp. 1\u20134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Max-min distance analysis by using sequential sdp relaxation for dimension reduction", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Anal. Mach. Intell., vol. 33, no. 5, pp. 1037\u20131050, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Asymptotic generalization bound of fisher\u2019s linear discriminant analysis", "author": ["\u2014\u2014"], "venue": "CoRR, vol. abs/1208.3030, 2012. [Online]. Available: http://arxiv.org/abs/1208.3030", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A scalable dual approach to semidefinite metric learning", "author": ["C. Shen", "J. Kim", "L. Wang"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011, pp. 2601\u20132608.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational and theoretical analysis of null space and orthogonal linear discriminant analysis", "author": ["J. Ye", "T. Xiong"], "venue": "J. Machine Learning Research, vol. 7, pp. 1183\u20131204, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "General averaged divergence analysis", "author": ["D. Tao", "X. Li", "X. Wu", "S.J. Maybank"], "venue": "Proc. of IEEE International Conference on Data Mining, 2007, pp. 302\u2013311.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Trace ratio problem revisited", "author": ["Y. Jia", "F. Nie", "C. Zhang"], "venue": "IEEE Trans. Neural Netw., vol. 20, no. 4, pp. 729\u2013735, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A generalized foleysammon transform based on generalized fisher discriminant criterion and its application to face recognition", "author": ["Y. Guo", "S. Li", "J. Yang", "T. Shu", "L. Wu"], "venue": "Pattern Recogn. Lett., vol. 24, no. 1-3, pp. 147\u2013158, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Trace ratio vs. ratio trace for dimensionality reduction", "author": ["H. Wang", "S. Yan", "D. Xu", "X. Tang", "T. Huang"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2007, pp. 1\u20138.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A geometric revisit to the trace quotient problem", "author": ["H. Shen", "K.Diepold", "K. Hueper"], "venue": "Proc. of 19th International Symposium of Mathematical Theory of Networks and Systems, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Trace optimization and eigenproblems in dimension reduction methods", "author": ["E. Kokiopoulou", "J. Chen", "Y. Saad"], "venue": "Numerical Linear Algebra with Applications, vol. 18, no. 3, pp. 565\u2013602, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 15, 2003, pp. 521\u2013528.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Distance metric learning for large margin nearest neighbour classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "J. Mach. Learn. Res., vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised dimensionality reduction via sequential semidefinite programming", "author": ["C. Shen", "H. Li", "M.J. Brooks"], "venue": "Pattern Recogn., vol. 41, pp. 3644\u20133652, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast semidefinite approach to solving binary quadratic problems", "author": ["P. Wang", "C. Shen", "A. van den Hengel"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013, pp. 1312\u20131319.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Least-squares covariance matrix adjustment", "author": ["S. Boyd", "L. Xiao"], "venue": "SIAM J. Matrix Anal. Appl., vol. 27, no. 2, pp. 532\u2013546, 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "A dual approach to semidefinite least-squares problems", "author": ["J. Malick"], "venue": "SIAM J. Matrix Anal. Appl., vol. 26, no. 1, pp. 272\u2013284, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "A newton-cg augmented lagrangian method for semidefinite programming", "author": ["X.-Y. Zhao", "D. Sun", "K.-C. Toh"], "venue": "SIAM J. Optim., vol. 20, no. 4, pp. 1737\u20131765, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Calibrating least squares covariance matrix problems with equality and inequality constraints", "author": ["Y. Gao", "D. Sun"], "venue": "SIAM J. Matrix Anal. Appl., vol. 31, pp. 1432\u20131457, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim., vol. 19, no. 4, pp. 1574\u20131609, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Alternating direction augmented lagrangian methods for semidefinite programming", "author": ["Z. Wen", "D. Goldfarb", "W. Yin"], "venue": "Mathematical Programming Computation, vol. 2, no. 3-4, pp. 203\u2013230, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "Proc. Int. Conf. Mach. Learn., 2013, pp. 80\u201388.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient low-rank stochastic gradient descent methods for solving semidefinite programs", "author": ["J. Chen", "T. Yang", "S. Zhu"], "venue": "Proc. Int. Workshop Artificial Intell. & Statistics, 2014, pp. 122\u2013130.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "Proc. Int. Conf. Mach. Learn., 2004, pp. 743\u2013750.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Informationtheoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proc. Int. Conf. Mach. Learn., 2007, pp. 209\u2013216.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 8, 2008, pp. 761\u2013768.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Positive semidefinite metric learning using boosting-like algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "J. Machine Learning Research, vol. 9, no. 1, pp. 1007\u20131036, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "On the sum of the largest eigenvalues of a symmetric matrix", "author": ["M.L. Overton", "R.S. Womersley"], "venue": "SIAM J. Matrix Anal. Appl., vol. 13, no. 1, pp. 41\u201345, 1992.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1992}, {"title": "Projection methods for conic feasibility problems, applications to polynomial sum-of-squares decompositions", "author": ["D. Henrion", "J. Malick"], "venue": "Optim. Methods and Softw., vol. 26, no. 1, pp. 23\u201346, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithm 778: L-BFGS- B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Transaction on Mathematical Software, vol. 23, pp. 550\u2013560, 1997.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1997}, {"title": "Remark on algorithm 778: L-bfgsb: Fortran subroutines for large-scale bound constrained optimization", "author": ["J.L. Morales", "J. Nocedal"], "venue": "ACM Transactions on Mathematical Software (TOMS), vol. 38, no. 1, p. 7, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "SDPT3\u2014a MATLAB software package for semidefinite programming", "author": ["K.C. Toh", "M. Todd", "R.H. Ttnc"], "venue": "Optimizat. Methods & Softw., vol. 11, pp. 545\u2013581, 1999.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones", "author": ["J.F. Sturm"], "venue": "Optimizat. Methods & Softw., vol. 11, pp. 625\u2013 653, 1999.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1999}, {"title": "UCI machine learning repository. University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Frank", "A. Asuncion"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Proc. of 2nd IEEE workshop on Applications of Computer Vision, 1994, pp. 138\u2013142.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "venue": "IEEE Trans. Anal. Mach. Intell., vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1997}, {"title": "Characterizing virtual eigensignatures for general purpose face recognition", "author": ["D.B. Graham", "N.M. Allinson"], "venue": "Face Recognition: From Theory to Applications; NATO ASI Series F, Computer and Systems Sciences, vol. 163, pp. 446\u2013456, 1998.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "Technical Report CUCS-005-96, Feb 1996.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1996}, {"title": "Columbia object image library (coil-100)", "author": ["\u2014\u2014"], "venue": "Technical Report CUCS-006-96, Feb 1996.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1996}, {"title": "The amsterdam library of object images", "author": ["J. Geusebroek", "G. Burghouts", "A. Smeulders"], "venue": "Int\u2019l J. Computer Vision, vol. 61, no. 1, pp. 103\u2013112, 2005.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In [5], the problem of WLDA was firstly relaxed to a metric learning problem on Z=WW>, which can be solved by a sequence of SDP optimization procedures, where SDP problems are solved by standard interior-point methods (We denote it as Zhang et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "An alternative optimization procedure was then proposed in [5] for large-scale WLDA problems, in which one column of the transformation matrix was found at each iteration while fixing the other directions.", "startOffset": 59, "endOffset": 62}, {"referenceID": 10, "context": "Motivated by [11], a Frobenius-norm regularized SDP formulation is used, and its Lagrangian dual can be solved effectively by quasi-Newton methods.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 232, "endOffset": 235}, {"referenceID": 2, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 259, "endOffset": 262}, {"referenceID": 3, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 286, "endOffset": 289}, {"referenceID": 5, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 337, "endOffset": 340}, {"referenceID": 6, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 389, "endOffset": 392}, {"referenceID": 7, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 440, "endOffset": 443}, {"referenceID": 8, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 482, "endOffset": 485}, {"referenceID": 0, "context": "Assuming dimensions with large within-class covariance are not relevant to subsequent classification tasks, RCA [1] assigns large weights to \u201crelevant dimensions\u201d and small weights to \u201cirrelevant dimensions\u201d, where the relevance is estimated using equivalence constraints.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "NCA [2] learns the transformation matrix W directly by minimizing the expected leave-one-out classification error of k-nearest neighbours on the transformed space.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "NLDA [3], OLDA [4] and EFDC [6] were proposed to address the problem that standard LDA fails when scatter matrices are singular.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "NLDA [3], OLDA [4] and EFDC [6] were proposed to address the problem that standard LDA fails when scatter matrices are singular.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "NLDA [3], OLDA [4] and EFDC [6] were proposed to address the problem that standard LDA fails when scatter matrices are singular.", "startOffset": 28, "endOffset": 31}, {"referenceID": 11, "context": "The resulting transformation matrices are both orthogonal for NLDA and OLDA, and they are equivalent to each other under a mild condition [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "[13] proposed a general averaged divergence analysis (GADA) framework, which presented a general mean function in place of the arithmetic mean used in LDA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "GMSS [7] investigates the effectiveness of the geometric mean-based subspace selection, which maximizes the geometric mean of Kullback-Leibler (KL) divergences between different class pairs.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "HMSS [8] maximizes the harmonic mean of the symmetric KL divergences between all class pairs.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Instead of assigning weights to class pairs, MMDA [9] directly maximizes the minimum pairwise distance of all class pairs in the low-dimensional subspace, which guarantees the separation of all class pairs.", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "[10] presented an asymptotic generalization analysis of LDA, which enriched the existing theory of LDA further.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Many dimensionality reduction algorithm such as PCA and LDA can be formulated into a trace ratio optimization problem [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "[15] presented a generalized Fisher discriminant criterion, which is essentially a trace ratio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] tackled the trace ratio problem directly by an efficient iterative procedure, where a trace difference problem was solved via the eigendecomposition method in each step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] provided a geometric revisit to the trace ratio problem in the framework of optimization on the Grassmann manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Different from [16], they proposed another efficient algorithm, which employed only one step of the parallel Rayleigh quotient iteration at each iteration.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "[18] also treated the dimensionality reduction problem as trace optimization problems, and gave an overview of the eigenvalue problems encountered in dimensionality reduction area.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Furthermore, different from the iterative algorithm for trace ratio optimization problem [16], we formulate the WLDA problem as a sequence of SDP problems, and propose an efficient SDP solving method.", "startOffset": 89, "endOffset": 93}, {"referenceID": 18, "context": "[19] formulated metric learning as a convex (SDP) optimization problem, and a globally optimal solution can be obtained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] presented a distance metric learning method, which optimizes a Mahalanobis metric", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "MMDA [9] was solved approximately by a sequence of SDP problems using standard interior-point methods.", "startOffset": 5, "endOffset": 8}, {"referenceID": 20, "context": "[21] proposed a novel SDP based method for directly solving trace quotient problems for dimensionality reduction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] proposed a fast SDP approach for solving Mahalanobis metric learning problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] also employed a similar dual approach to solve binary quadratic problems for computer vision tasks, such as image segmentation, co-segmentation, image registration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "The key motivation of [23], [24] is that the objective function of the corresponding dual problem is continuously differentiable but not twice differentiable, therefore first-order methods can be applied.", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "The key motivation of [23], [24] is that the objective function of the corresponding dual problem is continuously differentiable but not twice differentiable, therefore first-order methods can be applied.", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Malick [24] and Boyd and Xiao [23] proposed to use quasi-Newton methods and projected gradient methods respectively, to solve the Lagrangian dual of semidefinite leastsquares problems.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "Malick [24] and Boyd and Xiao [23] proposed to use quasi-Newton methods and projected gradient methods respectively, to solve the Lagrangian dual of semidefinite leastsquares problems.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "Semismooth Newton-CG methods [25] and smoothing Newton methods [26] are also exploited for semidefinite least-squares problems, which require much less number of iterations at the cost of higher computational complexity per iteration (full eigen-decomposition plus conjugate gradient).", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "Semismooth Newton-CG methods [25] and smoothing Newton methods [26] are also exploited for semidefinite least-squares problems, which require much less number of iterations at the cost of higher computational complexity per iteration (full eigen-decomposition plus conjugate gradient).", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "Alternatively, stochastic (sub)gradient descent (SGD) methods [27] were also employed to solve SDP problems.", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "Combining with alternating direction methods [28], [29], SGD can be used for SDP problems with inequality and equality constraints.", "startOffset": 45, "endOffset": 49}, {"referenceID": 28, "context": "Combining with alternating direction methods [28], [29], SGD can be used for SDP problems with inequality and equality constraints.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "[30] proposed a low-rank SGD method, in which rankk stochastic gradient is constructed and then the projection operation is simplified to compute at most k eigenpairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "Note that SGD methods usually need more iterations to converge than the dual approaches based on quasi-Newton methods [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "\u2019s [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We use similar SDP optimization technique as that in [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "However, SDP feasibility problems are considered in our paper while the work in [11] focuses on standard SDP problems with linear objective functions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "We briefly review WLDA problem proposed by [5] firstly.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "As stated in [5], this problem (7) is not easy to optimize with respect to W, so a new variable Z = WW> \u2208 Rd\u00d7d is introduced, and the problem (7) is formulated as a metric learning problem.", "startOffset": 13, "endOffset": 16}, {"referenceID": 34, "context": "This theorem has been widely used and its proof can be found in [35].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "variable Z [5]:", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "In [5], Zhang et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "This infeasibility condition can be deduced from a general conic feasibility problem presented in [37].", "startOffset": 98, "endOffset": 102}, {"referenceID": 36, "context": "In this work, we use L-BFGS-B [38], [39], a limitedmemory quasi-Newton algorithm package, which can handle the problem with box constraints.", "startOffset": 30, "endOffset": 34}, {"referenceID": 37, "context": "In this work, we use L-BFGS-B [38], [39], a limitedmemory quasi-Newton algorithm package, which can handle the problem with box constraints.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "in [5] (Zhang et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 38, "context": "The computational complexity is compared between SD-WLDA, standard interior-point algorithms to solve our SDP formulation (SDPT3 [40] and SeDuMi [41]), Zhang et al.", "startOffset": 129, "endOffset": 133}, {"referenceID": 39, "context": "The computational complexity is compared between SD-WLDA, standard interior-point algorithms to solve our SDP formulation (SDPT3 [40] and SeDuMi [41]), Zhang et al.", "startOffset": 145, "endOffset": 149}, {"referenceID": 40, "context": "Some UCI datasets [42] are used here firstly.", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "ORL [43] consists of 400 face images of 40 individuals, each with 10 images.", "startOffset": 4, "endOffset": 8}, {"referenceID": 42, "context": "The Yale dataset [44] contains 165 grey-scale images of 15 individuals, 11 images per subject.", "startOffset": 17, "endOffset": 21}, {"referenceID": 43, "context": "UMist dataset [46] 10 20 30 40 50 60 70 80 90 20 25 30 35 40 45", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "In order to illustrate the computational speed of SD-WLDA and both methods in [5] with respect to the number of classes and the input data dimension (here it refers to the dimension after PCA) respectively, more experiments are performed on Yale dataset.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "(SDP), as [5] presented.", "startOffset": 10, "endOffset": 13}, {"referenceID": 44, "context": "2) Object recognition: Three datasets are used here: Coil20 [47], Coil30 [48], and ALOI [49].", "startOffset": 60, "endOffset": 64}, {"referenceID": 45, "context": "2) Object recognition: Three datasets are used here: Coil20 [47], Coil30 [48], and ALOI [49].", "startOffset": 73, "endOffset": 77}, {"referenceID": 46, "context": "2) Object recognition: Three datasets are used here: Coil20 [47], Coil30 [48], and ALOI [49].", "startOffset": 88, "endOffset": 92}], "year": 2014, "abstractText": "In this paper, we propose an efficient semidefinite programming (SDP) approach to worst-case linear discriminant analysis (WLDA). Compared with the traditional LDA, WLDA considers the dimensionality reduction problem from the worstcase viewpoint, which is in general more robust for classification. However, the original problem of WLDA is non-convex and difficult to optimize. In this paper, we reformulate the optimization problem of WLDA into a sequence of semidefinite feasibility problems. To efficiently solve the semidefinite feasibility problems, we design a new scalable optimization method with quasi-Newton methods and eigen-decomposition being the core components. The proposed method is orders of magnitude faster than standard interior-point based SDP solvers. Experiments on a variety of classification problems demonstrate that our approach achieves better performance than standard LDA. Our method is also much faster and more scalable than standard interior-point SDP solvers based WLDA. The computational complexity for an SDP with m constraints and matrices of size d by d is roughly reduced from O(m+md+md) to O(d) (m > d in our case).", "creator": "LaTeX with hyperref package"}}}