{"id": "1411.7820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2014", "title": "Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic Models and Encyclopedic Knowledge", "abstract": "We present federative a etxeberria method for pre-1940 coarse - grained ekster cross - eif lingual alignment of screes comparable camdessus texts: segments consisting retyping of contiguous paragraphs snegur that discuss the ints same theme (localizing e. hesse-philippsthal g. history, grahame-smith economy) are aligned gobineau based on lip-synch induced jammie multilingual 1500cc topics. vietoris The ragdale method granulated combines three h7n7 ideas: tangestan a two - sub-components level same-gender LDA krisna model that filters vlcek out pontet words flohr that do not virat convey tevzadze themes, tamid an HMM that quraysh models the felber ordering graziosa of themes in the biotechnological collection commercialised of oprandi documents, mozersky and 90.47 language - independent concept annotations tipitina to greendale serve turfgrass as a cross - language bridge and yeomanry to 0-14 strengthen ohliger the emelec connection pancytopenia between staggering paragraphs in monarchic the same 1649 segment pgce through reflect concept relations. The hesitates method hinges is hij evaluated 1,385 on non-islamic English corgo\u0148 and assegai French biktagirova data previously used for 5-centimeter monolingual 828 alignment. campedelli The results rapoport show eighty-two state - of - the - emek art performance bene\u0161ov\u00e1 in orchids both li\u00eau monolingual ordaz and cross - mehdi lingual settings.", "histories": [["v1", "Fri, 28 Nov 2014 11:33:02 GMT  (178kb,D)", "http://arxiv.org/abs/1411.7820v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vivi nastase", "angela fahrni"], "accepted": false, "id": "1411.7820"}, "pdf": {"name": "1411.7820.pdf", "metadata": {"source": "CRF", "title": "Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic Models and Encyclopedic Knowledge", "authors": ["Vivi Nastase", "Angela Fahrni"], "emails": ["nastase@fbk.eu", "angela.fahrni@h-its.org"], "sections": [{"heading": "1 Introduction", "text": "Coarse-grained alignment of documents groups text segments in different documents that convey the same theme \u2013 e.g. history, geography in texts about cities. Cross-language alignment deals with the added challenge of aligning segments from documents in different languages. This could be useful as a prelude to fine-grained alignment, or for building coarsely aligned multilingual corpora for machine translation or text categorization. We are interested in cross-lingual alignment for the synchronization of web (wiki) pages with multiple language versions, where pages in different languages are independently edited. A coarse alignment would (i) reveal quickly text portions that are not shared, and must thus be translated and added, and (ii) show potentially parallel portions, to be further processed to produce a more finegrained alignment. We present a cross-lingual\nalignment of segments \u2013 consisting of one or more paragraphs \u2013 based on knowledge-enhanced topic modeling, illustrated in Figure 1, which implements the following ideas:\n2-level LDA A two-layer modeling framework (i) models paragraphs as a mixture of three topics \u2013 background, document-specific and themespecific (e.g. in a document about the city of Montreal, articles, prepositions, etc. would be background, words associated with Montreal but not specific to a theme \u2013 Montreal, Canada, French, Quebec \u2013 would be document-specific, and others such as hockey, sports, skating would be themespecific); (ii) models a document as a mixture of K topics/themes.\nHMM theme sequences Following the assumption that themes are not presented randomly in a document, we use an HMM to model their sequence. State transitions are modeled through a Dirichlet distribution, tuned to bias the system towards self-transitions and thus avoid rapid switching between states (Beal et al., 2002; Teh et al., 2003; Fox et al., 2010).\nLanguage-independent concept annotations We inject knowledge in the model by linking single and multi-word terms to concepts in a concept network obtained from Wikipedia (Nastase and Strube, 2013). Terms are replaced with language-independent identifiers (e.g. \u201cNew York\u201d becomes \u201cc553795\u201d). Enriching text with concept annotations allows to: (i) integrate naturally the identified multi-word expressions into the statistical process; (ii) deal with ambiguity (\u201cNew York\u201d the city becomes \u201cc553795\u201d, while \u201cNew York\u201d the state becomes \u201cc27491610\u201d); (iii) deal partly with coreferent mentions and synonymy (\u201cNew York City\u201d, \u201cThe Big Apple\u201d, \u201cNueva York\u201d share the same identifier); (iv) bridge languages and build multilingual topic models, as concepts are shared across languages; ar X iv :1\n41 1.\n78 20\nv1 [\ncs .C\nL ]\n2 8\nN ov\n2 01\n4\n(v) use additional encyclopedic knowledge extracted from the concept network, e.g. relations between concepts within the same or consecutive paragraphs to strengthen the cohesion of the topic.\nThe method is applied on Wikipedia articles about cities in English and French (Chen et al., 2009). This allows for comparison with related work (in the monolingual setting), and to extend the evaluation to a cross-lingual setting. The monolingual segment alignment is evaluated against a Hidden Topic Markov Model (Gruber et al., 2007) \u2013 and the GMM model (Chen et al., 2009). The method described here scores higher than both. In the cross-lingual setting, the baselines are the alignments produced using translation tables, and using concept annotations, respectively. The method which combines topic modeling with concept annotations outperforms both baselines."}, {"heading": "2 Previous Work", "text": "Text alignment For monolingual comparable corpora, text relatedness measures produce good alignment results (Barzilay and Elhadad, 2003; Yahyaei et al., 2011). Methods for sentence alignment in a parallel bilingual corpus typically rely on linearity of the alignment, existence of 1:1 mapping between aligned sentences and the correlation of sentence length (Tiedemann, 2011). These assumptions do not hold for comparable (but not parallel) corpora. Bilingual comparable corpora have been mainly used to extract parallel fragments (Gupta et al., 2013) or paraphrases and word translations (Fung and McKeown, 1997). We focus on an alignment of fragments that have the same theme, but are not necessarily parallel.\nSequence modeling Modeling a document as a sequence of topics brings HMMs naturally to mind. Work in this area started with Mulbregt et al. (1998) and Blei and Moreno (2001). This approach suffers from two shortcomings: the number of states needs to be fixed (to be able to perform the Baum-Welch or Viterbi algorithms), and the HMM tends to switch fast between different states. In situations where a state should be persistent, as in topic segmentation, this is a problem. A breakthrough has come when state transitions in an HMM were modeled through a Dirichlet process (Beal et al., 2002). This allows one to control state transitions, and to let the model induce the number of states that best fit the data. Beal et al. (2002) used three hyperparameters to control the HMM: for self-transitions, for transitions to previously used states, and for adding a new transition. Teh et al. (2003) and Fox et al. (2010) extended this model and showed how to manipulate parameters to avoid rapid switching between states.\nLanguage models In topic segmentation one can consider the words to be the basic unit (Gruber et al., 2007), or sentences/paragraphs (e.g. (Blei and Moreno, 2001), (Eisenstein and Barzilay, 2008)). Sentences are themselves heterogeneous. Daume\u0301 III and Marcu (2006) used topic models for query-driven summarization. The assumption is that each sentence consists of a mixture of language models \u2013 one corresponding to the general model of the English language, one corresponding to the overall theme of the document, and one that matches the topic of the given query. Zhai et al. (2004), Titov and McDonald (2008) and Paul and Girju (2009) model topics (aspects) that run throughout a collection of documents, and\ntopics that are collection-specific. Considering a sentence/paragraph as a small document, and one document as a collection of these small documents, it could be construed that the previously mentioned work models a sentence/paragraph as a mixture of topics, some of which are common throughout the document, some of which are sentence-specific. Wang et al. (2011) also assume a sentence to be generated from a mixture of topics, in particular two: a \u201cfunctional\u201d (i.e. background) topic, and a content topic. Additionally, this approach models transitions between topics through an HMM, but like previous work, without concerns about the rapid state switching. This does not pose a problem because of the nature of the data: short ads and reviews with short sentences and not much topic repetition.\nWSD and topic modeling Boyd-Graber et al. (2007) combine topic modeling with word sense disambiguation relative to WordNet, following the observation that words in different topics may exhibit different senses, and that the sense of a word depends on the context in which it appears. Each noun that appears in WordNet will enhance the probabilities of all paths from the root to each possible sense, and \u201ccorrect\u201d paths will have higher probability because of aggregated evidence from the words in the text. A downside of this is that, as topic models do, it deals with single word terms only, whereas texts contain numerous multi-word terms referring to real world entities. For this reason we apply the concept and entity identification process before the topic modeling step.\nMultilingual topic modeling Jagarlamudi and Daume\u0301 III (2010) use a bilingual dictionary to obtain multilingual topics from an unaligned multilingual corpus. They assume topics to be formed of concepts, which can have different lexicalizations depending on the language. These concepts consist of entries from a bilingual dictionary. Boyd-Graber and Blei (2009) simultaneously discover matching words and multilingual topics from a collection composed of documents in two languages, based on the assumption that similar words appear in similar contexts. Zhang et al. (2010) use a bilingual dictionary as a source of constraints for bridging texts in two languages, using the assumption that related words in different languages have similar distributions. The topic models produced contain words in different languages that are allowed to have different probabil-\nities, reflecting the difference between the data in the two languages. Ni et al. (2009) mine multilingual topics from Wikipedia, using the articles on the same theme in different languages as a source of language models. Mimno et al. (2009) build polylingual topic models from sets of documents on the same topics, in particular Wikipedia articles in several languages. They assume that the different language versions of an article have similar topic distributions, and topics consist of languagespecific word distribution. Vulic\u0301 et al. (2011) build bilingual topic models for information retrieval, using comparable corpora \u2013 in particular collections of Wikipedia articles on the same topics in the targeted languages."}, {"heading": "3 A Topic Model for Alignment", "text": "We build upon some of the ideas presented above to develop a knowledge-enhanced Hidden Topic Markov Model (HTMM): documents, made up of paragraphs, are modeled as sequences of topics, with transition between states controlled by a Dirichlet distribution. The process is detailed below."}, {"heading": "3.1 Overall generative process", "text": "We describe here the overall generative process, covering the two level LDA and HMM modeling, which will be detailed separately below.\nAssuming K t-topics (themes) represented in the documents d1:M in our collection, the generative process is as follows:\n1 Draw a word distribution \u03be1 \u223cDirichlet(\u03b71) for a background language model;\n2 Draw a word distribution \u03c6z \u223c Dirichlet(\u03b2) for each t-topic z \u2208 {1..K};\n3 draw a t-topic transition probability distribution \u03c0 \u223c Dirichlet(\u03b1+\u03ba) (\u03c00 is an initial state probability vector, \u03c0j is a transition probability distribution from state j);\n4 For each document dm,m = 1..M :\n4.1 draw a word distribution \u03be2m \u223c Dirichlet(\u03b72) for a document-specific language model; 4.2 draw a t-topic mixture \u03b8m \u223c Dirichlet(\u03bb) for document dm; 4.3 for each paragraph (sequence of words) wtm in dm:\n4.3.1 sample a t-topic zt \u223c Discrete(\u03c0zt\u22121 , \u03b8m); 4.3.2 draw a w-topic mixture \u03c8m \u223c Dirichlet(\u03b3); 4.3.3 for each word wti in sequence wtm:\nA. sample a topic sti \u223c Discrete(\u03c8m); B. if sti = 1, sample wti from the back-\nground language model \u03be1; C. if sti = 2, samplewti from the document-\nspecific language model \u03be2m; D. if sti = 3, sample wti from the t-\ntopic \u03c6zt ."}, {"heading": "3.2 A 2-level LDA", "text": "Paragraphs are modeled as generated from a mixture of three word(-level) topics (w-topics) corresponding to each of (1) background, (2) document-specific, (3) \u201ctheme-specific\u201d language model respectively, exemplified in the left side of Figure 1. This modeling is used as a filter \u2013 the (collection-specific) background and the document-specific words are not informative with respect to themes, and are skipped in the next processing step. While only side-effects with respect to the focus of this paper, the background and document-specific language models can be useful: the document-specific word probability distribution can be used to align, cluster or classify documents. We explore this briefly in Section 4.1.2. The w-topic mixture follows a Dirichlet distribution with hyperparameter \u03b3 (Dirichlet(\u03b3)), while words within a w-topic follow Dirichlet(\u03b7). The topic si assigned to word at position i in paragraph t is sampled according to:\np(si = l|s,wt) \u221d gl(wti) nlwti + \u03b7\nnl\u2217 +W\u03b7\nnlt\u2212i + \u03b3\nn\u2217t\u2212i + 3\u03b3\nwhere W is the size of the vocabulary, nlw is the number of times word w was assigned to wtopic l, \u2217 is a wild-card, nlt is the number of words in paragraph t assigned to topic l, n\u2217t is the number of words in paragraph t, and wt\u2212i is the sequence of words wt minus the wordwti at position i. g1..3 are normalized coefficients that capture the bias of each word for the three postulated topics, according to their document and collection frequencies1.\n1Background language g1(wti) = cPwti cP\u2217 (generally fre-\nquent words); document-specific g2(wti) = cdwti cd\u2217\n(common throughout the document, rare outside); topic-carrying g3(wti) = cDwti cD\u2217 \u00d7 (1 \u2212 g1(wti)) (appear throughout the collection, but not frequent within single documents). P is the total number of paragraphs in the set of documents D, d is a document (the document being processed), cyx is the number of ys in which x appears.Because gl are constant, they will not be affected by marginalizing \u03be: p(wt,g|s, \u03b7) =\u222b p(wt,g|s, \u03be)p(\u03be|\u03b7)d\u03be = \u222b p(g|s)p(wt|s, \u03be)p(\u03be|\u03b7)d\u03be =\u220f\ni gsi(wti)\n\u222b p(wt|s, \u03be)p(\u03be|\u03b7)d\u03be. These factors are nor-\nmalized such that for each word they sum to 1.\nDocuments are generated from a mixture of K themes (t-topics) following Dirichlet(\u03bb). The ttopic of a paragraph is determined based on the subset of its theme-specific words, as determined by the previous step. The words in a topic follow Dirichlet(\u03b2). The right side of Figure 1 shows an example of the t-topics within a document. 2 Contiguous paragraphs that express the same t-topic form a segment, and through this can be aligned across documents and languages. Paragraph t-topics are sampled as:\np(zt = j|z,wt, \u03bb, \u03b2) \u221d njwti + \u03b2\nnj\u2217 +W\u03b2\nnjt\u2212i + \u03bb\nn\u2217t\u2212i +K\u03bb"}, {"heading": "3.3 Modeling the sequence of topics/themes", "text": "The sequence of themes within a document is modeled through an HMM. The transition probabilities are modeled by a Dirichlet distribution (Beal et al., 2002): the transition probabilities from a state zt = j at time t can be interpreted as mixing proportions for the state at time t + 1: \u03c0j = {\u03c0j1, ..., \u03c0jK}. The persistence of states is encouraged by increasing the probability of self-transitions using a \u201csticky\u201d parameter \u03ba (Beal et al., 2002; Fox et al., 2010): \u03c0i|\u03b1, \u03ba \u223c Dirichlet(\u03b1 + \u03ba\u03b4i), where \u03ba > 0 is added to the ith component of the parameter vector \u03b13. ttopics are sampled as:\np(zt = j|z\u2212t, \u03b1, \u03ba, \u03bb) \u221d\nn (d) \u2212t,j+\u03bb\u2211\nl n (d) \u2212t,l+K\u03bb\ntopic mixing proportion\nn\u2212tzt\u22121j +\u03b1j+\u03ba\u2217\u03b4(zt\u22121,j) n\u2212tzt\u22121\u2217+ \u2211 x \u03b1x+\u03ba transition from previous paragraph\nn\u2212tjzt+1 +\u03b1j+\u03ba\u2217\u03b4(j,zt+1) n\u2212tj\u2217 + \u2211 x \u03b1x+\u03ba transition to next paragraph\nwhere \u03b4(i, j) = { 1 i == j 0 i 6= j (Kronecker\u2019s delta).\n2 The t-topics induced by the model are nameless, we name them in the figure to illustrate more clearly the point.\n3Fox et al. (2010) model the transitions through a Dirichlet process to allow the model to infer the number of states.\nAfter sampling the t-topic for paragraph t, the t-topics of the topic-carrying words in wt (assigned w-topic 3) are reassigned to the paragraph\u2019s t-topic, reflecting the assumption that there is one t-topic per paragraph."}, {"heading": "3.4 Language-independent concepts", "text": "To bridge languages, and address (at least partly) the issues of multi-word expressions, synonymy, polysemy, we introduce concept annotations, exemplified in Figure 2. To identify concepts:\n\u2022 locate possible concept lexicalizations extracted from Wikipedia in texts;\n\u2022 for each lexicalization, find all candidate concepts it could refer to;\n\u2022 disambiguate among possible candidate concepts.\nFor disambiguation, the text is represented as a complete n-partite graph G = (V1, ..Vn, E). Each partition Vi corresponds to a (possibly multi-word) term ti in the text, and contains as vertices all concepts cij that may be expressed by ti, found in the first step. Each vertex from a partition is connected to all vertices from the other partitions (making the n-partite graph complete) through edges evi,vj \u2208 E weighted by wvi,vj . The weights are learned from Wikipedia\u2019s link structure, using as features several measures: shared outgoing links between the articles corresponding to the two concepts, shared categories and their specificity; preference of each concept for the other concept\u2019s expression through the anchor in the current text.\nThe disambiguated concepts are the nodes in the maximum edge-weighted clique in this graph, and their expression in the text is replaced with the corresponding unique (and language independent) concept ID, resulting in the concept-annotated texts illustrated in Figure 2 (Fahrni et al., 2011).\nThis concept identification method scored high on both ACE 2005 (72.7% F-score) and in the Entity Linking TAC 2011 task.\nThe texts with concept identifiers are processed with the previously described topic model. To take advantage of the relations between concepts, we introduce a factor swkn, to give a boost to topics that are favoured by the concepts wr \u2208 Rwti directly connected towti in the concept network (for wti that represent a concept):\nswkn(wti|zi) = 1 |Rwti | \u2211\nwr\u2208Rwti\nnziwr n\u2217wr\nwhere\nRwti = {wr|(wr, wti) is an edge in the network of concepts}\nnziwr is the number of times concept wr was assigned t-topic zi (excluding the current occurrence), and n\u2217wr is the number of times concept wr was assigned any t-topic."}, {"heading": "3.5 Topic assignment", "text": "To make the final assignments of t-topics to paragraphs in the test data, we use the word probabilities under both w-topics and t-topics, and the transition probability distributions for t-topics induced as a result of the iterative sampling process in the HMM framework. States correspond to t-topic assignments to paragraphs. The final assignments of t-topics to the current document is determined through a Viterbi algorithm on the HMM with the parameters described above."}, {"heading": "4 Experiments", "text": "Data The data consists of a collection of articles about cities in English and French presented in (Chen et al., 2009).4 Table 1 presents the data statistics. These documents have structure \u2013 sections, such as History, Culture, Transportation \u2013 which is what we try to recreate through this process5. The English data was manually annotated with a \u201cclean\u201d set of headings, to allow mapping of sections that have the same topic but slightly different headings (e.g. Culture and arts/Culture). This manual annotation process revealed 18 topics that appear in more than one document. To evaluate the cross-lingual topic alignment, the French\n4http://groups.csail.mit.edu/rbg/code/ mallows.\n5Yahyaei et al. (2011)\u2019s data did not fit this structure, and we could not use it to test this method.\nparagraph labels were (manually) translated to English, if a corresponding label existed on the English side \u2013 e.g. histoire was translated to history; voir aussi was translated as further reading, its English side counterpart (instead of the more literal see also). Labels that had no correspondent in English were not translated.\nModeling parameters The parameters of the model reflect the differences between the two languages data (e.g. English has 1.5 the number of paragraphs that French does, and also a larger vocabulary):\n\u2022 for word-topic distributions: values < 1 to bias towards distributions that favour high probabilities for a small set of words for both w-topics and t-topics: \u03b2 = \u03b7 = W100000 (W is the size of the vocabulary in English and French respectively);\n\u2022 for topic mixtures: values> 1 to produce balanced topic mixtures within paragraphs (for w-topics) \u03b3 = W|P | (|P | is the number of paragraphs), and within documents \u03bb = 50/K (for both languages, following (Griffiths and Steyvers, 2004));\n\u2022 for state transitions: a high \u03ba = 1000 to encourage state persistence; and low \u03b1 = 0.01 to bias the state transition model to prefer a small number of possible succeeding topics."}, {"heading": "4.1 Monolingual alignment", "text": "The performance of the model is evaluated based on the assigned t-topics on the task of crossdocument alignment (Chen et al., 2009). Reference topic assignments are the section labels chosen by their authors. Following (Chen et al., 2009) we compute:\nRec =\n\u2211 h\u2208H maxk\u2208K(overlap(h, k))\nP\nPrec =\n\u2211 k\u2208K maxh\u2208H(overlap(h, k))\nP\nwhere P is the number of paragraphs, H is the set of section headings, K is the set of automatically assigned t-topics(themes), and overlap(h, k) is the number of paragraphs with section heading h and automatically assigned topic k."}, {"heading": "4.1.1 Settings", "text": "Baselines Gruber et al.\u2019s (2007) HTMM models a document as a sequence of sentences. All words in a sentence have the same topic, and a binomial transition parameter determines whether the next sentence has the same topic as the current one. Chen et al. (2009) propose GMM, a global model for documents in a collection. They assume that the documents in the collection share the same topics, which are similarly distributed within the documents. This is captured by modeling the mixture of topics as a distribution over permutations of a topic ordering.\nOur variations 2LDA is the two level LDA processing as described in Section 3.2, on the texts with concept annotations. 2LDA HMM adds the HMM to 2LDA to model the transition between paragraphs (Section 3.3). 2LDA c adds the score based on concept relations to 2LDA. 2LDA c HMM adds the score based on concept relations to 2LDA HMM.\n2LDA HMM no concepts is the best performing configuration without concept annotations."}, {"heading": "4.1.2 Results", "text": "Language models The first result consists of the language models induced by the system. Figure 4 shows examples of the general (background), document-specific (for the English and French Wikipedia articles about the city of Montreal) and two t-topic language models. The top section of the figure shows the background and documentspecific language models for the English and French data (shown side by side because the documents refer to the same cities). The background language model has captured what is usually included in a list of stopwords. This is an advantage, as stopwords may be collection-specific. The document-specific language model include words expected to be associated with the respective article titles. Sample t-topic language models are shown separately for each language.\nTerms in the text are replaced with language independent concept IDs. We combine the English and French data, and apply the topic model to this data. Samples of the resulting t-topics are included in Figure 4 \u2013 columns labeled \u201cmultilingual\u201d. The terms in italics represent concepts \u2013 for readability replaced with the name of the corresponding article in the English Wikipedia. We have also performed experiments with building separate topics for English and French, while shar-\ning only the language-independent concepts (i.e. after building the topics for English, we use the English topic-annotated data as observed data, and adjust the prior computed from this data while iterating over the French data). The results were very similar, and for reasons of space are not included.\nThe background and document-specific language models are \u201cside-effects\u201d of the process of cross-document and cross-language alignment. However, they can be useful. The documentspecific topics could be used for (cross-language) document clustering. We took the top 20 words and concepts ranked based on tf-idf, as well as the top 20 terms in the document-specific topics for each of the 200 documents (100 for each language), and used the cosine similarity to align the documents in the two languages (i.e., to pair up the documents in English and French that were about the same city). The baseline \u2013 using tf-idf scored words \u2013 is already high (94%), due to shared city and country names. Both topic probabilities and tf-idf scored concepts perform perfectly (100%). For the task and data described in this\npaper, document level alignment performed using document-specific topics can be compounded with the theme-based paragraph alignment to produce paragraph alignment between pairs of documents. We plan to study the usefulness of the documentspecific topics in a more difficult environment.\nAlignment Figure 5 presents the results in terms of precision, recall and F-score for the settings describe above to reveal the contribution of the HMM and the concept annotations: The results on the English data, for both 10 and 20 t-topics, show a nice progression of the results as we add more information in the model. Adding HMM leads to better results, as it allows transition probabilities \u2013 and through this the context \u2013 to influence a ttopic assignment for each paragraph in the document. Adding concepts and their relations in the model leads to increase in recall, which is again expected since this will lead to more links between terms in different parts of the document. The results are better than previous work, with the exception of the situation when we use 20 t-topics on the French data."}, {"heading": "4.2 Cross-language alignment", "text": "Cross-language alignment is performed as for monolingual alignment, but we compare topic assignments across languages."}, {"heading": "4.2.1 Settings", "text": "Baselines Baseline 1 is based on similarity between paragraphs computed using translation probabilities from a translation table. Baseline 2 computes paragraph alignment using a conceptbased representation of each paragraph with concepts weighted according to their tf-idf score and a cosine similarity metric. Both baselines are computationally intensive because of all the pairwise comparisons between paragraphs.\nOur approach 2LDA c HMM We experimented with concatenating the corpora in the two languages and inducing topics from the union, or alternatively using the distributions induced on one as priors when processing the other. The results are not significantly different."}, {"heading": "4.2.2 Results", "text": "Cross-lingual precision, recall and F-score computed over the entire (bilingual) collection are presented in Table 2, for 10 and 20 topics.\nThe high precision results for the two baselines are due in large part to singleton clusters, many of\nwhich are in fact correct. Building only singleton clusters leads to an F-score of 35.13."}, {"heading": "5 Conclusions", "text": "We have explored knowledge-enhanced \u2013 through concept annotations \u2013 topic models. The annotations not only help build cross-lingual topic models, but also deal with multi-word terms, polysemy and synonymy. The links between concepts are also beneficial by influencing the topic distribution and sequencing probabilities. The results are good despite the fact that we rely on an automatically built resource, and a concept identification step \u2013 itself an open research problem. We plan to investigate a joint approach to topic modeling and concept identification.\nWe have augmented the traditional HMM model with language models within a paragraph, and a Dirichlet distribution at the level of state (high-level topic) transitions. Modeling a text on two levels gives a layered view, which can be used to structure the commonly used bag-of-word representation of texts. Separating words that contribute to the topic segmentation from those that pertain to a background language or documentspecific model helps the system focus on the most relevant terms for segmentation. This could become an advantage when aligning corpora in different languages \u2013 one need only establish parallelism between topic-specific terms."}], "references": [{"title": "Sentence alignment for monolingual comparable corpora", "author": ["Barzilay", "Elhadad2003] Regina Barzilay", "Noemie Elhadad"], "venue": "In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Barzilay et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2003}, {"title": "The infinite hidden Markov model", "author": ["Beal et al.2002] Matthew Beal", "Zoubin Ghahramani", "Carl Edward Rasmussen"], "venue": "In NIPS,", "citeRegEx": "Beal et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Beal et al\\.", "year": 2002}, {"title": "Topic segmentation with an aspect hidden Markov model", "author": ["Blei", "Moreno2001] David M. Blei", "Pedro J. Moreno"], "venue": "In SIGIR,", "citeRegEx": "Blei et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2001}, {"title": "Multilingual topic models for unaligned text", "author": ["Boyd-Graber", "Blei2009] Jordan Boyd-Graber", "David M. Blei"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Boyd.Graber et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2009}, {"title": "A topic model for word sense disambiguation", "author": ["Boyd-Graber", "David M. Blei", "Xiaojin Zhu."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Boyd.Graber et al\\.,? 2007", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2007}, {"title": "Content modeling using latent permutations", "author": ["Chen et al.2009] Harr Chen", "S.R.K. Branavan", "Regina Barzilay", "David R. Karger"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Bayesian query-focused summarization", "author": ["III Daum\u00e9", "III Marcu2006] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2006}, {"title": "Bayesian unsupervised topic segmentation", "author": ["Eisenstein", "Barzilay2008] Jacob Eisenstein", "Regina Barzilay"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Eisenstein et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2008}, {"title": "HITS\u2019 graph-based system at the NTCIR-9 cross-lingual link discovery task", "author": ["Fahrni et al.2011] Angela Fahrni", "Vivi Nastase", "Michael Strube"], "venue": "In Proceedings of the 9th NTCIR Workshop Meeting, Tokyo, Japan,", "citeRegEx": "Fahrni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fahrni et al\\.", "year": 2011}, {"title": "A sticky HDP-HMM with application to speaker diarization", "author": ["Fox et al.2010] Emily B. Fox", "Erik B. Sudderth", "Michael I. Jordan", "Alan S Willsky"], "venue": null, "citeRegEx": "Fox et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2010}, {"title": "Finding terminology translations from non-parallel corpora", "author": ["Fung", "McKeown1997] Pascale Fung", "Kathleen McKeown"], "venue": "In Proceedings of 5th International Workshop of Very Large Corpora (WVLC-5), Hongkong", "citeRegEx": "Fung et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Fung et al\\.", "year": 1997}, {"title": "Finding scientific topics", "author": ["Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Hidden topic Markov models", "author": ["Gruber et al.2007] Amit Gruber", "Michael Rosen-Zvi", "Yair Weiss"], "venue": "In AISTATS", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Improving mt system using extracted parallel fragments of text from comparable corpora", "author": ["Gupta et al.2013] Rajdeep Gupta", "Santanu Pal", "Sivaji Bandyopadhyay"], "venue": "In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Extracting multilingual topics from unaligned comparable corpora", "author": ["Jagarlamudi", "Hal Daum\u00e9 III"], "venue": "In Advances in Information Retrieval,", "citeRegEx": "Jagarlamudi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jagarlamudi et al\\.", "year": 2010}, {"title": "Polylingual topic models", "author": ["Mimno et al.2009] David M. Mimno", "Hanna M. Wallach", "Jason Naradowsky", "David A. Smith", "Andrew McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Text segmentation and topic tracking on broadcast news via a hidden markov model approach", "author": ["I. Carp", "L. Gillick", "S. Lowe", "J. Yamron"], "venue": "In ICSLP-98,", "citeRegEx": "Mulbregt et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Mulbregt et al\\.", "year": 1998}, {"title": "Transforming Wikipedia into a large scale multilingual concept network", "author": ["Nastase", "Strube2013] Vivi Nastase", "Michael Strube"], "venue": "Artificial Intelligence,", "citeRegEx": "Nastase et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nastase et al\\.", "year": 2013}, {"title": "Mining multilingual topics from wikipedia", "author": ["Ni et al.2009] Xiaochuan Ni", "Jian-Tao Sun", "Jian Hu", "Zheng Chen"], "venue": "In Proceedings of the 18h International Conference on World Wide Web,", "citeRegEx": "Ni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2009}, {"title": "Cross-cultural analysis of blogs and forums with mixed-collection topic models", "author": ["Paul", "Girju2009] Michael Paul", "Roxana Girju"], "venue": "In EMNLP,", "citeRegEx": "Paul et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2009}, {"title": "Hierarchical dirichlet processes", "author": ["Teh et al.2003] Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "venue": null, "citeRegEx": "Teh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2003}, {"title": "Bitext Alignment. Synthesis Lectures on Human Language Technologies", "author": ["J\u00f6rg Tiedemann"], "venue": null, "citeRegEx": "Tiedemann.,? \\Q2011\\E", "shortCiteRegEx": "Tiedemann.", "year": 2011}, {"title": "Modeling online reviews with multigrain topic models", "author": ["Titov", "McDonald2008] Ivan Titov", "Ryan McDonald"], "venue": null, "citeRegEx": "Titov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2008}, {"title": "Cross-language information retrieval with latent topic models trained on a comparable corpus", "author": ["Vuli\u0107 et al.2011] Ivan Vuli\u0107", "Wim De Smet", "Marie-Francine Moens"], "venue": "In Proceedings of the 7th Asia Conference on Information Retrieval Technol-", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2011}, {"title": "Structural topic model for latent topical structure analysis", "author": ["Wang et al.2011] Hongning Wang", "Duo Zhang", "ChengXiang Zhai"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Cross-lingual text fragment alignment using divergence from randomness", "author": ["Marco Bonzanini", "Thomas Roelleke"], "venue": "In Proceedings of the 18th International Conference on String Processing and Information", "citeRegEx": "Yahyaei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yahyaei et al\\.", "year": 2011}, {"title": "A cross-collection mixture model for comparative text mining", "author": ["Zhai et al.2004] ChengXiang Zhai", "Atulya Velivelli", "Bei Yu"], "venue": null, "citeRegEx": "Zhai et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2004}, {"title": "Cross-lingual latent topic extraction", "author": ["Zhang et al.2010] Duo Zhang", "Qiaozhu Mei", "ChengXiang Zhai"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "State transitions are modeled through a Dirichlet distribution, tuned to bias the system towards self-transitions and thus avoid rapid switching between states (Beal et al., 2002; Teh et al., 2003; Fox et al., 2010).", "startOffset": 160, "endOffset": 215}, {"referenceID": 20, "context": "State transitions are modeled through a Dirichlet distribution, tuned to bias the system towards self-transitions and thus avoid rapid switching between states (Beal et al., 2002; Teh et al., 2003; Fox et al., 2010).", "startOffset": 160, "endOffset": 215}, {"referenceID": 9, "context": "State transitions are modeled through a Dirichlet distribution, tuned to bias the system towards self-transitions and thus avoid rapid switching between states (Beal et al., 2002; Teh et al., 2003; Fox et al., 2010).", "startOffset": 160, "endOffset": 215}, {"referenceID": 5, "context": "about cities in English and French (Chen et al., 2009).", "startOffset": 35, "endOffset": 54}, {"referenceID": 12, "context": "The monolingual segment alignment is evaluated against a Hidden Topic Markov Model (Gruber et al., 2007) \u2013 and the GMM model (Chen et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 25, "context": "Text alignment For monolingual comparable corpora, text relatedness measures produce good alignment results (Barzilay and Elhadad, 2003; Yahyaei et al., 2011).", "startOffset": 108, "endOffset": 158}, {"referenceID": 21, "context": "mapping between aligned sentences and the correlation of sentence length (Tiedemann, 2011).", "startOffset": 73, "endOffset": 90}, {"referenceID": 13, "context": "Bilingual comparable corpora have been mainly used to extract parallel fragments (Gupta et al., 2013) or paraphrases and word translations (Fung and McKeown, 1997).", "startOffset": 81, "endOffset": 101}, {"referenceID": 13, "context": "Bilingual comparable corpora have been mainly used to extract parallel fragments (Gupta et al., 2013) or paraphrases and word translations (Fung and McKeown, 1997). We focus on an alignment of fragments that have the same theme, but are not necessarily parallel. Sequence modeling Modeling a document as a sequence of topics brings HMMs naturally to mind. Work in this area started with Mulbregt et al. (1998) and Blei and Moreno (2001).", "startOffset": 82, "endOffset": 410}, {"referenceID": 13, "context": "Bilingual comparable corpora have been mainly used to extract parallel fragments (Gupta et al., 2013) or paraphrases and word translations (Fung and McKeown, 1997). We focus on an alignment of fragments that have the same theme, but are not necessarily parallel. Sequence modeling Modeling a document as a sequence of topics brings HMMs naturally to mind. Work in this area started with Mulbregt et al. (1998) and Blei and Moreno (2001). This ap-", "startOffset": 82, "endOffset": 437}, {"referenceID": 1, "context": "an HMM were modeled through a Dirichlet process (Beal et al., 2002).", "startOffset": 48, "endOffset": 67}, {"referenceID": 1, "context": "an HMM were modeled through a Dirichlet process (Beal et al., 2002). This allows one to control state transitions, and to let the model induce the number of states that best fit the data. Beal et al. (2002) used three hyperparameters to control the HMM: for self-transitions, for transitions to previously used states, and for adding a new transition.", "startOffset": 49, "endOffset": 207}, {"referenceID": 1, "context": "an HMM were modeled through a Dirichlet process (Beal et al., 2002). This allows one to control state transitions, and to let the model induce the number of states that best fit the data. Beal et al. (2002) used three hyperparameters to control the HMM: for self-transitions, for transitions to previously used states, and for adding a new transition. Teh et al. (2003) and Fox et al.", "startOffset": 49, "endOffset": 370}, {"referenceID": 1, "context": "an HMM were modeled through a Dirichlet process (Beal et al., 2002). This allows one to control state transitions, and to let the model induce the number of states that best fit the data. Beal et al. (2002) used three hyperparameters to control the HMM: for self-transitions, for transitions to previously used states, and for adding a new transition. Teh et al. (2003) and Fox et al. (2010) extended this model and showed how to manipulate parameters to avoid rapid switching between states.", "startOffset": 49, "endOffset": 392}, {"referenceID": 12, "context": "Language models In topic segmentation one can consider the words to be the basic unit (Gruber et al., 2007), or sentences/paragraphs (e.", "startOffset": 86, "endOffset": 107}, {"referenceID": 12, "context": "Language models In topic segmentation one can consider the words to be the basic unit (Gruber et al., 2007), or sentences/paragraphs (e.g. (Blei and Moreno, 2001), (Eisenstein and Barzilay, 2008)). Sentences are themselves heterogeneous. Daum\u00e9 III and Marcu (2006) used topic models for query-driven summarization.", "startOffset": 87, "endOffset": 265}, {"referenceID": 26, "context": "Zhai et al. (2004), Titov and McDonald (2008) and Paul and Girju (2009) model topics (aspects) that run throughout a collection of documents, and", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "Zhai et al. (2004), Titov and McDonald (2008) and Paul and Girju (2009) model topics (aspects) that run throughout a collection of documents, and", "startOffset": 0, "endOffset": 46}, {"referenceID": 26, "context": "Zhai et al. (2004), Titov and McDonald (2008) and Paul and Girju (2009) model topics (aspects) that run throughout a collection of documents, and", "startOffset": 0, "endOffset": 72}, {"referenceID": 24, "context": "Wang et al. (2011) also assume a sentence to be generated from a mixture of topics, in particular two: a \u201cfunctional\u201d (i.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "WSD and topic modeling Boyd-Graber et al. (2007) combine topic modeling with word sense", "startOffset": 23, "endOffset": 49}, {"referenceID": 26, "context": "Zhang et al. (2010) use a bilingual dictionary as a source of constraints for bridging texts in two languages, using the assumption that related words in different languages have similar distributions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "Ni et al. (2009) mine multilingual topics from Wikipedia, using the articles on", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Mimno et al. (2009) build polylingual topic models from sets of documents on the same topics, in particular Wikipedia articles in several languages.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "Mimno et al. (2009) build polylingual topic models from sets of documents on the same topics, in particular Wikipedia articles in several languages. They assume that the different language versions of an article have similar topic distributions, and topics consist of languagespecific word distribution. Vuli\u0107 et al. (2011) build bilingual topic models for information retrieval, using comparable corpora \u2013 in particular collections of Wikipedia articles on the same topics in the targeted languages.", "startOffset": 0, "endOffset": 324}, {"referenceID": 1, "context": "The transition probabilities are modeled by a Dirichlet distribution (Beal et al., 2002): the transition probabilities from a state zt = j at time t can be interpreted as mixing proportions for the state at time t + 1: \u03c0j = {\u03c0j1, .", "startOffset": 69, "endOffset": 88}, {"referenceID": 1, "context": "The persistence of states is encouraged by increasing the probability of self-transitions using a \u201csticky\u201d parameter \u03ba (Beal et al., 2002; Fox et al., 2010): \u03c0i|\u03b1, \u03ba \u223c", "startOffset": 119, "endOffset": 156}, {"referenceID": 9, "context": "The persistence of states is encouraged by increasing the probability of self-transitions using a \u201csticky\u201d parameter \u03ba (Beal et al., 2002; Fox et al., 2010): \u03c0i|\u03b1, \u03ba \u223c", "startOffset": 119, "endOffset": 156}, {"referenceID": 9, "context": "Fox et al. (2010) model the transitions through a Dirichlet process to allow the model to infer the number of states.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "The disambiguated concepts are the nodes in the maximum edge-weighted clique in this graph, and their expression in the text is replaced with the corresponding unique (and language independent) concept ID, resulting in the concept-annotated texts illustrated in Figure 2 (Fahrni et al., 2011).", "startOffset": 271, "endOffset": 292}, {"referenceID": 5, "context": "Data The data consists of a collection of articles about cities in English and French presented in (Chen et al., 2009).", "startOffset": 99, "endOffset": 118}, {"referenceID": 25, "context": "Yahyaei et al. (2011)\u2019s data did not fit this structure, and we could not use it to test this method.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "The performance of the model is evaluated based on the assigned t-topics on the task of crossdocument alignment (Chen et al., 2009).", "startOffset": 112, "endOffset": 131}, {"referenceID": 5, "context": "Following (Chen et al., 2009) we compute:", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "Baselines Gruber et al.\u2019s (2007) HTMM models", "startOffset": 10, "endOffset": 33}, {"referenceID": 5, "context": "Chen et al. (2009) propose GMM, a global model for documents in a collection.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "We present a method for coarse-grained cross-lingual alignment of comparable texts: segments consisting of contiguous paragraphs that discuss the same theme (e.g. history, economy) are aligned based on induced multilingual topics. The method combines three ideas: a two level LDA model that filters out words that do not convey themes, an HMM that models the ordering of themes in the collection of documents, and language-independent concept annotations to serve as a crosslanguage bridge and to strengthen the connection between paragraphs in the same segment through concept relations. The method is evaluated on English and French data previously used for monolingual alignment. The results show state-ofthe-art performance in both monolingual and cross-lingual settings.", "creator": "LaTeX with hyperref package"}}}