{"id": "1411.7974", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2014", "title": "Solving Games with Functional Regret Estimation", "abstract": "creus We 109.1 propose kayseri a absorber novel castaic online 77.18 learning ruichang method taunted for nmci minimizing 14.33 regret in large extensive - lisheng form games. The letra approach learns galleons a function conformably approximator online to hkc estimate the finanza regret gougeon for choosing mihdar a kanharith particular action. auron A no - regret algorithm uses these nutten estimates in mullet place caucausus of clearly the true regrets to erd\u0151s define trigonal a sequence sleekness of policies.", "histories": [["v1", "Fri, 28 Nov 2014 18:45:50 GMT  (292kb)", "https://arxiv.org/abs/1411.7974v1", "AAAI Conference on Artificial Intelligence 2015"], ["v2", "Wed, 31 Dec 2014 23:45:22 GMT  (113kb)", "http://arxiv.org/abs/1411.7974v2", "AAAI Conference on Artificial Intelligence 2015"]], "COMMENTS": "AAAI Conference on Artificial Intelligence 2015", "reviews": [], "SUBJECTS": "cs.AI cs.GT cs.MA", "authors": ["kevin waugh", "dustin morrill", "james andrew bagnell", "michael h bowling"], "accepted": true, "id": "1411.7974"}, "pdf": {"name": "1411.7974.pdf", "metadata": {"source": "CRF", "title": "Solving Games with Functional Regret Estimation", "authors": ["Kevin Waugh", "Dustin Morrill", "J. Andrew Bagnell", "Michael Bowling"], "emails": ["waugh@cs.cmu.edu", "morrill@ualberta.ca", "dbagnell@ri.cmu.edu", "mbowling@ualberta.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n79 74\nv2 [\ncs .A\nI] 3\n1 D\nec 2\n01 4"}, {"heading": "Introduction", "text": "Online learning in sequential decision-making scenarios has wide ranging applications (e.g., online path planning (Awerbuch and Kleinberg 2004), opponent exploitation (Southey, Hoehn, and Holte 2009), and portfolio optimization (Hazan et al. 2006)). While many such applications have considerable structure, often this structure is lost when formulating the application into a traditional problem specification such as a multi-armed bandit or extensive-form game. Since regret-minimizing algorithms usually require computational resources and learning time that grows with the problem representation size, losing this structure often makes the resulting problem intractable. Currently, the remedy is for the practitioner to abstract the original domain based on structural features into a problem with a tractable representation size (e.g., using domain knowledge to preselect a small number of likely-orthogonal choices in a multiarmed bandit).\nConsider the popular testbed of poker. The humanplayed game is highly-structured with a compact set of rules. Its unstructured extensive-form game representation, however, with nodes in the game tree for ev-\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nery possible sequence of states is anything but compact (viz., two-player, no-limit Texas hold\u2019em has 8.2 \u00b7 10160 such sequences (Johanson 2013, p. 12)). Even specialized equilibrium-finding algorithms have no hope of solving this game in its extensive-form representation. Here, much of the recent progress has employed abstraction, grouping together similar situations and actions based on domain knowledge (Gilpin, Sandholm, and Sorensen 2007; Johanson 2007).\nUsing abstraction as a preliminary step to capture all of the possible structure of the domain is unsatisfying. It can place limitations on how structural features are exploited, e.g., in extensive-form games the resulting abstraction must be discrete and result in perfect recall. Furthermore, it requires the practitioner to have foresight as to what the learner might face when it is actually making decisions.\nIn this paper, we present a new algorithm for online learning in adversarial sequential decision-making scenarios that makes use of structural features of the domain during learning. In particular, we employ an online regressor from domain features to approximate regrets incurred by the regret minimizing online algorithm. This simple change allows the algorithm to employ more general forms of abstraction, and can change and tune the abstraction to both the game and its solution. We prove a bound on our approach relating the accuracy of the regressor to the regret of the algorithm and demonstrate its efficacy in a simplified poker game."}, {"heading": "Regression Regret-Matching", "text": "Let us begin by presenting a new, yet simple, algorithm for the standard online learning framework. Let A be the set of N actions or experts. On each time step t \u2208 [T ] \u2261 {1, . . . , T }, the online algorithm chooses xt \u2208 \u2206A, a distribution over the experts, then observes ut, a bounded reward vector where \u2016ut\u2016\u221e \u2264 L2. The algorithm receives reward (xt \u00b7 ut) and then updates its prediction.\nAn algorithm\u2019s external regret compares its performance to the best expert in hindsight. To be no-regret is to have regret grow sublinearly in T ,\nRext = max x\u2217\u2208\u2206A\nT \u2211\nt=1\n(x\u2217 \u00b7 ut)\u2212 (xt \u00b7 ut) \u2208 o(T ).\nThat is, its average regret goes to zero in the worst-case.\nAlgorithm 1 Regret-matching with Regret Regression X \u2190 [], y \u2190 [] for t \u2208 [T ] do\nf \u2190 TRAINREGRESSOR(X, y) for a \u2208 A do\nz(a) \u2190 f(\u03d5(a)) end for xt \u221d max{0, zt} Observe ut for a \u2208 A do X \u2190 X \u222a \u03d5(a) y \u2190 y \u222a (ut(a)\u2212 (xt \u00b7 ut))\nend for end for\nA simple algorithm with this property is regret-matching.\nDefinition 1 (Regret-matching). Define the vector of cumulative regret as\nRT \u2261 T \u2211\nt=1\nut \u2212 (xt \u00b7 ut)e\nwhere e is the vector of all ones. Choose xt \u221d max{0, Rt\u22121}. Theorem 1 (from Hart & Mas-Colell (2000)). Regretmatching is no-regret. In particular, Rext \u2264 \u221a LNT .\nWe often have structure between the available actions, e.g., when an action corresponds to betting a particular amount or choosing an acceleration. It is common to model such situations by discretizing the range of available actions and using a single action per interval. In this case, the structure linking actions is completely lost after this abstraction.\nIn more complicated stateful settings, we can describe each action a \u2208 A with a feature vector, \u03d5(a) \u2208 X . In poker, there are numerous ways to quantify the strength of a particular hand. For example, its expected value against different possible opponent holdings as well as potential-aware statistics, like the variance of the expected value. Current techniques use eight to ten such statistics to describe the portion of the state space representing the player\u2019s hand.\nTo reduce the action space in these settings, prior work uses unsupervised clustering techniques on the actions in feature space. This can essentially be thought of as an informed discretization of the space. The hope is that actions with similar features can be collapsed, or made indistinguishable, without incurring too much loss (Shi and Littman 2002). We use this feature vector to retain the structure during learning. To reduce to the standard framework, we can simply use an indicator feature for each action.\nAlgorithm 1 contains the pseudocode for our approach. It employs an arbitrary mean-squared-error minimizing regressor to estimate the cumulative regret. In particular, the training procedure is expected to produce f where f(\u03d5(a)) \u2248 Rt(a)/t, or equivalently, the algorithm\u2019s regret estimate is R\u0303t(a) = tf(\u03d5(a)).\nWe consider the error of our regressor with the l2 distance between the approximate and true cumulative regret vectors: \u2016Rt \u2212 R\u0303t\u20162. Note that this quantity is related to the representational power and recall of the regressor. In particular, if we cannot represent the true cumulative regret then it will be non-zero due to this bias. The regret of Algorithm 1 can then be bounded in terms of this representational power.\nTheorem 2. If for each t \u2208 [T ], \u2016Rt \u2212 R\u0303t\u20162 \u2264 \u01eb, then regression regret-matching has regret bounded by \u221a\nTNL+ 2T \u221a L\u01eb.\nThe proof is in the appendix in the supplemental material. This is a worst-case bound. That is, so long as \u01eb is small regression regret-matching cannot be much worse than normal regret-matching. It is possible that there are cases where regression regret-matching can do better than regretmatching if the structure is favorable.\nThe first subtlety we must note is that the regressor minimizes the mean-squared error of the immediate regrets, i.e.\n{( \u03d5(a), ui(a)\u2212 ui \u00b7 xi ) | \u2200a \u2208 A, i \u2208 [t] }\nforms the training set. If the hypothesis class can represent the regrets, then tf(\u03d5(a)) = Rt(a) obtains the minimum mean-squared error. Note that this error is likely not zero even in the realizable case!\nIn words, the theorem states that if the error of the regressor decreases like O(1/T ) then the algorithm obtains a O( \u221a T ) regret bound. Note that the cumulative regret can grow like O(T ), so a fixed \u01eb across time implies a decreasing error rate. The algorithm remains no-regret so long as the bias goes to zero and recall goes to one, i.e., it is asymptotically unbiased. If the bias is constant then there is a constant term in the regret bound. When solving a game, this constant term constitutes the error introduced by a lossy abstraction.\nNote that it is sufficient to make the estimator asymptotically unbiased by including indicator features for each action with proper regularization. In a sense, this allows the algorithm to move from an estimate of the true regrets as time increases, i.e., as the true regrets stabilize.\nNext, we aim to use our algorithm for sequential decisionmaking and to relate it to current abstraction techniques. Before we can accomplish this, we must review the extensiveform game framework."}, {"heading": "Extensive-form Games", "text": "A two-player zero-sum extensive-form game is a tuple \u0393 = (H, p, \u03c3c, I, u) (Osborne and Rubinstein 1994). The set of histories, H, form a tree rooted at \u03c6 \u2208 H, the empty history. A(h) is the set of actions available at h \u2208 H and ha \u2208 H is a child of h. The subset Z \u2286 H is the set of terminal histories, i.e., if z \u2208 Z then A(z) is empty. p : H \\ Z \u2192 {1, 2, c} is the player choice function that determines if nature or a player is to act at any given history. For all histories h \u2208 H where chance acts (that is, all h where p(h) = c), \u03c3c(\u00b7|h) \u2208 \u2206A(h) is a probability distribution over available chance actions that defines nature\u2019s strategy. The information partition, I = I1\u222aI2, separates histories into information sets such that all histories in an information set\nare indistinguishable to the acting player and have the same set of actions. Finally, u : Z \u2192 R is the game\u2019s utility function. At a terminal history, z \u2208 Z , u1(z) = u(z) is the reward to the first player and u2(z) = \u2212u(z) the reward to the second. That is, the gains of one player are the loses to the other and the rewards sum to zero.\nA behavioral strategy for player i, \u03c3i \u2208 \u03a3i, defines a probability distribution at all information sets where player i acts. That is, if I \u2208 Ii, then \u03c3i(\u00b7|I) \u2208 \u2206A(I). We call a tuple of strategies (\u03c31, \u03c32) a strategy profile. Let \u03c0\u03c3(z) be the probability of reaching z by traversing the tree using \u03c3 from the root. Let \u03c0\u03c3\u2212i(z) be the probability of reaching z using \u03c3 assuming player i takes actions to reach z with probability one. Finally, let \u03c0\u03c3(h, z) be the probability of reaching z using \u03c3 while starting at history h instead of the root (\u03c0\u03c3(h, z) is zero if h is not an ancestor of z). With these definitions we can write the expected utility to player i under profile \u03c3 as ui(\u03c3) = \u2211 z\u2208Z \u03c0 \u03c3(z)ui(z).\nA strategy profile is an \u03b5-Nash equilibrium if\nu1(\u03c31, \u03c32) + \u03b5 \u2265 u1(\u03c3\u20321, \u03c32), and \u2200\u03c3\u20321 \u2208 \u03a31 u2(\u03c31, \u03c32) + \u03b5 \u2265 u2(\u03c31, \u03c3\u20322). \u2200\u03c3\u20322 \u2208 \u03a32\nThat is, if neither player can benefit by more than \u03b5 by deviating from \u03c3 unilaterally. In a two-player zero-sum game, a strategy belonging to a Nash equilibrium is minimax optimal so it attains the highest utility against an optimal opponent. Thus, it is safe to play an equilibrium strategy."}, {"heading": "Counterfactual Regret Minimization", "text": "Counterfactual regret minimization (CFR) is an algorithm for computing an \u03b5-equilibrium in extensive-form games (Zinkevich et al. 2008). It employs multiple no-regret online learners, customarily instances of regret-matching, minimizing counterfactual regret at every information set.\nThe counterfactual utility for taking action a \u2208 A(I) from information set I \u2208 Ii at time t is defined as\nuti(a|I) = \u2211\nh\u2208I\n\u2211\nz\u2208Z\n\u03c0t\u2212i(h)\u03c0 t(ha, z)ui(z).\nIn words, it is the expected utility for player i taking action a at information set I assuming they play to reach I , take action a, and then follow their current policy thereafter.\nUsing counterfactual utility, we define immediate counterfactual regret and cumulative counterfactual regret as\nrti(a|I) = uti(a|I)\u2212 \u2211\na\u2032\u2208A(I)\n\u03c3ti(a \u2032|I)uti(a\u2032|I), and\nRTi (a|I) = T \u2211\nt=1\nrti(a|I),\nrespectively. Each information set is equipped with its own no-regret learner that updates and maintains its cumulative regret. It is the cumulative regret from time t that defines the player\u2019s strategy at time t+ 1.\nTypically, regret-matching is the no-regret learner of choice with CFR. It defines the player\u2019s policy at time t as\n\u03c3ti(a|I) \u221d max{0, Rt\u22121i (a|I)}.\nThe following two theorems show counterfactual regret minimization in self-play converges to an equilibrium.\nTheorem 3. If two no-regret algorithms in self-play each have no more than \u03b5 external regret, RT,exti \u2264 \u03b5, then the average of their strategies form a 2\u03b5/T -Nash equilibrium.\nTheorem 4 (from Zinkevich et al. (2008)). If regretmatching minimizes counterfactual regret at each information set for T iterations, then RT,exti \u2264 |Ii| \u221a LNT , where L2 is the maximum possible utility and N is the maximum number of actions at any information set.\nCounterfactual regret minimization is the large-scale equilibrium-finding algorithm of choice. Though other algorithms boast better asymptotic rates (Gilpin et al. 2007), CFR more rapidly converges to an acceptable solution in practice. Furthermore, it is simple, highly parallelizable and amenable to sampling techniques (Lanctot et al. 2009) that dramatically reduce computational requirements."}, {"heading": "Regression CFR", "text": "We are now ready to put the pieces together to form our new regret algorithm for sequential decision-making scenarios: Regression CFR (RCFR). The algorithm is simple. We will minimize counterfactual regret at every information set using estimates of the counterfactual regret that comes from a single common regressor shared across all information sets. The common regressor uses features, \u03d5(I, a), that are a function of the information-set/action pair. This allows the regressor to generalize over similar actions and similar situations in building its regret estimates.\nAs with CFR, we can derive a regret bound, which in turn implies a worst-case bound on the quality of an approximate equilibrium resulting from self-play.\nTheorem 5. If for each t \u2208 [T ] at every information set I \u2208 Ii, \u2016Rt(\u00b7|I) \u2212 R\u0303t(\u00b7|I)\u20162 \u2264 \u01eb, then RCFR has external regret bounded by |Ii| \u221a TNL+ 2T \u221a L\u01eb.\nThe proof combines Theorem 2 together with the CFR convergence proof of Zinkevich et al. (2008).\nNow that we have presented RCFR, let us examine how it relates to modern abstraction techniques."}, {"heading": "Relationship to Abstraction", "text": "As noted in the introduction, often the problem we wish to solve, when viewed without any structure, is intractably large. For example, representing a behavioral strategy profile in no-limit Texas Hold\u2019em1 requires 8.2 \u00b7 10160 entries (Johanson 2013, p. 12). To surmount this, we typically first distill the game to a tractably-sized abstract game. Then, after solving it, we map its solution into the full game. The hope is that the abstract game and its solution roughly maintain the important strategic aspects of the full game. Though this turns out to be false (Waugh et al. 2008), it appears to not occur with any significance in the large games of interest (Johanson et al. 2011; Bard et al. 2013).\n1We consider the game with 50-100 blinds with 20,000 chip stacks as played in the Annual Computer Poker Competition.\nThe common way to abstract a game is to simply group together similar information sets (Gilpin, Sandholm, and Sorensen 2007; Johanson 2007). That is, we take situations in the full game that the player can differentiate and make them indistinguishable in the abstract game. If the two situations have similar behavior in equilibrium, then at the very least we have not lost representational power in doing so. This form of abstraction provides a function f mapping full game information sets to abstract game information sets.\nTo create such an abstraction requires some notion of similarity and compatibility of information sets, and a clustering algorithm, like k-means. Ultimately, the size of the abstract game is a function of the number of clusters\u2014how the information sets collapse together. Let us assume that we have a function \u03d5 : I \u00d7A \u2192 X that maps information-set/action pairs to domain-specific features in space X . We can use such a function to define the similarity of compatible information sets I and I \u2032 by, for example, the cumulative inner product, \u2211\na\u2208A \u3008\u03d5(I, a), \u03d5(I \u2032, a)\u3009, which can be passed to k-means to cluster the information sets (often subject to constraints such as preserving perfect recall).\nIn order to compare the approach to RCFR, consider a single iteration of the counterfactual regret update in both the original game and the abstract game where the players\u2019 strategies are fixed. In particular, we have\nr\u0303ti(a|Iabstract) = \u2211\nI full\u2208f\u22121(Iabstract)\nrti(a|I full).\nThat is, the regret at information set Iabstract in the abstract game, r\u0303ti(\u00b7|Iabstract) is the sum of the regrets in the full game of all information sets that map to it, f\u22121(Iabstract). Taking this view-point in reverse, using CFR on the abstract game is operationally equivalent to solving the full game where we maintain and update the regrets r\u0303ti(\u00b7|f(I full)), i.e., we approximate the true regrets with a tabular regressor rti(a|I full) \u2248 r\u0303ti(a|f(I full)). Thus, abstraction can be thought of as a special-case of RCFR with a particular choice for how to approximate the cumulative counterfactual regret.\nRCFR is, of course, not restricted to tabular regressors and so can capture structure that goes beyond traditional abstraction. For example, using linear or kernel regression provides a sort of \u201csoft\u201d abstraction. That is, the regret of two distinct (in feature space) actions can effect one another without making the two completely indistinguishable, which is the only option available to traditional \u201chard\u201d abstraction.\nUnlike traditional abstraction where f is chosen a priori, RCFR is also able to learn the structure of the abstraction online upon seeing actual data. And since the regressor is trained after each time step, RCFR can effectively re-abstract the game on-the-fly as necessary. Furthermore, the abstraction is informed by the game in a way that is compatible with the learner. Imperfect information games have an interesting property in that portions of the game that are never reached in optimal play have strategic effects. For example, if one player was to behave suboptimally, the other might benefit by deviating into an otherwise undesirable space. Without the possibility of deviating there may be\nno way to punish the poor play. Practically, this presents as a rather annoying hurdle for abstraction designers. In particular, seemingly ideal abstractions tailored perfectly to the structure of an equilibrium may actually lead to poor solutions due to missing important, but seemingly unnecessary, parts of the strategy space. RCFR avoids all of this as the regressor tunes the abstraction to the current policy, not the solution."}, {"heading": "Experimental Results", "text": "In order to illustrate the practicality of RCFR, we test its performance in Leduc Hold\u2019em, a simplified poker game. Our goal is to compare the strategies found by RCFR with varying regressors to strategies generated with conventional abstraction techniques. In addition, we examine the iterative behaviour of RCFR compared to CFR."}, {"heading": "Leduc Hold\u2019em", "text": "Leduc Hold\u2019em is a poker game based on Kuhn poker (Southey et al. 2005). It provides a good testbed as common operations, like best response and equilibrium computations, are tractable and exact.\nThe game has two betting rounds, the preflop and flop. At the beginning of the game both players ante a single chip into the pot and are dealt a single private card from a shuffled deck of six cards\u2014two jacks, two queens and two kings. Then begins the preflop betting round where the first player can either check or bet. If the first player checks, passing their turn, then the second player can end the betting round by checking as well, or continue by betting. When facing a bet; the player can raise by placing two chips into the pot; call by matching the bet in the pot and ending the round; or fold by forfeiting the pot to the opponent. There is a maximum of two wagers per round, i.e., one bet and one raise. A single public card is dealt face up for both players to see at the beginning flop. If the flop betting ends without either player folding, a showdown occurs and the player with the best hand takes the pot. A player that pairs, i.e., their card matches the public card, always has the best hand no matter the rank of the paired card or the opponent\u2019s card. If neither player pairs, the one with the highest rank card wins. In the event of a tie, the pot is split. The size of a wager preflop is two chips, and is doubled to four chips on the flop.\nLeduc Hold\u2019em has 672 sequences. At equilibrium, the first player is expected to lose 0.08 chips per hand. We show results in milliblinds/antes per hand (mb/h), a thousandth of a chip, i.e., optimally the first player loses 80 mb/h."}, {"heading": "Features and Implementation", "text": "We use a regression tree aiming to minimize mean-squared error as our regressor. When training, we examine all candidate splits on a single feature and choose the one that results in the best immediate error reduction. The data is then partitioned according to this split and we recursively train both sets. If the error improvement at a node is less than a threshold, or no improvement can be made by any split, a leaf is inserted that predicts the average. It is this error threshold that\nwe manipulate to control the complexity of the regressor\u2014 the size of the tree. All the training data is kept between iterations, as in Algorithm 1.\nEight features were chosen such that the set of features would be small, thus allowing fast regression tree training, which is done on every RCFR iteration, but still descriptive enough to have a unique feature expansion for every sequence: 1) the expected hand strength (E[HS]), or the probability of winning the hand given the available information, and marginalized over a uniform distribution of opponent hands and possible future board card; 2) the rank of the board card, or zero on the preflop; 3) the pot size; 4) the pot relative size of the wager being faced, or zero if not facing a wager; 5) the number of actions this hand; 6) the number of wagers this hand; 7) an indicator on whether or not the next action would be a fold action; and 8) the pot relative size of the wager that would be made by taking the next action, or zero if the next action would be a check or call.\nFeatures (1) and (2) refer to private and public card information, (3) through (6) are public chip and action information, while (7) and (8) fully describe the next potential action in the sequence2."}, {"heading": "Experiments", "text": "We evaluate the empirical performance of RCFR here according to three metrics: 1) convergence properties, 2) exploitability, and 3) one-on-one competitions.\nStrategies were computing using RCFR and four different error threshold values. Each threshold was chosen so that RCFR\u2019s regressor would have similar complexity to that of a conventional abstraction, or that of the full game. In Leduc Hold\u2019em, a typical abstraction is one that groups together cards on the preflop and only distinguishes between pairing and non-pairing board cards on the flop. These handcrafted abstractions are analogous to the E[HS] based abstractions commonly used in Texas Hold\u2019em. Abstractions are denoted, for example, J.QK to describe the abstraction that can distinguish between a jack and a queen or king, but cannot distinguish between a queen and king. The remaining three abstractions are then JQK, JQ.K, and J.Q.K. One may also note that J.Q.K is a strict refinement of the other three abstractions, and J.QK and JQ.K both are strict refinements of JQK. To generate strategies, chance sampling CFR (Zinkevich et al. 2007) was run for 100000 iterations to solve each abstract game.\nEach different RCFR strategy is denoted, for example, RCFR-22%, to describe RCFR using a regressor 22% the size of a strategy in the full game. RCFR-22%, RCFR-47%, and RCFR-66% correspond to JQK, J.QK/JQ.K, and J.Q.K, respectively, in terms of complexity. RCFR-96% corresponds to FULL, which denotes no abstraction, and it was made by setting the error threshold to zero, so the regressor was free to split on every feature and become as large as the full game. RCFR and CFR were run for 100000 iter-\n2No explicit check/call feature is necessary because it is implicitly encoded by features (7) and (8) in combination. The action would be a check/call if and only if both are zero (the action would not be a fold nor a wager, and the game has only three action types).\nations to generate the set of RCFR strategies and a FULL strategy, respectively.\nConvergence Figure 1 shows that all RCFR strategies improve at the same rate as an unabstracted CFR strategy (FULL) until a plateau is reached, the height of which is determined by the error threshold parameter of that RCFR instance. These plateaus are essentially the exploitability cost incurred by estimating regrets instead of computing and storing them explicitly. Larger thresholds, reducing regressor complexity, incur a greater cost and thus have a higher plateau. As expected, when the error threshold is set to zero, as in the case of RCFR-96%, RCFR\u2019s progression mimics unabstracted CFR for the full set of 100000 iterations.\nExploitability Figure 2 shows that RCFR, given complexity restrictions equivalent to those of conventional abstractions, finds significantly less exploitable strategies. RCFR-66%\u2019s regressor is 2% smaller than the size of the J.Q.K abstract game, yet J.Q.K is sixteen times more exploitable! The closest corresponding strategies in terms of exploitability are RCFR-47% and JQ.Kwhere JQ.K is only three and a half times more exploitable.\nAnother useful practical property of RCFR is that it appears to avoid non-monotonicities that have been observed in hand-crafted abstractions (Waugh et al. 2008). That is, increasing the complexity of the regressor appears to improve the full game exploitability of the resulting strategy.\nOne-on-one Competitions Table 1 is the one-on-one competition crosstable between each of the agents. Against almost every opponent, each RCFR variant outperforms its corresponding strategy. The exceptions, for example, JQ.K wins 100.70 mb/h against RCFR-22% while RCFR-47% wins only 97.61 mb/h against this same opponent, are small margins. In addition, each RCFR strategy defeats or, in the case of RCFR-96%, ties its counterparts. RCFR-22% and RCFR-47% even win against larger abstract strategies J.QK and J.Q.K, respectively. Dividing the agents into an RCFR\nteam and a conventional agents team, the RCFR team wins 2033.34 mb/h in aggregate."}, {"heading": "Future Work", "text": "In this paper we introduced RCFR, a technique that obviates the need for abstraction as a preprocessing step by employing a regression algorithm online. The regressor essentially learns and tunes an abstraction automatically as the algorithm progresses greatly simplifying abstraction design.\nThe experimental results show this technique is quite promising. The next step is to scale to larger games, e.g., no-limit Texas Hold\u2019em, where abstraction is necessary. It is likely that RCFR is up to this challenge, but a number of interesting questions need be answered along the way.\nFirst, CFR is the algorithm of choice for equilibriumfinding due to the powerful sampling schemes available to it. In this paper, we have not explored sampling with RCFR at all. Theoretically, there are no obvious restrictions that forbid sampling, but we posit that lower variance sampling\nschemes than are currently employed will be preferred. Second, the choice of features and regressor are extremely important and remain open. Online regression algorithms are appealing due to their simplicity as well as removing the need to store data from past iterations. If this route is pursued, care must be taken to ensure access to strategy and updating the regressor remain inexpensive. This is possible, for example, by using sparse features.\nThird, the average strategy is typically the result of the equilibrium computation. This average, too, must be encoded with a regressor in the case of large games. Conceptually, this poses no problems, but again care must be taken. In particular, due to the sequential nature of the policy, errors made by the regressor propagate and compound. The DAGGer algorithm corrects this by ensuring the regressor imposes the same distribution over future decisions as is observed in the true policy (Ross, Gordon, and Bagnell 2011)."}, {"heading": "Acknowledgments", "text": "This work is supported by the ONR MURI grant N0001409-1-1052, the National Sciences and Engineering Research Council of Canada (NSERC) and Alberta Innovative Technology Futures (AITF). Thanks to Compute Canada for computational resources and the Computer Poker Research Group (CPRG) for software infrastructure support."}], "references": [{"title": "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches", "author": ["Awerbuch", "B. Kleinberg 2004] Awerbuch", "R. Kleinberg"], "venue": "In ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "Awerbuch et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch et al\\.", "year": 2004}, {"title": "Online implicit agent modelling", "author": ["Bard"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS)", "citeRegEx": "Bard,? \\Q2013\\E", "shortCiteRegEx": "Bard", "year": 2013}, {"title": "Gradient-based algorithms for finding Nash equilibria in extensive form games", "author": ["Gilpin"], "venue": "In International Workshop on Internet and Network Economics (WINE)", "citeRegEx": "Gilpin,? \\Q2007\\E", "shortCiteRegEx": "Gilpin", "year": 2007}, {"title": "Potential-aware automated abstraction of sequential games, and holistic equilib", "author": ["Sandholm Gilpin", "A. Sorensen 2007] Gilpin", "T. Sandholm", "T. Sorensen"], "venue": null, "citeRegEx": "Gilpin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gilpin et al\\.", "year": 2007}, {"title": "A simple adaptive procedure leading to correlated equilibrium", "author": ["Hart", "S. Mas-Colell 2000] Hart", "A. Mas-Colell"], "venue": null, "citeRegEx": "Hart et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hart et al\\.", "year": 2000}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Hazan"], "venue": "In Conference on Learning Theory (COLT)", "citeRegEx": "Hazan,? \\Q2006\\E", "shortCiteRegEx": "Hazan", "year": 2006}, {"title": "Accelerating best response calculation in large extensive games", "author": ["Johanson"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Johanson,? \\Q2011\\E", "shortCiteRegEx": "Johanson", "year": 2011}, {"title": "Monte carlo sampling for regret minimization in extensive games", "author": ["Lanctot"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lanctot,? \\Q2009\\E", "shortCiteRegEx": "Lanctot", "year": 2009}, {"title": "A Course On Game Theory", "author": ["Osborne", "M. Rubinstein 1994] Osborne", "A. Rubinstein"], "venue": null, "citeRegEx": "Osborne et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 1994}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Gordon Ross", "S. Bagnell 2011] Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Abstraction methods for game theoretic poker", "author": ["Shi", "J. Littman 2002] Shi", "M. Littman"], "venue": "In International Conference on Computers and Games (CG),", "citeRegEx": "Shi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2002}, {"title": "bluff: Opponent modelling in poker", "author": ["Southey"], "venue": "In Conference on Uncertainty in AI (UAI)", "citeRegEx": "Southey,? \\Q2005\\E", "shortCiteRegEx": "Southey", "year": 2005}, {"title": "Effective short-term opponent exploitation in simplified poker. Machine Learning 74(2):159\u2013189", "author": ["Hoehn Southey", "F. Holte 2009] Southey", "B. Hoehn", "R. Holte"], "venue": null, "citeRegEx": "Southey et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Southey et al\\.", "year": 2009}, {"title": "Abstraction pathologies in extensive games", "author": ["Waugh"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS)", "citeRegEx": "Waugh,? \\Q2008\\E", "shortCiteRegEx": "Waugh", "year": 2008}, {"title": "Regret minimization in games with incomplete information", "author": ["Zinkevich"], "venue": "Technical Report TR07-14,", "citeRegEx": "Zinkevich,? \\Q2007\\E", "shortCiteRegEx": "Zinkevich", "year": 2007}, {"title": "Regret minimization in games with incomplete information", "author": ["Zinkevich"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich,? \\Q2008\\E", "shortCiteRegEx": "Zinkevich", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "Theorem 4 (from Zinkevich et al. (2008)).", "startOffset": 16, "endOffset": 40}, {"referenceID": 3, "context": "Though other algorithms boast better asymptotic rates (Gilpin et al. 2007), CFR more rapidly converges to an acceptable solution in practice.", "startOffset": 54, "endOffset": 74}, {"referenceID": 14, "context": "The proof combines Theorem 2 together with the CFR convergence proof of Zinkevich et al. (2008). Now that we have presented RCFR, let us examine how it relates to modern abstraction techniques.", "startOffset": 72, "endOffset": 96}], "year": 2014, "abstractText": "We propose a novel online learning method for minimizing regret in large extensive-form games. The approach learns a function approximator online to estimate the regret for choosing a particular action. A noregret algorithm uses these estimates in place of the true regrets to define a sequence of policies. We prove the approach sound by providing a bound relating the quality of the function approximation and regret of the algorithm. A corollary being that the method is guaranteed to converge to a Nash equilibrium in selfplay so long as the regrets are ultimately realizable by the function approximator. Our technique can be understood as a principled generalization of existing work on abstraction in large games; in our work, both the abstraction as well as the equilibrium are learned during self-play. We demonstrate empirically the method achieves higher quality strategies than state-of-the-art abstraction techniques given the same resources.", "creator": "gnuplot 5.0 patchlevel rc2"}}}