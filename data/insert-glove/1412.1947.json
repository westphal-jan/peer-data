{"id": "1412.1947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2014", "title": "A parallel sampling based clustering", "abstract": "vanderlyn The p\u00f5ltsamaa problem of automatically campinense clustering tape data porifera is oust an age venkayya old tujuh problem. People rymsha have created interview numerous algorithms to lehtinen tackle bikindi this pounamu problem. 60.5 The chapeltown execution snoble time 109.89 of najihah any of hentsch this jihads algorithm exhall grows with the number of gazetteers input shavar points and the mispronouncing number of cluster cd40 centers required. storaas To kimanzi reduce the jagmohan number kasparek of input points co-cathedral we regicides could average the kanta points locally and use the means reintroductions or c\u0113sis the local arado centers atlus as the mccuen input for clustering. However prokofiev since gluckin the required number of local centers breakneck is golfball very high, usjf running the clustering schriefer algorithm taxonomies on the entire eintracht dataset to meshoe obtain glucoside these khassan representational quick-thinking points is very time consuming. mutinous To sprowston remedy this pardo problem, in this paper nli we tibesti are semipermeable proposing two b\u00e8gles subclustering schemes where by kopecks we badung subdivide kota the dataset into littlefeather smaller d'urso sets and run alasdair the clustering hellenised algorithm ed.d. on naturalis the smaller abreu datasets stiller to obtain the required amoussouga number doot of tagi datapoints to run ingoldsby our hargesheimer clustering algorithm teres with. araks As rachlin we hiraoka are subdividing the given adulation dataset, we could tamisuke run clustering algorithm on agression each smaller piece of metzingen the documentum dataset treybig in parallel. underestimated We gromov found dundar that queirolo both a\u00eft parallel rundall and boolell serial glp execution stenciling of this compuware method to abdennabi be argall much carian faster than the \u0142\u0105ki original magennis clustering algorithm and error in 106.66 running the clustering norridge algorithm on badler a reduced 117-year set to napocor be very image-based less.", "histories": [["v1", "Fri, 5 Dec 2014 10:50:31 GMT  (68kb,D)", "http://arxiv.org/abs/1412.1947v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aditya av sastry", "kalyan netti"], "accepted": false, "id": "1412.1947"}, "pdf": {"name": "1412.1947.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Kmeans, clustering algorithm, CUDA, parallel computing.\nI. INTRODUCTION\nTHe amount of data that we are generating each day isway too huge to be manually categorized by any team, no matter how large it maybe. An automated system which can classify all kinds of data, which scales well to the size of the input is the need of the hour, if we wish to completely leverage the information available to us.\nVarious clustering algorithms were examined in Xu et al[1] The monothetic clustering algorithm or MONA proposed by Chavent et al[2] which is a divisive clustering algorithm. The possibilistic fuzzy C-Means clustering by Pal et al[3] uses the fuzzy approach to compute membership of each data point. However in this paper we will be using the traditional approach to clustering, the Lloyds algorithm for deriving the local centers in the sub clusters cause of its relative fastness.\nSeveral papers have explored the idea of subclustering in the past. Karypis et al[4] considers each datapoint as a subcluster initially and merges two subclusters if the proximity between two subclusters is higher than the internal interconnectivity of the points in a subcluster. Savaresi et al[5] examimes the two famous divisive bisecting K means algorithms. Essentially both of them take a larger cluster and divide it into two smaller clusters and continue to do the same on the smaller clusters till the required number of clusters is acquired. These two algorithms provide highly accurate and pure sub clusters but either they involve computing a expensive inter cluster proximity or computing a expensive\neigenvalue decomposition.\nIn this paper we propose two methods of sub dividing the dataspace into multiple regions. We run clustering algorithm in each of these regions to obtain local cluster centers on which we run clustering algorithm again to obtain the required number of centers. By running clustering algorithm on each region individually we are effectively negating the effect of any outliers present in the region. We found that by using sufficiently large datasets and retrieving sufficiently a lot of cluster centers from a region, the error in clustering is very less and the performance gain is very high.\nIn section II of the paper we cover equal sized subclustering algorithm and in section III we cover unequal sized subclustering algorithm. In section IV we cover the parallel implementation of this algorithm and in section V we show experimental results."}, {"heading": "II. EQUAL SIZED SUBCLUSTERING", "text": "Checking pair wise for similarity between points while sub grouping points is a very time consuming option. Instead we select few points that we will refer to as landmark points and based on each points similarity to a particular landmark we will gather the points into a subgroup. The similarity measure could be a distance measure like Euclidean distance, Manhattan distance or anything.\nA recursive algorithm for doing this would involve choosing a point farthest from all points as the landmark point and gathering all the points similar to this into one subgroup and redoing this process from the left out points. The iterative version of the algorithm is given below.\nAlgorithm 1 Equal Sized Subclustering 1: procedure SUBCLUSTER 2: Perform feature scaling on all the attributes. 3: Make a new point L with each attribute having the\nlowest value among all the points for that attribute. 4: Gather N points closest to L. Where N is the number of points in a subcluster. 5: Perform clustering on the N points to obtain required number of points. 6: Remove the N points from the dataset.\nEnd\nar X\niv :1\n41 2.\n19 47\nv1 [\ncs .L\nG ]\n5 D\nec 2\n01 4\n2"}, {"heading": "III. UNEQUAL SIZED SUBCLUSTERING", "text": "The problem with the above method is with case where the dataset has way too many outliers. This leads to some of the subclusters being filled only by the outlier points. To prevent that we ideally want the landmarks to be located in the dense regions of the dataset. To ensure that we take a point whose attributes are the lowest values for that attribute throughout our dataset and then we take a point whose attributes are the highest values for that attribute throughout our dataset. Then we connect these two points with a line, subdivide the line into required number of landmarks. Based on the similarity of each datum to these landmarks, they will be subclustered.\nAlgorithm 2 Unequal sized subclustering 1: procedure SUBCLUSTER 2: Perform feature scaling on all the attributes. 3: Make a new point L with each attribute having\nthe lowest value among all the points for that attribute.\n4: Make a new point H with each attribute having the highest value among all the points for that attribute 5: Divide the line segment between H and L into required number of points which are called Landmark points. 6: Group points according to the landmark point they are closest to and run clustering algorithms on"}, {"heading": "IV. CUDA", "text": "CUDA stands for Compute Unified Device Architecture. CUDA is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).\nOwing to the rapid performance growth in Graphic Cards, their ability to perform the same operations concurrently on multiple pieces of data extremely fast and the recent improvements in their programmability GPUs became very lucrative devices for wide range of application domains which involve a lot of data. A lot of research has been presented in recent times to program GPUs for the purposes of general computing.\nThere are two parts to a CUDA application. The device part which runs on the GPU is called a kernel. Kernels are implemented in CUDA programming language which is C extended with a bunch of keywords. A kernel is run by multiple threads. A collection of threads is known as a block. Threads within a block can communicate with each other using shared memory. But threads not belonging to the same block cannot communicate with each other. A kernel is called from the Host part of the application by specifying the number of blocks and number of threads in the blocks required. The host part of the application runs on the CPU is responsible for passing the required data to the CUDA kernel and also starting the CUDA kernels."}, {"heading": "V. PARALLEL IMPLEMENTATION", "text": "As we have mentioned in the previous section, each CUDA application has two parts: the host part and the device part. In the host part we subdivide the given dataset into required number of subclusters using either of our proposed algorithms. The device part of the code is the clustering algorithm with each thread running on one of the subclusters of the original dataset. Once each thread finishes executing and returns the sampled points, we return back to the host part of the application where we run the clustering algorithm on the sampled points to derive the required number of cluster centers.\nThe input to our application is a MxN 2 dimensional array. Where M represents the number of data points and N represents the number of attributes. First we run this 2 dimensional array through either of the sub clustering algorithm to obtain multiple 2 Dimensional arrays representing points in the sub region. Before calling the device code we need to transfer these multiple 2 Dimensional arrays to Device for the kernel to execute on. Converting these multiple 2 Dimensional arrays to 1 Dimensional arrays could take a lot of time. So instead we are generating a 1 dimensional array while subgrouping the points itself. This could be done in the following two ways.\n1) Row major flattening: we take a given datum and place all of its attributes in consecutive memory locations.\n2) Column major flattening: we take all values of all datums for a particular attribute place it into consecutive memory locations and then move on to the next attribute.\nAfter receiving the data from the host the GPU needs to convert the 1 dimensional array back to a 2 dimensional\n3 array. This is again done in two ways:\n1) Row major reconstruction: we take N consecutive values present starting from a given location and turn that into a datum. Where N is the number of attributes.\n2) Column major reconstruction: We read a first value from a given location and then skip the next N memory locations where N is the number of records. We continue to do this till we obtain M values where M is the number of attributes and construct the required datum.\nWe run each subcluster on a block of threads. After all the blocks have run we obtained the data from each block using reconstruction techniques mentioned above and run clustering algorithm on them to obtain the required number of cluster centers."}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "We ran this algorithm on Iris[7] and Seeds[8] dataset for accuracy comparision.\nIris Seeds Standard Kmeans 133 187 Equal Partitioning Best performance 138(6 Subclusters 6 times compression) 191(6 Subclusters 6 times compression)\nUnequal Partitioning 138(6 subcluster6 times compression) 191(6 Subclusters 6 times compression)\nThe Iris dataset has 150 datapoints with 3 classes and 4 attributes. The seeds dataset has 210 datapoints with 3 classes and 7 attributes.\nWe ran these algorithms on a NVIDIA TESLA C2075 GPU. On a system with Intel Xeon E5-1410 cpu running at 3.2 GHZ and 48GB of RAM. Our test dataset was a 2 dimensional synthetic dataset consisting of 100k, 250k, 500k elements. Each of these synthetic dataset contained 500 points per cluster. We used compression values of 5, 10, 15 the following table shows the execution time comparison\nDataset Size Traditional Kmeans Parallel Kmeans 100,000 2.328 2.78 250,000 25.6 4.96 500,000 156.8 6.2\nRoughly 30 times speedup has been observed for 500k elements with compression value 5 and about 55 times speedup for compression value 20\nCompression Value Execution Time 5 6.2 10 5.76 15 4.83 20"}], "references": [{"title": "A monothetic clustering method.", "author": ["Chavent", "Marie"], "venue": "Pattern Recognition Letters", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "A possibilistic fuzzy c-means clustering algorithm.", "author": ["Pal", "Nikhil R"], "venue": "Fuzzy Systems, IEEE Transactions on 13.4", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Chameleon: Hierarchical clustering using dynamic modeling.", "author": ["Karypis", "George", "Eui-Hong Han", "Vipin Kumar"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "On the performance of bisecting K-means and PDDP.", "author": ["Savaresi", "Sergio M", "Daniel L. Boley"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "K-means on commodity gpus with cuda.", "author": ["Hong-Tao", "Bai"], "venue": "Computer Science and Information Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Hierarchical Clustering with CUDA/GPU.", "author": ["Chang", "Dar-Jen", "Mehmed M. Kantardzic", "Ming Ouyang"], "venue": "ISCA PDCCS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images", "author": ["M. Charytanowicz", "J. Niewczas", "P. Kulczycki", "P.A. Kowalski", "S. Lukasik", "S. Zak"], "venue": "in: Information Technologies in Biomedicine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Various clustering algorithms were examined in Xu et al[1] The monothetic clustering algorithm or MONA proposed by Chavent et al[2] which is a divisive clustering algorithm.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Various clustering algorithms were examined in Xu et al[1] The monothetic clustering algorithm or MONA proposed by Chavent et al[2] which is a divisive clustering algorithm.", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "The possibilistic fuzzy C-Means clustering by Pal et al[3] uses the fuzzy approach to compute membership of each data point.", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Karypis et al[4] considers each datapoint as a subcluster initially and merges two subclusters if the proximity between two subclusters is higher than the internal interconnectivity of the points in a subcluster.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Savaresi et al[5] examimes the two famous divisive bisecting K means algorithms.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "We ran this algorithm on Iris[7] and Seeds[8] dataset for accuracy comparision.", "startOffset": 42, "endOffset": 45}], "year": 2014, "abstractText": "The problem of automatically clustering data is an age old problem. People have created numerous algorithms to tackle this problem. The execution time of any of this algorithm grows with the number of input points and the number of cluster centers required. To reduce the number of input points we could average the points locally and use the means or the local centers as the input for clustering. However since the required number of local centers is very high, running the clustering algorithm on the entire dataset to obtain these representational points is very time consuming. To remedy this problem, in this paper we are proposing two subclustering schemes where by we subdivide the dataset into smaller sets and run the clustering algorithm on the smaller datasets to obtain the required number of datapoints to run our clustering algorithm with. As we are subdividing the given dataset, we could run clustering algorithm on each smaller piece of the dataset in parallel. We found that both parallel and serial execution of this method to be much faster than the original clustering algorithm and error in running the clustering algorithm on a reduced set to be very less.", "creator": "LaTeX with hyperref package"}}}