{"id": "1412.2106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2014", "title": "Consistent optimization of AMS by logistic loss minimization", "abstract": "pashtoon In kraft this zhuji paper, we theoretically gazans justify an approach popular cervantino among transpire participants greenhouses of jishan the gudmundsdottir Higgs woodsman Boson Machine Learning .674 Challenge to mccarthy optimize approximate 441 median katsuyuki significance (fbf AMS ). The gottscheerish approach is bluechip based on mastrantonio the following two - garbed stage procedure. First, lembalemba a 5th/6th real - valued function pengo is documented learned by minimizing firsby a surrogate loss sommerlath for pokrzywnica binary classification, such as grep logistic h\u00e4xan loss, on darryll the batuque training terpenes sample. Then, shanan a wide-screen threshold libregts is yai tuned woolmark on a assael separate velez validation sample, groused by famu direct optimization blasphemies of AMS. oteiza We show wongan that the hinchley regret of 252.5 the resulting (apalis thresholded) rijswijk classifier duerson measured leaning with respect to bifida the totora squared elissa AMS, is atwitter upperbounded divergent by bughouse the regret 181,000 of the pseudo underlying mixco real - syros valued function measured with respect steeb to fayer the nemescu logistic chinga loss. Hence, we tamassos prove renaudin that 576 minimizing logistic surrogate stinkin is 27.99 a manchesters consistent ommatidia method dwelling-place of optimizing AMS.", "histories": [["v1", "Fri, 5 Dec 2014 19:28:15 GMT  (13kb)", "http://arxiv.org/abs/1412.2106v1", "9 pages, HEPML workshop at NIPS 2014"]], "COMMENTS": "9 pages, HEPML workshop at NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wojciech kot{\\l}owski"], "accepted": false, "id": "1412.2106"}, "pdf": {"name": "1412.2106.pdf", "metadata": {"source": "CRF", "title": "Consistent optimization of AMS by logistic loss minimization", "authors": ["Wojciech Kot lowski", "Wojciech Kot"], "emails": ["wkotlowski@cs.put.poznan.pl"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n21 06\nv1 [\ncs .L\nG ]\n5 D\nec 2\n01 4\nfrom thresholding f on \u03b8\u0302) measured with respect to the squared AMS, is upperbounded by the regret of f measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS.\nKeywords: Approximate median significance (AMS), Higgs Boson Machine Learning Challenge, Kaggle, logistic loss, regret bound, statistical consistency."}, {"heading": "1. Introduction", "text": "This paper concerns a problem of learning a classifier to optimize approximate median significance (AMS), which was the goal of the Higgs Boson Machine Learning Challenge (HiggsML), hosted by Kaggle website (see Adam-Bourdarios et al. (2014) for details on this contest and description of the problem).\nIn particular, we are interested in an approach to optimize AMS, based on the following two-stage procedure. First, a real-valued function f is learned by minimizing a surrogate loss for binary classification, such as logistic loss function, on the training sample. In the second stage, given f , a threshold is tuned on a separate \u201cvalidation\u201d sample, by direct optimization of AMS with respect to a classifier obtained from f by classifying all observations with value of f above the threshold as positive class (signal event), and all observations below the threshold as negative class (background event).\nThis approach became very popular among HiggsML challenge participants, mainly due to the fact that its first stage, learning a classifier, does not exploit the task evaluation metric (AMS) in any way and thus can employ without modifications any standard classification tools such as logistic regression, LogitBoost, Stochastic Gradient Boosting, Random Forest, etc. (see, e.g., Hastie et al. (2009)). Despite its simplicity, this approach proved to be very effective in achieving high leaderboard score in HiggsML. 1\n1. See the HiggsML forum https://www.kaggle.com/c/higgs-boson/forums for discussions and presentation of the top score solutions.\nc\u00a92014 Wojciech Kot lowski.\nThe intuition behind this approach is clear: minimization of logistic loss results in estimation of conditional probabilities of signal and background event, and the AMS is assumed to be maximized by classifying the events most likely to be signal as signal events.\nThis paper formalizes this intuition by showing that the approach described above constitutes a consistent method of optimizing AMS. More specifically, we use the notion of regret with respect to some evaluation metric, which is a difference between the performance of a given classifier and the performance of the optimal classifier with respect to this metric. Given a function f , and a classifier h\nf,\u03b8\u0302 obtained from f by thresholding f at \u03b8\u0302,\nwe give a bound on the regret of h f,\u03b8\u0302 measured with respect to the squared AMS by the regret of f measured with respect to the logistic loss, given that the threshold \u03b8\u0302 is tuned by optimization of AMS among all classifiers of the form hf,\u03b8 for any threshold value \u03b8.\nTo our knowledge, this is the first regret bound of this form applicable to a nondecomposable performance measure such as AMS. We also discuss generalization of our approach to different performance measures and surrogate loss functions.\nRelated work. The issue of consistent optimization of performance measures which are functions of true positive and true negative rates has received increasing attention recently in machine learning community (Narasimhan et al., 2014; Natarajan et al., 2014; Zhao et al., 2013). However, these works are mainly concerned with statistical consistency also known as calibration, which determines whether convergence to the minimizer of a surrogate loss implies convergence to the minimizer of the task performance measure as sample size goes to infinity. Here we give a much stronger result which bounds the regret with respect to squared AMS by the regret with respect to logistic loss. Our result is valid for all finite sample sizes and informs about the rates of convergence.\nRecently, Mackey and Bryan (2014) proposed a classification cascade approach to optimize AMS. Their method, based on the theory of Fenchel\u2019s duality, iteratively alternates between solving a cost-sensitive binary classification problem and updating misclassification costs. In contrast, the method described here requires solving an ordinary binary classification problem just once.\nOutline. The paper is organized as follows. In Section 2, we introduce basic concepts needed to state our main result presented in Section 3 and proved in Section 4. Section 5 discusses generalization of our results beyond AMS and logistic loss."}, {"heading": "2. Problem Setting", "text": "Binary classifier. In binary classification, the goal is, given an input (feature vector) x \u2208 X, to accurately predict the output (label) y \u2208 {\u22121, 1}. We assume input-output pairs (x, y), which we call observations, are generated i.i.d. according to Pr(x, y).2 A classifier is a mapping h : X \u2192 {\u22121, 1}. Given h, we define the following two quantities:\ns(h) = Pr(h(x) = 1, y = 1), b(h) = Pr(h(x) = 1, y = \u22121),\nwhich can be interpreted as true positive and false positive rates of h.\n2. The original HiggsML problem also involved observations\u2019 weights, but without loss of generality, they can be incorporated into the distribution Pr(x, y).\nAMS and regret. Given a classifier h, define its approximate median significance (AMS) score (Cowan et al., 2011) as AMS(h) = AMS(s(h), b(h)), where:3\nAMS(s, b) =\n\u221a\n2 ( (s+ b) log ( 1 + s\nb\n) \u2212 s ) .\nIt is easier to deal with a squared AMS, AMS2(h), and this quantity is used throughout the paper. It is easy to verify that AMS2(s, b) is increasing in s and decreasing in b. Moreover, AMS2(s, b) is jointly convex with respect to (s, b).\nLet h\u2217AMS be the classifier which maximizes the AMS 2 over all possible classifiers:\nh\u2217AMS = argmax h\u2208{\u22121,1}X AMS2(h).\nGiven h, we define its AMS regret as the distance of h from the optimal classifier h\u2217AMS measured by means of AMS2:\nRAMS(h) = AMS 2(h\u2217AMS)\u2212AMS2(h).\nLogistic loss and logistic regret. Given a real number f , and a label y, we define the logistic loss \u2113log : {\u22121, 1} \u00d7 R \u2192 R+ as:\n\u2113log(y, f) = log ( 1 + e\u2212yf ) .\nThe logistic loss is a commonly used surrogate loss function for binary classification, employed in various learning methods, such as logistic regression, LogitBoost or Stochastic Gradient Boosting (see, e.g., Hastie et al. (2009)). It is convex in f , so minimizing logistic loss over the training sample becomes a convex optimization problem, which can be solved efficiently. Another advantage of logistic loss is that the sigmoid transform of f , (1+e\u2212f )\u22121, can be used to obtain probability estimates Pr(y|x).\nGiven a real-valued function f : X \u2192 R, its expected logistic loss Llog(f) is defined as:\nLlog(f) = E(x,y)[\u2113log(y, f(x))].\nLet f\u2217log = argminf Llog(f) be the minimizer of Llog(f) among all functions f : X \u2192 R. We define the logistic regret of f as:\nRlog(f) = Llog(f)\u2212 Llog(f\u2217log)."}, {"heading": "3. Main Result", "text": "Any real-valued function f : X \u2192 R can be turned into a classifier hf,\u03b8 : X \u2192 {\u22121, 1}, by thresholding at some value \u03b8:\nhf,\u03b8(x) = sgn(f(x)\u2212 \u03b8),\n3. Comparing to the definition in (Adam-Bourdarios et al., 2014), we skip the regularization term breg. This comes without loss of generality, as breg can be incorporated into b and, since it affects all classifiers equally, will vanish in the definition of regret.\nwhere sgn(x) is the sign function, and we use the convention that sgn(0) = 1.\nThe purpose of this paper is to address the following problem: given a function f with logistic regret Rlog(f), and a threshold \u03b8, what is the maximum AMS regret of hf,\u03b8? In other words, can we bound RAMS(hf,\u03b8) in terms of Rlog(f)? We give a positive answer to this question, which based on the following regret bound:\nLemma 1 There exists a threshold \u03b8\u2217, such that for any f ,\nRAMS(hf,\u03b8\u2217) \u2264 s(h\u2217AMS)\nb(h\u2217AMS)\n\u221a\n1 2 Rlog(f).\nThe proof is quite long and hence is postponed to Section 4. Interestingly, the proof goes by an intermediate bound of the AMS regret by a cost-sensitive classification regret, with misclassification costs proportional to the gradient coordinates of the AMS.\nLemma 1 has the following interpretation. If we are able to find a function f with small logistic regret, we are guaranteed that there exists a threshold \u03b8\u2217 such that hf,\u03b8\u2217 has small AMS regret. Note that the same threshold \u03b8\u2217 will work for any f , and the right hand side of the bound is independent of \u03b8\u2217. We are now ready to prove the main result of the paper:\nTheorem 2 Given a real-valued function f , let \u03b8\u0302 = argmax\u03b8 AMS(hf,\u03b8). Then:\nRAMS(hf,\u03b8\u0302) \u2264 s(h\u2217AMS)\nb(h\u2217AMS)\n\u221a\n1 2 Rlog(f).\nProof The result follows immediately from Lemma 1 by noticing that solving max\u03b8 AMS(hf,\u03b8) is equivalent to solving min\u03b8 RAMS(hf,\u03b8), and that min\u03b8 RAMS(hf,\u03b8) \u2264 RAMS(hf,\u03b8\u2217). Theorem 2 motivates the following procedure for AMS maximization:\n1. Find f with small logistic regret, e.g. by employing a learning algorithm minimizing logistic loss on the training sample.\n2. Given f , solve \u03b8\u0302 = argmax\u03b8 AMS(hf,\u03b8).\nTheorem 2 states that the AMS regret of the classifier obtained by this procedure is upperbounded by the logistic regret of the underlying real-valued function.\nWe now discuss how to approach step 2 of the procedure in practice. In principle, this step requires maximizing AMS defined by means of an unknown distribution Pr(x, y). However, it is sufficient to optimize \u03b8 on the empirical counterpart of AMS calculated on a separate validation sample. Due to space limit, we only give a sketch of the proof of this fact: Step 2 involves optimization within a class of threshold functions (since f is fixed), which has VC-dimension equal to 2 (Devroye et al., 1996). By convexity of AMS2,\nAMS2(s, b)\u2212AMS2(s\u0302, b\u0302) \u2264 ( \u2202AMS2(s, b) \u2202s , \u2202AMS2(s, b) \u2202b\n)\u22a4\n(s\u2212 s\u0302, b\u2212 b\u0302) (1)\n(see, e.g. Boyd and Vandenberghe (2004)), where s\u0302 and b\u0302 are empirical counterparts of s and b. By VC theory, the deviations of s\u0302 from s, and b\u0302 from b can be upperbounded with high probability uniformly over the class of all threshold functions by O(1/ \u221a m), where m is the validation sample size. This and (1) implies, that AMS2(s, b) of the empirical maximizer is O(1/ \u221a m) close to the max\u03b8 AMS\n2(hf,\u03b8). Hence, step 2 can be performed within O(1/ \u221a m) accuracy on a validation sample independent from the training sample."}, {"heading": "4. Proof of Lemma 1", "text": "The proof consists of two steps. First, we bound the AMS regret of any classifier h by its cost-sensitive classification regret (introduced below). Next, we show that there exists a threshold \u03b8\u2217, such that for any f , the cost-sensitive classification regret of hf,\u03b8\u2217 is upperbounded by the logistic regret of f .\nBounding AMS regret by cost-sensitive classification regret. Given a real number c \u2208 (0, 1), define a cost-sensitive classification loss \u2113c : {\u22121, 1} \u00d7 {\u22121, 1} \u2192 R+ as:\n\u2113c(y, h) = c1[y = \u22121]1[h = 1] + (1\u2212 c)1[y = 1]1[h = \u22121],\nwhere 1[A] is the indicator function equal to 1 if predicate A is true, and 0 otherwise. The cost-sensitive loss assigns different costs of misclassification for positive and negative labels. Given classifier h, the expected cost-sensitive loss of h is:\nLc(h) = E(x,y)[\u2113c(y, h(x))] = cb(h) + (1\u2212 c)(Pr(y = 1)\u2212 s(h)),\nwhere s(h) and b(h) are true positive and false positive rates defined before. Let h\u2217c = argminh Lc(h) be the minimizer of the expected cost-sensitive loss among all classifiers. Define the cost-sensitive classification regret as:\nRc(h) = Lc(h)\u2212 Lc(h\u2217c).\nAny convex and differentiable function g(x) satisfies g(x) \u2265 g(y) +\u2207g(y)\u22a4(x \u2212 y) for any x, y in its convex domain (Boyd and Vandenberghe, 2004). Applying this inequality to AMS2(s, b) jointly convex in (s, b), we have for any s, b, s\u2217, b\u2217 \u2208 [0, 1]:\nAMS2(s, b) \u2265 AMS2(s\u2217, b\u2217) + ( \u2202AMS2(s\u2217, b\u2217) \u2202s\u2217 , \u2202AMS2(s\u2217, b\u2217) \u2202b\u2217\n)\u22a4\n(s\u2212 s\u2217, b\u2212 b\u2217). (2)\nGiven classifier h, we set s = s(h), b = b(h), s\u2217 = s(h\u2217AMS), b \u2217 = b(h\u2217AMS), and:\nC := \u2202AMS2(s\u2217, b\u2217) \u2202s\u2217 \u2212 \u2202AMS 2(s\u2217, b\u2217) \u2202b\u2217 , c := \u2212 1 C \u2202AMS2(s\u2217, b\u2217) \u2202b\u2217 .\nSince AMS2(s, b) is increasing in s and decreasing in b, both \u2202AMS 2(s\u2217,b\u2217) \u2202s\u2217 and \u2212\u2202AMS 2(s\u2217,b\u2217) \u2202b\u2217 are positive, which implies C > 0 and 0 < c < 1. In this notation, (2) boils down to:\nRAMS(h) = AMS 2(h\u2217AMS)\u2212AMS2(h) \u2264 C\n( c(b(h) \u2212 b(h\u2217AMS)) + (1\u2212 c)(s(h\u2217AMS)\u2212 s(h)) )\n= C ( Lc(h)\u2212 Lc(h\u2217AMS) ) \u2264 C (\nLc(h)\u2212 Lc(h\u2217c) ) = CRc(h),\nwhere the last inequality follows from the definition of h\u2217c . Thus, the AMS regret is upperbounded by the cost-sensitive classification regret with costs proportional to the gradient coordinates of AMS2(s\u2217, b\u2217) at optimum h\u2217AMS. 4\nBounding cost-sensitive classification regret by logistic regret. We first give a bound on cost-sensitive classification regret by means of logistic regret conditioned at a given x. This part relies on the techniques used by Bartlett et al. (2006). Then, the final bound is obtained by taking expectation with respect to x, and applying Jensen\u2019s inequality.\nGiven a label h \u2208 {\u22121, 1}, and \u03b7 \u2208 [0, 1], define conditional cost-sensitive classification loss as: \u2113c(\u03b7, h) = c(1 \u2212 \u03b7)1[h = 1] + (1\u2212 c)\u03b71[h = \u22121]. The reason this quantity is called \u201cconditional loss\u201d becomes clear if we note that for any classifier h, Lc(h) = Ex[\u2113c(\u03b7(x), h(x))], where \u03b7(x) = Pr(y = 1|x). In other words, \u2113c(\u03b7(x), h(x)) is the loss of h conditioned on x.\nGiven \u03b7, let h\u2217c = argminh\u2208{\u22121,1} \u2113c(\u03b7, h). It can be easily verified that:\nh\u2217c = sgn (\u03b7 \u2212 c) ,\nand \u2113c(\u03b7, h \u2217 c) = min{c(1\u2212 \u03b7), (1 \u2212 c)\u03b7}. The conditional regret of h is defined as rc(\u03b7, h) = \u2113c(\u03b7, h) \u2212 \u2113c(h\u2217c). Note that:\nrc(\u03b7, h) =\n{\n0 if h = h\u2217c , |\u03b7 \u2212 c| if h 6= h\u2217c .\nGiven a real number f , and \u03b7 \u2208 [0, 1], define conditional logistic loss as:\n\u2113log(\u03b7, f) = (1\u2212 \u03b7) log ( 1 + ef ) + \u03b7 log ( 1 + e\u2212f ) .\nLet f\u2217log = argminf\u2208R \u2113log(\u03b7, f). By differentiating \u2113log(\u03b7, f) with respect to f , and setting the derivative to 0, we get that:\nf\u2217log = log \u03b7\n1\u2212 \u03b7 ,\nand \u2113log(\u03b7, f \u2217 log) = \u2212\u03b7 log \u03b7 \u2212 (1 \u2212 \u03b7) log(1 \u2212 \u03b7), the binary entropy of \u03b7. The conditional logistic regret of f is given by rlog(\u03b7, f) = \u2113log(\u03b7, f)\u2212 \u2113log(f\u2217log). The conditional regret has a particularly simple form when f is re-expressed as a probability estimate \u03b7f :\nrlog(\u03b7, f) = D(\u03b7\u2016\u03b7f ), where \u03b7f := 1\n1 + e\u2212f ,\nand D(\u03b7\u2016\u03b7f ) = \u03b7 log \u03b7\u03b7f + (1 \u2212 \u03b7) log 1\u2212\u03b7 1\u2212\u03b7f is the Kullback-Leibler divergence. By Pinsker\u2019s inequality, D(\u03b7\u2016\u03b7f ) \u2265 2(\u03b7 \u2212 \u03b7f )2.\nGiven real number f , define hf,\u03b8\u2217 = sgn(f \u2212 \u03b8\u2217), where:\n\u03b8\u2217 = log c\n1\u2212 c .\n4. Note that the gradient at optimum does not vanish, as the optimum is with respect to h, not (s, b).\nWe will now bound the conditional cost-sensitive classification regret rc(\u03b7, hf,\u03b8\u2217) in terms of conditional logistic regret rlog(\u03b7, f). First note that:\nhf,\u03b8\u2217 = 1 \u21d0\u21d2 f \u2265 \u03b8\u2217 = log c 1\u2212 c \u21d0\u21d2 1 1 + e\u2212f \u2265 c \u21d0\u21d2 \u03b7f \u2265 c,\nso that we can equivalently write hf,\u03b8\u2217 = sgn(\u03b7f \u2212 c). Since h\u2217c = sgn(\u03b7\u2212 c), then whenever (\u03b7f \u2212 c)(\u03b7 \u2212 c) > 0, it holds hf,\u03b8\u2217 = h\u2217c , and rc(\u03b7, hf,\u03b8\u2217) = 0. On the other hand, when (\u03b7f \u2212 c)(\u03b7 \u2212 c) \u2264 0, it holds5 rc(\u03b7, hf,\u03b8\u2217) \u2264 |\u03b7 \u2212 c|, whereas:\nrlog(\u03b7, f) = D(\u03b7\u2016\u03b7f ) Pinsker\u2032s \u2265 2(\u03b7 \u2212 \u03b7f )2 = 2(\u03b7 \u2212 c+ c\u2212 \u03b7f )2\n= 2(\u03b7 \u2212 c)2 + 4(\u03b7 \u2212 c)(c\u2212 \u03b7f ) + 2(c \u2212 \u03b7f )2 \u2265 2(\u03b7 \u2212 c)2 \u2265 2r2c (\u03b7, hf,\u03b8\u2217),\nwhere the last but one inequality is implied by (\u03b7f \u2212 c)(\u03b7 \u2212 c) \u2264 0. Taking both cases together, we get:\nrc(\u03b7, hf,\u03b8\u2217) \u2264 \u221a rlog(\u03b7, f)/2.\nNow, given any function f ,\nRc(hf,\u03b8\u2217) = Ex[rc(\u03b7, hf,\u03b8\u2217)] \u2264 Ex [ \u221a rlog(\u03b7, f)/2 ] \u2264 \u221a Ex[rlog(\u03b7, f)]/2 = \u221a Rlog(f)/2,\nwhere the last inequality is from Jensen\u2019s inequality applied to the concave function x 7\u2192 \u221ax.\nFinishing the proof. Combining the results from both parts, we get:\nRAMS(hf,\u03b8\u2217) \u2264 CRc(hf,\u03b8\u2217) \u2264 C \u221a Rlog(f)/2,\nwhere \u03b8\u2217 = log c1\u2212c is independent of f . Recalling that C = \u2202AMS2(s\u2217,b\u2217) \u2202s\u2217 \u2212 \u2202AMS 2(s\u2217,b\u2217) \u2202b\u2217\n, we calculate:\nC = log\n(\n1 + s\u2217\nb\u2217\n) \u2212 ( log ( 1 + s\u2217\nb\u2217\n)\n\u2212 s \u2217\nb\u2217\n)\n= s\u2217\nb\u2217 ,\nwhere s\u2217 = s(h\u2217AMS) and b \u2217 = b(h\u2217AMS). This finished the proof.\nNote that the proof actually specifies the exact value of the universal threshold \u03b8\u2217:\n\u03b8\u2217 = log c 1\u2212 c , where c = 1\u2212 b\u2217 s\u2217 log\n(\n1 + s\u2217\nb\u2217\n)\n."}, {"heading": "5. Generalization beyond AMS and logistic loss", "text": "Results of this paper can be generalized beyond AMS metric and logistic loss surrogate. The AMS can be replaced by any other evaluation metric, which enjoys the following two properties: 1) is increasing in s, and decreasing in b; 2) is jointly convex in s and b. These were the only two properties of the AMS used in the proof of Lemma 1. The logistic loss\n5. rc(\u03b7, hf,\u03b8\u2217) = |\u03b7 \u2212 c| if (\u03b7f \u2212 c)(\u03b7 \u2212 c) < 0, and can be either 0 or |\u03b7 \u2212 c| when (\u03b7f \u2212 c)(\u03b7 \u2212 c) = 0.\nsurrogate can be replaced by any other convex surrogate loss \u2113, such that the following property holds: There exists a threshold \u03b8\u2217 which is a function of the cost c, such that for all f ,\nRc(hf,\u03b8\u2217) \u2264 \u03bb \u221a R\u2113(f),\nfor some positive constant \u03bb. This property is satisfied by, e.g., squared error loss \u2113sq(y, f) = (y \u2212 f)2 with \u03bb = 1, which can be verified by noticing that the logistic regret upperbounds the squared error regret by Pinsker\u2019s inequality. We conjecture that all strongly proper composite losses (Agarwal, 2014) hold this property."}, {"heading": "Acknowledgments", "text": "The author was supported by the Foundation For Polish Science Homing Plus grant, cofinanced by the European Regional Development Fund. The author would like to thank Krzysztof Dembczyn\u0301ski for interesting discussions and proofreading the paper."}], "references": [{"title": "Learning to discover: the Higgs boson machine learning", "author": ["Claire Adam-Bourdarios", "Glen Cowan", "C\u00e9cile Germain", "Isabelle Guyon", "Bal\u00e1zs K\u00e9gl", "David Rousseau"], "venue": null, "citeRegEx": "Adam.Bourdarios et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Adam.Bourdarios et al\\.", "year": 2014}, {"title": "Surrogate regret bounds for bipartite ranking via strongly proper losses", "author": ["Shivani Agarwal"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Agarwal.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal.", "year": 2014}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Asymptotic formulae for likelihood-based tests of new physics", "author": ["Glen Cowan", "Kyle Cranmer", "Eilam Gross", "Ofer Vitells"], "venue": "The European Physical Journal C-Particles and Fields,", "citeRegEx": "Cowan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cowan et al\\.", "year": 2011}, {"title": "Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Weighted classification cascades for optimizing discovery significance in the HiggsML challenge", "author": ["Lester Mackey", "Jordan Bryan"], "venue": "CoRR, abs/1409.2655,", "citeRegEx": "Mackey and Bryan.,? \\Q2014\\E", "shortCiteRegEx": "Mackey and Bryan.", "year": 2014}, {"title": "On the statistical consistency of plug-in classifiers for non-decomposable performance measures", "author": ["Harikrishna Narasimhan", "Rohit Vaish", "Shivani Agarwal"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Narasimhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2014}, {"title": "Consistent binary classification with generalized performance metrics", "author": ["Nagarajan Natarajan", "Oluwasanmi Koyejo", "Pradeep K. Ravikumar", "Inderjit S. Dhillon"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Natarajan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2014}, {"title": "Beyond Fano\u2019s inequality: Bounds on the optimal F-score, BER, and cost-sensitive risk and their implications", "author": ["Ming-Jie Zhao", "Narayanan Edakunni", "Adam Pocock", "Gavin Brown"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Introduction This paper concerns a problem of learning a classifier to optimize approximate median significance (AMS), which was the goal of the Higgs Boson Machine Learning Challenge (HiggsML), hosted by Kaggle website (see Adam-Bourdarios et al. (2014) for details on this contest and description of the problem).", "startOffset": 225, "endOffset": 255}, {"referenceID": 0, "context": "Introduction This paper concerns a problem of learning a classifier to optimize approximate median significance (AMS), which was the goal of the Higgs Boson Machine Learning Challenge (HiggsML), hosted by Kaggle website (see Adam-Bourdarios et al. (2014) for details on this contest and description of the problem). In particular, we are interested in an approach to optimize AMS, based on the following two-stage procedure. First, a real-valued function f is learned by minimizing a surrogate loss for binary classification, such as logistic loss function, on the training sample. In the second stage, given f , a threshold is tuned on a separate \u201cvalidation\u201d sample, by direct optimization of AMS with respect to a classifier obtained from f by classifying all observations with value of f above the threshold as positive class (signal event), and all observations below the threshold as negative class (background event). This approach became very popular among HiggsML challenge participants, mainly due to the fact that its first stage, learning a classifier, does not exploit the task evaluation metric (AMS) in any way and thus can employ without modifications any standard classification tools such as logistic regression, LogitBoost, Stochastic Gradient Boosting, Random Forest, etc. (see, e.g., Hastie et al. (2009)).", "startOffset": 225, "endOffset": 1326}, {"referenceID": 7, "context": "The issue of consistent optimization of performance measures which are functions of true positive and true negative rates has received increasing attention recently in machine learning community (Narasimhan et al., 2014; Natarajan et al., 2014; Zhao et al., 2013).", "startOffset": 195, "endOffset": 263}, {"referenceID": 8, "context": "The issue of consistent optimization of performance measures which are functions of true positive and true negative rates has received increasing attention recently in machine learning community (Narasimhan et al., 2014; Natarajan et al., 2014; Zhao et al., 2013).", "startOffset": 195, "endOffset": 263}, {"referenceID": 9, "context": "The issue of consistent optimization of performance measures which are functions of true positive and true negative rates has received increasing attention recently in machine learning community (Narasimhan et al., 2014; Natarajan et al., 2014; Zhao et al., 2013).", "startOffset": 195, "endOffset": 263}, {"referenceID": 6, "context": "Recently, Mackey and Bryan (2014) proposed a classification cascade approach to optimize AMS.", "startOffset": 10, "endOffset": 34}, {"referenceID": 4, "context": "Given a classifier h, define its approximate median significance (AMS) score (Cowan et al., 2011) as AMS(h) = AMS(s(h), b(h)), where:3", "startOffset": 77, "endOffset": 97}, {"referenceID": 5, "context": ", Hastie et al. (2009)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": "Comparing to the definition in (Adam-Bourdarios et al., 2014), we skip the regularization term breg.", "startOffset": 31, "endOffset": 61}, {"referenceID": 3, "context": "Boyd and Vandenberghe (2004)), where \u015d and b\u0302 are empirical counterparts of s and b.", "startOffset": 0, "endOffset": 29}, {"referenceID": 3, "context": "Any convex and differentiable function g(x) satisfies g(x) \u2265 g(y) +\u2207g(y)\u22a4(x \u2212 y) for any x, y in its convex domain (Boyd and Vandenberghe, 2004).", "startOffset": 115, "endOffset": 144}, {"referenceID": 2, "context": "This part relies on the techniques used by Bartlett et al. (2006). Then, the final bound is obtained by taking expectation with respect to x, and applying Jensen\u2019s inequality.", "startOffset": 43, "endOffset": 66}, {"referenceID": 1, "context": "We conjecture that all strongly proper composite losses (Agarwal, 2014) hold this property.", "startOffset": 56, "endOffset": 71}], "year": 2014, "abstractText": "In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function f is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, given f , a threshold \u03b8\u0302 is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting classifier (obtained from thresholding f on \u03b8\u0302) measured with respect to the squared AMS, is upperbounded by the regret of f measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS.", "creator": "LaTeX with hyperref package"}}}