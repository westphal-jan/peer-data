{"id": "1412.2485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2014", "title": "Accurate Streaming Support Vector Machines", "abstract": "realest A mfdc widely - varalakshmi used fenner tool oauth for leongatha binary classification itil is onalaska the mashek Support Vector Machine (bangabhaban SVM ), a bir\u017eai supervised shaho learning technique 79.90 that lefteris finds affi the \" blackburn maximum ashiq margin \" informationssysteme linear h\u00e4rn\u00f6sand separator yarram between the shangani two classes. konya While flipmode SVMs have been well elmquist studied b-66 in ginta the batch (offline) fell setting, there unbleached is crossway considerably ankeny less vairocana work 139.75 on the contrivances streaming (online) pereverzeva setting, butrint which requires kirchner only a single pass over the data fairland using sub - yeaman linear unger space. Existing baynham streaming asadho algorithms are not arvanitika yet competitive 11-cent with the batch implementation. 1668 In foundation this paper, we murray-darling use loner the th1 formulation of klosterneuburg the SVM solving as a drbombay minimum ann\u00e9e enclosing ball (MEB) 82.5 problem to provide investor-owned a cryosat streaming paschalidis SVM algorithm based off of the kienholz blurred souers ball pendennis cover 70000 originally proposed by Agarwal el1l and Sharathkumar. Our warna implementation ataxia consistently latinoamericana outperforms existing cognizable streaming allot SVM approaches and andjelic provides customs higher accuracies arohana than svetambara libSVM on several datasets, star-tribune thus renumberings making mailed it panayiotopoulos competitive pledges with 16.71 the standard left-right SVM boaventura batch wilmette implementation.", "histories": [["v1", "Mon, 8 Dec 2014 08:46:07 GMT  (177kb,D)", "http://arxiv.org/abs/1412.2485v1", "2 figures, 8 pages"]], "COMMENTS": "2 figures, 8 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vikram nathan", "sharath raghvendra"], "accepted": false, "id": "1412.2485"}, "pdf": {"name": "1412.2485.pdf", "metadata": {"source": "CRF", "title": "Accurate Streaming Support Vector Machines", "authors": ["Vikram Nathan", "Sharath Raghvendra"], "emails": [], "sections": [{"heading": null, "text": "Accurate Streaming Support Vector Machines\nVikram Nathan Sharath Raghvendra"}, {"heading": "1 Abstract", "text": "A widely-used tool for binary classification is the Support Vector Machine (SVM), a supervised learning technique that finds the \u201cmaximum margin\u201d linear separator between the two classes. While SVMs have been well studied in the batch (offline) setting, there is considerably less work on the streaming (online) setting, which requires only a single pass over the data using sub-linear space. Existing streaming algorithms are not yet competitive with the batch implementation. In this paper, we use the formulation of the SVM as a minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based off of the blurred ball cover originally proposed by Agarwal and Sharathkumar. Our implementation consistently outperforms existing streaming SVM approaches and provides higher accuracies than libSVM on several datasets, thus making it competitive with the standard SVM batch implementation."}, {"heading": "2 Introduction", "text": "Learning and classification with a large amount of data raises the need for algorithms that scale well in time and space usage with the number of data points being trained on. Streaming algorithms have properties that do just that: they run in a single pass over the data and use space polylogarithmic in the total number of points. The technique of making a single pass over the data has three key advantages: 1) points may be seen once and then discarded so they do not take up additional storage space; 2) the running time scales linearly in the size of the input, a practical necessity for data sets with sizes in the millions, and 3) it enables these algorithms to function in a streaming model, where instead of data is not immediately available, individual data points may arrive slowly over time. This third feature enables data to be learned and models to be updated \u201donline\u201d in real time, instead of periodically running a batch update over all existing data.\nSupport Vector Machines (SVMs) are one such learning model that would benefit from an efficient and accurate streaming representation. Standard 2-class SVMs models attempt to find the maximum-margin linear separator (i.e. hyperplane) between positive and negative instances and as such, they have a very small hypothesis complexity but provable generalization bounds [8]. There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3]. The connection between these two problems has made it possible to harness the work done on streaming MEBs and apply them to SVMs, as was done in [6]. In this paper, we utilize the Blurred Ball cover approximation to the MEB problem proposed in [5] to obtain a streaming SVM that is both fast and space efficient. We also show that our implementation not only outperforms existing streaming SVM implementations (including those not based off of MEB reductions) but also that our error rates are competitive with LibSVM, a state-of-the-art batch SVM open-source project available [here]. ar X\niv :1\n41 2.\n24 85\nv1 [\ncs .L\nG ]\n8 D\nec 2\n01 4"}, {"heading": "3 Background", "text": "The Core Vector Machine (CVM) was introduced by [3] as an new take on the standard Support Vector Machine (SVM). Instead of attempting to solve a quadratic system, the CVM makes use of the observation that many common kernels for the standard SVM can be viewed as Minimum Enclosing Ball (MEB) problems. Consider the following SVM optimization problem on m inputs (xi,yi):\nmin w,b,\u03c1,\u03bei\n\u2016w\u20162 +b2\u22122\u03c1 +C n\n\u2211 i=1 \u03be 2i (3.1)\ns.t. yi(w \u00b7\u03d5(xi)+b)\u2265 \u03c1\u2212\u03bei i = 1, . . . ,n\nwhere xi and yi are the data points and labels, respectively, and \u03c6 is a feature map induced by the kernel of choice. Here, the \u03bei are error cushions that specify the cost of misclassifying xi. Let w\u2217 be the optimal separating hyperplane. [3] showed that if the kernel K(x,y) = \u03d5(x) \u00b7\u03d5(y) satisfies K(x,x) = \u03ba , a constant, then w\u2217 can be found by finding the minimum enclosing ball of the points {\u03d5(xi,yi)}, where\n\u03d5 i(xi,yi) =  yi\u03d5(xi)yi 1\u221a C ei  where ei is the ith standard basis element (all zeroes except for the ith position, which is 1). If c\u2217 is the optimal MEB center, then w\u2217 = c\u2217 \u00b7 (e1 + . . .+ em).\nA couple of things to note about this equivalence: first, it transforms the supervised SVM training problem into an unsupervised one - the MEB is blind to the true label of the point. Second, the notion of a margin in the original SVM problem is transformed into a core set, a subset of inputs such that finding the MEB of the corset is a good approximation to finding the MEB over the entire input. As such, core sets can be thought of as the minimal amount of information that defines the MEB. The implementation of the CVM in [3] follows the MEB approximation algorithm described in [4]: given \u03b5 > 0, add to the core set C the point z that is the farthest from the current MEB center c. Recompute the MEB from the core set and continue until all points are within (1+ \u03b5)R from c, where R is the radius of the MEB.\nThe core vector machine achieves a 1 + \u03b5 approximation factor but makes |C| passes over the data and requires storage space linear in the size of the input, an approach that doesn\u2019t scale for streaming applications. To this end, [?] presented the StreamSVM, a streaming version of the CVM, which computes the MEB over the input {\u03d5 i} in a streaming fashion, keeping a running approximate MEB of all the data seen so far and adjusting it upon receiving a new input point. The StreamSVM used only constant space and using a small lookahead resulted in a favorable performance compared to libSVM (batch) as well as the streaming Perceptron, Pegasos, and LASVM implementations. However, the streaming MEB algorithm that powers StreamSVM is only approximate and offers a worst-case approximation ratio of between 1+ \u221a 2\n2 and 3 2 of the true MEB, leaving open the possibility of a better streaming algorithm to improve the performance of StreamSVM.\nIn this paper, we present the Blurred Ball SVM, a streaming algorithm based on the blurred ball cover proposed in [5]. It takes a parameter \u03b5 > 0 and keeps track of multiple core sets (and their corresponding MEBs), performing updates only when an incoming data point lies outside the union of the 1+\u03b5 expansion of all the maintained MEBs. The Blurred Ball SVM also makes a single pass over the input, with space complexity independent of n."}, {"heading": "4 Algorithm", "text": "The algorithm consists of two parts: a training procedure to update the blurred ball cover and a classification method, which harnesses the blurred ball to label the point with one of the two possible classes. For simplicity, we choose the classes to be \u00b11.\nAs described above, a ball with radius r and center c is a linear classifier consisting of a hyperplane passing through the origin with normal c. For the rest of this paper, we will require the following assumptions, established by Tsang et al:\n\u2022 The data points \u03c6(xi,yi), denoted by D, are linearly separable (this is always the case if C < \u221e).\n\u2022 |\u03c6(xi,yi)|= \u03ba , a constant.\nWith these assumptions, the training procedure is described in Algorithm 1 and is identical to the blurred ball cover update described in Algorithm 1 of [?].\nAlgorithm 1 Outline of training procedure for the Blurred Ball SVM 1: function TRAIN(xi,yi,L) 2: cores\u2190 [] 3: Compute x\u2032 = \u03d5(xi,yi) (normalized to norm \u03ba) as defined above 4: Add x\u2032 to the lookahead buffer buf 5: if buf < L then return 6: end if 7: if \u2203x\u2032 \u2208 B s.t. x\u2032 is not in the 1+ \u03b5 expansion of any MEB in cores then 8: c,B\u2190 new core set, MEB of {B} \u222a cores 9: Discard any core set with MEB radius smaller than r(B) \u00b7 \u03b5/4 10: cores\u2190 cores \u222a (c,B) 11: end if 12: buf \u2190{} 13: end function\nFor the purposes of analysis, we show the following properties of the linear classifier that results from the blurred balls:\nLemma 1. A ball B with center c and radius r corresponds to a linear classifier with hyperplane h having the following properties:\n1. |c|> 0 and r < \u03ba . 2. Its margin has size \u221a \u03ba2\u2212 r2.\n3. A point p lies inside B iff (p\u2212 c) \u00b7 c\u2265 0, with equality for support vectors, which lie on \u2202B.\nProof. First, note that |c|> 0. Suppose instead that |c|= 0. Then we use the following property of a MEB: any half-space H such that c \u2208 \u2202H contains at least one data point used to construct the MEB. Suppose that r = \u03ba , i.e. |c| = 0. Then this property shows that there is no hyperplane passing through the origin that contains all points entirely on one side. Now, assume that the data points are separable and let h be the normal of the hyperplane that separates the raw data points xi such that xi \u00b7h > 0 for positively labeled\npoints and < 0 for negatively classified points. Then, \u03c6 i \u00b7 h = yixi \u00b7 h > 0 for all i, a contradiction to the above property if |c|= 0. We can thus assume that |c|> 0 and r < \u03ba for a linearly separable dataset.\nThe reduction described in Section 3 shows that the linear separator defined by a MEB with center c and radius r is a hyperplane with normal parallel to c. Let d be the point farthest from the origin such that (p\u2212 d) \u00b7 d \u2265 0 for all data points p. In other words, the maximum margin is d and c\u2016d from the reduction. We can further conclude that c = d as follows: let S = {p|(p\u2212 c) \u00b7 c = 0} denote the support vectors, those that lie on the margin and are a distance \u2016d\u2016 from the maximum-margin hyperplane. The ball B(d,\u2016s\u2212d\u2016) for s \u2208 S includes all data points (it intersects B(0,\u03ba) along the margin). If c 6= d, then argmaxp\u2208D\u2016p\u2212 c\u20162 = \u2016p\u2212 d\u20162 + \u2016d\u2212 c\u20162 > \u2016s\u2212 d\u2016 and c is thus not the center of the MEB (since d is strictly better). So c = d and the MEB has radius \u2016s\u2212 c\u2016 = |c|2 +\u03ba2 for s \u2208 S. Therefore, the margin is \u2016c\u2016= \u221a \u03ba2\u2212 r2.\nSince B\u2217 = B(c,\u2016s\u2212 c\u2016) intersects B(0,\u03ba) only along the hyperplane that defines the margin, S \u2282 \u2202B\u2217, and D\\S\u2282 B\u2217 \\\u2202B\u2217.\nSince we have multiple linear separators, we have the ability to combine them in a non-linear fashion to classify a new point.\nDefinition 1. Define the support of a point p to be Sup(p) = {B \u2208 cores|p \u2208 B}, the cores in the blurred ball cover that contain p.\nDefinition 2. Define the score of a point p to be:\nS(p) = \u2211 B\u2208Sup(p) p \u00b7 cB \u2016cB\u2016 ,\nthe sum of the distances of p to the separator of all the classifiers containing p.\nNote that Sup(p)\u2229Sup(\u2212p) = /0, since each ball has r < \u03ba .\nAlgorithm 2 Example classification procedure. 1: function CLASSIFY WITH MAJORITY(xi) 2: return H(p) = sgn [S(p)\u2212S(\u2212p)] 3: end function"}, {"heading": "5 Results", "text": "We ran the Blurred Ball SVM on several canonical datasets and compared the accuracy of each run with the batch LibSVM implementation, the Stream SVM proposed by Subramanian, and the streaming setting of the Perceptron (which runs through the data only once but is otherwise identical to the perceptron training algorithm). Table 5 shows the experimental results. All trials were averaged over 20 runs with respect to random orderings of the input data stream. The Perceptron, LASVM and Stream SVM data were taken from the experiments documented in [6]. The Blurred Ball SVM on the MNIST dataset was run with \u03b5 = 0.001 and C = \u221e, and on the IJCNN dataset was run with \u03b5 = 10\u22126 and C = 105. The choice of \u03b5 and C was determined coarsely through experimentation. We offer two versions of the Blurred Ball SVM - using lookahead buffer sizes of L = 0 and L = 10. Figures 1 and 2 compare performance of different lookaheads\nas \u03b5 is varied. All experiments were run on a Macintosh laptop with a 1.7 GHz processor with 4 GB 1600 MHz standard flash memory.\nIt\u2019s clear that the Blurred Ball SVM outperforms other streaming SVMs, but even more surprising is that it also manages to outperform the batch implementation on the MNIST dataset. We suspect that this is due to the fact that our classifier allows for non-convex separators."}, {"heading": "6 Further Work", "text": "Being able to learn an SVM model in an online setting opens up myriad possibilities in the analysis of large amounts of data. There are several open questions whose answers may shed light on a streaming approach with higher accuracy than the Blurred Ball SVM presented here:\n1. Is there a streaming algorithm for maintaining an MEB with better guarantees than the Blurred Ball cover proposed by [5]? The paper originally provided a bound of 1+ \u221a 3\n2 \u2248 1.3661, which was improved by [7] to less than 1.22. Although [5] showed that it is impossible to achieve an arbitrarily small approximation factor, with 1+ \u03b5 for any \u03b5 > 0, it\u2019s possible that a better streaming MEB algorithm exists with provable bounds better than the 1.22 factor demonstrated by [7].\n2. The structure of the points in this SVM setup is unique: all data points lie on a sphere of radius \u03ba centered at the origin. Although there is no streaming MEB algorithm for unrestricted points, does this specific structure lend itself to a 1+ \u03b5 MEB approximation? If so, we would be able to construct an SVM with separator arbitrarily close to the optimal."}, {"heading": "7 Conclusion", "text": "We have presented a streaming, or \u201conline\u201d algorithm for SVM learning by making use of a reduction from the Minimum Enclosing Ball problem. Our training algorithm is tunable using the \u03b5 parameter to adjust the desired approximation ratio. We also came up with multiple types of classifiers, some of them\nnon-convex, and showed that our implementation surpassed the accuracy of other streaming implementations. One surprising finding is that our implementation surpasses the standard libSVM dataset on canonical MNIST binary digit classification datasets. Tests on other digit recognition datasets show similar results, suggesting that this better performance could be due to structural idiosyncrasies of the data."}], "references": [{"title": "Fast kernel classifiers with online and active learning", "author": ["A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research Vol", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain. Neurocomputing: Foundations of Research", "author": ["F. Rosenblatt"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Core vector machines: Fast SVM training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P-M Cheung"], "venue": "Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Optimal core-sets for balls", "author": ["M. B\u01cedoiu", "K.L. Clarkson"], "venue": "In Proceedings of DIMACS Workshop on Computational Geometry", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Streaming Algorithms for Extent Problems in High Dimensions", "author": ["P.K. Agarwal", "R. Sharathkumar"], "venue": "Proceedings of the 2014 Symposium of Discrete Algorithms", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Streamed Learning: One-Pass SVMs", "author": ["P. Rai", "H. Daum\u00e9 III", "S. Venkatasubramanian"], "venue": "International Joint Conferences on Artificial Intelligence", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Streaming and Dynamic Algorithms for Minimum Enclosing Balls in High Dimensions", "author": ["T.M. Chan", "V. Pathak"], "venue": "Computational Geometry Vol 47,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Wiley", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "hyperplane) between positive and negative instances and as such, they have a very small hypothesis complexity but provable generalization bounds [8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "There have been several implementations of a streaming SVM classifier ([1], [6], [2]), but so the most effective version has been based off the reduction from SVM to the Minimum Enclosing Ball (MEB) problem introduced by [3].", "startOffset": 221, "endOffset": 224}, {"referenceID": 5, "context": "The connection between these two problems has made it possible to harness the work done on streaming MEBs and apply them to SVMs, as was done in [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "In this paper, we utilize the Blurred Ball cover approximation to the MEB problem proposed in [5] to obtain a streaming SVM that is both fast and space efficient.", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "The Core Vector Machine (CVM) was introduced by [3] as an new take on the standard Support Vector Machine (SVM).", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "[3] showed that if the kernel K(x,y) = \u03c6(x) \u00b7\u03c6(y) satisfies K(x,x) = \u03ba , a constant, then w\u2217 can be found by finding the minimum enclosing ball of the points {\u03c6(xi,yi)}, where \u03c6 i(xi,yi) = \uf8ee\uf8f0 yi\u03c6(xi) yi 1 \u221a C ei \uf8f9\uf8fb", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The implementation of the CVM in [3] follows the MEB approximation algorithm described in [4]: given \u03b5 > 0, add to the core set C the point z that is the farthest from the current MEB center c.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "The implementation of the CVM in [3] follows the MEB approximation algorithm described in [4]: given \u03b5 > 0, add to the core set C the point z that is the farthest from the current MEB center c.", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "In this paper, we present the Blurred Ball SVM, a streaming algorithm based on the blurred ball cover proposed in [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 5, "context": "The Perceptron, LASVM and Stream SVM data were taken from the experiments documented in [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Is there a streaming algorithm for maintaining an MEB with better guarantees than the Blurred Ball cover proposed by [5]? The paper originally provided a bound of 1+ \u221a 3 2 \u2248 1.", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "3661, which was improved by [7] to less than 1.", "startOffset": 28, "endOffset": 31}, {"referenceID": 4, "context": "Although [5] showed that it is impossible to achieve an arbitrarily small approximation factor, with 1+ \u03b5 for any \u03b5 > 0, it\u2019s possible that a better streaming MEB algorithm exists with provable bounds better than the 1.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "22 factor demonstrated by [7].", "startOffset": 26, "endOffset": 29}], "year": 2014, "abstractText": "A widely-used tool for binary classification is the Support Vector Machine (SVM), a supervised learning technique that finds the \u201cmaximum margin\u201d linear separator between the two classes. While SVMs have been well studied in the batch (offline) setting, there is considerably less work on the streaming (online) setting, which requires only a single pass over the data using sub-linear space. Existing streaming algorithms are not yet competitive with the batch implementation. In this paper, we use the formulation of the SVM as a minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based off of the blurred ball cover originally proposed by Agarwal and Sharathkumar. Our implementation consistently outperforms existing streaming SVM approaches and provides higher accuracies than libSVM on several datasets, thus making it competitive with the standard SVM batch implementation.", "creator": "LaTeX with hyperref package"}}}