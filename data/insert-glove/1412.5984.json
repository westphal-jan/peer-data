{"id": "1412.5984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Stochastic Local Search for Pattern Set Mining", "abstract": "tiefenbach Local hourigan search methods telenovelas can quickly estad\u00edstica find pasi\u00f3n good apartness quality solutions untrustworthiness in cases rainout where systematic hameeduddin search 114.1 methods bikes might rawod take subjectivism a large tamaroa amount of olecranon time. imputed Moreover, in grd the context teays of beaudufe pattern geared set searchinger mining, exhaustive fpi search wuffingas methods matlock are rosensaft not amadeo applicable due to the 34.83 large 330-pound search space jowls they have bialya to explore. In bonebed this paper, tribunus we propose the application of kuze stochastic foray local search riverwalk to solve the formula pattern set mining. nyserda Specifically, gelli to the mulva task of concept radics learning. We applied a s\u00f3c number of 28.8 local search subsea algorithms amaryllis on lynn a harmonia standard rdr benchmark instances afn for bohemond pattern set mining and the triumphal results show thenia the potentials prema for further exploration.", "histories": [["v1", "Thu, 18 Dec 2014 18:16:52 GMT  (91kb,D)", "http://arxiv.org/abs/1412.5984v1", "The 8th International Conference on Software, Knowledge, Information Management and Applications (SKIMA 2014)"]], "COMMENTS": "The 8th International Conference on Software, Knowledge, Information Management and Applications (SKIMA 2014)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["muktadir hossain", "tajkia tasnim", "swakkhar shatabda", "dewan m farid"], "accepted": false, "id": "1412.5984"}, "pdf": {"name": "1412.5984.pdf", "metadata": {"source": "CRF", "title": "Stochastic Local Search for Pattern Set Mining", "authors": ["Muktadir Hossain", "Tajkia Tasnim", "Swakkhar Shatabda"], "emails": ["muktadir00@gmail.com,", "tajkiamim@gmail.com,", "swakkhar@cse.uiu.ac.bd,", "dewanfarid@cse.uiu.ac.bd"], "sections": [{"heading": null, "text": "Local search methods can quickly find good quality solutions in cases where systematic search methods might take a large amount of time. Moreover, in the context of pattern set mining, exhaustive search methods are not applicable due to the large search space they have to explore. In this paper, we propose the application of stochastic local search to solve the pattern set mining. Specifically, to the task of concept learning. We applied a number of local search algorithms on a standard benchmark instances for pattern set mining and the results show the potentials for further exploration."}, {"heading": "1 Introduction", "text": "There have been growing interest in the field of pattern set mining in stead of pattern mining in the recent years [1]. One of the most important task in pattern set mining is to find a particular set of patterns in data that successfully partitions the dataset and discriminates the classes from one another [2]. This is called concept learning task in the literature [2]. Such pattern sets are desirable incase of selecting a particular small set of patterns from a large dataset, where traditional pattern mining algorithms fail to produce good results.\nIn the concept learning task, we are given a set of classes and a set of patterns or features. The task is to select a small set of the features or the patterns so that the classification accuracy is maximized. This is a complex combinatorial optimization task. Most of the algorithms that are applied to solve the problem earlier in the literature are mostly exhaustive or greedy in nature [2]. Declarative frameworks like constraint programming methods [2, 3] have gained some significant success. However, with increasing problem size, these methods struggle to produce good quality solutions within a short period of time. On the other hand, local search methods in general can quickly find good quality solutions and have been very effective to find satisfactory results for many combinatorial optimization problems.\nIn this paper, we propose to apply a large variety to local search algorithms to solve the concept learning task in the context of pattern set mining. The set of algorithms that we use includes random walk, random valid walk, hill climbing, hill climbing with restart and genetic algorithm. We performed our experiments to solve standard benchmark datasets extensively used by the researchers in the literature. The key contributions in the paper are as follows:\n\u2022 Demonstrate the overall strength of stochastic local search methods solving the pattern set mining task.\nar X\niv :1\n41 2.\n59 84\nv1 [\ncs .A\nI] 1\n8 D\nec 2\n01 4\nThe rest of the paper is organized as follows: Section 2 describes the concept learning task in the context of pattern sets mining; Section 3 reviews the related work; Section 4 describes the algorithms used; Section 5 discusses and analyzes the experimental results; and finally, Section 6 presents our conclusions and outlines our future work."}, {"heading": "2 Preliminaries", "text": "In this section, we briefly describe the problem model that we use. We adopt the itemset mining [3] setting used earlier by Guns et al. in [2]. In the concept learning task we are given a database of transactions, D. Which is a binary matrix, meaning all items are either 0 or 1. The columns of this binary matric corresponds to a set of patterns or items, I and the rows corresponds to actual data items or transactions. The set of transactions is denoted by T . For example consider the dataset given in Table 1. Here, we have four items in the itemset, I = {A,B,C,D} and five transactions in the transaction set, T = {t1, t2, t3, t4, t5}. The itemsets or pattern sets and the transaction sets are generally represented by binary vectors. The coverage \u03d5D(I) of an itemset I consists of all transactions in which the itemset occurs:\n\u03d5D(I) = {t \u2208 T |\u2200i \u2208 I : Dti = 1}\nFor example, given an itemset, I = A,D, it is represented as \u30081, 0, 0, 1\u3009 and the the coverage is \u03d5D(I) = {t1, t3, t4} which is represented by \u30081, 0, 1, 1, 0\u3009. Support of the itemset is SupportD(I) = 3. Where, Support of an itemset is the size of its coverage set, SupportD(I) = |\u03d5D(I)|.\nAny item set I is closed over T , if no other supersets of this itemset covers the same transaction set T as I. Formally,\nclosedness \u03c8(I, T ) is defined as the following constraint:\n\u03c8(I, T ) = 1\u21d4 (\u2200i \u2208 I : Ii = 1\u21d4 \u2227 t\u2208T (Dti = 1 \u2228 Tt = 0))\nFor example, the itemset I1 = A,D is closed and the itemset, I = A,C is not closed. Each of the transactions in the dataset are associated with two distinct classes: positive transactions T + and negative transactions T \u2212. Now, we can\ndefine the accuracy of an itemset or patternset which is defined by its coverage set T as following:\naccuracy(T ) = \u2211 t\u2208T + Tt \u2212 \u2211 t\u2208T \u2212 Tt\nIn pattern set mining, we are interested to find k\u2212pattern sets [4]. A k\u2212pattern set \u03a0 is a set of k tuples, each of type\n\u3008Ip, T p\u3009. The pattern set is formally defined as following:\n\u03a0 = {\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0k}, where, \u2200p = 1, \u00b7 \u00b7 \u00b7 , k : \u03c0p = \u3008Ip, T p\u3009\nThe concept learning task is to find a set of itemsets or patterns that together cover as many positive transactions as\npossible, while covering only few negative transactions. This can be formalized as the following optimization problem:\nmaximize \u03a0,T accuracy(T ) where,\n\u2200t \u2208 T : Tt = (\u2228p=1\u00b7\u00b7\u00b7kT pt ) \u2200\u03c0 \u2208 \u03a0 : \u03d5D(Ip) = T p \u2200\u03c0 \u2208 \u03a0 : \u03c8(I, T +) = 1\nIn this paper, we attempt to solve this problem using different types of stochastic local search algorithms: random walk,\nrandom valid walk, hill climbing, hill climbing with restart and genetic algorithm."}, {"heading": "3 Related Works", "text": "The frequent itemset mining task was first proposed in [5] followed by a large number of algorithms and their application is various fields. In the the pattern mining problem [6], we are interested to find patterns in the dataset that are correlated [7], discriminative [8], contrast [9], diverse [10] etc. A large variety of algorithms has been proposed to solve the pattern set mining in the last decade [1].\nRecent advancements in the field shows the use of declarative programming frameworks i.e. constraint programming [3]. State-of-the-art constraint solvers i.e. COMET [11] are used to solve the pattern set mining and related tasks [2]. It opens a huge research area by combining data mining or machine learning tasks with constraint programming [12, 13]. However, most of these methods use systematic search methods to search and shrink the solution space. Most of the systematic search algorithms are exhaustive in nature and require huge amount of time and resources to solve the problem efficiently. Where as greedy methods does not guarantee optimality. The necessity of efficient global optimization algorithms and effective search heuristics for such problems is higher than ever.\nIn [10], stochastic search algorithms are applied to solve a related problem to find diverse pattern sets. However, Guns et al. [2] applied large neighborhood search, using constraint programming platform to solve both of the concept learning task and the diverse pattern set mining problem."}, {"heading": "4 Our Approach", "text": "In this section, we describe the algorithms that we have implemented to solve the concept learning task. The basic framework contains two important functions that calculates the coverage and checks the closedness or maximal property of the itemsets or patternsets.\nAlgorithm 1: getCoverage(ItemSet I, TransactionSet T ) 1 coverage = [0, \u00b7 \u00b7 \u00b7 , 0] 2 for each transaction t \u2208 T do 3 if all items in I appears in t then 4 coverage[t] = 1 5 return coverage\nThe getCoverage algorithm is given in Algorithm 1. It simply searches for occurrences of items in an itemset in all transactions in the dataset and returns them all. The next important function is the isClosed function that verifies the maximal property of an itemset. The algorithm is pictured in Algorithm 2.\nAlgorithm 2: isClosed(ItemSet I,Database D) 1 s0 = SupportD(I) 2 for each item i \u2208 D do 3 if i /\u2208 I then 4 In = I \u222a {i} 5 sn = SupportD(In) 6 if sn \u2265 s0 then 7 return false 8 return true\nThe calculation of accuracy is done by calculating the number of positive transactions, covered by any of the coverage sets of the itemsets in the pattern set and subtracting the number of negative transactions covered by any of the coverage sets of the itemsets, in the pattern set. The algorithm for calculating accuracy in Algorithm 3.\nRest of the section describes the algorithms that we have implemented on this setup using these functions."}, {"heading": "4.1 Random Walk", "text": "The random walk algorithm is similar to Monte Carlo simulations in nature. The pseudo-code for the algorithm is given in Algorithm 4. This algorithm continuously updates the maximum accuracy and the best solution \u03a0\u2217 and then returns the best solution found before the algorithm times out."}, {"heading": "4.2 Random Valid", "text": "The next algorithm we call random valid. It starts by initially generating a valid patterns set. A pattern set is valid if the itemsets are all maximal. The termination criteria is similar to the previous algorithm random walk. In this version, the algorithm make some random changes in the current solution and accepts the new solution only if the resulting pattern set is valid in each iteration. Our random valid algorithm is given in Algorithm 5.\nAlgorithm 3: getAccuracy(PatternSet \u03a0, Database D) 1 C+: set of positive coverages 2 C\u2212: set of negative coverages 3 for each itemset I \u2208 \u03a0 do 4 C+[i] = getCoverage(I, T +) 5 C\u2212[i] = getCoverage(I, T \u2212) 6 posN = 0 7 for each transaction t \u2208 T + do 8 for each coverage c \u2208 C+ do 9 if c[t] == 1 then\n10 posN + + 11 break 12 negN = 0 13 for each transaction t \u2208 T \u2212 do 14 for each coverage c \u2208 C\u2212 do 15 if c[t] == 1 then 16 negN + + 17 break 18 return posN \u2212 negN\nAlgorithm 4: RandomWalk()\n1 maxAccuracy = \u2212\u221e 2 \u03a0\u2217 = \u03c6 3 while timeout do 4 \u03a0 = randomly create a k-itemset 5 isMaximal = true 6 for each itemset I \u2208 \u03a0 do 7 if isClosed(I, T +) == false then 8 isMaximal = false 9 break\n10 if isMaximal == true then 11 accuracy = getAccuracy(\u03a0,D) 12 if accuracy > maxAccuracy then 13 maxAccuracy = accuracy 14 \u03a0\u2217 = \u03a0 15 return \u03a0\u2217\nAlgorithm 5: RandomValid()\n1 \u03a0 = generate a valid pattern set with k items 2 \u03a0\u2217 = \u03a0 3 maxAccuracy = getAccuracy(\u03a0,D) 4 while timeout do 5 \u03a0t= make random change in \u03a0 6 if isValid(\u03a0t,D) then 7 accuracy = getAccuracy(\u03a0,D) 8 if accuracy > maxAccuracy then 9 maxAccuracy = accuracy\n10 \u03a0\u2217 = \u03a0t 11 \u03a0 = \u03a0t 12 return \u03a0\u2217"}, {"heading": "4.3 Hill Climbing", "text": "The hill climbing algorithm is similar to the random valid algorithm. But it differs in how the next candidate solution is accepted at Lines 10-11. The new candidate solution is accepted only if it does not decrease the accuracy of the current candidate pattern set. The algorithm is given in Algorithm 6.\nAlgorithm 6: HillClimbing()\n1 \u03a0 = generate a valid pattern set with k items 2 maxAccuracy = getAccuracy(\u03a0,D) 3 while timeout do 4 \u03a0t= make random change in \u03a0 5 if isValid(\u03a0t,D) then 6 accuracy = getAccuracy(\u03a0,D) 7 if accuracy \u2265 maxAccuracy then 8 maxAccuracy = accuracy 9 \u03a0 = \u03a0t\n10 return \u03a0\u2217"}, {"heading": "4.4 Hill Climbing with Restart", "text": "The hill climbing algorithm pictured in Algorithm 6 is greedy in nature as it only accepts non-decreasing steps, in terms of accuracy. However, such strategy quickly leads the algorithm into local minima and cannot improve further. To tackle this situation, we propose the next algorithm which we call hill climbing with restart. It denoted by HC +Restart in the rest of the paper. The pseudo-code for HC + Restart is given in Algorithm 7. This algorithm is similar to that of hill climbing but it keeps track of the non improving steps and then it restarts the search after the number of non improving steps, crosses the limit of a parameter threshold. The value of threshold was kept 100 for these experiments.\nAlgorithm 7: HillClimbingWithRestart()\n1 \u03a0 = generate a valid pattern set with k items 2 \u03a0\u2217 = \u03a0 3 maxAccuracy = getAccuracy(\u03a0,D) 4 nonImprovingSteps = 0 5 while timeout do 6 \u03a0t= make random change in \u03a0 7 if isValid(\u03a0t,D) then 8 accuracy = getAccuracy(\u03a0,D) 9 if accuracy > maxAccuracy then\n10 maxAccuracy = accuracy 11 \u03a0\u2217 = \u03a0t 12 nonImprovingSteps = 0 13 else 14 nonImprovingSteps+ + 15 if nonImprovingSteps \u2265 threshold then 16 \u03a0 = generate a valid pattern set with k items 17 nonImprovingSteps = 0 18 else 19 \u03a0 = \u03a0t 20 return \u03a0\u2217"}, {"heading": "4.5 Genetic Algorithm", "text": "The single point search methods often starts from a solution in the search space and do not have much chance of divergence. Which is needed to cover the huge search space for global optimization problem. For this reason, multi-point search algorithms are preferred. They keep a number of solutions in the population and runs single point search in parallel. Moreover, combinations of the individuals in the population helps the search to create high quality solutions that resembles the natural process of evolution of species. One such algorithm is genetic algorithms. The genetic algorithm that we propose here keeps a population of pattern sets and then iteratively recombines and mutates the individuals to create new solutions. The pseudo-code for our genetic algorithm is given in Algorithm 8.\nAlgorithm 8: Genetic Algorithm()\n1 p : population size 2 P = generate p valid pattern sets 3 while timeout do 4 childCount = 0 5 Pc = {} 6 while childCount < p do 7 \u03a01 = selectParentAtRandom(P) 8 \u03a02 = selectParentAtRandom(P) 9 \u03a0c = crossover(\u03a01,\u03a02)\n10 if isValid(\u03a0c) then 11 Pc = Pc \u222a {\u03a0c} 12 childCount+ + 13 Pm = {} 14 for each \u03a0 \u2208 Pc do 15 while true do 16 \u03a0m = make random change in \u03a0 17 if isValid(\u03a0m) then 18 Pm = Pm \u222a {\u03a0m} 19 break 20 P = selectBest(P \u222a Pc \u222a Pm) 21 return globalBest\nIn our genetic algorithm, the population is initialized by a number of valid pattern sets. In each generation of the genetic algorithm, the algorithm goes through two phases: crossover and mutation. In the crossover phase, two parent individuals are choosen randomly from the population and a child individual in created. This new individual is created using the one point crossover operation. After the validity of the individual checking, only a valid solution is added to the crossover population. This process iterates until the crossover population size is equal to the population size. Then the mutation phase begins. From each of the valid individuals in the crossover population a valid individual is created and added to the mutation population list. At the end of the mutation phase best solutions from these three population lists are selected for the next generation."}, {"heading": "5 Experimental Results", "text": "We have implemented the algorithms in Ruby language and have run our experiments on an Intel core i3 2.40 GHz machine with 6 GB ram running 32bit Ubuntu 14.04 operating system."}, {"heading": "5.1 Dataset", "text": "The benchmark datasets that we use in this paper are taken from UCI Machine Learning repository [14] and originally used in [2]. The datasets are available to download freely from the website: https://dtai.cs.kuleuven.be/CP4IM/datasets/. The benchmarks are given in Table 2 with their properties."}, {"heading": "5.2 Results", "text": "We performed experiments by running different algorithms on each of the benchmark dataset and reported accuracy in Table 3. Due to the unavailability of COMET, we were unable to compare the search performance with the large neighborhood search used in [2]. Each algorithm were given 10 minutes to finish and the best accuracy found during the runtime is reported in the table. The pattern size setting was varied to see the effect of the size parameter on the performance of the algorithms. The value of the parameter k was varied for the values {2, 3, 4}. The best values in each row are shown in bold faced fonts."}, {"heading": "5.3 Analysis", "text": "From the results reported in Table 3, it is evident that genetic algorithms work better than other algorithms for most of the datasets for different pattern sizes. However, in a few cases hill climbing with restart (denoted by HC+Restart in the table) works better. However, the margin is not much higher. The random walk Monte Carlo simulation performs poorly since there not much of intelligent decisions taken during the search. The performance of the random valid search improves since random walk wastes a good amount of time finding valid pattern sets that are closed over the positive transaction set. The hill climbing search quickly gets stuck into the local maximum and once stagnates, it can not improve further due to lack of escaping mechanism. The situation improves when we add random restart strategy along with the hill climbing search. The performance of the algorithms somehow varies with the size of the pattern sets. The situation is pictured in Figure ??. In this figure, accuracy of the search algorithms are shown as vertical bars for all the datasets for different pattern set sizes. It is evident from the bar diagrams that the increase in the pattern set size favors the genetic algorithm.\nWe also depict the performance of the search algorithms in Figure 7. It shows the progress of different search algorithms in 10 minutes for a single run of the hepatitis dataset. The random walk algorithm lacking intelligent decision through the search can not improve much. Where as, hill climbing improves very quickly by taking greedy best choice moves. However, if it gets stuck and the improvement is possible with random restarts. Genetic algorithm shows continuous improvements\nof best accuracy achieved."}, {"heading": "6 Conclusion", "text": "In this paper, we showed the effectiveness of various stochastic local search algorithms to solve the concept learning task in pattern set mining. Population based algorithms like genetic algorithm shows promising results while local minima escaping strategies like random restart increases the effectiveness of greedy hill climbing search to a great extent. In future, we would like to improve the performance of the search techniques by incorporating further effective strategies within the framework of stochastic local search and solve pattern set mining related problems with realistic datasets."}], "references": [{"title": "Mining sets of patterns", "author": ["B. Bringmann", "S. Nijssen", "N. Tatti", "J. Vreeken", "A. Zimmerman"], "venue": "Tutorial at ECMLP- KDD, 2010. 10  Figure 5: Put a title here Figure 6: Put a title here Figure 7: Search progress for different algorithm for the hepatitis dataset with size of the pattern size k = 3. 11", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Declarative heuristic search for pattern set mining", "author": ["T. Guns", "S. Nijssen", "A. Zimmermann", "L. De Raedt"], "venue": "Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on. IEEE, 2011, pp. 1104\u20131111.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Itemset mining: A constraint programming perspective", "author": ["T. Guns", "S. Nijssen", "L. De Raedt"], "venue": "Artificial Intelligence, vol. 175, no. 12, pp. 1951\u20131983, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1951}, {"title": "k-pattern set mining under constraints", "author": ["T. Guns", "S. Nijssen", "L.D. Raedt"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 25, no. 2, pp. 402\u2013418, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining association rules between sets of items in large databases", "author": ["R. Agrawal", "T. Imieli\u0144ski", "A. Swami"], "venue": "ACM SIGMOD Record, vol. 22, no. 2. ACM, 1993, pp. 207\u2013216.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Transversing itemset lattices with statistical metric pruning", "author": ["S. Morishita", "J. Sese"], "venue": "Proceedings of the nineteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. ACM, 2000, pp. 226\u2013236.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Discriminative frequent pattern analysis for effective classification", "author": ["H. Cheng", "X. Yan", "J. Han", "C.-W. Hsu"], "venue": "Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on. IEEE, 2007, pp. 716\u2013725.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Supervised descriptive rule discovery: A unifying survey of contrast set, emerging pattern and subgroup mining", "author": ["P.K. Novak", "N. Lavra\u010d", "G.I. Webb"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 377\u2013403, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimizing feature sets for structured data", "author": ["U. R\u00fcckert", "S. Kramer"], "venue": "Machine Learning: ECML 2007. Springer, 2007, pp. 716\u2013723.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Constraint-based local search", "author": ["P.V. Hentenryck", "L. Michel"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Declarative modeling for machine learning and data mining", "author": ["L. De Raedt"], "venue": "Formal Concept Analysis. Springer, 2012, pp. 2\u20132.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Miningzinc: A modeling language for constraint-based mining", "author": ["T. Guns", "A. Dries", "G. Tack", "S. Nijssen", "L. De Raedt"], "venue": "Proceedings of the Twenty-Third international joint conference on Artificial Intelligence. AAAI Press, 2013, pp. 1365\u20131372.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "UCI machine learning repository", "author": ["A. Frank", "A. Asuncion"], "venue": "2010. 12", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "There have been growing interest in the field of pattern set mining in stead of pattern mining in the recent years [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "One of the most important task in pattern set mining is to find a particular set of patterns in data that successfully partitions the dataset and discriminates the classes from one another [2].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "This is called concept learning task in the literature [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Most of the algorithms that are applied to solve the problem earlier in the literature are mostly exhaustive or greedy in nature [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Declarative frameworks like constraint programming methods [2, 3] have gained some significant success.", "startOffset": 59, "endOffset": 65}, {"referenceID": 2, "context": "Declarative frameworks like constraint programming methods [2, 3] have gained some significant success.", "startOffset": 59, "endOffset": 65}, {"referenceID": 2, "context": "We adopt the itemset mining [3] setting used earlier by Guns et al.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "in [2].", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In pattern set mining, we are interested to find k\u2212pattern sets [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "The frequent itemset mining task was first proposed in [5] followed by a large number of algorithms and their application is various fields.", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": "In the the pattern mining problem [6], we are interested to find patterns in the dataset that are correlated [7], discriminative [8], contrast [9], diverse [10] etc.", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "In the the pattern mining problem [6], we are interested to find patterns in the dataset that are correlated [7], discriminative [8], contrast [9], diverse [10] etc.", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "In the the pattern mining problem [6], we are interested to find patterns in the dataset that are correlated [7], discriminative [8], contrast [9], diverse [10] etc.", "startOffset": 143, "endOffset": 146}, {"referenceID": 8, "context": "In the the pattern mining problem [6], we are interested to find patterns in the dataset that are correlated [7], discriminative [8], contrast [9], diverse [10] etc.", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "A large variety of algorithms has been proposed to solve the pattern set mining in the last decade [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "constraint programming [3].", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "COMET [11] are used to solve the pattern set mining and related tasks [2].", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "COMET [11] are used to solve the pattern set mining and related tasks [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "It opens a huge research area by combining data mining or machine learning tasks with constraint programming [12, 13].", "startOffset": 109, "endOffset": 117}, {"referenceID": 11, "context": "It opens a huge research area by combining data mining or machine learning tasks with constraint programming [12, 13].", "startOffset": 109, "endOffset": 117}, {"referenceID": 8, "context": "In [10], stochastic search algorithms are applied to solve a related problem to find diverse pattern sets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "[2] applied large neighborhood search, using constraint programming platform to solve both of the concept learning task and the diverse pattern set mining problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "The benchmark datasets that we use in this paper are taken from UCI Machine Learning repository [14] and originally used in [2].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "The benchmark datasets that we use in this paper are taken from UCI Machine Learning repository [14] and originally used in [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Due to the unavailability of COMET, we were unable to compare the search performance with the large neighborhood search used in [2].", "startOffset": 128, "endOffset": 131}], "year": 2014, "abstractText": "Local search methods can quickly find good quality solutions in cases where systematic search methods might take a large amount of time. Moreover, in the context of pattern set mining, exhaustive search methods are not applicable due to the large search space they have to explore. In this paper, we propose the application of stochastic local search to solve the pattern set mining. Specifically, to the task of concept learning. We applied a number of local search algorithms on a standard benchmark instances for pattern set mining and the results show the potentials for further exploration.", "creator": "LaTeX with hyperref package"}}}