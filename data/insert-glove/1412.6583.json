{"id": "1412.6583", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Discovering Hidden Factors of Variation in Deep Networks", "abstract": "mahgub We flamanville propose a method 52.20 for disruptions learning 290.5 latent emiri representations of yati the gauzy factors of variation finnbogadottir in data. cron By congresos augmenting deep slippage autoencoders mulva with a supervised babylone cost donita and an piol additional hallig unsupervised 9-by-5-inch cost, 1280s we seibel create a tgn semi - reposting supervised model that can discover maraniss and hit-and-run explicitly represent friederich factors .200 of variation beyond 89.97 those relevant for categorization. apulian We turves use a krrish novel unsupervised covariance an/apq penalty (XCov) to o'beirne disentangle merleau-ponty factors telli like 73.17 handwriting style ambasssador for 3,183 digits and subject identity hydroformylation in fulani faces. We demonstrate this on the llevan MNIST wynkyn handwritten fornaio digit cxd4 database, tepee the avanzar Toronto Faces betrothal Database (TFD) and sundari the thistlewood Multi - PIE forleo dataset gracin by generating zizhen manipulated instances muhith of the data. Our model 2129 discovers 99.49 additional iyc high - level natsuko latent logoi factors daming absent from hooverville the winterstein supervised canola signal.", "histories": [["v1", "Sat, 20 Dec 2014 02:52:03 GMT  (2795kb,D)", "http://arxiv.org/abs/1412.6583v1", "10 pages, 9 figures"], ["v2", "Fri, 27 Feb 2015 20:41:40 GMT  (2798kb,D)", "http://arxiv.org/abs/1412.6583v2", "12 pages, 9 figures"], ["v3", "Fri, 17 Apr 2015 17:15:02 GMT  (2798kb,D)", "http://arxiv.org/abs/1412.6583v3", "12 pages, 9 figures"], ["v4", "Wed, 17 Jun 2015 06:47:48 GMT  (3042kb,D)", "http://arxiv.org/abs/1412.6583v4", "Presented at International Conference on Learning Representations 2015 Workshop"]], "COMMENTS": "10 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["brian cheung", "jesse a livezey", "arjun k bansal", "bruno a olshausen"], "accepted": true, "id": "1412.6583"}, "pdf": {"name": "1412.6583.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEEP NETWORKS", "Brian Cheung", "Jesse A. Livezey", "Arjun K. Bansal", "Bruno A. Olshausen"], "emails": ["bcheung@berkeley.edu", "jesse.livezey@berkeley.edu", "baolshausen@berkeley.edu", "arjun@nervanasys.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "One of the goals of representation learning is to find an efficient representation of input data that simplifies tasks such as object classification (Krizhevsky et al., 2012) or image restoration (Eigen et al., 2013). Supervised algorithms approach this problem by learning features which transform the data into a space where different classes are linearly separable. However this often comes at the cost of discarding other variations such as style or pose that may be important for more general tasks. On the other hand, unsupervised learning algorithms such as autoencoders seek efficient representations of the data such that the input can be fully reconstructed, implying that the latent representation preserves all factors of variation in the data. However, without some explicit means for factoring apart the different sources of variation the factors relevant for a specific task such as categorization will be entangled with other factors across the latent variables. Our goal in this work is to combine these two approaches in order to develop a model that preserves but also separates apart the different factors of variation in the data.\nPrevious approaches to separating factors of variation in data, such as content vs. style (Tenenbaum & Freeman, 2000) or form vs. motion (Grimes & Rao, 2005; Olshausen et al., 2007; ?; ?; ?; ?), have relied upon a bilinear model architecture in which the units representing different factors are combined multiplicatively. This approach was also recently utilized to separate facial emotion vs. identity using higher-order restricted Boltzmann machines (Reed et al., 2014). One downside of bilinear approaches in general is that they require learning a weight tensor corresponding to all threeway multiplicative combinations of units. Oftentimes this is made more tractable by approximating the full tensor in terms of a sum of low-rank tensors, which reduces the number of weights to\nar X\niv :1\n41 2.\n65 83\nv1 [\ncs .L\nG ]\nbe learned. Despite the impressive results achieved with this approach, the question nevertheless remains as to whether there is a more straightforward way to separate factors of variation using standard nonlinearities in feedforward neural networks. For example, Kingma et al. (2014) utilized a variational autoencoder in a semi-supervised learning paradigm which learned to separate content and style in data. It is this work which is the inspiration for the model presented here.\nAutoencoder models have been shown to be useful for a variety of machine learning tasks (Rifai et al., 2011; Vincent et al., 2010; Le, 2013). The basic autoencoder architecture can be separated into an encoding stage and a decoding stage. During training, the two stages are jointly optimized to reconstruct the input data from the output of the decoder. Bengio et al. (2007) showed that greedy layerwise pre-training can speed up the convergence and improve generalization capabilities of deep networks. After pre-training, the decoder is discarded and the network is fine-tuned on labeled data. Such autoencoder models are now a standard framework in many machine learning pipelines, as they are easily trained by back propagation as compared to probabilistic generative models that require expensive sampling or other approximate methods to fit to the data.\nIn this work, we propose using both the encoding and decoding stages of the autoencoder to learn latent representations of the factors of variation contained in the data. The latent representation (or middle layer) is divided into two sets of variables. The first set is used in a discriminative task and during reconstruction. The second set is used only for reconstruction. In addition to the typical reconstruction cost, we add two additional costs to the network. The first is a discriminative cost on the supervised latent variables such as cross-entropy. The second is a novel cross-covariance penalty (XCov) between the supervised and unsupervised latent variables across a batch of data. This penalty prevents unsupervised latent variables from encoding input variations due to class label. In this way, the latent variables which represent class assignment are separated from those which are encoding other factors of variations in the data. The model is trained with backpropagation using the common MLP layer types (in this case, RELU and linear), and can use a combination of labeled, partially labeled, and unlabeled data."}, {"heading": "2 MODEL", "text": "Given a data sample x \u2208 RD and it\u2019s corresponding class label y \u2208 {1, ..., L} for a given dataset D, our model can be decomposed into an encoding and a decoding stage."}, {"heading": "2.1 ENCODING", "text": "The encoder F (x; \u03b8) is comprised of two functions with parameters \u03b8 = {\u03b8h, \u03b8z, \u03b8y}.\ny\u0302 = q(x; \u03b8y, \u03b8h)\nz = r(x; \u03b8z, \u03b8h)\n{y\u0302, z} = F (x; \u03b8) \u2261 {q(x), r(x)} (1)\nAs shown if Figure 1, the encoder first transforms the input x into a tuple of variables: {y\u0302, z}. The label prediction y\u0302 has an encoding process y\u0302 = q(x) which is functionally equivalent to a supervised feedforward network. z is an additional latent representation of x which is similar to the representation generated at any sufficiently deep layer during the feedforward process of an autoencoder. To keep our current formulation simple, z is at the same depth as y\u0302."}, {"heading": "2.2 DECODING", "text": "The decoder G(y\u0302, z;\u03c6) transforms the tuple provided by the encoder into a reconstruction x\u0302. Combining the encoding and decoding steps defines the forward process of our model:\nx\u0302 = G(y, z;\u03c6) = G(F (x; \u03b8);\u03c6) (2)\nThe decoder must create a reconstruction x\u0302 which is as close to x as possible given the feature representation y\u0302 and z. The supervised term y\u0302 often represents a lossy transformation of the input\nx such as the class label. In order to properly reconstruct x, the latent variable z must account for the remaining variation of x. For example, the class label \u20185\u2019 provided by y would not be sufficient information for the decoder to properly reconstruct the image of a particular digit \u20185\u2019. In this scenario, z would encode properties of the digit such as style, slant, width, etc. to provide the decoder sufficient information to reconstruct the original image."}, {"heading": "2.3 LEARNING", "text": "With the forward process of the network defined, the objective function to train the network can be described:\n\u03b8\u0302, \u03c6\u0302 = arg min \u03b8,\u03c6 \u2211 {x,y}\u2208D \u03b1U(x, x\u0302) + \u03b2S(y, y\u0302) + \u03b3C(y\u0302, z). (3)\nU(x, x\u0302) is a reconstruction cost, S(y, y\u0302) is a supervised cost of the encoder, and C(y\u0302, z) is the XCov cost which disentangles the supervised and unsupervised latent variables of the encoder.\nThis objective function naturally fits a semi-supervised learning framework. For unlabeled data, the multiplier \u03b2 for the supervised cost S is simply set to zero. Furthermore, when the objective, encoding, and decoding functions are differentiable, this model can be trained with stochastic gradient descent via backpropagation.\nWhile there are many potential choices for the reconstruction cost depending on the distribution of data vector x, for our experiments we use mean-squared-error for all datasets which is defined as:\nU(x, x\u0302) = ||x\u2212 x\u0302||2. (4)\nFor the supervised latent variables, the form of the cost function depends on the type of variables (categorical, binary, continuous). For our experiments, we had one or more categorical latent variables so we parametrized them as one-hot vectors and computed y\u0302 using a softmax activation with cross-entropy as the cost.\nThe XCov penalty to disentangle y\u0302 and z is simply a cross-covariance penalty between the activations across samples in a batch of size N with formula and gradient defined as:\nC(y\u0302, z) = 1\n2N \u2211 yi \u2211 zj ((yi \u2212 y\u0304i)(zj \u2212 z\u0304j))2 (5)\n\u2202C(y\u0302, z)\n\u2202y\u0302i =\n1\nN \u2211 zj (yi \u2212 y\u0304i)(zj \u2212 z\u0304j)2 (6)\n\u2202C(y\u0302, z)\n\u2202z\u0302j =\n1\nN \u2211 yi (yi \u2212 y\u0304i)2(zj \u2212 z\u0304j) (7)\nwhere y\u0304i and z\u0304j denote averages over batches. Unlike U(x, x\u0302) and S(y, y\u0302) in the objective, C(y\u0302, z) is a cost computed over a batch of datapoints. It is possible to approximate this quantity with a\nmoving average during training but we have found that this cost has been robust to small batch sizes and have not found any issues when training with mini-batches of N = 50.\nThis penalty will only truly factorize P (y\u0302, z|x) into P (y\u0302|x)P (z|x) when P (y\u0302, z|x) is normally distributed. Despite this fact, we have found this penalty to work well regardless of the form of activation function (i.e. softmax, tanh, linear). Rifai et al. (2012) proposed a similar penalty in Contractive Discriminant Analysis method which penalized the cross-derivatives between sets of supervised and unsupervised latent variables with respect to the input."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "We evaluate our model on three datasets of increasing complexity. The network is trained using AdaDelta (Zeiler, 2012) with gradients from standard backpropagation. Models were implemented in a modified version of Pylearn2 (Goodfellow et al., 2013a)."}, {"heading": "3.1 MNIST HANDWRITTEN DIGITS DATABASE", "text": "The MNIST handwritten digits database (LeCun & Cortes, 1998) consists of 60,000 training and 10,000 test images of handwritten digits 0-9 of size 28x28. Following previous work (Goodfellow et al., 2013b), we split the training set into 50,000 samples for training and 10,000 samples as a validation set for model selection."}, {"heading": "3.2 TORONTO FACES DATABASE", "text": "The Toronto Faces Database (Susskind et al., 2010) consists of 102,236 grayscale face images of size 48x48. Of these, 4,178 are labeled with 1 of 7 different expressions (anger, disgust, fear, happy, sad, surprise, and neutral). Examples are shown in Figure 2. The dataset also contains 3,784 identity labels which were not used in this paper. The dataset has 5 folds of training, validation and test examples."}, {"heading": "3.3 MULTI-PIE DATASET", "text": "The Multi-PIE datasets (Gross et al., 2010) consists of 754,200 high-resolution color images of 337 subjects. Each subject was recorded under 15 camera poses: 13 spaced at 15 degree intervals at head height, and 2 positioned above the subject. For each of these cameras, subjects were imaged under 19 illumination conditions and a variety of facial expressions. We discarded images from the two overhead cameras due to inconsistencies found in the image orientation for those two cameras. Camera pose and illumination data was retained as supervised labels.\nOnly a small subset of the images possess facial keypoint information for each camera pose. To perform a weak registration to appoximately localize the face region, we compute the maximum bounding box created by all available facial keypoint coordinates for a given camera pose. The bounding box is applied to all images for that camera pose. We then resized the cropped images to 48x48 pixels and convert to grayscale. We divide the dataset into 528,060 training, 65,000 validation and 60,580 test examples. Splits were determined by subject id. Therefore, the test set consists of subject identities never seen by our model. Example images from our test set are shown in Figure 3.\nThe Multi-PIE dataset contains significantly more factors of variation than MNIST or TFD. Comparing example images from TFD in Figure 2 and Multi-PIE in Figure 3, it it clear that the Multi-PIE images are substantially more complex. The weak registration causes variation in head position and scale. While this could be alleviated with facial registration software, we forgo this preprocessing to demonstrate the robustness of our method."}, {"heading": "3.4 EXPLORING DEEP SPACE", "text": "We begin our analysis using the MNIST dataset. After training the MNIST model described in Table 1, the function of hidden units in different layers can be explored. As shown in Figure 4, the unsupervised latent variables z take on a approximate Normal distribution with mean zero and standard deviation .\u030335. We can generate digits from this approximate distribution and explore this smooth 2D latent space using the generative decoding network."}, {"heading": "3.4.1 UNSUPERVISED LATENT VARIABLES", "text": "To visualize the transformations that the unsupervised variables are learning, the decoder can be used to create images with different values for z. We select latent variable values by setting the zs to a linear range with y set up one-hot vectors as seen in Figure 4. At the center of z-space, (0,0), we find the canonical MNIST digits. Moving further from the center, the digits become more stylized and also less probable."}, {"heading": "3.4.2 MOVING FROM LATENT SPACE TO IMAGE SPACE", "text": "After the supervised and unsupervised latent variables, there are two more layers of activations before the output of the model into image space. To visualize the function of these layers, we compute the jacobian of the output image, x\u0302, with respect to the activation of hidden units, hk, in a particular layer,\n\u2206x\u0302ki = \u2202x\u0302i \u2202hkj \u2206hj . (8)\nHere, i is the index of a pixel in the output of the network, j is the index of a hidden unit, and k is the layer number. We remove hidden units with zero activation from the jacobian since their derivatives are not meaningful. A summary of the results are plotted in Figure 5.\nFor the jacobian with respect to the z units, we plot the result in image space. As expected, the jacobian with respect to the z units locally mirror the transformations seen in Figure 4.\nInspired by Rifai et al. (2011), for the next two layers, we plot the singular value spectrum. For h\u22123, the spectrum is peaked and thus there are a small number of directions with large effect on the image output, so we plot singular vectors with largest singular value. For all digits besides \u201c1\u201d, the first component seems to create a template digit and the other componets make small style adjustments. For h\u22122, the spectrum is more degenerate, so we choose a random set of columns from the jacobian to plot which will better represent the layer\u2019s function. We notice that for each layer moving from the encoder to the output, their contributions become more localized."}, {"heading": "3.5 GENERATING EXPRESSION TRANSFORMATIONS", "text": "We demonstrate semi-supservised capabilities of our model on the TFD which contains substantially more complex images than MNIST and has far fewer labeled examples. We are able to change the expression while preserving identity of faces never before seen by the model. We first initialize {y\u0302, z} with an example from the test set. We then discard y\u0302 and fill it in with an expression label and\nretain z. Figure 6 shows the results of this process. Expressions can be changed while leaving other facial characteristics largely intact. This is not possible when the XCov cost is removed, because the expression information is distributed across all 793 z variables in addition to the 7 y variables. Modifications to y do significantly impact the representation provided to the decoder.\nAdditionally, y can be set to values well beyond those that the encoder could output with a softmax activation during training. We vary the expression variable given to the decoder from 5 to -5. This results in greatly exagerated expressions when set large and positive as seen in Figure 7. Remarkably, setting the variables to large negative values results in opposite facial expressions being displayed (e.g. eyes closed vs. eyes open, cheeks in vs. cheeks out.) It appears that the model has learned a meaningful representation of facial structure."}, {"heading": "3.6 MANIPULATING MULTIPLE FACTORS OF VARIATION", "text": "For Multi-PIE, we use two sets of supervised latent factors (camera pose and illumination) to train our model. As shown in Table 1, we have two softmax layers at the end of the encoder. The first encodes the camera pose of the input image and the second the illumination condition. Due to the increased complexity of this dataset, we made this network substantially deeper (9 layers) than the previous models. It was possible to train this autoencoder without pre-training because of the additional gradient information injected by the supervised and covariance cost at the end of the encoder similar to Szegedy et al. (2014).\nIn Figure 8, we show the images generated by the decoder while iterating through each camera pose. The network was tied to the illumination and unsupervised latent variables of images from the test\nset. Although blurry, the generated images preserve the subject\u2019s illumination and identity (i.e. shirt color, hair style, skin tone) as the camera pose changes. In Figure 9, we instead fix the camera position and iterate through different illumination conditions."}, {"heading": "4 CONCLUSIONS", "text": "With the addition of a supervised cost and an unsupervised cross-covariance penalty, our model learns to disentangle various transformations using standard feedforward neural network components. We show the model can make use of labeled and unlabled data simultaneously. Furthermore, the decoder of our model implicitly learns to generate novel manipulations of images on multiple sets of transformation variables. We show deep feedforward networks are capable of learning higher-order factors of variation beyond the supervised labels without the need to explicitly define these higher-order interactions."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to acknowledge everyone at the Redwood Center and beyond for their helpful discussion and comments. We thank Nervana Systems for supporting Brian Cheung during the summer\nwhen this project originated and for their continued collaboration. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPUs used for this research. Bruno Olshausen was supported by NSF grant IIS-1111765.\nAPPENDIX\nCLASSIFICATION PERFORMACE\nTable 2 shows classification results for MNIST and TFD networks described in Table 3. With this model, it is possible to trade-off classification accuracy for reconstruction accuracy. Our methods are amenable to different nonlinearities such as maxout (Goodfellow et al., 2013b) and different training algorithms such as SGD with momentum, SFO (Sohl-Dickstein et al., 2014), or Dropout (Srivastava et al., 2014). Performance of a fully connectedl maxout network used for the encoder similar to Goodfellow et al. (2013b) is also shown. Note that some models were selected based on classification accuracy rather than reconstruction accuracy."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Restoring an image taken through a window covered with dirt or rain", "author": ["Eigen", "David", "Krishnan", "Dilip", "Fergus", "Rob"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Eigen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2013}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Bilinear sparse coding for invariant vision", "author": ["Grimes", "David B", "Rao", "Rajesh PN"], "venue": "Neural computation,", "citeRegEx": "Grimes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Grimes et al\\.", "year": 2005}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Le and V.,? \\Q2013\\E", "shortCiteRegEx": "Le and V.", "year": 2013}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Lee", "Honglak", "Pham", "Peter", "Largman", "Yan", "Ng", "Andrew Y"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Bilinear models of natural images", "author": ["Olshausen", "Bruno A", "Cadieu", "Charles", "Culpepper", "Jack", "Warland", "David K"], "venue": "In Electronic Imaging", "citeRegEx": "Olshausen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 2007}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["Reed", "Scott", "Sohn", "Kihyuk", "Zhang", "Yuting", "Lee", "Honglak"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal", "Mirza", "Mehdi"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["Sohl-Dickstein", "Jascha", "Poole", "Ben", "Ganguli", "Surya"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "The toronto face database", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Separating style and content with bilinear models", "author": ["Tenenbaum", "Joshua B", "Freeman", "William T"], "venue": "Neural computation,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "One of the goals of representation learning is to find an efficient representation of input data that simplifies tasks such as object classification (Krizhevsky et al., 2012) or image restoration (Eigen et al.", "startOffset": 149, "endOffset": 174}, {"referenceID": 1, "context": ", 2012) or image restoration (Eigen et al., 2013).", "startOffset": 29, "endOffset": 49}, {"referenceID": 10, "context": "identity using higher-order restricted Boltzmann machines (Reed et al., 2014).", "startOffset": 58, "endOffset": 77}, {"referenceID": 11, "context": "Autoencoder models have been shown to be useful for a variety of machine learning tasks (Rifai et al., 2011; Vincent et al., 2010; Le, 2013).", "startOffset": 88, "endOffset": 140}, {"referenceID": 18, "context": "Autoencoder models have been shown to be useful for a variety of machine learning tasks (Rifai et al., 2011; Vincent et al., 2010; Le, 2013).", "startOffset": 88, "endOffset": 140}, {"referenceID": 3, "context": "For example, Kingma et al. (2014) utilized a variational autoencoder in a semi-supervised learning paradigm which learned to separate content and style in data.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "Bengio et al. (2007) showed that greedy layerwise pre-training can speed up the convergence and improve generalization capabilities of deep networks.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Rifai et al. (2012) proposed a similar penalty in Contractive Discriminant Analysis method which penalized the cross-derivatives between sets of supervised and unsupervised latent variables with respect to the input.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "2 TORONTO FACES DATABASE The Toronto Faces Database (Susskind et al., 2010) consists of 102,236 grayscale face images of size 48x48.", "startOffset": 52, "endOffset": 75}, {"referenceID": 11, "context": "Inspired by Rifai et al. (2011), for the next two layers, we plot the singular value spectrum.", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "It was possible to train this autoencoder without pre-training because of the additional gradient information injected by the supervised and covariance cost at the end of the encoder similar to Szegedy et al. (2014). In Figure 8, we show the images generated by the decoder while iterating through each camera pose.", "startOffset": 194, "endOffset": 216}, {"referenceID": 13, "context": ", 2013b) and different training algorithms such as SGD with momentum, SFO (Sohl-Dickstein et al., 2014), or Dropout (Srivastava et al.", "startOffset": 74, "endOffset": 103}, {"referenceID": 2, "context": "Our methods are amenable to different nonlinearities such as maxout (Goodfellow et al., 2013b) and different training algorithms such as SGD with momentum, SFO (Sohl-Dickstein et al., 2014), or Dropout (Srivastava et al., 2014). Performance of a fully connectedl maxout network used for the encoder similar to Goodfellow et al. (2013b) is also shown.", "startOffset": 69, "endOffset": 336}], "year": 2017, "abstractText": "We propose a method for learning latent representations of the factors of variation in data. By augmenting deep autoencoders with a supervised cost and an additional unsupervised cost, we create a semi-supervised model that can discover and explicitly represent factors of variation beyond those relevant for categorization. We use a novel unsupervised covariance penalty (XCov) to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Our model discovers additional high-level latent factors absent from the supervised signal.", "creator": "LaTeX with hyperref package"}}}