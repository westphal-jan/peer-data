{"id": "1412.6651", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Deep learning with Elastic Averaging SGD", "abstract": "We study the tomiyama problem bj\u00f8rgvin of stochastic optimization egans for deep learning wpbf in dowsing the parallel computing environment zpass under ladera communication constraints. mooting A new carlstrom algorithm is proposed in this ranheim setting jittering where serotina the elsheimer communication and coordination neritidae of work euro95 among concurrent m\u00e1ximo processes (local bobrov workers ), is xiushui based thesis on an elastic deadhorse force which abarkuh links the dae-jung parameter agrega vectors guzm\u00e1n they compute matzos with luba a antonius center variable stored by the raducan parameter 485,000 server (all-league master ). The steinfels algorithm alcine enables 124.85 the local saajan workers attleboro to gloria perform more exploration, thirumangai i. whenever e. minifigures the algorithm allows banovi\u0107 the local variables to fluctuate andreyeva further from monterosso the center caus variable by reducing the 1,226 amount of schaerer communication 04:30 between jklf local workers frameless and absorbs the master. gabali We empirically demonstrate that standing-room in jap the tortola deep secom learning setting, due o'more to the existence 119.10 of grewcock many cramer local gaj optima, followthrough allowing 123.80 more non-greek exploration can mette-marit lead to the switch-off improved kungyangon performance. truly We propose pinchon synchronous asyut and dankook asynchronous variants vivo of the new algorithm. rizwan We transgressor provide stirlingshire the morwood theoretical washburn analysis of t-neck the endowing synchronous arikara variant in singles the shamarpa quadratic case and prove powhatan it lvt achieves the basc highest pueschel possible asymptotic rate of convergence mahajana for sonet/sdh the center variable. kavi We additionally melieria propose the momentum - based version dysart of ruogu the loveliness algorithm zinsou that two-hybrid can be applied iriscan in 70.93 both misgivings synchronous chernorechye and montilla asynchronous settings. winster An 1-and-2 asynchronous winsen variant kappa of the portageville algorithm bel-imperia is likun applied eleveld to train convolutional neural networks joseon for 279th image parasailing classification on allaway the merchantmen CIFAR noosphere and hi-lo ImageNet vishwanathan datasets. Experiments zone-based demonstrate that the glucosinolates new sub-group algorithm accelerates a87 the training of lavrov deep illig architectures compared to DOWNPOUR and other dysphoric common baseline cadwalader approaches and jsdf furthermore hamri is prommin very communication efficient.", "histories": [["v1", "Sat, 20 Dec 2014 13:22:23 GMT  (2167kb,D)", "https://arxiv.org/abs/1412.6651v1", "submitted to iclr2015"], ["v2", "Mon, 29 Dec 2014 20:50:02 GMT  (2167kb,D)", "http://arxiv.org/abs/1412.6651v2", "submitted to iclr2015"], ["v3", "Mon, 5 Jan 2015 01:18:40 GMT  (4782kb,D)", "http://arxiv.org/abs/1412.6651v3", "submitted to iclr2015. The plots of ADOWNPOUR method in Figure 1 turned out to be the DOWNPOUR method. The current (3rd) version fixes this"], ["v4", "Wed, 25 Feb 2015 19:00:29 GMT  (1123kb,D)", "http://arxiv.org/abs/1412.6651v4", "submitted to iclr2015, version after rebuttal"], ["v5", "Wed, 29 Apr 2015 11:56:24 GMT  (1123kb,D)", "http://arxiv.org/abs/1412.6651v5", "submitted to iclr2015, workshop version"], ["v6", "Sat, 6 Jun 2015 00:20:58 GMT  (1289kb,D)", "http://arxiv.org/abs/1412.6651v6", "NIPS2015 Submission"], ["v7", "Sat, 8 Aug 2015 02:52:48 GMT  (1290kb,D)", "http://arxiv.org/abs/1412.6651v7", "NIPS2015 Submission(r)"], ["v8", "Sun, 25 Oct 2015 12:12:52 GMT  (3648kb,D)", "http://arxiv.org/abs/1412.6651v8", "NIPS2015 camera-ready version"]], "COMMENTS": "submitted to iclr2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sixin zhang", "anna choromanska", "yann lecun"], "accepted": true, "id": "1412.6651"}, "pdf": {"name": "1412.6651.pdf", "metadata": {"source": "CRF", "title": "Deep learning with Elastic Averaging SGD", "authors": ["Sixin Zhang", "Anna Choromanska"], "emails": ["zsx@cims.nyu.edu", "achoroma@cims.nyu.edu", "yann@cims.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "One of the most challenging problems in large-scale machine learning is how to parallelize the training of large models that use a form of stochastic gradient descent (SGD) [1]. There have been attempts to parallelize SGD-based training for large-scale deep learning models on large number of CPUs, including the Google\u2019s Distbelief system [2]. But practical image recognition systems consist of large-scale convolutional neural networks trained on few GPU cards sitting in a single computer [3, 4]. The main challenge is to devise parallel SGD algorithms to train large-scale deep learning models that yield a significant speedup when run on multiple GPU cards.\nIn this paper we introduce the Elastic Averaging SGD method (EASGD) and its variants. EASGD is motivated by quadratic penalty method [5], but is re-interpreted as a parallelized extension of the averaging SGD algorithm [6]. The basic idea is to let each worker maintain its own local parameter, and the communication and coordination of work among the local workers is based on an elastic force which links the parameters they compute with a center variable stored by the master. The center variable is updated as a moving average where the average is taken in time and also in space over the parameters computed by local workers. The main contribution of this paper is a new algorithm that provides fast convergent minimization while outperforming DOWNPOUR method [2] and other\nar X\niv :1\n41 2.\n66 51\nv8 [\ncs .L\nG ]\n2 5\nO ct\nbaseline approaches in practice. Simultaneously it reduces the communication overhead between the master and the local workers while at the same time it maintains high-quality performance measured by the test error. The new algorithm applies to deep learning settings such as parallelized training of convolutional neural networks.\nThe article is organized as follows. Section 2 explains the problem setting, Section 3 presents the synchronous EASGD algorithm and its asynchronous and momentum-based variants, Section 4 provides stability analysis of EASGD and ADMM in the round-robin scheme, Section 5 shows experimental results and Section 6 concludes. The Supplement contains additional material including additional theoretical analysis."}, {"heading": "2 Problem setting", "text": "Consider minimizing a function F (x) in a parallel computing environment [7] with p \u2208 N workers and a master. In this paper we focus on the stochastic optimization problem of the following form\nmin x F (x) := E[f(x, \u03be)], (1)\nwhere x is the model parameter to be estimated and \u03be is a random variable that follows the probability distribution P over \u2126 such that F (x) = \u222b \u2126 f(x, \u03be)P(d\u03be). The optimization problem in Equation 1 can be reformulated as follows\nmin x1,...,xp,x\u0303 p\u2211 i=1 E[f(xi, \u03bei)] + \u03c1 2 \u2016xi \u2212 x\u0303\u20162, (2)\nwhere each \u03bei follows the same distribution P (thus we assume each worker can sample the entire dataset). In the paper we refer to xi\u2019s as local variables and we refer to x\u0303 as a center variable. The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [8, 9]. The quadratic penalty term \u03c1 in Equation 2 is expected to ensure that local workers will not fall into different attractors that are far away from the center variable. This paper focuses on the problem of reducing the parameter communication overhead between the master and local workers [10, 2, 11, 12, 13]. The problem of data communication when the data is distributed among the workers [7, 14] is a more general problem and is not addressed in this work. We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [15]."}, {"heading": "3 EASGD update rule", "text": "The EASGD updates captured in resp. Equation 3 and 4 are obtained by taking the gradient descent step on the objective in Equation 2 with respect to resp. variable xi and x\u0303,\nxit+1 = x i t \u2212 \u03b7(git(xit) + \u03c1(xit \u2212 x\u0303t)) (3)\nx\u0303t+1 = x\u0303t + \u03b7 p\u2211 i=1 \u03c1(xit \u2212 x\u0303t), (4)\nwhere git(x i t) denotes the stochastic gradient of F with respect to x i evaluated at iteration t, xit and x\u0303t denote respectively the value of variables xi and x\u0303 at iteration t, and \u03b7 is the learning rate.\nThe update rule for the center variable x\u0303 takes the form of moving average where the average is taken over both space and time. Denote \u03b1 = \u03b7\u03c1 and \u03b2 = p\u03b1, then Equation 3 and 4 become\nxit+1 = x i t \u2212 \u03b7git(xit)\u2212 \u03b1(xit \u2212 x\u0303t) (5)\nx\u0303t+1 = (1\u2212 \u03b2)x\u0303t + \u03b2\n( 1\np p\u2211 i=1 xit\n) . (6)\nNote that choosing \u03b2 = p\u03b1 leads to an elastic symmetry in the update rule, i.e. there exists an symmetric force equal to \u03b1(xit \u2212 x\u0303t) between the update of each xi and x\u0303. It has a crucial influence on the algorithm\u2019s stability as will be explained in Section 4. Also in order to minimize the staleness [16] of the difference xit \u2212 x\u0303t between the center and the local variable, the update for the master in Equation 4 involves xit instead of x i t+1.\nNote also that \u03b1 = \u03b7\u03c1, where the magnitude of \u03c1 represents the amount of exploration we allow in the model. In particular, small \u03c1 allows for more exploration as it allows xi\u2019s to fluctuate further from the center x\u0303. The distinctive idea of EASGD is to allow the local workers to perform more exploration (small \u03c1) and the master to perform exploitation. This approach differs from other settings explored in the literature [2, 17, 18, 19, 20, 21, 22, 23], and focus on how fast the center variable converges. In this paper we show the merits of our approach in the deep learning setting."}, {"heading": "3.1 Asynchronous EASGD", "text": "We discussed the synchronous update of EASGD algorithm in the previous section. In this section we propose its asynchronous variant. The local workers are still responsible for updating the local variables xi\u2019s, whereas the master is updating the center variable x\u0303. Each worker maintains its own clock ti, which starts from 0 and is incremented by 1 after each stochastic gradient update of xi as shown in Algorithm 1. The master performs an update whenever the local workers finished \u03c4 steps of their gradient updates, where we refer to \u03c4 as the communication period. As can be seen in Algorithm 1, whenever \u03c4 divides the local clock of the ith worker, the ith worker communicates with the master and requests the current value of the center variable x\u0303. The worker then waits until the master sends back the requested parameter value, and computes the elastic difference \u03b1(x\u2212 x\u0303) (this entire procedure is captured in step a) in Algorithm 1). The elastic difference is then sent back to the master (step b) in Algorithm 1) who then updates x\u0303.\nThe communication period \u03c4 controls the frequency of the communication between every local worker and the master, and thus the trade-off between exploration and exploitation.\nAlgorithm 1: Asynchronous EASGD: Processing by worker i and the master\nInput: learning rate \u03b7, moving rate \u03b1, communication period \u03c4 \u2208 N Initialize: x\u0303 is initialized randomly, xi = x\u0303, ti = 0 Repeat x\u2190 xi if (\u03c4 divides ti) then\na) xi \u2190 xi \u2212 \u03b1(x\u2212 x\u0303) b) x\u0303 \u2190 x\u0303 + \u03b1(x\u2212 x\u0303)\nend xi \u2190 xi \u2212 \u03b7giti(x) ti \u2190 ti + 1\nUntil forever\nAlgorithm 2: Asynchronous EAMSGD: Processing by worker i and the master\nInput: learning rate \u03b7, moving rate \u03b1, communication period \u03c4 \u2208 N, momentum term \u03b4 Initialize: x\u0303 is initialized randomly, xi = x\u0303, vi = 0, ti = 0 Repeat x\u2190 xi if (\u03c4 divides ti) then\na) xi \u2190 xi \u2212 \u03b1(x\u2212 x\u0303) b) x\u0303 \u2190 x\u0303 + \u03b1(x\u2212 x\u0303)\nend vi \u2190 \u03b4vi \u2212 \u03b7giti(x+ \u03b4v\ni) xi \u2190 xi + vi ti \u2190 ti + 1\nUntil forever"}, {"heading": "3.2 Momentum EASGD", "text": "The momentum EASGD (EAMSGD) is a variant of our Algorithm 1 and is captured in Algorithm 2. It is based on the Nesterov\u2019s momentum scheme [24, 25, 26], where the update of the local worker of the form captured in Equation 3 is replaced by the following update\nvit+1 = \u03b4v i t \u2212 \u03b7git(xit + \u03b4vit) (7)\nxit+1 = x i t + v i t+1 \u2212 \u03b7\u03c1(xit \u2212 x\u0303t),\nwhere \u03b4 is the momentum term. Note that when \u03b4 = 0 we recover the original EASGD algorithm.\nAs we are interested in reducing the communication overhead in the parallel computing environment where the parameter vector is very large, we will be exploring in the experimental section the asynchronous EASGD algorithm and its momentum-based variant in the relatively large \u03c4 regime (less frequent communication)."}, {"heading": "4 Stability analysis of EASGD and ADMM in the round-robin scheme", "text": "In this section we study the stability of the asynchronous EASGD and ADMM methods in the roundrobin scheme [20]. We first state the updates of both algorithms in this setting, and then we study\ntheir stability. We will show that in the one-dimensional quadratic case, ADMM algorithm can exhibit chaotic behavior, leading to exponential divergence. The analytic condition for the ADMM algorithm to be stable is still unknown, while for the EASGD algorithm it is very simple1.\nThe analysis of the synchronous EASGD algorithm, including its convergence rate, and its averaging property, in the quadratic and strongly convex case, is deferred to the Supplement.\nIn our setting, the ADMM method [9, 27, 28] involves solving the following minimax problem2,\nmax \u03bb1,...,\u03bbp min x1,...,xp,x\u0303 p\u2211 i=1 F (xi)\u2212 \u03bbi(xi \u2212 x\u0303) + \u03c1 2 \u2016xi \u2212 x\u0303\u20162, (8)\nwhere \u03bbi\u2019s are the Lagrangian multipliers. The resulting updates of the ADMM algorithm in the round-robin scheme are given next. Let t \u2265 0 be a global clock. At each t, we linearize the function F (xi) with F (xit) + \u2329 \u2207F (xit), xi \u2212 xit \u232a + 12\u03b7 \u2225\u2225xi \u2212 xit\u2225\u22252 as in [28]. The updates become \u03bbit+1 = { \u03bbit \u2212 (xit \u2212 x\u0303t) if mod (t, p) = i\u2212 1; \u03bbit if mod (t, p) 6= i\u2212 1. (9)\nxit+1 =\n{ xit\u2212\u03b7\u2207F (x i t)+\u03b7\u03c1(\u03bb i t+1+x\u0303t)\n1+\u03b7\u03c1 if mod (t, p) = i\u2212 1; xit if mod (t, p) 6= i\u2212 1.\n(10)\nx\u0303t+1 = 1\np p\u2211 i=1 (xit+1 \u2212 \u03bbit+1). (11)\nEach local variable xi is periodically updated (with period p). First, the Lagrangian multiplier \u03bbi is updated with the dual ascent update as in Equation 9. It is followed by the gradient descent update of the local variable as given in Equation 10. Then the center variable x\u0303 is updated with the most recent values of all the local variables and Lagrangian multipliers as in Equation 11. Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [9, 27, 28], we have re-parametrized the Lagrangian multiplier to be \u03bbit \u2190 \u03bbit/\u03c1 in the above updates. The EASGD algorithm in the round-robin scheme is defined similarly and is given below\nxit+1 = { xit \u2212 \u03b7\u2207F (xit)\u2212 \u03b1(xit \u2212 x\u0303t) if mod (t, p) = i\u2212 1; xit if mod (t, p) 6= i\u2212 1.\n(12)\nx\u0303t+1 = x\u0303t + \u2211\ni: mod (t,p)=i\u22121\n\u03b1(xit \u2212 x\u0303t). (13)\nAt time t, only the i-th local worker (whose index i\u22121 equals tmodulo p) is activated, and performs the update in Equations 12 which is followed by the master update given in Equation 13.\nWe will now focus on the one-dimensional quadratic case without noise, i.e. F (x) = x 2\n2 , x \u2208 R.\nFor the ADMM algorithm, let the state of the (dynamical) system at time t be st = (\u03bb1t , x 1 t , . . . , \u03bb p t , x p t , x\u0303t) \u2208 R2p+1. The local worker i\u2019s updates in Equations 9, 10, and 11 are composed of three linear maps which can be written as st+1 = (F i3 \u25e6 F i2 \u25e6 F i1)(st). For simplicity, we will only write them out below for the case when i = 1 and p = 2:\nF 11=  1 \u22121 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 , F 12=  1 0 0 0 0 \u03b7\u03c1 1+\u03b7\u03c1 1\u2212\u03b7 1+\u03b7\u03c1 0 0 \u03b7\u03c1 1+\u03b7\u03c1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 , F 13=  1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 \u2212 1 p 1 p \u2212 1 p 1 p 0 .\nFor each of the p linear maps, it\u2019s possible to find a simple condition such that each map, where the ith map has the form F i3 \u25e6 F i2 \u25e6 F i1, is stable (the absolute value of the eigenvalues of the map are\n1This condition resembles the stability condition for the synchronous EASGD algorithm (Condition 17 for p = 1) in the analysis in the Supplement.\n2The convergence analysis in [27] is based on the assumption that \u201cAt any master iteration, updates from the workers have the same probability of arriving at the master.\u201d, which is not satisfied in the round-robin scheme.\nsmaller or equal to one). However, when these non-symmetric maps are composed one after another as follows F = F p3 \u25e6F p 2 \u25e6F p 1 \u25e6 . . .\u25e6F 13 \u25e6F 12 \u25e6F 11 , the resulting map F can become unstable! (more precisely, some eigenvalues of the map can sit outside the unit circle in the complex plane).\nWe now present the numerical conditions for which the ADMM algorithm becomes unstable in the round-robin scheme for p = 3 and p = 8, by computing the largest absolute eigenvalue of the map F . Figure 1 summarizes the obtained result.\nOn the other hand, the EASGD algorithm involves composing only symmetric linear maps due to the elasticity. Let the state of the (dynamical) system at time t be st = (x1t , . . . , x p t , x\u0303t) \u2208 Rp+1. The activated local worker i\u2019s update in Equation 12 and the master update in Equation 13 can be written as st+1 = F i(st). In case of p = 2, the map F 1 and F 2 are defined as follows\nF 1=\n( 1\u2212 \u03b7 \u2212 \u03b1 0 \u03b1\n0 1 0 \u03b1 0 1\u2212 \u03b1\n) , F 2= ( 1 0 0 0 1\u2212 \u03b7 \u2212 \u03b1 \u03b1 0 \u03b1 1\u2212 \u03b1 ) For the composite map F p \u25e6 . . . \u25e6 F 1 to be stable, the condition that needs to be satisfied is actually the same for each i, and is furthermore independent of p (since each linear map F i is symmetric).\nIt essentially involves the stability of the 2 \u00d7 2 matrix (\n1\u2212 \u03b7 \u2212 \u03b1 \u03b1 \u03b1 1\u2212 \u03b1\n) , whose two (real)\neigenvalues \u03bb satisfy (1\u2212 \u03b7 \u2212 \u03b1\u2212 \u03bb)(1\u2212 \u03b1\u2212 \u03bb) = \u03b12. The resulting stability condition (|\u03bb| \u2264 1) is simple and given as 0 \u2264 \u03b7 \u2264 2, 0 \u2264 \u03b1 \u2264 4\u22122\u03b74\u2212\u03b7 ."}, {"heading": "5 Experiments", "text": "In this section we compare the performance of EASGD and EAMSGD with the parallel method DOWNPOUR and the sequential method SGD, as well as their averaging and momentum variants.\nAll the parallel comparator methods are listed below3:\n\u2022 DOWNPOUR [2], the pseudo-code of the implementation of DOWNPOUR used in this paper is enclosed in the Supplement. \u2022 Momentum DOWNPOUR (MDOWNPOUR), where the Nesterov\u2019s momentum scheme is applied to the master\u2019s update (note it is unclear how to apply it to the local workers or for the case when \u03c4 > 1). The pseudo-code is in the Supplement. \u2022 A method that we call ADOWNPOUR, where we compute the average over time of the center variable x\u0303 as follows: zt+1 = (1\u2212 \u03b1t+1)zt + \u03b1t+1x\u0303t, and \u03b1t+1 = 1t+1 is a moving rate, and z0 = x\u03030. t denotes the master clock, which is initialized to 0 and incremented every time the center variable x\u0303 is updated. \u2022 A method that we call MVADOWNPOUR, where we compute the moving average of the\ncenter variable x\u0303 as follows: zt+1 = (1 \u2212 \u03b1)zt + \u03b1x\u0303t, and the moving rate \u03b1 was chosen to be constant, and z0 = x\u03030. t denotes the master clock and is defined in the same way as for the ADOWNPOUR method.\n3We have compared asynchronous ADMM [27] with EASGD in our setting as well, the performance is nearly the same. However, ADMM\u2019s momentum variant is not as stable for large communication periods.\nAll the sequential comparator methods (p = 1) are listed below:\n\u2022 SGD [1] with constant learning rate \u03b7. \u2022 Momentum SGD (MSGD) [26] with constant momentum \u03b4. \u2022 ASGD [6] with moving rate \u03b1t+1 = 1t+1 . \u2022 MVASGD [6] with moving rate \u03b1 set to a constant.\nWe perform experiments in a deep learning setting on two benchmark datasets: CIFAR-10 (we refer to it as CIFAR) 4 and ImageNet ILSVRC 2013 (we refer to it as ImageNet) 5. We focus on the image classification task with deep convolutional neural networks. We next explain the experimental setup. The details of the data preprocessing and prefetching are deferred to the Supplement."}, {"heading": "5.1 Experimental setup", "text": "For all our experiments we use a GPU-cluster interconnected with InfiniBand. Each node has 4 Titan GPU processors where each local worker corresponds to one GPU processor. The center variable of the master is stored and updated on the centralized parameter server [2]6.\nTo describe the architecture of the convolutional neural network, we will first introduce a notation. Let (c, y) denotes the size of the input image to each layer, where c is the number of color channels and y is both the horizontal and the vertical dimension of the input. Let C denotes the fully-connected convolutional operator and let P denotes the max pooling operator, D denotes the linear operator with dropout rate equal to 0.5 and S denotes the linear operator with softmax output non-linearity. We use the cross-entropy loss and all inner layers use rectified linear units. For the ImageNet experiment we use the similar approach to [4] with the following 11-layer convolutional neural network (3,221)C(96,108)P(96,36)C(256,32)P(256,16)C(384,14) C(384,13)C(256,12)P(256,6)D(4096,1)D(4096,1)S(1000,1). For the CIFAR experiment we use the similar approach to [29] with the following 7-layer convolutional neural network (3,28)C(64,24)P(64,12)C(128,8)P(128,4)C(64,2)D(256,1)S(10,1).\nIn our experiments all the methods we run use the same initial parameter chosen randomly, except that we set all the biases to zero for CIFAR case and to 0.1 for ImageNet case. This parameter is used to initialize the master and all the local workers7. We add l2-regularization \u03bb2 \u2016x\u2016\n2 to the loss function F (x). For ImageNet we use \u03bb = 10\u22125 and for CIFAR we use \u03bb = 10\u22124. We also compute the stochastic gradient using mini-batches of sample size 128."}, {"heading": "5.2 Experimental results", "text": "For all experiments in this section we use EASGD with \u03b2 = 0.98 , for all momentum-based methods we set the momentum term \u03b4 = 0.99 and finally for MVADOWNPOUR we set the moving rate to \u03b1 = 0.001. We start with the experiment on CIFAR dataset with p = 4 local workers running on a single computing node. For all the methods, we examined the communication periods from the following set \u03c4 = {1, 4, 16, 64}. For comparison we also report the performance of MSGD which outperformed SGD, ASGD and MVASGD as shown in Figure 6 in the Supplement. For each method we examined a wide range of learning rates (the learning rates explored in all experiments are summarized in Table 1, 2, 3 in the Supplement). The CIFAR experiment was run 3 times independently from the same initialization and for each method we report its best performance measured by the smallest achievable test error. From the results in Figure 2, we conclude that all DOWNPOURbased methods achieve their best performance (test error) for small \u03c4 (\u03c4 \u2208 {1, 4}), and become highly unstable for \u03c4 \u2208 {16, 64}. While EAMSGD significantly outperforms comparator methods for all values of \u03c4 by having faster convergence. It also finds better-quality solution measured by the test error and this advantage becomes more significant for \u03c4 \u2208 {16, 64}. Note that the tendency to achieve better test performance with larger \u03c4 is also characteristic for the EASGD algorithm.\n4Downloaded from http://www.cs.toronto.edu/\u02dckriz/cifar.html. 5Downloaded from http://image-net.org/challenges/LSVRC/2013. 6Our implementation is available at https://github.com/sixin-zh/mpiT. 7On the contrary, initializing the local workers and the master with different random seeds \u2019traps\u2019 the algo-\nrithm in the symmetry breaking phase. 8Intuitively the \u2019effective \u03b2\u2019 is \u03b2/\u03c4 = p\u03b1 = p\u03b7\u03c1 (thus \u03c1 = \u03b2\n\u03c4p\u03b7 ) in the asynchronous setting.\nWe next explore different number of local workers p from the set p = {4, 8, 16} for the CIFAR experiment, and p = {4, 8} for the ImageNet experiment9. For the ImageNet experiment we report the results of one run with the best setting we have found. EASGD and EAMSGD were run with \u03c4 = 10 whereas DOWNPOUR and MDOWNPOUR were run with \u03c4 = 1. The results are in Figure 3 and 4. For the CIFAR experiment, it\u2019s noticeable that the lowest achievable test error by either EASGD or EAMSGD decreases with larger p. This can potentially be explained by the fact that larger p allows for more exploration of the parameter space. In the Supplement, we discuss further the trade-off between exploration and exploitation as a function of the learning rate (section 9.5) and the communication period (section 9.6). Finally, the results obtained for the ImageNet experiment also shows the advantage of EAMSGD over the competitor methods."}, {"heading": "6 Conclusion", "text": "In this paper we describe a new algorithm called EASGD and its variants for training deep neural networks in the stochastic setting when the computations are parallelized over multiple GPUs. Experiments demonstrate that this new algorithm quickly achieves improvement in test error compared to more common baseline approaches such as DOWNPOUR and its variants. We show that our approach is very stable and plausible under communication constraints. We provide the stability analysis of the asynchronous EASGD in the round-robin scheme, and show the theoretical advantage of the method over ADMM. The different behavior of the EASGD algorithm from its momentumbased variant EAMSGD is intriguing and will be studied in future works.\n9For the ImageNet experiment, the training loss is measured on a subset of the training data of size 50,000."}, {"heading": "Acknowledgments", "text": "The authors thank R. Power, J. Li for implementation guidance, J. Bruna, O. Henaff, C. Farabet, A. Szlam, Y. Bakhtin for helpful discussion, P. L. Combettes, S. Bengio and the referees for valuable feedback."}, {"heading": "7 Additional theoretical results and proofs", "text": ""}, {"heading": "7.1 Quadratic case", "text": "We provide here the convergence analysis of the synchronous EASGD algorithm with constant learning rate. The analysis is focused on the convergence of the center variable to the local optimum. We discuss one-dimensional quadratic case first, then the generalization to multi-dimensional setting (Lemma 7.3) and finally to the strongly convex case (Theorem 7.1).\nOur analysis in the quadratic case extends the analysis of ASGD in [6]. Assume each of the p local workers xit \u2208 Rn observes a noisy gradient at time t \u2265 0 of the linear form given in Equation 14.\ngit(x i t) = Ax i t \u2212 b\u2212 \u03beit, i \u2208 {1, . . . , p}, (14)\nwhere the matrix A is positive-definite (each eigenvalue is strictly positive) and {\u03beit}\u2019s are i.i.d. random variables, with zero mean and positive-definite covariance \u03a3. Let x\u2217 denote the optimum solution, where x\u2217 = A\u22121b \u2208 Rn. In this section we analyze the behavior of the mean squared error (MSE) of the center variable x\u0303t, where this error is denoted as E[\u2016x\u0303t \u2212 x\u2217\u20162], as a function of t, p, \u03b7, \u03b1 and \u03b2, where \u03b2 = p\u03b1. Note that the MSE error can be decomposed as (squared) bias and variance10: E[\u2016x\u0303t \u2212 x\u2217\u20162] = \u2016E[x\u0303t \u2212 x\u2217]\u20162 + V[x\u0303t \u2212 x\u2217]. For one-dimensional case (n = 1), we assume A = h > 0 and \u03a3 = \u03c32 > 0. Lemma 7.1. Let x\u03030 and {xi0}i=1,...,p be arbitrary constants, then\nE[x\u0303t \u2212 x\u2217] = \u03b3t(x\u03030 \u2212 x\u2217) + \u03b3t \u2212 \u03c6t\n\u03b3 \u2212 \u03c6 \u03b1u0, (15)\nV[x\u0303t \u2212 x\u2217] = p2\u03b12\u03b72\n(\u03b3 \u2212 \u03c6)2\n( \u03b32 \u2212 \u03b32t\n1\u2212 \u03b32 + \u03c62 \u2212 \u03c62t 1\u2212 \u03c62 \u2212 2\u03b3\u03c6\u2212 (\u03b3\u03c6) t 1\u2212 \u03b3\u03c6\n) \u03c32\np , (16)\nwhere u0 = \u2211p i=1(x i 0\u2212x\u2217\u2212 \u03b11\u2212p\u03b1\u2212\u03c6 (x\u03030\u2212x \u2217)), a = \u03b7h+(p+1)\u03b1, c2 = \u03b7hp\u03b1, \u03b3 = 1\u2212 a\u2212 \u221a a2\u22124c2 2 , and \u03c6 = 1\u2212 a+ \u221a a2\u22124c2 2 .\nIt follows from Lemma 7.1 that for the center variable to be stable the following has to hold \u22121 < \u03c6 < \u03b3 < 1. (17)\nIt can be verified that \u03c6 and \u03b3 are the two zero-roots of the polynomial in \u03bb: \u03bb2 \u2212 (2\u2212 a)\u03bb+ (1\u2212 a+ c2). Recall that \u03c6 and \u03bb are the functions of \u03b7 and \u03b1. Thus (see proof in Section 7.1.2)\n\u2022 \u03b3 < 1 iff c2 > 0 (i.e. \u03b7 > 0 and \u03b1 > 0). \u2022 \u03c6 > \u22121 iff (2\u2212 \u03b7h)(2\u2212 p\u03b1) > 2\u03b1 and (2\u2212 \u03b7h) + (2\u2212 p\u03b1) > \u03b1. \u2022 \u03c6 = \u03b3 iff a2 = 4c2 (i.e. \u03b7h = \u03b1 = 0).\nThe proof the above Lemma is based on the diagonalization of the linear gradient map (this map is symmetric due to the relation \u03b2 = p\u03b1). The stability analysis of the asynchronous EASGD algorithm in the round-robin scheme is similar due to this elastic symmetry.\nProof. Substituting the gradient from Equation 14 into the update rule used by each local worker in the synchronous EASGD algorithm (Equation 5 and 6) we obtain\nxit+1 = x i t \u2212 \u03b7(Axit \u2212 b\u2212 \u03beit)\u2212 \u03b1(xit \u2212 x\u0303t), (18)\nx\u0303t+1 = x\u0303t + p\u2211 i=1 \u03b1(xit \u2212 x\u0303t), (19)\n10In our notation, V denotes the variance.\nwhere \u03b7 is the learning rate, and \u03b1 is the moving rate. Recall that \u03b1 = \u03b7\u03c1 and A = h.\nFor the ease of notation we redefine x\u0303t and xit as follows:\nx\u0303t , x\u0303t \u2212 x\u2217 and xit , xit \u2212 x\u2217. We prove the lemma by explicitly solving the linear equations 18 and 19. Let xt = (x1t , . . . , x p t , x\u0303t) T . We rewrite the recursive relation captured in Equation 18 and 19 as simply\nxt+1 = Mxt + bt,\nwhere the drift matrix M is defined as\nM =  1\u2212 \u03b1\u2212 \u03b7h 0 ... 0 \u03b1\n0 1\u2212 \u03b1\u2212 \u03b7h 0 ... \u03b1 ... 0 ... 0 ... 0 ... 0 1\u2212 \u03b1\u2212 \u03b7h \u03b1 \u03b1 \u03b1 ... \u03b1 1\u2212 p\u03b1  , and the (diffusion) vector bt = (\u03b7\u03be1t , . . . , \u03b7\u03be p t , 0) T .\nNote that one of the eigenvalues of matrixM , that we call \u03c6, satisfies (1\u2212\u03b1\u2212\u03b7h\u2212\u03c6)(1\u2212p\u03b1\u2212\u03c6) = p\u03b12. The corresponding eigenvector is (1, 1, . . . , 1,\u2212 p\u03b11\u2212p\u03b1\u2212\u03c6 )\nT . Let ut be the projection of xt onto this eigenvector. Thus ut = \u2211p i=1(x i t \u2212 \u03b11\u2212p\u03b1\u2212\u03c6 x\u0303t). Let furthermore \u03bet = \u2211p i=1 \u03be i t . Therefore we have\nut+1 = \u03c6ut + \u03b7\u03bet. (20)\nBy combining Equation 19 and 20 as follows\nx\u0303t+1 = x\u0303t + p\u2211 i=1 \u03b1(xit \u2212 x\u0303t) = (1\u2212 p\u03b1)x\u0303t + \u03b1(ut + p\u03b1 1\u2212 p\u03b1\u2212 \u03c6 x\u0303t)\n= (1\u2212 p\u03b1+ p\u03b1 2\n1\u2212 p\u03b1\u2212 \u03c6 )x\u0303t + \u03b1ut = \u03b3x\u0303t + \u03b1ut,\nwhere the last step results from the following relations: p\u03b1 2\n1\u2212p\u03b1\u2212\u03c6 = 1 \u2212 \u03b1 \u2212 \u03b7h \u2212 \u03c6 and \u03c6 + \u03b3 = 1\u2212 \u03b1\u2212 \u03b7h+ 1\u2212 p\u03b1. Thus we obtained\nx\u0303t+1 = \u03b3x\u0303t + \u03b1ut. (21)\nBased on Equation 20 and 21, we can then expand ut and x\u0303t recursively,\nut+1 = \u03c6 t+1u0 + \u03c6 t(\u03b7\u03be0) + . . .+ \u03c6 0(\u03b7\u03bet), (22)\nx\u0303t+1 = \u03b3 t+1x\u03030 + \u03b3 t(\u03b1u0) + . . .+ \u03b3 0(\u03b1ut). (23)\nSubstituting u0, u1, . . . , ut, each given through Equation 22, into Equation 23 we obtain\nx\u0303t = \u03b3 tx\u03030 +\n\u03b3t \u2212 \u03c6t\n\u03b3 \u2212 \u03c6 \u03b1u0 + \u03b1\u03b7 t\u22121\u2211 l=1 \u03b3t\u2212l \u2212 \u03c6t\u2212l \u03b3 \u2212 \u03c6 \u03bel\u22121. (24)\nTo be more specific, the Equation 24 is obtained by integrating by parts,\nx\u0303t+1 = \u03b3 t+1x\u03030 + t\u2211 i=0 \u03b3t\u2212i(\u03b1ui)\n= \u03b3t+1x\u03030 + t\u2211 i=0 \u03b3t\u2212i(\u03b1(\u03c6iu0 + i\u22121\u2211 l=0 \u03c6i\u22121\u2212l\u03b7\u03bel))\n= \u03b3t+1x\u03030 + t\u2211 i=0 \u03b3t\u2212i\u03c6i(\u03b1u0) + t\u22121\u2211 l=0 t\u2211 i=l+1 \u03b3t\u2212i\u03c6i\u22121\u2212l(\u03b1\u03b7\u03bel)\n= \u03b3t+1x\u03030 + \u03b3t+1 \u2212 \u03c6t+1\n\u03b3 \u2212 \u03c6 (\u03b1u0) + t\u22121\u2211 l=0 \u03b3t\u2212l \u2212 \u03c6t\u2212l \u03b3 \u2212 \u03c6 (\u03b1\u03b7\u03bel).\nSince the random variables \u03bel are i.i.d, we may sum the variance term by term as follows\nt\u22121\u2211 l=0 ( \u03b3t\u2212l \u2212 \u03c6t\u2212l \u03b3 \u2212 \u03c6 )2 = t\u22121\u2211 l=0 \u03b32(t\u2212l) \u2212 2\u03b3t\u2212l\u03c6t\u2212l + \u03c62(t\u2212l) (\u03b3 \u2212 \u03c6)2\n= 1\n(\u03b3 \u2212 \u03c6)2\n( \u03b32 \u2212 \u03b32(t+1)\n1\u2212 \u03b32 \u2212 2\u03b3\u03c6\u2212 (\u03b3\u03c6) t+1 1\u2212 \u03b3\u03c6 + \u03c62 \u2212 \u03c62(t+1) 1\u2212 \u03c62\n) . (25)\nNote that E[\u03bet] = \u2211p i=1 E[\u03beit] = 0 and V[\u03bet] = \u2211p i=1 V[\u03beit] = p\u03c32. These two facts, the equality in Equation 24 and Equation 25 can then be used to compute E[x\u0303t] and V[x\u0303t] as given in Equation 15 and 16 in Lemma 7.1.\n7.1.1 Visualizing Lemma 7.1\nIn Figure 5, we illustrate the dependence of MSE on \u03b2, \u03b7 and the number of processors p over time t. We consider the large-noise setting where x\u03030 = xi0 = 1, h = 1 and \u03c3 = 10. The MSE error is color-coded such that the deep blue color corresponds to the MSE equal to 10\u22123, the green color corresponds to the MSE equal to 1, the red color corresponds to MSE equal to 103 and the dark red color corresponds to the divergence of algorithm EASGD (condition in Equation 17 is then violated). The plot shows that we can achieve significant variance reduction by increasing the number of local workers p. This effect is less sensitive to the choice of \u03b2 and \u03b7 for large p."}, {"heading": "7.1.2 Condition in Equation 17", "text": "We are going to show that\n\u2022 \u03b3 < 1 iff c2 > 0 (i.e. \u03b7 > 0 and \u03b2 > 0). \u2022 \u03c6 > \u22121 iff (2\u2212 \u03b7h)(2\u2212 \u03b2) > 2\u03b2/p and (2\u2212 \u03b7h) + (2\u2212 \u03b2) > \u03b2/p. \u2022 \u03c6 = \u03b3 iff a2 = 4c2 (i.e. \u03b7h = \u03b2 = 0).\nRecall that a = \u03b7h+ (p+ 1)\u03b1, c2 = \u03b7hp\u03b1, \u03b3 = 1\u2212 a\u2212 \u221a a2\u22124c2 2 , \u03c6 = 1\u2212 a+ \u221a a2\u22124c2 2 , and \u03b2 = p\u03b1. We have\n\u2022 \u03b3 < 1\u21d4 a\u2212 \u221a a2\u22124c2 2 > 0\u21d4 a > \u221a a2 \u2212 4c2 \u21d4 a2 > a2 \u2212 4c2 \u21d4 c2 > 0. \u2022 \u03c6 > \u22121\u21d4 2 > a+ \u221a a2\u22124c2 2 \u21d4 4\u2212 a > \u221a a2 \u2212 4c2 \u21d4 4\u2212 a > 0, (4\u2212 a)2 > a2 \u2212 4c2 \u21d4\n4\u2212 a > 0, 4\u2212 2a+ c2 > 0\u21d4 4 > \u03b7h+ \u03b2 + \u03b1, 4\u2212 2(\u03b7h+ \u03b2 + \u03b1) + \u03b7h\u03b2 > 0. \u2022 \u03c6 = \u03b3 \u21d4 \u221a a2 \u2212 4c2 = 0\u21d4 a2 = 4c2.\nThe next corollary is a consequence of Lemma 7.1. As the number of workers p grows, the averaging property of the EASGD can be characterized as follows Corollary 7.1. Let the Elastic Averaging relation \u03b2 = p\u03b1 and the condition 17 hold, then\nlim p\u2192\u221e lim t\u2192\u221e\npE[(x\u0303t \u2212 x\u2217)2] = \u03b2\u03b7h (2\u2212 \u03b2)(2\u2212 \u03b7h) \u00b7 2\u2212 \u03b2 \u2212 \u03b7h+ \u03b2\u03b7h \u03b2 + \u03b7h\u2212 \u03b2\u03b7h \u00b7 \u03c3 2 h2 .\nProof. Note that when \u03b2 is fixed, limp\u2192\u221e a = \u03b7h+ \u03b2 and c2 = \u03b7h\u03b2. Then limp\u2192\u221e \u03c6 = min(1\u2212 \u03b2, 1\u2212 \u03b7h) and limp\u2192\u221e \u03b3 = max(1\u2212 \u03b2, 1\u2212 \u03b7h). Also note that using Lemma 7.1 we obtain\nlim t\u2192\u221e\nE[(x\u0303t \u2212 x\u2217)2] = \u03b22\u03b72\n(\u03b3 \u2212 \u03c6)2\n( \u03b32\n1\u2212 \u03b32 +\n\u03c62\n1\u2212 \u03c62 \u2212 2\u03b3\u03c6 1\u2212 \u03b3\u03c6\n) \u03c32\np\n= \u03b22\u03b72\n(\u03b3 \u2212 \u03c6)2\n( \u03b32(1\u2212 \u03c62)(1\u2212 \u03c6\u03b3) + \u03c62(1\u2212 \u03b32)(1\u2212 \u03c6\u03b3)\u2212 2\u03b3\u03c6(1\u2212 \u03b32)(1\u2212 \u03c62)\n(1\u2212 \u03b32)(1\u2212 \u03c62)(1\u2212 \u03b3\u03c6)\n) \u03c32\np\n= \u03b22\u03b72\n(\u03b3 \u2212 \u03c6)2\n( (\u03b3 \u2212 \u03c6)2(1 + \u03b3\u03c6)\n(1\u2212 \u03b32)(1\u2212 \u03c62)(1\u2212 \u03b3\u03c6)\n) \u03c32\np\n= \u03b22\u03b72 (1\u2212 \u03b32)(1\u2212 \u03c62) \u00b7 1 + \u03b3\u03c6 1\u2212 \u03b3\u03c6 \u00b7 \u03c3 2 p .\nCorollary 7.1 is obtained by plugining in the limiting values of \u03c6 and \u03b3.\nThe crucial point of Corollary 7.1 is that the MSE in the limit t \u2192 \u221e is in the order of 1/p which implies that as the number of processors p grows, the MSE will decrease for the EASGD algorithm. Also note that the smaller the \u03b2 is (recall that \u03b2 = p\u03b1 = p\u03b7\u03c1), the more exploration is allowed (small \u03c1) and simultaneously the smaller the MSE is."}, {"heading": "7.2 Generalization to multidimensional case", "text": "The next lemma (Lemma 7.2) shows that EASGD algorithm achieves the highest possible rate of convergence when we consider the double averaging sequence (similarly to [6]) {z1, z2, . . . } defined as below\nzt+1 = 1\nt+ 1 t\u2211 k=0 x\u0303k. (26)\nLemma 7.2 (Weak convergence). If the condition in Equation 17 holds, then the normalized double averaging sequence defined in Equation 26 converges weakly to the normal distribution with zero mean and variance \u03c32/ph2,\n\u221a t(zt \u2212 x\u2217) \u21c0 N (0, \u03c32\nph2 ), t\u2192\u221e. (27)\nProof. As in the proof of Lemma 7.1, for the ease of notation we redefine x\u0303t and xit as follows:\nx\u0303t , x\u0303t \u2212 x\u2217 and xit , xit \u2212 x\u2217.\nAlso recall that {\u03beit}\u2019s are i.i.d. random variables (noise) with zero mean and the same covariance \u03a3 0. We are interested in the asymptotic behavior of the double averaging sequence {z1, z2, . . . } defined as\nzt+1 = 1\nt+ 1 t\u2211 k=0 x\u0303k. (28)\nRecall the Equation 24 from the proof of Lemma 7.1 (for the convenience it is provided below):\nx\u0303k = \u03b3 kx\u03030 + \u03b1u0\n\u03b3k \u2212 \u03c6k\n\u03b3 \u2212 \u03c6 + \u03b1\u03b7 k\u22121\u2211 l=1 \u03b3k\u2212l \u2212 \u03c6k\u2212l \u03b3 \u2212 \u03c6 \u03bel\u22121,\nwhere \u03bet = \u2211p i=1 \u03be i t . Therefore\nt\u2211 k=0 x\u0303k = 1\u2212 \u03b3t+1 1\u2212 \u03b3 x\u03030 + \u03b1u0 1 \u03b3 \u2212 \u00b5 ( 1\u2212 \u03b3t+1 1\u2212 \u03b3 \u2212 1\u2212 \u03c6 t+1 1\u2212 \u03c6 ) + \u03b1\u03b7 t\u22121\u2211 l=1 t\u2211 k=l+1 \u03b3k\u2212l \u2212 \u03c6k\u2212l \u03b3 \u2212 \u03c6 \u03bel\u22121\n= O(1) + \u03b1\u03b7 t\u22121\u2211 l=1 1 \u03b3 \u2212 \u03c6 ( \u03b3 1\u2212 \u03b3t\u2212l 1\u2212 \u03b3 \u2212 \u03c61\u2212 \u03c6 t\u2212l 1\u2212 \u03c6 ) \u03bel\u22121\nNote that the only non-vanishing term (in weak convergence) of 1/ \u221a t \u2211t k=0 x\u0303k as t\u2192\u221e is\n1\u221a t \u03b1\u03b7 t\u22121\u2211 l=1 1 \u03b3 \u2212 \u03c6 ( \u03b3 1\u2212 \u03b3 \u2212 \u03c6 1\u2212 \u03c6 ) \u03bel\u22121. (29)\nAlso recall that V[\u03bel\u22121] = p\u03c32 and 1\n\u03b3 \u2212 \u03c6\n( \u03b3\n1\u2212 \u03b3 \u2212 \u03c6 1\u2212 \u03c6\n) =\n1\n(1\u2212 \u03b3)(1\u2212 \u03c6) =\n1\n\u03b7hp\u03b1 .\nTherefore the expression in Equation 29 is asymptotically normal with zero mean and variance \u03c32/ph2.\nThe asymptotic variance in the Lemma 7.2 is optimal with any fixed \u03b7 and \u03b2 for which Equation 17 holds. The next lemma (Lemma 7.3) extends the result in Lemma 7.2 to the multi-dimensional setting. Lemma 7.3 (Weak convergence). Let h denotes the largest eigenvalue of A. If (2\u2212 \u03b7h)(2\u2212 \u03b2) > 2\u03b2/p, (2 \u2212 \u03b7h) + (2 \u2212 \u03b2) > \u03b2/p, \u03b7 > 0 and \u03b2 > 0, then the normalized double averaging sequence converges weakly to the normal distribution with zero mean and the covariance matrix V = A\u22121\u03a3(A\u22121)T ,\n\u221a tp(zt \u2212 x\u2217) \u21c0 N (0, V ), t\u2192\u221e. (30)\nProof. Since A is symmetric, one can use the proof technique of Lemma 7.2 to prove Lemma 7.3 by diagonalizing the matrix A. This diagonalization essentially generalizes Lemma 7.1 to the multidimensional case. We will not go into the details of this proof as we will provide a simpler way to look at the system. As in the proof of Lemma 7.1 and Lemma 7.2, for the ease of notation we redefine x\u0303t and xit as follows:\nx\u0303t , x\u0303t \u2212 x\u2217 and xit , xit \u2212 x\u2217. Let the spatial average of the local parameters at time t be denoted as yt where yt = 1p \u2211p i=1 x i t,\nand let the average noise be denoted as \u03bet, where \u03bet = 1p \u2211p i=1 \u03be i t . Equations 18 and 19 can then be reduced to the following\nyt+1 = yt \u2212 \u03b7(Ayt \u2212 \u03bet) + \u03b1(x\u0303t \u2212 yt), (31) x\u0303t+1 = x\u0303t + \u03b2(yt \u2212 x\u0303t). (32)\nWe focus on the case where the learning rate \u03b7 and the moving rate \u03b1 are kept constant over time11. Recall \u03b2 = p\u03b1 and \u03b1 = \u03b7\u03c1.\nLet\u2019s introduce the block notation Ut = (yt, x\u0303t), \u039et = (\u03b7\u03bet, 0), M = I \u2212 \u03b7L and\nL =\n( A+ \u03b1\u03b7 I \u2212 \u03b1 \u03b7 I\n\u2212\u03b2\u03b7 I \u03b2 \u03b7 I\n) .\nFrom Equations 31 and 32 it follows that Ut+1 = MUt + \u039et. Note that this linear system has a degenerate noise \u039et which prevents us from directly applying results of [6]. Expanding this recursive relation and summing by parts, we have\nt\u2211 k=0 Uk = M 0U0 +\nM1U0 +M 0\u039e0 + M2U0 +M 1\u039e0 +M 0\u039e1 +\n... M tU0 +M t\u22121\u039e0 + \u00b7 \u00b7 \u00b7+M0\u039et\u22121.\nBy Lemma 7.4, \u2016M\u20162 < 1 and thus\nM0 +M1 + \u00b7 \u00b7 \u00b7+M t + \u00b7 \u00b7 \u00b7 = (I \u2212M)\u22121 = \u03b7\u22121L\u22121.\nSince A is invertible, we get\nL\u22121 =\n( A\u22121 \u03b1\u03b2A \u22121\nA\u22121 \u03b7\u03b2 + \u03b1 \u03b2A \u22121\n) ,\nthus\n1\u221a t t\u2211 k=0 Uk = 1\u221a t U0 + 1\u221a t \u03b7L\u22121 t\u2211 k=1 \u039ek\u22121 \u2212 1\u221a t t\u2211 k=1 Mk+1\u039ek\u22121.\nNote that the only non-vanishing term (in weak convergence) of 1\u221a t \u2211t k=0 Uk is 1\u221a t (\u03b7L)\u22121 \u2211t k=1 \u039ek\u22121 thus we have\n1\u221a t (\u03b7L)\u22121 t\u2211 k=1 \u039ek\u22121 \u21c0 N (( 0 0 ) , ( V V V V )) , (33)\nwhere V = A\u22121\u03a3(A\u22121)T .\nLemma 7.4. If the following conditions hold:\n(2\u2212 \u03b7h)(2\u2212 p\u03b1) > 2\u03b1 (2\u2212 \u03b7h) + (2\u2212 p\u03b1) > \u03b1\n\u03b7 > 0\n\u03b1 > 0\nthen \u2016M\u20162 < 1.\nProof. The eigenvalue \u03bb of M and the (non-zero) eigenvector (y, z) of M satisfy\nM ( y z ) = \u03bb ( y z ) . (34)\n11As a side note, notice that the center parameter x\u0303t is tracking the spatial average yt of the local parameters with a non-symmetric spring in Equation 31 and 32. To be more precise note that the update on yt+1 contains (x\u0303t\u2212yt) scaled by \u03b1, whereas the update on x\u0303t+1 contains \u2212(x\u0303t\u2212yt) scaled by \u03b2. Since \u03b1 = \u03b2/p the impact of the center x\u0303t+1 on the spatial local average yt+1 becomes more negligible as p grows.\nRecall that\nM = I \u2212 \u03b7L = ( I \u2212 \u03b7A\u2212 \u03b1I \u03b1I\n\u03b2I I \u2212 \u03b2I\n) . (35)\nFrom the Equations 34 and 35 we obtain{ y \u2212 \u03b7Ay \u2212 \u03b1y + \u03b1z = \u03bby \u03b2y + (1\u2212 \u03b2)z = \u03bbz . (36)\nSince (y, z) is assumed to be non-zero, we can write z = \u03b2y/(\u03bb + \u03b2 \u2212 1). Then the Equation 36 can be reduced to\n\u03b7Ay = (1\u2212 \u03b1\u2212 \u03bb)y + \u03b1\u03b2 \u03bb+ \u03b2 \u2212 1 y. (37)\nThus y is the eigenvector of A. Let \u03bbA be the eigenvalue of matrix A such that Ay = \u03bbAy. Thus based on Equation 37 it follows that\n\u03b7\u03bbA = (1\u2212 \u03b1\u2212 \u03bb) + \u03b1\u03b2\n\u03bb+ \u03b2 \u2212 1 . (38)\nEquation 38 is equivalent to\n\u03bb2 \u2212 (2\u2212 a)\u03bb+ (1\u2212 a+ c2) = 0, (39) where a = \u03b7\u03bbA + (p + 1)\u03b1, c2 = \u03b7\u03bbAp\u03b1. It follows from the condition in Equation 17 that \u22121 < \u03bb < 1 iff \u03b7 > 0, \u03b2 > 0, (2 \u2212 \u03b7\u03bbA)(2 \u2212 \u03b2) > 2\u03b2/p and (2 \u2212 \u03b7\u03bbA) + (2 \u2212 \u03b2) > \u03b2/p. Let h denote the maximum eigenvalue of A and note that 2\u2212 \u03b7\u03bbA \u2265 2\u2212 \u03b7h. This implies that the condition of our lemma is sufficient.\nAs in Lemma 7.2, the asymptotic covariance in the Lemma 7.3 is optimal, i.e. meets the Fisher information lower-bound. The fact that this asymptotic covariance matrix V does not contain any term involving \u03c1 is quite remarkable, since the penalty term \u03c1 does have an impact on the condition number of the Hessian in Equation 2."}, {"heading": "7.3 Strongly convex case", "text": "We now extend the above proof ideas to analyze the strongly convex case, in which the noisy gradient git(x) = \u2207F (x)\u2212 \u03beit has the regularity that there exists some 0 < \u00b5 \u2264 L, for which \u00b5 \u2016x\u2212 y\u2016\n2 \u2264 \u3008\u2207F (x)\u2212\u2207F (y), x\u2212 y\u3009 \u2264 L \u2016x\u2212 y\u20162 holds uniformly for any x \u2208 Rd, y \u2208 Rd. The noise {\u03beit}\u2019s is assumed to be i.i.d. with zero mean and bounded variance E[\n\u2225\u2225\u03beit\u2225\u22252] \u2264 \u03c32. Theorem 7.1. Let at = E\n\u2225\u2225\u2225 1p\u2211pi=1 xit \u2212 x\u2217\u2225\u2225\u22252, bt = 1p\u2211pi=1 E\u2225\u2225xit \u2212 x\u2217\u2225\u22252, ct = E \u2016x\u0303t \u2212 x\u2217\u20162, \u03b31 = 2\u03b7 \u00b5L \u00b5+L and \u03b32 = 2\u03b7L(1\u2212 2 \u221a \u00b5L \u00b5+L ). If 0 \u2264 \u03b7 \u2264 2\n\u00b5+L (1\u2212 \u03b1), 0 \u2264 \u03b1 < 1 and 0 \u2264 \u03b2 \u2264 1 then( at+1 bt+1 ct+1 ) \u2264 ( 1\u2212 \u03b31 \u2212 \u03b32 \u2212 \u03b1 \u03b32 \u03b1 0 1\u2212 \u03b31 \u2212 \u03b1 \u03b1 \u03b2 0 1\u2212 \u03b2 )( at bt ct ) + \u03b72 \u03c32p\u03b72\u03c32 0  . Proof. The idea of the proof is based on the point of view in Lemma 7.3, i.e. how close the center variable x\u0303t is to the spatial average of the local variables yt = 1p \u2211p i=1 x i t. To further simplify the notation, let the noisy gradient be \u2207f it,\u03be = git(xit) = \u2207F (xit) \u2212 \u03beit , and \u2207f it = \u2207F (xit) be its deterministic part. Then EASGD updates can be rewritten as follows,\nxit+1 = x i t \u2212 \u03b7\u2207f it,\u03be \u2212 \u03b1(xit \u2212 x\u0303t), (40)\nx\u0303t+1 = x\u0303t + \u03b2(yt \u2212 x\u0303t). (41) We have thus the update for the spatial average,\nyt+1 = yt \u2212 \u03b7 1\np p\u2211 i=1 \u2207f it,\u03be \u2212 \u03b1(yt \u2212 x\u0303t). (42)\nThe idea of the proof is to bound the distance \u2016x\u0303t \u2212 x\u2217\u20162 through \u2016yt \u2212 x\u2217\u20162 and 1 p \u2211p i \u2225\u2225xit \u2212 x\u2217\u2225\u22252. W start from the following estimate for the strongly convex function [31], \u3008\u2207F (x)\u2212\u2207F (y), x\u2212 y\u3009 \u2265 \u00b5L\n\u00b5+ L \u2016x\u2212 y\u20162 + 1 \u00b5+ L \u2016\u2207F (x)\u2212\u2207F (y)\u20162 .\nSince\u2207f(x\u2217) = 0, we have\u2329 \u2207f it , xit \u2212 x\u2217 \u232a \u2265 \u00b5L \u00b5+ L \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2207f it\u2225\u22252 . (43) From Equation 40 the following relation holds,\u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 = \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u03b72 \u2225\u2225\u2207f it,\u03be\u2225\u22252 + \u03b12 \u2225\u2225xit \u2212 x\u0303t\u2225\u22252\n\u2212 2\u03b7 \u2329 \u2207f it,\u03be, xit \u2212 x\u2217 \u232a \u2212 2\u03b1 \u2329 xit \u2212 x\u0303t, xit \u2212 x\u2217 \u232a + 2\u03b7\u03b1 \u2329 \u2207f it,\u03be, xit \u2212 x\u0303t \u232a . (44)\nBy the cosine rule (2 \u3008a\u2212 b, c\u2212 d\u3009 = \u2016a\u2212 d\u20162 \u2212 \u2016a\u2212 c\u20162 + \u2016c\u2212 b\u20162 \u2212 \u2016d\u2212 b\u20162), we have 2 \u2329 xit \u2212 x\u0303t, xit \u2212 x\u2217 \u232a = \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162 . (45) By the Cauchy-Schwarz inequality, we have\u2329 \u2207f it , xit \u2212 x\u0303t \u232a \u2264 \u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 . (46) Combining the above estimates in Equations 43, 44, 45, 46, we obtain\u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 \u2264 \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u03b72 \u2225\u2225\u2207f it \u2212 \u03beit\u2225\u22252 + \u03b12 \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 2\u03b7 ( \u00b5L\n\u00b5+ L \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2207f it\u2225\u22252 ) + 2\u03b7 \u2329 \u03beit, x i t \u2212 x\u2217 \u232a \u2212 \u03b1\n( \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162 ) + 2\u03b7\u03b1\n\u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 \u2212 2\u03b7\u03b1 \u2329\u03beit, xit \u2212 x\u0303t\u232a . (47) Choosing 0 \u2264 \u03b1 < 1, we can have this upper-bound for the terms \u03b12\n\u2225\u2225xit \u2212 x\u0303t\u2225\u22252\u2212\u03b1 \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 + 2\u03b7\u03b1\n\u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 = \u2212\u03b1(1 \u2212 \u03b1)\u2225\u2225xit \u2212 x\u0303t\u2225\u22252 + 2\u03b7\u03b1 \u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 \u2264 \u03b72\u03b11\u2212\u03b1 \u2225\u2225\u2207f it\u2225\u22252 by applying \u2212ax2 + bx \u2264 b 2\n4a with x = \u2225\u2225xit \u2212 x\u0303t\u2225\u2225. Thus we can further bound Equation 47 with\u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 \u2264 (1\u2212 2\u03b7 \u00b5L\u00b5+ L \u2212 \u03b1)\u2225\u2225xit \u2212 x\u2217\u2225\u22252 + (\u03b72 + \u03b72\u03b11\u2212 \u03b1 \u2212 2\u03b7\u00b5+ L )\u2225\u2225\u2207f it\u2225\u22252\n\u2212 2\u03b72 \u2329 \u2207f it , \u03beit \u232a + 2\u03b7 \u2329 \u03beit, x i t \u2212 x\u2217 \u232a \u2212 2\u03b7\u03b1 \u2329 \u03beit, x i t \u2212 x\u0303t \u232a (48)\n+ \u03b72 \u2225\u2225\u03beit\u2225\u22252 + \u03b1 \u2016x\u0303t \u2212 x\u2217\u20162 (49)\nAs in Equation 48 and 49, the noise \u03beit is zero mean (E\u03beit = 0) and the variance of the noise \u03beit is bounded (E \u2225\u2225\u03beit\u2225\u22252 \u2264 \u03c32), if \u03b7 is chosen small enough such that \u03b72 + \u03b72\u03b11\u2212\u03b1 \u2212 2\u03b7\u00b5+L \u2264 0, then E \u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 \u2264 (1\u2212 2\u03b7 \u00b5L\u00b5+ L \u2212 \u03b1)E\u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u03b72\u03c32 + \u03b1E \u2016x\u0303t \u2212 x\u2217\u20162 . (50)\nNow we apply similar idea to estimate \u2016yt \u2212 x\u2217\u20162. From Equation 42 the following relation holds, \u2016yt+1 \u2212 x\u2217\u20162 = \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it,\u03be \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7\n\u2329 1\np p\u2211 i=1 \u2207f it,\u03be, yt \u2212 x\u2217 \u232a \u2212 2\u03b1 \u3008yt \u2212 x\u0303t, yt \u2212 x\u2217\u3009\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it,\u03be, yt \u2212 x\u0303t\n\u232a . (51)\nBy \u2329\n1 p \u2211p i=1 ai, 1 p \u2211p j=1 bj \u232a = 1p \u2211p i=1 \u3008ai, bi\u3009 \u2212 1 p2 \u2211 i>j \u3008ai \u2212 aj , bi \u2212 bj\u3009, we have\u2329\n1\np p\u2211 i=1 \u2207f it , yt \u2212 x\u2217 \u232a = 1 p p\u2211 i=1 \u2329 \u2207f it , xit \u2212 x\u2217 \u232a \u2212 1 p2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f j t , x i t \u2212 x j t \u232a . (52)\nBy the cosine rule, we have\n2 \u3008yt \u2212 x\u0303t, yt \u2212 x\u2217\u3009 = \u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162 . (53)\nDenote \u03bet = 1p \u2211p i=1 \u03be i t , we can rewrite Equation 51 as\n\u2016yt+1 \u2212 x\u2217\u20162 = \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u2217 \u232a \u2212 2\u03b1 \u3008yt \u2212 x\u0303t, yt \u2212 x\u2217\u3009\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t\n\u232a . (54)\nBy combining the above Equations 52, 53 with 54, we obtain \u2016yt+1 \u2212 x\u2217\u20162 = \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7 ( 1\np p\u2211 i=1 \u2329 \u2207f it , xit \u2212 x\u2217 \u232a \u2212 1 p2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f j t , x i t \u2212 x j t \u232a ) (55)\n+ 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 \u03b1(\u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162)\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t\n\u232a . (56)\nThus it follows from Equation 43 and 56 that \u2016yt+1 \u2212 x\u2217\u20162 \u2264 \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7 1 p p\u2211 i=1 ( \u00b5L \u00b5+ L \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2207f it\u2225\u22252 )\n+ 2\u03b7 1\np2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f j t , x i t \u2212 x j t \u232a + 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 \u03b1(\u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162)\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t\n\u232a . (57)\nRecall yt = 1p \u2211p i=1 x i t, we have the following bias-variance relation,\n1\np p\u2211 i=1 \u2225\u2225xit \u2212 x\u2217\u2225\u22252 = 1p p\u2211 i=1 \u2225\u2225xit \u2212 yt\u2225\u22252 + \u2016yt \u2212 x\u2217\u20162 = 1p2 \u2211 i>j \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252 + \u2016yt \u2212 x\u2217\u20162 , 1\np p\u2211 i=1 \u2225\u2225\u2207f it\u2225\u22252 = 1p2 \u2211 i>j \u2225\u2225\u2225\u2207f it \u2212\u2207f jt \u2225\u2225\u22252 + \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2225\u2225\u2225\u2225\u2225 2 . (58)\nBy the Cauchy-Schwarz inequality, we have\n\u00b5L\n\u00b5+ L \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2225\u2207f it \u2212\u2207f jt \u2225\u2225\u22252 \u2265 2 \u221a \u00b5L \u00b5+ L \u2329 \u2207f it \u2212\u2207f j t , x i t \u2212 x j t \u232a . (59)\nCombining the above estimates in Equations 57, 58, 59, we obtain \u2016yt+1 \u2212 x\u2217\u20162 \u2264 \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7 ( \u00b5L\n\u00b5+ L \u2016yt \u2212 x\u2217\u20162 +\n1\n\u00b5+ L \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2225\u2225\u2225\u2225\u2225 2)\n+ 2\u03b7 ( 1\u2212 2 \u221a \u00b5L\n\u00b5+ L\n) 1\np2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f j t , x i t \u2212 x j t \u232a + 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 \u03b1(\u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162)\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t\n\u232a . (60)\nSimilarly if 0 \u2264 \u03b1 < 1, we can have this upper-bound for the terms \u03b12 \u2016yt \u2212 x\u0303t\u20162\u2212\u03b1 \u2016yt \u2212 x\u0303t\u20162 + 2\u03b7\u03b1 \u2225\u2225\u2225 1p\u2211pi=1\u2207f it\u2225\u2225\u2225 \u2016yt \u2212 x\u0303t\u2016 \u2264 \u03b72\u03b11\u2212\u03b1 \u2225\u2225\u2225 1p\u2211pi=1\u2207f it\u2225\u2225\u22252 by applying \u2212ax2 + bx \u2264 b24a with x =\n\u2016yt \u2212 x\u0303t\u2016. Thus we have the following bound for the Equation 60\n\u2016yt+1 \u2212 x\u2217\u20162 \u2264 (1\u2212 2\u03b7 \u00b5L\n\u00b5+ L \u2212 \u03b1) \u2016yt \u2212 x\u2217\u20162 + (\u03b72 +\n\u03b72\u03b1\n1\u2212 \u03b1 \u2212 2\u03b7 \u00b5+ L ) \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2225\u2225\u2225\u2225\u2225 2\n\u2212 2\u03b72 \u2329 1\np p\u2211 i=1 \u2207f it , \u03bet\n\u232a + 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 2\u03b7\u03b1 \u3008\u03bet, yt \u2212 x\u0303t\u3009\n+ 2\u03b7 ( 1\u2212 2 \u221a \u00b5L\n\u00b5+ L\n) 1\np2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f j t , x i t \u2212 x j t \u232a + \u03b72 \u2016\u03bet\u20162 + \u03b1 \u2016x\u0303t \u2212 x\u2217\u20162 . (61)\nSince 2 \u221a \u00b5L \u00b5+L \u2264 1, we need also bound the non-linear term \u2329 \u2207f it \u2212\u2207f j t , x i t \u2212 x j t \u232a \u2264 L \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252. Recall the bias-variance relation 1p \u2211p i=1\n\u2225\u2225xit \u2212 x\u2217\u2225\u22252 = 1p2 \u2211i>j \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252 + \u2016yt \u2212 x\u2217\u20162. The key observation is that if 1p \u2211p i=1\n\u2225\u2225xit \u2212 x\u2217\u2225\u22252 remains bounded, then larger variance\u2211 i>j\n\u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252 implies smaller bias \u2016yt \u2212 x\u2217\u20162. Thus this non-linear term can be compensated. Again choose \u03b7 small enough such that \u03b72 + \u03b7\n2\u03b1 1\u2212\u03b1 \u2212 2\u03b7 \u00b5+L \u2264 0 and take expectation in Equation 61,\nE \u2016yt+1 \u2212 x\u2217\u20162 \u2264 (1\u2212 2\u03b7 \u00b5L\n\u00b5+ L \u2212 \u03b1)E \u2016yt \u2212 x\u2217\u20162\n+ 2\u03b7L ( 1\u2212 2 \u221a \u00b5L\n\u00b5+ L\n)( 1\np p\u2211 i=1 E \u2225\u2225xit \u2212 x\u2217\u2225\u22252 \u2212 E \u2016yt \u2212 x\u2217\u20162)\n+ \u03b72 \u03c32\np + \u03b1E \u2016x\u0303t \u2212 x\u2217\u20162 . (62)\nAs for the center variable in Equation 41, we apply simply the convexity of the norm \u2016\u00b7\u20162 to obtain\n\u2016x\u0303t+1 \u2212 x\u2217\u20162 \u2264 (1\u2212 \u03b2) \u2016x\u0303t \u2212 x\u2217\u20162 + \u03b2 \u2016yt \u2212 x\u2217\u20162 . (63)\nCombing the estimates from Equations 50, 62, 63, and denote at = E \u2016yt \u2212 x\u2217\u20162, bt = 1 p \u2211p i=1 E \u2225\u2225xit \u2212 x\u2217\u2225\u22252, ct = E \u2016x\u0303t \u2212 x\u2217\u20162, \u03b31 = 2\u03b7 \u00b5L\u00b5+L , \u03b32 = 2\u03b7L(1\u2212 2\u221a\u00b5L\u00b5+L ), then( at+1 bt+1 ct+1 ) \u2264 ( 1\u2212 \u03b31 \u2212 \u03b32 \u2212 \u03b1 \u03b32 \u03b1 0 1\u2212 \u03b31 \u2212 \u03b1 \u03b1 \u03b2 0 1\u2212 \u03b2 )( at bt ct ) + \u03b72 \u03c32p\u03b72\u03c32 0\n , as long as 0 \u2264 \u03b2 \u2264 1, 0 \u2264 \u03b1 < 1 and \u03b72 + \u03b7\n2\u03b1 1\u2212\u03b1 \u2212 2\u03b7 \u00b5+L \u2264 0, i.e. 0 \u2264 \u03b7 \u2264 2 \u00b5+L (1\u2212 \u03b1)."}, {"heading": "8 Additional pseudo-codes of the algorithms", "text": ""}, {"heading": "8.1 DOWNPOUR pseudo-code", "text": "Algorithm 3 captures the pseudo-code of the implementation of the DOWNPOUR used in this paper.\nAlgorithm 3: DOWNPOUR: Processing by worker i and the master Input: learning rate \u03b7, communication period \u03c4 \u2208 N Initialize: x\u0303 is initialized randomly, xi = x\u0303, vi = 0, ti = 0 Repeat\nif (\u03c4 divides ti) then x\u0303 \u2190 x\u0303 + vi xi \u2190 x\u0303 vi \u2190 0 end xi \u2190 xi \u2212 \u03b7giti(x\ni) vi \u2190 vi \u2212 \u03b7giti(x\ni) ti \u2190 ti + 1\nUntil forever"}, {"heading": "8.2 MDOWNPOUR pseudo-code", "text": "Algorithms 4 and 5 capture the pseudo-codes of the implementation of momentum DOWNPOUR (MDOWNPOUR) used in this paper. Algorithm 4 shows the behavior of each local worker and Algorithm 5 shows the behavior of the master.\nAlgorithm 4: MDOWNPOUR: Processing by worker i Initialize: xi = x\u0303 Repeat\nReceive x\u0303 from the master: xi \u2190 x\u0303 Compute gradient gi = gi(xi) Send gi to the master\nUntil forever\nAlgorithm 5: MDOWNPOUR: Processing by the master Input: learning rate \u03b7, momentum term \u03b4 Initialize: x\u0303 is initialized randomly, vi = 0, Repeat\nReceive gi v \u2190 \u03b4v \u2212 \u03b7gi x\u0303\u2190 x\u0303+ \u03b4v\nUntil forever"}, {"heading": "9 Experiments - additional material", "text": ""}, {"heading": "9.1 Data preprocessing", "text": "For the ImageNet experiment, we re-size each RGB image so that the smallest dimension is 256 pixels. We also re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal flips) of size 3\u00d7 221\u00d7 221 pixels and present these to the network in mini-batches of size 128.\nFor the CIFAR experiment, we use the original RGB image of size 3 \u00d7 32 \u00d7 32. As before, we re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal flips) of size 3\u00d7 28\u00d7 28 pixels and present these to the network in mini-batches of size 128. The training and test loss and the test error are only computed from the center patch (3 \u00d7 28 \u00d7 28) for the CIFAR experiment and the center patch (3\u00d7 221\u00d7 221) for the ImageNet experiment."}, {"heading": "9.2 Data prefetching (Sampling the dataset by the local workers)", "text": "We will now explain precisely how the dataset is sampled by each local worker as uniformly and efficiently as possible. The general parallel data loading scheme on a single machine is as follows: we use k CPUs, where k = 8, to load the data in parallel. Each data loader reads from the memory-mapped (mmap) file a chunk of c raw images (preprocessing was described in the previous subsection) and their labels (for CIFAR c = 512 and for ImageNet c = 64). For the CIFAR, the mmap file of each data loader contains the entire dataset whereas for ImageNet, each mmap file of each data loader contains different 1/k fractions of the entire dataset. A chunk of data is always sent by one of the data loaders to the first worker who requests the data. The next worker requesting the data from the same data loader will get the next chunk. Each worker requests in total k data chunks from k different data loaders and then process them before asking for new data chunks. Notice that each data loader cycles through the data in the mmap file, sending consecutive chunks to the workers in order in which it receives requests from them. When the data loader reaches the end of the mmap file, it selects the address in memory uniformly at random from the interval [0, s], where s = (number of images in the mmap file modulo mini-batch size), and uses this address to start cycling again through the data in the mmap file. After the local worker receives the k data chunks from the data loaders, it shuffles them and divides it into mini-batches of size 128."}, {"heading": "9.3 Learning rates", "text": "In Table 1 we summarize the learning rates \u03b7 (we used constant learning rates) explored for each method shown in Figure 2. For all values of \u03c4 the same set of learning rates was explored for each method.\nIn Table 2 we summarize the learning rates \u03b7 (we used constant learning rates) explored for each method shown in Figure 3. For all values of p the same set of learning rates was explored for each method.\nIn Table 3 we summarize the initial learning rates \u03b7 we use for each method shown in Figure 4. For all values of p the same set of learning rates was explored for each method. We also used the rule of the thumb to decrease the initial learning rate twice, first time we divided it by 5 and the second time by 2, when we observed that the decrease of the online predictive (training) loss saturates.\n9.4 Comparison of SGD, ASGD, MVASGD and MSGD\nFigure 6 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the CIFAR experiment. For all CIFAR experiments we always start the averaging for the ADOWNPOUR and ASGD methods from the very beginning of each experiment. For all ImageNet experiments we start the averaging for the ASGD at the same time when we first reduce the initial learning rate.\nFigure 7 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the ImageNet experiment."}, {"heading": "9.5 Dependence of the learning rate", "text": "This section discusses the dependence of the trade-off between exploration and exploitation on the learning rate. We compare the performance of respectively EAMSGD and EASGD for different learning rates \u03b7 when p = 16 and \u03c4 = 10 on the CIFAR experiment. We observe in Figure 8 that higher learning rates \u03b7 lead to better test performance for the EAMSGD algorithm which potentially can be justified by the fact that they sustain higher fluctuations of the local workers. We conjecture that higher fluctuations lead to more exploration and simultaneously they also impose higher regularization. This picture however seems to be opposite for the EASGD algorithm for which larger learning rates hurt the performance of the method and lead to overfitting. Interestingly in this experiment for both EASGD and EAMSGD algorithm, the learning rate for which the best training performance was achieved simultaneously led to the worst test performance."}, {"heading": "9.6 Dependence of the communication period", "text": "This section discusses the dependence of the trade-off between exploration and exploitation on the communication period. We have observed from the CIFAR experiment that EASGD algorithm exhibits very similar convergence behavior when \u03c4 = 1 up to even \u03c4 = 1000, whereas EAMSGD can get trapped at worse energy (loss) level for \u03c4 = 100. This behavior of EAMSGD is most likely due to the non-convexity of the objective function. Luckily, it can be avoided by gradually decreasing the learning rate, i.e. increasing the penalty term \u03c1 (recall \u03b1 = \u03b7\u03c1), as shown in Figure 9. In contrast, the EASGD algorithm does not seem to get trapped at all along its trajectory. The performance of EASGD is less sensitive to increasing the communication period compared to EAMSGD, whereas for the EAMSGD the careful choice of the learning rate for large communication periods seems crucial.\nCompared to all earlier results, the experiment in this section is re-run three times with a new random12 seed and with faster cuDNN13 package14. All our methods are implemented in Torch15. The Message Passing Interface implementation MVAPICH216 is used for the GPU-CPU communication.\n12To clarify, the random initialization we use is by default in Torch\u2019s implementation. 13https://developer.nvidia.com/cuDNN 14https://github.com/soumith/cudnn.torch 15http://torch.ch 16http://mvapich.cse.ohio-state.edu"}, {"heading": "9.7 Breakdown of the wallclock time", "text": "In addition, we report in Table 4 the breakdown of the total running time for EASGD when \u03c4 = 10 (the time breakdown for EAMSGD is almost identical) and DOWNPOUR when \u03c4 = 1 into computation time, data loading time and parameter communication time. For the CIFAR experiment the reported time corresponds to processing 400 \u00d7 128 data samples whereas for the ImageNet experiment it corresponds to processing 1024 \u00d7 128 data samples. For \u03c4 = 1 and p \u2208 {8, 16} we observe that the communication time accounts for significant portion of the total running time whereas for \u03c4 = 10 the communication time becomes negligible compared to the total running time (recall that based on previous results EASGD and EAMSGD achieve best performance with larger \u03c4 which is ideal in the setting when communication is time-consuming)."}, {"heading": "9.8 Time speed-up", "text": "In Figure 10 and 11, we summarize the wall clock time needed to achieve the same level of the test error for all the methods in the CIFAR and ImageNet experiment as a function of the number of local workers p. For the CIFAR (Figure 10) we examined the following levels: {21%, 20%, 19%, 18%} and for the ImageNet (Figure 11) we examined: {49%, 47%, 45%, 43%}. If some method does not appear on the figure for a given test error level, it indicates that this method never achieved this level. For the CIFAR experiment we observe that from among EASGD, DOWNPOUR and MDOWNPOUR methods, the EASGD method needs less time to achieve a particular level of test error. We observe that with higher p each of these methods does not necessarily need less time to achieve the same level of test error. This seems counter intuitive though recall that the learning rate for the methods is selected based on the smallest achievable test error. For larger p smaller learning rates were selected than for smaller p which explains our results. Meanwhile, the EAMSGD method achieves significant speed-up over other methods for all the test error levels. For the ImageNet experiment we observe that all methods outperform MSGD and furthermore with p = 4 or p = 8 each of these methods requires less time to achieve the same level of test error. The EAMSGD consistently needs less time than any other method, in particular DOWNPOUR, to achieve any of the test error levels.\n."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We study the problem of stochastic optimization for deep learning in the paral-<lb>lel computing environment under communication constraints. A new algorithm<lb>is proposed in this setting where the communication and coordination of work<lb>among concurrent processes (local workers), is based on an elastic force which<lb>links the parameters they compute with a center variable stored by the parameter<lb>server (master). The algorithm enables the local workers to perform more explo-<lb>ration, i.e. the algorithm allows the local variables to fluctuate further from the<lb>center variable by reducing the amount of communication between local workers<lb>and the master. We empirically demonstrate that in the deep learning setting, due<lb>to the existence of many local optima, allowing more exploration can lead to the<lb>improved performance. We propose synchronous and asynchronous variants of<lb>the new algorithm. We provide the stability analysis of the asynchronous vari-<lb>ant in the round-robin scheme and compare it with the more common parallelized<lb>method ADMM. We show that the stability of EASGD is guaranteed when a simple<lb>stability condition is satisfied, which is not the case for ADMM. We additionally<lb>propose the momentum-based version of our algorithm that can be applied in both<lb>synchronous and asynchronous settings. Asynchronous variant of the algorithm<lb>is applied to train convolutional neural networks for image classification on the<lb>CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm<lb>accelerates the training of deep architectures compared to DOWNPOUR and other<lb>common baseline approaches and furthermore is very communication efficient.", "creator": "LaTeX with hyperref package"}}}