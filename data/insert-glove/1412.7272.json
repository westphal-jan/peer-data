{"id": "1412.7272", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Learning Non-deterministic Representations with Energy-based Ensembles", "abstract": "issueless The goal of 79-70 a confortola generative model is to capture the 60.67 distribution posteriori underlying nelia the galderisi data, officier typically musclemen through latent variables. After zabrze training, candiates these all-glass variables morvillo are often used u-13 as hushan a flaviano new representation, ahnenerbe more k\u00e1lm\u00e1n effective than the throbs original revealingly features in a discoidal variety of fathomless learning wbez tasks. lieut-col However, the 18.14 representations hoogervorst constructed by 344th contemporary generative models are intractability usually 0645 point - wise deterministic lamely mappings from the piat original askani feature gebhard space. beignets Thus, tribulus even declamation with kasbah representations robust 1,118 to class - specific 178.4 transformations, wellbeloved statistically .391 driven models tanaka trained godine on ljubicic them would 3.51 not be siddiqa able sickened to veddas generalize when the s.c.a. labeled 79.18 data novaya is cangrejeros scarce. enigmas Inspired names by telef\u00f3nica the ricciardi stochasticity qudratullah of the auvers-sur-oise synaptic connections in the brain, erectile we lochranza introduce Energy - antaryami based gargan Stochastic Ensembles. rondebosch These nntp ensembles dormael can learn verifying non - deterministic representations, luns i. aiv e. , mappings from the wolkonowicz feature triathletes space to a lrp family of omelette distributions in the latent space. These mappings are encoded 78.69 in zorana a keach distribution baji\u0107 over a151 a (possibly infinite) dmitriy collection andalusian of huilin models. cuomos By conditionally sampling models msida from narrow the b'alam ensemble, rutba we kazungula obtain rohtak multiple acomb representations for vogondy every kadri input tomates example and effectively bunky\u014d augment garuba the data. We silique propose notochord an algorithm similar to saint-john contrastive rogaine divergence for kintyre training restricted Boltzmann exista stochastic f13 ensembles. 2,460 Finally, lockouts we demonstrate the concept charanga of the sharifs stochastic representations b\u0103r\u0103gan on a kurama synthetic nai101 dataset balsamifera as well as kasagic test zabriski them bse in the one - harken shot non-roster learning barech scenario on winrow MNIST.", "histories": [["v1", "Tue, 23 Dec 2014 07:06:55 GMT  (970kb,D)", "http://arxiv.org/abs/1412.7272v1", "9 pages, 3 figures, ICLR-15"], ["v2", "Wed, 22 Apr 2015 10:04:49 GMT  (970kb,D)", "http://arxiv.org/abs/1412.7272v2", "9 pages, 3 figures, ICLR-15 workshop contribution"]], "COMMENTS": "9 pages, 3 figures, ICLR-15", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["maruan al-shedivat", "emre neftci", "gert cauwenberghs"], "accepted": true, "id": "1412.7272"}, "pdf": {"name": "1412.7272.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Maruan Al-Shedivat"], "emails": ["maruan.shedivat@kaust.edu.sa", "nemre@ucsd.edu", "gert@ucsd.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Learning data representations is a powerful technique that has been widely adopted in many fields of artificial intelligence (Bengio, 2009). Its main goal is usually to learn transformations of the data that disentangle different classes in the new space, that are robust to the noise in the input and tolerant to the variations along the class-specific manifolds (DiCarlo & Cox, 2007). A widely used technique of constructing representations is based on using probabilistic generative models of the data. Latent variables in such models can capture high-order correlations between the samples and are successful as new representations in a variety of tasks (Bengio, 2009). However, even high quality representations do not solve the problem of generalization: statistically derived discriminative models require a sufficient number of labeled training examples in order to exhibit good performance when tested.\nThe standard way of ameliorating the problem of overfitting due to the limited training data is based on enforcing a regularization (Bishop, 2006). Maaten et al. (2013) recently demonstrated that the artificial data augmentation via feature corruption effectively plays the role of a data-adaptive regularization. Wager et al. (2013) also showed that the dropout techniques applied to generalized linear models result into an adaptive regularization. In other words, these approaches confirm that having more (even corrupted) training data is equivalent to regularizing the model.\n\u2217Corresponding author: maruan.alshedivat.com.\nar X\niv :1\n41 2.\n72 72\nv1 [\ncs .L\nG ]\n2 3\nOn the other hand, instead of corrupting features one can try perform regularization by perturbing the learning model itself. Parameter perturbation leads to the notion of (possibly infinite) collection of models that we call stochastic ensemble. The Dropout (Hinton et al., 2012) and DropConnect (Wan et al., 2013) techniques are successful examples of using a particular form of stochastic ensemble in the context of feedforward neural networks. A unified framework for a collection of perturbed models (which also encompasses the data corruption methods) was recently introduced by Bachman et al. (2014): An arbitrary noise process was used to perturb a parametric parent model to generate an ensemble of child models. The training was performed by minimizing the marginalized loss function, and yielded an effectively regularized parent model that was robust to the introduced noise.\nThe above mentioned approaches use fixed corruption processes to regularize the original model and improve its generalization. Injection of arbitrary noise improves the robustness of the model to the corruption process (Maaten et al., 2013), but it does not necessarily capture the information about the generative process behind the actual data. In order to capture the variance of the data manifold, we propose to learn the noise that perturbs the model. Our work is inspired by the adaptive stochasticity of the synaptic connections in the brain. According to the experimental data, synapses between cortical neurons are highly unreliable (Branco & Staras, 2009). Moreover, this type of stochasticity is adaptive and adjusted by the learning (Stevens & Wang, 1994).\nIn this paper, we introduce energy-based stochastic ensembles (EBSEs) which can be trained in unsupervised fashion to fit a data distribution. These ensembles result from energy-based models (EBMs) with stochastic parameters. The EBSE learning procedure optimizes the log-likelihood of the ensemble by tuning a parametric distribution over the models. This distribution is first arbitrarily initialized, and then tuned by an expectation-maximization (EM) like procedure: In the E-step, we perform sampling to estimate the necessary expectations, while in the M-step we maximize the loglikelihood with respect to the ensemble parameters. We further develop an algorithm for learning restricted Boltzmann stochastic ensembles (RBSE) similar to contrastive divergence.\nUsing a pre-trained ensemble, we further introduce the notion of non-deterministic representations: instead of constructing point-wise mappings from the original feature space to a new latent space, we propose using stochastic mappings (Figure 1a). The stochasticity of the representations is based on the distribution stored in the ensemble. For every input object we can sample multiple models, and hence perform non-deterministic transformations to obtain a set of different representations. The ensemble is tuned to capture the variance of the data, hence the stochastic representations are likely to better capture the data manifold. We demonstrate these concepts visually by performing experiments on a two-dimensional synthetic dataset. Finally, we train a classifier on the representations of the MNIST hand-written digits generated by an RBSE and observe that the generalization capability in the one-shot learning scenario improves."}, {"heading": "2 ENERGY-BASED STOCHASTIC ENSEMBLES", "text": "The distribution of the binary data vectors v \u2208 {0, 1}D can be encoded with the following energybased model (EBM) that has some binary hidden variables h \u2208 {0, 1}K :\nP (v,h; \u03b8) = e\u2212E(v,h;\u03b8)\nZ(\u03b8) , (1)\nwhere \u03b8 denotes the model parameters, E(v,h; \u03b8) is a parametric scalar energy function (LeCun et al., 2006), and Z(\u03b8) is the normalizing coefficient (partition function). If we impose a distribution over the model parameters (i.e., perturb \u03b8 according to some noise generation process), we obtain an energy-based stochastic ensemble (EBSE). In order to optimize the distribution over the models in the ensemble, we parametrize the noise generation process with \u03b1:\nP (v,h, \u03b8;\u03b1) = P (v,h | \u03b8)P (\u03b8;\u03b1) = e \u2212E(v,h,\u03b8)\nZ(\u03b8) P (\u03b8;\u03b1) =\ne\u2212E(v,h,\u03b8)\u2212\u03c6(\u03b8;\u03b1)\n\u03b6(\u03b1) , (2)\nwhere \u03c6(\u03b8;\u03b1) is an additional energy potential, and \u03b6(\u03b1) is a new partition function. EBSE can be trained by maximizing the data log-likelihood logP (V ;\u03b1) with respect to parameters \u03b1.\nThe introduced form of the model (2) allows to think of P (\u03b8;\u03b1) as a prior. Hence, we can relate the optimization of EBSE with the Bayesian inference framework for a standard EBM by taking\nexpectation over the posterior P (\u03b8 | V ;\u03b1):\nlogP (V ;\u03b1) = \u222b P (\u03b8 | V ;\u03b1) logP (V | \u03b8)d\u03b8 \u2212DKL [P (\u03b8 | V ;\u03b1)\u2016P (\u03b8;\u03b1)]\n= E [logP (V | \u03b8)]P (\u03b8|V ;\u03b1)\ufe38 \ufe37\ufe37 \ufe38 Expected EBM log-likelihood \u2212DKL [P (\u03b8 | V ;\u03b1)\u2016P (\u03b8;\u03b1)]\ufe38 \ufe37\ufe37 \ufe38 KL divergence of posterior and prior\n. (3)\nSince the KL divergence is non-negative, by optimizing the log-likelihood of EBSE, we effectively maximize a lower bound on the EBM log-likelihood averaged over the posterior P (\u03b8 | V ;\u03b1). This relates our approach to the feature corruption method (Maaten et al., 2013, Eq. 2) which optimizes the expected loss directly, but over a simple posterior feature corruption distribution P (v\u0303 | v). Eventually, once we trained the stochastic ensemble of energy-based generative models, we get new parameters \u03b1\u0302 that make P (\u03b8; \u03b1\u0302) better tuned to capture the variance of the data.\nBelow, we derive the gradient of the EBSE log-likelihood (3) for the general energy case. Then, we focus on the restricted Boltzmann stochastic ensembles (RBSE), analyze their structure, and propose an efficient learning algorithm."}, {"heading": "2.1 LOG-LIKELIHOOD OPTIMIZATION", "text": "The gradient of the log-likelihood function (3) can be written in the following way:\n\u2202 logP (v;\u03b1) \u2202\u03b1 = \u2212\u2202F (v;\u03b1) \u2202\u03b1 \u2212 1 \u03b6(\u03b1) \u2202\u03b6(\u03b1) \u2202\u03b1 , (4)\nwhere F (v;\u03b1) = \u2212 log (\u222b e\u2212E(v,h,\u03b8)\u2212\u03c6(\u03b8;\u03b1)dhd\u03b8 ) is called free energy. It is easy to show that the gradients of the free energy and the partition function have the following form:\n\u2202F (v;\u03b1)\n\u2202\u03b1 =\n\u222b P (\u03b8 | v;\u03b1) ( \u2202\u03c6(\u03b8;\u03b1)\n\u2202\u03b1\n) d\u03b8,\n1\n\u03b6(\u03b1)\n\u2202\u03b6(\u03b1)\n\u2202\u03b1 = \u2212\n\u222b P (\u03b8;\u03b1) ( \u2202\u03c6(\u03b8;\u03b1)\n\u2202\u03b1\n) d\u03b8.\n(5) The final expression for the gradient has a contrastive divergence like form (Hinton, 2002):\n\u2202 logP (v;\u03b1)\n\u2202\u03b1 = E\n[ \u2202\u03c6(\u03b8;\u03b1)\n\u2202\u03b1\n]\nP (\u03b8;\u03b1)\n\u2212 E [ \u2202\u03c6(\u03b8;\u03b1)\n\u2202\u03b1\n]\nP (\u03b8|v;\u03b1) , (6)\nwhere the expectations E[ \u00b7 ]P (\u03b8;\u03b1) and E[ \u00b7 ]P (\u03b8|v;\u03b1) are taken over the marginal and conditional distributions over the models, respectively. Based on the properties of the P (\u03b8;\u03b1) distribution, these expectations can be either fully estimated by Monte Carlo sampling, or partly computed analytically.\nAlgorithm 1 Expectation-maximization k-step contrastive divergence for RBSE Input: S\u2014training (mini-)batch; learning rate \u03bb; se-RBM(v , h,W , b, c) Output: gradient approximation \u2206\u03b1 [depends on the parametrization of \u03c6(W ,b,c;\u03b1)]\n1: initialize \u2206\u03b1 = 0 2: for all the v \u2208 S do 3: v(0) \u2190 v 4: h(0) \u2190 persistent state (or sample) 5: sampleW (0), b(0), c(0) \u223c P (W ,b,c | v(0),h(0)) 6: for t = 0, . . . , k do . CD-k for sampling from P (v,h) 7: sampleW (t), b(t), c(t) \u223c P (W ,b,c | v(t),h(t)) 8: sample v(t) \u223c P (v | h(t),W (t), b(t), c(t)) 9: sample h(t) \u223c P (h | v(t),W (t), b(t), c(t))\n10: \u2206\u03b1m = E [ \u2202\u03c6 \u2202\u03b1 | v(k),h(k) ] 11: for t = 0, . . . , k do . CD-k for sampling from P (h | v) 12: sampleW (t), b(t), c(t) \u223c P (W ,b,c | v,h(t)) 13: sample h\u2032(t) \u223c P (h\u2032 | v,W (t), b(t), c(t)) 14: \u2206\u03b1c = E [ \u2202\u03c6 \u2202\u03b1 | v,h\u2032(k) ] 15: \u2206\u03b1\u2190 \u2206\u03b1+ \u03bb (\u2206\u03b1m \u2212\u2206\u03b1c) . Estimate SGD step 16: \u2206\u03b1 = \u2206\u03b1/ cord(S)\nSince the expectations depend on the parameter \u03b1, EBSE training is reminiscent of expectationmaximization (EM): After initializing \u03b1, in the E-step of the algorithm, we estimate E[ \u00b7 ]P (\u03b8;\u03b1) and E[ \u00b7 ]P (\u03b8|v;\u03b1) using the current value of \u03b1. In the M-step, we maximize the log-likelihood by using gradient-based optimization."}, {"heading": "2.2 MODEL STRUCTURE", "text": "We further consider a specific energy-based stochastic ensemble composed of restricted Boltzmann machines (RBM). The energy function of the RBM is linear in each of the variables v , h, and \u03b8:\nE(v,h, \u03b8) = \u2212(vTWh + bTv + cTh) = \u2212( \u2211\ni,j\nWijvihj + \u2211\ni\nbivi + \u2211\nj\ncjhj), (7)\nwhere parameters \u03b8 are represented by a tuple (W ,b,c), andW \u2208 RD\u00d7K , b \u2208 RD, c \u2208 RK . RBM is an undirected graphical model that can be represented by two layers of interconnected probabilistic units (Figure 1b). These units can be seen as neurons with a probabilistic sigmoid activation function, and the graphical model can be represented by a two-layer neural network. In this case, the parametersW play the role of connection strengths between the neurons. By imposing a noise distribution on the parameters, connection strengths become stochastic. From a graphical model perspective, this is equivalent to introducing new latent variables (Figure 1c). Notice that the model becomes a mixed directed and undirected\u2014an ensemble of undirected RBMs generated by a simple Bayesian network composed of (W ,b,c) random hidden variables.\nIn order to fully define the model, we need to choose a specific form of the marginal distribution P (\u03b8;\u03b1) that generates the ensemble. First, to make the expectations computationally feasible, we suppose thatW ,b,c are all marginally independent:\nP (\u03b8;\u03b1) = \u220f\nij\nP (Wij ;\u03b1Wij ) \u220f\ni\nP (bi;\u03b1bi) \u220f\nj\nP (cj ;\u03b1cj ). (8)\nThen, we consider two cases: (a) Bernoulli distributed parameters. In this case Wij can be either zero with probability 1 \u2212 pij , or equal to some W\u0304ij with probability pij . The tunable parameters are \u03b1ij = {pij , W\u0304ij}. This case is similar to DropConnect (Wan et al., 2013) technique but with adaptive distributions over the connections between visible and hidden layers in RBM. (b) In the second case, parameters are normally distributed, i.e., Wij \u223c N (\u00b5ij , \u03c3ij), and \u03b1Wij = {\u00b5ij , \u03c3ij}. In both cases, b and c are parametrized similarly as W . The number of parameters in the RBSE is twice the original number for the RBM.\nThis structure of the model (Figure 1c) implies the following set of independencies:\nvi \u22a5 vj | h, hi \u22a5 hj | v, Wij \u22a5 Wlk, Wij 6\u22a5 Wik | vi, Wij 6\u22a5 Wlj | hi. (9) The last three expressions show the marginal independencies we purposefully incorporated into P (\u03b8;\u03b1) and conditional dependences between the components of W . Due to these dependencies between stochastic connections though visible and hidden units, the model is able to capture the data variance in the distribution over stochastic parameters. Importantly, even though the components of W are dependent on each other given v and h, we are still able to factorize the conditional distribution P (\u03b8 | v,h) using renormalization procedure (see the supplementary material for details)."}, {"heading": "2.3 TRAINING", "text": "We propose the expectation-maximization k-step contrastive divergence algorithm for training RBSE (summarized in Algorithm 1) with two different types of stochastic connections\u2014Bernoulli and Gaussian\u2014between the visible and the hidden layers. To carry out the E-step, we need to compute the expectations in (6). We use Monte Carlo estimates of these expectations: E[ \u00b7 ]P (\u03b8;\u03b1) = \u222b dvdh P (v,h;\u03b1) \u222b d\u03b8 P (\u03b8 | v,h;\u03b1)[ \u00b7 ] \u2248 1\nM\n\u2211\nv\u0303,h\u0303\u223cP (v,h;\u03b1)\n\u222b d\u03b8 P (\u03b8 | v\u0303 , h\u0303;\u03b1)[ \u00b7 ],\nE[ \u00b7 ]P (\u03b8|v;\u03b1) \u2248 1\nM\n\u2211\nh\u0302\u223cP (h|v;\u03b1)\n\u222b d\u03b8 P (\u03b8 | v, h\u0302;\u03b1)[ \u00b7 ].\n(10) The states (v\u0303 , h\u0303) should be sampled from the marginal model distribution P (v,h;\u03b1), and h\u0302 sampled from the marginal model conditional distribution P (h | v;\u03b1). This can be achieved by running a Gibbs sampling procedure as in standard contrastive divergence algorithm (see Algorithm 1). As an example, we get the following eventual expressions for the gradient update of the Gaussian RBSE (for the Bernoulli case, the notation and other details see supplementary material):\n\u2202 logP (v;\u03b1)\n\u2202W\u0304ij = \u3008vihi\u3009data \u2212 \u3008vihi\u3009recon ,\n\u2202 logP (v;\u03b1) \u2202\u03c3\u0304ij = \u2329 v2i h 2 j\u03c3ij \u232a data \u2212 \u2329 v2i h 2 j\u03c3ij \u232a recon .\n(11) Lastly, we need to compute the expectation over the posterior P (\u03b8 | v,h;\u03b1) given a visible and a hidden state. In both Bernoulli and Gaussian cases, due to the structure of the ensemble distribution P (\u03b8;\u03b1) discussed in section 2.2, these expectations can be computed analytically (see supplementary material). For the models where the expectations are not analytically tractable, we can estimate them using Monte Carlo Markov chain sampling as well.\nThe main bottleneck of the algorithm is the number of samplings per gradient update. Since all the connections between the visible and the hidden units are probabilistic, the random variables need to be sampled a quadratic number of times compared to learning an RBM. However, due to the independencies introduced in Eq. (9), the variablesW , b, and c can be sampled in parallel."}, {"heading": "3 EXPERIMENTS", "text": "We introduce the concept of stochastic representations by considering a semi-supervised learning scenario where a large amount of data is available but with only a few labeled examples per class. RBSE is a generative model, and hence it can be trained in unsupervised fashion on the entire dataset. Once the ensemble distribution P (\u03b8;\u03b1) is tuned to the data, for every labeled training example, we can conditionally sample models \u03b8 \u223c P (\u03b8 | v) and then use each model to generate a representation based on the activation of the hidden units h. In other words, the generative stochastic ensembles can be used to store the information about the entire dataset and effectively augment the number of labeled examples by generating multiple stochastic representations. This is analogous to the concept of corrupted features (Maaten et al., 2013; Wager et al., 2013). The main difference is that RBSEs can learn from the unlabeled part of the dataset how to corrupt the data properly.\nTo test the concept, we implemented Bernoulli and a Gaussian RBSE using Theano library (Bergstra et al., 2010). We trained our generative models on a two-dimensional synthetic datasets and on MNIST digits. The following sections provide the details on each of the experiments."}, {"heading": "3.1 SYNTHETIC DATA", "text": "In order to visually test how a stochastic ensemble can capture the training data manifold, we generated several simple two-dimensional datasets {x1, x2} \u2208 [0, 1]2 (Figure 2). We trained an ordinary RBM with 2 visible and 8 hidden units and an RBSE of the same structure. Using these models, we mapped the testing points to the latent space multiple times, and then back to the original space.\nFor RBM, we used the mean field activations for new representations: hnew = P (h | v) and vnew = P (v | hnew). Unsurprisingly, the two consecutive transformations, from visible to hidden and back to visible space, performed by a properly trained RBM always mapped the testing points to themselves (Figure 2a). Notice that this holds not only for RBM. Other point-wise deterministic representation learning techniques, e.g., autoencoders (Bengio, 2009), exhibit the same behavior.\nWe performed a similar procedure multiple times for RBSE: first, we sampled a model from the conditional distribution P (\u03b8 | v), then using P (h | v, \u03b8\u0303), transformed testing points from the visible to the hidden space, then mapped backwards using the mean-field model from the ensemble. Stochastic representations of the testing data, when mapped back to the visible space, were distributed along the training data manifold near the testing points they belonged to (Figure 2b).\nThe experiments also demonstrated that the representations of an isolated testing point (an outlier) will be attracted towards the manifold captured by RBSE (Figure 2c). Finally, an entire manifold can be captured by the generated stochastic representations for only a small number of initial data points (Figure 2d). These experiments confirmed the capability of RBSEs to refine its internal distribution over the models and to capture the variance of the data. Therefore, the stochasticity of such tunable ensembles should provide better regularization than just arbitrary noise injection."}, {"heading": "3.2 ONE-SHOT LEARNING OF MNIST DIGITS", "text": "To test stochastic representations in the semi-supervised setting where only a few labeled data are available (one example per class), we performed experiments on MNIST digits dataset. We trained a Bernoulli RBSE model with 784 visible and 400 hidden units on 50,000 unlabeled MNIST digits using the proposed Algorithm 1 with a number of MCMC steps k = 1. The learned filters (i.e., values of W\u0304ij) and the Bernoulli connection probabilities (i.e., pij) arranged into tiles are presented on Figure 3. Also notice that the connection probabilities encode a lot of structure (Figure 3b). For the purpose of comparison, we also trained an RBM of the same configuration on the same unlabeled MNIST data.\nFurther, we sampled 1000 objects (100 per class) from the rest 20,000 MNIST digits: 1 training and 99 testing examples per class. For every training sample we used different approaches to construct the representations: (a) image pixels, (b) deterministic representations constructed by a pre-trained RBM, (c) stochastic representations constructed by an RBM with Bernoulli connection probabilities p = 0.5 for every connection (equivalent to DropConnect), and (d) RBSE-generated stochastic representations. In the (c) and (d) cases, we constructed 10 representations for every object.\nFinally, we trained and tested a logistic regression classifier (with no additional regularization) on the these representations under one-shot learning constraints. The classification experiments were done multiple times for differently sampled objects. The results are presented on Figure 3c. About 10% improvement in classification accuracy is due to better representations learned by RBM (disentanglement of the classes). When we regularize the classifier by generating stochastic representations with DropConnect noise applied to trained RBM, the performance slightly drops. On the contrary, when the classifier is regularized through the representations generated by RBSE, we get about another 5% increase in accuracy on average."}, {"heading": "4 DISCUSSION", "text": "In this paper, we introduced the concept non-deterministic representations that can be learned by Energy-based Stochastic Ensembles tuned to the data in unsupervised fashion. The ensemble stochasticity can capture the data variance and be used to adaptively regularize discriminative models and improve their performance in semi-supervised settings. The actual learning of a proper model perturbation from the data is the conceptual difference from the previous work (Bachman et al., 2014; Maaten et al., 2013; Wager et al., 2013). We demonstrated the power of stochastic ensembles visually on synthetic two-dimensional data and quantitatively on one-shot learning of the MNIST hand-written digits.\nThe inspiration from synaptic stochasticity observed in biological nerve cells provides a number of insights and hypotheses for experimental neuroscience, which we will report separately elsewhere. From the artificial neural networks perspective, the proposed approach of using stochastic connec-\ntions between neural units is interesting as well. For example, similar stochastic ensembles of feedforward neural networks should be able to capture complex multi-modal data manifolds (Tang & Salakhutdinov, 2013). Also, recently proposed Generative Stochastic Networks (GSNs), which are used to encode probabilities in their sampling behavior (Bengio & Thibodeau-Laufer, 2013; Bengio et al., 2013), can be naturally endorsed with non-deterministic connections and might potentially realize richer families of distributions.\nInterestingly, biological inspiration also suggests that neuromorphic computers that operate in a massively parallel fashion, while consuming a faction of the power of digital computers (Merolla et al., 2014) can be leveraged to carry out the operations necessary for RBSEs. They often natively support Bernoulli synaptic stochasticity (Goldberg et al., 2001), as well as neuromorphic variants of RBMs can be efficiently implemented and trained (Neftci et al., 2014). This suggests that the disadvantages associated to the computational overhead of RBSEs can be nullified by using an appropriate computational substrate."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Cian O\u2019Donnell for helpful discussions. This work was funded in part by the National Science Foundation (NSF EFRI-1137279) and the Office of Naval Research (ONR MURI 14-13-1-0205). M.A. was supported by KAUST Graduate Fellowship."}, {"heading": "A POSTERIOR BERNOULLI AND GAUSSIAN DISTRIBUTIONS", "text": "This section provides details on the posterior distributions for Bernoulli and Gaussian stochastic ensembles. RBM has three sets of parameters: connection strengths between the visible and hidden layers W \u2208 RD\u00d7K , bias b \u2208 RD for visible, and bias c \u2208 RK for hidden units. We use i to index D visible dimensions, and j for K hidden dimensions. We denote unnormalized measures by P\u0303 (\u00b7). When the equations for Wij , bi, and cj have the same form, we refer to these components as \u03b8k.\nFor some fixed (v,h) we know that RBSE\u2019s energy is a linear function of \u03b8. Since the prior P (\u03b8;\u03b1) factorizes over all the components of \u03b8 according to our definition. Thus, we can factorize unnormalized P\u0303 (\u03b8,v,h;\u03b1) over all \u03b8k:\nP\u0303 (Wij , vi, hj ;\u03b1ij) = P (Wij ;\u03b1ij) exp(vihjWij),\nP\u0303 (bi, vi;\u03b1i) = P (bi;\u03b1i) exp(vibi),\nP\u0303 (cj , hj ;\u03b1j) = P (cj ;\u03b1j) exp(hjcj).\n(A1)\nWe can further renormalize (A1) and obtain the following posterior distribution: P (W ,b,c | v,h;\u03b1) = \u220f ij P (Wij , vi, hj ;\u03b1ij) \u220f i P (bi, vi;\u03b1i) \u220f j P (cj , hj ;\u03b1j)\u222b \u220f\nij P (Wij , vi, hj ;\u03b1ij)dW \u220f i P (bi, vi;\u03b1i)db \u220f j P (cj , hj ;\u03b1j)dc\n= \u220f\nij\nP (Wij , vi, hj ;\u03b1ij)\u222b P (Wij , vi, hj ;\u03b1ij)dWij\n\u220f\ni\nP (bi, vi;\u03b1i)\u222b P (bi, vi;\u03b1i)dbi\n\u220f\nj\nP (cj , hj ;\u03b1j)\u222b P (cj , hj ;\u03b1j)dcj\n= \u220f\nij\nP (Wij | vi, hj ;\u03b1ij) \u220f\ni\nP (bi | vi;\u03b1i) \u220f\nj\nP (cj | hj ;\u03b1j).\n(A2) As we see, if the prior distribution P (\u03b8;\u03b1) factorizes over the components of \u03b8, the posterior distribution P (\u03b8 | v,h;\u03b1) is also factorisable. Finally, we need to find the posterior distribution for each of the components for the Bernoulli and Gaussian ensembles.\nFor Bernoulli case, a priori, every component \u03b8k can either take some non-zero value \u03b8\u0304k with probability (1\u2212pk), or be equal to zero with probability pk. Then, the distribution P (\u03b8;\u03b1) is parametrized by \u03b1 = (\u03b8\u0304k, pk) and has the following form:\nP (\u03b8;\u03b1) = \u220f\nk\nP (\u03b8k;\u03b1k) = \u220f\nk\n[ \u03b4(\u03b8k)(1\u2212 pk) + \u03b4(\u03b8k \u2212 \u03b8\u0304k)pk ] , (A3)\nwhere \u03b4(\u00b7) is the Dirac delta function. The posterior for Wij (similar for bi and cj) will be\nP (Wij | vi, hj ;\u03b1ij) = ( \u03b4(Wij)(1\u2212 pij) + \u03b4(Wij \u2212 W\u0304ij)pij ) exp(vihjWij)\n1\u2212 pij + pij exp(vihjWij) (A4)\nFor Gaussian case, components \u03b8k a priori have a Gaussian distribution with the mean \u03b8\u0304k and variance \u03c32k, i.e., \u03b1k = (\u03b8\u0304k, \u03c3k). Omitting the details of integration, the posterior for Wij will have the following form (similar expressions are easy to derive for bi and cj):\nP (Wij | vi, hi;\u03b1ij) = 1\u221a\n2\u03c0\u03c3ij exp\n \u2212 [ Wij \u2212 (W\u0304ij + vihi\u03c32ij)\u221a\n2\u03c3ij\n]2  . (A5)\nIn both Bernoulli and Gaussian cases, due to the linear structure of the RBSE energy function and the structure of the prior P (\u03b8;\u03b1), we are able to get analytical expressions for the posterior of each component of \u03b8. Moreover, the derivations will be the same not only for Bernoulli and Gaussian cases, but for any P (\u03b8;\u03b1) that is completely factorisable over {\u03b8k}.\nar X\niv :1\n41 2.\n72 72\nv1 [\ncs .L\nG ]\n2 3\nD ec\n2 01\n4"}, {"heading": "B EXPECTATIONS OVER THE POSTERIOR", "text": "Here we provide details on the analytical computation of the expected stochastic gradient update for the two types of stochastic ensembles: Bernoulli and Gaussian. For the sake of space, as in Appendix A, when there is no ambiguity, we denote the components ofW ,b,c generally as \u03b8k.\nThe general form of the gradient expectation is the following (see Eq. 10 in the main text): \u222b\n\u0398\nP (\u03b8 | v,h;\u03b1)\u2202\u03c6(\u03b8;\u03b1) \u2202\u03b1 d\u03b8. (B1)\nNow, we start with the Bernoulli case, where \u03b1k = (\u03b8k, pk). The prior distribution and the \u03c6 potential have the following form:\nP (\u03b8k;\u03b1k) \u2261 e\u2212\u03c6(\u03b8k;\u03b1k) = \u03b4(\u03b8k)(1\u2212 pk) + \u03b4(\u03b8k \u2212 \u03b8\u0304k)pk, \u2202\u03c6\n\u2202pk = \u03b4(\u03b8k)\u2212 \u03b4(\u03b8k \u2212 \u03b8\u0304k) e\u2212\u03c6(\u03b8k;\u03b1k) , \u2202\u03c6 \u2202\u03b8\u0304k = \u03b4\u2032(\u03b8k \u2212 \u03b8\u0304k)pk e\u2212\u03c6(\u03b8k;\u03b1k) , (B2)\nwhere \u03b4(\u00b7) is the Dirac delta function, and \u03b4\u2032(\u00b7) is its derivative. When we plug (A2) and (B2) into (B1), we obtain the following expressions for the expectation over Wij (similar for bi and ci):\nE [ \u2202\u03c6\n\u2202pij | vi, hi\n] =\n1\u2212 pij 1\u2212 pij + pij exp(vihiW\u0304ij)\n( 1\u2212 evihiW\u0304ij ) ,\nE [ \u2202\u03c6\n\u2202W\u0304ij | vi, hi\n] =\npij 1\u2212 pij + pijevihiW\u0304ij \u222b \u03b4\u2032(Wij \u2212 W\u0304ij) exp(vihiWij)dWij\n= [integrating by parts] = pije\nvihiW\u0304ij\n1\u2212 pij + pij exp(vihiW\u0304ij) (\u2212vihi).\n(B3)\nEventually, we get the following expressions for the gradient components of the log-likelihood:\n\u2202 logP (v;\u03b1)\n\u2202W\u0304ij = \u3008P (Wij 6= 0 | vi, hi) vihi\u3009data \u2212 \u3008P (Wij 6= 0 | vi, hi) vihi\u3009recon ,\n\u2202 logP (v;\u03b1) \u2202p\u0304ij = \u2329 P (Wij = 0 | vi, hi) ( 1\u2212 evihiW\u0304ij )\u232a data \u2212\n\u2329 P (Wij = 0 | vi, hi) ( 1\u2212 evihiW\u0304ij )\u232a recon .\n(B4)\nThe gradient over W\u0304ij resembles the original contrastive divergence but has additional posterior probability multipliers.\nSimilarly, we do the same derivations for the Gaussian case, where \u03b1k = (\u03b8\u0304k, \u03c3k). The prior and the \u03c6 derivatives have the following form:\nP (\u03b8k;\u03b1k) \u2261 e\u2212\u03c6(\u03b8k;\u03b1k) = 1\u221a\n2\u03c0\u03c3k exp\n[ \u2212 ( \u03b8k \u2212 \u03b8\u0304k\u221a\n2\u03c3k\n)2] ,\n\u2202\u03c6 \u2202\u03b8\u0304k = \u03b8\u0304k \u2212 \u03b8k \u03c32k , \u2202\u03c6 \u2202\u03c3k = 1 \u03c3k\n[ 1\u2212 ( \u03b8k \u2212 \u03b8\u0304k \u03c3k )2] .\n(B5)\nAfter marginalization, we obtain the following expressions for Wij (similar for bi and cj):\nE [ \u2202\u03c6\n\u2202W\u0304ij | vi, hi\n] = \u2212vihi, E [ \u2202\u03c6\n\u2202\u03c3ij | vi, hi\n] = \u2212v2i h2i\u03c3ij , (B6)\nand get the following expressions for the gradient of the log-likelihood components:\n\u2202 logP (v;\u03b1)\n\u2202W\u0304ij = \u3008vihi\u3009data \u2212 \u3008vihi\u3009recon ,\n\u2202 logP (v;\u03b1) \u2202\u03c3\u0304ij = \u2329 v2i h 2 j\u03c3ij \u232a data \u2212 \u2329 v2i h 2 j\u03c3ij \u232a recon .\n(B7)\nNotice that when W , b, c are deterministic (i.e., pij = 0 and \u03c3k = 0 for Bernoulli and Gaussian ensembles, respectively), the expected derivatives over W\u0304ij (the same is for b\u0304i and c\u0304j) give the same expressions as the standard contrastive divergence. Thus, RBM is the deterministic limit of RBSE.\nAnother point is related to the implementation: During the optimization, the algorithm steps might be suboptimal, hence we restrict probabilities be in the range [ , 1 \u2212 ] for some 1. In other words, we do not allow the model occasionally turn into classic RBM by keeping some - stochasticity in the connection strengths."}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Pattern recognition and machine learning, volume 1", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "The probability of neurotransmitter release: variability and feedback control at single synapses", "author": ["Branco", "Tiago", "Staras", "Kevin"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "Branco et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branco et al\\.", "year": 2009}, {"title": "Untangling invariant object recognition", "author": ["DiCarlo", "James J", "Cox", "David D"], "venue": "Trends in cognitive sciences,", "citeRegEx": "DiCarlo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "DiCarlo et al\\.", "year": 2007}, {"title": "Probabilistic synaptic weighting in a reconfigurable network of VLSI integrate-and-fire neurons", "author": ["D.H. Goldberg", "G. Cauwenberghs", "A.G. Andreou"], "venue": "Neural Networks,", "citeRegEx": "Goldberg et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2001}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "M Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Learning with marginalized corrupted features", "author": ["Maaten", "Laurens", "Chen", "Minmin", "Tyree", "Stephen", "Weinberger", "Kilian Q"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Maaten et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2013}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Merolla", "Paul A", "Arthur", "John V", "Alvarez-Icaza", "Rodrigo", "Cassidy", "Andrew S", "Sawada", "Jun", "Akopyan", "Filipp", "Jackson", "Bryan L", "Imam", "Nabil", "Guo", "Chen", "Nakamura", "Yutaka"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 2014}, {"title": "Event-driven contrastive divergence for spiking neuromorphic systems", "author": ["E. Neftci", "S. Das", "B. Pedroni", "K. Kreutz-Delgado", "G. Cauwenberghs"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "Neftci et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neftci et al\\.", "year": 2014}, {"title": "Changes in reliability of synaptic function as a mechanism for plasticity", "author": ["Stevens", "Charles F", "Wang", "Yanyan"], "venue": "Nature, 371(6499):704\u2013707,", "citeRegEx": "Stevens et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Stevens et al\\.", "year": 1994}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Dropout training as adaptive regularization", "author": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Maaten et al. (2013) recently demonstrated that the artificial data augmentation via feature corruption effectively plays the role of a data-adaptive regularization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Maaten et al. (2013) recently demonstrated that the artificial data augmentation via feature corruption effectively plays the role of a data-adaptive regularization. Wager et al. (2013) also showed that the dropout techniques applied to generalized linear models result into an adaptive regularization.", "startOffset": 0, "endOffset": 186}, {"referenceID": 8, "context": "The Dropout (Hinton et al., 2012) and DropConnect (Wan et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 16, "context": ", 2012) and DropConnect (Wan et al., 2013) techniques are successful examples of using a particular form of stochastic ensemble in the context of feedforward neural networks.", "startOffset": 24, "endOffset": 42}, {"referenceID": 10, "context": "Injection of arbitrary noise improves the robustness of the model to the corruption process (Maaten et al., 2013), but it does not necessarily capture the information about the generative process behind the actual data.", "startOffset": 92, "endOffset": 113}, {"referenceID": 8, "context": "The Dropout (Hinton et al., 2012) and DropConnect (Wan et al., 2013) techniques are successful examples of using a particular form of stochastic ensemble in the context of feedforward neural networks. A unified framework for a collection of perturbed models (which also encompasses the data corruption methods) was recently introduced by Bachman et al. (2014): An arbitrary noise process was used to perturb a parametric parent model to generate an ensemble of child models.", "startOffset": 13, "endOffset": 360}, {"referenceID": 9, "context": "where \u03b8 denotes the model parameters, E(v,h; \u03b8) is a parametric scalar energy function (LeCun et al., 2006), and Z(\u03b8) is the normalizing coefficient (partition function).", "startOffset": 87, "endOffset": 107}, {"referenceID": 16, "context": "This case is similar to DropConnect (Wan et al., 2013) technique but with adaptive distributions over the connections between visible and hidden layers in RBM.", "startOffset": 36, "endOffset": 54}, {"referenceID": 10, "context": "This is analogous to the concept of corrupted features (Maaten et al., 2013; Wager et al., 2013).", "startOffset": 55, "endOffset": 96}, {"referenceID": 15, "context": "This is analogous to the concept of corrupted features (Maaten et al., 2013; Wager et al., 2013).", "startOffset": 55, "endOffset": 96}, {"referenceID": 2, "context": "To test the concept, we implemented Bernoulli and a Gaussian RBSE using Theano library (Bergstra et al., 2010).", "startOffset": 87, "endOffset": 110}, {"referenceID": 10, "context": "The actual learning of a proper model perturbation from the data is the conceptual difference from the previous work (Bachman et al., 2014; Maaten et al., 2013; Wager et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 15, "context": "The actual learning of a proper model perturbation from the data is the conceptual difference from the previous work (Bachman et al., 2014; Maaten et al., 2013; Wager et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 0, "context": "Also, recently proposed Generative Stochastic Networks (GSNs), which are used to encode probabilities in their sampling behavior (Bengio & Thibodeau-Laufer, 2013; Bengio et al., 2013), can be naturally endorsed with non-deterministic connections and might potentially realize richer families of distributions.", "startOffset": 129, "endOffset": 183}, {"referenceID": 11, "context": "Interestingly, biological inspiration also suggests that neuromorphic computers that operate in a massively parallel fashion, while consuming a faction of the power of digital computers (Merolla et al., 2014) can be leveraged to carry out the operations necessary for RBSEs.", "startOffset": 186, "endOffset": 208}, {"referenceID": 6, "context": "They often natively support Bernoulli synaptic stochasticity (Goldberg et al., 2001), as well as neuromorphic variants of RBMs can be efficiently implemented and trained (Neftci et al.", "startOffset": 61, "endOffset": 84}, {"referenceID": 12, "context": ", 2001), as well as neuromorphic variants of RBMs can be efficiently implemented and trained (Neftci et al., 2014).", "startOffset": 93, "endOffset": 114}], "year": 2017, "abstractText": "The goal of a generative model is to capture the distribution underlying the data, typically through latent variables. After training, these variables are often used as a new representation, more effective than the original features in a variety of learning tasks. However, the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space. Thus, even with representations robust to class-specific transformations, statistically driven models trained on them would not be able to generalize when the labeled data is scarce. Inspired by the stochasticity of the synaptic connections in the brain, we introduce Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic representations, i.e., mappings from the feature space to a family of distributions in the latent space. These mappings are encoded in a distribution over a (possibly infinite) collection of models. By conditionally sampling models from the ensemble, we obtain multiple representations for every input example and effectively augment the data. We propose an algorithm similar to contrastive divergence for training restricted Boltzmann stochastic ensembles. Finally, we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on MNIST.", "creator": "LaTeX with hyperref package"}}}