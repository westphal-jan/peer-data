{"id": "1501.03273", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2015", "title": "Classification with Low Rank and Missing Data", "abstract": "bhikkhunis We hub consider classification and regression tasks demokratie where kharrazi we have geres missing soule data and poldi assume gaiser that vannatta the (clean) data conein resides in travelin a low specialised rank subspace. commingle Finding prologue a hidden subspace shefki is known methanethiol to be 500-600 computationally hard. Nevertheless, cbc-tv using a non - isometries proper formulation we ajet give unece an efficient agnostic algorithm 1.4250 that classifies as shoygu good nam as epo the jiaying best rocklahoma linear dommel classifier gilhooly coupled imamate with smartruck the best ratelle low - dimensional onoto subspace cibulkova in which the data cone-like resides. A direct www.thebastardmachine.com implication dough is pavillion that our 1709 algorithm can knopf linearly (safes and non - linearly frandsen through d'abbadie kernels) harrod classify shmueli provably as calibres well chatterley as the best classifier ardiden that railyard has self-handicapping access pilanesberg to the endothelial full songkran data.", "histories": [["v1", "Wed, 14 Jan 2015 08:16:50 GMT  (38kb,D)", "http://arxiv.org/abs/1501.03273v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hazan", "roi livni", "yishay mansour"], "accepted": true, "id": "1501.03273"}, "pdf": {"name": "1501.03273.pdf", "metadata": {"source": "CRF", "title": "Classification with Low Rank and Missing Data", "authors": ["Elad Hazan", "Roi Livni", "Yishay Mansour"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The importance of handling correctly missing data is a fundamental and classical challenge in machine learning. There are many reasons why data might be missing. For example, consider the medical domain, some data might be missing because certain procedures were not performed on a given patient, other data might be missing because the patient choose not to disclose them, and even some data might be missing due to malfunction of certain equipment. While it is definitely much better to have always complete and accurate data, this utopian desire is not the reality many times. For this reason we need to utilize the available data even if some of it is missing.\nAnother, very different motivation for missing data are recommendations. For example, a movie recommendations dataset might have users opinions on certain movies, which is the case, for example, in the Netflix motion picture dataset. Clearly, no user has seen or reviewed all movies, or even close to it. In this respect recommendation data is an extreme case: the vast majority is usually missing (i.e., it is sparse to the extreme).\nMany times we can solve the missing data problem since the data resides on a lower dimension manifold. In the above examples, if there are prototypical users (or patients) and any user is a mixture of the prototypical users, then this implicitly suggests that the data is low rank. Another way to formalize this assumption is to consider the data in a matrix form, say, the users are rows and movies are columns, then our assumption is that the true complete matrix has a low rank.\nOur starting point is to consider the low rank assumption, but to avoid any explicit matrix completion, and instead directly dive in to the classification problem. At the end of the introduction we show that matrix completion is neither sufficient and/or necessary.\nWe consider perhaps the most fundamental data analysis technique of the machine learning toolkit: linear (and kernel) classification, as applied to data where some (or even most) of the attributes in an example might be missing. Our main result is an efficient algorithm for linear and kernel classification that performs as well as the best classifier that has access to all data, under low rank assumption with natural non-degeneracy conditions.\nWe stress that our result is worst case, we do not assume that the missing data follows any probabilistic rule other than the underlying matrix having low rank. This is a clear contrast to most existing matrix completion algorithms. We also cast our results in a distributional setting, showing that the classification error that we achieve is close to the best classification using the subspace of the examples (and with no missing data). Notably, many variants of the problem \u2217Princeton University and Microsoft Research \u2020Hebrew University and Microsoft Research \u2021Tel Aviv University and Microsoft Research\nar X\niv :1\n50 1.\n03 27\n3v 1\n[ cs\n.L G\n] 1\n4 Ja\nn 20\n15\nof finding a hidden subspace are computationally hard (see e.g. Berthet & Rigollet (2013)), yet as we show, learning a linear classifier on a hidden subspace is non-properly learnable.\nAt a high level, we assume that a sample is a triplet (x,o, y), where x \u2208 Rd is the complete example, o \u2282 {1, . . . , d} is the set of observable attributes and y \u2208 Y is the label. The learner observes only (xo, y), where xo omits any attribute not in o. Our goal is given a sample S = {(x(i)o , y(i))}mi=1 to output a classifier hS such that w.h.p.:\nE [`(hS(xo), y)] \u2264 min w\u2208Rd \u2016w\u2016\u22641 E [`(w \u00b7 x, y)] + ,\nwhere ` is the loss function. Namely, we like our classifier hS to compete with the best linear classifier for the completely observable data.\nOur main result is achieving this task (under mild regularity conditions) using a computationally efficient algorithm for any convex Lipschitz-bounded loss function. Our basic result requires a sample size which is quasi-polynomial, but we complement it with a kernel construction which can guarantee efficient learning under appropriate large margin assumptions. Our kernel depends only on the intersection of observable values of two inputs, and is efficiently computable. (We give a more detailed overview of our main results in Section 2.)\nPreliminary experimental evidence indicates our theoretical contributions lead to promising classification performance both on synthetic data and on publicly-available recommendation data. This will be detailed in the full version of this paper.\nPrevious work. Classification with missing data is a well studied subject in statistics with numerous books and papers devoted to its study, (see, e.g., Little & Rubin (2002)). The statistical treatment of missing data is broad, and to a fairly large extent assumes parametric models both for the data generating process as well as the process that creates the missing data. One of the most popular models for the missing data process is Missing Completely at Random (MCAR) where the missing attributes are selected independently from the values.\nWe outline a few of the main approaches handling missing data in the statistics literature. The simplest method is simply to discard records with missing data, even this assumes independence between the examples with missing values and their labels. In order to estimate simple statistics, such as the expected value of an attribute, one can use importance sampling methods, where the probability of an attribute being missing can depend on it value (e.g., using the Horvitz-Thompson estimator Horvitz & Thompson (1952)). A large body of techniques is devoted to imputation procedures which complete the missing data. This can be done by replacing a missing attribute by its mean (mean imputation), or using a regression based on the observed value (regression imputation), or sampling the other examples to complete the missing value (hot deck). 1 The imputation methodologies share a similar goal as matrix completion, namely reduce the problem to one with complete data, however their methodologies and motivating scenarios are very different. Finally, one can build a complete Bayesian model for both the observed and unobserved data and use it to perform inference. As with almost any Bayesian methodology, its success depends largely on selecting the right model and prior, this is even ignoring the computational issues which make inference in many of those models computationally intractable.\nIn the machine learning community, missing data was considered in the framework of limited attribute observability Ben-David & Dichterman (1998) and its many refinements Dekel et al. (2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data. Their method, however, does not entail theoretical gaurantees on reconstruction in the worst case, and gives rise to non-convex programs.\nA natural and intuitive methodology to follow is to treat the labels (both known and unknown) as an additional column in the data matrix and complete the data using a matrix completion algorithm, thereby obtaining the classification. Indeed, this exactly was proposed by Goldberg et al. (2010). Although this is a natural approach, we show that\n1We remark that our model implicitly includes mean-imputation or 0-imputation method and therefore will always outperform them.\ncompletion is neither necessary nor sufficient for classification. Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011). The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al. (2012), which we were not able to use for our purposes.\nIs matrix completion sufficient and/or necessary? We demonstrate that classification with missing data is provably different from that of matrix completion. We start by considering a learner that tries to complete the missing entries in an unsupervised manner and then performs classification on the completed data, this approach is close akin to imputation techniques, generative models and any other two step \u2013 unsupervised/supervised algorithm. Our example shows that even under realizable assumptions, such an algorithm may fail. We then proceed to analyze the approach previously mentioned \u2013 to treat the labels as an additional column.\nTo see that unsupervised completion is insufficient for prediction, consider the example in Figure 1: the original data is represented by filled red and green dots and it is linearly separable. Each data point will have one of its two coordinates missing (this can even be done at random. In the figure the arrow from each instance points to the observed attribute. However, the rank-one completion of projection onto the pink hyperplane is possible, and admits no separation. The problem is clearly that the mapping to a low dimension is independent from the labels, and therefore we should not expect that properties that depend on the labels, such as linear separability, will be maintained.\nNext, consider a learner that treats the labels as an additional column. Goldberg et al. (2010) Considered the following problem:\nminimize Z rank(Z) subject to: Zi,j = xi,j , (i, j) \u2208 \u2126 , . (G)\nwhere \u2126 is the set of observed attributes (or observed labels for the corresponding columns). Now assume that we always see one of the following examples: [1, \u2217, 1, \u2217], [\u2217, \u2212 1, \u2217, \u2212 1], or [1, ,\u22121, 1, \u2212 1]. The observed labels are respectively 1,\u22121 and 1. A typical data matrix with one test point might be of the form:\nM =  1 \u2217 1 \u2217 1 \u2217 \u22121 \u2217 \u22121 \u22121 1 \u22121 1 \u22121 1 1 \u2217 1 \u2217 \u2217  (1) First note that there is no 1-rank completion of this matrix. On the other hand, we will show that there is more than one 2-rank completion each lead to a different classification of the test point. The first possible completion is to complete odd columns to a constant one vector, and even column vectors to a constant \u22121 vector. Then complete the labeling whichever way you choose. Clearly there is no hope for this completion to lead to any meaningful result as the label vector is independent of the data columns. On the other hand we may complete the first and last rows to a constant 1 vector, and the second row to a constant \u22121 vector. All possible completions lead to an optimal solution w.r.t Problem G but have different outcome w.r.t classification. We stress that this is not a sample complexity issue. Even if we observe abundant amount of data, the completion task is still ill-posed.\nFinally, matrix completion is also not necessary for prediction. Consider movie recommendation dataset with two separate populations, French and Chinese, where each population reviews a different set of movies. Even if each population has a low rank, performing successful matrix completion, in this case, is impossible (and intuitively it does not make sense in such a setting). However, linear classification in this case is possible via a single linear classifier, for example by setting all non-observed entries to zero. For a numerical example, return to the matrix M in Eq. 1. Note that we observe only three instances hence the classification task is easy but doesn\u2019t lead to reconstruction of the missing entries."}, {"heading": "2 Problem Setup and Main Result", "text": "We begin by presenting the general setting: A vector with missing entries can be modeled as a tuple x \u00d7 o, where x \u2208 Rd and o \u2208 2d is a subset of indices. The vector x represents the full data and the set o represents the observed attributes. Given such a tuple, let us denote by xo a vector in (R \u222a {\u2217})d such that\n(xo)i = { xi i \u2208 o \u2217 else\nThe task of learning a linear classifier with missing data is to return a target function over xo that competes with best linear classifier over x. Specifically, a sequence of triplets {(x(i).o(i).yi)}mi=1 is drawn iid according to some distribution D. An algorithm is provided with the sample S = {(xioi , yi)} m i=1 and should return a target function fS over missing data such that w.h.p:\nE [`(fS(xo), y))] \u2264 min w\u2208Bd(1) E [`(w \u00b7 x, y))] + , (2)\nwhere ` is the loss function and Bd(r) denotes the Euclidean ball in dimension d of radius \u221a r. For brevity, we will say that a target function fS is -good if Eq. 2 holds. Without any assumptions on the distribution D, the task is ill-posed. One can construct examples where the learner over missing data doesn\u2019t have enough information to compete with the best linear classifier. Such is the case when, e.g., yi is some attribute that is constantly concealed and independent of all other features. Therefore, certain assumptions on the distribution must be made.\nOne reasonable assumption is to assume that the marginal distribution D over x is supported on a small dimensional linear subspaceE and that for every set of observations, we can linearly reconstruct the vector x from the vector Pox, where Po : Rd \u2192 R|o| is the projection on the observed attributes. In other words, we demand that the mapping Po|E : E \u2192 PoE, which is the restriction of Po to E, is full-rank. As the learner doesn\u2019t have access to the subspace E, the learning task is still far from trivial.\nWe give a precise definition of the last assumption in Assumption 1. Though our results hold under the low rank assumption the convergence rates we give depend on a certain regularity parameter. Roughly, we parametrize the \u201ddistance\u201d of Po|E from singularity, and our results will quantitively depend on this distance. Again, we defer all rigorous definitions to Section 3.2.\nOur first result is a an upper bound on the sample complexity of the problem. We then proceed to a more general statement that entails an efficient kernel-based algorithm."}, {"heading": "2.1 Main Result", "text": "Theorem 1 (Main Result). Assume that ` is a L-Lipschitz convex loss function Let D be a \u03bb-regular distribution (see Definition 1) Let \u03b3( ) \u2265 log 2L/(\u03bb )\u03bb and\n\u0393( ) = d\u03b3( )+1 \u2212 d d\u2212 1 .\nThere exists an algorithm (independent ofD) that receives a sample S = {(xioi , yi)} m i=1 of sizem \u2208 \u2126\n( L2\u0393( )2 log 1/\u03b4\n2 ) and returns a target function fS that is -good with probability at least (1\u2212 \u03b4). The algorithm runs in time poly(|S|).\nTheorem 1 gives an upper bound on the computational and sample complexity of learning a linear classifier with missing data under the low rank assumption. As the sample complexity is quasipolynomial, this has limited practical value in many situations. However, as the next theorem states, fS can actually be computed by applying a kernel trick. Thus, under further large margin assumptions we can significantly improve performance.\nTheorem 2. For every \u03b3 \u2265 0, there exists an embedding over missing data\n\u03c6\u03b3 : xo \u2192 R\u0393 such that \u0393 = \u2211\u03b3 k=1 d k = d \u03b3+1\u2212d d\u22121 , and the scalar product between two samples \u03c6\u03b3(x 1 o1) and \u03c6\u03b3(x 2 o2) can be efficiently computed, specifically it is given by the formula:\nk\u03b3(x 1 o1 ,x 2 o2) := |o(1) \u2229 o(2)|\u03b3 \u2212 1 |o(1) \u2229 o(2)| \u2212 1\n\u00d7 \u2211\ni\u2208o(1)\u2229o(2) x\n(1) i \u00b7 x (2) i .\nIn addition, let ` be an L-Lipschitz loss function and S = {xioi} m i=1 a sample drawn iid according to a distribution D. We make the assumption that \u2016Pox\u2016 \u2264 1 a.s. The followings hold: 1. At each iteration of Alg. 1 we can efficiently compute v>t \u03c6\u03b3(x t ot) for any new example x t ot . Specifically it is\ngiven by the formula\nv>t \u03c6\u03b3(x t ot) := t\u2211 i=1 \u03b1 (t) i k(x i oi ,x t ot).\nHence Alg. 1 runs in poly(T ) time and sequentially produces target functions ft(xo) = v>t \u03c6\u03b3(xo) that can be computed at test time in poly(T ) time.\n2. Run Alg. 1 with \u03b7t = Ct , \u03c1 = 1 and T . Let v\u0304 = 1 T \u2211m t=1 vt, then with probability (1\u2212 \u03b4):\n1 2 \u2016v\u0304\u20162 + C m m\u2211 i=1 `(v\u0304>\u03c6\u03b3(x i oi), yi) \u2264 min 1 2 \u2016v\u20162 + C m m\u2211 i=1 [ `(v>\u03c6\u03b3(x i oi), yi) ] + O\u0303 ( (CL)2\u0393 ln 1/\u03b4 T ) . (3)\n3. For any > 0, if D is a \u03bb-regular distribution and \u03b3 \u2265 log 2L/(\u03bb )\u03bb then for some v \u2217 \u2208 B\u0393(\u0393)\nE [`(v\u2217 \u00b7 \u03c6\u03b3(xo, y)] \u2264 min w\u2208Bd(1) E [`(w \u00b7 \u03c6\u03b3(xo), y)] + .\nTo summarize, Theorem 2 states that we can embed the sample points with missing attributes in a high dimensional, finite, Hilbert space of dimension \u0393, such that:\n\u2022 The scalar product between embeded points can be computed efficiently. Hence, due to the conventional representer argument, the task of empirical risk minimization is tractable.\n\u2022 Following the conventional analysis of kernel methods: Under large margin assumptions in the ambient space, we can compute a predictor with scalable sample complexity and computational efficiency. \u2022 Finally, the best linear predictor over embedded sample points in a \u221a\n\u0393\u2013ball is comparable to the best linear predictor over fully observed data.\nTaken together, we can learn a predictor with sample complexity \u2126(\u03932( )/ 2 log 1\u03b4 ) and Theorem 1 holds. For completeness we present the method together with an efficient algorithm that optimizes the RHS of Eq. 3 via an SGD method. The optimization analysis is derived in a straightforward manner from the work of Shalev-Shwartz et al. (2011). Other optimization algorithms exist in the literature, and we chose this optimization method as it allows us to also derive regret bounds which are formally stronger (see Section 2.2). We stress that the main novelty of this paper is not in any specific optimization algorithm, but the introduction of a new kernel and our guarantees rely solely on it.\nFinally, note that \u03c61 induces the same scalar product as a 0-imputation. In that respect, by considering different \u03b3 = 1, 2, . . . and using a holdout set we can guarantee that our method will outperform the 0-imputation method. By normalizing or adding a bias term we can in fact compete with mean-imputation or any other first order imputation."}, {"heading": "2.2 Regret minimization for joint subspace learning and classification", "text": "A significant technical contribution of this manuscript is the agnostic learning of a subspace coupled with a linear classifier. A subspace is represented by a projection matrix Q \u2208 Rd\u00d7d, which satisfies Q2 = Q. Denote the following class of target functions\nF0 = {fw,Q : w \u2208 Bd, Q \u2208Md\u00d7d, Q2 = Q}\nwhere fw,Q(xo) is the linear predictor defined by w over subspace defined by the matrix Q, as formally defined in definition 2.\nGiven the aforementioned efficient kernel mapping \u03c6\u03b3 , we consider the following kernel-gradient-based online algorithm for classification called KARMA (Kernelized Algorithm for Risk-minimization with Missing Attributes).\nAlgorithm 1 KARMA: Kernelized Algorithm for Risk-minimization with Missing Attributes 1: Input: parameters \u03b3 > 1, {\u03b7t > 0}, 0 < \u03c1 < 1, B > 0 2: for t = 1 to T do 3: Observe example (xtot , yt), suffer loss `(v > t \u03c6\u03b3(x t ot), yt)\n4: Update\n\u03b1 (t) i =  (1\u2212 \u03b7t\u03c1) \u00b7 \u03b1(t\u22121)i i < t \u2212\u03b7t`\u2032(v>t \u03c6\u03b3(xtot)) i = t 0 else\nvt+1 = t\u2211 i=1 \u03b1 (t) i \u03c6\u03b3(x i oi)\n5: end for\nOur main result for the fully adversarial online setting is given next, and proved in the Appendix. Notice that the subspace E\u2217 and associated projection matrix Q\u2217 are chosen by an adversary and unknown to the algorithm.\nTheorem 3. For any \u03b3 > 1, \u03bb > 0, X > 0, \u03c1 > 0, B > 0, L-Lipschitz convex loss function `, and \u03bb-regular sequence {(xt,ot, yt)} w.r.t subspace E\u2217 and associated projection matrix Q\u2217 such that \u2016xt\u2016\u221e < X , Run Algorithm 1 with {\u03b7t = 1\u03c1t}, sequentially outputs {vt \u2208 R t} such that\n\u2211 t `(v>t \u03c6\u03b3(x t ot), yt)\u2212 min\u2016w\u2016\u22641 \u2211 t `(fw,Q\u2217(x t ot), yt) \u2264 2L2X2\u0393 \u03c1 (1 + log T ) + \u03c1 2 T \u00b7B + e \u2212\u03bb\u03b3 \u03bb LT\nIn particular, taking \u03c1 = LX \u221a\n\u0393\u221a BT , \u03b3 = 1\u03bb log T we obtain\u2211 t `(v>t \u03c6\u03b3(x t ot), yt)\u2212 min\u2016w\u2016\u22641 \u2211 t `(fw,Q\u2217(x t ot), yt) = O(XL \u221a \u0393BT )"}, {"heading": "3 Preliminaries and Notations", "text": ""}, {"heading": "3.1 Notations", "text": "As discussed, we consider a model where a distribution D is fixed over Rd \u00d7 O \u00d7 Y , where O = 2d consists of all subsets of {1, . . . , d}. We will generally denote elements of Rd by x,w,v,u and elements of O by o. We denote by Bd the unit ball of Rd, and by Bd(r) the ball of radius \u221a r.\nGiven a subset o we denote by Po : Rd \u2192 R|o| the projection onto the indices in o, i.e., if i1 \u2264 i2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 ik are the elements of o in increasing order then (Pox)j = xij . Given a matrix A and a set of indices o, we let\nAo,o = PoAP > o ."}, {"heading": "3.2 Model Assumptions", "text": "Definition 1 (\u03bb-regularity). We say that D is \u03bb-regular with associated subpsace E if the following happens with probability 1 (w.r.t the joint random variables (x,o)):\n1. \u2016Pox\u2016 \u2264 1.\n2. x \u2208 E.\n3. ker(PoPE) = ker(PE)\n4. If \u03bbo > 0 is a strictly positive singular value of the matrix PoPE then \u03bbo \u2265 \u03bb.\nAssumption 1 (Low Rank Assumption). We say that D satisfies the low rank assumption with asscoicated subspace E if it is \u03bb-regular with associated subspace E for some \u03bb > 0.\nNote that in our setting we assume that \u2016Pox\u2016 \u2264 1 a.s. If \u2016x\u2016 \u2264 1 then \u2016Pox\u2016 \u2264 1 hence our assumption is weaker then assuming x is contained in a fixed sized ball. Further, the assumption can be verified on a sample set with missing attributes.\nNote also that we\u2019ve normalized both w and xo. To achieve guarantees that scale with \u2016w\u2016, note that we can replace the loss function `(w \u00b7 x, y) with `(\u03c1 \u00b7 w \u00b7 x, y) for any constant \u03c1. This will replace L\u2013Lipschitness with \u03c1 \u00b7 L\u2013Lipschitzness in all results."}, {"heading": "4 Learning under low rank assumption and \u03bb-regularity.", "text": "Definition 2 (The class F0). We define the following class of target functions\nF0 = {fw,Q : w \u2208 Bd(1), Q \u2208Md\u00d7d, Q2 = Q}\nwhere fw,Q(xo) = (Pow) \u00b7Q\u2020o,o \u00b7 (Pox).\n(Here M\u2020 denotes the pseudo inverse of M .)\nThe following Lemma states that, under the low rank assumption, the problem of linear learning with missing data is reduced to the problem of learning the class F0, in the sense that the hypothesis class F0 is not less-expressive.\nLemma 1. Let D be a distribution that satisfies the low rank assumption. For every w\u2217 \u2208 Rd there is f\u2217w,Q \u2208 F0 such that a.s:\nf\u2217w,Q(xo) = w \u2217 \u00b7 x.\nIn particular Q = PE and w = P>Ew \u2217 , where PE is the projection matrix on the subspace E."}, {"heading": "4.1 Approximating F0 under regularity", "text": "We next define a surrogate class of target functions that approximates F0\nDefinition 3 (The classes F\u03b3). For every \u03b3 we define the following class\nF\u03b3 = {f\u03b3w,Q : w \u2208 Bd(1), Q \u2208 R d\u00d7d, Q2 = Q}\nwhere,\nf\u03b3w,Q(xo) = (Pow) \u00b7 t\u22121\u2211 j=0 (Qo,o) j \u00b7 (Pox)\nLemma 2. Let (x,o) be a sample drawn according to a \u03bb-regular distribution D with associated subspace E. Let Q = PE and \u2016w\u2016 \u2264 1 then a.s:\n\u2016f\u03b3w,I\u2212Q(xo)\u2212 fw,Q(xo)\u2016 \u2264 (1\u2212 \u03bb)\u03b3\n\u03bb .\nCorollary 1. Let ` be a L-Lipschitz function. Under \u03bb-regularity, for every \u03b3 \u2265 logL/\u03bb \u03bb the class F \u03b3 contains an\n-good target function."}, {"heading": "4.2 Improper learning of F\u03b3 and a kernel trick", "text": "Let G be the set of all finite, non empty, sequences of length at most \u03b3 over d. For each s \u2208 G denote |s|\u2013 the length of the sequence and send the last element of the sequence. Given a set of observations o we write s \u2286 o if all elements of the sequence s belong to o. We let\n\u0393 = \u03b3\u2211 j=1 dj = |G| = d \u03b3+1 \u2212 d d\u2212 1\nand we index the coordinates of R\u0393 by the elements of G:\nDefinition 4. We let \u03c6\u03b3 : (Rd \u00d7O)\u2192 R\u0393 be the embedding:\n(\u03c6\u03b3(xo))s = { xsend s \u2286 o 0 else\nLemma 3. For every Q and w we have:\nf\u03b3w,Q(xo) = \u2211 s1\u2208o ws1xs1 + \u2211 {s:s\u2286o, 2\u2264|s|\u2264t} ws1 \u00b7Qs1,s2 \u00b7Qs2,s3 \u00b7 \u00b7 \u00b7Qs|s|\u22121,send \u00b7 xsend\nCorollary 2. For every f\u03b3w,Q \u2208 F\u03b3 there is v \u2208 B\u0393(\u0393), such that:\nf\u03b3w,Q(xo) = v \u00b7 \u03c6\u03b3(xo).\nAs a corllary, for every loss function ` and distribution D we have that:\nmin v\u2208B\u0393(\u0393) E [`(v \u00b7 \u03c6(xo), y)] \u2264 min f\u03b3w,Q\u2208F\u03b3\nE [ `(f\u03b3w,Q(xo), y) ] Due to Corollary 2, learning F\u03b3 can be improperly done via learning a linear classifier over the embedded sample set {\u03c6\u03b3(xo)}mi=1. While the ambient space R\u0393 may be very large, the computational complexity of the next optimization scheme is actually dependent on the scalar product between the embedded samples. For that we give the following result that shows that the scalar product can be computed efficiently:\nTheorem 4. \u03c6\u03b3(x (1) o1 ) \u00b7 \u03c6\u03b3(x (2) o2 ) =\n|o1 \u2229 o2|\u03b3 \u2212 1 |o1 \u2229 o2| \u2212 1 \u2211 k\u2208o1\u2229o2 x (1) k x (2) k .\n(We use the convention that 1 \u03b3\u22121 1\u22121 = limx\u21921 x\u03b3\u22121 x\u22121 = \u03b3)"}, {"heading": "5 Discussion and future work", "text": "We have described the first theoretically-sound method to cope with low rank missing data, giving rise to a classification algorithm that attains competitive error to that of the optimal linear classifier that has access to all data. Our non-proper agnostic framework for learning a hidden low-rank subspace comes with provable guarantees, whereas heuristics based on separate data reconstruction and classification are shown to fail for certain scenarios.\nOur technique is directly applicable to classification with low rank missing data and polynomial kernels via kernel (polynomial) composition. General kernels can be handled by polynomial approximation, but it is interesting to think about a more direct approach.\nIt is possible to derive all our results for a less stringent condition than \u03bb-regularity: instead of bounding the smallest eigenvalue of the hidden subspace, it is possible to bound only the ratio of largest-to-smallest eigenvalue. This results in better bounds in a straightforward plug-and-play into our analysis, but was ommitted for simplicity."}, {"heading": "A Proofs of theorems and lemmas from main text", "text": ""}, {"heading": "A.1 Technical Claims", "text": "Claim 1. Let Q \u2208Md\u00d7d be a square projection matrix and P \u2208Mk\u00d7d a matrix. Recall that:\nIm(A) = {v : \u2203u Au = v}, and ker(A) = {v : Av = 0}.\nAnd that rank(A) is the size of the largest collection of linearly independent columns of A. The following statements are equivalent:\n1. ker(PQ) = ker(Q).\n2. rank(PQ) = rank(QP>) = rank(PQP>) = rank(Q).\n3. Im(QP>) = Im(Q)."}, {"heading": "Proof.", "text": "1\u21d2 2 Clearly rank(PQ) \u2264 rank(Q). If rank(PQ) < rank(Q) we must have some collection of linearly independent columns of Q that are linearly dependent in PQ this implies that there is v such that PQv = 0 but Qv 6= 0. Hence ker(PQ) 6= ker(Q) and thus a contradiction, we conclude that rank(PQ) = rank(Q). That rank(PQ) = rank(QP>) = rank(PQP>) follows from the fact that rank(A) = rank(A>) = rank(AA>) and using the fact that Q2 = Q since Q is a projection matrix.\n2\u21d2 3 We have that Im(QP>) \u2286 Im(Q). The two subspaces, Im(QP>) and Im(Q), are in fact the linear span of the columns of QP> and Q respectively.\nSince rank(QP>) = rank(Q) we conclude that the dimension of the two subspaces is equal. It follows that Im(QP>) = Im(Q).\n3\u21d2 1 Since Im(QP>) = Im(Q) we also have rank(QP>) = rank(Q) and as a corollary rank(PQ) = rank(Q). Now by the rank-nullity Theorem, for every A \u2208Mk\u00d7d, dim(ker(A)) = d\u2212 rank(A). Hence dim(ker(PQ)) = dim(ker(Q)). Since ker(PQ) \u2286 ker(Q) we must have . ker(PQ) = ker(Q).\nClaim 2. Let o \u2208 2d be drawn according to a distribution D that satisfies the low rank assumption. If Q = PE then:\nIm(Qo,o) = Im(PoQ)\nProof. ker(PoQ) = ker(Q) holds by assumption (assumption 3 in Definition 1). Im(Q) = Im(QP>o ) then follows from item 3. In particular Im(PoQ) = Im(PoQPo>) = Im(Qo,o)."}, {"heading": "A.2 proof of Lemma 1", "text": "By definition, if Pox \u2208 Im(Qo,o) then Qo,o (Qo,o)\u2020 Pox = Pox. We claim that due to the low rank assumption, Pox \u2208 Im(Qo,o).\nIndeed, recall that Q = PE and x \u2208 E hence Qx = x and Pox \u2208 Im(PoQ). By Claim 2 we have Im(Qo,o) = Im(PoQ), hence Pox \u2208 (ImQo,o).\nNext, we have that\nPoQP > o (Qo,o) \u2020 Pox = Qo,o (Qo,o) \u2020 Pox = Pox\nAlternatively\nPo(QP > o Q \u2020 o,oPox\u2212 x) = 0. (4)\nAgain, since Qx = x we have that:\nPoQ(P > o Q \u2020 o,oPox\u2212 x) = 0. (5)\nThe low rank assumption implies that PoQv = 0 if and only if Qv = 0. Apply this to v = P>o Q \u2020 oPox\u2212 x and get:\nQP>o Q \u2020 o,oPox = Qx = x.\nFinally we have that\nfw,Q(xo) = (PoQ >w\u2217) \u00b7Q\u2020o,oPox = w\u2217 \u00b7QP>o Q\u2020o,oPox = w\u2217 \u00b7 x."}, {"heading": "A.3 proof of Lemma 2", "text": "Let I denote the identity matrix in Md\u00d7d. First note that (Io,o \u2212 Qo,o) = (I \u2212 Q)o,o and that Io,o is the identity matrix in R|o|\u00d7|o|.\nLet v1, . . . ,vk be the normalized and orthogonal eigen-vectors of Qo,o with strictly positive eigenvalues \u03bb1 \u2265 . . . , \u03bbk. By \u03bb-regularity we have that \u03bbk \u2265 \u03bb and since the spectral norm of Qo,o is smaller than the spectral norm of Q we have that \u03bb1 \u2264 1.\nNote that for every vj we have Q\u2020o,ovj = 1 \u03bbj vj . Next, recall that Q = PE and x \u2208 E hence Qx = x and Pox \u2208 Im(PoQ). By Claim 2 we have Pox \u2208 Im(Qo,o). Since Im(Qo,o) = span(v1, . . . ,vk), we may write Pox = \u2211 \u03b1ivi. Since \u2016Pox\u2016 \u2264 1 and {v1, . . . ,vk} is an orthonormal system we have \u2211 \u03b12i \u2264 1.\nHence\n\u2016 \u03b3\u22121\u2211 j=0 (Io,o \u2212Qo,o)j \u2212Q\u2020o,o Pox\u2016 = \u2016\u2211 i \u03b1i \u03b3\u22121\u2211 j=0 (1\u2212 \u03bb)ji \u2212 1 \u03bbi vi\u2016 \u2264 max i \u2223\u2223\u2223\u2223\u2223\u2223 \u03b3\u22121\u2211 j=0 (1\u2212 \u03bbi)j \u2212 1 \u03bbi \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 max i \u2223\u2223\u2223\u22231\u2212 (1\u2212 \u03bbi)\u03b3\u03bbi \u2212 1\u03bbi \u2223\u2223\u2223\u2223 \u2264 (1\u2212 \u03bb)\u03b3\u03bb .\nFinally since \u2016Pow\u2016 \u2264 1 we get that\n\u2016f\u03b3w,I\u2212Q(xo)\u2212 fw,Q(xo)\u2016 \u2264 (1\u2212 \u03bb)\u03b3\n\u03bb"}, {"heading": "A.4 Proof of Lemma 3", "text": "Let o1 \u2264 o2,\u2264 . . . \u2264 o|o| be the elements of o ordered in increasing order. First by definition we have that:\nf\u03b3w,Q(xo) = \u03b3\u22121\u2211 j=0 |o|\u2211 n,k=1 won((Qo,o) j)n,kxok = \u2211 i\u2208o wixi + \u03b3\u22121\u2211 j=1 |o|\u2211 n,k=1 won((Qo,o) j)n,kxok (6)\nWe also have by definition that for j \u2265 1:\n((Qo,o) j)n,k = |o|\u2211 s=1 ((Qo,o) j\u22121)n,s((Qo,o))s,k = |o|\u2211 s=1 ((Qo,o) j\u22121)n,sQos,ok\nBy induction we can show that:\n((Qo,o) j)n,k = \u2211 s1\u2208o Qon,s1 \u2211 s2\u2208o Qs1,s2 \u2211 \u00b7 \u00b7 \u00b7  \u2211 sj\u22121\u2208o Qsj\u22122,sj\u22121Qsj\u22121,ok  \u00b7 \u00b7 \u00b7  .\nReordering the elements we get for j \u2265 1:\n((Qo,o) j)n,k = \u2211 {s:|s|=j+1,s1=on,sj+1=ok} Qs1,s2 \u00b7Qs2,s3 \u00b7 \u00b7 \u00b7Qsj ,sj+1 (7)\nThe result now follows from Eq. 6 and Eq. 7 by a change of indexes."}, {"heading": "A.5 Proof of Corollary 2", "text": "Choose\nvs = { ws1 |s| = 1 ws1 \u00b7Qs1,s2 \u00b7Qs2,s3 \u00b7 \u00b7 \u00b7Qs|s|\u22121,send |s| > 1\nIt is clear from Lemma 3 that f\u03b3w,Q(xo) = v \u00b7 \u03c6\u03b3(xo). We only need to show that \u2016v\u2016 \u2264 \u221a\n\u0393\u2016w\u2016. Note that since Q2 = Q we have max(|Qi,j |) < 1. Hence |vs| \u2264 |ws1 | and:\n\u2016v\u20162 = \u2211 s\u2208G v2s \u2264 \u2211 s\u2208G w2s1 \u2264 \u0393\u2016w\u2016 2"}, {"heading": "A.6 Proof of Theorem 4", "text": "By definition of \u03c6\u03b3 we have:\n\u03c6\u03b3(x (1) o1 ) \u00b7 \u03c6\u03b3(x (2) o2 ) = \u2211 s\u2286o1\u2229o2 x(1)send \u00b7 x (2) send = \u03b3\u2211 l=1 \u2211 k\u2208o1\u2229o2 \u2211 s\u2286o1\u2229o2,send=k,|s|=l x (1) k \u00b7 x (2) k\n= 1\u2211 l=1 \u2211 |s|=l\u22121,s\u2282o1\u2229o2 \u2211 k\u2208o1\u2229o2 x (1) k \u00b7 x (2) k\n= 1\u2211 l=1 |s : |{|s| = l \u2212 1, s \u2282 o1 \u2229 o2}| \u2211 k\u2208o1\u2229o2 x (1) k \u00b7 x (2) k = \u03b3\u2211 l=1 |o1 \u2229 o2|l\u22121 \u00b7 \u2211 k\u2208o1\u2229o2 x (1) k \u00b7 x (2) k =\n1\u2212 |o1 \u2229 o2|\u03b3 1\u2212 |o1 \u2229 o2| \u2211\nk\u2208o1\u2229o2\nx (1) k x (2) k"}, {"heading": "A.7 Proof of Theorem 2", "text": "We take \u03c6\u03b3 as in Definition 4. That \u03c6\u03b3(x1o1) \u00b7 \u03c6\u03b3(x2o2) = 1\u2212|o(1)\u2229o(2)|\u03b3 1\u2212|o(1)\u2229o(2)| \u2211 i\u2208o(1)\u2229o(2) x (1) i \u00b7 x (2) i is shown in Theorem 4. The analysis of sub-gradient descent methods to optimize problems of this form i.e:\n\u2016w\u20162 + C m m\u2211 i=1 (`(w>\u03c6(xi), yi).\nwas studied in Shalev-Shwartz et al. (2011) and the detailed analysis can be found there (with generalization to mercer kernels and general losses). We mention that since ` is L-Lipschitz and \u2016\u03c6\u03b3(xo)\u2016 \u2264 \u221a \u0393 a bound R on the gradient of\u2207`(v>\u03c6\u03b3(xo), y) = `\u2032(v>\u03c6\u03b3(xo), y)\u03c6\u03b3(xo) is given by L \u221a\n\u0393. This establishes items 1 and 2. Next we let ` be an L-Lipschitz loss function and D a \u03bb-regular distribution and we assume that \u03b3 \u2265 log 2L/(\u03bb )\u03bb . Due to Corollary 2, for some v\u2217 \u2208 B\u0393(\u0393)\nE [`(v\u2217 \u00b7 \u03c6\u03b3(xo), y)] \u2264 min f\u03b3w,I\u2212Q\u2208F\u03b3\nE [ `(f\u03b3w,I\u2212Q(xo), y) ] Applying Lemma 2 and L-Lipschitness, for every f\u2217w,Q \u2208 F0 we have:\nE [`(v\u2217 \u00b7 \u03c6\u03b3(xo), y)] \u2264 E [ `(f\u2217w,Q(xo, y)) ] + L (1\u2212 \u03bb)\u03b3\n\u03bb .\nThe result follows from choice of \u03b3 and the inequality \u2212\u03bb > log(1\u2212 \u03bb):\n(1\u2212 \u03bb) log 2L/(\u03bb ) \u03bb\n\u03bb \u2264 (1\u2212 \u03bb)\nlog(\u03bb )/(2L) log(1\u2212\u03bb) \u03bb = 2L ."}, {"heading": "A.8 Proof of Theorem 1", "text": "Fix a sample S = {xioi} m i=1 and \u03b3 \u2265 log 2L/\u03bb \u03bb . Let\nL(v) = E(`(v>\u03c6\u03b3(xo), y) L\u0302(v) = 1\nm m\u2211 i=1 `(v>\u03c6\u03b3(x i oi),\nthe expected and empirical losses of the vector v. Further denote by\nFc(v) = 1\n2C \u2016v\u20162 + L(v) F\u0302c(v) =\n1\n2C \u2016v\u20162 + L\u0302(v)\nLet C(m) \u2208 O (\u221a\nm log 1/\u03b4 ) . Run Alg. 1 with T = m and let v\u0304 = 1T \u2211T i=1 vt. By Theorem 2, item 2 we get:\nF\u0302C(m)(v\u0304) \u2264 min F\u0302C(m)(v) +O ( C(m)L2\u0393( )\nm\n)\nNote that \u2016\u03c6\u03b3(xo)\u2016 \u2264 \u221a d\u03b3\u22121 d\u22121 \u2016Pox\u2016 \u2264 \u221a \u0393( ). We now apply Corollary 4. in Sridharan et al. (2009) with\nB = \u221a \u0393( ) to obtain the following bound (with probability 1\u2212 \u03b4) for every w:\nL(v\u0304) \u2264 L(w) +O\n(\u221a L2\u0393( )\u2016w\u20162 log(1/\u03b4)\nm ) In particular for every \u2016w\u2016 \u2264 \u221a \u0393( ) we have\nL(v\u0304) \u2264 L(w) +O\n(\u221a L2\u0393( )2 log(1/\u03b4)\nm\n) .\nFrom Theorem 2, item 3 we have that for some \u2016w\u2016 \u2264 \u221a \u0393( ):\nL(w) \u2264 min \u2016w\u2016\u22641\nE(`(w>x, y) + .\nThe result now follows from the choice of m."}, {"heading": "A.9 Proof of Theorem 3", "text": "Before proving the theorem, we formally define the sequences for which the algorithm applies: a \u03bb-regular sequence is one such that the uniform distribution over the sequence elements is \u03bb-regular with associated subspace E.\nProof of Theorem 3. Let E\u2217 denote the adversarially chosen subspace andQ\u2217 The projection associated with it. Since the sequence {(xt,ot, yt) is \u03bb-regular w.r.t. subspace E\u2217, we have by Lemma 2,\n\u2200\u2016w\u2016 \u2264 1 . \u2016f\u03b3w,I\u2212Q\u2217(xo)\u2212 fw,Q\u2217(xo)\u2016 \u2264 (1\u2212 \u03bb)\u03b3 \u03bb \u2264 1 \u03bb e\u2212\u03bb\u03b3\nThus, taking f\u03b3w\u2217,I\u2212Q\u2217 \u2208 F\u03b3 we have\nminw\u2208Bd \u2211 t `(fw,Q\u2217(x t ot), yt)\u2212 \u2211 t `(f \u03b3 w\u2217,I\u2212Q\u2217(x t ot), yt)\n= \u2211 t `(f \u2217 w,Q(x t ot), yt)\u2212 \u2211 t `(f \u03b3 w\u2217,I\u2212Q\u2217(x t ot), yt)\n\u2264 \u2211 t L\u2016f\u2217w,Q(xtot)\u2212 f \u03b3,\u2217 w,Q(x t ot)\u2016 ` is L-Lipschitz\n\u2264 TL 1\u03bbe \u2212\u03bb\u03b3 Lemma 2\nHence it suffices to show that \u2211 t `(v > t \u03c6\u03b3(x t ot), yt)\u2212 \u2211 t `(f \u03b3 w\u2217,I\u2212Q\u2217(x t ot), yt)\n\u2264 \u2211 t `(v > t \u03c6\u03b3(x t ot), yt)\u2212minfw,Q\u2208F\u03b3 \u2211 t `(f \u03b3 w,Q(x t ot), yt) = O( \u221a T )\nCorollary 2 asserts that f\u03b3w,Q(xo) = v \u00b7 \u03c6\u03b3(xo)\nThus, the theorem statement can be further reduced to\u2211 t `(v>t \u03c6\u03b3(x t ot), yt)\u2212 minv\u2217\u2208B\u0393(\u0393) \u2211 t `(v>\u2217 \u03c6\u03b3(x t ot), yt) = O( \u221a T ) (8)\nWe proceed to prove equation Eq. 8 above. Algorithm 1 applies the following update rule\nvt+1 = t\u2211 i=1 \u03b1 (t) i \u03c6\u03b3(x i oi)\nwhere wt+1 can be re-written as:\nvt+1 = (1\u2212 \u03b7t\u03c1)vt \u2212 \u03b7t`\u2032(v>t \u03c6\u03b3(xtot))\u03c6\u03b3(x t ot)\n= vt \u2212 \u03b7t\u2207\u02dc\u0300t(vt) (9)\nwhere \u02dc\u0300 t(v) = `(v >\u03c6\u03b3(x t ot)) + \u03c1\n2 \u2016v\u20162\nThe above implies a bound on the norm of the gradients of \u02dc\u0300t, as given by the following lemma:\nLemma 4. For all iterations t \u2208 [T ] we have\n\u2016vt\u2016 \u2264 LX \u221a \u0393 , \u2016\u2207\u02dc\u0300t(vt)\u2016 \u2264 2LX \u221a \u0393\nEquation Eq. 9 implies that KARMA applies the online gradient descent algorithm on the functions \u02dc\u0300 which are \u03c1-strongly-convex. Hence, the bound of Theorem 3.3 in Hazan (2014), with appropriate learning rates \u03b7t and with \u03b1 = \u03c1, G = 2LX\n\u221a \u0393) by lemma 4, gives\u2211\nt\n\u02dc\u0300 t(vt)\u2212min\nv\u2217 \u2211 t \u02dc\u0300 t(v \u2217) \u2264 2L 2X2\u0393 \u03c1 (1 + log T )\nThis directly implies our theorem since (recall that \u2016v\u2217\u2016 \u2264 B by assumption):\u2211 t `(v > t \u03c6\u03b3(x t ot), yt)\u2212min\u2016w\u2016\u22641 \u2211 t `(fw,Q\u2217(x t ot), yt)\n= \u2211 t \u02dc\u0300 t(vt)\u2212minv\u2217 \u2211 t \u02dc\u0300 t(v \u2217) + \u03c12 ( \u2211 t \u2016v\u2217\u20162 \u2212 \u2016vt\u20162)\n\u2264 2L 2X2\u0393 \u03c1 (1 + log T ) + \u03c1 2T \u00b7B\nProof of Lemma 4. First, notice that the norms of the gradients of the loss functions ` can be bounded by\n\u2016\u2207`(v>t \u03c6\u03b3(xtot), yt)\u2016 = |` \u2032(v>t \u03c6\u03b3(x t ot), yt)| \u00b7 \u2016\u03c6\u03b3(xtot)\u2016 \u2264 LX\n\u221a \u0393\nwhere the last inequality follows from the Lipschitz property of ` and the fact that \u03c6\u03b3(xtot) is a vector in R\u0393, with coordinates from the vector xt, and the bound \u2016xt\u2016\u221e \u2264 X .\nNext, we prove by induction that \u2016vt\u2016 \u2264 LX \u221a\n\u0393. For t = 0 we have v1 = 0. Equation Eq. 9 implies that vt+1 is a convex combination of two vectors:\n\u2016vt+1\u2016 = \u2016(1\u2212 \u03b7tC)vt \u2212 \u03b7t`\u2032(v>t \u03c6\u03b3(xtot))\u03c6\u03b3(x t ot)\u2016 \u2264 max { C\u2016vt\u2016 , \u2016\u2207`(v>t \u03c6\u03b3(xtot))\u2016 } \u2264 max { CLX \u221a \u0393 , \u2016\u2207`(v>t \u03c6\u03b3(xtot))\u2016 } induction hypothesis\n\u2264 max { CLX \u221a \u0393 , LX \u221a \u0393 }\nabove bound on \u2207`\n\u2264 LX \u221a \u0393 C < 1\nWe can now conclude with the lemma, by definition of \u02dc\u0300t\n\u2016\u2207\u02dc\u0300t(vt)\u2016 \u2264 \u2016\u2207`(v>t \u03c6\u03b3(xtot))\u2016+ C\n2 \u2016vt\u2016 \u2264 LX\n\u221a \u0393 + C 2 LX \u221a \u0393 \u2264 2LX \u221a \u0393"}], "references": [{"title": "Learning with restricted focus of attention", "author": ["S. Ben-David", "E. Dichterman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Ben.David and Dichterman,? \\Q1998\\E", "shortCiteRegEx": "Ben.David and Dichterman", "year": 1998}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Q. Berthet", "P. Rigollet"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Berthet and Rigollet,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet", "year": 2013}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Candes and Recht,? \\Q2009\\E", "shortCiteRegEx": "Candes and Recht", "year": 2009}, {"title": "Efficient learning with partially observed attributes", "author": ["N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir"], "venue": "In Proceedings of the 27th international conference on Machine learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2010}, {"title": "Online learning of noisy data", "author": ["N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Max-margin classification of data with absent features", "author": ["Chechik", "Gal", "Heitz", "Geremy", "Elidan", "Abbeel", "Pieter", "Koller", "Daphne"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Chechik et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2008}, {"title": "Learning to classify with missing and corrupted features", "author": ["Dekel", "Ofer", "Shamir", "Ohad", "Xiao", "Lin"], "venue": "Mach. Learn.,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Discrete chebyshev classifiers", "author": ["Eban", "Elad", "Mezuman", "Globerson", "Amir"], "venue": null, "citeRegEx": "Eban et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eban et al\\.", "year": 2014}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Globerson", "Amir", "Roweis", "Sam"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Globerson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2006}, {"title": "Transduction with matrix completion: Three birds with one stone", "author": ["Goldberg", "Andrew B", "Zhu", "Xiaojin", "Recht", "Ben", "Xu", "Jun-Ming", "Nowak", "Robert D"], "venue": "In Proceedings of the 24th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Goldberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2010}, {"title": "Introduction to Online Convex Optimization", "author": ["Hazan", "Elad"], "venue": "URL http://ocobook.cs.princeton. edu/", "citeRegEx": "Hazan and Elad.,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2014}, {"title": "Linear regression with limited observation", "author": ["Hazan", "Elad", "Koren", "Tomer"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Near-optimal algorithms for online matrix prediction", "author": ["Hazan", "Elad", "Kale", "Satyen", "Shalev-Shwartz", "Shai"], "venue": "In COLT, pp", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["D.G. Horvitz", "D.J. Thompson"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Horvitz and Thompson,? \\Q1952\\E", "shortCiteRegEx": "Horvitz and Thompson", "year": 1952}, {"title": "Practical large-scale optimization for max-norm regularization", "author": ["J. Lee", "B. Recht", "R. Salakhutdinov", "N. Srebro", "J.A. Tropp"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Statistical Analysis with Missing Data, 2nd Edition", "author": ["Little", "Roderick J. A", "Rubin", "Donald B"], "venue": null, "citeRegEx": "Little et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Little et al\\.", "year": 2002}, {"title": "Collaborative filtering in a non-uniform world: Learning with the weighted trace norm", "author": ["R. Salakhutdinov", "N. Srebro"], "venue": "In NIPS,", "citeRegEx": "Salakhutdinov and Srebro,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov and Srebro", "year": 2010}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Collaborative filtering with the trace norm: Learning, bounding, and transducing", "author": ["O. Shamir", "S. Shalev-Shwartz"], "venue": "JMLR - Proceedings Track,", "citeRegEx": "Shamir and Shalev.Shwartz,? \\Q2011\\E", "shortCiteRegEx": "Shamir and Shalev.Shwartz", "year": 2011}, {"title": "Learning with Matrix Factorizations", "author": ["Srebro", "Nathan"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Srebro and Nathan.,? \\Q2004\\E", "shortCiteRegEx": "Srebro and Nathan.", "year": 2004}, {"title": "Fast rates for regularized objectives", "author": ["Sridharan", "Karthik", "Shalev-Shwartz", "Shai", "Srebro", "Nathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "In the machine learning community, missing data was considered in the framework of limited attribute observability Ben-David & Dichterman (1998) and its many refinements Dekel et al. (2010); Cesa-Bianchi et al.", "startOffset": 170, "endOffset": 190}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data.", "startOffset": 8, "endOffset": 63}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)).", "startOffset": 8, "endOffset": 369}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)).", "startOffset": 8, "endOffset": 396}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data.", "startOffset": 8, "endOffset": 573}, {"referenceID": 3, "context": "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data. Their method, however, does not entail theoretical gaurantees on reconstruction in the worst case, and gives rise to non-convex programs. A natural and intuitive methodology to follow is to treat the labels (both known and unknown) as an additional column in the data matrix and complete the data using a matrix completion algorithm, thereby obtaining the classification. Indeed, this exactly was proposed by Goldberg et al. (2010). Although this is a natural approach, we show that 1We remark that our model implicitly includes mean-imputation or 0-imputation method and therefore will always outperform them.", "startOffset": 8, "endOffset": 1074}, {"referenceID": 12, "context": "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011).", "startOffset": 180, "endOffset": 198}, {"referenceID": 12, "context": "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011).", "startOffset": 180, "endOffset": 229}, {"referenceID": 12, "context": "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011). The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al.", "startOffset": 180, "endOffset": 261}, {"referenceID": 11, "context": "The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al. (2012), which we were not able to use for our purposes.", "startOffset": 104, "endOffset": 124}, {"referenceID": 9, "context": "Goldberg et al. (2010) Considered the following problem: minimize Z rank(Z) subject to: Zi,j = xi,j , (i, j) \u2208 \u03a9 , .", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "The optimization analysis is derived in a straightforward manner from the work of Shalev-Shwartz et al. (2011). Other optimization algorithms exist in the literature, and we chose this optimization method as it allows us to also derive regret bounds which are formally stronger (see Section 2.", "startOffset": 82, "endOffset": 111}], "year": 2015, "abstractText": "We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.", "creator": "LaTeX with hyperref package"}}}