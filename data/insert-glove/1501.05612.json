{"id": "1501.05612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2015", "title": "Features modeling with an $\\alpha$-stable distribution: Application to pattern recognition based on continuous belief functions", "abstract": "auriol The miyata aim gheorghe of kickstarting this tuco paper is to show the iix interest in plitvice fitting jibal features with an $ \\ alpha $ - stable decc distribution to lonelyhearts classify ballentine imperfect rastaman data. asia/oceania The shamberg supervised plouhar pattern recognition unmovable is thus based on the theory vacanze of amiram continuous diego belief korologos functions, souzay which parenthetical is aclj a way gobowen to 118.88 consider imprecision defamer and uncertainty of timey data. kremen The distributions antibody of p23 features are privatisations supposed homelite to be kalisa unimodal helguera and degrazia estimated by gbr a kharkiv single 12:55 Gaussian nuradin and $ \\ ethnical alpha $ - stable model. Experimental indera results gape are modulation first bbq obtained from synthetic nylen data 1950/51 by erbate combining two corse features of somurray one multistate dimension and by considering ekranas a vector of two features. lutry Mass functions are bukusu calculated priding from cladonia plausibility functions by kreitzer using 946,000 the generalized barby Bayes petur theorem. The deindustrialization same study lek\u00eb is eboracum applied magno to the automatic .397 classification sandpoint of three onishi types facilitates of sea floor (borduas rock, silt bahutule and sand) sparwasser with demilitarized features vili acquired by a broadside mono - beam 1905 echo - benic\u00e0ssim sounder. l\u00f8ken We evaluate the heeled quality carrero of financial the $ \\ d'ivry alpha $ - stable fo model and faba the l'h\u00f4pital Gaussian reinstallation model by unbox analyzing mengin qualitative dhanapala results, using ornately a insoles Kolmogorov - Smirnov orgreave test (neotoma K - faliro S miljan test ), and hamidreza quantitative hushed results 80-seat with zuhl classification overclocking rates. The zvv performances of undetermined the belief classifier are 2,713 compared with a heybridge Bayesian approach.", "histories": [["v1", "Thu, 22 Jan 2015 19:55:58 GMT  (488kb,D)", "http://arxiv.org/abs/1501.05612v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["anthony fiche", "jean-christophe cexus", "arnaud martin", "ali khenchaf"], "accepted": false, "id": "1501.05612"}, "pdf": {"name": "1501.05612.pdf", "metadata": {"source": "CRF", "title": "Features modeling with an \u03b1-stable distribution: application to pattern recognition based on continuous belief functions", "authors": ["Anthony Fichea", "Jean-Christophe Cexusa", "Arnaud Martinb", "Ali Khenchafa"], "emails": [], "sections": [{"heading": null, "text": "The aim of this paper is to show the interest in fitting features with an \u03b1-stable distribution to classify imperfect data. The supervised pattern recognition is thus based on the theory of continuous belief functions, which is a way to consider imprecision and uncertainty of data. The distributions of features are supposed to be unimodal and estimated by a single Gaussian and \u03b1-stable model. Experimental results are first obtained from synthetic data by combining two features of one dimension and by considering a vector of two features. Mass functions are calculated from plausibility functions by using the generalized Bayes theorem. The same study is applied to the automatic classification of three types of sea floor (rock, silt and sand) with features acquired by a mono-beam echo-sounder. We evaluate the quality of the \u03b1-stable model and the Gaussian model by analyzing qualitative results, using a Kolmogorov-Smirnov test (K-S test), and quantitative results with classification rates. The performances of the belief classifier are compared with a Bayesian approach.\nKeywords: Gaussian and \u03b1-stable model, unimodal features, continuous belief functions, supervised pattern recognition, Kolmogorov-Smirnov test, Bayesian approach."}, {"heading": "1. Introduction", "text": "The choice of a model has an important role in the problem of estimation. For example, the Gaussian model is a very efficient model which fits data in many applications as it is very simple to use and saves computation time. However, as is the case for all distribution models, Gaussian laws have some weaknesses and results can end-up being skewed. Indeed, as the Gaussian probability density function (pdf) is symmetrical, it is not valid when the pdf is not symmetrical. It is therefore difficult to choose the right model which fits data for each application. The Gaussian distribution belongs to a family of distributions called stable distributions. This family of distributions allows the representation of heavy tails and skewness. A distribution is said to have a heavy tail if the tail decays slower than the tail of the Gaussian distribution. Therefore, the property of skewness means that it is impossible to find a mode where probability density function is symmetrical. The main property of stable laws introduced by Le\u0301vy [1] is that the sum of two independent stable random variables gives a stable random variable. \u03b1-stable distributions are used in different fields of research such as radar [2, 3], image processing [4] or finance [5, 6], ...\nThe aim of this study is to show the interest in fitting data with an \u03b1-stable distribution. In [7], the author proposes to characterize the sea floor from a vector of features modeled by Gaussian mixture models (GMMs) using an Autonomous Underwater Vehicle (AUV). However, the problem with the Bayesian approach is the difficulty in considering the uncertainty of data. We therefore favored the use of an approach based on the theory of belief functions [8, 9]. In [10], the authors compared a Bayesian and belief classifier where data from sensors are modeled using GMMs estimated via an Expectation-Maximization (EM) algorithm [11]. This work has been extended to data modeled by \u03b1-stable mixture models [12]. However, it is difficult to choose between these two models because the results\n\u2217Corresponding author. Email addresses: anthony.fiche@ensta-bretagne.fr (Anthony Fiche), jean-christophe.cexus@ensta-bretagne.fr\n(Jean-Christophe Cexus), arnaud.martin@univ-rennes1.fr (Arnaud Martin), ali.khenchaf@ensta-bretagne.fr (Ali Khenchaf)\nPreprint submitted to Information Fusion January 23, 2015\nar X\niv :1\n50 1.\n05 61\n2v 1\n[ cs\n.A I]\n2 2\nJa n\n20 15\nare roughly the same. This paper raises two problems. Firstly, it is necessary to work on a data set where features are modeled by \u03b1-stable pdfs. The second problem is to know how to apply the theory of belief functions when data from sensors are modeled by \u03b1-stable distributions. This point has been dealt with in [13].\nThe recent characterization of uncertain environments has taken an important place in several fields of application such as the SOund Navigation And Ranging (SONAR). SONAR has therefore been used for the detection of underwater mines [14]. In [15], the author developed techniques to perform automatic classification of sediments on the seabed from sonar images. In [10], the authors classified sonar images by extracting features and modeling them with GMMs. In [16], the authors characterized the sea floor using data from a mono-beam echo-sounder. A data set represents an echo signal amplitude according to time. They compare the time envelope of echo signal amplitude with a set of theoretical reference curves. In this paper, we build a classifier based on the theory of belief functions where a vector of features extracted from a mono-beam echo-sounder is modeled by a Gaussian and an \u03b1-stable distribution. The feature pdfs have only one mode. We finally show that it would be interesting to fit features with an \u03b1-stable model.\nThis paper is organized as follows: we first introduce the definition of stability, the method to construct \u03b1-stable probability density functions and the methods of estimation in Section 2. The definitions in the multivariate case are also presented. We introduce the notion of belief functions in discrete and continuous cases in Section 3. Belief functions are then calculated in the particular case of \u03b1-stable distributions and a belief classifier is constructed in Section 4. We finally classify data by modeling data with Gaussian and \u03b1-stable distributions before the generated data are classified by comparing the theory of belief functions with Gaussian and \u03b1-stable distributions in Section 5. We compare the results obtained with the theory of belief functions using a Bayesian approach."}, {"heading": "2. The \u03b1-stable distributions", "text": "Gauss introduced the probability density function called \u201cGaussian distribution\u201d (in honor of Gauss) in his study of astronomy. Laplace and Poisson developed the theory of characteristic function by calculating the analytical expression of Fourier transform of a probability density function. Laplace found that the Fourier transform of a Gaussian law is also a Gaussian law. Cauchy tried to calculate the Fourier transform of a \u201cgeneralized Gaussian\u201d function with the expression fn(x) = 1\u03c0 \u222b +\u221e 0 exp (\u2212ct\nn) cos (tx)dt, but he did not solve the problem. When the integer n is a real \u03b1, we define the family of \u03b1-stable distributions. However, Cauchy did not know at that time if he had defined a probability density function. With the results of Polya and Bernstein, Janicki and Weron [17] demonstrated that the family of \u03b1-stable laws are probability density functions. The mathematician Paul Le\u0301vy studied the central limit theorem and showed, with the constraint of infinite variance, that the limit law is a stable law [1]. Motivated by this property, Le\u0301vy calculated the Fourier transform for all \u03b1-stable distributions. These stable laws which satisfy the generalized central limit theorem are interesting as they allow the modeling of data as impulsive noise when the Gaussian model is not valid. Another property of these laws is the ability to model heavy tails and skewness. In the following, we define the notions of stability, characteristic function, the way to build a pdf from characteristic function, the different estimators of \u03b1-stable distributions and the extension of \u03b1-stable distribution to the multivariate case."}, {"heading": "2.1. Definition of stability", "text": "Stability is when the sum of two independent random variables which follow \u03b1 laws, gives an \u03b1 law. Mathematically, this definition means the following: a random variable X is stable, denoted X \u223c S \u03b1(\u03b2, \u03b3, \u03b4), if for all (a,b) \u2208 R+ \u00d7 R+, there are c \u2208 R+ and d \u2208 R such that:\naX1 + bX2 = cX + d, (1)\nwith X1 and X2 two independent \u03b1-stable random variables which follow the same distribution as X. If Equation (1) defines the notion of stability, it does not give any indication as to how to parameterize an \u03b1-stable distribution. We therefore prefer to use the definition given by characteristic function to refer to an \u03b1-stable distribution."}, {"heading": "2.2. Characteristic function", "text": "Several equivalent definitions have been suggested in the literature to parameterize an \u03b1-stable distribution from its characteristic function [18, 19]. Zolotarev [19] proposed the following:\n\u03c6S \u03b1(\u03b2,\u03b3,\u03b4)(t) =  exp( jt\u03b4 \u2212 |\u03b3t|\u03b1[1 + j\u03b2 tan(\u03c0\u03b1 2 )sign(t)(|t|1\u2212\u03b1 \u2212 1)]) if \u03b1 , 1,\nexp( jt\u03b4 \u2212 |\u03b3t|[1 + j\u03b22 \u03c0 sign(t) log |t|]) if \u03b1 = 1, (2)\nwhere each feature has specific values:\n\u2022 \u03b1 \u2208]0, 2] is the characteristic exponent.\n\u2022 \u03b2 \u2208 [\u22121, 1] is the skewness parameter.\n\u2022 \u03b3 \u2208 R+\u2217 represents the scale parameter.\n\u2022 \u03b4 \u2208 R is the location parameter.\nThe advantage of this parameterization compared to [18] is that the values of the characterization and probability density functions are continuous for all parameters. In fact, the parameterization defined by [18] is discontinuous when \u03b1 = 1 and \u03b2 = 0."}, {"heading": "2.3. The probability density function", "text": "The representation of an \u03b1-stable pdf, denoted fS \u03b1(\u03b2,\u03b3,\u03b4), is obtained by calculating the Fourier transform of its characteristic function:\nfS \u03b1(\u03b2,\u03b3,\u03b4)(x) = \u222b \u221e \u2212\u221e \u03c6S \u03b1(\u03b2,\u03b3,\u03b4)(t) exp(\u2212 jtx)dt. (3)\nHowever, this definition is problematic for two reasons: while the integrand function is complex, its bounds are infinite. Nolan [20] therefore proposed a way to represent normalized \u03b1-stable distributions (i.e. \u03b3 = 1 and \u03b4 = 0). The main idea of Nolan [20] is to use variable modifications so that the integral has finite bounds. Each parameter has an influence on the shape of the fS \u03b1(\u03b2,\u03b3,\u03b4). The curve has a large peak when \u03b1 is near 0 and a Gaussian shape when \u03b1 = 2 (Figure 1(a)). The shape of the distribution skews to the left if \u03b2 = 1, to the right if \u03b2 = \u22121 (Figure 1(b)) while the distribution is symmetrical when \u03b2 = 0. Finally, the scale parameter enlarges or compresses the shape of the distribution (Figure 1(c)) and the location parameter leads to the translation of the mode of the fS \u03b1(\u03b2,\u03b3,\u03b4) (Figure 1(d))."}, {"heading": "2.4. An overview of \u03b1-stable estimators", "text": "The estimators of \u03b1-stable distributions are decomposed into three families:\n\u2022 the sample quantile methods [21, 22]\n\u2022 the sample characteristic function methods [23, 24, 25, 26]\n\u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].\nFama and Roll [21] developed a method based on quantiles. However, the algorithm proposed by Fama and Roll suffers from a small asymptotic bias in \u03b1 and \u03b3 and restrictions on \u03b1 \u2208]0.6, 2] and \u03b2 = 0. McCulloch [22] extended the quantile method to the asymmetric case (i.e. \u03b2 = 0). The McCulloch estimator is valid for \u03b1 \u2208]0.6, 2].\nPress [23, 24] proposed a method based on transformations of characteristic function. In [31], the author compared the performances of several estimators. For example, the Press method is efficient for specific values. Koutrouvelis [25] extended the Press method and proposed a regression method to estimate the parameters of \u03b1-stable distributions. He proposed a second version of his algorithm which is distinct that it is iterative [26]. In [32], the authors proved that the method proposed by Koutrouvelis is better than both the quantile method and the Press method because it gives consistent and asymptotically unbiased estimates.\nThe Maximum Likelihood Estimation (MLE) was first studied in the symmetric case [27, 28]. Dumouchel [29] developed an approximate maximum likelihood method. The MLE was also developed in the asymmetric case.\nNolan [30] extended the MLE in general case. The problem with the MLE is the calculation of the \u03b1-stable probability density function because there is no closed-form expression. Moreover, the computational algorithm is timeconsuming.\nConsequently, we estimate a univariate \u03b1-stable distribution using the Koutrouvelis method [26]."}, {"heading": "2.5. Multivariate stable distributions", "text": "It is possible to extend the \u03b1-stable distributions to the multivariate case. A random vector X \u2208 R is stable if for all a, b \u2208 R+, there are c \u2208 R+ and D \u2208 Rd such that:\naX1 + bX2 = cX + D, (4)\nwhere X1 and X2 are two independent and identically distributed random vectors which follow the same distribution as X.\nThe characteristic function of an multivariate \u03b1-stable distribution, denoted X \u223c S \u03b1,d(\u03c3, \u03b4), has the form:\n\u03c6S \u03b1,d(\u03c3,\u03b4)(t) =  exp ( \u2212 \u222b S d | < t, s > |\u03b1(1 \u2212 jsgn(< t, s >) tan(\u03c0\u03b1 2 )\u03c3(ds) + j < \u03b4, t > ) if \u03b1 , 1, exp ( \u2212 \u222b S d | < t, s > |(1 + j\u03c0 2 sgn(< t, s >) ln(< t, s >))\u03c3(ds) + j < \u03b4, t > ) if \u03b1 = 1,\n(5)\nwith\n\u2022 S d = {x \u2208 Rd |||x|| = 1} the d-dimensional unit sphere.\n\u2022 \u03c3(.) a finite Borel measure on S d.\n\u2022 \u03b4, t \u2208 Rd.\nThe expression for the characteristic function involves an integration over the unit sphere S d. The measure \u03c3 is called the spectral measure and \u03b4 is called the location parameter. The problem with a multivariate \u03b1-stable distribution is that characteristic function forms a non-parametric set. To avoid this problem, it is possible to consider a discrete spectral measure [33] which has the form:\n\u03c3(.) = K\u2211\ni=1\n\u03b3i\u03b4si (.), (6)\nwith \u03b3i corresponding to weight and \u03b4si the Dirac measure in si. For an \u03b1-stable distribution in R2 with K mass points, the quantity si = {cos(\u03b8i), sin(\u03b8i)} with \u03b8i = 2\u03c0(i\u22121)K .\nThere are two methods to estimate an \u03b1-stable random vector:\n\u2022 the PROJection method [34] (PROJ)\n\u2022 the Empirical Characteristic Function method [35] (ECF)\nThese two algorithms have the same performances in terms of estimation and computation time. At this point, it is important to underline that the features extracted from mono-beam echo-sounders have the properties of heavy-tails and skewness. Consequently, we decided to estimate the data with an \u03b1-stable model. These data are however imprecise and uncertain: the imprecision and uncertainty of data can be linked to poor quality estimation of these data. To take these constraints into consideration, we used an uncertain theory called the theory of belief functions. Consequently, the goal of the next section is to present the theory of belief functions."}, {"heading": "3. The belief functions", "text": "The final objective of this paper is to classify synthetic and real data using the theory of belief functions. Data obtained from sensors are generally imprecise and uncertain as noises can disrupt their acquisition. It is possible to consider these constraints by using the theory of belief functions. We will first develop the theory of belief functions within a discrete framework before characterizing it in real numbers."}, {"heading": "3.1. Discrete belief functions", "text": "This section outlines basic tools in relation to the theory of belief functions."}, {"heading": "3.1.1. Definitions", "text": "Discrete belief functions were introduced by Dempster [8], and formalized by Shafer [9] where he considers a discrete set of n exclusive events Ci called the frame of discernment:\n\u0398 = {C1, . . . ,Cn}. (7)\n\u0398 can be interpreted as all the assumptions to a problem. Belief functions are defined as 2\u0398 onto [0,1]. The objective of discrete belief functions is to attribute a weight of belief to each element A \u2208 2\u0398. Belief functions must follow the normalization: \u2211\nA\u2286\u0398 m\u0398(A) = 1, (8)\nwhere m\u0398 is called the basic belief assignment (bba). A focal element is a subset of A where m\u0398(A) > 0 and several functions in one-to-one correspondence are built from bba:\nbel\u0398(A) = \u2211\nB\u2286A,B,\u2205 m\u0398(B), (9)\npl\u0398(A) = \u2211\nA\u2229B,\u2205 m\u0398(B), (10)\nq\u0398(A) = \u2211\nB\u2282\u0398,B\u2287A m\u0398(B). (11)\nThe credibility function of A called bel\u0398(A) is all the elements B \u2286 A which believe partially in A. This function can be interpreted as a minimum of belief in A. On the contrary, a plausibility function pl\u0398(A) illustrates the maximum belief in A while commonality function q\u0398(A) represents the sum of bba allocated to the superset of A. This function is very useful as shown below."}, {"heading": "3.1.2. Combination rule", "text": "We consider M different experts who give mass m\u0398i (i = 1, . . . ,M) on each element A \u2286 \u0398. Alternatively, it is possible to combine them using combination rules. There are several combination rules [36] in the literature which differently address conflicts between sources. The most common rule is the conjunctive combination [37] where the resultant mass of A is obtained by:\nm\u0398(A) = \u2211\nB1\u2229...Bn=A,\u2205 M\u220f i=1 m\u0398i (Bi) ,\u2200A \u2208 2\u0398. (12)\nThe mass of the empty set is given by:\nm\u0398(\u2205) = \u2211\nB1\u2229...Bn=\u2205 M\u220f i=1 m\u0398i (Bi) ,\u2200A \u2208 2\u0398. (13)\nThis rule allows us to stay in the open world. However, this is not practical as calculations are difficult. It is possible to calculate resultant mass with communality functions where each mass m\u0398i (i = 1, . . . ,M) must be converted into its communality function to calculate the resultant communality function as follows:\nq\u0398(A) = M\u220f\ni=1\nq\u0398i (A). (14)\nThe final mass is obtained by carrying out the inverse operation of Equation (11) [9]."}, {"heading": "3.1.3. Pignistic probability", "text": "To make a decision on \u0398, several operators exist such that maximum credibility or maximum plausibility with pignistic probability being the most commonly used operator [38]. This name comes from pignus, a bet, in Latin. This operator approaches the pair (bel,pl) by uniformly sharing a mass of focal elements on each singleton Ci. This operator is defined by:\nbetP(Ci) = \u2211\nA\u2282\u0398,Ci\u2208A\nm\u0398(A) |A|(1 \u2212 m\u0398(\u2205)) , (15)\nwhere |A| represents the cardinality of A. We choose the decision Ci by evaluating max\n1\u2264k\u2264n betP(Ck)."}, {"heading": "3.2. Continuous belief functions", "text": "The basic description of continuous belief functions was accomplished by Shafer [9], then by Nguyen [39] and Strat [40]. Recently, Smets [41] extended the definition of belief functions to the set of reals R = R \u222a {\u2212\u221e,+\u221e} and masses are only attributed to intervals of R."}, {"heading": "3.2.1. Definitions", "text": "Let us consider I = {[x, y], (x, y], [x, y), (x, y); x, y \u2208 R} as a set of closed, half-opened and opened intervals of R. Focal elements are closed intervals of R. The quantity mI(x, y) are basic belief densities linked to a specific pdf. If x > y, then mI(x, y) = 0. With these definitions, it is possible to define the same functions as in the discrete case. The interval [a, b] being a set of R with a \u2264 b, the previous functions can be defined as follows:\nbelR([a, b]) = \u222b x=b\nx=a \u222b y=b y=x mI(x, y)dydx, (16)\nplR([a, b]) = \u222b x=b\nx=\u2212\u221e \u222b y=+\u221e y=max(a,x) mI(x, y)dydx, (17)\nqR([a, b]) = \u222b x=a\nx=\u2212\u221e \u222b y=+\u221e y=b mI(x, y)dydx. (18)"}, {"heading": "3.2.2. Pignistic probability", "text": "The definition of pignistic probability for a < b is:\nBet f ([a, b]) = \u222b x=+\u221e\nx=\u2212\u221e \u222b y=+\u221e y=x |[a, b] \u2229 [x, y]| |[x, y]| m I(x, y)dxdy. (19)\nIt is possible to calculate pignistic probabilities to have basic belief densities. However, many basic belief densities exist for the same pignistic probability. To resolve this issue, we can use the consonant basic belief density. A basic belief density is said to be \u201cconsonant\u201d when focal elements are nested. Focal elements Iu can be labeled as an index u such that Iu \u2286 I\u2032u with u\u2032 > u. This definition is used to apply the least commitment principle, which consists in choosing the least informative belief function when a belief function is not totally defined and is only known to belong to a family of functions. The least commitment principle relies on an order relation between belief functions in order to determine if a belief function is more or less committed than another. For example, it is possible to define on order based on the commonality function:\n(\u2200A \u2286 \u0398, q\u03981 (A) \u2264 q\u03982 (A))\u21d4 (m\u03981 \u2286q m\u03982 ). (20)\nThe mass function m\u03982 is less committed than m \u0398 1 according to the commonality function.\nThe function Bet f can be induced by a set of isopignistic belief functions Biso(Bet f ). Many papers [41, 42, 43] deal with the particular case of continuous belief functions with nested focal elements. For example, Smets [41]\nproved that the least committed basic belief assignment mR for the commonality ordering attributed to an interval I = [x, y] with y > x related to a bell-shaped 1 pignistic probability function is determined by 2:\nmR([x, y]) = \u03b8(y)\u03b4d(x \u2212 \u03b6(y)), (21)\nwith x = \u03b6(y) satisfying Bet f (\u03b6(y)) = Bet f (y) and \u03b8(y):\n\u03b8(y) = (\u03b6(y) \u2212 y)dBet f (y) dy . (22)\nThe resultant basic belief assignment mR is consonant and belongs to the set Biso(Bet f ). However, it is difficult to build belief functions in the particular case of multimodal pdfs because the frame of discernment has connected sets. In [44, 45], the authors propose a way to build belief functions with connected sets by using a credal measure and an index function."}, {"heading": "3.3. Credal measure and index function", "text": "In [44], the authors propose a way to calculate belief functions from any probability density function. They use an index function f and a specific index space I to scan the set of focal elements F :\nf I : I \u2192 F , (23) y 7\u2212\u2192 f I(y). (24)\nThe authors introduce a positive measure \u00b5\u2126 such that \u222b\nI d\u00b5 \u2126(y) \u2264 1 describes unconnected sets. The pair ( f I , \u00b5\u2126)\ndefines a belief function. For all A \u2208 \u2126, they define subsets which belong to the Borel set:\nF\u2286A = {y \u2208 I| f I(y) \u2286 A}, (25) F\u2229A = {y \u2208 I| f I(y) \u2229 A , \u2205}, (26) F\u2287A = {y \u2208 I| f I(y) \u2287 A}. (27)\nFrom these definitions, they compute belief functions: bel\u2126(A) = \u222b\nF\u2286A d\u00b5\u2126(y), (28) pl\u2126(A) = \u222b\nF\u2229A d\u00b5\u2126(y), (29) q\u2126(A) = \u222b\nF\u2287A d\u00b5\u2126(y). (30)\nThey continue their study by considering consonant belief functions. The set of focal elements F must be ordered from the operator \u2286. They define an index function f from R+ to F such that:\ny \u2265 x =\u21d2 f (y) \u2286 f (x). (31)\nThey generate consonant sets by using a continuous function g from Rd to I = [0, \u03b1max[. The \u03b1-cuts are the set:\nf Ics = {x \u2208 Rd |g(x) \u2265 \u03b1}. (32)\nThey finally define the index function:\nf Ics : I = [0, \u03b1max] \u2192 { f Ics(\u03b1)|\u03b1 \u2208 I}, (33) \u03b1 7\u2212\u2192 f Ics(\u03b1). (34)\n1i.e. the pdf is unimodal with a mode \u00b5, continuous and strictly monotonous increasing (decreasing) at left (right) of the mode. 2\u03b4d refers to the Dirac\u2019s measure.\nThe information available is the conditional pignistic density Bet f . However, many basic belief densities exist for the same pignistic probability Bet f . The least commitment principle allows the least informative basic belief density to be chosen, where the focal elements are the \u03b1-cuts of Bet f such that 3:\nd\u00b5\u2126(y)(\u03b1) = \u03bb( f Ics(\u03b1))d\u03bb(\u03b1). (35)"}, {"heading": "4. Continuous belief functions and \u03b1-stable distribution", "text": "In this section, we model data distributions as a single \u03b1-stable distribution. We must however introduce the notion of plausibility function for an \u03b1-stable distribution. We first describe how to calculate the plausibility function knowing the pdf in R before we extend to in R d . We finally explain how we construct our belief classifier."}, {"heading": "4.1. Link between pignistic probability function and plausibility function in R", "text": "The information available is the conditional pignistic density Bet f [Ci] with Ci \u2208 \u03b8. The function Bet f [Ci] is supposed to be bell-shaped for all \u03b1-stable distributions (proved by Yamazato [46]). The plausibility function from a mass mR is obtained by an integral of Equation (22) between [x,+\u221e[:\nplR[Ci](I) = \u222b +\u221e\nx (\u03b6(t) \u2212 t)dBet f (t) dt dt. (36)\nBy assuming that Bet f is symmetrical, an integration by parts can simplify Equation (36) : plR[Ci](I) = 2(x \u2212 \u00b5)Bet f (x) + 2 \u222b +\u221e\nx Bet f (t)dt. (37)\nNow let us consider a particular case where symmetrical Bet f is an \u03b1-stable distribution (the parameter \u03b2 = 0). We already know that Bet f (x) = fS \u03b1(\u03b2,\u03b3,\u03b4)(x). We can calculate \u222b +\u221e x Bet f (t)dt by using the Chasles\u2019 theorem:\u222b +\u221e\n\u2212\u221e fS \u03b1(\u03b2,\u03b3,\u03b4)(t)dt = \u222b x \u2212\u221e fS \u03b1(\u03b2,\u03b3,\u03b4)(t)dt + \u222b +\u221e x fS \u03b1(\u03b2,\u03b3,\u03b4)(t)dt. (38)\nBy definition, a pd f has the quantity \u222b +\u221e \u2212\u221e fS \u03b1(\u03b2,\u03b3,\u03b4)(t)dt = 1 and \u222b x \u2212\u221e fS \u03b1(\u03b2,\u03b3,\u03b4)(t)dt represents the definition of the \u03b1-stable cumulative density function FS \u03b1(\u03b2,\u03b3,\u03b4). Consequently, Equation (37) can be simplified:\nplR[Ci](I) = 2(x \u2212 \u00b5) fS \u03b1(\u03b2,\u03b3,\u03b4)(x) + 2(1 \u2212 FS \u03b1(\u03b2,\u03b3,\u03b4)(x)). (39)\nThe plausibility function related to an interval I = [x, y] can also be seen as the area defined under the \u03b1cut-cut such that \u03b1cut = Bet f (x). The notation pl[Ci](x) is equivalent to pl[Ci](I).\nNow us let consider an asymmetric \u03b1-stable probability density function. We must proceed numerically to calculate plausibility function at point x1 > \u00b5, with \u00b5 the mode of the probability density function (Figure 2). The plausibility function related to an interval I1 = [x1, y1] is defined by the area defined under the \u03b1cut-cut such that \u03b1cut = Bet f (x1):\nplR[Ci](I1) = \u222b x1 \u2212\u221e Bet f (t)dt + (y1 \u2212 x1)Bet f (x1) + \u222b +\u221e y1 Bet f (t)dt. (40)\nBy definition, a pd f has the quantity \u222b +\u221e\ny1 fS \u03b1(\u03b2,\u03b3,\u03b4)(t)dt = 1 \u2212 FS \u03b1(\u03b2,\u03b3,\u03b4)(y1) and \u222b x1 \u2212\u221e fS \u03b1(\u03b2,\u03b3,\u03b4)(t)dt = FS \u03b1(\u03b2,\u03b3,\u03b4)(x1). In\ngeneral, we know only one point y1. We estimate numerically x1 such that fS \u03b1(\u03b2,\u03b3,\u03b4)(y1) = fS \u03b1(\u03b2,\u03b3,\u03b4)(x1). Finally, the plausibility function related to the interval I1 is:\nplR[Ci](I1) = 1 + FS \u03b1(\u03b2,\u03b3,\u03b4)(x1) \u2212 F(y1) + (y1 \u2212 x1) f (x1). (41)\n3\u03bb refers to the Lebesgue\u2019s measure.\nIn practice, we base the classification on several features defined for different classes \u0398 = {C1, ..,Cn}. For example, the Haralick parameters \u201ccontrast\u201d and \u201chomogeneity\u201d have different values for the classes rock, silt and sand. The features are modeled by a probability density function because the features have continuous values. We can calculate a plausibility function related to its probability density functions by using the least commitment principle. Several plausibility functions associated to the same feature can be combined by using the generalized Bayes theorem [47, 48] to calculate mass functions allocated to A of an interval I:\nmR[x](A) = \u220f C j\u2208A pl j(x) \u220f C j\u2208Ac (1 \u2212 pl j(x)). (42)\nFor several features, it is possible to combine the mass functions with combination rules. To validate our approach, we classify planes using kinematic data as in [42, 43] and compare the decision with the approach of Caron et al. [43]. We associate a Gaussian probability density function for each speed\u2019s target (Figure 3):\n\u2022 Commercial defined by the probability density function of speed S 2(0, 8, 722.5).\n\u2022 Bomber defined by the probability density function of speed S 2(0, 7, 690).\n\u2022 Fighter defined by the probability density function of speed S 2(0, 10, 730).\nWe can observe that the decision is the same with the both approaches in the particular case of Gaussian probability density functions.\n4.2. Link between pignistic probability function and plausibility function to R d\nIt is possible to extend plausibility function in R d . In [43], the authors calculate plausibility function in the Gaussian pdf situation of mode \u03b4 and matrix of covariance \u03a3. Mass function is built in such a way that isoprobability surfaces S i with 1 \u2264 i \u2264 n are focal elements. In R2, isoprobability points of a multivariate Gaussian pdf of mean \u03b4 and covariance matrix \u03a3 are ellipses. In dimension d, the authors defined focal elements as the nested sets HV\u03b1 enclosed by the isoprobability hyperconics HC\u03b1 = {x \u2208 Rd |(x\u2212 \u03b4) t\u03a3\u22121(x\u2212 \u03b4)}. They obtain bbd by applying the least commitment principle:\nmR d (HV\u03b1) = \u03b1\nd+2 2 \u22121\n2 d+2 2 \u0393( d+22 ) exp (\u22121 2 \u03b1) with \u03b1 \u2265 0. (43)\nEquation (43) defines a \u03c72 distribution with d + 2 degrees of freedom. The plausibility function at point x belonging to a surface S i corresponds to the volume delimited by S i (we give an example in Figure 4 for an \u03b1-stable pdf). Consequently, the plausibility function at point x is defined by:\nplR d (x \u2208 Rd) = \u222b \u03b1=+\u221e \u03b1=(x\u2212\u03b4) t\u03a3\u22121(x\u2212\u03b4) \u03b1 d+2 2 \u22121 2 d+2 2 \u0393( d+22 ) exp (\u22121 2 \u03b1)d\u03b1. (44)\nEquation (44) can be simplified by:\nplR d (x \u2208 Rd) = 1 \u2212 Fd+2((x \u2212 \u03b4)(\u03a3)\u22121(x \u2212 \u03b4)). (45)\nThe function Fd+2 is a cumulative density function of the \u03c72 distribution with d + 2 degrees of liberty (d is the dimension of vector x).\nThe authors also calculate plausibility functions in the particular case of GMMs with their pdfs denoted by pGMM:\npGMM(x) = k=n\u2211 k=1 wkN(x, \u03b4k,\u03a3k), (46)\nwhere N(x, \u03b4k,\u03a3k) is the normal distribution of the k components with mean \u03b4k and matrix of covariance \u03a3k and wk is the weight of each mixture. They assign belief to nested sets belonging to a component. Consequently, the\nplausibility function of the GMMs can be seen as a weighted sum of plausibility functions defined by each component of the mixture:\nplR d (x \u2208 Rd) = 1 \u2212 k=n\u2211 k=1 wkFd+2((x \u2212 \u03b4k)(\u03a3k)\u22121(x \u2212 \u03b4k)). (47)\nWe want to extend the calculation of plausibility function for any \u03b1-stable pdf. However, there is no closed-form expression for \u03b1-stable pdf. We use the approach developed by Dore\u0301 et al. [44] to build belief functions."}, {"heading": "4.3. Belief classifier", "text": "Let us consider a data set with N samples from d sensors. For example, each feature of a vector x \u2208 Rd can be seen as a piece of information from a sensor. The classification is divided into two steps. N \u00d7 p (with p \u2208]0, 1[) samples are first picked out randomly from the data set for the learning base, noted X, such that:\nX =  x11 . . . x N\u00d7p 1 ... ... ...\nx1d . . . x N\u00d7p d  All columns of X belong to a class Ci with 1 \u2264 i \u2264 n. Probability density functions (Gaussian or \u03b1-stable models) are estimated from samples belonging to classes. The rest of the samples is used for the test base (N \u00d7 (1 \u2212 p) vectors). We use a validation test to determine if samples belong to an estimated model. If the test is not valid, we stop the classification, otherwise we continue the classification (Figure 5). The classification step diagram in one and d dimensions is shown in Figure 6.\nPlausibility functions knowing classes for x are calculated either from their probability density functions by using Equation (41) for an unsymmetric \u03b1-stable probability density function or by using Equation (45) for a Gaussian probability density function. We obtain d mass functions (one for each feature) at point x with the generalized Bayes theorem [47, 48] (Equation (42)). These d mass functions are combined by a combination rule to obtain a single mass function (section 3.1.2). Finally, the mass function is transformed into pignistic probability (Equation (19)) to make the decision.\nIt is also possible to work with a vector x of d dimensions. For an \u03b1-stable probability density function, plausibility functions are calculated for each feature by using the approach of Dore\u0301 et al [44] (section 3.3). For a Gaussian probability density function, we use Equation (45) to calculate plausibilty functions. We calculate one mass function at point x by using the generalized Bayes theorem (Equation (42)). There is no combination step because we are calculating a single mass function. We use Equation (19) to transform the mass function into pignistic probabilities. The decision is chosen by using the maximum number of pignistic probabilities."}, {"heading": "5. Application to pattern recognition", "text": "The aim is to build a belief classifier and to perform a classification of synthetic and real data by estimating features using a Gaussian and \u03b1-stable pdf. In [12], synthetic data are classified by modeling features using Gaussian and \u03b1-stable mixture models. By observing confidence intervals, the hypothesis of \u03b1-stable mixture models is significantly better than the hypothesis of Gaussian mixture models. However, when the number of Gaussian distributions increases, classification accuracies are significantly the same. Images from a side-scan sonar are automatically classified by extracting Haralick features [49, 50]. However, classification accuracies are roughly the same.\nIn this section, we limit algorithms with a vector of features in dimension d \u2264 2. Indeed, the generalization of \u03b1-stable pdf in dimension d > 2 increases CPU time because there is no closed-form expression. Consequently, we distinguish two cases during the classification step:\n\u2022 The one dimension case: each dimension is considered as a feature.\n\u2022 The two case: we considered a vector of two features.\nWe also use a statistical test called Kolmogorov-Smirnov test (K-S test) to evaluate the quality of each model. A Kolmogorov-Smirnov test (K-S test) with a significance level of 5 % is also used to determine if two datasets differ significantly. Let us assume two null hypotheses:\n\u2022 H1: \u201cData follow a Gaussian distribution\u201d.\n\u2022 H2: \u201cData follow an \u03b1-stable distribution\u201d.\nThe notation H1 and H2 are used for the rest of the paper. If the test rejects a null hypothesis Hi, we stop the classification. Otherwise, if the test is valid, we then classify the samples belonging to the test base by using the belief classifier.\nThe results obtained with the belief functions are compared using a Bayesian approach. The prior probability p(Ci) is first calculated corresponding to the proportion of each class in the learning set. For each class, the application of Bayes theorem gives posterior probabilities:\np(Ci/x) = p(x/Ci)p(Ci) n\u2211 u=1 p(x/Cu)p(Cu) . (48)\nFinally, the decision is chosen by using the maximum nimber of posterior probabilities. We first present a classification of synthetic data, belonging to Gaussian distributions, by modeling features using a Gaussian distribution and \u03b1-stable pdf. We then compare the \u03b1-stable and Gaussian model with synthetic data belonging to \u03b1-stable distributions. The same approach is finally applied to real data from a mono-beam echo-sounder."}, {"heading": "5.1. Application to synthetic data generated by Gaussian distributions", "text": "In this subsection, we show that the \u03b1-stable and Gaussian model have the same behavior during the classification when synthetic data are generated by Gaussian distributions.\n5.1.1. Presentation of the data\nWe simulated three Gaussian pdfs in R \u00d7R (c.f. Table 1). Each distribution holds 3000 samples. A mesh between [\u22124, 4] \u00d7 [\u22124, 4] enables the level curves of pdf to be plotted (Figure 7)."}, {"heading": "5.1.2. Results", "text": "We generated data and split samples randomly into two sets: one third for the learning base and the rest for the test base. The learning base was used to estimate each mean and variance for the Gaussian distributions and each parameter \u03b1, \u03b2, \u03b3 and \u03b4 for the \u03b1-stable distributions [6, 25, 51]. We needed to make a mesh between [\u22124, 4]\u00d7 [\u22124, 4] to calculate plausibility functions in R2 with the \u03b1-stable model. Consequently, samples were chosen possessing features with dimensions within [\u22124, 4].\nThe one dimension case. For each class Ci, we consider each feature as a source of information. The estimated probability density functions can be seen as pignistic probability functions. For each pignistic probability function, we associate a mass function which will subsequently be combined with other mass functions.\nThe K-S test is valid for the two null hypotheses H1 and H2 (Table 2). We can assert that samples are drawn from the same distribution. The proposed method performs well in the particular case of synthetic data belonging to Gaussian distributions. However, the classification rates (Table 3) are very low because there is confusion between classes (Figure 8). The classification accuracy obtained with the Bayesian approach is significantly the same as the belief functions. Indeed, the Bayesian performs well provided that the probability density functions are wellestimated. The classification results depend on the prior probabilitiy estimation: if a prior probability of a class is underestimated, the confusion between classes increases.\nWe extended our study to a vector of two dimensions to try to improve the classification rate.\nThe two dimension case. We generalized the study by considering a vector of two features. The learning base was used to estimate each mean and matrix of covariance for the Gaussian distributions (Figure 9) and each parameter \u03b1, \u03c3, \u03b8 and \u03b4 for the \u03b1-stable distributions [35] (Figure 10).\nAs in one dimension, the two null hypotheses H1 and H2 were verified (Table 4). The classification results (Table 3) are somewhat less than impressive because there is confusion between classes (between C1 and C2 cf. Figure 7). Furthermore, the work in two dimensions enables better results to be obtained than the combination. The Bayesian approach gives significantly the same results as the theory of belief functions. Indeed, the estimations of model and the estimation of prior probabilities are well-estimated.\nWe extended our study by considering synthetic data generated by \u03b1-stable distributions."}, {"heading": "5.2. Application to synthetic data generated by \u03b1-stable distributions", "text": "In this subsection, we show the interest in modeling features in one and two dimensions with a single \u03b1-stable distribution."}, {"heading": "C1 0.1509 0.0873 0.1509 0.0873", "text": ""}, {"heading": "C1 0.0490 0.0650 0.1997 0.0836", "text": ""}, {"heading": "C3 0.1399 0.0870 0.1665 0.0841", "text": ""}, {"heading": "5.2.1. Presentation of the data", "text": "Three classes of artificial data sets were generated from 2D \u03b1-stable distributions [33] (c.f. Table 5 and [52] for the significance of \u03c3 and \u03b8) with 3000 samples. A mesh between [\u22124, 4] \u00d7 [\u22124, 4] enables the level curves of pdf to be plotted (Figure 11)."}, {"heading": "5.2.2. Results The one dimension case.", "text": "We can assert that the estimation with the \u03b1-stable hypothesis is better than the Gaussian hypothesis (Figure 12). There is a difference between the mode of the Gaussian model and the mode of \u03b1-stable model. Indeed, the mean, calculated from samples, for the Gaussian model does not correspond to the mode when samples do not belong to a Gaussian law. Moreover, the values of pdfs are not the same compared to the real pdfs.\nThe K-S test is not satisfied for the null hypothesis H1 for each class and each component. Consequently, we stopped the classification step for the Gaussian model. However, the K-S test is valid for the null hypothesis H2 (Table 6). We can observe that the results obtained with the theory of belief functions are significantly the same as the results obtained using the Bayesian approach. This phenomenon can be explained by the fact that prior probabilities were well-estimated. Consequently, we changed voluntarily the values of prior probabilities. We fixed arbitrarily the prior probabilities: p(C1) = 1/6, p(C2) = 2/3 and p(C3) = 1/6. The classification accuracy decreased to 53.10 % (Table 7). Indeed, the learning base and the test base can sometimes be significantly different. For example, the prior probability of class sand can be greater than the prior probabilities of classes rock and silt, although it does not\nrepresent the truth, and the classification rate of class sand can be favoured. This phenomenon introduces confusion between classes and the classification rate can decrease. Consequently, the learning step is an important step for the Bayesian approach and can have a bad effect on the classification rate whereas the theory of belief functions is less sensitive to the learning step.\nThe two dimension case. We considered a vector of two features. In Figure 13, we draw the iso-contour lines of data estimated with the Gaussian model and in Figure 14 the iso-contour lines of data estimated with the \u03b1-stable model As in one dimension, the null hypothesis H1 is rejected at significance level 5 %, but the null hypothesis H2 is valid (Table 8). The classification results (Table 9) are somewhat less than impressive because there is confusion between classes. Furthermore, we obtain the same classification accuracies with the vector of two dimensions compared to the combination of two features. However, the classification decreases when the prior probabilities are not well-estimated (Table 7).\nWe also compared these results with a GMM. It is well-known that the GMM can accurately model an arbitrary continuous distribution. This model estimation can be performed efficiently using the Expectation-Maximization (EM) algorithm. We compared classification accuracies by increasing the number of components. We drew the classification accuracy (Figure 15) in relation to the number of components. Let us assume the null hypothesis:\n\u2022 Gn: \u201cData follow a Gaussian mixture model with n components\u201d.\nThe K-S test rejects the null hypothesis Gn for a number of components n < 4. The representations of iso-contour lines (Figure 16, Figure 17 and Figure 18) confirm the K-S test. Furthermore, the classification accuracies are lower than classification rates with a single \u03b1-stable pdf.\nHowever, we can not increase indefinitely the number of components because a phenomenon of overfitting can appear (for example the representation of iso-contour by using a GMM with 5 components). Consequently, the number of 4 components seems to be a good compromise between the overfitting and good estimation. The classification accuracies are roughly the same between the single \u03b1-stable model and the GMM with 4 components."}, {"heading": "C1 0.0683 0.1003", "text": ""}, {"heading": "C1 0.3694 0.0688", "text": ""}, {"heading": "C3 0.1154 0.0932", "text": ""}, {"heading": "5.3. Application to real data from a mono-beam echo-sounder", "text": ""}, {"heading": "5.3.1. Presentation of data", "text": "We used a data set from a mono-beam echo-sounder given by the Service Hydrographique et Oce\u0301anique de la Marine (SHOM). Raw data represents an echo signal amplitude according to time. In [16], the authors classify seven types of sea floor by comparing the time envelope of echo signal amplitude with a set of theoretical references curves. In our application, raw data (Figure 19) were processed using the Quester Tangent Corporation (QTC) software [53] to\nobtain some features, which were normalized between [0,1]. These data had already been used for the navigation of an Autonomous Underwater Vehicle (AUV) [54]. The frame of discernment is \u0398 = {silt, rock, sand}, with 4853 samples from silt, 6017 from rock and 7338 from sand. The selection of features plays an important role in classification because the classification accuracy can be different according to the features. In [55], the author gave an overview of methods based on features to classify data and choose features which discriminate all classes. Some features calculated by the QTC software can be modeled by an \u03b1-stable pdf. There is a graphical test called converging variance test [56] which enables us to determine if samples belong to an \u03b1-stable distribution. This test calculates the estimation of variances by varying the number of samples. If the estimation of variance diverges, the samples can belong to an \u03b1-stable distribution. We give an example of this test for the feature called \u201cthird quantile calculated on echo signal amplitude\u201d of the class rock (Figure 20). We chose the features called the \u201cthird quantile calculated on echo signal amplitude\u201d and the \u201c25th quantile calculated on cumulative energy\u201d."}, {"heading": "5.3.2. Results", "text": "We randomly select 5000 samples for the data set. Half the samples were used for the learning base and the rest for the test base. We first consider features in one dimension, i.e. two mass functions were calculated and combined to obtain a single mass function.\nThe one dimension case. We can observe that the assumption of the \u03b1-stable model can easily accommodate the data compared to the Gaussian model (Figure 21).\nThe \u03b1-stable hypothesis is valid with the K-S test whereas the K-S test rejects the Gaussian hypothesis (Table 10). The approach using the theory of belief functions (classification accuracy of 82.68 %) give better results than the Bayesian approach (classification accuracy of 80.64 %) but not significantly. This phenomenon can be explained by the fact that the estimated models and prior probabilities are well-estimated.\nThe two dimensions case. We extended the study by using a vector of two features. The level curves of empirical data (Figure 22) show there is confusion between the class silt and the class sand. The \u03b1-stable model (Figure 23) considers the confusion between classes compared to the Gaussian model (Figure 24)."}, {"heading": "6. Conclusions", "text": "In this paper, we have shown the advantages in using the \u03b1-stable distributions to model data with heavy-tails. We considered the imprecision and uncertainty of data for modeling pdf and the theory of belief functions offers a way to model these constraints. We have examined the fundamental definitions of this theory and have shown a way to calculate plausibility functions when the knowledge of sensors is an \u03b1-stable pdf. We finally validated our model by making a comparison with the model examined by Caron et al. [43] where the pdf is a Gaussian.\nSynthetic and real data were finally classified using the theory of belief functions. We estimated the learning base, calculated plausibility functions, combined plausibility functions and calculated maximum pignistic probability to obtain the classification accuracy. The K-S test shows that synthetic data can be modeled by an \u03b1-stable model. The proposed method performs well for synthetic data. The consideration of two features improves the classification rate compared to the combination of two features for the Gaussian data. This study was then extended to a real application by using features extracted from a mono-beam echo-sounder. The Gaussian model is limited when modeling features compared to the \u03b1-stable model because the \u03b1-stable model has more degrees of freedom. To sum up, the interest in using an \u03b1-stable model is that data, in both one or two dimensions, can be modeled. The problem with the \u03b1-stable model is the computation time. Indeed, we proceed numerically to calculate plausibility functions (discretization for the bivariate \u03b1-stable pdf). Furthermore, it is difficult to generalize in dimension d > 2 without increasing CPU time.\nWe present preliminary results about classification problem by modeling features with \u03b1-stable probability density functions in dimension d \u2264 2. In future works, we plan to make experiments in the high-dimensional case with d \u2265 3. One solution in perspective to improve the computation time is to optimize the code in another language such that C/C++. Another perspective to this work is the use of other features from sensors to improve the classification accuracy, images from side-scan sonar."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the Service Hydrographique et Oce\u0301anique de la Marine (SHOM) for the data and G. Le Chenadec for his advice concerning the data."}], "references": [{"title": "Th\u00e9orie des erreurs : La loi de Gauss et les lois exceptionnelles", "author": ["P. L\u00e9vy"], "venue": "Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France. 52 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1924}, {"title": "SAR image denoising via Bayesian wavelet shrinkage based on heavy-tailed modeling", "author": ["A. Achim", "P. Tsakalides", "A. Bezerianos"], "venue": "IEEE Transactions on Geoscience and Remote Sensing. 41 (8) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive target detection in foliage-penetrating SAR images using alpha-stable models", "author": ["A. Banerjee", "P. Burlina", "R. Chellappa"], "venue": "IEEE Transactions on Image Processing. 8 (12) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Skewed \u03b1-stable distributions for modelling textures", "author": ["E.E. Kuruoglu", "J. Zerubia"], "venue": "Pattern Recognition Letters. 24 (1-3) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Financial applications of stable distributions", "author": ["J.H. McCulloch"], "venue": "Handbook of statistics, Statistical Method in Finance. 14 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "The behavior of stock-market prices", "author": ["E.F. Fama"], "venue": "Journal of business. 38 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1965}, {"title": "Bayesian Data Fusion of Multi-View Synthetic Aperture Sonar Imagery for Seabed Classification", "author": ["D.P. Williams"], "venue": "IEEE Transactions on Image Processing. 18 (6) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Upper and Lower probabilities induced by a multivalued mapping", "author": ["A. Dempster"], "venue": "Annals of Mathematical Statistics. 38 (2) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1967}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": "Princeton University Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1976}, {"title": "Bayesian approach and continuous belief functions for classification", "author": ["A. Fiche", "A. Martin"], "venue": "in: Proceedings of the Rencontre francophone sur la Logique Floue et ses Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society B. 39 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1977}, {"title": "Influence de l\u2019estimation des param\u00e8tres de texture pour la classification de donn\u00e9es complexes", "author": ["A. Fiche", "A. Martin", "J.C. Cexus", "A. Khenchaf"], "venue": "in: Proceedings of the Workshop Fouilles de donne\u0301es complexes,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Continuous belief functions and \u03b1-stable distributions", "author": ["A. Fiche", "A. Martin", "J.C. Cexus", "A. Khenchaf"], "venue": "in: Proceedings of the 13th International Conference on Information Fusion, Edinburgh, United Kingdom,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Mine classification based on raw sonar data: an approach combining Fourier descriptors", "author": ["I. Quidu", "J.P. Malkasse", "G. Burel", "P. Vilb\u00e9"], "venue": "statistical models and genetic algorithms, in: Proceedings OCEANS2000 MTS/IEEE, Providence, Rhode Island", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Martin,Multi-view fusion based on belief functions for seabed recognition", "author": ["A.H. Laanaya"], "venue": "in: Proceedings of the 12th International Conference on Information Fusion, Seattle,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Sea-bed identification using echo-sounder signals", "author": ["E. Pouliquen", "X. Lurton"], "venue": "in: Proceedings of the European Conference on Underwater Acoustics, Elsevier Applied Science, London and New York", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Simulation and chaotic behavior of \u03b1-stable stochastic processes", "author": ["A. Janicki", "A. Weron"], "venue": "Marcel Dekker, New York", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Stable non-gaussian random processes", "author": ["M.S. Taqqu", "G. Samorodnisky"], "venue": "Chapman and Hall", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "One-dimensional stable distributions", "author": ["V.M. Zolotarev"], "venue": "Translations of Mathematical Monographs Volume 65, American Mathematical Society", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "Numerical calculation of stable densities and distribution functions", "author": ["J.P. Nolan"], "venue": "Communications in Statistics-Stochastic Models. 13 (4) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Parameter estimates for symmetric stable distributions", "author": ["E.F. Fama", "R. Roll"], "venue": "Journal of the American Statistical Association. 66 (334) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1971}, {"title": "Simple consistent estimators of the parameters of stable laws", "author": ["J.H. McCulloch"], "venue": "Journal of the American Statistical Association. 75 (372) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1980}, {"title": "Applied multivariate analysis", "author": ["S.J. Press"], "venue": "Holt, Rinehart and Winston New York", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1972}, {"title": "Estimation in univariate and multivariate stable distributions", "author": ["S.J. Press"], "venue": "Journal of the American Statistical Association. 67 (340) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1972}, {"title": "Regression-type estimation of the parameters of stable laws", "author": ["I.A. Koutrouvelis"], "venue": "Journal of the American Statistical Association. 75 (372) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1980}, {"title": "An iterative procedure for the estimation of the parameters of stable laws", "author": ["I.A. Koutrouvelis"], "venue": "Communications in Statistics-Simulation and Computation. 10 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1981}, {"title": "Maximum likelihood estimates of symmetric stable distribution parameters", "author": ["B.W. Brorsen", "S.R. Yang"], "venue": "Communications in Statistics- Simulation and Computation. 19 (4) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1990}, {"title": "Linear regression with stable disturbances", "author": ["J.H. McCulloch"], "venue": "in: R. Adler, R. Feldman, M.S. Taqqu (Eds.), A practical guide to heavy tailed data, Birkh\u00e4user, Boston", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Stable distributions in statistical inference", "author": ["W.H. Dumouchel"], "venue": "Ph.D. dissertation, Department of statistics, Yale University", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1971}, {"title": "Maximum likelihood estimation and diagnostics for stable distributions", "author": ["J.P. Nolan"], "venue": "in: O.E. Barndoff-Nielsen, T. Mikosh, S. Resnick (Eds.), L\u00e9vy processes: theory and applications, Birkh\u00e4user, Boston", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Performance of the estimators of stable law parameters", "author": ["R. Weron"], "venue": "Technical Report HSC/95/01, Hugo Steinhaus Center, Wroclaw University of Technology", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1995}, {"title": "Estimation of stable-law parameters: A comparative study", "author": ["V. Akgiray", "C.G. Lamoureux"], "venue": "Journal of Business & Economic Statistics. 7 (1) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1989}, {"title": "A method for simulating stable random vectors", "author": ["R. Modarres", "J.P. Nolan"], "venue": "Computational Statistics. 9 (4) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1994}, {"title": "Estimation of the bivariate stable spectral representation by the projection method", "author": ["J.H. McCulloch"], "venue": "Computational Economics. 16 (1) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "Estimation of stable spectral measures", "author": ["J.P. Nolan", "A.K. Panorska", "J.H. McCulloch"], "venue": "Mathematical and Computer Modelling. 34 (9-11) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Combination of evidence in Dempster-Shafer theory", "author": ["K. Sentz", "S. Ferson"], "venue": "Technical Report", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "The combination of evidence in the transferable belief model, IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["Ph. Smets"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1990}, {"title": "Constructing the pignistic probability function in a context of uncertainty", "author": ["Ph. Smets"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1990}, {"title": "On random sets and belief functions", "author": ["H.T. Nguyen"], "venue": "Journal of Mathematical Analysis and Applications. 65 (3), ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1978}, {"title": "Continuous belief functions for evidential reasoning", "author": ["T.M. Strat"], "venue": "in: Proceedings of the National Conference on Artificial Intelligence, Austin, Texas", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1984}, {"title": "Belief functions on real numbers, International Journal of Approximate Reasoning", "author": ["Ph. Smets"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Ph", "author": ["B. Ristic"], "venue": "Smets, Target classification approach based on the belief function theory, IEEE Transactions on Aerospace and Electronic Systems. 42 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "Least Committed basic belief density induced by a multivariate Gaussian pdf", "author": ["F. Caron", "B. Ristic", "E. Duflos", "P. Vanheeghe"], "venue": "International Journal of Approximate Reasoning. 48 (2) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Constructing of a consonant belief function induced by a multimodal probability density function, in: Proceedings of COGnitive systems with Interactive Sensors", "author": ["P.E. Dor\u00e9", "A. Martin", "A. Khenchaf"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Continuous belief functions: singletons plausibility function in conjunctive and disjunctive combination operations of consonant bbds", "author": ["J.M. Vannobel"], "venue": "in: Proceedings of the Workshop on the Theory of Belief Function,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Unimodality of infinitely divisible distribution functions of class L", "author": ["M. Yamazato"], "venue": "The Annals of Probability. 6 ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1978}, {"title": "Belief functions: The disjunctive rule of combination and the generalized Bayesian theorem, International Journal of Approximate Reasoning", "author": ["Ph. Smets"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1993}, {"title": "Ph", "author": ["F. Delmotte"], "venue": "Smets, Target identification based on the transferable belief model interpretation of Dempster\u2013Shafer model, IEEE Transactions on Systems, Man and Cybernetics. 34 (4) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2004}, {"title": "Textural features for image classification", "author": ["R.M. Haralick", "K. Shanmugam", "I.H. Dinstein"], "venue": "IEEE Transactions on Systems, Man and Cybernetics. 3 (6) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1973}, {"title": "Statistical and structural approaches to texture", "author": ["R.M. Haralick"], "venue": "Proceedings of the IEEE. 67 (5) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1979}, {"title": "Simple consistent estimators of stable distribution parameters", "author": ["J.H. McCulloch"], "venue": "Communications in Statistics-Simulation and Computation. 15 (4) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1986}, {"title": "Calculation of multidimensional stable densities", "author": ["J.P. Nolan", "B. Rajput"], "venue": "Communications in Statistics-Simulation and Computation. 24 (3) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1995}, {"title": "Sea bottom classification from echo sounding data", "author": ["D. Caughey", "B. Prager", "J. Klymak"], "venue": "Quester Tangent Corporation, Marine Technology Center, British Columbia, V8L 3S1, Canada", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1994}, {"title": "Autonomous Underwater Vehicle sensors fusion by the theory of belief functions for Rapid Environment Assessment", "author": ["A. Martin", "J.C. Cexus", "G. Le Chenadec", "E. Cant\u00e9ro", "T. Landeau", "Y. Dupas", "R. Courtois"], "venue": "in: Proceedings of the 10th European Conference on Underwater Acoustics,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2010}, {"title": "Seabed segmentation using optimized statistics of sonar textures", "author": ["I. Karoui", "R. Fablet", "J.M. Boucher", "J.M. Augustin"], "venue": "IEEE Transactions on Geoscience and Remote Sensing. 47 (16) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The main property of stable laws introduced by L\u00e9vy [1] is that the sum of two independent stable random variables gives a stable random variable.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "\u03b1-stable distributions are used in different fields of research such as radar [2, 3], image processing [4] or finance [5, 6], .", "startOffset": 78, "endOffset": 84}, {"referenceID": 2, "context": "\u03b1-stable distributions are used in different fields of research such as radar [2, 3], image processing [4] or finance [5, 6], .", "startOffset": 78, "endOffset": 84}, {"referenceID": 3, "context": "\u03b1-stable distributions are used in different fields of research such as radar [2, 3], image processing [4] or finance [5, 6], .", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "\u03b1-stable distributions are used in different fields of research such as radar [2, 3], image processing [4] or finance [5, 6], .", "startOffset": 118, "endOffset": 124}, {"referenceID": 5, "context": "\u03b1-stable distributions are used in different fields of research such as radar [2, 3], image processing [4] or finance [5, 6], .", "startOffset": 118, "endOffset": 124}, {"referenceID": 6, "context": "In [7], the author proposes to characterize the sea floor from a vector of features modeled by Gaussian mixture models (GMMs) using an Autonomous Underwater Vehicle (AUV).", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "We therefore favored the use of an approach based on the theory of belief functions [8, 9].", "startOffset": 84, "endOffset": 90}, {"referenceID": 8, "context": "We therefore favored the use of an approach based on the theory of belief functions [8, 9].", "startOffset": 84, "endOffset": 90}, {"referenceID": 9, "context": "In [10], the authors compared a Bayesian and belief classifier where data from sensors are modeled using GMMs estimated via an Expectation-Maximization (EM) algorithm [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [10], the authors compared a Bayesian and belief classifier where data from sensors are modeled using GMMs estimated via an Expectation-Maximization (EM) algorithm [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "This work has been extended to data modeled by \u03b1-stable mixture models [12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "This point has been dealt with in [13].", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "SONAR has therefore been used for the detection of underwater mines [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "In [15], the author developed techniques to perform automatic classification of sediments on the seabed from sonar images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "In [10], the authors classified sonar images by extracting features and modeling them with GMMs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [16], the authors characterized the sea floor using data from a mono-beam echo-sounder.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "With the results of Polya and Bernstein, Janicki and Weron [17] demonstrated that the family of \u03b1-stable laws are probability density functions.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "The mathematician Paul L\u00e9vy studied the central limit theorem and showed, with the constraint of infinite variance, that the limit law is a stable law [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 17, "context": "Characteristic function Several equivalent definitions have been suggested in the literature to parameterize an \u03b1-stable distribution from its characteristic function [18, 19].", "startOffset": 167, "endOffset": 175}, {"referenceID": 18, "context": "Characteristic function Several equivalent definitions have been suggested in the literature to parameterize an \u03b1-stable distribution from its characteristic function [18, 19].", "startOffset": 167, "endOffset": 175}, {"referenceID": 18, "context": "Zolotarev [19] proposed the following:", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "The advantage of this parameterization compared to [18] is that the values of the characterization and probability density functions are continuous for all parameters.", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "In fact, the parameterization defined by [18] is discontinuous when \u03b1 = 1 and \u03b2 = 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "Nolan [20] therefore proposed a way to represent normalized \u03b1-stable distributions (i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "The main idea of Nolan [20] is to use variable modifications so that the integral has finite bounds.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 142, "endOffset": 150}, {"referenceID": 21, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 142, "endOffset": 150}, {"referenceID": 22, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 196, "endOffset": 212}, {"referenceID": 23, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 196, "endOffset": 212}, {"referenceID": 24, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 196, "endOffset": 212}, {"referenceID": 25, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 196, "endOffset": 212}, {"referenceID": 26, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 249, "endOffset": 265}, {"referenceID": 27, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 249, "endOffset": 265}, {"referenceID": 28, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 249, "endOffset": 265}, {"referenceID": 29, "context": "An overview of \u03b1-stable estimators The estimators of \u03b1-stable distributions are decomposed into three families: \u2022 the sample quantile methods [21, 22] \u2022 the sample characteristic function methods [23, 24, 25, 26] \u2022 the Maximum Likelihood Estimation [27, 28, 29, 30].", "startOffset": 249, "endOffset": 265}, {"referenceID": 20, "context": "Fama and Roll [21] developed a method based on quantiles.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "McCulloch [22] extended the quantile method to the asymmetric case (i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "Press [23, 24] proposed a method based on transformations of characteristic function.", "startOffset": 6, "endOffset": 14}, {"referenceID": 23, "context": "Press [23, 24] proposed a method based on transformations of characteristic function.", "startOffset": 6, "endOffset": 14}, {"referenceID": 30, "context": "In [31], the author compared the performances of several estimators.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Koutrouvelis [25] extended the Press method and proposed a regression method to estimate the parameters of \u03b1-stable distributions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "He proposed a second version of his algorithm which is distinct that it is iterative [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "In [32], the authors proved that the method proposed by Koutrouvelis is better than both the quantile method and the Press method because it gives consistent and asymptotically unbiased estimates.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "The Maximum Likelihood Estimation (MLE) was first studied in the symmetric case [27, 28].", "startOffset": 80, "endOffset": 88}, {"referenceID": 27, "context": "The Maximum Likelihood Estimation (MLE) was first studied in the symmetric case [27, 28].", "startOffset": 80, "endOffset": 88}, {"referenceID": 28, "context": "Dumouchel [29] developed an approximate maximum likelihood method.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Nolan [30] extended the MLE in general case.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "Consequently, we estimate a univariate \u03b1-stable distribution using the Koutrouvelis method [26].", "startOffset": 91, "endOffset": 95}, {"referenceID": 32, "context": "To avoid this problem, it is possible to consider a discrete spectral measure [33] which has the form: \u03c3(.", "startOffset": 78, "endOffset": 82}, {"referenceID": 33, "context": "There are two methods to estimate an \u03b1-stable random vector: \u2022 the PROJection method [34] (PROJ) \u2022 the Empirical Characteristic Function method [35] (ECF) These two algorithms have the same performances in terms of estimation and computation time.", "startOffset": 85, "endOffset": 89}, {"referenceID": 34, "context": "There are two methods to estimate an \u03b1-stable random vector: \u2022 the PROJection method [34] (PROJ) \u2022 the Empirical Characteristic Function method [35] (ECF) These two algorithms have the same performances in terms of estimation and computation time.", "startOffset": 144, "endOffset": 148}, {"referenceID": 7, "context": "Definitions Discrete belief functions were introduced by Dempster [8], and formalized by Shafer [9] where he considers a discrete set of n exclusive events Ci called the frame of discernment:", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Definitions Discrete belief functions were introduced by Dempster [8], and formalized by Shafer [9] where he considers a discrete set of n exclusive events Ci called the frame of discernment:", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "Belief functions are defined as 2 onto [0,1].", "startOffset": 39, "endOffset": 44}, {"referenceID": 35, "context": "There are several combination rules [36] in the literature which differently address conflicts between sources.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "The most common rule is the conjunctive combination [37] where the resultant mass of A is obtained by:", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "The final mass is obtained by carrying out the inverse operation of Equation (11) [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 37, "context": "Pignistic probability To make a decision on \u0398, several operators exist such that maximum credibility or maximum plausibility with pignistic probability being the most commonly used operator [38].", "startOffset": 190, "endOffset": 194}, {"referenceID": 8, "context": "Continuous belief functions The basic description of continuous belief functions was accomplished by Shafer [9], then by Nguyen [39] and Strat [40].", "startOffset": 108, "endOffset": 111}, {"referenceID": 38, "context": "Continuous belief functions The basic description of continuous belief functions was accomplished by Shafer [9], then by Nguyen [39] and Strat [40].", "startOffset": 128, "endOffset": 132}, {"referenceID": 39, "context": "Continuous belief functions The basic description of continuous belief functions was accomplished by Shafer [9], then by Nguyen [39] and Strat [40].", "startOffset": 143, "endOffset": 147}, {"referenceID": 40, "context": "Recently, Smets [41] extended the definition of belief functions to the set of reals R = R \u222a {\u2212\u221e,+\u221e} and masses are only attributed to intervals of R.", "startOffset": 16, "endOffset": 20}, {"referenceID": 40, "context": "Many papers [41, 42, 43] deal with the particular case of continuous belief functions with nested focal elements.", "startOffset": 12, "endOffset": 24}, {"referenceID": 41, "context": "Many papers [41, 42, 43] deal with the particular case of continuous belief functions with nested focal elements.", "startOffset": 12, "endOffset": 24}, {"referenceID": 42, "context": "Many papers [41, 42, 43] deal with the particular case of continuous belief functions with nested focal elements.", "startOffset": 12, "endOffset": 24}, {"referenceID": 40, "context": "For example, Smets [41]", "startOffset": 19, "endOffset": 23}, {"referenceID": 43, "context": "In [44, 45], the authors propose a way to build belief functions with connected sets by using a credal measure and an index function.", "startOffset": 3, "endOffset": 11}, {"referenceID": 44, "context": "In [44, 45], the authors propose a way to build belief functions with connected sets by using a credal measure and an index function.", "startOffset": 3, "endOffset": 11}, {"referenceID": 43, "context": "Credal measure and index function In [44], the authors propose a way to calculate belief functions from any probability density function.", "startOffset": 37, "endOffset": 41}, {"referenceID": 45, "context": "The function Bet f [Ci] is supposed to be bell-shaped for all \u03b1-stable distributions (proved by Yamazato [46]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 46, "context": "Several plausibility functions associated to the same feature can be combined by using the generalized Bayes theorem [47, 48] to calculate mass functions allocated to A of an interval I: mR[x](A) = \u220f", "startOffset": 117, "endOffset": 125}, {"referenceID": 47, "context": "Several plausibility functions associated to the same feature can be combined by using the generalized Bayes theorem [47, 48] to calculate mass functions allocated to A of an interval I: mR[x](A) = \u220f", "startOffset": 117, "endOffset": 125}, {"referenceID": 41, "context": "To validate our approach, we classify planes using kinematic data as in [42, 43] and compare the decision with the approach of Caron et al.", "startOffset": 72, "endOffset": 80}, {"referenceID": 42, "context": "To validate our approach, we classify planes using kinematic data as in [42, 43] and compare the decision with the approach of Caron et al.", "startOffset": 72, "endOffset": 80}, {"referenceID": 42, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "In [43], the authors calculate plausibility function in the Gaussian pdf situation of mode \u03b4 and matrix of covariance \u03a3.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "[44] to build belief functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "We obtain d mass functions (one for each feature) at point x with the generalized Bayes theorem [47, 48] (Equation (42)).", "startOffset": 96, "endOffset": 104}, {"referenceID": 47, "context": "We obtain d mass functions (one for each feature) at point x with the generalized Bayes theorem [47, 48] (Equation (42)).", "startOffset": 96, "endOffset": 104}, {"referenceID": 43, "context": "For an \u03b1-stable probability density function, plausibility functions are calculated for each feature by using the approach of Dor\u00e9 et al [44] (section 3.", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "In [12], synthetic data are classified by modeling features using Gaussian and \u03b1-stable mixture models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "Images from a side-scan sonar are automatically classified by extracting Haralick features [49, 50].", "startOffset": 91, "endOffset": 99}, {"referenceID": 49, "context": "Images from a side-scan sonar are automatically classified by extracting Haralick features [49, 50].", "startOffset": 91, "endOffset": 99}, {"referenceID": 5, "context": "The learning base was used to estimate each mean and variance for the Gaussian distributions and each parameter \u03b1, \u03b2, \u03b3 and \u03b4 for the \u03b1-stable distributions [6, 25, 51].", "startOffset": 157, "endOffset": 168}, {"referenceID": 24, "context": "The learning base was used to estimate each mean and variance for the Gaussian distributions and each parameter \u03b1, \u03b2, \u03b3 and \u03b4 for the \u03b1-stable distributions [6, 25, 51].", "startOffset": 157, "endOffset": 168}, {"referenceID": 50, "context": "The learning base was used to estimate each mean and variance for the Gaussian distributions and each parameter \u03b1, \u03b2, \u03b3 and \u03b4 for the \u03b1-stable distributions [6, 25, 51].", "startOffset": 157, "endOffset": 168}, {"referenceID": 34, "context": "The learning base was used to estimate each mean and matrix of covariance for the Gaussian distributions (Figure 9) and each parameter \u03b1, \u03c3, \u03b8 and \u03b4 for the \u03b1-stable distributions [35] (Figure 10).", "startOffset": 180, "endOffset": 184}, {"referenceID": 32, "context": "Presentation of the data Three classes of artificial data sets were generated from 2D \u03b1-stable distributions [33] (c.", "startOffset": 109, "endOffset": 113}, {"referenceID": 51, "context": "Table 5 and [52] for the significance of \u03c3 and \u03b8) with 3000 samples.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "In [16], the authors classify seven types of sea floor by comparing the time envelope of echo signal amplitude with a set of theoretical references curves.", "startOffset": 3, "endOffset": 7}, {"referenceID": 52, "context": "In our application, raw data (Figure 19) were processed using the Quester Tangent Corporation (QTC) software [53] to 14", "startOffset": 109, "endOffset": 113}, {"referenceID": 0, "context": "obtain some features, which were normalized between [0,1].", "startOffset": 52, "endOffset": 57}, {"referenceID": 53, "context": "These data had already been used for the navigation of an Autonomous Underwater Vehicle (AUV) [54].", "startOffset": 94, "endOffset": 98}, {"referenceID": 54, "context": "In [55], the author gave an overview of methods based on features to classify data and choose features which discriminate all classes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 42, "context": "[43] where the pdf is a Gaussian.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "The aim of this paper is to show the interest in fitting features with an \u03b1-stable distribution to classify imperfect data. The supervised pattern recognition is thus based on the theory of continuous belief functions, which is a way to consider imprecision and uncertainty of data. The distributions of features are supposed to be unimodal and estimated by a single Gaussian and \u03b1-stable model. Experimental results are first obtained from synthetic data by combining two features of one dimension and by considering a vector of two features. Mass functions are calculated from plausibility functions by using the generalized Bayes theorem. The same study is applied to the automatic classification of three types of sea floor (rock, silt and sand) with features acquired by a mono-beam echo-sounder. We evaluate the quality of the \u03b1-stable model and the Gaussian model by analyzing qualitative results, using a Kolmogorov-Smirnov test (K-S test), and quantitative results with classification rates. The performances of the belief classifier are compared with a Bayesian approach.", "creator": "LaTeX with hyperref package"}}}