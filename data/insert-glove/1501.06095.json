{"id": "1501.06095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2015", "title": "Between Pure and Approximate Differential Privacy", "abstract": "We 6-feet-3 show a clotaire new szijjarto lower nyenze bound toil on the schneid sample 82.15 complexity of $ (\\ cedo varepsilon, \\ legere delta) $ - homebuilding differentially private algorithms raetia that accurately brennus answer raftopoulos statistical queries altesse on high - dimensional f\u00fa databases. dewaniya The overage novelty marinucci of rony our bound muravyev is suranaree that 12.46 it depends marguerite optimally stager on vicious the in-car parameter $ \\ 9/11/01 delta $, mesud which emittance loosely 159.7 corresponds stri to adriaenssens the hjelmeset probability levitating that the algorithm geniza fails to ycbcr be private, and well-to-do is cangzhou the unrounded first unscriptural to cowpeas smoothly vlasotince interpolate judum between approximate differential mobisodes privacy ($ \\ israelson delta & antoin gt; 0 $) kursh and pure bhatta differential materialised privacy ($ \\ lft delta = 0 $ ).", "histories": [["v1", "Sat, 24 Jan 2015 23:26:21 GMT  (18kb,D)", "http://arxiv.org/abs/1501.06095v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.LG", "authors": ["thomas steinke", "jonathan ullman"], "accepted": false, "id": "1501.06095"}, "pdf": {"name": "1501.06095.pdf", "metadata": {"source": "CRF", "title": "Between Pure and Approximate Differential Privacy", "authors": ["Thomas Steinke", "Jonathan Ullman"], "emails": ["tsteinke@seas.harvard.edu.", "jullman@cs.columbia.edu."], "sections": [{"heading": null, "text": "Specifically, we consider a database D \u2208 {\u00b11}n\u00d7d and its one-way marginals, which are the d queries of the form \u201cWhat fraction of individual records have the i-th bit set to +1?\u201d We show that in order to answer all of these queries to within error \u00b1\u03b1 (on average) while satisfying (\u03b5,\u03b4)-differential privacy, it is necessary that\nn \u2265\u2126 \u221ad log(1/\u03b4)\u03b1\u03b5  , which is optimal up to constant factors. To prove our lower bound, we build on the connection between fingerprinting codes and lower bounds in differential privacy (Bun, Ullman, and Vadhan, STOC\u201914).\nIn addition to our lower bound, we give new purely and approximately differentially private algorithms for answering arbitrary statistical queries that improve on the sample complexity of the standard Laplace and Gaussian mechanisms for achieving worst-case accuracy guarantees by a logarithmic factor.\n\u2217Harvard University School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616. Email: tsteinke@seas.harvard.edu. \u2020Columbia University Department of Computer Science. Supported by a Junior Fellowship from the Simons Society of Fellows. Email: jullman@cs.columbia.edu.\nar X\niv :1\n50 1.\n06 09\n5v 1\n[ cs\n.D S]\n2 4\nJa n\n20 15\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 Average-Case Versus Worst-Case Error . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3"}, {"heading": "2 Preliminaries 4", "text": ""}, {"heading": "3 Lower Bounds for Approximate Differential Privacy 5", "text": ""}, {"heading": "4 New Mechanisms for L\u221e Error 7", "text": "4.1 Pure Differential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.2 Approximate Differential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nReferences 10\nA Alternative Lower Bound for Pure Differential Privacy 12"}, {"heading": "1 Introduction", "text": "The goal of privacy-preserving data analysis is to enable rich statistical analysis of a database while protecting the privacy of individuals whose data is in the database. A formal privacy guarantee is given by (\u03b5,\u03b4)-differential privacy [DMNS06, DKM+06], which ensures that no individual\u2019s data has a significant influence on the information released about the database. The two parameters \u03b5 and \u03b4 control the level of privacy. Very roughly, \u03b5 is an upper bound on the amount of influence an individual\u2019s record has on the information released and \u03b4 is the probability that this bound fails to hold1, so the definition becomes more stringent as \u03b5,\u03b4\u2192 0.\nA natural way to measure the tradeoff between privacy and utility is sample complexity\u2014 the minimum number of records n that is sufficient in order to publicly release a given set of statistics about the database, while achieving both differential privacy and statistical accuracy. Intuitively, it\u2019s easier to achieve these two goals when n is large, as each individual\u2019s data will have only a small influence on the aggregate statistics of interest. Conversely, the sample complexity n should increase as \u03b5 and \u03b4 decrease (which strengthens the privacy guarantee).\nThe strongest version of differential privacy, in which \u03b4 = 0, is known as pure differential privacy. The sample complexity of achieving pure differential privacy is well known for many settings (e.g. [HT10]). The more general case where \u03b4 > 0 is known as approximate differential privacy, and is less well understood. Recently, Bun, Ullman, and Vadhan [BUV14] showed how to prove strong lower bounds for approximate differential privacy that are essentially optimal for \u03b4 \u2248 1/n, which is essentially the weakest privacy guarantee that is still meaningful.2\nSince \u03b4 bounds the probability of a complete privacy breach, we would like \u03b4 to be very small. Thus we would like to quantify the cost (in terms of sample complexity) as \u03b4\u2192 0. In this work we give lower bounds for approximately differentially private algorithms that are nearly optimal for every choice of \u03b4, and smoothly interpolate between pure and approximate differential privacy.\nSpecifically, we consider algorithms that compute the one-way marginals of the database\u2014an extremely simple and fundamental family of queries. For a databaseD \u2208 {\u00b11}n\u00d7d , the d one-way marginals are simply the mean of the bits in each of the d columns. Formally, we define\nD := 1 n n\u2211 i=1 Di \u2208 [\u00b11]d\nwhere Di \u2208 {\u00b11}d is the i-th row of D. A mechanism M is said to be accurate if, on input D, its output is \u201cclose to\u201dD. Accuracy may be measured in a worst-case sense\u2014i.e. \u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223\u221e \u2264 \u03b1, meaning every one-way marginal is answered with accuracy \u03b1\u2014or in an average-case sense\u2014 i.e. \u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223 1 \u2264 \u03b1d, meaning the marginals are answered with average accuracy \u03b1.\nSome of the earliest results in differential privacy [DN03, DN04, BDMN05, DMNS06] give a simple (\u03b5,\u03b4)-differentially private algorithm\u2014the Laplace mechanism\u2014that computes the oneway marginals of D \u2208 {\u00b11}n\u00d7d with average error \u03b1 as long as\nn \u2265O min  \u221a d log(1/\u03b4) \u03b5\u03b1 , d \u03b5\u03b1   . (1)\n1This intuition is actually somewhat imprecise, although it is suitable for this informal discussion. See [KS08] for a more precise semantic interpretation of (\u03b5,\u03b4)-differential privacy.\n2When \u03b4 \u2265 1/n there are algorithms that are intuitively not private, yet satisfy (0,\u03b4)-differential privacy.\nThe previous best lower bounds are n \u2265\u2126(d/\u03b5\u03b1) [HT10] for pure differential privacy and n \u2265 \u2126\u0303( \u221a d/\u03b5\u03b1) for approximate differential privacy with \u03b4 = o(1/n) [BUV14]. Our main result is an optimal lower bound that combines the previous lower bounds.\nTheorem 1.1 (Main Theorem). For every \u03b5 \u2264 O(1), every 2\u2212\u2126(n) \u2264 \u03b4 \u2264 1/n1+\u2126(1) and every \u03b1 \u2264 1/10, if M : {\u00b11}n\u00d7d \u2192 [\u00b11]d is (\u03b5,\u03b4)-differentially private and E\nM\n[ \u2016M(D)\u2212D\u20161 ] \u2264 \u03b1d, then\nn \u2265\u2126 \u221ad log(1/\u03b4)\u03b5\u03b1  . More generally, this is the first result showing that the sample complexity must grow by a\nmultiplicative factor of \u221a\nlog(1/\u03b4) for answering any family of queries, as opposed to an additive dependence on \u03b4. We also remark that the assumption on the range of \u03b4 is necessary, as the Laplace mechanism gives accuracy \u03b1 and satisfies (\u03b5,0)-differential privacy when n \u2265O(d/\u03b5\u03b1)."}, {"heading": "1.1 Average-Case Versus Worst-Case Error", "text": "Our lower bound holds for mechanisms with an average-case (L1) error guarantee. Thus, it also holds for algorithms that achieve worst-case (L\u221e) error guarantees. The Laplace mechanism gives a matching upper bound for average-case error. In many cases worst-case error guarantees are preferrable. For worst-case error, the sample complexity of the Laplace mechanism degrades by an additional logd factor compared to (1).\nSurprisingly, this degradation is not necessary. We present algorithms that answer every one-way marginal with \u03b1 accuracy and improve on the sample complexity of the Laplace mechanism by roughly a logd factor. These algorithms demonstrate that the widely used technique of adding independent noise to each query is suboptimal when the goal is to achieve worst-case error guarantees.\nOur algorithm for pure differential privacy satisfies the following.\nTheorem 1.2. For every \u03b5,\u03b1 > 0, d \u2265 1, and n \u2265 4d/\u03b5\u03b1, there exists an efficient mechanism M : {\u00b11}n\u00d7d \u2192 [\u00b11]d that is (\u03b5,0)-differentially private and\n\u2200D \u2208 {\u00b11}n\u00d7d P M [\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223\u221e \u2265 \u03b1] \u2264 (2e)\u2212d . And our algorithm for approximate differential privacy is as follows.\nTheorem 1.3. For every \u03b5,\u03b4,\u03b1 > 0, d \u2265 1, and n \u2265O \u221ad \u00b7 log(1/\u03b4) \u00b7 loglogd\u03b5\u03b1  , there exists an efficient mechanism M : {\u00b11}n\u00d7d \u2192 [\u00b11]d that is (\u03b5,\u03b4)-differentially private and\n\u2200D \u2208 {\u00b11}n\u00d7d P M [\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223\u221e \u2265 \u03b1] \u2264 1d\u03c9(1) . These algorithms improve over the sample complexity of the best known mechanisms for each privacy and accuracy guarantee by a factor of (log(d))\u2126(1). Namely, the Laplace mechanism requires n \u2265O(d \u00b7 logd/\u03b5\u03b1) samples for pure differential privacy and the Gaussian mechanism requires n \u2265O( \u221a d \u00b7 log(1/\u03b4) \u00b7 logd/\u03b5\u03b1) samples for approximate differential privacy."}, {"heading": "1.2 Techniques", "text": "Lower Bounds: Our lower bound relies on a combinatorial objected called a fingerprinting code [BS98]. Fingerprinting codes were originally used in cryptography for watermarking digital content, but several recent works have shown they are intimately connected to lower bounds for differential privacy and related learning problems [Ull13, BUV14, HU14, SU14]. In particular, Bun et al. [BUV14] showed that fingerprinting codes can be used to construct an attack demonstrating that any mechanism that accurately answers one-way marginals is not differentially private. Specifically, a fingerprinting code gives a distribution on individuals\u2019 data and a corresponding \u201ctracer\u201d algorithm such that, if a database is constructed from the data of a fixed subset of the individuals, then the tracer algorithm can identify at least one of the individuals in that subset given only approximate answers to the one-way marginals of the database. Specifically, their attack shows that a mechanism that satisfies (1, o(1/n))-differential privacy requires n \u2265 \u2126\u0303( \u221a d) samples to accurately compute one-way marginals.\nOur proof uses a new, more general reduction from breaking fingerprinting codes to differentially private data release. Specifically, our reduction uses group differential privacy. This property states that if an algorithm is (\u03b5,\u03b4)-differentially private with respect to the change of one individual\u2019s data, then for any k, it is roughly (k\u03b5,ek\u03b5\u03b4)-differentially private with respect to the change of k individuals\u2019 data. Thus an (\u03b5,\u03b4)-differentially private algorithm provides a meaningful privacy guarantee for groups of size k \u2248 log(1/\u03b4)/\u03b5.\nTo use this in our reduction, we start with a mechanism M that takes a database of n rows and is (\u03b5,\u03b4)-differentially private. We design a mechanismMk that takes a database of n/k rows, copies each of its rows k times, and uses the result as input to M. The resulting mechanism Mk is roughly (k\u03b5,ek\u03b5\u03b4)-differentially private. For our choice of k, these parameters will be small enough to apply the attack of [BUV14] to obtain a lower bound on the number of samples used by Mk , which is n/k. Thus, for larger values of k (equivalently, smaller values of \u03b4), we obtain a stronger lower bound. The remainder of the proof is to quantify the parameters precisely.\nUpper Bounds: Our algorithm for pure differential privacy and worst-case error is an instantiation of the exponential mechanism [MT07] using the L\u221e norm. That is, the mechanism\nsamples y \u2208 Rd with probability proportional to exp(\u2212\u03b7 \u2223\u2223\u2223\u2223\u2223\u2223y\u2223\u2223\u2223\u2223\u2223\u2223\u221e) and outputs M(D) = D + y. In contrast, adding independent Laplace noise corresponds to using the exponential mechanism with the L1 norm and adding independent Gaussian noise corresponds to using the exponential mechanism with the L2 norm squared. Using this distribution turns out to give better tail bounds than adding independent noise.\nFor approximate differential privacy, we use a completely different algorithm. We start by adding independent Gaussian noise to each marginal. However, rather than using a union bound to show that each Gaussian error is small with high probability, we use a Chernoff bound to show that most errors are small. Namely, with the sample complexity that we allow M, we can ensure that all but a 1/polylog(d) fraction of the errors are small. Now we \u201cfix\u201d the d/polylog(d) marginals that are bad. The trick is that we use the sparse vector algorithm, which allows us to do indentify and fix these d/polylog(d) marginals with sample complexity corresponding to only d/polylog(d) queries, rather than d queries."}, {"heading": "2 Preliminaries", "text": "We define a database D \u2208 {\u00b11}n\u00d7d to be a matrix of n rows, where each row corresponds to an individual, and each row has dimension d (consists of d binary attributes). We say that two databases D,D \u2032 \u2208 {\u00b11}n\u00d7d are adjacent if they differ only by a single row, and we denote this by D \u223c D \u2032. In particular, we can replace the ith row of a database D with some fixed element of {\u00b11}d to obtain another database D\u2212i \u223cD. Definition 2.1 (Differential Privacy [DMNS06]). Let M : {\u00b11}n\u00d7d \u2192R be a randomized mechanism. We say that M is (\u03b5,\u03b4)-differentially private if for every two adjacent databases D \u223c D \u2032 and every subset S \u2286R,\nP [M(D) \u2208 S] \u2264 e\u03b5 \u00b7P [ M(D \u2032) \u2208 S ] + \u03b4.\nA well known fact about differential privacy is that it generalizes smoothly to databases that differ on more than a single row. We say that two databases D,D \u2032 \u2208 {\u00b11}n\u00d7d are k-adjacent if they differ by at most k rows, and we denote this by D \u223ck D \u2032. Fact 2.2 (Group Differential Privacy). For every k \u2265 1, if M : {\u00b11}n\u00d7d \u2192 R is (\u03b5,\u03b4)-differentially private, then for every two k-adjacent databases D \u223ck D \u2032, and every subset S \u2286R,\nP [M(D) \u2208 S] \u2264 ek\u03b5 \u00b7P [ M(D \u2032) \u2208 S ] + ek\u03b5 \u2212 1 e\u03b5 \u2212 1 \u00b7 \u03b4.\nAll of the upper and lower bounds for one-way marginals have a multiplicative 1/\u03b1\u03b5 dependence on the accuracy \u03b1 and the privacy loss \u03b5. This is no coincidence - there is a generic reduction:\nFact 2.3 (\u03b1 and \u03b5 dependence). Let p \u2208 [1,\u221e] and \u03b1,\u03b5,\u03b4 \u2208 [0,1/10]. Suppose there exists a (\u03b5,\u03b4)-differentially private mechanism M : {\u00b11}n\u00d7d \u2192 [\u00b11]d such that for every database D \u2208 {\u00b11}n\u00d7d , E M [ \u2016M(D)\u2212D\u2016p ] \u2264 \u03b1d1/p.\nThen there exists a (1,\u03b4/\u03b5)-differentially private mechanism M \u2032 : {\u00b11}n\u2032\u00d7d \u2192 [\u00b11]d for n\u2032 = \u0398(\u03b1\u03b5n) such that for every database D \u2032 \u2208 {\u00b11}n\u2032\u00d7d ,\nE M \u2032\n[ \u2016M \u2032(D \u2032)\u2212D \u2032\u2016p ] \u2264 d1/p/10.\nThis fact allows us to suppress the accuracy parameter \u03b1 and the privacy loss \u03b5 when proving our lower bounds. Namely, if we prove a lower bound of n\u2032 \u2265 n\u2217 for all (1,\u03b4)-differentially private mechanisms M \u2032 : {\u00b11}n\u2032\u00d7d \u2192 [\u00b11]d with E\nM \u2032\n[ \u2016M \u2032(D \u2032)\u2212D \u2032\u2016p ] \u2264 d1/p/10, then we obtain\na lower bound of n \u2265 \u2126(n\u2217/\u03b1\u03b5) for all (\u03b5,\u03b5\u03b4)-differentially private mechanisms M : {\u00b11}n\u00d7d \u2192 [\u00b11]d with E\nM\n[ \u2016M(D)\u2212D\u2016p ] \u2264 \u03b1d1/p. So we will simply fix the parameters \u03b1 = 1/10 and \u03b5 = 1 in\nour lower bounds."}, {"heading": "3 Lower Bounds for Approximate Differential Privacy", "text": "Our main theorem can be stated as follows.\nTheorem 3.1 (Main Theorem). Let M : {\u00b11}n\u00d7d \u2192 [\u00b11]d be a (1,\u03b4)-differentially private mechanism that answers one-way marginals such that\n\u2200D \u2208 {\u00b11}n\u00d7d E M [\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223 1 ] \u2264 d 10 ,\nwhere D is the true answer vector. If 2\u2212\u2126(n) \u2264 \u03b4 \u2264 1/n1+\u2126(1) and n is sufficiently large, then\nd \u2264O ( n2\nlog(1/\u03b4)\n) .\nTheorem 1.1 in the introduction follows by rearranging terms, and applying Fact 2.3. The statement above is more convenient technically, but the statement in the introduction is more consistent with the literature.\nFirst we must introduce fingerprinting codes. The following definition is tailored to the application to privacy. Fingerprinting codes were originally defined by Boneh and Shaw [BS98] with a worst-case accuracy guarantee. Subsequent works [BUV14, SU14] have altered the accuracy guarantee to an average-case one, which we use here.\nDefinition 3.2 (L1 Fingerprinting Code). A \u03b5-complete \u03b4-sound \u03b1-robust L1 fingerprinting code for n users with length d is a pair of random variables D \u2208 {\u00b11}n\u00d7d and Trace : [\u00b11]d \u2192 2[n] such that the following hold.\nCompleteness: For any fixed M : {\u00b11}n\u00d7d \u2192 [\u00b11]d ,\nP [(\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223\n1 \u2264 \u03b1d\n) \u2227 (Trace(M(D)) = \u2205) ] \u2264 \u03b5.\nSoundness: For any i \u2208 [n] and fixed M : {\u00b11}n\u00d7d \u2192 [\u00b11]d ,\nP [i \u2208 Trace(M(D\u2212i))] \u2264 \u03b4,\nwhere D\u2212i denotes D with the ith row replaced by some fixed element of {\u00b11}d .\nFingerprinting codes with optimal length were first constructed by Tardos [Tar08] (for worst-case error) and subsequent works [BUV14, SU14] have adapted Tardos\u2019 construction to work for average-case error guarantees, which yields the following theorem.\nTheorem 3.3. For every n \u2265 1, \u03b4 > 0, and d \u2265 dn,\u03b4 = O(n2 log(1/\u03b4)), there exists a 1/100-complete \u03b4-sound 1/8-robust L1 fingerprinting code for n users with length d.\nWe now show how the existence of fingerprinting codes implies our lower bound.\nProof of Theorem 3.1 from Theorem 3.3. Let M : {\u00b11}n\u00d7d \u2192 [\u00b11]d be a (1,\u03b4)-differentially private mechanism such that\n\u2200D \u2208 {\u00b11}n\u00d7d E M [\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223 1 ] \u2264 d 10 .\nThen, by Markov\u2019s inequality,\n\u2200D \u2208 {\u00b11}n\u00d7d P M [\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223 1 > d 9 ] \u2264 9 10 . (2)\nLet k be a parameter to be chosen later. Let nk = bn/kc. Let Mk : {\u00b11}nk\u00d7d \u2192 [\u00b11]d be the following mechanism. On input D\u2217 \u2208 {\u00b11}nk\u00d7d , Mk creates D \u2208 {\u00b11}n\u00d7d by taking k copies of D\u2217 and filling the remaining entries with 1s. Then Mk runs M on D and outputs M(D).\nBy group privacy (Fact 2.2),Mk is a ( \u03b5k = k,\u03b4k = ek\u22121 e\u22121 \u03b4 ) -differentially private mechanism. By the triangle inequality, \u2223\u2223\u2223\u2223\u2223\u2223Mk(D\u2217)\u2212D\u2217\u2223\u2223\u2223\u2223\u2223\u22231 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u22231 + \u2223\u2223\u2223\u2223\u2223\u2223D \u2212D\u2217\u2223\u2223\u2223\u2223\u2223\u22231 . (3) Now\nDj = k \u00b7nk n D\u2217j + n\u2212 k \u00b7nk n 1.\nThus \u2223\u2223\u2223\u2223Dj \u2212D\u2217j \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 ( k \u00b7nk n \u2212 1 ) D\u2217j + n\u2212 k \u00b7nk n \u2223\u2223\u2223\u2223\u2223\u2223 = n\u2212 k \u00b7nkn \u2223\u2223\u2223\u22231\u2212D\u2217j \u2223\u2223\u2223\u2223 \u2264 2n\u2212 k \u00b7nkn . We have\nn\u2212 k \u00b7nk n = n\u2212 kbn/kc n \u2264 n\u2212 k(n/k \u2212 1) n = k n .\nThus \u2223\u2223\u2223\u2223\u2223\u2223D \u2212D\u2217\u2223\u2223\u2223\u2223\u2223\u2223\n1 \u2264 2k/n. Assume k \u2264 n/200. Thus \u2223\u2223\u2223\u2223\u2223\u2223D \u2212D\u2217\u2223\u2223\u2223\u2223\u2223\u2223 1 \u2264 d/100 and, by (2) and (3),\nP Mk [\u2223\u2223\u2223\u2223\u2223\u2223Mk(D\u2217)\u2212D\u2217\u2223\u2223\u2223\u2223\u2223\u22231 > d8 ] \u2264 P M [\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223 1 > d 9 ] \u2264 9 10 . (4)\nAssume d \u2265 dnk ,\u03b4, were dnk ,\u03b4 = O(n 2 k log(1/\u03b4)) is as in Theorem 3.3. We will show by contradiction that this cannot be \u2013 that is d \u2264O(n2k log(1/\u03b4)). LetD \u2217 \u2208 {\u00b11}nk\u00d7d and Trace : [\u00b11]d \u2192 2[nk] be a 1/100-complete \u03b4-sound 1/8-robust L1 fingerprinting code for nk users of length d. By the completeness of the fingerprinting code,\nP [\u2223\u2223\u2223\u2223\u2223\u2223Mk(D\u2217)\u2212D\u2217\u2223\u2223\u2223\u2223\u2223\u22231 \u2264 d8 \u2227Trace(M(D)) = \u2205 ] \u2264 1 100 . (5)\nCombinging (4) and (5), gives\nP [Trace(Mk(D \u2217)) , \u2205] \u2265 9\n100 > 1 12 .\nIn particular, there exists i\u2217 \u2208 [nk] such that\nP [i\u2217 \u2208 Trace(Mk(D\u2217))] > 1\n12nk . (6)\nWe have that Trace(Mk(D\u2217)) is a (\u03b5k ,\u03b4k)-differentially private function of D\u2217, as it is only postprocessing Mk(D\u2217). Thus\nP [i\u2217 \u2208 Trace(Mk(D\u2217))] \u2264 e\u03b5kP [ i\u2217 \u2208 Trace(Mk(D\u2217\u2212i\u2217)) ] + \u03b4k \u2264 e\u03b5k\u03b4+ \u03b4k , (7)\nwhere the second inequality follows from the soundness of the fingerprinting code. Combining (6) and (7) gives\n1 12nk \u2264 e\u03b5k\u03b4+ \u03b4k = ek\u03b4+ ek \u2212 1 e \u2212 1 \u03b4 = ek+1 \u2212 1 e \u2212 1 \u03b4 < ek+1\u03b4. (8)\nIf k \u2264 log(1/12nk\u03b4) \u2212 1, then (8) gives a contradiction. Let k = blog(1/12n\u03b4) \u2212 1c. Assuming \u03b4 \u2265 e\u2212n/200 ensures k \u2264 n/200, as required. Assuming \u03b4 \u2264 1/n1+\u03b3 implies k \u2265 log(1/\u03b4)/(1+1/\u03b3)\u2212 5 \u2265\u2126(log(1/\u03b4)). This setting of k gives a contradiction, which implies that\nd < dnk ,\u03b4 =O(n 2 k log(1/\u03b4)) =O\n( n2\nk2 log(1/\u03b4)\n) =O ( n2\nlog(1/\u03b4)\n) ,\nas required."}, {"heading": "4 New Mechanisms for L\u221e Error", "text": "Adding independent noise seems very natural for one-way marginals, but it is suboptimal if one is interested in worst-case (i.e. L\u221e) error bounds, rather than average-case (i.e. L1) error bounds."}, {"heading": "4.1 Pure Differential Privacy", "text": "Theorem 1.2 follows from Theorem 4.1. In particular, the mechanism M : {\u00b11}n\u00d7d \u2192 [\u00b11]d in Theorem 1.2 is given by M(D) = D + Y , where Y \u223c D and D is the distribution from Theorem 4.1 with \u2206 = 2/n.3\nTheorem 4.1. For all \u03b5 > 0, d \u2265 1, and \u2206 > 0, there exists a continuous distribution D on Rd with the following properties.\n\u2022 Privacy: If x,x\u2032 \u2208Rd with ||x \u2212 x\u2032 ||\u221e \u2264 \u2206, then\nP Y\u223cD [x+Y \u2208 S] \u2264 e\u03b5 P Y\u223cD\n[ x\u2032 +Y \u2208 S ] for all measurable S \u2286Rd .\n3Note that we must truncate the output of M to ensure that M(D) is always in [\u00b11]d .\n\u2022 Accuracy: For all \u03b1 > 0,\nP Y\u223cD [||Y ||\u221e \u2265 \u03b1] \u2264 ( \u2206d \u03b5\u03b1 )d ed\u2212\u03b1\u03b5/\u2206.\nIn particular, if d \u2264 \u03b5\u03b1/2\u2206, then P Y\u223cD [||Y ||\u221e \u2265 \u03b1] \u2264 (2e)\u2212d .\n\u2022 Efficiency: D can be efficiently sampled.\nProof. The distribution D is simply an instantiation of the exponential mechanism [MT07]. In particular, the probability density function is given by\npdfD(y) \u221d exp ( \u2212 \u03b5 \u2206 \u2223\u2223\u2223\u2223\u2223\u2223y\u2223\u2223\u2223\u2223\u2223\u2223\u221e) . Formally, for every measurable S \u2286Rd ,\nP Y\u223cD [Y \u2208 S] =\n\u222b S exp ( \u2212 \u03b5\u2206 \u2223\u2223\u2223\u2223\u2223\u2223y\u2223\u2223\u2223\u2223\u2223\u2223\u221e)dy\u222b Rd exp ( \u2212 \u03b5\u2206\n\u2223\u2223\u2223\u2223\u2223\u2223y\u2223\u2223\u2223\u2223\u2223\u2223\u221e)dy . Firstly, this is clearly a well-defined distribution as long as \u03b5/\u2206 > 0.\nPrivacy is easy to verify: It suffices to bound the ratio of the probability densities for the shifted distributions. For x,x\u2032 \u2208Rd with ||x\u2032 \u2212 x||\u221e \u2264 \u2206, by the triangle inequality,\npdfD(x+ y) pdfD(x\u2032 + y) = exp\n( \u2212 \u03b5\u2206 \u2223\u2223\u2223\u2223\u2223\u2223x+ y\u2223\u2223\u2223\u2223\u2223\u2223\u221e) exp ( \u2212 \u03b5\u2206 \u2223\u2223\u2223\u2223\u2223\u2223x\u2032 + y\u2223\u2223\u2223\u2223\u2223\u2223\u221e) = exp ( \u03b5 \u2206 (\u2223\u2223\u2223\u2223\u2223\u2223x\u2032 + y\u2223\u2223\u2223\u2223\u2223\u2223\u221e \u2212 \u2223\u2223\u2223\u2223\u2223\u2223x+ y\u2223\u2223\u2223\u2223\u2223\u2223\u221e)) \u2264 exp( \u03b5\u2206 \u2223\u2223\u2223\u2223\u2223\u2223x\u2032 \u2212 x\u2223\u2223\u2223\u2223\u2223\u2223\u221e) \u2264 e\u03b5. Define a distribution D\u2217 on [0,\u221e) to by Z \u223c D\u2217 meaning Z = ||Y ||\u221e for Y \u223c D. To prove\naccuracy, we must give a tail bound on D\u2217. The probability density function of D\u2217 is given by\npdfD\u2217(z) \u221d z d\u22121 \u00b7 exp ( \u2212 \u03b5 \u2206 z ) ,\nwhich is obtained by integrating the probability density function of D over the infinity-ball of radius z, which has surface area d2dzd\u22121 \u221d zd\u22121. Thus D\u2217 is precisely the gamma distribution with shape d and mean d\u2206/\u03b5. The moment generating function is therefore\nE Z\u223cD\u2217\n[ etZ ] = ( 1\u2212 \u2206\n\u03b5 t )\u2212d for all t < \u03b5/\u2206. By Markov\u2019s inequality\nP Z\u223cD\u2217\n[Z \u2265 \u03b1] \u2264 E Z\u223cD\u2217\n[ etZ ] et\u03b1 = ( 1\u2212 \u2206 \u03b5 t )\u2212d e\u2212t\u03b1 .\nSetting t = \u03b5/\u2206\u2212 d/\u03b1 gives the required bound. It is easy to verify that Y \u223c D can be sampled by first sampling a radius R from a gamma distribution with shape d + 1 and mean (d + 1)\u2206/\u03b5 and then sampling Y \u2208 [\u00b1R]d uniformly at random. To sample R we can set R = \u2206\u03b5 \u2211d i=0 logUi , where each Ui \u2208 (0,1] is uniform and independent. This gives an algorithm (in the form of an explicit circuit) to sample D that uses onlyO(d) real arithmetic operations, d+1 logarithms, and 2d+1 independent uniform samples from [0,1]."}, {"heading": "4.2 Approximate Differential Privacy", "text": "Our algorithm for approximate differential privacy makes use of a powerful tool from the literature [DNR+09, HR10, DNPR10, RR10] called the sparse vector algorithm:\nTheorem 4.2 (Sparse Vector). For every c,k \u2265 1, \u03b5,\u03b4,\u03b1,\u03b2 > 0, and\nn \u2265O \u221ac log(1/\u03b4) log(k/\u03b2)\u03b1\u03b5  , there exists a mechanism SV with the following properties.\n\u2022 SV takes as input a database D \u2208 X n and provides answers a1, \u00b7 \u00b7 \u00b7 , ak \u2208 [\u00b11] to k (adaptive) linear queries q1, \u00b7 \u00b7 \u00b7 ,qk : X \u2192 [\u00b11].\n\u2022 SV is (\u03b5,\u03b4)-differentially private. \u2022 Assuming \u2223\u2223\u2223\u2223{j \u2208 [k] : |qj(D)| > \u03b1/2}\u2223\u2223\u2223\u2223 \u2264 c, we have\nP SV\n[ \u2200j \u2208 [k] |aj \u2212 qj(D)| \u2264 \u03b1 ] \u2265 1\u2212 \u03b2.\nA proof of this theorem can be found in [DR13, Theorem 3.28].4 We now describe our approximately differentially private mechanism.\n4Note that the algorithms in the literature are designed to sometimes output \u22a5 as an answer or halt prematurely. To modify these algorithms into the form given by Theorem 4.2 simply output 0 in these cases.\nProof of Theorem 1.3. Firstly, we consider the privacy of M: a\u0303 is the output of the Gaussian mechanism with parameters to ensure that it is a (\u03b5/2,\u03b4/2)-differentially private function of D. Likewise a\u0302 is the output of SV with parameters to ensure that it is also a (\u03b5/2,\u03b4/2)-differentially private function of D. Since the output is a\u0303 + 2a\u0302, composition implies that M as a whole is (\u03b5,\u03b4)-differentially private, as required.\nNow we must prove accuracy. Suppose that |a\u0302j \u2212 qj(D)| \u2264 \u03b1SV = \u03b1/2 for all j \u2208 [d]. Then\n|aj \u2212Dj | =|a\u0303j + 2a\u0302j \u2212Dj | =|a\u0303j \u2212Dj + 2(qj(D) + (a\u0302j \u2212 qj(D)))| \u2264|a\u0303j \u2212Dj + 2qj(D)|+ 2|a\u0302j \u2212 qj(D))| \u2264|a\u0303j \u2212Dj + (D \u2212 a\u0303j )|+ 2\u03b1SV =\u03b1,\nas required. So we need only show that |a\u0302j \u2212 qj(D)| \u2264 \u03b1SV for all j \u2208 [d], which sparse vector guarantees will happen with probability at least 1\u2212 \u03b2SV as long as\u2223\u2223\u2223\u2223{j \u2208 [d] : |qj(D)| > \u03b1SV/2}\u2223\u2223\u2223\u2223 \u2264 cSV. (9) Now we verify that (9) holds with high probability.\nBy our setting of parameters, we have qj(D) = \u2212zj /2. This means\nP [ |qj(D)| > \u03b1SV/2 ] = P [ |zj | > \u03b1/2 ] \u2264 e\u2212\u03b1 2/8\u03c32 = 1\nlog8d .\nLet Ej \u2208 {0,1} be the indicator of the event |qj(D)| > \u03b1SV/2. Since the zjs are independent, so are the Ejs. Thus we can apply a Chernoff bound:\nP [\u2223\u2223\u2223\u2223{j \u2208 [d] : |qj(D)| > \u03b1SV/2}\u2223\u2223\u2223\u2223 > cSV] = P \u2211 j\u2208[d] Ej > 2d log8d  \u2264 e\u22122d/ log16 d . (10) The failure probability of M is bounded by the failure probability of SV plus (10), which is dominated by \u03b2SV = exp(\u2212 log4d).\nFinally we consider the sample complexity. The accuracy is bounded by\n\u03b1 \u2264 40\n\u221a d \u00b7 log(1/\u03b4) \u00b7 loglogd\n\u03b5n ,\nwhich rearranges to\nn \u2265 40\n\u221a d \u00b7 log(1/\u03b4) \u00b7 loglogd\n\u03b1\u03b5 .\nTheorem 4.2 requires n \u2265O \u221acSV log(1/\u03b4) log(d/\u03b2SV)\u03b1\u03b5  =O\u221ad log(1/\u03b4)\u03b1\u03b5 \nfor sparse vector to work, which is also satisfied.\nWe remark that we have not attempted to optimize the constant factors in this analysis."}, {"heading": "A Alternative Lower Bound for Pure Differential Privacy", "text": "It is known [HT10] that any \u03b5-differentially private mechanism that answers d one-way marginals requires n \u2265\u2126(d/\u03b5) samples. Our techniques yield an alternative simple proof of this fact.\nTheorem A.1. Let M : {\u00b11}n\u00d7d \u2192 [\u00b11]d be a \u03b5-differentially private mechanism. Suppose\n\u2200D \u2208 {\u00b11}n\u00d7d E M [\u2223\u2223\u2223\u2223\u2223\u2223M(D)\u2212D\u2223\u2223\u2223\u2223\u2223\u2223 1 ] \u2264 0.9d\nThen n \u2265\u2126(d/\u03b5).\nThe proof uses a special case of Hoeffding\u2019s Inequality:\nLemma A.2 (Hoeffding\u2019s Inequality). Let X \u2208 {\u00b11}n be uniformly random and a \u2208Rn fixed. Then\nP X\n[\u3008a,X\u3009 > \u03bb ||a||2] \u2264 e\u2212\u03bb 2/2\nfor all \u03bb \u2265 0.\nProof of Theorem A.1. Let x,x\u2032 \u2208 {\u00b11}d be independent and uniform. Let D \u2208 {\u00b11}n\u00d7d be n copies of x and, likewise, let D \u2032 \u2208 {\u00b11}n\u00d7d be n copies of x\u2032. Let Z = \u3008M(D),x\u3009 and Z \u2032 = \u3008M(D \u2032),x\u3009.\nNow we give conflicting tail bounds for Z and Z \u2032, which we can relate by privacy. By our hypothesis and Markov\u2019s inequality,\nP [Z \u2264 d/20] =P [\u3008M(D),x\u3009 \u2264 0.05d] =P [ \u3008D,x\u3009 \u2212 \u3008D \u2212M(D),x\u3009 \u2264 0.05d ] =P [ \u3008D \u2212M(D),x\u3009 \u2265 0.95d\n] \u2264P [\u2223\u2223\u2223\u2223\u2223\u2223D \u2212M(D)\u2223\u2223\u2223\u2223\u2223\u2223 1 \u2265 0.95d\n] \u2264 E [\u2223\u2223\u2223\u2223\u2223\u2223D \u2212M(D)\u2223\u2223\u2223\u2223\u2223\u2223 1 ] 0.95d \u2264 0.9 0.95 < 0.95.\nSince M(D \u2032) is independent from x, we have\n\u2200\u03bb \u2265 0 P [ Z \u2032 > \u03bb \u221a d ] \u2264 P [ \u3008M(D \u2032),x\u3009 > \u03bb \u2223\u2223\u2223\u2223\u2223\u2223M(D \u2032)\u2223\u2223\u2223\u2223\u2223\u2223 2 ] \u2264 e\u2212\u03bb 2/2,\nby Lemma A.2. In particular, setting \u03bb = \u221a d/20 gives P [Z \u2032 > d/20] \u2264 e\u2212d/800.\nNow D and D \u2032 are databases that differ in n rows, so privacy implies that\nP [M(D) \u2208 S] \u2264 en\u03b5P [ M(D \u2032) \u2208 S ] for all S. Thus\n1 20 < P\n[ Z >\nd 20\n] = P [M(D) \u2208 Sx] \u2264 en\u03b5P [ M(D \u2032) \u2208 Sx ] = en\u03b5P [ Z \u2032 >\nd 20\n] \u2264 en\u03b5e\u2212d/800,\nwhere\nSx = { y \u2208 [\u00b11]d : \u3008y,x\u3009 > d\n20\n} .\nRearranging 1/20 < en\u03b5e\u2212d/800, gives\nn > d\n800\u03b5 \u2212 log(20) \u03b5 ,\nas required."}], "references": [{"title": "Practical privacy: the sulq framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In PODS, pages 128\u2013138", "citeRegEx": "Blum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2005}, {"title": "Collusion-secure fingerprinting for digital data", "author": ["Dan Boneh", "James Shaw"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Boneh and Shaw.,? \\Q1998\\E", "shortCiteRegEx": "Boneh and Shaw.", "year": 1998}, {"title": "Fingerprinting codes and the price of approximate differential privacy", "author": ["Mark Bun", "Jonathan Ullman", "Salil P. Vadhan"], "venue": "In STOC,", "citeRegEx": "Bun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bun et al\\.", "year": 2014}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor"], "venue": "In EUROCRYPT,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Revealing information while preserving privacy", "author": ["Irit Dinur", "Kobbi Nissim"], "venue": "In PODS, pages 202\u2013210", "citeRegEx": "Dinur and Nissim.,? \\Q2003\\E", "shortCiteRegEx": "Dinur and Nissim.", "year": 2003}, {"title": "Privacy-preserving datamining on vertically partitioned databases", "author": ["Cynthia Dwork", "Kobbi Nissim"], "venue": "In CRYPTO,", "citeRegEx": "Dwork and Nissim.,? \\Q2004\\E", "shortCiteRegEx": "Dwork and Nissim.", "year": 2004}, {"title": "Differential privacy under continual observation", "author": ["Cynthia Dwork", "Moni Naor", "Toniann Pitassi", "Guy N. Rothblum"], "venue": "In Proceedings of the Forty-second ACM Symposium on Theory of Computing,", "citeRegEx": "Dwork et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2010}, {"title": "On the complexity of differentially private data release: efficient algorithms and hardness results", "author": ["Cynthia Dwork", "Moni Naor", "Omer Reingold", "Guy N. Rothblum", "Salil P. Vadhan"], "venue": "In STOC,", "citeRegEx": "Dwork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2009}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Dwork and Roth.,? \\Q2013\\E", "shortCiteRegEx": "Dwork and Roth.", "year": 2013}, {"title": "A multiplicative weights mechanism for privacypreserving data analysis", "author": ["Moritz Hardt", "Guy Rothblum"], "venue": "In Proc. 51st Foundations of Computer Science (FOCS),", "citeRegEx": "Hardt and Rothblum.,? \\Q2010\\E", "shortCiteRegEx": "Hardt and Rothblum.", "year": 2010}, {"title": "On the geometry of differential privacy", "author": ["Moritz Hardt", "Kunal Talwar"], "venue": "In Proceedings of the Forty-second ACM Symposium on Theory of Computing,", "citeRegEx": "Hardt and Talwar.,? \\Q2010\\E", "shortCiteRegEx": "Hardt and Talwar.", "year": 2010}, {"title": "Preventing false discovery in interactive data analysis is hard", "author": ["Moritz Hardt", "Jonathan Ullman"], "venue": "In FOCS. IEEE, October", "citeRegEx": "Hardt and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Ullman.", "year": 2014}, {"title": "On the \u201csemantics\u201d of differential privacy: A bayesian formulation", "author": ["Shiva Prasad Kasiviswanathan", "Adam Smith"], "venue": "CoRR, abs/0803.3946,", "citeRegEx": "Kasiviswanathan and Smith.,? \\Q2008\\E", "shortCiteRegEx": "Kasiviswanathan and Smith.", "year": 2008}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "McSherry and Talwar.,? \\Q2007\\E", "shortCiteRegEx": "McSherry and Talwar.", "year": 2007}, {"title": "Interactive privacy via the median mechanism", "author": ["Aaron Roth", "Tim Roughgarden"], "venue": "In Proc. 42nd Symposium on Theory of Computing (STOC),", "citeRegEx": "Roth and Roughgarden.,? \\Q2010\\E", "shortCiteRegEx": "Roth and Roughgarden.", "year": 2010}, {"title": "Interactive fingerprinting codes and the hardness of preventing false discovery", "author": ["Thomas Steinke", "Jonathan Ullman"], "venue": "CoRR, abs/1410.1228,", "citeRegEx": "Steinke and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Steinke and Ullman.", "year": 2014}, {"title": "Optimal probabilistic fingerprint codes", "author": ["G\u00e1bor Tardos"], "venue": "J. ACM,", "citeRegEx": "Tardos.,? \\Q2008\\E", "shortCiteRegEx": "Tardos.", "year": 2008}, {"title": "Answering n2+o(1) counting queries with differential privacy is hard", "author": ["Jonathan Ullman"], "venue": "In STOC, pages 361\u2013370", "citeRegEx": "Ullman.,? \\Q2013\\E", "shortCiteRegEx": "Ullman.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "We show a new lower bound on the sample complexity of (\u03b5,\u03b4)-differentially private algorithms that accurately answer statistical queries on high-dimensional databases. The novelty of our bound is that it depends optimally on the parameter \u03b4, which loosely corresponds to the probability that the algorithm fails to be private, and is the first to smoothly interpolate between approximate differential privacy (\u03b4 > 0) and pure differential privacy (\u03b4 = 0). Specifically, we consider a database D \u2208 {\u00b11}n\u00d7d and its one-way marginals, which are the d queries of the form \u201cWhat fraction of individual records have the i-th bit set to +1?\u201d We show that in order to answer all of these queries to within error \u00b1\u03b1 (on average) while satisfying (\u03b5,\u03b4)-differential privacy, it is necessary that n \u2265\u03a9 \uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u221ad log(1/\u03b4) \u03b1\u03b5 \uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 , which is optimal up to constant factors. To prove our lower bound, we build on the connection between fingerprinting codes and lower bounds in differential privacy (Bun, Ullman, and Vadhan, STOC\u201914). In addition to our lower bound, we give new purely and approximately differentially private algorithms for answering arbitrary statistical queries that improve on the sample complexity of the standard Laplace and Gaussian mechanisms for achieving worst-case accuracy guarantees by a logarithmic factor. \u2217Harvard University School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616. Email: tsteinke@seas.harvard.edu. \u2020Columbia University Department of Computer Science. Supported by a Junior Fellowship from the Simons Society of Fellows. Email: jullman@cs.columbia.edu. ar X iv :1 50 1. 06 09 5v 1 [ cs .D S] 2 4 Ja n 20 15", "creator": "LaTeX with hyperref package"}}}