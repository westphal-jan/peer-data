{"id": "1501.07008", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2015", "title": "A Distance-Based Decision in the Credal Level", "abstract": "swordfighting Belief kamikazes function opting theory pratichetti provides superstrate a flexible way carmelita to hagiographical combine information ketchmark provided nolvadex by alar different nissay sources. pepple This sandspit combination is usually 4.26 followed westby by karnavas a kfor decision dra making minehead which can be handled bertelsen by a range of decision neureuther rules. Some rules coerces help to choose whittingstall the centre-forward most likely hypothesis. Others allow danja that selfridge a pilchard decision ogrodzieniec is digitech made pales on a set facchini of hypotheses. kasrils In [sidebar 6 ], 72-arrow we vapid proposed a imtech decision rule based on atlanteans a vernetti distance webometrics measure. gefs First, grivko in backround this volchkova paper, we vogue aim murrumbidgee to demonstrate that our proposed beran decision rule is a particular chri case dyle of the rule 69.18 proposed in [4 ]. badian Second, we e.e. give j.k experiments showing that our colm rule paradoxically is jettenbach able to holotype decide on a 121.39 set vente of side-chain hypotheses. Some checker experiments are atrophied handled on a 51.25 set of earshot mass functions minivans generated bray@globe.com randomly, twits others azeredo on theatines real realized databases.", "histories": [["v1", "Wed, 28 Jan 2015 07:24:12 GMT  (19kb)", "http://arxiv.org/abs/1501.07008v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["amira essaid", "arnaud martin", "gr\\'egory smits", "boutheina ben yaghlane"], "accepted": false, "id": "1501.07008"}, "pdf": {"name": "1501.07008.pdf", "metadata": {"source": "CRF", "title": "A distance-based decision in the credal level", "authors": ["Amira Essaid", "Arnaud Martin", "Gr\u00e9gory Smits", "Boutheina Ben Yaghlane"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n07 00\n8v 1\n[ cs\n.A I]\n2 8\nJa n\n20 15\nKeywords: belief function theory, imprecise decision, distance"}, {"heading": "1 Introduction", "text": "Belief function theory [2,9] allows us to represent all kinds of ignorance and offers rules for combining several imperfect information provided by different sources in order to get a more coherent one. The combination process helps to make decisions later. Decision making consists in selecting, for a given problem, the most suitable actions to take. Today, we are often confronted with the challenge of making decisions in cases where information is imprecise or even not available. In [12], Smets proposed the transferable belief model (TBM) as an interpretation of the theory of belief functions. The TBM emphasizes a distinction between knowledge modeling and decision making. Accordingly, we distinguish the credal level and the pignistic level. In the credal level, knowledge is represented as belief functions and then combined. The pignistic level corresponds to decision making, a stage in which belief functions are transformed into probability functions.\nThe pignistic probability, the maximum of credibility and the maximum of plausibility are rules that allow a decision on a singleton of the frame of discernment. Sometimes and depending on application domains, it seems to be more convenient to decide on composite hypotheses rather than a simple one. In the literature, there are few works that propose a rule or an approach for making decision on a union of hypotheses [4,1,8]. Recently, we proposed a decision rule based on a distance measure [6]. This rule calculates the distance between a combined mass function and a categorical one. The most likely hypothesis to\nchoose is the hypothesis whose categorical mass function is the nearest to the combined one.\nThe main topic of this paper is to demonstrate that our proposed decision rule is a particular case of that detailed in [4] and to extend our rule so that it becomes able to give decisions even with no categorical mass functions. We present also our experiments on mass functions generated randomly as well as on real databases.\nThe remainder of this paper is organized as follows: in section 2 we recall the basic concepts of belief function theory. Section 3 presents our decision rule based on a distance measure proposed in [6]. In section 4, we demonstrate that our proposed rule is a particular case of that proposed in [4]. Section 5 presents experiments and the main results. Section 6 concludes the paper."}, {"heading": "2 The theory of belief functions", "text": "The theory of belief functions [2,9] is a general mathematical framework for representing beliefs and reasoning under uncertainty. In this section, we recall some concepts of this theory.\nThe frame of discernment \u0398 = {\u03b81, \u03b82, . . . , \u03b8n} is a set of n elementary hypotheses related to a given problem. These hypotheses are exhaustive and mutually exclusive. The power set of \u0398, denoted by 2\u0398 is the set containing singleton hypotheses of \u0398, all the disjunctions of these hypotheses as well as the empty set.\nThe Basic belief assignment (bba), denoted by m is a mass function defined on 2\u0398. It affects a value from [0, 1] to each subset. It is defined as:\n\u2211\nA\u22862\u0398\nm(A) = 1. (1)\nA focal element A is an element of 2\u0398 such that m(A) > 0. A categorical bba is a bba with a unique focal element such that m(A) = 1. When this focal element is a disjunction of hypotheses then the bba models imprecision.\nBased on the basic belief assignment, other belief functions (credibility function ad plausibility function) can be deduced.\n\u2013 Credibility function bel(A) expresses the total belief that one allocates to A. It is a mapping from elements of 2\u0398 to [0, 1] such that:\nbel(A) = \u2211\nB\u2286A,B 6=\u2205\nm(B). (2)\n\u2013 Plausibility function pl(A) is defined as:\npl(A) = \u2211\nA\u2229B 6=\u2205\nm(B). (3)\nThe plausibility function measures the maximum amount of belief that supports the proposition A by taking into account all the elements that do not contradict. The value pl(A) quantifies the maximum amount of belief that might support a subset A of \u0398.\nThe theory of belief function is a useful tool for data fusion. In fact, for a given problem and for the same frame of discernment, it is possible to get a mass function synthesizing knowledge from separate and independent sources of information through applying a combination rule. Mainly, there exists three modes of combination:\n\u2013 Conjunctive combination is used when two sources are distinct and fully reliable. In [10], the author proposed the conjunctive combination rule which is defined as:\nm1 \u2229\u00a92(A) = \u2211\nB\u2229C=A\nm1(B) \u00d7m2(C). (4)\nThe Dempster\u2019s rule of combination [2] is a normalized form of the rule described previously and is defined as:\nm1\u22952(A) =\n\n   \n   \n\u2211\nB\u2229C=A\nm1(B)\u00d7m2(C)\n1\u2212\n\u2211\nB\u2229C=\u2205\nm1(B)\u00d7m2(C) \u2200A \u2286 \u0398, A 6= \u2205\n0 if A = \u2205\n(5)\nThis rule is normalized through 1\u2212 \u2211\nB\u2229C=\u2205\nm1(B)\u00d7m2(C) and it works under\nthe closed world assumption where all the possible hypotheses of the studied problem are supposed to be enumerated on \u0398.\n\u2013 Disjunctive combination: In [11], Smets introduced the disjunctive combination rule which combines mass functions when an unknown source is unreliable. This rule is defined as:\nm1 \u222a\u00a92(A) = \u2211\nB\u222aC=A\nm1(B)\u00d7m2(C) (6)\n\u2013 Mixed combination: In [5], the authors proposed a compromise in order to consider the benefits of the two combination modes previously described. This combination is given for every A \u2208 2\u0398 by the following formula:\n\n\n\nmDP (A) = m1 \u2229\u00a9(A) + \u2211\nB\u2229C=\u2205,B\u222aC=A\nm1(B)m2(C) \u2200A \u2208 2\u0398, A 6= \u2205\nmDP (\u2205) = 0 (7)"}, {"heading": "3 Decision Making in the theory of belief functions", "text": "In the transferable belief model, decision is made on the pignistic level where the belief functions are transformed into a probability function, named pignistic probability. This latter, noted as BetP is defined for each X \u2208 2\u0398, X 6= 0 as:\nbetP (X) = \u2211\nY \u22082\u0398,Y 6=\u2205\n|X \u2229 Y |\n|Y |\nm(Y )\n1\u2212m(\u2205) (8)\nwhere |Y | represents the cardinality of Y .\nBased on the obtained pignistic probability, we select the most suitable hypothesis with the maximum BetP. This decision results from applying tools of decision theory [4]. In fact, if we consider an entity represented by a feature vector x. A is a finite set of possible actions A = {a1, . . . , aN} and \u0398 a finite set of hypotheses, \u0398 = {\u03b81, . . . , \u03b8M}. An action aj corresponds to the action of choosing the hypothesis \u03b8j . But, if we select ai as an action whereas the hypothesis to be considered is rather \u03b8j then the loss occurred is \u03bb(ai|\u03b8j). The expected loss associated with the choice of the action ai is defined as:\nRbetP (ai|x) = \u2211\n\u03b8j\u2208\u0398\n\u03bb(ai|\u03b8j)BetP (\u03b8j). (9)\nThen, the decision consists in selecting the action which minimizes the expected loss. In addition to minimizing pignistic expected loss, other risks are presented in [4].\nDecision can be made on composite hypotheses [1,8]. We present in this paper the Appriou\u2019s rule [1] which helps to choose a solution of a given problem by considering all the elements contained in 2\u0398. This approach weights the decision functions (maximum of credibility, maximum of plausibility and maximum of pignistic probability) by an utility function depending on the cardinality of the elements. A \u2208 2\u0398 is chosen if:\nA = argmax X\u22082\u0398 (md(X)pl(X)) (10)\nwhere md is a mass defined by:\nmd(X) = Kd\u03bbX\n(\n1\n|X |r\n)\n(11)\nThe value r is a parameter in [0, 1] helping to choose a decision which varies from a total indecision when r is equal to 0 and a decision based on a singleton when r is equal 1. \u03bbX helps to integrate the lack of knowledge about one of the elements of 2\u0398. Kd is a normalization factor and pl(X) is a plausibility function.\nIn the following, we present our decision rule based on a distance measure."}, {"heading": "4 Decision rule based on a distance measure", "text": "In [6], we proposed a decision rule based on a distance measure. It is defined as:\nA = argmin(d(mcomb,mA)) (12)\nThis rule aims at deciding on a union of singletons. It is based on the use of categorical bba which helps to adjust the degree of imprecision that has to be kept when deciding. Depending on cases, we can decide on unions of two elements or three elements, etc. The rule calculates the distance between a combined bba mcomb and a categorical onemA . The minimum distance is kept and the decision corresponds to the categorical bba\u2019s element having the lowest distance with the combined bba. The rule is applied as follows:\n\u2013 We consider the elements of 2\u0398. In some applications, 2\u0398 can be of a large cardinality. For this reason, we may choose some elements to work on. For example, we can keep the elements of 2\u0398 whose cardinality is less or equal to 2. \u2013 For each selected element, we construct its corresponding categorical bba. \u2013 Finally, we apply Jousselme distance [7] to calculate the distance between\nthe combined bba and a categorical bba. The distance with the minimum value is kept. The most likely hypothesis to select is the hypothesis whose categorical bba is the nearest to the combined bba.\nJousselme distance is defined for two bbas m1 and m2 as follows:\nd(m1,m2) =\n\u221a\n1 2 (m1 \u2212m2)tD(m1 \u2212m2) (13)\nwhere D is a matrix based on Jaccard distance as a similarity measure between focal elements. This matrix is defined as:\nD(A,B) =\n{\n1 if A=B=\u2205 |A\u2229B| |A\u222aB| \u2200A,B \u2208 2 \u0398 (14)\nIn this paper, we propose to apply the rule through two different manners:\n\u2013 Distance type 1 is calculated with categorical bbas (m(A) = 1) for all elements of 2\u0398 except \u0398 to have an imprecise result rather than a total ignorance. \u2013 Distance type 2 is calculated with simple bbas such as m(A) = \u03b1, m(\u0398) = 1\u2212 \u03b1.\nIn the following, we show that our proposed rule can be seen as a particular case of that proposed in section 3.\nJousselme distance can be written as:\nd(m1,m2) = 1\n2\n\u2211\nY\u2286\u0398\n\u2211\nX\u2286\u0398\n|X \u2229 Y | |X \u222a Y | m(X)m(Y ) (15)\nIf we consider the expected loss of choosing ai, then it can be written as:\nRbetP (ai|x) = \u2211\nY \u2208\u0398\n\u03bb(ai|Y )BetP (Y ).\nRbetP (ai|x) = \u2211\nY \u2208\u0398\n\u03bb(ai|Y ) \u2211\nX\u2208\u0398\n|X \u2229 Y |\n|X |\nm(X)\n1\u2212m(\u2205) .\nRbetP (ai|x) = \u2211\nY \u2208\u0398\n\u2211\nX\u2208\u0398\n\u03bb(ai|Y ) |X \u2229 Y |\n|X |\nm(X)\n1\u2212m(\u2205) .\n(16)\nThe equation relative to decision is equal to that for the risk for a value of \u03bb that has to be equal to:\n\u03bb(ai|Y ) = |X |(1\u2212m(\u2205))\n|X \u222a Y | m(X) (17)\nIn this section, we showed that for a particular value of \u03bb, our proposed decision rule can be considered as a particular case of that proposed in [4]. In the following section, we give experiments and present comparisons between our decision rule based on a distance measure and that presented in [1]."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experiments on generated mass functions", "text": "We tested the proposed rule [6] on a set of mass functions generated randomly. To generate the bbas, one needs to specify the cardinality of the frame of discernment, the number of mass functions to be generated as well as the number of focal elements. The generated bbas are then combined. We use the Dempster\u2019s rule of combination, the disjunctive rule and the mixed rule. Suppose we have a frame of discernment represented as \u0398 = {\u03b81, \u03b82, \u03b83} and three different sources for which we generate their corresponding bbas as given in Table 1.\nOnce the combination is performed, we can make decision. In Table 3, we compare between the results of three decision rules, namely the pignistic probability, the Appriou\u2019s rule with r equal to 0.5 as well as our proposed decision rule based on distance measure.\nTable 3 shows the decision results obtained after applying some combination rules. We depict from this table that not all the time the rule proposed by Appriou gives a decision on a composite hypotheses. In fact, as shown in Table 3, the application of disjunctive rule as well as the mixed rule lead to a decision on a singleton which is \u03b81. This is completely different from what we obtain when we apply our proposed rule which promotes a decision on union of singletons when combining bbas. The obtained results seems to be convenient especially that the disjunctive and the mixed rules help to get results on unions of singletons."}, {"heading": "5.2 Experiments on real databases", "text": "To test our proposed decision rule, we do some experiments on real databases (IRIS1 and HaberMan\u2019s survival2). Iris is a dataset contaning 150 instances, 4 attributes and 3 classes where each class refers to a type of iris plant. HaberMan is a dataset containing results study conducted at the University of Chicago\u2019s Billings Hospital on the survival of patients who had undergone surgery for breast cancer. This dataset contains 306 instances, 3 attributes and 2 classes\n1 http://archive.ics.uci.edu/ml/datasets/Iris 2 http://archive.ics.uci.edu/ml/datasets/Haberman%27s+Survival\n(1: patient survived 5 years or longer, 2: patient died within 5 years). For the classification, our experiments are handled in two different manners.\n\u2013 First, we apply the k-NN classifier [3]. The results are illustrated in a confusion matrix as shown in Table 4 (left side). \u2013 Second, we modify the k-NN classifier\u2019s algorithm based on the use of Dempster rule of combination, to make it able to combine belief functions through the mixed rule. Then, Appriou\u2019s rule and our proposed decision rule are applied to make decision. Results are illustrated in Table 4.\nThe same tests are done for HaberMan\u2019s survival dataset. The results of applying k-NN classifier, Appriou\u2019s rule and our decision rule are given respectively in Table 5. For the classification of 40 sets chosen randomly from Iris, we remark that with the k-NN classifier, all the sets having \u03b81 and \u03b83 as corresponding classes are well classified and only two originally belonging to class \u03b82 were classified as \u03b83. Appriou\u2019s rule gives a good classification for sets originally belonging to classes \u03b81 and \u03b82 and thus promoting a result on singletons rather than on a union of singletons.\nConsidering the results obtained when applying our decision rule based on a distance type 1, we note that only 2 sets are not well classified and that 3 have \u03b82 \u222a \u03b83 as a class. The obtained results are good because our method is based on an imprecise decision which is underlined by the fact of obtaining \u03b82 \u222a \u03b83 as a class.\nConsidering HaberMan\u2019s survival dataset, we note that the k-NN classifier, Appriou\u2019s rule as well as our decision rule give the same results where among the sets originally belonging to \u03b81, 34 are well classified and among the 18 belonging to \u03b82, only 6 are well classified. We obtain the same results as the other rules\nbecause the HaberMan\u2019s survival dataset has only two classes and our method is based on getting imprecise decisions and excluding the ignorance.\nAll the experiments given previously are based on the use of distance type 1. The results shown below are based on distance type 2. In fact, we consider a simple bba and each time, we assign a value \u03b1 to an element of 2\u0398. The tested rule on Iris as illustrated in Table 6 (left side) gives better results with an \u03b1 < 0.8. In addition to that, we obtained decisions on a union of singletons. The tests done on HaberMan\u2019s survival as given in Table 6 (right side) shows that with \u03b1 > 0.5, we obtain a better rate of good classification although we did not obtain a good classification for the class \u03b82 and no set belongs to \u0398. We aim in the future to make experiments on other datasets because HaberMan\u2019s survival, for example, does only have 2 classes, so we do not have enough imprecise elements."}, {"heading": "6 Conclusion", "text": "In this paper, we presented a rule based on a distance measure. This decision rule helps to choose the most likely hypothesis based on the calculation of the distance between a combined bba and a categorical bba. The aim of the proposed decision rule is to give results on composite hypotheses. In this paper, we demonstrated that our proposed rule can be seen as a particular case of that proposed in [4]. We presented also the different experiments handled on generated mass functions as well as on real databases."}], "references": [{"title": "Approche g\u00e9n\u00e9rique de la gestion de l\u2019incertain dans les processus de fusion multisenseur", "author": ["A. Appriou"], "venue": "Traitement du Signal", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Upper and Lower probabilities induced by a multivalued mapping", "author": ["A.P. Dempster"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1967}, {"title": "Representation and combination of uncertainty with belief functions and possibility measures", "author": ["D. Dubois", "H. Prade"], "venue": "Computational Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Uncertainty in ontology matching: a decision rule based approach", "author": ["A. Essaid", "A. Martin", "G. Smits", "B. Ben Yaghlane"], "venue": "In proceeding of the International Conference on Information Processing and Mangement Uncertainty,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A New Distance Between Two Bodies of Evidence", "author": ["A.L. Jousselme", "D. Grenier", "E. Boss\u00e9"], "venue": "Information Fusion,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Decision support with belief functions theory for seabed characterization", "author": ["A. Martin", "I. Quidu"], "venue": "In proceeding of the International Conference on Information Fusion,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1976}, {"title": "The Combination of Evidence in the Transferable Belief Model", "author": ["P. Smets"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Belief functions: The disjunctive rule of combination and the generalized Bayesian theorem", "author": ["P. Smets"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "The Transferable Belief Model", "author": ["P. Smets", "R. Kennes"], "venue": "Artificial Intelligent", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "On the Dempster-Shafer framework and new combination rules", "author": ["R.R. Yager"], "venue": "Information Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1987}], "referenceMentions": [{"referenceID": 3, "context": "In [6], we proposed a decision rule based on a distance measure.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "1 Introduction Belief function theory [2,9] allows us to represent all kinds of ignorance and offers rules for combining several imperfect information provided by different sources in order to get a more coherent one.", "startOffset": 38, "endOffset": 43}, {"referenceID": 6, "context": "1 Introduction Belief function theory [2,9] allows us to represent all kinds of ignorance and offers rules for combining several imperfect information provided by different sources in order to get a more coherent one.", "startOffset": 38, "endOffset": 43}, {"referenceID": 9, "context": "In [12], Smets proposed the transferable belief model (TBM) as an interpretation of the theory of belief functions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In the literature, there are few works that propose a rule or an approach for making decision on a union of hypotheses [4,1,8].", "startOffset": 119, "endOffset": 126}, {"referenceID": 5, "context": "In the literature, there are few works that propose a rule or an approach for making decision on a union of hypotheses [4,1,8].", "startOffset": 119, "endOffset": 126}, {"referenceID": 3, "context": "Recently, we proposed a decision rule based on a distance measure [6].", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "Section 3 presents our decision rule based on a distance measure proposed in [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "2 The theory of belief functions The theory of belief functions [2,9] is a general mathematical framework for representing beliefs and reasoning under uncertainty.", "startOffset": 64, "endOffset": 69}, {"referenceID": 6, "context": "2 The theory of belief functions The theory of belief functions [2,9] is a general mathematical framework for representing beliefs and reasoning under uncertainty.", "startOffset": 64, "endOffset": 69}, {"referenceID": 0, "context": "It affects a value from [0, 1] to each subset.", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "It is a mapping from elements of 2 to [0, 1] such that: bel(A) = \u2211", "startOffset": 38, "endOffset": 44}, {"referenceID": 7, "context": "In [10], the author proposed the conjunctive combination rule which is defined as: m1 \u2229 \u00a92(A) = \u2211", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "The Dempster\u2019s rule of combination [2] is a normalized form of the rule described previously and is defined as:", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "\u2013 Disjunctive combination: In [11], Smets introduced the disjunctive combination rule which combines mass functions when an unknown source is unreliable.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "\u2013 Mixed combination: In [5], the authors proposed a compromise in order to consider the benefits of the two combination modes previously described.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Decision can be made on composite hypotheses [1,8].", "startOffset": 45, "endOffset": 50}, {"referenceID": 5, "context": "Decision can be made on composite hypotheses [1,8].", "startOffset": 45, "endOffset": 50}, {"referenceID": 0, "context": "We present in this paper the Appriou\u2019s rule [1] which helps to choose a solution of a given problem by considering all the elements contained in 2.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "The value r is a parameter in [0, 1] helping to choose a decision which varies from a total indecision when r is equal to 0 and a decision based on a singleton when r is equal 1.", "startOffset": 30, "endOffset": 36}, {"referenceID": 3, "context": "4 Decision rule based on a distance measure In [6], we proposed a decision rule based on a distance measure.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "\u2013 Finally, we apply Jousselme distance [7] to calculate the distance between the combined bba and a categorical bba.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "In the following section, we give experiments and present comparisons between our decision rule based on a distance measure and that presented in [1].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "1 Experiments on generated mass functions We tested the proposed rule [6] on a set of mass functions generated randomly.", "startOffset": 70, "endOffset": 73}], "year": 2015, "abstractText": "Belief function theory provides a flexible way to combine information provided by different sources. This combination is usually followed by a decision making which can be handled by a range of decision rules. Some rules help to choose the most likely hypothesis. Others allow that a decision is made on a set of hypotheses. In [6], we proposed a decision rule based on a distance measure. First, in this paper, we aim to demonstrate that our proposed decision rule is a particular case of the rule proposed in [4]. Second, we give experiments showing that our rule is able to decide on a set of hypotheses. Some experiments are handled on a set of mass functions generated randomly, others on real databases.", "creator": "LaTeX with hyperref package"}}}