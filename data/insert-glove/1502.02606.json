{"id": "1502.02606", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "The Power of Randomization: Distributed Submodular Maximization on Massive Datasets", "abstract": "jovanovich A wide philanthropist variety ailun of aleatoric problems schutzpolizei in machine learning, including postmortems exemplar 100-87 clustering, cybercast document summarization, and monogram sensor placement, raubenheimer can be cast jonesy as serologic constrained submodular historias maximization lauries problems. Unfortunately, shkirko the resulting kinchen submodular sf4 optimization problems anti-globalisation are often too large to split-off be solved on sitiawan a single muffling machine. camera We naenae develop ccecc a simple shingles distributed midmarket algorithm that straumann is embarrassingly rapamycin parallel 16:05 and greyling it 34-run achieves chignons provable, sonor constant factor, arginine worst - case approximation masinde guarantees. cyclingnews.com In our experiments, mott we davlin demonstrate 106.64 its iconographer efficiency in unconscionably large pletka problems with different n\u00f3i kinds of xxx constraints with objective values chatenay always ahwatukee close to searl what ariadna is vahsel achievable geopolymer in void the centralized setting.", "histories": [["v1", "Mon, 9 Feb 2015 19:04:43 GMT  (1448kb,D)", "https://arxiv.org/abs/1502.02606v1", null], ["v2", "Wed, 22 Apr 2015 17:49:22 GMT  (1448kb,D)", "http://arxiv.org/abs/1502.02606v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DC", "authors": ["rafael da ponte barbosa", "alina ene", "huy l nguyen", "justin ward"], "accepted": true, "id": "1502.02606"}, "pdf": {"name": "1502.02606.pdf", "metadata": {"source": "CRF", "title": "The Power of Randomization Distributed Submodular Maximization on Massive Datasets\u2217", "authors": ["Rafael da Ponte Barbosa", "Alina Ene", "Huy L. Nguy\u1ec5n", "Justin Ward"], "emails": ["J.D.Ward}@dcs.warwick.ac.uk", "hlnguyen@cs.princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "A set function f : 2V \u2192 R\u22650 on a ground set V is submodular if f(A)+f(B) \u2265 f(A\u2229B)+f(A\u222aB) for any two sets A,B \u2286 V . Several problems of interest can be modeled as maximizing a submodular objective function subject to certain constraints:\nmax f(A) subject to A \u2208 C,\nwhere C \u2286 2V is the family of feasible solutions. Indeed, the general meta-problem of optimizing a constrained submodular function captures a wide variety of problems in machine learning applications, including exemplar clustering, document summarization, sensor placement, image segmentation, maximum entropy sampling, and feature selection problems. \u2217The authors are listed alphabetically. \u2020Work supported by EPSRC grant EP/J021814/1.\nar X\niv :1\n50 2.\n02 60\n6v 2\n[ cs\nAt the same time, in many of these applications, the amount of data that is collected is quite large and it is growing at a very fast pace. For example, the wide deployment of sensors has led to the collection of large amounts of measurements of the physical world. Similarly, medical data and human activity data are being captured and stored at an ever increasing rate and level of detail. This data is often high-dimensional and complex, and it needs to be stored and processed in a distributed fashion.\nIn these settings, it is apparent that the classical algorithmic approaches are no longer suitable and new algorithmic insights are needed in order to cope with these challenges. The algorithmic challenges stem from the following competing demands imposed by huge datasets: the computations need to process the data that is distributed across several machines using a minimal amount of communication and synchronization across the machines, and at the same time deliver solutions that are competitive with the centralized solution on the entire dataset.\nThe main question driving the current work is whether these competing goals can be reconciled. More precisely, can we deliver very good approximate solutions with minimal communication overhead? Perhaps surprisingly, the answer is yes; there is a very simple distributed greedy algorithm that is embarrassingly parallel and it achieves provable, constant factor, worst-case approximation guarantees. Our algorithm can be easily implemented in a parallel model of computation such as MapReduce [2]."}, {"heading": "1.1 Background and Related Work", "text": "In the MapReduce model, there are m independent machines. Each of the machines has a limited amount of memory available. In our setting, we assume that the data is much larger than any single machine\u2019s memory and so must be distributed across all of the machines. At a high level, a MapReduce computation proceeds in several rounds. In a given round, the data is shuffled among the machines. After the data is distributed, each of the machines performs some computation on the data that is available to it. The output of these computations is either returned as the final result or becomes the input to the next MapReduce round. We emphasize that the machines can only communicate and exchange data during the shuffle phase.\nIn order to put our contributions in context, we briefly discuss two distributed greedy algorithms that achieve complementary trade-offs in terms of approximation guarantees and communication overhead.\nMirzasoleiman et al. [10] give a distributed algorithm, called GreeDi, for maximizing a monotone submodular function subject to a cardinality constraint. The GreeDi algorithm partitions the data arbitrarily on the machines and on each machine it runs the classical Greedy algorithm to select a feasible subset of the items on that machine. The Greedy solutions on these machines are then placed on a single machine and the Greedy algorithm is used once more to select the final solution. The GreeDi algorithm is very simple and embarrassingly parallel, but its worst-case approximation guarantee1 is 1/\u0398 ( min {\u221a k,m }) , where m is the number of machines and k is\n1Mirzasoleiman et al. [10] give a family of instances where the approximation achieved is only 1/min {k,m} if the solution picked on each of the machines is the optimal solution for the set of items on the machine. These instances are not hard for the GreeDi algorithm. We show in Sections A and B that the GreeDi algorithm achieves an 1/\u0398 ( min {\u221a k,m }) approximation.\nthe cardinality constraint. Despite this, Mirzasoleiman et al. show that the GreeDi algorithm achieves very good approximations for datasets with geometric structure.\nKumar et al. [8] give distributed algorithms for maximizing a monotone submodular function subject to a cardinality or more generally, a matroid constraint. Their algorithm combines the Threshold Greedy algorithm of [4] with a sample and prune strategy. In each round, the algorithm samples a small subset of the elements that fit on a single machine and runs the Threshold Greedy algorithm on the sample in order to obtain a feasible solution. This solution is then used to prune some of the elements in the dataset and reduce the size of the ground set. The Sample&Prune algorithms achieve constant factor approximation guarantees but they incur a higher communication overhead. For a cardinality constraint, the number of rounds is a constant but for more general constraints such as a matroid constraint, the number of rounds is \u0398(log \u2206), where \u2206 is the maximum increase in the objective due to a single element. The maximum increase \u2206 can be much larger than even the number of elements in the entire dataset, which makes the approach infeasible for massive datasets.\nOn the negative side, Indyk et al. [5] studied coreset approaches to develop distributed algorithms for finding representative and yet diverse subsets in large collections. While succeeding in several measures, they also showed that their approach provably does not work for k-coverage, which is a special case of submodular maximization with a cardinality constraint."}, {"heading": "1.2 Our Contribution", "text": "In this paper, we show that we can achieve both the communication efficiency of the GreeDi algorithm and a provable, constant factor, approximation guarantee. Our algorithm is in fact the GreeDi algorithm with a very simple and crucial modification: instead of partitioning the data arbitrarily on the machines, we randomly partition the dataset. Our analysis may perhaps provide some theoretical justification for the very good empirical performance of the GreeDi algorithm that was established previously in the extensive experiments of [10]. It also suggests the approach can deliver good performance in much wider settings than originally envisioned.\nThe GreeDi algorithm was originally studied in the special case of monotone submodular maximization under a cardinality constraint. In contrast, our analysis holds for any hereditary constraint. Specifically, we show that our randomized variant of the GreeDi algorithm achieves a constant factor approximation for any hereditary, constrained problem for which the classical (centralized) Greedy algorithm achieves a constant factor approximation. This is the case not only for cardinality constraints, but also for matroid constraints, knapsack constraints, and p-system constraints [6], which generalize the intersection of p matroid constraints. Table 1 gives the approximation ratio \u03b1 obtained by the greedy algorithm on a variety of problems, and the corresponding constant factor obtained by our randomized GreeDi algorithm.\nAdditionally, we show that if the greedy algorithm satisfies a slightly stronger technical condition, then our approach gives a constant factor approximation for constrained non-monotone submodular maximization. This is indeed the case for all of the aforementioned specific classes of problems. The resulting approximation ratios for non-monotone maximization problems are given in the last\ncolumn of Table 1."}, {"heading": "1.3 Preliminaries", "text": "MapReduce Model. In a MapReduce computation, the data is represented as \u3008key, value\u3009 pairs and it is distributed across m machines. The computation proceeds in rounds. In a given, the data is processed in parallel on each of the machines by map tasks that output \u3008key, value\u3009 pairs. These pairs are then shuffled by reduce tasks; each reduce task processes all the \u3008key, value\u3009 pairs with a given key. The output of the reduce tasks either becomes the final output of the MapReduce computation or it serves as the input of the next MapReduce round.\nSubmodularity. As noted in the introduction, a set function f : 2V \u2192 R\u22650 is submodular if, for all sets A,B \u2286 V ,\nf(A) + f(B) \u2265 f(A \u222aB) + f(A \u2229B).\nA useful alternative characterization of submodularity can be formulated in terms of diminishing marginal gains. Specifically, f is submodular if and only if:\nf(A \u222a {e})\u2212 f(A) \u2265 f(B \u222a {e})\u2212 f(B)\nfor all A \u2286 B \u2286 V and e /\u2208 B.\nThe Lov\u00e1sz extension f\u2212 : [0, 1]V \u2192 R\u22650 of a submodular function f is given by:\nf\u2212(x) = E \u03b8\u2208U(0,1) [f({i : xi \u2265 \u03b8})].\nFor any submodular function f , the Lov\u00e1sz extension f\u2212 satisfies the following properties: (1) f\u2212(1S) = f(S) for all S \u2286 V , (2) f\u2212 is convex, and (3) f\u2212(c \u00b7 x) \u2265 c \u00b7 f\u2212(x) for any c \u2208 [0, 1]. These three properties immediately give the following simple lemma:\nLemma 1. Let S be a random set, and suppose that E[1S ] = c \u00b7p (for c \u2208 [0, 1]). Then, E[f(S)] \u2265 c \u00b7 f\u2212(p).\n3The best-known values of \u03b1 are taken from [11] (cardinality), [3] (matroid and p-system), and [13] (knapsack). In the case of a knapsack constraint, Wolsey in fact employs a slightly modified variant of the greedy algorithm. We note that the modified algorithm still satisfies all technical conditions required for our analysis (in particular, those for Lemma 2).\nAlgorithm 1 The standard greedy algorithm Greedy S \u2190 \u2205 loop Let C = {e \u2208 V \\ S : S \u222a {e} \u2208 I} Let e = arg maxe\u2208C{f(S \u222a {e})\u2212 f(S)} if C = \u2205 or f(S \u222a {e})\u2212 f(S) < 0 then\nreturn S end if\nend loop\nProof. We have:\nE[f(S)] = E[f\u2212(1S)] \u2265 f\u2212(E[1S ]) = f\u2212(c \u00b7 p) \u2265 c \u00b7 f\u2212(p),\nwhere the first equality follows from property (1), the first inequality from property (2), and the final inequality from property (3).\nHereditary Constraints. Our results hold quite generally for any problem which can be formulated in terms of a hereditary constraint. Formally, we consider the problem\nmax{f(S) : S \u2286 V, S \u2208 I}, (1)\nwhere f : 2V \u2192 R\u22650 is a submodular function and I \u2286 2V is a family of feasible subsets of V . We require that I be hereditary in the sense that if some set is in I, then so are all of its subsets. Examples of common hereditary families include cardinality constraints (I = {A \u2286 V : |A| \u2264 k}), matroid constraints (I corresponds to the collection independent sets of the matroid), knapsack constraints (I = {A \u2286 V : \u2211 i\u2208Awi \u2264 b}), as well as arbitrary combinations of such constraints. Given some constraint I \u2286 2V , we shall also consider restricted instances in which we are presented only with a subset V \u2032 \u2286 V , and must find a set S \u2286 V \u2032 with S \u2208 I that maximizes f . We say that an algorithm is an \u03b1-approximation for maximizing a submodular function subject to a hereditary constraint I if, for any submodular function f : 2V \u2192 R\u22650 and any subset V \u2032 \u2286 V the algorithm produces a solution S \u2286 V \u2032 with S \u2208 I, satisfying f(S) \u2265 \u03b1 \u00b7 f(OPT), where OPT \u2208 I is any feasible subset of V \u2032."}, {"heading": "2 The Standard Greedy Algorithm", "text": "Before describing our general algorithm, let us recall the standard greedy algorithm, Greedy, shown in Algorithm 1. The algorithm takes as input \u3008V, I, f\u3009, where V is a set of elements, I \u2286 2V is a hereditary constraint, represented as a membership oracle for I, and f : 2V \u2192 R\u22650 is a nonnegative submodular function, represented as a value oracle. Given \u3008V, I, f\u3009, Greedy iteratively constructs a solution S \u2208 I by choosing at each step the element maximizing the marginal increase of f . For some A \u2286 V , we let Greedy(A) denote the set S \u2208 I produced by the greedy algorithm that considers only elements from A.\nThe greedy algorithm satisfies the following property:\nAlgorithm 2 The distributed algorithm RandGreeDi for e \u2208 V do Assign e to a machine i chosen uniformly at random\nend for Let Vi be the elements assigned to machine i Run Greedy(Vi) on each machine i to obtain Si Place S = \u22c3 i Si on machine 1 Run Alg(S) on machine 1 to obtain T Let S\u2032 = arg maxi{f(Si)} return arg max{f(T ), f(S\u2032)}\nLemma 2. Let A \u2286 V and B \u2286 V be two disjoint subsets of V . Suppose that, for each element e \u2208 B, we have Greedy(A \u222a {e}) = Greedy(A). Then Greedy(A \u222aB) = Greedy(A).\nProof. Suppose for contradiction that Greedy(A \u222a B) 6= Greedy(A). We first note that, if Greedy(A\u222aB) \u2286 A, then Greedy(A\u222aB) = Greedy(A); this follows from the fact that each iteration of the Greedy algorithm chooses the element with the highest marginal value whose addition to the current solution maintains feasibility for I. Therefore, if Greedy(A \u222a B) 6= Greedy(A), the former solution contains an element of B. Let e be the first element of B that is selected by Greedy on the input A\u222aB. Then Greedy will also select e on the input A\u222a{e}, which contradicts the fact that Greedy(A \u222a {e}) = Greedy(A)."}, {"heading": "3 A Randomized, Distributed Greedy Algorithm for Monotone Submodular Maximization", "text": "Algorithm. We now describe our general, randomized distributed algorithm, RandGreeDi, shown in Algorithm 2. Suppose we have m machines. Our algorithm runs in two rounds. In the first round, we randomly distribute the elements of the ground set V to the machines, assigning each element to a machine chosen independently and uniformly at random. On each machine i, we execute Greedy(Vi) to select a feasible subset Si of the elements on that machine. In the second round, we place all of these selected subsets on a single machine, and run some algorithm Alg on this machine in order to select a final solution T . We return whichever is better: the final solution T or the best solution amongst all the Si from the first phase.\nAnalysis. We devote the rest of this section to the analysis of the RandGreeDi algorithm. Fix \u3008V, I, f\u3009, where I \u2286 2V is a hereditary constraint, and f : 2V \u2192 R\u22650 is any non-negative, monotone submodular function. Suppose that Greedy is an \u03b1-approximation and Alg is a \u03b2-approximation for the associated constrained monotone submodular maximization problem of the form (1). Let n = |V | and suppose that OPT = arg maxA\u2208I f(A) is a feasible set maximizing f .\nLet V(1/m) denote the distribution over random subsets of V where each element is included independently with probability 1/m. Let p \u2208 [0, 1]n be the following vector. For each element\ne \u2208 V , we have\npe =  PrA\u223cV(1/m)[e \u2208 Greedy(A \u222a {e})] if e \u2208 OPT0 otherwise Our main theorem follows from the next two lemmas, which characterize the quality of the best solution from the first round and that of the solution from the second round, respectively. Recall that f\u2212 is the Lov\u00e1sz extension of f .\nLemma 3. For each machine i, E[f(Si)] \u2265 \u03b1 \u00b7 f\u2212 (1OPT \u2212 p) .\nProof. Consider machine i. Let Vi denote the set of elements assigned to machine i in the first round. Let Oi = {e \u2208 OPT: e /\u2208 Greedy(Vi \u222a {e})}. We make the following key observations.\nWe apply Lemma 2 with A = Vi and B = Oi \\Vi to obtain that Greedy(Vi) = Greedy(Vi\u222aOi) = Si. Since OPT \u2208 I and I is hereditary, we must have Oi \u2208 I as well. Since Greedy is an \u03b1-approximation, it follows that\nf(Si) \u2265 \u03b1 \u00b7 f(Oi).\nSince the distribution of Vi is the same as V(1/m), for each element e \u2208 OPT, we have\nPr[e \u2208 Oi] = 1\u2212 Pr[e /\u2208 Oi] = 1\u2212 pe E[1Oi ] = 1OPT \u2212 p.\nBy combining these observations with Lemma 1, we obtain\nE[f(Si)] \u2265 \u03b1 \u00b7 E[f(Oi)] \u2265 \u03b1 \u00b7 f\u2212 (1OPT \u2212 p) .\nLemma 4. E[f(Alg(S))] \u2265 \u03b2 \u00b7 f\u2212(p). Proof. Recall that S = \u22c3 i Greedy(Vi). Since OPT \u2208 I and I is hereditary, S \u2229 OPT \u2208 I. Since Alg is a \u03b2-approximation, we have\nf(Alg(S)) \u2265 \u03b2\u00b7 f(S \u2229OPT). (2)\nConsider an element e \u2208 OPT. For each machine i, we have\nPr[e \u2208 S | e is assigned to machine i] = Pr[e \u2208 Greedy(Vi) | e \u2208 Vi] = Pr\nA\u223cV(1/m) [e \u2208 Greedy(A) | e \u2208 A]\n= Pr B\u223cV(1/m) [e \u2208 Greedy(B \u222a {e})]\n= pe.\nThe first equality follows from the fact that e is included in S if and only if it is included in Greedy(Vi). The second equality follows from the fact that the distribution of Vi is identical to\nV(1/m). The third equality follows from the fact that the distribution of A \u223c V(1/m) conditioned on e \u2208 A is identical to the distribution of B \u222a {e} where B \u223c V(1/m). Therefore\nPr[e \u2208 S \u2229OPT] = pe E[1S\u2229OPT] = p. (3)\nBy combining (2), (3), and Lemma 1, we obtain\nE[f(Alg(S))] \u2265 \u03b2\u00b7E[f(S \u2229OPT)] \u2265 \u03b2 \u00b7 f\u2212(p).\nCombining Lemma 4 and Lemma 3 gives us our main theorem.\nTheorem 5. Suppose that Greedy is an \u03b1-approximation algorithm and Alg is a \u03b2-approximation algorithm for maximizing a monotone submodular function subject to a hereditary constraint I. Then RandGreeDi is (in expectation) an \u03b1\u03b2\u03b1+\u03b2 -approximation algorithm for the same problem. Proof. Let Si = Greedy(Vi), S = \u22c3 i Si be the set of elements on the last machine, and T = Alg(S) be the solution produced on the last machine. Then, the output D produced by RandGreeDi satisfies f(D) \u2265 maxi(f(Si)) and f(D) \u2265 f(T ). Thus, from Lemmas 3 and 4 we have:\nE[f(D)] \u2265 \u03b1 \u00b7 f\u2212(1OPT \u2212 p) (4) E[f(D)] \u2265 \u03b2 \u00b7 f\u2212(p). (5)\nBy combining (4) and (5), we obtain\n(\u03b2 + \u03b1)E[f(D)] \u2265 \u03b1\u03b2 ( f\u2212(p) + f\u2212(1OPT \u2212 p) ) \u2265 \u03b1\u03b2 \u00b7 f\u2212(1OPT) = \u03b1\u03b2 \u00b7 f(OPT).\nIn the second inequality, we have used the fact that f\u2212 is convex and f\u2212(c\u00b7x) \u2265 cf\u2212(x) for any constant c \u2208 [0, 1].\nIf we use the standard greedy algorithm for Alg, we obtain the following simplified corollary of Theorem 5.\nCorollary 6. Suppose that Greedy is an \u03b1-approximation algorithm for maximizing a monotone submodular function, and use Greedy as the algorithm Alg in RandGreeDi. Then, the resulting algorithm is (in expectation) an \u03b12 -approximation algorithm for the same problem."}, {"heading": "4 Non-Monotone Submodular Functions", "text": "We consider the problem of maximizing a non-monotone submodular function subject to a hereditary constraint. Our approach is a slight modification of the randomized, distributed greedy algorithm described in Section 3, and it builds on the work of [4]. Again, we show how to combine the\nstandard Greedy algorithm, together with any algorithm Alg for the non-monotone case in order to obtain a randomized, distributed algorithm for the non-monotone submodular maximization.\nAlgorithm. Our modified algorithm, NMRandGreeDi, works as follows. As in the monotone case, in the first round we distribute the elements of V uniformly at random amongst the m machines. Then, we run the standard greedy algorithm twice to obtain two disjoint solutions S1i and S2i on each machine. Specifically, each machine first runs Greedy on Vi to obtain a solution S1i , then runs Greedy on Vi \\ S1i to obtain a disjoint solution S2i . In the second round, both of these solutions are sent to a single machine, which runs Alg on S = \u22c3 i(S1i \u222a S2i ) to produce a solution T . The best solution amongst T and all of the solutions S1i and S2i is then returned.\nAnalysis. We devote the rest of this section to the analysis of the algorithm. In the following, we assume that we are working with an instance \u3008V, I, f\u3009 of non-negative, non-monotone submodular maximization for which the Greedy algorithm has the following property:\nFor all S \u2208 I: f(Greedy(V )) \u2265 \u03b1 \u00b7 f(Greedy(V ) \u222a S) (GP)\nThe standard analysis of the Greedy algorithm shows that (GP) is satisfied with constant \u03b1 for hereditary constraints such as matroids, knapsacks, and p-systems (see Table 1).\nThe analysis is similar to the approach from the previous section. We define V(1/m) as before. We modify the definition of the vector p as follows. For each element e \u2208 V , we have\npe =  Pr A\u223cV(1/m) [ e \u2208 Greedy(A \u222a {e}) or e \u2208 Greedy((A \u222a {e}) \\Greedy(A \u222a {e})) ] if e \u2208 OPT 0 otherwise\nWe now derive analogues of Lemmas 3 and 4.\nLemma 7. Suppose that Greedy satisfies (GP). For each machine i,\nE [ f(S1i ) + f(S2i ) ] \u2265 \u03b1\u00b7 f\u2212(1OPT \u2212 p),\nand therefore E [ max { f(S1i ), f(S2i ) }] \u2265 \u03b12 \u00b7 f \u2212(1OPT \u2212 p).\nProof. Consider machine i and let Vi be the set of elements assigned to machine i in the first round. Let\nOi = {e \u2208 OPT: e /\u2208 Greedy(Vi \u222a {e}) and e /\u2208 Greedy((Vi \u222a {e}) \\Greedy(Vi \u222a {e}))}\nNote that, since OPT \u2208 I and I is hereditary, we have Oi \u2208 I.\nIt follows from Lemma 2 that\nS1i = Greedy(Vi) = Greedy(Vi \u222aOi), (6) S2i = Greedy(Vi \\ S1i ) = Greedy((Vi \\ S1i ) \u222aOi). (7)\nBy combining the equations above with the greedy property (GP), we obtain\nf(S1i ) (6)= f(Greedy(Vi \u222aOi)) (GP) \u2265 \u03b1\u00b7 f(Greedy(Vi \u222aOi) \u222aOi)\n(6)= \u03b1\u00b7 f(S1i \u222aOi), (8)\nf(S2i ) (7)= f(Greedy((Vi \\ S1i ) \u222aOi)) (GP) \u2265 \u03b1\u00b7 f(Greedy((Vi \\ S1i ) \u222aOi) \u222aOi)\n(7)= \u03b1\u00b7 f(S2i \u222aOi). (9)\nNow we observe that\nf(S1i \u222aOi) + f(S2i \u222aOi) \u2265 f((S1i \u222aOi) \u2229 (S2i \u222aOi)) + f(S1i \u222a S2i \u222aOi) (f is submodular) = f(Oi) + f(S1i \u222a S2i \u222aOi) (S1i \u2229 S2i = \u2205) \u2265 f(Oi). (f is non-negative) (10)\nBy combining (8), (9), and (10), we obtain\nf(S1i ) + f(S2i ) \u2265 \u03b1\u00b7 f(Oi). (11)\nSince the distribution of Vi is the same as V(1/m), for each element e \u2208 OPT, we have\nPr[e \u2208 Oi] = 1\u2212 Pr[e /\u2208 Oi] = 1\u2212 pe, E[1Oi ] = 1OPT \u2212 p. (12)\nBy combining (11), (12), and Lemma 1, we obtain\nE[f(S1i ) + f(S2i )] \u2265 \u03b1\u00b7E[f(Oi)] (By (11)) \u2265 \u03b1\u00b7 f\u2212(1OPT \u2212 p). (By (12) and Lemma 1)\nLemma 8. E[f(Alg(S))] \u2265 \u03b2 \u00b7 f\u2212(p). Proof. Recall that S1i = Greedy(Vi), S2i = Greedy(Vi\\S1i ), and S = \u22c3 i(S1i \u222aS2i ). Since OPT \u2208 I and I is hereditary, S \u2229OPT \u2208 I. Since Alg is a \u03b2-approximation, we have\nf(Alg(S)) \u2265 \u03b2\u00b7 f(S \u2229OPT). (13)\nConsider an element e \u2208 OPT. For each machine i, we have\nPr[e \u2208 S | e is assigned to machine i] = Pr[e \u2208 Greedy(Vi) or e \u2208 Greedy(Vi \\Greedy(Vi)) | e \u2208 Vi] = Pr\nA\u223cV(1/m) [e \u2208 Greedy(A) or e \u2208 Greedy(A \\Greedy(A)) | e \u2208 A]\n= Pr B\u223cV(1/m) [e \u2208 Greedy(B \u222a {e}) or e \u2208 Greedy((B \u222a {e}) \\Greedy(B \u222a {e}))]\n= pe.\nThe first equality above follows from the fact that e is included in S iff e is included in either S1i or S2i . The second equality follows from the fact that the distribution of Vi is the same as V(1/m). The third equality follows from the fact that the distribution of A \u223c V(1/m) conditioned on e \u2208 A is identical to the distribution of B \u222a {e} where B \u223c V(1/m). Therefore\nPr[e \u2208 S \u2229OPT] = pe, E[1S\u2229OPT] = p. (14)\nBy combining (13), (14), and Lemma 1, we obtain\nE[f(Alg(S))] \u2265 \u03b2\u00b7E[f(S \u2229OPT)] \u2265 \u03b2 \u00b7 f\u2212(p).\nWe can now combine Lemmas 8 and 7 to obtain our main result for non-monotone submodular maximization.\nTheorem 9. Consider the problem of maximizing a submodular function under some hereditary constraint I, and suppose that Greedy satisfies (GP) and Alg is a \u03b2-approximation algorithm for this problem. Then NMRandGreeDi is (in expectation) an \u03b1\u03b2\u03b1+2\u03b2 -approximation algorithm for the same problem. Proof. Let S1i = Greedy(Vi), S2i = Greedy(Vi \\ S1i ), and S = \u22c3 i(S1i \u222a S2i ) be the set of elements on the last machine, and T = Alg(S) be the solution produced on the last machine. Then, the output D produced by RandGreeDi satisfies f(D) \u2265 maxi max{f(S1i ), f(S2i )} and f(D) \u2265 f(T ). Thus, from Lemmas 7 and 8 we have:\nE[f(D)] \u2265 \u03b12 \u00b7 f \u2212(1OPT \u2212 p), (15) E[f(D)] \u2265 \u03b2 \u00b7 f\u2212(p). (16)\nBy combining (15) and (16), we obtain\n(2\u03b2 + \u03b1)E[f(D)] \u2265 \u03b1\u03b2[f\u2212(p) + f\u2212(1OPT \u2212 p)] \u2265 \u03b1\u03b2 \u00b7 f\u2212(1OPT) = \u03b1\u03b2 \u00b7 f(OPT).\nIn the second inequality, we have used the fact that f\u2212 is convex and f\u2212(c\u00b7x) \u2265 cf\u2212(x) for any constant c \u2208 [0, 1].\nWe remark that one can use the following approach on the last machine [4]. As in the first round, we run Greedy twice to obtain two solutions T1 = Greedy(S) and T2 = Greedy(S \\ T1). Additionally, we select a subset T3 \u2286 T1 using an unconstrained submodular maximization algorithm on T1, such as the Double Greedy algorithm of [1], which is a 12 -approximation. The final solution T is the best solution among T1, T2, T3. If Greedy satisfies property GP, then it follows from the analysis of [4] that the resulting solution T satisfies f(T ) \u2265 \u03b12(1+\u03b1) \u00b7 f(OPT). This gives us the following corollary of Theorem 9:\nCorollary 10. Consider the problem of maximizing a submodular function subject to some hereditary constraint I and suppose that Greedy satisfies (GP) for this problem. Let Alg be the algorithm described above that uses Greedy twice and Double Greedy. Then NMRandGreeDi achieves (in expectation) an \u03b14+2\u03b1 -approximation for the same problem.\nProof. By (GP) and the approximation guarantee of the Double Greedy algorithm, we have:\nf(T ) \u2265 f(T1) \u2265 \u03b1 \u00b7 f(T1 \u222aOPT) (17) f(T ) \u2265 f(T2) \u2265 \u03b1 \u00b7 f(T2 \u222a (OPT \\ T1)) (18) f(T ) \u2265 f(T3) \u2265 1 2f(T1 \u2229OPT). (19)\nAdditionally, from [4, Lemma 2], we have:\nf(T1 \u222aOPT) + f(T2 \u222a (OPT \\ T1)) + f(T1 \u2229OPT) \u2265 f(OPT)\nBy combining the inequalities above, we obtain:\n(1 + \u03b1)f(T ) \u2265 \u03b12 (f(T1 \u222aOPT) + f(T2 \u222a (OPT \\ T1)) + f(T1 \u2229OPT)) \u2265 \u03b1 2 f(OPT)\nand hence f(T ) \u2265 \u03b12(1+\u03b1) \u00b7 f(OPT) as claimed. Setting \u03b2 = \u03b1 2(\u03b1+1) in Theorem 9, we obtain an approximation ratio of \u03b14+2\u03b1 ."}, {"heading": "5 Experiments", "text": "We experimentally evaluate and compare the following distributed algorithms for maximizing a monotone submodular function subject to a cardinality constraint: the RandGreeDi algorithm described in Section 3, the deterministic GreeDi algorithm of [10], and the Sample&Prune algorithm of [8]. We run these algorithms in several scenarios and we evaluate their performance relative to the centralized Greedy solution on the entire dataset.\nExemplar based clustering. Our experimental setup is similar to that of [10]. Our goal is to find a representative set of objects from a dataset by solving a k-medoid problem [7] that aims to minimize the sum of pairwise dissimilarities between the chosen objects and the entire dataset. Let V denote the set of objects in the dataset and let d : V \u00d7 V \u2192 R be a dissimilarity function; we assume that d is symmetric, that is, d(i, j) = d(j, i) for each pair i, j. Let L : 2V \u2192 R be the function such that L(A) = 1|V | \u2211 v\u2208V mina\u2208A d(a, v) for each set A \u2286 V . We can turn the problem of minimizing L into the problem of maximizing a monotone submodular function f by introducing an auxiliary element v0 and by defining f(S) = L({v0})\u2212 L(S \u222a {v0}) for each set S \u2286 V .\nTiny Images experiments: In our experiments, we used a subset of the Tiny Images dataset consisting of 32\u00d732 RGB images [12], each represented as 3, 072 dimensional vector. We subtracted from each vector the mean value and normalized the result, to obtain a collection of 3, 072-dimensional vectors of unit norm. We considered the distance function d(x, y) = \u2016x\u2212 y\u20162 for every pair x, y of vectors. We used the zero vector as the auxiliary element v0 in the definition of f .\nIn our smaller experiments, we used 10,000 tiny images, and compared the utility of each algorithm to that of the centralized greedy. The results are summarized in Figures 1(c) and 1(f).\nIn our large scale experiments, we used one million tiny images, and m = 100 machines. In the first round of the distributed algorithm, each machine ran the Greedy algorithm to maximize a restricted objective function f , which is based on the average dissimilarity L taken over only those images assigned to that machine. Similarly, in the second round, the final machine maximized an objective function f based on the total dissimilarity of all those images it received . We also considered a variant similar to that described by [10], in which 10,000 additional random images from the original dataset were added to the final machine. The results are summarized in Figure 1(i).\nRemark on the function evaluation. In decomposable cases such as exemplar clustering, the function is a sum of distances over all points in the dataset. By concentration results such as Chernoff bounds, the sum can be approximated additively with high probability by sampling a few points and using the (scaled) empirical sum. The random subset each machine receives can readily serve as the samples for the above approximation. Thus the random partition is useful for for evaluating the function in a distributed fashion, in addition to its algorithmic benefits.\nMaximum Coverage experiments. We ran several experiments using instances of the Maximum Coverage problem. In the Maximum Coverage problem, we are given a collection C \u2286 2V of subsets of a ground set V and an integer k, and the goal is to select k of the subsets in C that cover as many elements as possible.\nKosarak and accidents datasets4: We evaluated and compared the algorithms on the datasets used by Kumar et al. [8]. In both cases, we computed the optimal centralized solution using CPLEX, and calculated the actual performance ratio attained by the algorithms. The results are summarized in Figures 1(a), 1(d), 1(b), 1(e).\nSynthetic hard instances: We generated a synthetic dataset with hard instances for the deterministic GreeDi. We describe the instances in Section B. We ran the GreeDi algorithm with a worst-case partition of the data. The results are summarized in Figure 1(h).\nFinding diverse yet relevant items. We evaluated our NMRandGreeDi algorithm on the following instance of non-monotone submodular maximization subject to a cardinality constraint. We used the objective function of Lin and Bilmes [9]: f(A) = \u2211 i\u2208V \u2211 j\u2208A sij \u2212 \u03bb \u2211 i,j\u2208A sij , where \u03bb is a redundancy parameter and {sij}ij is a similarity matrix. We generated an n \u00d7 n similarity matrix with random entries sij \u2208 U(0, 100) and we set \u03bb = n/k. The results are summarized in Figure 1(g).\nMatroid constraints. In order to evaluate our algorithm on a matroid constraint, we considered the following variant of maximum coverage: we are given a space containing several demand points and n facilities (e.g. wireless access points or sensors). Each facility can operate in one of r modes, each with a distinct coverage profile. The goal is to find a subset of at most k facilities to activate, along with a single mode for each activated facility, so that the total number of demand points covered is maximized. In our experiment, we placed 250,000 demand points in a grid in the unit square, together with a grid of n facilities. We modeled coverage profiles as ellipses centered at each facility with major axes of length 0.1`, minor axes of length 0.1/` rotated by \u03c1 where ` \u2208 N (3, 13) and \u03c1 \u2208 U(0, 2\u03c0) are chosen randomly for each ellipse. We performed two series of experiments. In the first, there were n = 900 facilities, each with r = 5 coverage profiles, while in the second there\n4The data is available at http://fimi.ua.ac.be/data/.\nwere n = 100 facilities, each with r = 100 coverage profiles.\nThe resulting problem instances were represented as ground set comprising a list of ellipses, each with a designated facility, together with a partition matroid constraint ensuring that at most one ellipse per facility was chosen. As in our large-scale exemplar-based clustering experiments, we considered 3 approaches for assigning ellipses to machines: assigning consecutive blocks of ellipses to each machine, assigning ellipses to machines in round-robin fashion, and assigning ellipses to machines uniformly at random. The results are summarized in Figures 1(j) and 1(k); in these plots, GreeDi(rr) and GreeDi(block) denote the results of GreeDi when we assign the ellipses to machines deterministically in a round-robin fashion and in consecutive blocks, respectively.\nIn general, our experiments show that random and round robin are the best allocation strategies. One explanation for this phenomenon is that both of these strategies ensure that each machine receives a few elements from several distinct partitions in the first round. This allows each machine to return a solution containing several elements.\nAcknowledgements. We thank Moran Feldman for suggesting a modification to our original analysis that led to the simpler and stronger analysis included in this version of the paper."}, {"heading": "B A tight example for Deterministic GreeDI", "text": "Here we give a family of examples that show that the GreeDI algorithm of Mirzasoleiman et al. cannot achieve an approximation better than 1/ \u221a k.\nConsider the following instance of Max k-Coverage. We have `2 +1 machines and k = `+`2. Let N be a ground set with `2 + `3 elements, N = { 1, 2, . . . , `2 + `3 } . We define a coverage function on a\ncollection S of subsets of N as follows. In the following, we define how the sets of S are partitioned on the machines.\nOn machine 1, we have the following ` sets from OPT: O1 = {1, 2, . . . , `}, O2 = {`+ 1, . . . , 2`}, . . . , O` = { `2 \u2212 `+ 1, . . . , `2 } . We also pad the machine with copies of the empty set. On machine i > 1, we have the following sets. There is a single set from OPT, namely O\u2032i ={ `2 + (i\u2212 1)`+ 1, `2 + (i\u2212 1)`+ 2, . . . , `2 + i` } . Additionally, we have ` sets that are designed to\nfool the greedy algorithm; the j-th such set is Oj \u222a { `2 + (i\u2212 1)`+ j } . As before, we pad the machine with copies of the empty set.\nThe optimal solution is O1, . . . , O`, O\u20321, . . . , O\u2032`2 and it has a total coverage of ` 2 + `3. On the first machine, Greedy picks the ` sets O1, . . . , Om from OPT and `2 copies of the empty set. On each machine i > 1, Greedy first picks the ` sets Aj = Oj \u222a { `2 + (i\u2212 1)`+ j } , since each of them has marginal value greater than O\u2032i. Once Greedy has picked all of the Aj \u2019s, the marginal value of O\u2032i becomes zero and we may assume that Greedy always picks the empty sets instead of O\u2032i.\nNow consider the final round of the algorithm where we run Greedy on the union of the solutions from each of the machines. In this round, regardless of the algorithm, the sets picked can only cover{ 1, . . . , `2 } (using the set O1, . . . , O`) and one additional item per set for a total of 2`2 elements. Thus the total coverage of the final solution is at most 2`2. Hence the approximation is at most 2`2 `2+`3 = 2 1+` \u2248 1\u221a k ."}], "references": [{"title": "A tight linear time (1/2)approximation for unconstrained submodular maximization", "author": ["Niv Buchbinder", "Moran Feldman", "Joseph Naor", "Roy Schwartz"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["Jeffrey Dean", "Sanjay Ghemawat"], "venue": "Commun. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014II", "author": ["M L Fisher", "G L Nemhauser", "L A Wolsey"], "venue": "Mathematical Programming Studies,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1978}, {"title": "Constrained nonmonotone submodular maximization: Offline and secretary algorithms", "author": ["Anupam Gupta", "Aaron Roth", "Grant Schoenebeck", "Kunal Talwar"], "venue": "In Internet and Network Economics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Composable core-sets for diversity and coverage maximization", "author": ["Piotr Indyk", "Sepideh Mahabadi", "Mohammad Mahdian", "Vahab S Mirrokni"], "venue": "In Proceedings of the 33rd ACM SIGMOD- SIGACT-SIGART symposium on Principles of database systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "The efficacy of the \"greedy\" algorithm", "author": ["T A Jenkyns"], "venue": "In Proceedings of the 7th Southeastern Conference on Combinatorics, Graph Theory, and Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1976}, {"title": "Finding groups in data: an introduction to cluster analysis, volume 344", "author": ["Leonard Kaufman", "Peter J Rousseeuw"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["Ravi Kumar", "Benjamin Moseley", "Sergei Vassilvitskii", "Andrea Vattani"], "venue": "In Proceedings of the twenty-fifth annual ACM symposium on Parallelism in algorithms and architectures,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "How to select a good training-data subset for transcription: Submodular active selection for sequences", "author": ["Hui Lin", "Jeff A. Bilmes"], "venue": "In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Distributed submodular maximization: Identifying representative elements in massive data", "author": ["Baharan Mirzasoleiman", "Amin Karbasi", "Rik Sarkar", "Andreas Krause"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014I", "author": ["George L Nemhauser", "Laurence A Wolsey", "Marshall L Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1978}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Antonio Torralba", "Robert Fergus", "William T Freeman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1958}], "referenceMentions": [{"referenceID": 1, "context": "Our algorithm can be easily implemented in a parallel model of computation such as MapReduce [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "[10] give a distributed algorithm, called GreeDi, for maximizing a monotone submodular function subject to a cardinality constraint.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] give a family of instances where the approximation achieved is only 1/min {k,m} if the solution picked on each of the machines is the optimal solution for the set of items on the machine.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] give distributed algorithms for maximizing a monotone submodular function subject to a cardinality or more generally, a matroid constraint.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Their algorithm combines the Threshold Greedy algorithm of [4] with a sample and prune strategy.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "[5] studied coreset approaches to develop distributed algorithms for finding representative and yet diverse subsets in large collections.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Our analysis may perhaps provide some theoretical justification for the very good empirical performance of the GreeDi algorithm that was established previously in the extensive experiments of [10].", "startOffset": 192, "endOffset": 196}, {"referenceID": 5, "context": "This is the case not only for cardinality constraints, but also for matroid constraints, knapsack constraints, and p-system constraints [6], which generalize the intersection of p matroid constraints.", "startOffset": 136, "endOffset": 139}, {"referenceID": 0, "context": "The Lov\u00e1sz extension f\u2212 : [0, 1]V \u2192 R\u22650 of a submodular function f is given by: f\u2212(x) = E \u03b8\u2208U(0,1) [f({i : xi \u2265 \u03b8})].", "startOffset": 26, "endOffset": 32}, {"referenceID": 0, "context": "For any submodular function f , the Lov\u00e1sz extension f\u2212 satisfies the following properties: (1) f(1S) = f(S) for all S \u2286 V , (2) f\u2212 is convex, and (3) f\u2212(c \u00b7 x) \u2265 c \u00b7 f\u2212(x) for any c \u2208 [0, 1].", "startOffset": 185, "endOffset": 191}, {"referenceID": 0, "context": "Let S be a random set, and suppose that E[1S ] = c \u00b7p (for c \u2208 [0, 1]).", "startOffset": 63, "endOffset": 69}, {"referenceID": 10, "context": "3The best-known values of \u03b1 are taken from [11] (cardinality), [3] (matroid and p-system), and [13] (knapsack).", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "3The best-known values of \u03b1 are taken from [11] (cardinality), [3] (matroid and p-system), and [13] (knapsack).", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "Let p \u2208 [0, 1]n be the following vector.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "In the second inequality, we have used the fact that f\u2212 is convex and f\u2212(c\u00b7x) \u2265 cf\u2212(x) for any constant c \u2208 [0, 1].", "startOffset": 108, "endOffset": 114}, {"referenceID": 3, "context": "Our approach is a slight modification of the randomized, distributed greedy algorithm described in Section 3, and it builds on the work of [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "In the second inequality, we have used the fact that f\u2212 is convex and f\u2212(c\u00b7x) \u2265 cf\u2212(x) for any constant c \u2208 [0, 1].", "startOffset": 108, "endOffset": 114}, {"referenceID": 3, "context": "We remark that one can use the following approach on the last machine [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Additionally, we select a subset T3 \u2286 T1 using an unconstrained submodular maximization algorithm on T1, such as the Double Greedy algorithm of [1], which is a 2 -approximation.", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "If Greedy satisfies property GP, then it follows from the analysis of [4] that the resulting solution T satisfies f(T ) \u2265 \u03b1 2(1+\u03b1) \u00b7 f(OPT).", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "We experimentally evaluate and compare the following distributed algorithms for maximizing a monotone submodular function subject to a cardinality constraint: the RandGreeDi algorithm described in Section 3, the deterministic GreeDi algorithm of [10], and the Sample&Prune algorithm of [8].", "startOffset": 246, "endOffset": 250}, {"referenceID": 7, "context": "We experimentally evaluate and compare the following distributed algorithms for maximizing a monotone submodular function subject to a cardinality constraint: the RandGreeDi algorithm described in Section 3, the deterministic GreeDi algorithm of [10], and the Sample&Prune algorithm of [8].", "startOffset": 286, "endOffset": 289}, {"referenceID": 9, "context": "Our experimental setup is similar to that of [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Our goal is to find a representative set of objects from a dataset by solving a k-medoid problem [7] that aims to minimize the sum of pairwise dissimilarities between the chosen objects and the entire dataset.", "startOffset": 97, "endOffset": 100}, {"referenceID": 11, "context": "Tiny Images experiments: In our experiments, we used a subset of the Tiny Images dataset consisting of 32\u00d732 RGB images [12], each represented as 3, 072 dimensional vector.", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "We also considered a variant similar to that described by [10], in which 10,000 additional random images from the original dataset were added to the final machine.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We used the objective function of Lin and Bilmes [9]: f(A) = \u2211 i\u2208V \u2211 j\u2208A sij \u2212 \u03bb \u2211 i,j\u2208A sij , where \u03bb is a redundancy parameter and {sij}ij is a similarity matrix.", "startOffset": 49, "endOffset": 52}], "year": 2015, "abstractText": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We develop a simple distributed algorithm that is embarrassingly parallel and it achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.", "creator": "LaTeX with hyperref package"}}}