{"id": "1502.04149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation", "abstract": "spookier Monaural source boadilla separation snowboarder is important sahakari for ceallaigh many surdo real sonnenfeld world romashchenko applications. parring It is challenging in that, d\u00e9jeuner given homeowner only checchinato single a\u00f1os channel demmin information is bernarde available, viropharma there 286.2 is an ampera infinite grabow number of selima solutions macherla without proper jaxon constraints. dubroff In this paper, vln we explore explosions joint ballyfermot optimization of masking nelo functions steeves and deep recurrent 50.94 neural networks wga for monaural source separation manzala tasks, including the bhawan monaural makhtal speech separation stma task, vanquisher monaural singing girgis voice separation lepreau task, spion and speech denoising tiele task. subtyping The joint optimization of calligrapher the zdenci deep vejar recurrent 12.69 neural networks with 12-9 an felons extra masking layer enforces six-man a reconstruction constraint. Moreover, 57.82 we explore incorrigible a hoyles discriminative creditability training uzair criterion elbaradei for chorusing the panca neural networks chenel to 93,500 further kundi enhance supercarrier the ital!and!off separation rocastle performance. dongwon We finse evaluate gericht our j\u00e1 proposed system satir on rivne TSP, MIR - rallus 1K, and TIMIT 46a dataset 117.68 for skolimowski speech sagres separation, cingulate singing 110-page voice picaridin separation, and synnot speech mawddwy denoising globally tasks, 78.22 respectively. supt Our approaches achieve 2. 1981-86 30 ~ 4. 98 erener dB nanos SDR gain compared ulpiana to NMF interview models nmsp in the speech separation task, bo\u017eidar 2. schoelcher 30 ~ 2. hetal 48 dB GNSDR annet gain therd and 4. nasserist 32 ~ khodabandeh 5. 42 dB spectroscopic GSIR remedial gain .61 compared to previous jemez models odak in the rome singing voice enounters separation algimantas task, porky and outperform NMF nutcase and DNN baseline in briana the speech denoising sinikka task.", "histories": [["v1", "Fri, 13 Feb 2015 23:22:16 GMT  (8542kb,D)", "http://arxiv.org/abs/1502.04149v1", "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. 11 pages"], ["v2", "Tue, 2 Jun 2015 04:22:20 GMT  (8630kb,D)", "http://arxiv.org/abs/1502.04149v2", "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. 12 pages"], ["v3", "Thu, 13 Aug 2015 04:20:33 GMT  (1892kb,D)", "http://arxiv.org/abs/1502.04149v3", "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015 12 pages"], ["v4", "Thu, 1 Oct 2015 02:58:01 GMT  (1754kb,D)", "http://arxiv.org/abs/1502.04149v4", null]], "COMMENTS": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. 11 pages", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.LG cs.MM", "authors": ["po-sen huang", "minje kim", "mark hasegawa-johnson", "paris smaragdis"], "accepted": false, "id": "1502.04149"}, "pdf": {"name": "1502.04149.pdf", "metadata": {"source": "CRF", "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation", "authors": ["Po-Sen Huang", "Minje Kim", "Mark Hasegawa-Johnson"], "emails": ["huang146@illinois.edu;", "jhasegaw@illinois.edu)", "minje@illinois.edu)", "paris@illinois.edu)"], "sections": [{"heading": null, "text": "Index Terms\u2014Monaural Source Separation, Time-Frequency Masking, Deep Recurrent Neural Network, Discriminative Training\nI. INTRODUCTION\nSOURCE separation are problems in which several signalshave been mixed together and the objective is to recover the original signals from the combined signal. Source separation is important for several real-world applications. For example, the accuracy of chord recognition and pitch estimation can be improved by separating singing voice from music [1]. The accuracy of automatic speech recognition (ASR) can be improved by separating noise from speech signals [2]. Monaural source separation, i.e., source separation from monaural recordings, is more challenging in that, without prior knowledge, there are an infinite number of solutions given only single channel information is available. In this paper, we focus on source separation from monaural recordings for applications of speech separation, singing voice separation, and speech denoising tasks.\nP.-S. Huang and M. Hasegawa-Johnson are with the Department of Electrical and Computer Engineering, University of Illinois at UrbanaChampaign, Illinois, IL, 61801 USA (email: huang146@illinois.edu; jhasegaw@illinois.edu)\nM. Kim is with the Department of Computer Science, University of Illinois at Urbana-Champaign, Illinois, IL, 61801 USA (email: minje@illinois.edu)\nP. Smaragdis is with the Department of Computer Science and Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Illinois, IL, 61801 USA, and Adobe Research (email: paris@illinois.edu)\nManuscript received XXX; revised XXX.\nSeveral different approaches have been proposed to address the monaural source separation problem. We can categorize them into domain-specific and domain-agnostic approaches. For domain-specific approach, models are designed according to the prior knowledge and assumption of the tasks. For example, in the singing voice separation task, several approaches have been proposed to utilize the assumption of the low rank and sparsity of the music and speech signals, respectively [1], [3]\u2013[5]. In the speech denoising task, spectral subtraction [6] subtracts a short-term noise spectrum estimate to generate the spectrum of clean speech. By assuming the underlying properties of speech and noise, statistical modelbased methods infer speech spectral coefficients given noisy observations [7]. However, in real-world scenarios, these strong assumptions may not always be valid. For example, in the singing voice separation task, the drum sounds may lie in the sparse subspace instead of being low rank. In the speech denoising task, the models often fail to predict the acoustic environments due to the non-stationary nature of noise.\nFor domain-agnostic approach, models are learned from data directly and can be expected to apply equally well to different domains. Non-negative matrix factorization (NMF) [8] and probabilistic latent semantic indexing (PLSI) [9], [10] learn the non-negative reconstruction bases and weights of different sources and use them to factorize time-frequency spectral representations. NMF and PLSI can be viewed as a linear transformation of the given mixture features (e.g. magnitude spectra) during prediction time. Moreover, by the minimum mean square estimate (MMSE) criterion, E[Y|X] is a linear model if Y and X are jointly Gaussian, where X and Y are the separated and mixture signals, respectively. In realworld scenarios, since signals might not always be Gaussian, we often consider the mapping relationship between mixture signals and different sources as a nonlinear transformation, and hence non-linear models such as neural networks are desirable.\nRecently, deep learning based methods have been used in many applications, including automatic speech recognition [11], image classification [12], etc. Deep learning based methods have also started to draw attention from the source separation research community by modeling the nonlinear mapping relationship between input and output. Previous work on deep learning based source separation can be further categorized into three ways: (1) Given a mixture signal, deep neural networks predict one of the sources. Maas et al. proposed using a deep recurrent neural network (DRNN) for robust automatic speech recognition tasks [2]. Given noisy features, the authors apply a DRNN to predict clean speech features.\nar X\niv :1\n50 2.\n04 14\n9v 1\n[ cs\n.S D\n] 1\n3 Fe\nb 20\n15\nXu et al. proposed a deep neural network (DNN)-based speech enhancement system, including global variance equalization and noise-aware training, to predict clean speech spectra for speech enhancement tasks [13]. Weninger et al. [14] trained two long-short term memory (LSTM) RNNs for predicting speech and noise, respectively. Final prediction is made by creating a mask out of the two source predictions, which eventually masks out the noise part from the noisy spectrum. Liu et al. explored using a deep neural network for predicting clean speech signals in various denoising settings [15]. These approaches, however, only model one of the mixture signals, which is less optimal compared to a framework that models all sources together. (2) Given a mixture signal, deep neural networks predict the time-frequency mask between the two sources. In the ideal binary mask estimation task, Nie et al. utilized deep stacking networks with time series inputs and a re-threshold method to predict the ideal binary mask [16]. Narayanan and Wang [17] and Wang and Wang [18] proposed a two-stage framework using deep neural networks. In the first stage, the authors use d neural networks to predict each output dimension separately, where d is the target feature dimension; in the second stage, a classifier (one layer perceptron or an SVM) is used for refining the prediction given the output from the first stage. The proposed framework is not scalable when the output dimension is high and there are redundancies between the neural networks in neighboring frequencies. Wang et al. [19] recently proposed using deep neural networks to train different targets, including ideal ratio mask and FFT-mask, for speech separation tasks. These mask-based approaches focus on predicting the masking results of clean speech, instead of considering multiple sources simultaneously. (3) Given mixture signals, deep neural networks predict two different sources. Tu et al. proposed modeling two sources as the output targets for a robust ASR task [20]. However, the constraint that the sum of two different sources is the original mixture is not considered. Grais et al. [21] proposed using a deep neural network to predict two scores corresponding to the probabilities of two different sources respectively for a given frame of normalized magnitude spectrum.\nIn this paper, we further extend our previous work in [22] and [23] and propose a general framework for the monaural source separation task for speech separation, singing voice separation, and speech denoising. Our proposed framework models two sources simultaneously and jointly optimizes timefrequency masking functions together with the deep recurrent networks. The proposed approach directly reconstructs the prediction of two sources. In addition, given that there are two competing sources, we further propose a discriminative training criterion for enhancing source to interference ratio.\nThe organization of this paper is as follows: Section II introduces the proposed methods, including the deep recurrent neural networks, joint optimization of deep learning models and a soft time-frequency masking function, and the training objectives. Section III presents the experimental setting and results using the TSP, MIR-1K, and TIMIT dateset for speech separation, singing voice separation, and speech denoising task, respectively. We conclude the paper in Section IV."}, {"heading": "II. PROPOSED METHODS", "text": ""}, {"heading": "A. Deep Recurrent Neural Networks", "text": "To capture the contextual information among audio signals, one way is to concatenate neighboring features together as input features to the deep neural network. However, the number of parameters increases proportionally to the input dimension and the number of neighbors in time. Hence, the size of the concatenating window is limited. A recurrent neural network (RNN) can be considered as a DNN with indefinitely many layers, which introduce the memory from previous time steps. The potential weakness for RNNs is that RNNs lack hierarchical processing of the input at the current time step. To further provide the hierarchical information through multiple time scales, deep recurrent neural networks (DRNNs) are explored [24], [25]. DRNNs can be explored in different schemes as shown in Figure 1. The left of Figure 1 is a standard RNN, folded out in time. The middle of Figure 1 is an L intermediate layer DRNN with temporal connection at the l-th layer. The right of Figure 1 is an L intermediate layer DRNN with full temporal connections (called stacked RNN (sRNN) in [25]). Formally, we can define different schemes of DRNNs as follows. Suppose there is an L intermediate layer DRNN with the recurrent connection at the l-th layer, the l-th hidden activation at time t is defined as:\nhlt = fh(xt,h l t\u22121) = \u03c6l ( Ulhlt\u22121 +W l\u03c6l\u22121 ( Wl\u22121 ( . . . \u03c61 ( W1xt )))) (1)\nand the output, yt, can be defined as:\nyt = fo(h l t) = WL\u03c6L\u22121 ( WL\u22121 ( . . . \u03c6l ( Wlhlt ))) (2)\nwhere xt is the input to the network at time t, \u03c6l is an elementwise nonlinear function, Wl is the weight matrix for the l-th layer, and Ul is the weight matrix for the recurrent connection at the l-th layer. The output layer is a linear layer.\nThe stacked RNNs have multiple levels of transition functions, defined as:\nhlt = fh(h l\u22121 t ,h l t\u22121)\n= \u03c6l(U lhlt\u22121 +W lhl\u22121t ) (3)\nwhere hlt is the hidden state of the l-th layer at time t. U l and Wl are the weight matrices for the hidden activation at time t\u2212 1 and the lower level activation hl\u22121t , respectively. When l = 1, the hidden activation is computed using h0t = xt.\nFunction \u03c6l(\u00b7) is a nonlinear function, and we empirically found that using the rectified linear unit f(x) = max(0,x) [26] performs better compared to using a sigmoid or tanh function. For a DNN, the temporal weight matrix Ul is a zero matrix."}, {"heading": "B. Model Architecture", "text": "At time t, the training input, xt, of the network is the concatenation of features from a mixture within a window. We use magnitude spectra as features in this paper. The output targets, y1t and y2t , and output predictions, y\u03021t and y\u03022t , of the network are the magnitude spectra of different sources.\nSince our goal is to separate one of the sources from a mixture, instead of learning one of the sources as the target, we adapt the framework from [27] to model all different sources simultaneously. Figure 2 shows an example of the architecture.\nMoreover, we find it useful to further smooth the source separation results with a time-frequency masking technique, for example, binary time-frequency masking or soft timefrequency masking [1], [27]. The time-frequency masking function enforces the constraint that the sum of the prediction results is equal to the original mixture.\nGiven the input features, xt, from the mixture, we obtain the output predictions y\u03021t and y\u03022t through the network. The soft time-frequency mask mt is defined as follows:\nmt(f) = |y\u03021t(f)|\n|y\u03021t(f)|+ |y\u03022t(f)| (4)\nwhere f \u2208 {1, . . . , F} represents different frequencies. Once a time-frequency mask mt is computed, it is applied to the magnitude spectra zt of the mixture signals to obtain the estimated separation spectra s\u03021t and s\u03022t , which correspond to sources 1 and 2, as follows:\ns\u03021t(f) = mt(f)zt(f) s\u03022t(f) = (1\u2212mt(f)) zt(f)\n(5)\nwhere f \u2208 {1, . . . , F} represents different frequencies. The time-frequency masking function can be viewed as a layer in the neural network as well. Instead of training the network and applying the time-frequency masking to the results separately, we can jointly train the deep learning models with the time-frequency masking functions. We add an extra layer to the original output of the neural network as follows:\ny\u03031t = |y\u03021t |\n|y\u03021t |+ |y\u03022t | zt\ny\u03032t = |y\u03022t |\n|y\u03021t |+ |y\u03022t | zt\n(6)\nwhere the operator is the element-wise multiplication (Hadamard product).\nIn this way, we can integrate the constraints to the network and optimize the network with the masking function jointly. Note that although this extra layer is a deterministic layer, the network weights are optimized for the error metric between y\u03031t , y\u03032t and y1t , y2t , using back-propagation. The time domain signals are reconstructed based on the inverse short time Fourier transform (ISTFT) of the estimated magnitude spectra along with the original mixture phase spectra."}, {"heading": "C. Training Objectives", "text": "Given the output predictions y\u03031t and y\u03032t (or y\u03021t and y\u03022t ) of the original sources y1t and y2t , we explore optimizing neural network parameters by minimizing the squared error,\nas follows:\nJMSE = \u2016y\u03031t \u2212 y1t\u201622 + \u2016y\u03032t \u2212 y2t\u201622 (7)\nEq. (7) measures the difference between predicted and actual targets. When targets have similar spectra, it is possible for the DNN to minimize Eq. (7) by being too conservative: when a feature could be attributed to either source 1 or source 2, the neural network attributes it to both. The conservative strategy is effective in training, but leads to reduced SIR (signal-to-interference ratio) in testing, as the network allows ambiguous spectral features to bleed through partially from one source to the other. Interference can be reduced, possibly at the cost of increased artifact, by the use of a discriminative network training criterion. For example, suppose that we define\nJDIS = \u2212(1\u2212 \u03b3) ln p12(y)\u2212 \u03b3D(p12\u2016p21) (8)\nwhere 0 \u2264 \u03b3 \u2264 1 is a regularization constant. p12(y) is the likelihood of the training data under the assumption that the neural net computes the MSE estimate of each feature vector (i.e., its conditional expected value given knowledge of the mixture), and that all residual noise is Gaussian with unit covariance, thus\nln p12(y) = \u2212 1\n2 T\u2211 t=1 ( \u2016y1t \u2212 y\u03031t\u20162 + \u2016y2t \u2212 y\u03032t\u20162 ) (9)\nThe discriminative term, D(p12\u2016p21), is a point estimate of the KL divergence between the likelihood model p12(y) and the model p21(y), where the latter is computed by swapping affiliation of spectra to sources, thus\nD(p12\u2016p21) = 1\n2 T\u2211 t=1 ( \u2016y1t \u2212 y\u03032t\u20162 + \u2016y2t \u2212 y\u03031t\u20162\u2212\n\u2016y1t \u2212 y\u03031t\u20162 \u2212 \u2016y2t \u2212 y\u03032t\u20162 ) (10)\nCombining Eqs. (8)\u2013(10) gives a discriminative criterion with a simple and useful form:\nJDIS = \u2016y1t \u2212 y\u03031t\u20162 + \u2016y2t \u2212 y\u03032t\u20162\u2212 \u03b3\u2016y1t \u2212 y\u03032t\u20162 \u2212 \u03b3\u2016y2t \u2212 y\u03031t\u20162 (11)"}, {"heading": "III. EXPERIMENTS", "text": "In this section, we evaluate our proposed models on a speech separation task, and a singing voice separation task, and a speech denoising task. The source separation evaluation is measured by using three quantitative values: Source to Interference Ratio (SIR), Source to Artifacts Ratio (SAR), and Source to Distortion Ratio (SDR), according to the BSS-EVAL metrics [28]. Higher values of SDR, SAR, and SIR represent better separation quality. The suppression of interference is reflected in SIR. The artifacts introduced by the separation process are reflected in SAR. The overall performance is reflected in SDR. For speech denoising task, we additionally compute the shorttime objective intelligibility measure (STOI) which is a quantitative estimate of the intelligibility of the denoised speech [29]. We use the abbreviations DRNN-k, sRNN to denote the DRNN with the recurrent connection at the k-th hidden layer,\nor at all hidden layers, respectively. We select the models based on the results on the development set. We optimize our models by back-propagating the gradients with respect to the training objectives. The limited-memory Broyden-Fletcher-GoldfarbShanno (L-BFGS) algorithm is used to train the models from random initialization. An example of the separation results is shown in Figure 5. The sound examples are available online.1"}, {"heading": "A. Speech Separation Setting", "text": "We evaluate the performance of the proposed approaches for monaural speech separation using the TSP corpus [31]. In the TSP dataset, we choose four speakers, FA, FB, MC, and MD, from the TSP speech database. After concatenating all 60 sentences per each speaker, we use 90% of the signal for training and 10% for testing. Note that in the neural network experiments, we further divide the training set into 8:1 to set aside 10% of the data for validation. The signals are downsampled to 16kHz, and then transformed with 1024 point DFT with 50% overlap for generating spectra. The neural networks are trained on three different mixing cases: FA versus MC, FA versus FB, and MC versus MD. Since FA and FB are female speakers while MC and MD are male, the latter two cases are expected to be more difficult due to the similar frequency ranges of the same gender. After normalizing the signals to have 0 dB input SNR, the neural networks are trained to learn the mapping between an input mixture spectrum and the the corresponding pair of clean spectra.\nAs for the NMF experiments, 10 to 100 speaker-specific basis vectors are trained from the training part of the signal. The NMF separation is done by fixing the the known speakers\u2019 basis vectors during the test NMF procedure while learning the speaker-specific activation matrices.\nIn the experiments, we explore two different input features: spectral and log-mel filterbank features. The spectral representation is extracted using a 1024-point short time Fourier transform (STFT) with 50% overlap. In the speech recognition literature [32], the log-mel filterbank is found to provide better results compared to mel-frequency cepstral coefficients (MFCC) and log FFT bins. The 40-dimensional log-mel representation and the first and second order derivative features are used in the experiments.\nFor neural network training, in order to increase the variety of training samples, we circularly shift (in the time domain) the signals of one speaker and mix them with utterances from the other speaker."}, {"heading": "B. Speech Separation Results", "text": "We use the standard NMF with the generalized KLdivergence metric using 1024-point STFT as our baselines. We report the best NMF results among models with different basis vectors, as shown in the first column of Figure 6, 7, and 8. Note that NMF uses spectral features, and hence the results in the second row (log-mel features) of each figure are the same as the first row (spectral features).\n1http://www.ifp.illinois.edu/\u223chuang146/DNN separation/demo.zip\nThe speech separation results of the cases, FA versus MC, FA versus FB, and MC versus MD, are shown in Figure 6, 7, and 8, respectively. We train models with two hidden layers of 300 hidden units, where the architecture and the hyperparameters are chosen based on the development set performance. We report the results of single frame spectra and log-mel features in the top and bottom rows of Figure 6, 7, and 8, respectively. To further understand the strength of the models, we compare the experimental results in several aspects. In the second and third columns of Figure 6, 7, and 8, we examine the effect of joint optimization of the masking layer using the DNN. Jointly optimizing masking\nlayer significantly outperforms the cases where a masking layer is applied separately (the second column). In the FA vs. FB case, DNN without joint masking achieves high SAR, but with low SDR and SIR. In the top and bottom rows of Figures 6, 7, and 8, we compare the results between spectral features and log-mel features. In the joint optimization case, (columns 3\u201310), log-mel features achieve better results compared to spectral features. On the other hand, spectral features achieve better results in the case where DNN is not jointly trained with a masking layer, as shown in the first column. In the FA vs. FB and MC vs. MD cases, the log-mel features outperform spectral features greatly.\nBetween columns 3, 5, 7, and 9, and columns 4, 6, 8, and 10 of Figures 6, 7, and 8, we make comparisons between different network architectures, including DNN, DRNN-1, DRNN-2, and sRNN. DRNN-2 and sRNN. In many cases, recurrent neural network models (DRNN-1, DRNN-2, or sRNN) out-\nperfom DNN. Between columns 3 and 4, columns 5 and 6, columns 7 and 8, and columns 9 and 10 of Figures 6, 7, and 8, we compare the effectiveness of using the discriminative training criterion, i.e., \u03b3 > 0 in Eq. (11). In most cases, SIRs are improved. The results match our expectation when we design the objective function. However, it also leads to some artifacts which result in slightly lower SARs in some cases. Empirically, the value \u03b3 is in the range of 0.01\u223c0.1 in order to achieve SIR improvements and maintain reasonable SAR and SDR.\nFinally, we compare the NMF results with our proposed models with the best architecture using spectral and log-mel features in Figure 9. NMF models learn activation matrices from different speakers and hence perform poorly in the same sex speech separation cases, FA vs. FB and MC vs. MD. Our proposed models greatly outperform NMF models for all three cases. Especially for the FA vs. FB case, our proposed model\n1. NMF 2. DRNN+discrim+spectra 3. DRNN+discrim+logmel 6 8\n10 12 14 16 18 20 22 d B\n6.34\n10.36 10.359.24\n14.46 14.79\n7.23\n11.16 11.08\nFemale (FA) vs. Male (MC)\nSDR SIR SAR\n1. NMF 2. DRNN+discrim+spectra 3. DRNN+discrim+logmel 4 6 8\n10 12 14 16 18 d B\n3.58\n6.62 8.56\n5.63\n9.44 11.99\n4.22\n7.98 9.62\nFemale (FA) vs. Female (MB)\nSDR SIR SAR\n1. NMF 2. DRNN+discrim+spectra 3. DRNN+discrim+logmel 4 6 8\n10 12 14 16 18 d B\n3.82 5.93 6.665.51\n8.46 9.59\n7.07 7.47 8.24\nMale (MC) vs. Male (MD)\nSDR SIR SAR\nFig. 9. TSP speech separation result summary, ((a). Female vs. Male, (b). Female vs. Female, and (c). Male vs. Male), with NMF, the best DRNN+discrim architecture with spectra features, and the best DRNN+discrim architecture with logmel features.\nachieve around 5 dB SDR gain compared to the NMF model while maintaining better SIR and SAR."}, {"heading": "C. Singing Voice Separation Setting", "text": "Our proposed system can be applied to signing voice separation tasks, where one source is the singing voice and the other source is the background music. The goal of the task is to separate singing voice from music recordings.\nWe evaluate our proposed system using the MIR-1K dataset [33].2 A thousand song clips are encoded with a sample rate of 16 KHz, with a duration from 4 to 13 seconds. The clips were extracted from 110 Chinese karaoke songs performed by both male and female amateurs. There are manual annotations of the pitch contours, lyrics, indices and types for unvoiced frames, and the indices of the vocal and non-vocal frames; none of the annotations were used in our experiments. Each clip contains the singing voice and the background music in different channels.\nFollowing the evaluation framework in [3], [4], we use 175 clips sung by one male and one female singer (\u2018abjones\u2019 and \u2018amy\u2019) as the training and development set.3 The remaining 825 clips of 17 singers are used for testing. For each clip, we mixed the singing voice and the background music with equal energy (i.e., 0 dB SNR).\nTo quantitatively evaluate source separation results, we report the overall performance via Global NSDR (GNSDR), Global SIR (GSIR), and Global SAR (GSAR), which are the weighted means of the NSDRs, SIRs, SARs, respectively, over all test clips weighted by their length. Normalized SDR (NSDR) is defined as:\nNSDR(v\u0302,v,x) = SDR(v\u0302,v)\u2212 SDR(x,v), (12)\n2https://sites.google.com/site/unvoicedsoundseparation/mir-1k 3Four clips, abjones 5 08, abjones 5 09, amy 9 08, amy 9 09, are used\nas the development set for adjusting the hyper-parameters.\nwhere v\u0302 is the resynthesized singing voice, v is the original clean singing voice, and x is the mixture. NSDR is for estimating the improvement of the SDR between the preprocessed mixture x and the separated singing voice v\u0302.\nFor the neural network training, in order to increase the variety of training samples, we circularly shift (in the time domain) the signals of the singing voice and mix them with the background music. In the experiments, we use magnitude spectra as input features to the neural network. The spectral representation is extracted using a 1024-point short time Fourier transform (STFT) with 50% overlap. Empirically, we found that using log-mel filterbank features or log power spectrum provide worse performance."}, {"heading": "D. Singing Voice Separation Results", "text": "In this section, we compare different deep learning models from several aspects, including the effect of different input context sizes, the effect of different circular shift steps, the effect of different output formats, the effect of different deep recurrent neural network structures, and the effect of the discriminative training objectives.\nFor simplicity, unless mentioned explicitly, we report the results using 3 hidden layers of 1000 hidden units with the mean squared error criterion, joint masking training, and 10K samples as the circular shift step size using features with context window size 3.\nTable I presents the results with different output layer formats. We compare using single source as a target (row 1) and using two sources as targets in the output layer (row 2 and row 3). We observe that modeling two sources simultaneously provides better performance. Comparing row 2 and row 3 in Table I, we observe that using the joint mask training further improves the results.\nTable II presents the results of different deep recurrent neural network architectures (DNN, DRNN with different recurrent connections, and sRNN) with and without discriminative training. We can observe that discriminative training further improves GSIR while maintaining similar GNSDR and GSAR.\nFinally, we compare our best results with other previous work under the same setting. Table III shows the results with unsupervised and supervised settings. Our proposed models achieve 2.30 \u223c 2.48 dB GNSDR gain, 4.32 \u223c 5.42 dB GSIR gain with similar GSAR performance, compared with the RNMF model [3].4\n4We thank the authors in [3] for providing their trained model for comparison."}, {"heading": "E. Speech Denoising Setting", "text": "Our proposed framework can be extended to a speech denoising task as well, where one source is the clean speech and the other source is the noise. The goal of the task is to separate clean speech from noisy speech. In the experiments, we use magnitude spectra as input features to the neural network. The spectral representation is extracted using a 1024- point short time Fourier transform (STFT) with 50% overlap. Empirically, we found that log-mel filterbank features provide worse performance. We use 2 hidden layers of 1000 hidden units neural networks with the mean squared error criterion, joint masking training, and 10K samples as the circular shift step size. The results of different architectures are shown in Figure 10. We can observe that deep recurrent networks achieve similar results compared to deep neural networks. With discriminative training, though SDRs and SIRs are improved, STOIs are similar and SARs are slightly worse.\nTo understand the effect of degradation in the mismatch condition, we set up the experimental recipe as follows. We use a hundred utterances, spanning ten different speakers, from the TIMIT database. We also use a set of five noises: Airport, Train, Subway, Babble, and Drill. We generate a number of noisy speech recordings by selecting random subsets of noises and overlaying them with speech signals. We also specify the signal to noise ratio when constructing the noisy mixtures. After we complete the generation of the noisy signals, we split them into a training set and a test set."}, {"heading": "F. Speech Denoising Results", "text": "In the following experiments, we examine the effect of the proposed methods under different scenarios. We can observe that the recurrent neural network architectures (DRNN-1, DRNN-2, sRNN) achieve similar performance compared to the DNN model. Including discriminative training objectives improves SDR and SIR, but results in slightly degraded SAR and similar STOI values.\nTo further evaluate the robustness of the model, we examine our model under a variety of situations in which it is presented with unseen data, such as unseen SNRs, speakers and noise types. In Figure 11 we show the robustness of this model under\nvarious SNRs. The model is trained on 0dB SNR mixtures and it is evaluated on mixtures ranging from 20 dB SNR to -18dB SNR. We compare the results between NMF, DNN without joint optimization of a masking layer, and DNN with joint optimization of a masking layer. In most cases, DNN with joint optimization achieves the best results. For 20 dB SNR case, NMF achieves the best performance. DNN without joint optimization achieves highest SIR in high SNR cases, though SDR/SAR/STOI are lower.\nNext we evaluate the robustness of the proposed methods for data which is unseen in the training stage. These tests provide a way of understanding the performance of the proposed approach to work when applied on unseen noise and speakers. We evaluate the models with three different cases: (1) the testing noise is unseen in training, (2) the testing speaker is unseen in training, and (3) both the testing noise and testing speaker are unseen in training stage. For the unseen noise case, we train the model on mixtures with Babble, Airport, Train and Subway noises, and evaluate it on mixtures that\ninclude a Drill noise (which is significantly different from the training noises in both spectral and temporal structure). For the unknown speaker case, we hold out some of the speakers from the training data. For the case where both the noise and speaker are unseen, we use the combination of the above.\nWe compare our proposed approach with NMF model and DNN without joint optimizing the masking layer [15]. These experimental results are shown in Figure 12. For the case where the speaker is unknown, we observe that there is only a mild degradation in performance for all models, which means that the approaches can be easily used in speaker variant situations. With the unseen noise we observe a larger degradation in results, which is expected due to the drastically different nature of the noise type. The result is still good enough compared to other NMF and DNN without joint optimizing the masking function. The result of the case where both the noise and the speaker are unknown, the proposed model performs slightly worse compared to DNN without joint optimization with the masking function. Overall, it suggests\nthat the proposed approach is good at generalizing across speakers."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "In this paper, we explore different deep learning architectures, including deep neural networks and deep recurrent neural networks for monaural source separation problems. We further enhance the results by jointly optimizing a soft mask layer with the networks and exploring the discriminative training criteria. We evaluate our proposed method for speech separation, singing voice separation, and speech denoising tasks. Overall, our proposed models achieve 2.30\u223c4.98 dB SDR gain compared to the NMF baseline, while maintaining better SIRs and SARs in the TSP speech separation task. In the MIR-1K singing voice separation task, our proposed models achieve 2.30\u223c2.48 dB GNSDR gain and 4.32\u223c5.42 dB GSIR gain, compared to the previous proposed methods, while maintaining similar GSARs. Moreover, our proposed method also outperforms NMF and DNN baseline in various mismatch conditions in the TIMIT speech denoising task. To further improve the performance, one direction is to further explore using long short-term memory (LSTM) to model longer temporal information [34], which has shown great performance compared to conventional recurrent neural network as avoiding vanishing gradient properties. In addition, our proposed models can also be applied to many other applications such as robust ASR."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by U.S. ARL and ARO under grant number W911NF-09-1-0383. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1053575."}], "references": [{"title": "Singing-voice separation from monaural recordings using robust principal component analysis", "author": ["P.-S. Huang", "S.D. Chen", "P. Smaragdis", "M. Hasegawa-Johnson"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 57\u201360.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural networks for noise reduction in robust ASR", "author": ["A.L. Maas", "Q.V. Le", "T.M. O\u2019Neil", "O. Vinyals", "P. Nguyen", "A.Y. Ng"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time online singing voice separation from monaural recordings using robust low-rank modeling", "author": ["P. Sprechmann", "A. Bronstein", "G. Sapiro"], "venue": "Proceedings of the 13th International Society for Music Information Retrieval Conference, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank representation of both singing voice and music accompaniment via learned dictionaries", "author": ["Y.-H. Yang"], "venue": "Proceedings of the 14th International Society for Music Information Retrieval Conference, November 4-8 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "On sparse and low-rank matrix decomposition for singing voice separation", "author": ["Y.-H. Yang"], "venue": "ACM Multimedia, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Suppression of acoustic noise in speech using spectral subtraction", "author": ["S. Boll"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 27, no. 2, pp. 113\u2013120, Apr 1979.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1979}, {"title": "Speech enhancement using a minimummean square error short-time spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 32, no. 6, pp. 1109\u20131121, Dec 1984.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1984}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999, pp. 50\u201357.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "A probabilistic latent variable model for acoustic modeling", "author": ["P. Smaragdis", "B. Raj", "M. Shashanka"], "venue": "Advances in models for acoustic processing, NIPS, vol. 148, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, pp. 82\u201397, Nov. 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. PP, no. 99, pp. 1\u20131, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Single-channel speech separation with memory-enhanced recurrent neural networks", "author": ["F. Weninger", "F. Eyben", "B. Schuller"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp. 3709\u20133713.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Experiments on deep learning for speech denoising", "author": ["D. Liu", "P. Smaragdis", "M. Kim"], "venue": "Proceedings of the annual conference of the International Speech Communication Association (INTERSPEECH), 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep stacking networks with time series for speech separation", "author": ["S. Nie", "H. Zhang", "X. Zhang", "W. Liu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, May 2014, pp. 6667\u20136671.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Ideal ratio mask estimation using deep neural networks for robust speech recognition", "author": ["A. Narayanan", "D. Wang"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing. IEEE, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards scaling up classification-based speech separation", "author": ["Y. Wang", "D. Wang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7, pp. 1381\u20131390, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "On training targets for supervised speech separation", "author": ["Y. Wang", "A. Narayanan", "D. Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1849\u20131858, Dec 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1849}, {"title": "Deep neural network based speech separation for robust speech recognition", "author": ["Y. Tu", "J. Du", "Y. Xu", "L.-R. Dai", "C.-H. Lee"], "venue": "International Symposium on Chinese Spoken Language Processing, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for single channel source separation", "author": ["E. Grais", "M. Sen", "H. Erdogan"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, May 2014, pp. 3734\u20133738.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning for monaural speech separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 1562\u20131566.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Singing-voice separation from monaural recordings using deep recurrent neural networks", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "International Society for Music Information Retrieval (ISMIR), 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Training and analysing deep recurrent neural networks", "author": ["M. Hermans", "B. Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "International Conference on Learning Representations, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "JMLR W&CP: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2011), 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning for monaural speech separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 14, no. 4, pp. 1462 \u20131469, July 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "An algorithm for intelligibility prediction of time-frequency weighted noisy speech", "author": ["C. Taal", "R. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2125\u20132136, Sept 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "TIMIT: acoustic-phonetic continuous speech corpus", "author": ["J.S. Garofolo", "L.D. Consortium"], "venue": "Linguistic Data Consortium,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1993}, {"title": "Improving wideband speech recognition using mixed-bandwidth training data in CD-DNN-HMM", "author": ["J. Li", "D. Yu", "J.-T. Huang", "Y. Gong"], "venue": "IEEE Spoken Language Technology Workshop (SLT). IEEE, 2012, pp. 131\u2013136.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "On the improvement of singing voice separation for monaural recordings using the MIR-1K dataset", "author": ["C.-L. Hsu", "J.-S. Jang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 2, pp. 310 \u2013319, Feb. 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. PLACE PHOTO HERE Po-Sen Huang Biography text here. Minje Kim Biography text here. Mark Hasegawa-Johnson Biography text here. Paris Smaragdis Biography text here.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "For example, the accuracy of chord recognition and pitch estimation can be improved by separating singing voice from music [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "The accuracy of automatic speech recognition (ASR) can be improved by separating noise from speech signals [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "For example, in the singing voice separation task, several approaches have been proposed to utilize the assumption of the low rank and sparsity of the music and speech signals, respectively [1], [3]\u2013[5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "For example, in the singing voice separation task, several approaches have been proposed to utilize the assumption of the low rank and sparsity of the music and speech signals, respectively [1], [3]\u2013[5].", "startOffset": 195, "endOffset": 198}, {"referenceID": 4, "context": "For example, in the singing voice separation task, several approaches have been proposed to utilize the assumption of the low rank and sparsity of the music and speech signals, respectively [1], [3]\u2013[5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 5, "context": "In the speech denoising task, spectral subtraction [6] subtracts a short-term noise spectrum estimate to generate the spectrum of clean speech.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "By assuming the underlying properties of speech and noise, statistical modelbased methods infer speech spectral coefficients given noisy observations [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "Non-negative matrix factorization (NMF) [8] and probabilistic latent semantic indexing (PLSI) [9], [10] learn the non-negative reconstruction bases and weights of different sources and use them to factorize time-frequency spectral representations.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "Non-negative matrix factorization (NMF) [8] and probabilistic latent semantic indexing (PLSI) [9], [10] learn the non-negative reconstruction bases and weights of different sources and use them to factorize time-frequency spectral representations.", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "Non-negative matrix factorization (NMF) [8] and probabilistic latent semantic indexing (PLSI) [9], [10] learn the non-negative reconstruction bases and weights of different sources and use them to factorize time-frequency spectral representations.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "Recently, deep learning based methods have been used in many applications, including automatic speech recognition [11], image classification [12], etc.", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "Recently, deep learning based methods have been used in many applications, including automatic speech recognition [11], image classification [12], etc.", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "proposed using a deep recurrent neural network (DRNN) for robust automatic speech recognition tasks [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 12, "context": "proposed a deep neural network (DNN)-based speech enhancement system, including global variance equalization and noise-aware training, to predict clean speech spectra for speech enhancement tasks [13].", "startOffset": 196, "endOffset": 200}, {"referenceID": 13, "context": "[14] trained two long-short term memory (LSTM) RNNs for predicting speech and noise, respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "explored using a deep neural network for predicting clean speech signals in various denoising settings [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "utilized deep stacking networks with time series inputs and a re-threshold method to predict the ideal binary mask [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Narayanan and Wang [17] and Wang and Wang [18] proposed a two-stage framework using deep neural networks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "Narayanan and Wang [17] and Wang and Wang [18] proposed a two-stage framework using deep neural networks.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "[19] recently proposed using deep neural networks to train different targets, including ideal ratio mask and FFT-mask, for speech separation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "proposed modeling two sources as the output targets for a robust ASR task [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "[21] proposed using a deep neural network to predict two scores corresponding to the probabilities of two different sources respectively for a given frame of normalized magnitude spectrum.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In this paper, we further extend our previous work in [22] and [23] and propose a general framework for the monaural source separation task for speech separation, singing voice separation, and speech denoising.", "startOffset": 54, "endOffset": 58}, {"referenceID": 22, "context": "In this paper, we further extend our previous work in [22] and [23] and propose a general framework for the monaural source separation task for speech separation, singing voice separation, and speech denoising.", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "To further provide the hierarchical information through multiple time scales, deep recurrent neural networks (DRNNs) are explored [24], [25].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "To further provide the hierarchical information through multiple time scales, deep recurrent neural networks (DRNNs) are explored [24], [25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 24, "context": "The right of Figure 1 is an L intermediate layer DRNN with full temporal connections (called stacked RNN (sRNN) in [25]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 25, "context": "Function \u03c6l(\u00b7) is a nonlinear function, and we empirically found that using the rectified linear unit f(x) = max(0,x) [26] performs better compared to using a sigmoid or tanh function.", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "Since our goal is to separate one of the sources from a mixture, instead of learning one of the sources as the target, we adapt the framework from [27] to model all different sources simultaneously.", "startOffset": 147, "endOffset": 151}, {"referenceID": 0, "context": "Moreover, we find it useful to further smooth the source separation results with a time-frequency masking technique, for example, binary time-frequency masking or soft timefrequency masking [1], [27].", "startOffset": 190, "endOffset": 193}, {"referenceID": 26, "context": "Moreover, we find it useful to further smooth the source separation results with a time-frequency masking technique, for example, binary time-frequency masking or soft timefrequency masking [1], [27].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "The source separation evaluation is measured by using three quantitative values: Source to Interference Ratio (SIR), Source to Artifacts Ratio (SAR), and Source to Distortion Ratio (SDR), according to the BSS-EVAL metrics [28].", "startOffset": 222, "endOffset": 226}, {"referenceID": 28, "context": "For speech denoising task, we additionally compute the shorttime objective intelligibility measure (STOI) which is a quantitative estimate of the intelligibility of the denoised speech [29].", "startOffset": 185, "endOffset": 189}, {"referenceID": 30, "context": "In the speech recognition literature [32], the log-mel filterbank is found to provide better results compared to mel-frequency cepstral coefficients (MFCC) and log FFT bins.", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "We evaluate our proposed system using the MIR-1K dataset [33].", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": "Following the evaluation framework in [3], [4], we use 175 clips sung by one male and one female singer (\u2018abjones\u2019 and \u2018amy\u2019) as the training and development set.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Following the evaluation framework in [3], [4], we use 175 clips sung by one male and one female singer (\u2018abjones\u2019 and \u2018amy\u2019) as the training and development set.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "42 dB GSIR gain with similar GSAR performance, compared with the RNMF model [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "4 4We thank the authors in [3] for providing their trained model for comparison.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Unsupervised Model GNSDR GSIR GSAR RPCA [1] 3.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "09 RPCAh [5] 3.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "10 RPCAh + FASST [5] 3.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "Supervised Model GNSDR GSIR GSAR MLRR [4] 3.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "70 RNMF [3] 4.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "The left/back, middle, right/front bars in each pairs show the results of NMF, DNN without joint optimization of a masking layer [15], and DNN with joint optimization of a masking layer, respectively.", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "Speech denoising experimental results comparison between NMF, NN (without jointly optimizing masking function [15]), and DNN (with jointly optimizing masking function), when used on data that is not represented in training.", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "We compare our proposed approach with NMF model and DNN without joint optimizing the masking layer [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 32, "context": "To further improve the performance, one direction is to further explore using long short-term memory (LSTM) to model longer temporal information [34], which has shown great performance compared to conventional recurrent neural network as avoiding vanishing gradient properties.", "startOffset": 145, "endOffset": 149}], "year": 2015, "abstractText": "Monaural source separation is important for many real world applications. It is challenging in that, given only single channel information is available, there is an infinite number of solutions without proper constraints. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including the monaural speech separation task, monaural singing voice separation task, and speech denoising task. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative training criterion for the neural networks to further enhance the separation performance. We evaluate our proposed system on TSP, MIR-1K, and TIMIT dataset for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30\u223c4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30\u223c2.48 dB GNSDR gain and 4.32\u223c5.42 dB GSIR gain compared to previous models in the singing voice separation task, and outperform NMF and DNN baseline in the speech denoising task.", "creator": "LaTeX with hyperref package"}}}