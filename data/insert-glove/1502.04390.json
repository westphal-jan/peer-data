{"id": "1502.04390", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2015", "title": "Equilibrated adaptive learning rates for non-convex optimization", "abstract": "Parameter - incubation specific paet adaptive learning rate shkolnikova methods are computationally efficient 64-year ways to golmud reduce the sequent ill - conditioning problems undestroyed encountered tblisi when training large deep networks. wbk Following lisztomania recent lakhmids work that strongly makis suggests entertainments that cces most of tofiq the friendswood critical points encountered prng when greenglass training such networks montevina are tonu'u saddle 2242 points, koss we find how ctrl considering collegiates the 4.9375 presence libanaise of negative eigenvalues of 11.91 the cesario Hessian chathams could kak\u00e1 help us immunogenetics design 54.75 better suited adaptive fascisti learning zamloch rate schemes, szkutak i. nechako e. , diagonal preconditioners. magura We show para\u00edba that andrean the berhanu optimal rastus preconditioner 162.1 is based on vern taking assn. the decan absolute value of the Hessian ' s eigenvalues, blethyn which descending is top-flight not what designees Newton and nonlegal classical bundang preconditioners like enamora Jacobi ' s cadenas do. In augustenborg this ortsgemeinde paper, sentimentalism we bobik propose aiwan a mellin novel one-volume adaptive learning lempiras rate spoksman scheme based puttered on the addi equilibration jik preconditioner autoethnography and show that sentayehu RMSProp rogne approximates it, satori which may 7-8th explain boxford some westdale of its 101-102 success in r200 the presence mefistofele of fakher saddle points. pentathlete Whereas RMSProp is three-tenths a callvantage biased wdl estimator khine of ccx the \u00e1ras equilibration petrochemical preconditioner, anurans the braben proposed spadix stochastic estimator, ESGD, bonnichsen is flavorless unbiased matheu and only adds how-to a small percentage to mamizara computing konsthall time. We krawcheck find that halh both breathtaking schemes jwaneng yield very mcternan similar step 05:30 directions barnala but 40,800 that ESGD sometimes surpasses RMSProp in terms of convergence osment speed, always clearly gondi improving over shandon plain stochastic kyoya gradient descent.", "histories": [["v1", "Sun, 15 Feb 2015 23:41:33 GMT  (433kb,D)", "http://arxiv.org/abs/1502.04390v1", null], ["v2", "Sat, 29 Aug 2015 23:04:39 GMT  (456kb,D)", "http://arxiv.org/abs/1502.04390v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yann dauphin", "harm de vries", "yoshua bengio"], "accepted": true, "id": "1502.04390"}, "pdf": {"name": "1502.04390.pdf", "metadata": {"source": "META", "title": "RMSProp and equilibrated adaptive learning rates for non-convex optimization", "authors": ["Yann N. Dauphin", "Harm de Vries", "Junyoung Chung", "Yoshua Bengio"], "emails": ["YANN@DAUPHIN.IO", "MAIL@HARMDEVRIES.COM", "ELECEGG@GMAIL.COM", "BENGIOY@IRO.MONTREAL.CA"], "sections": [{"heading": "1. Introduction", "text": "One of the challenging aspects of deep learning is the optimization of the training criterion over millions of parameters: the difficulty comes from both the size of these neural networks and because the training objective is non-convex in the parameters. Stochastic gradient descent (SGD) has remained the method of choice for most practitioners of neural networks since the 80\u2019s, in spite of a rich literature in numerical optimization and it remains unclear how to\nbest exploit second-order structure of the objective function when training deep networks. One of the questions regarding SGD is how to set the learning rate adaptively, both over time and for different parameters, and several methods have been proposed (Schaul et al., 2013), including the RMSProp method studied in this paper. Because of the large number of parameters, storing the full Hessian (or other full-rank preconditioner) or even a low-rank approximation is not practical, while these adaptive learning rate methods can be seen as estimating a diagonal preconditioner.\nOn the other hand, recent work (Dauphin et al., 2014; Choromanska et al., 2014) has brought theoretical and empirical evidence suggesting that local minima are with high probability not the main obstacle to optimizing large and deep neural networks, contrary to what was previously believed: instead, saddle points are the most prevalent critical points on the optimization path (except when we approach the value of the global minimum). These saddle points can considerably slow down training, mostly because the objective function tends to be flat in many directions and ill-conditioned in the neighborhood of these saddle points. This raises the question: can we take advantage of the saddle structure to design good and computationally efficient preconditioners? This suggests diagonal preconditioners, i.e. specifying how to set local learning rates adaptively.\nIn this paper, we bring these two threads together: we first show that RMSProp provides a biased estimator of the diagonal of the absolute value of the Hessian (Section 4), which would correspond to the equilibration preconditioner. We then show that when the objective function is non-convex, the inverse of the absolute value of the Hessian is the best possible preconditioner (Section 5), being as good or better than using the Hessian (Newton\u2019s method). Practically, we find that when there are negative eigenvalues of the Hessian, the equilibration preconditioner is much more efficient at improving the condition number. This last result also serves as a justification for the method presented by Dauphin et al. (2014), which however involved\nar X\niv :1\n50 2.\n04 39\n0v 1\n[ cs\n.L G\n] 1\n5 Fe\nb 20\neither the full Hessian or a low-rank approximation. Bringing these two contributions together justifies exploring estimators of the equilibration preconditioner such as RMSProp, and suggests that such preconditioners are particularly well-suited to efficiently improve the condition number when the objective function is non-convex, in the neighborhood of ill-conditioned saddle points. However, understanding the way in which RMSProp estimates the quantity of interest and the bias it incurs in doing so suggests that better estimators could be devised. Section 6 proposes such an unbiased alternative to estimate the equilibration preconditioner. In Section 7 we setup experiments to validate the above claims, and Section 8 presents the results, which confirm the claims. In addition, a surprising observation was made: these preconditioners yield trajectories that are very different from SGD; they go through much better behaved paths where the Hessian is dominated much more by its diagonal. This may be because they can tolerate more saturated hidden units, corresponding to stronger non-linearities, which is consistent with the observed ability of RMSProp and of the new stochastic equilibration variant, equilibrated SGD (ESGD), to train neural nets to a point that corresponds to strongly non-linear input-output relationships, compared to SGD training."}, {"heading": "2. Preconditioning", "text": "The loss functions of neural networks notoriously exhibit pathological curvature (Martens, 2010). The loss surface may be highly curved in some directions while being flat in others. This is problematic for gradient descent since a single learning rate is not suitable for all search directions. A high learning rate will do well for directions of low curvature, but will diverge for those of high curvature. Some methods have recently been proposed to tune a separate learning rate for each parameter but they assume convexity of the function (Zeiler, 2012; Duchi et al., 2011). It is not clear that applying these methods to neural networks will produce good results.\nPreconditioning is a geometric solution to the problem of pathological curvature that does not require convexity. Preconditioning aims to locally transform the optimization landscape so that its curvature is equal in all directions, as illustrated in Figure 1 for the equilibration preconditioner studied here. Consider the problem of minimizing the function over parameters \u2206\u03b8 \u2208 RN\nf(\u03b8 + \u2206\u03b8) = f(\u03b8) +\u2207fT\u2206\u03b8 + \u2206\u03b8TH\u2206\u03b8.\nThe critical points are the solutions to the secant equation \u2207f(\u03b8 + \u2206\u03b8) = \u2207f + H\u2206\u03b8 = 0. It is clear that multiplying each side by a non-singular matrix D\u22121 does not change the solution. This transformed function has derivative D\u22121\u2207f and Hessian D\u22121H. The key idea of preconditioning is to choose a matrix D that reduces the condition number of the transformed Hessian D\u22121H, such that a gradient descent update on this transformed surface\n\u03b8t = \u03b8t\u22121 + D\u22121\u2207f(\u03b8t\u22121).\nis close to the optimum of the quadratic approximation. One perfect preconditioning matrix is H\u22121 which gives us the famous Newton step. However, computing this matrix exactly is usually intractable for neural networks, and therefore diagonal approximations are typically preferred. Such diagonal preconditioners are commonly known as adaptive learning rates in the neural network literature.\nThe most well-known approximation is the Jacobi preconditioner where H is approximated by its diagonal\nDJacobi = |diag(H)|.\nwhere | \u00b7 | is the absolute value. There has been some success in applying this preconditioner to neural networks using a Gauss-Newton approximation of the Hessian (LeCun et al., 2012). However, the Hessian of neural networks are most likely indefinite and the Jacobi has not been found to be competitive for indefinite matrices (Bradley & Murray, 2011). The evidence for the presence of both positive eigenvalues and negative eigenvalues (i.e., for saddle points) on the training trajectory of deep neural networks is both theoretical and empirical (Dauphin et al., 2014; Choromanska et al., 2014). In this paper, we will investigate the qualities that make a good preconditioner for deep neural networks. As discussed in Section 5, preconditioners which approximate the diagonal of the Hessian are not appropriate in the neighborhood of saddle points, whereas the best preconditioners in this region involve taking the absolute value of the Hessian."}, {"heading": "3. Equilibration", "text": "Equilibration is a type of preconditioning that has been found to work well for indefinite matrices (Bradley & Murray, 2011). A matrix H is row equilibrated if all its rows\nare of equal norm. This can be achieved by the diagonal preconditioning matrix with entries\nDEquilibrationi = \u2016Hi,\u00b7\u2016.\nFor the 2-norm, equilibration can be reformulated as DEquilibration = \u221a diag(H2).\nwhich reveals its relation to the absolute value of the Hessian: changing the order of the square root and diagonal gives us the diagonal of the absolute Hessian diag( \u221a H2). As we discuss in Section 5, this link can help explain the success of equilibration for indefinite matrices, and clarifies the relation between equilibration and the Jacobi preconditioner. The key difference resides in the use of H2 to gather curvature information instead of H. Like the absolute value of the Hessian, the squared Hessian does not have negative curvature, and this may allow equilibration to side-step some of the problems associated with non-convexity. One issue with indefinite preconditioning matrices is that they can transform directions of descent into directions of ascent.\nThe equilibration matrix can be estimated using the matrixfree estimator (Hv)2 where the square is element-wise and v \u223c N (0, 1). As shown by Bradley & Murray (2011), this estimator is unbiased, i.e.,\ndiag(H2) = E[(Hv)2]. (1)\nSince multiplying the Hessian by a vector can be done efficiently without ever computing the Hessian, this method can be efficiently used in the context of neural networks using the R-operator (Schraudolph, 2002). The Roperator computation only uses gradient-like computations and costs about the same as two backpropagations. We will adopt this estimator to propose a stochastic equilibration preconditioner for neural networks, called ESGD."}, {"heading": "4. RMSProp approximates equilibration", "text": "In this section we uncover an unsuspected link between RMSProp and equilibration. RMSProp is an adaptative learning rate method that has found much success in practice (Tieleman & Hinton, 2012; Korjus et al.; Carlson et al., 2015). Tieleman & Hinton (2012) propose to normalize the gradients by an exponential moving average of the magnitude of the gradient for each parameter:\nvt = \u03b1vt\u22121 + (1\u2212 \u03b1)(\u2207f)2 (2)\nwhere 0 < \u03b1 < 1 denotes the decay rate. The update step is given by\n\u03b8t = \u03b8t\u22121 + \u2207f(\u03b8t\u22121) \u221a vt + \u03bb .\nwhere is the learning rate and \u03bb is a damping factor. Although it is observed in practice that normalizing gradients helps the optimization process, there was until this paper no published theoretical justification for RMSProp. Here, we show that RMSProp is in fact a biased estimator of the equilibration preconditioner.\nWe observe that the vector vt estimates the expectation\nvt \u2248 E[(\u2207f(\u03b8t))2] (3)\nWe will see that this expectation gathers curvature information related to equilibration similarly to Equation 1. Consider the N -dimensional function\nf(\u03b8 + \u2206\u03b8) = f(\u03b8) +\u2207fT\u2206\u03b8 + 1 2 \u2206\u03b8TH\u2206\u03b8.\nwhich we take to be quadratic for simplicity. A nonquadratic function will incur an approximation error. Differentiating this quantity by \u2206\u03b8 gives us the secant equation \u2207f(\u03b8 + \u2206\u03b8) = \u2207f(\u03b8) + H\u2206\u03b8 which can be re-ordered to give us an expression for the product\nH\u2206\u03b8 = \u2207f(\u03b8 + \u2206\u03b8)\u2212\u2207f(\u03b8).\nWithout loss of generality we take the root of the Taylor expansion to be the nearest critical point \u03b8\u2217, i.e., with the property \u2207f(\u03b8\u2217) = 0, hence\nH\u2206\u03b8t\u2217 = \u2207f(\u03b8\u2217 + \u2206\u03b8t\u2217)\u2212\u2207f(\u03b8\u2217). = \u2207f(\u03b8\u2217 + \u2206\u03b8t\u2217) = \u2207f(\u03b8t) (4)\nwhere \u2206\u03b8t\u2217 = \u03b8t \u2212 \u03b8\u2217 and \u03b8t is the value of the parameters at the timestep t. Note that this is a re-ordering of the familiar Newton step \u2206\u03b8t\u2217 = H\u22121\u2207f(\u03b8t).\nCombining this with the expression of RMSProp (Equation 3) we have vt = E[(\u2207f(\u03b8t))2] = E[(H\u2206\u03b8t\u2217)2]. This is the form of the estimator for equilibration described in Equation 1. The main difference is that elements of the vector \u2206\u03b8t\u2217 may not be Gaussian distributed with zero mean\nand unit variance, in which case, RMSProp will incur a bias in its estimation of the equilibration preconditioner. However, we have found the bias to be comparatively small in practice even for very high-dimensional problems (Figure 4). The reason we believe that elements of \u2206\u03b8t\u2217 are randomly distributed is two-fold. First, the sampling noise introduced by SGD lets RMSProp jump in random directions around the saddle point. Second, the nature of gradient descent also causes random fluctuations around the critical point \u03b8\u2217 as pictured in Figure 2. This effect is especially pronounced in directions of high curvature and it is the reason why methods like momentum can be effective. This same phenomenon is what helps RMSProp gather curvature information. Since \u2206\u03b8t\u2217 = \u03b8t \u2212 \u03b8\u2217 is centered at the critical point this causes \u2206\u03b8t\u2217 to oscillate roughly randomly. Thus RMSProp can be thought as a biased estimator of equilibration and this may explain some of its success in optimizing in the presence of negative eigenvalues of the Hessian, as is the case when training neural networks (Dauphin et al., 2014; Choromanska et al., 2014)."}, {"heading": "5. The absolute Hessian is better for non-convex problems", "text": "Here we will explain why using the absolute value of the Hessian instead of the Hessian is useful for non-convex problems. First, we show that the absolute value of the Hessian is the only symmetric positive definite ideal preconditioner. Second, we will show theoretically that the absolute value of the Hessian provides a better second order step than the Hessian. Third, we will show that when approximating the Hessian to obtain curvature information you will consistently underestimate the curvature in a given direction. This can lead to taking a completely wrong step. This problem is avoided completely by the absolute value of the Hessian."}, {"heading": "5.1. The symmetric positive-definite preconditioner", "text": "A preconditioner D is called ideal if it maximally reduces the condition number \u03ba(HD) = 1. Another useful property for a preconditioner to have is to be positive definite. If the preconditioner is not positive definite then multiplying it by the gradient can actually flip the sign of the gradient. This can make the optimizer take steps that increase the objective instead of decreasing it. The absolute value of the Hessian is unique in that it is the only symmetric positive definite preconditioner.\nTheorem 1. Let H be a symmetric non-singular matrix in RN\u00d7N . Then it holds that the matrix |H|\u22121 is the only symmetric positive definite ideal preconditioner.\nProof. By definition the condition number is given by\n\u03ba(AM) = \u2016(AM)\u22121\u2016\u2016AM\u2016.\nwhere M is a symmetric preconditioning matrix and \u2016 \u00b7\u2016 is the Frobenius norm without loss of generality. We can recover the ideal preconditioning by minimizing the squared condition number over M, giving us\n= \u2202tr((M\u22121A\u22121)2) \u2202M \u2016AM\u20162 + \u2016M\u22121A\u22121\u20162 \u2202tr((A TMT )2) \u2202M = \u2212M\u22122A\u22122M\u22121\u2016AM\u20162 + \u2016M\u22121A\u22121\u20162MA2,\nSetting to zero on the left side we have\nM\u22122A\u22122M\u22121 = MA2\u2016M\u22121A\u22121\u20162\u2016AM\u2016\u22122\nM\u22122A\u22122 = MA2M\u2016M\u22121A\u22121\u20162\u2016AM\u2016\u22122.\nMA2M is symmetric so we have M\u22122A\u22122 = A\u22122M\u22122. This implies that A and M are commuting matrices and they must have the same eigenvectors. \u2016M\u22121A\u22121\u2016 = \u2016AM\u2016\u22121 = \u03b1 is just a scaling factor. Leveraging this we have M\u22122A\u22122 = M2A2. Proving M = 4 \u221a A\u22124 = |A|\u22121, the other solutions all differ by sign flips of the absolute value of the eigenvalues."}, {"heading": "5.2. Comparison to the Newton step", "text": "The fact that the absolute value of the Hessian is positive definite and the Hessian may be indefinite directly affects optimization. The theoretical study of non-convex optimization is still very much an open question. It is not clear how to show a faster convergence rate for a non-convex problem. However, we can show that at each step the absolute value of the Hessian decreases the function equally or more than the Newton step. This difference arises because an indefinite Hessian will lead to the wrong step in directions of negative curvature. The result is given by Theorem 2.\nLemma 1. Let A be a non-singular square matrix in RN\u00d7N , and x \u2208 RN be some vector. Then it holds that x>Ax \u2264 x>|A|x, where |A| is the matrix obtained by taking the absolute value of each of the eigenvalues of A.\nProof. Let q1, . . .qN be the different eigenvectors of A and \u03bb1, . . . \u03bbN the corresponding eigenvalues. We have x>Ax = \u2211 i \u03bbi(x >qi) 2. It holds that\u2211\ni \u03bbi(x >qi) 2 \u2264 | \u2211 i \u03bbi(x >qi) 2| and by the triangular\ninequality |x>Ax| \u2264 \u2211 i |\u03bbi(x>qi)2| which is equal to x>|A|x.\nLemma 2. Let A be a non-singular square matrix in RN\u00d7N , and v \u2208 RN be some vector. Then it holds that \u2016Av\u2016 = \u2016|A|v\u2016, where |A| is the matrix obtained by taking the absolute value of each of the eigenvalues of A.\nProof. The norm is given by \u2016Av\u20162 = \u2016 \u2211N i \u03bbiqiq T i v\u20162 where \u03bbi is the eigenvalue of the eigenvector qi of A. The eigenvectors are orthogonal thus \u2016( \u2211N i \u03bbiqiq T i v) T ( \u2211N i \u03bbiqiq\nT i v)\u20162 simplifies to\u2211N\ni \u2016\u03bbi(qiqTi )v\u20162. We can pull the eigenvalues out of the squared norms giving us \u2211N i \u03bb 2 i \u2016(qiqTi )\u2207f\u20162. Squaring the eigenvalues is invariant to the sign, which proves that the proposition.\nTheorem 2. Let the matrix H \u2208 RN\u00d7N be the Hessian of the function f with gradient \u2207f . Then it holds that taking a gradient step with |H|\u22121 minimizes the function equally or better than the Newton step, that is f(\u03b8 \u2212 |H|\u22121\u2207f) \u2264 f(\u03b8 \u2212H\u22121\u2207f).\nProof. We can express any function f using its Taylor expansion\nf(\u03b8+ \u2206\u03b8) = f(\u03b8) +\u2207fT\u2206\u03b8+ 1 2 \u2206\u03b8TH\u2206\u03b8+ o(\u2016\u2206\u03b8\u20163).\nThe Newton step for this function is given by \u2206\u03b8N = \u2212H\u22121\u2207f and the step with the absolute Hessian is \u2206\u03b8A = \u2212|H|\u22121\u2207f . We can recover the error after the Newton step by taking \u2206\u03b8 = \u2206\u03b8N in the previous equation\nf(\u03b8+\u2206\u03b8N ) = f(\u03b8)+\u2207fT\u2206\u03b8N+ 1\n2 \u2206\u03b8TNH\u2206\u03b8N+o(\u2016\u2206\u03b8N\u20163).\nFrom Lemma 2 we have o(\u2016\u2206\u03b8N\u20163) = o(\u2016\u2206\u03b8A\u20163), and we observe that\n\u2206\u03b8TNH\u2206\u03b8N = \u2206\u03b8 TH\u22121HH\u22121\u2206\u03b8\n= \u2206\u03b8TH\u2206\u03b8\n= \u2206\u03b8T |H|\u22121H|H|\u22121\u2206\u03b8 = \u2206\u03b8TAH\u2206\u03b8A.\nThis gives us\nf(\u03b8+\u2206\u03b8N ) = f(\u03b8)+\u2207fT\u2206\u03b8N+ 1\n2 \u2206\u03b8TAH\u2206\u03b8A+o(\u2016\u2206\u03b8A\u20163).\nFurthermore, it follows from Lemma 1 that\n\u2207fT\u2206\u03b8N = \u2212\u2206\u03b8TH\u22121\u2206\u03b8 \u2265 \u2212\u2206\u03b8T |H|\u22121\u2206\u03b8 = \u2207fT\u2206\u03b8A.\nThen it must hold that f(\u03b8\u2212|H|\u22121\u2207f) \u2264 f(\u03b8\u2212H\u22121\u2207f).\nThis formalizes the intuitions of (Dauphin et al., 2014). Thus the use of the absolute Hessian is justified for second order methods. As we shall see this also extends to even crude approximations of the second order step."}, {"heading": "5.3. Better approximation of the curvature", "text": "As argued below, Krylov subspace approximations based on indefinite Hessians will tend to underestimate curvature. Therefore, even for approximate methods it is preferable to work with a quantity close to the absolute Hessian. Take the diagonal Jacobi approximation\nDJacobi = \u221a diag(H)2.\nThe elements DJacobii estimate the curvature of the euclidean axis V = I. More precisely, they are equivalent to the Raleigh quotient in the direction of each axis direction DJacobii = |R(H,Vi)| = | \u2211N j \u03bbjq 2 ji| where \u03bbi and qi are the eigenvalues and eigenvectors of H. We can see that the terms associated with negative eigenvalues \u03bbi will cancel the positive terms. This can gravely underestimate the curvature and lead to bad preconditioning near saddle points. Specifically, this makes high curvature directions have even more curvature. This is problematic because of the proliferation of saddle points in neural networks (Dauphin et al., 2014).\nThis problem is avoided by equilibration methods like RMSProp. The curvature information\nDEquilibration = \u221a diag(H2)\nis given by the Raleigh quotient of the squared Hessian DEquilibration = \u221a R(H2,Vi) = \u221a\u2211 j \u03bb 2 jq 2 ji. Thus the eigenvalues are all positive and will not cancel, ensuring that the preconditioner will not enlarge already high curvature directions. We observe experimentally that the elements in the Jacobi matrix are much closer to 0 than the equilibration matrix. This suggests that negative curvature does cancel out other curvatures in practice. This explains why equilibration has been found to work well for indefinite matrices (Bradley & Murray, 2011).\nAs a first step, we have verified this claim experimentally for random neural networks. The neural networks have 1 hidden layer of a 100 sigmoid units with zero mean unitvariance Gaussian distributed inputs, weights and biases. The output layer is a softmax with the target generated randomly. We also give results for similarly sampled logistic regressions. We compare the capacity of the Jacobi and equilibration preconditioners to reduce the condition number of the Hessian. Reducing the condition number of the Hessian makes optimization much faster and it has been found to be very important in practice. In the convex case, the condition number is tied directly to the convergence rate (Wright & Nocedal, 1999). LeCun et al. (2012) proposed a Gauss-Newton approximation to the Jacobi preconditioner for neural networks. However, it is arguably less successful in the literature than RMSProp. We will see that this can be attributed to the better preconditioning properties given by\napproximating the absolute Hessian. Figure 3 gives the histograms of the condition number reductions. We obtained these graphs by sampling a hundred networks and computing the ratio of the condition number before and after preconditioning. On the left we have the convex case, and on the right the non-convex case. We clearly observe that the Jacobi and equilibration method are closely matched for the convex case. However, in the non-convex case the Jacobi does not properly handle negative curvature and we see a striking difference between the two methods. This confirms that properly addressing the negative curvature that arises near saddle points is beneficial. As we will see in Section 8 these results extend to practical high-dimensional problems."}, {"heading": "6. Implementation", "text": "We propose to build a scalable algorithm for preconditioning neural networks using equilibration. This method will estimate the same curvature information \u221a diag(H2) with the unbiased estimator described in Equation 1. It is prohibitive to compute the full expectation at each learning step. Instead we will simply update our running average at each learning step much like RMSProp. The pseudo-code is given in Algorithm 1. The cost of this is one product with the Hessian which is roughly the cost of two additional gradient calculations and the cost of sampling a vector from a random Gaussian. In practice we greatly amortize the cost by only performing the update every 20 iterations. This brings the cost of equilibration very close to that of regular SGD. The only added hyper-parameter is the damping \u03bb. We find that a good setting for that hyper-parameter is \u03bb = 10\u22122 and it is robust over the tasks we considered.\nIn the interest of comparison, we will evaluate SGD preconditioned with the Jacobi preconditioner. This will al-\nAlgorithm 1 Equilibrated Gradient Descent Require: Function f(\u03b8) to minimize, learning rate and\ndamping factor \u03bb D\u2190 0 for i = k \u2192 K do\nv \u223c N (0, 1) D\u2190 D + (Hv)2 \u03b8 \u2190 \u03b8 \u2212 \u2207f(\u03b8)\u221a D/k+\u03bb\nend for\nlow us to verify the claims that the equilibration preconditioner is better suited for non-convex problems. Bekas et al. (2007) show that the diagonal of a matrix can be recovered by the expression\ndiag(H) = E[v Hv] (5)\nwhere v are random vectors with entries \u00b11 and is the element-wise product. We use this estimator to precondition SGD in the same fashion as that described in Algorithm 1. The computational complexity is the same as equilibrated SGD."}, {"heading": "7. Experimental setup", "text": "We aim to confirm experimentally the theoretical results proposed in the previous sections. First, we measure how well RMSProp estimates the equilibration matrix\u221a\ndiag(H2). Second, we evaluate the equilibrated SGD proposed in Algorithm 1. Finally, we want to confirm that there is a significant difference in performance between the Jacobi preconditioner and equilibration methods for highdimensional non-convex problems.\nIn these experiments, we consider the challenging opti-\nmization benchmark of training very deep neural networks. Following (Martens, 2010; Sutskever et al., 2013; Vinyals & Povey, 2011), we train deep auto-encoders which have to reconstruct their input under the constraint that one layer is very low-dimensional. This makes the reconstruction task difficult because it requires the optimizer to finely tune the parameters. The low-dimensional bottleneck layer learns a low-dimensional map similar in principle to non-linear Principal Component Analysis. The networks have up to 11 layers of sigmoidal hidden units and have on the order of a million parameters. We use the standard network architectures described in (Martens, 2010) for the MNIST and CURVES dataset. Both of these datasets have 784 input dimensions and 60,000 and 20,000 examples respectively.\nWe tune the hyper-parameters of the optimization methods with random search. We have sampled the learning rate from a logarithmic scale between [0.1, 0.01] for stochastic gradient descent (SGD) and equilibrated SGD (ESGD). The learning rate for RMSProp and the Jacobi preconditioner are sampled from [0.001, 0.0001]. We note that the learning rate for RMSProp is smaller than ESGD because the RMSProp vector is multiplied by the small variance E[(\u2206\u03b8t\u2217j )\n2]. The damping factor \u03bb used before dividing the gradient is taken from either {10\u22124, 10\u22125, 10\u22126} while the exponential decay rate of RMSProp is taken from either {0.9, 0.95}. The networks are initialized using the sparse initialization described in (Martens, 2010). We initialize 15 connections per neuron with a zero mean unit variance Gaussian and the others are set to zero. The minibatch size for all methods in 200. We do not make use of momentum in these experiments in order to evaluate the strength of each preconditioning method on its own. Similarly we do not use any regularization because we are only concerned with optimization performance. For these reasons, we report training error in our graphs.\nThe networks and algorithms were implemented using Theano (Bastien et al., 2012), simplifying the use of the Roperator in Jacobi and equilibrated SGD. All experiments were run on GPU\u2019s."}, {"heading": "8. Results", "text": ""}, {"heading": "8.1. Measuring the bias of RMSProp", "text": "In Section 4 we argued that under certain conditions RMSProp estimates the diagonal of the absolute Hessian. In this section we verify to what extent these conditions are valid, and experimentally determine the bias of RMSProp in estimating the equilibration matrix. We train deep autoencoders with RMSProp and measure every 10 epochs the equilibration matrix DEquilibration = \u221a diag(H2) and\nJacobi matrix DJacobi = \u221a\ndiag(H)2 using 100 samples of the unbiased estimators described in Equations 1, respectively. We then measure the pairwise differences between these quantities in terms of the cosine distance cosine(u, v) = 1 \u2212 u\u00b7v\u2016u\u2016\u2016v\u2016 , which measures the angle between two vectors and ignores their norms. Note further that the cosine distance ranges from 0 (zero degree angle) to 2 (180 degrees angle), and that the cosine distance between two preconditioners should not exceed one for preconditioned SGD to converge.\nFigure 4 shows the resulting cosine distances over training on MNIST and CURVES. For the latter dataset we observe that RMSProp remains remarkably close (around 0.05) to equilibration, while it is significantly different from Jacobi (in the order of 0.2). The same order of difference is observed when we compare equilibration and Jacobi, confirming the observations of Section 5 that both quantities are rather different in practice. For the MNIST dataset we see that RMSProp fairly well estimates \u221a diag(H)2 in the beginning of training, but then quickly diverges. After 1000\nepochs this difference has exceeded the difference between Jacobi and equilibration, and RMSProp no longer successfully estimates the equilibration. Interestingly, at the same time that RMSProp starts diverging, we observe in Figure 5 that also the performance of the optimizer drops in comparison to ESGD. This suggests that the success of RMSProp as a optimizer is tied to its success in estimating the equilibration matrix."}, {"heading": "8.2. Comparison of preconditioned SGD methods", "text": "We present the optimization curves of SGD and its preconditioned variants in Figure 5. We observe that the preconditioned methods beat SGD on both problems. Our results for MNIST show that the proposed equilibrated SGD significantly outperforms both RMSProp and Jacobi SGD, and the difference in performance becomes especially notable after 250 epochs. The gap with classical SGD is even more substantial, and we observe a convergence speed that is approximately three times faster. ESGD also performs best for CURVES, although the difference with RMSProp and Jacobi SGD is not as significant as for MNIST. These results correlate very well with the observation in the previous section. Figure 4 shows that the Jacobi is closer to equilibration on CURVES compared to MNIST. This explains their closer performance during optimization and may be because the Hessian on CURVES has less negative curvature. These graphs also justify the use of RMSProp as a preconditioning method."}, {"heading": "8.3. Measuring diagonal dominance", "text": "We also measured the degree of diagonal dominance\nd(H) =\n\u221a\u2211 iH 2 ii\n\u2016H\u2016F \u00b7 100 (6)\nof the Hessian during training. Fig. 6 shows the results on MNIST for the preconditioned SGD methods. Remarkably, there is a significant degree of diagonal dominance.\nIt exceeds 15% for all optimization methods, which is bigger than we would expect from a random symmetric matrix. For example, if all elements of a matrix were independently drawn from a distribution with zero mean and unit variance, then the expected degree of diagonal dominance is E[d(H)] = 1\u221a\nN \u00b7 100 where N is the dimension of the\nparameters. For the million parameters of our deep autoencoders, the expected degree of diagonal dominance is only 0.01%. The Hessian is thus considerably diagonally dominant, which is good news for diagonally preconditioned SGD methods, since most ill-conditioning can then be removed by the diagonal of the absolute Hessian.\nAnother surprising observation from Fig. 6 is that the preconditioned SGD methods follow a very different optimization trajectory than SGD. They have tendency to move to part of the parameter space where the Hessian is much more diagonally dominant. We speculate that preconditioned SGD methods, in contrast to standard SGD, are still able to optimize in regions of parameter space which correspond to many saturated hidden units."}, {"heading": "9. Conclusion", "text": "We have proposed a new preconditioning method for neural networks dubbed equilibrated SGD (ESGD), which we found to be closely related to RMSProp. We have shown that this method approximates the absolute value of the Hessian and enjoys better performance on non-convex problems."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "An estimator for the diagonal of a matrix", "author": ["Bekas", "Costas", "Kokiopoulou", "Effrosyni", "Saad", "Yousef"], "venue": "Applied numerical mathematics,", "citeRegEx": "Bekas et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bekas et al\\.", "year": 2007}, {"title": "Matrixfree approximate equilibration", "author": ["Bradley", "Andrew M", "Murray", "Walter"], "venue": "arXiv preprint arXiv:1110.2805,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Stochastic spectral descent for restricted boltzmann machines", "author": ["Carlson", "David", "Cevher", "Volkan", "Carin", "Lawrence"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2015}, {"title": "The loss surface of multilayer", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "Grard Ben", "LeCun", "Yann"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in highdimensional non-convex optimization", "author": ["Dauphin", "Yann", "Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": "In NIPS\u20192014,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In ICML\u20192010, pp", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Unit tests for stochastic optimization", "author": ["Schaul", "Tom", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1312.6055,", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Schraudolph", "Nicol N"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph and N.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph and N.", "year": 2002}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Krylov subspace descent for deep learning", "author": ["Vinyals", "Oriol", "Povey", "Daniel"], "venue": "arXiv preprint arXiv:1111.4259,", "citeRegEx": "Vinyals et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2011}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "One of the questions regarding SGD is how to set the learning rate adaptively, both over time and for different parameters, and several methods have been proposed (Schaul et al., 2013), including the RMSProp method studied in this paper.", "startOffset": 163, "endOffset": 184}, {"referenceID": 5, "context": "On the other hand, recent work (Dauphin et al., 2014; Choromanska et al., 2014) has brought theoretical and empirical evidence suggesting that local minima are with high probability not the main obstacle to optimizing large and deep neural networks, contrary to what was previously believed: instead, saddle points are the most prevalent critical points on the optimization path (except when we approach the value of the global minimum).", "startOffset": 31, "endOffset": 79}, {"referenceID": 4, "context": "On the other hand, recent work (Dauphin et al., 2014; Choromanska et al., 2014) has brought theoretical and empirical evidence suggesting that local minima are with high probability not the main obstacle to optimizing large and deep neural networks, contrary to what was previously believed: instead, saddle points are the most prevalent critical points on the optimization path (except when we approach the value of the global minimum).", "startOffset": 31, "endOffset": 79}, {"referenceID": 5, "context": "This last result also serves as a justification for the method presented by Dauphin et al. (2014), which however involved ar X iv :1 50 2.", "startOffset": 76, "endOffset": 98}, {"referenceID": 8, "context": "The loss functions of neural networks notoriously exhibit pathological curvature (Martens, 2010).", "startOffset": 81, "endOffset": 96}, {"referenceID": 6, "context": "Some methods have recently been proposed to tune a separate learning rate for each parameter but they assume convexity of the function (Zeiler, 2012; Duchi et al., 2011).", "startOffset": 135, "endOffset": 169}, {"referenceID": 7, "context": "There has been some success in applying this preconditioner to neural networks using a Gauss-Newton approximation of the Hessian (LeCun et al., 2012).", "startOffset": 129, "endOffset": 149}, {"referenceID": 5, "context": ", for saddle points) on the training trajectory of deep neural networks is both theoretical and empirical (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 106, "endOffset": 154}, {"referenceID": 4, "context": ", for saddle points) on the training trajectory of deep neural networks is both theoretical and empirical (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 106, "endOffset": 154}, {"referenceID": 3, "context": "RMSProp is an adaptative learning rate method that has found much success in practice (Tieleman & Hinton, 2012; Korjus et al.; Carlson et al., 2015).", "startOffset": 86, "endOffset": 148}, {"referenceID": 3, "context": "; Carlson et al., 2015). Tieleman & Hinton (2012) propose to normalize the gradients by an exponential moving average of the magnitude of the gradient for each parameter:", "startOffset": 2, "endOffset": 50}, {"referenceID": 5, "context": "Thus RMSProp can be thought as a biased estimator of equilibration and this may explain some of its success in optimizing in the presence of negative eigenvalues of the Hessian, as is the case when training neural networks (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 223, "endOffset": 271}, {"referenceID": 4, "context": "Thus RMSProp can be thought as a biased estimator of equilibration and this may explain some of its success in optimizing in the presence of negative eigenvalues of the Hessian, as is the case when training neural networks (Dauphin et al., 2014; Choromanska et al., 2014).", "startOffset": 223, "endOffset": 271}, {"referenceID": 5, "context": "This formalizes the intuitions of (Dauphin et al., 2014).", "startOffset": 34, "endOffset": 56}, {"referenceID": 5, "context": "This is problematic because of the proliferation of saddle points in neural networks (Dauphin et al., 2014).", "startOffset": 85, "endOffset": 107}, {"referenceID": 7, "context": "LeCun et al. (2012) proposed a Gauss-Newton approximation to the Jacobi preconditioner for neural networks.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Bekas et al. (2007) show that the diagonal of a matrix can be recovered by the expression", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Following (Martens, 2010; Sutskever et al., 2013; Vinyals & Povey, 2011), we train deep auto-encoders which have to reconstruct their input under the constraint that one layer is very low-dimensional.", "startOffset": 10, "endOffset": 72}, {"referenceID": 11, "context": "Following (Martens, 2010; Sutskever et al., 2013; Vinyals & Povey, 2011), we train deep auto-encoders which have to reconstruct their input under the constraint that one layer is very low-dimensional.", "startOffset": 10, "endOffset": 72}, {"referenceID": 8, "context": "We use the standard network architectures described in (Martens, 2010) for the MNIST and CURVES dataset.", "startOffset": 55, "endOffset": 70}, {"referenceID": 8, "context": "The networks are initialized using the sparse initialization described in (Martens, 2010).", "startOffset": 74, "endOffset": 89}, {"referenceID": 0, "context": "The networks and algorithms were implemented using Theano (Bastien et al., 2012), simplifying the use of the Roperator in Jacobi and equilibrated SGD.", "startOffset": 58, "endOffset": 80}], "year": 2015, "abstractText": "Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes, i.e., diagonal preconditioners. We show that the optimal preconditioner is based on taking the absolute value of the Hessian\u2019s eigenvalues, which is not what Newton and classical preconditioners like Jacobi\u2019s do. In this paper, we propose a novel adaptive learning rate scheme based on the equilibration preconditioner and show that RMSProp approximates it, which may explain some of its success in the presence of saddle points. Whereas RMSProp is a biased estimator of the equilibration preconditioner, the proposed stochastic estimator, ESGD, is unbiased and only adds a small percentage to computing time. We find that both schemes yield very similar step directions but that ESGD sometimes surpasses RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.", "creator": "LaTeX with hyperref package"}}}