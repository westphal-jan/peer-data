{"id": "1502.05988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Deep Learning for Multi-label Classification", "abstract": "disgorged In multi - anaerobe label classification, the calexico main lvm focus zinner has trusses been to develop ways blackland of learning s-5 the underlying virostko dependencies between cipro labels, mmorgansfchronicle.com and sesame to take 28.44 advantage dulness of retests this acadian at werdegar classification paccent time. retevision Developing olinga better feature - gr\u00f6nemeyer space gas-cooled representations has splattered been predominantly employed adoptees to 16/34 reduce complexity, incuding e. g. , by eliminating chhapra non - berlinski helpful feature attributes backhoes from boom-boom the input metalious space prior to (or during) training. This fisherville is an 123-foot important lipowa task, since matheu many multi - qv label bandleader methods giacosa typically dipeptidase create jenner many different cornel copies fabricio or sp1 views mercenaries of sathorn the pilar same morfogen input 2-man data 100-share as uralvagonzavod they khondaker transform furtherance it, fleuriot and hemoptysis considerable ramis memory desecrate can be w10 saved fujio by bixi taking advantage sassetti of exams redundancy. In saloonkeeper this paper, bentgrass we eight-minute show that a proper development of the feature trpa1 space can make 89-85 labels less interdependent frankston and barha easier catchings to noncompetitive model and predict at inference time. countersunk For nyk this task we use a gtlds deep learning clemmer approach with pottawattamie restricted latifi Boltzmann machines. chilmanov We present oresund a deep network vassilieva that, in an empirical sebaceous evaluation, accusation outperforms hannut a lunae number shiyi of competitive masse methods slumped from blade the literature", "histories": [["v1", "Wed, 17 Dec 2014 12:06:47 GMT  (339kb,D)", "http://arxiv.org/abs/1502.05988v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jesse read", "fernando perez-cruz"], "accepted": false, "id": "1502.05988"}, "pdf": {"name": "1502.05988.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Multi-label Classification", "authors": ["Jesse Read", "Fernando Perez-Cruz"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMulti-label classification is the supervised learning problem where an instance may be associated with multiple labels. This is opposed to the traditional task of single-label classification (i.e., multi-class, or binary) where each instance is only associated with a single class label. The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, audio data, still images and video, and bioinformatics, [12], [22], [23] and the references therein.\nThe most well-known approach to multi-label classification is to simply train an independent classifier for each label. This is usually known in the literature as the binary relevance (BR) transformation, e.g., [22], [15]. Essentially, a multi-label problem is transformed into one binary problem for each label and any off-the-shelf binary classifier is applied to each of these problems individually. Practically all the multi-label literature identifies that this method is limited by the fact that dependencies between labels are not explicitly modelled and proposes algorithms to take these dependencies into account.\nTo date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4]. These methods make many copies of the feature space in memory (or make many passes over it). Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].\nThat is to say, most competitive methods from the large part of the literature could benefit tremendously from more concise representations of the feature space, relatively much more so than in the singe-label context; the initial investment in reducing the number of feature variables in a multi-label problem is much more likely to offer considerable speed-ups\nduring learning and classification. However, relatively little work in the multi-label literature has considered this approach.\nUsing the raw instance data to construct a model makes the implicit assumption that the labels originate from this data and that they can be recovered directly from it. Usually, however, both the labels and the feature variables originate from particular abstract concepts. For example, we generally think of an image as being labelled beach, not because its pixel-data vector is beach-like, but rather because the image itself meets some criteria of our abstract idea of what a beach is. Ideally then, a feature set would include (for example) variables for a grainy surface such as sand or pebbles, and for being adjacent to a (significant) body of water. Hence, it is highly desirable to recover the hidden dependencies and structure from the original concepts behind the learning task. A good representation of these dependencies make the problem easier to learn.\nA Restricted Boltzmann Machine (RBM) [9] learns a layer of hidden features in an unsupervised fashion. This hidden layer can capture complex dependencies and structure from the input space, and represent it more compactly (whenever the number of hidden units is smaller than the number of original feature attributes). The methods we detail in this paper using RBMs offer some interesting benefits to multilabel classification in a variety of domains:\n\u2022 The predictive performance of existing state-of-the-art methods is generally improved. \u2022 Many classification paradigms previously relatively uncompetitive in multi-label learning can often obtain much higher predictive performance and become competitive and thus now offer their respective advantages to this context, such as better posterior-probability estimates, lower memory consumption, faster performance, easier implementation, and incremental learning. \u2022 The output feature space can be updated incrementally. This not only makes incremental learning feasible, but also means that cost savings are magnified for batchlearners that need to be retrained at intervals on new data. \u2022 The model can be built using unlabeled examples, which are typically obtained much more cheaply than labelled examples; especially in multi-label contexts, since examples are assigned multiple labels.\nWe also stack several RBMs to create two varieties of Deep Belief Networks (DBNs). We look at two approaches using DBNs. In a first approach, we learn the final layer together with the labels and use an existing multi-label classifier. In a second approach, we use back-propagation to fine-tune the weights of our neural network for discriminative prediction, and augment this with a second multi-label predictive layer.\nWe develop a framework to experiment with RBMs and DBNs in a variety of multi-label classification contexts. Within\nar X\niv :1\n50 2.\n05 98\n8v 1\n[ cs\n.L G\n] 1\n7 D\nec 2\n01 4\n2 this framework we carry out an empirical evaluation with many different methods from the literature, on a collection of real-world datasets from diverse domains (to the best of our knowledge, this is also the largest and varied collection of datasets analysed with an RBM framework). The results indicate the benefits of this style of learning for multi-label classification."}, {"heading": "II. PRIOR WORK", "text": "Multi-label datasets and classification methods have rapidly become more numerous in recent years, and classification performance has steadily improved. An overview of the most well known and influential work in this area is provided in [22], [12].\nThe binary relevance approach (BR) does not obtain high predictive performance because it does not model dependencies between labels. A number of methods have improved on this predictive performance with methods that do model label dependence.\nA well-known alternative is the label powerset (LP) method [23] which transforms the multi-label problem into singlelabel problem with a single class, having the powerset as the set of values (i.e., all possible 2L combinations). In LP, label dependencies are modelled directly and predictive performance is greater than BR, but computational complexity is too high for most practical applications. The complexity issue has been addressed in works such as [24] and [13]. The former presents RAkEL (RAndom k-labEL sets), an ensemble method that selects m subsets of k labels and uses LP to learn each of these subproblems.\nThe classifier chain approach (CC) [15] has received recent attention, for example in [3] and [26]. This method employs one classifier for each label, like BR, but the classifiers are not independent. Rather, each classifier predicts the binary relevance of each label given the input space plus the predictions of the previous classifiers (hence the chain).\nAnother type of binary-classification approach is the pairwise transformation method (PW), where a binary model is trained for each pair of labels. The predictions result more naturally in a set of pairwise preferences than a multi-label prediction (thus becoming popular in ranking schemes), but PW methods can be adapted to make multi-label predictions, for example [5]. These methods performs well in several domains, although their application can easily be prohibitive on many datasets due to its quadratic complexity.\nAn alternative to problem transformation is algorithm adaptation, where a specific single-label method is adapted directly for multi-label classification. MLkNN [30] is a k-nearest neighbours method adapted for multi-label learning by voting from the labels found in the neighbours. IBLR is a related method that also incorporates a second layer of logistic regression. BPMLL [29] is a back-propagation neural network adapted for multi-label classification by having multiple binary outputs as the label variables.\nProcessing the feature space of multi-label data has already been studied in the literature. [20] presents an overview of the main techniques with respect to problem transformation\nmethods. In [27] a clustering-based supervised approach is used to obtain label-specific features for each label. The advantages of this method are reduced where label-relevances are not trained separately, for example in LP methods (which learns all labels together as a single multi-class meta label). In any case, this a meta technique that can easily be applied independently of other preprocessing and learning techniques, such as the one we describe in this paper.\nIn [25] redundancy is eliminated from the learning space of the BR method by taking random subsets of the training space across an ensemble. This work centers on the fact that a standard BR approach considers the full input space for each label, even though only a subset of the variables may be relevant to any particular label. Compressive sensing techniques have also been used in the literature for reducing the complexity multi-label data by taking advantage of label sparsity [21], [11].\nThese methods are mainly motivated by reducing an algorithm\u2019s running-time by reducing the number of feature variables in the input space, rather than learning or modelling the dependencies between them. More examples of featurespace reduction for multi-label classification are reviewed in [22].\nThe authors of [7] use a fully-connected network closely related to a Boltzmann machine for multi-label classification, using Gibbs sampling for inference. They use this network to model dependencies in the label space for prediction, rather than to improve the feature space. Since this is a fully connected network, it is tractable only for problems with a relatively small number of labels.\nFigure 1 roughly illustrates the way some of the different classifiers model correlations among attributes and labels, assuming a linear base classifier.\n3"}, {"heading": "III. DEEP LEARNING WITH RESTRICTED BOLTZMANN MACHINES", "text": "A well-known approach to deep learning is to model each layer of higher level features in a restricted Boltzmann machine [9]. We base our approaches on this strategy."}, {"heading": "A. Preliminaries", "text": "In all that follows: X \u2282 Rd is the input domain of all possible feature values. An instance is represented as a vector of d feature values x = [x1, . . . , xd]. The set \u0141 = {\u03bb1, . . . , \u03bbL} is the output domain of L possible labels. Each instance x is associated with a subset of these labels Y \u2286 \u0141 typically represented by a binary vector y = [y1, . . . , yL], where yj = 1 \u21d4 \u03bbj \u2208 Y ; i.e., yj = 1 if and only if the jth label is associated with instance x, and 0 otherwise.\nWe assume a set of training data of N labelled examples {(xi,yi)}Ni=1; yi is the label vector (labelset) assignment of the ith example; y(i)j is the relevance of the jth label to the ith example.\nIn the BR context, for example, L binary classifiers h1, . . . , hL are trained, where each hj models the binary problem relating to the jth label, such\ny\u0302 = h(x\u0303)\n[y1, . . . , yL] = h1(x\u0303), . . . , hL(x\u0303)\noutputs prediction vector y\u0302 \u2208 {0, 1}L for any test instance x\u0303."}, {"heading": "B. Restricted Boltzmann Machines", "text": "A Boltzmann machine is a type of fully-connected neural network that can be used to discover the underlying regularities of the (observed) training data [1]. When many features are involved, this type of network is only tractable in the restricted Boltzmann machine setting [9], where units are fully connected between layers, but are unconnected within layers.\nAn RBM learns a layer of u hidden feature variables from the original d feature variables of a training set (usually u < d). These hidden variables can provide a compact representation of the underlying patterns and structure of the input. In fact, an RBM can capture 2u input space regions, whereas standard clustering requires O(2u) parameters and examples to capture this much complexity.\nFigure 2 shows an RBM can as a graphical model with two sets of nodes: visible (X-variables, shaded) and hidden (Zvariables). Each Xj is connected to all Zk|k = 1, . . . , u by weight Wjk (the same for both directions).\nRBMs are energy-based models, where the joint probability of visible and hidden units is proportional to the energy between them:\nP (x, z) \u221d e\u2212E(x,z).\nHence, by manipulating the energy E we can in turn generate the probability P (x, z). Specifically, we minimize the energy\nE(x, z) = \u2212xW z\nby learning the weight matrix W to find low energy states. Contrastive divergence [8] is typically used for this task."}, {"heading": "C. Deep Belief Networks", "text": "RBMs can be stacked to form so-called DBNs [9]. The RBMs are trained greedily: the first RBM takes the input space X and produces output Z(1), then the second RBM treats Z(1) as if it were the input space, and produces Z(2), and so on and so forth.\nWhen used for single-label classification, the final output layer is typically a softmax function, (which is appropriate where only one of the output units should be on, to indicate one of K classes). In the following section we outline our approach, creating DBNs suitable for multi-label classification."}, {"heading": "IV. DEEP BELIEF NETWORKS (DBNS) FOR MULTI-LABEL CLASSIFICATION", "text": "Ideally, an RBM would produce hidden variables that correspond directly to the label variables, and thus we could recover the label vector directly given any input vector; i.e., y \u2261 z(`) or deterministically mappable z(`) 7\u2192 y. Unfortunately, this is seldom the case, because the abstract hidden variables do not need to correspond directly to the labels. However, we should expect the hidden layer of data to be more closely related to the labels than the original data, and thus it makes sense to use it as a feature space to classify instances.\nHence, by using the hidden space created by the RBM, we would expect any multi-label classifier to obtain better performance (than when using the original feature space). We do this simply by using the hidden representation of each instance as the input feature space, and associating it with the labels to create training set {(zi,yi)}Ni=1. We can then train any multi-label classifier h on this dataset. To evaluate a test instance x\u0303, we feed it through the RBM and obtain z\u0303 from the upper layer, and then acquire a prediction y\u0302 = h(z\u0303), and thus so for each test instance.\nFrom here we take two approaches. Since the sub-optimality produced by greedy learning is not necessarily harmful to many discriminative supervised methods [10], we can treat the final hidden layer variables Z` as the feature input variables, and train any off-the-shelf multi-label model h that can predict\ny\u0302 = h(z\u0303`)\nwhere z\u0303` is produced by the RBM for some test instance x\u0303; see Figure 3a.\nIn a second approach, we add a final layer of weights W (`) on top; see Figure 3b. Now, the structure is similar\n4 to the neural network of BPMLL [29], except that create the layers and initialize the weights using RBMs. Later we will show that our methods performs much better. We can employ back propagation to fine-tune the network in a supervised fashion (with respect to label assignments) as in, for example, [9] (for single-label classification). For a number of epochs, each training instance xi is propagated forward (upward) through the network and output as the prediction y\u0302i. The errors i = yi \u2212 y\u0302i are then propagated backward through the network, updating the weights (previously initialized by the RBMs). Due to the initialisation with RBMs, far fewer epochs are required than would usually be typical for back propagation (and we actually observed that more than around 100 epochs tends to result in overfitting).\nOn both these approaches it is possible to add more depth in the form including an additional classification layer. In the multi-label context, this has previously been done to the basic BR method in [6], where a second BR is trained on the outputs of the first (a stacking approach). A related technique in the neural network context, often called a \u201cskip layer\u201d has been used in, e.g., [19], [16]. In our case we allow for generic classifiers. This helps add some further discriminative power for taking into account the dependencies in the label space.\nNote that we have also experimented with a DBN that models the instance space and label space together generatively P (x,y, z). In the multi-label setting this complicates the inference, since there are 2L possible y. We tried using Gibbs sampling, but could not obtain competitive results from this model in the multi-label setting compared to our other approaches (even after reducing x in an RBM first). However, this seems like an interesting direction, and we intend to follow this idea further in future work."}, {"heading": "V. EXPERIMENTS", "text": "We carry out an empirical evaluation to gauge the effectiveness and efficiency of RBMs and DBNs in a number of different multi-label classification scenarios, using different learning algorithms and a wide collection of databases. We have implemented these methods in the MEKA framework1; an open-source Java-based framework with a number of important benchmark multi-label methods. In this framework RBMs can easily be used in a wide variety of multi-label schemes. The source code of our implementations will be made available as part of the MEKA framework.\nWe selected commonly-used datasets from a variety of domains, listed in Table I along with some basic statistics about them. The datasets vary considerably with respect to the type of data, and their dimensions (the number of labels, features, and examples). In Music, instances of music are associated with emotions; in Scene, images belong to categories; in Yeast proteins may be associated with multiple biological functions, and in Genbase gene sequences. Medical, Enron and Reuters are text datasets where text documents are associated with categories. These datasets are described in greater detail in [12]."}, {"heading": "A. RBM performance", "text": "We first compare the performance of introducing an RBM, blindly trained, for reducing the input dimension and then try out three of the common paradigms in multi-label classification (namely BR, LP and PW) to test the improvements proposed for this feature extraction algorithm. The RBM would improve the performance of the multi-label classification paradigms, if the extracted features are relevant for better describing the task at hand and will be neutral or negative if those features that have been extracted blindly do not correspond with relevant features for assigning labels.\nThe RBM has several parameters that need to be fine-tuned (i.e. number of hidden units, learning rate and momentum) and we use three-fold cross validation to set them. We considered the number of hidden units u \u2208 {30, 60, 120, 240}, the learning rate \u03b7 \u2208 {0.1, 0.01, 0.001}, and momentum \u03b1 \u2208 {0.2, 0.4, 0.8}. We used weight costs of 2 \u00b7 10\u22125 and E = 1000 epochs throughout.\n1http://meka.sourceforge.net\n5 1) Ensemble of Classifier Chains: CC is a competitive BR method that uses the chain rule to improve the prediction for each potential label. As it is unclear what should be the best ordering, we use an ensemble of 50 CC, in which the labels are randomly ordered in each realization (as in [15]). In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:\naccuracy = 1\nN N\u2211 i=1 |yi \u2227 y\u0302i| |yi \u2228 y\u0302i| ,\nwhere \u2227 and \u2228 are the bitwise AND and OR functions, respectively, for {0, 1}L \u00d7 {0, 1}L \u2192 {0, 1}L.\nIn Table IIa, ECCR and ECC, respectively, denote the accuracy of the ECC with the RBM-generated features and with the original input space. We have used two different classifiers: nonlinear SVM and logistic regression (linear classifier), both of them have been trained with the default parameters in WEKA. It can be seen that the for the logistic regression classifier the achieved accuracy with the generated features by the RBM are significantly better for the Music, Scene, Enron, Reuters datasets, it only underperforms for the Medical dataset, and they are comparable for Yeast and Genbase datasets. The RBM not only reduces the dimensionality of the input space for the classifier, but it also makes the features suitable for linear classifiers, which allows interpreting the RBM\n2There are a variety of multi-label evaluation measures used in multi-label experiments in the literature; [22] provides an overview of some of the most popular. The accuracy provides a good balance to gauge the overall predictive performance of multi-label methods [12], [15].\nfeatures and understand how each one of them participate in the prediction for each label.\nFor the SVM-based ECC classifiers there is not a significant difference when we use the RBM processed features compared to using the raw data directly, as the RBF kernel in the SVM can compensate for the preprocessing done by the RBM. In this case, almost all the results are comparable, except for the Scene and Medical, in which, respectively, the ECCR and ECC outperform. We should remark that the linear logistic regression is as good as the nonlinear SVM in most cases, so it seams that using the RBM features reduces the input dimension and makes the classification problem easier, as a linear classifier performs as well as a state-of-the-art nonlinear classifier.\nIn Figure 4 we show the accuracy for the seven data bases for the ECC and ECCR multi-label classifier with an SVM classifier, as a function of the number of hidden units of the RBM. In this plot, it can be seen that once we have enough features, using the RBM is comparable to not using it and it is clear that for the Medical the number of features is too little and we would have needed to increase the number of extracted features3 to achieved the same performance as the SVM does.\nFinally, in Table III we show the accuracy for the SVMbased classifier for the Scene dataset for all the tested combinations of the learning rate and the momentum, in which the number of hidden units is fixed to 120. The accuracy for the ECC (without RBM generated features) is 0.695 and in this case any combination of learning rate and momentum does better, which indicates that with a sufficient number of hidden units, the RBM learning is quite robust and not overly sensitive to hyperparameter settings.\n2) RAndom K labEL subsets: RAkEL is a truncated power set method in which we try all combinations for 3 labels and we report an ensemble with 2L classifiers. We use the same hyperparameter setting as we did for the ECC to make the\n3We did not do so, to keep the experimental setting uniform for all proposed methods, as we think it is important that hyper-parameter setting should be general and not finely tuned for each application.\n6 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\n0 20 40 60 80 100 120 140 160 180 200\nMusic Music Medical Medical\nFig. 5: The difference in accuracy (shown here on Music and Medical datasets) between baseline BR (dashed lines) and more-advanced CC (solid lines) \u2013 both built on RBM-produced outputs \u2013 decreases with more hidden units (horizontal axis). For \u03b7 = 0.1, \u03b1 = 0.1.\nresults comparable across multi-label classification paradigms, as reported in Table IIb and we report the acuracy in Table IV.\nThe results for this paradigm are similar to the ones that we reported for the ECC in the previous section. For the logistic regression (a linear classifier) the RBM generated features lend themselves for accurate predictions when compared with the unprocessed features with the same baseline classifier and they are comparable to the results achieved for the nonlinear SVM classifier. After processing the features with an RBM we might not need to rely on a nonlinear classifier. For the SVM using the RBM generated features does not help, but it does not hurt either, in terms of accuracy, as the SVM nonlinear mapping\nis versatile to learn any nonlinear mapping. 3) Pairwise Classification: We implemented a pairwise approach, namely Four-class pairWise classifier (FW), in which we build models to learn classes yjk \u2208 {00, 01, 10, 11} for each label pair 1 \u2264 j < k \u2264 L, dividing each into votes for the individual labels y\u0302j and y\u0302k and using a threshold at classification time. We find that overall it obtains better predictive performance than the pairwise methods that create decision boundaries between labels (where yjk \u2208 {01, 10}), as in [5], for example, especially with SVMs. We report the accuracy in Table V, using the same hyper parameters as we did for the ECC to make the results comparable across multi-label classification paradigms, as reported in Table IIb.\nThe conclusions are similar to the other two paradigms. The linear classifier (logistic regression) does significantly better with the RBM generated features than with the original input space, while the SVM nonlinear classifier is versatile enough to provide accurate predictions with or without RBM generated features. Fortunately, the linear classifier with RBM generated features is quite close to the SVM-based classifier and allows to interpret which RBM features contribute to each label, hence we can provide intuitive interpretations for each RBM features, while it is hard to get such interpretation from the SVM nonlinear mapping."}, {"heading": "B. DBN performance", "text": "After analyzing the performance of the RBM generated features, we focus on two DBN structures for multi-label classification:\n\u2022 DBN2ECC: a network of two hidden layers, the final of which is united with the labels in a new dataset and trained with ECC (see Figure 3a) \u2022 DBN3bp: a network of three hidden layers where the final layer represents the labels; fine-tuned with back propagation (see Figure 3b) Both setups can be visualised in Figure 6, where h \u2261W ` in the case of DBN3bp.\nWe use u = d/5 hidden units, 1000 RBM epochs, 100 BP epochs (on DBN3bp), and the best of either \u03b1 = 0.8, \u03bb = 0.1 and \u03b1 = 0.8, \u03bb = 0.1 on a 67:33 percent internal train/test validation (taking advantage of the fact, as we explained earlier, that the choice of learning rate and momentum is fairly robust given enough hidden units).\n7 y1, . . . , yL\nz1, . . . , zu\nz1, . . . , zu\nx1, . . . , xd W1\nW2\nh\nFig. 6: A deep learning setup for multi-label classification.\nIn Table VI, we compare the accuracy for the proposed DBMs structures and the previously proposed methods. We have also added MLkNN, BPMLL, and IBLR (see Section II for details). In this table we can see that the DBN2ECC is either the best classifier or close to the best, which give sense that the features generated by the second layer improve the first layer. For example, the only database (Medical) in which the ECCR was not good enough compared to the ECC now the DBN2ECC and DBN 3 bp do almost as good as ECC and the performance on the other databases is also improved (or not degraded). This structure seems to be amenable for multi-label classification and competitive with all the proposed paradigms in the literature."}, {"heading": "VI. CONCLUSIONS", "text": "Our empirical evaluation over a variety of multi-label datasets shows that a selection of high-performing multi-label methods from the literature can be improved upon by using an RBM-processed feature space. The labels become easier to model at training time, and predict at inference time. We obtained an improvement of up to 15 percentage points in accuracy than when using the original feature space directly. Our study showed that important improvements can be obtained in multi-label classification with respect to both scalability and predictive performance when using deep learning in the area of multi-label classification. As a result, we can recommend to multi-labellers to focus more on feature modelling, rather than solely on modelling dependencies between the output labels. Our multi-label DBN models achieved the best predictive performance overall compared with seven competing methods from the multi-label literature."}], "references": [{"title": "A learning algorithm for boltzmann machines", "author": ["David H. Ackley", "Geoffrey E. Hinton", "Terrence J. Sejnowski"], "venue": "Cognitive Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["Weiwei Cheng", "Krzysztof Dembczy\u0144ski", "Eyke H\u00fcllermeier"], "venue": "In ICML\u201910: 27th International Conference on Machine", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Combining instance-based learning and logistic regression for multilabel classification", "author": ["Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Multilabel classification via calibrated label ranking", "author": ["Johannes F\u00fcrnkranz", "Eyke H\u00fcllermeier", "Eneldo Loza Menc\u0131\u0301a", "Klaus Brinker"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Discriminative methods for multi-labeled classification", "author": ["Shantanu Godbole", "Sunita Sarawagi"], "venue": "Eighth Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Multi-label classification using conditional dependency networks", "author": ["Yuhong Guo", "Suicheng Gu"], "venue": "In IJCAI \u201911: 24th International Conference on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey Hinton"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey Hinton", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Multilabel prediction via compressed sensing", "author": ["Daniel Hsu", "Sham M. Kakade", "John Langford", "Tong Zhang"], "venue": "In NIPS \u201909: Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Scalable Multi-label Classification", "author": ["Jesse Read"], "venue": "PhD thesis, University of Waikato,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Multi-label classification using ensembles of pruned sets", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoff Holmes"], "venue": "In ICDM \u201908: Eighth IEEE International Conference on Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Classifier chains for multi-label classification", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoff Holmes", "Eibe Frank"], "venue": "In ECML \u201909: 20th European Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Classifier chains for multi-label classification", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoffrey Holmes", "Eibe Frank"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Neural networks and related methods for classification", "author": ["B.D. Ripley"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["Robert E. Schapire", "Yoram Singer"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Predicting gene function using hierarchical multi-label decision tree ensembles", "author": ["Leander Schietgat", "Celine Vens", "Jan Struyf", "Hendrik Blockeel", "Dragi Kocev", "Saso Dzeroski"], "venue": "BMC Bioinformatics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Feedforward nets for interpolation and classification", "author": ["Eduardo D. Sontag"], "venue": "J. Comp. Syst. Sci,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "A comparison of multi-label feature selection methods using the problem transformation approach", "author": ["Newton Spolar", "Everton Alvares Cherman", "Maria Carolina Monard", "Huei Diana Lee"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Multi-label classification with principle label space transformation", "author": ["Farbound Tai", "Hsuan-Tien Lin"], "venue": "In Workshop Proceedings of Learning from Multi-Label", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Mining multi-label data", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "Data Mining and Knowledge Discovery Handbook", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Multi label classification: An overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Random k-labelsets: An ensemble method for multilabel classification", "author": ["Grigorios Tsoumakas", "Ioannis P. Vlahavas"], "venue": "In ECML \u201907: 18th European Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Model-shared subspace boosting for multi-label classification", "author": ["Rong Yan", "Jelena Tesic", "John R. Smith"], "venue": "In KDD \u201907: 13th ACM SIGKDD International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Bayesian chain classifiers for multidimensional classification", "author": ["Julio H. Zaragoza", "Luis Enrique Sucar", "Eduardo F. Morales", "Concha Bielza", "Pedro Larra\u00f1aga"], "venue": "In IJCAI\u201911: 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "LIFT: Multi-label learning with label-specific features", "author": ["Min-Ling Zhang"], "venue": "In IJCAI\u201911: 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Multi-label learning by exploiting label dependency", "author": ["Min-Ling Zhang", "Kun Zhang"], "venue": "ACM SIGKDD International conference on Knowledge Discovery and Data mining,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "ML-KNN: A lazy learning approach to multi-label learning", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}], "referenceMentions": [{"referenceID": 11, "context": "The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, audio data, still images and video, and bioinformatics, [12], [22], [23] and the references therein.", "startOffset": 177, "endOffset": 181}, {"referenceID": 21, "context": "The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, audio data, still images and video, and bioinformatics, [12], [22], [23] and the references therein.", "startOffset": 183, "endOffset": 187}, {"referenceID": 22, "context": "The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, audio data, still images and video, and bioinformatics, [12], [22], [23] and the references therein.", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": ", [22], [15].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [22], [15].", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 220, "endOffset": 223}, {"referenceID": 4, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 225, "endOffset": 228}, {"referenceID": 13, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 230, "endOffset": 234}, {"referenceID": 23, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 236, "endOffset": 240}, {"referenceID": 3, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 242, "endOffset": 245}, {"referenceID": 13, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 162, "endOffset": 166}, {"referenceID": 27, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 185, "endOffset": 189}, {"referenceID": 24, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "A Restricted Boltzmann Machine (RBM) [9] learns a layer of hidden features in an unsupervised fashion.", "startOffset": 37, "endOffset": 40}, {"referenceID": 21, "context": "An overview of the most well known and influential work in this area is provided in [22], [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "An overview of the most well known and influential work in this area is provided in [22], [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "A well-known alternative is the label powerset (LP) method [23] which transforms the multi-label problem into singlelabel problem with a single class, having the powerset as the set of values (i.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "The complexity issue has been addressed in works such as [24] and [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "The complexity issue has been addressed in works such as [24] and [13].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "The classifier chain approach (CC) [15] has received recent attention, for example in [3] and [26].", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "The classifier chain approach (CC) [15] has received recent attention, for example in [3] and [26].", "startOffset": 86, "endOffset": 89}, {"referenceID": 25, "context": "The classifier chain approach (CC) [15] has received recent attention, for example in [3] and [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "The predictions result more naturally in a set of pairwise preferences than a multi-label prediction (thus becoming popular in ranking schemes), but PW methods can be adapted to make multi-label predictions, for example [5].", "startOffset": 220, "endOffset": 223}, {"referenceID": 29, "context": "MLkNN [30] is a k-nearest neigh-", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "BPMLL [29] is a back-propagation neural network adapted for multi-label classification by having multiple binary outputs as the label variables.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "[20] presents an overview of the main techniques with respect to problem transformation methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In [27] a clustering-based supervised approach is used to obtain label-specific features for each label.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [25] redundancy is eliminated from the learning space of the BR method by taking random subsets of the training space across an ensemble.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Compressive sensing techniques have also been used in the literature for reducing the complexity multi-label data by taking advantage of label sparsity [21], [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Compressive sensing techniques have also been used in the literature for reducing the complexity multi-label data by taking advantage of label sparsity [21], [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 21, "context": "More examples of featurespace reduction for multi-label classification are reviewed in [22].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "The authors of [7] use a fully-connected network closely related to a Boltzmann machine for multi-label classification, using Gibbs sampling for inference.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "A well-known approach to deep learning is to model each layer of higher level features in a restricted Boltzmann machine [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "A Boltzmann machine is a type of fully-connected neural network that can be used to discover the underlying regularities of the (observed) training data [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "When many features are involved, this type of network is only tractable in the restricted Boltzmann machine setting [9], where units are fully connected between layers, but are unconnected within layers.", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Contrastive divergence [8] is typically used for this task.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "RBMs can be stacked to form so-called DBNs [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "Since the sub-optimality produced by greedy learning is not necessarily harmful to many discriminative supervised methods [10], we can treat the final hidden layer variables Z as the feature input variables, and train any off-the-shelf multi-label model h that can predict", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "to the neural network of BPMLL [29], except that create the layers and initialize the weights using RBMs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "We can employ back propagation to fine-tune the network in a supervised fashion (with respect to label assignments) as in, for example, [9] (for single-label classification).", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "In the multi-label context, this has previously been done to the basic BR method in [6], where a second BR is trained on the outputs of the first (a stacking approach).", "startOffset": 84, "endOffset": 87}, {"referenceID": 18, "context": ", [19], [16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [19], [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "These datasets are described in greater detail in [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "As it is unclear what should be the best ordering, we use an ensemble of 50 CC, in which the labels are randomly ordered in each realization (as in [15]).", "startOffset": 148, "endOffset": 152}, {"referenceID": 22, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 58, "endOffset": 61}, {"referenceID": 14, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "2There are a variety of multi-label evaluation measures used in multi-label experiments in the literature; [22] provides an overview of some of the most popular.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The accuracy provides a good balance to gauge the overall predictive performance of multi-label methods [12], [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The accuracy provides a good balance to gauge the overall predictive performance of multi-label methods [12], [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "We find that overall it obtains better predictive performance than the pairwise methods that create decision boundaries between labels (where yjk \u2208 {01, 10}), as in [5], for example, especially with SVMs.", "startOffset": 165, "endOffset": 168}], "year": 2015, "abstractText": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multilabel methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature.", "creator": "LaTeX with hyperref package"}}}