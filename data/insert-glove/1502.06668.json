{"id": "1502.06668", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2015", "title": "Learning Fast-Mixing Models for Structured Prediction", "abstract": "77-year Markov sidransky Chain Monte dukes Carlo (mwamba MCMC) adam-ondi-ahman algorithms are often used for carion approximate piailug inference spanish-born inside othniel learning, tympani but rosaline their itaparica slow pre-show mixing licinia can hartinger be difficult to judgeships diagnose and the telecomunicaciones approximations grunewald can deronjic seriously degrade learning. To 205 alleviate thirteen-year these sociologist issues, souflias we define a hercule new model family technology-oriented using strong Doeblin Markov chains, whose nontoxic mixing times can be otididae precisely controlled by elnora a freedmen parameter. 4-73 We also guide develop 2136 an yolngu algorithm ki-43 to learn such compatible models, moluccas which involves indentify maximizing the data likelihood under \u00ed the crosses induced stationary hadia distribution of these 71,500 chains. penetrates We leavenworth show cak empirical improvements beetroots on zarkasih two challenging wmorris inference utility tasks.", "histories": [["v1", "Tue, 24 Feb 2015 01:42:09 GMT  (75kb,D)", "http://arxiv.org/abs/1502.06668v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jacob steinhardt", "percy liang"], "accepted": true, "id": "1502.06668"}, "pdf": {"name": "1502.06668.pdf", "metadata": {"source": "META", "title": "Learning Fast-Mixing Models for Structured Prediction", "authors": ["Jacob Steinhardt", "Percy Liang"], "emails": ["JSTEINHARDT@CS.STANFORD.EDU", "PLIANG@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "Conventional wisdom suggests that rich features and highly-dependent variables necessitate intractable inference. Indeed, the dominant paradigm is to first define a joint model, and then use approximate inference (e.g., MCMC) to learn that model. While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).\nIn this paper, we instead use MCMC to define the model family itself: For a given T , we construct a family of Markov chains using arbitrary rich features, but whose mixing time is guaranteed to be at most O(T ). The corresponding stationary distributions make up the model family. We can think of our Markov chains as parameterizing a family of \u201ccomputationally accessible\u201d distributions, where the amount of computation is controlled by T .\nFor concreteness, suppose we are performing a structured prediction task from input x to a complex output y. We construct Markov chains of the following form, called\nstrong Doeblin chains (Doeblin, 1940):\nA\u0303\u03b8(yt | yt\u22121, x) = (1\u2212 )A\u03b8(yt | yt\u22121, x) + u\u03b8(yt | x), (1)\nwhere is a mixture coefficient and \u03b8 parameterizesA\u03b8 and u\u03b8. Importantly, u\u03b8 does not depend on the previous state yt\u22121. For intuition, think of u\u03b8 as a simple tractable model and A\u03b8 as Gibbs sampling in a complex intractable model. With probability 1 \u2212 , we progress according to A\u03b8, and with probability we draw a fresh sample from u\u03b8, which performs an informed random restart. When = 1, we are drawing i.i.d. samples from u\u03b8; we therefore mix in a single step, but our stationary distribution must necessarily be very simple. When = 0, the stationary distribution can be much richer, but we have no guarantees on the mixing time. For intermediate values of , we trade off between representational power and mixing time.\nA classic result is that a given strong Doeblin chain mixes in time at most 1 (Doeblin, 1940), and that we can draw an exact sample from the stationary distribution in expected timeO( 1 ) (Corcoran and Tweedie, 1998). In this work, we prove new results that help us understand the strong Doeblin model families. Let F and F\u0303 be the family of stationary distributions corresponding to A\u03b8 and A\u0303\u03b8 as defined in (1), respectively. Our first result is that as decreases, the stationary distribution of any A\u0303\u03b8 monotonically approaches the stationary distribution of the correspondingA\u03b8 (as measured by either direction of the KL divergence). Our second result is that if 1 is much larger than the mixing time of A\u03b8, then the stationary distributions of A\u03b8 and A\u0303\u03b8 are close under a certain Mahalanobis distance. This shows that any member of F that is computationally accessible via the Markov chain is well-approximated by its counterpart in F\u0303 .\nF F\u0303\nF0\nThe figure above shows F and F\u0303 , together with the subset\nar X\niv :1\n50 2.\n06 66\n8v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n15\nF0 of F whose Markov chains mix quickly. F\u0303 (approximately) covers F0, and contains some distributions outside of F entirely.\nIn order to learn over F\u0303 , we show how to maximize the likelihood of the data under the stationary distribution of A\u0303\u03b8. Specifically, we show that we can compute a stochastic gradient of the log-likelihood in expected timeO( 1 ). Thus, in a strong sense, our objective function explicitly accounts for computational constraints.\nWe also generalize strong Doeblin chains, which are a mixture of two base chains, u\u03b8 and A\u03b8, to staged strong Doeblin chains, which allow us to combine more than two base chains. We introduce an auxiliary variable z representing the \u201cstage\u201d that the chain is in. We then transition between stages, using the base chain corresponding to the current stage z to advance the concrete state y. A common application of this generalization is defining a sequence of increasingly more complex chains, similar in spirit to annealing. This allows sampling to become gradually more sensitive to the structure of the problem.\nWe evaluated our methods on two tasks: (i) inferring words from finger gestures on a touch screen and (ii) inferring DNF formulas for program verification. Unlike many structured prediction problems where local potentials provide a large fraction of the signal, in the two tasks above, local potentials offer a very weak signal; reasoning carefully about the higher-order potentials is necessary to perform well. On word inference, we showed that learning strong Doeblin chains obtained a 3.6% absolute improvement in character accuracy over Gibbs sampling while requiring 5x fewer samples. On DNF formula inference, our staged strong Doeblin chain obtains an order of magnitude speed improvement over plain Metropolis-Hastings.\nTo summarize, the contributions of this paper are: We formally define a family of MCMC algorithms based on strong Doeblin chains with guaranteed fast mixing times (Section 2). We provide an extensive analysis of the theoretical properties of this family (Section 3), together with a generalization to a staged version (Section 3.1). We provide an algorithm for learning the parameters of a strong Doeblin chain (Section 4). We demonstrate superior experimental results relative to baseline MCMC samplers on two tasks, word inference and DNF formula synthesis (Section 5)."}, {"heading": "2. A Fast-Mixing Family of Markov Chains", "text": "Given a Markov chain with transition matrix A(yt | yt\u22121) and a distribution u(yt), define a new Markov chain with transitions given by A\u0303(yt | yt\u22121) def = (1\u2212 )A(yt | yt\u22121) +\nu(yt). (We suppress the dependence on \u03b8 and x for now.)\nIn matrix notation, we can write A\u0303 as\nA\u0303 def = (1\u2212 )A+ u1>. (2)\nIn other words, with probability we restart from u; otherwise, we transition according to A. Intuitively, A\u0303 should mix quickly because a restart from u renders the past independent of the future (we formalize this in Section 3). We think of u as a simple tractable model that provides coverage, and A as a complex model that provides precision.\nSimple example. To gain some intuition, we work through a simple example with the Markov chain A depicted in Figure 1. The stationary distribution of this chain is [\n1 2+3\u03b4 3\u03b4 2+3\u03b4 1 2+3\u03b4\n] , splitting most of the probabil-\nity mass evenly between states 1 and 3. The mixing time of this chain is approximately 1\u03b4 , since once the chain falls into either state 1 or state 3, it will take about 1\u03b4 steps for it to escape back out. If we run this Markov chain for T steps with T 1\u03b4 , then our samples will be either almost all in state 1 or almost all in state 3, and thus will provide a poor summary of the distribution. If instead we perform random restarts with probability from a uniform distribution u over {1, 2, 3}, then the restarts give us the opportunity to explore both modes of the distribution. After a restart, however, the chain will more likely fall into state 3 than state 1 ( 59 probability vs. 4 9 ), so for \u03b4 the stationary distribution will be noticeably perturbed by the restarts. If \u03b4, then there will be enough time for the chain to mix between restarts, so this bias will vanish. See Figure 1 for an illustration of this phenomenon."}, {"heading": "3. Theoretical Properties", "text": "Markov chains that can be expressed according to (2) are said to have strong Doeblin parameter (Doeblin, 1940). In this section, we characterize the stationary distribution and mixing time of A\u0303, and also relate the stationary distribution of A\u0303 to that of A as a function of . Often the easiest way to study the mixing time of A\u0303 is via its spectral gap,\nwhich is defined as 1\u2212 \u03bb2(A\u0303), where \u03bb2(A\u0303) is the secondlargest eigenvalue (in complex norm). A standard result for Markov chains is that, under mild assumptions, the mixing time of A\u0303 isO ( 1\n1\u2212\u03bb2(A\u0303)\n) . We assume throughout this sec-\ntion thatA is ergodic but not necessarily that it is reversible. See Section 12.4 of Levin et al. (2009) for more details.\nOur first result relates the spectral gap (and hence the mixing time) to . This result (as well as the next) are wellknown but we include them for completeness. For most results in this section, we sketch the proof here and provide the full details in the appendix.\nProposition 3.1. The spectral gap of A\u0303 is at least ; that is, 1\u2212 \u03bb2(A\u0303) \u2265 . In particular, A\u0303 mixes in time O( 1 ).\nThe key idea is that all eigenvectors of A\u0303 and A (except for the stationary distribution) are equal, and that \u03bbk(A\u0303) = (1\u2212 )\u03bbk(A) for k > 1.\nHaving established that A\u0303 mixes quickly, the next step is to determine its stationary distribution:\nProposition 3.2. Let \u03c0\u0303 be the stationary distribution of A\u0303. Then\n\u03c0\u0303 = \u221e\u2211 j=0 (1\u2212 )jAju = (I \u2212 (1\u2212 )A)\u22121u. (3)\nThis can be directly verified algebraically. The summation over j shows that we can in fact draw an exact sample from \u03c0\u0303 by drawing T \u223c Geometric( ),1 initializing from u, and transitioning T times according to A. This is intuitive, since at a generic point in time we expect the most recent sample from u to have occurred Geometric( ) steps ago. Note that E[T + 1] = 1 , which is consistent with the fact that the mixing time is O( 1 ) (Proposition 3.1).\nWe would like to relate the stationary distributions \u03c0\u0303 and \u03c0 of A\u0303 and A. The next two results (which are new) do so.\nLet \u03c0\u0303 denote the stationary distribution of A\u0303 at a particular value of ; note that \u03c0\u03031 = u and \u03c0\u03030 = \u03c0. We will show that \u03c0\u0303 approaches \u03c0 monotonically, for both directions of the KL divergence. In particular, for any < 1, \u03c0\u0303 is at least as good as u at approximating \u03c0.\nTo show this, we make use of the following lemma from Murray and Salakhutdinov (2008):\nLemma 3.3. If B is a transition matrix with stationary distribution \u03c0, then KL (\u03c0 \u2016 B\u03c0\u2032) \u2264 KL (\u03c0 \u2016 \u03c0\u2032) and KL (B\u03c0\u2032 \u2016 \u03c0) \u2264 KL (\u03c0\u2032 \u2016 \u03c0).\nUsing this lemma, we can prove the following monotonicity result:\n1If T \u223c Geometric( ), we have P[T = j] = (1 \u2212 )j for j \u2265 0.\nProposition 3.4. Both KL (\u03c0\u0303 \u2016 \u03c0) and KL (\u03c0 \u2016 \u03c0\u0303 ) are monotonic functions of .\nThe idea is to construct a transition matrix B that maps \u03c0\u0303 1 to \u03c0\u0303 2 for given 2 < 1, then show that its stationary distribution is \u03c0 and apply Lemma 3.3.\nWith Proposition 3.4 in hand, a natural next question is how small must be before \u03c0\u0303 is reasonably close to \u03c0. Proposition 3.5 provides one such bound: \u03c0\u0303 is close to \u03c0 if is small compared to the spectral gap 1\u2212 \u03bb2(A). Proposition 3.5. Suppose that A satisfies detailed balance with respect to \u03c0. Let \u03c0\u0303 be the stationary distribution of A\u0303. Define d\u03c0(\u03c0\u2032) def = \u2016\u03c0\u2212\u03c0\u2032\u2016diag(\u03c0)\u22121 = \u221a \u22121 + \u2211 y \u03c0\u2032(y)2 \u03c0(y) , where \u2016v\u2016M is the Mahalonobis distance \u221a v>Mv. Then d\u03c0(\u03c0\u0303) \u2264 1\u2212\u03bb2(A) \u00b7 d\u03c0(u). (In particular, d\u03c0(\u03c0\u0303) 1 if (1\u2212 \u03bb2(A))/d\u03c0(u).)\nThe proof is somewhat involved, but the key step is to establish that d\u03c0(\u03c0\u2032) is convex in \u03c0\u2032 and contractive with respect to A (more precisely, that d\u03c0(A\u03c0\u2032) \u2264 \u03bb2(A)d\u03c0(\u03c0\u2032)).\nProposition 3.5 says that if A mixes quickly, then A\u0303 and A will have similar stationary distributions. This serves as a sanity check: if A already mixes quickly, then \u03c0\u0303 is a good approximation to \u03c0; otherwise, the Doeblin construction ensures that we are at least converging to some distribution, which by Proposition 3.4 approximates \u03c0 at least as well as u does."}, {"heading": "3.1. Staged strong Doeblin chains", "text": "Recall that to run a strong Doeblin chain A\u0303, we first sample from u, and then transition according to A for approximately 1 steps. The intuition is that sampling from the crude distribution u faciliates global exploration of the state space, while the refined transition A hones in on a mode. However, for complex problems, there might be a considerable gap between what is possible with exact inference (u) and what is needed for accurate modeling (A). This motivates using multiple stages of MCMC to bridge the gap.\nTo do this, we introduce an auxiliary variable z \u2208 Z denoting which stage of MCMC we are currently in. For each\nstage z, we have a Markov chain Az(yt | yt\u22121) over the original state space. We also define a Markov chain C(zt | zt\u22121) over the stages. To transition from (yt\u22121, zt\u22121) to (yt, zt), we first sample zt from C(zt | zt\u22121) and then yt from Azt(yt | yt\u22121). If there is a special state z\u2217 for which Az\u2217(yt | yt\u22121) = u(yt) (i.e., Az\u2217 does not depend on yt\u22121), then we call the resulting chain a staged strong Doeblin chain.\nFor example, if z \u2208 {0, 1} and we transition from 0 to 1 with probability 1 \u2212 and from 1 to 0 with probability , then we recover strong Doeblin chains assuming z\u2217 = 0 (Figure 2(a)). As another example (Figure 2(b)), let z \u2208 {0, 1, 2}. When z = z\u2217 = 0, transition according to a restart distribution u1>; when z = 1, transition according to a simple chain A1; and when z = 2 transition according to a more complex chain A2. If we transition from z = 0 to z = 1 with probability 1, from z = 1 to z = 2 with probability 1, and from z = 2 to z = 0 with probability 2, then we will on average draw 1 sample from u, 1 1 samples from A1, and 1 2 samples from A2.\nWe now show that staged strong Doeblin chains mix quickly as long as we visit z\u2217 reasonably often. In particular, the following theorem provides guarantees on the mixing time that depend only on z\u2217 and on the structure of C(zt | zt\u22121), analogous to the previous dependence only on . The condition of the theorem asks for times a and b such that the first time after a that we hit z\u2217 is almost independent of the starting state z0, and is less than b with high probability.\nTheorem 3.6. Let M be a staged strong Doeblin chain on Z \u00d7 Y . Let \u03c4a be the earliest time s \u2265 a for which zs = z\u2217. Let \u03b2a,s = minz\u2208Z P[\u03c4a = s | z0 = z] and \u03b3a,b\ndef =\u2211b\ns=a \u03b2a,s. Then M b has strong Doeblin parameter \u03b3a,b. In particular, the spectral gap ofM is at least \u03b3a,bb . (Setting a = b = 1 recovers Proposition 3.1.)\nThe key idea is that, conditioned on \u03c4a, (yb, zb) is independent of (y0, z0) for all b \u2265 \u03c4a. For the special case that the stages form a cycle as in Figure 2, we have the following corollary:\nCorollary 3.7. Let C be a transition matrix on {0, . . . , k\u2212 1} such that C(zt = i | zt\u22121 = i) = 1 \u2212 \u03b4i and C(zt = (i + 1) mod k | zt\u22121 = i) = \u03b4i. Suppose that \u03b4k\u22121 \u2264 1 max(2,k\u22121) min{\u03b40, . . . , \u03b4k\u22122}. Then the spectral gap of the joint Markov chain is at least \u03b4k\u2212178 .\nThe key idea is that, when restricting to the time interval [2/\u03b4k\u22121, 3/\u03b4k\u22121], the time of first transition from k \u2212 1 to 0 is approximately Geometric(\u03b4k\u22121)-distributed (independent of the initial state), which allows us to invoke Theorem 3.6. We expect the optimal constant to be much smaller than 78."}, {"heading": "4. Learning strong Doeblin chains", "text": "Section 3 covered properties of strong Doeblin chains (1\u2212 )A\u03b8 + u\u03b81\n> for a fixed parameter vector \u03b8. Now we turn to the problem of learning \u03b8 from data. We will focus on the discriminative learning setting where we are given a dataset {(x(i), y(i))}ni=1 and want to maximize the conditional loglikelihood:\nO(\u03b8) = 1 n n\u2211 i=1 log p\u03b8(y (i) | x(i)), (4)\nwhere now p\u03b8 is the stationary distribution of A\u0303\u03b8 = (1 \u2212 )A\u03b8 + u\u03b81\n>. We assume for simplicity that the chains A\u03b8 and u\u03b8 are given by conditional exponential families:\nA\u03b8(y | y\u2032, x) def = exp ( \u03b8>\u03c6(x, y\u2032, y)\u2212 logZ(\u03b8;x, y) ) ,\nu\u03b8(y | x) def = exp ( \u03b8>\u03c6(x, y)\u2212 logZ(\u03b8;x) ) , (5)\nwhere each \u03c6 outputs a feature vector and the Z are partition functions. By Proposition 3.1, A\u0303\u03b8 mixes quickly for all \u03b8. On the other hand, the parameterization ofA\u03b8 captures a rich family of transition kernels, including Gibbs sampling.\nAt a high level, our learning algorithm performs stochastic gradient descent on the negative log-likelihood. However, the negative log-likelihood is only defined implicitly in terms of the stationary distribution of a Markov chain, so the main challenge is to show that it can be computed efficiently. To start, we assume that we can operate on the base chains u\u03b8 and A\u03b8 for one step efficiently: Assumption 4.1. We can efficiently sample y from u\u03b8(\u00b7 | x) and A\u03b8(\u00b7 | y\u2032, x), as well as compute \u2202 log u\u03b8(y|x)\u2202\u03b8 and \u2202 logA\u03b8(y|y\u2032,x)\n\u2202\u03b8 .\nUnder Asssumption 4.1, we will show how to efficiently compute the gradient of log p\u03b8(y(i) | x(i)) with respect to \u03b8. The impatient reader may skip ahead to the final pseudocode, which is given in Algorithm 1.\nFor convenience, we will suppress the dependence on x and i and just refer to p\u03b8(y) instead of p\u03b8(y(i) | x(i)). Computing the gradient of log p\u03b8(y) is non-trivial, since the formula for p\u03b8 is somewhat involved:\np\u03b8(y) = \u221e\u2211 j=0 (1\u2212 )j [Aj\u03b8u\u03b8](y). (6)\nWe are helped by the following generic identity on gradients of conditional log-probabilities, proved in the appendix. Lemma 4.2. Let z have distribution p\u03b8(z) parameterized by a vector \u03b8. Let S be any measurable set. Then\n\u2202 log p\u03b8(z \u2208 S) \u2202\u03b8\n= Ez [ \u2202 log p\u03b8(z)\n\u2202\u03b8\n\u2223\u2223\u2223\u2223z \u2208 S] . (7)\nWe can utilize Lemma 4.2 by interpreting y | \u03b8 as the output of the following generative process, which by Proposition 3.2 yields the stationary distribution of A\u0303\u03b8:\n\u2022 Sample y0 from u\u03b8 and yt+1 | yt from A\u03b8 for t = 0, 1, . . ..\n\u2022 Sample T \u223c Geometric( ) and let y = yT .\nWe then invoke Lemma 4.2 with z = (T, y0:T ) and S encoding the event that yT = y. As long as we can sample from the posterior distribution of (T, y0:T ) conditioned on yT = y, we can compute an estimate of \u2202\u2202\u03b8 log p\u03b8(y) as follows:\n\u2022 Sample (T, y0:T ) | yT = y. \u2022 Return \u2202 log p\u03b8(T,y0:T )\u2202\u03b8 = \u2202 log u\u03b8(y0) \u2202\u03b8\n+ \u2211T t=1 \u2202 logA\u03b8(yt|yt\u22121) \u2202\u03b8 .\n4.1. Sampling schemes for (T, y0:T )\nBy the preceding comments, it suffices to construct a sampler for (T, y0:T ) | yT = y. A natural approach is to use importance sampling: sample (T, y0:T\u22121), then weight by p(yT = y | yT\u22121). However, this throws away a lot of work \u2014 we make T MCMC transitions but obtain only one sample (T, y0:T ) with which to estimate the gradient.\nWe would like to ideally make use of all the MCMC transitions when constructing our estimate of (T, y0:T ) | yT = y. For any t \u2264 T , the pair (t, y0:t) would itself have been a valid sample under different randomness, and we would like to exploit this. Suppose that we sample T from some distribution F and let q(t) be the probability that T \u2265 t under F . Then we can use the following scheme:\n\u2022 Sample T from F , then sample y0:T\u22121.\n\u2022 For t = 0, . . . , T , weight (t, y0:t\u22121) by (1\u2212 ) t\nq(t) \u00d7 p(yt = y | yt\u22121).\nFor any q, this yields unbiased (although unnormalized) weights (see Section B in the appendix). Typically we will choose q(t) = (1 \u2212 )t, e.g. F is a geometric distribution. If the yt are perfectly correlated, this will not be any more effective than vanilla importance sampling, but in practice this method should perform substantially better. Even though we obtain weights on all of y0:T , these weights will typically be highly correlated, so we should still repeat the sampling procedure multiple times to minimize the bias from estimating the normalization constant. The full procedure is given as pseudocode in Algorithm 1."}, {"heading": "4.2. Implementation", "text": "With the theory above in place, we now describe some important implementation details of our learning algorithm.\nAlgorithm 1 Algorithm for computing an estimate of \u2202 \u2202\u03b8 log p\u03b8(y | x). This estimate is unbiased in the limit of infinitely many samples k, but will be biased for a finite number of samples due to variance in the estimate of the partition function.\nSampleGradient(x, y, \u03b8, , k) k is the number of samples to take Z \u2190 0; g \u2190 0 Z is the total importance mass of all samples, gZ is the gradient for i = 1 to k do\nSample T \u223c Geometric( ) Sample y0 from u\u03b8(\u00b7 | x) For 1 \u2264 t \u2264 T\u22121: sample yt fromA\u03b8(\u00b7 |yt\u22121, x) w0 \u2190 \u00b7 u\u03b8(y) For 1 \u2264 t \u2264 T : wt \u2190 \u00b7A\u03b8(y | yt\u22121, x) Z \u2190 Z + \u2211T t=0 wt g\u2190g + w0 \u2202 log u\u03b8(y|x)\u2202\u03b8 + \u2211T t=1 wt ( \u2202 log u\u03b8(y0|x) \u2202\u03b8\n+ \u2211t\u22121 s=1 \u2202 logA\u03b8(ys|ys\u22121,x) \u2202\u03b8 + \u2202 logA\u03b8(y|yt\u22121,x) \u2202\u03b8 ) .\nend for Output gZ\nAt a high level, we can just use Algorithm 1 to compute estimates of the gradient and then apply an online learning algorithm such as ADAGRAD (Duchi et al., 2011) to identify a good choice of \u03b8. Since the log-likelihood is a nonconvex function of \u03b8, the initialization is important. We make the following (weak) assumption:\nAssumption 4.3. The chains u\u03b8 and A\u03b8 are controlled by disjoint coordinates of \u03b8, and for any setting of u\u03b8 there is a corresponding choice of A\u03b8 that leaves u\u03b8 invariant (i.e., A\u03b8u\u03b8 = u\u03b8).\nIn practice, Assumption 4.3 is easy to satisfy. For instance, suppose that \u03c6 : Y \u2192 Rd is a feature function, \u03b8 = [\u03b80 \u03b81] \u2208 Rd0+d are the features controlling u and A, and u\u03b80 is made tractable by zeroing some features out: u\u03b80(y) \u221d exp([\u03b80 ~0d\u2212d0 ]>\u03c6(y)). Also suppose that A\u03b81 is a Gibbs sampler that uses all the features: A\u03b81(y | y\u2032) \u221d exp(\u03b8>1 \u03c6(yi, y\u2032\u00aci)), where i is a randomly chosen coordinate of y. Then, we can satisfy Assumption 4.3 by setting \u03b81 = [\u03b80 ~0d\u2212d0 ].\nUnder Assumption 4.3, we can initialize \u03b8 by first training u in isolation (which is a convex problem if u\u03b8 parameterizes an exponential family), then initializing A to leave u invariant; this guarantees that the initial log-likelihood is what we would have obtained by just using u by itself. We found this to work well empirically.\nAs another note, Algorithm 1 na\u0131\u0308vely looks like it takes O(T 2) time to compute the gradient for each sample, due\nto the nested sum. However, most terms are of the form wt \u2202 logA\u03b8(ys|ys\u22121,x) \u2202\u03b8 ; by grouping them for a fixed s we can compute the sum in O(T ) time, leading to expected runtime O ( k ) for Algorithm 1 (since E[T + 1] = 1 )."}, {"heading": "5. Experiments", "text": "We validated our method on two challenging inference tasks. These tasks are difficult due to the importance of high-arity factors; local information is insufficient to even identify high-probability regions of the space.\nInferring Words from Keyboard Gestures We first considered the task of inferring words from keyboard gestures. We generated the data by sampling words from the New York Times corpus (Sandhaus, 2008). For each word, we used a time series model to synthetically generate finger gestures for the word. A typical instantiation of this process is given in Figure 3. The learning task is to discriminatively infer the intended word y given the sequence of keys x that the finger was over (for instance, predicting banana from bdsadbnnnfaassjjj). In our model, we posit a latent alignment z between key presses and intended letter. Given an input x of length l, the alignment z also has length l; each zi is either \u2018c\u2019 (xi starts an output letter c), \u2018-c\u2019 (xi continues an output letter c), or \u2018#\u2019 (xi is unaligned); see Figure 3 for an example. Note that y is a deterministic function of z.\nThe base model u\u03b8 consists of indicator features on (xi, zi), (xi, zi\u22121, zi), and (xi, xi\u22121, zi). The full A\u03b8 is a Gibbs sampler in a model where we include the following features in addition to those above:\n\u2022 indicator features on (xi, yi, yi\u22121) \u2022 indicator of y being in the dictionary, as well as log of\nword frequency (conditioned on being in the dictionary) \u2022 for each i, indicator of y1:i matching a prefix of a word\nin the dictionary\nWe compared three approaches:\n\u2022 Our approach (Doeblin sampling) \u2022 Regular Gibbs sampling, initialized by setting zi = xi\nfor all i (basic-Gibbs) \u2022 Gibbs sampling initialized from u\u03b8 (u\u03b8-Gibbs)\nAt test time, all three of these methods are almost identical: they all initialize from some distribution, then make a certain number of Gibbs samples. For basic-Gibbs and u\u03b8-Gibbs, this is always a fixed number of steps T , while for Doeblin sampling the number of steps is a geometric distribution with mean T .\nThe main difference is in how the methods are trained. Our method is trained using the ideas in Section 4; for the other two methods, we train by approximating the gradient:\n\u2207 log p\u03b8(y | x) = Ez\u0302\u223cp\u03b8(z|x,y)[\u03c6(y, z\u0302, x)] \u2212 Ey\u0302,z\u0302\u223cp\u03b8(y,z|x)[\u03c6(y\u0302, z\u0302, x)],\nwhere \u03c6(y, z, x) is the feature function and p\u03b8 is the stationary distribution of A\u03b8. For the second term, we use MCMC samples from A\u03b8 to approximate p\u03b8(y, z | x). For the first term, we could take the subset of samples where y\u0302 = y, but this is problematic if no such samples exist. Instead, we reweight all samples with y\u0302 6= y by exp(\u2212(D+1)), whereD is the edit distance between y and y\u0302. We use the same reweighting approach for the Doeblin sampler, using this as the importance weight rather than using A\u03b8(y | yt\u22121) as in Algorithm 1.\nTo provide a fair comparison of the methods, we set in the Doeblin sampler to the inverse of the number of transitions T , so that the expected number of transitions of all algorithms is the same. We also devoted the first half of each chain to burn-in.\nAll algorithms are trained with AdaGrad (Duchi et al., 2011) with 16 independent chains run for each example. We measure word-level accuracy by computing the fraction of (non-burn-in) samples whose output y is correct.\nThe results are reported in Figure 4. Overall, our Doeblin sampler outperforms u\u03b8-Gibbs by a significant margin, which in turn outperforms basic-Gibbs. Interestingly, while the accuracy of our method continues to improve with more training time, u\u03b8-Gibbs quickly asymptotes and then slightly decreases, even for training accuracy.\nWhat is happening to u\u03b8-Gibbs? Since the inference problem in this task is hard, the samples provide a poor gradient approximation. As a result, optimization methods that take the approximation at face value may not converge to even a local optimum. This phenomenon has already been studied in other contexts, for instance by Kulesza and Pereira\n(2007) and Huang et al. (2012).\nIn contrast, our method directly optimizes the loglikelihood of the data under the distribution \u03c0\u0303\u03b8, so that accuracy continues to increase with more passes through the training data. This demonstrates that the MCMC samples do provide enough signal to train from, but that na\u0131\u0308vely plugging them into a method designed for exact inference will fail to exploit that signal.\nInferring DNF Formulas We next study the use of our staged Doeblin chain construction as a tool for hierarchical initialization. We ignore learning for now, instead treating MCMC as a stochastic search algorithm. Our task of interest is to infer a DNF formula f from its inputoutput behavior. This is an important subroutine in loop invariant synthesis, where MCMC methods have recently shown great promise (Gulwani and Jojic, 2007; Sharma and Aiken, 2014).\nConcretely, the input x might look like this:\nf(1, 2, 3) = True\nf(1, 4, 4) = True\nf(0, 1, 0) = False\nf(0, 2, 2) = True\nOur task is to reconstruct f ; in this case, f(x1, x2, x3) = [x1 6= 0] \u2228 [x2 = x3].\nMore formally, we consider DNF formulae with linear inequality predicates: f(x) = \u2228n i=1 \u2227m j=1[a > ijx \u2264 bij ], where aij , x \u2208 Zd and bij \u2208 Z. The formula f maps input vectors to {True, False}. Given a collection of example inputs and outputs, our goal is to find an f consistent with all examples. Our evaluation metric is the time to find such\na formula.\nThe search space for this problem is extremely large. Even if we set n = m = 3 and restrict our search to aij \u2208 {\u22121, 0, 1}5, b \u2208 {\u22121, 0, 1}, the total number of candidate formulae is still ( 36 )3\u00d73 \u2248 5.8\u00d7 1025.\nWe consider three MCMC methods: no restarts (0-stage), uniformly random restarts (1-stage), and a staged method (2-stage) as in Section 3.1. All base chains perform Metropolis-Hastings using proposals that edit individual atoms (e.g., [a>ijx \u2264 bij ]), either by changing a single entry of [aij bij ] or by changing all entries of [aij bij ] at once. For the staged method, we initialize uniformly at random, then take Geometric(0.04) transitions based on a simplified cost function, then take Geometric(0.0002) steps with the full cost (this is the staged Doeblin chain in Figure 2).\nThe full cost function is I(f), the number of examples f errs on. We stop the Markov chain when it finds a formula with I(f) = 0. The simplified cost function decomposes over the disjuncts: for each disjunct d(x), if f(x) = False while d(x) = True, we incur a large cost (since in order for f(x) to be false, all disjuncts comprising f(x) must also be false). If f(x) = True while d(x) = False, then we incur a smaller cost. If f(x) = d(x) then we incur no cost.\nWe used all three methods as a subroutine in verifying properties of C programs; each such verification requires solving many instances of DNF formula inference. Using the staged method we are able to obtain a 30% speedup over uniformly random restarts and a 50x improvement over no restarts, as shown in Table 1."}, {"heading": "6. Discussion", "text": "We have proposed a model family based on strong Doeblin Markov chains, which guarantee fast mixing. Our construction allows us to simultaneously leverage a simple, tractable model (u\u03b8) that provides coverage together with a complex, accurate model (A\u03b8) that provides precision. As such, we sidestep a typical dilemma\u2014whether to use a simple model with exact inference, or to deal with the consequences of approximate inference in a more complex model.\nWhile our approach works well in practice, there are still some outstanding issues. One is the non-convexity of the learning objective, which makes the procedure dependent on initialization. Another issue is that the gradients returned by Algorithm 1 can be large, heterogeneous, and high-variance. The adaptive nature of ADAGRAD alleviates this somewhat, but it would still be ideal to have a sampling procedure that had lower variance than Algorithm 1.\nThough Gibbs sampling is the de facto method for many practitioners, there are also many more sophisticated approaches to MCMC (Green, 1995; Earl and Deem, 2005). Since our framework is orthogonal to the particular choice of transition kernel, it would be interesting to apply our method in these contexts.\nFinally, we would like to further explore the staged construction from Section 3.1. As the initial results on DNF formula synthesis are promising, it would be interesting to apply the construction to high-dimensional feature spaces as well as rich, multi-level hierarchies. We believe this might be a promising approach for extremely rich models in which a single level of re-initialization is insufficient to capture the complexity of the cost landscape.\nRelated work. Our learning algorithm is reminiscent of policy gradient algorithms in reinforcement learning (Sutton et al., 2000), as well as Searn, which tries to learn an optimal search policy for structured prediction (Daume\u0301 III et al., 2009); see also Shi et al. (2015), who apply reinforcement learning in the context of MCMC. Our staged construction is also similar in spirit to path sampling (Gelman and Meng, 1998), as it uses a multi-stage approach to smoothly transition from a very simple to a very complex\ndistribution.\nOur staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).\nOn the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978). The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998). We are the first to use strong Doeblin properties to construct model families and learn them from data.\nAt a high level, our idea is to identify a family of models for which an approximate inference algorithm is known to work well, thereby constructing a computationally tractable model family that is nevertheless more expressive than typical tractable families such as low-treewidth graphical models. We think this general program is very interesting, and could be applied to other inference algorithms as well, thus solidfying the link between statistical theory and practical reality."}, {"heading": "A. Proofs", "text": "Proof of Proposition 3.1. We will in fact show that, for all k > 1, \u03bbk(A\u0303) = (1 \u2212 )\u03bbk(A), with the same eigenvector for both matrices. In other words, while the stationary distribution of A\u0303 is different from A, all other eigenvectors are unchanged.\nLet wk be the eigenvector of A corresponding to \u03bbk. First note that 1>wk = 0. This is because\n1>Awk = 1 >wk (8)\nsince A is stochastic, and\n1>Awk = \u03bbk1 >wk (9)\nsince wk is an eigenvector of A. Since \u03bbk 6= 1, this implies that 1>wk = 0.\nNow we have\nA\u0303wk = (1\u2212 )Awk + u1>wk = (1\u2212 )\u03bbkwk,\nwhich proves that \u03bbk(A\u0303) = (1\u2212 )\u03bbk(A). In particular, \u03bb2(A\u0303) = (1\u2212 )\u03bb2(A) \u2264 1\u2212 . The mixing time of A\u0303 is 11\u2212\u03bb2(A\u0303) , and is therefore upper bounded by 1 , which completes the proof.\nProof of Proposition 3.2. We can verify algebraically that A\u0303\u03c0\u0303 = \u03c0\u0303, as follows:\nA\u0303\u03c0\u0303 = (1\u2212 )A\u03c0\u0303 + u1>\u03c0\u0303 = (1\u2212 )A(I \u2212 (1\u2212 )A)\u22121u+ u = [ (1\u2212 )A(I \u2212 (1\u2212 )A)\u22121 + I ] u\n= [(1\u2212 )A+ (I \u2212 (1\u2212 )A)] (I \u2212 (1\u2212 )A)\u22121u = (I \u2212 (1\u2212 )A)\u22121u = \u03c0\u0303,\nso that \u03c0\u0303 is indeed the stationary distribution of A\u0303.\nProof of Proposition 3.5. From the characterization of \u03c0\u0303 in Proposition 3.2, we know that \u03c0\u0303 is equal to\n\u221e\u2211 j=0 (1\u2212 )jAju. (10)\nThe rest of the proof consists of determining some useful properties of d\u03c0(\u03c0\u2032). The most important (and the motivation for defining d\u03c0 in the first place) is given in the following lemma, which we prove separately:\nLemma A.1. If \u03c0 is the stationary distribution of A and A satisfies detailed balance, then d\u03c0(A\u03c0\u2032) \u2264 \u03bb2(A)d\u03c0(\u03c0\u2032).\nThe other important property of d\u03c0(\u03c0\u2032) is convexity: d\u03c0(w\u03c0\u2032 + (1\u2212 w)\u03c0\u2032\u2032) \u2264 wd\u03c0(\u03c0\u2032) + (1\u2212 w)d\u03c0(\u03c0\u2032\u2032), which follows directly from the characterization of d\u03c0(\u03c0\u2032) as a Mahalanobis distance.\nPutting these two properties together, we have\nd\u03c0(\u03c0\u0303) \u2264 \u221e\u2211 j=0 (1\u2212 )jd\u03c0(Aju)\n\u2264 \u221e\u2211 j=0 (1\u2212 )j\u03bb2(A)jd\u03c0(u)\n\u2264 1\u2212 (1\u2212 )\u03bb2(A) d\u03c0(u)\n\u2264 1\u2212 \u03bb2(A) d\u03c0(u),\nwhich completes the proof.\nProof of Lemma A.1. Recall we want to show that d\u03c0(A\u03c0\u2032) \u2264 \u03bb2(A)d\u03c0(\u03c0\u2032). To see this, first define S = diag(\u03c0)\u22121/2Adiag(\u03c0)1/2, which is symmetric by the detailed balance condition, and satisfies \u03bbk(S) = \u03bbk(A) by similarity. Furthermore, the top eigenvector of S is 1> diag(\u03c0)1/2. Putting these together, we have\nd\u03c0(A\u03c0 \u2032) = \u2016 diag(\u03c0)\u22121/2(\u03c0 \u2212A\u03c0\u2032)\u20162\n= \u2016 diag(\u03c0)\u22121/2A(\u03c0 \u2212 \u03c0\u2032)\u20162 = \u2016S diag(\u03c0)\u22121/2(\u03c0 \u2212 \u03c0\u2032)\u20162 \u2264 \u03bb2(S)\u2016 diag(\u03c0)\u22121/2(\u03c0 \u2212 \u03c0\u2032)\u20162 = \u03bb2(S)d\u03c0(\u03c0 \u2032) = \u03bb2(A)d\u03c0(\u03c0 \u2032).\nThe inequality step \u2016S diag(\u03c0)\u22121/2(\u03c0 \u2212 \u03c0\u2032)\u20162 \u2264 \u03bb2(S)\u2016diag(\u03c0)\u22121/2(\u03c0 \u2212 \u03c0\u2032)\u20162 follows because diag(\u03c0)\u22121/2(\u03c0 \u2212 \u03c0\u2032) is orthogonal to the top eigenvector 1> diag(\u03c0)1/2 of S.\nProof of Proposition 3.4. For any value of , the stationary distribution \u03c0\u0303 of (1\u2212 )A+ u1> can be obtained by applying A a Geometric( )-distributed number of times to u (by Proposition 3.2). For 2 < 1, it therefore suffices to construct a random variable F \u2265 0 such that if s \u223c F and t \u223c Geometric( 1) then s + t \u223c Geometric( 2); if we can do this, then we can let B be the matrix that applies A an F -distributed number of times, and we would have B\u03c0\u0303 1 = \u03c0\u0303 2 ; but B would clearly have stationary distribution \u03c0, and so Lemma 3.3 would give KL (\u03c0 \u2016 \u03c0\u0303 2) \u2264 KL (\u03c0 \u2016 \u03c0\u0303 1) and KL (\u03c0\u0303 2 \u2016 \u03c0) \u2264 KL (\u03c0\u0303 1 \u2016 \u03c0), which is the desired result.\nTo construct the desired F , we use the fact that addition of random variables corresponds to convolution of the probability mass functions, and furthermore represent the probability mass functions as formal power series; in particular, we let\nf(x) = \u221e\u2211 n=0 P[t = n | t \u223c F ]xn, (11)\nand similarly\ng (x) = \u221e\u2211 n=0 P[t = n | t \u223c Geometric( )]xn\n= \u221e\u2211 n=0 (1\u2212 )nxn\n= 1\u2212 (1\u2212 )x .\nWe want f(x)g 1(x) to equal g 2(x), so we take\nf(x) = g 2(x)\ng 1(x)\n= 2 1 1\u2212 (1\u2212 1)x 1\u2212 (1\u2212 2)x\n= 2 1\n( 1 +\n\u221e\u2211 n=1 [ (1\u2212 2)n \u2212 (1\u2212 2)n\u22121(1\u2212 1) ] xn\n)\n= 2 1\n( 1 +\n\u221e\u2211 n=1\n(1\u2212 2)n\u22121( 1 \u2212 2)xn ) .\nFrom this we see that the random variable F with probability function P[t = n | t \u223c F ] = { 2 1 : n = 0\n2 1\n(1\u2212 2)n\u22121( 1 \u2212 2) : n > 0 (12)\nsatisfies the required property that F + Geometric( 1) = Geometric( 2). Note that the condition 2 < 1 is necessary here so that all of the probability masses are positive in the above expression.\nWe can also prove the result purely algebraically. Motivated by the above construction, we define\nB = 2(I \u2212 (1\u2212 2)A)\u22121 [ 1(I \u2212 (1\u2212 1)A)\u22121 ]\u22121 = 2 1 [ I + ( 1 \u2212 2)A(I \u2212 (1\u2212 2)A)\u22121 ] .\nBy construction we have B\u03c0\u0303 1 = \u03c0\u0303 2 , but Taylor expanding the second expression for B shows that we can write it as a (infinite) convex combination of non-negative powers of A, and hence that B has stationary distribution \u03c0. This again yields the desired result by Lemma 3.3.\nProof of Theorem 3.6. We use an equivalent characterization of the strong Doeblin parameter as the quantity \u0393(A) =\u2211 y infy\u2032 A(y | y\u2032). In the context of the Markov chain M , this yields\n\u0393(M b) = \u2211\n(zb,yb)\ninf (z0,y0)\nP[zb, yb | z0, y0]\n= \u2211\n(zb,yb)\ninf (z0,y0) b\u2211 \u03c4=a P[\u03c4a = \u03c4 | y0]\u00d7 P[zb, yb | \u03c4a = \u03c4 ]\n\u2265 \u2211\n(zb,yb)\nb\u2211 \u03c4=a inf y0 P[\u03c4a = \u03c4 | y0]\u00d7 P[zb, yb | \u03c4a = \u03c4 ]\n= b\u2211 \u03c4=a inf y0 P[\u03c4a = \u03c4 | y0] \u2211\n(zb,yb)\nP[zb, yb | \u03c4a = \u03c4 ]\n= b\u2211 \u03c4=a inf y0 P[\u03c4a = \u03c4 | y0]\n= \u03b3a,b.\nFinally, by Proposition 3.1, the spectral gap of M b is at least \u03b3a,b, hence the spectral gap of M is at least 1b\u03b3a,b, which proves the theorem.\nProof of Corollary 3.7. Note that the time to transition from i to i + 1 is Geometric(\u03b4i)-distributed. Suppose we start from an arbitrary j \u2208 {0, . . . , k \u2212 1}. Then the time t\u2032 to get to k \u2212 1 is distributed as \u2211k\u22122 i=j Geometric(\u03b4i). t \u2032 has\nmean \u2211k\u22122 i=j 1 \u03b4i \u2264 1\u03b4k\u22121 , and variance \u2211k\u22122 i=j 1\u2212\u03b4i \u03b42i \u2264 1 2\u03b42k\u22121 . In particular, with probability 12 , t\n\u2032 lies between 0 and\u230a 2\n\u03b4k\u22121\n\u230b . Now, consider the time t\u2032\u2032 to get from k \u2212 1 to 0; this is Geometric(\u03b4k\u22121)-distributed, and conditioned on\nt\u2032\u2032 being at least 2\u03b4k\u22121 , t \u2032\u2032 + t\u2032 \u2212 2\u03b4k\u22121 is also Geometric(\u03b4k\u22121)-distributed. But t \u2032\u2032 is at least b 2\u03b4k\u22121 c with probability at least (1 \u2212 \u03b4k\u22121)2/\u03b4k\u22121 \u2265 116 (since \u03b4k\u22121 \u2264 1 2 ). Hence independently of j, t\n\u2032\u2032 + t\u2032 is, with probability at least 116 , distributed according to b 2\u03b4k\u22121 c + Geometric(\u03b4k\u22121). But t\n\u2032\u2032 + t\u2032 = \u03c4 by construction, and so we need only compute the probability that \u03c4 \u2264 d 3\u03b4k\u22121 c; but this is just the probability that the geometric distribution is less than 1 \u03b4k\u22121 , which is 1 \u2212 (1 \u2212 \u03b4k\u22121)1/\u03b4k\u22121 \u2265 1 \u2212 e\u22121. Therefore, \u03b3(t0, t) \u2265 1\u2212e \u22121 16 \u2265 1 26 ; expanding the definitions of t0 and t, we have that \u03b3b2/\u03b4k\u22121c,d3/\u03b4k\u22121e \u2265 126 . Applying Theorem 3.6 then implies that the spectral gap of M is at least \u03b4k\u22121 78 , as was to be shown.\nProof of Lemma 4.2. The key idea is to use the identity \u2202f\u2202x = f(x) \u2202 log f(x) \u2202x in two places. We have\np\u03b8(z \u2208 S) \u2202 log p\u03b8(z \u2208 S)\n\u2202\u03b8 =\n\u2202\n\u2202\u03b8 p\u03b8(z \u2208 S)\n= \u222b S \u2202p\u03b8(z) \u2202\u03b8 dz\n= \u222b S p\u03b8(z) \u2202 log p\u03b8(z) \u2202\u03b8 dz\n= p\u03b8(z \u2208 S)Ez [ \u2202 log p\u03b8(z)\n\u2202\u03b8 \u2223\u2223\u2223\u2223z \u2208 S] , which completes the lemma."}, {"heading": "B. Correctness of Importance Sampling Algorithm", "text": "In Section 4.1 of the main text, we had a distribution u over Y and a Markov chain A(yt | yt\u22121) on the same space. We then built a distribution over Y\u2217 def= \u22c3 T\u22650{T} \u00d7 YT by sampling T \u223c Geometric( ), y0 \u223c u, and yt | yt\u22121 \u223c A for y = 1, . . . , T (we use pT (y0:T ) to represent the distribution over y0:T given T ).\nFor a given y, we were interested in constructing an importance sampler for (T, y0:T ) | yT = y. The following Lemma establishes the correctness of the importance sampler that was presented. We assume we are interested in computing the expectation of some function g : Y\u2217 \u2192 R and show that the given importance weights correctly estimate E[g]. Lemma B.1. For a distribution F , suppose that we sample T \u223c F and then sample y0:T\u22121 \u223c pT\u22121. Let wt = (1\u2212 )t P[T\u2265t|T\u223cF ]A(y | yt\u22121). Consider the random variable\ng\u0302 def = T\u2211 t=0 wtg(t, y0:t\u22121, y). (13)\nThen\nET\u223cF,y0:T\u22121\u223cpT\u22121 [g\u0302] = ET\u223cGeometric( ),y0:T\u223cpT [g(T, y0:T )I[yT = y]] . (14)\nProof. We have\nET\u223cF,y0:T\u22121\u223cp[g\u0302] = ET\u223cF,y0:T\u22121\u223cpT\u22121 [ T\u2211 t=0 wtg(t, y0:t\u22121, y) ]\n= \u221e\u2211 t=0 P[T \u2265 t | T \u223c F ]Ey0:t\u22121\u223cpt\u22121 [wtg(t, y0:t\u22121, y)]\n= \u221e\u2211 t=0 (1\u2212 )tEy0:t\u22121\u223cpt\u22121 [A(y | yt\u22121)g(t, y0:t\u22121, y)]\n= \u221e\u2211 t=0 (1\u2212 )tEy0:t\u223cpt [g(t, y0:t)I[yt = y]]\n= ET\u223cGeometric( ),y0:T\u223cpT [g(T, y0:T )I[yT = y]] ,\nas was to be shown."}], "references": [{"title": "A new approach to the limit theory of recurrent Markov chains", "author": ["KB Athreya", "P Ney"], "venue": "Transactions of the AMS,", "citeRegEx": "Athreya and Ney.,? \\Q1978\\E", "shortCiteRegEx": "Athreya and Ney.", "year": 1978}, {"title": "Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo"], "venue": "In CoNLL,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Multilevel coarse-to-fine PCFG parsing", "author": ["I Haxton", "C Hill", "R Shrivaths", "J Moore", "M Pozar"], "venue": "In NAACL,", "citeRegEx": "Haxton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Haxton et al\\.", "year": 2006}, {"title": "Discriminative reranking for natural language parsing", "author": ["Michael Collins", "Terry Koo"], "venue": "Computational Linguistics,", "citeRegEx": "Collins and Koo.,? \\Q2005\\E", "shortCiteRegEx": "Collins and Koo.", "year": 2005}, {"title": "Perfect sampling of Harris recurrent Markov chains", "author": ["JN Corcoran", "RL Tweedie"], "venue": null, "citeRegEx": "Corcoran and Tweedie.,? \\Q1998\\E", "shortCiteRegEx": "Corcoran and Tweedie.", "year": 1998}, {"title": "Markov chain Monte Carlo convergence diagnostics: a comparative review", "author": ["MK Cowles", "BP Carlin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cowles and Carlin.,? \\Q1996\\E", "shortCiteRegEx": "Cowles and Carlin.", "year": 1996}, {"title": "Search-based structured prediction", "author": ["H Daum\u00e9 III", "J Langford", "D Marcu"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Elements d\u2019une theorie generale des chaines simples constantes de markoff", "author": ["W Doeblin"], "venue": "In Annales scientifiques de l\u2019E\u0301cole Normale Supe\u0301rieure,", "citeRegEx": "Doeblin.,? \\Q1940\\E", "shortCiteRegEx": "Doeblin.", "year": 1940}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J Duchi", "E Hazan", "Y Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Parallel tempering: Theory, applications, and new perspectives", "author": ["David J Earl", "Michael W Deem"], "venue": "Physical Chemistry Chemical Physics,", "citeRegEx": "Earl and Deem.,? \\Q2005\\E", "shortCiteRegEx": "Earl and Deem.", "year": 2005}, {"title": "Simulating normalizing constants: From importance sampling to bridge sampling to path sampling", "author": ["A Gelman", "XL Meng"], "venue": "Statistical science,", "citeRegEx": "Gelman and Meng.,? \\Q1998\\E", "shortCiteRegEx": "Gelman and Meng.", "year": 1998}, {"title": "A single series from the Gibbs sampler provides a false sense of security", "author": ["A Gelman", "DB Rubin"], "venue": "Bayesian statistics,", "citeRegEx": "Gelman and Rubin.,? \\Q1992\\E", "shortCiteRegEx": "Gelman and Rubin.", "year": 1992}, {"title": "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination", "author": ["PJ Green"], "venue": null, "citeRegEx": "Green.,? \\Q1995\\E", "shortCiteRegEx": "Green.", "year": 1995}, {"title": "Recognition using regions", "author": ["Chunhui Gu", "Joseph J Lim", "Pablo Arbel\u00e1ez", "Jitendra Malik"], "venue": "In CVPR,", "citeRegEx": "Gu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2009}, {"title": "Program verification as probabilistic inference", "author": ["S Gulwani", "N Jojic"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani and Jojic.,? \\Q2007\\E", "shortCiteRegEx": "Gulwani and Jojic.", "year": 2007}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Structured learning with approximate inference", "author": ["Alex Kulesza", "Fernando Pereira"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kulesza and Pereira.,? \\Q2007\\E", "shortCiteRegEx": "Kulesza and Pereira.", "year": 2007}, {"title": "Markov chains and mixing times", "author": ["DA Levin", "Y Peres", "EL Wilmer"], "venue": null, "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Computable bounds for geometric convergence rates of Markov chains", "author": ["SP Meyn", "RL Tweedie"], "venue": "The Annals of Applied Probability,", "citeRegEx": "Meyn and Tweedie.,? \\Q1994\\E", "shortCiteRegEx": "Meyn and Tweedie.", "year": 1994}, {"title": "Exact sampling from a continuous state space", "author": ["DJ Murdoch", "PJ Green"], "venue": "Scandinavian Journal of Stat.,", "citeRegEx": "Murdoch and Green.,? \\Q1998\\E", "shortCiteRegEx": "Murdoch and Green.", "year": 1998}, {"title": "Notes on the KLdivergence between a Markov chain and its equilibrium distribution", "author": ["I Murray", "R Salakhutdinov"], "venue": null, "citeRegEx": "Murray and Salakhutdinov.,? \\Q2008\\E", "shortCiteRegEx": "Murray and Salakhutdinov.", "year": 2008}, {"title": "Coarse-to-fine natural language processing", "author": ["S Petrov"], "venue": null, "citeRegEx": "Petrov.,? \\Q2011\\E", "shortCiteRegEx": "Petrov.", "year": 2011}, {"title": "Exact sampling with coupled Markov chains and applications to statistical mechanics", "author": ["JG Propp", "DB Wilson"], "venue": "Random structures and Algorithms,", "citeRegEx": "Propp and Wilson.,? \\Q1996\\E", "shortCiteRegEx": "Propp and Wilson.", "year": 1996}, {"title": "Bounds on regeneration times and convergence rates for Markov chains", "author": ["GO Roberts", "RL Tweedie"], "venue": "Stochastic Processes and their applications,", "citeRegEx": "Roberts and Tweedie.,? \\Q1999\\E", "shortCiteRegEx": "Roberts and Tweedie.", "year": 1999}, {"title": "Minorization conditions and convergence rates for markov chain monte carlo", "author": ["JS Rosenthal"], "venue": "JASA,", "citeRegEx": "Rosenthal.,? \\Q1995\\E", "shortCiteRegEx": "Rosenthal.", "year": 1995}, {"title": "The new york times annotated corpus", "author": ["Evan Sandhaus"], "venue": "Linguistic Data Consortium, Philadelphia,", "citeRegEx": "Sandhaus.,? \\Q2008\\E", "shortCiteRegEx": "Sandhaus.", "year": 2008}, {"title": "Cascaded models for articulated pose estimation", "author": ["B Sapp", "A Toshev", "B Taskar"], "venue": "In ECCV,", "citeRegEx": "Sapp et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sapp et al\\.", "year": 2010}, {"title": "From invariant checking to invariant inference using randomized search", "author": ["R Sharma", "A Aiken"], "venue": "In CAV,", "citeRegEx": "Sharma and Aiken.,? \\Q2014\\E", "shortCiteRegEx": "Sharma and Aiken.", "year": 2014}, {"title": "Discriminative reranking for machine translation", "author": ["Libin Shen", "Anoop Sarkar", "Franz Josef Och"], "venue": "In NAACL,", "citeRegEx": "Shen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2004}, {"title": "Learning where to sample in structured prediction", "author": ["Tianlin Shi", "Jacob Steinhardt", "Percy Liang"], "venue": null, "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["RS Sutton", "D Mcallester", "S Singh", "Y Mansour"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Robust real-time face detection", "author": ["Paul Viola", "Michael J Jones"], "venue": "International journal of computer vision,", "citeRegEx": "Viola and Jones.,? \\Q2004\\E", "shortCiteRegEx": "Viola and Jones.", "year": 2004}, {"title": "Estimating the wrong graphical model: Benefits in the computation-limited setting", "author": ["Martin J Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Wainwright.,? \\Q2006\\E", "shortCiteRegEx": "Wainwright.", "year": 2006}, {"title": "Sidestepping intractable inference with structured ensemble cascades", "author": ["David Weiss", "Benjamin Sapp", "Ben Taskar"], "venue": "In NIPS,", "citeRegEx": "Weiss et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2010}, {"title": "Discriminative re-ranking of diverse segmentations", "author": ["Payman Yadollahpour", "Dhruv Batra", "Gregory Shakhnarovich"], "venue": "In CVPR,", "citeRegEx": "Yadollahpour et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yadollahpour et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 154, "endOffset": 203}, {"referenceID": 5, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 154, "endOffset": 203}, {"referenceID": 32, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 288, "endOffset": 333}, {"referenceID": 16, "context": "While this recipe can generate good results in practice, it has two notable drawbacks: (i) diagnosing convergence of Markov chains is extremely difficult (Gelman and Rubin, 1992; Cowles and Carlin, 1996); and (ii) approximate inference can be highly suboptimal in the context of learning (Wainwright, 2006; Kulesza and Pereira, 2007).", "startOffset": 288, "endOffset": 333}, {"referenceID": 7, "context": "We construct Markov chains of the following form, called strong Doeblin chains (Doeblin, 1940):", "startOffset": 79, "endOffset": 94}, {"referenceID": 7, "context": "A classic result is that a given strong Doeblin chain mixes in time at most 1 (Doeblin, 1940), and that we can draw an exact sample from the stationary distribution in expected timeO( 1 ) (Corcoran and Tweedie, 1998).", "startOffset": 78, "endOffset": 93}, {"referenceID": 4, "context": "A classic result is that a given strong Doeblin chain mixes in time at most 1 (Doeblin, 1940), and that we can draw an exact sample from the stationary distribution in expected timeO( 1 ) (Corcoran and Tweedie, 1998).", "startOffset": 188, "endOffset": 216}, {"referenceID": 7, "context": "Markov chains that can be expressed according to (2) are said to have strong Doeblin parameter (Doeblin, 1940).", "startOffset": 95, "endOffset": 110}, {"referenceID": 17, "context": "4 of Levin et al. (2009) for more details.", "startOffset": 5, "endOffset": 25}, {"referenceID": 20, "context": "To show this, we make use of the following lemma from Murray and Salakhutdinov (2008): Lemma 3.", "startOffset": 54, "endOffset": 86}, {"referenceID": 8, "context": "At a high level, we can just use Algorithm 1 to compute estimates of the gradient and then apply an online learning algorithm such as ADAGRAD (Duchi et al., 2011) to identify a good choice of \u03b8.", "startOffset": 142, "endOffset": 162}, {"referenceID": 25, "context": "We generated the data by sampling words from the New York Times corpus (Sandhaus, 2008).", "startOffset": 71, "endOffset": 87}, {"referenceID": 8, "context": "All algorithms are trained with AdaGrad (Duchi et al., 2011) with 16 independent chains run for each example.", "startOffset": 40, "endOffset": 60}, {"referenceID": 15, "context": "(2007) and Huang et al. (2012). In contrast, our method directly optimizes the loglikelihood of the data under the distribution \u03c0\u0303\u03b8, so that accuracy continues to increase with more passes through the training data.", "startOffset": 11, "endOffset": 31}, {"referenceID": 14, "context": "This is an important subroutine in loop invariant synthesis, where MCMC methods have recently shown great promise (Gulwani and Jojic, 2007; Sharma and Aiken, 2014).", "startOffset": 114, "endOffset": 163}, {"referenceID": 27, "context": "This is an important subroutine in loop invariant synthesis, where MCMC methods have recently shown great promise (Gulwani and Jojic, 2007; Sharma and Aiken, 2014).", "startOffset": 114, "endOffset": 163}, {"referenceID": 12, "context": "Though Gibbs sampling is the de facto method for many practitioners, there are also many more sophisticated approaches to MCMC (Green, 1995; Earl and Deem, 2005).", "startOffset": 127, "endOffset": 161}, {"referenceID": 9, "context": "Though Gibbs sampling is the de facto method for many practitioners, there are also many more sophisticated approaches to MCMC (Green, 1995; Earl and Deem, 2005).", "startOffset": 127, "endOffset": 161}, {"referenceID": 30, "context": "Our learning algorithm is reminiscent of policy gradient algorithms in reinforcement learning (Sutton et al., 2000), as well as Searn, which tries to learn an optimal search policy for structured prediction (Daum\u00e9 III et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 10, "context": "Our staged construction is also similar in spirit to path sampling (Gelman and Meng, 1998), as it uses a multi-stage approach to smoothly transition from a very simple to a very complex distribution.", "startOffset": 67, "endOffset": 90}, {"referenceID": 31, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 28, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 3, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 1, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 13, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 33, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 26, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 21, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 34, "context": "Our staged Doeblin construction belongs to the family of coarse-to-fine inference methods, which operate on progressively more complex models (Viola and Jones, 2004; Shen et al., 2004; Collins and Koo, 2005; Charniak et al., 2006; Carreras et al., 2008; Gu et al., 2009; Weiss et al., 2010; Sapp et al., 2010; Petrov, 2011; Yadollahpour et al., 2013).", "startOffset": 142, "endOffset": 350}, {"referenceID": 7, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 23, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 18, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 0, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978).", "startOffset": 165, "endOffset": 254}, {"referenceID": 22, "context": "The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 264, "endOffset": 341}, {"referenceID": 4, "context": "The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 264, "endOffset": 341}, {"referenceID": 19, "context": "The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 264, "endOffset": 341}, {"referenceID": 2, "context": ", 2000), as well as Searn, which tries to learn an optimal search policy for structured prediction (Daum\u00e9 III et al., 2009); see also Shi et al. (2015), who apply reinforcement learning in the context of MCMC.", "startOffset": 106, "endOffset": 152}, {"referenceID": 0, "context": "On the theoretical front, we make use of the well-developed theory of strong Doeblin chains, often also referred to with the terms minorization or regeneration time (Doeblin, 1940; Roberts and Tweedie, 1999; Meyn and Tweedie, 1994; Athreya and Ney, 1978). The strong Doeblin property is typically used to study convergence of continuousspace Markov chains, but Rosenthal (1995) has used it to analyze Gibbs sampling, and several authors have provided algorithms for sampling exactly from arbitrary strong Doeblin chains (Propp and Wilson, 1996; Corcoran and Tweedie, 1998; Murdoch and Green, 1998).", "startOffset": 232, "endOffset": 378}], "year": 2015, "abstractText": "Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the approximations can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks.", "creator": "LaTeX with hyperref package"}}}