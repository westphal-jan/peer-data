{"id": "1503.00185", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "abstract": "Recursive 206.2 neural models, endocannabinoid which use givebacks syntactic parse 2029 trees unornamented to recursively generate representations 7.70 bottom - zew up grunde from spinosum parse antiterrorism children, are a popular new sk\u00e5nes architecture, promising to leptis capture structural properties vopi like fernandomania the scope shobokshi of fried negation or vigier long - neo-marxist distance semantic 168th dependencies. But sines understanding minnig exactly elee@ajc.com which non-bank tasks this aeu parse - 577,000 based method romine is kemayoran appropriate henryson for nogay remains occidentalis an open stallman question. rivi\u00e8re-des-prairies In fescue this lonwabo paper coxey we benchmark reloadable recursive neural 69.83 models palindromes against health sequential 792,000 recurrent ecms neural models, 82.54 which zhuan are mencken structured sherlock solely moen on couldnt word speckling sequences. dels We garvie investigate 5 3,180 tasks: sentiment impey classification barakah on (clonmore 1) cwr sentences jacksonian and (2) mircea syntactic phrases; (3) cochell question answering; (damle 4) discourse parsing; (8,125 5) cluley semantic meroe relations (negasso e. g. , component - whole jettisoning between caring nouns ); We dictations find darey that due\u00f1as recurrent chaturdashi models 5-13 have equal or superior dodik performance to civlians recursive naqoura models sarojini on entreri all tasks except one: semantic relations qu\u00fd between palen nominals. fitkin Our hasselhoff analysis ferriere suggests that tasks 73.27 relying mankin on the shoda scope of negation (aprilia like sentiment) doclea are aigburth well - gunfires handled by sequential models. mahadi Recursive models help only n'gotty with +1.50 tasks that margai require endaya representing long - distance l.m. relations between re-selected words. sauls Our spiddal results khon offer manne insights 24x7 on janjigian the design ulemas of euro500 neural architectures sonett for neo-fascist representation learning.", "histories": [["v1", "Sat, 28 Feb 2015 21:39:31 GMT  (578kb,D)", "http://arxiv.org/abs/1503.00185v1", null], ["v2", "Fri, 6 Mar 2015 18:16:50 GMT  (584kb,D)", "http://arxiv.org/abs/1503.00185v2", null], ["v3", "Fri, 24 Apr 2015 17:14:49 GMT  (585kb,D)", "http://arxiv.org/abs/1503.00185v3", null], ["v4", "Thu, 18 Jun 2015 22:07:45 GMT  (679kb,D)", "http://arxiv.org/abs/1503.00185v4", null], ["v5", "Tue, 18 Aug 2015 05:59:18 GMT  (261kb,D)", "http://arxiv.org/abs/1503.00185v5", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["jiwei li", "thang luong", "dan jurafsky", "eduard h hovy"], "accepted": true, "id": "1503.00185"}, "pdf": {"name": "1503.00185.pdf", "metadata": {"source": "CRF", "title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "authors": ["Jiwei Li", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu,", "jurafsky@stanford.edu,", "ehovy@andrew.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep learning based methods learn low-dimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., [1, 2, 3]), successfully capturing capturing syntactic and semantic aspects of text.\nFor tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a convolutional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Model for achieving this usually fall into two categories: recurrent models and recursive models:\nRecurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10]. They were also applied early on to NLP [11], modeling a sentence\nar X\niv :1\n50 3.\n00 18\n5v 1\n[ cs\n.A I]\n2 8\nFe b\nas tokens processed sequentially, at each step combining the current token with previously built embeddings. These models generally consider no linguistic structure aside from word order.\nRecursive neural models [12], by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of the parse tree, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of tree is reached. Figure 1 shows a recursive neural networks building a distributed representation for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].\nOne possible advantage of recursive models ones is their potential for capturing compositional semantics, such as the way negation, modality, or quantification compose in a tree. Another is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree [18]. But we don\u2019t know which of these advantages is more important, and for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, since parsing is relatively slow, can be errorful, and parsers are also usually quite domain-dependent.\nOn the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequence-to-sequence generation [25] for machine translation (e.g., [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.\nOur goal in this paper is thus to investigate a number of task with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate five tasks with different properties. Binary sentiment classification at the sentence level ([30]) and the phrase level (Sentiment Treebank [17]) can help us understand the role of recursive models in dealing with semantic compositionality. Question answering on the UMD QA dataset [20] can helps see whether parsing is useful for finding similarities between source sentences and target. Discourse parsing (RST dataset) is useful for measuring the extent to which parsing improves discourse tasks that need to combine meanings of larger tex units. and Semantic Relation Classification on the SemEval-2010 [31] data helps us understand whether parsing is helpful in dealing with long-term dependencies, such as relations between two words that are far apart in the sequence."}, {"heading": "2 Recursive and Recurrent Models", "text": "In this section, we introduce the recursive and recurrent models we consider. Note that our paper applies existing models to existing tasks, offering no novel algorithms or tasks or new state-of-theart results; . Our goal is rather an analytic one, to investigate different versions of recursive and recurrent models. We therefore pair each recurrent model with a matching recursive one (standard recurrent vs standard recursive, bidirectional recurrent vs bidirectional recursive, etc.). 1"}, {"heading": "2.1 Notations", "text": "We assume that the text unit S, which could be a phrase, a sentence or a document, is comprised of a sequence of tokens/words: S = {w1,w2, ...,wNS }, where Ns denotes the number of tokens in S. Each word w is associated with a K-dimensional vector embedding ew = {e1w, e 2 w, ..., e K w}. The\n1Note that we are unable to investigate various sophisticated but less general algorithm variants, such as the use of auto-encoder pre-training in the recursive models of [32], or the MV-RNN use of representing each word with both a matrix and a vector in word-word-convolutions [21] since these approaches are usually tailored to specific task scenarios.\ngoal of recursive and recurrent models is to map the sequence to a K-dimensional eS, based on its tokens and their correspondent embeddings."}, {"heading": "2.2 Recurrent Models", "text": "Standard Recurrent Models A recurrent network successively takes wordwi at step i, combines its vector representation ewt with previously built embedding ei\u22121 from time i \u2212 1, calculates the resulting current embedding et, and passes it to the next step. The embedding et for the current time t is thus: et = f(W \u00b7 et\u22121 + V \u00b7 ewt) (1) where W and V are K\u00d7 K dimensional convolutional matrixes and f(\u00b7) denotes the activation function. Bias vectors are omitted. If Ns denote the length of the sequence, eNs represents the whole sequence S.\nBidirectional Recurrent Models [33] add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly:\ne\u2192t = f(W \u2192 \u00b7 e\u2192t\u22121 + V\u2192 \u00b7 ewt) e\u2190t = f(W \u2190 \u00b7 e\u2190t\u22121 + V\u2190 \u00b7 ewt)\n(2)\nNormally, bidirectional models feed the concatenation vector calculated from both directions [e\u21901 , e \u2192 NS ] to the classifier.\nLong Short Term Memory (LSTM) LSTM [34, 35] constitutes a sophisticated version of recurrent models by associating each timestep with an input, memory and output gate, denoted by it, ft and ot, the values of which are given by\nit = \u03c3(Wi \u00b7 ext + Vi \u00b7 et\u22121) ft = \u03c3(Wf \u00b7 ext + Vf \u00b7 et\u22121) ot = \u03c3(Wo \u00b7 ext + Vo \u00b7 et\u22121)\n(3)\nwhere \u03c3 denotes the sigmoid function, and it, ft and ot are scalars in [0,1]. The current timestep embedding et is then given by:\nlt = tanh(Wl \u00b7 ext + Vl \u00b7 et\u22121) mt = ft \u00b7mt\u22121 + it \u00b7 lt et = ot \u00b7mt\n(4)\nSee [34, 27] for further details on LSTM.\nBidirectional LSTM As in bidirectional recurrent models, Bi-LSTM obtains embeddings forwardly and backwardly and concatenates vectors calculated from both directions for classification.\nRecurrent Tensor Network We extended the RNTN [17] recursive model to the recurrent version which enables richer convolution between early time output et\u22121 and current word embedding ewt . Adjusted from the recursive RNTN, let e = [et\u22121, ewt ] denote the concatenation of et\u22121 and ewt , the output embedding at et is given by:\net = f(e T \u00b7W \u00b7 e+ V \u00b7 e) (5)"}, {"heading": "2.3 Recursive Models", "text": "Standard Recursive Models Recursive models rely on the structure of parse trees, where each leaf node corresponds to a word from the original sentence, computing a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. Concretely, for a given node \u03b7 in the tree and its left child \u03b7left (associated with vector representation eleft) and right child \u03b7right (associated with vector representation eright), the standard recursive network calculates e\u03b7 as follows:\ne\u03b7 = f(W \u00b7 ecleft + V \u00b7 ecright) (6) whereW and V are K\u00d7 K convolutional matrixes and f(\u00b7) denotes the activation function.\nBidirectional Recursive Models extend standard recursive models and consider both downward and upward information propagations along the tree [18]. Each node p is associated with two vectors, an upward vector e\u2191\u03b7 and a downward vector e \u2193 \u03b7. e \u2191 \u03b7 is computed similarly as in standard recursive models based on two immediate children:\ne\u2191\u03b7 = f(W \u2191 \u00b7 e\u2191left + V \u2191 \u00b7 e\u2191right) (7)\nThe downward vector e\u2193\u03b7 is computed based on its parent h(\u03b7) and upward vector e \u2191 \u03b7:\ne\u2193\u03b7 = f(W \u2193e\u2193 h(\u03b7) + V \u2193e\u2191\u03b7) (8)\nwhere W\u2193, W\u2191, V\u2193 and V\u2191 denote convolutional matrixes. Intuitively, e\u2191\u03b7 includes evidence from the descendents of \u03b7 while e\u2193\u03b7 includes information from ancestors.\nRecursive Neural Tensor Network (RNTN) RNTN [17] enables richer composition between children embeddings. Let e = [eleft, eright] denote the concatenation of the two children\u2019s vectors. RNTN computes the parent representation e\u03b7 as follows:\ne\u03b7 = f(e T \u00b7W \u00b7 e+ V \u00b7 e) (9)\nGate Based Recursive Models (Gate-Recursive) We adjust the idea of LSTM to recursive model where the representation of a node preserves part of the previous information and adds in the convolutional part. Similarly, let r\u03b7 and z\u03b7 denote the control gates associated with node \u03b7, given by:\nz\u03b7 = \u03c3(Wz \u00b7 eleft + Vz \u00b7 eright) r\u03b7 = \u03c3(Wr \u00b7 eleft + Vr \u00b7 eright)\n(10)\ne\u03b7 is then given by: e\u03b7 = z\u03b7 \u00b7 (eleft + eright) + (1 \u2212 z\u03b7)e \u2032\u03b7 e \u2032\u03b7 = f(W \u00b7 r\u03b7 \u00b7 eleft + V \u00b7 r\u03b7 \u00b7 eright)\n(11)"}, {"heading": "3 Experiments", "text": "In this section, we detail our experimental settings and results. We consider 5 tasks, each representative of a different class of NLP task.\n1. Binary sentiment classification on the [30] dataset. This addresses the issues where supervisions only exist globally at top of long sequences.\n2. Sentiment Classification on the Stanford Sentiment Treebank [17]: comprehensive labels are found at token and phrase levels where local compositionally (such as from negation, mood, or other phrase-structured modifications) are to be learned.\n3. Question Answering on the UMD QA dataset [20]: Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different timesteps for recurrent models.\n4. Semantic Relation Classification on the SemEval-2010 task [31]. Learns long-distance relationships between two words that may be far apart sequentially.\n5. Discourse Parsing [24, 36]: Learns sentence-to-sentence relations based on calculated representations.\nIn each case we followed the protocols described in the original papers. We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad [37] with minibatches [38]. Parameters are tuned using development dataset if already found from original datasets or from cross-validation if not. Derivatives are calculated from standard back-propagation [39]. Parameters to tune include size of mini batches, learning rate, parameters for L2 penalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance in the development set is used as the final model to be evaluated.2"}, {"heading": "3.1 Binary Sentiment Classification (Pang)", "text": "The sentiment dataset of [30] consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pre-training procedure as described in [32] is employed. Word embeddings are initialized using GloVe [40] and kept fixed in the learning procedure3. Sentence level embeddings are fed into a sigmoid classifier: For bidirectional recurrent models, concatenation of embeddings calculated leftwardly and rightwardly are used as sentence-level vectors.\nPerformances are given in Figure 2; no large difference appears between standard recursive and recurrent models or RNTN-Recursive and RNTN-recurrent models. LSTM/BiLSTM achieve the best performances, slightly outperforming Gate-Recursive.\nWhy don\u2019t parse trees help on this task? One possible explanation is the distance of the supervision signal from the local compositional structure. The Pang et al. dataset has an average sentence length of 22.5 words, which means it takes multiple steps before sentiment related evidence comes up to the surface. It is therefore unclear whether local compositional operators such as negation can be learned; there is only a small amount of training data and the sentiment supervision only at the level of the sentence may not be easy to propogate down to deeply buried local phrases. By contrast, gates in LSTM have the capabilities of filtering out irrelevant information and preserving useful evidence, which may explain the better performances achieved by LSTM."}, {"heading": "3.2 Stanford Sentiment TreeBank", "text": "Since sentiment labels at the sentence level may be too coarse to show the benefits of parse structure, we turn to the Stanford Sentiment TreeBank [17]. This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words. The task can either be 5-way fine-grained classification or binary coarse classification.\nFor recursive models, we followed the protocols in [17] where node embeddings in the parse trees are obtained from recursive models and then fed to a softmax classifier. Word embeddings are initialized from the uniform distribution [-0.1, 0.1] and are optimized throughout the frameworks.\nThe original treebank dataset is tailored for recursive models as sentiment labels are tagged at parse tree nodes. We transformed the dataset for recurrent model use as illustrated in Figure 3. Each phrase is reconstructed from parse tree nodes and treated as a separate data point. As the treebank contains\n2Note that bi-directional recursive models are suited for classification at the node level by considering upward and downward information, but not sentence level classification. No downward vector is computed for roots\u2014 downward vectors are computed based on parents\u2014concatenating root embedding (upwardly computed) with leaf embeddings (downwardly computed) leads to vectors with different dimensionality. We therefore omit the bi-directional recursive model for sentence-level classification tasks.\n3Slightly better performances are observed by using GloVe than Word2Vect (either skip-grammar or CBOW) [41, 42] and SENNA [3]. Better performances are observed when keeping word embeddings fixed.\n11,855 sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples4.\nWe report accuracies on both multi-class and binary classification in Figure 5. For binary classification, we did not train a separate model, just decoding the obtained probability vector into a binary decision as in [14]5. Again we see no difference between standard-recurrent vs standard-recursive models, or RNTN-Recursive vs RNTN-Recurrent modes. Unlike the earlier sentence-level classification task, gate based models (Gate-Recursive, LSTM and Bi-LSTM) do not yield better performances than standard models. This may be because each sentence has been chunked into very small pieces, so that the negative impact from the gradient vanishing is significantly alleviated.\nFigure 4 illustrates sentiment predictions at each timestep, computed by feeding vectors from the one-directional (left to right) LSTM into the softmax classifier. The figure suggests that the kind of local compositions that recursive models handle, like negation flipping the sentiment of positive examples [17], can also be captured in sequential models. Figure 4 also illustrates that despite the fact that the recurrent formulation truncatest Recurrent models are thus able to preserve information with respect to the objective (in this case sentiment) despite the fact that representations are obtained sequentially at each word, which may be mid-constituent, violating syntactic or semantic structure. This suggests that for tasks like sentiment analysis, full compositional semantic structure may not be as important as capturing the few key words that capture relevant sentiment information.\n4Recursive models obtain all node embeddings in trees jointly. For recurrent models, as we treat each phrase as a separate example, the embedding for each example is calculated from scratch, which means recurrent models take much more time to train in this case.\n5The performance of our implementations of recursive models is not exactly identical to that reported in [17], but the relative difference is less than 1%.\nAs mentioned above, none of the models we describe are state-of-the-art. State-of-the-art models [2, 14] employ more sophisticated recursive structures. Again, our goal is only to compare equivalent recursive and recurrent models, and so we do not describe the details or performances of other models, here or for the following tasks."}, {"heading": "3.3 UMD Question Answering Dataset", "text": "The question-answering dataset QANTA6 is comprised of 1,713 history questions (8,149 questions) and 2,246 literature questions (10,552 sentences), each question paired with an answer [20]. We give an illustration example here:\nQuestion: He left unfinished a novel whose title character forges his father\u2019s signature to get out of school and avoids the draft by feigning desire to join. Name this German author of The Magic Mountain and Death in Venice.\nAnswer: Thomas Mann.\nThe model of [20] minimizes the distances between answer embeddings and node embeddings along the parse tree of the question. Concretely, let c denote the correct answer to question S, with embedding ~c, and z denoting any random wrong answer. The objective function wsums over the dot product between representation for every node \u03b7 along the question parse trees and the answer rep-\n6http://cs.umd.edu/\u02dcmiyyer/qblearn/. Because the publicly released dataset is smaller than the version used in [20] due to privacy issues, our numbers are not comparable to those in [20].\nresentations: L = \u2211 \u03b7\u2208[parse tree] \u2211 z max(0, 1 \u2212 ~c \u00b7 e\u03b7 + ~z \u00b7 e\u03b7) (12)\nwhere ~s denotes the embedding for parse tree node s calculated from the recursive neural model. Here the parse trees are dependency parses following [20].\nRecurrent version models minimize the distance between the answer embedding and embeddings calculated from each timestep of the sequence:\nL = \u2211\nt\u2208[1,Ns]\n\u2211 z max(0, 1 \u2212 ~c \u00b7 et + ~z \u00b7 et) (13)\nThe concatenation of e\u2190t and e \u2192 t (or e \u2191 t and e \u2193 t) in bi-directional models will double the dimensionality of embedding, a further convolutional operation is used to preserve vector dimensionality:\n~et = f(WL \u00b7 [e\u2190t , e\u2192t ]) (14) where WL denotes a 2K \u00d7 K dimensional matrix. As demonstrated in Figure 6, we see no great difference between recurrent and recursive models.\nThe UMD-QA task represents one group of situations where because we have insufficient supervision about matching (it\u2019s hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps). Similar ideas can be found in pooling structures (e.g. [43]). We can draw a similar conclusion as from Stanford Sentiment Bank: in this task, recursive models look at the relevance to the target of linguistically meaningful units from parse trees, but this does not necessarily lead to better performance than simple sequential units."}, {"heading": "3.4 Semantic Relationship Classification", "text": "SemEval-2010 Task 8 [31] is to find semantic relationships between pairs of nominals, e.g., in \u201cMy [apartment]e1 has a pretty large [kitchen]e2\u201d classifying the relation between [apartment] and [kitchen] as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see [31, 21] for details.\nFor the recursive implementations, we follow the neural framework defined in [21]. The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier. 7 Retrieved paths are transformed for the recurrent models as shown in Figure 9.\n7[21] achieve state-of-art performance by combining a sophisticated model, MV-RNN, in which each word is presented with both a matrix and a vector with human-feature engineering. Again, because MV-RNN is difficult to adapt to a recurrent version, we do not employ this state-of-the-art model, adhering only to the general versions of recursive models described in Section 2, since our main goal is to compare equivalent recursive and recurrent models rather than implement the state of the art.\nResults are shown in Figure 8. Unlike for earlier tasks, here recursive models yield much better performance than the corresponding recurrent versions. Even standard recursive models perform better than LSTM.\nThese results suggest that it is the need to integrate structures far apart in the sentence that characterizes the tasks where recursive models surpass recurrent models. In parse-based models, the two target words are drawn together much earlier in the decision process than in recurrent models, which remembers one target until the other one appears."}, {"heading": "3.5 Discourse Parsing", "text": "Our final task, discourse parsing based on the RST-DT corpus [44], is to build a discourse tree for a document, based on assigning Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs). Because discourse relations express the coherence structure of discourse, they presumably express different aspects of compositional meaning than sentiment or nominal relations. A discourse relation classifier takes as input a vector embedding representation of each of the two EDUs, and computes the most likely relation [24]. We compare recursive versus recurrent methods for computing EDU embeddings. See [45, 36] for more details on discourse parsing and the RST-DT corpus.\nRepresentations for adjacent EDUs are fed into binary classification (whether two EDUs are related) and multi-class relation classification models, as defined in [24]. Following [24], a CKY bottomup algorithm is used to compute the most likely discourse parse tree using dynamic programming. Models are evaluated in terms of three matrices: Span (on blank tree structures), nuclearity (on tree structures with nuclearity indication), and relation (on tree structures with rhetorical relation indication but no nuclearity indication)9. To serve the purposes for plain comparison, no additional human-developed features are added.\nResults are reported in Table 1. We see no large differences between equivalent recurrent and recursive models. Again Bi-LSTM achieves the best performance. Our results suggest that the aspects of meaning compositionality necessary for computing discourse relations between clauses are sufficiently handeled by recurrent models.\n9Nuclearity indicates which one of the two adjacent EDUs are more salient."}, {"heading": "4 Discussions", "text": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20]. For all but one, semantic relations, recurrent models perform as well or better than recursive models.\nOur results suggest that recurrent models, especially LSTMs, are sufficient to capture the kind of compositional semantics necessary to build sentiment for phrases or sentences, or discourse relations between sentences, or link answers to parts of questions. It is only in tasks like semantic relation extratcion, in which single headwords need to be associated across a long distance, that recursive models shine. Improving sequential models to be able to model such phenomena will require methods for modeling such long-distance dependencies."}], "references": [{"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Learning state space trajectories in recurrent neural networks", "author": ["Barak A Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Neural networks for time series processing", "author": ["Georg Dorffner"], "venue": "In Neural Network World. Citeseer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "The use of recurrent neural networks in continuous speech recognition", "author": ["Tony Robinson", "Mike Hochberg", "Steve Renals"], "venue": "In Automatic speech and speaker recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Review of neural networks for speech recognition", "author": ["Richard P Lippmann"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Alex Graves", "Juergen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Alex Graves"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Samuel R Bowman"], "venue": "arXiv preprint arXiv:1312.6192,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Recursive neural networks for learning logical semantics", "author": ["Samuel R Bowman", "Christopher Potts", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1406.1827,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Bidirectional recursive neural networks for token-level labeling with structure", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "arXiv preprint arXiv:1312.0493,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["Kazuma Hashimoto", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama"], "venue": "In EMNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "A model of coherence based on distributed sentence representation", "author": ["Jiwei Li", "Eduard Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": "In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Lstm can solve hard long time lag problems", "author": [], "venue": "In Advances in Neural Information Processing Systems 9: Proceedings of the 1996 Conference,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1997}, {"title": "Hilda: a discourse parser using support vector machine classification", "author": ["Hugo Hernault", "Helmut Prendinger", "Mitsuru Ishizuka"], "venue": "Dialogue & Discourse,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory", "author": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2, 3]), successfully capturing capturing syntactic and semantic aspects of text.", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": ", [1, 2, 3]), successfully capturing capturing syntactic and semantic aspects of text.", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ", [1, 2, 3]), successfully capturing capturing syntactic and semantic aspects of text.", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 57, "endOffset": 63}, {"referenceID": 4, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 57, "endOffset": 63}, {"referenceID": 5, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 88, "endOffset": 97}, {"referenceID": 6, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 88, "endOffset": 97}, {"referenceID": 7, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 88, "endOffset": 97}, {"referenceID": 8, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 125, "endOffset": 132}, {"referenceID": 9, "context": "Recurrent models deal successfully with time-series data [4, 5] like speech recognition [6, 7, 8] or handwriting recognition [9, 10].", "startOffset": 125, "endOffset": 132}, {"referenceID": 10, "context": "They were also applied early on to NLP [11], modeling a sentence", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "Recursive neural models [12], by contrast, are structured by syntactic parse trees.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 15, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 16, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 98, "endOffset": 110}, {"referenceID": 17, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 98, "endOffset": 110}, {"referenceID": 18, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 98, "endOffset": 110}, {"referenceID": 19, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 161, "endOffset": 169}, {"referenceID": 21, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 161, "endOffset": 169}, {"referenceID": 22, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 23, "context": ", [13, 14]), and applied to various NLP tasks, among them entailment [15, 16], sentiment analysis [17, 18, 19], question-answering [20], relation classification [21, 22], and discourse [23, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 17, "context": "For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "Recurrent models without parse structures have shown good results in sequence-to-sequence generation [25] for machine translation (e.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 2, "endOffset": 14}, {"referenceID": 27, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 2, "endOffset": 14}, {"referenceID": 28, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 98, "endOffset": 101}, {"referenceID": 16, "context": ", [26, 27, 28]), parsing [29], and sentiment, where for example recurrent-based paragraph vectors [2] outperform recursive models [17] on the Stanford sentiment-bank dataset.", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "Binary sentiment classification at the sentence level ([30]) and the phrase level (Sentiment Treebank [17]) can help us understand the role of recursive models in dealing with semantic compositionality.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "Binary sentiment classification at the sentence level ([30]) and the phrase level (Sentiment Treebank [17]) can help us understand the role of recursive models in dealing with semantic compositionality.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "Question answering on the UMD QA dataset [20] can helps see whether parsing is useful for finding similarities between source sentences and target.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "and Semantic Relation Classification on the SemEval-2010 [31] data helps us understand whether parsing is helpful in dealing with long-term dependencies, such as relations between two words that are far apart in the sequence.", "startOffset": 57, "endOffset": 61}, {"referenceID": 31, "context": "The 1Note that we are unable to investigate various sophisticated but less general algorithm variants, such as the use of auto-encoder pre-training in the recursive models of [32], or the MV-RNN use of representing each word with both a matrix and a vector in word-word-convolutions [21] since these approaches are usually tailored to specific task scenarios.", "startOffset": 175, "endOffset": 179}, {"referenceID": 20, "context": "The 1Note that we are unable to investigate various sophisticated but less general algorithm variants, such as the use of auto-encoder pre-training in the recursive models of [32], or the MV-RNN use of representing each word with both a matrix and a vector in word-word-convolutions [21] since these approaches are usually tailored to specific task scenarios.", "startOffset": 283, "endOffset": 287}, {"referenceID": 32, "context": "Bidirectional Recurrent Models [33] add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly: et = f(W \u2192 \u00b7 et\u22121 + V\u2192 \u00b7 ewt) et = f(W \u2190 \u00b7 et\u22121 + V\u2190 \u00b7 ewt) (2)", "startOffset": 31, "endOffset": 35}, {"referenceID": 33, "context": "Long Short Term Memory (LSTM) LSTM [34, 35] constitutes a sophisticated version of recurrent models by associating each timestep with an input, memory and output gate, denoted by it, ft and ot, the values of which are given by it = \u03c3(Wi \u00b7 ext + Vi \u00b7 et\u22121) ft = \u03c3(Wf \u00b7 ext + Vf \u00b7 et\u22121) ot = \u03c3(Wo \u00b7 ext + Vo \u00b7 et\u22121) (3)", "startOffset": 35, "endOffset": 43}, {"referenceID": 34, "context": "Long Short Term Memory (LSTM) LSTM [34, 35] constitutes a sophisticated version of recurrent models by associating each timestep with an input, memory and output gate, denoted by it, ft and ot, the values of which are given by it = \u03c3(Wi \u00b7 ext + Vi \u00b7 et\u22121) ft = \u03c3(Wf \u00b7 ext + Vf \u00b7 et\u22121) ot = \u03c3(Wo \u00b7 ext + Vo \u00b7 et\u22121) (3)", "startOffset": 35, "endOffset": 43}, {"referenceID": 0, "context": "where \u03c3 denotes the sigmoid function, and it, ft and ot are scalars in [0,1].", "startOffset": 71, "endOffset": 76}, {"referenceID": 33, "context": "See [34, 27] for further details on LSTM.", "startOffset": 4, "endOffset": 12}, {"referenceID": 26, "context": "See [34, 27] for further details on LSTM.", "startOffset": 4, "endOffset": 12}, {"referenceID": 16, "context": "Recurrent Tensor Network We extended the RNTN [17] recursive model to the recurrent version which enables richer convolution between early time output et\u22121 and current word embedding ewt .", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Bidirectional Recursive Models extend standard recursive models and consider both downward and upward information propagations along the tree [18].", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "Recursive Neural Tensor Network (RNTN) RNTN [17] enables richer composition between children embeddings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "Binary sentiment classification on the [30] dataset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Sentiment Classification on the Stanford Sentiment Treebank [17]: comprehensive labels are found at token and phrase levels where local compositionally (such as from negation, mood, or other phrase-structured modifications) are to be learned.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Question Answering on the UMD QA dataset [20]: Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different timesteps for recurrent models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "Semantic Relation Classification on the SemEval-2010 task [31].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "Discourse Parsing [24, 36]: Learns sentence-to-sentence relations based on calculated representations.", "startOffset": 18, "endOffset": 26}, {"referenceID": 35, "context": "Discourse Parsing [24, 36]: Learns sentence-to-sentence relations based on calculated representations.", "startOffset": 18, "endOffset": 26}, {"referenceID": 36, "context": "We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad [37] with minibatches [38].", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad [37] with minibatches [38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "Derivatives are calculated from standard back-propagation [39].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "The sentiment dataset of [30] consists of sentences with a sentiment label for each sentence.", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "No pre-training procedure as described in [32] is employed.", "startOffset": 42, "endOffset": 46}, {"referenceID": 39, "context": "Word embeddings are initialized using GloVe [40] and kept fixed in the learning procedure3.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "Since sentiment labels at the sentence level may be too coarse to show the benefits of parse structure, we turn to the Stanford Sentiment TreeBank [17].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "For recursive models, we followed the protocols in [17] where node embeddings in the parse trees are obtained from recursive models and then fed to a softmax classifier.", "startOffset": 51, "endOffset": 55}, {"referenceID": 40, "context": "3Slightly better performances are observed by using GloVe than Word2Vect (either skip-grammar or CBOW) [41, 42] and SENNA [3].", "startOffset": 103, "endOffset": 111}, {"referenceID": 41, "context": "3Slightly better performances are observed by using GloVe than Word2Vect (either skip-grammar or CBOW) [41, 42] and SENNA [3].", "startOffset": 103, "endOffset": 111}, {"referenceID": 2, "context": "3Slightly better performances are observed by using GloVe than Word2Vect (either skip-grammar or CBOW) [41, 42] and SENNA [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 13, "context": "For binary classification, we did not train a separate model, just decoding the obtained probability vector into a binary decision as in [14]5.", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "The figure suggests that the kind of local compositions that recursive models handle, like negation flipping the sentiment of positive examples [17], can also be captured in sequential models.", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "5The performance of our implementations of recursive models is not exactly identical to that reported in [17], but the relative difference is less than 1%.", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "State-of-the-art models [2, 14] employ more sophisticated recursive structures.", "startOffset": 24, "endOffset": 31}, {"referenceID": 13, "context": "State-of-the-art models [2, 14] employ more sophisticated recursive structures.", "startOffset": 24, "endOffset": 31}, {"referenceID": 19, "context": "The question-answering dataset QANTA6 is comprised of 1,713 history questions (8,149 questions) and 2,246 literature questions (10,552 sentences), each question paired with an answer [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "The model of [20] minimizes the distances between answer embeddings and node embeddings along the parse tree of the question.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Because the publicly released dataset is smaller than the version used in [20] due to privacy issues, our numbers are not comparable to those in [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Because the publicly released dataset is smaller than the version used in [20] due to privacy issues, our numbers are not comparable to those in [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "Here the parse trees are dependency parses following [20].", "startOffset": 53, "endOffset": 57}, {"referenceID": 42, "context": "[43]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "SemEval-2010 Task 8 [31] is to find semantic relationships between pairs of nominals, e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see [31, 21] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see [31, 21] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "For the recursive implementations, we follow the neural framework defined in [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "7[21] achieve state-of-art performance by combining a sophisticated model, MV-RNN, in which each word is presented with both a matrix and a vector with human-feature engineering.", "startOffset": 1, "endOffset": 5}, {"referenceID": 43, "context": "Our final task, discourse parsing based on the RST-DT corpus [44], is to build a discourse tree for a document, based on assigning Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs).", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "A discourse relation classifier takes as input a vector embedding representation of each of the two EDUs, and computes the most likely relation [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 35, "context": "See [45, 36] for more details on discourse parsing and the RST-DT corpus.", "startOffset": 4, "endOffset": 12}, {"referenceID": 23, "context": "Representations for adjacent EDUs are fed into binary classification (whether two EDUs are related) and multi-class relation classification models, as defined in [24].", "startOffset": 162, "endOffset": 166}, {"referenceID": 23, "context": "Following [24], a CKY bottomup algorithm is used to compute the most likely discourse parse tree using dynamic programming.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}, {"referenceID": 16, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}, {"referenceID": 23, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}, {"referenceID": 19, "context": "We compared recursive and recurrent neural models for representation learning on five separate NLP tasks for which recursive neural models are known to achieve good performance [21, 17, 24, 20].", "startOffset": 177, "endOffset": 193}], "year": 2015, "abstractText": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up from parse children, are a popular new architecture, promising to capture structural properties like the scope of negation or long-distance semantic dependencies. But understanding exactly which tasks this parse-based method is appropriate for remains an open question. In this paper we benchmark recursive neural models against sequential recurrent neural models, which are structured solely on word sequences. We investigate 5 tasks: sentiment classification on (1) sentences and (2) syntactic phrases; (3) question answering; (4) discourse parsing; (5) semantic relations (e.g., component-whole between nouns); We find that recurrent models have equal or superior performance to recursive models on all tasks except one: semantic relations between nominals. Our analysis suggests that tasks relying on the scope of negation (like sentiment) are well-handled by sequential models. Recursive models help only with tasks that require representing long-distance relations between words. Our results offer insights on the design of neural architectures for representation learning.", "creator": "LaTeX with hyperref package"}}}