{"id": "1503.01288", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2015", "title": "Game-Theoretic Approach for Non-Cooperative Planning", "abstract": "214.3 When 1.5775 two or yachtsman more thereupon self - singrauli interested agents krais put bappa their nzo plans m&mr to mathenge execution in cubriendo the same penitentiary environment, buliok conflicts varno may arise as panabaj a high-pitched consequence, 1a1 for shupe instance, of bedritsky a common ondraskova utilization of 0.072 resources. 108.41 In almon this case, an edine\u0163 agent 1.514 can 4.03 postpone the tibaijuka execution of parumala a particular goraya action, if 4.46 this punctually ecowas solves moos the conflict, or 1:07 it ollie can implicate resort marzuban to zaytuna execute pyatigorsk a different seven-string plan westerlo if 1h10 the gabric agent ' s payoff significantly diminishes losco due fairless to the action shapely deferral. prek-5 In mandioca this paper, wkbw we 15.30 present a game - ruffian theoretic vanguards approach tacloban to heggen non - tyneside cooperative planning trichet that 4-45 helps predict before execution what plan schedules agents will rockin adopt so that kunti the 44-foot set tamilian of strategies of rosell all 1,883 agents constitute a 9/1 Nash equilibrium. uninterruptible We unrest perform some experiments and discuss pyongtaek the solutions obtained with rohri our game - odt theoretical approach, analyzing how proximities the theismann conflicts between autocorrect the wbr plans determine haixi the strategic saalekreis behavior chuni of tootsie the agents.", "histories": [["v1", "Wed, 4 Mar 2015 11:46:33 GMT  (145kb,D)", "http://arxiv.org/abs/1503.01288v1", "Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15) 2015"]], "COMMENTS": "Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15) 2015", "reviews": [], "SUBJECTS": "cs.AI cs.GT", "authors": ["jaume jord\u00e1n", "eva onaindia"], "accepted": true, "id": "1503.01288"}, "pdf": {"name": "1503.01288.pdf", "metadata": {"source": "CRF", "title": "Game-Theoretic Approach for Non-Cooperative Planning", "authors": ["Jaume Jord\u00e1n", "Eva Onaindia"], "emails": ["jjordan@dsic.upv.es", "onaindia@dsic.upv.es"], "sections": [{"heading": "Introduction", "text": "Multi-agent Planning (MAP) with self-interested agents is the problem of coordinating a group of agents that compete to make their strategic behavior prevail over the others\u2019: agents competing for a particular goal or the utilization of a common resource, agents competing to maximize their benefit or agents willing to form coalitions with others in order to achieve better their own goals or preferences. In this paper, we focus on game-theoretic MAP approaches for self-interested agents.\nBrafman et al (Brafman et al. 2009) introduce the Coalition-Planning Game (CoPG), a game-theoretic approach for self-interested agents which have personal goals and costs but may find it beneficial to cooperate with each other provided that the coalition formation helps increase their personal net benefit. In particular, authors propose a theoretical framework for stable planning in acyclic CoPG which is limited to one goal per agent. Following the line of CoPG, the work in (Crosby and Rovatsos 2011) presents an approach that combines heuristic calculations in existing planners for solving a restricted subset of CoPGs. In general, there has been a rather intensive research on cooperative self-interest agents as, for example, for modeling the behavior of planning agents in groups (Hadad et al. 2013) and\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nin coalitional resource game scenarios (Dunne et al. 2010), among others.\nOn the other hand, game-theoretic non-cooperative MAP approaches aim, in general, at finding a Nash Equilibrium joint plan out of the individual plans of the agents. \u2019Pure\u2019 game-theoretic approaches, like (Bowling, Jensen, and Veloso 2003) and (Larbi, Konieczny, and Marquis 2007) perform a strategic analysis of all possible agent plans and define notions of equilibria by analyzing the relationships between different solutions in game-theoretic terms. In (Bowling, Jensen, and Veloso 2003), MAP solutions are classified according to the agents\u2019 possibility of reaching their goals and the paths of execution (combinations of local plans). Similarly, satisfaction profiles in (Larbi, Konieczny, and Marquis 2007) are defined by the level of assurance of reaching the agent\u2019s goals. A different approach using bestresponse was proposed to solve congestion games and to perform plan improvement in general MAP scenarios from an available initial joint plan (Jonsson and Rovatsos 2011).\nGame-theoretic approaches that evaluate every strategy of every agent against all other strategies are ineffective for planning, since even if plan length is bounded polynomially, the number of available strategies is exponential (Nissim and Brafman 2013). However, in environments where cooperation is not allowed or calculating an initial joint plan is not possible, game-theoretic approaches are useful. Take, for instance, the modeling of a transportation network, sending packets through the Internet or a network traffic, where individuals need to evaluate routes in the presence of the congestion resulting from the decisions made by themselves and everyone else (Easley and Kleinberg 2010). In this sense, we argue that game-theoretic reasoning is a valid approach for this specific type of planning problems, among others.\nIn this paper, we present a novel game-theoretic noncooperative model to MAP with self-interested agents that solves the following problem. We consider a group of agents where each agent has one or several plans that achieve one or more goals. Executing a particular plan reports a benefit to the agent depending on the number of goals achieved, makespan of the plan or cost of the actions. Agents operate in a common environment, what may provoke interactions between the agents\u2019 plans and thus preventing a concurrent execution. Each agent is willing to execute the plan that maximizes its benefit but it ignores which plan the other\nar X\niv :1\n50 3.\n01 28\n8v 1\n[ cs\n.A I]\n4 M\nar 2\n01 5\nagents will point out, how his plan will be interleaved with theirs and the impact of such coordination on his benefit.\nWe present a two-game proposal to tackle this problem. A general game in which agents take a strategic decision on which joint plan to execute, and an internal game that, given one plan per agent, returns an equilibrium joint plan schedule. Agents play the internal game to simulate the simultaneous execution of their plans, find out the possibilities to coordinate in case of interactions and the effect of such coordination on their final benefit. The approach of the general game is very similar to the work described in (Larbi, Konieczny, and Marquis 2007); specifically, our proposal contributes with several novelties:\n\u2022 Introduction of soft goals to account for the case in which a joint plan that achieves all the goals of every agent is not feasible due to the interactions between the agents\u2019 plans. The aim of the general game is precisely to select an equilibrium joint plan that encompasses the \u2019best\u2019 plan of each agent.\n\u2022 An explicit handling of conflicts between actions and a mechanism for updating the plan benefit based on the penalty derived from the conflict repair. This is precisely the objective of the internal game and the key contribution that makes our model a more realistic approach to MAP with self-interested agents.\n\u2022 An implementation of the theoretical framework, using the Gambit tool (McKelvey, McLennan, and Turocy 2014) for solving the general game and our own program for the internal game.\nWe wish to highlight that the model presented in this paper is not intended to solve a complete planning problem due to the exponential complexity inherent to game-theoretic approaches. The model is aimed at solving a specific situation where the alternative plans of the agents are particularly limited to such situation and thus plans would be of a relatively similar and small size.\nThe paper is organized as follows. The next section provides an overview of the problem, introduces the notation that we will use throughout the paper and describes the general game in detail. The following section is devoted to the specification of the internal game, which we call the joint plan schedule game. Section \u2019Experimental results\u2019 shows some experiments carried out with our model and last section concludes."}, {"heading": "Problem Specification", "text": "The problem we want to solve is specified as follows. There is a set of n rational, self-interested agents N = {1, ..., n} where each agent i has a collection of independent plans \u03a0i that accomplish one or several goals. Executing a particular plan \u03c0 provides the owner agent a real-valued benefit given by the function \u03b2 : \u03a0 \u2192 R. The benefit that agent i obtains from plan \u03c0 is denoted by \u03b2i(\u03c0); in this work, we make this value dependent on the number of goals achieved by \u03c0 and the makespan of \u03c0 but different measures of reward and cost might be used, like the relevance of the achieved goals to agent i or the cost of the actions of \u03c0. Each agent i wishes\nto execute a plan \u03c0 such that max(\u03b2i(\u03c0)),\u2200\u03c0 \u2208 \u03a0i; however, since agents have to execute their plans simultaneously in a common environment, conflicts may arise that prevent agents from executing their preferable plans. Let\u2019s assume that \u03c0 and \u03c0\u2032 are the maximum benefit plans of agents i and j, respectively, and that the simultaneous execution of \u03c0 and \u03c0\u2032 is not possible due to a conflict between the two plans. If this happens, several options are analyzed:\n\u2022 agent i (agent j, respectively) considers to adapt the execution of its plan \u03c0 (\u03c0\u2032, respectively) to the plan of the other agent by, for instance, delaying the execution of one or more actions of \u03c0 so that this delay solves the conflict. This has an impact in \u03b2i(\u03c0) since any delay in the execution of \u03c0 diminishes the value of its original benefit.\n\u2022 agent i (agent j, respectively) considers to switch to another plan in \u03a0i (\u03a0j , respectively) which does not cause any conflict with the plan \u03c0\u2032 (\u03c0, respectively).\nAgents wish to choose their maximum benefit plan but then the choices of the other agents can affect each other\u2019s benefits. This is the reason we propose a game-theoretic approach to solve this problem.\nA plan \u03c0 is defined as a sequence of non-temporal actions \u03c0 = [a1, . . . , am]\n1. Assuming t = 0 is the start time when the agents begin the execution of one of their plans, the execution of \u03c0 would ideally take m units of time, executing a1 at time t = 0 and the rest of actions at consecutive time instants, thus finishing the execution of \u03c0 at time t = m\u2212 1 (last action is scheduled at m \u2212 1). This is called the earliest plan execution as it denotes that the start time and finish time of the execution \u03c0 are scheduled at the earliest possible times. However, if conflicts between \u03c0 and the plans of other agents arise, then the actions of \u03c0 might likely not to be realized at their earliest times, in which case a tentative solution could be to delay the execution of some action in \u03c0 so as to avoid the conflict. Therefore, given a plan \u03c0, we can find infinite schedules for the execution of \u03c0.\nDefinition 1 Given a plan \u03c0 = [a1, . . . , am], \u03a8\u03c0 is an infinite set that contains all possible schedules for \u03c0. Particularly, we define as \u03c80 the earliest plan execution of \u03c0 that finishes at time m \u2212 1. Given two different schedules \u03c8j , \u03c8j+1 \u2208 \u03a8\u03c0 , the finish time of \u03c8j is prior or equal to the finish time of \u03c8j+1.\nLet \u03c8j , where j 6= 0, be a schedule for \u03c0 that finishes at time t > m \u2212 1. The net benefit that the agent obtains with \u03c8j diminishes with respect to \u03b2i(\u03c0). The loss of benefit is a consequence of the delayed execution of \u03c0 and this delay may affect agents differently. For instance, if for agent i the delay of \u03c8j wrt to \u03c80 has a low incidence in \u03b2i(\u03c0), then i might still wish to execute \u03c8j . However, for a different agent k, a particular schedule of a plan \u03c0\u2032 \u2208 \u03a0k may have a great impact in \u03b2k(\u03c0\u2032) even resulting in a negative net benefit. How delays affect the benefit of the agents depends on the intrinsic characteristics of the agents.\nDefinition 2 We define a utility function \u00b5 : \u03a8 \u2192 R that returns the net value of a plan schedule. Thus, \u00b5i(\u03c8j), \u03c8j \u2208\n1In this first approach, we consider only instantaneous actions\n\u03a8\u03c0 , is the utility that agent i receives from executing the schedule \u03c8j for plan \u03c0. By default, for any given plan \u03c0 and \u03c80 \u2208 \u03a8\u03c0 , \u00b5i(\u03c80) = \u03b2i(\u03c0).\nA rational way of solving the conflicts of interest that arise among a set of self-interested agents who all wish to execute their maximum benefit plan comes from the non-cooperative game theory. Therefore, our general game is modeled as a non-cooperative game in the Normal-Form. The agents are the players of the game; the set of actions Ai is modeled as the game actions (plans) available to agent i, and the payoff function is defined as the result of a rational selection of a plan schedule for each agent. Formally:\nDefinition 3 We define our general game as a tuple (N,P, \u03c1), where: \u2022 N = {1, . . . , n} is the set of n self-interested players. \u2022 P = P1\u00d7 ...\u00d7Pn, where Pi = \u03a0i,\u2200i \u2208 N . Each agent i\nhas a finite set of strategies which are the plans contained in \u03a0i. We will then call a plan profile the n-tuple p = (p1, p2, . . . , pn), where pi \u2208 \u03a0i for each agent i. \u2022 \u03c1 = (\u03c11, ..., \u03c1n) where \u03c1i : P \u2192 R is a real-valued payoff function for agent i. \u03c1i(p) is defined as the utility of the schedule of plan pi when pi is executed simultaneously with (p1, . . . , pi\u22121, pi+1, . . . , pn).\nThe plan profile p represents the plan choice of each agent. Every agent i wishes to execute the schedule \u03c80 \u2208 \u03a8pi . Since this may not be feasible, agents have to agree on a joint plan schedule. We define a procedure named joint plan schedule that receives as input a plan profile p and returns a schedule profile s = (s1, s2, . . . , sn), where \u2200i \u2208 N, si \u2208 \u03a8pi . The schedule profile s is a consistent joint plan schedule; i.e., all of the individual plan schedules in s can be simultaneously executed without provoking any conflict. The joint plan schedule procedure, whose details are given in the next section, defines our internal game.\nLet p = (p1, p2, . . . , pn) be a plan profile and s = (s1, s2, . . . , sn) the schedule profile for p. Then, we have that \u03c1i(p) = \u00b5i(si).\nThe game returns a scheduled plan profile that is a Nash Equilibrium (NE) solution. This represents a stable solution from which no agent benefits from invalidating another agent\u2019s plan schedule.\nThe joint plan schedule game This section describes the internal game. The problem consists in finding a feasible joint plan schedule for a given plan profile p = (p1, p2, . . . , pn), where each agent i wishes to execute its plan pi under the earliest plan schedule (\u03c80). Since potential conflicts between the actions of the plans of different agents may prevent some of them from executing \u03c80, agents get engaged in a game in order to come up with a rational decision that maximizes their expected utility.\nFor a particular plan \u03c0, an action a \u2208 \u03c0 is given by the triple a = \u3008pre(a), add(a), del(a)\u3009, where pre(a) is the set of conditions that must hold in a state S for the action to be applicable, add(a) is its add list, and del(a) is its delete list, each a set of literals. Let a and a\u2032 be two actions, both scheduled at time t, in the plans of two different agents; a\nconflict between a and a\u2032 occurs at t if the two actions are mutually exclusive (mutex) at t (Blum and Furst 1997).\nThe joint plan schedule game is actually the result of simulating the execution of all the agents\u2019 plans. At each time t, every agent i makes a move, which consists in executing the next action a in its plan pi or executing the empty action (\u22a5). The empty action is the default mechanism to avoid two actions that are mutex at t, and this implies a deferral in the execution of a. A concept similar to \u22a5, called the empty sequence, is used in (Larbi, Konieczny, and Marquis 2007) as a neutral element for calculating the permutations of the plans of two agents, although the particular implication of this empty sequence in the plan or in the evaluation of the satisfaction profiles is not described.\nSearch space of the internal game Several issues must be considered when creating the search space of the internal game:\n1) Simultaneous and sequential execution of the game. The internal game is essentially a multi-round sequential game since the simulation of the plans execution occurs along time, one action of each player at a time. Then, the execution at time t+ 1 only takes place when every agent has moved at time t, so that players observe the choices of the rest of agents at t. In contrast, the game at time t represents the simultaneous moves of the agents at that time. Simultaneous moves can always be rephrased as sequential moves with imperfect information, in which case agents would likely get \u2019stuck\u2019 if their actions are mutex; that is, agents would not have the possibility of coordinating their actions. Therefore, simultaneous moves at t are also simulated as sequential moves as if agents would know the intention of the other agents. In essence, this can be interpreted as agents analyzing the possibilities of avoiding the conflict and then playing simultaneously the choice that reports a stable solution. Obviously, this means that agents would know the strategies of the others at time t, what seems reasonable if they are all interested in maximizing their utility.\n2) Applicability of the actions. Unlike other games where the agents\u2019 strategies are always applicable, in planning it may happen that an action a of a plan is not executable at time t in the state resulting from the execution of the t\u22121 previous steps. In such a case, the schedule profile is discarded. In our model, a schedule profile s is a solution if s comprises a plan schedule for every agent. Otherwise, we would be considering coalitions of agents that discard strategies that do not fit with the strategies of the coalition members. On the other hand,\u22a5 is only applicable at t if at least any other agent applies a non-empty action at t . The empty action is also applicable when the agent has played all the actions of its plan.\nExample. Consider a plan profile p = (p1, p2) of two agents, where p1 = [a1, a2, a3] and p2 = [b1, b2, b3]. s = (s1, s2) with s1 = (a1,\u22a5,\u22a5, a2, a3) and s2 = (\u22a5, b1, b2, b3,\u22a5) is a valid joint schedule if all the actions scheduled at each time t are not mutex.\nDefinition 4 Given a plan profile p = (p1, p2, . . . , pn), s = (s1, s2, . . . , sn) is a valid schedule profile to p if every si is a non-empty plan schedule and the actions of every pi scheduled at each time t are not mutex.\nFollowing, we formally define our internal game.\nDefinition 5 A perfect-information extensive-form game consists of:\n\u2022 a set of players, N = {1, . . . , n} \u2022 a finite set X of nodes that form the tree, with S \u2282 X\nbeing the terminal nodes \u2022 a set of functions that describe each x 6\u2208 S:\n\u2013 the player i(x) who moves at x \u2013 the set A(x) of possible actions at x \u2013 the successor node n(x, a) resulting from action a\n\u2022 n payoff functions that assign a payoff to each player as a function of the terminal node reached\nLet pi = [a1, . . . , am] be the plan of agent i. The setA(x) of possible actions of i at x isA(x) = {a,\u22a5}, where a is the action of pi that has to be executed next, which comes determined by the evolution of the game so far. Only in the case that agent i has already played the m actions of pi, A(x) = {\u22a5}. As commented above, each agent makes a move at a time so the first n levels of the tree represent the moves of the n agents at time t, the next n levels represent the moves of the n agents at t+ 1 and so on.\nA node x of the game tree represents the planning state after executing the path from the root node until x. For each node x, there are at most two successor nodes, each corresponding to the application of the actions in A(x). A terminal node s denotes a valid schedule profile.\nLet s = (s1, s2, . . . , sn) be a terminal node; the payoff of player i at s is given by \u00b5i(si). Note that the solution of the internal game for a plan profile p = (p1, p2, . . . , pn) is one of the terminal nodes of the game tree, and the payoff for each player i represents the value of \u03c1i(p). Then, the payoff vector of the solution terminal node is the payoff vector of one of the cells in the general game."}, {"heading": "Subgame Perfect Equilibrium (SPE)", "text": "The solution concept we apply in our internal game is the Subgame Perfect Equilibrium (Shoham and Leyton-Brown 2009, Chapter 5), a concept that refines a NE in perfectinformation extensive-form games by eliminating those unwanted Nash Equilibra. The SPE of a game are all strategy profiles that are NE for any subgame. By definition, every SPE is also a NE, but not every NE is SPE. The SPE eliminates the so-called \u201cnoncredible threats\u201d, that is, those situations in which an agent i threatens the other agents to choose a node that is harmful for all of them, with the intention of forcing the other players to change their decisions, thus allowing i to reach a more profitable node. However, this type of threats are non credible because a self-interested agent would not jeopardize its utility.\nA common method to find a SPE in a finite perfectinformation extensive-form game is the backward induction algorithm. This algorithm has the advantage that it can be\ncomputed in linear time in the size of the game tree, in contrast to the best known methods to find NE that require time exponential in the size of the normal-form. In addition, it can be implemented as a single depth-first traversal of the game tree. We consider the SPE as the most adequate solution concept for our joint plan schedule game since SPE reflects the strategic behavior of a self-interested agent taking into account the decision of the rest of agents to reach the most preferable solution in a common environment.\nThe SPE solution concept provides us a strong argument to solve the problem of selecting a joint plan schedule as a perfect-information extensive-form game instead of using, for example, a planner that returns all possible combinations of the agents\u2019 plans. In this latter case, the question would be which policy to apply to choose one schedule over the other. We could apply criteria such as Pareto-optimality2 or the maximum social welfare3. However, a Pareto-dominant solution does not always exist in all problems and the highest social welfare solution may be different from the SPE solution. That is, neither of these solution concepts would actually reflect how the fate of one agent is impacted by the actions of others.\nThe SPE solution concept has also some limitations. First, there could exist multiple SPE in a game, in which case one SPE may be chosen randomly. Second, the order of the agents when building the tree is relevant for the game in some situations. Consider, for instance, the case of a twoagent game. The application of the backward induction algorithm would give some advantage to the first agent in those cases for which there exist two different schedules to avoid the mutex (delaying one agent\u2019s action over the other or viceversa). In this case, the first agent will then select the solution that does not delay its conflicting action. Notice that in these situations both solutions are SPE and thus equally good from a game-theoretic perspective. Any other conflictsolving mechanism would also favour one agent over the other one depending on the used criteria; for instance, a planner would favour the agent whose delay returns the shortest makespan solution, and a more social-oriented approach would give advantage to the agent whose delay minimizes the overall welfare. In order to alleviate the impact of the order of the agents in the SPE solution, agents are randomly chosen in the tree generation.\nAn example of an extensive-form tree for a particular joint plan schedule problem can be seen in Figure 1. The tree represents the internal game of two agents A and B with plans \u03c0A = [a1, a2] and \u03c0B = [b1, b2]. The letter above an action represent its precondition, the letter below represents its effects. Thus, p \u2208 pre(a2), p \u2208 pre(b1), \u00acp \u2208 del(b1) and p \u2208 add(b2). At each non-terminal node, the corresponding agent generates its successors; in case of a non-applicable action, the branch is pruned. For example, in node 2 agent A tries to put its action a2, but this is not possible because in that state a previous action b1 deleted p. Another exam-\n2A vector Pareto-dominates another one if each of the components of the first one is greater or equal to the corresponding component in the second one.\n3The sum of all agents\u2019 utility\nple of non applicable action is shown in the right branch of node 6. In this case, agent B tries to apply the empty action \u22a5, but this option is also discarded because agent A has also applied an empty action in the same time step (t = 0).\nIn the tree example of Figure 1, we assume that both agents A and B have the same utility function, that a delay means a penalty proportional to the utility, and that \u03b2(\u03c0A) = \u03b2(\u03c0B) = 10. If we apply the backward induction algorithm to the this extensive-form game, it returns the joint schedule profile js1, or its equivalent js4. This schedule profile reports the highest possible utility for agent B, and a penalty of one unit (generic penalty) for agent A. Let\u2019s see how the backward induction algorithm obtains the SPE in this example. The payoffs of js1 are back up to node 2, where they will be compared with the values of node 5. The joint schedule js2 is backed up to node 5 because agent A is who chooses at node 5. Then, in node 1 agent B chooses between node 2 and node 5 and hence, js1 is chosen. In the other branches, in node 8 js4 will prevail over js5 and then, when compared in node 7 with js6, the choice of agent A is js4. This results in agent A choosing at node 0 between js1 and js4, both with the same payoffs, and so both are equivalent SPE solutions. If the tree is developed following a different agent order the SPE solution will be the same."}, {"heading": "Experimental results", "text": "In this section, we present some experimental results in order to validate and discuss our approach. As several factors can affect the solutions of the general game, we show different examples of game situations.\nWe implemented a program to generation the extensiveform tree and apply the backward induction algorithm. The NE in the normal-form game is computed with the tool Gambit (McKelvey, McLennan, and Turocy 2014).\nFor the experiments we used problems of the well-known Zeno-Travel domain from the International Planning Competition (IPC-3)4. However, for simplicity and the sake of clarity, we show generic actions in the figures.\n4http://ipc.icaps-conference.org/\nThe experiments were carried out for two agents, A and B. Both agents have a set of individual plans that solve one or more goals. The more goals achieved by a plan, the more the benefit of the plan. In addition, the benefit of a plan depends on the makespan of such plan. Given a plan \u03c0, which earliest plan execution is denoted by \u03c80, \u03b2i(\u03c0) is calculated as follows: \u03b2i(\u03c0) = nGoals(\u03c0) \u2217 10\u2212makespan(\u03c80), where nGoals(\u03c0) represents the number of goals solved by \u03c0 and makespan(\u03c80) represents the minimum duration schedule for \u03c0.\nThe utility of a particular schedule \u03c8 \u2208 \u03a8\u03c0 is a function of \u03b2i(\u03c0) and the number of time units that the actions of \u03c0 are delayed in \u03c8 with respect to the earliest plan execution \u03c80; in other words, the difference in the makespan of \u03c8 and \u03c80. Thus, \u00b5i(\u03c8) = \u03b2i(\u03c0) if \u03c8 = \u03c80. Otherwise, \u00b5i(\u03c8) = \u03b2i(\u03c0)\u2212 delay(\u03c8), where delay(\u03c8) is the delay in the makespan of \u03c8 with respect to the makespan of \u03c80.\nTable 1 shows the problems used in these experiments: the set of initial plans of each agent, the number of actions of each plan and its utility.\nIn Table 2 we can see the results of the general game for problem 1. Each cell is the result of a joint plan schedule game that combines a plan of agent A and a plan of agent B. In each cell, we show the payoff of \u03c0Ax and \u03c0By as well as the values of delay(\u03c8) for each plan (delay values are shown between parenthesis). The values in each cell are the result of the schedule profile returned by the internal game.\nThe NE of this problem is the combination of \u03c0A1 and \u03c0B1, with an utility of (15,16) for agent A and B, respectively. Agent A uses the plan that solves its goals g1 and g2 delayed two time steps. Agent B uses the plan that solves its goals g1 and g2, also delayed two time steps. The solution for both agents is to use the plan that solves more goals (with a higher initial benefit) a bit delayed. This can be a\ntypical situation if there are not many conflicts and if the delay is not very punishing to the agents. The schedule of this solution is shown in Figure 2. We can see in the figure that agent A starts the execution of its plan \u03c0A1 at t = 0, but after having scheduled its first two actions, the strategy of agent A introduces a delay of two time steps (empty actions) until it can finally execute its final action without causing a mutex with the actions of agent B. Regarding agent B, its first action in \u03c0B1 is delayed two time units to avoid the conflict with agent A. In this example, both agents have a conflict with each other (both have an action which deletes a condition that the other agent needs).\nTable 3 represents the game in normal-form of problem 2 shown in Table 1. In this case, we find three different equilibria: (\u03c0A1, \u03c0B2) with payoffs (15,14) and delays (3,2) for agent A and B, respectively; another NE is (\u03c0A2, \u03c0B1), with payoffs (14,15) and a delay of (2,3) time steps, respectively; the last NE is a mixed strategy with probabilities 0.001 and 0.999 for \u03c0A1 and \u03c0A2 of agent A, and probabilities 0.001 and 0.999 for strategies \u03c0B1 and \u03c0B2 of agent B. In this problem we have a cell with\u2212\u221e as payoff of the two agents. This payoff represents that there does not exist a valid joint schedule for the plans due to an unsolvable conflict as the one shown in Figure 3.\nThe game in Table 4 is the same game as the one in Table 3 but, in this case, the agents suffer a delay penalty of 3.5 (instead of 1) per each action delayed in their plan schedules. Under this new evaluation, we can see how this affects the general game. In this situation, the only NE solution is (\u03c0A2, \u03c0B2) with utility values (9,9) and a delay of two time\nsteps for each agent. Note that this solution is neither Paretooptimal (solution (16,9) is Pareto-optimal) nor it maximizes the social welfare. However, these two solution concepts can be applied in case of multiple NE.\nIn conclusion, our approach simulates how agents behave with several strategies and it returns an equilibrium solution that is stable for all of the agents. All agents participate in the schedule profile solution and their utilities are dependent on the strategies of the other agents regarding the conflicts that appear in the problem."}, {"heading": "Conclusions and future work", "text": "In this paper, we have presented a complete game-theoretic approximation for non-cooperative agents. The strategies of the agents are determined by the different ways of solving mutex actions at a time instant and the loss of utility of the solutions in the plan schedules. We also present some experiments carried out in a particular planning domain. The results show that the SPE solution of the extensive-form game in combination with the NE of the general game return a stable solution that responds to the strategic behavior of all of the agents.\nAs for future work, we intend to explore two different lines of investigation. The exponential cost of this approach represents a major limitation for being used as a general MAP method for self-interested agents. Our combination of a general+internal game can be successively applied in subproblems of the agents. Considering that this approach solves a subset of goals of an agent, the agent could get engaged in a new game to solve the rest of his goals, and likewise for the rest of agents. Then, a MAP problem can be viewed as solving a subset of goals in each repetition of the whole game. In this line, the utility functions of the agents can be modeled not only to consider the benefit of the current schedule profile but also to predict the impact of this strategy profile in the resolution of the future goals. That is, we can define payoffs as a combination of the utility gained in the current game plus an estimate of how the joint plan schedule would impact in the resolution of the remaining goals.\nAnother line of investigation is to extend this approach to cooperative games, allowing the formation of coalitions of agents if the coalition represents a more advantageous strategy than playing alone."}, {"heading": "Acknowledgments", "text": "This work has been partly supported by the Spanish MICINN under projects Consolider Ingenio 2010 CSD2007-00022 and TIN2011-27652-C03-01, and the Valencian project PROMETEOII/2013/019."}], "references": [{"title": "M", "author": ["A. Blum", "Furst"], "venue": "L.", "citeRegEx": "Blum and Furst 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "M", "author": ["M.H. Bowling", "R.M. Jensen", "Veloso"], "venue": "M.", "citeRegEx": "Bowling. Jensen. and Veloso 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "R", "author": ["Brafman"], "venue": "I.; Domshlak, C.; Engel, Y.; and Tennenholtz, M.", "citeRegEx": "Brafman et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Rovatsos", "author": ["M. Crosby"], "venue": "M.", "citeRegEx": "Crosby and Rovatsos 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["Dunne"], "venue": "E.; Kraus, S.; Manisterski, E.; and Wooldridge, M.", "citeRegEx": "Dunne et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "J", "author": ["D.A. Easley", "Kleinberg"], "venue": "M.", "citeRegEx": "Easley and Kleinberg 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "I", "author": ["M. Hadad", "S. Kraus", "Hartman"], "venue": "B.-A.; and Rosenfeld, A.", "citeRegEx": "Hadad et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Rovatsos", "author": ["A. Jonsson"], "venue": "M.", "citeRegEx": "Jonsson and Rovatsos 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "R", "author": ["Larbi"], "venue": "B.; Konieczny, S.; and Marquis, P.", "citeRegEx": "Larbi. Konieczny. and Marquis 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "T", "author": ["R.D. McKelvey", "A.M. McLennan", "Turocy"], "venue": "L.", "citeRegEx": "McKelvey. McLennan. and Turocy 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["R. Nissim", "Brafman"], "venue": "I.", "citeRegEx": "Nissim and Brafman 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and LeytonBrown", "author": ["Y. Shoham"], "venue": "K.", "citeRegEx": "Shoham and Leyton.Brown 2009", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [], "year": 2015, "abstractText": "When two or more self-interested agents put their plans to execution in the same environment, conflicts may arise as a consequence, for instance, of a common utilization of resources. In this case, an agent can postpone the execution of a particular action, if this punctually solves the conflict, or it can resort to execute a different plan if the agent\u2019s payoff significantly diminishes due to the action deferral. In this paper, we present a game-theoretic approach to non-cooperative planning that helps predict before execution what plan schedules agents will adopt so that the set of strategies of all agents constitute a Nash equilibrium. We perform some experiments and discuss the solutions obtained with our game-theoretical approach, analyzing how the conflicts between the plans determine the strategic behavior of the agents.", "creator": "LaTeX with hyperref package"}}}