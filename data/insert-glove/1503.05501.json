{"id": "1503.05501", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Probabilistic Argumentation. An Equational Approach", "abstract": "There methuselah is a generic mdpv way yampolsky to add any unresponsiveness new wilkin feature stonehaven to a system. burong It hatori involves namely 1) identifying hrbaty the sensuality basic units pendleton which hizo build zatoichi up the system and 2) rumbling introducing the new pyongchang feature sahara to each of 6-for-13 these dzemaili basic wodiczko units.", "histories": [["v1", "Wed, 18 Mar 2015 17:29:24 GMT  (53kb,D)", "http://arxiv.org/abs/1503.05501v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["d m gabbay", "o rodrigues"], "accepted": false, "id": "1503.05501"}, "pdf": {"name": "1503.05501.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Argumentation. An Equational Approach", "authors": ["D. Gabbay"], "emails": ["dov.gabbay@kcl.ac.uk", "odinaldo.rodrigues@kcl.ac.uk"], "sections": [{"heading": null, "text": "In the case where the system is argumentation and the feature is probabilistic we have the following. The basic units are: a. the nature of the arguments involved; b. the membership relation in the set S of arguments; c. the attack relation; and d. the choice of extensions.\nGenerically to add a new aspect (probabilistic, or fuzzy, or temporal, etc) to an argumentation network \u3008S,R\u3009 can be done by adding this feature to each component a\u2013d. This is a brute-force method and may yield a non-intuitive or meaningful result.\nA better way is to meaningfully translate the object system into another target system which does have the aspect required and then let the target system endow the aspect on the initial system. In our case we translate argumentation into classical propositional logic and get probabilistic argumentation from the translation.\nOf course what we get depends on how we translate. In fact, in this paper we introduce probabilistic semantics to abstract argumentation theory based on the equational approach to argumentation networks. We then compare our semantics with existing proposals in the literature including the approaches by M. Thimm and by A. Hunter. Our methodology in general is discussed in the conclusion.\nar X\niv :1\n50 3.\n05 50\n1v 1"}, {"heading": "1 Introduction", "text": "The objective of this paper is to provide some orientation to underpin probabilistic semantics for abstract argumentation. We feel that a properly developed probabilistic argumentation framework cannot be obtained by simply imposing an arbitrary probability distribution on the components of an argumentation system that does not agree with the dynamic aspects of these networks. We need to find a probability distribution that is compatible with their underlying motivation.\nWe shall use the methodology of \u201cLogic by Translation\u201d, which works as follows: Given a new area for which we want to study certain aspect properties AP, we translate this area to classical logic, study AP in classical logic and then translate back and evaluate what we have obtained.\nLet us start by looking at interpretations of an abstract argumentation network \u3008S,R\u3009, S 6= \u2205, R \u2286 S\u00d7S, into logics which already have probabilistic versions. This way we can import the probability aspect from there and it will have a meaning. We begin with translating abstract argumentation frames into classical propositional logic. In the abstract form, the elements of S are just atoms waiting to be instantiated as arguments coming from another application system. R may be defined using the source application system or may represent additional constraints. At any rate, in this abstract form, S is just a set of atoms and all we have about it is R. In translating \u3008S,R\u3009 into classical propositional logic, we view S as a set of atomic propositions and we use R to generate a classical theory \u2206\u3008S,R\u3009. Consider Figure 1, which describes the basic attack formation of all the attackers Att(x) = {y \u2208 S | (y, x) \u2208 R} = {y1, . . . , yn} of the node x in a network \u3008S,R\u3009.\nThe essential logic translation of the attack on each node x is given by (E1) below, where x, yi are propositional symbols representing the elements x, yi \u2208 S:\nx\u2194 \u2227 i \u00acyi (E1)\nSo \u3008S,R\u3009 corresponds to a classical propositional theory \u2206\u3008S,R\u3009 = {x \u2194\u2227 i \u00acyi | x \u2208 S}.1 Note that in classical logic, this theory may be inconsistent and have no models. For example, if S contains a single node x and R is\n1If there is a logical relationship between the arguments of S that can be captured by formulae, then we can alternatively instantiate x 7\u2212\u2192 \u03d5x, giving \u2206\u3008S,R\u3009 = {\u03d5x \u2194\u2227\ni \u00ac\u03d5y | x, y \u2208 S}.\n{(x, x)}, i.e., the network has a single self-attacking node, then the associated theory is {x \u2194 \u00acx}, which has no model. For this reason it is convenient to regard these theories as theories of Kleene three-valued logic, with values in {0, 12 , 1}. In this 3-valued semantics, a valuation would satisfy x \u2194 \u00acx if and only if it gives the value 12 to x. 2\nIf we consider the equational approach [5], then we can write x = \u2227 i \u00acyi (E2)\nwhere (E2) is a numerical equation over the real interval [0, 1], with conjunction and negation interpreted as numerical functions expressing the correspondence of the values of the two sides.\nA complete extension of \u3008S,R\u3009 is a solution to the equations of the form of (E2) when they are viewed as a set of Boolean equations in Kleene\u2019s 3-valued logic with values { 0, 12 , 1 } , where\nx = 0 means that x = out (at least one attacker yi = in) (1)\nx = 1 means that x = in (all attackers yi = out) (2)\nx = 1\n2 means that x = und (no attacker yi = in and at least (3)\none attacker yj = und)\nThe acceptability semantics above can be re-written in terms of the semantics of Kleene\u2019s logic as\nv(x) = min{1\u2212 v(yi)}\nwhich in equational form can be simplified to\nx = 1\u2212max{yi} (E2*)\nThe reader should note that we actually solve the equations over the unit interval [0, 1] and project onto Kleene\u2019s 3-valued logic by letting\nx = 0 mean x = out (at least one attacker yi = in)\n0 < x < 1 mean x = und (no attacker yi = in and at least\none attacker yj = und)\nx = 1 mean x = in (all attackers yi = out)\nNow there are probabilistic approaches to two-valued classical logic. The simplest two methods are described in Gabbay\u2019s book Logic for Artificial Intelligence and Information Technology [4]. Our idea is to bring the probabilistic approach through the above translation into argumentation theory.\nLet us start with a description of the probabilistic approaches to classical propositional logic.\n2In Kleene\u2019s logic, one can interpret \u00ac as complement to 1; \u2227 as min; and \u2228 as max. Thus, if the values of A,B are v(A), v(B), then v(\u00acA) = 1 \u2212 v(A), v(A \u2227 B) = min(v(A), v(B)) and v(A \u2228B) = max(v(A), v(B)).\nMethod 1: Syntactic. Impose probability P (q) on the atoms q of the language and propagate this probability to arbitrary well-formed formulas (wffs). So if \u03d5(q1, . . . , qm) is built up from the atoms q1, . . . , qm, we can calculate P (\u03d5) if we know P (qi), i = 1, . . . ,m.\nMethod 2: Semantic. Impose probability on the models of the language of {q1, . . . , qm}. The totality of models is the space W of all {0, 1}-vectors in 2m. We give values P (\u03b5), for any \u03b5 \u2208 2m, with the restriction that \u03a3\u03b5\u22082mP (\u03b5) = 1. The probability of any wff \u03d5 is then\nP (\u03d5) = \u03a3\u03b5 \u03d5P (\u03b5) (P1)\nThe motivation for the syntactical Method 1 is that the atoms {q1, . . . , qm} are all independent. So for example, the date of birth of a person (p) is independent of whether it is going to rain heavily on that person\u2019s 21st birthday (q). However, if we want to hold a birthday party r in the garden on the 21st birthday, then we have that q attacks r.\nIf, on the other hand, we have:\na = John comes to the party\nb = Mary comes to the party\nthen a and b may be dependent, especially if some relationship exists between John and Mary. We may decide that the probability of a \u2227 b is 0, but the probabilities of \u00aca \u2227 b and of a \u2227 \u00acb are 14 each and the probability of \u00aca \u2227 \u00acb is 12 . Assigning probability in this way depends on the likelihood we attach to a particular situation (model). This is the semantic approach.\nExample 1.1 shows that these two methods are orthogonal.\nExample 1.1 What can \u2206\u3008S,R\u3009 mean in classical logic? It is a generalisation of the \u201cLiar\u2019s paradox\u201d. x attacking itself is like x saying \u201cI am lying\u201d: x = > if and only if x = \u22a5. Figure 1 represents yi saying x is a lie. \u2206\u3008S,R\u3009 represents a system of lying accusations: a community liar paradox.\nSimilarly, S can represent people possibly invited to a birthday party. y \u2192 x means y saying \u201cif I come, x cannot come\u201d. So Figure 1 is saying \u201cinvite x if and only if you do not invite any of the yi\u201d.\nSuppose we instantiate x 7\u2212\u2192 \u03d5x. Then we must have P (\u03d5x) = P ( \u2227 i \u00ac\u03d5yi). However, there may be also a connection between \u03d5x and some \u03d5yk , e.g., \u03d5x ` \u03d5yk . This will impose further restrictions on P (\u03d5x) and P (\u03d5yk), and it may be the case that no such probability function exists.\nRemark 1.2 The two approaches are of course, connected. If we are given a probability on each qi, then we get probability on each \u03b5 \u2208 2m by letting\nP (\u03b5) = \u03a0\u03b5 qP (q)\u00d7\u03a0\u03b5 \u00acq(1\u2212 P (q)) (P2)\nThe qi\u2019s are considered independent, so the probability of \u2227 i\u00b1qi is the product\nof the probabilities P ( \u2227 i\u00b1qi) = \u03a0iP (\u00b1qi)\nwhere P (\u00acqi) = 1\u2212 P (qi) and the probability of A \u2228B is\nP (A \u2228B) = P (A) + P (B)\nwhen \u00ac(A \u2227B), as is the case with disjuncts in a disjunctive normal form. So, for example\nP ((a \u2227 b) \u2228 (a \u2227 \u00acb)) = P (a \u2227 b) + P (a \u2227 \u00acb) = P (a)P (b) + P (a)(1\u2212 P (b)) = P (a)(P (b) + 1\u2212 P (b)) = P (a)."}, {"heading": "2 The syntactical approach (Method 1)", "text": "Let us investigate the use of the syntactical approach. Let \u3008S,R\u3009 be an argumentation network. In the equational approach, according to the syntactical Method 1, we assign probabilities to all the atoms and are required to solve the equation (E3) below for each x, where Att(x) = {yi} and x and all yi are numbers in [0, 1]:\nP (x) = P ( \u2227 i \u00acyi), (E3)\nSince in Method 1, all atoms are independent, (E3) is equivalent to (E3*):\nP (x) = \u03a0i(1\u2212 P (yi)). (E3*)\nSuch equations always have a solution. Let us check whether this makes sense. Let us try to identify the argument\nx equationally with its probability, namely we let P (x) = x.\nIf x = in, let P (x) = 1\nIf x = out, let P (x) = 0.\nIf x = und, let 0 < P (x) < 1\nto be determined by the solution to the equations. Equation (E3*) becomes, under P (x) = x, the following:\nx = \u03a0(1\u2212 yi) for x \u2208 S. (E4)\nThis is the Eqinv equation in the equational approach (see [5]). The following definition will be useful in the interpretation of values from\n[0, 1] and their counterparts in Caminada\u2019s labelling functions.\nDefinition 2.1 A valuation function f can be mapped into a labelling function \u03bb(f) as follows.\nf(x) = 1 \u2192 \u03bb(f)(x) = in f(x) = 0 \u2192 \u03bb(f)(x) = out\nf(x) \u2208 (0, 1) \u2192 \u03bb(f)(x) = und\nWhat do we know about Eqinv? We quote the following from [5].\nTheorem 2.2 Let f be a solution to equations (E4). Then \u03bb(f) defined according to Definition 2.1 is a legal Caminada labelling (see [1]) and leads to a complete extension.\nTheorem 2.3 Let \u03bb0 be a legal Caminada labelling leading to a preferred extension. Then there exists a solution f0, such that \u03bb0 = \u03bb(f0).\nRemark 2.4 There are (complete) extensions \u03bb\u2032 such that there does not exist an f \u2032 with \u03bb\u2032 = \u03bb(f \u2032).\nFor example, in Figure 2, the extension a = b = und cannot be obtained by any f . Only b = in, a = out can be obtained as a solution to equations (E4).3\nExample 2.5 Let \u3008S,R\u3009 be given and let \u03bb be a complete extension which is not preferred! The reason that \u03bb is not preferred, is that we have by definition, a \u03bb1 extending \u03bb, which gives more {in,out} values to points z, for which \u03bb gives the value und. Therefore, we can prevent the existence of such an extension \u03bb1, if we force such points z to be undecided. This we do by attacking such points z by a new self-attacking point u. The construction is therefore as follows. We are given \u3008S,R\u3009 and a complete extension \u03bb, which is not preferred. We now construct a new \u3008S\u2032, R\u2032\u3009 which is dependent on \u03bb. Consider \u3008S\u2032, R\u2032\u3009 where S\u2032 = S \u222a {u}, where u 6\u2208 S, is a new point. Let R\u2032 be\nR\u2032 = R \u222a {(u, u)} \u222a {(u, v) | v \u2208 S and \u03bb(v) = und}. 3The equations are\n1. a = (1\u2212 a)\u00d7 (1\u2212 b) 2. b = 1\u2212 a.\nFrom the above two equations we get\n3. a = (1\u2212 a)\u00d7 a The only possibility is a = 0.\nThen \u03bb\u2032 = \u03bb \u222a {(u,und)} is a preferred extension of \u3008S\u2032, R\u2032\u3009 and can therefore be obtained from a function f \u2032 using the equations (E4).\nLet us see what the construction above does to our example in Figure 2, and let us look at the extension \u03bb(a) = \u03bb(b) = und.\nConsider the network in Figure 3. Its equations (E4) are:\n1. u = 1\u2212 u\n2. a = (1\u2212 u)(1\u2212 a)(1\u2212 b)\n3. b = (1\u2212 u)(1\u2212 a) From (1) we get u = 12 . So we have:\n2. a = 12 (1\u2212 a)(1\u2212 b) 3. b = 12 (1\u2212 a)\n1\u2212 b = 1\u2212 12 (1\u2212 a) = 2\u22121+a2\n= 1+a2 therefore substituting in (1) we get\na = 12 (1\u2212 a)( 1+a 2 )\n= 14 (1\u2212 a 2)\n4a+ a2 \u2212 1 = 0\n(a+ 2)2 \u2212 4\u2212 1 = 0\n(a+ 2)2 = 5\na = \u221a 5\u2212 2 \u2248 0.236\nb = 12 (1\u2212 a)\n= 12 (1\u2212 \u221a 5 + 2) = 3\u2212 \u221a 5\n2 \u2248 0.382.\nThe extension of the network is a = b = und.\nSummary of the results so far for the syntactical probabilistic method. Given an argumentation network \u3008S,R\u3009, we can find all Method 1 complete probabilistic extensions for it by solving all Eqinv equations. Such complete probabilistic extensions will also be complete extensions in the traditional sense (i.e., Dung\u2019s), which will also include all preferred extensions (Theorems 2.2 and 2.3).4\nHowever, not all complete extensions can be obtained in this manner (i.e., by Method 1, see remark 2.4 and compare with Example 3.6).\nWe can, nevertheless, for any complete extension E which cannot be obtained by Method 1, obtain it from the solutions of the equations generated for a larger network \u3008S\u2032, R\u2032\u3009 as shown in Example 2.5.\nWe shall say more about this in a later section.\nRemark 2.6 Evaluation of the results so far for the syntactical probabilistic method.\n1. We discovered a formal mathematical connection between the syntactical probabilistic approach (Method 1) and the Equational Eqinv approach. Is this just a formal similarity or is there also a conceptual connection?\nThe traditional view of an abstract argumentation frame \u3008S,R\u3009, is that the arguments are abstract, some of them abstractly attack each other. We do not know the reason, but we seek complete extensions of arguments that can co-exist (i.e., being attack-free), and that protect themselves. The equational approach is an equational way of finding such extensions. Each solution f to the equations give rise to a complete extension. The numbers we get from such solutions f of the equational approach can be interpreted as giving the degree of being in the complete extension (associated with f) or being out of it.\nDue to the mathematical similarity with the probability approach, these numbers are now interpreted as probabilities.\nTo what extent is this justified? Can we do this at all?\nLet us recall the syntactical probabilistic method. We start with an abstract argumentation framework \u3008S,R\u3009 and add the probability P (x) for each x \u2208 S. We can interpret P (x) as the probability that x \u201cis a player\u201d to be considered (this is a vague statement which could mean anything but is sufficient for our purpose). The problem is how do we take into account the attack relation? Our choice was to require equation (E3). It is this\n4Note that in traditional Dung semantics a preferred extension E is maximal in the sense that there is no extension E\u2032 such that\n1. If x is considered in (resp. out) by E then x is also considered in (resp. out) by E\u2032. 2. There exists at least one node considered in (resp. out) by E\u2032 and considered und by E.\nThe above definition holds for numerical or probabilistic semantics, where the value 1 (resp. 0) is understood as in (resp. out) and values in (0, 1) are understood as und.\nchoice that allowed the connection between the syntactical probabilistic approach and the Equational approach with Eqinv.\nSo our syntactical probabilistic approach should work as follows.\nLet P be the independent probability on each x \u2208 S. This is an arbitrary number in [0, 1]. Such a P cannot be used for calculating extensions because it does not take into consideration the attack relation R. So modify P to a P \u2032 which does respect R via Equation (E3).\nHow do we modify P to find P \u2032?\nWell, we can use a numerical iteration method. The details are not important here, the importance is in the idea, which can be applied to the traditional notion of extensions as well. Given \u3008S,R\u3009 and an arbitrary desired assignment E of elements that are in (and consequently also determining elements that are out) for S, this E may not be legitimate in taking into account R, so we need to modify it to get the best proper extension E\u2032 nearest to E (cf. [2, 6]).\nSo our syntactical probabilistic approach yielding a P satisfying Equation (E3) can be interpreted as Eqinv extensions obtained from initial values which are probabilities (as opposed to, say, initial values being a result of voting) corrected via iteration procedures using R.\nAlternatively, we can look at the Eqinv equations as a mathematical means of finding all those syntactical probabilities P which respect the attack relation R (via Equation (E3)).\nOr we can see the solutions of the Eqinv as giving probabilities for being included or excluded in the complete extension defined by these solutions (as opposed to the interpretation of the degree of being in or out).\n2. The discussion in item 1. above hinged upon the choice we made to take account of R by respecting Equation (E3). There are other alternatives for taking R into account. We can give direct, well-motivated definitions of how to propagate probabilities along attack arrows. This is similar to the well-known problem of how to propagate probabilities along proofs (provability support arrows, or modus ponens, etc). Such an analysis is required anyway for instantiated networks, for example in ASPIC+ style [10]). We shall deal with this in a subsequent paper."}, {"heading": "3 The semantical approach (Method 2)", "text": "Let us now check what can be obtained if we use Method 2, i.e., giving probability to the models of the language. In this case the equation (for {yi} = Att(x)) (E3) P (x) = P ( \u2227 i \u00acyi) still holds, but the \u00acyi are not independent. So we cannot write equation (E3*) for them and get Eqinv. Instead we need to use the schema P (A\u2228B) = P (A)+P (B)\u2212P (A\u2227B). We begin with a key lemma, which will enable us to compare later with the work of M. Thimm, see [13].\nLemma 3.1 Let \u3008S,R\u3009 be a network and let P be a probability measure on the space W of all models of the language whose set of atoms is S. For x \u2208 S, let the following hold\nP (x) = P ( n\u2227 i=1 \u00acyi)\nwhere Att(x) = {y1, . . . , yn}. Then we have\n1. P (x) \u2264 P (\u00acyi), 1 \u2264 i \u2264 n\n2. P (x) \u2265 1\u2212 \u03a3ni=1P (yi)\nProof. By induction on n.\n1. If x = \u00acy then P (x) = 1\u2212 P (y) and the above holds. 2. Assume the above holds for m, show for m+1. Let z = \u2228m i=1 yi, y = ym+1.\nThen x = \u00acz \u2227 \u00acy. We have by the induction hypothesis\n\u2022 P (\u00acz) \u2264 P (\u00acyi), i = 1, . . . ,m \u2022 P (\u00acz) \u2265 1\u2212 \u03a3mi=1P (yi)\nConsider now:\nP (\u00acz \u2227 \u00acy) = 1\u2212 P (y \u2228 z) = 1\u2212 (P (y) + P (z)\u2212 P (y \u2227 z)) = 1\u2212 P (y)\u2212 P (z) + P (y \u2227 z) = 1\u2212 P (y)\u2212 (P (z)\u2212 P (y \u2227 z))\nBut P (A \u2227B) \u2264 P (B) is always true. So\nP (\u00acz \u2227 \u00acy) \u2264 1\u2212 P (y) = P (\u00acy)\nOn the other hand, by our assumption\n1\u2212 P (z) = P (\u00acz) \u2265 1\u2212 \u03a3mi=1P (yi)\nSo P (\u00acz \u2227 \u00acy) = 1\u2212 P (y)\u2212 P (z) + P (y \u2227 z)\n(1\u2212 P (z))\u2212 P (y) + P (y \u2227 z) \u2265 1\u2212 \u03a3P (yi)\u2212 P (y) + P (y \u2227 z) \u2265 1\u2212 \u03a3m+1i=1 P (yi)\nRemark 3.2 The converse of Lemma 3.1 does not hold, as we shall see in Example 3.5 below.\nLet us look at some examples illustrating the use of Method 2.\nExample 3.3 Consider the network in Figure 4. This figure is taken from Thimm\u2019s \u201cA probabilistic semantics for abstract argumentation\u201d [13, Figure 1]. We include it here for two reasons:\n1. To illustrate or probabilistic semantic approach.\n2. To use it later to compare our work with Thimm\u2019s approach.\nLet us apply Method 2 to it and assign probabilities to the models of the propositional language with the atoms {a1, a2, a3, a4, a5}. We assign P as follows.\nP (a1 \u2227 \u00aca2 \u2227 a3 \u2227 \u00aca4 \u2227 a5) = 0.3 P (a1 \u2227 \u00aca2 \u2227 \u00aca3 \u2227 a4 \u2227 \u00aca5) = 0.45 P (\u00aca1 \u2227 a2 \u2227 \u00aca3 \u2227 \u00aca4 \u2227 a5) = 0.1 P (\u00aca1 \u2227 a2 \u2227 \u00aca3 \u2227 a4 \u2227 \u00aca5) = 0.15 P (any other conjunctive model) = 0.\nLet us compute P (ai), for i = 1, . . . , 5. We have P (X) = \u2211 \u03b5 X P (\u03b5).\nWe get P (a1) = 0.3 + 0.45 = 0.75\nP (a2) = 0.1 + 0.15 = 0.25\nP (a3) = 0.3\nP (a4) = 0.45 + 0.15 = 0.6\nP (a5) = 0.3 + 0.1 = 0.4.\nTo be a legitimate probabilistic model P must satisfy equation (E3) relating to the attack relation of Figure 4. Namely we must have\nP (X) = P ( \u2227\nY \u2208Att(X)\n\u00acY ) (E3)\nTherefore P (a1) = P (\u00aca2) P (a2) = P (\u00aca1) P (a3) = P (\u00aca2 \u2227 \u00aca5) P (a4) = P (\u00aca3 \u2227 \u00aca5) P (a5) = P (\u00aca4)\nLet us calculate the P in the right hand side of the above equations.\nP (\u00aca2) = 1\u2212 0.25 = 0.75 P (\u00aca1) = 1\u2212 0.75 = 0.25 P (\u00aca2 \u2227 \u00aca5) = 0.45 P (\u00aca3 \u2227 \u00aca5) = 0.45 + 0.15 = 0.6 P (\u00aca4) = 0.4\nWe see that P (a3) = 0.3 6= P (\u00aca2 \u2227 \u00aca5) = 0.45.\nTherefore this distribution P is not legitimate according to our Method 2. It does not satisfy equations (E3) because\nP (a3) 6= P (\u00aca2 \u2227 \u00aca5)\nTherefore Lemma 3.1 does not apply and indeed, condition (2) of Lemma 3.1 does not hold for a3. We have P (a3) = 0.3 but 1\u2212 P (a2)\u2212 P (a5) = 0.35.\nExample 3.4 Let us look at Figure 5. This is also taken from Thimm\u2019s paper [13, Figure 2]. It shall be used later to compare our methods with Thimm\u2019s.\n1. We use Method 2. Consider the following probability distribution on models\nP (a1 \u2227 \u00aca2 \u2227 \u00aca3) = 0.5 P (a1 \u2227 \u00aca2 \u2227 a3) = 0 P (a1 \u2227 a2 \u2227 \u00aca3) = 0 P (a1 \u2227 a2 \u2227 a3) = 0 P (\u00aca1 \u2227 a2 \u2227 a3) = 0 P (\u00aca1 \u2227 a2 \u2227 \u00aca3) = 0.5 P (\u00aca1 \u2227 \u00aca2 \u2227 a3) = 0 P (\u00aca1 \u2227 \u00aca2 \u2227 \u00aca3) = 0.\nIn this model we get P (a1) = 0.5\nP (a2) = 0.5\nP (a3) = 0\nLet us check whether this probability distribution satisfies equation (E3), namely P (X) = P ( \u2227\nY \u2208Att(X)\n\u00acY ) (E3)\nWe need to have P (a1) = P (\u00aca2) P (a2) = P (\u00aca1) P (a3) = P (\u00aca2 \u2227 \u00aca2)\nIndeed P (\u00aca1) = 1\u2212 P (a1) = 0.5 P (\u00aca2) = 1\u2212 P (a2) = 0.5 P (\u00aca1 \u2227 \u00aca2) = 0.\nThus we have a legitimate model.\n2. We use Method 1. Let us use Eqinv on this figure, namely we try and solve the equations\na1 = 1\u2212 a2 a2 = 1\u2212 a1 a3 = (1\u2212 a1)(1\u2212 a2)\nLet us use a parameter 0 \u2264 x \u2264 1 and let\na1 = x,\na2 = 1\u2212 x, a3 = x(1\u2212 x)\nThe probabilities we get with parameter x as well as for x = 0.5 are given below.\nP (a1 \u2227 a2 \u2227 a3) = x2(1\u2212 x)2 = 116 P (a1 \u2227 a2 \u2227 \u00aca3) = x(1\u2212 x)(1\u2212 x(1\u2212 x)) = 316 P (a1 \u2227 \u00aca2 \u2227 a3) = x3(1\u2212 x) = 116 P (a1 \u2227 \u00aca2 \u2227 \u00aca3) = x2(1\u2212 x(1\u2212 x)) = 316 P (\u00aca1 \u2227 a2 \u2227 a3) = x(1\u2212 x)3 = 116 P (\u00aca1 \u2227 a2 \u2227 \u00aca3) = (1\u2212 x)2(1\u2212 x(1\u2212 x)) = 316 P (\u00aca1 \u2227 \u00aca2 \u2227 a3) = x2(1\u2212 x)2 = 116 P (\u00aca1 \u2227 \u00aca2 \u2227 \u00aca3) = x(1\u2212 x)(1\u2212 x(1\u2212 x)) = 316\nIf we choose x = 0.5 we get P (a1) = P (a2) = 0.5 and P (a3) = 1 4 .\nExample 3.5 This example shows that the converse of Lemma 3.1 does not hold. Consider the network in Figure 6.\nAny legitimate probability assigned to models would be required to satisfy the following\nP (a) = P (\u00aca \u2227 \u00acb) P (b) = P (\u00aca \u2227 \u00acb)\nCase 1. Try the following probability P1.\nP1(a \u2227 b) = P1(a \u2227 \u00acb) = P1(\u00aca \u2227 b) = P1(\u00aca \u2227 \u00acb) = 0.25.\nTherefore P1(a) = 0.5\nP1(b) = 0.5\nNote that we also have\nP1(a) = 1 2 \u2264 1\u2212 P1(b) = 1 2 P1(a) = 1 2 \u2264 1\u2212 P1(a) = 1 2 .\nSimilarly for P1(b) by symmetry. Also\nP1(a) = 1\n2 \u2265 1\u2212 P1(a)\u2212 P1(b) = 1\u2212\n1 2 \u2212 1 2 = 0.\nThus the conditions of the conclusions of Lemma 3.1 hold. However the assumptions of Lemma 3.1 do not hold, because\nP1(a) = 1\n2 6= P1(\u00aca \u2227 \u00acb) =\n1 4 .\nCase 2. Let us check whether we can find a probability P2 which is indeed acceptable to Method 2. Let us try with variables y, z and create equations and solve them:\nP2(a \u2227 b) = y P2(\u00aca \u2227 b) = z.\nTherefore P2(b) = y + z.\nP2(\u00aca \u2227 \u00acb) = y + z\nand what is left is P2(a \u2227 \u00acb) = 1\u2212 2y \u2212 2z\nbut we must also have P2(a) = P2(\u00aca \u2227 \u00acb)\nand hence we must have\nP2(a) = 1\u2212 2y \u2212 2z + y = P2(\u00aca \u2227 \u00acb) = y + z.\nSo we get the equation 1\u2212 2y \u2212 3z = 0 2y + 3z = 1\ny = (1\u22123z)2\nSince 0 \u2264 y, z \u2264 1 so z must be less than 13 . Let us choose z = 0.2 and so y = 0.2. We get, for example\nP2(a \u2227 b) = 0.2 P2(\u00aca \u2227 b) = 0.2 P2(\u00aca \u2227 \u00acb) = 0.4 P2(a \u2227 \u00acb) = 0.2\nWe could also have chosen z = 13 and y = 0. This would give P3, where\nP3(a \u2227 b) = 0 P3(\u00aca \u2227 b) = 13 P3(\u00aca \u2227 \u00acb) = 13 P3(a \u2227 \u00acb) = 13\nSo we get P3(b) = P (a) = 1 3\nP3(\u00aca \u2227 \u00acb) = 13 .\nExample 3.6 Consider the network of Figure 2. Let us try to find a probabilistic semantics for it according to Method 2. Assume we have\nP (a \u2227 b) = x1 P (a \u2227 \u00acb) = x2 P (\u00aca \u2227 b) = x3 P (\u00aca \u2227 \u00acb) = 1\u2212 x1 \u2212 x2 \u2212 x3.\nWe need to satisfy P (a) = P (\u00aca \u2227 \u00acb) P (b) = P (\u00aca)\nThis means we need to solve the following equations.\n1. x1 + x2 = 1\u2212 x1 \u2212 x2 \u2212 x3\n2. x1 + x3 = 1\u2212 x1 \u2212 x2.\nBy adding x1 + x2 to both sides (1) can be written as\n2(x1 + x2) = 1\u2212 x3,\nand by swapping x3 to the right and \u2212x1 \u2212 x2 to the left (2) can be written as\n2x1 + x2 = 1\u2212 x3.\nThus we get\n3. 2x1 + x2 = 2x1 + 2x2.\nTherefore x2 = 0. There remains, therefore\n4. 2x1 = 1\u2212 x3.\nWe can choose values for x3.\nSample choice 1. x3 = 1, so x1 = 0. We get P1(a \u2227 b) = P (a \u2227 \u00acb) = P1(\u00aca \u2227 \u00acb) = 0 and P1(\u00aca \u2227 b) = 1. This yields P (a) = 0, P (b) = 1. This is also the Eqinv solution to\nb = 1\u2212 a a = (1\u2212 a)(1\u2212 b)\nSample choice 2. x3 = 1 2 . So x1 = 1 4 and the probabilities are\nP2(a \u2227 b) = 14 P2(a \u2227 \u00acb) = 0 P2(\u00aca \u2227 b) = 12 P2(\u00aca \u2227 \u00acb) = 14 .\nP2 is a Method 2 probability, which cannot be given by Method 1.\nSample choice 3. x3 = 0. Then x1 = 1 2 . We get\nP3(a \u2227 b) = 12 P3(a \u2227 \u00acb) = 0 P3(\u00aca \u2227 b) = 0 P3(\u00aca \u2227 \u00acb) = 12 .\nTherefore P3(a) = P3(b) = 1 2 .\nLemma 3.7 Let \u3008S,R\u3009 be a network and let P be a semantic probability (Method 2) for \u3008S,R\u3009. Let x \u2208 S and let {yi} = Att(x). Then\n1. If for some yi, P (yi) = 1 then P (x) = 0.\n2. If for all yi, P (yi) = 0 then P (x) = 1.\nProof. Let us use Figure 1 where {yi} = Att(x).\nCase 1. Assume that P (y1) = 1. We need to show that P (x) = 0. We have: P (x) = P ( \u2227 i \u00acyi) (E3)\nWe also have P (A) = \u2211 \u03b5 A P (\u03b5) (P1)\nTherefore P (x) = \u2211 \u03b5 \u2227 i \u00acyi P (\u03b5)\nP (x) = \u2211\n\u03b5 \u00acy1\u2227 \u2227n j=1 \u00acyj P (\u03b5) (i)\nbut P (y1) = \u2211 \u03b5 y1 P (\u03b5) = 1 Therefore we have \u2211 \u03b5 \u00acy1 P (\u03b5) = 0 (ii)\nFrom (i) and (ii) we get that P (x) = 0.\nCase 2. We assume that for all i, P (yi) = 0 and we need to show that P (x) = 1.\nWe have P (x) = P ( \u2227 \u00acyi)\nP (x) = 1\u2212 P ( \u2228 yi) (iii)\nWe also have P ( \u2228 yi) = \u03a3\u03b5 \u2228 yiP (\u03b5) (iv)\nSuppose for some \u03b5\u2032 such that \u03b5\u2032 \u2228 yi we have P (\u03b5 \u2032) > 0. But \u03b5\u2032 \u2228 yi implies \u03b5\u2032 yi, for some i. Say i = 1. Thus we have \u03b5\u2032 y1 and P (y1) = 0 and P (\u03b5\u2032) > 0. This is impossible since\nP (y1) = \u2211 \u03b5 y1 P (\u03b5) (v)\nTherefore for all \u03b5 such that \u03b5 \u2228 yi we have that P (\u03b5) = 0. Therefore by (iii) and (iv) we get P (x) = 1.\nRemark 3.8 Let \u3008S,R\u3009 be a network and let P be a semantic probability for \u3008S,R\u3009 (Method 2).\nLet \u03bb be defined as follows, for x \u2208 S.\n\u03bb(x) =  in, if P (x) = 1 out, if P (x) = 0\nund, if 0 < P (x) < 1\nThe perceptive reader might expect us to say that \u03bb is a legitimate Caminada labelling, especially in view of Lemma 3.7. This is not the case as Example 3.9 shows.\nExample 3.9 This example shows that in the probabilistic semantics it is possible to have P (x) = 0, while for all attackers y of x we have 0 < P (y) < 1. Thus the nature of the probabilistic attack is different from the traditional Dung one. If Att(x) is the set of all attackers of x and P ( \u2228 y\u2208Att(x) y) = 1, then, and only then P (x) = 0. Thus the attackers of x can attack with joint probability. The example we give is the network of Figure 7. This has a Method 1 probability of P1(a) = 1 2 , P1(b) = 1 4 and P1(x) = 1 4 .\nThus for any model m = \u00b1a \u2227 \u00b1b \u2227 x we have\nP1(m) = 1 2 \u00d7 1 2 \u00d7 1 4 = 1 16\nand for any model m\u2032 = \u00b1a \u2227 \u00b1 \u2227 \u00acx\nwe have\nP1(m) = 1 2 \u00d7 1 2 \u00d7 3 4 = 3 16 .\nFigure 7 also has a Method 2 probability model. We can have\nP2(a) = P2(b) = 1 2\nP2(x) = 0.\nLet us check what values to give to the models. The models are:\nm1 = x \u2227 a \u2227 b m2 = x \u2227 a \u2227 \u00acb m3 = x \u2227 \u00aca \u2227 b m4 = x \u2227 \u00aca \u2227 \u00acb m5 = \u00acx \u2227 a \u2227 b m6 = \u00acx \u2227 a \u2227 \u00acb m7 = \u00acx \u2227 \u00aca \u2227 b m8 = \u00acx \u2227 \u00aca \u2227 \u00acb.\nWe want the following equations to be satisfied.\n1. P2(x) = 0. This means we need to let\nP2(mi) = 0, i = 1, . . . , 4.\n2. P2(a) = 1 2 . This means we need to let\nP2(m5) + P2(m6) = 1 2 P2(m7) + P2(m8) = 1 2 .\n3. P2(b) = 1 2 , yields the equations\nP2(m5) + P2(m7) = 1 2 P2(m6) + P2(m8) = 1 2 .\n4. We also need to have the equation\n0 = P2(x) = P2(\u00aca \u2227 \u00acb)\nTherefore P2(m8) = 0.\nWe thus have the following equations left\n(a) P2(m5) + P2(m6) = 1 2 (b) P2(m7) = 1 2 (c) P2(m5) + P2(m7) = 1 2 (d) P2(m6) = 1 2 .\nFrom (b) and ( c) we get P2(m5) = 0. This makes P2(m6) = 1 2 . Thus we get the following solution:\nP2(mi) = 0, for i = 1, 2, 3, 4, 5, 8 P2(m6) = P2(m7) = 1 2 .\nNote that the equations (E3) hold for P1 and P2:\nP (a) = 1\u2212 P (a) P (b) = 1\u2212 P (b)\nhold of both P1 and P2. As for P (x) = P (\u00aca \u2227 \u00acb) we check 1 4 = P1(x) = P1(\u00aca \u2227 \u00acb)\n= P1(\u00aca)\u00d7 P1(\u00acb) = 14 .\nFor P2 we have\n0 = P2(x) = P2(\u00aca \u2227 \u00acb) P2(\u00ac(a \u2228 b) = 1\u2212 P2(a \u2228 b)\nP2(a \u2228 b) = P2(m1) + P2(m2) +P2(m3) + P2(m5) + P2(m6)\n+P2(m7) = 0 + 0 + 0 + 1 2 + 1 2 = 1.\nThus P2(\u00aca \u2227 \u00acb) = 0. So P1 and P2 are legitimate probabilities on Figure 7. P1 is a Method 1 probability and P2 is a Method 2 probability.\nDefinition 3.10 We now define the Gabbay\u2013Rodrigues Probabilistic Labelling \u03a0 on a network \u3008S,R\u3009. \u03a0 is a {in, out, und}-labelling satisfying the following.\nThere exists a semantic probability P on \u3008S,R\u3009 such that for all x \u2208 S 1. \u03a0(x) = in, if P ( \u2228 Att(x)) = 0\n2. \u03a0(x) = out, if P ( \u2228 Att(x)) = 1\n3. \u03a0(x) = und, if 0 < P ( \u2228 Att(x)) < 1\nExample 3.11 This example is due to M. Thimm, oral communication, 24th October 2014. Consider Figure 8.\nThis figure contains Figure 7 and its mirror image. We saw that in Figure 7 (as well as in this Figure 8) any probability on the figures must yield\nP (a) = P (b) = 1\n2 .\nFigure 7 allowed for two possibilities for x. P1(x) = 1 4 and P2(x) = 0. Let us try P for our Figure 8 with\nP (x1) = 1\n4 and P (x2) = 0.\nThis is not possible because we must have\nP (xi) = P (\u00aca \u2227 \u00acb).\nSo P (x1) must be equal to P (x2). This example will show in the comparison with the literature section that our probability semantics is different from that of M. Thimm in [13]. See also Example 3.5.\nTheorem 3.12 Let \u3008S,R\u3009 be a network and let \u03bb be a legitimate Caminada labelling on S, giving rise to a complete extension. Then there exists a probability P\u03bb on the models (Method 2 probabilistic semantics) such that for all x \u2208 S:\n\u2022 P\u03bb(x) = 1, if \u03bb(x) = in\n\u2022 P\u03bb(x) = 0, if \u03bb(x) = out\n\u2022 P\u03bb(x) = 12 , if \u03bb(x) = und.\nProof. (We use an idea from M. Thimm [13]) Let S = {s1, . . . , sk}. Then when we regard the elements of S as atomic propositions in classical propositional logic, there are 2k models based on S. Each of these models gives values 0 (false) or 1 (true) to each atomic proposition. Each such a model can be represented by a conjunction of the form \u03b1 = \u2227 i\u00b1si. \u03b1 represents the model which gives value 1 to si if +si appears in \u03b1 and gives value 0 to si if \u2212si appears in \u03b1. Given a model we can construct the respective \u03b1 for it. Let\n\u03b11 = \u2227\n\u03bb(s)=in\ns; \u03b10 = \u2227\n\u03bb(s)=out\n\u00acs; \u03b1 1 2\n= \u2227\n\u03bb(s)=und\ns; and \u03b2 1 2\n= \u2227\n\u03bb(s)=und\n\u00acs.\nWe now define a Method 2 probability P\u03bb on the models.\n1. P\u03bb(\u03b11 \u2227 \u03b10 \u2227 \u03b1 1 2 ) = 12\n2. P\u03bb(\u03b11 \u2227 \u03b10 \u2227 \u03b2 1 2 ) = 12\n3. P\u03bb(m) = 0, for any other model, m different from the above.\nClearly P\u03bb is a probability. We examine its properties\n(i) Let x be such that \u03bb(x) = in.\nThen P\u03bb(x) = \u2211 m x P\u03bb(m).\nOnly (1) and (2) can contribute to P\u03bb(x), so the value is 1.\n(ii) Let \u03bb(x) = out.\nThe only two models that can contribute to P\u03bb(x) are in (1) and (2) above, but they prove \u00acx. So P\u03bb(x) = 0.\n(iii) Let P\u03bb(x) = und.\nThen clearly P\u03bb(x) gets a contribution from (1) only. We get P\u03bb(x) = 1 2 .\nWe now need to verify that P\u03bb actually satisfies the equations of (E3). Let x \u2208 S and let yi be its attackers. We want to show that\nP\u03bb(x) = P\u03bb( \u2227 i \u00acyi)\nor P\u03bb(x) = 1\u2212 P\u03bb( \u2228 i yi).\n(iv) Assume P\u03bb(x) = 1. Then P\u03bb(x) gets contributions from both (1) and (2). The only option is that then \u03bb(x) = in, and so all attackers of yi of x are out, so \u03b10 \u2227 \u00acyi and so P\u03bb( \u2227 i \u00acyi) = 1, because it gets contributions\nfrom both (1) and (2).\n(v) Assume P\u03bb(x) = 0.\nThus neither (1) nor (2) contribute to P\u03bb(x). Therefore \u03b10 x and so \u03bb(x) = out and so for some attacker yi, \u03bb(yi) = in and so \u03b11 yi and so P\u03bb( \u2227 i \u00acyi) cannot get any contribution either from (1) or from (2) and\nso P\u03bb( \u2227 i \u00acyi) = 0.\n(vi) Assume that P\u03bb(x) = 1 2 .\nSo P\u03bb(x) can get a contribution either from (1) or from (2), but not from both. So \u03bb(x) must be undecided.\nSo the attackers yi of x are either out (with P\u03bb(yi) = 0)) or und (with P\u03bb(yi) = 1 2 ), and we have that at least one attacker y of x is und.\nLet y0i be the attackers that are out and let y 1 2 j be the undecided attackers.\nConsider e = \u2227 i \u00acy0i \u2227 \u2227 j \u00acy 1 2 j .\nThe only model which can both contribute to P\u03bb(e) is \u03b11 \u2227 \u03b10 \u2227 \u03b2 1 2 and thus P\u03bb(e) = 1 2 .\nThus from (iv), (v) and (vi) we get that (E3) holds for P\u03bb.\nRemark 3.13 Note that the P\u03bb of Theorem 3.12 is strictly Method 2 probability. For example we saw that the network of Figure 2.3 with a = b = und cannot solve Method 1 probability. The next section will see how far we can go with Method 1 probability.\nSummary of the results so far for the semantical probabilistic Method 2. We saw that Dung\u2019s traditional complete extensions strictly contain the probabilistic Method 1 extensions and is strictly contained in the probabilistic Method 2 extensions."}, {"heading": "4 Approximating the semantic probability by", "text": "syntactic probability\nWe have seen in Theorem 3.12 that the Method 2 probabilistic semantics can give us all the traditional Dung complete extensions. This result, together with the probabilistic semantics P2 of Example 3.9 would show that Method 2 semantics is stronger than traditional Dung complete extensions semantics.\nThis section examines how far we can stretch the applicability of the syntactical probability approach (Method 1). We know from the \u201call-undecided\u201d extension for the network in Figure 2 that there are cases where we cannot give Method 1 probability. We ask in this section, can we approximate such extensions by Method 1 probabilities?\nWe find that the answer is yes. Let \u3008S,R\u3009 be a network. Let \u03bb be a legitimate Caminada labelling giving rise to a complete extension E = E\u03bb. If the extension is a preferred extension, then there exists a solution f to the Eqinv equations which yield \u03bb and f is actually a Method 1 (and here also a Method 2) probabilistic semantics for \u3008S,R\u3009. The question remains as to what happens in the case where \u03bb is not a preferred extension. In this case we are not sure whether \u03bb can be realised by a solution f of the\nEqinv equations. In fact there are examples of networks where no such f exists. We know from Theorem 3.12 that there exists a probability function P\u03bb on models that would yield \u03bb according to Definition 3.10. We seek an Eqinv function which approximates this probability.\nWe shall use the ideas of Example 2.5.\nRemark 4.1 We need to use some special networks.\n1. Consider Figure 9, which we shall call Un. n = 1, 2, 3, . . ..\nThe Eqinv equations solve for this figure as ui = 1 2 , i = 1, . . . , n.\nu = 1\n2n\nThus if u attacks any node x, its \u201cimpact\u201d on x is the multiplicative value 1\u2212 12n . For n very large, the attack is almost negligible.\n2. Let \u3008S,R\u3009 be any network. Let u be a node not in S. If we add u to S and let it attack all elements of S, we can assume in view of (1) above that the Eqinv value of u is 1 2n . Figure 10 depicts this scenario.\nWe suppress {u1, . . . , un} and just record that u = 12n .\nConstruction 4.2 Let \u3008S,R\u3009 be given and let \u03bb be a legitimate Caminada labelling giving rise to a non-preferred extension.\nLet u 6\u2208 S be a new point and assume in view of Remark 4.1 that the value of u is very very small. Let\nS\u2032 = S \u222a {u}\nand let R\u2032 = R \u222a {(u, v)|\u03bb(v) = und}.\nLet \u03bb\u2032 = \u03bb \u222a {(u,und)}. Let Att(x) be the set of all attackers of x in \u3008S,R\u3009 and let Att\u2032(x) be the set\nof all attackers of x in \u3008S\u2032, R\u2032\u3009. We have if \u03bb\u2032(x) \u2208 {in, out}, then u 6\u2208 Att\u2032(x). If \u03bb\u2032(x) = und, then y \u2208 Att\u2032(u). Consider the following set of equations on \u3008S\u2032, R\u2032\u3009.\nx = 1, if \u03bb\u2032(x) = in (EQ1)\nx = 0, if \u03bb\u2032(x) = out (EQ0)\nx = \u03a0(1\u2212 y)y\u2208Att\u2032(x)in \u3008S\u2032, R\u2032\u3009, if \u03bb \u2032(x) = und (EQU)\nThis set of equations has a solution f. We claim the following\n1. \u03bb(f) is a complete extension\n2. \u03bb(f) = \u03bb\u2032\nIt is clear that \u03bb(f)(x) = \u03bb\u2032(x), for \u03bb\u2032(x) \u2208 {in, out}. Does \u03bb(f) agree with \u03bb\u2032 on undecided points of \u03bb\u2032? The answer is that it must be so, because \u03bb\u2032 is a preferred extension. So \u03bb(f) cannot be an extension with more zeros and ones than \u03bb\u2032.\nRemark 4.3 The perceptive reader might ask why do we use those particular equations in Construction 4.2 (page 24)? The answer can be seen from Figure 11.\nConsider \u03bb(a) = in, \u03bb(b) = out, \u03bb(c) = \u03bb(d) = und.\nWe create Figure 12. We take the equation\na = 1, b = 0\nc = (1\u2212 d)(1\u2212 u) d = (1\u2212 c)(1\u2212 u) u = 1\u2212 u.\nThe solution for the equations for c, d and u are\nu = 12 c = d = 13\nWe have to insist on a = 1, b = 0. If we do not insist and write the usual equations\na = 1\u2212 b b = 1\u2212 a,\nwe might get a different solution, e.g.\nb = 1, a = 0.\nThis not the original \u03bb.\nRemark 4.4 This remark motivates and proves the next Theorem 4.5. We need some notation. Let Q be a set of atoms. By the models of Q (based on\nQ) we mean all conjunction normal forms of atoms from Q or their negations. So, for example, if Q = {a, b, c}, we get 8 models, namely\nm1 = a \u2227 b \u2227 c ...\nm8 = \u00aca \u2227 \u00acb \u2227 \u00acc.\nIf we have atoms Q1 = {ai}, Q2 = {bj}, Q3 = {ck}\nwhere Qi are pairwise disjoint we can write the models of Q1 \u222aQ2 \u222aQ3 in the form\n\u03b1 \u2227 \u03b2 \u2227 \u03b3\nwhere \u03b1 is a model of Q1, \u03b2 of Q2 and \u03b3 of Q3. For example\n\u03b11 \u2227 \u03b21 \u2227 \u03b31 = (a1 \u2227 a2 \u2227 . . .) \u2227 (\u00acb1 \u2227 b2 \u2227 . . .) \u2227 (c2 \u2227 . . .).\nNow let \u3008S,R\u3009 and \u03bb be as in Construction 4.2. Remember we assume that the value of u is very very small, and so the attack value (1\u2212 u) is very close to 1. Consider \u03bb\u2032 and f and \u03bb(f) again as in Construction 4.2. f is a solution of Eqinv equations (EQ1), (EQ0) and (EQU). Therefore any model of S\n\u2032, say \u03b1 = \u00b1s1\u2227\u00b1s2\u2227\u00b1 . . .\u2227\u00b1sk\u2227\u00b1u where S = {s1, . . . , sk} will have its probability semantics as\nPf (\u03b1 = \u03a0 k i=1f(\u00b1sk)))\u00d7 f(\u00b1u) (\u2217)\nwhere f(+s) = f(s)\nf(\u2212s) = 1\u2212 f(s). In particular, we have the following:\n1. Let E+ = {e+1 , . . .} be the subset of S such that \u03bb(e + i ) = in. Let E \u2212 = {e\u2212j } be the subset of S such that \u03bb(e \u2212 j ) = out. Let Eund = {bk} be the\nset of all nodes in S such that \u03bb(bk) = und. We therefore have that any model \u03b4 of S\u2032 has the form\n\u03b4 = \u2227 i\u00b1e + i \u2227 \u2227 i\u00b1e \u2212 j \u2227 \u2227 k \u00b1bk \u2227 \u00b1u\n= \u03b1 \u2227 \u03b2 \u00b1 u\nwhere \u03b1 is a model of E+ \u222a E\u2212 and \u03b2 is a model of Eund. Let \u03b11,0 be the particular conjunction\n\u03b11,0 = \u2227 i e+i \u2227 \u2227 j \u00ace\u2212j .\nLet \u03b2 be any model of Eund. Consider Pf (\u03b4), \u03b4 = \u03b1 \u2227 \u03b2 \u2227 \u00b1u. Then by (*) we have that\nPf (\u03b4) = 0, if \u03b1 6= \u03b11,0. (\u2217\u2217)\nSince Pf is a probability, we have for any s \u2208 S\u2032\nPf (s) = Pf ( \u2227\ny\u2208Att\u2032(s)\n\u00acy).\nNote that for s \u2208 S, s 6= u such that \u03bb(s) \u2208 {in, out}, u does not attack s, and so we have\nPf (s) = P (f)( \u2227 y\u2208Att(s) \u00acy)\n= \u03a0y\u2208Att(s)(1\u2212 f(y)) (]1)\nFor u we have that u is very small and so Pf (u) = 1 2n . For s \u2208 S such that \u03bb(s) = und, we have that u attacks s and so\nPf (s) = Pf ( \u2227 y\u2208Att\u2032(s) \u00acy)\n= (\u03a0y\u2208Att(s)(1\u2212 f(y))\u00d7 (1\u2212 12n ) (]2)\nThe (1\u2212 12n ) is the attack of u.\nWe ask what are the attackers of s \u2208 Eund? They cannot be nodes x such that \u03bb(x) = in, because then s would be out. So the value of f(y), (for y \u2208 Att(s)) is either 0 or a value in (0, 1).\nSo we can continue and write\nPf (s) = (1\u2212 1\n2n )\u03a0y \u2208 Att(s)\n\u03bb(y) = und\n(1\u2212 f(y)) (]3)\nNote that 0 < Pf (s) < 1, because all the f(y), for \u03bb(y) = und, satisfy 0 < f(y) < 1.\nWe also have \u2211 all models m Pf (m) = 1. (]4)\nSince(**) holds, we need consider only models m of the form \u03b11,0 \u2227 \u03b2 \u2227 \u00b1u. We can write 1 = \u2211 \u03b2\u2227\u00b1u Pf (\u03b11,0 \u2227 \u03b2 \u2227 \u00b1u) (]5)\nwhere \u03b2 is a model of Eund. Let us analyse (]5) a bit more. Assume \u03b2 = \u2227 k \u00b1bk.\nSo Pf (\u03b10,1 \u2227 \u03b2 \u2227 u) + Pf (\u03b10,1 \u2227 \u03b2 \u2227 \u00acu) = \u03a0kf(\u00b1bk). (]6)\nWe thus get that: \u2211 \u03b2 \u03a0kf(\u00b1bk) = 1. (]7)\n(]7) says something very interesting. It says that f restricted to Eund gives a proper probability distribution on the models of Eund.\nThis combined with (]3) gives us the following result.\nConsider (Eund, Rund) where Rund = R Eund. Then f Eund is a proper probability distribution on (Eund, Rund).\nDoes it satisfy the proper equations? Let s \u2208 Eund. Do we have\nPund(s) ? = Pund( \u2227 y \u2208 Eund yRx \u00acy)\nLet us check. The real equation is\nPund(s) = Pund( \u2227\ny\u2032 \u2208 Eund yRx\n\u00acy)\u00d7 (1\u2212 u) (]8)\nSince u is very small, we have a very good approximation.5\nWe can now define a probability P on \u3008S,R\u3009. Let m = \u03b1 \u2227 \u03b2 be a model, where \u03b1 is a model for E+ \u222a E\u2212 and \u03b2 is a model for Eund.\nThen define P as follows\nP (\u03b1 \u2227 \u03b2) = 0, if \u03b1 = \u00ac\u03b11,0 P (\u03b1 \u2227 \u03b2) = Pund(\u03b2), if \u03b1 = \u03b11,0\nWe need to show that approximately P (s) = P ( \u2227\ny\u2208Att(s)\n\u00acy)\nIf s \u2208 E+ \u222a E\u2212 this follows from (]1). If s \u2208 Eund, this follows from (]3) and (]8). Note that since the f involved came from Eqinv equations, P satisfies the following on \u3008S,R\u3009.\nP (s) = 0, if some y \u2208 Att(s)P (y) = 1 P (s) = 1, if for all y \u2208 Att(s), P (y) = 0 P (s) = undecided, otherwise.\n(]9)\nTheorem 4.5\n5The perceptive reader might ask what happens if we let u converge to 0? The answer is that we get a proper Eqinv extension. However, this may be an all undecided extension (which is what we do want), or it may be a complete extension properly containing all the undecided extensions (which is not what we want!).\nWe may decide to do what physicists do to their equations. Write the equations in full and simply neglect any item containing higher order u, i.e., u2, u3, etc. This is reasonable when the value of each node is small.\n1. Let \u3008S,R\u3009 be a network and let \u03bb be a legitimate Caminada labelling on S. Then there exists a Method 1 probability distribution P\u03bb, which almost satisfies equation (E3), namely for every \u03b5, there exists a Method 1 probability P\u03bb depending on \u03b5, such that for every x and its attackers yi, we have |P\u03bb(x)\u2212 P\u03bb(\u2227\u00acyi)| < \u03b5, such that\n\u03bb(x) = in, if P\u03bb(x) = 1\n\u03bb(x) = out, if P\u03bb(x) = 0\n\u03bb(x) = und, if 0 < P\u03bb(x) < 1.\n2. P is obtained as follows\nCase 1. \u03bb is a preferred extension. Then let f be a solution of Eqinv for \u3008S,R\u3009. Let P\u03bb = f .\nCase 2. \u03bb is not a preferred extension. Let E\u03bbund = {x|\u03bb(x) = und}. Consider \u3008S\u2032, R\u2032\u3009, where S\u2032 = E\u03bbund \u222a {u}, where u is a new point not in S with value almost 0.\nR\u2032 = R E\u03bbund \u222a {u} \u00d7 E\u03bbund.\nThen \u3008S\u2032, R\u2032\u3009 has only one extension (all undecided). Let f \u2032 be a solution to Eqinv on \u3008S\u2032, R\u2032\u3009. We now define P\u03bb on \u3008S,R\u3009. Let \u03b11,0 = \u2227 \u03bb(x)= in x \u2227 \u2227 \u03bb(y)= out \u00acy. Let m = \u03b1\u2227\u03b2 be an arbitrary model of S, where \u03b1 is a model of {x|\u03bb(x) \u2208 {in, out} and \u03b2 is a model of E\u03bbund. Define P\u03bb(\u03b1 \u2227 \u03b2) to be\nP\u03bb(\u03b1 \u2227 \u03b2 = 0 if \u03b1 6= \u03b11,0 P\u03bb(\u03b11,0 \u2227 \u03b2) = f \u2032(\u03b2) where \u03b2 = \u2227 s\u2208E\u03bbund\n\u00b1s and f(\u03b2) = \u03a0\u00b1s in \u03b2f(\u00b1s).\nProof. Follows from the considerations of Remark 4.4.\nExample 4.6 Let us show how Theorem 4.5 works by doing a few examples.\n1. Consider the network of Figure 11 and the extension \u03bb mentioned there, namely \u03bb(a) = in, \u03bb(b) = out, \u03bb(c) = \u03bb(d) = und.\nFollowing our algorithms we look at the {c, d, u} part of Figure 12 and solve the equations. We get u = 12 , c = d = 1 3 . The probability P\u03bb will be as follows:\nP\u03bb(\u03b1 \u2227 \u03b2) = 0 if \u03b1 6= a \u2227 \u00acb.\nNow look at P\u03bb(a \u2227 \u00acb \u2227 c \u2227 d) = 13 \u00d7 1 3 = 1 9\nP\u03bb(a \u2227 \u00acb \u2227 c \u2227 \u00acd) = 13 \u00d7 2 3 = 2 9 P\u03bb(a \u2227 \u00acb \u2227 \u00acc \u2227 d) = 23 \u00d7 1 3 = 2 9 P\u03bb(a \u2227 \u00acb \u2227 \u00acc \u2227 \u00acd) = 23 \u00d7 2 3 = 4 9 .\n2. Let us look at Figure 13.\nWith \u03bb(a) = in, \u03bb(b) = out, \u03bb(c) = \u03bb(d) = und.\nThe {c, d} part is Figure 2. Here we solve the equations on the {c, d, u} part associated with {c, d}, which is the same as Figure 3. The solution is found in Example 2.5, with u = 12 . We get u = 12 ; c = 0.36, 1 \u2212 c = 0.764, d = 0.382, 1 \u2212 d = 0.618. The probability P\u03bb of this case is P\u03bb(\u03b1 \u2227 \u03b2) = 0, if \u03b1 6= a \u2227 \u00acb.\nP\u03bb(a \u2227 \u00acb \u2227 c \u2227 d) = 0.236\u00d7 0.382 = 0.09 P\u03bb(a \u2227 \u00acb \u2227 c \u2227 \u00acd) = 0.236\u00d7 0618 = 0.146 P\u03bb(a \u2227 \u00acb \u2227 \u00acc \u2227 d) = 0.764\u00d7 0.382 = 0.292 P\u03bb(a \u2227 \u00acb \u2227 \u00acc \u2227 \u00acd) = 0.764\u00d7 0.618 = 0.472.\nIndeed 0.09 + 0.146 + 0.292 + 0.472 = 1.000.\nWe now discuss imposing probability on instantiated networks such as ASPIC+. We begin with simple instantiations into classical propositional logic.\nDefinition 4.7 1. An abstract instantiated network (into classical propositional logic) has the form A = \u3008S,R, I\u3009, where \u3008S,R\u3009 is an abstract argumentation network and I is a mapping associating with each x \u2208 S, a well-formed formula I(x) = \u03d5x of classical propositional logic.\n2. For any A as in 1, we associate the theory \u2206A = {\u03d5x \u2194 \u2227(y,x)\u2208R\u00ac\u03d5y | x \u2208 S}.\n3. A semantic probability model P on A is a probability distribution on the models based on S such that for all x \u2208 S, we have:\nP (\u03d5x) = P (\u2227(y,x)\u2208R\u00ac\u03d5y)\nExample 4.8 Consider Figure 14 where part (b) is an instatiation of part (a) with I(x) = a1 \u2228 a2 and I(a3) = a3. The equations any probability assignment needs to satisfy are\nP (a1 \u2228 a2) = 1\nP (a3) = P (\u00ac(a1 \u2228 a2))\n= P (\u00aca1 \u2227 \u00aca2)\n= 0.\nIf we let P (a1) = x, P (a2) = 1\u2212 x, P (a3) = 0, with x \u2208 [0, 1], then P satisfies the equations. Compare with Example 3.4."}, {"heading": "5 Comparison with the literature", "text": "There are several probabilistic argumentation papers around. This is a hot topic in 2014. We highlight two main points of view. The external and the internal views.\nLet \u3008S,R\u3009 be a network and let f be a function from S to [0, 1]. We can regard f as giving a probability number to each x \u2208 S. The internal probability is where the above numbers signify the value of the argument. Its truth, its reliability, its probability of being effective, etc., or whatever measure we attach to it as an argument. Figure 15 represents in this case the Eqinv solution (and hence probability) of the network of Figures 2 and 3. The external view is to think of f(x) as the probability of the predicate \u201cx \u2208 S\u201d. That is, the probability that the argument x is present in S. Consider again Figure 15.\nThe probability that a is in the network is 0.236 and the probability that b is in the network is 0.382. Therefore, the probability that the network contains\nboth {a, b} is 0.236 \u00d7 0.388 = 0.09. The probability that the network contains only a is 0.236\u00d7(1\u22120.382) = 0.1458. The probability that the network contains only b is 0.382 \u00d7 (1 \u2212 0.236) = 0.292 and the probability that the network is empty is (1 \u2212 0.236) \u00d7 (1 \u2212 0.382) = 0.472. It is clear why we are calling this view an external probability view. It imposes probability externally expressing uncertainty on what the network graph is. This is done either by giving the probability to points or more generally by giving probability directly to subsets G of S, expressing the probability that the graph is really that subset of S with R restricted to G. This external view has value in dialogue argumentation or negotiation when we try to estimate what network our opponent is reasoning with. The problem with this external view is how to connect with the attack relation. Note that mathematically in the external view we have probabilities on points in S or probabilities on subsets of S, which are the same options as in our internal view, but the understanding of them is different. We in the internal view considered the subset as a classical model, while the external view considers it as a subnetwork. When we use the internal view, we can connect it with the attack relation via the equational approach (Equation (E3)), but how would the external view connect with the attack relation? We can ask, for example, how to get a value for a single point to be \u201cin\u201d an extension? Intuitively, looking back at Figure 15, we can say the point a for example is \u201cin\u201d in case the network is {a} and is also \u201cin\u201d in one of the three extensions in case the network is {a, b}. So we might take the \u201cin\u201d value to be 0.1458 + 0.09/3 = 0.1458 + 0.03 = 0.1758. The connection with the attack relation can be done perhaps through the probabilities for admissible sets, since being admissible is connected with the attack relation. There are problems, however, with this approach.\nHunter [7] was trying to lay some foundations for this view, following the papers [3, 9]. See also a good summary in Hunter[8]. Hunter was trying to find a connection between the external probability view and some reasonable values we can give to admissible subsets. He proposes restrictions on the probability function on S. We are not going to discuss or reproduce Hunter\u2019s arguments here. It suffices to say that possibly a subsequent paper of ours will critically examine the external view and compare with the internal view.\nLet us now compare our work with that of M. Thimm, [13], whose approach is also internal. We quote from [13]:\n\u201cIn this paper we use another interpretation for probability, that of subjective probability [11]. There, a probability P (X) for some X \u2208 X denotes the degree of belief we put into X. Then a probability function P can be seen as an epistemic state of some agent that\nhas uncertain beliefs with respect to X . In probabilistic reasoning [11, 12], this interpretation of probability is widely used to model uncertain knowledge representation and reasoning.\nIn the following, we consider probability functions on sets of arguments of an abstract argumentation frameworks. Let AF = (Arg, \u2192) be some fixed abstract argumentation framework and let E = 2Arg be the set of all sets of arguments. Let now PAF be the set of probability functions of the form P : 2\u03b5 \u2192 [0, 1]. A probability function P \u2208 PAF assigns to each set of possible extensions of AF a probability, i.e. P (e) for e \u2208 E is the probability that e is an extension and P (E) for E \u2286 E is the probability that any of the sets in E is an extension. In particular, note the difference between e.g. P ({A,B}) = P ({{A,B}}) and P ({{A}, {B}}) for arguments A,B. While the former denotes the probability that {A,B} is an extension the latter denotes the probability that {A} or {B} is an extension. In general, it holds P ({A,B}) 6= P ({{A}, {B}}) .\nFor P \u2208 PAF and A \u2208 Arg we abbreviate P (A) = \u2211\nA\u2208e\u2286Arg\nP (e).\nGiven some probability function P , the probability P (A) represents the degree of belief that A is in an extension (according to P ), i.e. P (A) is the sum of the probabilities of all possible extensions that contain A. The set PAF contains all possible views one can take on the arguments of an abstract argumentation framework AF. Example 4. We continue Ex. 1. (Comment by Gabbay and Rodrigues: This is the network of our Figure 4.) Consider the function P \u2208 PAF defined via P ({A1,A3,A5}) = 03, P ({A1,A4}) = 0.45, P ({A5,A2}) = 0.1, P ({A2,A4}) = 0.15, and P (3) = 0 for all remaining e \u2208 E . Due to Prop. 1 the function P is well-defined as in, e.g.,\nP ({{A5,A2}, {A2,A4}, {A3}}) = P ({A5,A2}) + P ({A2,A4}) + P ({A3}) = 0.1 + 0.15 + 0 = 0.25.\nTherefore, P is a probability function according to Def. 3. According to P the probabilities to reach argument of AF compute to P (A1) = 0.75, P (A2) = 0.25, P (A3 = 0.3, P (A4) = 0.6, and P (A5) = 0.4.\nIn the following, we are only interested in those probability functions of PAF that agree with our intuition on the interrelationships of arguments and attack. For example, if an argument A is not attacked we should completely believe in its validity if no further information is available. We propose the following notion of justifiability to describe this intuition. Definition 4. A probability function P \u2208 PAF is called p-justifiable wrt. AF, denoted by P J AF, if it satisfies for all A \u2208 Arg.\n1. P (A) \u2264 1\u2212 P (B) for all B,\u2208 Arg with B \u2192 A and 2. P (A) \u2265 1\u2212 \u2211 B\u2208F P (B) where F = {B|B \u2192 A}.\nLet PJAF be the set of all p-justifiable probability functions wrt. AF. The notion of p-justifiability generalizes the concept of complete semantics to the probabilistic setting. Property 1.) says that the degree of belief we assign to an argument A is bounded from above by the complement to 1 of the degrees of belief we put into the attackers of A. As a special case, note that if we completely believe in an attacker of A, i.e., P (B) = 1 for some B with B \u2192 A, then it follows P (A) = 0. This corresponds to property 1.) of a complete labelling (see Section 2). Property 2.) of Def. 4 says that the degree of belief we assign to an argument A is bounded from below by the inverse of the sum of the degrees of belief we put into the attacks of A. As a special case, note that if we completely disbelieve in all attackers of A, i.e. P (B) = 0 for all B with B \u2192 A, then it follows P (A) = 1. This corresponds to property 2.) of a complete labeling, see Section 2. The following proposition establishes the probabilistic analogue of the third property of a complete labelling. Proposition 2. Let P be p-justifiable and A \u2208 Arg. If P (A) \u2208 (0, 1) then\n1. there is no B \u2208 Arg with B \u2192 A and P (B) = 1 and 2. there is a B\u2032 \u2208 Arg with B\u2032 \u2192 A and P (B\u2032) > 0.\nFrom our point of view, Thimm\u2019s approach is a variant of our semantic Method 2 approach without the strong equation (E3) but the weaker Definition 4 of Thimm. Thus Thimm will allow for different values for nodes x1 and x2 in our Figure 8, while we would not (see Example 3.5).\nAlthough Thimm\u2019s approach is mathematically close to us, conceptually we are far apart. Thimm motivates his approach as a degree of belief in a subset E \u2286 S, considering E as an extension. We consider E as representing a classical model m of the classical propositional logic with atoms S\nm = \u2227 s\u2208E s \u2227 \u2227 s6\u2208E \u00acs\nand assign probability to it and then we export this probability to argumentation via the equational approach, equation (E3).\nThis is an instance of our methodology of \u201cLogic by Translation\u201d, From our point of view, equations (E3) are essential, conceptual and non-technical. For Thimm, the inequalities of his Definition 4 appear to be technical to enable the probabilities to work of ground extension.\nOur point of view also leads us to the Eqinv Method 1 probabilities and to the approximation results of Section 4.\nIn Thimm\u2019s conceptual approach, this way of thinking does not even arise. To summarise, this paper presented an internal view of probabilistic argu-\nmentation. There is a need for two subsequent research papers\n1. The external view done coherently and its connection to the internal view\n2. A conditional probability view and its connection with Bayesian Networks views as Argumentation Networks"}, {"heading": "6 Conclusions", "text": "This section explains and sets our approach in a general generic context. Suppose we are given a system S such as an argumentation system \u3008S,R\u3009 and we want to add to it some aspect A. There is a generic way to add any new feature to a system. It involves 1) identifying the basic units which build up the system and 2) introducing the new feature to each of these basic units. In the case where the system is argumentation and the feature is probabilistic we have the following: the basic units are a. the nature of the arguments involved; b. the membership relation in the set S of arguments;6 c. the attack relation; and d. the choice of extensions.\nGenerically to add a new aspect (probabilistic, or fuzzy, or temporal, etc) to an argumentation network \u3008S,R\u3009 can be done by adding this feature to each component. a. We make the effective strength of the argument probabilistic; b. we give probability to whether an argument is included in S;7 c. we make the attack relation probabilistic; and d. we put probability on the extensions.\nThese features interact and need to be chosen with care and coordination. We need a methodological approach to make our choices. One such methodology is what we called logic by translation.\nWe meaningfully translate the argumentation system into classical logic which does have probabilistic models and then let probabilistic classical logic endow the probability on the argumentation system. As we mentioned, this of course depends on how we translate.\nWe gave in this paper an object-level translation. The arguments of S became atoms of classical propositional logic, we then used probability on the models of classical logic and used the attack relation R to express equational restrictions on the probabilities. In this kind of translation, the attack relation did not become probabilistic.\nWe could have used a meta-level translation into classical predicate logic, using a binary relation R for expressing in classical logic the attack relation and using unary predicates to express that an argument x is \u201cin\u201d, x is \u201cout\u201d, etc., with suitable coordinating axioms. In this case all predicates would have become probabilistic including the attack relation R. As far as we know nobody has done this to R.\nIn this context of possible options what we have done is one systematic approach and we compared it with other approaches. It should be noted that we could have followed the same steps to get fuzzy argumentation networks;\n6Note that the set S itself may not be fully or accurately known, especially modelling an opponent in dialogue systems.\n7a. and b. are distinct, because a. represents how effective an argument is, whereas b. is the decision of whether or not to include an argument for consideration. An argument may be deemed very effective but not included for consideration for completely different reasons.\ntemporal argumentation networks; or indeed any other feature available for classical propositional logic."}], "references": [{"title": "A logical account of formal argumentation", "author": ["M. Caminada", "D. Gabbay"], "venue": "Studia Logica,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "On judgment aggregation in abstract argumentation", "author": ["M. Caminada", "G. Pigozzi"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Towards (probabilistic) argumentation for jury-based depute resolution", "author": ["P.M. Dung", "P. Thang"], "venue": "Proceedings of COMMA III, Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Logics for Artificial Intelligence and Information Technology", "author": ["D. Gabbay"], "venue": "College Publications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Equational approach to argumentation networks. Argument and Computation, pages 87\u2013142", "author": ["D. Gabbay"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "A self-correcting iteration schema for argumentation networks", "author": ["D.M. Gabbay", "O. Rodrigues"], "venue": "Proceedings of COMMA V, Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Some foundations for probabilistic abstract argumentation", "author": ["A. Hunter"], "venue": "Proceedings of COMMA IV, Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A probabilistic approach to modelling uncertain logical arguments", "author": ["A. Hunter"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Probabilistic argumentation frameworks", "author": ["H. Li", "N. Oren", "T. Norman"], "venue": "In Proceedings of the First International Workshop on the Theory and Applications of Formal Argumentation (TAFA\u201911),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "ASPIC+ framework for structured argumentation: A tutorial", "author": ["S. Modgil", "H. Prakken"], "venue": "Argument & Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "The Uncertain Reasoner\u2019s Companion. A Mathematical Perspective", "author": ["J.B. Paris"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Probabilistic Reasoning in Intelligent Systems. Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "A probabilistic semantics for abstract argumentation", "author": ["M. Thimm"], "venue": "In Proceedings of the 20th European Conference on Artificial Intelligence (ECAI\u201912),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "If we consider the equational approach [5], then we can write", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "where (E2) is a numerical equation over the real interval [0, 1], with conjunction and negation interpreted as numerical functions expressing the correspondence of the values of the two sides.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "The reader should note that we actually solve the equations over the unit interval [0, 1] and project onto Kleene\u2019s 3-valued logic by letting", "startOffset": 83, "endOffset": 89}, {"referenceID": 3, "context": "The simplest two methods are described in Gabbay\u2019s book Logic for Artificial Intelligence and Information Technology [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 0, "context": "In the equational approach, according to the syntactical Method 1, we assign probabilities to all the atoms and are required to solve the equation (E3) below for each x, where Att(x) = {yi} and x and all yi are numbers in [0, 1]:", "startOffset": 222, "endOffset": 228}, {"referenceID": 4, "context": "This is the Eqinv equation in the equational approach (see [5]).", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "The following definition will be useful in the interpretation of values from [0, 1] and their counterparts in Caminada\u2019s labelling functions.", "startOffset": 77, "endOffset": 83}, {"referenceID": 4, "context": "What do we know about Eqinv? We quote the following from [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "1 is a legal Caminada labelling (see [1]) and leads to a complete extension.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "This is an arbitrary number in [0, 1].", "startOffset": 31, "endOffset": 37}, {"referenceID": 1, "context": "[2, 6]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[2, 6]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "Such an analysis is required anyway for instantiated networks, for example in ASPIC+ style [10]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "Thimm, see [13].", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "Figure 4: Figure 1 of \u201cA probabilistic semantics for abstract argumentation\u201d [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "Figure 5: Figure 2 of \u201cA probabilistic semantics for abstract argumentation\u201d [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "Thimm in [13].", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "Thimm [13]) Let S = {s1, .", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "If we let P (a1) = x, P (a2) = 1\u2212 x, P (a3) = 0, with x \u2208 [0, 1], then P satisfies the equations.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Let \u3008S,R\u3009 be a network and let f be a function from S to [0, 1].", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "Hunter [7] was trying to lay some foundations for this view, following the papers [3, 9].", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Hunter [7] was trying to lay some foundations for this view, following the papers [3, 9].", "startOffset": 82, "endOffset": 88}, {"referenceID": 8, "context": "Hunter [7] was trying to lay some foundations for this view, following the papers [3, 9].", "startOffset": 82, "endOffset": 88}, {"referenceID": 7, "context": "See also a good summary in Hunter[8].", "startOffset": 33, "endOffset": 36}, {"referenceID": 12, "context": "Thimm, [13], whose approach is also internal.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "We quote from [13]:", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "\u201cIn this paper we use another interpretation for probability, that of subjective probability [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "In probabilistic reasoning [11, 12], this interpretation of probability is widely used to model uncertain knowledge representation and reasoning.", "startOffset": 27, "endOffset": 35}, {"referenceID": 11, "context": "In probabilistic reasoning [11, 12], this interpretation of probability is widely used to model uncertain knowledge representation and reasoning.", "startOffset": 27, "endOffset": 35}, {"referenceID": 0, "context": "Let now PAF be the set of probability functions of the form P : 2 \u2192 [0, 1].", "startOffset": 68, "endOffset": 74}], "year": 2015, "abstractText": "There is a generic way to add any new feature to a system. It involves 1) identifying the basic units which build up the system and 2) introducing the new feature to each of these basic units. In the case where the system is argumentation and the feature is probabilistic we have the following. The basic units are: a. the nature of the arguments involved; b. the membership relation in the set S of arguments; c. the attack relation; and d. the choice of extensions. Generically to add a new aspect (probabilistic, or fuzzy, or temporal, etc) to an argumentation network \u3008S,R\u3009 can be done by adding this feature to each component a\u2013d. This is a brute-force method and may yield a non-intuitive or meaningful result. A better way is to meaningfully translate the object system into another target system which does have the aspect required and then let the target system endow the aspect on the initial system. In our case we translate argumentation into classical propositional logic and get probabilistic argumentation from the translation. Of course what we get depends on how we translate. In fact, in this paper we introduce probabilistic semantics to abstract argumentation theory based on the equational approach to argumentation networks. We then compare our semantics with existing proposals in the literature including the approaches by M. Thimm and by A. Hunter. Our methodology in general is discussed in the conclusion. 1 ar X iv :1 50 3. 05 50 1v 1 [ cs .A I] 1 8 M ar 2 01 5", "creator": "LaTeX with hyperref package"}}}