{"id": "1503.05671", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "abstract": "wellingtons We propose r.i.a.a. an glissandos efficient 700-strong method for futari approximating natural korir gradient descent in timi\u0219oara neural khrushcheva networks which tpn5 we call hearst Kronecker - factored banyamurenge Approximate loeff Curvature (K - delite FAC ). K - 52-53 FAC scarification is 150-250 based ferrocarrils on madagascans an efficiently kmh invertible approximation of norad a pensford neural network ' sh\u012b s underperformers Fisher information answers.com matrix autopistas which is spectroscopically neither pecks diagonal 11.82 nor low - rank, and samal in some cases atias is simplifies completely nyama non - sparse. shouter It is derived by csio approximating godwinson various burnaston large 7,340 blocks of isidro the Fisher (1970/71 corresponding niedermayer to entire volker layers) vexillology as footboard factoring as Kronecker 2.935 products between bayview two jayadratha much smaller 5-35 matrices. While pby only shorthorns several eroded times more discuss expensive mda to barseback compute than grevemberg the plain stochastic unneeded gradient, the 1776 updates ishar produced by bdb K - dedalus FAC enumerator make much more progress referees optimizing the objective, which ju-52 results keepmoat in nalepa an tii algorithm that xsi can 97.36 be czuma much faster than stochastic tamagotchis gradient uchi descent with atelectasis momentum in practice. And wait unlike chatham-kent some defendent previously proposed approximate natural - gradient / goodner Newton aupe methods such chitimacha as Hessian - strobridge free thevenot methods, saddlers K - edgars FAC konen works very ironies well qf1 in 110.04 highly chi-squared stochastic optimization regimes.", "histories": [["v1", "Thu, 19 Mar 2015 08:30:24 GMT  (995kb,D)", "http://arxiv.org/abs/1503.05671v1", null], ["v2", "Fri, 3 Apr 2015 20:19:14 GMT  (977kb,D)", "http://arxiv.org/abs/1503.05671v2", "Fixed various typos. Tweaked experiments and updated figures. Expanded a few discussions and added a small subsection discussing approximation quality plots"], ["v3", "Tue, 28 Apr 2015 05:48:59 GMT  (1020kb,D)", "http://arxiv.org/abs/1503.05671v3", "Changes in this version: Tweaked experiments and updated figures (using uniform l_2 = 1e-5 now). Added a section with pseudocode. Revamped damping section, adding lots of discussion, and now recommending separate damping strength adjustment for approximate Fisher (see Section 6.6). Various other minor tweaks and fixes"], ["v4", "Thu, 21 May 2015 00:25:06 GMT  (1066kb,D)", "http://arxiv.org/abs/1503.05671v4", "Changes in this version: Redid matrix quality plots to use tanh units to better highlight difference between exact Fisher and approximation (previously used logistic units added a big low-rank component which was well-approximated). Added discussion to damping section, and a graph looking at the importance of re-scaling. Adding comparisons to momentumless K-FAC. Other minor fixes and tweaks"], ["v5", "Fri, 24 Jul 2015 02:30:35 GMT  (1090kb,D)", "http://arxiv.org/abs/1503.05671v5", "Added some diagrams of neural networks (original and transformed versions), and added a conclusions section. Made a few other minor tweaks and fixes"], ["v6", "Wed, 4 May 2016 00:29:33 GMT  (1090kb,D)", "http://arxiv.org/abs/1503.05671v6", "Various minor additions, corrections and tweaks. Added link to code"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["james martens", "roger b grosse"], "accepted": true, "id": "1503.05671"}, "pdf": {"name": "1503.05671.pdf", "metadata": {"source": "CRF", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "authors": ["James Martens", "Roger Grosse"], "emails": ["jmartens@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "The problem of training neural networks is one of the most important and highly investigated ones in machine learning. Despite work on layer-wise pretraining schemes, and various sophisticated optimization methods which try to approximate Newton-Raphson updates or natural gradient updates, stochastic gradient descent (SGD), possibly augmented with momentum, remains the method of choice for large-scale neural network training (Sutskever et al., 2013).\nFrom the work on Hessian-free optimization (HF) (Martens, 2010) and related methods (e.g. Vinyals and Povey, 2012) we know that updates computed using local curvature information can make much more progress per iteration than the scaled gradient. The reason that HF sees fewer practical applications than SGD are twofold. Firstly, its updates are much more expensive to compute, as they involve running linear conjugate gradient (CG) for potentially hundreds of iterations,\n\u2217jmartens@cs.toronto.edu \u2020rgrosse@cs.toronto.edu\nar X\niv :1\n50 3.\n05 67\n1v 1\n[ cs\n.L G\n] 1\n9 M\nar 2\neach of which requires a matrix-vector product with the curvature matrix (which are as expensive to compute as the stochastic gradient on the current mini-batch). Secondly, HF\u2019s estimate of the curvature matrix must remain fixed while CG iterates, and thus the method is able to go through much less data than SGD can in a comparable amount of time, making it less well suited to stochastic optimizations.\nAs discussed in Martens and Sutskever (2012), CG has the potential to be much faster at local optimization than gradient descent, when applied to quadratic objective functions. Thus, insofar as the objective can be locally approximated by a quadratic, each step of CG could potentially be doing a lot more work than each iteration of SGD, which would result in HF being much faster overall than SGD.\nHowever, there are examples of quadratic functions (e.g. Li, 2005), characterized by curvature matrices with highly spread-out eigenvalue distributions, where CG will have no distinct advantage over well-tuned gradient descent with momentum. Thus, insofar as the quadratic functions being optimized by CG within HF are of this character, HF shouldn\u2019t in principle be faster than welltuned SGD with momentum. The extent to which neural network objective functions give rise to such quadratics is unclear, although (Sutskever et al., 2013) provides some preliminary evidence that they do.\nCG falls victim to this worst-case analysis because it is a first-order method. This motivates us to consider methods which don\u2019t rely on first-order methods like CG as their primary engines of optimization. One such class of methods which have been widely studied are those which work by directly inverting a diagonal, block-diagonal, or low-rank approximation to the curvature matrix (e.g. Becker and LeCun, 1989; Schaul et al., 2013; Le Roux et al., 2008; Ollivier, 2013). In fact, a diagonal approximation of the Fisher information matrix is used within HF as a preconditioner for CG. However, to the best of our knowledge, these methods provide only a limited performance improvement in practice, especially compared to SGD with momentum (see for example Zeiler, 2013), and many practitioners tend to forgo them in favor of SGD or SGD with momentum.\nWe know that the curvature associated with neural network objective functions is highly nondiagonal, and that updates which properly respect and account for this non-diagonal curvature, such as those generated by HF, can make much more progress minimizing the objective than the plain gradient or updates computed from diagonal approximations of the curvature. Thus, if we had an efficient and direct way to compute the inverse of a high-quality non-diagonal approximation to the curvature matrix (i.e. without relying on first-order methods like CG) this could potentially yield an optimization method whose updates would be large and powerful like HF\u2019s, while being (almost) as cheap to compute as the stochastic gradient.\nIn this work we develop such a method, which we call Kronecker-factored Approximate Curvature (K-FAC). We show that our method can be much faster in practice than even highly tuned implementations of SGD with momentum on certain standard neural network optimization benchmarks.\nThe main ingredient in K-FAC is a sophisticated approximation to the Fisher information\nmatrix, which despite being neither diagonal nor low-rank, nor even block-diagonal with small blocks, can be inverted very efficiently, and can be estimated in an online fashion using arbitrarily large subsets of the training data (without increasing the cost of inversion).\nThis approximation is built in two stages. In the first, the rows and columns of the Fisher are divided into groups, each of which corresponds to all the weights in a given layer, and this gives rise to a block-partitioning of the matrix (where the blocks are much larger than those used by Le Roux et al. (2008) or Ollivier (2013)). These blocks are then approximated as Kronecker products between much smaller matrices, which we show is equivalent to making certain approximating assumptions regarding the statistics of the network\u2019s gradients.\nIn the second stage, this matrix is further approximated as having an inverse which is either block-diagonal or block-tridiagonal. We justify this approximation through a careful examination of the relationships between inverse covariances, tree-structured graphical models, and linear regression. Notably, this justification doesn\u2019t apply to the Fisher itself, and our experiments confirm that while the inverse Fisher does indeed possess this structure (approximately), the Fisher itself does not.\nThe rest of this paper is organized as follows. Section 2 gives basic background and notation for neural networks and the natural gradient. Section 3 describes our initial Kronecker product approximation to the Fisher. Section 4 describes our further block-diagonal and block-tridiagonal approximations of the inverse Fisher, and how these can be used to derive an efficient inversion algorithm. Section 5 describes how we compute online estimates of the quantities required by our approximation. Section 6 describes how we use our approximate Fisher to obtain a practical and robust optimization algorithm which requires very little manual tuning, through the careful application of various \u201cdamping\u201d techniques (which compensate both for the local quadratic approximation being made to the objective, and for our further approximation of the Fisher). Section 7 describes a simple and effective way of adding a type of \u201cmomentum\u201d to K-FAC, which we have found works very well in practice. Section 8 describes the computational costs associated with KFAC, and various ways to reduce them to the point where each update is at most only several times more expensive than computing the stochastic gradient. Section 9 characterizes a broad class of network transformations and reparameterizations to which K-FAC is essentially invariant. Section 10 considers some related prior methods for neural network optimization. Proofs of formal results are located in the appendix."}, {"heading": "2 Background and notation", "text": ""}, {"heading": "2.1 Neural Networks", "text": "In this section we will define the basic notation for feed-forward neural networks which we will use throughout this paper. Note that this presentation closely follows the one from Martens (2014).\nA neural network transforms its input a0 = x to an output f(x, \u03b8) = a` through a series of ` layers, each of which consists of a bank of units/neurons. The units each receive as input a weighted sum of the outputs of units from the previous layer and compute their output via a nonlinear \u201cactivation\u201d function. We denote by si the vector of these weighted sums for the i-th layer, and by ai the vector of unit outputs (aka \u201cactivities\u201d). The precise computation performed at each layer i \u2208 {1, . . . , `} is given as follows:\nsi = Wia\u0304i\u22121\nai = \u03c6i(si)\nwhere \u03c6i is an element-wise nonlinear function, Wi is a weight matrix, and a\u0304i is defined as the vector formed by appending an additional homogeneous coordinate with value 1 to ai. Note that we do not include explicit bias parameters here as these are captured implicitly through our use of homogeneous coordinates. In particular, the last column of each weight matrix Wi corresponds to what is usually thought of as the \u201cbias vector\u201d.\nWe will define \u03b8 = [vec(W1)> vec(W2)> . . . vec(W`)>]>, which is the vector consisting of all of the network\u2019s parameters concatenated together, where vec is the operator which vectorizes matrices by stacking their columns together.\nWe let L(y, z) denote the loss function which measures the disagreement between a prediction z made by the network, and a target y. The training objective function h(\u03b8) is the average (or expectation) of losses L(y, f(x, \u03b8)) with respect to a training distribution Q\u0302x,y over input-target pairs (x, y). h(\u03b8) is a proxy for the objective which we actually care about but don\u2019t have access to, which is the expectation of the loss taken with respect to the true data distribution Qx,y.\nWe will assume that the loss is given by the negative log probability associated with a simple predictive distribution Ry|z for y parameterized by z, i.e. that we have\nL(y, z) = \u2212 log r(y|z)\nwhere r is Ry|z\u2019s density function. This is the case for both the standard least-squares and crossentropy objective functions, where the predictive distributions are multivariate normal and multinomial, respectively.\nWe will let Py|x(\u03b8) = Ry|f(x,\u03b8) denote the conditional distribution defined by the neural network, as parameterized by \u03b8, and p(y|x, \u03b8) = r(y|f(x, \u03b8)) its density function. Note that minimizing the objective function h(\u03b8) can be seen as maximum likelihood learning of the model Py|x(\u03b8).\nFor convenience we will define the following additional notation:\nDv = dL(y, f(x, \u03b8)) dv = \u2212d log p(y|x, \u03b8) dv\nand gi = Dsi\nAlgorithm 1 shows how to compute the gradient of the loss function of a neural network using standard backpropagation.\nAlgorithm 1 An algorithm for computing the gradient of the loss L(y, f(x, \u03b8)) for a given (x, y). Note that we are assuming here for simplicity that the \u03c6i are defined as coordinate-wise functions.\ninput: a0 = x; \u03b8 mapped to (W1,W2, . . . ,W`).\n/* Forward pass */ for all i from 1 to ` do si \u2190 Wia\u0304i\u22121 ai \u2190 \u03c6i(si) end for\n/* Loss derivative computation */ Da` \u2190 \u2212 \u2202L(y, z)\n\u2202z\n\u2223\u2223\u2223\u2223 z=a`\n/* Backwards pass */ for all i from ` downto 1 do gi \u2190 Dai \u03c6\u2032i(si) DWi \u2190 gia\u0304>i\u22121 Dai\u22121 \u2190 W>i gi end for\noutput: D\u03b8 = [vec(DW1)> vec(DW2)> . . . vec(DW`)>]>"}, {"heading": "2.2 The Natural Gradient", "text": "Because our network defines a conditional model Py|x(\u03b8), it has an associated Fisher information matrix (which we will simply call \u201cthe Fisher\u201d) which is given by\nF = E\n[ d log p(y|x, \u03b8)\nd\u03b8\nd log p(y|x, \u03b8) d\u03b8\n> ]\n= E[D\u03b8D\u03b8>]\nHere, the expectation is taken with respect to the data distributionQx over inputs x, and the model\u2019s predictive distribution Py|x(\u03b8) over y. Since we usually don\u2019t have access to Qx, and the above expectation would likely be intractable even if we did, we will instead compute F using the training distribution Q\u0302x over inputs x.\nThe well-known natural gradient (Amari, 1998) is defined as F\u22121\u2207h(\u03b8). Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence. This is to be contrasted with the standard gradient, which can be defined as the direction in parameter space which gives the largest change in the objective per unit of change in the parameters, as measured by the standard Euclidean metric.\nThe natural gradient also has links to several classical ideas from optimization. It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function. Indeed, the GGN has served as the curvature matrix of choice in HF and related methods, and so in light of its equivalence to the Fisher, these methods can all be seen as approximate natural gradient methods.\nIn particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.1\nFor some good recent discussion and analysis of the natural gradient, see Arnold et al. (2011); Martens (2014); Pascanu and Bengio (2014)."}, {"heading": "3 A block-wise Kronecker-factored Fisher approximation", "text": "The main computational challenge associated with using the natural gradient is computing F\u22121 (or its product with \u2207h). For large networks, with potentially millions of parameters, computing this\n1Note that the condition that z is the natural parameters might require one to formally include the nonlinear transformation usually performed by the final nonlinearity \u03c6` of the network (such as the logistic-sigmoid transform before a cross-entropy error) as part of the loss function L instead. Equivalently, one could linearize the network only up to the input s` to \u03c6` when computing the GGN (see Martens and Sutskever (2012)).\ninverse naively is computationally impractical. In this section we develop an initial approximation of F which will be a key ingredient in deriving our efficiently computable approximation to F\u22121 and the natural gradient.\nNote that D\u03b8 = [vec(DW1)> vec(DW2)> \u00b7 \u00b7 \u00b7 vec(DW`)>]> and so F can be expressed as F = E [ D\u03b8D\u03b8> ] = E [ [vec(DW1)> vec(DW2)> \u00b7 \u00b7 \u00b7 vec(DW`)>]>[vec(DW1)> vec(DW2)> \u00b7 \u00b7 \u00b7 vec(DW`)>]\n] =  E [ vec(DW1) vec(DW1)> ] E [ vec(DW1) vec(DW2)> ] \u00b7 \u00b7 \u00b7 E [ vec(DW1) vec(DW`)> ] E [ vec(DW2) vec(DW1)> ] E [ vec(DW2) vec(DW2)> ] \u00b7 \u00b7 \u00b7 E [ vec(DW2) vec(DW`)> ] ... ... . . . ...\nE [ vec(DW`) vec(DW1)> ] E [ vec(DW`) vec(DW2)> ] \u00b7 \u00b7 \u00b7 E [ vec(DW`) vec(DW`)>\n] \nThus, we see that F can be viewed as an ` by ` block matrix, with the (i, j)-th block Fi,j given by Fi,j = E [ vec(DWi) vec(DWj)> ] .\nNoting that DWi = gia\u0304>i\u22121 and that vec(uv>) = v \u2297 u we have vec(DWi) = vec(gia\u0304>i\u22121) = a\u0304i\u22121 \u2297 gi, and thus we can rewrite Fi,j as\nFi,j = E [ vec(DWi) vec(DWj)> ] = E [ (a\u0304i\u22121 \u2297 gi)(a\u0304j\u22121 \u2297 gj)> ] = E [ (a\u0304i\u22121 \u2297 gi)(a\u0304>j\u22121 \u2297 g>j ) ] = E [ a\u0304i\u22121a\u0304 > j\u22121 \u2297 gig>j\n] where A\u2297B denotes the Kronecker product between A and B.\nOur initial approximation F\u0303 to F will be defined by the following block-wise approximation: Fi,j = E [ a\u0304i\u22121a\u0304 > j\u22121 \u2297 gig>j ] \u2248 E [ a\u0304i\u22121a\u0304 > j\u22121 ] \u2297 E [ gig > j ] = A\u0304i\u22121,j\u22121 \u2297Gi,j = F\u0303i,j (1)\nwhere A\u0304i,j = E [ a\u0304ia\u0304 > j ] and Gi,j = E [ gig > j ] .\nThis gives\nF\u0303 =  A\u03040,0 \u2297G1,1 A\u03040,1 \u2297G1,2 \u00b7 \u00b7 \u00b7 A\u03040,`\u22121 \u2297G1,` A\u03041,0 \u2297G2,1 A\u03041,1 \u2297G2,2 \u00b7 \u00b7 \u00b7 A\u03041,`\u22121 \u2297G2,` ... ... . . .\n... A\u0304`\u22121,0 \u2297G`,1 A\u0304`\u22121,1 \u2297G`,2 \u00b7 \u00b7 \u00b7 A\u0304`\u22121,`\u22121 \u2297G`,`  which has the form of what is known as a Khatri-Rao product in multivariate statistics.\nThe expectation of a Kronecker product is, in general, not equal to the Kronecker product of expectations, and so this is indeed a strong approximation to make, and one which likely won\u2019t become exact under any realistic set of assumptions, or as a limiting case in some kind of asymptotic analysis. Nevertheless, it seems to be fairly accurate in practice, as demonstrated in Figure 1 for an example network.\nAs we will see in later sections, this approximation leads to significant computational savings in terms of storage and inversion, which we will be able to leverage in order to design an efficient algorithm for computing an approximation to the natural gradient."}, {"heading": "3.1 Interpretations of this approximation", "text": "Consider an arbitrary pair of weights [Wi]k1,k2 and [Wj]k3,k4 from the network, where [\u00b7]i,j denotes the value of the (i, j)-th entry. We have that the corresponding derivatives of these weights are given by D[Wi]k1,k2 = a\u0304(1)g(1) and D[Wj]k3,k4 = a\u0304(2)g(2), where we denote for convenience a\u0304(1) = [a\u0304i\u22121]k1 , a\u0304 (2) = [a\u0304j\u22121]k3 , g (1) = [gi]k2 , and g (2) = [gj]k4 .\nThe approximation given by eqn. 1 is equivalent to making the following approximation for each pair of weights:\nE [D[Wi]k1,k2D[Wj]k3,k4 ] = E [ (a\u0304(1)g(1))(a\u0304(2)g(2)) ] = E [ a\u0304(1)a\u0304(2) g(1)g(2) ] \u2248 E [ a\u0304(1)a\u0304(2) ] E [ g(1)g(2) ] (2)\nAnd thus one way to interpret the approximation in eqn. 1 is that we are assuming statistical independence between products a\u0304(1)a\u0304(2) of unit activities and products g(1)g(2) of unit input derivatives.\nAnother more detailed interpretation of the approximation emerges by considering the following expression for the approximation error E [ a\u0304(1)a\u0304(2) g(1)g(2) ] \u2212E [ a\u0304(1)a\u0304(2) ] E [ g(1)g(2) ] (which is derived in the appendix):\n\u03ba(a\u0304(1), a\u0304(2), g(1), g(2)) + E[a\u0304(1)]\u03ba(a\u0304(2), g(1), g(2)) + E[a\u0304(2)]\u03ba(a\u0304(1), g(1), g(2)) (3)\nwhere \u03ba(\u00b7) denotes the cumulant of its arguments.\nCumulants are a natural generalization of the concept of mean and variance to higher orders, and indeed 1st-order cumulants are means and 2nd-order cumulants are covariances. Intuitively,\ncumulants of order k measure the degree to which the interaction between variables is intrinsically of order k, as opposed to arising from many lower-order interactions.\nA basic upper bound for the approximation error is thus\n|\u03ba(a\u0304(1), a\u0304(2), g(1), g(2))|+ |E[a\u0304(1)]||\u03ba(a\u0304(2), g(1), g(2))|+ |E[a\u0304(2)]||\u03ba(a\u0304(1), g(1), g(2))| (4)\nwhich will be small if all of the higher-order cumulants are small (i.e. those of order 3 or higher). Note that in principle this upper bound may be loose due to possible cancellations between the terms in eqn. 3.\nBecause higher-order cumulants are zero for variables jointly distributed according to a multivariate Gaussian, it follows that this upper bound on the approximation error will be small insofar as the joint distribution over a\u0304(1), a\u0304(2), g(1), and g(2) is well approximated by a multivariate Gaussian. And while we are not aware of an argument for why this should be the case in practice, we can provide some empirical evidence that it is indeed the smallness of the higher-order cumulants which is responsible for approximation error being relatively small. In particular, for the example network from Figure 1, the total approximation error, summed over all pairs of weights in the middle 4 layers, is 291.7, while the corresponding upper bound, whose size is tied to that of the higher order cumulants (due to the impossibility of cancellations in eqn. 4), is 446.8."}, {"heading": "4 Additional approximations to F\u0303 and inverse computations", "text": "To the best of our knowledge there is no efficient general method for inverting a Khatri-Rao product like F\u0303 . Thus, we must make further approximations if we hope to obtain an efficiently computable approximation of the inverse Fisher.\nIn the following subsections we argue that the inverse of F\u0303 can be reasonably approximated as having one of two special structures, either of which make it efficiently computable. The second of these will be slightly less restrictive than the first (and hence a better approximation) at the cost of some additional complexity. We will then show how matrix-vector products with these approximate inverses can be efficiently computed, which will thus give an efficient algorithm for computing an approximation to the natural gradient."}, {"heading": "4.1 Structured inverses and the connection to linear regression", "text": "Suppose we are given a multivariate distribution over some variables, whose covariance matrix is \u03a3.\nDefine the matrixB so that for i 6= j [B]i,j is the coefficient on the j-th variable in the optimal linear predictor of the i-th variable from all the other variables, and [B]i,j = 0 for i = j. Then define the matrix D to be the diagonal matrix where [D]i,i is the variance of the error associated with such a predictor of the i-th variable.\nPourahmadi (2011) showed that B can be obtained from the inverse covariance \u03a3\u22121 by the formula\n[B]i,j = \u2212 [\u03a3\u22121]i,j [\u03a3\u22121]i,i and [D]i,i = 1 [\u03a3\u22121]i,i\nFrom this it follows that the inverse covariance of the data can be expressed as\n\u03a3\u22121 = D\u22121(I \u2212B)\nIntuitively, this result says that each row of the inverse covariance \u03a3\u22121 is given by the coefficients of the optimal linear predictor of the i-th variable from the others, up to a scaling factor. So if the j-th variable is much less useful than the other variables for predicting the i-th variable, we can expect that the (i, j)-th entry of the inverse covariance will be relatively small.\nNoting that the Fisher F is a covariance matrix over D\u03b8 w.r.t. the model\u2019s predictive distribution (because E[D\u03b8] = 0), we can thus apply the above analysis to the distribution over D\u03b8 to gain insight into the approximate structure of F\u22121, and by extension its approximation F\u0303\u22121.\nConsider the derivative DWi of the loss with respect to the weights Wi of layer i. Intuitively, if we are trying to predict one of the entries of DWi from the other entries of D\u03b8, those entries of D\u03b8 also in DWi will be the most useful in this regard. Thus, it stands to reason that the largest entries of F\u0303\u22121 will be those on the diagonal blocks (i.e. those between different entries of a given DWi), so that F\u0303\u22121 will be well approximated as block-diagonal, with each block corresponding to a different DWi.\nBeyond the other entries of DWi, it is the entries of DWi+1 and DWi\u22121 (i.e. those associated with adjacent layers) that will arguably be the most useful in predicting a given entry of DWi. This is because the true process for computing the loss gradient only uses information from the layer below (during the forward pass) and from the layer above (during the backwards pass). Thus, approximating F\u0303\u22121 as block-tridiagonal seems like a reasonable and milder alternative than taking it to be block-diagonal.\nIndeed, this approximation would be exact if the distribution over D\u03b8 was given by a directed graphical model which generated each of the DWi\u2019s, one layer at a time, from either DWi+1 or DWi\u22121. Or equivalently, if DWi were distributed according to an undirected Gaussian graphical model with binary potentials only between entries in the same or adjacent layers. And while in reality the DWi\u2019s are generated using information from adjacent layers according to a process that is neither linear nor Gaussian, it nonetheless stands to reason that their joint statistics might be reasonably approximated by such a model. In fact, the idea of approximating the distribution over loss gradients with a directed graphical model forms the basis of the recent FANG method (Grosse and Salakhutdinov, 2014).\nFigure 2 examines the extent to which the inverse Fisher is well approximated as blockdiagonal or block-tridiagonal for an example network.\nIn the following two subsections we show how both the block-diagonal and block-tridiagonal approximations to F\u0303\u22121 give rise to computationally efficient methods for computing matrix-vector products with it. And at the end of Section 4 we present two figures (Figures 4 and 5) which examine the quality of these approximations for an example network."}, {"heading": "4.2 Approximating F\u0303\u22121 as block-diagonal", "text": "Approximating F\u0303\u22121 as block-diagonal is equivalent to approximating F\u0303 as block-diagonal. A natural choice for such an approximation F\u0306 of F\u0303 , is to take the block-diagonal of F\u0306 to be that of F\u0303 .\nThis gives the matrix F\u0306 = diag ( F\u03031,1, F\u03032,2, . . . , , F\u0303`,` ) = diag ( A\u03040,0 \u2297G1,1, A\u03041,1 \u2297G2,2, . . . , A\u0304`\u22121,`\u22121 \u2297G`,` ) whose inverse is easily computed as\nF\u0306\u22121 = diag ( A\u0304\u221210,0 \u2297G\u221211,1, A\u0304\u221211,1 \u2297G\u221212,2, . . . , A\u0304\u22121`\u22121,`\u22121 \u2297G \u22121 `,` ) Thus, computing F\u0306\u22121 amounts to computing the inverses of 2` smaller matrices.\nThen to compute u = F\u0306\u22121v, we can make use of the well-known identity (A\u2297B) vec(X) = vec(BXA>) to get\nUi = G \u22121 i,i ViA\u0304 \u22121 i\u22121,i\u22121\nwhere v maps to (V1, V2, . . . , V`) and similarly u maps to (U1, U2, . . . , U`), in an analogous way to how \u03b8 maps to (W1,W2, . . . ,W`).\nNote that block-diagonal approximations to the Fisher information have been proposed before (Le Roux et al., 2008), where each block corresponds to the weights associated with a particular unit. In our block-diagonal approximation, the blocks correspond to all the parameters in a given layer, and are thus much larger. In fact, they are so large that they would be impractical to invert as general matrices."}, {"heading": "4.3 Approximating F\u0303\u22121 as block-tridiagonal", "text": "Note that unlike in the above block-diagonal case, assuming that F\u0303\u22121 is block-tridiagonal is not equivalent to assuming that F\u0303 is block-tridiagonal. Thus we require a more sophisticated approach to deal with such an approximation, which we develop in this subsection.\nTo start, we will define F\u0302 to be the matrix which agrees with F\u0303 on the tridiagonal blocks, and which satisfies the property that F\u0302\u22121 is block-tridiagonal. Note that this definition implies certain values for the off-tridiagonal blocks of F\u0302 which will differ from those of F\u0303 insofar as F\u0303\u22121 isn\u2019t well approximated as block-tridiagonal.\nTo establish that this definition of F\u0302 makes sense and is useful, we will show that under the assumption that F\u0302\u22121 is block-tridiagonal, the values of the tridiagonal blocks of F\u0302 uniquely determine F\u0302\u22121. Moreover, this mapping can be efficiently computed in a way that takes advantage of the fact that the blocks are factored as Kronecker products of smaller matrices.\nOur strategy for doing this will be as follows. First, we will show that assuming that F\u0302\u22121 is block-tridiagonal is equivalent to assuming that it is the precision matrix of an directed Gaussian graphical model (DGGM) where the ` nodes correspond to each of the DWi\u2019s (or more precisely, their vectorizations vec(DWi)), and where nodes corresponding to adjacent layers are connected, thus giving the model a path-like structure. We will then show that the parameters of this model are determined by the tridiagonal blocks of F\u0302 . Finally, we will use a generic formula for finding the inverse covariance matrix of DGGMs to obtain a formula for F\u0302\u22121. Critically, it will turn out that the particular structure of this formula allows us to take advantage of the Kronecker-product structure of the blocks to simplify the computations.\nTo establish the aforementioned equivalence, we note that a block-tridiagonal inverse covariance gives rise to an undirected Gaussian graphical model (UGGM) over D\u03b8 with the desired path structure, whose density function is proportional to exp(\u2212D\u03b8>F\u0302\u22121D\u03b8). As paths are examples of trees, we can then use the fact that tree-structured undirected graphical models are equivalent to directed graphical models with the same graph structure (e.g. Bishop, 2006), with directionality added to the edges so that the resulting graph is a DAG. Moreover, this equivalent directed model will also be linear/Gaussian, and hence a DGGM.\nNext we will show how the parameters of the DGGM corresponding to F\u0302 can be computed from the tridiagonal blocks of F\u0302 . We will assume here that the direction of the edges are from\nthe higher layers to lower ones. Note that a different choice for these directions would produce a slightly different algorithm that would nonetheless yield the same output.\nFor each i, we will denote the conditional covariance matrix of vec(DWi) on vec(DWi+1) by \u03a3i|i+1 and the linear coefficients from vec(DWi+1) to vec(DWi) by the matrix \u03a8i,i+1, so that the conditional distributions defining the model are\nvec(DWi) \u223c N ( \u03a8i,i+1vec(DWi+1), \u03a3i|i+1 ) and vec(DW`) \u223c N ( ~0, \u03a3` ) See Figure 3 for a depiction of the UGGM corresponding to F\u0302\u22121 and its equivalent DGGM.\nSince \u03a3` is just the covariance of vec(DW`), it is given simply by F\u0302`,` = F\u0303`,`. And for i \u2264 `\u2212 1, we can see that \u03a8i,i+1 is given by\n\u03a8i,i+1 = F\u0302i,i+1F\u0302 \u22121 i+1,i+1 = F\u0303i,i+1F\u0303 \u22121 i+1,i+1 = ( A\u0304i\u22121,i \u2297Gi,i+1 ) ( A\u0304i,i \u2297Gi+1,i+1 )\u22121 = \u03a8A\u0304i\u22121,i \u2297\u03a8Gi,i+1\nwhere\n\u03a8A\u0304i\u22121,i = A\u0304i\u22121,iA\u0304 \u22121 i,i and \u03a8 G i,i+1 = Gi,i+1G \u22121 i+1,i+1\nThe conditional covariance \u03a3i|i+1 is thus given by\n\u03a3i|i+1 = F\u0302i,i \u2212\u03a8i,i+1F\u0302i+1,i+1\u03a8>i,i+1 = F\u0303i,i \u2212\u03a8i,i+1F\u0303i+1,i+1\u03a8>i,i+1 = A\u0304i\u22121,i\u22121 \u2297Gi,i \u2212\u03a8A\u0304i\u22121,iA\u0304i,i\u03a8A\u0304>i\u22121,i \u2297\u03a8Gi,i+1Gi+1,i+1\u03a8G>i,i+1\nFollowing the work of Grosse and Salakhutdinov (2014), we use the block generalization of well-known \u201cCholesky\u201d decomposition of the precision matrix of DGGMs (Pourahmadi, 1999),\nwhich gives\nF\u0302\u22121 = \u039e>\u039b\u039e\nwhere,\n\u039b = diag (\n\u03a3\u221211|2,\u03a3 \u22121 2|3, . . . , \u03a3 \u22121 `\u22121|`,\u03a3 \u22121 `\n) and \u039e =  I \u2212\u03a81,2 I \u2212\u03a82,3 I . . .\n. . . \u2212\u03a8`\u22121,` I  Thus, matrix-vector multiplication with F\u0302\u22121 amounts to performing matrix-vector multipli-\ncation by \u039e, followed by \u039b, and then by \u039e>.\nAs in the block-diagonal case considered in the previous subsection, matrix-vector products with \u039e (and \u039e>) can be efficiently computed by using the well-known property (A \u2297 B)\u22121 = A\u22121 \u2297B\u22121. In particular, u = \u039e>v can be computed as\nUi = Vi \u2212\u03a8G>i\u22121,iVi\u22121\u03a8A\u0304i\u22122,i\u22121 and U1 = V1\nand similarly u = \u039ev can be computed as\nUi = Vi \u2212\u03a8Gi,i+1Vi+1\u03a8A\u0304>i\u22121,i and U` = V`\nwhere the Ui\u2019s and Vi\u2019s are defined in terms of u and v as in the previous subsection.\nMultiplying a vector v by \u039b amounts to multiplying each vec(Vi) by the corresponding \u03a3\u22121i|i+1. This is slightly tricky because \u03a3i|i+1 is the difference of Kronecker products, so we cannot use the straightforward identity (A \u2297 B)\u22121 = A\u22121 \u2297 B\u22121. Fortunately, there are efficient techniques for inverting such matrices which we discuss in detail in Section B."}, {"heading": "5 Estimating the required A\u0304i,j\u2019s and Gi,j\u2019s", "text": "Recall that A\u0304i,j = E [ a\u0304ia\u0304 > j ] and Gi,j = E [ gig > j ] . Both approximate Fisher inverses discussed in Section 4 require some subset of these. In particular, the block-diagonal approximation requires them for i = j, while the block-tridiagonal approximation requires them for j \u2208 {i, i+ 1} (noting that A\u0304>i,j = A\u0304j,i and G > i,j = Gj,i).\nSince the a\u0304i\u2019s don\u2019t depend on y, we can take the expectation E [ a\u0304ia\u0304 > j ] with respect to just\nthe training distribution Q\u0302x over the inputs x. On the other hand, the gi\u2019s do depend on y, and so the expectation2 E [ gig > j ] must be taken with respect to both Q\u0302x and the network\u2019s predictive\n2It is important to note this expectation should not be taken with respect to the training/data distribution over y (i.e. Q\u0302y|x or Qy|x). Using the training/data distribution for y would perhaps give an approximation to a quantity known as the \u201cempirical Fisher information matrix\u201d, which lacks the previously discussed equivalence to the Generalized Gauss-\ndistribution Py|x.\nWhile computing matrix-vector products with the Gi,j could be done exactly and efficiently for a given input x (or small mini-batch of x\u2019s) by adapting the methods of Schraudolph (2002), there doesn\u2019t seem to be a sufficiently efficient method for computing the entire matrix itself. Indeed, the hardness results of Martens et al. (2012) suggest that this would require, for each example x in the mini-batch, work that is asymptotically equivalent to matrix-matrix multiplication involving matrices the same size as Gi,j . While a small constant number of such multiplications is arguably an acceptable cost (see Section 8), a number which grows with the size of the mini-batch would not be.\nInstead, we will approximate the expectation over y by a standard Monte-Carlo estimate obtained by sampling y\u2019s from the network\u2019s predictive distribution and then rerunning the backwards phase of backpropagation (see Algorithm 1) as if these were the training targets.\nNote that computing/estimating the required A\u0304i,j/Gi,j\u2019s involves computing averages over outer products of various a\u0304i\u2019s from network\u2019s usual forward pass, and gi\u2019s from the modified backwards pass (with targets sampled as above). Thus we can compute/estimate these quantities on the same input data used to compute the gradient\u2207h, at the cost of one or more additional backwards passes, and a few additional outer-product averages. Fortunately, this turns out to be quite inexpensive, as we have found that just one modified backwards pass is sufficient to obtain a good quality estimate in practice, and the required outer-product averages are similar to those already used to compute the gradient in the usual backpropagation algorithm.\nIn the case of online/stochastic optimization we have found that the best strategy is to maintain running estimates of the required A\u0304i,j\u2019s andGi,j\u2019s using a simple exponentially decaying averaging scheme. In particular, we take the new running estimate to be the old one weighted by \u03b1, plus the estimate on new mini-batch weighted by 1\u2212 \u03b1, for \u03b1 = 0.95 (or thereabouts).\nNote that the more naive averaging scheme where the estimates from each iteration are given equal weight would be inappropriate here. This is because the A\u0304i,j\u2019s and Gi,j\u2019s depend on the network\u2019s parameters \u03b8, and these will slowly change over time as optimization proceeds, so that estimates computed many iterations ago will become stale.\nThis kind of exponentially decaying averaging scheme is commonly used in methods involving diagonal or block-diagonal approximations (with much smaller blocks than ours) to the curvature matrix (e.g. Park et al., 2000; Schaul et al., 2013). Such schemes have the desirable property that they allow the curvature estimate to depend on much more data than can be reasonably processed in a single mini-batch.\nNotably, for methods like HF which deal with the exact Fisher indirectly via matrix-vector products, such a scheme would be impossible to implement efficiently, as the exact Fisher matrix\nNewton matrix, and would not be compatible with the theoretical analysis performed in Section 3.1 (in particular, Lemma 4 would break down). Moreover, such a choice would not give rise to what is usually thought of as the natural gradient, and based on the findings of Martens (2010), would likely perform worse in practice as part of an optimization algorithm. See Martens (2014) for a more detailed discussion of the empirical Fisher.\n(or GGN) seemingly cannot be summarized using a compact data structure whose size would be independent of the amount of data used to estimate it. Indeed, it seems that the only representation of the exact Fisher which wouldn\u2019t grow in size with the amount of training cases used to compute it would be an explicit n \u00d7 n matrix (which is far to big to be practical). Because of this, HF and related methods must base their curvature estimates only on subsets of data that can be reasonably processed all at once, which limits their effectiveness in the stochastic optimization regime."}, {"heading": "6 Update damping", "text": "The idealized natural gradient approach is to follow the smooth path3 in the Riemannian manifold (implied by the Fisher information matrix viewed as a metric tensor) that is generated by taking a series of infinitesimally small steps (in the original parameter space) in the direction of the natural gradient (which gets recomputed at each point). While this is clearly impractical as a real optimization method, one can take larger steps and still follow these paths approximately. But in our experience, to obtain an update which satisfies the minimal requirement of not worsening the objective function value, one must make the step size so small that the resulting optimization algorithm is not practical.\nThe reason that the natural gradient can only be reliably followed a short distance is that it is defined merely as an optimal direction (which trades off improvement in the objective versus change in the predictive distribution), and not a discrete update. Fortunately, as observed by Martens (2014), the natural gradient can be understood using a more traditional optimizationtheoretic perspective which implies how it can be used to generate updates that will be useful over larger distances. In particular, when Ry|z is an exponential family model with z as its natural parameters (as it will be in our experiments), Martens (2014) showed that the Fisher becomes equivalent to the Generalized Gauss-Newton matrix (GGN), which is a positive semi-definite approximation of the Hessian of h. From this it follows that\nM(\u03b4) = 1\n2 \u03b4>F\u03b4 +\u2207h(\u03b8)>\u03b4 + h(\u03b8) (5)\ncan be viewed as an approximation of the 2nd-order Taylor series of expansion of h(\u03b4+ \u03b8), whose minimizer is the (negative) natural gradient \u2212F\u22121\u2207h(\u03b8).\nNote that if we add an `2 or \u201cweight-decay\u201d regularization term to h of the form \u03b7\n2 \u2016\u03b8\u201622, then\nsimilarly F + \u03b7I can be viewed as an approximation of the Hessian of h, and replacing F with F +\u03b7I in M(\u03b4) yields an approximation of the 2nd-order Taylor series, whose minimizer is a kind of \u201cregularized\u201d (negative) natural gradient \u2212(F + \u03b7I)\u22121\u2207h(\u03b8) (which is what we end up using in practice).\n3Which has the interpretation of being a geodesic in the Riemannian manifold from the current predictive distribution towards the training distribution when using a likelihood or KL-divergence based objective function (see Martens (2014)).\nFrom the interpretation of the natural gradient as the minimizer of M(\u03b4), we can see that it fails to be useful as a local update only insofar as M(\u03b4) fails to be a good local approximation to h(\u03b4+\u03b8). And so as argued by Martens (2014), it is natural to make use of the various sophisticated \u201cdamping\u201d techniques that have been developed in the optimization literature for dealing with the breakdowns in local quadratic approximations. These include techniques such as Tikhonov damping/regularization, line-searches, and trust regions, etc., all of which tend to be much more effective in practice than merely re-scaling the update. Indeed, a subset of these techniques were exploited in the work of Martens (2010), and have appeared implicitly in older works such as Becker and LeCun (1989). For detailed discussion of these and other damping techniques, we refer the reader to Martens and Sutskever (2012).\nMethods like HF which use the exact Fisher seem to work reasonably well with an adaptive Tikhonov regularization technique where \u03bbI is added to F + \u03b7I , and where \u03bb is adapted according to Levenberg-Marquardt style adjustment rule. However, we found that this simple technique is insufficient when used with our approximate natural gradient update proposals. In particular, we have found that there never seems to be a \u201cgood\u201d choice for \u03bb that gives rise to updates which are of a quality comparable to those produced by methods that use the exact Fisher, such as HF.\nOne possible explanation for this finding is that, unlike quadratic models based on the exact Fisher (or equivalently, the GGN), the one underlying K-FAC has no guarantee of being accurate up to 2nd-order. Thus, \u03bb must remain large in order to compensate for this this intrinsic 2nd-order inaccuracy of the model, which has the side effect of \u201cwashing out\u201d the small eigenvalues (which represent important low-curvature directions).\nFortunately, through trial and error, we were able to find a relatively simple and highly effective damping method which works well for K-FAC. This involves the use of an adaptive Tikhonov damping/regularization technique, combined with a re-scaling technique which makes very limited and economical use of the exact Fisher F (as estimated on the current mini-batch). We discuss the details of this method in the following subsections."}, {"heading": "6.1 A factored Tikhonov regularization technique", "text": "In the usual Tikhonov regularization/damping technique, one adds (\u03bb+\u03b7)I to the curvature matrix (where \u03b7 accounts for the `2 regularization), which is equivalent to adding a term of the form \u03bb+ \u03b7\n2 \u2016\u03b4\u201622 toM(\u03b4). For the block-diagonal approximation F\u0306 of F\u0303 (from Section 4.2) this amounts to adding (\u03bb + \u03b7)I (for a lower dimensional I) to each of the individual diagonal blocks, which gives modified diagonal blocks of the form\nA\u0304i\u22121,i\u22121 \u2297Gi,i + (\u03bb+ \u03b7)I = A\u0304i\u22121,i\u22121 \u2297Gi,i + (\u03bb+ \u03b7)I \u2297 I (6)\nBecause this is the sum of two Kronecker products we cannot anymore use the simple identity (A \u2297 B)\u22121 = A\u22121 \u2297 B\u22121. Fortunately however, there are efficient techniques for inverting such matrices, which we discuss in detail in Section B.\nIf we try to apply this same Tikhonov technique to our more sophisticated approximation F\u0302 of F\u0303 (from Section 4.3) by adding (\u03bb+\u03b7)I to each of the diagonal blocks of F\u0302 , it is no longer clear how to efficiently invert F\u0302 . Instead, a solution which we have found works very well in practice is to add \u03c0i( \u221a \u03bb+ \u03b7)I and 1 \u03c0i ( \u221a \u03bb+ \u03b7)I for a scalar constant \u03c0i to the individual Kronecker factors A\u0304i\u22121,i\u22121 and Gi,i (resp.) of each diagonal block, giving( A\u0304i\u22121,i\u22121 + \u03c0i( \u221a \u03bb+ \u03b7)I ) \u2297 ( Gi,i + 1 \u03c0i ( \u221a \u03bb+ \u03b7)I ) (7)\nAs this is a single Kronecker product, all of the computations described in Sections 4.2 and 4.3 can still be used here too, simply by replacing each A\u0304i\u22121,i\u22121 and Gi,i with their modified versions A\u0304i\u22121,i\u22121 + \u03c0i( \u221a \u03bb+ \u03b7)I and Gi,i + 1 \u03c0i ( \u221a \u03bb+ \u03b7)I .\nTo see why the expression in eqn. 7 is a reasonable approximation to eqn. 6, note that expanding it gives\nA\u0304i\u22121,i\u22121 \u2297Gi,i + \u03c0i( \u221a \u03bb+ \u03b7)I \u2297Gi,i + 1 \u03c0i ( \u221a \u03bb+ \u03b7)A\u0304i\u22121,i\u22121 \u2297 I + (\u03bb+ \u03b7)I \u2297 I\nwhich differs from eqn. 6 by the residual error expression\n\u03c0i( \u221a \u03bb+ \u03b7)I \u2297Gi,i + 1 \u03c0i ( \u221a \u03bb+ \u03b7)A\u0304i\u22121,i\u22121 \u2297 I\nWhile a choice of \u03c0i = 1 is simple and works well in practice, a slightly more principled choice can be found by minimizing the obvious upper bound (following from the triangle inequality) on the matrix norm of this residual expression, for some matrix norm \u2016 \u00b7 \u2016\u03c5. This gives\n\u03c0i = \u221a \u2016A\u0304i\u22121,i\u22121 \u2297 I\u2016\u03c5 \u2016I \u2297Gi,i\u2016\u03c5\nEvaluating this expression can be done efficiently for various common choices of the matrix norm \u2016 \u00b7 \u2016\u03c5. For example, for a general B we have \u2016I \u2297B\u2016F = \u2016B \u2297 I\u2016F = \u221a d\u2016B\u2016F where d is the height/dimension of I , and also \u2016I \u2297B\u20162 = \u2016B \u2297 I\u20162 = \u2016B\u20162."}, {"heading": "6.2 Re-scaling according to the exact F", "text": "Given an update proposal \u2206 produced by multiplying the negative gradient \u2212\u2207h by our approximate Fisher inverse (subject to the Tikhonov technique described in the previous subsection), the second stage of our proposed damping technique re-scales \u2206 according to the quadratic model M(\u03b4). More precisely, we will choose \u03b1 so that \u03b4 = \u03b1\u2206 optimizes the quadratic model M(\u03b4), as\ncomputed using an estimate of the exact Fisher F (to which we also add the `2 regularization + Tikhonov term (\u03bb+ \u03b7)I). In particular, we minimize the following function with respect to \u03b1:\nM(\u03b4) = M(\u03b1\u2206) = \u03b12\n2 \u2206>(F + (\u03bb+ \u03b7)I)\u2206 + \u03b1\u2207h>\u2206 + h(\u03b8)\nBecause this is a 1-dimensional minimization, the formula for the optimal \u03b1 can be computed simply as\n\u03b1 = \u2212\u2207h>\u2206\n\u2206>F\u2206 + (\u03bb+ \u03b7)\u2016\u2206\u201622\nTo evaluate this formula we use the current stochastic gradient \u2207h (i.e. the same one used to produce \u2206), and compute matrix-vector products with F using the input data from the same minibatch. While using a mini-batch to compute F gets away from the idea of basing our estimate of the curvature on a long history of data (as we do with the approximate Fisher), it is made slightly less objectionable by the fact that we are only using it estimate a single scalar quantity (\u2206>F\u2206). This is to be contrasted with methods like HF which perform a long and careful optimization of M(\u03b4) using such an estimate of F .\nBecause the matrix-vector products with F are only used to compute scalar quantities in KFAC, we can reduce their computational cost by roughly one half (versus standard matrix-vector products with F ) using a simple trick which is discussed in Section C.\nIt is worth mentioning here that by re-scaling \u2206 according to M(\u03b4) K-FAC can be viewed as a version of HF which uses our approximate Fisher as a preconditioning matrix (instead of the traditional diagonal preconditioner), and runs CG for only 1 step, initializing it from 0. This observation suggests running CG for longer, thus obtaining an algorithm which is even closer to HF (although using a much better preconditioner for CG). Indeed, this approach works reasonably well in our experience, but suffers from some of the same problems that HF has in the stochastic setting, due its much stronger use of the mini-batch-estimated F ."}, {"heading": "6.3 Adapting \u03bb", "text": "It is well known (e.g. Nocedal and Wright, 2006) that optimizing a quadratic function whose curvature matrix is modified by adding \u03bbI to it is equivalent to doing the same without this modification, but subject to the constraints that the solution lie within some spherical \u201ctrust-region\u201d of radius \u03c4 . The relationship of \u03c4 to \u03bb is complicated and depends on the properties of the curvature matrix (which is changing constantly), so it is often easier to simply adjust \u03bb directly.\nThe Levenberg-Marquardt style rule used by HF for doing this, which we will also adopt, is given by\nif \u03c1 > 3/4 then \u03bb\u2190 \u03c9\u03bb\nif \u03c1 < 1/4 then \u03bb\u2190 1 \u03c9 \u03bb\nwhere \u03c1 \u2261 h(\u03b8 + \u03b4)\u2212 h(\u03b8) M(\u03b4) is the \u201creduction ratio\u201d and 0 < \u03c9 < 1 is some decay constant, and all quantities are computed on the current mini-batch. In our experiments we applied this rule every T iterations of K-FAC, with \u03c9 = (19/20)T and T = 5, from a starting value of \u03bb = 150. Note that the optimal value of \u03c9 and the starting value of \u03bb may be application dependent, and setting them inappropriately could significantly slow down K-FAC in practice.\nComputing \u03c1 can be done quite efficiently. Note that for the optimal \u03b4, M(\u03b4) = 1 2 \u2207h>\u03b4, and\nh(\u03b8) is available from the usual forward pass. The only remaining quantity which is needed to evaluate \u03c1 is thus h(\u03b8+ \u03b4), which will require an additional forward pass. But fortunately, we only need to perform this once every T iterations."}, {"heading": "7 Momentum", "text": "Sutskever et al. (2013) found that momentum (Polyak, 1964; Plaut et al., 1986) was very helpful in the context of stochastic gradient descent optimization of deep neural networks. A version of momentum is also present in the original HF method, and it plays an arguably even more important role in more \u201cstochastic\u201d versions of HF (Martens and Sutskever, 2012; Kiros, 2013).\nA natural way of adding a momentum-like effect to K-FAC, and one which we have found works well in practice, is to take the update to be \u03b4 = \u03b1\u2206 + \u03b2\u03b40, where \u03b40 is the final update computed at the previous iteration, and where \u03b1 and \u03b2 are chosen to minimize M(\u03b4). This allows K-FAC to effectively build up a better solution to the local quadratic optimization problem over many iterations.\nThe optimal solution for \u03b1 and \u03b2 can be computed as[ \u03b1 \u03b2 ] = \u2212 [ \u2206>F\u2206 \u2206>F\u03b40 \u03b4>0 F\u2206 \u03b4 > 0 F\u03b40 ]\u22121 [\u2207h>\u2206 \u2207h>\u03b40 ]\nThe main cost in evaluating this formula is computing the two matrix-vector products F\u2206 and F\u03b40. Fortunately, the technique discussed in Section C can be applied here to compute the 4 required scalars at the cost of only two forwards passes (equivalent to the cost of only one matrixvector product with F ).\nEmpirically we have found that this type of momentum provides substantial acceleration in regimes where the gradient signal has a low noise to signal ratio, which is usually the case in the early to mid stages of stochastic optimization, but can also be the case in later stages if the mini-batch size is made sufficiently large. These findings are consistent with predictions made by\nconvex optimization theory, and with older empirical work done on neural network optimization (LeCun et al., 1998).\nNotably, because the implicit \u201cmomentum constant\u201d \u03b2 in our method is being computed on the fly, one doesn\u2019t have to worry about setting schedules for it, or adjusting it via heuristics, as one often does in the context of SGD.\nInterestingly, if h is a quadratic function (so the definition of M(\u03b4) remains fixed at each iteration), then using this type of momentum makes K-FAC equivalent to performing preconditioned linear CG on M(\u03b4), with the preconditioner given by our approximate Fisher. This follows from the fact that linear CG can be interpreted as a momentum method where the learning rate \u03b1 and momentum decay constant \u03b2 are chosen to jointly minimize M(\u03b4) at each iteration."}, {"heading": "8 Computational Costs and Efficiency Improvements", "text": "Let d be the average/typical number of units in each layer and m the mini-batch size. The significant computational tasks required to compute a single update/iteration of K-FAC, and rough estimates of their associated computational costs, are as follows:\n1. standard forwards and backwards pass: 2C1`d2m\n2. computation of the gradient \u2207h on the current mini-batch using quantities computed in backwards pass: C2`d2m\n3. additional backwards pass with random targets (as described in Section 5): C1`d2m\n4. updating the estimates of the required A\u0304i,j\u2019s and Gi,j\u2019s from quantities computed in the forwards pass and the additional randomized backwards pass: 2C2`d2m\n5. matrix inverses (along with SVDs and matrix square roots for the block-tridiagonal inverse, as described in Section B) required to compute the inverse of the approximate Fisher: C3`d3\nor C4`d3\n6. various matrix-matrix products required to compute the matrix-vector product of the approximate inverse with the stochastic gradient: C4`d3 for the block-diagonal inverse, C5`d3 for the block-tridiagonal inverse\n7. matrix-vector products with the exact F on the current mini-batch using the approach from Section C: 2C1`d2m with momentum, C1`d2m without momentum\n8. additional forward pass required to evaluate the reduction ratio \u03c1 to apply the \u03bb adjustment rule described in Section 6.3: C1`d2m every T iterations (T = 5 in our experiments)\nHere the Ci are various constants that account for implementation details, and we are assuming the use of the naive cubic matrix-matrix multiplication and inversion algorithms when producing the cost estimates. Note that it it is hard to assign precise values to the constants, as they very\nmuch depend on how these various tasks are implemented, and the extent to which parallelism is exploited.\nTasks 1 and 2 represent the standard stochastic gradient computation.\nThe costs of tasks 3 and 4 are similar and slightly smaller than those of tasks 1 and 2. One way to significantly reduce them is to use only a subset of the current mini-batch to update the estimates of the required A\u0304i,j\u2019s andGi,j\u2019s. One can similarly reduce the cost of task 7 by computing the (factored) matrix-vector product with F using such a subset, although we recommend caution when doing this, as using inconsistent sets of data for the quadratic and linear terms in M(\u03b4) can hypothetically cause instability problems which are avoided by using consistent data (see Martens and Sutskever (2012), Section 13.1). In our experiments we used a random 1/8-th of the current mini-batch to update the A\u0304i,j\u2019s and Gi,j\u2019s, and a random 1/4-th to compute the matrix-vector products with F . Doing this seemed to have a negligible effect on the quality of the resultant updates, while significantly reducing per-iteration computation time.\nThe cost of task 8 can be made relatively insignificant by making the adjustment period T for \u03bb large enough. We used T = 5 in our experiments.\nThe costs of tasks 5 and 6 are hard to compare directly with the costs associated with computing the gradient, as their relative sizes will depend on factors such as the architecture of the neural network being trained, as well as the particulars of the implementation. However, one quick observation we can make is that both tasks 5 and 6 involve computations that be performed in parallel across the different layers, which is to be contrasted with many of the other tasks which require sequential passes over the layers of the network.\nClearly, if m d, then the cost of tasks 5 and 6 becomes negligible in comparison to the others. However, it is more often the case that m is comparable or perhaps smaller than d. Moreover, while algorithms for inverses and SVDs tend to have the same asymptotic cost as matrix-matrix multiplication, they are at least several times more expensive in practice, in addition to being harder to parallelize on modern GPU architectures (indeed, CPU implementations are often faster in our experience). Thus, in a naive implementation of K-FAC, task 5 can dominate the overall cost.\nFortunately, there are several possible ways to mitigate the cost of task 5. As mentioned above, one way is to perform the computations for each layer in parallel, and simultaneously with the gradient computation and other tasks. In the case of our block-tridiagonal approximation to the inverse, one can avoid computing any SVDs or matrix square roots by using an iterative Steinequation solver (see Section B). And there are also ways of reducing matrix-inversion (and even matrix square-root) to a short sequence of matrix-matrix multiplications using iterative methods (Pan and Schreiber, 1991). Furthermore, because the matrices in question only change slowly over time, one can consider hot-starting these iterative inversion methods from previous solutions.\nWhile these ideas work reasonable well in practice, perhaps the simplest method, and the one we ended up settling on for our experiments, is to simply recompute the approximate inverses only occasionally, say every 20-50 iterations. As it turns out, the curvature properties of the objective stay relatively stable, especially in later stages of optimization, and so in our experience this strat-\negy of only occasionally recomputing the inverse results in only a modest decrease in the quality of the updates.\nIfm is much smaller than d, the costs associated with task 6 can begin to dominate. And unlike task 5, task 6 must be performed at every iteration. While the simplest solution is to increase m (while reaping the benefits of a less noisy gradient), in the case of the block-diagonal inverse it turns out that we can change the cost of task 6 from C4`d3 to C4`d2m by taking advantage of the low-rank structure of the stochastic gradient. The method for doing this is described below.\nLet A\u0304i and Gi be matrices whose columns are the m a\u0304i\u2019s and gi\u2019s (resp.) associated with the current mini-batch. Let \u2207Wih denote the gradient of h with respect to Wi, shaped as a matrix (instead of a vector). The estimate of \u2207Wih over the mini-batch is given by 1mGiA\u0304 > i\u22121, which is of rank-m. From Section 4.2, computing the F\u0306\u22121\u2207h amounts to computing Ui = G\u22121i,i (\u2207Wih)A\u0304\u22121i\u22121,i\u22121. Substituting in our mini-batch estimate of\u2207Wih gives\nUi = G \u22121 i,i\n( 1\nm GiA\u0304>i\u22121\n) A\u0304\u22121i\u22121,i\u22121 = 1\nm\n( G\u22121i,i Gi ) ( A\u0304>i\u22121A\u0304\u22121i\u22121,i\u22121 ) Direct evaluation of the expression on the right-hand side involves only matrix-matrix multiplications between matrices of size m\u00d7 d and d\u00d7m (or between those of size d\u00d7 d and d\u00d7m), and thus we can reduce the cost of task 6 to C5`d2m.\nNote that the use of standard `2 weight-decay is not compatible with this trick. This is because the contribution of the weight-decay term to each \u2207Wih is \u03bdWi, which will typically not be lowrank. Some possible ways around this issue include computing the weight-decay contribution \u03bdF\u0306\u22121\u03b8 separately and refreshing it only occasionally, or using a different regularization method, such as drop-out (Hinton et al., 2012) or weight-magnitude constraints."}, {"heading": "9 Invariance Properties and the Relationship to Whitening and", "text": "Centering\nWhen computed with the exact Fisher, the natural gradient specifies a direction in the space of predictive distributions which is invariant to the specific way that the model is parameterized. This invariance means that the smooth path through distribution space produced by following the natural gradient with infinitesimally small steps will be similarly invariant.\nFor a practical natural gradient based optimization method which takes large discrete steps in the direction of the natural gradient, this invariance of the optimization path will only hold approximately. As shown by Martens (2014), the approximation error will go to zero as the effects of damping diminish and the reparameterizing function \u03b6 tends to a locally linear function. Note that the latter will happen as \u03b6 becomes smoother, or the local region containing the update shrinks to zero.\nBecause K-FAC uses an approximation of the natural gradient, these invariance results are not applicable in our case. Fortunately, as was shown by Martens (2014), one can establish invariance of an update direction with respect to a given reparameterization of the model by verifying certain simple properties of the curvature matrix C used to compute the update. We will use this result to show that, under the assumption that damping is absent (or negligible in its affect), K-FAC is invariant to a broad and natural class of affine transformations of the network.\nThis class of transformations is given by the following modified network definition (c.f. the definition in Section 2.1):\ns\u2217i = Wia\u0304 \u2217 i\u22121 a\u0304\u2217i = \u2126i\u03c6\u0304i(\u03a6is \u2217 i )\nwhere \u03c6\u0304i is the function that computes \u03c6i and then appends a homogeneous coordinate (with value 1), \u2126i and \u03a6i are arbitrary invertible matrices of the appropriate sizes (except that we assume \u2126` = I), a\u0304\u22170 = \u21260a\u03040, and where the network\u2019s output is given by f\n\u2217(x, \u03b8) = a\u2217` . Note that because \u2126i multiplies \u03c6\u0304i(\u03a6is\u2217i ), it can implement arbitrary translations of the unit activities \u03c6i(\u03a6is \u2217 i ) in addition to arbitrary linear transformations.\nHere, and going forward, we will add a \u201c\u2217\u201d superscript to any network-dependent quantity in order to denote the analogous version of it computed by the transformed network. Note that under this identification, the loss derivative formulas for the transformed network are analogous to those of the original network, and so our various Fisher approximations are still well defined.\nThe following theorem describes the main technical result of this section.\nTheorem 1. There exists an invertible linear function \u03b8 = \u03b6(\u03b8\u2217) so that f \u2217(x, \u03b8\u2217) = f(x, \u03b8) = f(x, \u03b6(\u03b8\u2217)), and thus the transformed network can be viewed as a reparameterization of the original network by \u03b8\u2217. Moreover, additively updating \u03b8 with \u2212\u03b1F\u0306\u22121\u2207h or \u2212\u03b1F\u0302\u22121\u2207h in the original network is equivalent to additively updating \u03b8\u2217 by \u2212\u03b1F\u0306 \u2217\u22121\u2207h\u2217 or \u2212\u03b1F\u0302 \u2217\u22121\u2207h\u2217 (resp.) in the transformed network.\nThis immediately implies the following corollary which characterizes the invariance of a basic version of K-FAC to the given class of network transformations.\nCorollary 2. The optimization path taken by K-FAC (using either of our Fisher approximations F\u0306 or F\u0302 ) through the space of predictive distributions is the same for the default network as it is for the transformed network (where the \u2126i\u2019s and \u03a6i\u2019s remain fixed). This assumes the use of an equivalent initialization (\u03b80 = \u03b6(\u03b8\u22170)), and a basic version of K-FAC where damping is absent or negligible in effect, and where the learning rates are chosen in a way that is independent of the network\u2019s parameterization.\nWhile this corollary assumes that the \u2126i\u2019s and \u03a6i\u2019s are fixed, if we relax this assumption so that they are allowed to vary smoothly with \u03b8, then \u03b6 will be a smooth function of \u03b8, and so as discussed in Martens (2014), invariance of the optimization path will hold approximately in a way that depends on the smoothness of \u03b6 (which measures how quickly the \u2126i\u2019s and \u03a6i\u2019s change) and\nthe size of the update. Moreover, invariance will hold exactly in the limit as the learning rate goes to 0.\nNote that the network transformations can be interpreted as replacing the network\u2019s nonlinearity \u03c6\u0304i(si) at each layer i with a \u201ctransformed\u201d version \u2126i\u03c6\u0304i(\u03a6isi). So since the well-known logistic sigmoid and tanh functions are related to each other by such a transformation, an immediate consequence of Corollary 2 is that K-FAC is invariant to the choice of logistic sigmoid vs. tanh activation functions (provided that equivalent initializations are used and that the effect of damping is negligible, etc.).\nAlso note that because the network inputs are also transformed by \u21260, K-FAC is thus invariant to arbitrary affine transformations of the input, which includes many popular training data preprocessing techniques.\nMany other natural network transformations, such as ones which \u201ccenter\u201d and normalize unit activities so that they have mean 0 and variance 1 can be described using diagonal choices for the \u2126i\u2019s and \u03a6i\u2019s which vary smoothly with \u03b8. In addition to being approximately invariant to such transformations (or exactly, in the limit as the step size goes to 0), K-FAC is similarly invariant to a more general class of such transformations, such as those which transform the units so that they have a mean of 0, so they are \u201ccentered\u201d, and a covariance matrix of I , so they are \u201cwhitened\u201d, which is a much stronger condition than the variances of the individual units each being 1.\nIn the case where we use the block-diagonal approximation F\u0306 and compute updates without damping, Theorem 1 affords us an additional elegant interpretation of what K-FAC is doing. In particular, the updates produced by K-FAC end up being equivalent to those produced by standard gradient descent using a network which is transformed so that the unit activities and the unitgradients are both centered and whitened (with respect to the model\u2019s distribution). This is stated formally in the following corollary.\nCorollary 3. Additively updating \u03b8 with \u03b1F\u0306\u22121\u2207h in the original network is equivalent to additively updating \u03b8\u2217 by \u2207h\u2217 (where \u03b8 = \u03b6(\u03b8\u2217) as in Theorem 1) in a network which is transformed so that the unit activities a\u2217i and the unit-gradients g \u2217 i are both centered and whitened with respect to the model\u2019s distribution."}, {"heading": "10 Related Work", "text": "The Hessian-free optimization method of Martens (2010) uses linear conjugate gradient (CG) to optimize local quadratic models of the form of eqn. 5 (subject to an adaptive Tikhonov damping technique) in lieu of directly solving it using matrix inverses. As discussed in the introduction, the main advantages of K-FAC over HF are twofold. Firstly, K-FAC uses an efficiently computable direct solution for the inverse of the curvature matrix and thus avoids the costly matrix-vector products associated with running CG within HF. Secondly, it can estimate the curvature matrix from a lot of data by using an online exponentially-decayed average, as opposed to relatively\nsmall-sized fixed mini-batches used by HF. The cost of doing this is of course the use of an inexact approximation to the curvature matrix.\nLe Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error.\nCentering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6\u2032i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied.\nIt is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient. However, this argument uses the strong approximating assumption that the correlations between various network-dependent quantities, such as the activities of different units within a given layer, are zero. In our notation, this would be like assuming that the Gi,i\u2019s are diagonal, and that the A\u0304i,i\u2019s are rank-1 plus a diagonal term. Indeed, using such an approximation within the block-diagonal version of K-FAC would yield an algorithm similar to standard centering, although without the need for skip connections.\nAs shown in Corollary 3, K-FAC can be interpreted as automatically performing centering, along with the much stronger operation of whitening, of the unit activities and various other quantities which appear in the formula for the gradient. Intuitively, it is this whitening which accounts for the correlations between activities (or back-propagated gradients) within a given layer.\nOllivier (2013) proposed a neural network optimization method which uses a block-diagonal approximation of the Fisher, with the blocks corresponding to the incoming weights (and bias) of each unit. This method is similar to TONGA, except that it approximates the Fisher instead of the empirical Fisher (see Martens (2014) for a discussion of the difference between these). Because computing blocks of the Fisher is expensive (it requires k backpropagations, where k is the number of output units), this method uses a biased deterministic approximation which can be computed more efficiently, and is similar in spirit to the deterministic approximation used by LeCun et al. (1998). Note that while such an approximation could hypothetically be used within K-FAC to compute the Gi,j\u2019s, we have found that our basic unbiased stochastic approximation works nearly\nas well as the exact values in practice.\nThe work most closely related to ours is that of Heskes (2000), who proposed an approximation of the Fisher of feed-forward neural networks similar to our Kronecker-factored blockdiagonal approximation F\u0306 from Section 4.2, and used it to derive an efficient approximate naturalgradient based optimization method by exploiting the identity (A \u2297 B)\u22121 = A\u22121 \u2297 B\u22121. K-FAC differs from Heskes\u2019 method in several important ways which turn out to be crucial to it working well in practice.\nIn Heskes\u2019 method, update damping is accomplished using a basic factored Tikhonov technique where \u03b3I is added to each Gi,i and A\u0304i,i for a fixed parameter \u03b3 > 0 which is set by hand. By contrast, K-FAC uses a factored Tikhonov technique where \u03b3 = \u221a \u03b7 + \u03bb with \u03bb adapted dynamically as described in Section 6.3, combined with a re-scaling technique based on a local quadratic model computed using the exact Fisher (see Section 6.2). Note that the adaptation of \u03b3 (via the adaptation of \u03bb) is important since what constitutes a good or even merely acceptable value of \u03b3 will change significantly over the course of optimization. And the use of our rescaling technique, or something similar to it, is also crucial as we have observed empirically that basic Tikhonov damping is incapable of producing high quality updates by itself, even when \u03b3 is chosen optimally at each iteration.\nAlso, while Heskes\u2019 method computes the Gi,i\u2019s exactly, K-FAC uses a stochastic approximation which scales efficiently to neural networks with much higher-dimensional outputs (see Section 5).\nOther advances we have introduced include the more accurate block-tridiagonal approximation to the inverse Fisher, a parameter-free type of momentum (see Section 7), online estimation of the Gi,i and A\u0304i,i matrices, and various improvements in computational efficiency (see Section 8). We have found that each of these additional elements is important in order for K-FAC to work as well as it does in various settings.\nHeskes discussed an alternative interpretation of the block-diagonal approximation which yields some useful insight to complement our own theoretical analysis. In particular, he observed that the block-diagonal Fisher approximation F\u0306 is the curvature matrix corresponding to the following quadratic function which measures the difference between the new parameter value \u03b8\u2032 and the current value \u03b8:\nD(\u03b8\u2032, \u03b8) = 1\n2 \u2211\u0300 i=1 E [ (si \u2212 s\u2032i)>Gi,i(si \u2212 s\u2032i) ] Here, s\u2032i = W \u2032 i a\u0304i\u22121, and the si\u2019s and a\u0304i\u2019s are determined by \u03b8 and are independent of \u03b8\n\u2032 (which determines the W \u2032i \u2019s).\nD(\u03b8\u2032, \u03b8) can be interpreted as a reweighted sum of squared changes of each of the si\u2019s. The reweighing matrix Gi,i is given by\nGi,i = E [ gig > i ] = E[F\nP (i) y|si\n]\nwhere P (i)y|si is the network\u2019s predictive distribution as parameterized by si, and FP (i)y|si is its Fisher information matrix, and where the expectation is taken w.r.t. the distribution on si (as induced by the distribution on the network\u2019s input x). Thus, the effect of reweighing by the Gi,i\u2019s is to (approximately) translate changes in si into changes in the predictive distribution over y, although using the expected/average Fisher Gi,i = E[FP (i)\ny|si ] instead of the more specific Fisher F P (i) y|si .\nInterestingly, if one used F P\n(i) y|si\ninstead of Gi,i in the expression for D(\u03b8\u2032, \u03b8), then D(\u03b8\u2032, \u03b8)\nwould correspond to a basic layer-wise block-diagonal approximation of F where the blocks are computed exactly (i.e. without the Kronecker-factorizing approximation introduced in Section 3). Such an approximate Fisher would have the interpretation of being the Hessian w.r.t. \u03b8\u2032 of either of the measures \u2211\u0300\ni=1\nE [ KL ( P\n(i) y|si \u2016 P (i) y|s\u2032i\n)] or \u2211\u0300 i=1 E [ KL ( P (i) y|s\u2032i \u2016 P (i)y|si )] Note that each term in either of these sums is a function measuring an intrinsic quantity (i.e. changes in the output distribution), and so overall these are intrinsic measures except insofar as they assume that \u03b8 is divided into ` independent groups that each parameterize one of the ` different predictive distributions (which are each conditioned on their respective ai\u22121\u2019s).\nIt is not clear whether F\u0306 , with its Kronecker-factorizing structure can similarly be interpreted as the Hessian of such a self-evidently intrinsic measure. If it could be, then this would considerably simplify the proof of our Theorem 1 (e.g. using the techniques of Arnold et al. (2011)). Note that D(\u03b8\u2032, \u03b8) itself doesn\u2019t work, as it isn\u2019t obviously intrinsic. Despite this, as shown in Section 9, both F\u0306 and our more advanced approximation F\u0302 produce updates which have strong invariance properties."}, {"heading": "11 Experiments", "text": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets). Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013).\nAs a baseline we used the version of SGD with momentum described in Sutskever et al. (2013), which was calibrated to work well on these particular deep autoencoder problems. For each problem we followed the prescription given by Sutskever et al. (2013) for determining the learning rate, and the schedule for the decay constant \u00b5.\nOur implementation of K-FAC used most of the efficiency improvements described in Section 8, except that all \u201ctasks\u201d were computed serially (and thus with better engineering and more hard-\nware, a faster and more parallel implementation could likely be obtained). Because the mini-batch sizem tended to be comparable to or larger than the typical/average layer size d, we did not use the technique described at the end of Section 8 for accelerating the computation of the approximate inverse, as this only improves efficiency in the case where m < d, and will otherwise decrease efficiency.\nBoth K-FAC and the baseline were implemented using vectorized MATLAB code accelerated with the GPU package Jacket. All tests were performed on a single computer with a fast 6 core Intel CPU and an NVidia GTX 580 GPU with 3GB of memory.\nEach method used the same initial parameter setting, which was generated using the \u201csparse initialization\u201d technique from Martens (2010) (which was also used by Sutskever et al. (2013)). We report the error on the training set as we are chiefly interested in optimization speed.\nTo help mitigate the detrimental effect that the noise in the stochastic gradient has on the convergence of both the baseline and K-FAC we used a exponentially decayed iterate averaging approach based loosely on Polyak averaging (e.g Swersky et al., 2010). In particular, at each iteration we took the \u201caveraged\u201d parameter estimate to be the previous such estimate, multiplied by \u03be, plus the new iterate produced by the optimizer, multiplied by 1 \u2212 \u03be, for \u03be = 0.99. Since the training error associated with the optimizer\u2019s current iterate may sometimes be lower than the training error associated with the averaged estimate (which will often be the case when the minibatch size m is very large), we report minimum of these two quantities.\nIn our first experiment we examined the relationship between between the mini-batch size m and the per-iteration rate of progress made by K-FAC and the baseline on the MNIST problem. The results from this experiment are plotted in Figure 6. They strongly suggest that the per-iteration rate of progress of K-FAC tends to a super-linear function of m (which can be most clearly seen by examining the plots of training error vs training cases processed), which is to be contrasted with the baseline, where increasing m has a much smaller effect on the per-iteration progress. It thus appears that the main limiting factor in K-FAC is the noise in the gradient, at least in later stages of optimization, and that this is not true of the baseline to nearly the same extent.\nThe fact that the per-iteration rate of progress tends to a super-linear function of m, while the per-iteration computational cost of K-FAC is a linear or slightly sub-linear function of m, suggests that in order to obtain the best per-second rate of progress with K-FAC, we should use a rapidly increasing schedule for m. To this end we designed an exponentially increasing schedule for m, given bymi = min(m1 exp((i\u22121)/b), |S|), where i is the current iteration,m1 = 1000, and where b is chosen so that m500 = |S|. Note that for other neural net optimization problems, such as ones involving much larger training datasets than these autoencoder problems, a more slowly increasing schedule, or one that stops increasing before m reaches |S|, may be more appropriate.\nIn our second experiment we evaluated the performance of K-FAC versus the baseline on all 3 deep autoencoder problems, where we used the above described exponentially increasing schedule for m within K-FAC, and a fixed setting of m within the baseline (which was chosen from a small range of candidates to give the best overall per-second rate of progress). Note that in order to process very large mini-batches without overwhelming the memory of the GPU, we partitioned the mini-batches into smaller \u201cchunks\u201d and performed all computation involving the mini-batches, or subsets thereof, one chunk at a time.\nThe results from this second experiment are plotted in Figures 7 and 8. For each problem K-FAC had a per-iteration rate of progress which was orders of magnitude higher than that of the baseline\u2019s (Figure 8), which translated into an overall much higher per-second rate of progress (Figure 7), despite the higher cost of K-FAC\u2019s iterations (due mostly to the much larger mini-batch sizes used).\nAlso apparent from these results is that the block-tridiagonal version of K-FAC has a periteration rate of progress which is typically 25% to 40% larger than the simpler block-diagonal version. This observation provides empirical support for the idea that the block-tridiagonal approximate inverse Fisher F\u0302\u22121 is a more accurate approximation of F\u22121 than the block-diagonal approximation F\u0306\u22121. However, due to the higher cost of the iterations in the block-tridiagonal version, its overall per-second rate of progress seems to be only slightly higher than the block-diagonal version\u2019s.\nWhile matrix-matrix multiplication, matrix inverse, and SVD computation all have the same computational complexity, in practice their costs differ significantly (in increasing order as listed). Computation of the approximate Fisher inverse, which is performed in our experiments once every 20 iterations (and for the first 10 iterations), requires matrix inverses for the block-diagonal version, and SVDs for the block-tridiagonal version. For the FACES problem, where the layers can have as many as 2000 units, this accounts for a significant portion of the difference in their average per-iteration computational cost (as these operations must be performed on 2000 \u00d7 2000 sized matrices).\nWhile our results suggest that the block-diagonal version is probably the better option overall due to its greater simplicity, the situation may be different given a more efficient implementation of K-FAC where the inverses or SVDs are computed approximately and/or in parallel with the other tasks, or perhaps even while the network is being optimized."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge support from Google and NSERC."}, {"heading": "A Derivation of the expression for the approximation from Sec-", "text": "tion 3.1\nIn this section we will show that E [ a\u0304(1)a\u0304(2) g(1)g(2) ] \u2212E [ a\u0304(1)a\u0304(2) ] E [ g(1)g(2) ] = \u03ba(a\u0304(1), a\u0304(2), g(1), g(2)) + \u03ba(a\u0304(1))\u03ba(a\u0304(2), g(1), g(2)) + \u03ba(a\u0304(2))\u03ba(a\u0304(1), g(1), g(2))\nThe only specific property of the distribution over a\u0304(1), a\u0304(2), g(1), and g(2) which we will require to do this is captured by the following lemma.\nLemma 4. Suppose u is a scalar variable which is independent of y when conditioned on the network\u2019s output f(x, \u03b8), and v is some intermediate quantity computed during the evaluation of f(x, \u03b8) (such as the activities of the units in some layer). Then we have\nE [uDv] = 0\nOur proof of this lemma (which is at the end of this section) makes use of the fact that the expectations are taken with respect to the network\u2019s predictive distribution Py|x as opposed to the training distribution Q\u0302y|x.\nIntuitively, this lemma says that the intermediate quantities computed in the forward pass of Algorithm 1 (or various functions of these) are statistically uncorrelated with various derivative quantities computed in the backwards pass, provided that the targets y are sampled according to the network\u2019s predictive distribution Py|x (instead of coming from the training set). Valid choices for u include a\u0304(k), a\u0304(k) \u2212 E [ a\u0304(k) ]\nfor k \u2208 {1, 2}, and products of these. Examples of invalid choices for u include expressions involving g(k), since these will depend on the derivative of the loss, which is not independent of y given f(x, \u03b8). According to a well-known general formula relating moments to cumulants we may write E [ a\u0304(1)a\u0304(2) g(1)g(2) ] as a sum of 15 terms, each of which is a product of various cumulants corresponding to one of the 15 possible ways to partition the elements of {a\u0304(1), a\u0304(2), g(1), g(2)} into nonoverlapping sets. For example, the term corresponding to the partition {{a\u0304(1)}, {a\u0304(2), g(1), g(2)}} is \u03ba(a\u0304(1))\u03ba(a\u0304(2), g(1), g(2)).\nObserving that 1st-order cumulants correspond to means and 2nd-order cumulants correspond to covariances, for k \u2208 {1, 2} Lemma 4 gives\n\u03ba(g(k)) = E [ g(k) ] = E [ Dx(k) ] = 0\nwhere x(1) = [xi]k2 , and x (2) = [xj]k4 (so that g (k) = Dx(k)). And similarly for k,m \u2208 {1, 2} it gives\n\u03ba(a\u0304(k), g(m)) = E [( a\u0304(m) \u2212 E [ a\u0304(m) ]) ( g(k) \u2212 E [ g(k) ])] = E [( a\u0304(m) \u2212 E [ a\u0304(m) ]) g(k) ] = 0\nUsing these identities we can eliminate 10 of the terms. The remaining expression for E [ a\u0304(1)a\u0304(2) g(1)g(2) ] is thus\n\u03ba(a\u0304(1), a\u0304(2), g(1), g(2)) + \u03ba(a\u0304(1))\u03ba(a\u0304(2), g(1), g(2)) + \u03ba(a\u0304(2))\u03ba(a\u0304(1), g(1), g(2))\n+ \u03ba(a\u0304(1), a\u0304(2))\u03ba(g(1), g(2)) + \u03ba(a\u0304(1))\u03ba(a\u0304(2))\u03ba(g(1), g(2))\nNoting that\n\u03ba(a\u0304(1), a\u0304(2))\u03ba(g(1), g(2)) + \u03ba(a\u0304(1))\u03ba(a\u0304(2))\u03ba(g(1), g(2)) = Cov(a\u0304(1), a\u0304(2)) E [ g(1)g(2) ] + E [ a\u0304(1) ] E [ a\u0304(2) ] E [ g(1)g(2) ] = E [ a\u0304(1)a\u0304(2) ] E [ g(1)g(2) ] it thus follows that\nE [ a\u0304(1)a\u0304(2) g(1)g(2) ] \u2212E [ a\u0304(1)a\u0304(2) ] E [ g(1)g(2) ] = \u03ba(a\u0304(1), a\u0304(2), g(1), g(2)) + \u03ba(a\u0304(1))\u03ba(a\u0304(2), g(1), g(2)) + \u03ba(a\u0304(2))\u03ba(a\u0304(1), g(1), g(2))\nas required.\nIt remains to prove Lemma 4.\nProof of Lemma 4. The chain rule gives\nDv = \u2212d log p(y|x, \u03b8) dv = \u2212d log r(y|z) dz \u2223\u2223\u2223\u2223> z=f(x,\u03b8) df(x, \u03b8) dv\nFrom which it follows that E [uDv] = EQ\u0302x [ EPy|x [uDv] ] = EQ\u0302x [ ERy|f(x,\u03b8) [uDv] ] = EQ\u0302x [ ERy|f(x,\u03b8) [ \u2212u d log r(y|z)\ndz\n\u2223\u2223\u2223\u2223> z=f(x,\u03b8) df(x, \u03b8) dv ]]\n= EQ\u0302x \u2212uERy|f(x,\u03b8) [ d log r(y|z) dz \u2223\u2223\u2223\u2223 z=f(x,\u03b8) ]> df(x, \u03b8) dv  = EQ\u0302x [\u2212u~0> df(x, \u03b8)dv ] = 0\nThat the inner expectation above is ~0 follows from the fact that the expected score of a distribution, when taken with respect to that distribution, is ~0."}, {"heading": "B Efficient techniques for inverting A\u2297B \u00b1 C \u2297D", "text": "It is well known that (A \u2297 B)\u22121 = A\u22121 \u2297 B\u22121, and that matrix-vector products with this matrix can thus be computed as (A\u22121 \u2297 B\u22121)v = vec(B\u22121V A\u2212>), where V is the matrix representation of v (so that v = vec(V )).\nSomewhat less well known is that there are also formulas for (A\u2297B \u00b1C \u2297D)\u22121 which can be efficiently computed and likewise give rise to efficient methods for computing matrix-vector products.\nFirst, note that (A\u2297B \u00b1 C \u2297D)\u22121v = u is equivalent to (A\u2297B \u00b1 C \u2297D)v = u, which is equivalent to the linear matrix equationBUA>\u00b1DUC> = V , where u = vec(U) and v = vec(V ). This is known as a generalized Stein equation, and different examples of it have been studied in the control theory literature, where they have numerous applications. For a recent survey of this topic, see Simoncini (2014).\nOne well-known class of methods called Smith-type iterations (Smith, 1968) involve rewriting this matrix equation as a fixed point iteration and then carrying out this iteration to convergence. Interestingly, through the use of a special squaring trick, one can simulate 2i of these iterations with only O(i) matrix-matrix multiplications.\nAnother class of methods for solving Stein equations involves the use of matrix decompositions (e.g. Chu, 1987; Gardiner et al., 1992). Here we will present such a method particularly well suited for our application, as it produces a formula for (A \u2297 B + C \u2297D)\u22121v, which after a fixed overhead cost (involving the computation of SVDs and matrix square roots), can be repeatedly evaluated for different choices of v using only a few matrix-matrix multiplications.\nWe will assume that A, B, C, and D are symmetric positive semi-definite, as they always are in our applications. We have\nA\u2297B \u00b1 C \u2297D = (A1/2 \u2297B1/2)(I \u2297 I \u00b1 A\u22121/2CA\u22121/2 \u2297B\u22121/2DB\u22121/2)(A1/2 \u2297B1/2)\nInverting both sides of the above equation gives\n(A\u2297B \u00b1 C \u2297D)\u22121 = (A\u22121/2 \u2297B\u22121/2)(I \u2297 I \u00b1 A\u22121/2CA\u22121/2 \u2297B\u22121/2DB\u22121/2)\u22121(A\u22121/2 \u2297B\u22121/2)\nUsing the symmetric eigen/SVD-decomposition, we can write A\u22121/2CA\u22121/2 = E1S1E>1 and B\u22121/2DB\u22121/2 = E2S2E > 2 , where for i \u2208 {1, 2} the Si are diagonal matrices and the Ei are unitary matrices.\nThis gives\nI \u2297 I \u00b1 A\u22121/2CA\u22121/2 \u2297B\u22121/2DB\u22121/2 = I \u2297 I \u00b1 E1S1E>1 \u2297 E2S2E>2 = E1E > 1 \u2297 E2E>2 \u00b1 E1S1E>1 \u2297 E2S2E>2\n= (E1 \u2297 E2)(I \u2297 I \u00b1 S1 \u2297 S2)(E>1 \u2297 E>2 )\nso that\n(I \u2297 I \u00b1 A\u22121/2CA\u22121/2 \u2297B\u22121/2DB\u22121/2)\u22121 = (E1 \u2297 E2)(I \u2297 I \u00b1 S1 \u2297 S2)\u22121(E>1 \u2297 E>2 )\nNote that both I\u2297I and S1\u2297S2 are diagonal matrices, and thus the middle matrix (I\u2297I\u00b1S1\u2297S2)\u22121 is just the inverse of a diagonal matrix, and so can be computed efficiently.\nThus we have\n(A\u2297B \u00b1 C \u2297D)\u22121 = (A\u22121/2 \u2297B\u22121/2)(E1 \u2297 E2)(I \u2297 I \u00b1 S1 \u2297 S2)\u22121(E>1 \u2297 E>2 )(A\u22121/2 \u2297B\u22121/2) = (K1 \u2297K2)(I \u2297 I \u00b1 S1 \u2297 S2)\u22121(K>1 \u2297K>2 )\nwhere K1 = A\u22121/2E1 and K2 = B\u22121/2E2.\nAnd so matrix-vector products with (A\u2297B \u00b1 C \u2297D)\u22121 can be computed as\n(A\u2297B \u00b1 C \u2297D)\u22121v = vec ( K2 [ (K>2 V K1) ( 11> \u00b1 s2s>1 )] K>1 )\nwhere E F denotes element-wise division of E by F , si = diag(Si), and 1 is the vector of ones (sized as appropriate).\nNote that in the considerably simpler case where A and B are both scalar multiples of the identity, and \u03be is the product of these multiples, we have\n(\u03beI \u2297 I \u00b1 C \u2297D)\u22121 = (E1 \u2297 E2)(\u03beI \u2297 I \u00b1 S1 \u2297 S2)\u22121(E>1 \u2297 E>2 )\nwhere E1S1E>1 and E2S2E > 2 are the symmetric eigen/SVD-decompositions of C and D, respectively. And so matrix-vector products with (\u03beI \u2297 I \u00b1 C \u2297D)\u22121 can be computed as\n(\u03beI \u2297 I \u00b1 C \u2297D)\u22121v = vec ( E2 [ (E>2 V E1) ( \u03be11> \u00b1 s2s>1 )] E>1 )\nC Computing v>Fv and u>Fv more efficiently\nNote that the Fisher is given by\nF = EQ\u0302x [ J>FRJ ] where J is the Jacobian of f(x, \u03b8) and FR is the Fisher information matrix of the network\u2019s predictive distribution Ry|z, evaluated at z = f(x, \u03b8) (where we treat z as the \u201cparameters\u201d).\nTo compute the matrix-vector product Fv as estimated from a mini-batch we simply compute J>FRJv for each x in the mini-batch, and average the results. This latter operation can be computed in 3 stages (Martens, 2014), which correspond to multiplication of the vector v first by J , then by FR, and then by J>.\nMultiplication by J can be performed by a forward pass which is like a linearized version of the standard forward pass of Algorithm 1. As FR is usually diagonal or diagonal plus rank1, matrix-vector multiplications with it are cheap and easy. Finally, multiplication by J> can be performed by a backwards pass which is essentially the same as that of Algorithm 1. See Schraudolph (2002); Martens (2014) for further details.\nThe naive way of computing v>Fv is to compute Fv as above, and then compute the inner product of Fv with v. Additionally computing u>Fv and u>Fu would require another such matrix-vector product Fu.\nHowever, if we instead just compute the matrix-vector products Jv (which requires only half the work of computing Fv), then computing v>Fv as (Jv)>FR(Jv) is essentially free. And with Ju computed, we can similarly obtain u>Fv as (Ju)>FR(Jv) and u>Fu as (Ju)>FR(Ju).\nThis trick thus reduces the computational cost associated with computing these various scalars by roughly half."}, {"heading": "D Proofs for Section 9", "text": "Proof of Theorem 1. First we will show that the given network transformation can be viewed as reparameterization of the network according to an invertible linear function \u03b6 .\nDefine \u03b8\u2217 = (W \u22171 ,W \u2217 2 , . . . ,W \u2217 ` ), where W \u2217 i = \u03a6 \u22121 i Wi\u2126 \u22121 i\u22121 (so that Wi = \u03a6iW \u2217 i \u2126i\u22121) and let\n\u03b6 be the function which maps \u03b8\u2217 to \u03b8. Clearly \u03b6 is an invertible linear transformation.\nIf the transformed network uses \u03b8\u2217 in place of \u03b8 then we have\na\u0304\u2217i = \u2126ia\u0304i and s \u2217 i = \u03a6 \u22121 i si\nwhich we can prove by a simple induction. First note that a\u0304\u22170 = \u21260a\u03040 by definition. Then, assuming by induction that a\u0304\u2217i\u22121 = \u2126i\u22121a\u0304i\u22121, we have\ns\u2217i = W \u2217 i a\u0304 \u2217 i\u22121 = \u03a6 \u22121 i Wi\u2126 \u22121 i\u22121\u2126i\u22121a\u0304i\u22121 = \u03a6 \u22121 i Wia\u0304i\u22121 = \u03a6 \u22121 i si\nand therefore also\na\u0304\u2217i = \u2126i\u03c6\u0304i(\u03a6is \u2217 i ) = \u2126i\u03c6\u0304i(\u03a6i\u03a6 \u22121 i si) = \u2126i\u03c6\u0304i(si) = \u2126ia\u0304i\nAnd because \u2126` = I , we have a\u0304\u2217` = a\u0304`, or simply that a \u2217 ` = a`, and thus both the original\nnetwork and the transformed one have the same output (i.e. f(x, \u03b8) = f \u2217(x, \u03b8\u2217)). From this it follows that f \u2217(x, \u03b8\u2217) = f(x, \u03b8) = f(x, \u03b6(\u03b8\u2217)), and thus the transformed network can be viewed as a reparameterization of the original network by \u03b8\u2217. Similarly we have that h\u2217(\u03b8\u2217) = h(\u03b8) = h(\u03b6(\u03b8\u2217)).\nThe following lemma is adapted from (Martens, 2014).\nLemma 5. Let \u03b6 be some invertible affine function mapping \u03b8\u2217 to \u03b8, which reparameterizes the objective h(\u03b8) as h(\u03b6(\u03b8\u2217)). Suppose that C and C\u2217 are invertible matrices satisfying\nJ>\u03b6 CJ\u03b6 = C \u2217\nThen, additively updating \u03b8 with\u2212\u03b1C\u22121\u2207h is equivalent to additively updating \u03b8\u2217 with\u2212\u03b1C\u2217\u22121\u2207\u03b8\u2217h(\u03b6(\u03b8\u2217)).\nBecause h\u2217(\u03b8\u2217) = h(\u03b8) = h(\u03b6(\u03b8\u2217)) we have that \u2207h\u2217 = \u2207\u03b8\u2217h(\u03b6(\u03b8\u2217)). So, by the above lemma, to prove the theorem it suffices to show that J>\u03b6 F\u0306 J\u03b6 = F\u0306 \u2217 and J>\u03b6 F\u0303 J\u03b6 = F\u0303 \u2217.\nUsing Wi = \u03a6iW \u2217i \u2126i\u22121 it is straightforward to verify that\nJ\u03b6 = diag(\u2126 > 0 \u2297 \u03a61,\u2126>1 \u2297 \u03a62, . . . , \u2126>`\u22121 \u2297 \u03a6`)\nBecause si = \u03a6is\u2217i and the fact that the networks compute the same outputs (so the loss derivatives are identical), we have by the chain rule that, g\u2217i = Ds\u2217i = \u03a6>i Dsi = \u03a6>i gi, and therefore\nG\u2217i,j = E [ g\u2217i g \u2217> j ] = E [ \u03a6>i gi(\u03a6 > i gi) >] = \u03a6>i E [gig>i ]\u03a6j = \u03a6>i Gi,j\u03a6j Furthermore,\nA\u0304\u2217i,j = E [ a\u0304\u2217i a\u0304 \u2217> j ] = E [ (\u2126ia\u0304i)(\u2126j a\u0304j) >] = \u2126i E [a\u0304ia\u0304>j ]\u2126>j = \u2126iA\u0304i,j\u2126>j Using these results we may express the Kronecker-factored blocks of the approximate Fisher\nF\u0303 \u2217, as computed using the transformed network, as follows:\nF\u0303 \u2217i,j = A\u0304 \u2217 i\u22121,j\u22121 \u2297G\u2217i,j = \u2126i\u22121A\u0304i\u22121,j\u22121\u2126>j\u22121 \u2297 \u03a6>i Gi,j\u03a6j = (\u2126i\u22121 \u2297 \u03a6>i )(A\u0304i\u22121,j\u22121 \u2297Gi,j)(\u2126>j\u22121 \u2297 \u03a6j)\n= (\u2126i\u22121 \u2297 \u03a6>i )F\u0303i,j(\u2126>j\u22121 \u2297 \u03a6j)\nGiven this identity we thus have F\u0306 \u2217 = diag ( F\u0303 \u22171,1, F\u0303 \u2217 2,2, . . . , F\u0303 \u2217 `,` ) = diag ( (\u21260 \u2297 \u03a6>1 )F\u03031,1(\u2126>0 \u2297 \u03a61), (\u21261 \u2297 \u03a6>2 )F\u03032,2(\u2126>1 \u2297 \u03a62), . . . , (\u2126`\u22121 \u2297 \u03a6>` )F\u0303`,`(\u2126>`\u22121 \u2297 \u03a6`)\n) = diag(\u21260 \u2297 \u03a6>1 ,\u21261 \u2297 \u03a6>2 , . . . , \u2126`\u22121 \u2297 \u03a6>` ) diag ( F\u03031,1, F\u03032,2, . . . , , F\u0303`,`\n) \u00b7 diag(\u2126>0 \u2297 \u03a61,\u2126>1 \u2297 \u03a62, . . . , \u2126>`\u22121 \u2297 \u03a6`)\n= J>\u03b6 F\u0306 J\u03b6\nWe now turn our attention to the F\u0302 (see Section 4.3 for the relevant notation).\nFirst note that\n\u03a8\u2217i,i+1 = F\u0303 \u2217 i,i+1F\u0303 \u2217\u22121 i+1,i+1 = (\u2126i\u22121 \u2297 \u03a6>i )F\u0303i,i+1(\u2126>i \u2297 \u03a6i+1) ( (\u2126i \u2297 \u03a6>i+1)F\u0303i+1,i+1(\u2126>i \u2297 \u03a6i+1) )\u22121 = (\u2126i\u22121 \u2297 \u03a6>i )F\u0303i,i+1(\u2126>i \u2297 \u03a6i+1)(\u2126>i \u2297 \u03a6i+1)\u22121F\u0303\u22121i+1,i+1(\u2126i \u2297 \u03a6>i+1)\u22121\n= (\u2126i\u22121 \u2297 \u03a6>i )F\u0303i,i+1F\u0303\u22121i+1,i+1(\u2126i \u2297 \u03a6>i+1)\u22121 = (\u2126i\u22121 \u2297 \u03a6>i )\u03a8i,i+1(\u2126i \u2297 \u03a6>i+1)\u22121\nand so\n\u03a3\u2217i|i+1 = F\u0303 \u2217 i,i \u2212\u03a8\u2217i,i+1F\u0303 \u2217i+1,i+1\u03a8\u2217>i,i+1\n= (\u2126i\u22121 \u2297 \u03a6>i )F\u0303i,i(\u2126>i\u22121 \u2297 \u03a6i) \u2212 (\u2126i\u22121 \u2297 \u03a6>i )\u03a8i,i+1(\u2126i \u2297 \u03a6>i+1)\u22121(\u2126i \u2297 \u03a6>i+1)F\u0303i+1,i+1(\u2126>i \u2297 \u03a6i+1)(\u2126i \u2297 \u03a6>i+1)\u2212>\n\u00b7\u03a8>i,i+1(\u2126i\u22121 \u2297 \u03a6>i )>\n= (\u2126i\u22121 \u2297 \u03a6>i )(F\u0303i,i \u2212\u03a8i,i+1F\u0303i+1,i+1\u03a8>i,i+1)(\u2126>i\u22121 \u2297 \u03a6i) = (\u2126i\u22121 \u2297 \u03a6>i )\u03a3i|i+1(\u2126>i\u22121 \u2297 \u03a6i)\nAlso, \u03a3\u2217` = F\u0303 \u2217 `,` = (\u2126`\u22121 \u2297 \u03a6>` )F\u0303`,`(\u2126>`\u22121 \u2297 \u03a6`) = (\u2126`\u22121 \u2297 \u03a6>` )\u03a3`(\u2126>`\u22121 \u2297 \u03a6`).\nFrom these facts it follows that\n\u039b\u2217\u22121 = diag ( \u03a3\u22171|2,\u03a3 \u2217 2|3, . . . , \u03a3 \u2217 `\u22121|`,\u03a3 \u2217 ` ) = diag ( (\u21260 \u2297 \u03a6>1 )\u03a31|2(\u21260 \u2297 \u03a6>1 ), (\u21261 \u2297 \u03a6>2 )\u03a32|3(\u21261 \u2297 \u03a6>2 ), . . . ,\n(\u2126`\u22122 \u2297 \u03a6>`\u22121)\u03a3`\u22121|`(\u2126`\u22122 \u2297 \u03a6>`\u22121), (\u2126`\u22121 \u2297 \u03a6>` )\u03a3`(\u2126>`\u22121 \u2297 \u03a6`) )\n= diag(\u21260 \u2297 \u03a6>1 ,\u21261 \u2297 \u03a6>2 , . . . , \u2126`\u22122 \u2297 \u03a6>`\u22121,\u2126`\u22121 \u2297 \u03a6>` ) diag ( \u03a31|2,\u03a32|3, . . . , \u03a3`\u22121|`,\u03a3` ) diag(\u2126>0 \u2297 \u03a61,\u2126>1 \u2297 \u03a62, . . . , \u2126>`\u22122 \u2297 \u03a6`\u22121,\u2126>`\u22121 \u2297 \u03a6`) = J>\u03b6 \u039b \u22121J\u03b6\nInverting both sides gives \u039b\u2217 = J\u22121\u03b6 \u039bJ \u2212> \u03b6 .\nNext, observe that\n\u03a8\u2217>i,i+1(\u2126 > i\u22121 \u2297 \u03a6i)\u22121 = (\u2126i \u2297 \u03a6>i+1)\u2212>\u03a8>i,i+1(\u2126i\u22121 \u2297 \u03a6>i )>(\u2126>i\u22121 \u2297 \u03a6i)\u22121 = (\u2126>i \u2297 \u03a6i+1)\u22121\u03a8>i,i+1\nfrom which it follows that\n\u039e\u2217>J\u22121\u03b6 =  I \u2212\u03a8\u2217>1,2 I\n\u2212\u03a8\u2217>2,3 I . . . . . . \u2212\u03a8\u2217>`\u22121,` I\n diag((\u2126>0 \u2297 \u03a61)\u22121, (\u2126>1 \u2297 \u03a62)\u22121, . . . , (\u2126>`\u22121 \u2297 \u03a6`)\u22121)\n=  (\u2126>0 \u2297 \u03a61)\u22121 \u2212\u03a8\u2217>1,2(\u2126>0 \u2297 \u03a61)\u22121 (\u2126>1 \u2297 \u03a62)\u22121 \u2212\u03a8\u2217>2,3(\u2126>1 \u2297 \u03a62)\u22121 (\u2126>2 \u2297 \u03a63)\u22121\n. . . . . . \u2212\u03a8\u2217>`\u22121,`(\u2126>`\u22122 \u2297 \u03a6`\u22121)\u22121 (\u2126>`\u22121 \u2297 \u03a6`)\u22121\n\n=  (\u2126>0 \u2297 \u03a61)\u22121 \u2212(\u2126>0 \u2297 \u03a61)\u22121\u03a8>1,2 (\u2126>1 \u2297 \u03a62)\u22121 \u2212(\u2126>1 \u2297 \u03a62)\u22121\u03a8>2,3 (\u2126>2 \u2297 \u03a63)\u22121\n. . . . . . \u2212(\u2126>`\u22122 \u2297 \u03a6`\u22121)\u22121\u03a8>`\u22121,` (\u2126>`\u22121 \u2297 \u03a6`)\u22121\n\n= diag((\u2126>0 \u2297 \u03a61)\u22121, (\u2126>1 \u2297 \u03a62)\u22121, . . . , (\u2126>`\u22121 \u2297 \u03a6`)\u22121)  I \u2212\u03a8>1,2 I\n\u2212\u03a8>2,3 I . . . . . . \u2212\u03a8>`\u22121,` I  = J\u22121\u03b6 \u039e >\nCombining \u039b\u2217 = J\u22121\u03b6 \u039bJ \u2212> \u03b6 and \u039e \u2217>J\u22121\u03b6 = J \u22121 \u03b6 \u039e > we have\nF\u0302 \u2217\u22121 = \u039e\u2217>\u039b\u2217\u039e\u2217 = \u039e\u2217>J\u22121\u03b6 \u039bJ \u2212> \u03b6 \u039e \u2217 = (\u039e\u2217>J\u22121\u03b6 )\u039b(\u039e \u2217>J\u22121\u03b6 ) > = (J\u22121\u03b6 \u039e >)\u039b(J\u22121\u03b6 \u039e >)>\n= J\u22121\u03b6 \u039e >\u039b\u039eJ\u2212>\u03b6 = J\u22121\u03b6 F\u0302 \u22121J\u2212>\u03b6\nInverting both sides gives F\u0302 \u2217 = J>\u03b6 F\u0302 J\u03b6 as required.\nProof of Corollary 3. First note that a network which is transformed so that G\u2217i,i = I and A\u0304 \u2217 i,i = I will satisfy the required properties. To see this, note that E[g\u2217i g \u2217> i ] = G \u2217 i,i = I means that g \u2217 i is whitened with respect to the model\u2019s distribution by definition (since the expectation is taken with respect to the model\u2019s distribution), and furthermore we have that E[g\u2217i ] = 0 by default (e.g. using Lemma 4), so g\u2217i is centered. And since E[a \u2217 i a \u2217> i ] is the square submatrix of A\u0304 \u2217 i,i = I which\nleaves out the last row and column, we also have that E[a\u2217i a \u2217> i ] = I and so a \u2217 i is whitened. Finally, observe that E[a\u2217i ] is given by the final column (or row) of A\u0304i,i, excluding the last entry, and is thus equal to 0, and so a\u2217i is centered.\nNext, we note that if G\u2217i,i = I and A\u0304 \u2217 i,i = I then\nF\u0306 \u2217 = diag ( A\u0304\u22170,0 \u2297G\u22171,1, A\u0304\u22171,1 \u2297G\u22172,2, . . . , A\u0304\u2217`\u22121,`\u22121 \u2297G\u2217`,` ) = diag (I \u2297 I, I \u2297 I, . . . , I \u2297 I) = I\nand so \u2212\u03b1F\u0306\u22121\u2207h\u2217 = \u2212\u03b1\u2207h\u2217 is indeed a standard gradient descent update.\nFinally, we observe that there are choices of \u2126i and \u03a6i which will make the transformed model satisfy G\u2217i,i = I and A\u0304 \u2217 i,i = I . In particular, from the proof of Theorem 1 we have that G\u2217i,j = \u03a6 > i Gi,j\u03a6j and A\u0304 \u2217 i,j = \u2126iA\u0304i,j\u2126 > j , and so taking \u03a6i = G \u22121/2 i,i and \u2126i = A\u0304 \u22121/2 i,i works.\nThe result now follows from Theorem 1."}], "references": [{"title": "Methods of Information Geometry, volume 191 of Translations of Mathematical monographs", "author": ["S. Amari", "H. Nagaoka"], "venue": null, "citeRegEx": "Amari and Nagaoka.,? \\Q2000\\E", "shortCiteRegEx": "Amari and Nagaoka.", "year": 2000}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Information-geometric optimization algorithms: A unifying picture via invariance principles", "author": ["Ludovic Arnold", "Anne Auger", "Nikolaus Hansen", "Yann Ollivier"], "venue": null, "citeRegEx": "Arnold et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arnold et al\\.", "year": 2011}, {"title": "Improving the Convergence of Back-Propagation Learning with Second Order Methods", "author": ["Sue Becker", "Yann LeCun"], "venue": "Proceedings of the 1988 Connectionist Models Summer School,", "citeRegEx": "Becker and LeCun.,? \\Q1989\\E", "shortCiteRegEx": "Becker and LeCun.", "year": 1989}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M. Bishop"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Solution of the sylvester matrix equation AXB + CXD = E", "author": ["Judith D. Gardiner", "Alan J. Laub", "James J. Amato", "Cleve B. Moler"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Gardiner et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Gardiner et al\\.", "year": 1992}, {"title": "Scaling up natural gradient by factorizing fisher information", "author": ["Roger Grosse", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Grosse and Salakhutdinov.,? \\Q2014\\E", "shortCiteRegEx": "Grosse and Salakhutdinov.", "year": 2014}, {"title": "On \u201cnatural\u201d learning and pruning in multilayered perceptrons", "author": ["Tom Heskes"], "venue": "Neural Computation,", "citeRegEx": "Heskes.,? \\Q2000\\E", "shortCiteRegEx": "Heskes.", "year": 2000}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Training neural networks with stochastic Hessian-free optimization", "author": ["Ryan Kiros"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kiros.,? \\Q2013\\E", "shortCiteRegEx": "Kiros.", "year": 2013}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas Le Roux", "Pierre-antoine Manzagol", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. M\u00fcller"], "venue": "Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sharpness in rates of convergence for cg and symmetric Lanczos methods", "author": ["R-C Li"], "venue": "Technical Report 05-01,", "citeRegEx": "Li.,? \\Q2005\\E", "shortCiteRegEx": "Li.", "year": 2005}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "New perspectives on the natural gradient method, 2014, arXiv:1411.7717", "author": ["J. Martens"], "venue": null, "citeRegEx": "Martens.,? \\Q2014\\E", "shortCiteRegEx": "Martens.", "year": 2014}, {"title": "Training deep and recurrent networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens and Sutskever.,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2012}, {"title": "Estimating the Hessian by backpropagating curvature", "author": ["J. Martens", "I. Sutskever", "K. Swersky"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2012}, {"title": "Riemannian metrics for neural networks. 2013, arXiv:1303.0818", "author": ["Yann Ollivier"], "venue": null, "citeRegEx": "Ollivier.,? \\Q2013\\E", "shortCiteRegEx": "Ollivier.", "year": 2013}, {"title": "An improved newton iteration for the generalized inverse of a matrix, with applications", "author": ["V. Pan", "R. Schreiber"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Pan and Schreiber.,? \\Q1991\\E", "shortCiteRegEx": "Pan and Schreiber.", "year": 1991}, {"title": "Adaptive natural gradient learning algorithms for various stochastic models", "author": ["H. Park", "S.-I. Amari", "K. Fukumizu"], "venue": "Neural Networks,", "citeRegEx": "Park et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Park et al\\.", "year": 2000}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Pascanu and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu and Bengio.", "year": 2014}, {"title": "Experiments on learning by back propagation", "author": ["D. Plaut", "S. Nowlan", "G.E. Hinton"], "venue": "Technical Report CMU-CS-86-126,", "citeRegEx": "Plaut et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Plaut et al\\.", "year": 1986}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1964\\E", "shortCiteRegEx": "Polyak.", "year": 1964}, {"title": "Joint mean-covariance models with applications to longitudinal data: unconstrained parameterisation", "author": ["M. Pourahmadi"], "venue": null, "citeRegEx": "Pourahmadi.,? \\Q1999\\E", "shortCiteRegEx": "Pourahmadi.", "year": 1999}, {"title": "Covariance Estimation: The GLM and Regularization Perspectives", "author": ["M. Pourahmadi"], "venue": "Statistical Science,", "citeRegEx": "Pourahmadi.,? \\Q2011\\E", "shortCiteRegEx": "Pourahmadi.", "year": 2011}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Tapani Raiko", "Harri Valpola", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "Raiko et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "No More Pesky Learning Rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}, {"title": "Centering neural network gradient factors", "author": ["Nicol N. Schraudolph"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "Schraudolph.,? \\Q1998\\E", "shortCiteRegEx": "Schraudolph.", "year": 1998}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Nicol N. Schraudolph"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph.", "year": 2002}, {"title": "Computational methods for linear matrix equations", "author": ["V. Simoncini"], "venue": null, "citeRegEx": "Simoncini.,? \\Q2014\\E", "shortCiteRegEx": "Simoncini.", "year": 2014}, {"title": "Matrix equation XA+BX = C", "author": ["R.A. Smith"], "venue": "SIAM J. Appl. Math.,", "citeRegEx": "Smith.,? \\Q1968\\E", "shortCiteRegEx": "Smith.", "year": 1968}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "A tutorial on stochastic approximation algorithms for training restricted boltzmann machines and deep belief nets", "author": ["K. Swersky", "Bo Chen", "B. Marlin", "N. de Freitas"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "Swersky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2010}, {"title": "Pushing stochastic gradient towards second-order methods \u2013 backpropagation learning with transformations in nonlinearities", "author": ["Tommi Vatanen", "Tapani Raiko", "Harri Valpola", "Yann LeCun"], "venue": null, "citeRegEx": "Vatanen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vatanen et al\\.", "year": 2013}, {"title": "Krylov subspace descent for deep learning", "author": ["O. Vinyals", "D. Povey"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Vinyals and Povey.,? \\Q2012\\E", "shortCiteRegEx": "Vinyals and Povey.", "year": 2012}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler.", "year": 2013}], "referenceMentions": [{"referenceID": 32, "context": "Despite work on layer-wise pretraining schemes, and various sophisticated optimization methods which try to approximate Newton-Raphson updates or natural gradient updates, stochastic gradient descent (SGD), possibly augmented with momentum, remains the method of choice for large-scale neural network training (Sutskever et al., 2013).", "startOffset": 310, "endOffset": 334}, {"referenceID": 14, "context": "From the work on Hessian-free optimization (HF) (Martens, 2010) and related methods (e.", "startOffset": 48, "endOffset": 63}, {"referenceID": 32, "context": "The extent to which neural network objective functions give rise to such quadratics is unclear, although (Sutskever et al., 2013) provides some preliminary evidence that they do.", "startOffset": 105, "endOffset": 129}, {"referenceID": 27, "context": "One such class of methods which have been widely studied are those which work by directly inverting a diagonal, block-diagonal, or low-rank approximation to the curvature matrix (e.g. Becker and LeCun, 1989; Schaul et al., 2013; Le Roux et al., 2008; Ollivier, 2013).", "startOffset": 178, "endOffset": 266}, {"referenceID": 18, "context": "One such class of methods which have been widely studied are those which work by directly inverting a diagonal, block-diagonal, or low-rank approximation to the curvature matrix (e.g. Becker and LeCun, 1989; Schaul et al., 2013; Le Roux et al., 2008; Ollivier, 2013).", "startOffset": 178, "endOffset": 266}, {"referenceID": 11, "context": "As discussed in Martens and Sutskever (2012), CG has the potential to be much faster at local optimization than gradient descent, when applied to quadratic objective functions.", "startOffset": 16, "endOffset": 45}, {"referenceID": 11, "context": "In the first, the rows and columns of the Fisher are divided into groups, each of which corresponds to all the weights in a given layer, and this gives rise to a block-partitioning of the matrix (where the blocks are much larger than those used by Le Roux et al. (2008) or Ollivier (2013)).", "startOffset": 251, "endOffset": 270}, {"referenceID": 11, "context": "In the first, the rows and columns of the Fisher are divided into groups, each of which corresponds to all the weights in a given layer, and this gives rise to a block-partitioning of the matrix (where the blocks are much larger than those used by Le Roux et al. (2008) or Ollivier (2013)).", "startOffset": 251, "endOffset": 289}, {"referenceID": 14, "context": "Note that this presentation closely follows the one from Martens (2014).", "startOffset": 57, "endOffset": 72}, {"referenceID": 1, "context": "The well-known natural gradient (Amari, 1998) is defined as F\u22121\u2207h(\u03b8).", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence.", "startOffset": 55, "endOffset": 80}, {"referenceID": 15, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 16, "endOffset": 57}, {"referenceID": 21, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 16, "endOffset": 57}, {"referenceID": 29, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 133, "endOffset": 181}, {"referenceID": 16, "context": "It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function.", "startOffset": 133, "endOffset": 181}, {"referenceID": 15, "context": "In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.", "startOffset": 15, "endOffset": 30}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence. This is to be contrasted with the standard gradient, which can be defined as the direction in parameter space which gives the largest change in the objective per unit of change in the parameters, as measured by the standard Euclidean metric. The natural gradient also has links to several classical ideas from optimization. It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function. Indeed, the GGN has served as the curvature matrix of choice in HF and related methods, and so in light of its equivalence to the Fisher, these methods can all be seen as approximate natural gradient methods. In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.1 For some good recent discussion and analysis of the natural gradient, see Arnold et al. (2011); Martens (2014); Pascanu and Bengio (2014).", "startOffset": 56, "endOffset": 1502}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence. This is to be contrasted with the standard gradient, which can be defined as the direction in parameter space which gives the largest change in the objective per unit of change in the parameters, as measured by the standard Euclidean metric. The natural gradient also has links to several classical ideas from optimization. It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function. Indeed, the GGN has served as the curvature matrix of choice in HF and related methods, and so in light of its equivalence to the Fisher, these methods can all be seen as approximate natural gradient methods. In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.1 For some good recent discussion and analysis of the natural gradient, see Arnold et al. (2011); Martens (2014); Pascanu and Bengio (2014).", "startOffset": 56, "endOffset": 1518}, {"referenceID": 0, "context": "Motivated from the perspective of information geometry (Amari and Nagaoka, 2000), the natural gradient defines the direction in parameter space which gives the largest change in the objective per unit of change in the model, as measured by the KL-divergence. This is to be contrasted with the standard gradient, which can be defined as the direction in parameter space which gives the largest change in the objective per unit of change in the parameters, as measured by the standard Euclidean metric. The natural gradient also has links to several classical ideas from optimization. It can be shown (Martens, 2014; Pascanu and Bengio, 2014) that the Fisher is equivalent to the Generalized Gauss-Newton matrix (GGN) (Schraudolph, 2002; Martens and Sutskever, 2012) in certain important cases, which is a well-known PSD approximation to the Hessian of the objective function. Indeed, the GGN has served as the curvature matrix of choice in HF and related methods, and so in light of its equivalence to the Fisher, these methods can all be seen as approximate natural gradient methods. In particular, (Martens, 2014) showed that when the GGN is defined so that the network is linearized up to the loss function, and the loss function corresponds to the negative log probability of observations under an exponential family model Ry|z with natural parameters z, then the Fisher corresponds exactly to the GGN.1 For some good recent discussion and analysis of the natural gradient, see Arnold et al. (2011); Martens (2014); Pascanu and Bengio (2014).", "startOffset": 56, "endOffset": 1545}, {"referenceID": 14, "context": "Equivalently, one could linearize the network only up to the input s` to \u03c6` when computing the GGN (see Martens and Sutskever (2012)).", "startOffset": 104, "endOffset": 133}, {"referenceID": 6, "context": "In fact, the idea of approximating the distribution over loss gradients with a directed graphical model forms the basis of the recent FANG method (Grosse and Salakhutdinov, 2014).", "startOffset": 146, "endOffset": 178}, {"referenceID": 24, "context": "Following the work of Grosse and Salakhutdinov (2014), we use the block generalization of well-known \u201cCholesky\u201d decomposition of the precision matrix of DGGMs (Pourahmadi, 1999),", "startOffset": 159, "endOffset": 177}, {"referenceID": 6, "context": "Following the work of Grosse and Salakhutdinov (2014), we use the block generalization of well-known \u201cCholesky\u201d decomposition of the precision matrix of DGGMs (Pourahmadi, 1999),", "startOffset": 22, "endOffset": 54}, {"referenceID": 27, "context": "This kind of exponentially decaying averaging scheme is commonly used in methods involving diagonal or block-diagonal approximations (with much smaller blocks than ours) to the curvature matrix (e.g. Park et al., 2000; Schaul et al., 2013).", "startOffset": 194, "endOffset": 239}, {"referenceID": 23, "context": "While computing matrix-vector products with the Gi,j could be done exactly and efficiently for a given input x (or small mini-batch of x\u2019s) by adapting the methods of Schraudolph (2002), there doesn\u2019t seem to be a sufficiently efficient method for computing the entire matrix itself.", "startOffset": 167, "endOffset": 186}, {"referenceID": 14, "context": "Indeed, the hardness results of Martens et al. (2012) suggest that this would require, for each example x in the mini-batch, work that is asymptotically equivalent to matrix-matrix multiplication involving matrices the same size as Gi,j .", "startOffset": 32, "endOffset": 54}, {"referenceID": 14, "context": "Moreover, such a choice would not give rise to what is usually thought of as the natural gradient, and based on the findings of Martens (2010), would likely perform worse in practice as part of an optimization algorithm.", "startOffset": 128, "endOffset": 143}, {"referenceID": 14, "context": "Moreover, such a choice would not give rise to what is usually thought of as the natural gradient, and based on the findings of Martens (2010), would likely perform worse in practice as part of an optimization algorithm. See Martens (2014) for a more detailed discussion of the empirical Fisher.", "startOffset": 128, "endOffset": 240}, {"referenceID": 14, "context": "Fortunately, as observed by Martens (2014), the natural gradient can be understood using a more traditional optimizationtheoretic perspective which implies how it can be used to generate updates that will be useful over larger distances.", "startOffset": 28, "endOffset": 43}, {"referenceID": 14, "context": "Fortunately, as observed by Martens (2014), the natural gradient can be understood using a more traditional optimizationtheoretic perspective which implies how it can be used to generate updates that will be useful over larger distances. In particular, when Ry|z is an exponential family model with z as its natural parameters (as it will be in our experiments), Martens (2014) showed that the Fisher becomes equivalent to the Generalized Gauss-Newton matrix (GGN), which is a positive semi-definite approximation of the Hessian of h.", "startOffset": 28, "endOffset": 378}, {"referenceID": 14, "context": "3Which has the interpretation of being a geodesic in the Riemannian manifold from the current predictive distribution towards the training distribution when using a likelihood or KL-divergence based objective function (see Martens (2014)).", "startOffset": 223, "endOffset": 238}, {"referenceID": 13, "context": "And so as argued by Martens (2014), it is natural to make use of the various sophisticated \u201cdamping\u201d techniques that have been developed in the optimization literature for dealing with the breakdowns in local quadratic approximations.", "startOffset": 20, "endOffset": 35}, {"referenceID": 13, "context": "And so as argued by Martens (2014), it is natural to make use of the various sophisticated \u201cdamping\u201d techniques that have been developed in the optimization literature for dealing with the breakdowns in local quadratic approximations. These include techniques such as Tikhonov damping/regularization, line-searches, and trust regions, etc., all of which tend to be much more effective in practice than merely re-scaling the update. Indeed, a subset of these techniques were exploited in the work of Martens (2010), and have appeared implicitly in older works such as Becker and LeCun (1989).", "startOffset": 20, "endOffset": 514}, {"referenceID": 3, "context": "Indeed, a subset of these techniques were exploited in the work of Martens (2010), and have appeared implicitly in older works such as Becker and LeCun (1989). For detailed discussion of these and other damping techniques, we refer the reader to Martens and Sutskever (2012).", "startOffset": 135, "endOffset": 159}, {"referenceID": 3, "context": "Indeed, a subset of these techniques were exploited in the work of Martens (2010), and have appeared implicitly in older works such as Becker and LeCun (1989). For detailed discussion of these and other damping techniques, we refer the reader to Martens and Sutskever (2012). Methods like HF which use the exact Fisher seem to work reasonably well with an adaptive Tikhonov regularization technique where \u03bbI is added to F + \u03b7I , and where \u03bb is adapted according to Levenberg-Marquardt style adjustment rule.", "startOffset": 135, "endOffset": 275}, {"referenceID": 23, "context": "(2013) found that momentum (Polyak, 1964; Plaut et al., 1986) was very helpful in the context of stochastic gradient descent optimization of deep neural networks.", "startOffset": 27, "endOffset": 61}, {"referenceID": 22, "context": "(2013) found that momentum (Polyak, 1964; Plaut et al., 1986) was very helpful in the context of stochastic gradient descent optimization of deep neural networks.", "startOffset": 27, "endOffset": 61}, {"referenceID": 16, "context": "A version of momentum is also present in the original HF method, and it plays an arguably even more important role in more \u201cstochastic\u201d versions of HF (Martens and Sutskever, 2012; Kiros, 2013).", "startOffset": 151, "endOffset": 193}, {"referenceID": 10, "context": "A version of momentum is also present in the original HF method, and it plays an arguably even more important role in more \u201cstochastic\u201d versions of HF (Martens and Sutskever, 2012; Kiros, 2013).", "startOffset": 151, "endOffset": 193}, {"referenceID": 12, "context": "convex optimization theory, and with older empirical work done on neural network optimization (LeCun et al., 1998).", "startOffset": 94, "endOffset": 114}, {"referenceID": 19, "context": "And there are also ways of reducing matrix-inversion (and even matrix square-root) to a short sequence of matrix-matrix multiplications using iterative methods (Pan and Schreiber, 1991).", "startOffset": 160, "endOffset": 185}, {"referenceID": 14, "context": "One can similarly reduce the cost of task 7 by computing the (factored) matrix-vector product with F using such a subset, although we recommend caution when doing this, as using inconsistent sets of data for the quadratic and linear terms in M(\u03b4) can hypothetically cause instability problems which are avoided by using consistent data (see Martens and Sutskever (2012), Section 13.", "startOffset": 341, "endOffset": 370}, {"referenceID": 9, "context": "Some possible ways around this issue include computing the weight-decay contribution \u03bdF\u0306\u22121\u03b8 separately and refreshing it only occasionally, or using a different regularization method, such as drop-out (Hinton et al., 2012) or weight-magnitude constraints.", "startOffset": 201, "endOffset": 222}, {"referenceID": 14, "context": "As shown by Martens (2014), the approximation error will go to zero as the effects of damping diminish and the reparameterizing function \u03b6 tends to a locally linear function.", "startOffset": 12, "endOffset": 27}, {"referenceID": 14, "context": "Fortunately, as was shown by Martens (2014), one can establish invariance of an update direction with respect to a given reparameterization of the model by verifying certain simple properties of the curvature matrix C used to compute the update.", "startOffset": 29, "endOffset": 44}, {"referenceID": 14, "context": "While this corollary assumes that the \u03a9i\u2019s and \u03a6i\u2019s are fixed, if we relax this assumption so that they are allowed to vary smoothly with \u03b8, then \u03b6 will be a smooth function of \u03b8, and so as discussed in Martens (2014), invariance of the optimization path will hold approximately in a way that depends on the smoothness of \u03b6 (which measures how quickly the \u03a9i\u2019s and \u03a6i\u2019s change) and", "startOffset": 203, "endOffset": 218}, {"referenceID": 14, "context": "The Hessian-free optimization method of Martens (2010) uses linear conjugate gradient (CG) to optimize local quadratic models of the form of eqn.", "startOffset": 40, "endOffset": 55}, {"referenceID": 28, "context": "Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 26, "context": "Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.", "startOffset": 127, "endOffset": 169}, {"referenceID": 34, "context": "Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.", "startOffset": 127, "endOffset": 169}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit.", "startOffset": 3, "endOffset": 22}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient.", "startOffset": 3, "endOffset": 1535}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient. However, this argument uses the strong approximating assumption that the correlations between various network-dependent quantities, such as the activities of different units within a given layer, are zero. In our notation, this would be like assuming that the Gi,i\u2019s are diagonal, and that the \u0100i,i\u2019s are rank-1 plus a diagonal term. Indeed, using such an approximation within the block-diagonal version of K-FAC would yield an algorithm similar to standard centering, although without the need for skip connections. As shown in Corollary 3, K-FAC can be interpreted as automatically performing centering, along with the much stronger operation of whitening, of the unit activities and various other quantities which appear in the formula for the gradient. Intuitively, it is this whitening which accounts for the correlations between activities (or back-propagated gradients) within a given layer. Ollivier (2013) proposed a neural network optimization method which uses a block-diagonal approximation of the Fisher, with the blocks corresponding to the incoming weights (and bias) of each unit.", "startOffset": 3, "endOffset": 2618}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient. However, this argument uses the strong approximating assumption that the correlations between various network-dependent quantities, such as the activities of different units within a given layer, are zero. In our notation, this would be like assuming that the Gi,i\u2019s are diagonal, and that the \u0100i,i\u2019s are rank-1 plus a diagonal term. Indeed, using such an approximation within the block-diagonal version of K-FAC would yield an algorithm similar to standard centering, although without the need for skip connections. As shown in Corollary 3, K-FAC can be interpreted as automatically performing centering, along with the much stronger operation of whitening, of the unit activities and various other quantities which appear in the formula for the gradient. Intuitively, it is this whitening which accounts for the correlations between activities (or back-propagated gradients) within a given layer. Ollivier (2013) proposed a neural network optimization method which uses a block-diagonal approximation of the Fisher, with the blocks corresponding to the incoming weights (and bias) of each unit. This method is similar to TONGA, except that it approximates the Fisher instead of the empirical Fisher (see Martens (2014) for a discussion of the difference between these).", "startOffset": 3, "endOffset": 2924}, {"referenceID": 11, "context": "Le Roux et al. (2008) proposed a neural network optimization method known as TONGA based on a block-diagonal approximation of the empirical Fisher where each block corresponds to the weights associated with a particular unit. By contrast, K-FAC uses much larger blocks, each of which corresponds to all the weights within a particular layer. The matrices which are inverted in K-FAC are roughly the same size as those which are inverted in TONGA, but rather than there being one per unit as in TONGA, there are only two per layer. Therefore, K-FAC is significantly less computationally intensive than TONGA, despite using what is arguably a much more accurate approximation to the Fisher. Note that to help mitigate the cost of the many matrix inversions it requires, TONGA approximates the blocks as being low-rank + a diagonal term, although this introduces further approximation error. Centering methods work by either modifying the gradient (Schraudolph, 1998) or dynamically reparameterizing the network itself (Raiko et al., 2012; Vatanen et al., 2013), so that various unit-wise scalar quantities like the activities (the ai\u2019s) and local derivatives (the \u03c6i(si)\u2019s) are 0 on average (i.e. \u201ccentered\u201d), as they appear in the formula for the gradient. Typically, these methods require the introduction of additional \u201cskip\u201d connections (which bypass the nonlinearities of a given layer) in order to preserve the expressive power/efficiency of the network after these transformations are applied. It is argued by Raiko et al. (2012) that the application of centering methods moves the Fisher closer to a diagonal matrix, and thus makes the gradient a reasonable approximation to the natural gradient. However, this argument uses the strong approximating assumption that the correlations between various network-dependent quantities, such as the activities of different units within a given layer, are zero. In our notation, this would be like assuming that the Gi,i\u2019s are diagonal, and that the \u0100i,i\u2019s are rank-1 plus a diagonal term. Indeed, using such an approximation within the block-diagonal version of K-FAC would yield an algorithm similar to standard centering, although without the need for skip connections. As shown in Corollary 3, K-FAC can be interpreted as automatically performing centering, along with the much stronger operation of whitening, of the unit activities and various other quantities which appear in the formula for the gradient. Intuitively, it is this whitening which accounts for the correlations between activities (or back-propagated gradients) within a given layer. Ollivier (2013) proposed a neural network optimization method which uses a block-diagonal approximation of the Fisher, with the blocks corresponding to the incoming weights (and bias) of each unit. This method is similar to TONGA, except that it approximates the Fisher instead of the empirical Fisher (see Martens (2014) for a discussion of the difference between these). Because computing blocks of the Fisher is expensive (it requires k backpropagations, where k is the number of output units), this method uses a biased deterministic approximation which can be computed more efficiently, and is similar in spirit to the deterministic approximation used by LeCun et al. (1998). Note that while such an approximation could hypothetically be used within K-FAC to compute the Gi,j\u2019s, we have found that our basic unbiased stochastic approximation works nearly", "startOffset": 3, "endOffset": 3282}, {"referenceID": 7, "context": "The work most closely related to ours is that of Heskes (2000), who proposed an approximation of the Fisher of feed-forward neural networks similar to our Kronecker-factored blockdiagonal approximation F\u0306 from Section 4.", "startOffset": 49, "endOffset": 63}, {"referenceID": 2, "context": "using the techniques of Arnold et al. (2011)).", "startOffset": 24, "endOffset": 45}, {"referenceID": 35, "context": "Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013).", "startOffset": 132, "endOffset": 201}, {"referenceID": 32, "context": "Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013).", "startOffset": 132, "endOffset": 201}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets).", "startOffset": 117, "endOffset": 149}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets).", "startOffset": 117, "endOffset": 254}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets). Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013). As a baseline we used the version of SGD with momentum described in Sutskever et al. (2013), which was calibrated to work well on these particular deep autoencoder problems.", "startOffset": 117, "endOffset": 620}, {"referenceID": 8, "context": "To investigate the practical performance of K-FAC we applied it to the 3 deep-autoencoder optimization problems from Hinton and Salakhutdinov (2006), which use the \u201cMNIST\u201d, \u201cCURVES\u201d, and \u201cFACES\u201d datasets respectively (see Hinton and Salakhutdinov (2006) for a complete description of the network architectures and datasets). Due to their high difficulty, performance on these problems has become a standard benchmark for neural network optimization methods (e.g. Martens, 2010; Vinyals and Povey, 2012; Sutskever et al., 2013). As a baseline we used the version of SGD with momentum described in Sutskever et al. (2013), which was calibrated to work well on these particular deep autoencoder problems. For each problem we followed the prescription given by Sutskever et al. (2013) for determining the learning rate, and the schedule for the decay constant \u03bc.", "startOffset": 117, "endOffset": 781}, {"referenceID": 14, "context": "Each method used the same initial parameter setting, which was generated using the \u201csparse initialization\u201d technique from Martens (2010) (which was also used by Sutskever et al.", "startOffset": 122, "endOffset": 137}, {"referenceID": 14, "context": "Each method used the same initial parameter setting, which was generated using the \u201csparse initialization\u201d technique from Martens (2010) (which was also used by Sutskever et al. (2013)).", "startOffset": 122, "endOffset": 185}], "year": 2017, "abstractText": "We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network\u2019s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as factoring as Kronecker products between two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods such as Hessian-free methods, K-FAC works very well in highly stochastic optimization regimes.", "creator": "LaTeX with hyperref package"}}}