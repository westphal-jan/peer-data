{"id": "1503.06169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2015", "title": "Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies", "abstract": "In abarema this paper, 3.4-billion we hibernated investigate a chrome-plated largely myositis extended thorsten version conserver of komtar classical manville MAB kutir problem, kolkatta called betteridge networked cincinnatians combinatorial bandit reticulatum problems. In endometriosis particular, we coeruleus consider the pickett setting of a rosebank decision maker over empiricism a networked bandits phips as follows: 326.7 each goostrey time jorginho a combinatorial sina-1 strategy, beres e. g. , 1772-1945 2 0.82993 -0.68783 -1.3246 -0.50882 0.39304 -0.53409 0.67345 0.32066 -0.073885 -0.76871 0.75713 -0.087604 -1.58 0.13979 0.24736 1.1105 0.054053 0.052679 0.78005 1.7752 -0.17977 -0.37445 1.1616 -1.3771 -0.23966 -0.19785 -1.5824 1.0829 0.85604 0.51759 0.17718 0.33685 -0.3677 0.38581 0.00071188 -0.61799 1.0323 -1.4076 -0.2779 0.062811 -0.34447 1.4799 -0.68798 -0.6023 1.8026 -1.2605 -2.227 -1.8394 1.107 a yesler group of arms, cavils is chosen, and gyang the 0735 decision camp-dependent maker j-7 receives a reward resulting non-mainstream from rohland her discounting strategy fossum and also combustion receives braca a side thiol bonus redway resulting from non-human that self-defense strategy for each arm ' macpaint s 32.75 neighbor. ofce This supiadin is cardholders motivated yixiang by many real heavy-metal applications such miyano as presage on - thermodynamic line social networks 221.4 where wcf friends hesse-nassau can provide rossiyskaya their lashings feedback tomtom on changquan shared content, therefore if gelles we promote annelise a product to chauzy a sayer user, we klesken can stodart also collect 31.71 feedback sidelocks from her pruden friends on boardercross that zoospores product. To ridala this end, synapsids we 63.88 consider georg two demain types of tusk side bonus in kirkintilloch this study: kasperczak side observation and side neigboring reward. fagg Upon the number conde of 281.3 arms pulled at multicasting each 71.84 time slot, we anderstorp study unsalable two x\u00e0tiva cases: single - camile play and combinatorial - date play. permissive Consequently, this leaves employer us summercamp four histiocytosis scenarios to 47.44 investigate noix in the 18:08 presence olander of zinie side euro534 bonus: numismatic Single - bertoli play sonoita with Side decatur Observation, advocation Combinatorial - okeyo play fire-tube with mits Side patail Observation, Single - shareholders play kuzma with Side Reward, and monomachos Combinatorial - play sale with 139th Side Reward. bassets For suti each oedipus case, we present schneebaum and analyze a series marija of \\ powwow emph {zero retching regret} polices aloys where syriacus the expect mears of searcher regret rachele over time bavarii approaches zna zero as time rictor goes to bayrou infinity. Extensive simulations asterius validate estell the effectiveness of merchiston our results.", "histories": [["v1", "Fri, 20 Mar 2015 17:21:12 GMT  (2194kb)", "http://arxiv.org/abs/1503.06169v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shaojie tang", "yaqin zhou"], "accepted": false, "id": "1503.06169"}, "pdf": {"name": "1503.06169.pdf", "metadata": {"source": "CRF", "title": "Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies", "authors": ["Shaojie Tang", "Yaqin Zhou"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n06 16\n9v 1\n[ cs\n.L G\n] 2\n0 M\nar 2\n01 5\nI. INTRODUCTION\nA multi-armed bandits problem (MAB) problem is a basic sequential decision making problem defined by a set of strategies. At each decision epoch, a decision maker selects a strategy that involves a combination of random bandits or variables, and then obtains an observable reward. The decision maker learns to maximize the total reward obtained in a sequence of decisions through history observation. MAB problems naturally capture the fundamental tradeoff between exploration and exploitation in sequential experiments. That is, the decision maker must exploit strategies that did well in the past on one hand, and explore strategies that might have higher gain on the other hand. MAB problems now play an important role in online computation under unknown environment, such as pricing and bidding in electronic commerce [?], [?], Ad placement on web pages [?], source routing in dynamic networks [?], and opportunistic channel accessing in cognitive radio networks [?], [?]. In this paper, we investigate a largely extended version of classical MAB problem, called networked combinatorial bandit problems. In particular, we consider the setting of a decision maker over a networked bandits as follows: each time a combinatorial strategy, e.g., a group of arms, is chosen, and the decision maker receives a direct reward resulting from her strategy and also receives a\nside bonus (either observation or reward) resulting from that strategy for each arm\u2019s neighbors.\nIn this study, we take as input a relation graph G that represents the correlation among K arms. In the standard setting, pulling an arm i gets reward and observation Xi,t, while in the networked combinatorial bandit problem with side bonus, one also gets side observation or even reward due to the similarity or potential influence among neighboring arms. We consider two types of side bonus in this work: (1) Side-observation: by pulling arm i at time t one gains the direct reward associated with i and also observes the reward of her neighboring arms. Such side-observation [?] is made possible in settings of on-line social networks where friends can provide their feedback on shared content, therefore if we promote a product to a user, we can also collect feedback from her friends on that product; (2) Side-reward: in many practical applications such as recommendation in social networks, pulling an arm i not only yields side observation on neighbors, but also receives extra rewards. That is by pulling arm i one gains the reward associated with i together with her neighboring arms directly. This setting is motivated by the observation that users are usually influenced by her friends when making purchasing decisions. [?].\nDespite of many existing results on MAB problems against unknown stochastic environment [?], [?], [?], [?], [?], their adopted formulations do not fit those applications that involve either side bonus or exponentially large number of candidate strategies. There are several challenges facing our new study. First of all, under combinatorial setting, the number of candidate strategies could be exponentially large, if one simply treats each strategy as an arm, the resulting regret bound is exponential in the number of variables or arms. Traditional MAB assumes that all the arms are independent, which is inappropriate in our setting. In the presence of side bonus, how to appropriately leverage additional information in order to gain higher rewards is another challenge. To this end, we explore a more general formulation for networked combinatorial bandit problems under four scenarios, namely, single/combinatorial play with side observation, single/combinatorial play with side reward. The objective is to minimize the upper bound of regret (or maximize the total reward) over time.\nThe contributions of this paper are listed as follows: \u2022 For Single-play with Side Observation case, we present\nthe first distribution-free learning (DFL) policy, whose\ntime and space complexity are bounded by O(K). Our policy achieves zero regret that does not depend on \u2206min, the minimum distance between the best static strategy and any other strategy. \u2022 For Combinatorial-play with Side Observation case, we present a learning policy with zero regret. Compared with traditional MAB problem without side bonus, we reduce the regret bound significantly. \u2022 For Single-play with Side Rewards case, we develop a distribution-free zero regret learning policy. We theoretically show that this scheme converges faster than any existing method. \u2022 For Combinatorial-play with Side Rewards case, by assuming that the combinatorial problem at each decision point can be solved optimally, we present the first distribution-free zero regret policy.\nWe evaluate our proposed learning policy through extensive simulations and simulation results validate the effectiveness of our schemes.\nThe remainder of this paper is organized as follows. We first give a formal description of networked combinatorial multiarmed bandits problem in Section II. We study Single-play with Side Observation case in Section III. In Section IV, we study Combinatorial-play with Side Observation case. Singleplay with Side Rewards case has been discussed in Section V. In Section VI, we study Combinatorial-play with Side Rewards case. We evaluate our policies via extensive simulations in Section VII. We review related works in Section VIII. We conclude this paper, and discuss limitations as well as future works in Section IX. Most notations used in this paper are summarized in Table I."}, {"heading": "II. MODELS AND PROBLEM FORMULATION", "text": "In the standard MAB problem, a K-armed bandit problem is defined by K distributions P1, . . . ,PK , each arm with respective means \u00b51, . . . , \u00b5K . When the decision maker pulls arm i at time t, she receives a reward Xi,t. We assume all rewards {Xi,t, i \u2208 [1,K], t \u2265 1} are independent, and all {Pi} have support in [0, 1]. Let i = 1 denote the optimal arm, and \u2206i = \u00b51 \u2212\u00b5i be the difference between the best arm and arm i.\nThe relation graph G = (V,E) over the K arms describes the correlations among them, where an undirected link e(i, j) \u2208 E indicates the correlation between two neighboring arms i and j. In the standard setting, pulling an arm i gets reward and observation Xi,t, while in the networked combinatorial bandit problem with side bonus, one also gets side observation or even reward from neighboring arms due to the similarity or potential influence among them. Let N(i) denote the set of neighboring arms of arm i and Ni = {i}\u222aN(i). In this work, we consider two types of side bonus:\n\u2022 Side observation: by pulling arm i at time t one gains the reward Xi,t associated with i and also observes the reward Xj,t of i\u2019s neighboring arm j \u2208 Ni. This is motivated by many real applications, for example, in today\u2019s online social network, friends can provide their\nfeedback on shared content, therefore if we promote a product to one user, we can also collect feedback from her friends on that product; \u2022 Side reward: by pulling an arm i not only yields side observation on neighbors, but also receives rewards from them, i.e., the total rewards would be\n\u2211 j\u2208Ni Xj,t. This\nsetting is motivated by the observation that in many practical applications such as recommendation in social networks, users are usually influenced by her friends when making purchasing decisions.\nUpon the number of arms pulled at each time slot, we will study single-play case and combinatorial-play case.\n\u2022 In the single-play case, the decision maker selects one arm at each time slot, e.g., traditional MAB problem belongs to this category; \u2022 In the combinatorial-play case, the decision maker requires to select a combination of M(M \u2264 K) arms that satisfies given constraints. One such example is online advertising, assume an advertiser can only place up to m advertisements on his website, he repeatedly selects a set of m advertisements, observes the click-through-rate, with the goal of maximizing the average click-throughrate. This problem can be formulated as a combinatorial MAB problem where each arm represents one advertisement, subject to the constraint that one can play at most m arms at each time slot. In the combinatorial case, at each time slot t, an M -dimensional strategy vector sx is selected under some policy from the feasible strategy set F . By feasible we mean that each strategy satisfies the underlying constraints imposed to F . We use x = 1, . . . , |F | to index strategies of feasible set F in the decreasing order of average reward \u03bbx, e.g., s1 has the largest average reward. Note that a strategy may consist of less than M random variables, as long as it satisfies the given constraints. We then set i = 0 for any empty entry i.\nIn either case, the objective is to minimize long-term regret after n time slots, defined by cumulative difference between the received reward and the optimal reward.\nConsequently, this leaves us four scenarios to investigate: Single-play with Side Observation, Combinatorial-play with Side Observation, Single-play with Side Reward, and Combinatorial-play with Side Reward. We then describe the problem formulation for each case. We use It to denote index of selected arm (resp. strategy) by the decision maker at time slot t, and subscript 1 to denote the optimal arm (resp. strategy) in the four cases. We evaluate policies using regret, Rn, which is defined as the difference in the total expected reward (over n rounds) between always playing the optimal strategy and playing arms according to the policy. We say a policy achieves zero regret if the expected average regret over time approaches zero as time goes to infinity, i.e., Rn/n \u2192 0 as n \u2192 \u221e.\n1) Single-play with Side Observation (SSO). In this case, the decision maker pulls an arm i, observes all Xj,t, j \u2208 Ni, and gets a reward Xi,t. The regret by time slot n is written\n4) Combinatorial-play Side Rewards (CSR). Different from combinatorial-play with side observation, the decision maker directly obtains the rewards from all neighboring arms. That is, the totally received reward includes direct reward by strategy x and side reward by its neighbors. Let Yx = \u222ai\u2208sxNi be the set of neighboring arms for strategy x, and \u03c3x = \u2211 i\u2208Yx \u00b5i be the expected reward\nof sx. The combinatorial reward at time slot t is written as CBIt,t = \u2211 i\u2208YIt Xi,t. We define the regret as\nRn =\nn\u2211\nt=1\n\u03c31 \u2212 n\u2211\nt=1\nCBIt,t. (4)"}, {"heading": "III. SINGLE-PLAY WITH SIDE OBSERVATION", "text": "We start with the case of Single-play with Side Observation. In this case, the decision maker learns to select an arm (resp. strategy) with maximum reward, meanwhile observes side information of its neighbors defined in relation graph. Our proposed policy, which is the first distribution free learning policy for SSO reffered to as DFL-SSO, is shown in Algorithm 1. As shown in Line 2-5, the decision maker updates all neighbors\u2019 side information, i.e., number of observation up to current time, and time-averaged reward. The key idea behind the algorithm is that side-observation potentially reduces the regret as the decision maker can explore more without pain, thus gain more history information to exploit.\nTo theoretically analyze the benefit of side observation, we novelly leverage the technique of graph partition and clique cover. The basic idea in standard analysis of regret bound with side observation in distribution-dependent case is to use clique cover of relation graph, and use the arm with maximum \u2206i inside each cilque to represent the clique for analysis. While standard proof of distribution-free regret bound is to divide the arms into two sets via a threshold \u2206c0 on \u2206i, and then respectively analyze the bounds of the two sets of arms. Therefore, to obtain a distribution-free result, we cannot directly use the arm with maximum \u2206i inside a clique for representation to prove distribution-free regret bound, as the arms with \u2206i smaller than \u2206c0 are distributed inside cliques. To address this issue, we first partition the relation graph G using the predefined threshold, and then mainly analyze the benefit of side observation in one vertex-induced subgraph H for arms having \u2206i above \u2206c0 . In the subgraph H , it is then possible to analyze the distribution-free regret bound using the technique of clique cover.\nTheorem 1 quantifies the benefit brought about by it, where it shows that the more side observation (e.g., smaller clique number) is, the smaller the upper bound of regret is.\nTheorem 1: The expected regret of Algorithm 1 after n time slots is bounded by\nRn \u2264 15.94 \u221a nK + 0.74C \u221a n/K, (6)\nwhere C is clique cover of vertex-induced subgraph H with arms of \u2206i above threshold \u03b40 in relation graph G.\nProof: The proof is based on our novel combination of graph partition and clique cover. We first partition relation\nAlgorithm 1 Distribution-Free Learning policy for single-play with side observation (DFL-SSO)\n1: For each time slot t = 0, 1, . . . , n Select an arm i by maximizing\nXi,t +\n\u221a log (t/(KOi,t))\nOi,t (5)\nto pull 2: for k \u2208 Ni do 3: Ok,t+1 \u2190 Ok,t + 1 4: Xk,t+1 \u2190 Xk,t/Ok,t + (1\u2212 1/Ok,t)Xk,t 5: end for 6: end for\ngraph to rewrite regret in terms of cliques, and then mainly tighten the upper bound by analyzing regret of cliques.\n1. Partition relation graph and rewrite regret of subgraph H in terms of cliques.\nWe order the arms in an increasing order of \u2206i. We use \u2206c0 \u2264 \u03b40 = \u03b1 \u221a K/n \u2264 \u2206c0+1 to split the K arms into two disjoint sets, one set K1 with \u2206x \u2264 \u2206c0 and the other set K2 with \u2206x > \u2206c0 (We will set the value of \u03b1 in later analysis). Let c0 be the smallest index of arm satisfying \u2206k \u2264 \u2206c0 . We remove all arms in K1 from the relation graph G, as well as adjacent edges to nodes in K1. In this way, we get a subgraph H of G, over arms in K2. The regret satisfies,\nR(n) \u2264 n\u2206c0 +RH(n), (7)\nwhere RH(n) is regret generated by selecting suboptimal arms in K2.\nConsider a clique covering C of H , i.e., a set of cliques such that each c \u2208 C is a clique and V = \u222ac\u2208Cc. We define the clique regret Rc(n) for any c \u2208 C by\nRc(n) = \u2211\nt<n\n\u2211 i\u2208c \u2206i1{It = i}. (8)\nSince the set of cliques covers the whole graph H , we have\nRH(n) \u2264 \u2211\nc\u2208C Rc(n). (9)\nWe give an illustration of the partition process in Fig. 1, where the relation graph G contains one small set of blue nodes representing K1 with \u2206i below \u2206c0 , and the other large set of white nodes denoting K2 with \u2206i above \u2206c0 . The\nvertex-induced subgraph H of K2 is covered by a minimum of 3 cliques, respectively marked by black, gray and dash lines.\n2. Regret analysis for regret of subgraph H In the rest part, we focus on proving upper bound of regret\nRH(n). Let \u2206c = maxi\u2208c \u2206i, and Tc(t) = \u2211\ni\u2208c Ti(t) denote the number of times (any arm in) clique c has been played up to time t, where Ti(t) is the number of times arm i has been selected up to time t. Similarly, we suppose that cliques are ordered in the increasing order of \u2206c. Let vj = \u00b51 \u2212 \u2206j2 for cliques in K2, c0 \u2264 j \u2264 K , and vc0 = \u00b51 \u2212 \u2206c0 2 . Let zc0 = +\u221e and \u2206K+1 = +\u221e. For better description, we use c0 to denote the case of c = 0.\nAs every arms in a clique c must be observed for the same number of times, then for each clique and l0 \u2265 0, we have\nRc = \u2211\ni\u2208c \u2206iTi(n) \u2264 l0 max i\u2208c \u2206i +\n\u2211\ni\u2208C\n\u221e\u2211\nl=l0\n1{It = i, t \u2265 l0} (10)\nMeanwhile,\nRH(n) = \u2211\nc\u2208K Rc =\n\u2211 c\u2208C l0\u2206c + K\u2211 i=1 \u2206iT \u2032 i (n), (11)\nWhere T \u2032i (n) denotes the number of arm i played after t = l0, and we refer to the second term as R\u2032H\nDefine\nW = min 1\u2264t\u2264n W1,t, (12)\nand\nUj,i = 1W\u2208[vj+1,vj)\u2206iT \u2032 i (n). (13)\nWe have the following for R\u2032H(n),\nR \u2032 H(n) =\nK\u2211\ni=c0\n\u2206iT \u2032 i (n) (14)\n= K\u2211\nj=c0\nj\u2211\ni=1\nUj,i + K\u2211\nj=c0\nC\u2211\ni=j+1\nUj,i. (15)\nFor the first term of Equation (15), we have:\nK\u2211\nj=c0\nj\u2211\ni=1\nUj,i \u2264 K\u2211\nj=c0\n1W\u2208[vj+1,vj)n\u2206j (16)\n= n\u2206c0 + n\nC\u2211\nc=1\n1W\u2264vc(\u2206c \u2212\u2206c\u22121).(17)\nWe have the first equation as \u2206j \u2265 \u2206i and Ti \u2264 n. To bound the second term of Equation (15), we record\n\u03c4i = {min t : Wi,t < vi} (18)\nafter l0. To pull a suboptimal arm i at t, one must have Wi,t > W1,t \u2265 W . By Algorithm 1, we have {W \u2265 vi} \u2282 {T \u2032i (n) \u2264 \u03c4i}, since once we have pulled \u03c4i times arm i its index will always be lower than the index of arm 1.\nTherefore, we have\nR(n) \u2264 2n\u2206c0 + \u2211\nc\u2208C l0\u2206c +\nK\u2211\ni=1\n\u2206iE(\u03c4i|t > l0)\n+n\nC\u2211\nc=1\n1W<vc(\u2206c \u2212\u2206c\u22121). (19)\nFor any l0 > 0,\n\u2206iE(\u03c4i|\u03c4i > l0) (20)\n\u2264 +\u221e\u2211\nl=l0\nP(\u03c4i \u2265 l)\n= +\u221e\u2211\nl=l0\nP(\u2200t \u2264 l,Wi,t > vi)\n\u2264 +\u221e\u2211\nl=l0\nP ( Xi,l \u2212 \u00b5i \u2265\n\u2206i 2\n\u2212 \u221a log+(n/Kl)\nl\n)\n(21)\nLet l0 = 8 log ( nK\u2206 2 i )/\u2206 2 i . For l \u2265 l0, we have\nlog+(t/(Kl)) \u2264 log+(n/(Kl0)) \u2264 ( n K \u00d7 \u2206\n2 i\n8 ) (22)\n\u2264 l0\u2206 2 i 8 \u2264 l\u2206 2 i 8 . (23)\nTherefor, we have\n\u2206i 2\n\u2212 \u221a log+(n/Kl)\nl \u2265 \u2206i 2 \u2212 \u2206i\u221a 8 = a\u2206i (24)\nwith a = 12 \u2212 1\u221a8 ,\n\u2206cl0 \u2264 8 log ( n\nK \u22062i )/\u2206i \u2264\n2\ne\n\u221a n/K (25)\nTo bound (21) using Hoeffding Bound, i.e.,\nE{\u03c4i|t > l0} \u2264 +\u221e\u2211\nl=l0\nP(X i,l \u2212 \u00b5i \u2265 a\u2206i) (26)\n\u2264 +\u221e\u2211\nl=l0\nexp (\u22122l(a\u2206i)2) (27)\n=\n+\u221e\u2211\nl=l0\n1\u2212 2l0(a\u2206i)2 1\u2212 exp(\u22122(a\u2206i)2)\n(28)\n\u2264 1 1\u2212 exp(\u22122(a\u2206i)2)\n(29)\n\u2264 1 (2a\u2206i)2 \u2212 (\u22122(a\u2206i)2)\n(30)\n= 1\n2a\u22062i (1\u2212 a2) . (31)\nThen we have\n\u2206iE{\u03c4i|t > l0} \u2264 8 log ( n\nK \u22062i )/\u2206i +\n1\n2a\u2206i(1\u2212 a2)\n\u2264 2 e\n\u221a n/K +\n\u03b1\u22121 2a(1\u2212 a2) \u221a n/K. (32)\nNow we prove to bound n \u2211C\nc=0 P(W \u2264 vc)(\u2206c \u2212\u2206c\u22121). Recall that \u2206c0 \u2264 \u03b40 \u2264 \u2206c0+1, and let \u03b4c0 be \u2206c=0. Taking P(W \u2264 \u00b51 \u2212 \u2206c2 ) as an nonincreasing function of \u2206c, we have\nC\u2211\nc=1\nP(W \u2264 vc)(\u2206c \u2212\u2206c\u22121)\n\u2264 \u03b40 \u2212\u2206c0 + \u222b\n\u03b40\n1P(W \u2264 \u00b51 \u2212 u\n2 )du. (33)\nFor a fixed u \u2208 [\u03b40, 1] and f(u) = 8 log( \u221a\nn/Ku)/u2, we have\nP(W \u2264 \u00b51 \u2212 u\n2 )\n= P ( \u22031 \u2264 l \u2264 n : X1,l + \u221a log (n/(Kl))\nl < \u00b51 \u2212\nu\n2\n)\n\u2264 P ( \u22031 \u2264 l \u2264 f(u) : \u00b51 \u2212X1,l > \u221a log (n/(Kl))\nl\n)\n+P ( \u22031 \u2264 l \u2264 f(u) : \u00b51 \u2212X1,l > u\n2\n) (34)\nLet P1 denote the first term of (34), using the form of 1 2m+1 f(u) \u2264 l \u2264 12m f(u), we have\nP1 \u2264 \u221e\u2211\nm=1\nP ( \u2203 1 2m+1 f(u) \u2264 l \u2264 1 2m f(u) :\nl(\u00b51 \u2212Xm,l) > \u221a f(u)\n2m+1 log(\nn2m\nKf(u) )\n)\n\u2264 \u221e\u2211\nm=1\nexp ( \u22122 f(u)2\u2212(m+1) log( n2 m Cf(u))\nf(u)2\u2212m\n)\n= 2 Kf(u)\nn (35)\nLet P2 denote the first term of (34), using the form of 2mf(u) \u2264 l \u2264 2m+1f(u), we have similarly,\nP2 \u2264 \u221e\u2211\nm=1\nP ( \u22032mf(u) \u2264 l \u2264 2m+1f(u) :\nl(\u00b51 \u2212Xm,l) > lu\n2\n)\n\u2264 \u221e\u2211\nm=0\nexp ( \u22122(2 m\u22121f(u)u)2\nf(u)2m+1\n)\n\u2264 1 exp(f(u)u2/4)\u2212 1\n\u2264 1 nu2/K \u2212 1 (36)\nThe last inequality comes from f(u) is upper bounded by 4n/(eK).\nBy taking integrity on P1 and P2, we respectively have\nn\n\u222b 1\n\u03b40\nP1du \u2264 n 2K\nn\n\u222b 1\n\u03b40\nf(u)du (37)\n= n 2K\nn\n[ 8 log(e \u221a n/Ku)\nu\n]\u03b40\n1\n\u2264 8 log(e\u03b1) \u03b1\n\u221a nK, (38)\nand\nn\n\u222b 1\n\u03b40\nP2du \u2264 1\n2 log\n( \u03b1+ 1\n\u03b1\u2212 1\n)\u221a nK. (39)\nInstantly we have\nn\nC\u2211\nc=0\nP(W \u2264 vc)(\u2206c \u2212\u2206c\u22121)\n\u2264 n(\u03b40 \u2212\u2206c0) + ( 8 log(e\u03b1)\n\u03b1 +\n1 2 log\n( \u03b1+ 1\n\u03b1\u2212 1\n))\u221a nK\nFinally, we get the regret bounded by\nRn \u2264 \u2211\nc\u2208C\n2\ne\n\u221a n/K + ( 3\u03b1+ 8 log(e\u03b1)\n\u03b1 +\n1 2 log\n( \u03b1+ 1\n\u03b1\u2212 1\n) +\n\u03b1\u22121\n2a(1\u2212 a2)\n)\u221a nK (40)\nLet \u03b1 = e, and we already have a = 12 \u2212 1\u221a8 , then\nRn \u2264 15.94 \u221a nK + 0.74C \u221a n/K. (41)"}, {"heading": "IV. COMBINATORIAL-PLAY WITH SIDE OBSERVATION", "text": "In this section, we consider combinatorial-play with side observation. In this case, an intuitively extension is to take each strategy as an arm ( we name it com-arm), and then apply the algorithm for SSO to solve the problem. However, the key question is how to utilize the side-observation on arms defined in relation graph to gain more observation on comarms, that is, how to define neighboring com-arms. To this end, we introduce the concept of strategy relation graph to model the correlation among com-arms, by which we convert the problem of CSO to SSO.\nThe construction process for strategy relation graph is as follows. We define strategy relation graph SG(F,L) for strategies in F , where F is vertex set, and L is edge set. Each strategy sx is denoted by a vertex, and a link l = (sx, sy) in L connects two distinct vertexes sx and sy if sy \u2208 Yx and vice versa. The neighbor definition for strategies is natural as once a strategy is played, the union of neighbors of arms in this strategy could be observed according to neighbor definition for arms in G, which surely reward of any strategy composed by these observed arms is also observed. We give an example in Fig. 2. There are 4 arms in relation graph G, indexed by i = 1, 2, 3, 4. The combinatorial MAB problem is to select a maximum weighted independent set of arms where unknown\nbandit is weight. As shown in Fig. 2, the feasible strategy set for this problem consists of 7 feasible strategies, i.e., independent sets of arms in G:\ns1 = {1},\u222ai\u2208s1Ni = {1, 2} s2 = {2},\u222ai\u2208s2Ni = {1, 2, 3} s3 = {3},\u222ai\u2208s3Ni = {2, 3, 4} s4 = {4},\u222ai\u2208s4Ni = {3, 4} s5 = {1, 3},\u222ai\u2208s5Ni = {1, 2, 3, 4} s6 = {1, 4},\u222ai\u2208s6Ni = {1, 2, 3, 4} s7 = {2, 4},\u222ai\u2208s7Ni = {1, 2, 3, 4}\nTaking s2 and s5 for illustration, the component arms of s2, i.e., {2}, is a subset of \u222ai\u2208s5Ni = {1, 2, 3, 4}, and the component arms of s5, i.e., {1, 3} is also a subset of \u222ai\u2208s2Ni = {1, 2, 3}. Therefore, the two strategies are connected in the relation graph SG.\nConsequently, we can convert the combinatorial-play MAB with side observation to a single-MAB with side observation. More specifically, taking each strategy as an arm, SG(F,L) is exactly a relation graph for com-arms in F . The problem turns into a single-play MAB problem where at each time slot the decision maker selects one com-arm from |F | ones to maximize her long-term reward.\nThe algorithm is shown in Algorithm 2, and we derive the regret bound below directly.\nTheorem 2: The expected regret of Algorithm 2 after n time slots is bounded by\nRn \u2264 15.94 \u221a n|F |+ 0.74C \u221a n/|F |. (43)\nIn the traditional distribution-free MAB by taking each comarm as an unknown variable [?], the regret bound would be 49 \u221a n|F |. Our theoretical result significantly reduces the regret and tightens the bound.\nAlgorithm 2 Distribution-Free Learning policy for combinatorial-play with side observation (DFL-CSO)\n1: For each time slot t = 0, 1, . . . , n Select a com-arm sx by maximizing\nRx,t +\n\u221a log (t/(KOx,t))\nOx,t (42)\nto pull 2: UPDATE: for y \u2208 Nx do 3: Oy,t+1 \u2190 Oy,t + 1 4: Ry,t+1 \u2190 Ry,t/Oy,t + (1 \u2212 1/Oy,t)Ry,t 5: end for 6: end for"}, {"heading": "V. SINGLE-PLAY WITH SIDE REWARDS", "text": "Though the single-play MAB with side reward have the same observation as the single-play MAB with side observation, the distinction on reward function makes the problem different. In the case of SSR, the reward function is side reward of the selected arm It, instead of its direct reward. Here we treat the side reward of each arm as a new unknown random variable, i.e., we require to learn Bi,t that is a combination of all direct rewards in Ni. As direct rewards of arms in Ni are observed asynchronously, we cannot update the observation on Bi,t as the way in SSO where observation is symmetric between two neighboring nodes. The trick is updating the number of observation on Bi,t only when direct rewards of all arm in Ni are renewed. We use Obi,t to denote this quantity to differ from Oi,t which denotes the number of direct reward is observed. Therefore, whenever an arm is played or its neighbor is played, the number of observation on side reward Obi,t can be updated only when the least frequently observed arm in Ni is updated. That is,\nObi,t =\n{ Obi,t\u22121 + 1 if minj\u2208Ni Oj,t is updated\nObi,t Otherwise. (44)\nThe algorithm for single-play MAB with side reward is summarized in Algorithm 3 where we directly use side reward Bi,t as observation, and update Obi,t according to (44). The regret bound of our proposed algorithm is presented in Theorem 3.\nTheorem 3: The expected regret of Algorithm 3 after n time slots is bounded by\nRn \u2264 49K \u221a nK (46)\nProof: In this case, Bi,t \u2208 [0,K], which indicates that the range of received reward is scaled by K at most. We normalize Bi,t \u2208 [0, 1]. Using the same techniques in proof of MOSS algorithm [?], we get the normalized regret bound, and then the regret bound in (46) by scaling the normalized regret bound by K . In Algorithm 3, the number of observation times on side reward should be no less than the scenario without side observation. Therefore, Algorithm 3 would convergence to the optimality faster than the MOSS algorithm without side observation.\nAlgorithm 3 Distribution-Free Learning policy for single-play with side reward (DFL-SSR)\n1: For each time slot t = 0, 1, . . . , n Select an arm i by maximizing\nBi,t +\n\u221a log (t/(KObi,t))\nObi,t (45)\nto pull 2: for k \u2208 Ni do 3: Ok,t+1 \u2190 Ok,t + 1 4: if minj\u2208Nk Oj,t is updated 5: Obk,t+1 = O b k,t + 1 6: Bk,t+1 = Bk,t/O b k,t + (1\u2212 1/Obk,t)Bk,t 7: end if 8: end for 9: end for"}, {"heading": "VI. COMBINATORIAL-PLAY WITH SIDE REWARDS", "text": "Now we consider the combinatorial-play case with side reward. Recall that in this scenario, it requires to select a comarm sx with maximum side reward, where the side reward is the sum of observed rewards of all arms neighboring to arms in sx. The case is more complicated than previous three cases, due to: 1) Asymmetric observations on side reward for neighboring nodes in one clique; 2) Probably exponential number of strategies caused arbitrary constraint. Therefore, it is complicated to analyze the regret bound if adopting the same techniques of combinatory-play with side observation. Instead of learning side reward of strategies directly, we learn the direct reward of arms that compose com-arms.\nAlgorithm 4 Distribution-Free Learning policy for combinatorial-play with side reward (DFL-CSR)\n1: For each time slot t = 0, 1, . . . , n Select a com-arm sx by maximizing\n\u2211\ni\u2208Yx\n( X i,t +\n\u221a\u221a\u221a\u221amax (ln t2/3 KOi,t , 0)\nOi,t\n) (47)\nto pull 2: for k \u2208 Yx do 3: Ok,t+1 \u2190 Ok,t + 1 4: Xk,t+1 = Xk,t/O b k,t + (1\u2212 1/Obk,t)Xk,t 5: end for 6: end for\nTheorem 4: The expected regret of Algorithm 4 after n time slots is bounded by\nR(n) \u2264 NK + (\u221a eK + 8(1 +N)N3 ) n 2 3\n+(1 + 4 \u221a KN2\ne )N2Kn 5 6 . (48)\nwhere N \u2264 K is the maximum of |Yx|, x = 1 . . . |F |. Proof: See Appendix."}, {"heading": "VII. SIMULATION", "text": "In this section, we evaluate the performance of the proposed 4 algorithms in simulations. We mainly analyze the regret generated by each algorithm after a long time slot n = 10000.\nWe first evaluate regret generated by DFL-SSO, and compare with MOSS learning policy. The experiment setting is as follows. We randomly generate a relation graph with 100 arms, each following an i.i.d random process over time with mean between [0, 1]. We then plot the accumulated regret and expected regret over time, as shown in Fig. 3(a). Though the expected regret over time by MOSS converges to a value around 0 that coincides with its theoretical bound in Fig. 3(a), it shows that its accumulated regret grows dramatically. It is oblivious the proposed algorithm with side information performs much better than MOSS, e.g., the accumulated regret and expected regret of our proposed algorithm (DFL-SSO) both converge to 0.\nFor other 3 algorithms, as we first study the 3 variants of MAB problem, there are no candidate algorithms to compare. We show the trend of expected regret over time for each case. In evaluation of Algorithm 2, we note that the regret bound contains the terms: number of com-arms and number of cliques. The upper bound becomes huge if the number of com-arms is voluminous, and a small clique number can significantly reduce the bound. In order to investigate the impact experimentally, we then test for regret both under sparse relation graph and dense relation graph. In Fig. 4(a), where the arms are uniformly and randomly connected with a low probability of 0.3, it shows that the expected regret slowly increases beyond 0. While in Fig. 4(b), where the arms are uniformly and randomly connected with a higher probability\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0\n0.2\n0.4\n0.6\n0.8\n1\nTime slot\nE x\np e\nc te\nd r\ne g\nre t\nFig. 5. Expected regret of DFL-SSR\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 \u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nTime slot\nE xp e ct e d r e g re t\nFig. 6. Expected regret of DFL-CSR\nof 0.6, it shows that the expected regret gradually approaches 0. It implicates that the side observation indeed helps to reduce regret if one can observe more, even for the case that previous literature show that it will introduce exponential regret by learning each individual com-arm of a huge feasible strategy set [?]. The simulation results for Algorithm 3 and 4 are shown in Fig. 5 and 6, where the expected regret in both figures converges to 0 dramatically."}, {"heading": "VIII. RELATED WORKS", "text": "The classical multi-armed bandit problem does not assume that existence of side bonus. More recently, [?] and [?] considered the networked bandit problem in the presence of side observations. They study single play case and propose several policies whose regret bound depends on \u2206min, e.g., an arbitrarily small \u2206min will invalidate the zero-regret result. In this work, we present the first distribution free policy for single play with side observation case.\nFor the variant with combinatorial play without side bonus, Anantharam et al. [?] firstly consider the problem that exactly N arms are selected simultaneously without constraint among arms. Gai et al. recently extend this version to a more general problem with arbitrary constraints [?]. The model is also relaxed to a linear combination of no more than N arms. However, the results presented in [?] are distributiondependent. To this end, we are the first to study combinatorial play case in the presence of side bonus. In particular, for the combinatorial play with side observation case, we develop a distribution-free zero regret learning policy. We theoretically show that this scheme converges faster than existing method. And for the combinatorial play with side reward case, we propose the first distribution-free learning policy that has zeroregret."}, {"heading": "IX. CONCLUSION", "text": "In this paper, we investigate networked combinatorial bandit problems under four cases. This is motivated by the existence of potential correlation or influence among neighboring arms. We present and analyze a series of zero regret polices for each case. In the future, we are interested in investigating some heuristics to improve the received regret in practice. For example, at each time slot, instead of playing the selected arm/strategy with maximum index value (Equation (5), (42)), we will play the arm/strategy that has maximum experimental average observation among the neighbors of It. Therefore, we ensure that the received reward is better than the one with maximum index value."}, {"heading": "X. APPENDIX", "text": ""}, {"heading": "A. Proof of Theorem 4", "text": "To prove the theorem, we will use Chernoff-Hoeffding bound and the maximal inequality by Hoeffding [?].\nLemma 1: (Chernoff-Hoeffding Bound [?]) \u03be1, . . . , \u03ben are random variables within range [0, 1], and E[\u03bet|\u03be1, ..., \u03bet\u22121] = \u00b5, \u22001 \u2264 t \u2264 n. Let Sn = \u2211 \u03bei, then for all a > 0\nP(Sn \u2265 n\u00b5+ a) \u2264 exp (\u22122a2/n), P(Sn \u2264 n\u00b5\u2212 a) \u2264 exp (\u22122a2/n). (49)\nLemma 2: (Maximal inequality) [?] \u03be1, . . . , \u03ben are i.i.d random variables with expect \u00b5, then for any y > 0 and n > 0,\nP ( \u2203\u03c4 \u2208 1, . . . , n, \u03c4\u2211\nt=1\n(\u00b5\u2212 \u03bet) > y ) < exp(\u22122y 2\nn ). (50)\nEach com-arm sx and its neighboring arm set Yx actually compose a new com-arm, which could be denoted by Yx as\nsx \u2282 Yx. Each new com-arm Yx corresponds to a unknown bonus CBx,t with mean \u03c3x. Recall that we have assumed \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3|F |. As com-arm Y1 is the optimal com-arm, we have \u2206x = \u03c31 \u2212 \u03c3x, and let Zx = \u03c31 \u2212 \u2206x2 . We further define W1 = min1\u2264t\u2264n W1,t. We may assume the first time slot z = argmin1\u2264t\u2264n W1,t.\n1. Rewrite regret in terms of arms Separating the strategies in two sets by \u2206x0 of some com-\narm sx0(we will define x0 later in the proof), we have\nRn =\nx0\u2211\nx=1\n\u2206xE[Tx,n] +\n|F |\u2211\nx=x0+1\n\u2206xE[Tx,n]\n\u2264 \u2206x0n+ |F |\u2211\nx=x0+1\n\u2206xE[Tx,n]. (51)\nWe then analyze the second term of (51). As there may be exponential number of strategies, counting Tx,n of each comarm by the classic upper-confidence-bound analysis yields regret growing linearly with the number of strategies. Note that each com-arm consists of N arms at most, we can rewrite the regret in terms of arms instead of strategies. We then introduce a set of counters {T\u0303x,n|k = 1, . . . ,K}. At each time slot, either 1) a com-arm with \u2206x \u2264 \u2206x0 or 2) a comarm with \u2206x > \u2206x0 is played. In the first case, no T\u0303x,n will get updated. In the second case, we increase T\u0303x,n by 1 for any arm k = argminj\u2208Yx{Oj,t}. Thus whenever a com-arm with \u2206x > \u2206x0 is chosen, exactly one element in {T\u0303x,n} increases by 1. This implies that the total number that strategies of \u2206x > \u2206x0 have been played is equal to sum of all counters in {T\u0303x,n}, i.e., \u2211|F | x=x0+1 E[Tx,n] = \u2211K\nk=1 T\u0303x,n. Thus, we can rewrite the second term of (51) as\n|F |\u2211\nx=x0+1\n\u2206xE[Tx,n] \u2264 \u2206X |F |\u2211\nx=x0+1\nE[Tx,n]\u2264\u2206X K\u2211\nk=1\nE[T\u0303x,n].\n(52)\nLet Ik,t be the indicator function that equals 1 if T\u0303x,n is updated at time slot t. Define the indicator function 1{y} = 1 if the event y happens and 0 otherwise. When Ik,t = 1, a com-arm Yx with x > x0 has been played for which Ok,t = min{Oj,t : \u2200j \u2208 Yx}. Then\nT\u0303x,n = n\u2211\nt=1\n1{Ik,t = 1} (53)\n\u2264 n\u2211\nt=1\n1{W1,t \u2264 Wx,t} (54)\n\u2264 n\u2211\nt=1\n1{W1 \u2264 Wx,t} (55)\n\u2264 n\u2211\nt=1\n1{W1 \u2264 Wx,t,W1 \u2265 Zx} (56)\n+\nn\u2211\nt=1\n1{W1 \u2264 Wx,t,W1 < Zx} (57)\n= T\u0303 1k,n + T\u0303 2 k,n. (58)\nWe use T\u0303 1k,n and T\u0303 2 k,n to respectively denote Equation (56) and (57) for short. Next we show that both of the terms are bounded.\n2. Bounding T\u0303 1k,n Here we note the event {W1 \u2265 Zx} and {Wx,t > W1} implies event {Wx,t > Zx}. Let ln+(y) = max(ln(y), 0). For any positive integer l0, we then have,\nT\u0303 1k,n \u2264 n\u2211\nt=1\n1{Wx,t \u2265 Zx} (59)\n\u2264 l0 + n\u2211\nt=l0\n1{Wx,t \u2265 Zx, T\u0303 1k,t > l0} (60)\n= l0 +\nn\u2211\nt=l0\nP{Wx,t \u2265 Zx, T\u0303 1k,t > l0} (61)\n= l0 + n\u2211\nt=l0\nP\n{\u2211\nj\u2208Yx\n( Xj,t +\n\u221a\u221a\u221a\u221a ln+( t 2/3\nKOj,t ) l0\n)\n\u2265 \u2211\nj\u2208Yx\n\u00b5j + \u2206x 2 , T\u0303 1k,t > l0\n} . (62)\nThe event {\u2211 j\u2208Yx ( Xj,t+ \u221a ln+(t2/3/KOj,t) Oj,t ) \u2265 \u2211j\u2208Yx \u00b5j+\n\u2206x 2\n} indicates that the following must be true,\n\u2203j \u2208 Yx, Xj,t + \u221a ln+(t2/3/KOj,t)\nOj,t \u2265 \u00b5j + \u2206x 2N . (63)\nUsing union bound one directly obtains:\nT\u0303 1k,n \u2264 l0 + n\u2211\nt=l0\n\u2211 j\u2208Yx P\n{ Xj,t + \u221a ln+(t2/3/KOj,t)\nOj,t\n\u2265 \u00b5j + \u2206x 2N\n} (64)\n\u2264 l0 + n\u2211\nt=l0\n\u2211 j\u2208Yx P\n{ Xj,t \u2212 \u00b5j\n\u2265 \u2206x 2N\n\u2212 \u221a ln+(t2/3/KOj,t)\nOj,t\n} . (65)\nNow we let l0 = 16N2\u2308ln(n 3/4 K \u2206 2 x)/\u2206 2 x)\u2309 with \u2308y\u2309 the smallest integer larger than y. We further set \u03b40 = e1/2 \u221a K/n2/3 and set x0 such that \u2206x0 \u2264 \u03b40 < \u2206x0+1. As Oj,t \u2265 l0,\nln+\n( t3/4\nKOj,t\n) \u2264 ln+ ( n3/4\nKOj,t\n) \u2264 ln+(n3/4/Kl0)\n\u2264 ln+( n3/4 K \u00d7 \u2206 2 x 16N2 ) \u2264 l0\u2206 2 x 16N2 \u2264 Oj,t\u2206 2 x 16N2 . (66)\nHence we have,\n\u2206x 2N\n\u2212 \u221a ln+(t3/4/KOj,t)\nOj,t \u2265 \u2206x 2N \u2212 \u2206x\u221a 16N2 = c\u2206x (67)\nwith c = 12N \u2212 1\u221a16N2 = 1 4N . Therefor, using Hoeffding\u2019s inequality and Equation (65),\nand then plugging into the value of l0, we get,\nT\u0303 1k,n \u2264 l0 + n\u2211\nt=l0\n\u2211 j\u2208Yx P\n{ Xj,t \u2212 \u00b5j \u2265 c\u2206x }\n\u2264 l0 + n\u2211\nt=l0\n\u2211 j\u2208Yx exp(\u22122Oj,t(c\u2206x)2)\n\u2264 l0 +K \u00b7 n \u00b7 exp(\u22122l0(c\u2206x)2)\n= 1 + 16N2 ln(n\n3/4 K \u2206 2 x)\n\u22062x +K \u00b7 n \u00b7 exp(\u22122 ln(n 112 e)).\n(68)\nAs \u03b40 = e1/2 \u221a K/n 2 3 and \u2206x > \u03b40, the second term in\n(68) is bounded by\n16N2(1 + lnn1/12)\nKe \u00b7 n2/3 < 16N\n2(n2/3 + n3/4)\nKe\nThe last term of (68) is bounded by\nK \u00b7 n \u00b7 exp(\u22122 ln(n 112 e)) \u2264 K e2 \u00b7 n 56\nFinally we get\nT\u0303 1k,n = 1 + 16N2(n2/3 + n3/4)\nKe +\nK e2 \u00b7 n 56 . (69)\n3. Bounding T\u0303 2k,n\nT\u0303 2k,n = n\u2211\nt=1\n1{W1 \u2264 Wx,t,W1 < Zx}\n\u2264 n\u2211\nt=1\nP{W1 < Zx} \u2264 nP{W1 < Zx}. (70)\nRemember that at time slot z, we have W1 = minW1,t. For the probability {W1 < Zx} of fixed x, we have\nP{W1 < \u03c31 \u2212 \u2206x 2 } (71)\n= P\n{ N\u2211\nj\u2208N1,j=1\nwj,z < \u03c31 \u2212 \u2206x 2\n} (72)\n\u2264 \u2211\nj\u2208N1\nP { wj,z < \u00b5j \u2212 \u2206x\n2N\n} . (73)\nWe define function f(u) = e ln( \u221a n1/3\nK u)/u 3 for u \u2208\n[\u03b40, N ]. Then we have,\nP { wj,z < \u00b5j \u2212\n\u2206x 2N\n}\n= P { \u22031 \u2264 l \u2264 n : l\u2211\n\u03c4=1\n( Xj,\u03c4 +\n\u221a ln+( \u03c42/3\nKl )\nl\n) < l\u00b5j \u2212 l\u2206x\n2N\n}\n\u2264 P { \u22031 \u2264 l \u2264 n : l\u2211\n\u03c4=1\n(\u00b5j \u2212Xj,\u03c4 ) > \u221a l ln+( \u03c4 2/3\nKl ) + l\u2206x 2N\n}\n\u2264 P { \u22031 \u2264 l \u2264 f(\u2206x) : l\u2211\n\u03c4=1\n(\u00b5j \u2212Xj,\u03c4 ) > \u221a l ln+( \u03c4 2/3\nKl )\n}\n+P { \u2203f(\u2206x) < l \u2264 n : l\u2211\n\u03c4=1\n(\u00b5j \u2212Xj,\u03c4 ) > l\u2206x 2N\n} . (74)\nFor the first term we use a peeling argument with a geometric grid of the form 12g+1 f(\u2206x) \u2264 l \u2264 12g f(\u2206x):\nP { \u22031 \u2264 l \u2264 f(\u2206x) : l\u2211\n\u03c4=1\n(\u00b5j \u2212Xj,\u03c4 )\n> \u221a l ln+( \u03c4 2/3\nKl )\n}\n\u2264 \u221e\u2211\ng=0\nP { \u2203 1 2g+1 f(\u2206x) \u2264 l \u2264 1 2g f(\u2206x) : l\u2211\n\u03c4=1\n(\u00b5j \u2212Xj,\u03c4 )\n>\n\u221a f(\u2206x)\n2g+1 ln+(\n\u03c4 2/32g\nKf(\u2206x) )\n}\n\u2264 \u221e\u2211\ng=0\nexp ( \u22122 f(\u2206x) 1 2g+1 ln+( \u03c42/32g Kf(\u2206x) )\nf(\u2206x) 1 2g\n)\n\u2264 \u221e\u2211\ng=0\n[ Kf(\u2206x)\nn2/3 1 2g\n] \u2264 2Kf(\u2206x)\nn2/3 (75)\nwhere in the second inequality we use Lemma 2. As the special design of function f(u), we have f(u) takes\nmaximum of n 1/2\n3K3/2 when u = e1/3\n\u221a K/n1/3. For \u2206x >\ne1/3 \u221a K/n1/3 , we have\n2Kf(\u2206x) n2/3 \u2264 2 3 \u221a K n\u22121/6. (76)\nFor the second term we also use a peeling argument but with a geometric grid of the form 2gf(\u2206x) \u2264 l < 2g+1f(\u2206x):\nP { \u2203f(\u2206x) < l \u2264 n : l\u2211\n\u03c4=1\n(\u00b5j \u2212Xj,\u03c4 ) > l\u2206x 2N\n}\n\u2264 \u221e\u2211\ng=0\nP { \u22032gf(\u2206x) \u2264 l \u2264 2g+1f(\u2206x) : l\u2211\n\u03c4=1\n(\u00b5j \u2212Xj,\u03c4 )\n> 2g\u22121f(\u2206x)\u2206x\nN\n}\n\u2264 \u221e\u2211\ng=0\nexp\n( \u22122gf(\u2206x)\u22062x\n4N2\n)\n\u2264 \u221e\u2211\ng=0\nexp ( \u2212(g + 1)f(\u2206x)\u22062x/4N2 )\n= 1\nexp(f(\u2206x)\u22062x/4N2)\u2212 1 . (77)\nWe note that f(u)u2 has a minimum of e\u221a K n1/6 when u =\nx0. Thus for (77), we further have,\n1\nexp( f(\u2206x)\u22062x 4N2 )\u2212 1 \u2264 1 exp ( en1/6\n4 \u221a KN2\n) \u22121\n\u2264 4 \u221a KN2n\u2212 1 6\ne .\n(78)\nCombining (73) and (70), we then have\nT\u0303 2k,n \u2264 2Nn5/6\n3 \u221a K\n+ 4 \u221a KN3n5/6\ne \u2264 (1 + 4\n\u221a KN2\ne )Nn\n5 6 . (79)\n4. Results without dependency on \u2206min\nSumming T\u0303 1k,n and T\u0303 2 k,n, we have\nT\u0303x,n \u2264 T\u0303 1k,n + T\u0303 2k,n\n= 1 + 16N2\nKe (1 +\n8N\n15 )n\n2 3 + (1 +\n4 \u221a KN2\ne )Nn\n5 6\nand using \u2206X \u2264 N and \u2206x \u2264 \u03b40 for x \u2264 x0, we have\nR(n) \u2264 \u221a Ken 2 3 +NK [ 1 + 16N2\nKe (1 +\n8N\n15 )n\n2 3\n+(1 + 4 \u221a KN2\ne )Nn\n5 6\n]\n\u2264 NK + (\u221a eK + 8(1 +N)N3 ) n 2 3\n+(1 + 4 \u221a KN2\ne )N2Kn 5 6 ."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In this paper, we investigate a largely extended version of classical MAB problem, called networked combinatorial bandit problems. In particular, we consider the setting of a decision maker over a networked bandits as follows: each time a combinatorial strategy, e.g., a group of arms, is chosen, and the decision maker receives a reward resulting from her strategy and also receives a side bonus resulting from that strategy for each arm\u2019s neighbor. This is motivated by many real applications such as on-line social networks where friends can provide their feedback on shared content, therefore if we promote a product to a user, we can also collect feedback from her friends on that product. To this end, we consider two types of side bonus in this study: side observation and side reward. Upon the number of arms pulled at each time slot, we study two cases: single-play and combinatorial-play. Consequently, this leaves us four scenarios to investigate in the presence of side bonus: Single-play with Side Observation, Combinatorial-play with Side Observation, Single-play with Side Reward, and Combinatorial-play with Side Reward. For each case, we present and analyze a series of zero regret polices where the expect of regret over time approaches zero as time goes to infinity. Extensive simulations validate the effectiveness of our results.", "creator": "LaTeX with hyperref package"}}}