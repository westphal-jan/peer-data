{"id": "1503.06468", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2015", "title": "Machine Learning Methods for Attack Detection in the Smart Grid", "abstract": ".573 Attack detection withering problems in the associational smart grid are tsibliev posed as animate statistical learning problems 75-minute for oven different attack scenarios in which gennifer the dadaism measurements tiebreak are purling observed in batch or dellcptr online settings. structural In this brigid approach, machine bowrey learning rudimentary algorithms eynon are sarcomeres used hayman to classify measurements 1-46 as being sayliyah either secure bar-le-duc or oyer attacked. aetos An attack detection framework is 38,438 provided to undisguised exploit beston any available sixshot prior pt knowledge about haruchika the system qingguo and surmount constraints derryl arising from the sparse christiania structure of the problem in heth the proposed phylogeny approach. verbania Well - known batch bootstrapping and zinna online learning arbogast algorithms (supervised and semi - supervised) neust\u00e4dter are employed ansgar with decision eysseric and s\u0119dziejowice feature carmelite level annointed fusion to model the hooligan attack brigand detection halcion problem. The paroles relationships maasland between balck statistical petrale and wso geometric fmb properties of attack mtp02 vectors employed gagliardo in wels the gamlin attack tunic scenarios and protist learning tziona algorithms maurilio are analyzed to wismilak detect unobservable attacks using snack statistical prometheus learning amoron methods. meik The menudo proposed algorithms are chattman examined moravce on kinlaza various carbs IEEE tatooine test systems. underplays Experimental mp3s analyses nayagam show 92.25 that machine learning algorithms perseptive can lecco detect attacks 2,531 with performances higher than trongsa the valente attack detection algorithms which hispida employ tovuz state 691 vector 66-57 estimation methods in chmiel the sadanand proposed attack dassen detection reissues framework.", "histories": [["v1", "Sun, 22 Mar 2015 19:38:45 GMT  (1962kb)", "http://arxiv.org/abs/1503.06468v1", "14 pages, 11 Figures"]], "COMMENTS": "14 pages, 11 Figures", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.SY", "authors": ["mete ozay", "inaki esnaola", "fatos t yarman vural", "sanjeev r kulkarni", "h vincent poor"], "accepted": false, "id": "1503.06468"}, "pdf": {"name": "1503.06468.pdf", "metadata": {"source": "CRF", "title": "Machine Learning Methods for Attack Detection in the Smart Grid", "authors": ["Mete Ozay"], "emails": ["m.ozay@cs.bham.ac.uk).", "poor}@princeton.edu).", "vural@ceng.metu.edu.tr)."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n06 46\n8v 1\n[ cs\n.L G\n] 2\n2 M\nar 2\n01 5\nIndex Terms\u2014Smart grid security, sparse optimization, classification, attack detection, phase transition.\nI. INTRODUCTION\nMachine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4]. Rudin et al. [1] suggest an intelligent framework for system design in which machine learning algorithms are employed to predict the failures of system components. Anderson et al. [2] employ machine learning algorithms for the energy management of loads and sources in smart grid networks. Malicious activity prediction and intrusion detection problems have been analyzed using machine learning techniques at the network layer of smart grid communication systems [3], [4].\nIn this paper, we focus on the false data injection attack detection problem in the smart grid at the physical layer. We use the Distributed Sparse Attacks model proposed by Ozay et al. [5], where the attacks are directed by injecting false data into the local measurements observed by either local network operators or smart Phasor Measurement Units\nM. Ozay is with the School of Computer Science, University of Birmingham, B15 2TT, UK (e-mail: m.ozay@cs.bham.ac.uk). I. Esnaola, S. R. Kulkarni and H. V. Poor are with the Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA (e-mail: {jesnaola, kulkarni, poor}@princeton.edu). I. Esnaola is also with the Department of Automatic Control and Systems Engineering, University of Sheffield, S1 3JD, UK. F. T. Yarman Vural is with the Department of Computer Engineering, Middle East Technical University, Ankara, Turkey (e-mail: vural@ceng.metu.edu.tr).\nThis research was supported in part by the U. S. National Science Foundation under Grant CMMI-1435778.\n(PMUs) in a network with a hierarchical structure, i.e. the measurements are grouped into clusters. In addition, network operators who employ statistical learning algorithms for attack detection know the topology of the network, measurements observed in the clusters and the measurement matrix [5].\nIn attack detection methods that employ state vector estimation, first the state of the system is estimated from the observed measurements. Then, the residual between the observed and the estimated measurements is computed. If the residual is greater than a given threshold, a data injection attack is declared [5], [6], [7], [8]. However, exact recovery of state vectors is a challenge for state vector estimation based methods in sparse networks [5], [9], [10], where the Jacobian measurement matrix is sparse. Sparse reconstruction methods can be employed to solve the problem, but the performance of this approach is limited by the sparsity of the state vectors [5], [11], [12]. In addition, if false data injected vectors reside in the column space of the Jacobian measurement matrix and satisfy some sparsity conditions (e.g., the number of nonzero elements is at most \u03ba\u2217, which is bounded by the size of the Jacobian matrix), then false data injection attacks, called unobservable attacks, cannot be detected [7], [8].\nThe contributions of this paper are as follows: 1) We conduct a detailed analysis of the techniques pro-\nposed by Ozay et al. [13] who employ supervised learning algorithms to predict false data injection attacks. In addition, we discuss the validity of the fundamental assumptions of statistical learning theory in the smart grid. Then, we propose semi-supervised, online learning, decision and feature level fusion algorithms in a generic attack construction framework, which can be employed in hierarchical and topological networks for different attack scenarios. 2) We analyze the geometric structure of the measurement space defined by measurement vectors, and the effect of false data injection attacks on the distance function of the vectors. This leads to algorithms for learning the distance functions, detecting unobservable attacks, estimating the attack strategies and predicting future attacks using a set of observations. 3) We empirically show that the statistical learning algorithms are capable of detecting both observable and unobservable attacks with performance better than the attack detection algorithms that employ state vector estimation methods. In addition, phase transitions can be observed in the performance of Support Vector Machines (SVM) at a value of \u03ba\u2217 [14].\n2 In the next section, the attack detection problem is formulated as a statistical classification problem in a network according to the model proposed by Ozay et al. [5]. In Section II, we establish the relationship between statistical learning methods and attack detection problems in the smart grid. Supervised, semi-supervised, decision and feature level fusion, and online learning algorithms are used to solve the classification problem in Section III. In Section IV, our approach is numerically evaluated on IEEE test systems. A summary of the results and discussion on future work are given in Section V."}, {"heading": "II. PROBLEM FORMULATION", "text": "In this section, the attack detection problem is formalized as a machine learning problem."}, {"heading": "A. False Data Injection Attacks", "text": "False Data Injection Attacks are defined in the following model:\nz =Hx + n, (1)\nwhere x \u2208 RD contains the voltage phase angles at the buses, z \u2208 RN is the vector of measurements, H \u2208 RN\u00d7D is the measurement Jacobian matrix and n \u2208 RN is the measurement noise, which is assumed to have independent components [7]. The attack detection problem is defined as that of deciding whether or not there is an attack on the measurements. If the noise is distributed normally with zero mean, then a State Vector Estimation (SVE) method can be employed by computing\nx\u0302 = (HT\u039bH)\u22121HT\u039bz, (2)\nwhere \u039b is a diagonal matrix whose diagonal elements are given by \u039bii = \u03bd\u22122i , and \u03bd 2\ni is the variance of ni, \u2200i = 1,2, . . . ,N [7], [13]. The goal of the attacker is to inject a false data vector a \u2208 RN into the measurements without being detected by the operator. The resulting observation model is\nz\u0303 =Hx + a + n. (3)\nThe false data injection vector, a, is a nonzero vector, such that ai \u2260 0, \u2200i \u2208 A, where A is the set of indices of the measurement variables that will be attacked. The secure variables satisfy the constraint ai = 0, \u2200i \u2208 A\u0304, where A\u0304 is the set complement of A [13].\nIn order to detect an attack, the measurement residual [7], [13] is examined in \u21132-norm \u03c1 = \u2225z\u0303 \u2212Hx\u0302\u222522, where x\u0302 \u2208 R D is the state vector estimate. If \u03c1 > \u03c4 , where \u03c4 \u2208 R is an arbitrary threshold which determines the trade-off between the detection and false alarm probabilities, then the network operator declares that the measurements are attacked.\nOne of the challenging problems of this approach is that the Jacobian measurement matrices of power systems in the smart grid are sparse under the DC power flow model [13], [15]. Therefore, the sparsity of the systems determines the performance of sparse state vector estimation methods [11], [12]. In addition, unobservable attacks can be constructed even if the network operator can estimate the state vector correctly. For instance, if a =Hc, where c \u2208 RD is an attack vector, then the attack is unobservable by using the measurement residual\n\u03c1 [7], [8]. In this work, we show that statistical learning methods can be used to detect the unobservable attacks with performance higher than the attack detection algorithms that employ a state vector estimation approach. Following the motivation mentioned above, a new approach is proposed using statistical learning methods."}, {"heading": "B. Attack Detection using Statistical Learning Methods", "text": "Given a set of samples S = {si}Mi=1 and a set of labels Y = {yi} M i=1, where (si, yi) \u2208 S \u00d7 Y are independent and identically distributed (i.i.d.) with joint distribution P , the statistical learning problem can be defined as constructing a hypothesis function f \u2236 S \u2192 Y , that captures the relationship between the samples and labels [16]. Then, the attack detection problem is defined as a binary classification problem, where\nyi = \u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 1, if ai \u2260 0 \u22121, if ai = 0 . (4)\nIn other words, yi = 1, if the i-th measurement is attacked, and yi = \u22121 when there is no attack.\nIn this paper, the model proposed by Ozay et al. [5] is employed for attack construction where the measurements are observed in clusters in the network. Measurement matrices, and observation and attack vectors are partitioned into G blocks, denoted by Gg with \u2223Gg \u2223 = Ng for g = 1,2, . . . ,G. Therefore, the observation model is defined as\n\u23a1 \u23a2\u23a2 \u23a2 \u23a2\u23a2 \u23a3 z\u03031 \u22ee z\u0303G \u23a4 \u23a5\u23a5 \u23a5 \u23a5\u23a5 \u23a6 = \u23a1 \u23a2\u23a2 \u23a2 \u23a2\u23a2 \u23a3 H1 \u22ee HG \u23a4 \u23a5\u23a5 \u23a5 \u23a5\u23a5 \u23a6 x + \u23a1 \u23a2\u23a2 \u23a2 \u23a2\u23a2 \u23a3 a1 \u22ee aG \u23a4 \u23a5\u23a5 \u23a5 \u23a5\u23a5 \u23a6 + \u23a1 \u23a2\u23a2 \u23a2 \u23a2\u23a2 \u23a3 n1 \u22ee nG \u23a4 \u23a5\u23a5 \u23a5 \u23a5\u23a5 \u23a6 , (5)\nwhere z\u0303g \u2208 RNg is the measurement observed in the g-th cluster of nodes through measurement matrix Hg \u2208 RNg\u00d7D and noise ng \u2208 RNg , and which is under attack ag \u2208 RNg with g = 1,2, . . . ,G [5]. Within this framework, each observed measurement vector is considered as a sample, i.e., si \u225c z\u0303g , where z\u0303g \u2208 RNg 1. Taking this into account, the measurements are classified in two groups, secure and attacked, by computing f(si),\u2200i = 1,2, . . . ,M .\nThe crucial part of the traditional attack detection algorithm, which we call State Vector Estimation (SVE), is the estimation of x\u0302. If the attack vectors, a, are constructed in the column space of H, then they are annihilated in the computation of the residual [7]. Therefore, SVE cannot detect the attacks and these attacks are called unobservable. On the other hand, we observe that the distance between the attacked and the secure measurement vectors is defined by the attack vector in S. If the attacks are unobservable, i.e. ai = Hci and aj = Hcj , where ci \u2208 RD and cj \u2208 RD are the attack vectors, then the distance between z\u0303i = zi + ai and z\u0303j = zj + aj is computed as\n\u2225z\u0303i \u2212 z\u0303j\u22252 = \u23a7\u23aa\u23aa\u23aa\u23aa \u23a8 \u23aa\u23aa\u23aa\u23aa\u23a9 \u2225zi \u2212 zj\u22252 + \u2225ai \u2212 aj\u22252, if i, j \u2208 A \u2225zi \u2212 zj\u22252 + \u2225ai\u22252, if i \u2208 A, j \u2208 A\u0304 \u2225zi \u2212 zj\u22252, if i, j \u2208 A\u0304 ,\n(6)\n1For simplicity of notation, we use i as the index of measurements z\u0303i, zi, and attack vectors ai, \u2200i = 1,2, . . . ,M .\n3 where z\u0303i \u2208 S and z\u0303j \u2208 S. In (6), we can extract information on the attack vectors by observing the measurements. Since the distances between secure and attacked measurements are discriminated by the attack vectors, the attacks can be recognized by the learning algorithms which use the information of these distances, even if the attacks are unobservable.\nTwo main assumptions from statistical learning theory need to be taken into account to classify measurements which satisfy (6):\n1) We assume that (si, yi) \u2208 S\u00d7Y are distributed according to a joint distribution P [17]. In a smart grid setting, this distribution assumption is satisfied for the attack models in which the measurements z\u0303 are functions of a, and we can extract statistical information about both the attacked and secure measurements from the observations. 2) We assume that (si, yi), \u2200i, are sampled from P , independently and identically. This assumption is also satisfied in the smart grid if the entries of n and a are i.i.d. random variables [16].\nIn order to explain the significance of the above assumptions in the smart grid, we consider the following example. Assume that measurements 1,2 \u2208 A and 3,4 \u2208 A\u0304, are given such that y1, y2 = 1 and y3, y4 = \u22121. Furthermore, assume that z1 = 3 \u22c5I, z2 = 5 \u22c5 I, z3 = 2 \u22c5 I and z4 = 4 \u22c5 I, where I = (1,1)T . If the attack vectors are identical but not independent, then the attack vectors can be constructed as a1 = a2 = \u22121 \u22c5 I. As a result, we observe that z\u03031 = z\u03033 = 2 \u22c5 I and z\u03032 = z\u03034 = 4 \u22c5 I. Therefore, our assumption about the existence of a joint distribution P is not satisfied and we cannot classify the measurements with the aforementioned approach."}, {"heading": "III. ATTACK DETECTION USING MACHINE LEARNING METHODS", "text": "In this section, the attack detection problem is modeled by statistical classification of measurements using machine learning methods."}, {"heading": "A. Supervised Learning Methods", "text": "In the following, the classification function f is computed in a supervised learning framework by a network operator using a set of training data Tr = {(si, yi)}M Tr i=1 . The class label, y \u2032 i, of a new observation, s\u2032i, is predicted using y \u2032 i = f(s \u2032 i). We employ four learning algorithms for attack detection. 1) Perceptron: Given a sample si, a perceptron predicts yi using the classification function f(si) = sign(w \u22c5 si), where w \u2208 RNi is a weight vector and sign(w \u22c5 si) is defined as [17]\nsign(w \u22c5 si) = \u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 \u22121, if w \u22c5 si < 0 1, otherwise.\n(7)\nIn the training phase, the weights are adjusted at each iteration t = 1,2, . . . , T of the algorithm for each training sample using\nw(t + 1) \u2236=w(t) +\u2206w, (8)\nwhere \u2206w = \u03b3(yi \u2212 f(si))si and \u03b3 is the learning rate. The algorithm is iterated until a stopping criterion, such as the number of algorithm steps, or an error threshold, is achieved.\nIn the testing phase, the label of a new test sample is predicted by f(s\u2032i) = sign(w(T ) \u22c5 s\u2032i).\nDespite its success in various machine learning applications, the convergence of the algorithm is assured only when the samples are linearly separable [17]. For that reason, the perceptron can be successfully used for the detection of the attacks only if the measurements can be separated by a hyperplane. In the following sections, we give examples of classification algorithms which overcome this limitation by employing non-linear classification rules or feature extraction methods.\n2) k-Nearest Neighbor (k-NN): This algorithm labels an unlabeled sample s\u2032i according to the labels of its k-nearest neighborhood in the feature space [17]. Specifically, the observed measurements si \u2208 S, \u2200i = 1,2, . . . ,M , are taken as feature vectors. The set of k-nearest neighbors of s\u2032i, \u2135(s\u2032i) = {si(1), si(2), . . . , si(k)}, is constructed by computing the Euclidean distances between the samples [18], where i(1), i(2), . . . , i(M) are defined as\n\u2225s\u2032i \u2212 si(1)\u22252 \u2264 \u2225s \u2032 i \u2212 si(2)\u22252 \u2264 . . . \u2264 \u2225s \u2032 i \u2212 si(M)\u22252. (9)\nThen, the most frequently observed class label is computed using majority voting among the class labels of the samples in the neighborhood, and assigned as the class label of s\u2032i [19]. One of the challenges of k-NN is the curse of dimensionality, which is the difficulty of the learning problem when the sample size is small compared to the dimension of the feature vector [17], [19], [20]. In attack detection, this problem can be handled using the following approaches:\n\u25cf Feature selection algorithms can be used to reduce the dimension of the feature vectors [19], [20]. Development of feature selection algorithms may be a promising direction for smart grid security, and is an interesting topic for future work. \u25cf Kernel machines, such as SVMs, can be used to map the feature vectors in S to Hilbert spaces, where the feature vectors are processed implicitly in the mappings and the computation of the learning models. We give the details of the kernel machines and SVMs in the following sections. \u25cf The samples can be processed in small sizes, e.g. by selecting a single measurement vector as a sample, which leads to one-dimensional samples. We employ this approach in Section IV. If the sample size is large, distributed learning and optimization methods can be used [5], [15].\n3) Support Vector Machines: We seek a hyperplane that linearly separates attacked and secure measurements into two half spaces using hyperplanes in a D\u2032 dimensional feature space, F , which is constructed by a non-linear mapping \u03a8 \u2236 S \u2192 F [13], [21]. A hyperplane is represented by a weight vector w\u03a8 \u2208 RD \u2032 and a bias variable b \u2208 R, which results in\nw\u03a8 \u22c5\u03a8(s)+ b = 0, (10)\nwhere \u03a8(s) is the feature vector of the sample that lies on the hyperplane in F as shown in Fig. 1. We choose the hyperplane that is at the largest distance from the closest positive and\n4\nnegative samples. This constraint can be formulated as\nyi(w\u03a8 \u22c5\u03a8(s)+ b) \u2212 1 \u2265 0, \u2200i = 1,2, . . . ,MTr. (11)\nSince d+ = d\u2212 = 1\u2225w\u03a8\u22252 , where d+ and d\u2212 are the shortest distances from the hyperplane to the closest positive and negative samples respectively, a maximum margin hyperplane can be computed by minimizing \u2225w\u03a8\u22252.\nIf the training examples in the transformed space are not linearly separable (see Fig. 1.b), then the optimization problem can be modified by introducing slack variables \u03bei \u2265 0, \u2200i = 1,2, . . . ,MTr, in (11) which yields\nyi(w\u03a8 \u22c5\u03a8(si) + b) \u2212 1 + \u03bei \u2265 0 ,\u2200i = 1,2, . . . ,MTr. (12)\nThe hyperplane w\u03a8 is computed by solving the following optimization problem in primal or dual form [21], [22], [23]\nminimize \u2225w\u03a8\u222522 +C MTr\n\u2211 i=1 \u03bei\nsubject to yi(w\u03a8 \u22c5\u03a8(si) + b) \u2212 1 + \u03bei \u2265 0 \u03bei \u2265 0, \u2200i = 1,2, . . . ,MTr\n(13)\nwhere C is a constant that penalizes (an upper bound on) the training error of the soft margin SVM.\n4) Sparse Logistic Regression: In utilizing this approach for attack detection, we solve the classification problem using the Alternating Direction Method of Multipliers (ADMM) [24] considering the sparse state vector estimation approach of Ozay et al. [5]. Note that, the hyperplanes defined in (10) can be computed by employing the generalized logistic regression models presented in [19], which provide the distributions\nP (yi\u2223si) = 1\n1 + exp(\u2212yi(w \u22c5 si + b)) , (14)\nP (yi\u2223\u03a8(si)) = 1\n1 + exp(\u2212yi(w\u03a8 \u22c5\u03a8(si) + b)) , (15)\nin S and F , respectively. For this purpose, we minimize the logistic loss functions\nL(si, yi) = log (1 + exp (\u2212yi(w \u22c5 si + b))) , (16)\nL(\u03a8(si), yi) = log (1 + exp(\u2212yi(w\u03a8 \u22c5\u03a8(si) + b))) . (17)\nDefining a feature matrix S = (sT1 , s T 2 , . . . , s T Mtr )T and a label vector Y = (y1, y2, . . . , yMtr)\nT , the ADMM optimization problem [24] is constructed as\nminimize L(S,Y) + \u00b5(r) subject to w \u2212 r = 0 (18)\nwhere w is a weight vector, r is a vector of optimization variables, \u00b5(r) = \u03bb\u2225r\u22251 is a regularization function, and \u03bb is a regularization parameter which is introduced to control the sparsity of the solution [24]."}, {"heading": "B. Semi-supervised Learning Methods", "text": "In semi-supervised learning methods, the information obtained from the unlabeled test samples is used during the computation of the learning models [25].\nIn this section, a semi-supervised Support Vector Machine algorithm, called Semi-supervised SVM (S3VM) [26], [27] is employed to establish the analytical relationship between supervised and semi-supervised learning algorithms. In this setting, the unlabeled samples are incorporated into cost function of the optimization problem (13) as\nminimize \u2225w\u222522 +C1 MTr\n\u2211 i=1\nLTr(si, yi) +C2 MTe\n\u2211 i=1\nLTe(s\u2032i), (19)\nwhere C1 and C2 are confidence parameters, and LTr(si, yi) = max(0,1\u2212yi(wsi+ b)) and LTe(s\u2032i) =max(0,1\u2212\u2225s\u2032i\u22251) are the loss functions of the training and test samples, respectively.\nThe main assumption of the S3VM is that the samples in the same cluster have the same labels and the number of subclusters is not large [27]. In other words, attacked and secure measurement vectors should be clustered in distinct regions in the feature spaces. Moreover, the difference between the number of attacked and secure measurements should not be large in order to avoid the formation of sub-clusters.\nThis requirement can be validated by analyzing the feature space. Following (6), if \u2225zi\u2212zj\u22252+\u2225ai\u2212aj\u22252 \u2264 \u2225ai\u22252+\u2225aj\u22252, and \u2225zk \u2212 zl\u22252 \u2264 \u2225ai\u22252 + \u2225aj\u22252, \u2200i, j \u2208 A and \u2200k, l \u2208 A\u0304, then\n5 the samples belonging to different classes are well-separated in different classes. Moreover, this requirement is satisfied in (19) by adjusting C2 [27]. A survey of the methods which are used to provide optimal C2 and solve (19) is given in [27]."}, {"heading": "C. Decision and Feature Level Fusion Methods", "text": "One of the challenges of statistical learning theory is to find a classification rule that performs better than a set of rules of individual classifiers, or to find a feature set that represents the samples better than a set of individual features. One approach to solve this problem is to combine a collection of classifiers or a set of features to boost the performance of the individual classifiers. The former approach is called decision level fusion or ensemble learning, and the latter approach is called feature level fusion. In this section, we consider Adaboost [28] and Multiple Kernel Learning (MKL) [29] for ensemble learning and feature level fusion.\n1) Ensemble Learning for Decision Level Fusion: Various methods such as bagging, boosting and stacking have been developed to combine classifiers in ensemble learning situations [17], [30]. In the following, Adaboost is explained as an ensemble learning approach, in which a collection of weak classifiers are generated and combined using a combination rule to construct a stronger classifier which performs better than the weak classifiers [17], [28], [31].\nAt each iteration t = 1,2, . . . , T of the algorithm, a decision or hypothesis ft(\u22c5) of the weak classifier is computed with respect to the distribution on the training samples Dt(\u22c5) at t by minimizing the weighted error \u01ebt = MTr\n\u2211 i=1 Dt(i)I(ft(si) \u2260 yi),\nwhere I(\u22c5) is the indicator function. The distribution is initialized uniformly D1(i) = 1MTr at t = 1, and is updated by a parameter \u03b1t = 12 log( 1\u2212\u01ebt \u01ebt ) as follows [31]\nDt+1(i) = Dt(i) exp\u2212\u03b1tyift(si)\nZt , (20)\nwhere Zt is a normalization parameter, called the partition function. At the output of the algorithm, a strong classifier H(\u22c5) is constructed for a sample s\u2032 using H(s\u2032) = sign( T\n\u2211 t=1\n\u03b1tft(s \u2032)).\n2) Multiple Kernel Learning for Feature Level Fusion: Feature level fusion methods combine the feature spaces instead of the decisions of the classifiers. One of the feature level fusion methods is MKL in which different feature mappings are represented by kernels that are combined to produce a new kernel which represents the samples better than the other kernels [29]. Therefore, MKL provides an approach to solve the feature mapping selection problem of SVM. In order to see this relationship, we first give the dual form of (13)\nmaximize MTr\n\u2211 i=1\n\u03b2i \u2212 12 MTr\n\u2211 i=1\nMTr \u2211 j=1 \u03b2i\u03b2jyiyjk(si, sj)\nsubject to MTr\n\u2211 i=1 \u03b2iyi = 0\n0 \u2264 \u03b2i \u2264 C, \u2200i = 1,2, . . . ,MTr,\n(21)\nwhere \u03b2i is the dual variable and k(si, sj) = \u03a8(si) \u22c5\u03a8(sj) is the kernel function. Therefore, (21) is a single kernel learning\nalgorithm which employs a single kernel matrix K \u2208 RM Tr\u00d7MTr with elements K(i, j) = k(si, sj). If we define the weighted combination of U kernels as K = U\n\u2211 u=1 duKu, where du \u2265 0 are\nthe normalized weights such that U\n\u2211 u=1 du = 1, then we obtain\nthe following optimization problem of the MKL [32]:\nmaximize MTr\n\u2211 i=1\n\u03b2i \u2212 12 MTr\n\u2211 i=1\nMTr \u2211 j=1 \u03b2i\u03b2jyiyj U \u2211 u=1 duKu(si, sj)\nsubject to MTr\n\u2211 i=1 \u03b2iyi = 0\n0 \u2264 \u03b2i \u2264 C, \u2200i = 1,2, . . . ,MTr.\n(22)\nIn (22), the kernels with du = 0 are eliminated, and therefore MKL can be considered as a kernel selection method. In the experiments, SVM algorithms are implemented with different kernels and these kernels are combined under MKL."}, {"heading": "D. Online Learning Methods for Real-time Attack Detection", "text": "In the smart grid, the measurements are observed in realtime where the samples are collected sequentially in time. In this scenario, we relax the distribution assumption of Section II.B, since the samples are observed in an arbitrary sequence [33]. Moreover, smart PMUs which employ learning algorithms, may be required to detect the attacks when the measurements are observed without processing the whole set of training samples. In order to solve these challenging problems, we may use online versions of the learning algorithms given in the previous sections.\nIn a general online learning setting, a sequence of training samples (or a single sample) is given to the learning algorithm at each observation or algorithm processing time. Then, the algorithm computes the learning model using only the given samples and predicts the labels. The learning model is updated with respect to the error of the algorithm which is computed using a loss function on the given samples. Therefore, the perceptron and Adaboost are convenient for online learning in this setting. For instance, an online perceptron is implemented by predicting the label yi of a single sample si at each time t, and updating the weight vector w using \u2206w for the misclassified samples with yi \u2260 sign(f(si)) [34]. This simple approach is applied for the development of online MKL [34] and regression algorithms [35]."}, {"heading": "E. Performance Analysis", "text": "In smart grid networks, the major concern is not just the detection of attacked variables, but also that of the secure variables with high performance. In other words, we require the algorithms to predict the samples with high precision and recall performance in order to avoid false alarms. Therefore, we measure the true positives (tp), the true negatives (tn), the false positives (fp), and the false negatives (fn), which are defined in Table I.\nIn addition, the learning abilities and memorization properties of the algorithms are measured by Precision (Prec), Recall (Rec) and Accuracy (Acc) values which are defined as [13]\nPrec = tp tp+fp , Rec = tp tp+fn , Acc = tp+tn tp+tn+fp+fn . (23)\n6\nPrecision values give information about the prediction performance of the algorithms. On the other hand, Recall values measure the degree of attack retrieval. Finally, the total classification performance of the algorithms is measured by Accuracy. For instance, if Prec = 1, then none of the secure measurements is misclassified as attacked. If Rec = 1, then none of the attacked measurements is misclassified as secure. If Acc = 1, then each measurement classified as attacked is actually exposed to an attack, and each measurement classified as secure is actually a secure measurement."}, {"heading": "IV. EXPERIMENTS", "text": "The classification algorithms are analyzed in IEEE 9-bus, 57-bus and 118-bus test systems in the experiments. The measurement matrices H of the systems are obtained from the MATPOWER toolbox [36]. The operating points of the test systems provided in the MATPOWER case files are used in generating z. Training and test data are generated by repeating this process 50 times for each simulated point and dataset. In the experiments, we assume that the attacker has access to \u03ba measurements which are randomly chosen to generate a \u03ba-sparse attack vector a with Gaussian distributed nonzero elements with the same mean and variance as the entries of z [5], [13], [15]. We assume that concept drift [37] and dataset shift [38] do not occur. Therefore, we use G = N in the simulations following the results of Ozay et al. [5].\nWe analyze the behavior of each algorithm on each system for both observable and unobservable attacks by generating attack vectors with different values of \u03ba\nN \u2208 [0,1]. More\nprecisely, if \u03ba \u2265N \u2212D + 1, then attack vectors that are not observable by SVE, i.e. unobservable attacks, are generated [5]. Otherwise, the generated attacks are observable.\nThe LIBSVM [39] implementation is used for the SVM, and the ADMM [24] implementation is used for Sparse Logistic Regression (SLR). k values of the k-NN algorithm are optimized by searching k \u2208 {1,2, . . . , \u221a MTr} using leaveone-out cross-validation, where MTr is the number of training samples. Both the linear and Gaussian kernels are used for the implementation of SVM. A grid search method [39], [40], [41] is employed to search the parameters of the SVM in an interval I = [Imin,Imax], where Imin and Imax are user defined values. In order to follow linear paths in the search space, log values of parameters are considered in the grid search method [41]. Keerthi and Lin [41] analyzed the asymptotic properties of the SVM for I = [0,\u221e). In the experiments, Imin = \u221210 is chosen to compute a lower limit 2\u221210 of the parameter values following the theoretical results given in [39] and [41]. Since the classification performance of the SVM does not change for parameter values that are greater than a threshold [41], we used Imax = 10 as employed in the experimental analyses in [41]. Therefore, the kernel width parameter \u03c3 of a Gaussian\nkernel is searched in the interval log(\u03c3) \u2208 [\u221210,10] and the cost penalization parameter C of the SVM is searched in the interval log(C) \u2208 [\u221210,10]. The regularization parameter of the SLR is computed as\n\u03bb = \u2126\u03bbmax, (24)\nwhere \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u2126 is searched for in the interval \u2126 \u2208 [10\u22123,1] [5], [24], [42]. An optimal \u03bb\u0302 is computed by analyzing the solution (or regularization) path of the LASSO type optimization algorithms using a given training dataset. As the sparsity of the systems that generate datasets increases, lower values are calculated for \u2126 [5], [24], [42]. The absolute and relative tolerances, which determine values of upper bounds on the Euclidean norms of primal and dual residuals, are chosen as 10\u22124 and 10\u22122, respectively [24]. The penalty parameter is initially selected as 1 and dynamically updated at each iteration of the algorithm [24]. The maximum number of iterations is chosen as 104 [5], [24].\nIn the experiments, we observe that the selection of tolerance parameters does not affect the convergence rates if their relative values do not change. In addition, selection of the initial value of the penalty parameter also does not affect the convergence rate if relative values of tolerance parameters are fixed [24]. For instance, similar convergence rates are observed when we chose 10\u22124 and 10\u22122, or 10\u22126 and 10\u22124, as tolerance parameters. \u2225z\u0303 \u2212 Hx\u0302b\u2225 \u2264 \u03c4 is computed in order to decide whether there is an attack using the SVE and assuming a chisquare test with 95% confidence in the computation of \u03c4 [6], [13]."}, {"heading": "A. Results for Supervised Learning Algorithms", "text": "The performance of different algorithms is compared for the IEEE 57-bus system in Fig. 2. Accuracy values of the SVE and perceptron increase as \u03ba\nN increases in Fig. 2.a and\nFig. 2.b. Additionally, Recall values of the SVE increase linearly as \u03ba\nN increases. Precision values of the perceptron are\nhigh and do not decrease, and Accuracy and Recall values increase, since fn values decrease and tn values increase. In Fig. 2.c, a phase transition around \u03ba\u2217 = N \u2212 D + 1, is observed for the performance of the SVM. Since the distance between measurement vectors of attacked and secure variables increases as \u03ba\nN increases following (6), we observe that the\nAccuracy, Precision and Recall values of the k-NN increase in Fig. 2.d. Accuracy and Recall values of the k-NN and SLR are above 0.9 and do not change as \u03ba\nN increases in Fig. 2.e.\nThe class-based performance values of the algorithms are measured using class-wise performance indices, where Class-1 and Class-2 denotes the class of attacked and secure variables, respectively. The class-wise performance indices are defined as follows:\nClass \u2212 1 \u2236 Prec \u2212 1 = tp tp+fp\n, Rec \u2212 1 = tp\ntp + fn , (25)\nClass \u2212 2 \u2236 Prec \u2212 2 = tn tn+fn\n, Rec \u2212 2 = tn\nfp + tn . (26)\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03ba / N\nP e\nrf o\nrm a\nn ce\nLinear SVM\nAccuracy Precision Recall\n(c) SVM with linear kernel.\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03ba / N\nP e\nrf o\nrm a\nn ce\nk-NN\nAccuracy Precision Recall\n(d) k-NN.\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03ba / N\nP e\nrf o\nrm a\nn ce\nSLR\nAccuracy Precision Recall\n(e) SLR.\nFig. 2: Results for the IEEE 57-bus system. Accuracy values of the SVE and perceptron increase while Precision values of the k-NN and SLR increase as \u03ba N increases. Both Accuracy and Precision values of the SVM increase and phase transitions occur.\nIn Fig. 3.a, we observe that the Precision, Recall and Accuracy values of the SVE increase as \u03ba\nN increases for Class-\n1. Note that the first value of Acc-1 is observed at 0.008. In Fig. 3.b, Precision values for Class-2 decrease with the percentage of attacked variables, i.e. the number of secure variables that are incorrectly classified by the SVE increases as the number of attacked variables increases. Although the SVE may correctly detect the attacked variables as \u03ba\nN increases, the\nsecure variables are incorrectly labelled as attacked variables, and therefore, the SVE gives more false alarms than the other algorithms.\nPerformance values for the perceptron are given in Fig. 4. We observe that Precision values for Class-1 increase and Recall values do not change drastically for both of the classes as \u03ba\nN increases. Moreover, we do not observe any performance increase for the Recall values of the secure class in the perceptron.\nIn Fig. 5, the results for k-NN are shown. We observe that performance values for Class-1 increase and the values for Class-2 decrease as \u03ba\nN increases since k-NN is sensitive\nto class-balance and sparsity of the data [43]. In addition, classification hypotheses are computed by forming neighborhoods in Euclidean spaces, and the \u21132 norm of vectors of attacked measurements increases as \u03ba\nN increases in (6);\ntherefore, decision boundaries of the hypotheses are biased towards Class-1.\nFig. 6 depicts the results for the SLR, where the performance values for Class-2 (secure variables) increase as the system size increases. Moreover, we observe that the performance values for Class-2 do not decrease rapidly as \u03ba N\nincreases, compared to the other supervised algorithms. In addition, the performance values for Class-1 are higher than the values of the other algorithms, especially for lower \u03ba\nN\nvalues. The reason is that the SLR can handle the variety in the sparsity of the data as \u03ba\nN changes. This task is accomplished\nby controlling and learning the sparsity of the solution in (18) using the training data in order to learn the sparse structure of the measurements defined in the observation model (5).\nThe results of the experiments for the SVM are shown in Fig. 7, where a phase transition for the performance values is observed. It is worth noting that the values of \u03ba at which the phase transition occurs correspond to the minimum number of measurement variables, \u03ba\u2217, that the attacker needs to compromise in order to construct unobservable attacks [7]. \u03ba\u2217 is depicted as a vertical dotted line in Fig. 7. For instance, \u03ba\u2217 = 10 and \u03ba \u2217\nN = 0.56 for the IEEE 9-bus test system.\nThe transitions are observed before the critical points when the linear kernel SVM is employed in the experiments for IEEE 57-bus and 118-bus systems. In addition, the phase transitions of performance values occur at the critical points when Gaussian kernels are used."}, {"heading": "B. Results for Semi-supervised Learning Algorithms", "text": "We use the S3VM with default parameters as suggested in [44]. The results of the semi-supervised SVM are shown in Fig. 8. We do not observe sharp phase transitions in the semi-supervised SVM unlike the supervised SVM, since the information obtained from unlabeled data contributes to the performance values in the computation of the learning models. For instance, Precision values of Class-2 decrease sharply near the critical point for the supervised SVM in Fig. 7. However, the semi-supervised SVM employs the unlabeled samples during the computation of the learning model in (19), and partially solves this problem."}, {"heading": "C. Results for Decision and Feature Level Fusion Algorithms", "text": "In this section, we analyze Adaboost and MKL. Decision stumps are used as weak classifiers in Adaboost [31]. Each decision stump is a single-level two-leaf binary decision tree which is used to construct a set of dichotomies consisting of binary labelings of samples [31]. The number of weak classifiers is selected using leave-one-out cross-validation in the training set. We use MKL with a linear and a Gaussian kernel with the default parameters suggested in the Simple MKL implementation [32]. The results given in Fig. 9 show that Recall values of MKL for Class-1 are less than the values of Adaboost. In addition, Precision values of MKL decrease faster than the values of Adaboost as \u03ba\nN increases\nfor Class-2. Therefore, the fn values of MKL are greater than the values of Adaboost, or in other words, the number of attacked measurements misclassified as secure by MKL is greater than that of Adaboost. This phenomenon is observed in the results for semi-supervised and supervised SVM given in the previous sections. However, there are no phase transitions of the performance values of MKL compared to the supervised SVM."}, {"heading": "D. Results for Online Learning Algorithms", "text": "We consider four online learning algorithms, namely Online Perceptron (OP), Online Perceptron with Weighted Models (OPWM), Online SVM and Online SLR. Note that these algorithms are the online versions of the batch learning algorithms given in Section III-A and developed considering the online algorithm design approach given in Section III-D. The details of the implementations of the OP, OPWM, Online SVM and SLR are given in [34], [35] and [45].\nWhen the OP is used, only the model w(t) computed using the last observed measurement at time t is considered for the classification of the test samples. On the other hand, we consider an average of the models wave(t) = 1T T\n\u2211 t=1 w(t) which is computed by minimizing margin errors in the OPWM. Results are given for the OP in Fig. 10. In the weighted models, we observe phase transitions of the performance values for Class2 in Fig. 10.e-Fig. 10.h. However, the phase transitions occur before the critical values, and the values of the phase transition points decrease as the system size increases. Additionally, we do not observe sharp phase transitions in the OP.\nIn the OP, if the label of a measurement s is not correctly labeled, then the measurement vector is added to a set of supporting measurements S that are used to update the hypotheses in the training process. However, the hypotheses are updated in the OPWM if a measurement s\u2032 is not correctly labeled, and the vectors of s\u2032 and s \u2208 S are linearly independent. Since the smallest number of linearly dependent measurements increases as \u03ba\nN increases [5], [46], the size of S decreases and the bias is decreased towards Class-1. Therefore, false negative (fn) values decrease and false positive (fp) values increase [47]. As\na result, we observe that Recall values of the OP are less than that of the OPWM for Class-1. The results of the Online SVM and Online SLR are provided in Fig. 10 for different IEEE test systems. We observe phase transitions of performance values in the Online SVM similar to the batch supervised SVM.\nLearning curves of online learning algorithms are given in Fig. 11 for both observable attacks generated with \u03ba\nN = 0.33\nand unobservable attacks generated with \u03ba N = 0.66. Since the cost function of each online learning algorithm is different, the learning performance is measured and depicted using accuracy (Acc) defined in (23). In the results, performance values of the Online SVM and OPWM increase as the number of samples increases, since the algorithms employ margin learning approaches which provide better learning rates as the number of training samples increases [34], [45].\nBriefly, we suggest using Online SLR for the scenarios in which the precision of the classification of secure variables is important to avoid false alarms. On the other hand, if the classification of attacked variables with high Precision and Recall values is an important task, we suggest using the Online Perceptron.\n10\n11"}, {"heading": "V. SUMMARY AND CONCLUSION", "text": "The attack detection problem has been reformulated as a machine learning problem and the performance of supervised, semi-supervised, classifier and feature space fusion and online learning algorithms have been analyzed for different attack scenarios.\nIn a supervised binary classification problem, the attacked and secure measurements are labeled in two separate classes.\nIn the experiments, we have observed that state of the art machine learning algorithms perform better than the wellknown attack detection algorithms which employ a state vector estimation approach for the detection of both observable and unobservable attacks.\nWe have observed that the perceptron is less sensitive and the k-NN is more sensitive to the system size than the other algorithms. In addition, the imbalanced data problem affects\n12\nthe performance of the k-NN. Therefore, k-NN may perform better in small sized systems and worse in large sized systems when compared to other algorithms. The SVM performs better than the other algorithms in large-scale systems. In the performance tests of the SVM, we observe a phase transition at \u03ba\u2217, which is the minimum number of measurements that are required to be accessible by the attackers in order to construct unobservable attacks. Moreover, a large value of \u03ba does not necessary imply high impact of data injection attacks. For example, if the attack vector a has small values in all elements, then the impact of a may still be limited. More important, if a is a vector with small values compared to the noise, then even machine learning-based approaches may fail.\nWe observe two challenges of SVMs in their application to attack detection problems in smart grid. First, the performance of the SVM is affected by the selection of kernel types. For instance, we observe that the linear and Gaussian kernel SVM perform similarly in the IEEE 9-bus system. However, for the IEEE 57-bus system the Gaussian kernel SVM outperforms its linear counterparts. Moreover, the values of the phase transition points of the performance of the Gaussian kernel SVM coincide with the theoretically computed \u03ba\u2217 values. This implies that the feature vectors in F , which are computed using Gaussian kernels, are linearly separable for higher values of \u03ba. Interestingly, the transition points miss \u03ba\u2217 in the IEEE 118-bus system, which means that alternative kernels are needed for this system. Second, the SVM is sensitive to the sparsity of the systems. In order to solve this problem, sparse SVM [48] and kernel machines [49] can be employed. In this paper, we approached this problem using the SLR. However, obtaining an optimal regularization parameter, \u03bb\u0302, is computationally challenging [24].\nIn order to use information extracted from test data in the computation of the learning models, semi-supervised methods have been employed in the proposed approach. In semisupervised learning algorithms, we have used test data together with training data in an optimization algorithm used to compute the learning model. The numerical results show that the semi-supervised learning methods are more robust to the degree of sparsity of the data than the supervised learning methods.\nWe have employed Adaboost and MKL as decision and feature level fusion algorithms. Experimental results show that\nfusion methods provide learning models that are more robust to changes in the system size and data sparsity than the other methods. On the other hand, computational complexities of most of the classifier and feature fusion methods are higher than that of the single classifier and feature extraction methods.\nFinally, we have analyzed online learning methods for realtime attack detection problems. Since a sequence of training samples or just a single sample is processed at each time, the computational complexity of most of the online algorithms is less than the batch learning algorithms. In the experiments, we have observed that classification performance of online learning algorithms are comparable to that of the batch algorithms.\nIn future work, we plan to first apply the proposed approach and the methods to an attack classification problem for deciding which of several possible attack types have occurred given that an attack have been detected. Then, we plan to consider the relationship between measurement noise and bias-variance properties of learning models for the development of attack detection and classification algorithms. Additionally, we plan to expand our analyses for varying number of clusters G and cluster sizes Ng, \u2200g = 1,2, . . . ,G, by relaxing the assumptions made in this work for attack detection in smart grid systems, e.g. when the samples are not independent and identically distributed and obtained from non-stationary distributions, in other words, concept drift [37] and dataset shift [38] occur."}], "references": [{"title": "Machine learning for the New York City power grid", "author": ["C. Rudin", "D. Waltz", "R. Anderson", "A. Boulanger", "A. Salleb-Aouissi", "M. Chow", "H. Dutta", "P. Gross", "B. Huang", "S. Ierome"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, pp. 328\u2013345, Feb. 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive stochastic control for the smart grid", "author": ["R.N. Anderson", "A. Boulanger", "W.B. Powell", "W. Scott"], "venue": "Proc. IEEE, vol. 99, pp. 1098\u2013 1115, Jun. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "An early warning system against malicious activities for smart grid communications", "author": ["Z. Fadlullah", "M. Fouda", "N. Kato", "X. Shen", "Y. Nozaki"], "venue": "IEEE Netw., vol. 25, pp. 50\u201355, Sep. 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed intrusion detection system in a multi-layer network architecture of smart grids", "author": ["Y. Zhang", "L. Wang", "W. Sun", "R. Green", "M. Alam"], "venue": "IEEE Trans. Smart Grid, vol. 2, pp. 796\u2013808, Dec. 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse attack construction and state estimation in the smart grid: Centralized and distributed models", "author": ["M. Ozay", "I. Esnaola", "F.T. Yarman Vural", "S.R. Kulkarni", "H.V. Poor"], "venue": "IEEE J. Sel. Areas Commun., vol. 31, pp. 1306\u20131318, Jul. 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "False data injection attacks against state estimation in electric power grids", "author": ["Y. Liu", "P. Ning", "M.K. Reiter"], "venue": "Proc. 16th ACM Conf. Computer and Communications Security, Chicago, Illinois, Nov. 2009, pp. 21\u201332.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Malicious data attacks on the smart grid", "author": ["O. Kosut", "L. Jia", "R.J. Thomas", "L. Tong"], "venue": "IEEE Trans. Smart Grid, vol. 2, pp. 645\u2013658, Dec. 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparing the topological and electrical structure of the North American electric power infrastructure", "author": ["E. Cotilla-Sanchez", "P. Hines", "C. Barrows", "S. Blumsack"], "venue": "IEEE Syst. J., vol. 6, pp. 616\u2013626, Dec. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Strategic protection against data injection attacks on power grids", "author": ["T.T. Kim", "H.V. Poor"], "venue": "IEEE Trans. Smart Grid, vol. 2, no. 2, pp. 326\u2013333, Jun. 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Decoding by linear programming", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Inf. Theor., vol. 51, no. 12, pp. 4203\u20134215, Dec. 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inf. Theor., vol. 52, no. 4, pp. 1289\u20131306, Apr. 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Smarter security in the smart grid", "author": ["M. Ozay", "I. Esnaola", "F. Yarman Vural", "S.R. Kulkarni", "H.V. Poor"], "venue": "Proc. 3rd IEEE Int. Conf. Smart Grid Communications, Tainan City, Nov. 2012, pp. 312\u2013317.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Cornujols, Phase Transitions in Machine Learning", "author": ["L. Saitta", "A. Giordana"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distributed models for sparse attack construction and state vector estimation in the smart grid", "author": ["M. Ozay", "I. Esnaola", "F. Yarman Vural", "S.R. Kulkarni", "H.V. Poor"], "venue": "Proc. 3rd IEEE Int. Conf. Smart Grid Communications, Tainan City, Nov. 2012, pp. 306\u2013311.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to statistical learning theory", "author": ["O. Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "Advanced Lectures on Machine Learning, O. Bousquet, U. von Luxburg, and G. Rtsch, Eds. Berlin: Springer, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "An Elementary Introduction to Statistical Learning Theory", "author": ["S. Kulkarni", "G. Harman"], "venue": "Hoboken, NJ: Wiley Publishing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Divergence estimation for multidimensional densities via k-nearest-neighbor distances", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "IEEE Trans. Inf. Theor., vol. 55, pp. 2392\u20132405, May 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Support Vector Machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "New York: Springer Publishing Company, Incorporated,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Statistical learning theory: A tutorial", "author": ["S. Kulkarni", "G. Harman"], "venue": "Wiley Interdisciplinary Reviews: Computational Statistics, vol. 3, no. 6, pp. 543\u2013556, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Training a support vector machine in the primal", "author": ["O. Chapelle"], "venue": "Neural Comput., vol. 19, no. 5, pp. 1155\u20131178, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1\u2013122, Jan. 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "Eds"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "Proc. 16th Int. Conf. Mach. Learn., Bled, Jun. 1999, pp. 200\u2013209.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimization techniques for semi-supervised support vector machines", "author": ["O. Chapelle", "V. Sindhwani", "S.S. Keerthi"], "venue": "J. Mach. Learn. Res., vol. 9, no. 6, pp. 203\u2013233, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "Proc. 21st Int. Conf. Mach. Learn., Banff, AB, Jul. 2004, pp. 6\u201313.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. Kuncheva"], "venue": "Hoboken, NJ: Wiley-Interscience,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Boosting: Foundations and Algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "J. Mach. Learn. Res., vol. 9, pp. 2491\u20132521, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "From batch to transductive online learning", "author": ["S. Kakade", "A. Kalai"], "venue": "Advances in Neural Information Processing Systems, Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, Eds. Cambridge, MA: The MIT Press, 2005, pp. 611\u2013618.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Bounded kernel-based online learning", "author": ["F. Orabona", "J. Keshet", "B. Caputo"], "venue": "J. Mach. Learn. Res., vol. 10, pp. 2643\u20132666, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "An interior-point stochastic approximation method and an l1-regularized delta rule", "author": ["P. Carbonetto", "M. Schmidt", "N.D. Freitas"], "venue": "Advances in Neural Information Processing Systems, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. Red Hook, NY: Curran Associates, Inc., 2008, pp. 233\u2013240.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "MAT- POWER: Steady-state perations, planning, and analysis tools for power systems research and education", "author": ["R.D. Zimmerman", "C.E. Murillo-S\u00e1nchez", "R.J. Thomas"], "venue": "IEEE Trans. Power Syst., vol. 26, pp. 12\u201319, Feb. 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning changing concepts by exploiting the structure of change", "author": ["P.L. Bartlett", "S. Ben-David", "S.R. Kulkarni"], "venue": "Mach. Learn., vol. 41, no. 2, pp. 153\u2013174, 2000.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Libsvm: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intell. Syst. Technol., vol. 2, no. 3, pp. 1\u201327, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Mach. Learn. Res., vol. 9, pp. 1871\u20131874, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1871}, {"title": "Asymptotic behaviors of support vector machines with Gaussian kernel", "author": ["S.S. Keerthi", "C.-J. Lin"], "venue": "Neural Comput., vol. 15, no. 7, pp. 1667\u20131689, 2003.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2003}, {"title": "An interior-point method for largescale l1-regularized logistic regression", "author": ["K. Koh", "S.-J. Kim", "S. Boyd"], "venue": "J. Mach. Learn. Res., vol. 8, pp. 1519\u20131555, 2007.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "Making large-scale support vector machine learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods, B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, Eds. Cambridge, MA: The MIT Press, 1999, pp. 169\u2013 184.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1999}, {"title": "DOGMA: A MATLAB toolbox for online learning", "author": ["F. Orabona"], "venue": "2009. [Online]. Available: http://dogma.sourceforge.net", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization", "author": ["D.L. Donoho", "M. Elad"], "venue": "Proc. Nat. Acad. Sci., vol. 100, no. 5, pp. 2197\u20132202, 2003.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2003}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Dimensionality reduction via sparse support vector machines", "author": ["J. Bi", "K. Bennett", "M. Embrechts", "C. Breneman", "M. Song"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1229\u20131243, 2003.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "A direct method for building sparse kernel learning algorithms", "author": ["M. Wu", "B. Scholkopf", "G. Bakir"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 603\u2013624, 2006.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "[1] suggest an intelligent framework for system design in which machine learning algorithms are employed to predict the failures of system components.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] employ machine learning algorithms for the energy management of loads and sources in smart grid networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Malicious activity prediction and intrusion detection problems have been analyzed using machine learning techniques at the network layer of smart grid communication systems [3], [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Malicious activity prediction and intrusion detection problems have been analyzed using machine learning techniques at the network layer of smart grid communication systems [3], [4].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "[5], where the attacks are directed by injecting false data into the local measurements observed by either local network operators or smart Phasor Measurement Units", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In addition, network operators who employ statistical learning algorithms for attack detection know the topology of the network, measurements observed in the clusters and the measurement matrix [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 4, "context": "If the residual is greater than a given threshold, a data injection attack is declared [5], [6], [7], [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "If the residual is greater than a given threshold, a data injection attack is declared [5], [6], [7], [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "If the residual is greater than a given threshold, a data injection attack is declared [5], [6], [7], [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "However, exact recovery of state vectors is a challenge for state vector estimation based methods in sparse networks [5], [9], [10], where the Jacobian measurement matrix is sparse.", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "However, exact recovery of state vectors is a challenge for state vector estimation based methods in sparse networks [5], [9], [10], where the Jacobian measurement matrix is sparse.", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "However, exact recovery of state vectors is a challenge for state vector estimation based methods in sparse networks [5], [9], [10], where the Jacobian measurement matrix is sparse.", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "Sparse reconstruction methods can be employed to solve the problem, but the performance of this approach is limited by the sparsity of the state vectors [5], [11], [12].", "startOffset": 153, "endOffset": 156}, {"referenceID": 9, "context": "Sparse reconstruction methods can be employed to solve the problem, but the performance of this approach is limited by the sparsity of the state vectors [5], [11], [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Sparse reconstruction methods can be employed to solve the problem, but the performance of this approach is limited by the sparsity of the state vectors [5], [11], [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 5, "context": ", the number of nonzero elements is at most \u03ba, which is bounded by the size of the Jacobian matrix), then false data injection attacks, called unobservable attacks, cannot be detected [7], [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 6, "context": ", the number of nonzero elements is at most \u03ba, which is bounded by the size of the Jacobian matrix), then false data injection attacks, called unobservable attacks, cannot be detected [7], [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 11, "context": "[13] who employ supervised learning algorithms to predict false data injection attacks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In addition, phase transitions can be observed in the performance of Support Vector Machines (SVM) at a value of \u03ba [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "False Data Injection Attacks False Data Injection Attacks are defined in the following model: z =Hx + n, (1) where x \u2208 R contains the voltage phase angles at the buses, z \u2208 R is the vector of measurements, H \u2208 R is the measurement Jacobian matrix and n \u2208 R is the measurement noise, which is assumed to have independent components [7].", "startOffset": 331, "endOffset": 334}, {"referenceID": 5, "context": ",N [7], [13].", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": ",N [7], [13].", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "The secure variables satisfy the constraint ai = 0, \u2200i \u2208 \u0100, where \u0100 is the set complement of A [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "In order to detect an attack, the measurement residual [7], [13] is examined in l2-norm \u03c1 = \u2225z\u0303 \u2212Hx\u0302\u222522, where x\u0302 \u2208 R D", "startOffset": 55, "endOffset": 58}, {"referenceID": 11, "context": "In order to detect an attack, the measurement residual [7], [13] is examined in l2-norm \u03c1 = \u2225z\u0303 \u2212Hx\u0302\u222522, where x\u0302 \u2208 R D", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "One of the challenging problems of this approach is that the Jacobian measurement matrices of power systems in the smart grid are sparse under the DC power flow model [13], [15].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "One of the challenging problems of this approach is that the Jacobian measurement matrices of power systems in the smart grid are sparse under the DC power flow model [13], [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 9, "context": "Therefore, the sparsity of the systems determines the performance of sparse state vector estimation methods [11], [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Therefore, the sparsity of the systems determines the performance of sparse state vector estimation methods [11], [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 5, "context": "For instance, if a =Hc, where c \u2208 R is an attack vector, then the attack is unobservable by using the measurement residual \u03c1 [7], [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "For instance, if a =Hc, where c \u2208 R is an attack vector, then the attack is unobservable by using the measurement residual \u03c1 [7], [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 14, "context": ") with joint distribution P , the statistical learning problem can be defined as constructing a hypothesis function f \u2236 S \u2192 Y , that captures the relationship between the samples and labels [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "[5] is employed for attack construction where the measurements are observed in clusters in the network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": ",G [5].", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "If the attack vectors, a, are constructed in the column space of H, then they are annihilated in the computation of the residual [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 15, "context": "Two main assumptions from statistical learning theory need to be taken into account to classify measurements which satisfy (6): 1) We assume that (si, yi) \u2208 S\u00d7Y are distributed according to a joint distribution P [17].", "startOffset": 213, "endOffset": 217}, {"referenceID": 14, "context": "random variables [16].", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "1) Perceptron: Given a sample si, a perceptron predicts yi using the classification function f(si) = sign(w \u22c5 si), where w \u2208 Ri is a weight vector and sign(w \u22c5 si) is defined as [17]", "startOffset": 178, "endOffset": 182}, {"referenceID": 15, "context": "Despite its success in various machine learning applications, the convergence of the algorithm is assured only when the samples are linearly separable [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "2) k-Nearest Neighbor (k-NN): This algorithm labels an unlabeled sample si according to the labels of its k-nearest neighborhood in the feature space [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 16, "context": ", si(k)}, is constructed by computing the Euclidean distances between the samples [18], where i(1), i(2), .", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "One of the challenges of k-NN is the curse of dimensionality, which is the difficulty of the learning problem when the sample size is small compared to the dimension of the feature vector [17], [19], [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 4, "context": "If the sample size is large, distributed learning and optimization methods can be used [5], [15].", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "If the sample size is large, distributed learning and optimization methods can be used [5], [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "3) Support Vector Machines: We seek a hyperplane that linearly separates attacked and secure measurements into two half spaces using hyperplanes in a D dimensional feature space, F , which is constructed by a non-linear mapping \u03a8 \u2236 S \u2192 F [13], [21].", "startOffset": 238, "endOffset": 242}, {"referenceID": 17, "context": "3) Support Vector Machines: We seek a hyperplane that linearly separates attacked and secure measurements into two half spaces using hyperplanes in a D dimensional feature space, F , which is constructed by a non-linear mapping \u03a8 \u2236 S \u2192 F [13], [21].", "startOffset": 244, "endOffset": 248}, {"referenceID": 17, "context": "(12) The hyperplane w\u03a8 is computed by solving the following optimization problem in primal or dual form [21], [22], [23]", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "(12) The hyperplane w\u03a8 is computed by solving the following optimization problem in primal or dual form [21], [22], [23]", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "(12) The hyperplane w\u03a8 is computed by solving the following optimization problem in primal or dual form [21], [22], [23]", "startOffset": 116, "endOffset": 120}, {"referenceID": 20, "context": "4) Sparse Logistic Regression: In utilizing this approach for attack detection, we solve the classification problem using the Alternating Direction Method of Multipliers (ADMM) [24] considering the sparse state vector estimation approach of Ozay et al.", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": ", yMtr) T , the ADMM optimization problem [24] is constructed as minimize L(S,Y) + \u03bc(r) subject to w \u2212 r = 0 (18)", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "where w is a weight vector, r is a vector of optimization variables, \u03bc(r) = \u03bb\u2225r\u22251 is a regularization function, and \u03bb is a regularization parameter which is introduced to control the sparsity of the solution [24].", "startOffset": 208, "endOffset": 212}, {"referenceID": 21, "context": "Semi-supervised Learning Methods In semi-supervised learning methods, the information obtained from the unlabeled test samples is used during the computation of the learning models [25].", "startOffset": 181, "endOffset": 185}, {"referenceID": 22, "context": "In this section, a semi-supervised Support Vector Machine algorithm, called Semi-supervised SVM (S3VM) [26], [27] is employed to establish the analytical relationship between supervised and semi-supervised learning algorithms.", "startOffset": 103, "endOffset": 107}, {"referenceID": 23, "context": "In this section, a semi-supervised Support Vector Machine algorithm, called Semi-supervised SVM (S3VM) [26], [27] is employed to establish the analytical relationship between supervised and semi-supervised learning algorithms.", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "The main assumption of the S3VM is that the samples in the same cluster have the same labels and the number of subclusters is not large [27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "Moreover, this requirement is satisfied in (19) by adjusting C2 [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "A survey of the methods which are used to provide optimal C2 and solve (19) is given in [27].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "In this section, we consider Adaboost [28] and Multiple Kernel Learning (MKL) [29] for ensemble learning and feature level fusion.", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "In this section, we consider Adaboost [28] and Multiple Kernel Learning (MKL) [29] for ensemble learning and feature level fusion.", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "1) Ensemble Learning for Decision Level Fusion: Various methods such as bagging, boosting and stacking have been developed to combine classifiers in ensemble learning situations [17], [30].", "startOffset": 178, "endOffset": 182}, {"referenceID": 26, "context": "1) Ensemble Learning for Decision Level Fusion: Various methods such as bagging, boosting and stacking have been developed to combine classifiers in ensemble learning situations [17], [30].", "startOffset": 184, "endOffset": 188}, {"referenceID": 15, "context": "In the following, Adaboost is explained as an ensemble learning approach, in which a collection of weak classifiers are generated and combined using a combination rule to construct a stronger classifier which performs better than the weak classifiers [17], [28], [31].", "startOffset": 251, "endOffset": 255}, {"referenceID": 24, "context": "In the following, Adaboost is explained as an ensemble learning approach, in which a collection of weak classifiers are generated and combined using a combination rule to construct a stronger classifier which performs better than the weak classifiers [17], [28], [31].", "startOffset": 257, "endOffset": 261}, {"referenceID": 27, "context": "In the following, Adaboost is explained as an ensemble learning approach, in which a collection of weak classifiers are generated and combined using a combination rule to construct a stronger classifier which performs better than the weak classifiers [17], [28], [31].", "startOffset": 263, "endOffset": 267}, {"referenceID": 27, "context": "The distribution is initialized uniformly D1(i) = 1 M at t = 1, and is updated by a parameter \u03b1t = 12 log( 1\u2212\u01ebt \u01ebt ) as follows [31]", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "One of the feature level fusion methods is MKL in which different feature mappings are represented by kernels that are combined to produce a new kernel which represents the samples better than the other kernels [29].", "startOffset": 211, "endOffset": 215}, {"referenceID": 28, "context": "the normalized weights such that U \u2211 u=1 du = 1, then we obtain the following optimization problem of the MKL [32]:", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "B, since the samples are observed in an arbitrary sequence [33].", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "For instance, an online perceptron is implemented by predicting the label yi of a single sample si at each time t, and updating the weight vector w using \u2206w for the misclassified samples with yi \u2260 sign(f(si)) [34].", "startOffset": 209, "endOffset": 213}, {"referenceID": 30, "context": "This simple approach is applied for the development of online MKL [34] and regression algorithms [35].", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "This simple approach is applied for the development of online MKL [34] and regression algorithms [35].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "In addition, the learning abilities and memorization properties of the algorithms are measured by Precision (Prec), Recall (Rec) and Accuracy (Acc) values which are defined as [13] Prec = tp tp+fp , Rec = tp tp+fn , Acc = tp+tn tp+tn+fp+fn .", "startOffset": 176, "endOffset": 180}, {"referenceID": 32, "context": "The measurement matrices H of the systems are obtained from the MATPOWER toolbox [36].", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "In the experiments, we assume that the attacker has access to \u03ba measurements which are randomly chosen to generate a \u03ba-sparse attack vector a with Gaussian distributed nonzero elements with the same mean and variance as the entries of z [5], [13], [15].", "startOffset": 237, "endOffset": 240}, {"referenceID": 11, "context": "In the experiments, we assume that the attacker has access to \u03ba measurements which are randomly chosen to generate a \u03ba-sparse attack vector a with Gaussian distributed nonzero elements with the same mean and variance as the entries of z [5], [13], [15].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": "In the experiments, we assume that the attacker has access to \u03ba measurements which are randomly chosen to generate a \u03ba-sparse attack vector a with Gaussian distributed nonzero elements with the same mean and variance as the entries of z [5], [13], [15].", "startOffset": 248, "endOffset": 252}, {"referenceID": 33, "context": "We assume that concept drift [37] and dataset shift [38] do not occur.", "startOffset": 29, "endOffset": 33}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We analyze the behavior of each algorithm on each system for both observable and unobservable attacks by generating attack vectors with different values of \u03ba N \u2208 [0,1].", "startOffset": 162, "endOffset": 167}, {"referenceID": 4, "context": "unobservable attacks, are generated [5].", "startOffset": 36, "endOffset": 39}, {"referenceID": 34, "context": "The LIBSVM [39] implementation is used for the SVM, and the ADMM [24] implementation is used for Sparse Logistic Regression (SLR).", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "The LIBSVM [39] implementation is used for the SVM, and the ADMM [24] implementation is used for Sparse Logistic Regression (SLR).", "startOffset": 65, "endOffset": 69}, {"referenceID": 34, "context": "A grid search method [39], [40], [41] is employed to search the parameters of the SVM in an interval I = [Imin,Imax], where Imin and Imax are user defined values.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "A grid search method [39], [40], [41] is employed to search the parameters of the SVM in an interval I = [Imin,Imax], where Imin and Imax are user defined values.", "startOffset": 27, "endOffset": 31}, {"referenceID": 36, "context": "A grid search method [39], [40], [41] is employed to search the parameters of the SVM in an interval I = [Imin,Imax], where Imin and Imax are user defined values.", "startOffset": 33, "endOffset": 37}, {"referenceID": 36, "context": "In order to follow linear paths in the search space, log values of parameters are considered in the grid search method [41].", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "Keerthi and Lin [41] analyzed the asymptotic properties of the SVM for I = [0,\u221e).", "startOffset": 16, "endOffset": 20}, {"referenceID": 34, "context": "In the experiments, Imin = \u221210 is chosen to compute a lower limit 2 of the parameter values following the theoretical results given in [39] and [41].", "startOffset": 135, "endOffset": 139}, {"referenceID": 36, "context": "In the experiments, Imin = \u221210 is chosen to compute a lower limit 2 of the parameter values following the theoretical results given in [39] and [41].", "startOffset": 144, "endOffset": 148}, {"referenceID": 36, "context": "Since the classification performance of the SVM does not change for parameter values that are greater than a threshold [41], we used Imax = 10 as employed in the experimental analyses in [41].", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "Since the classification performance of the SVM does not change for parameter values that are greater than a threshold [41], we used Imax = 10 as employed in the experimental analyses in [41].", "startOffset": 187, "endOffset": 191}, {"referenceID": 8, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 147, "endOffset": 153}, {"referenceID": 0, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 154, "endOffset": 157}, {"referenceID": 20, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 159, "endOffset": 163}, {"referenceID": 37, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 165, "endOffset": 169}, {"referenceID": 4, "context": "As the sparsity of the systems that generate datasets increases, lower values are calculated for \u03a9 [5], [24], [42].", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "As the sparsity of the systems that generate datasets increases, lower values are calculated for \u03a9 [5], [24], [42].", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "As the sparsity of the systems that generate datasets increases, lower values are calculated for \u03a9 [5], [24], [42].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "The absolute and relative tolerances, which determine values of upper bounds on the Euclidean norms of primal and dual residuals, are chosen as 10 and 10, respectively [24].", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "The penalty parameter is initially selected as 1 and dynamically updated at each iteration of the algorithm [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "The maximum number of iterations is chosen as 10 [5], [24].", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "The maximum number of iterations is chosen as 10 [5], [24].", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "In addition, selection of the initial value of the penalty parameter also does not affect the convergence rate if relative values of tolerance parameters are fixed [24].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "\u2225z\u0303 \u2212 Hx\u0302b\u2225 \u2264 \u03c4 is computed in order to decide whether there is an attack using the SVE and assuming a chisquare test with 95% confidence in the computation of \u03c4 [6], [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "It is worth noting that the values of \u03ba at which the phase transition occurs correspond to the minimum number of measurement variables, \u03ba, that the attacker needs to compromise in order to construct unobservable attacks [7].", "startOffset": 220, "endOffset": 223}, {"referenceID": 38, "context": "Results for Semi-supervised Learning Algorithms We use the S3VM with default parameters as suggested in [44].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Decision stumps are used as weak classifiers in Adaboost [31].", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "Each decision stump is a single-level two-leaf binary decision tree which is used to construct a set of dichotomies consisting of binary labelings of samples [31].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "We use MKL with a linear and a Gaussian kernel with the default parameters suggested in the Simple MKL implementation [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": "The details of the implementations of the OP, OPWM, Online SVM and SLR are given in [34], [35] and [45].", "startOffset": 84, "endOffset": 88}, {"referenceID": 31, "context": "The details of the implementations of the OP, OPWM, Online SVM and SLR are given in [34], [35] and [45].", "startOffset": 90, "endOffset": 94}, {"referenceID": 39, "context": "The details of the implementations of the OP, OPWM, Online SVM and SLR are given in [34], [35] and [45].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "Since the smallest number of linearly dependent measurements increases as \u03ba N increases [5], [46], the size of S decreases and the bias is decreased towards Class-1.", "startOffset": 88, "endOffset": 91}, {"referenceID": 40, "context": "Since the smallest number of linearly dependent measurements increases as \u03ba N increases [5], [46], the size of S decreases and the bias is decreased towards Class-1.", "startOffset": 93, "endOffset": 97}, {"referenceID": 41, "context": "Therefore, false negative (fn) values decrease and false positive (fp) values increase [47].", "startOffset": 87, "endOffset": 91}, {"referenceID": 30, "context": "In the results, performance values of the Online SVM and OPWM increase as the number of samples increases, since the algorithms employ margin learning approaches which provide better learning rates as the number of training samples increases [34], [45].", "startOffset": 242, "endOffset": 246}, {"referenceID": 39, "context": "In the results, performance values of the Online SVM and OPWM increase as the number of samples increases, since the algorithms employ margin learning approaches which provide better learning rates as the number of training samples increases [34], [45].", "startOffset": 248, "endOffset": 252}], "year": 2015, "abstractText": "Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semi-supervised) are employed with decision and feature level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than the attack detection algorithms which employ state vector estimation methods in the proposed attack detection framework.", "creator": "LaTeX with hyperref package"}}}