{"id": "1504.01783", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2015", "title": "Proximal Operators for Multi-Agent Path Planning", "abstract": "t24 We address hidipo the problem of planning collision - ashby-de-la-zouch free 23,900 paths 146.2 for 50,600 multiple agents archuletta using guidance optimization methods undercarriages known as proximal balvanera algorithms. crude Recently this approach parsecs was explored in .681 Bento et m5b al. x.xx.xx.xx.x 2013, which longyearbyen demonstrated villalpando its sabil ease of parallelization dousman and naletilic decentralization, the speed qingsong with rriggs which the coziness algorithms diablo generate steffen good quality sukarnoputri solutions, and its ability coila to wherwell incorporate agrippina different proximal tdma operators, lavizan each ensuring that paths 1690 satisfy hassle a desired keaten property. morizet Unfortunately, the zwolle operators derived only apply ambiguus to garraud paths stylidium in 2D and myanma require that dhanya any outperforms intermediate todorovi\u0107 waypoints com.the we might want coined agents to bika follow be kour preassigned to swatara specific agents, matisse limiting their range vavilov of liseberg applicability. capoeirista In dowd this quebec-based paper we bilodeau resolve liwen these flashnet limitations. shenstone We introduce 1419 new operators non-logical to deal yongmin with agents sveinbj\u00f6rn moving in clarecastle arbitrary dimensions 1:04 that are 1-to-1 faster to waihopai compute than pulkovo their ilich 2D predecessors magnificus and 15.49 we 88.1 introduce landmarks, space - time 800-1000 positions candaba that fossorial are automatically assigned halpert to zalamea the qsl set of agents under different treutlen optimality criteria. Finally, mbowe we pollinia report the performance forgea of 1862 the new operators zarembski in kabarole several stressed numerical 22.54 experiments.", "histories": [["v1", "Tue, 7 Apr 2015 23:49:31 GMT  (257kb,D)", "http://arxiv.org/abs/1504.01783v1", "See movie atthis http URL"]], "COMMENTS": "See movie atthis http URL", "reviews": [], "SUBJECTS": "cs.RO cs.AI math.OC", "authors": ["jos\u00e9 bento", "nate derbinsky", "charles mathy", "jonathan s yedidia"], "accepted": true, "id": "1504.01783"}, "pdf": {"name": "1504.01783.pdf", "metadata": {"source": "CRF", "title": "Proximal Operators for Multi-Agent Path Planning", "authors": ["Jos\u00e9 Bento", "Nate Derbinsky", "Charles Mathy", "Jonathan S. Yedidia"], "emails": ["jose.bento@bc.edu", "derbinskyn@wit.edu", "cmathy@disneyresearch.com", "yedidia@disneyresearch.com"], "sections": [{"heading": "1 Introduction", "text": "In this paper we provide a novel set of algorithmic building blocks (proximal operators) to plan paths for a system of multiple independent robots that need to move optimally across a set of locations and avoid collisions with obstacles and each other. This problem is crucial in applications involving automated storage, exploration and surveillance.\nEven if each robot has few degrees of freedom, the joint system is complex and this problem is hard to solve (Reif 1979; Hopcroft, Schwartz, and Sharir 1984). We can divide existing algorithms for this problem into global planners, if they find collision-free beginning-to-end paths connecting two desired configurations, or local planners, if they find short collision-free paths that move the system only a bit closer to the final configuration.\nWe briefly review two of the most rigorous approaches. Random sampling methods, first introduced in (Kavraki and Latombe 1994; Kavraki et al. 1996), are applicable to global planning and explore the space of possible robot configurations with discrete structures. The rapidly-exploring random\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. \u2020A special thanks to Lucas Foguth for valuable discussions.\ntree algorithm (RRT; LaValle and Kuffner 2001), is guaranteed to asymptotically find feasible solutions with highprobabilty while the RRT* algorithm (Karaman and Frazzoli 2010) asymptotically finds the optimal solution. However, their convergence rate degrades as the dimension of the joint configuration space increases, as when considering multiple robots, and they cannot easily find solutions where robots move in tight spaces. In addition, even approximately solving some simple problems requires many samples, e.g., approximating a shortest path solution for a single robot required to move between two points with no obstacles (Karaman and Frazzoli 2010, see Fig. 1). These methods explore a continuous space using discrete structures and are different from methods that only consider agents that move on a graph with no concern about their volume or dynamics, e.g. (Standley and Korf ; Sharon et al. 2013).\nAn optimization-based approach has been used by several authors, including Mellinger, Kushleyev, and Kumar (2012), who formulate global planning as a mixed-integer quadratic problem and, for up to four robots, solve it using branch and bound techniques. Sequential convex programming was used in (Augugliaro, Schoellig, and D\u2019Andrea 2012) to efficiently obtain good local optima for global planning up to twelve robots. State-of-the-art optimization-based algorithms for local planning typically have real-time performance and are based on the velocity-obstacle (VO) idea of (Fiorini and Shiller 1998), which greedily plans paths only a few seconds into the future and then re-plans. These methods scale to hundreds of robots. Unlike sampling algorithms, optimization-based methods easily find solutions where robots move tightly together and solve simple problems very fast. However, they do not perform as well in problems involving robots in complex mazes.\nOur work builds on the work of Bento et al. (2013), which formulates multi-agent path planning as a large non-convex optimization problem and uses proximal algorithms to solve it. More specifically, the authors use the Alternating Direction Method of Multipliers (ADMM) and the variant introduced in Derbinsky et al. (2013) called the Three Weight Algorithm (TWA). These are iterative algorithms that do not access the objective function directly but indirectly through multiple (simple) algorithmic blocks called proximal operators, one per function-term in the objective. At each iteration these operators can be applied independently and so both the\nar X\niv :1\n50 4.\n01 78\n3v 1\n[ cs\n.R O\n] 7\nA pr\n2 01\n5\nTWA and the ADMM are easily parallelized. A brief explanation of this optimization formulation is given in Section 2. A self-contained explanation about proximal algorithms is found in Parikh and Boyd (2013) and a good review on the ADMM is Boyd et al. (2011). In general the ADMM and the TWA are not guaranteed to converge for non-convex problems. There is some work on solving non-convex problems using proximal algorithms with guarantees (see Udell and Boyd 2014 and references in Parikh and Boyd 2013) but the settings considered are not applicable to the optimization problem at hand. Nonetheless, the empirical results in Bento et al. (2013) are very satisfactory. For global planning, their algorithm scales to many more robots than other optimization based methods and finds better solutions than VO-based methods. Their method also can be implemented in the (useful) form of a decentralized message-passing scheme and new proximal operators can be easily added or removed to account for different aspects of planning, such as, richer dynamic and obstacle constraints.\nThe main contributions of Bento et al. (2013) are the proximal operators that enforce no robot-robot collisions and no robot-wall collisions. These operators involve solving a nontrivial problem with an infinite number of constraints and a finite number of variables, also known as a semi-infinite programming problem (SIP). The authors solve this SIP problem only for robots moving in 2D by means of a mechanical analogy, which unfortunately excludes applications in 3D such as those involving fleets of unmanned aerial vehicles (UAVs) or autonomous underwater vehicles (AUVs). Another limitation of their work is that it does not allow robots to automatically select waypoint positions from a set of reference positions. This is required, for example, in problems involving robots in formations (Bahceci, Soysal, and Sahin 2003). In Bento et al. (2013), any reference position must be pre-assigned to a specific robot.\nIn this paper we propose a solution to these limitations. Our contributions are (i) we rigorously prove that the SIP problem involved in collision proximal operators can be reduced to solving a single-constraint non-convex problem that we solve explicitly in arbitrary dimensions and numerically show our novel approach is substantially faster for 2D than Bento et al. (2013) and (ii) we derive new proximal operators that automatically assign agents to a subset of reference positions and penalize non-optimal assignments. Our contributions have an impact beyond path planning problems. Other applications in robotics, computer vision or CAD that can be tackled via large optimization problems involving collision constraints or the optimal assignment of objects to positions (e.g. Kuffner et al. 2002; Witkin and Kass 1988; Andreev, Pavisic, and Raspopovic 2001) might benefit from our new building blocks (cf. Section 6).\nWhile there is an extensive literature on how to solve SIP problems (see Stein (2012) for a good review), as far as we know, previous methods are either too general and, when applied to our problem, computationally more expensive than our approach, or too restrictive and thus not applicable.\nFinally, we clarify that our paper is not so much about showing the merits of the framework used in Bento et al. (2013; a point already made), as it is about overcoming un-\nsolved critical limitations. However, our numerical results and supplementary video do confirm that the framework produces very good results, although there are no guarantees that the method avoids local minima."}, {"heading": "2 Background", "text": "Here we review the formulation of Bento et al. (2013) of path planning as an optimization problem, explain what proximal operators are, and explain their connection to solving this optimization problem.\nWe have p spherical agents in Rd of radius {ri}pi=1. Our objective is to find collision-free paths {xi(t)}i\u2208[p],t\u2208[0,T ] for all agents between their specified initial positions {xiniti } p i=1 at time 0 and specified final positions {xfinali } p i=1 at time T . In the simplest case, we divide time in intervals of equal length and the path {xi(t)}Tt=0 of agent i\u2208 [p] is parametrized by a set of break-points {xi(s)}\u03b7s=0 such that xi(t)=xi(s) for t=sT/\u03b7 and all s. Between break-points agents have zero-acceleration. We discuss the practical impact of this assumption in Appendix A.\nWe express global path planning as an optimization problem with an objective function that is a large sum of simple cost functions. Each function accounts for a different aspect of the problem. Using similar notation to Bento et al. (2013), we need to minimize the objective function\u2211 i fpos(xi(0), x init i )+ \u2211 i fpos(xi(\u03b7), x final i )+ \u2211 i>j,s (1)\nf colli,j (xi(s), xi(s+1), xj(s), xj(s+1)) + \u2211 i,s f veli (xi(s), xi(s+1))\n+ \u2211 W,i,s fwallW (xi(s), xi(s+1)) + \u2211 i,s f diri (xi(s), xi(s+1), xi(s+2)).\nThe function f coll prevents agent-agent collisions: it is zero if \u2016\u03b1xi(s)+(1\u2212\u03b1)xi(s+1)\u2212(\u03b1xj(s)+(1\u2212\u03b1)xj(s+ 1))\u2016 \u2265 ri+rj for all \u03b1 \u2208 [0, 1] and infinity otherwise. The fwall function prevents agents from colliding with obstacles: it is zero if \u2016\u03b1xi(s) + (1\u2212\u03b1)xi(s+ 1)\u2212 y\u2016 \u2265 ri for all \u03b1 \u2208 [0, 1], y \u2208 W , where W is a set of points defining an obstacle, and is infinity otherwise. In Bento et al. (2013),W is a line between two points xL and xR in the plane and the summation \u2211 W is across a set of obstacles. The functions f vel and f dir impose restrictions on the velocities and direction changes of paths. The function f pos imposes boundary conditions: it is zero if xi(0) = xiniti (or if xi(\u03b7) = x final i ) and infinity otherwise. The authors also re-implement the local path planning method of Alonso-Mora et al. (2013), based on velocity obstacles (Fiorini and Shiller 1998), by solving an optimization algorithm similar to (1).\nBento et al. (2013) solve (1) using the TWA, a variation of the ADMM. The ADMM is an iterative algorithm that minimizes objectives that are a sum of many different functions. The ADMM is guaranteed to solve convex problems, but, empirically, the ADMM (and the TWA) can find good feasible solutions for large non-convex problems (Derbinsky et al. 2013; Bento et al. 2013).\nLoosely speaking, the ADMM proceeds by passing messages back and forth between two types of blocks: proximal operators and consensus operators. First, each function in\nthe objective is queried separately by its associated proximal operator to estimate the optimal value of the variables the function depends on. For example, the proximal operator associated with f coll1,2 produces estimates for optimal value of x1(s), x2(s), x1(s + 1) and x2(s + 1). These estimates are then sent to the consensus operators. Second, a consensus value for each variable is produced by its associated consensus operator by combining all the received different estimates for the values of the variable that the proximal operators produced. For example, the proximal operators associated with f coll1,2 and f vel 1 give two different estimates for the optimal value of x1(s) and the consensus operator associated with x1(s) needs to combine them into a single estimate. The consensus estimates produced by the consensus operators are then communicated back to and used by the proximal operators to produce new estimates, and the cycle is repeated until convergence. See Appendix B for an illustration of the blocks that solve a problem for two agents.\nIt is important to be more specific here. Consider a function f(x) in the objective. From the consensus value for its variables x, the corresponding consensus nodes form consensus messages n that are sent to the proximal operator associated with f . The proximal operator then estimates the optimal value for x as a tradeoff between a solution that is close to the minimizer of f and one that is close to the consensus information in n (Parikh and Boyd 2013),\nx \u2208 arg min x\u2032\nf(x\u2032) + \u03c1\n2 \u2016x\u2032 \u2212 n\u20162, (2)\nwhere we use \u2208 instead of = to indicate that, for a nonconvex function f , the operator might be one-to-many, in which case some extra tie-breaking rule needs to be implemented. The variable \u03c1 is a free parameter of the ADMM that controls this tradeoff and whose value affects its performance. In the TWA the performance is improved by dynamically assigning to the \u03c1\u2019s of the different proximal operators values in {0, const.,\u221e} (cf. Appendix C).\nWe emphasize that the implementation of these proximal operators is the crucial inner-loop step of the ADMM/TWA. For example, when f = f vel takes a quadratic (kinetic energy) form, the operator (2) has a simple closed-form expression. However, for f = f coll or f = fwall the operator involves solving a SIP problem. In Section 3 we explain how to compute these operators more efficiently and in a more general setting than in Bento et al. (2013)."}, {"heading": "3 No-collision proximal operator", "text": "Here we study the proximal operator associated with the function f coll that ensures there is no collision between two agents of radius r and r\u2032 that move between two consecutive break-points. We distinguish the variables associated to the two agents using \u2019 and distinguish the variables associated to the two break-points using \u2212 and a, respectively. For concreteness, just imagine, for example, that x = x1(0), x\u2032 = x2(0), x = x1(1) and x\u2032 = x2(1) and think of n, n\u2032, n\u2032 and n\u2032 as the associated received consensus messages. Following (2), the operator associated to f coll outputs the minimizer of\nmin x,x\u2032,x,x\u2032\n\u03c1 2 \u2016x\u2212n\u20162 + \u03c1 2 \u2016x\u2212n\u20162 +\n\u03c1\u2032 2 \u2016x\u2032\u2212n\u2032\u20162 + \u03c1 \u2032 2 \u2016x\u2032\u2212n\u2032\u20162\ns.t. \u2016\u03b1(x\u2212 x\u2032)+(1\u2212\u03b1)(x\u2212x\u2032)\u2016 \u2265 r+r\u2032, for all \u03b1\u2208[0, 1]. (3)\nOur most important contribution here is an efficient procedure to solve the above semi-infinite programming problem for agents in arbitrary dimensions by reducing it to a max-min problem. Concretely, Theorem 1 below shows that (3) is essentially equivalent to the \u2018most costly\u2019 of the problems in the following family of single-constraint problems parametrized by \u03b1,\nmin x,x\u2032,x,x\u2032\n\u03c1 2 \u2016x\u2212n\u20162 + \u03c1 2 \u2016x\u2212n\u20162 +\n\u03c1\u2032 2 \u2016x\u2032\u2212n\u2032\u20162 + \u03c1 \u2032 2 \u2016x\u2032\u2212n\u2032\u20162\ns.t. \u2016\u03b1(x\u2212x\u2032)+(1\u2212\u03b1)(x\u2212x\u2032)\u2016 \u2265 r+r\u2032. (4)\nSince problem (4) has a simple closed-form solution, we can solve (3) faster than in Bento et al. (2013) for 2D objects. We support this claim with numerical results in Section 5. In the supplementary video we use our new operator to do planning in 3D and, for illustration purposes, in 4D. Theorem 1. If \u2016\u03b1(n\u2212n\u2032) + (1\u2212\u03b1)(n\u2212n\u2032)\u2016 6= 0, then (4) has a unique minimizer, x\u2217(\u03b1), and if this condition holds for \u03b1 = \u03b1\u2217 \u2208 arg max\u03b1\u2032\u2208[0,1] h(\u03b1\u2032), where 2h2(\u03b1) is the minimum value of (4), then x\u2217(\u03b1\u2217) is also a minimizer of (3). In addition, if \u2016\u03b1(n\u2212n\u2032) + (1\u2212\u03b1)(n\u2212n\u2032)\u2016 6= 0, then\nh(\u03b1) = max 0, (r + r\u2032)\u2212 \u2016\u03b1\u2206n+ (1\u2212 \u03b1)\u2206n\u2016\u221a\u03b12/\u03c1\u02dc+ (1\u2212 \u03b1)2/\u03c1\u0303  , (5)\nand the unique minimizer of (4) is\nx\u2217 = n\u2212 \u03b3\u03c1(\u03b12\u2206n+ \u03b1(1\u2212 \u03b1)\u2206n), (6)\nx\u2032\u2217 = n\u2032 + \u03b3\u03c1\u2032(\u03b12\u2206n+ \u03b1(1\u2212 \u03b1)\u2206n), (7)\nx\u2217 = n\u2212 \u03b3\u03c1((1\u2212 \u03b1)\u03b1\u2206n+ (1\u2212 \u03b1)2\u2206n), (8) x\u2032\u2217 = n\u2032 + \u03b3\u03c1\u2032((1\u2212 \u03b1)\u03b1\u2206n+ (1\u2212 \u03b1)2\u2206n), (9)\nwhere \u03c1\u02dc = (\u03c1\u22121 + \u03c1\u2032\u22121)\u22121, \u03c1\u0303 = (\u03c1\u22121 + \u03c1\u2032\u22121)\u22121, \u03b3 = 2\u03bb1+2\u03bb(\u03b12/\u03c1\u02dc+(1\u2212\u03b1)2/\u03c1\u0303) , \u03bb = \u2212 h(\u03b1)2(r+r\u2032)\u221a\u03b12/\u03c1\u02dc+(1\u2212\u03b1)2/\u03c1\u0303 , \u2206n = n\u2212 n\u2032 and \u2206n = n\u2212 n\u2032. Remark 2. Under a few conditions, we can use Theorem 1 to find one solution to problem (3) by solving the simpler problem (4) for a special value of \u03b1. In numerical implementations however, the conditions of Theorem 1 are easy to satisfy, and the x\u2217(\u03b1\u2217) obtained is the unique minimizer of problem (3). We sketch why this is the case in Appendix D.\nIn a nutshell, to find one solution to (3) we simply find \u03b1\u2217 by maximizing (5) and then minimize (4) using (6)-(9) with \u03b1 = \u03b1\u2217. We can carry both steps efficiently, as shown in Section 5. The intuition behind Theorem 1 is that if we solve the optimization problem (3) for the \u2018worst\u2019 constraint (the \u03b1\u2217 that gives largest minimum value), then the solution also satisfies all other constraints, that is, it holds for all other \u03b1 \u2208 [0, 1]. We make this precise in the following general lemma that we use to prove Theorem 1. We denote by \u2202i the derivative of a function with respect to the ith variable. The proof of this Lemma is in Appendix E and that of Theorem 1 is in Appendix F. Lemma 3. Let A be a convex set in R, g : Rd \u00d7 A \u2192 R, (x, \u03b1) 7\u2192 g(x, \u03b1), be convex in \u03b1 and continuously differentiable in (x, \u03b1) and let f : Rd \u2192 R, x 7\u2192 f(x),\nbe continuously differentiable and have a unique minimizer. For every \u03b1 \u2208 A, let h(\u03b1) denote the minimum value of minx:g(x,\u03b1)\u22650 f(x) and if the minimum is attained by some feasible point let this be denoted by x\u2217(\u03b1). Under these conditions, if \u03b1\u2217 \u2208 arg max\u03b1\u2208A h(\u03b1), and if x\u2217(\u03b1) exists around a neighborhood of \u03b1\u2217 in A and is differentiable at \u03b1\u2217, and if \u22021g(x\u2217(\u03b1\u2217), \u03b1\u2217) 6= 0, then x\u2217(\u03b1\u2217) minimizes minx:g(x,\u03b1)\u22650\u2200\u03b1\u2208A f(x).\nOther collision operators Using similar ideas to those just described, we now explain how to efficiently extend to higher dimensions the wallagent collision operator that Bento et al. (2013) introduced. In the supplementary video we use these operators for path planning with obstacles in 3D.\nTo avoid a collision between agent 1, of radius r, and a line between points y1, y2 \u2208 Rd, we include the following constraint in the overall optimization problem: \u2016\u03b1x1(s) + (1\u2212\u03b1)x1(s+ 1)\u2212 (\u03b2y1 + (1\u2212\u03b2)y2)\u2016 \u2265 r for all \u03b1, \u03b2 \u2208 [0, 1] and all s+ 1 \u2208 [\u03b7 + 1]. This constraint is associated with the proximal operator that receives (n, n\u2032) and finds (x, x) that minimizes \u03c1\n2\u2016x \u2212 n\u2016 2 + \u03c12\u2016x \u2212 n\u2016 2 subject to \u2016\u03b1x+(1\u2212\u03b1)x\u2212(\u03b2y1+(1\u2212\u03b2)y2)\u2016 \u2265 r, for all \u03b1, \u03b2 \u2208 [0, 1]. Using ideas very similar to those behind Theorem 1 and Lemma 3, we solve this problem for dimensions strictly greater than two by maximizing over \u03b1, \u03b2 \u2208 [0, 1] the minimum value the single-constraint version of the problem. In fact, it is easy to generalize Lemma 3 to A \u2282 Rk and the single-constraint version of this optimization problem can be obtained from (4) by replacing n\u2032 and n\u2032 with \u03b2y1+(1\u2212\u03b2)y2, and letting \u03c1\u2032, \u03c1\u2032 \u2192 \u221e. Thus, we use (5) and (6)-(9) under this replacement and limit to generalize the line-agent collision proximal operator of Bento et al. (2013) to dimensions greater than two. We can also use the same operator to avoid collisions between agents and a line of thickness \u03bd, by replacing r with \u03bd+r.\nUnfortunately, we cannot implement a proximal operator to avoid collisions between an agent and the convex envelope of an arbitrary set of points y1, y2, ..., yq by maximizing over \u03b1, \u03b21, ..., \u03b2q\u22121 \u2208 [0, 1] the minimum of the singleconstraint problem obtained from (4) after replacing n\u2032 and n\u2032 with \u03b21y1+...+\u03b2q\u22122yq\u22122+(1\u2212\u03b21\u2212...\u2212\u03b2q\u22121)yq , and letting \u03c1\u2032, \u03c1\u2032 \u2192 \u221e. We can only do so when d > q, otherwise we observe that the condition \u22021g(x\u2217(\u03b1\u2217), \u03b1\u2217) 6= 0 of Lemma 3 does not hold and x\u2217(\u03b1\u2217) is not feasible for the original SIP problem. In particular, we cannot directly apply our maxmin approach to re-derive the line-agent collision operator for agents in 2D but only for dimensions \u2265 3. When d \u2264 q, we believe that a similar but more complicated principle can be applied to solve the original SIP problem. Our intuition from a few examples is that this involves considering different portions of the space A separately, computing extremal points instead of maximizing and minimizing and choosing the best feasible solution among these. We will explore this further in future work.\nSpeeding up computations The computational bottleneck for our collision operators is maximizing (5). Here we describe two scenarios, denoted\nas trivial and easy, when we avoid this expensive step to improve performance.\nFirst notice that one can readily check whether x = n is a trivial feasible solution. If it is yes, it must be optimal, because it has 0 cost, and the operator can return it as the optimal solution. This is the case if the segment from \u2206n = n \u2212 n\u2032 to \u2206n = n \u2212 n\u2032 does not intersect the sphere of radius r+r\u2032 centered at zero, which is equivalent to \u2016\u03b1\u2206n+ (1 \u2212 \u03b1)\u2206n\u2016 \u2265 r + r\u2032 with \u03b1 = max{1,min{0, \u03b1\u2032}} and \u03b1\u2032 = \u2206n\u2020(\u2206n\u2212\u2206n)/\u2016\u2206n\u2212\u2206n\u20162.\nThe second easy case is a shortcut to directly determine if the maximizer of max\u03b1\u2208[0,1] h(\u03b1) is either 0 or 1. We start by noting that empirically h has at most one extreme point in [0, 1] (the curious reader can convince him/herself of this by plotting h(\u03b1) for different values of \u2206n and \u2206n). This being the case, if \u22021h(0) > 0 and \u22021h(1) > 0 then \u03b1\u2217 = 1 and if \u22021h(0) < 0 and \u22021h(1) < 0 then \u03b1\u2217 = 0. Evaluating two derivatives of h is much easier than maximizing h and can save computation time. In particular, \u22021h(0) = C(\u2212(r + r\u2032) + \u2016\u2206n\u2016 + (\u2206n\u2020(\u2206n \u2212 \u2206n)/\u2016\u2206n\u2016)) and \u22021h(1) = C\n\u2032((r+ r\u2032)\u2212\u2016\u2206n\u2016+ (\u2206n\u2020(\u2206n\u2212\u2206n)/\u2016\u2206n\u2016)) for constants C,C \u2032 > 0.\nIf these cases do not hold, we cannot avoid maximizing (5), a scenario we denote as expensive. In Section 5 we profile how often each scenario occurs in practice and the corresponding gain in speed.\nLocal path planning The optimization problem (1) finds beginning-to-end collision-free paths for all agents simultaneously. This is called global path planning. It is also possible to solve path planning greedily by solving a sequence of small optimization problems, i.e. local path planning. Each of these problems plans the path of all agents for the next \u03c4 seconds such that, as a group, they get as close as possible to their final desired positions. This is done, for example, in Fiorini and Shiller (1998) and followup work (Alonso-Mora et al. 2012a; Alonso-Mora et al. 2013). The authors in Bento et al. (2013) solve these small optimization problems using a special case of the no-collision operator we study in Section 3 and show this approach is computationally competitive with the results in Alonso-Mora et al. (2013). Therefore, our results also extend this line of research on local path planning to arbitrary dimensions and improve solving-times even further. See Section 5 for details on these improvements in speed."}, {"heading": "4 Landmark proximal operator", "text": "In this section we introduce the concept of landmarks that, automatically and jointly, (i) produce reference points in space-time that, as a group, agents should try to visit, (ii) produce a good assignment between these reference points and the agents, and (iii) produce collision-free paths for the agents that are trying to visit points assigned to them.\nPoints (i) to (iii) are essential, for example, to formation control in multi-robot systems and autonomous surveillance or search (Bahceci, Soysal, and Sahin 2003), and are also related to the problem of assigning tasks to robots, if the tasks are seen as groups of points to visit (Michael et al.\n2008). Many works focus on only one of these points or treat them in isolation. One application where points (i) to (iii) are considered, although separately, is the problem of using color-changing robots as pixels in a display (Alonso-Mora et al. 2012b; Alonso-Mora et al. 2012c; Ratti and Frazzoli 2010). The pixel-robots arrangement is planned frame-byframe and does not automatically guarantee that the same image part is represented by the same robots across frames, creating visual artifacts. Our landmark formalism allows us to penalize these situations.\nWe introduce landmarks as extra terms in the objective function (1); we now explain how to compute their associated proximal operators. Consider a set of landmark trajectories {yj(s)}j\u2208[m],sinit\u2264s\u2264send and, to each trajectory j, assign a cost c\u0303j > 0, which is the cost of ignoring the entire landmark trajectory. In addition, to each landmark yj(s) \u2208 Rd that is assigned to an agent, assign a penalty cj(s) > 0 for deviating from yj(s). Landmark trajectories extend the objective function (1) by adding to it the following term\u2211\nj:\u03c3j 6=\u2217 send\u2211 s=sinit cj(s)\u2016x\u03c3j (s)\u2212 yj(s)\u2016 2 + \u2211 j:\u03c3j=\u2217 c\u0303j , (10)\nwhere the variable \u03c3j indicates which agent should follow trajectory j. If \u03c3j = \u2217, that means trajectory j is unassigned. Each trajectory can be assigned to at most one agent and vice-versa, which it must follow throughout its duration. So we have \u03c3j \u2208 [p] \u222a {\u2217} as well as the condition that if \u03c3j = \u03c3j\u2032 then either j = j\u2032 or \u03c3j = \u2217. We optimize the overall objective function over x and \u03c3. Note that it is not equally important to follow every point in the trajectory. For example, by setting some c\u2019s equal to zero we can effectively deal with trajectories of different lengths, different beginnings and ends, and even trajectories with holes. By setting some of the c\u2019s equal to infinity we impose that, if the trajectory is followed, it must be followed exactly. In (10) we use the Euclidean metric but other distances can be considered, even non-convex ones, as long as the resulting proximal operators are easy to compute. Finally, notice that, a priori, we do not need {yj(s)} to describe collision free trajectories. The other terms in the overall objective function will try to enforce no-collision constraints and additional dynamic constraints. Of course, if we try to satisfy an unreasonable set of path specifications, the ADMM or TWA might not converge.\nThe proximal operator associated to term (10) receives as input {ni(s)} and outputs {x\u2217i (s)} where i \u2208 [p], sinit \u2264 s \u2264 send and {x\u2217i (s)} minimizes\nmin x,\u03c3 \u2211 j:\u03c3j 6=\u2217 send\u2211 s=sinit cj(s)\u2016x\u03c3j (s)\u2212 yj(s)\u2016 2 + \u2211 j:\u03c3j=\u2217 c\u0303j\n+ p\u2211 i=1 send\u2211 s=sinit \u03c1i 2 \u2016xi(s)\u2212 ni(s)\u20162. (11)\nThe variables \u03c3\u2019s are used only internally in the computation of the proximal operator because they are not shared with other terms in the overall objective function. The above proximal operator can be efficiently computed as follows. We first optimize (11) over the x\u2019s as a function of \u03c3 and then we optimize the resulting expression over the \u03c3\u2019s. If\nwe optimize over the x\u2019s we obtain \u2211 j \u03c9j,\u03c3j where, if \u03c3j = \u2217, \u03c9j,\u2217 = c\u0303j and, if \u03c3j = i 6= \u2217, then \u03c9j,i = minx \u2211send s=sinit\ncj(s)\u2016xi(s) \u2212 yj\u20162 + \u03c1i2 \u2016xi(s) \u2212 ni(s)\u2016 2 =\u2211send\ns=sinit \u03c1icj(s) 2cj(s)+\u03c1i \u2016ni(s) \u2212 yj(s)\u20162. The last equality follows from solving a simple quadratic problem. We can optimize over the \u03c3\u2019s by solving a linear assignment problem with cost matrix \u03c9, which can be done, for example, using Hungarian method of Kuhn (1955), using more advanced methods such as those after Goldberg and Tarjan (1988), or using scalable but sub-optimal algorithms as in Bertsekas (1988). Once an optimal \u03c3\u2217 is found, the output of the operator can be computed as follows. If i is such that {j : \u03c3\u2217j=i}= \u2205 then x\u2217i (s) = ni(s) for all sinit \u2264 s \u2264 send and if i is such that i = \u03c3j for some j \u2208 [m] then x\u2217i (s) = (\u03c1ini(s)+2ci(s)yj(s))/(2ci(s)+\u03c1i) for all sinit \u2264 s \u2264 send.\nThe term (10) corresponds to a set of trajectories between break-points s = sinit and s = send for which the different agents must compete, that is, each agent can follow at most one trajectory. We might however want to allow an agent to be assigned to and cover multiple landmark trajectories. One immediate way of doing so is by adding more terms of the form (10) to the overall objective function such that the kth term has all its m(k) trajectories within the interval [s(k)init , s (k) end ], and different intervals for different k\u2019s are disjoint. However, just doing this does not allow us to impose a constraint like the following: \u201cthe jth trajectory in the set corresponding to the interval [s(k)init , s (k) end ] must be covered by the same agent as the the (j\u2032)th trajectory in the set corresponding to the interval [s(k+1)init , s (k+1) end ].\u201d To do so we need to impose the additional constraint that some of the \u03c3(k) variables across different terms of the form (10) are the same, e.g. in the previous example, \u03c3(s)j = \u03c3 (s+1) j\u2032 . Since the variables \u03c3\u2019s can now be shared across different terms, the proximal operator (11) needs to change. Now it receives as input a set of values {ni(s)}s,i and {n\u2032j}j and outputs a set of values {x\u2217i (s)}i,s and {\u03c3\u2217j }j that minimize\u2211 j:\u03c3j 6=\u2217 \u2211send s=sinit cj(s)\u2016x\u03c3j (s) \u2212 yj(s)\u20162 + \u2211 j:\u03c3j=\u2217 c\u0303j +\u2211\ni,s \u03c1i 2 \u2016xi(s)\u2212ni(s)\u2016 2 + \u2211m j=1 \u03c1\u2032j 2 \u2016\u03c3j\u2212n \u2032 j\u20162.\nIn the expression above, {\u03c3j}j and {n\u2032j}j are both vectors of length p + 1, where the last component encodes for no assignment and \u03c3j must be binary with only one 1 entry. For example, if p = 5 and \u03c32 = [0, 0, 1, 0, 0, 0] we mean that the second trajectory is assigned to the third agent, or if \u03c34 = [0, 0, 0, 0, 0, 1] we mean that the fourth trajectory is not assigned to any agent. However, n\u2032 can have real values and several nonzero components.\nWe also solve the problem above by first optimizing over x and then over \u03c3. Optimizing over x we obtain \u2211 j \u03c9\u0303j,\u03c3j , where \u03c9\u0303j,i = \u03c9j,i+ \u03c1\u2032j 2 \u2016[0, ...0, 1, 0, ..., 0]\u2212n \u2032 j\u20162 = \u03c9j,i+ \u03c1\u2032j 2 \u2016n \u2032 j\u20162+\u20161\u2212n \u2032(i) j \u20162\u2212\u2016n \u2032(i) j \u20162 = \u03c9j,i+ \u03c1\u2032j 2 \u2016n \u2032 j\u20162+1\u22122n \u2032(i) j . Given the cost matrix \u03c9\u0303, we find the optimal \u03c3\u2217 by solving a linear assignment problem. Given \u03c3\u2217, we compute the optimal x\u2217 using exactly the same expressions as for (11).\nFinally, to include constraints of the kind \u03c3(k)j = \u03c3 (k\u2032) j\u2032\nwe add to the objective a term that takes the value infinity whenever the constraint is violated and zero otherwise. This term is associated with a proximal operator that receives as input n\u2032j = (n \u2032(1) j , ..., n \u2032(n) j ) and n \u2032 j\u2032 = (n \u2032(1) j\u2032 , ..., n \u2032(n) j\u2032 ) and\noutputs (\u03c3\u2217j , \u03c3 \u2217 j\u2032) \u2208 arg min\u03c3j=\u03c3j\u2032 \u03c1j 2 \u2016\u03c3j\u2212n \u2032 j\u20162+\n\u03c1\u2032 j\u2032\n2 \u2016\u03c3j\u2032\u2212 n\u2032j\u2032\u20162. Again \u03c3j and \u03c3j\u2032 are binary vectors of length p + 1 with exactly one non-zero entry. The solution has the form \u03c3\u2217j = \u03c3 \u2217 j\u2032 = [0, 0, ..., 0, 1, 0, ...0] where the 1 is in position i\u2217 = arg maxi\u2208[p] \u03c1jn \u2032(i) j + \u03c1 \u2032 j\u2032n \u2032(i) j\u2032 ."}, {"heading": "5 Numerical experiments", "text": "We gathered all results with a Java implementation of the ADMM and the TWA as described in Bento et al. (2013; see Appendix C) using JDK7 and Ubuntu v12.04 run on a desktop machine with 2.4GHz cores.\nWe first compare the speed of the implementation of the collision operator as described in this paper, which we shall refer to as \u201cNEW,\u201d with the implementation described in Bento et al. (2013), which we denote \u201cOLD.\u201d We run the TWA using OLD on the 2D scenario called \u201cCONF1\u201d in Bento et al. (2013) with p = 8 agents of radius r = 0.918, equally spaced around a circle of radius R = 3, each required to exchange position with the corresponding antipodal agent (cf. Fig. 1-(a)). While running the TWA using OLD, we record the trace of all n variables input into the OLD operators. We compare the execution speed of OLD and NEW on this trace of inputs, after segmenting the n variables into trivial, easy, or expensive according to \u00a73. For global planning, the distribution of trivial, easy, expensive inputs is {0.814, 0.001, 0.185}. Although the expensive inputs are infrequent, the total wall-clock time that NEW takes to process them is 76 msec compared to 54 msec to process all trivial and easy inputs. By comparison, OLD takes a total time of 551 msec on the expensive inputs and so our new implementation yields an average speedup of 7.25\u00d7 on the inputs that are most expensive to process. Similarly, we collect the trace of the n variables input into the collision operator when using the local planning method described in Bento et al. (2013) on this same scenario. We observe a distribution of the trivial, easy, expensive inputs equal to {0.597, 0.121, 0.282}, we get a total time spent in the easy and trivial cases of 340 msec for NEW and a total time spent in the expensive cases of 2802 msec for NEW and 24157 msec for OLD. This is an average speedup of 8.62\u00d7 on the expensive inputs. For other scenarios, we observe similar speedup on the expensive inputs, although scenarios easier than CONF1 normally have fewer expensive inputs. E.g., if the initial and final positions are chosen at random instead of according to CONF1, this distribution is {0.968, 0, 0.032}.\nFigure 1-(b) shows the convergence time for instances of CONF1 in 3D (see Fig. 1-(a)) using NEW for a different number of agents using both the ADMM and the TWA. We recall that OLD cannot be applied to agents in 3D. Our results are similar to those in Bento et al. (2013) for 2D: (i) convergence time seems to grow polynomially with p; (ii) the TWA is faster than the ADMM; and, (iii) the proximal operators lend themselves to parallelism, and thus added\ncores decrease time (we see \u223c 2\u00d7 with 8 cores). In Figure 1-(c) we show that the paths found when the TWA solves CONF1 in 3D over 1000 random initializations are not very different and seem to be good (in terms of objective value).\nIn the supplementary movie we demonstrate the use of the landmark operators. First we show the use of these operators on six toy problems involving two agents and four landmark trajectories where we can use intuition to determine if the solutions found are good or bad. We solve these six scenarios using the ADMM with 100 different random initializations to avoid local minima and reliably find very good solutions. With 1 core it always takes less than 3 seconds to converge and typically less than 1 second. We also solve a more complex problem involving 10 agents and about 100 landmarks whose solution is a \u2018movie\u2019 where the different robots act as pixels. With our landmark operators we do not have to pre-assign the robots to the pixels in each frame."}, {"heading": "6 Conclusion", "text": "We introduced two novel proximal operators that allow the use of proximal algorithms to plan paths for agents in 3D, 4D, etc. and also to automatically assign waypoints to agents. The growing interest in coordinating large swarms of quadcopters in formation, for example, illustrates the importance of both extensions. For agents in 2D, our collision operator is substantially faster than its predecessor. In particular, it leads to an implementation of the velocity-obstacle local planning method that is faster than its implementation in both Alonso-Mora et al. (2013) and Bento et al. (2013). The impact of our work goes beyond path planning. We are currently working on two other projects that use our results. One is related to visual tracking of multiple non-colliding large objects and the other is related to the optimal design of layouts, such as for electronic circuits. In the first, the speed of the new no-collision operator is crucial to achieve realtime performance and in the second we apply Lemma 3 to derive no-collision operators for non-circular objects.\nThe proximal algorithms used can get stuck in local minima, although empirically we find good solutions even for hard instances with very few or no random re-initializations. Future work might explore improving robustness, possibly by adding a simple method to start the TWA or the ADMM from a \u2018good\u2019 initial point. Finally, it would be valuable to implement wall-agent collision proximal operators that are more general than what we describe in Section 3, perhaps by exploring other methods to solve SIP problems."}, {"heading": "Appendix for \u201cProximal Operators for Multi-Agent Path Planning\u201d", "text": ""}, {"heading": "A A comment on the impact of the assumption of piece-wise linear paths in practice", "text": "A direct application of the approach of Bento et al. (2013) can result in very large accelerations at the break-points. It is however not hard to overcome this apparent limitation in practical applications. We now explain one way of doing it. First notice that by increasing the number of break-points we can obtain trajectories arbitrarily close to smooth trajectories with finite acceleration everywhere. Since in practice it is not efficient to work with a very large number of breakpoints, we can keep the number of break-points at a reasonable level and increase the effective radius of the robots. This would allow us to fit a polynomial through the breakpoints and obtain smooth trajectories that are never distant from the piece-wise linear paths by more than the difference between the true robots radii and their effective radii. Using this approach, we would obtain a set of non-colliding finite-acceleration trajectories. We can also impose specific maximum-acceleration constrains if, at the same time, we restrict the maximum permitted change of velocity at breakpoints using additional proximal operators."}, {"heading": "B An illustration of the two kinds of blocks used by the ADMM/TWA", "text": "As explained in Section 2, the ADMM/TWA is an iterative scheme that alternates between (i) producing different estimates of the optimal value of the variables each function in the objective depends on and (ii) producing consensus values from the different estimates that pertain the same variable. The blocks that produce the estimates we call proximal operators and the blocks that produce consensus values we call consensus operators. The proximal operator blocks only receive messages from the consensus blocks (and send estimates back to them) and the consensus blocks only receive estimates from the proximal operators (and send consensus messages back to the proximal operators). Hence, the ADMM iteration scheme can be interpreted as messages passing back and forth along the edges of a bipartite graph.\nWe now illustrate this. Imagine that we want to compute non-colliding paths for two agents and that trajectories are parametrized by three break-points. In this case the optimization problem (1) has six variables, namely, x1(0), x1(1), x1(2) for agent 1 and x2(0), x2(1), x2(2) for agent 2. See Figure 2-(top).\nThe ADMM has one consensus operator associated with the position of each agent at each break-point (blue blocks with \u2018=\u2019 sign on it) and has one proximal operator associated with each function in the objective (red blocks with function names on it). In our small example, we have two no-collision operators. One ensures there are no-collisions between the segment connecting x1(0) and x1(1) and the segment connecting x2(0) and x2(1). The other acts similarly on x1(1), x1(2), x2(1) and x2(2). We also have four velocity operators that penalize trajectories in which the path segments\nhave large velocities. Finally, we have four position proximal operators that enforce agent 1 and 2 to start and finish their paths at specified locations. See Figure 2-(center).\nThe crucial part of implementing the ADMM or the TWA is the construction of the proximal operators. The proximal operators receive consensus messages n from the consensus nodes and produce estimates for the values of the variables of the function associated to them. For example, at each iteration, the left-most no-collision proximal operator in Figure 2-(center) receives messages n from the consensus nodes associated to the variables x1(0), x1(1), x2(0) and x2(1) and produces new estimates for their optimal value. The center-\ntop consensus operator receives fours estimates for x1(1), from two no-collision proximal operators and two velocity proximal operators, and produces a single estimate its optimal value. See Figure 2-(bottom)."}, {"heading": "C A comment on our implementation of the TWA", "text": "Implementing the TWA requires computing proximal operators and specifying, at every iteration, what Derbinsky et al. (2013) calls the outgoing weights, \u2212\u2192\u03c1 , of each proximal operator. In the TWA there are also incoming weights, \u2190\u2212\u03c1 , which correspond to the \u03c1\u2019s that appear in the definition of all our proximal operators, but their update scheme at every iteration is fixed (Derbinsky et al. 2013, Section 4.1).\nFor the collision operator and landmark operator we introduce in this paper, we compute the outgoing weights using the same principle as in (Bento et al. 2013). Namely, if an operator maps a set of input variables (n1, n2, ..., nk) to a set of output variables (x1, x2, ..., xk) then, if ni 6= xi we set \u2212\u2192\u03c1 i = 1 otherwise, when ni = xi, we set \u2212\u2192\u03c1 i = 0. In other words, if a variable is unchanged by the operators, the outgoing weight associated with it should be zero, otherwise it is 1."}, {"heading": "D More details about Remark 2", "text": "The next three points sketch why Remark 2 is true.\nFirst, if the \u03c1\u2019s are all positive, the objective function of problem (3) is strictly convex and we can add the constraint \u2016(x, x\u2032, x, x\u2032)\u2016 \u2264 M , for M large enough, without changing its minimum value. The new extended set of constraints amounts to the intersection of closed sets with a compact set and by the continuity of the objective function and the extreme value theorem it follows that the problem has a minimizer.\nSecond, problem (3) can be interpreted as resolving the following conflict. \u201cAgent 1 wants to move from n at time 0 to n at time 1 and agent 2 wants to move from n\u2032 at time 0 to n\u2032 at time 1, however, if both move in a straight-line, they collide. How can we minimally perturb their initial and final reference positions so they they avoid collision?\u201d If the vectors (n, 0), (n\u2032, 0), (n, 1),(n\u2032, 1) lie in the same threedimensional plane there can be ambiguity on how to minimally perturb the agents\u2019 initial and final positions: agent 1\u2019s reference positions can either move \u2018up\u2019 and agent 2\u2019s \u2018down\u2019 or agent 1\u2019s reference positions \u2018down\u2019 and agent 2\u2019s \u2018up\u2019 (\u2018up\u2019 and \u2018down\u2019 relative to the plane defined by the vectors (n, 0), (n\u2032, 0), (n, 1),(n\u2032, 1)). In numerical implementations however, it almost never happens that (n, 0), (n\u2032, 0), (n, 1),(n\u2032, 1) lie in the same plane and, in fact, this can be avoided by adding a very small amount of random noise to the n\u2019s before solving problem (3).\nThird, one can show from the continuity the objective function of problem (3) , the continuous-differentiability of g(x, \u03b1) = \u2016\u03b1(x \u2212 x\u2032) + (1 \u2212 \u03b1)(x \u2212 x\u2032)\u20162 \u2212 (r + r\u2032)2 and the fact that the level sets of g(x, \u03b1) as a function of x never have \u2018flat\u2019 sections that h(\u03b1) is a continuous function. Since [0, 1] is compact, it follows that there always exists an \u03b1\u2217. Also, for the purpose of a numerical implementation,\nwe can consider \u2016\u03b1\u2217(n\u2212 n\u2032) + (1\u2212 \u03b1\u2217)(n\u2212 n\u2032)\u2016 6= 0. In fact, this can be avoided by adding a very small amount of random noise to the n\u2019s before solving problem (4). Therefore, for practical purposes, we can consider that for each \u03b1\u2217 there exists a unique minimizer to problem (4). Finally, a careful inspection of (5) shows that if there exists \u03b1 such that h(\u03b1) > 0 then \u03b1\u2217 is unique and if h(\u03b1) = 0 for all \u03b1 then (6)-(9) always give x = n. In short, for the purpose of a numerical implementation, we always find a unique x\u2217(\u03b1\u2217) and because, in practice, as argued in the two points above, problem (3) can be considered to have only one solution, it follows that this unique x\u2217(\u03b1\u2217) is the unique minimizer of problem (3)."}, {"heading": "E Proof of Lemma 3", "text": "We need to consider two separate cases.\nIn the first case we assume that x\u2217(\u03b1\u2217) is such that g(x\u2217(\u03b1\u2217), \u03b1\u2217) > 0. This implies that x\u2217(\u03b1\u2217) is a minimizer of minx f(x), which implies that minx:g(x,\u03b1\u2217)\u22650 f(x) = minx f(x), which implies that minx:g(x,\u03b1)\u22650 f(x) \u2265 minx f(x) = minx:g(x,\u03b1\u2217)\u22650 f(x) = max\u03b1\u2032\u2208Aminx:g(x,\u03b1\u2032)\u22650 f(x) \u2265 minx:g(x,\u03b1)\u22650 f(x). In other words, minx:g(x,\u03b1)\u22650 f(x) = f(x\u2217(\u03b1\u2217)) for all \u03b1 \u2208 A. Since f(x) has a unique minimizer we have that x\u2217(\u03b1\u2217) must be feasible for the problem minx:g(x,\u03b1)\u22650 f(x), which implies that g(x\u2217(\u03b1\u2217), \u03b1) \u2265 0 for all \u03b1 \u2208 A. In other words, x\u2217(\u03b1\u2217) is feasible point of minx:g(x,\u03b1)\u22650\u2200\u03b1\u2208A f(x) and attains the smallest possible objective value, hence it minimizes it.\nIn the second case we assume that g(x\u2217(\u03b1\u2217), \u03b1\u2217) = 0. To finish the proof it suffices to show that v\u22022g(x\u2217(\u03b1\u2217), \u03b1\u2217) \u2265 0 for all v \u2208 R such that \u03b1\u2217 + v \u2208 A. To see this, we first notice that, if this is the case, then, for all \u03b1 \u2208 A, we have by convexity that g(x\u2217(\u03b1\u2217), \u03b1) \u2265 g(x\u2217(\u03b1\u2217), \u03b1\u2217) + (\u03b1 \u2212 \u03b1\u2217)\u22022g(x\u2217(\u03b1\u2217), \u03b1\u2217) = (\u03b1 \u2212 \u03b1\u2217)\u22022g(x\u2217(\u03b1\u2217), \u03b1\u2217) \u2265 0, which implies that x\u2217(\u03b1\u2217) is a feasible point for the problem minx:g(x,\u03b1)\u22650,\u2200\u03b1\u2208A f(x). Secondly, we notice that f(x\u2217(\u03b1\u2217)) \u2265 minx:g(x,\u03b1)\u22650,\u2200\u03b1\u2208A f(x) \u2265 max\u03b1\u2208Aminx:g(x,\u03b1)\u22650 f(x) = f(x\n\u2217(\u03b1\u2217)). This implies that x\u2217(\u03b1\u2217) minimizes minx:g(x,\u03b1)\u22650,\u2200\u03b1\u2208A f(x).\nWe now show that v\u22022g(x\u2217(\u03b1\u2217), \u03b1\u2217) \u2265 0 for all v \u2208 R such that \u03b1\u2217 + v \u2208 A.\nFirst we notice that since f is differentiable and x\u2217(\u03b1) exists and is differentiable at x\u2217(\u03b1\u2217), then, by the chain rule, h(\u03b1) is differentiable at \u03b1\u2217. In addition, we notice that if \u03b1\u2217 maximizes h then, if v \u2208 R is such that \u03b1\u2217 + v \u2208 A, the directional derivative of h in the direction of v evaluated at \u03b1\u2217 must be non-positive. In other words, v\u22021h(\u03b1 \u2217) = v\u22021f(x \u2217(\u03b1\u2217))\u2020\u22021x\n\u2217(\u03b1\u2217) \u2264 0. Second, we notice that in a small neighborhood around \u03b1\u2217, x\u2217(\u03b1) exists and is continuous (because x\u2217(\u03b1\u2217) is differentiable) which, by the continuous-differentiability of g and the fact that \u22021g(x\u2217(\u03b1\u2217), \u03b1\u2217) 6= 0, implies that \u22021g(x\u2217(\u03b1), \u03b1) 6= 0 in this neighborhood. Therefore, in a small neighborhood around \u03b1\u2217, the problem minx:g(x,\u03b1)\u22650 f(x) has a single inequality constraint and \u22021g(x\u2217(\u03b1), \u03b1) 6= 0 which implies that x\u2217(\u03b1), which we are assuming exists in this neighborhood, is a feasible regular point and satisfies the first-order\nnecessary optimality conditions (Bertsekas 1999)1\n\u22021f(x \u2217(\u03b1)) + \u03bb\u22021g(x \u2217(\u03b1), \u03b1) = 0, (12) \u03bbg(x\u2217(\u03b1), \u03b1) = 0, (13) g(x\u2217(\u03b1), \u03b1) \u2265 0, (14)\n\u03bb \u2264 0. (15)\nNow, we take the directional derivative of (14) with respect to \u03b1 in the direction v evaluated at (x\u2217(\u03b1\u2217), \u03b1\u2217) and obtain\nv\u22021g(x \u2217(\u03b1\u2217), \u03b1\u2217)\u2020\u22021x \u2217(\u03b1\u2217) + v\u22022g(x \u2217(\u03b1\u2217), \u03b1\u2217) \u2265 0.\n(16) At the same time, by computing the inner product between (12) and \u22021x\u2217 evaluated at \u03b1\u2217 and multiplying by v we obtain v\u22021f(x\u2217(\u03b1\u2217))\u2020\u22021x\u2217(\u03b1\u2217) + \u03bbv\u22021g(x \u2217(\u03b1\u2217), \u03b1\u2217)\u2020\u22021x \u2217(\u03b1\u2217) = 0. But we have already proved that v\u22021f(x\u2217(\u03b1\u2217))\u2020\u22021x\u2217(\u03b1\u2217) \u2264 0 therefore \u03bbv\u22021g(x \u2217(\u03b1\u2217), \u03b1\u2217)\u2020\u22021x \u2217(\u03b1\u2217) \u2265 0. Now recall that we are assuming g(x\u2217(\u03b1\u2217), \u03b1\u2217) = 0 therefore, \u03bb < 0. It thus follows that v\u22021g(x\u2217(\u03b1\u2217), \u03b1\u2217)\u2020\u22021x\u2217(\u03b1\u2217) \u2264 0 and from (16) we conclude that v\u22022g(x\u2217(\u03b1\u2217), \u03b1\u2217) \u2265 0."}, {"heading": "F Proof of Theorem 1", "text": "We first observe that the solutions to problem (3) and (4) remain the same if we replace \u2016\u03b1(x \u2212 x\u2032) + (1 \u2212 \u03b1)(x \u2212 x\u2032)\u2016 \u2265 r+r\u2032 by \u2016\u03b1(x\u2212x\u2032)+(1\u2212\u03b1)(x\u2212x\u2032)\u20162\u2212(r+r\u2032)2 \u2265 0. We prove the theorem with this replacement in mind.\nWe first prove the theorem assuming that we have proved that that expressions (5) to (9) hold when \u2016\u03b1(n\u2212n\u2032) + (1\u2212 \u03b1)(n\u2212 n\u2032)\u2016 6= 0. This is then proved last.\nTo prove the theorem we consider two separate cases. In the first case we consider r + r\u2032 \u2264 min\u03b1\u2208[0,1] \u2016\u03b1(n \u2212 n\u2032) + (1 \u2212 \u03b1)(n \u2212 n\u2032)\u2016. This implies that the minimum of problem (3) is 0 and, because the \u03c1\u2019s are positive, there is a unique minimizer which equals (n, n\u2032, n, n\u2032). In this case we also have h(\u03b1) = 0 for all \u03b1, which implies that the solution of problem (4) is x\u2217(\u03b1) = n for all \u03b1. Therefore, for any optimal \u03b1\u2217 for which \u2016\u03b1\u2217(n\u2212n\u2032)+(1\u2212\u03b1\u2217)(n\u2212n\u2032)\u2016 6= 0, we have that the minimizer of problem (4) is equal to the unique solution of problem (3). Hence the theorem is true in this case.\nNow we assume that r + r\u2032 > min\u03b1\u2208[0,1] \u2016\u03b1(n \u2212 n\u2032) + (1 \u2212 \u03b1)(n \u2212 n\u2032)\u2016. This implies that there exists an \u03b1 for which the right-hand-side of (5) is positive and for which \u2016\u03b1(n \u2212 n\u2032) + (1 \u2212 \u03b1)(n \u2212 n\u2032)\u2016 6= 0. This implies that, there exists an \u03b1 for which h(\u03b1) > 0. Therefore, for any optimal \u03b1\u2217 it must the case that h(\u03b1\u2217) > 0. If \u2016\u03b1\u2217(n \u2212 n\u2032) + (1 \u2212 \u03b1\u2217)(n \u2212 n\u2032)\u2016 6= 0 then (5) holds around a neighborhood of \u03b1\u2217 and in this neighborhood h(\u03b1) > 0. Therefore, in this neighborhood x\u2217(\u03b1) exists, is unique, and is differentiable. Now we notice that the objective function of problem (3) is continuously-differentiable, and that g(x, \u03b1) \u2261 \u2016\u03b1(x \u2212 x\u2032) + (1 \u2212 \u03b1)(x \u2212 x\u2032)\u20162 \u2212 (r + r\u2032)2 is continuously-differentiable in (x, \u03b1) and convex in \u03b1. If it is true that \u22021g(x\u2217(\u03b1\u2217), \u03b1\u2217) 6= 0 then we can apply Lemma 3 with A = [0, 1] and conclude that x\u2217(\u03b1\u2217) is also a solution\n1Also known as also known as Karush-Kuhn-Tucker (KKT) conditions.\nof problem (3). Hence the theorem will be true in this case as well.\nWe now show that in this case, indeed, \u22021g(x\n\u2217(\u03b1\u2217), \u03b1\u2217) 6= 0. First we notice that \u22021g = (\u2202xg, \u2202xg, \u2202x\u2032g, \u2202x\u2032g) = 2\u2016\u03b1\u2217(x\u2217 \u2212 x\u2217\u2032) + (1\u2212 \u03b1\u2217)(x\u2217 \u2212 x\u2217\u2032)\u2016(\u03b1\u2217, 1 \u2212 \u03b1\u2217,\u2212\u03b1\u2217,\u22121 + \u03b1\u2217). We now recall that, as explain above, if \u2016\u03b1\u2217(n \u2212 n\u2032) + (1 \u2212 \u03b1\u2217)(n \u2212 n\u2032)\u2016 6= 0 then h(\u03b1\u2217) > 0. Therefore, we conclude that r + r\u2032 = \u2016\u03b1\u2217(x\u2217 \u2212 x\u2217\u2032) + (1 \u2212 \u03b1\u2217)(x\u2217 \u2212 x\u2217\u2032)\u2016 because otherwise \u2016\u03b1\u2217(x\u2217 \u2212 x\u2217\u2032) + (1 \u2212 \u03b1\u2217)(x\u2217 \u2212 x\u2217\u2032)\u2016 < r + r\u2032 implies that the constraint of problem (4) for \u03b1 = \u03b1\u2217 is inactive which implies that the solution must be x = n with objective value 0 which contradicts the fact that h(\u03b1\u2217) > 0. Hence, \u22021g = (r + r\u2032)(\u03b1\u2217, 1 \u2212 \u03b1\u2217,\u2212\u03b1\u2217,\u22121 + \u03b1\u2217), which is always non-zero, and proves that the theorem is true in this case as well.\nFinally, we now prove that that expressions (5) to (9) hold when \u2016\u03b1(n \u2212 n\u2032) + (1 \u2212 \u03b1)(n \u2212 n\u2032)\u2016 6= 0. This amounts to a relatively long calculus computation and is written in Appendix G."}, {"heading": "G Computation of minimum value and", "text": "minimizer of problem (4)\nWe do not solve problem (4) but instead solve the equivalent (more smooth) problem\nmin x,x\u2032,x,x\u2032\n\u03c1 2 \u2016x\u2212 n\u20162 + \u03c1 2 \u2016x\u2212 n\u20162 (17)\n+ \u03c1\u2032\n2 \u2016x\u2032 \u2212 n\u2032\u20162 + \u03c1\n\u2032\n2 \u2016x\u2032 \u2212 n\u2032\u20162\ns.t. \u2016\u03b1(x\u2212 x\u2032) + (1\u2212 \u03b1)(x\u2212 x\u2032)\u20162 \u2212 (r + r\u2032)2 \u2265 0. To begin, we notice that if\n\u2016\u03b1(n\u2212 n\u2032) + (1\u2212 \u03b1)(n\u2212 n\u2032)\u2016 > (r + r\u2032) then the constraint is inactive and the minimizer of (17) is (n, n\u2032, n, n\u2032) with minimum value 0. At the same time, if \u2016\u03b1(n \u2212 n\u2032) + (1 \u2212 \u03b1)(n \u2212 n\u2032)\u2016 > (r + r\u2032) we have that h(\u03b1) = 0 and the minimizer obtained from (6)-(9) is also (n, n\u2032, n, n\u2032). Therefore, we only need to show that equations (5) and (6)-(9) hold in the case when the constraint is active, which corresponds to the case when \u2016\u03b1(n \u2212 n\u2032) + (1\u2212 \u03b1)(n\u2212 n\u2032)\u2016 \u2264 (r + r\u2032).\nTo do so, we first introduce a few block variables (written in boldface) and express the above problem in a shorter form. Namely, we define x = (x, x\u2032, x, x\u2032) \u2208 R4\u00d7d, n = (n, n\u2032, n, n\u2032) \u2208 R4\u00d7d and \u03b1 = (\u03b1,\u2212\u03b1, (1\u2212\u03b1),\u2212(1\u2212\u03b1)) \u2208 R4 and D = diag(\u03c1, \u03c1\u2032, \u03c1, \u03c1\u2032) \u2208 R4\u00d74, and, rewrite (17) as\nmin x\n1 2 tr{(x\u2212 n)\u2020D(x\u2212 n)}\ns.t. \u2016\u03b1\u2020x\u20162 \u2212 (r + r\u2032)2 \u2265 0. (18)\nThen we notice that it is necessary that the solutions to this problem are among the points that satisfy the KKT conditions. Namely, those points that satisfy\nD(x\u2212 n) + 2\u03b1v = 0 (19) v = \u03bb(\u03b1\u2020x) (20)\n\u2016v\u2016/|\u03bb = r + r\u2032 (21)\nwhere \u03bb 6= 0 is the Lagrange multiplier associated to the problem\u2019s constraint and is non-zero because we are assuming the constraint is active. In the rest of the proof we show that there are only two points that satisfy the KKT conditions and show that, between them, the one that corresponds to the global optimum satisfies (5) and (6)-(9).\nWe first write the two equations even more compactly as( 1 2D \u03b1 \u03b1\u2020 \u22121/\u03bb )( x v ) = ( 1 2Dn 0 ) . (22)\nWe claim that, if 1 + 2\u03bb\u03b1\u2020D\u22121\u03b1 6= 0, the inverse of the block matrix (\n1 2D \u03b1 \u03b1\u2020 \u22121/\u03bb\n) (23)\nis ( 2 ( D\u22121 \u2212 2D \u22121\u03bb\u03b1\u03b1\u2020D\u22121\n1+2\u03bb\u03b1\u2020D\u22121\u03b1\n) 2\u03bbD\u22121\u03b1\n1+2\u03bb\u03b1\u2020D\u22121\u03b1\n2\u03bb\u03b1\u2020D\u22121\n1+2\u03bb\u03b1\u2020D\u22121\u03b1 \u2212\u03bb 1+2\u03bb\u03b1\u2020D\u22121\u03b1\n) . (24)\nTo prove this we could use the formula for the inverse of a block matrix. Instead, and much more simply, we simply compute the product of (24) and (23) and show it equals the identify. It is immediate to see that the block diagonal entries of the resulting product are indeed identity matrices. Since both matrices are symmetric, all that is left to check is that one of the non-diagonal block entries is zero. Indeed,\n(\u03b1\u2020) ( 2 ( D\u22121 \u2212 2D \u22121\u03bb\u03b1\u03b1\u2020D\u22121\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1 )) + (\u22121/\u03bb) ( 2\u03bb\u03b1\u2020D\u22121\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1 ) = 2\u03b1\u2020D\u22121(1 + 2\u03bb\u03b1\u2020D\u22121\u03b1)\u2212 4\u03b1\u2020D\u22121\u03bb\u03b1\u03b1\u2020D\u22121 \u2212 2\u03b1\u2020D\u22121\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1\n= 4\u03bb\u03b1\u2020D\u22121\u03b1\u2020D\u22121\u03b1\u2212 4\u03b1\u2020D\u22121\u03bb\u03b1\u03b1\u2020D\u22121\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1\n= (\u03b1\u2020D\u22121\u03b1)(4\u03bb\u03b1\u2020D\u22121 \u2212 4\u03bb\u03b1\u2020D\u22121)\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1 = 0.\nWe now solve the linear system (22) by multiplying both sides by the inverse matrix (24) and, we conclude that\nv = 2\u03bb\u03b1\u2020D\u22121\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1\n( 1\n2 Dn\n) =\n\u03bb\u03b1\u2020n\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1 , (25)\nx = 2 ( D\u22121 \u2212 2D \u22121\u03bb\u03b1\u03b1\u2020D\u22121\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1\n)( 1\n2 Dn ) = n\u2212 2D \u22121\u03bb\u03b1\u03b1\u2020n\n1 + 2\u03bb\u03b1\u2020D\u22121\u03b1 . (26)\nUsing equation (26) we can express the objective value of (18) as\n1 2 tr{(x\u2212 n)\u2020D(x\u2212 n)} = tr{n\u2020\u03b1\u03b1\u2020\u03bbD\u22121(\u22122)D(\u22122)D\u22121\u03bb\u03b1\u03b1\u2020n}\n2(1 + 2\u03bb\u03b1\u2020D\u22121\u03b1)2\n= tr{n\u2020\u03b1\u03b1\u2020n}4\u03bb2(\u03b1\u2020D\u22121\u03b1)\n2(1 + 2\u03bb\u03b1\u2020D\u22121\u03b1)2\n= 2\u03bb2\u2016\u03b1\u2020n\u20162(\u03b1\u2020D\u22121\u03b1)\n(1 + 2\u03bb\u03b1\u2020D\u22121\u03b1)2 = 2\u2016v\u20162(\u03b1\u2020D\u22121\u03b1),\nwhere in the last equality we made use of (25). We now recall that from the third equation in the KKT conditions we have that \u2016v\u2016/\u03bb = r + r\u2032 and so we conclude that\n\u03bb = 1\n2(\u03b1\u2020D\u22121\u03b1)\n( \u22121\u00b1 \u2016\u03b1\n\u2020n\u2016 r + r\u2032\n) , (27)\nand therefore,\n1 2 tr{(x\u2212 n)\u2020D(x\u2212 n)}\n= 2(r + r\u2032)2 1\n4(\u03b1\u2020D\u22121\u03b1)2\n( \u22121\u00b1 \u2016\u03b1\n\u2020n\u2016 r + r\u2032\n)2 (\u03b1\u2020D\u22121\u03b1)\n= (r + r\u2032 \u00b1 \u2016\u03b1\u2020n\u2016)2\n2(\u03b1\u2020D\u22121\u03b1) . (28)\nSince we are seeking the global minimum, and since we are assuming that \u2016\u03b1\u2020n\u2016 = \u2016\u03b1(n\u2212 n\u2032) + (1\u2212 \u03b1)(n\u2212 n\u2032)\u2016 \u2264 (r + r\u2032) and hence the constraint is active, we conclude that\n1 2 tr{(x\u2217(\u03b1)\u2212 n)\u2020D(x\u2217(\u03b1)\u2212 n)} = (r + r\u2032 \u2212 \u2016\u03b1\u2020n\u2016)2\n2(\u03b1\u2020D\u22121\u03b1) =\n1 2 (h(\u03b1))2. (29)\nAbove we have used the fact that \u03b1\u2020n = \u03b1\u2206n+(1\u2212\u03b1)\u2206n and that \u03b1\u2020D\u22121\u03b1 = \u03b12/\u03c1\u02dc+ (1 \u2212 \u03b1)2/\u03c1\u0303. This proves that(5) is valid when \u2016\u03b1\u2020n\u2016 = \u2016\u03b1(n\u2212n\u2032)+(1\u2212\u03b1)(n\u2212n\u2032)\u2016 \u2264 (r+ r\u2032) as long as 1 + 2\u03bb\u03b1\u2020D\u22121\u03b1 = \u2016\u03b1\u2020n\u2016/(r+ r\u2032) 6= 0. Recall that we have already proved that (5) holds when when \u2016\u03b1\u2020n\u2016 = \u2016\u03b1(n\u2212 n\u2032) + (1\u2212 \u03b1)(n\u2212 n\u2032)\u2016 > (r + r\u2032).\nTo finish the proof we now prove the validity of (6)-(9) when \u2016\u03b1\u2020n\u2016 = \u2016\u03b1(n \u2212 n\u2032) + (1 \u2212 \u03b1)(n \u2212 n\u2032)\u2016 > (r + r\u2032). Recall that we have already proved their validity when \u2016\u03b1\u2020n\u2016 = \u2016\u03b1(n\u2212 n\u2032) + (1\u2212 \u03b1)(n\u2212 n\u2032)\u2016 \u2264 (r + r\u2032). Now notice that, when \u2016\u03b1\u2020n\u2016 = \u2016\u03b1(n\u2212n\u2032)+(1\u2212\u03b1)(n\u2212n\u2032)\u2016 > (r + r\u2032), we can write,\n\u03bb = 1\n2(\u03b1\u2020D\u22121\u03b1)\n( \u22121 + \u2016\u03b1\n\u2020n\u2016 r + r\u2032 ) = \u2212 h(\u03b1)\n2(r + r\u2032) \u221a \u03b1\u2020D\u22121\u03b1 . (30)\nIf we define \u03b3 = 2\u03bb 1+2\u03bb\u03b1\u2020D\u22121\u03b1 , we can write the following expression for x\u2217(\u03b1)\nx\u2217(\u03b1) = n\u2212 \u03b3D\u22121\u03b1\u03b1\u2020n, (31)\nwhich holds if \u2016\u03b1\u2020n\u2016 = \u2016\u03b1(n\u2212n\u2032)+(1\u2212\u03b1)(n\u2212n\u2032)\u2016 6= 0. With a little bit of algebra one can see that this is exactly the same expression as (6)-(9) and finish the proof."}], "references": [{"title": "Reciprocal collision avoidance for multiple car-like robots", "author": ["J. Alonso-Mora", "A. Breitenmoser", "P. Beardsley", "R. Siegwart"], "venue": "In Robotics and Automation, IEEE Intern. Conf", "citeRegEx": "Alonso.Mora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alonso.Mora et al\\.", "year": 2012}, {"title": "Image and animation display with multiple mobile robots", "author": ["J. Alonso-Mora", "A. Breitenmoser", "M. Rufli", "R. Siegwart", "P. Beardsley"], "venue": "The International Journal of Robotics Research 31(6):753\u2013773", "citeRegEx": "Alonso.Mora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alonso.Mora et al\\.", "year": 2012}, {"title": "Object and animation display with multiple aerial vehicles", "author": ["J. Alonso-Mora", "M. Schoch", "A. Breitenmoser", "R. Siegwart", "P. Beardsley"], "venue": "In Intelligent Robots and Systems, IEEE/RSJ Intern. Conf. on,", "citeRegEx": "Alonso.Mora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alonso.Mora et al\\.", "year": 2012}, {"title": "Collision avoidance for multiple agents with joint utility maximization", "author": ["J. Alonso-Mora", "M. Rufli", "R. Siegwart", "P. Beardsley"], "venue": "In Robotics and Automation, IEEE Intern. Conf", "citeRegEx": "2013", "shortCiteRegEx": "2013", "year": 2013}, {"title": "Metal layer assignment. US Patent 6,182,272", "author": ["A. Andreev", "I. Pavisic", "P. Raspopovic"], "venue": null, "citeRegEx": "2001", "shortCiteRegEx": "2001", "year": 2001}, {"title": "Generation of collision-free trajectories for a quadrocopter fleet: A sequential convex programming approach", "author": ["F. Augugliaro", "A.P. Schoellig", "R. D\u2019Andrea"], "venue": "In Intelligent Robots and Systems, IEEE/RSJ Intern. Conf. on,", "citeRegEx": "2012", "shortCiteRegEx": "2012", "year": 2012}, {"title": "A review: Pattern formation and adaptation in multi-robot systems", "author": ["E. Bahceci", "O. Soysal", "E. Sahin"], "venue": null, "citeRegEx": "2003", "shortCiteRegEx": "2003", "year": 2003}, {"title": "A message-passing algorithm for multiagent trajectory planning", "author": ["J. Bento", "N. Derbinsky", "J. Alonso-Mora", "J.S. Yedidia"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2013", "shortCiteRegEx": "2013", "year": 2013}, {"title": "The auction algorithm: A distributed relaxation method for the assignment problem", "author": ["D.P. Bertsekas"], "venue": "Annals of Operations Research", "citeRegEx": "1988", "shortCiteRegEx": "1988", "year": 1988}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning 3(1):1\u2013122", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": null, "citeRegEx": "2011", "shortCiteRegEx": "2011", "year": 2011}, {"title": "An improved three-weight message-passing algorithm. arXiv:1305.1961 [cs.AI", "author": ["N. Derbinsky", "J. Bento", "V. Elser", "J.S. Yedidia"], "venue": null, "citeRegEx": "2013", "shortCiteRegEx": "2013", "year": 2013}, {"title": "Motion planning in dynamic environments using velocity obstacles", "author": ["P. Fiorini", "Z. Shiller"], "venue": "The International Journal of Robotics Research", "citeRegEx": "1998", "shortCiteRegEx": "1998", "year": 1998}, {"title": "A new approach to the maximum-flow problem", "author": ["A.V. Goldberg", "R.E. Tarjan"], "venue": "Journal of the ACM (JACM)", "citeRegEx": "1988", "shortCiteRegEx": "1988", "year": 1988}, {"title": "On the complexity of motion planning for multiple independent objects; pspace-hardness of the \u201dwarehouseman\u2019s problem", "author": ["J.E. Hopcroft", "J.T. Schwartz", "M. Sharir"], "venue": "The International Journal of Robotics Research 3(4):76\u201388", "citeRegEx": "1984", "shortCiteRegEx": "1984", "year": 1984}, {"title": "Incremental sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": null, "citeRegEx": "2010", "shortCiteRegEx": "2010", "year": 2010}, {"title": "Randomized preprocessing of configuration for fast path planning", "author": ["L. Kavraki", "Latombe", "J.-C"], "venue": "In Robotics and Automation, IEEE Intern. Conf. on,", "citeRegEx": "1994", "shortCiteRegEx": "1994", "year": 1994}, {"title": "Probabilistic roadmaps for path planning in high-dimensional configuration spaces. Robotics and Automation, IEEE Transactions on 12(4):566\u2013580", "author": ["L.E. Kavraki", "P. Svestka", "J.-C. Latombe", "M.H. Overmars"], "venue": null, "citeRegEx": "1996", "shortCiteRegEx": "1996", "year": 1996}, {"title": "Self-collision detection and prevention for humanoid robots", "author": ["J. Kuffner", "K. Nishiwaki", "S. Kagami", "Y. Kuniyoshi", "M. Inaba", "H. Inoue"], "venue": "In Robotics and Automation,", "citeRegEx": "2002", "shortCiteRegEx": "2002", "year": 2002}, {"title": "The hungarian method for the assignment problem", "author": ["H.W. Kuhn"], "venue": "Naval Research Logistics Quarterly", "citeRegEx": "1955", "shortCiteRegEx": "1955", "year": 1955}, {"title": "Randomized kinodynamic planning", "author": ["S.M. LaValle", "J.J. Kuffner"], "venue": "The International Journal of Robotics Research", "citeRegEx": "2001", "shortCiteRegEx": "2001", "year": 2001}, {"title": "Mixed-integer quadratic program trajectory generation for heterogeneous quadrotor teams", "author": ["D. Mellinger", "A. Kushleyev", "V. Kumar"], "venue": "In Robotics and Automation, IEEE Intern. Conf. on,", "citeRegEx": "2012", "shortCiteRegEx": "2012", "year": 2012}, {"title": "Distributed multi-robot task assignment and formation control", "author": ["N. Michael", "M.M. Zavlanos", "V. Kumar", "G.J. Pappas"], "venue": "In Robotics and Automation, IEEE Intern. Conf. on,", "citeRegEx": "2008", "shortCiteRegEx": "2008", "year": 2008}, {"title": "Proximal algorithms. Foundations and Trends in Optimization 1(3):123\u2013231", "author": ["N. Parikh", "S. Boyd"], "venue": null, "citeRegEx": "2013", "shortCiteRegEx": "2013", "year": 2013}, {"title": "Complexity of the mover\u2019s problem and generalizations", "author": ["J.H. Reif"], "venue": "In IEEE Annual Symposium on Foundations of Computer Science,", "citeRegEx": "1979", "shortCiteRegEx": "1979", "year": 1979}, {"title": "The increasing cost tree search for optimal multiagent pathfinding", "author": ["G. Sharon", "R. Stern", "M. Goldenberg", "A. Felner"], "venue": "Artificial Intelligence", "citeRegEx": "2013", "shortCiteRegEx": "2013", "year": 2013}, {"title": "How to solve a semi-infinite optimization problem", "author": ["O. Stein"], "venue": "European Journal of Operational Research", "citeRegEx": "2012", "shortCiteRegEx": "2012", "year": 2012}, {"title": "Bounding duality gap for problems with separable objective", "author": ["M. Udell", "S. Boyd"], "venue": null, "citeRegEx": "2014", "shortCiteRegEx": "2014", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "(2013), which formulates multi-agent path planning as a large non-convex optimization problem and uses proximal algorithms to solve it. More specifically, the authors use the Alternating Direction Method of Multipliers (ADMM) and the variant introduced in Derbinsky et al. (2013) called the Three Weight Algorithm (TWA).", "startOffset": 1, "endOffset": 280}, {"referenceID": 3, "context": "A self-contained explanation about proximal algorithms is found in Parikh and Boyd (2013) and a good review on the ADMM is Boyd et al. (2011). In general the ADMM and the TWA are not guaranteed to converge for non-convex problems.", "startOffset": 84, "endOffset": 142}, {"referenceID": 3, "context": "A self-contained explanation about proximal algorithms is found in Parikh and Boyd (2013) and a good review on the ADMM is Boyd et al. (2011). In general the ADMM and the TWA are not guaranteed to converge for non-convex problems. There is some work on solving non-convex problems using proximal algorithms with guarantees (see Udell and Boyd 2014 and references in Parikh and Boyd 2013) but the settings considered are not applicable to the optimization problem at hand. Nonetheless, the empirical results in Bento et al. (2013) are very satisfactory.", "startOffset": 84, "endOffset": 530}, {"referenceID": 3, "context": "(2013) are the proximal operators that enforce no robot-robot collisions and no robot-wall collisions. These operators involve solving a nontrivial problem with an infinite number of constraints and a finite number of variables, also known as a semi-infinite programming problem (SIP). The authors solve this SIP problem only for robots moving in 2D by means of a mechanical analogy, which unfortunately excludes applications in 3D such as those involving fleets of unmanned aerial vehicles (UAVs) or autonomous underwater vehicles (AUVs). Another limitation of their work is that it does not allow robots to automatically select waypoint positions from a set of reference positions. This is required, for example, in problems involving robots in formations (Bahceci, Soysal, and Sahin 2003). In Bento et al. (2013), any reference position must be pre-assigned to a specific robot.", "startOffset": 1, "endOffset": 816}, {"referenceID": 0, "context": "The authors also re-implement the local path planning method of Alonso-Mora et al. (2013), based on velocity obstacles (Fiorini and Shiller 1998), by solving an optimization algorithm similar to (1).", "startOffset": 64, "endOffset": 90}, {"referenceID": 0, "context": "This is done, for example, in Fiorini and Shiller (1998) and followup work (Alonso-Mora et al. 2012a; Alonso-Mora et al. 2013). The authors in Bento et al. (2013) solve these small optimization problems using a special case of the no-collision operator we study in Section 3 and show this approach is computationally competitive with the results in Alonso-Mora et al.", "startOffset": 76, "endOffset": 163}, {"referenceID": 0, "context": "This is done, for example, in Fiorini and Shiller (1998) and followup work (Alonso-Mora et al. 2012a; Alonso-Mora et al. 2013). The authors in Bento et al. (2013) solve these small optimization problems using a special case of the no-collision operator we study in Section 3 and show this approach is computationally competitive with the results in Alonso-Mora et al. (2013). Therefore, our results also extend this line of research on local path planning to arbitrary dimensions and improve solving-times even further.", "startOffset": 76, "endOffset": 375}, {"referenceID": 16, "context": "We can optimize over the \u03c3\u2019s by solving a linear assignment problem with cost matrix \u03c9, which can be done, for example, using Hungarian method of Kuhn (1955), using more advanced methods such as those after Goldberg and Tarjan (1988), or using scalable but sub-optimal algorithms as in Bertsekas (1988).", "startOffset": 152, "endOffset": 234}, {"referenceID": 8, "context": "We can optimize over the \u03c3\u2019s by solving a linear assignment problem with cost matrix \u03c9, which can be done, for example, using Hungarian method of Kuhn (1955), using more advanced methods such as those after Goldberg and Tarjan (1988), or using scalable but sub-optimal algorithms as in Bertsekas (1988). Once an optimal \u03c3\u2217 is found, the output of the operator can be computed as follows.", "startOffset": 228, "endOffset": 303}, {"referenceID": 3, "context": "(2013), which we denote \u201cOLD.\u201d We run the TWA using OLD on the 2D scenario called \u201cCONF1\u201d in Bento et al. (2013) with p = 8 agents of radius r = 0.", "startOffset": 1, "endOffset": 113}, {"referenceID": 3, "context": "(2013), which we denote \u201cOLD.\u201d We run the TWA using OLD on the 2D scenario called \u201cCONF1\u201d in Bento et al. (2013) with p = 8 agents of radius r = 0.918, equally spaced around a circle of radius R = 3, each required to exchange position with the corresponding antipodal agent (cf. Fig. 1-(a)). While running the TWA using OLD, we record the trace of all n variables input into the OLD operators. We compare the execution speed of OLD and NEW on this trace of inputs, after segmenting the n variables into trivial, easy, or expensive according to \u00a73. For global planning, the distribution of trivial, easy, expensive inputs is {0.814, 0.001, 0.185}. Although the expensive inputs are infrequent, the total wall-clock time that NEW takes to process them is 76 msec compared to 54 msec to process all trivial and easy inputs. By comparison, OLD takes a total time of 551 msec on the expensive inputs and so our new implementation yields an average speedup of 7.25\u00d7 on the inputs that are most expensive to process. Similarly, we collect the trace of the n variables input into the collision operator when using the local planning method described in Bento et al. (2013) on this same scenario.", "startOffset": 1, "endOffset": 1165}, {"referenceID": 0, "context": "In particular, it leads to an implementation of the velocity-obstacle local planning method that is faster than its implementation in both Alonso-Mora et al. (2013) and Bento et al.", "startOffset": 139, "endOffset": 165}, {"referenceID": 0, "context": "In particular, it leads to an implementation of the velocity-obstacle local planning method that is faster than its implementation in both Alonso-Mora et al. (2013) and Bento et al. (2013). The impact of our work goes beyond path planning.", "startOffset": 139, "endOffset": 189}], "year": 2015, "abstractText": "We address the problem of planning collision-free paths for multiple agents using optimization methods known as proximal algorithms. Recently this approach was explored in Bento et al. (2013), which demonstrated its ease of parallelization and decentralization, the speed with which the algorithms generate good quality solutions, and its ability to incorporate different proximal operators, each ensuring that paths satisfy a desired property. Unfortunately, the operators derived only apply to paths in 2D and require that any intermediate waypoints we might want agents to follow be preassigned to specific agents, limiting their range of applicability. In this paper we resolve these limitations. We introduce new operators to deal with agents moving in arbitrary dimensions that are faster to compute than their 2D predecessors and we introduce landmarks, spacetime positions that are automatically assigned to the set of agents under different optimality criteria. Finally, we report the performance of the new operators in several numerical experiments.", "creator": "LaTeX with hyperref package"}}}