{"id": "1505.01072", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2015", "title": "Mining Measured Information from Text", "abstract": "jalrez We barbarense present an approach mofro to furling extract measured planning information from text (programas e. full-text g. , a varada 1370 jean-s\u00e9bastien degrees autolite C uncover melting obermaier point, a fieldworks BMI greater than 29. bridgman 9 66-59 kg / m ^ 2 ). shead Such extractions are colada critically semling important us3 across neachtain a wide antipode range pelasgus of blonds domains - especially those bwana involving dho search 7-18 and exploration preludio of over-reliance scientific and technical enthalpy documents. parr We first propose watterson a rule - based entity kalalau extractor to mine hujar measured ghanima quantities (ya'acov i. e. , block a specialise numeric value takefumi paired zaireans with seabrooks a organismal measurement unit ), t.i.p. which mid-1985 supports musayyib a meet vast and garcetti comprehensive 36.54 set xiaowen of raquel both kalika common refi and obscure measurement units. mandodari Our three-class method is highly breteau robust and volo can correctly 2,903 recover westcombe valid 2141 measured hupa quantities even abattoirs when bloomingdales significant errors 29-november are interstates introduced dunnes through brundtland the process siragusa of entrance/exit converting kingsize document avedon formats white-breasted like osaze PDF riyo to moner plain text. Next, we describe 242.8 an approach to extracting the expiated properties 200.1 being measured (herlovsen e. 1,913 g. , positionally the satterberg property \" luwan pixel pitch \" in marita the 38.91 phrase \" tocqueville a pixel lisin pitch as tackler high as rapp 352 {\\ bottecchia mu} radioprogramas m \" ). Finally, beaded we monch present MQSearch: the www.icasualties.org realization gapp of terminal a udomsirikul search engine with gediman full 100-plus support sallie for measured february/march information.", "histories": [["v1", "Tue, 5 May 2015 16:36:27 GMT  (76kb)", "http://arxiv.org/abs/1505.01072v1", "4 pages; 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15)"]], "COMMENTS": "4 pages; 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15)", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["arun s maiya", "dale visser", "rew wan"], "accepted": false, "id": "1505.01072"}, "pdf": {"name": "1505.01072.pdf", "metadata": {"source": "CRF", "title": "Mining Measured Information from Text", "authors": ["Arun S. Maiya", "Dale Visser", "Andrew Wan"], "emails": ["awan}@ida.org", "Permissions@acm.org."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n01 07\n2v 1\n[ cs\n.C L\n] 5\nM ay\n2 01\nCategories and Subject Descriptors I.2.7 [Artificial Intelligence]: Natural Language Processing\u2014Text Analysis; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval\u2014Search Process\nKeywords text mining, information retrieval, information extraction, measured quantities, numerical queries"}, {"heading": "1. INTRODUCTION AND MOTIVATION", "text": "Scientific and technical documents describe methods and results using measured quantities: a numeric value paired with a unit of measurement. Examples of text snippets containing such measured quantities include:\n\u2022 average gravity curvature \u03b6 = (1.3999\u00b10.003)\u00d710\u22125s\u22122m\u22121\n\u2022 12 \u25e6C melting point\n\u2022 distance from Earth to the Sun is 9.3\u00d7 107 miles\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR \u201915, August 9\u201313, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767789.\n\u2022 average responsivity as low as 6.2 pA/K\nNote that these measured quantities (e.g., 6.2 pA/K) are typically associated with a specificmeasured property (e.g., average responsivity). In this paper, we study ways in which to extract these kinds of measured information from documents.1 The mining of such information is critically important across many domains \u2014 especially those involving search and exploration of scientific and technical articles. For instance, an optics researcher may wish to know if the performance of Nd:YAG laser-pumped KTP parametric oscillators has ever been tested at wavelengths longer than 2.4 \u00b5m. Full-text search engines using inverted indexes allow ad hoc queries on terms such as \u201cKTP parametric oscillator\u201d, but the ability to further filter search results based on wavelengths greater than 2.4 \u00b5m is not typically supported. To accomplish this, one must first identify and extract valid measured quantities (e.g., 2.4 \u00b5m) in unstructured text and, then, identify and extract the properties being measured (e.g., wavelength). These extractions could then be stored in the index of a search engine in a way that supports subsequent document queries on measured information (e.g., faceted navigation, numeric range queries).\nSurprisingly, there is very little existing work on how best to realize this process. Lines of research most closely related to the present work include extracting numerical attributes (e.g., [1,4]), supporting numerical document queries (e.g., [5,12]), and formula identification (e.g., [7]). However, none of these existing works address the comprehensive extraction of and search for measured information in document data, as described above. Indeed, numerous challenges exist in such scenarios. Many widely-used, full-text search engines (e.g., Apache Solr) convert the original document format to plain text prior to indexing and storage \u2014 an extremely error-ridden process. For instance, in the extracted text, exponents are typically lost (e.g., 105 becomes 105, s\u22122 becomes s\u20132). Moreover, the conversion of some characters can be highly inconsistent and unpredictable. A simple minus sign can be converted to a range of different dash characters or even \u201cgarbage\u201d characters. The same is true for other symbols such as \u00b5, multiplication signs, and degree symbols. It is this inconsistent and error-ridden text, then, that is ultimately stored in the index of the search engine making it virtually impossible to adequately locate documents by measured quantities. Without the correct identification of measured quantities, it is virtually impossible to identify properties being measured, which are critical in efficiently\n1We definemeasured information as measured quantities and the measured properties to which they are associated.\nnavigating scientific and technical articles for state-of-theart information. In general, there is a great deal of heterogeneity in how measured quantities and measured properties appear in text \u2013 both naturally and through corruption. This, then, motivates the current investigation of how best to extract such information.\nRecent studies [3, 6] have revealed that rule-based approaches to information extraction tend to be more effective, interpretable, and customizable than state-of-the-art machine learning approaches. We employ rule-based extraction methods in this work. Our contributions are as follows:\n\u2022 We propose and describe a rule-based entity extractor to identify measured quantities in unstructured text documents. Our method includes an error-correcting procedure that recovers from aforementioned text conversion errors by 1) reverse engineering the corrupted and mangled measured quantities back to their original, correct form and 2) standardizing this form for storage in an inverted index and subsequent query processing.\n\u2022 Using these extracted measured quantities, we show how to further extract the measured properties to which they are associated.\n\u2022 Finally, we present MQSearch: the realization of a search engine with full support for measured information. MQSearch is a facet-based navigation system that allows users to navigate large document sets based on measured quantities, measured properties, and the topics and themes to which they are associated. To the best of our knowledge, no other search engine in existence fully supports such a capability.\nWe begin with describing the extraction of measured quantities."}, {"heading": "2. MEASURED QUANTITIES", "text": "We view measured quantities as a 5-tuple of the form: (sign, number, error, scientific notation, units), where underlined elements are mandatory and others are optional. As an example, a team of researchers in Italy recently reported the first direct measurement of gravity\u2019s curvature as (1.3999 \u00b1 0.003) \u00d7 10\u22125s\u22122m\u22121 [10]. The corresponding 5-tuple representation of this2 is:\n(<empty>, 1.3999, 0.003, 10\u22125, s\u22122m\u22121).\n5-tuples such as this are populated using a series of extraction rules that operate on individual sentences. These rules fall into four broad categories: 1) pre-processing, 2) units, 3) quantities, and 4) post-processing. Simplified forms of some of the rules for units and quantities are shown in Table 1.3 We refer to the algorithm implementing such rules as Measured Quantity Extractor or MQE. We begin with pre-processing rules.\nPre-Processing. As mentioned previously, when extracting text from various document formats (e.g., PDF, MS Office), characters often appear inconsistently. Minus signs,\n2Since there is no explicit sign in this example, the first element is left empty. 3Rules are shown in Perl-like syntax, the de facto standard for regular expressions.\nmultiplication signs (e.g., \u00d7, \u00b7), equal-like symbols (e.g., \u2248, \u2243, \u223c=), degree symbols, and the \u00b5 character can appear in a variety of ways or, in some cases, as \u201cgarbage\u201d characters. For instance, minus can appear as the en dash character or appear corrupted as a\u0302e. Pre-processing rules identify these variations in text and perform the necessary normalization for accurate extraction of units and quantities.\nUnits. A measurement unit preceded by a numeric string conforming to the 5-tuple structure is the base indicator of a measured quantity. Thus, to identify valid measured quantities, we require a comprehensive ontology of units. We obtained an initial units ontology from the OBO Foundry,4 but this was quite incomplete. We, then, expanded the ontology using largely public sources (e.g., convert-me.com, DoD technical reports, Physical Review, Nature Communications). Each unit has an associated rule. An example rule for m (i.e., symbol for meters) is shown in Rule 5 of Table 1. Note that such rules include optional prefixes for submultiples and multiples (e.g., \u00b5 before m, kilo before meter). Unit rules, when combined with pre-processing rules described previously, can accurately extract units under a range of noisy conditions. For instance, the corrupted unit ma\u0302e1 is correctly recovered as m\u22121 by MQE. Finally, as shown in Rules 6 and 7, compound units are also supported (e.g., km/h, kilometer per hour, s\u22122 \u00b7m\u22121).\nQuantities. Like units, quantities (i.e., numbers with optional error ranges and scientific notation) can appear in a range of ways due to both corruptions and natural variation. These variations are collectively captured by rules such as those shown in Table 1 (i.e., Rules 1-4), which populate the remainder of the 5-tuple structure. As shown in Table 1, such rules capture a wide range of quantity formats (e.g., 10, 000 with a comma, 1.3999\u00b10.003\u00d710\u22125 with both an error range and scientific notation, 1.23\u00d7105 with lost exponent in 105). To support numeric range queries, extracted quantities are standardized prior to storage in a search engine index (e.g., the extracted quantity 1.3999\u00b10.003\u00d710\u22125 is stored simply as 0.000013999) [11].\nPost-Processing. We have already seen that text extracted from various document formats can be noisy. For instance, information from tables, headers, and figures can sometimes result in seemingly random sequences of numbers and letters in extracted text. In some cases, such information can erroneously be picked up by aforementioned rules as measured quantities. This is especially true for single letter units (e.g., m for meters, A for Ampere, etc.). Post-processing rules are employed to reject such extractions and minimize false positives. Examples of such rejection rules include context-based rules (e.g., reject when preceded by \u201cTable\u201d or \u201cFigure\u201d), repetition-based rules such as rejecting compound units consisting of repeated single letter units (e.g., 3 AJmm), and allowing a dash only between certain quantities and units (e.g., 10-cm is okay but not 10-A).\nAs we will show in Section 4, when used in combination, these rules collectively enable highly accurate extractions of measured quantities \u2013 which, in turn, can be exploited to extract the properties being measured, as described next.\n4http://www.obofoundry.org/"}, {"heading": "3. MEASURED PROPERTIES", "text": "We now turn our attention to the extraction of measured properties. To better illustrate the problem, we show several example snippets containing measured quantities. In each example, the measured quantity is shown in blue, the property being measured is highlighted in red, and the characters connecting them are underlined:\n\u2022 a pixel pitch as high as roughly 352 \u00b5m\n\u2022 a 352 \u00b5m pixel pitch\n\u2022 The pixel pitch employed was 352 \u00b5m.\n\u2022 average gravity curvature \u03b6 =(1.3999\u00b10.003)\u00d710\u22125s\u22122m\u22121\n\u2022 with 50mL of 30% fuming sulfuric acid\n\u2022 size \u223c= 0.1m2\n\u2022 frequency of longitudinal scan was approximately 300 Hz.\n\u2022 a nominal current density of 1.3 A/cm2 to 0.03 A/cm2\n\u2022 panel strength lower than 8.90 ksi (61.4 MPa)\n\u2022 wavelengths at least 2.4 \u00b5m\n\u2022 large fields of about, or above 10 kV/cm\nFrom just the examples shown, it is easy to see that there is an extremely high degree of variability in the words connecting ameasured property with ameasured quantity. These examples represent just a small sample of the many possible variations. However, upon closer inspection, we find that this variability can be reduced to a small number of syntactic patterns based on part-of-speech (POS) that capture most scenarios. Table 2 shows some syntactic patterns that we employ to extract measured properties. We refer to the extractor applying such syntactic rules as Measured Property Extractor or MPE.\nIn Table 2, noun phrases shown in red (i.e., NP) are extracted and taken as the measured property. Measured quantities are represented in blue by mq. The EQ tag represents all symbols related to \u2019=\u2019 (e.g., \u2248, \u2243). The SYM tag matches one or two character symbols (e.g., a greek letter). Other symbols (e.g., JJ, RB, IN, CC, VP) are part-of-speech tags in Penn Treebank format. Note that tags such as RB (i.e., an adverb) should be taken to include variations such as the comparative and superlative forms. This is not explicitly shown for reasons of brevity. This small set of patterns matches a very wide range of possible phrase combinations\nfor measured properties and are executed sequentially in the order shown. We implemented MPE using the Brill partof-speech tagger [2]. As we will show in the next section, the accuracy with which our algorithms are able to extract measured properties and measured quantities is remarkable \u2014 especially given the aforementioned issues with noisy and corrupted input text."}, {"heading": "4. EXPERIMENTAL EVALUATION", "text": "Since our research is sponsored by the U.S. Department of Defense (DoD), we evaluate our approach on a text corpus consisting of 40,807 unclassified research reports published in the 2008-2010 time frame and hosted by the Defense Technical Information Center (DTIC). This rich collection describes a wide range of research funded by the DoD spanning numerous fields from engineering and physical science to biomedical research and social science. The DTIC documents considered in this paper have been approved for public release and unlimited distribution. All documents are in PDF format, and text was extracted from them using the pdftotext utility.5 From this collection, we generated samples using the following procedure. To evaluate the ability of MQE to extract measured quantities, we sampled uniform random sentences from the population of all sentences containing a numeric value. By examining sentences with a number (but not necessarily a measurement unit), we are able to accurately identify false negatives in addition to false positives. Next, to evaluate the ability of MPE to extract measured properties, we generated a random sample of sentences from the population of all sentences containing a measured quantity, as identified by MQE. We employed sample sizes of 1000 and 500 for MQE and MPE, respectively. This produced sufficient 95% confidence bounds on our estimates for precision and recall over the entire corpus. Different fields employ different measures in different ways. By considering sentences sampled randomly in this fashion, we are able to evaluate our methods on text data that capture the diverse ways in which measured information is reported across different fields. To the best of our knowledge, no other approaches exist for extracting such measured information from scientific and technical documents. Thus, there are no appropriate baselines against which our meth-\n5http://www.foolabs.com/xpdf/home.html\nods can be compared. Table 3 shows the precision and recall estimates for both the measured quantity extractor and the measured property extractor over the entire corpus.\nAs can be seen in the table, both MQE and MPE perform extraordinarily well in extracting measured quantities and the properties they describe from documents across disparate fields. Having demonstrated the success with which measured information can be mined, we now demonstrate how these extractions can be exploited in novel search applications."}, {"heading": "5. AN APPLICATION: MQSEARCH", "text": "Here, we present MQSearch: a realization of a search engine with full support formeasured information. MQSearch is implemented using Apache Solr6 and AJAX Solr7, both of which support full-text search, faceted navigation, and numeric range queries. During the process of indexing and ingesting the DTIC document set into our search engine, we apply our extractors to encountered text and store bothmeasured quantities andmeasured properties in the search engine index. In addition, the search engine performs keyphrase extraction on documents using the KERA algorithm described in [8]. Using Solr filter queries, extracted keyphrases can be used to produce a tag cloud for any subset of the document set. Figure 1 shows the faceted navigation panel of MQSearch, which allows users to filter documents based on discovered measurement units, quantity ranges, and measured properties. In Figure 1, the measurement unit U/mL is selected. We see that there are 153 documents (out of roughly 40,000) mentioning this unit with quantities ranging from 0.001 U/mL to 10, 000 U/mL. The property most frequently measured in U/mL is penicillin. From the tag cloud, we see that documents containing quantities measured in U/mL tend to cover topics such as breast cancer and prostate cancer research.8 The search results can be filtered further along any of these dimensions. Filtering by LDA-discovered topics is also supported but not shown in the figure [9]. To the best of our knowledge, ours is the first search engine with such support for measured information."}, {"heading": "6. CONCLUSION", "text": "In this paper, we have proposed a demonstrably effective approach to extracting measured information from unstructured text data. We showed both how to extract measured quantities and the properties being measured. We further demonstrated how such extractions might be used in a search engine for documents rich in measured information. To the best of our knowledge, no other search engine in existence supports such functionality. Our extraction methods\n6http://lucene.apache.org/solr/ 7https://github.com/evolvingweb/ajax-solr 8This cancer research was funded by U.S. Army MRMC through a congressionally directed research program.\nhave the potential to substantially improve search, navigation, and exploratory analysis of large or even massive collections of scientific and technical articles. For future work, we plan on marrying our proposed approaches with other well-studied techniques for exploratory search."}, {"heading": "7. REFERENCES", "text": "[1] A. Bakalov, A. Fuxman, P. P. Talukdar, and S. Chakrabarti.\nScad: collective discovery of attribute values. In WWW \u201911.\n[2] E. Brill. A Simple Rule-based Part of Speech Tagger. In ANLC \u201992. [3] L. Chiticariu, Y. Li, and F. R. Reiss. Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems! In EMNLP \u201913. [4] D. Davidov and A. Rappoport. Extraction and Approximation of Numerical Attributes from the Web. In ACL \u201910. [5] M. Fontoura, R. Lempel, R. Qi, and J. Zien. Inverted Index Support for Numeric Search. Internet Math., 3(2):153\u2013186, 2006. [6] S. Gupta and C. Manning. SPIED: Stanford Pattern based Information Extraction and Diagnostics. In Proc. 2014 Workshop on Interactive Language Learning, Visualization, and Interfaces. [7] X. Lin, L. Gao, Z. Tang, X. Lin, and X. Hu. Mathematical Formula Identification in PDF Documents. In ICDAR \u201911. [8] A. S. Maiya, J. P. Thompson, F. L. Lemos, and R. M. Rolfe. Exploratory Analysis of Highly Heterogeneous Document Collections. In KDD \u201913. [9] A. K. McCallum. MALLET: A Machine Learning for Language Toolkit, 2002.\n[10] G. Rosi, L. Cacciapuoti, F. Sorrentino, M. Menchetti, M. Prevedelli, and G. M. Tino. Measurement of the Gravity-Field Curvature by Atom Interferometry. Physical Review Letters, 114(1), Jan. 2015. [11] U. Schindler and M. Diepenbroek. Generic XML-based Framework for Metadata Portals. Comput. Geosci., 34(12):1947\u20131955, Dec. 2008. [12] H. Seidl, F. I. Informatik, T. Schwentick, and A. Muscholl. Numerical Document Queries. In PODS 2003. ACM, June 2003."}], "references": [{"title": "Inverted Index Support for Numeric Search", "author": ["M. Fontoura", "R. Lempel", "R. Qi", "J. Zien"], "venue": "Internet Math.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "SPIED: Stanford Pattern based Information Extraction and Diagnostics", "author": ["S. Gupta", "C. Manning"], "venue": "In Proc. 2014 Workshop on Interactive Language Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "MALLET: A Machine Learning for Language Toolkit", "author": ["A.K. McCallum"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Measurement of the Gravity-Field Curvature by Atom Interferometry", "author": ["G. Rosi", "L. Cacciapuoti", "F. Sorrentino", "M. Menchetti", "M. Prevedelli", "G.M. Tino"], "venue": "Physical Review Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Generic XML-based Framework for Metadata Portals", "author": ["U. Schindler", "M. Diepenbroek"], "venue": "Comput. Geosci.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Numerical Document Queries", "author": ["H. Seidl", "F.I. Informatik", "T. Schwentick", "A. Muscholl"], "venue": "In PODS 2003. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": ", [5,12]), and formula identification (e.", "startOffset": 2, "endOffset": 8}, {"referenceID": 5, "context": ", [5,12]), and formula identification (e.", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "Recent studies [3, 6] have revealed that rule-based approaches to information extraction tend to be more effective, interpretable, and customizable than state-of-the-art machine learning approaches.", "startOffset": 15, "endOffset": 21}, {"referenceID": 3, "context": "003) \u00d7 10sm [10].", "startOffset": 12, "endOffset": 16}, {"referenceID": 4, "context": "000013999) [11].", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": ", [fpn\u03bcmcdk]?m([\\\u02c6]?[2-6] | [\\-][1-6]) \u2014 m normalized to m\u02c6# \u03bcm, m\u20131 (m), cm2 (cm), cm\u02c62", "startOffset": 20, "endOffset": 25}, {"referenceID": 1, "context": ", [fpn\u03bcmcdk]?m([\\\u02c6]?[2-6] | [\\-][1-6]) \u2014 m normalized to m\u02c6# \u03bcm, m\u20131 (m), cm2 (cm), cm\u02c62", "startOffset": 20, "endOffset": 25}, {"referenceID": 0, "context": ", [fpn\u03bcmcdk]?m([\\\u02c6]?[2-6] | [\\-][1-6]) \u2014 m normalized to m\u02c6# \u03bcm, m\u20131 (m), cm2 (cm), cm\u02c62", "startOffset": 32, "endOffset": 37}, {"referenceID": 1, "context": ", [fpn\u03bcmcdk]?m([\\\u02c6]?[2-6] | [\\-][1-6]) \u2014 m normalized to m\u02c6# \u03bcm, m\u20131 (m), cm2 (cm), cm\u02c62", "startOffset": 32, "endOffset": 37}, {"referenceID": 2, "context": "Filtering by LDA-discovered topics is also supported but not shown in the figure [9].", "startOffset": 81, "endOffset": 84}], "year": 2015, "abstractText": "We present an approach to extract measured information from text (e.g., a 1370 \u25e6C melting point, a BMI greater than 29.9 kg/m). Such extractions are critically important across a wide range of domains \u2014 especially those involving search and exploration of scientific and technical documents. We first propose a rule-based entity extractor to mine measured quantities (i.e., a numeric value paired with a measurement unit), which supports a vast and comprehensive set of both common and obscure measurement units. Our method is highly robust and can correctly recover valid measured quantities even when significant errors are introduced through the process of converting document formats like PDF to plain text. Next, we describe an approach to extracting the properties being measured (e.g., the property pixel pitch in the phrase \u201ca pixel pitch as high as 352 \u03bcm\u201d). Finally, we present MQSearch: the realization of a search engine with full support for measured information.", "creator": "LaTeX with hyperref package"}}}