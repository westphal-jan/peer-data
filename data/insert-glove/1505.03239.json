{"id": "1505.03239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2015", "title": "Feature selection using Fisher's ratio technique for automatic speech recognition", "abstract": "eustis Automatic Speech 410th Recognition 602,000 involves countersuit mainly hcs two steps; feature lopardo extraction and ricardo classification. Mel Frequency 2-pointer Cepstral wiggled Coefficient is fffff used kimba as catholic one brignone of the dadullah prominent saigo feature extraction nemr techniques g-4 in ASR. northerns Usually, gladiolus the cobbins set of urban all 12 MFCC i-qula coefficients is loots used as janakpur the feature vector mentewab in the regina classification step. autorities But velikov the togas question is dismemberment whether alley-oop the thierer same geter or improved numancia classification accuracy can fanni be sundeen achieved obersturmbannf\u00fchrer by using a reassessments subset rahardjo of 12 MFCC wgr as badiane feature upazilla vector. In this paper, isidore Fisher ' sestroretsk s ratio heyburn technique is lockey used for hautamaeki selecting burgomaster a subset manjunatha of 10/22 12 MFCC tingitana coefficients bipartition that exportadora contribute more jumonville in kazuhito discriminating 23.79 a pattern. wardrobes The selected coefficients angraecum are unbalance used in classification with biopiracy Hidden Markov Model hynde algorithm. non-executive The asialink classification muslin accuracies that we upsilon get by using 12 coefficients and menora by using biomanufacturing the selected coefficients are compared.", "histories": [["v1", "Wed, 13 May 2015 04:50:27 GMT  (225kb)", "http://arxiv.org/abs/1505.03239v1", "in International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 4, No. 2, April 2015"]], "COMMENTS": "in International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 4, No. 2, April 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sarika hegde", "k k achary", "surendra shetty"], "accepted": false, "id": "1505.03239"}, "pdf": {"name": "1505.03239.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sarika Hegde", "K. K. Achary", "Surendra Shetty"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121/ijci.2015.4204 45\nAutomatic Speech Recognition (ASR) involves mainly two steps; feature extraction and classification (pattern recognition). Mel Frequency Cepstral Coefficient (MFCC) is used as one of the prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC coefficients is used as the feature vector in the classification step. But the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 MFCC as feature vector. In this paper, Fisher\u2019s ratio technique is used for selecting a subset of 12 MFCC coefficients that contribute more in discriminating a pattern. The selected coefficients are used in classification with Hidden Markov Model (HMM) algorithm. The classification accuracies that we get by using 12 coefficients and by using the selected coefficients are compared."}, {"heading": "KEYWORDS", "text": ""}, {"heading": "Automatic Speech Recognition, Statistical Technique, Fisher\u2019s Ratio, Hidden Markov Model, Mel Frequency Cepstral Coeffecients (MFCC), Classification", "text": ""}, {"heading": "1. INTRODUCTION", "text": "Feature extraction is one of the important steps in any pattern recognition task. The efficient feature extraction technique extracts the feature which is able to discriminate one pattern from another accurately. There are many feature extraction techniques available for representing speech sound pattern in Automatic Speech Recognition (ASR) task. Mel Frequency Cepstral Coefficients (MFCC) and Linear Predictive Coefficients (LPC) have been identified as the most successful feature representation for the speech signal [1]. But recently, MFCC gained more importance than LPC due to its high discriminating capability [2]. MFCC is one of the signal processing techniques when applied on speech sound, extract the 12 coefficients. This set of 12 MFCC coefficients is used as a feature vector in classification task.\nIf the feature vector is of high dimension, it is reduced to lower dimensional subspace using the techniques like Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). These techniques are also known as dimensionality reduction techniques. Dimensionality reduction of the features is done, so that the computational cost and system complexity for subsequent processing can be decreased. Linear Discriminant Analysis [3][4][5] finds a linear transform by maximizing the Fisher\u2019s ratio and is found to be good in discriminating patterns.\nPCA technique computes the feature vector subspace with maximum variance [6]. These techniques are also used successfully in classification with HMM [7][8][9].\nUsually the above techniques are applied when the dimension is very large and when the features are computed using more than one feature extraction techniques. But, while using MFCC features for ASR, usually all the 12 coefficients are considered in the feature vector. There is no clear guideline in literature that whether the subset of 12 coefficients can be used as feature vector instead of using all the 12 MFCC. Rarely, one can find a work where, a specific technique is used to select the subset of 12 MFCC coefficients for classification. One can find that the Fishers ratio (F-ratio) technique is used in Linear Discriminant Analysis (LDA) as computing the linear combination of feature vectors which maximizes the F-ratio value. It is also sometimes used to compare the two feature extraction technique for their capability of discriminating patterns in classification [2][10][11]. F-ratio technique is used in this work for selecting the subset of MFCC coefficients best suitable for classification.\nThe recorded audio sounds of five vowels in Kannada language is considered as dataset. The objective is to apply MFCC feature extraction technique on this dataset and classify using Hidden Markov Model (HMM) classification algorithm. It is analyzed, whether the 12 coefficients can be replaced by a subset of coefficients as feature vector for classification without degrading the classification accuracy. Using Fisher\u2019s ratio (F-ratio) as the criterion for selecting the subset of MFCC coefficients, computational experiment has been carried out for classification of vowels. F- ratio uses between vowel class variance and the within vowel class variance for each coefficients and thus reflects the capability of an MFCC coefficient in discriminating one vowel class with others.\nThe organization of the rest of the paper is as follows. The second section describes MFCC feature extraction technique, Fisher\u2019s ratio technique and HMM classification technique. In the third section, the results of the experiments are discussed and analyzed. The last section concludes the work and highlights the future work also."}, {"heading": "2. METHODOLOGY", "text": "In this section, the techniques used for feature extraction, F-ratio calculation and classification are explained."}, {"heading": "2.1. Mel Frequency Cepstral Coefficient", "text": "The Mel Frequency Cepstral Coefficient (MFCC) is a feature extracted by applying more than one Fourier Transform sequentially to the original signal [6][12]. The first step is preprocessing which consists of framing and windowing of the signal. Framing is the process of breaking the set of sample observations of an entire audio file into smaller chunks called as frame [13]. We have used a frame size of 30ms which is normally used in speech recognition applications. For each of\nthe frame if of size N, DFT coefficients are calculated by applying the following equation.\n(1)\nThe resulting value ][miX is a complex number and the power spectrum for this is computed as,\n, (2)\nThe power spectrum is then transformed to mel frequency scale by using a filter bank consisting of triangular filters, spaced uniformly on the mel scale. To start with, the lower and higher\nvalues of frequencies are converted into mel scale. An approximate conversion between a frequency value in Hertz (f) and mel is given by:\n(3)\nBased on the number of filters to be designed, the intermediate mel frequencies are computed. Each of the mel frequency is then converted back into normal frequency with the inversion formula. By doing this we get the list of frequencies with mel-scale spacing. For each of these frequency responses, a triangular filter is created and the series of such filters is called as mel filter bank. Each triangular filter is then multiplied with power spectrum and the\ncoefficients are added up, which will be an indication of how much energy is within each filter. Finally, the cepstral coefficients are calculated from these calculated energy values of filter-bank by applying Discrete Cosine Transform (DCT) of the logarithm of the filter-bank energy. This is given by,\n(4)\n\u2018L\u2019 is the number of MFCC coefficients considered, iC is the th i MFCC coefficient, kS is the energy of th\nk filterbank channel (i.e. the sum of the power spectrum bins on that channel), \u2018K\u2019 is the number of filterbanks. A more detailed description of the mel-frequency cepstral coefficients can be found in [6].\n2.2. Fisher\u2019s Ratio (F-ratio) Technique\nA set of MFCC feature measurements would be effective in discriminating between vowels if the distributions of different vowels are concentrated at widely different locations in the parameter space. For a feature value, a good measure of effectiveness would be the ratio of inter vowel to intra-vowel (within class) variance, often referred to as the F-ratio [10][14]. Consider the MFCC feature dataset for the five vowels given as,\nwhere the value of \u2018i\u2019 varies from 1 to \u2018L\u2019 number of observations. F-ratio is given as,\nIt can also be represented as,\n(5)\nN, indicates the number of vowels, i\u00b5 is the mean of MFCC coefficient for th i vowel class, \u00b5 is\nthe overall mean value for the MFCC coefficient from all the vowel classes. iS , the within vowel variance is given by,\n(6)\nijx is the MFCC coefficient for th j observation of th i vowel and iM is the number of\nobservations in th i vowel . This technique is applied for all the 12 MFCC coefficients\nindividually. Higher F-ratio value for an MFCC coefficient indicates that it can be used for good classification [10][11]."}, {"heading": "2.3. Hidden Markov Model (HMM)", "text": "A Hidden Markov Model (HMM) is a statistical model in which the system being modelled is assumed to be a Markov chain with unobserved (hidden) states [6]. A Markov chain is a stochastic process in which the future state of the process is based solely on its present state. HMM is a very common technique in classification, especially in the case of sequential data processes such as speech, music and text. HMMs are used to specify a joint probability distribution over hidden state sequences S={s1, s2,.., sT} and observed output sequences X={x1, x2,.., xT} . The logarithm of this joint distribution is given by:\n(7)\nHere the hidden states st and observed outputs xt denote vowels labels and corresponding feature vectors respectively. The distributions P(xt|st) are typically modelled by multivariate Gaussian mixture models (GMMs).\nWhere, N(xt; \u00b5, \u03a3) denotes the Gaussian distribution with mean vector \u00b5 and covariance matrix \u03a3, and M denotes the number of mixture components per GMM. The mixture weights wjm are constrained to be nonnegative and normalized:\n(8)\nLet \u03b8 denote the vector of model parameters including transition probabilities, mixture weights, mean vectors, and covariance matrices. The goal of parameter estimation in HMMs is to compute the optimal *\u03b8 given N pairs of observations and target label sequences {Xi, Yi} where Yi is the class label [15][16]."}, {"heading": "3. RESULTS AND DISCUSSIONS", "text": "For the experiment, audio sounds of 5 vowels in Kannada language are used. The list of vowels is shown in the following table (Table 1) with their corresponding Unicode name. Twenty five patterns of each vowel are recorded with the voice of single female Kannada speaker constituting totally 125 audio clips.\nEach of the audio sound is divided into frames with duration of 30ms. Then the frames are processed to extract 12 MFCC coefficients called as a feature vector. For each audio file, we get\nnumber of feature vectors equal to the number of frames. The entire dataset of MFCC coefficients are analyzed with Fisher\u2019s ratio technique. F-ratio is computed for each coefficient individually using the steps explained in section 2.2. The result of this computation is shown in Figure 1, where x-axis indicates the MFCC coefficient number and y-axis indicates the corresponding Fratio value.\nFrom the figure, it can be observed that for the 3rd MFCC coefficient, F-ratio value is highest. Does it mean that the 3 rd MFCC coefficient for vowel sound has high discriminating capability than other coefficient? To analyze this property, we have retrieved a subset of MFCC coefficients with high F-ratio and used this subset as a feature vector in classification. The classification experiments are done by varying the number of MFCC coefficients for subset selection. The number of coefficients considered is varied from 3 to 12 and is selected in the order of high Fratio value. The classification is done using HMM algorithm.\nIn the process of classification, first the dataset is divided into training and testing set using hold out approach. Using the training datasets, classification parameters like, prior probability table, emission table and transition table are computed for each of the vowel class separately. Here we get five HMM models corresponding to each vowel class. In the process of testing, the test data is tested against each of the HMM models individually to compute the maximum likelihood\nprobability. The HMM model against which the test data has highest likelihood probability is considered as the predicted output. If the predicted output is matching with the actual output we consider it as correct prediction. In this way, accuracy of classification is computed. More detail on how HMM is used for classification can be found in [17]. Since the dataset is randomly divided into testing and training set, each time we repeat the experiment we get different training and testing dataset. We have repeated 50 iterations of classification for each dataset and found the result getting saturated over 50 iterations. The average accuracy over fifty iterations is considered as the final classification accuracy. The following table (Table 2) summarizes the classification accuracy result for different types of experiment.\nThe following observations can be made from Table 2. The number of coefficients selected has an effect on the classification accuracy. The classification accuracy increases as the number of coefficients increases but only up to certain point. Selection of 8 MFCC coefficients gives a classification accuracy which is comparable or even better than selecting all 12 coefficients. This clearly shows that there is an effect of selecting the MFCC coefficients based on high F-ratio value. Instead of using all the MFCC coefficients as feature vector for classification, we\ncan use the 8 coefficients selected in the order of high F-ratio values. This technique also helps in reducing the feature dimension without much affecting the classification accuracy."}, {"heading": "3. CONCLUSIONS", "text": "Computational experiments are done to analyze whether the number of MFCC coefficients used in feature vector can be reduced using Fisher\u2019s ratio measure. The experimental results show that there is improvement in classification accuracy when eight MFC coefficients are selected based on F-ratio criterion compared to the conventional method of using twelve MFC coefficients. The classification accuracy is affected based on the number of MFCC coefficient selected. F-ratio technique has been proved promising in the selection of subset of MFCC feature. We are conducting more experiments for analyzing whether the same effect is found for consonants and simple syllables."}, {"heading": "AUTHORS", "text": "Sarika Hegde received B.E (Information Science) degree from NMAM.I.T Nitte in 2003 and M.Tech (Computer Science) from NMAM.I.T Nitte in 2007. Presently she is working as Associate Professor in the department of Computer Applications at NMAM.I.T Nitte and also pursuing PhD in the area of Automatic Speech Recognition at Mangalore University, Mangalore. Her area of interest includes Speech Recognition, Audio Data Mining, Big Data, and Pattern Recognition.\nDr. K. K. Achary is a Professor of Statistics & Biostatistics in Yenepoya Research Centre, Yenepoya University, Mangalore, India. He holds M.Sc degree in Statistics and PhD in Applied Mathematics from Indian Institute of Science, Bangalore, India. His current research interests include Stochastic Models, Inventory Theory, Face Recognition Techniques, Audio Data Mining and Speech Recognition. His research papers have appeared in European Journal of Operational Research, Journal of Operational Research Society, Opsearch, CCERO, International Journal Information and Management Sciences, Statistical Methods and American Journal of Mathematics and Managem ent, International Journal of Speech Technology.\nDr. Surendra Shetty is working as Professor in the Department of Computer Applications, NMAM.I.T, Nitte, Udupi, India. He holds MCA, MBA (HR) degrees. He did PhD in the area of Au dio Data Mining. His main research area includes Audio Data Mining, Speech Recognition, Big Data, Data Mining, Management Information System, and Software Testing. He has presented and published more than 10 papers in Journals and conferences."}], "references": [{"title": "Statistical analysis of features and classification of alphasyllabary sounds in Kannada language", "author": ["Hegde", "K.K. Achary", "S. Shetty"], "venue": "International Journal of Speech Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Pattern recognition and machine learning (Vol", "author": ["C.M. Bishop"], "venue": "4, No. 4, p. 12). New York: springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Heteroscedastic discriminant analysis and reduced rand HMMs for improved speech recognition", "author": ["N. Kumar", "A.G. Andreou"], "venue": "Speech Communication, vol. 26, pp. 283\u2013297", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": " D", "author": ["R.O. Duda", "P.E. Hart"], "venue": "G. Stork, Pattern classification, John Wiley & Sons", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": " B", "author": ["L.R. Rabiner"], "venue": "H. Juang, Fundamentals of speech recognition, Prentice Hall", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": " H", "author": ["R. Haeb-Umbach"], "venue": "Ney, \u201cLinear discriminant analysis for improved large vocabulary continuous speech recognition\u201d, In Acoustics, Speech, and Signal Processing", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Optimal linear feature transformations for semi-continuous hidden markov models.\" Acoustics, Speech, and Signal Processing", "author": ["Schukat-Talamazzini", "E. Gunter", "J. Hornegger", "H. Niemann"], "venue": "IEEE International Conference on", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Feature combination using linear discriminant analysis and its pitfalls,", "author": ["R. Schluter", "A. Zolnay", "H. Ney"], "venue": "Proceedings of the 9th International Conference on Spoken Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Automatic recognition of speakers from their voices", "author": ["B.S. Atal"], "venue": "Proceedings IEEE, 64(4), pp. 460\u2013 476", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1976}, {"title": "Auditory toolbox: A MATLAB Toolbox for auditory modeling work", "author": ["M. Slaney"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Applied Speech and Audio Processing", "author": ["I. McLoughlin"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Introduction to Machine Learning", "author": ["E. Alpaydin"], "venue": "PHI Publications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": " L", "author": ["F. Sha"], "venue": "K. Saul., Large Margin Training of Continuous Density Hidden Markov Models. In J. Keshet and S. Bengio (Eds.), Automatic speech and speaker recognition: Large margin and kernel methods. Wiley-Blackwell", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "K K Achary and S", "author": ["S. Hegde"], "venue": "Shetty., \u201cA Multiple Classifier System for Automatic Speech Recognition\u201d, International Journal of Computer Applications, 101(9):38-43, September 2014. International Journal on Cybernetics & Informatics (IJCI) Vol. 4, No. 2, April 2015 52  AUTHORS Sarika Hegde received B.E (Information Science) degree from NMAM.I.T Nitte in 2003 and M.Tech ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "But recently, MFCC gained more importance than LPC due to its high discriminating capability [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "Linear Discriminant Analysis [3][4][5] finds a linear transform by maximizing the Fisher\u2019s ratio and is found to be good in discriminating patterns.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "Linear Discriminant Analysis [3][4][5] finds a linear transform by maximizing the Fisher\u2019s ratio and is found to be good in discriminating patterns.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Linear Discriminant Analysis [3][4][5] finds a linear transform by maximizing the Fisher\u2019s ratio and is found to be good in discriminating patterns.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "46 PCA technique computes the feature vector subspace with maximum variance [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "These techniques are also used successfully in classification with HMM [7][8][9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "These techniques are also used successfully in classification with HMM [7][8][9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "These techniques are also used successfully in classification with HMM [7][8][9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "It is also sometimes used to compare the two feature extraction technique for their capability of discriminating patterns in classification [2][10][11].", "startOffset": 140, "endOffset": 143}, {"referenceID": 8, "context": "It is also sometimes used to compare the two feature extraction technique for their capability of discriminating patterns in classification [2][10][11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "The Mel Frequency Cepstral Coefficient (MFCC) is a feature extracted by applying more than one Fourier Transform sequentially to the original signal [6][12].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "The Mel Frequency Cepstral Coefficient (MFCC) is a feature extracted by applying more than one Fourier Transform sequentially to the original signal [6][12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Framing is the process of breaking the set of sample observations of an entire audio file into smaller chunks called as frame [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "A more detailed description of the mel-frequency cepstral coefficients can be found in [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "For a feature value, a good measure of effectiveness would be the ratio of inter vowel to intra-vowel (within class) variance, often referred to as the F-ratio [10][14].", "startOffset": 160, "endOffset": 164}, {"referenceID": 8, "context": "Higher F-ratio value for an MFCC coefficient indicates that it can be used for good classification [10][11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "A Hidden Markov Model (HMM) is a statistical model in which the system being modelled is assumed to be a Markov chain with unobserved (hidden) states [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 11, "context": "The goal of parameter estimation in HMMs is to compute the optimal * \u03b8 given N pairs of observations and target label sequences {Xi, Yi} where Yi is the class label [15][16].", "startOffset": 165, "endOffset": 169}, {"referenceID": 12, "context": "The goal of parameter estimation in HMMs is to compute the optimal * \u03b8 given N pairs of observations and target label sequences {Xi, Yi} where Yi is the class label [15][16].", "startOffset": 169, "endOffset": 173}, {"referenceID": 13, "context": "More detail on how HMM is used for classification can be found in [17].", "startOffset": 66, "endOffset": 70}], "year": 2015, "abstractText": "Automatic Speech Recognition (ASR) involves mainly two steps; feature extraction and classification (pattern recognition). Mel Frequency Cepstral Coefficient (MFCC) is used as one of the prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC coefficients is used as the feature vector in the classification step. But the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 MFCC as feature vector. In this paper, Fisher\u2019s ratio technique is used for selecting a subset of 12 MFCC coefficients that contribute more in discriminating a pattern. The selected coefficients are used in classification with Hidden Markov Model (HMM) algorithm. The classification accuracies that we get by using 12 coefficients and by using the selected coefficients are compared.", "creator": "PScript5.dll Version 5.2.2"}}}