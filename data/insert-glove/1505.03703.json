{"id": "1505.03703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2015", "title": "A PCA-Based Convolutional Network", "abstract": "In this gueux paper, bedgebury we ccostot propose a novel unsupervised deep treachery learning kallstrom model, augers called 117.40 PCA - based headlined Convolutional Network (corsa PCN ). The architecture 11-minute of penetanguishene PCN is composed 4.26 of ndogo several ultra-wideband feature extraction stages mehtas and 76.06 a nonlinear sesi output stage. construct Particularly, each feature aeolid extraction stage includes knauer two layers: j\u00f8rn a convolutional 114,000 layer and columna a feature pooling gruszka layer. In 1960 the zeda convolutional 10.34 layer, trobe the 2.4 filter banks baia are simply learned by khengarji PCA. livry In the silver-haired nonlinear habeeb output lewine stage, helms binary 2493 hashing is tile applied. For the apprentices higher higher-income convolutional 77-70 layers, stemmons the filter banks ryoji are canzonetta learned kerrs from the khuddaka feature fatter maps that 115.6 were obtained in manichean the samp previous 12-27 stage. To lameduck test PCN, ki-27 we 49.5 conducted taxidermists extensive estefano experiments systra on some devanter challenging midnapur tasks, including ramo handwritten digits recognition, hovin face recognition interdicted and texture eufaula classification. 8.00 The results show genuflection that pullout PCN performs sub-province competitive rookmangud with or 1.2250 even bittel better pakhoed than longboats state - of - purveyors the - lehmberg art tadworth deep learning models. More bathala importantly, -1.25 since purveyor there 72.64 is no back propagation 97.57 for petrography supervised finetuning, sierakowice PCN is upington much mazamet more fleming efficient havant than s19 existing overrule deep networks.", "histories": [["v1", "Thu, 14 May 2015 12:35:19 GMT  (3701kb,D)", "http://arxiv.org/abs/1505.03703v1", "8 pages,5 figures"]], "COMMENTS": "8 pages,5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["yanhai gan", "jun liu", "junyu dong", "guoqiang zhong"], "accepted": false, "id": "1505.03703"}, "pdf": {"name": "1505.03703.pdf", "metadata": {"source": "CRF", "title": "A PCA-Based Convolutional Network", "authors": ["Yanhai Gan", "Jun Liu", "Junyu Dong", "Guoqiang Zhong", "Qing Dao"], "emails": ["gyh5421@163.com", "liujunqd@163.com", "dongjunyu@ouc.edu.cn", "gqzhong@ouc.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Traditional models for classification tasks are generally composed of hand-crafted feature extraction and a trainable classifier. The most popular hand-crafted features include Gabor features [Tao et al., 2007], locally binary patterns (LBP) [Guo and Zhang, 2010], Hog [Onishi et al., 2008] and SIFT [Ke and Sukthankar, 2004]. They have been successfully applied in texture classification, face recognition and object recognition tasks. However, features extracted by hand-crafted methods are always low-level and suited to specific data and tasks with prior knowledge.\nRecently, deep learning has become a popular way of automatically learning features from data that disentangles the underlying factor of variations. The proposed deep approaches always include layerwise stacking of feature extractors. For example, deep belief networks are composed of stacking pre-trained restricted Boltzmann machines (RBMs) and deep auto-encoders are stacked by RBMs or auto-encoders. Deep architectures lead to learn more hierarchical and more abstract features at higher layers of representations.\nOne of the most powerful deep architectures is a biologically inspired model \u2013 convolutional networks (ConvNets). ConvNets are a trainable multi-stage architecture with each stage composed of three layers: the filter banks layer, nonlinearity layer and feature pooling layer. Weight sharing in the convolution layer and pooling operations are the key of the ConvNets which lead to features invariant to small variations. A deep ConvNets with multistage architecture can learn hierarchical features, from local low-level features to global high-level ones. However, training such a deep network typically uses gradient descent method in a supervised mode, which always need a large scale of labeled samples for training. In addition, good results sometimes depend on the tricks of the trade for parameter tuning, e.g. using \u201ddropout\u201d for regulation [Hinton et al., 2012].\nRecent research has shown that using unsupervised learning in each stage of ConvNets helped reducing the requirement of labeled data significantly. PCANet is such a variation of deep convolutional networks of which convolution filter banks in each stage are simply chosen from PCA filters [Chan et al., 2014]. Surprisingly, when such simple filters are used in a deep network architecture, it has demonstrated competitive performance with other deep networks. However, PCANet dispenses with the pooling layer in the feature learning stage, but only uses block-wise histogram together with nonlinear operation in the output stage. This results in the exponentially growing dimensions and training time with increasing number of samples.\nIn this paper, we propose a convolutional architecture in which the filters are learned from PCA in an unsupervised mode. The network is composed of feature extraction stage which could be stacked to multiple stages and a nonlinearity stage. Feature extraction stage includes a convolution layer and a pooling layer and can be easily cascaded to a deep architecture. The nonlinearity stage includes binary hashing and histogram statistics; the output is then fed into a trainable classifier. The filter bank in convolution layer is learned by PCA, and the generated feature maps are aggregated by pooling layers. This results in multiple sets of feature maps corresponding to different filters which probably detect distinctive features (e.g. detect features at similar orientations) of the input. The filter banks in the higher convolution layer are computed based on combinations of multiple sets of feature maps. This is inspired by the intuition that high level ar X\niv :1\n50 5.\n03 70\n3v 1\n[ cs\n.L G\n] 1\n4 M\nay 2\n01 5\nfeatures are the combinations and abstract of low level features. Multiple feature maps corresponding to an input represent different features extracted from the same input. Experiments show the comparative performance in classification tasks against state-of-the-art approaches."}, {"heading": "2 Related Work", "text": "In the past few years, variations of convolutional network have been proposed with respect to the pooling and convolutional operation. Recently, unsupervised learning was used for pre-training in each stage that would alleviate the need of labeled data. When all the stages were pretrained, the network was fine-tuned by using stochastic gradient descent method. Many methods were proposed to pre-train filter banks of convolution layers in an unsupervised feature learning mode. The convolutional versions of sparse RBMs [Jarrett et al., 2009] [Lee et al., 2009a] , sparse coding [Bruna and Mallat, 2013] and predictive sparse decomposition(PSD) [Jarrett et al., 2009] [Henaff et al., 2011] [Kavukcuoglu et al., 2009] [Kavukcuoglu et al., 2010] were reported and achieved high accuracy on several benchmarks.\nAlternatively, some networks similar to ConvNets were proposed but used pre-fixed filters in convolution layer and yielded good performance on several benchmarks. In [Serre et al., 2005] [Mutch and Lowe, 2006], Gabor filters were used in the first convolution layer. Meanwhile, wavelet scattering networks (ScatNet) [Bruna and Mallat, 2013] [Sifre and Mallat, 2013] also used pre-fixed convolutional filters which were called scattering operators. By using a similar multiple levels of ConvNets, the algorithm had achieved impressive results in the applications of handwritten digits and texture recognition. One more closely related work is called PCANet [Chan et al., 2014], which simply use PCA filters in an unsupervised learning mode at the convolution layer. Built upon a multiple convolution layers, a nonlinear output stage was applied with hashing and block-wise histogram. Just a few cascaded convolution layers were demonstrated to be able to achieve new records in several challenging vision tasks, such as face and handwritten recognition, and comparative results on texture classification and object recognition."}, {"heading": "3 The PCA-Based Convolutional Network", "text": "The PCN is essentially a multi-stage convolutional network that can be trained layer-wise in an unsupervised manner. It is composed of cascaded feature extraction stages and a nonlinear output stage. Figure 1 illustrates the structure of a typical PCN with three stages including the output stage. Each feature extraction stage consists of a convolutional layer and a pooling layer. The inputs are first convoluted with PCAbased filters to produce a set of feature maps. The pooling layer generally computes the average or maximum value over a neighborhood. The purpose of a pooling layer is to build robustness to small distortions and reduce the resolution of feature maps by a factor p horizontally and q vertically. The propagated feature maps through the pooling layer are then fed into the next stage as input. The final output stage of PCN comprises binary hashing and block-wise histogram statistics.\nSuppose we are given N input images which are denoted as {Ii}Ni=1 ; the size of each input image is m\u00d7n. The filter size used in each stage is represented as k1 \u00d7 k2. In the following we describe each stage of PCN in detail."}, {"heading": "3.1 The first feature extraction stage in PCN", "text": "Inspired by weight sharing of receptive fields in ConvNets [Jarrett et al., 2009], for each input image, we sample a number of patches with a size of k1\u00d7k2 at every k pixel locations, i.e. the sample interval is k pixels. Each patch is vectorized to form a column with k1k2 elements. Then all patches sampled from the same input image are put together to form a matrix of size (k1k2)\u00d7((dm\u2212k1k e+1)(d n\u2212k2 k e+1)), denoted as Xi = [xi,1, xi,2, xi,3, \u00b7 \u00b7 \u00b7 , xi,(dm\u2212k1k e+1)(dn\u2212k2k e+1)] \u2208 R(k1k2)\u00d7((d m\u2212k1 k e+1)(d n\u2212k2\nk e+1)) , where xi,j represents the vector of the jth patch in Ii.\nIn order to introduce competitions between adjacent features within a neighbourhood, each column vector in the matrix Xi subtracts the mean value of the corresponding patch to obtain the matrix X\u0304i = [x\u0304i,1, x\u0304i,2, x\u0304i,3, \u00b7 \u00b7 \u00b7 , x\u0304i,(dm\u2212k1k e+1)(dn\u2212k2k e+1)] . This operation is reminiscent to the local contrast normalization used by ImageNet [Krizhevsky et al., 2012]. Once matrices for all input images are constructed in the same way, they are assembled to form a large matrix X = [X\u03041, X\u03042, X\u03043, \u00b7 \u00b7 \u00b7 , X\u0304N ] . Subsequently, each row of X subtracts its mean, the result is also denoted as X . Eigenvalue decomposition is then performed on the matrix XXT . The convolutional filters are selected as the first L1 principle eigenvectors of XXT . Thus, the learned filters can be described as Wl = matk1,k2(ql(XX\nT )) \u2208 Rk1\u00d7k2 , l = 1, 2, 3, \u00b7 \u00b7 \u00b7 , L1, where matk1,k2(v) denote the mapping relationship from vector v to a matrix W \u2208 Rk1\u00d7k2 , ql(XXT ) represent the lth eigenvector of matrix XXT . The eigenvectors are reshaped to the size k1\u00d7 k2. In this way, we obtain L1 filters of size k1\u00d7 k2. We subsequently convolute the learned filters with the input images to generate filter responses at each pixel location; we call the filtering results feature maps.\nI li = Ii \u2217Wl, i = 1, 2, 3, \u00b7 \u00b7 \u00b7 , N (1)\nwhere: \u2217 is 2D convolution operation; Ii is padded with zeros before convolution.\nThe convolution with each input image produces L1 feature maps. Each feature map represents particular features extracted at corresponding location in the image. We divide the feature maps (padded with zeros) generated by the convolutional layer into several non-overlapping pooling regions of size p \u00d7 q. Then the max pooling or average pooling is applied to the pooling regions. The pooling operation results in feature maps with reduced resolution, and these pooling features are robust to small distortions. We use Sli to represent the pooling result of the lth feature map of the ith input image. Given a collection {Ii}Ni=1 of N input images, through the convolution and pooling operation using the lth filter we obtain N feature maps, which are denoted as {Sli}Ni=1, l = 1, 2, 3, \u00b7 \u00b7 \u00b7 , L1. Since there are L1 filters in the first extraction stage, we obtain NL1 feature maps in total."}, {"heading": "3.2 The second feature extraction stage in PCN", "text": "The pooled feature maps in the first stage are treated as the original input to the second stage. These NL1 feature maps are divided into L1 subsets. Each subset includes N feature maps which are produced by convoluting the input images with the same filter in the previous stage, and they are denoted as Sl = {Sli}Ni=1, l = 1, 2, 3, \u00b7 \u00b7 \u00b7 , L1. Feature maps in one subset capture certain features of the input images, whereas those in different subsets capture different types of features. Figure 2 shows the structure of the proposed PCN.\nSince high level features are the combinations and abstract of low level features [Gutmann and Hyva\u0308rinen, 2013], we combine subsets {Sl}L1l=1 according to certain rule to form several groups. Table 1 demonstrates one way to combine the subsets. In each group(corresponding to a column in the table), feature maps(marked with\u00d7) corresponding to the same input image are added. The combined subsets are then used as the actual input to the second feature extraction stage.\nIn Table 1, each row represents a subset of feature maps obtained from the previous stage, and each column represents a group combining these subsets in a particular way. There are 5 filters in the first stage which result in 5 subsets. Two of the subsets are added and 5 groups are formed. In the table, \u00d7 indicates corresponding subsets are combined to form one group. In practice, an indexing matrix is used to define the\nway of combination. In the indexing matrix, most entries are zeros and a few of entries are ones, which indicate the subsets belonging to one group. Thus, different indexing matrices can be defined. If the indexing matrix is defined as an identity matrix, there will be no combination of subsets.\nThe combination produces several new subsets and each new subset is denoted as {Sl\u2032i }Ni=1 , which also consists of N feature maps.\nBy repeating the same procedure as in the first stage, for each {Sl\u2032i }Ni=1, we sample patches from each feature map in this subset. Then we also subtract patch mean values and join all vectors together to form a matrix denoted as Y\u0304 l \u2032\ni = [y\u0304l\u2032,i,1, y\u0304l\u2032,i,2, y\u0304l\u2032,i,3, \u00b7 \u00b7 \u00b7 , y\u0304l\u2032,i,(dm\u2212k1k e+1)(dn\u2212k2k e+1)], where y\u0304l\u2032,i,j represents the mean removed vector of the jth patch of the ith feature map in the l\u2032th subset. We further collect patches from all the feature maps in this subset, remove the patch mean, and concatenate the matrixes Y\u0304 l \u2032\ni as Y l \u2032 = [Y\u0304 l \u2032 1 , Y\u0304 l\u2032 2 , Y\u0304 l\u2032 3 , \u00b7 \u00b7 \u00b7 , Y\u0304 l \u2032\nN ]. Afterwards the row mean is removed form Y l \u2032 . Since there are L1 subsets, we obtain L1 such matrixes Y l \u2032 , l\u2032 = 1, 2, 3, \u00b7 \u00b7 \u00b7 , L1.\nFor each subset {Sl\u2032i }Ni=1, we construct filters using the following equation separately:\nV l \u2032 l = matk1,k2(ql(Y l\u2032Y l \u2032T )) \u2208 Rk1\u00d7k2 , l = 1, 2, 3, \u00b7 \u00b7 \u00b7 , L2\n(2)\nFor each subset, we choose the first L2 principle eigenvectors as PCA filters, denoted as {V l\u2032l } L2 l=1. Each input feature map in this subset is convoluted with L2 filters, which resulted in L2 new feature maps. Since there are L1 subsets(produced by L1 groups), we produce L1NL2 feature maps in total in the second stage, and they are the output of the second feature extraction stage (C2 in Figure 2).\nThe pooling process in the second stage is the same as in the first stage. The output feature maps of C2 are divided into several non-overlapping patches with size p\u00d7 q and the maximum or average value is calculated over the pooling region.\nIf there are more feature extraction stages, the process is repeated in the same way as described above."}, {"heading": "3.3 The output stage in PCN", "text": "In the output stage we reconstruct feature maps to form final representations of the input image. We use binary hashing and histogram statistics (called \u201dhashingHist\u201d) as in PCANet [Chan et al., 2014]. Each input feature map Sl \u2032\ni to the second stage produces L2 output maps. We binarize these output maps and calculate H(Sl \u2032\ni \u2217Vl), where H(.) is a Heaviside step function whose value is one for positive entries and zero otherwise. For each pixel location, we treat the vector of L2 binary bits as a decimal number. This converts the L2 outputs generated in the second stage back into a single integer-valued image.\nFor each of the L1 integer-valued images, we partition it into B blocks. We compute the histogram of the decimal values in each block, and concatenate all the B histograms into one vector. After this encoding process, the feature of the input image Ii becomes the set of block-wise histograms. The local blocks can be either overlapping or non-overlapping, depending on applications."}, {"heading": "4 Experiments", "text": "In all experiments, a three-stage (including the final output stage) PCN is applied to different data sets for simplicity. The final output features of the PCN are sent to a linear SVM for classification. All these configurations keep fixed. We compared the efficiency of PCN for different recognition tasks using the same desktop PC with an Intel i5-3570 CPU and 32GB memory."}, {"heading": "4.1 Digit Recognition based on the MNIST Datasets", "text": "Because images in the MNIST Datasets are small, we set the patching sampling interval as 1, i.e. we sample a patch at each pixel location. The patch size is set as 7 \u00d7 7. In the output stage, we set the block size as 7 \u00d7 7, and we set the block overlapping ratio as 0.5. The three parameters keep unchanged during the experiment. In particular the pooling layer is disabled in every feature extraction stage, and it can be easily controlled by a parameter in our code. We select an identity matrix as the indexing matrix, that is, we make every group in the second stage contain only one subset.\nDigit recognition on the basic MNIST Dataset\nThe basic MNIST dataset is a smaller subset of MNIST. It contains 10000 training images, 2000 validation images and 50000 testing images. We first perform our experiment on the basic dataset. The hyper-parameters were selected to maximize the performance on the validation set. Then, the system was trained over the entire training set and validation set. We achieve the highest accuracy of 99.20% when the numbers of filters in the first stage and second stage are set to 6 and 11 respectively.This is higher than related methods in literature.\nDigit recognition on the standard MNIST Dataset\nThe standard MNIST dataset consists of 60000 training images and 10000 testing images. To adjust hyperparameters, a validation set of 5 samples per class was taken out of the training sets. The hyper-parameters were selected to maximize the performance on the validation set. Then, the system was trained over the entire training set. We found the best configuration when the numbers of filters in the first and second stage were set to 8 and 10 respectively, and the accuracy reached 99.41%, which outperformed the related works, as shown in table 3. Overall, PCN can achieve competitive performance compared to the state-of-the-art, but with much less computation due to its simple network structure."}, {"heading": "4.2 Face Recognition on the Extended Yale B Dataset", "text": "The extended Yale B dataset contains 2414 frontal-face images of 38 individuals. The cropped and normalized 192 \u00d7 168 face images were captured under various lighting conditions. For each subject, we randomly select 5 images as our testing images, and the rest for training. A validation set of 5 images per subject was taken out of the training sets. The hyper-parameters were selected to maximize the performance on the validation set. Then, the system was trained over the entire training set. In the end the patch size was set as 5\u00d7 5, and the numbers of filters in the first and second stage were set as 11 and 8 respectively. The patch sampling interval was set as 1. The max pooling module used a 2\u00d7 2 boxcar filter with a 2\u00d72 down-sampling step. We used non-overlapping blocks in the output stage and the block size was set as 8\u00d78. Identity matrix was used as the indexing matrix in the second stage. We achieve the average accuracy of 99.58% over 10 experiments, as shown in table 4. The training time of our method including PCN plus SVM is 2054.42s, and the testing time per sample is 0.27. This is much more efficient compared to PCANet. The filters in the first stage are shown in figure 3a; it is obvious that each filter in the first stage captures directionrelated features of an input face image. Each column in Figure 3b contains filters in one group in the second stage; it can be seen that the filter banks in different groups are similar to a large extend, but there are still some differences, so we can\u2019t use just the same filters in different groups. We found that an identity matrix was better than other matrix when used as the indexing matrix, this may be caused by the blur effect that all subsets in on group connected to the same filter bank, so we maybe use different filters in the future."}, {"heading": "4.3 Texture Classification on CUReT Dataset", "text": "The CUReT texture dataset contains 61 categories of textures. Each category contains images of the same material with different pose and illumination conditions. In this experiment, following PCANet [Chan et al., 2014], a subset of the original data with azimuthal viewing angles less than 60 degrees was selected, thereby yielding 92 images in each class. A central 200 \u00d7 200 region was cropped from each of the selected images. The dataset was randomly split into a training and a testing set, with 46 training images for each class. The hyper-parameters were selected according to literature. The filter size was set as 5 \u00d7 5; the patch sampling interval was set as 1. The number of filters in both stage was set as 8, and non-overlapping block size was 50 \u00d7 50. The pooling layer was disabled in each extraction stage. Identity matrix was used as indexing matrix in the second stage. The accuracy reached 99.71%, which was higher than the result of 99.61% achieved by PCANet."}, {"heading": "4.4 Texture Classification on Outex Dataset", "text": "Outex is a framework for empirical evaluation of texture classification and segmentation algorithms. Problems are encapsulated into welldefined test suites having precise specifications of input and output data. Outex database contains surface textures and natural scenes. The collection of surface textures is expanding continuously. At this very moment the database contains 320 surface textures, both macrotextures and microtextures. Many textures have variations in local color content, which results in challenging local gray scale variations in intensity images. Some of the source textures have a large tactile dimension, which can induce considerable local gray scale distortions. Each source texture is imaged according to certain procedure. The images used in a texture classification suite are extracted from the given set of source images (particular texture classes, illuminations, spatial resolutions, and rotation angles) by centering the sampling grid so that equally many pixels are left over on each side of the sampling grid. If the training and testing images of a particular texture classification problem are extracted from the same set of source images, the images are divided randomly to two halves of equal size for the purpose of obtaining an unbiased performance estimate. The directory images in each test suite includes the images needed in the test suite. The directory indexed by three numbers includes the specified problem in this test suite. Each one of these directories has three files:classes.txt,test.txt and train.txt which define the problem. The problem indexed by 000 of the OutexTC00004 test suite was selected in our experiment. After several validating trails,\nthe patch size was set as 5 \u00d7 5. The patch sampling interval was set as 1, and the numbers of filters in the first and second stage were set as 18 and 6 respectively. The pooling layer was disabled in each stage. The block size was set as 14\u00d7 14 and the block overlapping ratio was 0.5. An identity matrix is used as the indexing matrix in the second stage. We achieve the classification accuracy of 99.91%, and the training time is 260.84s including PCN and SVM. What\u2019s more the test time per sample is 0.15s."}, {"heading": "4.5 Texture Classification on Our Dataset", "text": "Procedural models are widely used in computer graphics for generating realistic, natural-looking textures. A number of procedural models have been proposed and these models can produce various textures. Through render these textures are presented as surface images. Given a surface image, it is important to know which model can produce such kind of texture. This is a typical texture classification problem. Our procedural texture dataset contains a number of rendered textures generated by 23 procedural texture models and then rendered by Luxrender given fixed light conditions. Textures generated by one method normally are different from those generated by other methods; however, some textures produced by different models may be perceived similar. This forms a challenging classification task. Figure 4 shows example samples of our texture dataset.\nThe size of surface images in our dataset is 256*256. In this experiment, we use a total of 3600 surface images, which will be available together with the source code in the near future.\nWe randomly choose 25% of the images from each method as our testing set and the rest are used for training. A validation set of 5 samples per method was taken out of the training set. The hyper-parameters were selected to maximize the performance on the validation set. We found the best configuration that patch size 7\u00d7 7, patch sampling interval 3, the numbers of filters in both extraction stages L1 = 16, L2 = 38. A 2\u00d7 2 boxcar filter with a 2\u00d7 2 down-sampling step was used in the pooling layer. In particular the output non-linear stage was removed. All feature maps from the feature extraction phase were reshaped and concatenated to form a vector as the input to the linear SVM classifier. Then the PCN was trained\nover the entire training set using the best configuration. The accuracy reaches 99.89% which is higher than the result of 99.62% achieved by PCANet. These results are shown in Table 6. More importantly, our algorithm is much more efficient than PCANet in terms of computation cost. The large numbers of filters suggest that surface images in our dataset contains complex structure.\nFigure 5(a) shows that most filters in the first stage extract orientation related features of input images. Due to the complex structure of our texture, some filters look complicated. A fact is that some surface images have no obvious edge information. In figure 5(b), each row contains filters of one group in the second stage. It can be observed that prior filters in a group can extract large scale features, and posterior filters extract more detailed features.\nAs a comparison, we also use a traditional CNN for the same classification task with the same computation resources. After running 10 hours for 50000 iterations, we only achieve an accuracy of 43.2%. The performance becomes worse as the number of iterations increases. It is obvious that the CNN falls into overfitting because we do not have enough training samples."}, {"heading": "5 Conclusion", "text": "We propose a PCA-based Convolutional Network (PCN), which essentially has the advantage of both CNN [Jarrett et al., 2009] and PCANet [Chan et al., 2014], i.e. it can achieve competitive performance compared with state-of-theart methods but is much more efficient in terms of computation. The PCN used in our experiments simply comprises two feature extraction stages and a non-linearity output stage. However, instead of training the network by using iteration methods, PCN simply uses PCA to learn filters in convolution layer. The eigenvectors are used as the filters to convolute with the input images.\nSimilar to other deep networks, it should be noted that a proper configuration of PCN is very important for different types of inputs. If training images are relatively simple in terms of structure and have a large size, we can use a relatively large interval to sample the patches and enable the pooling layer to rapidly reduce the feature dimension of the input image. On the other hand, if the input image is small enough, we may simply set the patch sampling interval to one and disable the pooling layer. In the grouping process, all subsets in one group are connected to the same filter bank, so we can add up all the subsets to form a new subset. But different filter banks maybe work more effectively. We consider to use different filter banks in one group in the future."}, {"heading": "Acknowledgment", "text": "This Work Was Supported By National Natural Science Foundation Of China(NSFC) (No. 61271405) ; The Ph.D. Program Foundation Of Ministry Of Education Of China (No. 20120132110018);"}], "references": [{"title": "IEEE Transactions on", "author": ["Serge Belongie", "Jitendra Malik", "Jan Puzicha. Shape matching", "object recognition using shape contexts. Pattern Analysis", "Machine Intelligence"], "venue": "24(4):509\u2013522,", "citeRegEx": "Belongie et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "IEEE Transactions on", "author": ["Joan Bruna", "St\u00e9phane Mallat. Invariant scattering convolution networks. Pattern Analysis", "Machine Intelligence"], "venue": "35(8):1872\u20131886,", "citeRegEx": "Bruna and Mallat. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Yi Ma", "author": ["Tsung-Han Chan", "Kui Jia", "Shenghua Gao", "Jiwen Lu", "Zinan Zeng"], "venue": "Pcanet: A simple deep learning baseline for image classification? arXiv preprint arXiv:1404.3606,", "citeRegEx": "Chan et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE Transactions on", "author": ["Zhenhua Guo", "David Zhang. A completed modeling of local binary pattern operator for texture classification. Image Processing"], "venue": "19(6):1657\u20131663,", "citeRegEx": "Guo and Zhang. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Journal of Physiology-Paris", "author": ["Michael U Gutmann", "Aapo Hyv\u00e4rinen. A three-layer model of natural image statistics"], "venue": "107(5):369\u2013398,", "citeRegEx": "Gutmann and Hyv\u00e4rinen. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "volume 11", "author": ["Mikael Henaff", "Kevin Jarrett", "Koray Kavukcuoglu", "Yann LeCun. Unsupervised learning of sparse features for scalable audio classification. In ISMIR"], "venue": "page 276,", "citeRegEx": "Henaff et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "M Ranzato", "Yann LeCun"], "venue": "2009 IEEE 12th International Conference on, pages 2146\u2013 2153. IEEE,", "citeRegEx": "Jarrett et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "2009", "author": ["Koray Kavukcuoglu", "MarcAurelio Ranzato", "Rob Fergus", "Yann Le-Cun. Learning invariant features through topographic filter maps. In Computer Vision", "Pattern Recognition"], "venue": "CVPR 2009. IEEE Conference on, pages 1605\u20131612. IEEE,", "citeRegEx": "Kavukcuoglu et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Advances in neural information processing systems", "author": ["Koray Kavukcuoglu", "Pierre Sermanet", "Y-Lan Boureau", "Karol Gregor", "Micha\u00ebl Mathieu", "Yann L Cun. Learning convolutional feature hierarchies for visual recognition"], "venue": "pages 1090\u20131098,", "citeRegEx": "Kavukcuoglu et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Pca-sift: A more distinctive representation for local image descriptors", "author": ["Yan Ke", "Rahul Sukthankar"], "venue": "Computer Vision and Pattern Recognition, 2004. CVPR", "citeRegEx": "Ke and Sukthankar. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "IEEE Transactions on", "author": ["Daniel Keysers", "Thomas Deselaers", "Christian Gollan", "Hermann Ney. Deformation models for image recognition. Pattern Analysis", "Machine Intelligence"], "venue": "29(8):1422\u20131435,", "citeRegEx": "Keysers et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee et al", "2009a] Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee et al", "2009b] Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "localized features", "author": ["Jim Mutch", "David G Lowe. Multiclass object recognition with sparse"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, volume 1, pages 11\u201318. IEEE,", "citeRegEx": "Mutch and Lowe. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "3d human posture estimation using the hog features from monocular image", "author": ["Katsunori Onishi", "Tetsuya Takiguchi", "Yasuo Ariki"], "venue": "Pattern Recognition, 2008. ICPR 2008. 19th International Conference on, pages 1\u20134. IEEE,", "citeRegEx": "Onishi et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "2005", "author": ["Thomas Serre", "Lior Wolf", "Tomaso Poggio. Object recognition with features inspired by visual cortex. In Computer Vision", "Pattern Recognition"], "venue": "CVPR 2005. IEEE Computer Society Conference on, volume 2, pages 994\u20131000. IEEE,", "citeRegEx": "Serre et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "scaling and deformation invariant scattering for texture discrimination", "author": ["Laurent Sifre", "St\u00e9phane Mallat. Rotation"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1233\u20131240. IEEE,", "citeRegEx": "Sifre and Mallat. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IEEE Transactions on", "author": ["Dacheng Tao", "Xuelong Li", "Xindong Wu", "Stephen J Maybank. General tensor discriminant analysis", "gabor features for gait recognition. Pattern Analysis", "Machine Intelligence"], "venue": "29(10):1700\u20131715,", "citeRegEx": "Tao et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In Computer Vision and Pattern Recognition (CVPR)", "author": ["Kai Yu", "Yuanqing Lin", "John Lafferty. Learning image representations from the pixel level via hierarchical sparse coding"], "venue": "2011 IEEE Conference on, pages 1713\u20131720. IEEE,", "citeRegEx": "Yu et al.. 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 19, "context": "The most popular hand-crafted features include Gabor features [Tao et al., 2007], locally binary patterns (LBP) [Guo and Zhang, 2010], Hog [Onishi et al.", "startOffset": 62, "endOffset": 80}, {"referenceID": 3, "context": ", 2007], locally binary patterns (LBP) [Guo and Zhang, 2010], Hog [Onishi et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 16, "context": ", 2007], locally binary patterns (LBP) [Guo and Zhang, 2010], Hog [Onishi et al., 2008] and SIFT [Ke and Sukthankar, 2004].", "startOffset": 66, "endOffset": 87}, {"referenceID": 10, "context": ", 2008] and SIFT [Ke and Sukthankar, 2004].", "startOffset": 17, "endOffset": 42}, {"referenceID": 6, "context": "using \u201ddropout\u201d for regulation [Hinton et al., 2012].", "startOffset": 31, "endOffset": 52}, {"referenceID": 2, "context": "PCANet is such a variation of deep convolutional networks of which convolution filter banks in each stage are simply chosen from PCA filters [Chan et al., 2014].", "startOffset": 141, "endOffset": 160}, {"referenceID": 7, "context": "The convolutional versions of sparse RBMs [Jarrett et al., 2009] [Lee et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 1, "context": ", 2009a] , sparse coding [Bruna and Mallat, 2013] and predictive sparse decomposition(PSD) [Jarrett et al.", "startOffset": 25, "endOffset": 49}, {"referenceID": 7, "context": ", 2009a] , sparse coding [Bruna and Mallat, 2013] and predictive sparse decomposition(PSD) [Jarrett et al., 2009] [Henaff et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 5, "context": ", 2009] [Henaff et al., 2011] [Kavukcuoglu et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 8, "context": ", 2011] [Kavukcuoglu et al., 2009] [Kavukcuoglu et al.", "startOffset": 8, "endOffset": 34}, {"referenceID": 9, "context": ", 2009] [Kavukcuoglu et al., 2010] were reported and achieved high accuracy on several benchmarks.", "startOffset": 8, "endOffset": 34}, {"referenceID": 17, "context": "In [Serre et al., 2005] [Mutch and Lowe, 2006], Gabor filters were used in the first convolution layer.", "startOffset": 3, "endOffset": 23}, {"referenceID": 15, "context": ", 2005] [Mutch and Lowe, 2006], Gabor filters were used in the first convolution layer.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "Meanwhile, wavelet scattering networks (ScatNet) [Bruna and Mallat, 2013] [Sifre and Mallat, 2013] also used pre-fixed convolutional filters which were called scattering operators.", "startOffset": 49, "endOffset": 73}, {"referenceID": 18, "context": "Meanwhile, wavelet scattering networks (ScatNet) [Bruna and Mallat, 2013] [Sifre and Mallat, 2013] also used pre-fixed convolutional filters which were called scattering operators.", "startOffset": 74, "endOffset": 98}, {"referenceID": 2, "context": "One more closely related work is called PCANet [Chan et al., 2014], which simply use PCA filters in an unsupervised learning mode at the convolution layer.", "startOffset": 47, "endOffset": 66}, {"referenceID": 7, "context": "Inspired by weight sharing of receptive fields in ConvNets [Jarrett et al., 2009], for each input image, we sample a number of patches with a size of k1\u00d7k2 at every k pixel locations, i.", "startOffset": 59, "endOffset": 81}, {"referenceID": 12, "context": "This operation is reminiscent to the local contrast normalization used by ImageNet [Krizhevsky et al., 2012].", "startOffset": 83, "endOffset": 108}, {"referenceID": 4, "context": "Since high level features are the combinations and abstract of low level features [Gutmann and Hyv\u00e4rinen, 2013], we combine subsets {Sl}1 l=1 according to certain rule to form several groups.", "startOffset": 82, "endOffset": 111}, {"referenceID": 2, "context": "We use binary hashing and histogram statistics (called \u201dhashingHist\u201d) as in PCANet [Chan et al., 2014].", "startOffset": 83, "endOffset": 102}, {"referenceID": 20, "context": "HSC [Yu et al., 2011] 99.", "startOffset": 4, "endOffset": 21}, {"referenceID": 0, "context": "23 K-NN-SCM [Belongie et al., 2002] 99.", "startOffset": 12, "endOffset": 35}, {"referenceID": 11, "context": "37 K-NN-IDM [Keysers et al., 2007] 99.", "startOffset": 12, "endOffset": 34}, {"referenceID": 7, "context": "18 ConvNet [Jarrett et al., 2009] 99.", "startOffset": 11, "endOffset": 33}, {"referenceID": 1, "context": "47 ScatNet-2(SVMrbf ) [Bruna and Mallat, 2013] 99.", "startOffset": 22, "endOffset": 46}, {"referenceID": 2, "context": "In this experiment, following PCANet [Chan et al., 2014], a subset of the original data with azimuthal viewing angles less than 60 degrees was selected, thereby yielding 92 images in each class.", "startOffset": 37, "endOffset": 56}, {"referenceID": 7, "context": "We propose a PCA-based Convolutional Network (PCN), which essentially has the advantage of both CNN [Jarrett et al., 2009] and PCANet [Chan et al.", "startOffset": 100, "endOffset": 122}, {"referenceID": 2, "context": ", 2009] and PCANet [Chan et al., 2014], i.", "startOffset": 19, "endOffset": 38}], "year": 2015, "abstractText": "In this paper, we propose a novel unsupervised deep learning model, called PCA-based Convolutional Network (PCN). The architecture of PCN is composed of several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a convolutional layer and a feature pooling layer. In the convolutional layer, the filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For the higher convolutional layers, the filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-theart deep learning models. More importantly, since there is no back propagation for supervised finetuning, PCN is much more efficient than existing deep networks.", "creator": "LaTeX with hyperref package"}}}