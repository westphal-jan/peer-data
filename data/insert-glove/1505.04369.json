{"id": "1505.04369", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2015", "title": "Shrinkage degree in $L_2$-re-scale boosting for regression", "abstract": "mankahlana Re - scale boosting (RBoosting) decades is a yungay variant overpressure of odon boosting which jetting can essentially answered improve avalanches the generalization disinflationary performance staurakios of 249th boosting \u02bfabd learning. decriminalise The lapua key escala feature chevallier of 27.0 RBoosting city/31 lies collectable in introducing a shrinkage degree 120.42 to re - 32-31 scale the ensemble huascaran estimate wc2003 in each triads gradient - descent step. 2,829 Thus, 53.33 the yanqiu shrinkage degree determines iipa the performance dornan of RBoosting.", "histories": [["v1", "Sun, 17 May 2015 08:09:43 GMT  (152kb,D)", "http://arxiv.org/abs/1505.04369v1", "11 pages, 27 figures"]], "COMMENTS": "11 pages, 27 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lin xu", "shaobo lin", "yao wang", "zongben xu"], "accepted": false, "id": "1505.04369"}, "pdf": {"name": "1505.04369.pdf", "metadata": {"source": "CRF", "title": "Shrinkage degree in L2-re-scale boosting for regression", "authors": ["Lin Xu", "Shaobo Lin", "Yao Wang", "Zongben Xu"], "emails": [], "sections": [{"heading": null, "text": "1 Shrinkage degree in L2-re-scale boosting for regression\nLin Xu, Shaobo Lin, Yao Wang and Zongben Xu\nAbstract\u2014Re-scale boosting (RBoosting) is a variant of boosting which can essentially improve the generalization performance of boosting learning. The key feature of RBoosting lies in introducing a shrinkage degree to re-scale the ensemble estimate in each gradient-descent step. Thus, the shrinkage degree determines the performance of RBoosting. The aim of this paper is to develop a concrete analysis concerning how to determine the shrinkage degree in L2-RBoosting. We propose two feasible ways to select the shrinkage degree. The first one is to parameterize the shrinkage degree and the other one is to develope a data-driven approach of it. After rigorously analyzing the importance of the shrinkage degree in L2-RBoosting learning, we compare the pros and cons of the proposed methods. We find that although these approaches can reach the same learning rates, the structure of the final estimate of the parameterized approach is better, which sometimes yields a better generalization capability when the number of sample is finite. With this, we recommend to parameterize the shrinkage degree of L2RBoosting. To this end, we present an adaptive parameterselection strategy for shrinkage degree and verify its feasibility through both theoretical analysis and numerical verification. The obtained results enhance the understanding of RBoosting and further give guidance on how to use L2-RBoosting for regression tasks.\nIndex Terms\u2014Learning system, boosting, re-scale boosting, shrinkage degree, generalization capability.\nI. INTRODUCTION\nBOOSTING is a learning system which combines manyparsimonious models to produce a model with prominent predictive performance. The underlying intuition is that combines many rough rules of thumb can yield a good composite learner. From the statistical viewpoint, boosting can be viewed as a form of functional gradient decent [1]. It connects various boosting algorithms to optimization problems with specific loss functions. Typically, L2-Boosting [2], [3] can be interpreted as an stepwise additive learning scheme that concerns the problem of minimizing the L2 risk. Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].\nAlthough the universal consistency of boosting has already been verified in [9], the numerical convergence rate of boosting is a bit slow [9], [10]. The main reason for such a drawback is that the step-size derived via linear search in boosting can not always guarantee the most appropriate one [11], [12]. Under this circumstance, various variants of boosting, comprising\nL. Xu, Y. Wang, and Z. Xu are with the Institute for Information and System Sciences, School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an 710049, China; S. Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China\nthe regularized boosting via shrinkage (RSBoosting) [13], regularized boosting via truncation (RTBoosting) [14] and \u03b5Boosting [15] have been developed via introducing additional parameters to control the step-size. Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent. However, it also needs verifying whether the learning performances of these variants can be further improved, say, to the best of our knowledge, there is not any related theoretical analysis to illustrate the optimality of these variants, at least for a certain aspect, such as the generalization capability, population (or numerical) convergence rate, etc.\nMotivated by the recent development of relaxed greedy algorithm [17] and sequential greedy algorithm [18], Lin et al. [12] introduced a new variant of boosting named as the rescale boosting (RBoosting). Different from the existing variants that focus on controlling the step-size, RBoosting builds upon re-scaling the ensemble estimate and implementing the linear search without any restrictions on the step-size in each gradient descent step. Under such a setting, the optimality of the population convergence rate of RBoosting was verified. Consequently, a tighter generalization error of RBoosting was deduced. Both theoretical analysis and experimental results in [12] implied that RBoosting is better than boosting, at least for the L2 loss.\nAs there is no free lunch, all the variants improve the learning performance of boosting at the cost of introducing an additional parameter, such as the truncated parameter in RTBoosting, regularization parameter in RSBoosting, \u03b5 in \u03b5Boosting, and shrinkage degree in RBoosting. To facilitate the use of these variants, one should also present strategies to select such parameters. In particular, Elith et al. [19] showed that 0.1 is a feasible choice of \u03b5 in \u03b5-Boosting; Bu\u0308hlmann and Hothorn [5] recommended the selection of 0.1 for the regularization parameter in RSBoosting; Zhang and Yu [14] proved that O(k\u22122/3) is a good value of the truncated parameter in RTBoosting, where k is the number of iterations. Thus, it is interesting and important to provide a feasible strategy for selecting shrinkage degree in RBoosting.\nOur aim in the current article is to propose several feasible strategies to select the shrinkage degree in L2-RBoosting and analyze their pros and cons. For this purpose, we need to justify the essential role of the shrinkage degree in L2RBoosting. After rigorously theoretical analysis, we find that, different from other parameters such as the truncated value, regularization parameter, and \u03b5 value, the shrinkage degree does not affect the learning rate, in the sense that, for arbitrary finite shrinkage degree, the learning rate of corresponding L2-\nar X\niv :1\n50 5.\n04 36\n9v 1\n[ cs\n.L G\n] 1\n7 M\nay 2\n01 5\n2 RBoosting can reach the existing best record of all boosting type algorithms. This means that if the number of samples is infinite, the shrinkage degree does not affect the generalization capability of L2-RBoosting. However, our result also shows that the essential role of the shrinkage degree in L2RBoosting lies in its important impact on the constant of the generalization error, which is crucial when there are only finite number of samples. In such a sense, we theoretically proved that there exists an optimal shrinkage degree to minimize the generalization error of L2-RBoosting.\nWe then aim to develop two effective methods for a \u201cright\u201d value of the shrinkage degree. The first one is to consider the shrinkage degree as a parameter in the learning process of L2-RBoosting. The other one is to learn the shrinkage degree from the samples directly and we call it as the L2 data-driven RBoosting (L2-DDRBoosting). We find that the above two approaches can reach the same learning rate and the number of parameters in L2-DDRBoosting is less than that of L2RBoosting. However, we also prove that the estimate deduced from L2-RBoosting possesses a better structure (smaller l1 norm), which sometimes leads a much better generalization capability for some special weak learners. Thus, we recommend the use of L2-RBoosting in practice. Finally, we develop an adaptive shrinkage degree selection strategy for L2-RBoosting. Both the theoretical and experimental results verify the feasibility and outperformance of L2-RBoosting.\nThe rest of paper is organized as follows. In Section 2, we give a brief introduction to the L2-Boosting, L2-RBoosting and L2-DDRBoosting. In Section 3, we study the related theoretical behaviors of L2-RBoosting. In Section 4, a series of simulations and real data experiments are employed to illustrate our theoretical assertions. In Section 5, we provide the proof of the main results. In the last section, we draw a simple conclusion.\nII. L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING\nEnsemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability. In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14]. The aim of this section is to introduce some concrete boosting-type learning schemes for regression.\nIn a regression problem with a covariate X on X \u2286 Rd and a real response variable Y \u2208 Y \u2286 R, we observe m i.i.d. samples Dm = {(xi, yi)}mi=1 from an unknown underlying distribution \u03c1. Without loss of generality, we always assume Y \u2286 [\u2212M,M ], where M <\u221e is a positive real number. The aim is to find a function to minimize the generalization error\nE(f) = \u222b \u03c6(f(x), y)d\u03c1,\nwhere \u03c6 : R \u00d7 R \u2192 R+ is called a loss function [14]. If \u03c6(f(x), y) = (f(x)\u2212y)2, then the known regression function\nf\u03c1(x) = E{Y |X = x}\nminimizes the generalization error. In such a setting, one is interested in finding a function fD based on Dm such that E(fD) \u2212 E(f\u03c1) is small. Previous study [2] showed that L2Boosting can successfully tackle this problem.\nLet S = {g1, . . . , gn} be the set of weak learners (regressors) and define\nspan(S) =  n\u2211 j=1 ajgj : gj \u2208 S, aj \u2208 R, n \u2208 N  . Let\n\u2016f\u2016m = \u221a\u221a\u221a\u221a 1 m m\u2211 i=1 f(xi)2, and \u3008f, g\u3009m = 1 m m\u2211 i=1 f(xi)g(xi)\nbe the empirical norm and empirical inner product, respectively. Furthermore, we define the empirical risk as\nED(f) = 1\nm m\u2211 i=1 |f(xi)\u2212 yi|2.\nThen the gradient descent view of L2-Boosting [1] can be interpreted as follows.\nAlgorithm 1 Boosting Step 1(Initialization): Given data {(xi, yi) : i = 1, . . . ,m}, dictionary S, iteration number k\u2217 and f0 \u2208 span(S). Step 2(Projection of gradient ): Find g\u2217k \u2208 S such that\ng\u2217k = argmax g\u2208S |\u3008rk\u22121, g\u3009m|,\nwhere residual rk\u22121 = y \u2212 fk\u22121 and y is a function satisfying y(xi) = yi. Step 3(Linear search):\nfk = fk\u22121 + \u3008rk\u22121, g\u2217k\u3009mg\u2217k.\nStep 4 (Iteration) Increase k by one and repeat Step 2 and Step 3 if k < k\u2217.\nRemark 2.1: In the step 3 in Algorithm 1, it is easy to check that\n\u3008rk\u22121, g\u2217k\u3009m = arg min \u03b2k\u2208R ED(fk\u22121 + \u03b2kg\u2217k).\nTherefore, we call it as the linear search step. In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant. The main reason is that the linear search in Algorithm 1 makes fk+1 to be not always the greediest one [11], [12]. Hence, an advisable method is to control the step-size in the linear search step of Algorithm 1. Thus, various variants of boosting, such as the \u03b5-Boosting [15] which specifies the step-size as a fixed small positive number \u03b5 rather than using the linear search, RSBoosting[13] which multiplies a small regularized factor to the step-size deduced from the linear search and RTBoosting[14] which truncates the linear search in a small interval have been developed. It is obvious that the core difficulty of these schemes roots in how\n3 to select an appropriate step-size. If the step size is too large, then these algorithms may face the same problem as that of Algorithm 1. If the step size is too small, then the population convergence rate is also fairly slow.\nOther than the aforementioned strategies that focus on controlling the step-size of g\u2217k, Lin et al. [12] also derived a new backward type strategy, called the re-scale boosting (RBoosting), to improve the population convergence rate and consequently, the generalization capability of boosting. The core idea is that if the approximation (or learning) effect of the k-th iteration may not work as expected, then fk is regarded to be too aggressive. That is, if a new iteration is employed, then the previous estimator fk should be re-scaled. The following Algorithm 2 depicts the main idea of L2-RBoosting.\nAlgorithm 2 RBoosting Step 1(Initialization): Given data {(xi, yi) : i = 1, . . . ,m}, dictionary S, a set of shrinkage degree {\u03b1k}k \u2217\nk=1 where \u03b1k = 2/(k + u), u \u2208 N, iteration number k\u2217 and f0 \u2208 span(S). Step 2(Projection of gradient): Find g\u2217k \u2208 S such that\ng\u2217k = argmax g\u2208S |\u3008rk\u22121, g\u3009m|,\nwhere the residual rk\u22121 = y \u2212 fk\u22121 and y is a function satisfying y(xi) = yi. Step 3( Re-scaled linear search):\nfk = (1\u2212 \u03b1k)fk\u22121 + \u3008rsk\u22121, g\u2217k\u3009mg\u2217k,\nwhere the shrinkage residual rsk\u22121 = y \u2212 (1\u2212 \u03b1k)fk\u22121. Step 4 (Iteration): Increase k by one and repeat Step 2 and Step 3 if k < k\u2217.\nRemark 2.2: It is easy to see that\n\u3008rsk\u22121, g\u2217k\u3009m = arg min \u03b2k\u2208R ED((1\u2212 \u03b1k)fk\u22121 + \u03b2kg\u2217k).\nThis is the only difference between boosting and RBoosting. Here we call \u03b1k as the shrinkage degree. It can be found in the above Algorithm 2 that the shrinkage degree is considered as a parameter. L2-RBoosting stems from the \u201cgreedy algorithm with fixed relaxation\u201d [28] in nonlinear approximation. It is different from the L2-Boosting algorithm proposed in [24], which adopts the idea of \u201cX-greedy algorithm with relaxation\u201d [29]. In particular, we employ rk\u22121 in Step 2 to represent residual rather than the shrinkage residual rsk\u22121 in Step 3. Such a difference makes the design principles of RBoosting and the boosting algorithm in [24] to be totally distinct. In RBoosting, the algorithm comprises two steps: the projection of gradient step to find the optimum weak learner g\u2217k and the re-scale linear search step to fix its step-size \u03b2k. However, the boosting algorithm in [24] only concerns the optimization problem\narg min g\u2217k\u2208S,\u03b2k\u2208R\n\u2016(1\u2212 \u03b1k)fk\u22121 + \u03b2kg\u2217k\u20162m.\nThe main drawback is, to the best of our knowledge, the closed-form solution of the above optimization problem only holds for the L2 loss. When faced with other loss, the\nboosting algorithm in [24] cannot be efficiently numerical solved. However, it can be found in [12] that RBoosting is feasible for arbitrary loss. We are currently studying the more concrete comparison study between these two re-scale boosting algorithms [30].\nIt is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14]. Therefore, it is urgent to develop a feasible method to select the shrinkage degree. There are two ways to choose a good shrinkage degree value. The first one is to parameterize the shrinkage degree as in Algorithm 2. We set the shrinkage degree \u03b1k = 2/(k + u) and hope to choose an appropriate value of u via a certain parameter-selection strategy. The other one is to learn the shrinkage degree \u03b1k from the samples directly. As we are only concerned with L2-RBoosting in present paper, this idea can be primitively realized by the following Algorithm 3, which is called as the data-driven RBoosting (DDRBoosting).\nAlgorithm 3 DDRBoosting Step 1(Initialization): Given data {(xi, yi) : i = 1, . . . ,m}, dictionary S,iteration number k\u2217 and f \u20320 \u2208 span(S). Step 2(Projection of gradient): Find g\u2217k \u2208 S such that\ng\u2217k = argmax g\u2208S |\u3008rk\u22121, g\u3009m|,\nwhere residual rk\u22121 = y \u2212 f \u2032k\u22121 and y is a function satisfying y(xi) = yi. Step 3(Two dimensional linear search): Find \u03b1\u2217k and \u03b2 \u2217 k \u2208 R such that\nED((1\u2212\u03b1\u2217k)f \u2032k\u22121+\u03b2\u2217kg\u2217k) = inf (\u03b1k,\u03b2k)\u2208R2 ED((1\u2212\u03b1k)f \u2032k\u22121+\u03b2kg\u2217k)\nUpdate f \u2032k = (1\u2212 \u03b1\u2217k)f \u2032k\u22121 + \u03b2\u2217kg\u2217k. Step 4 (Iteration): Increase k by one and repeat Step 2 and Step 3 if k < k\u2217.\nThe above Algorithm 3 is motivated by the \u201cgreedy algorithm with free relaxation\u201d [31]. As far as the L2 loss is concerned, it is easy to deduce the close-form representation of f \u2032k+1 [28]. However, for other loss functions, we have not found any papers concerning the solvability of the optimization problem in step 3 of the Algorithm 3."}, {"heading": "III. THEORETICAL BEHAVIORS", "text": "In this section, we present some theoretical results concerning the shrinkage degree. Firstly, we study the relationship between shrinkage degree and generalization capability in L2RBoosting. The theoretical results reveal that the shrinkage degree plays a crucial role in L2-RBoosting for regression with finite samples. Secondly, we analyze the pros and cons of L2RBoosting and L2-DDRBoosting. It is shown that the potential performance of L2-RBoosting is somewhat better than that of L2-DDRBoosting. Finally, we propose an adaptive parameter-\n4 selection strategy for the shrinkage degree and theoretically verify its feasibility."}, {"heading": "A. Relationship between the generalization capability and shrinkage degree", "text": "At first, we give a few notations and concepts, which will be used throughout the paper. Let L1(S) := {f : f = \u2211 g\u2208S agg} endowed with the norm\n\u2016f\u2016L1(S) := inf \u2211 g\u2208S |ag| : f = \u2211 g\u2208S agg  . For r > 0, the space Lr1 is defined to be the set of all functions f such that, there exists h \u2208 span{S} such that\n\u2016h\u2016L1(S) \u2264 B, and \u2016f \u2212 h\u2016 \u2264 Bn \u2212r, (III.1)\nwhere \u2016 \u00b7 \u2016 denotes the uniform norm for the continuous function space C(X ). The infimum of all such B defines a norm for f on Lr1. It follows from [29] that (III.1) defines an interpolation space which has been widely used in nonlinear approximation [29], [26], [28].\nLet \u03c0M t denote the clipped value of t at \u00b1M , that is, \u03c0M t := min{M, |t|}sgn(t). Then it is obvious that [32] for all t \u2208 R and y \u2208 [\u2212M,M ] there holds\nE(\u03c0Mfk)\u2212 E(f\u03c1) \u2264 E(fk)\u2212 E(f\u03c1).\nBy the help of the above descriptions, we are now in a position to present the following Theorem 3.1, which depicts the role that the shrinkage degree plays in L2-RBoosting.\nTheorem 3.1: Let 0 < t < 1, and fk be the estimate defined in Algorithm 2. If f\u03c1 \u2208 Lr1, then for arbitrary k, u \u2208 N ,\nE(\u03c0Mfk)\u2212 E(f\u03c1) \u2264 C(M+B)2 ( 2 3u2+14u+20 8u+8 k\u22121+(m/k)\u22121 logm log 2\nt + n\u22122r ) holds with probability at least 1 \u2212 t, where C is a positive constant depending only on d.\nLet us first give some remarks of Theorem 3.1. If we set the number of iterations and the size of dictionary to satisfy k = O(m1/2), and n \u2265 O(m 14r ), then we can deduce a learning rate of \u03c0Mfk asymptotically as O(m\u22121/2 logm). This rate is independent of the dimension and is the same as the optimal \u201crecord\u201d for greedy learning [29] and boosting-type algorithms [14]. Furthermore, under the same assumptions, this rate is faster than those of boosting [9] and RTBoosting [14]. Thus, we can draw a rough conclusion that the learning rate deduced in Theorem 3.1 is tight. Under this circumstance, we think it can reveal the essential performance of L2-RBoosting.\nThen, it can be found in Theorem 3.1 that if u is finite and the number of samples is infinite, the shrinkage degree u does not affect the learning rate of L2-RBoosting, which means its generalization capability is independent of u. However, it is known that in the real world application, there are only finite number of samples available. Thus, u plays a crucial role in the learning process of L2-RBoosting in practice. Our results in Theorem 3.1 implies two simple guidance to deepen the understanding of L2-RBoosting. The first one is that there\nindeed exist an optimal u (may be not unique) minimizing the generalization error of L2-RBoosting. Specifically, we can deduce a concrete value of optimal u via minimizing 3u2+14u+20\n8u+8 . As it is very difficult to prove the optimality of the constant, we think it is more reasonable to reveal a rough trend for choosing u rather than providing a concrete value. The other one is that when u \u2192 \u221e, L2-RBoosting behaves as L2-Boosting, the learning rate cannot achieve O(m\u22121/2 logm). Thus, we indeed present a theoretical verification that L2-RBoosting outperforms L2-Boosting.\nB. Pros and cons of L2-RBoosting and L2-DDRBoosting There is only one parameter, k\u2217, in L2-DDRBoosting, as showed in Algorithm 3. This implies that L2-DDRBoosting improves the performance of L2-Boosting without tuning another additional parameter \u03b1k, which is superior to the other variants of boosting. The following Theorem 3.2 further shows that, as the same as L2-RBoosting, L2-DDRBoosting can also improve the generalization capability of L2-Boosting.\nTheorem 3.2: Let 0 < t < 1, and f \u2032k be the estimate defined in Algorithm 3. If f\u03c1 \u2208 Lr1, then for any arbitrary k \u2208 N , E(\u03c0Mf \u2032k)\u2212 E(f\u03c1) \u2264\nC(M + B)2 ( k\u22121 + (m/k)\u22121 logm log 2\nt + n\u22122r ) holds with probability at least 1 \u2212 t, where C is a constant depending only on d.\nBy Theorem 3.2, it seems that L2-DDRBoosting can perfectly solve the parameter selection problem in the re-scaletype boosting algorithm. However, we also show in the following that compared with L2-DDRBoosting, L2-RBoosting possesses an important advantage, which is crucial to guaranteeing the outperformance of L2-RBoosting. In fact, noting that L2-DDRBoosting depends on a two dimensional linear search problem (step 3 in Algorithm 3), the structure of the estimate (L1 norm), can not always be good. If the estimate f \u2032k\u22121 and g \u2217 k are almost linear dependent, then the values of \u03b1k and \u03b2k may be very large, which automatically leads a huge L1 norm of f \u2032k. We show in the following Proposition 3.3 that L2-RBoosting can avoid this phenomenon.\nProposition 3.3: If the fk is the estimate defined in Algorithm 2, then there holds\n\u2016fk\u2016L1(S) \u2264 C((M + \u2016h\u2016L1(S))k 1/2 + kn\u2212r).\nProposition 3.3 implies the estimate defined in Algorithm 2 possesses a controllable structure. This may significantly improve the learning performance of L2-RBoosting when faced with some specified weak learners. For this purpose, we need to introduce some definitions and conditions to qualify the weak learners.\nDefinition 3.4: Let (M, d) be a pseudo-metric space and T \u2282 M a subset. For every \u03b5 > 0, the covering number N (T, \u03b5, d) of T with respect to \u03b5 and d is defined as the minimal number of balls of radius \u03b5 whose union covers T , that is,\nN (T, \u03b5, d) := min l \u2208 N : T \u2282 l\u22c3\nj=1\nB(tj , \u03b5) \n5 for some {tj}lj=1 \u2282 M, where B(tj , \u03b5) = {t \u2208 M : d(t, tj) \u2264 \u03b5}.\nThe l2-empirical covering number of a function set is defined by means of the normalized l2-metric d2 on the Euclidean space Rd given in [33] with d2(a,b) =(\n1 m \u2211m i=1 |ai \u2212 bi|2 ) 1 2 for a = (ai)mi=1,b = (bi) m i=1 \u2208 Rm.\nDefinition 3.5: Let F be a set of functions on X , x = (xi) m i=1 \u2282 Xm, and let\nF|x := {(f(xi))mi=1 : f \u2208 F} \u2282 Rm.\nSet N2,x(F , \u03b5) = N (F|x, \u03b5, d2). The l2-empirical covering number of F is defined by\nN2(F , \u03b5) := sup m\u2208N sup x\u2208Sm N2,x(F , \u03b5), \u03b5 > 0.\nBefore presenting the main result in the subsection, we shall introduce the following Assumption 3.6.\nAssumption 3.6: Assume the l2-empirical covering number of span(S) satisfies\nlogN2(B1, \u03b5) \u2264 L\u03b5\u2212\u00b5, \u2200\u03b5 > 0,\nwhere BR = {f \u2208 span(S) : \u2016f\u2016L1(S) \u2264 R}.\nSuch an assumption is widely used in statistical learning theory. For example, in [33], Shi et al. proved that linear spanning of some smooth kernel functions satisfies Assumption 3.6 with a small \u00b5. By the help of Assumption 3.6, we can prove that the learning performance of L2-RBoosting can be essentially improved due to the good structure of the corresponding estimate.\nTheorem 3.7: Let 0 < t < 1, \u00b5 \u2208 (0, 1) and fk be the estimate defined in Algorithm 2. If f\u03c1 \u2208 Lr1 and Assumption 3.6 holds, then we have\nE(fk)\u2212 E(f\u03c1) \u2264\nC log 2\nt (3M+B)2 n\u22122r + k\u22121 +( (kn\u2212r +\u221ak)\u00b5 m ) 2\u2212\u00b5 2+\u00b5  . It can be found in Theorem 3.7 that if \u00b5 \u2192 0, then the learning rate of L2-RBoosting can be near to m\u22121. This depicts that, with good weak learners, L2-RBoosting can reach a fairly fast learning rate.\nC. Adaptive parameter-selection strategy for L2-RBoosting\nIn the previous subsection, we point out that L2-RBoosting is potentially better than L2-DDRBoosting. In consequence, how to select the parameter, u, is of great importance in L2RBoosting. We present an adaptive way to fix the shrinkage degree in this subsection and show that, the estimate based on such a parameter-selection strategy does not degrade the generalization capability very much. To this end, we split the samples Dm = (xi, yi)mi=1 into two parts of size [m/2] and m \u2212 [m/2], respectively (assuming m \u2265 2). The first half is denoted by Dlm (the learning set), which is used to construct the L2-RBoosting estimate fDlm,\u03b1k,k. The second half, denoted\nby Dvm (the validation set), is used to choose \u03b1k by picking \u03b1k \u2208 I := [0, 1] to minimize the empirical risk\n1\nm\u2212 [m/2] m\u2211 i=[m/2]+1 (yi \u2212 fDlm,\u03b1\u2217k,k) 2.\nThen, we obtain the estimate\nf\u2217Dlm,\u03b1k,k = fDlm,\u03b1 \u2217 k,k .\nSince y \u2208 [\u2212M,M ], a straightforward adaptation of [34, Th.7.1] yields that, for any \u03b4 > 0,\nE[\u2016f\u2217Dlm,\u03b1k,k\u2212f\u03c1\u2016 2 \u03c1] \u2264 1+\u03b4 inf \u03b1k\u2208I E[\u2016fDlm,\u03b1\u2217k,k\u2212f\u03c1\u2016 2 \u03c1]+C\nlogm\nm ,\nholds some positive constant C depending only on M , d and \u03b4. Immediately from Theorem 3.1, we can conclude:\nTheorem 3.8: Let f\u2217Dlm,\u03b1k,k be the adaptive L2-RBoosting estimate. If f\u03c1 \u2208 Lr1, then for arbitrary constants k, u \u2208 N ,\nE { E(\u03c0Mf\u2217Dlm,\u03b1k,k)\u2212 E(f\u03c1) } \u2264\nC(M + B)2 ( 2 3u2+14u+20 8u+8 k\u22121 + (m/k)\u22121 logm+ n\u22122r ) ,\nwhere C is an absolute positive constant."}, {"heading": "IV. NUMERICAL RESULTS", "text": "In this section, a series of simulations and real data experiments will be carried out to illustrate our theoretical assertions."}, {"heading": "A. Simulation experiments", "text": "In this part, we first introduce the simulation settings, including the data sets, weak learners and experimental environment. Secondly, we analyze the relationship between shrinkage degree and generalization capability for the proposed L2-RBoosting by means of ideal performance curve. Thirdly, we draw a performance comparison of L2-Boosting, L2-RBoosting and L2-DDRBoosting. The results illustrate that L2-RBoosting with an appropriate shrinkage degree outperforms other ones, especially for the high dimensional data simulations. Finally, we justify the feasibility of the adaptive parameter-selection strategy for shrinkage degree in L2RBoosting.\n1) Simulation settings: In the following simulations, we generate the data from the following model:\nY = m(X) + \u03c3 \u00b7 \u03b5, (IV.1)\nwhere \u03b5 is standard gaussian noise and independent of X . The noise level \u03c3 varies among in {0, 0.5, 1}, and X is uniformly distributed on [\u22122, 2]d with d \u2208 {1, 2, 10}. 9 typical regression functions are considered in this set of simulations, where these functions are the same as those in section IV of [24]. \u2022 m1(x) = 2 \u2217max(1,min(3 + 2 \u2217 x, 3\u2212 8 \u2217 x)),\n\u2022 m2(x) =\n{ 10 \u221a \u2212x sin(8\u03c0x) \u22120.25 \u2264 x < 0,\n0 else, ,\n\u2022 m3(x) = 3 \u2217 sin(\u03c0 \u2217 x/2),\n6 \u2022 m4(x1, x2) = x1 \u2217 sin(x21)\u2212 x2 \u2217 sin(x22),\n\u2022 m5(x1, x2) = 4/(1 + 4 \u2217 x21 + 4 \u2217 x22),\n\u2022 m6(x1, x2) = 6\u2212 2 \u2217min(3, 4 \u2217 x21 + 4 \u2217 |x2|), \u2022 m7(x1, . . . , x10) = 10\u2211 j=1 (\u22121)j\u22121xj sin(xj2),\n\u2022 m8(x1, . . . , x10) = m6(x1 + \u00b7 \u00b7 \u00b7+ x5, x6 + \u00b7 \u00b7 \u00b7+ x10),\n\u2022 m9(x1, . . . , x10) = m2(x1 + \u00b7 \u00b7 \u00b7+ x10).\nFor each regression function and each value of \u03c3 \u2208 {0, 0.5, 1}, we first generate a training set of size m = 500 and an independent test set , including m\u2032 = 1000 noiseless observations. We then evaluate the generalization capability of each boosting algorithm in terms of root mean squared error (RMSE).\nIt is known that the boosting trees algorithm requires the specification of two parameters. One is the number of splits (or the number of nodes) that are used for fitting each regression tree. The number of leaves equals the number of splits plus one. Specifying J splits corresponds to an estimate with up to J-way interactions. Hastie et al. [35] suggest that 4 \u2264 J \u2264 8 generally works well and the estimate is typically not sensitive to the exact choice of J within that range. Thus, in the following simulations, we use the CART [36] (with the number of splits J = 4) to build up the week learners for regression. Another parameter is the number of iterations or the number of trees to be fitted. A suitable value of iterations can range from a few dozen to several thousand, depending on the the shrinkage degree parameter and which data set we used. Considering the fact that we mainly focus on the impact of the shrinkage degree, the easiest way to do it is to select the theoretically optimal number of iterations via the test data set. More precisely, we select the number of iterations, k\u2217, as the best one according to Dm\u2032 directly. Furthermore, for the additional shrinkage degree parameter, \u03b1k = 2/(k + u), u \u2208 N, in L2-RBoosting, we create 20 equally spaced values of u in logarithmic space between 1 to 106.\nAll numerical studies are implemented using MATLAB R2014a on a Windows personal computer with Core(TM) i73770 3.40GHz CPUs and RAM 4.00GB, and the statistics are averaged based on 20 independent trails for each simulation.\n2) Relationship between shrinkage degree and generalization performance : For each given re-scale factor u \u2208 [1, 106], we employ L2-RBoosting to train the corresponding estimates on the whole training samples Dm, and then use the independent test samples Dm\u2032 to evaluate their generalization performance.\nFig.1-Fig.3 illustrate the performance curves of the L2RBoosting estimates for the aforementioned nine regression functions m1, . . . ,m9. It can be easily observed from these figures that, except for m8, u has a great influence on the learning performance of L2-RBoosting. Furthermore, the per-\n10 0\n10 2\n10 4\n10 6\n0.0308\n0.031\n0.0312\n0.0314\n0.0316\n0.0318\n0.032\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.205\n0.21\n0.215\n0.22\n0.225\n0.23\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.345\n0.35\n0.355\n0.36\n0.365\n0.37\n0.375\n0.38\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.081\n0.082\n0.083\n0.084\n0.085\n0.086\n0.087\n0.088\n0.089\n0.09\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.165\n0.17\n0.175\n0.18\n0.185\n0.19\n0.195\n0.2\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.255\n0.26\n0.265\n0.27\n0.275\n0.28\n0.285\n0.29\n0.295\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.0175\n0.018\n0.0185\n0.019\n0.0195\n0.02\n0.0205\n0.021\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.32\n0.34\n0.36\n0.38\n0.4\n0.42\n0.44\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n7 10 0 10 2 10 4 10 6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.7\n0.8\n0.9\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.68\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\nRe\u2212scale factor u\nTe st\ne rro\nr ( RM\nSE )\n10 0\n10 2\n10 4\n10 6\n0.84\n0.85\n0.86\n0.87\n0.88\n0.89\n0.9\n0.91\n0.92\n0.93\nRe\u2212scale factor u Te\nst e\nrro r (\nRM SE\n)\nFig. 3: Three rows denote the 1-dimension regression functions m7,m8,m9 and three columns indicate the noise level \u03c3 varies among in {0, 0.5, 1}, respectively.\nformance curves generally imply that there exists an optimal u, which may be not unique, to minimize the generalization error. This is consistent with our theoretical assertions. For m8, the test error curve of L2-RBoosting is \u201cflat\u201d with respect to u, that is, the generalization performance of L2-RBoosting is irrelevant with u. As the uniqueness of the optimal u is not imposed, such numerical observations do not count our previous theoretical conclusion. The reason can be concluded as follows. The first one is that in Theorem 3.1, we impose a relatively strong restriction to the regression function and m8 might be not satisfy it. The other one is that the adopted weak learner is too strong (we pre-set the number of splits J = 4). Over grown tree trained on all samples are liable to autocracy and re-scale operation does not bring performance benefits at all in such case. All these numerical results illustrate the importance of selecting an appropriate shrinkage degree in L2-RBoosting.\n3) Performance comparison of L2-Boosting, L2-RBoosting and L2-DDRBoosting: In this part, we compare the learning performances among L2-Boosting, L2-RBoosting and L2DDRBoosting. Table I-Table III document the generalization errors (RMSE) of L2-Boosting, L2-RBoosting and L2DDRBoosting for regression functions m1, . . . ,m9, respectively (the bold numbers denote the optimal performance). The standard errors are also reported (numbers in parentheses).\nForm the tables we can get clear results that except for the noiseless 1-dimensional cases, the performance of L2RBoosting dominates both L2-Boosting and L2-DDRBoosting for all regression functions by a large margin. Through this series of numerical studies, including 27 different learning tasks, firstly, we verify the second guidance deduced from Thm.I that L2-RBoosting outperforms L2-Boosting with finite sample available. Secondly, although L2-DDRBoosting can perfectly solve the parameter selection problem in the re-scaletype boosting algorithm, the table results also illustrate that L2-RBoosting endows better performance once an appropriate u is selected.\n4) Adaptive parameter-selection strategy for shrinkage degree: We employ the simulations to verify the feasibility of the\nproposed parameter-selection strategy. As described in subsection 3.3, we random split the train samples Dm = (Xi, Yi)500i=1 into two disjoint equal size subsets, i.e., a learning set and a validation set. We first train on the learning set Dlm to construct the L2-RBoosting estimates fDlm,\u03b1k,k and then use the validation set Dvm to choose the appropriate shrinkage degree \u03b1\u2217k and iteration k\n\u2217 by minimizing the validation risk. Thirdly, we retrain the obtained \u03b1\u2217k on the entire training set Dm to construct fDm,\u03b1\u2217k,k (Generally, if we have enough training samples at hand, this step is optional). Finally, an independent test set of 1000 noiseless observations are used to evaluate the performance of fDm,\u03b1\u2217k,k.\nTable IV-Table VI document the test errors (RMSE) for regression functions m1, . . . ,m9. The corresponding bold numbers denote the ideal generalization performance of the L2-RBoosting (choose optimal iteration k\u2217 and optimal shrinkage degree \u03b1\u2217k both according to minimize the test error via the test sets). We also report the standard errors (numbers in parentheses) of selected re-scale parameter u over 20 independent runs in order to check the stability of such parameterselection strategy. From the tables, we can easily find that the performance with such strategy approximates the ideal one. More important, comparing the mean values and standard errors of u with the performance curves in Fig.1-Fig.3, apart from m8, we can distinctly detect that the selected u values by the proposed parameter-selection strategy are all located near the low valleys.\n8"}, {"heading": "B. Real data experiments", "text": "We have verified that L2-RBoosting outperforms L2Boosting and L2-DDRBoosting on the 3\u00d79 = 27 different distributions in the previous simulations. We now further compare the learning performances of these boosted-type algorithms on six real data sets.\nThe first data set is a subset of the Shanghai Stock Price Index (SSPI), which can be extracted from http://www.gw.com.cn. This data set contains 2000 trading days\u2019 stock index which records five independent variables, i.e., maximum price, minimum price, closing price, day trading quota, day trading volume, and one dependent variable, i.e., opening price. The second one is the Diabetes data set[11]. This data set contains 442 diabetes patients that were measured on ten independent variables, i.e., age, sex, body mass index etc. and one response variable, i.e., a measure of disease progression. The third one is the Prostate cancer data set derived from a study of prostate cancer by Blake et al.[37]. The data set consists of the medical records of 97 patients who were about to receive a radical prostatectomy. The predictors are eight clinical measures, i.e., cancer volume, prostate weight, age etc. and one response variable, i.e., the logarithm of prostatespecific antigen. The fourth one is the Boston Housing data set created form a housing values survey in suburbs of Boston by Harrison[38]. This data set contains 506 instances which include thirteen attributions, i.e., per capita crime rate by town, proportion of non-retail business acres per town, average number of rooms per dwelling etc. and one response variable, i.e., median value of owner-occupied homes. The fifth one is the Concrete Compressive Strength (CCS) data set created from[39]. The data set contains 1030 instances including eight quantitative independent variables, i.e., age and ingredients etc. and one dependent variable, i.e., quantitative concrete compressive strength. The sixth one is the Abalone data set, which comes from an original study in [40] for predicting the age of abalone from physical measurements. The data set contains 4177 instances which were measured on eight independent variables, i.e., length, sex, height etc. and one response variable, i.e., the number of rings.\nSimilarly, we divide all the real data sets into two disjoint equal parts (except for the Prostate Cancer data set, which were divided into two parts beforehand: a training set with 67 observations and a test set with 30 observations). The first half serves as the training set and the second half serves as the test set. For each real data experiment, weak learners are changed to the decision stumps (specifying one split of each tree, J = 1) corresponding to an additive model with only main effects. Table VII documents the performance (test RMSE) comparison results of L2-Boosting, L2-RBoosting and L2-DDRBoosting on six real data sets, respectively (the bold numbers denote the optimal performance). It is observed from the table that the performance of L2-RBoosting with u selected via our recommended strategy outperforms both L2-Boosting and L2-DDRBoosting on all real data sets, especially for some data sets, i.e., Diabetes, Prostate and CCS, makes a great improvement.\n9"}, {"heading": "V. PROOFS", "text": "In this section, we provide the proofs of the main results. At first, we aim to prove Theorem 3.1. To this end, we shall give an error decomposition strategy for E(\u03c0Mfk) \u2212 E(f\u03c1). Using the similar methods that in [26], [41], we construct an f\u2217k \u2208 span(Dn) as follows. Since f\u03c1 \u2208 Lr1, there exists a h\u03c1 := \u2211n i=1 aigi \u2208 span(S) such that\n\u2016h\u03c1\u2016L1 \u2264 B, and \u2016f\u03c1 \u2212 h\u03c1\u2016 \u2264 Bn\u2212r. (V.1)\nDefine\nf\u22170 = 0, f \u2217 k =\n( 1\u2212 1\nk\n) f\u2217k\u22121 + \u2211n i=1 |ai|\u2016gi\u2016\u03c1\nk g\u2217k, (V.2)\nwhere\ng\u2217k := argmax g\u2208S\u2032\n\u2329 h\u03c1 \u2212 ( 1\u2212 1\nk\n) f\u2217k\u22121, g \u232a \u03c1 ,\nand\nD\u2032n := {gi(x)/\u2016gi\u2016\u03c1} n i=1 \u22c3 {\u2212gi(x)/\u2016gi\u2016\u03c1}ni=1\nwith gi \u2208 S. Let fk and f\u2217k be defined as in Algorithm 2 and (V.2), respectively. We have\nE(\u03c0Mfk)\u2212 E(f\u03c1) \u2264 E(f\u2217k )\u2212 E(f\u03c1) + Ez(\u03c0Mf,k)\u2212 ED(f\u2217k ) + ED(f\u2217k )\u2212 E(f\u2217k ) + E(\u03c0Mfk)\u2212 ED(fk).\nUpon making the short hand notations\nD(k) := E(f\u2217k )\u2212 E(f\u03c1),\nS(D, k) := ED(f\u2217k )\u2212 E(f\u2217k ) + E(\u03c0Mfk)\u2212 ED(\u03c0Mfk),\nand P(D, k) := ED(\u03c0Mfk)\u2212 ED(f\u2217k )\nrespectively for the approximation error, sample error and hypothesis error, we have\nE(\u03c0Mfk)\u2212 E(f\u03c1) = D(k) + S(D, k) + P(D, k). (V.3)\nTo bound estimate D(k), we need the following Lemma 5.1, which can be found in [26, Prop.1].\nLemma 5.1: Let f\u2217k be defined in (V.2). If f\u03c1 \u2208 Lr1, then\nD(k) \u2264 B2(k\u22121/2 + n\u2212r)2. (V.4)\nTo bound the hypothesis error, we need the following two lemmas. The first one can be found in [12], which is a direct generalization of [17, Lemma 2.3].\nLemma 5.2: Let j0 > 2 be a natural number. Suppose that three positive numbers c1 < c2 \u2264 j0, C0 be given. Assume that a sequence {an}\u221en=1 has the following two properties:\n(i) For all 1 \u2264 n \u2264 j0,\nan \u2264 C0n\u2212c1 ,\nand, for all n \u2265 j0,\nan \u2264 an\u22121 + C0(n\u2212 1)\u2212c1 .\n(ii) If for some v \u2265 j0 we have\nav \u2265 C0v\u2212c1 ,\nthen av+1 \u2264 av(1\u2212 c2/v).\nThen, for all n = 1, 2, . . . , we have\nan \u2264 21+ c21+c1 c2\u2212c1 C0n\u2212c1 .\nThe second one can be easily deduced from [17, Lemma 2.2].\nLemma 5.3: Let h \u2208 span(S), f,k be the estimate defined in Algorithm 2 and y(\u00b7) is an arbitrary function satisfying y(xi) = yi. Then, for arbitrary k = 1, 2, . . . , we have \u2016fk \u2212 y\u2016m \u2264 \u2016fk\u22121 \u2212 y\u2016m( 1\u2212\u03b1k ( 1\u2212 \u2016y \u2212 h\u2016m \u2016fk\u22121 \u2212 y\u2016m ) +2 ( \u03b1k(\u2016y\u2016m +\u2016h\u2016L1(S)) (1\u2212 \u03b1k)\u2016fk\u22121 \u2212 y\u2016m )2) .\nNow, we are in a position to present the hypothesis error estimate.\nLemma 5.4: Let fk be the estimate defined in Algorithm 2. Then, for arbitrary h \u2208 span(S) and u \u2208 N , there holds\n\u2016fk\u2212y\u20162m \u2264 2\u2016y\u2212h\u20162m+2(M+\u2016h\u2016L1(S)) 22\n3u2+14u+20 8u+8 k\u22121.\nProof: By Lemma 5.3, for k \u2265 1, we obtain\n\u2016fk \u2212 y\u2016m \u2212 \u2016y \u2212 h\u2016m \u2264 (1\u2212 \u03b1k)(\u2016fk\u22121 \u2212 y\u2016m \u2212 \u2016y \u2212 h\u2016m)\n+ C\u2016fk\u22121 \u2212 y\u2016m ( \u03b1k(\u2016y\u2016m + \u2016h\u2016L1(S)) \u2016fk\u22121 \u2212 y\u2016m )2 .\nLet ak+1 = \u2016fk \u2212 y\u2016m \u2212 \u2016y \u2212 h\u2016m.\nThen, by noting \u2016y\u2016m \u2264M , we have\nak+1 \u2264 (1\u2212 \u03b1k)ak + C \u03b12k(M + \u2016h\u2016L1(S))2\nak .\nWe plan to apply Lemma 5.2 to the sequence {an}. Let C0 = max{1, \u221a C}2(M + \u2016h\u2016l1(Dn)) According to the definitions of {ak}\u221ek=1 and fk, we obtain\na1 = \u2016y\u2016m \u2212 \u2016y \u2212 h\u2016m \u2264 2M + \u2016h\u2016L1(S) \u2264 C0,\nand ak+1 \u2264 ak + \u03b1k\u2016y\u2016m \u2264 ak + C0k\u22121/2.\nLet ak \u2265 C0k\u22121/2, since \u03b1k = 2k+u , we then obtain\nak \u2264 k + u\u2212 2 k + u ak\u22121+Cak\u22121k 4 C20(k + u)2 (M+\u2016h\u2016L1(S)) 2.\nThat is, ak \u2264 ak\u22121 ( 1\u2212 2\nk + u + C\n4k\nC20(k + u)2 (M + \u2016h\u2016L1(S))\n2 ) \u2264 (1\u2212 ( 1\n2 +\n2u+ 2\n(2 + u)2\n) 1\nk \u2212 1 ).\n10\nNow, it follows from Lemma 5.2 with c1 = 12 and c2 = 1 2 + 2u+2 (2+u)2 that\nan \u2264 max{1, \u221a C}2(M + \u2016h\u2016L1(S))2 1+ 3(u+2)2 8u+8 n\u22121/2.\nTherefore, we obtain\n\u2016fk \u2212 y\u2016m \u2264 \u2016y\u2212 h\u2016m + (M + \u2016h\u2016L1(S))2 3u2+14u+20 8u+8 k\u22121/2.\nThis finishes the proof of Lemma 5.4. Now we proceed the proof of Theorem 3.1.\nProof of Theorem 3.1: Based on Lemma 5.4 and the fact \u2016f\u2217k\u2016L1(S) \u2264 B [26, Lemma 1], we obtain\nP(D, k) \u2264 2ED(\u03c0Mfk)\u2212ED(f\u2217k ) \u2264 2(M+B)22 3u2+14u+20\n8u+8 k\u22121. (V.5)\nTherefore, both the approximation error and hypothesis error are deduced. The only thing remainder is to bound bound the sample error S(D, k). Upon using the short hand notations\nS1(D, k) := {ED(f\u2217k )\u2212 ED(f\u03c1)} \u2212 {E(f\u2217k )\u2212 E(f\u03c1)}\nand\nS2(D, k) := {E(\u03c0Mfk)\u2212 E(f\u03c1)} \u2212 {ED(\u03c0Mfk)\u2212 ED(f\u03c1)},\nwe write S(D, k) = S1(D, k) + S2(D, k). (V.6)\nIt can be found in [26, Prop.2] that for any 0 < t < 1, with confidence 1\u2212 t2 ,\nS1(D, k) \u2264 7(3M + B log 2t )\n3m +\n1 2 D(k) (V.7)\nIt also follows from [42, Eqs(A.10)] that\nS2(D, k) \u2264 1\n2 E(\u03c0Mfk)\u2212 E(f\u03c1) + log\n2\nt\nCk logm\nm (V.8)\nholds with confidence at least 1\u2212 t/2. Therefore, (V.3), (V.4), (V.5), (V.6), (V.7) and (V.8) yield that\nE(\u03c0Mfk)\u2212 E(f\u03c1) \u2264 C(M+B)2 ( 2 3u2+14u+20 8u+8 k\u22121+(m/k)\u22121 logm log 2\nt + n\u22122r ) holds with confidence at least 1\u2212 t. This finishes the proof of Theorem 3.1.\nProof of Theorem 3.2: It can be deduced from [17, Theorem 1.2] and the same method as in the proof of Theorem 3.1. For the sake of brevity, we omit the details.\nProof of Proposition 3.3: It is easy to check that\nfk = (1\u2212 \u03b1k)fk\u22121 + \u3008y \u2212 (1\u2212 \u03b1k)fk\u22121, gk\u30092gk.\nAs \u2016gk\u2016 \u2264 1, we obtain from the Ho\u0308lder inequality that\n\u3008y \u2212 (1\u2212 \u03b1k)fk\u22121, gk\u30092 \u2264 \u2016y \u2212 (1\u2212 \u03b1k)fk\u22121\u20162 \u2264 (1\u2212 \u03b1k)\u2016y \u2212 fk\u22121\u20162 + \u03b1kM.\nAs\n\u2016y \u2212 fk\u22121\u20162 \u2264 C(M + \u2016h\u2016L1(S))k \u22121/2 + n\u2212r,\nwe can obtain\n\u2016fk\u20161 \u2264 C((M + \u2016h\u2016L1(S))k 1/2 + kn\u2212r).\nThis finishes the proof of Proposition 3.3. Now we turn to prove Theorem 3.7. The following concentration inequality [43] plays a crucial role in the proof. Lemma 5.5: Let F be a class of measurable functions on Z. Assume that there are constants B, c > 0 and \u03b1 \u2208 [0, 1] such that \u2016f\u2016\u221e \u2264 B and Ef2 \u2264 c(Ef)\u03b1 for every f \u2208 F . If for some a > 0 and \u00b5 \u2208 (0, 2),\nlogN2(F , \u03b5) \u2264 a\u03b5\u2212\u00b5, \u2200\u03b5 > 0, (V.9)\nthen there exists a constant c\u2032p depending only on p such that for any t > 0, with probability at least 1\u2212 e\u2212t, there holds\nEf \u2212 1m \u2211m i=1 f(zi) \u2264 (V.10)\n1 2\u03b7 1\u2212\u03b1(Ef)\u03b1 + c\u2032\u00b5\u03b7 + 2 ( ct m ) 1 2\u2212\u03b1 + 18Btm , \u2200f \u2208 F ,\nwhere\n\u03b7 := max { c 2\u2212\u00b5 4\u22122\u03b1+\u00b5\u03b1 ( a m ) 2 4\u22122\u03b1+\u00b5\u03b1 , B 2\u2212\u00b5 2+\u00b5 ( a m ) 2 2+\u00b5 } .\nWe continue the proof of Theorem 3.7. Proof of Theorem 3.7: For arbitrary h \u2208 span(S),\nE(fk)\u2212 E(h) =E(fk)\u2212 E(h)\u2212 (ED(fk)\u2212 ED(h)) + ED(fk)\u2212 ED(h).\nSet GR := { g(z) = (\u03c0Mf(x)\u2212 y)2 \u2212 (h(x)\u2212 y)2 : f \u2208 BR } . (V.11) Using the obvious inequalities \u2016\u03c0Mf\u2016\u221e \u2264M , |y| \u2264M a.e., we get the inequalities\n|g(z)| \u2264 (3M + \u2016h\u2016L1(S)) 2\nand Eg2 \u2264 (3M + \u2016h\u2016L1(S)) 2Eg.\nFor g1, g2 \u2208 GR, it follows that\n|g1(z)\u2212 g2(z)| \u2264 (3M + \u2016h\u2016L1(S))|f1(x)\u2212 f2(x)|.\nThen\nN2(GR, \u03b5) \u2264 N2,x ( BR,\n\u03b5\n3M + \u2016h\u2016L1(S) ) \u2264 N2,x ( B1, \u03b5\nR(3M + \u2016h\u2016L1(S))\n) .\nUsing the above inequality and Assumption 3.6, we have\nlogN2(FR, \u03b5) \u2264 L(R(3M + \u2016h\u2016L1(S))) \u00b5\u03b5\u2212\u00b5.\nBy Lemma 5.5 with B = c = (3M + \u2016h\u2016L1(S))2, \u03b1 = 1 and a = L(R(3M + \u2016h\u20161))\u00b5, we know that for any t \u2208 (0, 1), with confidence 1 \u2212 t2 , there exists a constant C depending only on d such that for all g \u2208 GR\nEg\u2212 1 m m\u2211 i=1 g(zi) \u2264 1 2 Eg+c\u2032\u00b5\u03b7+20(3M+\u2016h\u2016L1(S)) 2 log 4/t m .\nHere\n\u03b7 = ((3M+\u2016h\u2016L1(S)) 2)\n2\u2212\u00b5 2+\u00b5 (L(R(3M + \u2016h\u2016L1(S)))\u00b5 m ) 2\u2212\u00b5 2+\u00b5 .\n11\nIt then follows from Proposition 3.3 that\nE(fk)\u2212 E(f\u03c1) \u2264\nC log 2\nt (3M + B)2 n\u22122r + k\u22121 +( (kn\u2212r +\u221ak)\u00b5 m ) 2\u2212\u00b5 2+\u00b5  . This finishes the proof of Theorem 3.7."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we draw a concrete analysis concerning how to determine the shrinkage degree into L2-RBoosting. The contributions can be concluded in four aspects. Firstly, we theoretically deduced the generalization error bound of L2RBoosting and demonstrated the importance of the shrinkage degree. It is shown that, under certain conditions, the learning rate of L2-RBoosting can reach O(m\u22121/2 logm), which is the same as the optimal \u201crecord\u201d for greedy learning and boosting-type algorithms. Furthermore, our result showed that although the shrinkage degree did not affect the learning rate, it determined the constant of the generalization error bound, and therefore, played a crucial role in L2-RBoosting learning with finite samples. Then, we proposed two schemes to determine the shrinkage degree. The first one is the conventional parameterized L2-RBoosting, and the other one is to learn the shrinkage degree from the samples directly (L2DDRBoosting). We further provided the theoretically optimality of these approaches. Thirdly, we compared these two approaches and proved that, although L2-DDRBoosting reduced the parameters, the estimate deduced from L2-RBoosting possessed a better structure (l1 norm). Therefore, for some special weak learners, L2-RBoosting can achieve better performance than L2-DDRBoosting. Fourthly, we developed an adaptive parameter-selection strategy for the shrinkage degree. Our theoretical results demonstrated that, L2-RBoosting with such a shrinkage degree selection strategy did not degrade the generalization capability very much. Finally, a series of numerical simulations and real data experiments have been carried out to verify our theoretical assertions. The obtained results enhanced the understanding of RBoosting and could provide guidance on how to utilize L2-RBoosting for regression tasks."}], "references": [{"title": "Greedy function approximation: a gradient boosting machine", "author": ["J. Friedman"], "venue": "Ann. Stat., vol. 29, no. 5, pp. 1189\u20131232, 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosting with the l2 loss: regression and classification", "author": ["P. Buhlmann", "B. Yu"], "venue": "J. Amer. Stat. Assoc., vol. 98, no. 462, pp. 324\u2013339, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "ICML, vol. 96, pp. 148\u2013156, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Ann. Stat., vol. 28, no. 2, pp. 337\u2013407, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Boosting algorithms: regularization, prediction and model fitting", "author": ["P. Buhlmann", "T. Hothorn"], "venue": "Stat. Sci., vol. 22, no. 4, pp. 477\u2013505, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Boosting methods for regression", "author": ["N. Duffy", "D. Helmbold"], "venue": "Mach. Learn., vol. 47, no. 2-3, pp. 153\u2013200, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "Inform. Comput., vol. 121, no. 2, pp. 256\u2013285, 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "The strength of weak learnability", "author": ["R.E. Schapire"], "venue": "Mach. Learn., vol. 5, no. 2, pp. 1997\u20132027, 1990.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Adaboost is consistent", "author": ["P. Bartlett", "M. Traskin"], "venue": "J. Mach. Learn. Res., vol. 8, pp. 2347\u20132368, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Lower bounds for the rate of convergence of greedy algorithms", "author": ["E.D. Livshits"], "venue": "Izvestiya: Mathematics, vol. 73, no. 6, pp. 1197\u20131215, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibsirani"], "venue": "Ann. Stat., vol. 32, no. 2, pp. 407\u2013451, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Re-scale boosting for regression and classification", "author": ["S.B. Lin", "Y. Wang", "L. Xu"], "venue": "ArXiv:1505.01371, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Characterizing l2 boosting", "author": ["J. Ehrlinger", "H. Ishwaran"], "venue": "Ann. Stat., vol. 40, no. 2, pp. 1074\u20131101, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Boosting with early stopping: convergence and consistency", "author": ["T. Zhang", "B. Yu"], "venue": "Ann. Stat., vol. 33, no. 4, pp. 1538\u20131579, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Forward stagewise regression and the monotone lasso", "author": ["T. Hastie", "J. Taylor", "R. Tibshirani", "G. Walther"], "venue": "Electron. J. Stat., vol. 1, pp. 1\u201329, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Stagewise lasso", "author": ["P. Zhao", "B. Yu"], "venue": "J. Mach. Learn. Res., vol. 8, pp. 2701\u20132726, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Relaxation in greedy approximation", "author": ["V. Temlyakov"], "venue": "Constr. Approx., vol. 28, no. 1, pp. 1\u201325, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequential greedy approximation for certain convex optimization problems", "author": ["T. Zhang"], "venue": "IEEE Trans. Inform. Theory, vol. 49, no. 3, pp. 682\u2013691, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "A working guide to boosted regression trees", "author": ["J. Elith", "J.R. Leathwick", "T. Hastie"], "venue": "J. Anim. Ecol., vol. 77, no. 4, pp. 802\u2013813, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn., vol. 24, no. 2, pp. 123\u2013140, 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Linearly combining density estimators via stacking", "author": ["P. Smyth", "D. Wolpert"], "venue": "Mach. Learn., vol. 36, no. 1-2, pp. 59\u201383, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Bayesian methods for adaptive models", "author": ["D. MacKay"], "venue": "PhD thesis, California Institute of Technology, 1991.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1991}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Mach. Learn., vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "An l2 boosting algorithm for estimation of a regression function", "author": ["A. Bagirov", "C. Clausen", "M. Kohler"], "venue": "IEEE Trans. Inform. Theory, vol. 56, no. 3, pp. 1417\u20131429, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Some theory for generalized boosting algorithms", "author": ["P. Bickel", "Y. Ritov", "A. Zakai"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 705\u2013732, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning capability of relaxed greedy algorithms", "author": ["S.B. Lin", "Y.H. Rong", "X.P. Sun", "Z.B. Xu"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 10, pp. 1598\u20131608, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Some remarks on greedy algorithms", "author": ["R. DeVore", "V. Temlyakov"], "venue": "Adv. Comput. Math., vol. 5, no. 1, pp. 173\u2013187, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Greedy approximation", "author": ["V. Temlyakov"], "venue": "Acta Numer., vol. 17, pp. 235\u2013 409, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation and learning by greedy algorithms", "author": ["A. Barron", "A. Cohen", "W. Dahmen", "R. DeVore"], "venue": "Ann. Stat., vol. 36, no. 1, pp. 64\u201394, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "A comparison study between different re-scale boosting", "author": ["L. Xu", "S.B. Lin", "Y. Wang", "Z.B. Xu"], "venue": "Manuscript, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Greedy approximation in convex optimization", "author": ["V. Temlyakov"], "venue": "Constr. Approx., pp. 1\u201328, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximation with polynomial kernels and svm classifiers", "author": ["D.X. Zhou", "K. Jetter"], "venue": "Adv. Comput. Math., vol. 25, no. 1-3, pp. 323\u2013344, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Concentration estimates for learning with l1-regularizer and data dependent hypothesis spaces", "author": ["L. Shi", "Y.L. Feng", "D.X.Zhou"], "venue": "Appl. Comput. Harmon. Anal., vol. 31, no. 2, pp. 286\u2013302, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "A distributionfree theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer Science & Business Media, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "The elements of statistical learning", "author": ["T. Hastie", "R.Tibshirani", "J. Friedman"], "venue": "New York: springer, 2001.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "C. Stone", "R. Olshen"], "venue": "CRC press, 1984.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1984}, {"title": "UCI} repository of machine learning databases", "author": ["C. Blake", "C. Merz"], "venue": "1998.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Hedonic prices and the demand for clean air", "author": ["D. Harrison", "D.L. Rubinfeld"], "venue": "J. Environ. Econ., vol. 5, no. 1, pp. 81\u2013102, 1978.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1978}, {"title": "Modeling of strength of high performance concrete using artificial neural networks", "author": ["I.C. Ye"], "venue": "Cement and Concrete Research, vol. 28, no. 12, pp. 1797\u20131808, 1998.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1808}, {"title": "The population biology of abalone (haliotis species) in tasmania. i. blacklip abalone (h. rubra) from the north coast and islands of bass strait", "author": ["W.J. Nash", "T.L. Sellers", "S.R. Talbot", "A.J. Cawthorn", "W.B. Ford"], "venue": "Sea Fisheries Division, Technical Report, no. 48, 1994.  12", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1994}, {"title": "Greedy metric in orthogonal greedy learning", "author": ["L. Xu", "S.B. Lin", "J.S. Zeng", "Z.B. Xu"], "venue": "Manuscript, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient greedy learning for massive data", "author": ["C. Xu", "S.B. Lin", "J. Fang", "R.Z. Li"], "venue": "Manuscript, 2014.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-kernel regularized classifiers", "author": ["Q. Wu", "Y. Ying", "D.X. Zhou"], "venue": "J. Complex., vol. 23, no. 1, pp. 108\u2013134, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "From the statistical viewpoint, boosting can be viewed as a form of functional gradient decent [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Typically, L2-Boosting [2], [3] can be interpreted as an stepwise additive learning scheme that concerns the problem of minimizing the L2 risk.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Typically, L2-Boosting [2], [3] can be interpreted as an stepwise additive learning scheme that concerns the problem of minimizing the L2 risk.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "Although the universal consistency of boosting has already been verified in [9], the numerical convergence rate of boosting is a bit slow [9], [10].", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "Although the universal consistency of boosting has already been verified in [9], the numerical convergence rate of boosting is a bit slow [9], [10].", "startOffset": 138, "endOffset": 141}, {"referenceID": 9, "context": "Although the universal consistency of boosting has already been verified in [9], the numerical convergence rate of boosting is a bit slow [9], [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": "The main reason for such a drawback is that the step-size derived via linear search in boosting can not always guarantee the most appropriate one [11], [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "The main reason for such a drawback is that the step-size derived via linear search in boosting can not always guarantee the most appropriate one [11], [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China the regularized boosting via shrinkage (RSBoosting) [13], regularized boosting via truncation (RTBoosting) [14] and \u03b5Boosting [15] have been developed via introducing additional parameters to control the step-size.", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China the regularized boosting via shrinkage (RSBoosting) [13], regularized boosting via truncation (RTBoosting) [14] and \u03b5Boosting [15] have been developed via introducing additional parameters to control the step-size.", "startOffset": 213, "endOffset": 217}, {"referenceID": 14, "context": "Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China the regularized boosting via shrinkage (RSBoosting) [13], regularized boosting via truncation (RTBoosting) [14] and \u03b5Boosting [15] have been developed via introducing additional parameters to control the step-size.", "startOffset": 232, "endOffset": 236}, {"referenceID": 0, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "Motivated by the recent development of relaxed greedy algorithm [17] and sequential greedy algorithm [18], Lin et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Motivated by the recent development of relaxed greedy algorithm [17] and sequential greedy algorithm [18], Lin et al.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "[12] introduced a new variant of boosting named as the rescale boosting (RBoosting).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Both theoretical analysis and experimental results in [12] implied that RBoosting is better than boosting, at least for the L2 loss.", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "[19] showed that 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "1 is a feasible choice of \u03b5 in \u03b5-Boosting; B\u00fchlmann and Hothorn [5] recommended the selection of 0.", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "1 for the regularization parameter in RSBoosting; Zhang and Yu [14] proved that O(k\u22122/3) is a good value of the truncated parameter in RTBoosting, where k is the number of iterations.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 96, "endOffset": 99}, {"referenceID": 20, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 158, "endOffset": 162}, {"referenceID": 23, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 109, "endOffset": 112}, {"referenceID": 24, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 125, "endOffset": 128}, {"referenceID": 25, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "where \u03c6 : R \u00d7 R \u2192 R+ is called a loss function [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "Previous study [2] showed that L2Boosting can successfully tackle this problem.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Then the gradient descent view of L2-Boosting [1] can be interpreted as follows.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 83, "endOffset": 86}, {"referenceID": 26, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "The main reason is that the linear search in Algorithm 1 makes fk+1 to be not always the greediest one [11], [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "The main reason is that the linear search in Algorithm 1 makes fk+1 to be not always the greediest one [11], [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Thus, various variants of boosting, such as the \u03b5-Boosting [15] which specifies the step-size as a fixed small positive number \u03b5 rather than using the linear search, RSBoosting[13] which multiplies a small regularized factor to the step-size deduced from the linear search and RTBoosting[14] which truncates the linear search in a small interval have been developed.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "Thus, various variants of boosting, such as the \u03b5-Boosting [15] which specifies the step-size as a fixed small positive number \u03b5 rather than using the linear search, RSBoosting[13] which multiplies a small regularized factor to the step-size deduced from the linear search and RTBoosting[14] which truncates the linear search in a small interval have been developed.", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Thus, various variants of boosting, such as the \u03b5-Boosting [15] which specifies the step-size as a fixed small positive number \u03b5 rather than using the linear search, RSBoosting[13] which multiplies a small regularized factor to the step-size deduced from the linear search and RTBoosting[14] which truncates the linear search in a small interval have been developed.", "startOffset": 287, "endOffset": 291}, {"referenceID": 11, "context": "[12] also derived a new backward type strategy, called the re-scale boosting (RBoosting), to improve the population convergence rate and consequently, the generalization capability of boosting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "L2-RBoosting stems from the \u201cgreedy algorithm with fixed relaxation\u201d [28] in nonlinear approximation.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "It is different from the L2-Boosting algorithm proposed in [24], which adopts the idea of \u201cX-greedy algorithm with relaxation\u201d [29].", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "It is different from the L2-Boosting algorithm proposed in [24], which adopts the idea of \u201cX-greedy algorithm with relaxation\u201d [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "Such a difference makes the design principles of RBoosting and the boosting algorithm in [24] to be totally distinct.", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "However, the boosting algorithm in [24] only concerns the optimization problem", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "When faced with other loss, the boosting algorithm in [24] cannot be efficiently numerical solved.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "However, it can be found in [12] that RBoosting is feasible for arbitrary loss.", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "We are currently studying the more concrete comparison study between these two re-scale boosting algorithms [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 256, "endOffset": 260}, {"referenceID": 12, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 300, "endOffset": 304}, {"referenceID": 13, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 345, "endOffset": 349}, {"referenceID": 30, "context": "The above Algorithm 3 is motivated by the \u201cgreedy algorithm with free relaxation\u201d [31].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "As far as the L2 loss is concerned, it is easy to deduce the close-form representation of f \u2032 k+1 [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "It follows from [29] that (III.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "1) defines an interpolation space which has been widely used in nonlinear approximation [29], [26], [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "1) defines an interpolation space which has been widely used in nonlinear approximation [29], [26], [28].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "1) defines an interpolation space which has been widely used in nonlinear approximation [29], [26], [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 31, "context": "Then it is obvious that [32] for all t \u2208 R and y \u2208 [\u2212M,M ] there holds", "startOffset": 24, "endOffset": 28}, {"referenceID": 28, "context": "This rate is independent of the dimension and is the same as the optimal \u201crecord\u201d for greedy learning [29] and boosting-type algorithms [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "This rate is independent of the dimension and is the same as the optimal \u201crecord\u201d for greedy learning [29] and boosting-type algorithms [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "Furthermore, under the same assumptions, this rate is faster than those of boosting [9] and RTBoosting [14].", "startOffset": 84, "endOffset": 87}, {"referenceID": 13, "context": "Furthermore, under the same assumptions, this rate is faster than those of boosting [9] and RTBoosting [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "The l2-empirical covering number of a function set is defined by means of the normalized l2-metric d2 on the Euclidean space R given in [33] with d2(a,b) = ( 1 m \u2211m i=1 |ai \u2212 bi| ) 1 2 for a = (ai)i=1,b = (bi) m i=1 \u2208 R.", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "For example, in [33], Shi et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The second half, denoted by D m (the validation set), is used to choose \u03b1k by picking \u03b1k \u2208 I := [0, 1] to minimize the empirical risk", "startOffset": 96, "endOffset": 102}, {"referenceID": 23, "context": "9 typical regression functions are considered in this set of simulations, where these functions are the same as those in section IV of [24].", "startOffset": 135, "endOffset": 139}, {"referenceID": 34, "context": "[35] suggest that 4 \u2264 J \u2264 8 generally works well and the estimate is typically not sensitive to the exact choice of J within that range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Thus, in the following simulations, we use the CART [36] (with the number of splits J = 4) to build up the week learners for regression.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "2) Relationship between shrinkage degree and generalization performance : For each given re-scale factor u \u2208 [1, 10], we employ L2-RBoosting to train the corresponding estimates on the whole training samples Dm, and then use the independent test samples Dm\u2032 to evaluate their generalization performance.", "startOffset": 109, "endOffset": 116}, {"referenceID": 9, "context": "2) Relationship between shrinkage degree and generalization performance : For each given re-scale factor u \u2208 [1, 10], we employ L2-RBoosting to train the corresponding estimates on the whole training samples Dm, and then use the independent test samples Dm\u2032 to evaluate their generalization performance.", "startOffset": 109, "endOffset": 116}, {"referenceID": 10, "context": "The second one is the Diabetes data set[11].", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "The fourth one is the Boston Housing data set created form a housing values survey in suburbs of Boston by Harrison[38].", "startOffset": 115, "endOffset": 119}, {"referenceID": 38, "context": "The fifth one is the Concrete Compressive Strength (CCS) data set created from[39].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "The sixth one is the Abalone data set, which comes from an original study in [40] for predicting the age of abalone from physical measurements.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "Using the similar methods that in [26], [41], we construct an f\u2217 k \u2208 span(Dn) as follows.", "startOffset": 34, "endOffset": 38}, {"referenceID": 40, "context": "Using the similar methods that in [26], [41], we construct an f\u2217 k \u2208 span(Dn) as follows.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "The first one can be found in [12], which is a direct generalization of [17, Lemma 2.", "startOffset": 30, "endOffset": 34}, {"referenceID": 42, "context": "The following concentration inequality [43] plays a crucial role in the proof.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "Assume that there are constants B, c > 0 and \u03b1 \u2208 [0, 1] such that \u2016f\u2016\u221e \u2264 B and Ef \u2264 c(Ef) for every f \u2208 F .", "startOffset": 49, "endOffset": 55}], "year": 2015, "abstractText": "Re-scale boosting (RBoosting) is a variant of boosting which can essentially improve the generalization performance of boosting learning. The key feature of RBoosting lies in introducing a shrinkage degree to re-scale the ensemble estimate in each gradient-descent step. Thus, the shrinkage degree determines the performance of RBoosting. The aim of this paper is to develop a concrete analysis concerning how to determine the shrinkage degree in L2-RBoosting. We propose two feasible ways to select the shrinkage degree. The first one is to parameterize the shrinkage degree and the other one is to develope a data-driven approach of it. After rigorously analyzing the importance of the shrinkage degree in L2-RBoosting learning, we compare the pros and cons of the proposed methods. We find that although these approaches can reach the same learning rates, the structure of the final estimate of the parameterized approach is better, which sometimes yields a better generalization capability when the number of sample is finite. With this, we recommend to parameterize the shrinkage degree of L2RBoosting. To this end, we present an adaptive parameterselection strategy for shrinkage degree and verify its feasibility through both theoretical analysis and numerical verification. The obtained results enhance the understanding of RBoosting and further give guidance on how to use L2-RBoosting for regression tasks.", "creator": "LaTeX with hyperref package"}}}