{"id": "1506.00482", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Learning Regular Languages over Large Ordered Alphabets", "abstract": "This work is toward concerned pugwash with regular steinfels languages archchancellor defined street-porter over larryl large 1.3950 alphabets, either x31 infinite or just too feltz large orha to be vedia expressed enumeratively. ngamu We resolves define neukomm a generic model maiorescu where transitions are 9066 labeled embers by kalaa elements non-random of durfort a http://www.healthscout.com finite partition of marquessate the alphabet. We then longline extend crankcase Angluin ' s L * algorithm segal for learning hard-edged regular pasteurizing languages kktv from .619 examples desbrisay for such automata. We have implemented london-born this 42.02 algorithm and xuanwei we demonstrate its 800-678-1147 behavior burstyn where slaughterers the undecided alphabet gernrode is a subset further of the cheeky natural or acvb real mid-pacific numbers. l\u00fathien We sketch galliamova the gunnell extension 2nd-century of the algorithm to 50,000-capacity a mazeroski class of 262.50 languages dinaburg over partially ordered alphabets.", "histories": [["v1", "Mon, 1 Jun 2015 13:12:54 GMT  (32kb)", "https://arxiv.org/abs/1506.00482v1", null], ["v2", "Wed, 16 Sep 2015 19:04:30 GMT  (44kb)", "http://arxiv.org/abs/1506.00482v2", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.FL", "authors": ["irini-eleftheria mens", "oded maler"], "accepted": false, "id": "1506.00482"}, "pdf": {"name": "1506.00482.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["O. MALER"], "emails": ["irini-eleftheria.mens@imag.fr", "oded.maler@imag.fr"], "sections": [{"heading": null, "text": "Introduction\nThe main contribution of this paper is a generic algorithm for learning regular languages defined over a large alphabet \u03a3. Such an alphabet can be infinite, like N or R or just so large, like Bn for very large n or large subsets of N, so that it is impossible or impractical to treat it in an enumerative way, that is, to write down the entries of the transition function \u03b4(q, a) for every a \u2208 \u03a3. The obvious solution is to use a symbolic representation where transitions are labeled by predicates which are applicable to the alphabet in question. Learning algorithms infer an automaton from a finite set of words (the sample) for which membership is known. Over small alphabets, the sample should include the set S of all the shortest words that lead to each state (access sequences) and, in addition, the set S \u00b7\u03a3 of all their \u03a3-continuations. Over large alphabets this is not a practical option and as an alternative we develop a symbolic learning algorithm over symbolic words which are only partially backed up by the sample. In a sense, our algorithm is a combination of automaton learning and learning of non-temporal predicates. Before getting technical, let us discuss briefly some motivation. Finite automata are among the corner stones of Computer Science. From a practical point of view they are used routinely in various domains ranging from syntactic analysis, design of user interfaces or administrative procedures to implementation of digital hardware and verification of software and hardware protocols. Regular languages admit a very\n2012 ACM CCS: [Theory of computation]: Formal languages and automata theory; Theory and algorithms for application domains\u2014Machine learning theory.\nKey words and phrases: symbolic automata, active learning. This paper is an extended version of [MM14].\nLOGICAL METHODS l IN COMPUTER SCIENCE DOI:10.2168/LMCS-11(3:13)2015\nc\u00a9 I-E. Mens and O. Maler CC\u00a9 Creative Commons\nnice, clean and comprehensive theory where different formalisms such as automata, logic, regular expressions, semigroups and grammars are shown to be equivalent. The problem of learning automata from examples was introduced already in 1956 by Moore [Moo56]. This problem, like the problem of automaton minimization, is closely related to the Nerode right-congruence relation over words associated with every language or sequential function [Ner58]. This relation declares two input histories as equivalent if they lead to the same future continuations, thus providing a crisp characterization of what a state in a dynamical system is in terms of observable input-output behavior. All algorithms for learning automata from examples, starting with the seminal work of Gold [Gol72] and culminating in the well- known L\u2217 algorithm of Angluin [Ang87] are based on this concept [DlH10].\nOne weakness, however, of the classical theory of regular languages is that it is rather \u201cthin\u201d and \u201cflat\u201d. In other words, the alphabet is often considered as a small set devoid of any additional structure. On such alphabets, classical automata are good for expressing and exploring the temporal (sequential, monoidal) dimension embodied by the concatenation operations, but less good in expressing \u201chorizontal\u201d relationships. To make this statement more concrete, consider the verification of a system consisting of n automata running in parallel, making independent as well as synchronized transitions. To express the set of joint behaviors of this product of automata as a formal language, classical theory will force you to use the exponential alphabet of global states and indeed, a large part of verification is concerned with fighting this explosion using constructs such as BDDs and other logical forms that exploit the sparse interaction among components. This is done, however, without a real interaction with classical formal language theory (one exception is the theory of traces [DR95] which attempts to treat this issue but in a very restricted context).\nThese and other considerations led us to use symbolic automata as a generic framework for recognizing languages over large alphabets where transitions outgoing from a state are labeled, semantically speaking, by subsets of the alphabet. These subsets are expressed syntactically according to the specific alphabet used: Boolean formulae when \u03a3 = Bn or by some classes of inequalities when \u03a3 \u2286 N or \u03a3 \u2286 R. Determinism and completeness of the transition relation, which are crucial for learning and minimization, can be enforced by requiring that the subsets of \u03a3 that label the transitions outgoing from a given state form a partition of the alphabet. Such symbolic automata have been used in the past for Boolean vectors [HJJ+95] and have been studied extensively in recent years as acceptors and transducers where transitions are guarded by predicates of various theories [HV11, VHL+12]. Readers working on program verification or hybrid automata are, of course, aware of automata with symbolic transition guards but it should be noted that in the model that we use, no auxiliary variables are added to the automaton. Let us stress this point by looking at a popular extension of automata to infinite alphabets, initiated in [KF94] using register automata to accept data languages (see [BLP10] for a good exposition of theoretical properties and [HSJC12] for learning algorithms). In that framework, the automaton is augmented with additional registers that can store some input letters. The registers can then be compared with newly-read letters and influence transitions. With register automata one can express, for example, the requirement that the password at login is the same as the password at sign-up. This very restricted use of memory makes register automata much simpler than more notorious automata with variables whose emptiness problem is typically undecidable. The downside is that beyond equality they do not really exploit the potential richness of the alphabets and their corresponding theories.\nOur approach is different: we do allow the values of the input symbols to influence transitions via predicates, possibly of a restricted complexity. These predicates involve domain constants and they partition the alphabet into finitely many classes. For example, over the integers a state may have transitions labeled by conditions of the form c1 \u2264 x \u2264 c2 which give real (but of limited resolution) access to the input domain. On the other hand, we insist on a finite (and small) memory so that the exact value of x cannot be registered and has no future influence beyond the transition it has triggered. Many control systems, artificial (sequential machines working on quantized numerical inputs) as well as natural (central nervous system, the cell), are believed to operate in this manner. The automata that we use, like the symbolic automata and transducers studied in [HV11, VHL+12, VB12], are geared toward languages recognized by automata having a large alphabet and a relativelysmall state space. We then develop a symbolic version of Angluin\u2019s L\u2217 algorithm for learning regular sets from queries and counter-examples whose output is a symbolic automaton. The main difference relative to the concrete algorithm is that in the latter, every transition \u03b4(q, a) in a conjectured automaton has at least one word in the sample that exercises it. In the symbolic case, a transition \u03b4(q,a) where a stands for a set of concrete symbols, will be backed up in the sample only by a subset of a. Thus, unlike concrete algorithms where a counter-example always leads to a discovery of one or more new states, in our algorithm it may sometimes only modify the boundaries between partition blocks without creating new states. There are some similarities between our work and another recent adaptation of the L\u2217 algorithm to symbolic automata, the \u03a3\u2217 algorithm of [BB13]. This work is incomparable to ours as they use a richer model of transducers and more general predicates on inputs and outputs. Consequently their termination result is weaker and is relative to the termination of the counter-example guided abstraction refinement procedure. The rest of the paper is organized as follows. In Section 1 we provide a quick summary of learning algorithms over small alphabets. In Section 2 we define symbolic automata and then extend the structure which underlies all automaton learning algorithms, namely the observation table, to be symbolic, where symbolic letters represent sets, and where entries in the table are supported only by partial evidence. In Section 4 we write down a symbolic learning algorithm, an adaptation of L\u2217 for totally ordered alphabets such as R or N and illustrate the behavior of a prototype implementation. The algorithm is then extended to languages over partially ordered alphabets such as Nd and Rd where in each state, the labels of outgoing transition from a monotone partition of the alphabet are represented by finitely many points. We conclude by a discussion of past and future work."}, {"heading": "1. Learning Regular Sets", "text": "We briefly survey Angluin\u2019s L\u2217 algorithm [Ang87] for learning regular sets from membership queries and counter-examples, with slightly modified definitions to accommodate for its symbolic extension. Let \u03a3 be a finite alphabet and let \u03a3\u2217 be the set of sequences (words) over \u03a3. Any order relation < over \u03a3 can be naturally lifted to a lexicographic order over \u03a3\u2217. With a language L \u2286 \u03a3\u2217 we associate a characteristic function f : \u03a3\u2217 \u2192 {+,\u2212}, where f(w) = + if the word w \u2208 \u03a3\u2217 belongs to L and f(w) = \u2212, otherwise. A deterministic finite automaton over \u03a3 is a tuple A = (\u03a3, Q, \u03b4, q0, F ), where Q is a non-empty finite set of states, q0 \u2208 Q is the initial state, \u03b4 : Q \u00d7 \u03a3 \u2192 Q is the transition function, and F \u2286 Q is the set of final or accepting states. The transition function \u03b4 can\nbe extended to \u03b4 : Q\u00d7 \u03a3\u2217 \u2192 Q, where \u03b4(q, \u01eb) = q, and \u03b4(q, u \u00b7 a) = \u03b4(\u03b4(q, u), a) for q \u2208 Q, a \u2208 \u03a3 and u \u2208 \u03a3\u2217. A word w \u2208 \u03a3\u2217 is accepted by A if \u03b4(q0, w) \u2208 F , otherwise w is rejected. The language recognized by A is the set of all accepted words and is denoted by L(A). Learning algorithms, represented by the learner, are designed to infer an unknown regular language L (the target language). The learner aims to construct a finite automaton that recognizes L by gathering information from the teacher. The teacher knows L and can provide information about it. It can answer two types of queries: membership queries, i.e., whether a given word belongs to the target language, and equivalence queries, i.e., whether a conjectured automaton suggested by the learner is the right one. If this automaton fails to accept L the teacher responds to the equivalence query by a counter-example, a word miss-classified by the conjectured automaton. In the L\u2217 algorithm, the learner starts by asking membership queries. All information provided is suitably gathered in a table structure, the observation table. Then, when the information is sufficient, the learner constructs a hypothesis automaton and poses an equivalence query to the teacher. If the answer is positive then the algorithm terminates and returns the conjectured automaton. Otherwise the learner accommodates the information provided by the counter-example into the table, asks additional membership queries until it can suggest a new hypothesis and so on, until termination. A prefix-closed set S \u228e R \u2282 \u03a3\u2217 is a balanced \u03a3-tree if \u2200a \u2208 \u03a3: 1) For every s \u2208 S s \u00b7 a \u2208 S \u222a R, and 2) For every r \u2208 R, r \u00b7 a 6\u2208 S \u222a R. Elements of R are called boundary elements or leaves. 1\nDefinition 1.1 (Observation Table). An observation table is a tuple T = (\u03a3, S,R,E, f) such that \u03a3 is an alphabet, S\u222aR is a balanced \u03a3-tree, E is a subset of \u03a3\u2217 and f : (S\u222aR)\u00b7E \u2192 {\u2212,+} is the classification function, a restriction of the characteristic function of the target language L.\nThe set (S \u222aR) \u00b7E is the sample associated with the table, that is, the set of words whose membership is known. The elements of S admit a tree structure isomorphic to a spanning tree of the transition graph rooted in the initial state. Each s \u2208 S corresponds to a state q of the automaton for which s is an access sequence, one of the shortest words that lead from the initial state to q. The elements of R should tell us about the back- and cross-edges in the automaton and the elements of E are \u201cexperiments\u201d that should be sufficient to distinguish between states. This works by associating with every s \u2208 S \u222a R a specialized classification function fs : E \u2192 {\u2212,+}, defined as fs(e) = f(s \u00b7 e), which characterizes the row of the observation table labeled by s. To build an automaton from a table it should satisfy certain conditions.\nDefinition 1.2 (Closed, Reduced and Consistent Tables). An observation table T is:\n\u2022 Closed if for every r \u2208 R, there exists an s \u2208 S, such that fr = fs; \u2022 Reduced if for every s, s\u2032 \u2208 S fs 6= fs\u2032 ; \u2022 Consistent if for every s, s\u2032 \u2208 S, fs = fs\u2032 implies fs\u00b7a = fs\u2032\u00b7a,\u2200a \u2208 \u03a3.\nNote that a reduced table is trivially consistent and that for a closed and reduced table we can define a function g : R \u2192 S mapping every r \u2208 R to the unique s \u2208 S such that fs = fr. From such an observation table T = (\u03a3, S,R,E, f) one can construct an automaton\n1We use \u228e for disjoint union.\nAT = (\u03a3, Q, q0, \u03b4, F ) where Q = S, q0 = \u01eb, F = {s \u2208 S : fs(\u01eb) = +} and\n\u03b4(s, a) = { s \u00b7 a when s \u00b7 a \u2208 S g(s \u00b7 a) when s \u00b7 a \u2208 R\nThe learner attempts to keep the table closed at all times. The table is not closed when there is some r \u2208 R such that fr is different from fs for all s \u2208 S. To close the table, the learner moves r from R to S and adds the \u03a3-successors of r, i.e., all words r \u00b7 a for a \u2208 \u03a3, to R. The extended table is then filled up by asking membership queries until it becomes closed. Variants of the L\u2217 algorithm differ in the way they treat counter-examples, as described in more detail in [BR04]. The original algorithm [Ang87] adds all the prefixes of the counterexample to S and thus possibly creating inconsistency that should be fixed. The version proposed in [MP95] for learning \u03c9-regular languages adds all the suffixes of the counterexample to E. The advantage of this approach is that the table always remains consistent and reduced with S corresponding exactly to the set of states. A disadvantage is the possible introduction of redundant columns that do not contribute to further discrimination between states. The symbolic algorithm that we develop in this paper is based on an intermediate variant, referred to in [BR04] as the reduced observation algorithm, where some prefixes of the counter-example are added to S and some suffixes are added to E.\nExample 1.3. We illustrate the behavior of the L\u2217 algorithm while learning a language L over \u03a3 = {1, 2, 3, 4, 5}. We use the tuple (w,+) to indicate a counter-example w \u2208 L rejected by the conjectured automaton, and (w,\u2212) for the opposite case. Initially, the observation table is T0 = (\u03a3, S,R,E, f) with S = E = {\u01eb} and R = \u03a3 and we ask membership queries for all words in (S \u222a R) \u00b7 E to obtain table T0, shown in Fig. 1. The table is not closed so we move word 1 to S, add its continuations, 1 \u00b7 \u03a3 to R and ask membership queries to obtain table T1 which is now closed. We construct an hypothesis A1 (Fig. 2) from this table, and pose an equivalence query for which the teacher returns counter-example (3 \u00b7 1,\u2212). We add 3 \u00b7 1 and its prefix 3 to set S and add all their continuations to the boundary of the table resulting table T2 of Fig. 1. This table is not consistent: two elements \u01eb and 3 in S are equivalent but their successors 1 and 3 \u00b7 1 are not. In order to distinguish the two strings we add to E the suffix 1 and end up with a closed and consistent table T3. The new hypothesis for this table is A3, shown in Fig. 2. Once more the equivalence query will return a counter-example, (1 \u00b7 3 \u00b7 3,\u2212). We again add the counter-example and prefixes to the table, ask membership queries to fill in the table and solve the inconsistency that appears for 1 and 1 \u00b7 3 by adding suffix 3 to the table. The table corresponds now to the correct hypothesis A5, and the algorithm terminates."}, {"heading": "2. Symbolic Automata", "text": "In this section we introduce the variant of symbolic automata that we use. Symbolic automata [HV11, VB12] give a more succinct representation for languages over large finite alphabets and can also represent languages over infinite alphabets such as N, R, or Rn. The size of a standard automaton for a language grows linearly with the size of the alphabet and so does the complexity of learning algorithms such as L\u2217. As we shall see, symbolic automata admit a variant of the L\u2217 algorithm whose complexity is independent of the alphabet size.\nLet \u03a3 be a large, possibly infinite, alphabet, to which we will refer from now on as the concrete alphabet. We define a symbolic automaton to be an automaton over \u03a3 where each state has a small number of outgoing transitions labeled by symbols that represent subsets of \u03a3. For every state, these subsets form a (possibly different) partition of \u03a3 and hence the automaton is complete and deterministic. We start with an arbitrary alphabet viewed as an unstructured set and present the concept in purely semantic manner before we move to ordered sets and inequalities in subsequent sections.\nLet \u03a3 be a finite alphabet, that we call the symbolic alphabet and its elements symbolic letters or symbols. Let \u03c8 : \u03a3 \u2192 \u03a3 map concrete letters into symbolic ones. The \u03a3-semantics of a symbolic letter a \u2208 \u03a3 is defined as [a]\u03c8 = {a \u2208 \u03a3 : \u03c8(a) = a} and the set {[a]\u03c8 : a \u2208 \u03a3} forms a partition of \u03a3. We will often omit \u03c8 from the notation and use [a] when \u03c8, which is always present, is clear from the context. The \u03a3-semantics can be extended to symbolic words of the form w = a1 \u00b7 a2 \u00b7 \u00b7 \u00b7ak \u2208 \u03a3\n\u2217 as the concatenation of the concrete one-letter languages associated with the respective symbolic letters or, recursively speaking, [\u01eb] = {\u01eb} and [w \u00b7 a] = [w] \u00b7 [a] for w \u2208 \u03a3\u2217, a \u2208 \u03a3.\nDefinition 2.1 (Symbolic Automaton). A deterministic symbolic automaton is a tuple"}, {"heading": "A = (\u03a3,\u03a3, \u03c8,Q, \u03b4, \u03b4, q0 , F ), where", "text": "\u2022 \u03a3 is the input alphabet,\n\u2022 \u03a3 is a finite alphabet, decomposable into \u03a3 = \u228e\nq\u2208Q\u03a3q,\n\u2022 \u03c8 = {\u03c8q : q \u2208 Q} is a family of surjective functions \u03c8q : \u03a3 \u2192 \u03a3q, \u2022 Q is a finite set of states, \u2022 \u03b4 : Q \u00d7 \u03a3 \u2192 Q and \u03b4 : Q \u00d7 \u03a3 \u2192 Q are the concrete and symbolic transition functions respectively, such that \u03b4(q, a) = \u03b4(q, \u03c8q(a)), \u2022 q0 is the initial state and F is a set of accepting states.\nThe transition function is extended to words as in the concrete case and the symbolic automaton can be viewed as an acceptor of a concrete language. When at q and reading a concrete letter a, the automaton will take the transition \u03b4(q,a) where a is the unique element of \u03a3q satisfying a \u2208 [a]. Hence L(A) consists of all concrete words whose run leads from q0 to a state in F . A language L over alphabet \u03a3 is symbolic recognizable if there exists a symbolic automaton A such that L = L(A). Remark: The association of a symbolic language with a symbolic automaton is more subtle because we allow different partitions of \u03a3 and hence different symbolic input alphabets at different states. The transition to be taken while being in a state q and reading a symbol a 6\u2208 \u03a3q is well defined only when [a] \u2286 [a\n\u2032] for some a\u2032 \u2208 \u03a3q. Such a model can be transformed into an automaton which is complete over a symbolic alphabet which is common to all states as follows. Let\n\u03a3\u2032 = \u220f\nq\u2208Q\n\u03a3q, with the \u03a3-semantics [(a1, . . . ,an)] = [a1] \u2229 . . . \u2229 [an],\nand let \u03a3\u0303 = {b \u2208 \u03a3\u2032 : [b] 6= \u2205}. Then we define A\u0303 = (\u03a3\u0303, Q, \u03b4\u0303, q0, F ) where, by construction, for every b \u2208 \u03a3\u0303 and every q \u2208 Q, there is a unique a \u2208 \u03a3q such that [b] \u2286 [a] and hence one can define the transition function as \u03b4\u0303(q, b) = \u03b4(q,a). This model is more comfortable for language-theoretic studies but in the learning context it introduces an unnecessary blowup in the alphabet size and the number of queries for every state. For this reason we stick in this paper to the Definition 2.1 which is more economical. A similar approach of state-local abstraction has been taken in [IHS13] for learning parameterized language. The construction of \u03a3\u2032 is similar to the minterm construction of [DV14] used to create a common alphabet in order to apply the minimization algorithm of Hopcroft to symbolic automata. Anyway, in our learning framework symbolic automata are used to read concrete and not symbolic words.\nIt is straightforward that for a finite concrete alphabet \u03a3 the set of languages accepted by symbolic automata coincides with the set of recognizable regular languages over \u03a3. Moreover, even when the alphabet is infinite, closure under Boolean operations is preserved.\nProposition 2.2 (Closure under Boolean Operations). Languages accepted by deterministic symbolic automata are effectively closed under Boolean operations.\nProof. Closure under complement is immediate by complementing the set of accepting states. For intersection the standard product construction is adapted as follows. Let L1, L2 be languages recognized by the symbolic automata A1 = (\u03a3,\u03a31, \u03c81, Q1, \u03b41, \u03b41, q01, F1), and A2 = (\u03a3,\u03a32, \u03c82, Q2, \u03b42, \u03b42, q02, F2), respectively. Let A = (\u03a3,\u03a3, \u03c8,Q, \u03b4, \u03b4, q0 , F ), where\n\u2022 Q = Q1 \u00d7Q2, q0 = (q01, q02), F = F1 \u00d7 F2, \u2022 For every (q1, q2) \u2208 Q \u2013 \u03a3(q1,q2) = {(a1,a2) \u2208 \u03a31 \u00d7\u03a32 | [a1] \u2229 [a2] 6= \u2205} \u2013 \u03c8(q1,q2)(a) = (\u03c81,q1(a), \u03c82,q2(a)), \u2200a \u2208 \u03a3\n\u2013 \u03b4((q1, q2), (a1,a2)) = (\u03b41(q1,a1), \u03b42(q2,a2)), \u2200(a1,a2) \u2208 \u03a3(q1,q2) It is sufficient to observe that the corresponding implied concrete automata A1, A2 and A satisfy \u03b4((q1, q2), a) = (\u03b41(q1, a), \u03b42(q2, a)) and the standard proof that L(A) = L(A1) \u2229 L(A2) follows. Closure under union and set difference is then evident.\nThe above product construction is used to implement equivalence queries where both the target language and the current conjecture are represented by symbolic automata. A counter-example is found by looking for a shortest path in the product automaton from the initial state to a state in F1 \u00d7 (Q2 \u2212 F2) \u222a (Q1 \u2212 F1)\u00d7 F2 and selecting a lexicographically minimal concrete word along that path.\nExample 2.3. Figure 3 shows a symbolic automaton equivalent to automaton A5 of Figure 2. The symbolic alphabets for the states are \u03a3q0 = {a0,a1}, \u03a3q1 = {a2,a3}, \u03a3q2 = {a4,a5,a6,a7}, \u03a3q3 = {a8}, and the \u03a3-semantics for the symbols is [a0] = {1, 2}, [a1] = {3, 4, 5}, [a2] = {3}, [a3] = {1, 2, 4, 5}, etc.. The same automaton can accept a language over the uncountable alphabet \u03a3 = [0, 100) \u2282 R, defining \u03c8 as shown in Figure 4."}, {"heading": "3. Symbolic Observation Tables", "text": "In this section we adapt observation tables to the symbolic setting. They are similar to the concrete case with the additional notions of evidences and evidence compatibility.\nDefinition 3.1 (Balanced Symbolic \u03a3-Tree). A balanced symbolic \u03a3-tree is a tuple (\u03a3,S,R, \u03c8) where\n\u2022 S \u228eR is a prefix-closed subset of \u03a3\u2217 \u2022 \u03a3 = \u228e\ns\u2208S \u03a3s is a symbolic alphabet\n\u2022 \u03c8 = {\u03c8s}s\u2208S is a family of total surjective functions of the form \u03c8s : \u03a3 \u2192 \u03a3s.\nIt is required that for every s \u2208 S and a \u2208 \u03a3s, s \u00b7a \u2208 S \u222aR and for any r \u2208 R and a \u2208 \u03a3, r \u00b7 a 6\u2208 S \u222aR . Elements of R are called boundary elements of the tree.\nWe will use observation tables whose rows are symbolic words and hence an entry in the table will constitute a statement about the inclusion or exclusion of a large set of concrete words in the language. We will not ask membership queries concerning all those concrete words, but only for a small representative subset that we call evidence.\nDefinition 3.2 (Symbolic Observation Table). A symbolic observation table is a tuple"}, {"heading": "T = (\u03a3,\u03a3,S,R, \u03c8,E,f , \u00b5) such that", "text": "\u2022 \u03a3 is an alphabet, \u2022 (\u03a3,S,R, \u03c8) is a balanced symbolic \u03a3-tree (with R being its boundary), \u2022 E is a subset of \u03a3\u2217, \u2022 f : (S \u222aR) \u00b7E \u2192 {\u2212,+} is the symbolic classification function \u2022 \u00b5 : (S \u222aR) \u00b7E \u2192 2\u03a3 \u2217 \u2212 {\u2205} is an evidence function satisfying \u00b5(w) \u2286 [w]. The image of\nthe evidence function is prefix-closed: w \u00b7 a \u2208 \u00b5(w \u00b7 a) \u21d2 w \u2208 \u00b5(w).\nAs for the concrete case we use fs : E \u2192 {\u2212,+} to denote the partial evaluation of f to some symbolic word s \u2208 S \u222aR, such that, fs(e) = f(s \u00b7 e). Note that the set E consists of concrete words but this poses no problem because elements of E are used only to distinguish between states and do not participate in the derivation of the symbolic automaton from the table. Concatenation of a symbolic word and a concrete one follows concatenation of symbolic words as defined above where each concrete letter a is considered as a symbolic letter a with [a] = {a} and \u00b5(a) = a. The notions of closed, consistent and reduced table are similar to the concrete case. The set MT = (S \u222aR) \u00b7E is called the symbolic sample associated with T . We require that for each word w \u2208 MT there is at least one concrete w \u2208 \u00b5(w) whose membership in L, denoted by f(w), is known. The set of such words is called the concrete sample and is defined as MT = {s \u00b7 e : s \u2208 \u00b5(s), s \u2208 S \u222a R, e \u2208 E}. A table where all evidences of the same symbolic word admit the same classification is called evidence-compatible.\nDefinition 3.3 (Table Conditions). A table T = (\u03a3,\u03a3,S,R, \u03c8,E,f , \u00b5) is\n\u2022 Closed if \u2200r \u2208 R, \u2203s = g(r) \u2208 S, fr = fs, \u2022 Reduced if \u2200s, s\u2032 \u2208 S, fs 6= fs\u2032 , \u2022 Consistent if \u2200s, s\u2032 \u2208 S, fs = fs\u2032 implies fs\u00b7a = fs\u2032\u00b7a,\u2200a \u2208 \u03a3s. \u2022 Evidence compatible if \u2200w \u2208 MT , \u2200w1, w2 \u2208 \u00b5(w), f(w1) = f(w2).\nWhen a table T is evidence compatible the symbolic classification function f can be defined for every s \u2208 (S \u222aR) and e \u2208 E as f(s \u00b7 e) = f(s \u00b7 e), s \u2208 \u00b5(s).\nTheorem 3.4 (Automaton from Table). From a closed, reduced and evidence compatible table one can construct a deterministic symbolic automaton compatible with the concrete\nsample.\nProof. The proof is similar to the concrete case. Let T = (\u03a3,\u03a3,S,R, \u03c8,E,f , \u00b5) be such a table, which is reduced and closed and thus a function g : R \u2192 S such that g(r) = s iff fr = fs is well defined. The automaton derived from the table is then AT = (\u03a3,\u03a3, \u03c8,Q, \u03b4, q0, F ) where:\n\u2022 Q = S, q0 = \u01eb\n\u2022 F = {s \u2208 S | fs(\u01eb) = +}\n\u2022 \u03b4 : Q\u00d7\u03a3 \u2192 Q is defined as \u03b4(s,a) = { s \u00b7 a when s \u00b7 a \u2208 S g(s \u00b7 a) when s \u00b7 a \u2208 R\nBy construction and like the L\u2217 algorithm, AT classifies correctly the symbolic sample and, due to evidence compatibility, this holds also for the concrete sample."}, {"heading": "4. Learning Languages over Ordered Alphabets", "text": "In this section we present a symbolic learning algorithm starting with an intuitive verbal description. The algorithmic scheme is similar to the concrete L\u2217 algorithm but differs in the treatment of counter-examples and the new concept of evidence compatibility. Whenever the table is not closed, S \u222a R is extended until closure. Then a conjectured automaton AT is constructed and an equivalence query is posed. If the answer is positive we are done. Otherwise, the teacher provides a counter-example leading to the extension of S\u222aR and/or E. Whenever such an extension occurs, additional membership queries are posed to fill the table. The table is always kept evidence compatible and reduced except temporarily during the processing of counter-examples.\nFrom now on we assume \u03a3 to be a totally ordered alphabet with a minimal element a0 and restrict ourselves to symbolic automata where the concrete semantics for every symbolic letter is an interval. In the case of a dense order like in R, we assume the intervals to be left-closed and right-open. The order on the alphabet can be extended naturally to a lexicographic order on \u03a3\u2217. Our algorithm also assumes that the teacher provides a counterexample of minimal length which is minimal with respect to the lexicographic order. This strong assumption improves the performance of the algorithm and its relaxation is discussed in Section 7.\nThe rows of the observation table consist of symbolic words because we want to group together all concrete letters and words that are assumed to induce the same behavior in the automaton. New symbolic letters are introduced in two occasions: when a new state is discovered or when a partition is modified due to a counter-example. In both cases we set the concrete semantics [a] to the largest possible subset of \u03a3, given the current evidence (in the first case it will be \u03a3). As an evidence we always select the smallest possible a \u2208 [a] (a0 when [a] = \u03a3). The choice of the right evidences is a key point for the performance of the algorithm as we want to keep the concrete sample as small as possible and avoid posing unnecessary queries. For infinite concrete alphabets this choice of evidence guarantees termination. The initial symbolic table is T = (\u03a3,\u03a3, S,R, \u03c8,E,f , \u00b5), where \u03a3 = {a0}, [a0] = \u03a3, S = {\u01eb}, R = {a0}, E = {\u01eb}, and \u00b5(a0) = {a0}. The table is filled by membership queries concerning \u01eb and a0. Whenever T is not closed, there is some r \u2208 R such that fr 6= fs for every s \u2208 S. To close the table we move r from R to S, recognizing it as a new state, and checking the behavior of its continuation. To this end we add to R the word r\u2032 = r \u00b7a,\nAlgorithm 1 The symbolic algorithm\n1: procedure Symbolic\n2: learned = false 3: Initialize the table T = (\u03a3,\u03a3,S,R, \u03c8,E,f , \u00b5) 4: \u03a3 = {a}; \u03c8\u01eb(a) = a,\u2200a \u2208 \u03a3 5: S = {\u01eb}; R = {a}; E = {\u01eb} 6: \u00b5(a) = {a0} 7: Ask MQ on \u01eb and a0 to fill f\n8: if T is not closed then 9: Close\n10: end if\n11: repeat 12: if EQ(AT ) then \u22b2 AT is correct 13: learned = true 14: else \u22b2 A counter-example w is provided 15: M = M \u222a {w} 16: Counter-ex(w) \u22b2 Process counter-example 17: end if 18: until learned 19: end procedure\nProcedure 2 Close the table\n1: procedure Close 2: while \u2203r \u2208 R such that \u2200s \u2208 S, fr 6= fs do 3: S\u2032 = S \u222a {r} \u22b2 r becomes a new state 4: \u03a3\u2032 = \u03a3 \u222a {anew} 5: \u03c8\u2032 = \u03c8 \u222a {\u03c8r} with \u03c8r(a) = anew, \u2200a \u2208 \u03a3 6: R\u2032 = (R\u2212 {r}) \u222a {r \u00b7 anew} 7: \u00b5(r \u00b7 anew) = \u00b5(r) \u00b7 a0 8: Ask MQ for all words in {\u00b5(r \u00b7 anew) \u00b7 e : e \u2208 E} 9: T = (\u03a3,\u03a3\u2032,S\u2032,R\u2032, \u03c8\u2032, E,f \u2032, \u00b5\u2032)\n10: end while\n11: end procedure\nwhere a is a new symbolic letter with [a] = \u03a3. We extend the evidence function by letting \u00b5(r\u2032) = \u00b5(r) \u00b7 a0, assuming that all elements of \u03a3 behave as a0 from r. Once T is closed we construct a hypothesis automaton as described in the proof of Theorem 3.4. When a counter-example w is presented, it is of course not part of the concrete sample. A miss-classified word in the conjectured automaton means that somewhere a wrong transition is taken. Hence w admits a factorization w = u \u00b7 b \u00b7 v where u \u2208 \u03a3\u2217 and b \u2208 \u03a3 is where the first wrong transition is taken. Obviously we do not know u and b in advance but know that this happens in the following two cases. Either b leads to an undiscovered state in the automaton of the target language, or letter b does not belong to the interval it was assumed to belong in the conjectured automaton. The latter case happens only when b does not belong to the evidence function. Since counter-example w is minimal, it admits\nProcedure 3 Process counter-example\n1: procedure Counter-ex(w) 2: Find a factorization w = u \u00b7 b \u00b7 v, b \u2208 \u03a3, u, v \u2208 \u03a3\u2217 such that 3: \u2203u \u2208 MT , u \u2208 \u00b5(u) and \u2200u \u2032 \u2208 MT , u \u00b7 b /\u2208 \u00b5(u \u2032)\n4: if u \u2208 S then \u22b2 u is already a state 5: Find a \u2208 \u03a3u such that b \u2208 [a] \u22b2 refine [a] 6: \u03a3\u2032 = \u03a3 \u222a {anew} 7: R\u2032 = R \u222a {u \u00b7 anew} 8: \u00b5(u \u00b7 anew) = \u00b5(u) \u00b7 b 9: Ask MQ for all words in {\u00b5(u \u00b7 anew) \u00b7 e : e \u2208 E}\n10: \u03c8\u2032u(a) =    \u03c8u(a) if a /\u2208 [a] anew if a \u2208 [a] and a \u2265 b a otherwise\n11: T = (\u03a3,\u03a3\u2032, S,R\u2032, \u03c8\u2032, E,f \u2032, \u00b5\u2032)\n12: else \u22b2 u is in the boundary\n13: S\u2032 = S \u222a {u} \u22b2 and becomes a state 14: if b = a0 then 15: \u03a3\u2032 = \u03a3 \u222a {anew} 16: \u03c8\u2032 = \u03c8 \u222a {\u03c8u}, with \u03c8u(a) = anew,\u2200a \u2208 \u03a3 17: R\u2032 = (R\u2212 {u}) \u222a {u \u00b7 anew} 18: E\u2032 = E \u222a {suffixes of b \u00b7 v} 19: \u00b5(u \u00b7 anew) = \u00b5(u) \u00b7 a0 20: Ask MQ for all words in {\u00b5(u \u00b7 anew) \u00b7 e : e \u2208 E\n\u2032} 21: else 22: \u03a3\u2032 = \u03a3 \u222a {anew,a \u2032 new }\n23: \u03c8\u2032 = \u03c8 \u222a {\u03c8u}, with \u03c8u(a) =\n{ a\u2032 new\nif a \u2265 b anew otherwise\n24: R\u2032 = (R\u2212 {u}) \u222a {u \u00b7 anew,u \u00b7 a \u2032 new } 25: E\u2032 = E \u222a {suffixes of b \u00b7 v} 26: \u00b5(u \u00b7 anew) = \u00b5(u) \u00b7 a0; \u00b5(u \u00b7 a \u2032 new ) = \u00b5(u) \u00b7 b 27: Ask MQ for all words in {(\u00b5(u \u00b7 anew) \u222a \u00b5(u \u00b7 a \u2032 new )) \u00b7 e : e \u2208 E\u2032} 28: end if 29: T = (\u03a3,\u03a3\u2032,S\u2032,R\u2032, \u03c8\u2032, E\u2032,f \u2032, \u00b5\u2032) 30: end if 31: if T is not closed then 32: close 33: end if\n34: end procedure\na factorization w = u \u00b7 b \u00b7 v, where u is the largest prefix of w such that u \u2208 \u00b5(u) for some u \u2208 S \u222aR but s \u00b7 b /\u2208 \u00b5(u\u2032) for any word u\u2032 in the symbolic sample. We consider two cases, u \u2208 S and u \u2208 R.\nIn the first case, when u \u2208 S, u is already a state in the hypothesis but b indicates that the partition boundariues are not correctly defined and need refinement. That is, u \u00b7 b was wrongly considered to be part of [u \u00b7a] for some a \u2208 \u03a3u, and thus b was wrongly considered\nto be part of [a]. Due to minimality, all letters in [a] less than letter b behave like \u00b5(a). We assume that all remaining letters in [a] behave like b and map them to a new symbol anew that we add to \u03a3u. We then update \u03c8u such that \u03c8 \u2032\nu(a) = anew for all a \u2208 [a], a \u2265 b, and \u03c8\u2032u(a) = \u03c8u(a), otherwise. The evidence function is updated by letting \u00b5(u \u00b7anew) = \u00b5(u) \u00b7b and u \u00b7 anew is added to R.\nIn the second case, the symbolic word u is part of the boundary. From the counterexample we deduce that u is not equivalent to any of the existing states in the hypothesis and should form a new state. Specifically, we find the prefix s that was considered to be equivalent to u, that is g(u) = s \u2208 S. Since the table is reduced fu 6= fs\u2032 for any other s\n\u2032 \u2208 S. Because w is the shortest counter-example, the classification of s \u00b7 b \u00b7 v in the automaton is correct (otherwise s \u00b7 b \u00b7 v, for some s \u2208 [s] would constitute a shorter counter-example) and different from that of u \u00b7 b \u00b7 v. We conclude that u is a new state, which is added to S. To distinguish between u and s we add to E the word b \u00b7 v, possibly with some of its suffixes (see [BR04] for a more detailed discussion of counter-example processing). As u is a new state we need to add its continuations to R. We distinguish two subcases depending on b. If b = a0, the smallest element of \u03a3, then a new symbolic letter anew is added to \u03a3, with [anew] = \u03a3 and \u00b5(u \u00b7 anew) = \u00b5(u) \u00b7 a0, and the symbolic word u \u00b7 anew is added to R. If b 6= a0 then two new symbolic letters, anew and a \u2032 new , are added to \u03a3 with [anew] = {a : a < b}, [a \u2032 new ] = {a : a \u2265 b}, \u00b5(u \u00b7 anew) = \u00b5(u) \u00b7 a0 and \u00b5(u \u00b7 a \u2032 new ) = \u00b5(u) \u00b7 b. The words u \u00b7 anew and u \u00b7 a \u2032 new are added to R. A detailed description of the algorithm is given in Algorithm 1 and its major procedures, table closing and counter-example treatment are described in Procedures 2 and 3 respectively. A statement of the form \u03a3\u2032 = \u03a3 \u222a {a} indicates the introduction of a new symbolic letter a 6\u2208 \u03a3. We use MQ and EQ as shorthands for membership and equivalence queries, respectively. In the following we illustrate the symbolic algorithm as applied to a language over an infinite alphabet.\nExample 4.1. Let \u03a3 = [0, 100) \u2282 R with the usual order and let L \u2286 \u03a3\u2217 be a target language. Fig. 5 shows the evolution of the symbolic observation tables and Fig. 6 depicts the corresponding automata and the concrete semantics of the symbolic alphabets.\nWe initialize the table with S = {\u01eb}, R = {a0}, \u00b5(a0) = {0} and E = {\u01eb} and ask membership queries for \u01eb (rejected) and 0 (accepted). The obtained table, T0 is not closed so we move a0 to S, introduce \u03a3a0 = {a1}, where a1 is a new symbol, and add a0 \u00b7 a1 to R with \u00b5(a0 \u00b7 a1) = 0 \u00b7 0. Asking membership queries we obtain the closed table T1 and its automaton A1. We pose an equivalence query and obtain (50,\u2212) as a (minimal) counter-example which implies that all words smaller than 50 are correctly classified. We add a new symbol a2 to \u03a3\u01eb and redefine the concrete semantics to [a0] = {a < 50} and [a2] = {a \u2265 50}. As evidence we select the smallest possible letter, \u00b5(a2) = 50, ask membership queries to obtain the closed table T2 and automaton A2.\nFor this hypothesis we get a counter-example (0 \u00b7 30,\u2212) whose prefix 0 is already in the sample, hence the misclassification occurs in the second transition. We refine the alphabet partition for state a0 by introducing a new symbol a3 and letting [a1] = {a < 30} and [a3] = {a \u2265 30}. Table T3 is closed but automaton A3 is still incorrect and a counterexample (50 \u00b7 0,\u2212) is provided. The prefix 50 belongs to the evidence of a2 and is moved from the boundary to become a new state and its successor a2 \u00b7 a4, for a new symbol a4, is added to R. To distinguish a2 from \u01eb, the suffix 0 of the counter-example is added to E resulting in T4 which is not closed. The newly discovered state a0 \u00b7 a1 is added to S, the filled table T5 is closed and the conjectured automaton A5 has two additional states.\nSubsequent equivalence queries result counter-examples (50 \u00b720,+), (50 \u00b780,\u2212) and (50 \u00b7 50 \u00b70,+) which are used to refine the alphabet partition at state a2 and modify its outgoing transitions progressively as seen in automata A6, A7 and A8, respectively. Automaton A8 accepts the target language and the algorithm terminates.\nNote that for the language in Example 1.3, the symbolic algorithm needs around 30 queries instead of the 80 queries required by L\u2217. If we choose to learn a language as the one described in Example 4.1, restricting the concrete alphabet to the finite alphabet \u03a3 = {1, . . . , 100}, then L\u2217 requires around 1000 queries compared to 17 queries required by our symbolic algorithm. As we shall see in Section 6, the complexity of the symbolic algorithm does not depend on the size of the concrete alphabet, only on the number of transitions."}, {"heading": "5. Learning Languages over Partially-ordered Alphabets", "text": "In this section we sketch the extension of the results of this paper to partially-ordered alphabets of the form \u03a3 = Xd where X is a totally-ordered set such as an interval [0, k) \u2286 R. Letters of \u03a3 are d-tuples of the form x = (x1, . . . , xd) and the minimal element is 0 = (0, . . . , 0). The usual partial order on this set is defined as x \u2264 y if and only if xi \u2264 yi for all i = 1, . . . , d. When x \u2264 y and xi 6= yi for some i the inequality is strict, denoted by x < y, and we say then that x dominates y. Two elements are incomparable, denoted by x||y, if xi < yi and xj > yj for some i and j.\nFor partially-ordered sets, a natural extension of the partition of an ordered set into intervals is a monotone partition, where for each partition block P there are no three points\nsuch that x < y < z, x, z \u2208 P , and y 6\u2208 P . We define in the following such partitions represented by a finite set of points.\nA forward cone B+(x) \u2282 \u03a3 is the set of all points dominated by a point x \u2208 \u03a3 (see Fig. 7a). Let F = {x1, . . . ,xl} be a set of points, then B +(F ) = B+(x1) \u222a . . . \u222aB +(xl) as shown in Fig. 7b. From a family of sets of points F = {F0, . . . , Fm\u22121}, such that F0 = {0} satisfying for every i: 1) \u2200y \u2208 Fi, \u2203x \u2208 Fi\u22121 such that x < y, and 2) \u2200y \u2208 Fi, \u2200x \u2208 Fi\u22121, y 6< x, we can define a monotone partition of the form P = {P1, . . . , Pm\u22121}, where Pi = B +(Fi\u22121)\u2212B +(Fi), see Fig. 7c.\nA subset P of \u03a3, as defined above, may have several mutually-incomparable minimal elements, none of which being dominated by any other element of P . One can thus apply the symbolic learning algorithm but without the presence of unique minimal evidence and minimal counter-example. For this reason a symbolic word may have more than one evidence. Evidence compatibility is preserved though due to the nature of the partition.\nThe teacher is assumed to return a counter-example chosen from a set of incomparable minimal counter-examples. Like in the algorithm for totally ordered alphabet, every counterexample either discovers a new state or refines a partition. The learning algorithm for partially-ordered alphabets is similar to Algorithm 1 and can be applied with only a minor modification in the treatment of the counterexamples and specifically in the refinement procedure. Lines 6-8 of Procedure 3 should be ignored in the case where there exists a symbolic letter a\u2032, as illustrated in Fig. 8a, such that f(u \u00b7 b \u00b7 e) = f(u \u00b7a\u2032 \u00b7 e) for all e \u2208 E. In such a case, function \u03c8 is updated as in line 9 by replacing anew by a \u2032 and b should\nbe added to \u00b5(a\u2032). In Fig. 8b, one can see the partition after refinement, where all letters above b have been moved from [a] to [a\u2032].\nExample 5.1. Let us illustrate the learning process for a target language L defined over \u03a3 = [0, 100]2. All tables, hypotheses automata and alphabet partitions for this example are shown in Figures 9, 10, and 11, respectively. The learner starts asking MQs for the empty word. A symbolic letter a0 is chosen to represent its continuations with the minimal element of \u03a3 as evidence, i.e., \u00b5(a0) = (0 0 ) . The symbolic word a0 is moved to S for the table T0 to be closed. The symbolic letter a1 is added to the alphabet of state a0, and the learner asks a MQ for (0 0 )(0 0 ) , the evidence of the symbolic word a0a1. The first hypothesis automaton is A0 with \u03a3-semantics [a0] = [a1] = \u03a3. The\ncounter-example ( (45 50 ) ,\u2212) refines the partition for the initial state. The symbolic alphabet\nis extended to \u03a3\u01eb = {a0,a2} with [a2] = {x >= (45 50 ) }, [a0] = \u03a3 \u2212 [a2], and \u00b5(a2) = (45 50 ) . The new observation table and hypothesis are T1 and A1. Two more counter-examples will come to refine the partition for the initial state, ( (60 0 ) ,\u2212) and ( ( 0 70 ) ,\u2212), that will modify the\npartition for the initial state, moving all letters greater than ( 60 0 ) and ( 0 70 ) to the \u03a3-semantics of a2 as can be seen in \u03c82 and \u03c83 respectively. After the hypothesis A3, the counter-example ( ( 0 0 )( 0 80 ) ,\u2212) adds a new symbol a3 and a new transition in the hypothesis automaton. The counter-examples that follow, namely, ( ( 0 0 )( 80 0 ) ,\u2212), ( ( 0 0 )( 40 15 ) ,\u2212), and ( ( 0 0 )( 30 30 ) ,\u2212) refine the \u03a3-semantics for symbols in \u03a3a0 as shown in \u03c84\u22127. Then counter-example ( ( 45 50 )( 0 0 ) ,+) is presented. As we can see, the prefix ( 45 50 ) exist already in \u00b5(a2) and a2 \u2208 R which means a2 becomes a state, and to distinguish it from\nthe state represented by the empty word the learner adds to E the suffix of the counterexample (0 0 ) . The resulting table T8 is not closed and a0a1 is moved to S. The new table T9 is closed and evidence compatible. The hypothesis A9 has now four states and the symbolic alphabet and \u03a3-semantics for each state can be seen in \u03c89. The counter-examples that follow will refine the partition at state a2. The new transitions discovered and all refinements are shown in A10\u221218 and \u03c810 \u2212 \u03c818. The language was learned using 20 membership queries and 17 counter-examples."}, {"heading": "6. On Complexity", "text": "The complexity of the symbolic algorithm is influenced not by the size of the alphabet but by the resolution (partition size) with which we observe it. Let L \u2282 \u03a3 be the target language and let A be the minimal symbolic automaton recognizing this language with state set Q of size n and a symbolic alphabet \u03a3 = \u228e q \u03a3q such that |\u03a3q| \u2264 m for every q. Each counter-example improves the hypothesis in one out of two ways. Either a new state is discovered or a partition gets refined. Hence, at most n \u2212 1 equivalence queries of the first type can be asked and n(m\u2212 1) of the second, resulting in O(mn) equivalence queries. Concerning the size of the table, the set of prefixes S is monotonically increasing and reaches the size of exactly n elements. Since the table, by construction, is always kept\nreduced, the elements in S represent exactly the states of the automaton. The size of the boundary is always smaller than the total number of transitions in the automaton, that is mn \u2212 n + 1. The number of suffixes in E, playing a distinguishing role for the states of the automaton, range between log2 n and n. Hence, the size of the table ranges between (n+m) log2 n and n(mn+ 1). For a totally ordered alphabet the size of the concrete sample coincides with the size of the symbolic sample associated with the table and hence the number of membership queries asked is O(mn2). For a partially ordered alphabet with each Fi defined by at most l points, some additional queries are asked. For every row in S, at most n(m\u2212 1)(l \u2212 1) additional words are added to the concrete sample, hence more membership queries might need to be asked. Furthermore, at most l \u2212 1 more counter-examples are given to refine a partition. To conclude, the number of queries in total asked to learn language L is O(mn2) if l < n and O(lmn) otherwise."}, {"heading": "7. Conclusion", "text": "We have defined a generic algorithmic scheme for automaton learning, targeting languages over large alphabets that can be recognized by finite symbolic automata having a modest number of states and transitions. Some ideas similar to ours have been proposed for the particular case of parametric languages [BJR06] and recently in a more general setting\n[HSM11, IHS13, BB13] including partial evidential support and alphabet refinement during the learning process.\nThe genericity of our algorithm is due to a semantic approach (alphabet partitions) but of course, each and every domain will have its own semantic and syntactic specialization in terms of the size and shape of the alphabet partitions. In this work we have implemented an instantiation of this scheme for alphabets such as (N,\u2264) and (R,\u2264). When dealing with numbers, the partition into a finite number of intervals (and monotone sets in higher dimensions) is very natural and used in many application domains ranging from quantization of sensor readings to income tax regulations. It will be interesting to compare the expressive\npower and succinctness of symbolic automata with other approaches for representing numerical time series and to compare our algorithm with other inductive inference techniques for sequences of numbers.\nAs a first excursion into the domain, we have made quite strong assumptions on the nature of the equivalence oracle, which, already for small alphabets, is a bit too strong and pedagogical to be realistic. We assumed that it provides the shortest counter-example and also that it chooses always the minimal available concrete symbol. We can relax the latter (or both) and even omit this oracle altogether and replace it by random sampling, as already proposed in [Ang87] for concrete learning. Over large alphabets, it might be even more appropriate to employ probabilistic convergence criteria a-la PAC learning [Val84] and be content with a correct classification of a large fraction of the words, thus tolerating imprecise tracing of boundaries in the alphabet partitions. This topic is subject to ongoing work. Another challenging research direction is the adaptation of our framework to languages over Boolean vectors."}, {"heading": "Acknowledgement", "text": "This work was supported by the French project EQINOCS (ANR-11-BS02-004). We thank Peter Habermehl, Eugene Asarin and anonymous referees for useful comments and pointers to the literature."}], "references": [{"title": "Learning regular sets from queries and counterexamples", "author": ["Dana Angluin"], "venue": "Information and Computation,", "citeRegEx": "Angluin.,? \\Q1987\\E", "shortCiteRegEx": "Angluin.", "year": 1987}, {"title": "Sigma*: Symbolic learning of Input-Output specifications", "author": ["Matko Botin\u010dan", "Domagoj Babi\u0107"], "venue": "In POPL,", "citeRegEx": "Botin\u010dan and Babi\u0107.,? \\Q2013\\E", "shortCiteRegEx": "Botin\u010dan and Babi\u0107.", "year": 2013}, {"title": "Regular inference for state machines with parameters", "author": ["Therese Berg", "Bengt Jonsson", "Harald Raffelt"], "venue": "In FASE,", "citeRegEx": "Berg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2006}, {"title": "What you must remember when processing data words", "author": ["Michael Benedikt", "Clemens Ley", "Gabriele Puppis"], "venue": "In AMW,", "citeRegEx": "Benedikt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Benedikt et al\\.", "year": 2010}, {"title": "Model checking. In Model-Based Testing of Reactive Systems, volume 3472 of LNCS, pages 557\u2013603", "author": ["Therese Berg", "Harald Raffelt"], "venue": null, "citeRegEx": "Berg and Raffelt.,? \\Q2004\\E", "shortCiteRegEx": "Berg and Raffelt.", "year": 2004}, {"title": "Grammatical inference: learning automata and grammars", "author": ["Colin De la Higuera"], "venue": null, "citeRegEx": "Higuera.,? \\Q2010\\E", "shortCiteRegEx": "Higuera.", "year": 2010}, {"title": "The Book of Traces", "author": ["Volker Diekert", "Grzegorz Rozenberg"], "venue": "World Scientific,", "citeRegEx": "Diekert and Rozenberg.,? \\Q1995\\E", "shortCiteRegEx": "Diekert and Rozenberg.", "year": 1995}, {"title": "Minimization of symbolic automata", "author": ["Loris D\u2019Antoni", "Margus Veanes"], "venue": "In POPL,", "citeRegEx": "D.Antoni and Veanes.,? \\Q2014\\E", "shortCiteRegEx": "D.Antoni and Veanes.", "year": 2014}, {"title": "System identification via state characterization", "author": ["E. Mark Gold"], "venue": null, "citeRegEx": "Gold.,? \\Q1972\\E", "shortCiteRegEx": "Gold.", "year": 1972}, {"title": "Mona: Monadic second-order logic in practice", "author": ["Jesper G. Henriksen", "Ole J.L. Jensen", "Michael E. Jrgensen", "Nils Klarlund", "Robert Paige", "Theis Rauhe", "Anders B. Sandholm"], "venue": "In TACAS,", "citeRegEx": "Henriksen et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Henriksen et al\\.", "year": 1995}, {"title": "Inferring canonical register automata", "author": ["Falk Howar", "Bernhard Steffen", "Bengt Jonsson", "Sofia Cassel"], "venue": "In VMCAI,", "citeRegEx": "Howar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Howar et al\\.", "year": 2012}, {"title": "Automata learning with automated alphabet abstraction refinement", "author": ["Falk Howar", "Bernhard Steffen", "Maik Merten"], "venue": "In VMCAI,", "citeRegEx": "Howar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Howar et al\\.", "year": 2011}, {"title": "An evaluation of automata algorithms for string analysis", "author": ["Pieter Hooimeijer", "Margus Veanes"], "venue": "In VMCAI,", "citeRegEx": "Hooimeijer and Veanes.,? \\Q2011\\E", "shortCiteRegEx": "Hooimeijer and Veanes.", "year": 2011}, {"title": "Inferring automata with state-local alphabet abstractions", "author": ["Malte Isberner", "Falk Howar", "Bernhard Steffen"], "venue": "In NASA Formal Methods,", "citeRegEx": "Isberner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Isberner et al\\.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "This work is concerned with regular languages defined over large alphabets, either infinite or just too large to be expressed enumeratively. We define a generic model where transitions are labeled by elements of a finite partition of the alphabet. We then extend Angluin\u2019s L algorithm for learning regular languages from examples for such automata. We have implemented this algorithm and we demonstrate its behavior where the alphabet is a subset of the natural or real numbers. We sketch the extension of the algorithm to a class of languages over partially ordered alphabets.", "creator": "LaTeX with hyperref package"}}}