{"id": "1506.01195", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Implementation of Training Convolutional Neural Networks", "abstract": "Deep \u00e7atl\u0131 learning imbeciles refers to stephie a polyps shining speeding branch of machine rotas learning decorating that inten is based lesney on learning own levels 2337 of elphick representations. negrin Convolutional Neural gokdeniz Networks (saintly CNN) is one kind launched of deep maylin neural 329th network. It can pelli study zollner concurrently. In 32-12 this victorianism article, massa we use convolutional neural network to implement the anyhow typical ligot face recognition protais problem yunfeng which deguchi can overcome feathering the ichinose influence hich of 14:24 pose affiliations or retainers resolution koshkonong in marktplatz face vtu recognition. Then, a baren parallel strategy meymand was proposed glycolipid in section4. kempster In addition, playfirst by stoyer measuring parasara the actual time of forward and gigu\u00e8re backward computing, we tom-tom analysed the stressing maximal speed up kaestner and euro275 parallel chukka efficiency agricultura theoretically.", "histories": [["v1", "Wed, 3 Jun 2015 10:18:49 GMT  (382kb)", "http://arxiv.org/abs/1506.01195v1", "10 pages, 5 figures"], ["v2", "Thu, 4 Jun 2015 02:10:39 GMT  (383kb)", "http://arxiv.org/abs/1506.01195v2", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["tianyi liu", "shuangsang fang", "yuehui zhao", "peng wang", "jun zhang"], "accepted": false, "id": "1506.01195"}, "pdf": {"name": "1506.01195.pdf", "metadata": {"source": "META", "title": "Implementation of Training Convolutional Neural Networks", "authors": ["Tianyi Liu", "Shuangsang Fang", "Yuehui Zhao", "Peng Wang", "Jun Zhang"], "emails": ["{liutianyi14@mails.ucas.ac.cn}"], "sections": [{"heading": null, "text": "of representations. Convolutional Neural Networks (CNN) is one kind of deep neural network. It can study concurrently. In this article, we use convolutional neural network to implement the typical face recognition problem which can overcome the influence of pose or resolution in face recognition. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of forward and backward computing, we analysed the maximal speed up and parallel efficiency theoretically.\nKeywords: Convolutional Neural Networks, face training, Parallel Strategy, Maximal speedup"}, {"heading": "1. INTRODUTION", "text": "Deep learning refers to a subfield of machine learning that is based on learning levels of representations, corresponding to a hierarchy of features, factors or concepts, where higher-lever concepts are defined from lower-lever ones, and the same lower-lever concepts can help to define many higher-lever concepts. Deep learning is learning multiple levels of representation and abstraction, helps to understand the data such as images, audio and text. The concept of Deep Learning comes from the study of Artificial Neural Network, Multilayer Perceptron which contains more hidden layers is a Deep Learning structure. In the late 1980s, the invention of Back Propagation algorithm used in Artificial Neural Network brings hope to machine learning and creates a trend of machine learning based on statistical models. In the 1990s, a variety of Shallow Learning models have been proposed such as Support Vector Machines (SVM), Boosting, Logistic Regression (LR). The structure of these models can be seen as one hidden node (SVM, Boosting), or no hidden nodes (LR). These models gained a great success both in theoretical analysis and applications. In 2006, Geoffrey Hinton who is the professor of University of Toronto, Canada and the dean of machine learning and his students Ruslan Salakhutdinov published an article in \u201cScience\u201d, led to a trend of machine learning in academia and industry. The article had two points: 1) Artificial Neural Network with multiple hidden layers has an excellent ability of characteristic learning. The\ncharacteristics obtained from learning have an essential description to data, then facilitate visualization or classification. 2) The difficulties of deep neural network in training can overcome by layer-wise pre-training. In this article, the implementation of layer-wise pre-training is achieved through unsupervised learning. Feedforward neural network or Multilayer Perceptron with multiple hidden layers in artificial neural networks is usually known as Deep Neural Networks (DNNs). Convolutional Neural Networks (CNN) is one kind of feedforward neural network. In 1960s, when Hubel and Wiesel researched the neurons used for local sensitive orientation-selective in the cat\u2019s visual system, they found the special network structure can effectively reduce the complexity of Feedback Neural Networks and then proposed Convolution Neural Network. CNN is an efficient recognition algorithm which is widely used in pattern recognition and image processing. It has many features such as simple structure, less training parameters and adaptability. It has become a hot topic in voice analysis and image recognition. Its weight shared network structure make it more similar to biological neural networks. It reduces the complexity of the network model and the number of weights. Generally, the structure of CNN includes two layers one is feature extraction layer, the input of each neuron is connected to the local receptive fields of the previous layer, and extracts the local feature. Once the local features is extracted, the positional relationship between it and other features also will be determined. The other is feature map layer, each computing layer of the network is composed of a plurality of feature map. Every feature map is a plane, the weight of the neurons in the plane are equal. The structure of feature map uses the sigmoid function as activation function of the convolution network, which makes the feature map have shift invariance. Besides, since the neurons in the same mapping plane share weight, the number of free parameters of the network is reduced. Each convolution layer in the convolution neural network is followed by a computing layer which is used to calculate the local average and the second extract, this unique two feature extraction structure reduces the resolution. CNN is mainly used to identify displacement, zoom and other forms of distorting invariance of two-dimensional graphics. Since the feature detection layer of CNN learns by training data, it avoids explicit feature extraction and implicitly learns from the training data when we use CNN. Furthermore, the neurons in the same feature map plane have the identical weight, so the network can study concurrently. This is a major advantage of the convolution network with respect to the neuronal network connected to each other. Because of the special structure of the CNN\u2019s local shared weights makes it have a unique advantage in speech recognition and image processing. Its layout is closer to the actual biological neural network. Shared weights reduces the complexity of the network. In particular multi-dimensional input vector image can directly enter the network, which avoids the complexity of data reconstruction in feature extraction and classification process. Face recognition is a biometric identification technology based on the facial features of persons. The study of face recognition system began in the 1960s, in the late 1980s with the development of computer technology and optical imaging techniques it has been improved; in the late 1990s it truly entered the stages of initial applications. In practical applications, such as monitoring system, the collected face images captured by cameras are often low resolution and with great pose variations. Affected by pose variation and low resolution, the performance of face recognition degrades sharply. And pose variations bring great challenge to face recognition. They bring nonlinear factors into face recognition. And some of the existing machine learning method\nmostly use shallow structure. Deep learning can achieve the approximation of complex function by a deep nonlinear network structure. In this article, we use convolution neural network to solve face recognition. It can overcome the influence of pose or resolution in face recognition. Due to the long training time and the large recognition computing, it is difficult to meet the real-time requirements, or the delay exceeds the range of tolerance. So we use the cloud platform to concurrently speed up the computing process."}, {"heading": "2. BACKGROUNDAND RELATEDWORK", "text": "Convolutional Neural Networks can be applied in many different areas. Yann LeCun and his team specially designed Convoutional Neural Networks to deal with the variability of 2D shapes, which are shown to outperform all other techniques.[1] Dan C and his team present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants for Image Classification. [2] Another team proposes two novel frontends for robust language identification (LID) using a convolutional neural network (CNN) trained for automatic speech recognition (ASR). [5] What\u2019s more, Convolutional Neural Networks are used in Visual Recognition[9] and many other areas, such as Facial Point Detection[6], House Numbers Digit Classification[10], Multi-digit Number Recognition from Street View Imagery[11]. Besides these works, many teams are focusing on the speed up of ConvNets. For example, Multi-GPU Training of ConvNets. In this work , Facebook AI Group consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs.[4]"}, {"heading": "3. PRINCIPLE OFCONVELUTIONALNEURALNETWORKS", "text": ""}, {"heading": "3.1 Methodology", "text": "Convolution neural network algorithm is a multilayer perceptron that is the special design for identification of two-dimensional image information . Always has more layers: input layer, convolution layer, sample layer and output layer. In addition, in a deep network architecturethe convolution layer and sample layer can have multiple. CNN is not as restricted boltzmann machine, need to be before and after the layer of neurons in the adjacent layer for all connections, convolution neural network algorithms, each neuron don't need to do feel global image, just feel the local area of the image. In addition, each neuron parameter is set to the same, namely, the sharing of weights , namely each neuron with the same convolution kernels to deconvolution image.\n(the first stage is the input image, the input of the after convolution is the feature image of each layer, namely Feature Map), then add a bias bx, we can get convolution layer Cx. A sampling process: n pixels of each neighborhood through pooling steps, become a pixel, and then by scalar weighting Wx + 1 weighted, add bias bx + 1, and then by an activation function, produce a narrow n times feature map Sx + 1.\nThe key technology of CNN is the local receptive field, sharing of weights ,\nsub sampling by time or space, so as to extract feature and reduce the size of the training parameters.The advantage of CNN algorithm is that to avoid the explicit feature extraction, and implicitly to learn from the training data;The same neuron weights on the surface of the feature mapping, thus network can learn parallelly , reduce the complexity of the network;Adopting sub sampling structure by time or space, can achieve some degree of robustness, scale and deformation displacement;Input information and network topology can be a very good match, It has unique advantages in speech recognition and image processing."}, {"heading": "3.2 CNNArchitecture Design", "text": "CNN algorithm need experience in architecture design, and need to debug unceasingly in the practical application, in order to obtain the most suitable for a particular application architecture of CNN. Based on gray image as the input of 96  96, in the preprocess stage, turning it into 32  32 of the size of the image. Design depth of the layer 7 convolution model: input layer, convolution layer C1, sub sampling layer S1, convolution layer C2, sampling layer S2, hidden layer H and output layer F.\nFig 3 architecture of CNN in training faces\nIn view of the 32  32 input after preprocessing, There is a total of 17 different pictures. C1 layer for convolution, convolution layer adopts 6 convolution kernels, each the size of the\nconvolution kernels is 5  5, can produce six feature map, each feature map contains (32-5 + 1)  (32-5 + 1) = 28  28 = 784 neurons. At this point, a total of 6  (5  5 + 1) = 156 parameters to be trained . S1 layer for sub sampling, contains six feature map, each feature map contains 14 * 14 = 196 neurons. the sub sampling window is 2  2 matrix, sub sampling step size is 1, so the S1 layer contains 6  196  (2  2 + 1) = 5880 connections. Every feature map in the S1 layer contains a weights and bias, so a total of 12 parameters can be trained in S1 layer . C2 is convolution layer, containing 16 feature graph, each feature graph contains (14-5 + 1) (14-5 + 1) = 100 neurons and adopts full connection, namely each characteristic figure used to belong to own 6 convolution kernels with six characteristics of the sample layer S1 convolution and figure. Each feature graph contains 6  5  5 = 150 weights and a bias. So, C2 layer contains a total of 16  (150 + 1) = 150 parameters to be trained. S2 is sub sampling layer, containing 16 feature map, each feature map contains 5  5 neurons, S2 total containing 25  16 = 400 neurons. S2 on characteristic figure of sub sampling window for 2  2, so there is 32 trainable S2 parameters. As a whole connection layer, hidden layer H contains 170 neurons, each neuron is connected to 400 neurons on S2. So H layer contains 170  (400 + 1) = 48120 parameters feature map. Output layer F for all connections, including 17 neurons. A total of 17  (170 + 1) = 2907 parameters to be trained.\n3.3 CNNAlgorithm and Back propagation algorithm 3.1.1 Forward pass\noutput of neuron of row k , column y in the l th convolution layer and k th feature pattern\uff1a\n  )tanh( ),(),1( ),(\n1\n0 0 0\n),( ),( , , BiasOW kltl cxrx\nf\nt r c\ntk\ncr kl yx\nk k O h w\n  \n\n    \uff083.1\uff09\namong them, f is the number of convolution cores in a feature pattern\u3002\noutput of neuron of row x , column y in the l th sub sample layer and k th feature pattern\uff1a\n  )tanh( ),( 0 0 ),1( ),( )(, , BiasOW kl r c kl cyrx kkl yx s s ssO h w wh       \uff083.2\uff09\nthe output of the j th neuron in l th hide layer H \uff1a\n)tanh( ),(),1( ),(\n1\n0 0 0\n),( ),(),( BiasOWO jlkl yx s\nk x y\nkj\nyxjl\ns sh w   \n    \uff083.3\uff09\namong them, s is the number of feature patterns in sample layer. output of the i th neuron l th output layer F\n)tanh( ),( 0\n),(),1(),( BiasWOO il H\nj\nl\njijlil     \uff083.4\uff09\n3.1.2 Back propagation output deviation of the k th neuron in output layer O :   tyO kkOkd  \uff083.5\uff09 input deviation of the k th neuron in output layer:\n         OvvtyI okkkkkOk dd  ''  \uff083.6\uff09\nweight and bias variation of k th neuron in output O :\n yIW xkOkO xk d ,,  \uff083.7\uff09  IBias OkOk d \uff083.8\uff09\noutput bias of k th neuron in hide layer H :\n   WIO ki i\ni\nO i H k dd , 17 0     \uff083.9\uff09\ninput bias of k th neuron in hide layer H :\n     OvI HkkHk dd  ' \uff083.10\uff09 weight and bias variation in row x , column y in the m th feature pattern ,a former layer in front of k neurons in hide layer H\n yIW m yxHkkH yxm d ,, ,,  \uff083.11\uff09  IBias HkHk d \uff083.12\uff09\noutput bias of row x , column y in m th feature pattern ,sub sample layer S\n   WIO kH yxm k H yxm mS yx dd , ,,\n170\n,,\n, ,  \uff083.13\uff09\ninput bias of row x , column y in m th feature pattern ,sub sample layer S\n     OvI mS yxkmS yx dd ,,',,  \uff083.14\uff09 weight and bias variation of row x , column y in m th feature pattern ,sub sample layer S\n     OIW mC yx\nfh\nx\nfw\ny\nmS yx mS d , ,\n0 0\n, 2/,2/ ,     \uff083.15\uff09\namong them,C represents convolution layer.\n     fh x fw y mS yx mS OBias d 0 0 , , , \uff083.16\uff09\noutput bias of row x ,column y in k th feature patter ,convolution layer C\n       WIO kkS yxkC yx dd , 2/,2/,,  \uff083.17\uff09 iutput bias of row x ,column y in k th feature patter ,convolution layer C\n     OvI kC yxkkC yx dd ,,',,  \uff083.18\uff09 weight variation of row r ,column c in m th convolution core,corresponding to k th feature pattern in l th layer ,convolution C .\n OIW ml cyrx fh\nx\nfw\ny\nkC yx mk cr d ,1\n, 0 0\n, , , ,\n\n    \uff083.19\uff09\ntotal bias variation of the convolution core\n     fh x fw y kC yx kC IBias d 0 0 , , , \uff083.20\uff09"}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1 Setups", "text": "CPU AMDA8-3500 APU with Radeon(tm) HD Graphics The number of CPU cores 4 Memory Size 4G Operation System Ubuntu 14.04 LS"}, {"heading": "4.2 Parallel Strategy and Parallel Efficiency", "text": "This analysis is based on a hypothesis that both serial and parallel method have the same number of training In serial realization method, the total execution time is N times of the sum of t1 and t2\nTime of serial execution: 1 2 3( ) serialNt t t t   \nTime of parallel execution : 1 2 3max{( ' ')} ( / n) ' parallelt t N t t    speed-up ratio = /serical parallelt t Speed-up efficiency=speed-up ratio/n N: num of images n: num of nodes 1t :time of forward pass for training a picture 2t :time of backward propagation for training a picture 3t :time for updating weight and bias of convolution neural network"}, {"heading": "4.3 Results Analysis", "text": "The data set we used is from Yale Face Database. We choose 136 images to analysis. When we run our algorithm we need to divide it into two phases. First, we need to train our algorithm. The purpose of this phase is to determine the minimum error which will be used in the next phase. So we must ensure that the algorithm can converge at a certain point. During the training process, the error will be reduced until it becomes a constant. The constant will be used as a threshold in the next step. Figure 1 shows the error will not change after repeating 4 times. So the best error is 4. The horizontal axis represents the number of iterations, and the vertical axis represents error. Seconds, we can use the constant obtained from the first step as the threshold to judge whether the algorithm can stop. Table I shows when the training process is successful the time consumed by the algorithm. In the table, \u201cyes\u201d represents the algorithm is succeed, in contrast \u201cno\u201d represents the algorithm is failed. We can see that the average time used by the algorithm is 12374.3 milliseconds. The reason why the algorithm is failed is it fell into the local optimum."}, {"heading": "4.4 Theoretically Analysis of The Maximal Speedup", "text": "By measuring the time overhead of training, especially the average time of t1 , t2 and t3 , the maximal speedup and speed-up efficiency are listed below:\nTime of serial execution: 5317.000000 Time of parallel execution :2665.000000 speed-up ratio \uff1a1.995122 Speed-up efficiency \uff1a0.997561"}, {"heading": "5. CONCLUSION", "text": "In this work, we accomplish face recognition by using deep learning algorithm. We mainly apply the algorithm of convolution neural network to excavate the deep information of multi-layer network in the process of face recognition .And we also utilize the algorithm to make parallel computing on the cloud platform for accelerating the process of face recognition, analyzing theoretical acceleration ratio, and experimental verification. Experimental results show that we have achieved good results. Of course, the parallelism we do is coarse-grained, and there are still many modules that can be fine-grained in the algorithm. This will be the focus for us in the future to continue to improve the work.\nACKNOWLEDGMENT\nDuring the time we work together to complete the course task, Prof. Chen takes much effort to offer us guide and help. So the first person we must offer our thanks to is Mr.Chen. At the same time, our major team leader Tianyi Liu, also deserves our sincerest thanks. He has done much work to organize teammates and coordinate everyone\u2019s work. And lastly,thanks to everybody in our team, we reach a consensus and we make concerted efforts, then we can complete our work in time and publish it on the website ."}], "references": [{"title": "Gradient-based learning applied to document recognition[ J", "author": ["Y Lecun", "L Bottou", "Y Bengio"], "venue": "Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Flexible, High Performance Convolutional Neural Networks for Image Classification[J", "author": ["Dan C. Ciresan", "Ueli Meier", "Jonathan Masci"], "venue": "PROCEEDINGS OF THE TWENTY-SECOND IN TERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "ImageNet classification with deep convolutional neural networks,\u201d NIPS[J", "author": ["G. Hinton A K I S"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Application of convolutional neural networks to language ide ntification in noisy conditions[C]//Proc", "author": ["Y Lei", "L Ferrer", "A Lawson"], "venue": "Speaker Odyssey Workshop (submitted)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep Convolutional Network Cascade for Facial Point Detection[C]", "author": ["Y Sun", "X Wang", "X. Tang"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Applying Convolutional Neural Networks con cepts to hybrid NN-HMM model for speech recognition[C]// Acoustics", "author": ["O Abdel-Hamid", "R Mohamed A", "H Jiang"], "venue": "Speech and Signal Proces sing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A Theoretical Analysis of Feature Pooling in Visual Recognit ion[J", "author": ["L Boureau Y", "J Ponce", "Y. Lecun"], "venue": "International Conference on Machine Learning Haifa Israel,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Convolutional networks and applications in vision[C]// Circuits and Systems (ISCAS)", "author": ["Y Lecun", "K Kavukcuoglu", "C. Farabet"], "venue": "Proceedings of 2010 IEEE International Symposium on. IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Convolutional Neural Networks Applied to HouseNumbers Digit Classification[C", "author": ["S. Lecun P"], "venue": "Pattern Recognition (ICPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Multi-digit number recognition from street view ima gery using deep convolutional neural networks[J", "author": ["J Goodfellow I", "Y Bulatov", "J Ibarz"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "[1] Dan C and his team present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants for Image Classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Another team proposes two novel frontends for robust language identification (LID) using a convolutional neural network (CNN) trained for automatic speech recognition (ASR).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] What\u2019s more, Convolutional Neural Networks are used in Visual Recognition[9] and many other areas, such as Facial Point Detection[6], House Numbers Digit Classification[10], Multi-digit Number Recognition from Street View Imagery[11].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5] What\u2019s more, Convolutional Neural Networks are used in Visual Recognition[9] and many other areas, such as Facial Point Detection[6], House Numbers Digit Classification[10], Multi-digit Number Recognition from Street View Imagery[11].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "[5] What\u2019s more, Convolutional Neural Networks are used in Visual Recognition[9] and many other areas, such as Facial Point Detection[6], House Numbers Digit Classification[10], Multi-digit Number Recognition from Street View Imagery[11].", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "[5] What\u2019s more, Convolutional Neural Networks are used in Visual Recognition[9] and many other areas, such as Facial Point Detection[6], House Numbers Digit Classification[10], Multi-digit Number Recognition from Street View Imagery[11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 9, "context": "[5] What\u2019s more, Convolutional Neural Networks are used in Visual Recognition[9] and many other areas, such as Facial Point Detection[6], House Numbers Digit Classification[10], Multi-digit Number Recognition from Street View Imagery[11].", "startOffset": 233, "endOffset": 237}, {"referenceID": 0, "context": "In this work , Facebook AI Group consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "In this work , Facebook AI Group consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs.", "startOffset": 102, "endOffset": 105}], "year": 2015, "abstractText": "Deep learning refers to a shining branch of machine learning that is based on learning levels of representations. Convolutional Neural Networks (CNN) is one kind of deep neural network. It can study concurrently. In this article, we use convolutional neural network to implement the typical face recognition problem which can overcome the influence of pose or resolution in face recognition. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of forward and backward computing, we analysed the maximal speed up and parallel efficiency theoretically.", "creator": "WPS Office \u4e2a\u4eba\u7248"}}}