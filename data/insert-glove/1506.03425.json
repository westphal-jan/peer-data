{"id": "1506.03425", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Fast Online Clustering with Randomized Skeleton Sets", "abstract": "We present madeco a dash-8 new fast a-18 online combustibles clustering ampex algorithm cheapness that hideyoshi reliably qaissi recovers arbitrary - l\u00e9vi shaped data 1896-1897 clusters salk in high anwr throughout data elkhorn streams. unserved Unlike the existing year-to-year state - of - jinshan the - faradje art online 681 clustering z\u00fcndel methods based on rust k - means or biob\u00edo k - indo-scythians medoid, it 1992-2003 does stereograms not make any seagram restrictive olazabal generative assumptions. scabbards In addition, runscorers in contrast to existing nonparametric clustering techniques deus such as DBScan standard-times or columbae DenStream, syp it gives nodded provable theoretical guarantees. trafico To elejalde achieve fast terris clustering, gelada we lyndall propose ferrill to represent odgen each junimist cluster by a skeleton set which riviera is seku updated cherkos continuously ayuso as linked new kallo data is jharkhand seen. A skeleton set statale consists flatlands of icewind weighted galman samples from the kalevipoeg data forms where reines weights chrysolepis encode local middelkoop densities. christe The bargon size of abortive each p.t. skeleton set ulmi is adapted trung according vojin to southerly the cluster geometry. 2003-07 The proposed 1981 technique mangalorean automatically sinosauropteryx detects diglossia the lightyears number lakesha of clusters hardy and is bloodstreams robust to comas outliers. earings The algorithm works for the 39.22 infinite minyans data stream hensher where more howen than one staufer pass cardarelli over andreasen the data 1,671 is peacebuilding not feasible. We fr\u00f6bel provide diems theoretical batista guarantees wildaid on the djerassi quality warrenville of the talgo clustering and also demonstrate tshopo its advantage rohillas over -1.6 the lymphatics existing state - guessers of - the - art on several datasets.", "histories": [["v1", "Wed, 10 Jun 2015 18:41:55 GMT  (1991kb,D)", "http://arxiv.org/abs/1506.03425v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["krzysztof choromanski", "sanjiv kumar", "xiaofeng liu"], "accepted": false, "id": "1506.03425"}, "pdf": {"name": "1506.03425.pdf", "metadata": {"source": "META", "title": "Fast Online Clustering with Randomized Skeleton Sets", "authors": ["Krzysztof Choromanski", "Sanjiv Kumar", "Xiaofeng Liu"], "emails": ["KCHORO@GOOGLE.COM", "SANJIVK@GOOGLE.COM", "XIAOFENGLIU@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "Online clustering in massive data streams is becoming important as data in a variety of fields including social media, finance and web applications arrives as a high throughput stream. In social networks, detecting and tracking clusters or communities is important to analyze evolutionary patterns. Similarly, online clustering can lead to spam and fraud detection in web applications such as detecting unusual mass activities in email services or online reviews.\nThere exist several challenges in developing a good clustering algorithm in a high throughput online scenario. In realworld applications, the number and shape of the clusters is typically unknown. The existing state-of-the-art online clustering methods with provable theoretical guarantees are primarily based on k-means or k-median/medoid, which assume apriori knowledge of the number of clusters and inherently make strong generative assumptions about the clusters. These assumptions force the retrieved clusters to be convex, leading to poor clustering for many real-world tasks. There exist several nonparametric techniques that do not make simplistic generative assumptions, but are mostly based on heuristics and lack theoretical guarantees. Moreover, in a true online scenario one needs to deal with continuos streams precluding the use of multiple passes over data applicable for finite-size streams commonly assumed by many techniques. Potential drift in data distribution over time is another practical difficulty that needs to be handled effectively. Finally, the clustering procedure should be efficient both in space and time to be able to handle massive data streams.\nIn this paper we propose a novel Skeleton-based Online Clustering (SOC) algorithm to address the above challenges. The basic idea of SOC is to represent each cluster via a compact skeleton set which faithfully captures the geometry of the cluster. Each skeleton set maintains a small random sample from the corresponding cluster in an online fashion, which is updated fast using the new data points. Each skeleton point is weighted according to local density around it. The number of skeleton points is automatically adapted to the structure of the cluster in such a way that more complicated shapes are approximated by more skeleton points. The skeleton sets are updated by a random procedure which provides robustness in the presence of outliers. The proposed algorithm automatically recovers the correct number of clusters in the data with high probability as more and more data is seen. The update strategy of ar X\niv :1\n50 6.\n03 42\n5v 1\n[ cs\n.A I]\n1 0\nJu n\nthe skeleton sets also allows the clustering method to automatically adapt to any drift in data distribution. In SOC, clusters can be merged as well as split over time. We also provide theoretical guarantees on the quality of clusters obtained from the proposed method."}, {"heading": "1.1. Related work", "text": "In comparison to the huge literature in offline clustering, work on online clustering has been somewhat limited. Most of the existing online clustering algorithms that have theoretical guarantees fall under model-based techniques such as k-mean, k-median or k-medoid (Guha et al., 2003; Ailon et al., 2009; Shindler et al., 2011; Bagirov et al., 2011). They assume specific shape of the clusters such as spheres that trivially leads to their compact representation using just a few parameters, e.g., center, radius and the number of points. However, as discussed before, these model based algorithms fail to capture arbitrary clusters in the data and can perform poorly.\nThere exist several nonparametric clustering methods where no assumption is made about the cluster shapes. Popular among them are DBScan (Ester et al., 1996), CluStream (Aggarwal et al., 2003), and DenStream (Cao et al., 2006). Recent surveys have described several variants of these algorithms (de Andrade Silva et al., 2013; Amini et al., 2014). The DenStream and CluStream methods create microclusters based on local densities, and combine them to form bigger clusters over time. However, these methods need to periodically perform offline partitioning of all the microclusters to form the clusters, which is not suitable for online clustering of massive data streams. LeaderFollower algorithm is another popular method and there exist several variants of it (Duda et al., 2000)(Shah & Zaman, 2010). These techniques typically encode every cluster by one center which is updated continuously as new points belonging to the cluster are detected. Such a cluster representation is not rich enough to encode more complex clusters. Overall, the main drawback of the above nonparametric online clustering algorithms is that they are mostly based on heuristics and lack any theoretical guarantees. They also require extensive hand tuning of the parameters. In (Shah & Zaman, 2010), the authors assume each cluster to be clique in order to provide theoretical guarantees, which is very restrictive in real-world.\nAnother popular method used in the context of incremental clustering is doubling algorithm (Charikar et al., 1997). Its standard version encodes every cluster by just one point. Furthermore, even though it allows for merging clusters, it does not permit to split them. We implement a variant of the method, where instead of one center several centers are kept per cluster. As we will show in experimental section, this purely deterministic approach, even though with some\ntheoretical guarantees, is too sensitive to outliers.\nOur proposed SOC algorithm shares a few similarities with two existing techniques: CURE algorithm (Guha et al., 2001), and core-set (Ba\u0304doiu et al., 2002). In CURE, similar to SOC, each cluster is represented by a random sample of data instead of just one center to handle arbitrary cluster shapes. CURE, however, is an offline hierarchical agglomerative clustering approach with running time quadratic in the size of data, which is too slow for online applications. In core-set based clustering, the aim is to encode a complicated cluster shape via a compact sample of points. The existing state-of-the-art algorithms that use the idea of the core-set (Gonzalez, 1985; Alon et al., 2000; Harpeled & Varadarajan, 2001; Ba\u0304doiu et al., 2002) are computationally too intensive to be useful for online clustering in practice. The running time is exponential in the number of stored skeleton points. Furthermore, the variants that give provable theoretical guarantees are inherently offline methods that often require several passes over the data to produce good-quality clustering. For instance, the algorithm presented in (Ba\u0304doiu et al., 2002) needs to be rerun 2O(k) log(n) times, where k is the size of the core-set an n is dataset size.\nNonparameteric graph-based techniques such as spectral clustering can recover arbitrary shaped clusters but they are appropriate mainly for the offline setting (Ng et al., 2001). Moreover, they also assume a priori knowledge of the number of clusters. Several relaxations such as iterative biclustering have been proposed to overcome the need to know the number of clusters apriori but these methods cannot be extended to an online setting. Recently, there has been some work on incremental spectral clustering which essentially iteratively modifies the Graph Laplacian (Ning et al., 2010; Langone et al., 2014; Jia, 2012; Chia et al., 2009). In a true online setting, however, even building a good initial Graph Laplacian is infeasible either due to the lack of enough data or the computational bottlenecks."}, {"heading": "2. Clustering Framework", "text": "In this work, since we are interested in retrieving arbitraryshaped clusters, it is important to first define what constitutes a good cluster. The traditional model-based techniques assume a global distance measure (e.g., L2 distance in k-means), which restricts one to convex-shaped clusters. Instead, the proposed algorithm works in a nonparametric setting where clusters are defined by an intuitive notion of paths with \u2019short-edges\u2019. Two points are more likely to belong to the same cluster if there exist many paths in the data neighborhood graph such that any two consecutive points on each path are not \u201dfar\u201d from each other. Clearly, the overall distance between two points can be large but they can still be in the same cluster. Such a setting enables us\nto consider many complicated cluster shapes. To emphasize, the idea of the neighborhood graph is to understand the cluster definition we implicitly utilize in this work. We do not need to explicitly construct a graph in our approach."}, {"heading": "2.1. Skeleton-based Online Clustering (SOC) Algorithm", "text": "The key idea behind the SOC algorithm is to represent each cluster via a set of pseudorandom samples called the skeleton set. The algorithm stores and constantly updates a set of the skeleton sets P = {S1, ..., St}. Note that the size of the set P corresponds to the number of clusters, which may change over time as new data is seen. Each skeleton set Si represents a cluster and consists of a sample (vi1, ..., v i hi\n) of all the points belonging to that cluster up to time t together with some carefully chosen random numbers (mi1, ...,m i hi ) and weights (wi1, ..., w i hi\n). Thus, a skeleton set Si is the set of elements of the form: (vij ,m i j , w i j) for j = 1, 2, ..., hi. Sometimes we will also call {vi1, ..., vihi} a skeleton set, which should be clear from the context. Let us define a map \u03be(vij) = m i j . We denote bywy the weight of the skeleton point y and by my the corresponding random number. Weights encode the local density around a skeleton point. We denote by Wi the sum of weights of all the skeleton points representing the ith cluster and by W (A) the sum of weights for all the points belonging to set A. The skeleton set is updated in such a way that at any given time t skeleton points are pseudouniformly distributed in the entire cluster. As mentioned before, the number hi of skeleton points of the ith cluster (alternative notation: hS , where S stands for the skeleton set) is not fixed and can vary over time. When the cluster arises, it is initialized with hinit skeleton points. In the algorithm we take hinit = 1 and in the theoretical section we show how the lower bounds on hinit can be translated to strict provable guarantees regarding the quality of the produced clustering. The algorithm tries to maintain a skeleton set in such a way that between any two skeleton points there exists a path of relatively short edges consisting entirely of other skeleton points from the same skeleton set. If the cluster grows and the average distance between skeleton points is too large, the number of skeleton points is increased. This number never exceeds H which is a given upper bound determining the memory the user is inclined to delegate to encode any cluster.\nThe overall algorithm works by first initializing the number of samples stored in each skeleton set. In this work, without loss of generality we assume that each skeleton set initially has only one sample. We will propose two variants of the algorithm - one where only lazy cluster-merging is performed (MergeOnlySOC) and the other, where merging can be more aggressive since splitting clusters is also allowed (MergeSplitSOC). As we will see in the experimen-\nInput: Infinite data stream D; Output: Cluster assignment for the observed data; Pick r, \u03b1; P \u2190 \u2205,G \u2190 \u2205; while true do\nfor each Si do if exists v \u2208 Si such that wv \u2264 Wi2|Si| then\nCheckSplit(G, v, r, Si,P); end\nend Read next x \u2208 D; Compute Ti = Si \u2229B(x, r) for each Si \u2208 P; U \u2190 \u2205; for each Ti do\nif W (Ti) \u2265 \u03b1Wi then U \u2190 U \u222a {Si};\nend end if U 6= \u2205 then\nMerge(x,U ,P,G, r); else\nAddSingleton(x,P,G); end\nend Algorithm 1: SOC clustering - main procedure\ntal section, the latter produces a good approximation of the groundtruth clustering in fewer steps, but at the cost of extra time needed to check and perform splitting. If MergeSplitSOC version is turned on then the algorithm keeps a set of undirected graphs G. Each element of G is associated with a different skeleton set and encodes the topology of connections between points in that set. We denote by GS an element of G associated with the skeleton set S.\nThe overview of the SOC algorithm is given in Algorithm 1. If splitting is turned on, at each time step t, given the existing skeleton set for each cluster, the algorithm checks if any cluster should be split. Then, as a new data point x arrives in the stream, first a ball of radius r centered at x i.e., B(x, r) is created. Then, the intersection of this ball with each existing skeleton set is computed. If the weight of the skeleton points in this intersection is more than \u03b1Wi for 0 < \u03b1 < 1, then x is assumed to belong to that cluster and the corresponding skeleton set is updated. Note that it is possible that multiple clusters claim the point x, in which case, all those clusters are considered for merging. If the intersection of the ball with all the skeleton sets is empty, then a new singleton cluster is created.\nWe will start with describing the MergeOnlySOC variant (i.e., assume that the splitting-related procedures: CheckSplit and UpdatedGraph in Algorithm 1 and Algorithm 2 are turned off). The MergeSplitSOC variant will discussed\nInput: Datapoint x, subset U = {Si1 , ..., Sik} \u2286 P , current clustering P , family of graphs G, radius r; Output: Updated P after merging x with clusters from G; Denote: W = \u2211 y\u2208(Si1\u222a...\u222aSik )\u2229B(x,r) wy ,\ndav(x, r) = \u2211\ny\u2208(Si1\u222a...\u222aSik )\u2229B(x,r) wy W \u2016x\u2212 y\u20162, hun = min( \u2211\nj=1,...,k hSij , H); Compute: Sextij = Ext(Sij , hun) for j = 1, ..., k; Let: S \u2190 {Sexti1 , ..., S ext ik }; if dav(x, r) \u2264 r2 or (hun = H) then S \u2190 S \u222a {Ext({x}, hun)}; end Denote by vij the jth skeleton point of the ith skeleton set of S; initialize: Smerged \u2190 \u2205; for j = 1, ..., hun do\nv \u2190 argmin(\u03be(v1j ), ..., \u03be(v |S| j )); m\u2190 min(\u03be(v1j ), ..., \u03be(v |S| j )); w \u2190 wv; Smerged \u2190 Smerged \u222a {(v,m,w)};\nend if dav(x, r) > r2 and (hun < H) then\ngenerate r according to Gen(s); Smerged \u2190 Smerged \u222a {(x, r, 1)};\nelse Let vmin \u2208 S, where S \u2208 Smerged be such that: \u2016x\u2212 vmin\u20162 = minS\u2208Smerged minz\u2208S \u2016x\u2212 z\u20162; if vmin 6= x then\nwvmin \u2192 wvmin + 1; mvmin \u2192 min(mvmin , r), where r is generated according to Gen(s);\nend end Let G0 = {GSi1 , ..., GSik }; P \u2190 (P \u222a {Smerged})\\{Si1 , ..., Sik}; UpdatedGraph(G, x, r,G0, Smerged);\nAlgorithm 2: SOC clustering - Merge subprocedure\nlater in section 2.2.\n2.1.1. SUBPROCEDURE Merge\nThe goal of the Merge subprocedure is to merge a new point x with one or more clusters (Algorithm 2). When x is assigned to multiple clusters, it basically acts as a linking point to merge them, resulting in a unified cluster. This step is crucial in online clustering scenario as points from the same cluster may be assigned to different clusters initially but as more evidence builds up from the new data, one can combine these clusters to recover the true underlying cluster. In the Merge subprocedure the skeleton set is updated when a new cluster is constructed. The skeleton sets Si1 , ..., Sik representing clusters that will be merged are given as input. Before describing the Merge procedure let us introduce an important subroutine. Let S = {(v1,m1, w1), ..., (vt,mt, wt)} be a skeleton set of size hS < h for some h > 0. We denote by Ext(S, h) the\nextension of S obtained by adding to S exactly h\u2212hS more triples according to the following procedure. Each newly added triple has weight 1. Each newly added skeleton point is chosen independently at random from the set {v1, ..., vt} in such a way that skeleton point vi is chosen with probability wi\u2211t\nj=1 wj . The corresponding random number m is\ngenerated by a pseudorandom number generator Gen(s) with seed s. The seed can be initialized randomly or alternatively it can be chosen as a function of the skeleton point by conceptually partitioning the entire input space with a grid of lengh \u03b4, and using the id of the cell occupied by x in this grid. The latter procedure is useful for infinite streams to avoid correlated random sequences for far away points in the space. Subroutine Ext can be run also if its first argument is a single point x. In this case the output is a skeleton set of the form {(x, r1, 1), ..., (x, rh, 1)}, where r1, ..., rh is a sequence of h random numbers generated according to Gen(s).\nThe Merge algorithm computes an average weighted distance dav(x, r) from the new point to those skeleton points from Si1 , ..., Sik that reside in B(x, r). Next, two cases are considered. If dav(x, r) \u2264 r2 or the number of skeleton points in all the skeleton sets to be merged is H then the algorithm decides not to increase the size of the merged skeleton set (since the linking point is relatively close to the merged clusters or the union of skeleton sets under consideration is already saturated). Denote by hun the minimum of H and the total number of skeleton points in all skeleton sets to be merged. The merged skeleton set will be of size hun. Let us describe how the jth skeleton point of the newly formed cluster for j = 1, ..., hun will be computed. First, each contributing skeleton set is extended to size hun by weighted random sampling from it (see procedure Ext described above). This is also the case for x (we treat x as a skeleton set consisting of hun copies of x).\nWe take the jth skeleton points from all the clusters to be merged and x, and choose the new point and the corresponding value as shown in Algorithm (2), using random sequence generated for x. Each newly added point has to contribute to the weight distribution in the cluster. If point x is not in the new skeleton set then the closest point vmin from the skeleton set is found and its weight is increased by one (x contributes to the total weight of vmin). The new skeleton set replaces the skeleton sets of all the clusters that are merged.\nNow let us assume that dav(x, r) > r2 and the total number of all skeleton points in the skeleton sets to be merged is smaller than H . Intuitivaly speaking, this means that the cluster had grown too much (the local density within linking point x is too small) and thus the number of skeleton points encoding the cluster has to increase (since the pool of the skeleton points that can be used to represent a clus-\nInput: Datapoint x, current clustering P , family of graphs G; Output: Updated version of P after adding cluster-singleton {x}; Generate a random number rx according to Gen(s); Snew \u2190 {(x, rx, 1)}; P \u2190 P \u222a {Snew}; G \u2190 G \u222a {Gsin};\nAlgorithm 3: SOC clustering - AddSingleton subprocedure\nter has not been used entirely). If this is the case then the same procedure as for the previous case is conducted but the skeleton set corresponding to x is excluded. Finally, x is added as the last skeleton-singleton of weight 1 (and with corresponding random number selected according to a given random number generator) to the newly formed cluster.\n2.1.2. SUBPROCEDURE AddSingleton\nThe procedure AddSingleton adds a new cluster consisting of just x when no existing cluster is found to be close enough to x based on the intersection with the skeleton sets (Algorithm 3). Next, a skeleton set for this new singleton cluster is created. Since a skeleton set aims to cover uniformly the entire mass of the cluster using hinit random samples, point x is repeated hinit times to form the skeleton set for the cluster-singleton.\nFurthermore, a sequence of hinit random values is generated from Gen(s), and each copy of x in the skeleton set is assigned one of the values and weight w = 1 to build the hinit triples and complete the skeleton. In the proposed implementation we initialize each newly created clustersingleton with only one skeleton point, thus hinit = 1 (see Algorithm 3). For the newly created skeleton set an undirected graph-singleton Gsin is created."}, {"heading": "2.2. Splitting clusters", "text": "We now describe the cluster splitting procedure in the MergeSplitSOC variant of the algorithm. It is handled by two additional procedures: CheckSplit and UpdatedGraph.\nCheckSplit determines whether a given skeleton set should be split by looking for a breaking point v, which is the skeleton point whose weight is at most half of the average weight of the points within the skeleton set. If such a point v is found then the algorithm determines whether the cluster should be split as follows. First, all the points from B(v, r2 ) are deleted from the corresponding graph G of the skeleton set. A connected component analysis is then conducted on the remaining graph. If more than one connected components is found, it means v is a breaking point and\nInput: Family of graphs G, skeleton point v, radius r, skeleton set S corresponding to v, current clustering P; Output: Updated version of G and P; Let Rem = {y \u2208 S : \u2016v \u2212 y\u20162 \u2264 r2}; Let Gdel = GS \\Rem; Run CC algorithm on Gdel to obtain {C1, ..., Ct}; if t > 1 then\nDenote by Si the subset of S corresponding to Ci for i = 1, ..., t; Update: G \u2190 (G \u222a {C1, ..., Ct}) \\ {GS} and P \u2190 (P \u222a {S1, ..., St} \\ {S}); end Algorithm 4: SOC clustering - CheckSplit subroutine\nInput: Family of graphs G, skeleton point x, radius r, subfamily {GS1 , ..., GSk} \u2286 G, skeleton set Snew; Output: Updated version of G; Let G = GS1 \u2297 ...\u2297GSk ; Let E = {{x, y} : x, y \u2208 Snew \u2229B(x, r),\u2203i 6=jx \u2208 GSi , y \u2208 GSj}; Let Gmerged be a graph obtained from G by adding edges from E; Update: G \u2190 (G \u222a {Gmerged}) \\ {GS1 , ..., GSk};\nAlgorithm 5: SOC clustering - UpdatedGraph subroutine\nthe cluster is split so that each connected component forms a new cluster and the points in the connected components consitute the new skeleton sets.\nThe UpdatedGraph procedure is shown in Algorithm 5, and is responsible for constructing a graph Gunion for the newly formed cluster and replacing with it all graphs corresponding to merged skeleton sets. The graph Gunion is constructed by combining all elements of G corresponding to the skeleton sets that need to be merged. Those graphs are combined by adding edges between skeleton points from the newly constructed skeleton set that are in the close neighborhood of the linking point x. In the description of UpdatedGraph we denote by G = G1 \u2297 G2... \u2297 Gk the graph with vertex set V (G) = V (G1) \u222a ... \u222a V (Gk) and edge set E(G) = E(G1) \u222a ... \u222a E(Gk)."}, {"heading": "3. Theoretical analysis", "text": "In this section we provide theoretical results regarding SOC algorithm for the clustering model described in Sec. 2. We start by introducing the general mathematical model we are about to analyze. It is one of the many variants of planted partition models used to construct data with hard clustering and outliers. Notice that our algorithm does not require the input to be produced according to this model. In particular, we do not use any specific parameters of the model in the algorithm.\nFor a set of D-dimensional data, we assume it contains k\ndisjoint compact sets , which are called cores and denoted as R1, ...,Rk \u2286 RD. The cores are called \u2206-separable if the minimum distance between any two cores is greater than \u2206, i.e.: \u22001\u2264i<j\u2264k,x\u2208Ri,y\u2208Rj\u2016x \u2212 y\u20162 > \u2206. These cores can have arbitrary shapes giving rise to the observed clusters such that the points in the ith cluster Ci come from coreRi with high probability and from the rest of the space with low probability. Formally, given a set of probabilities {p1, ...pk}, points in the cluster Ci are sampled from the coreRi with probability pi and from outsideRi with probability 1\u2212 pi, where pi (1\u2212 pi). It is important to note that even though the cores are separable, this is no longer the case for clusters due to the presence of \u201doutliers\u201d. In other words, short-edge paths between points from different clusters may exist, but not many in expectation. We call the clustering model presented above as a (R\u0303, p\u0303)-model, where p\u0303 = (p1, ..., pk) and R\u0303 = (R1, ...,Rk). This is a quite general model which has good-quality clustering of cores (because of \u2206-separability). However due to the outliers, the task of recovering the clusters is nontrivial even in an offline setting. Simple heuristics such as connectedcomponents cannot be used to recover the clusters in the offline mode. The online setting brings additional algorithmic and computational challenges. Below, we give details of the proposed clustering mechanism.\nWe need the following definition.\nDefinition 3.1. A set W \u2286 RD is called to be (s, r)coverable if W can be covered by s balls, each of radius r.\nLet \u03c0i be the probability that a new point in the data stream belongs to cluster Ci. Fix a covering C of Ri with |C| \u2264 s (for every core Ri). Let B \u2208 C be an arbitrary ball from the covering C. Furthermore, let pbi be a lower bound on the probability that a new point is from set (Ri \u2229 B) given it belongs to core Ri, which can be expressed as pbi \u223c 1s . Denote \u0393i = \u03c0ipip b i\npf+\u03c0ipi , and \u03b3i = p\nf pf+\u03c0ipi , where pf =\u2211k\ni=1 p f i , and p f i = 1\u2212pi. Here \u0393i is the probability that a new point came from a fixed ball of covering C ofRi given that it belongs to cluster Ci. Similarly, \u03b3i is the probabiltiy that a new point is an outlier given that it belongs to cluster Ci. Since outliers are expected to be lower than the points from the cluster cores, \u03b3i \u0393i. Denote \u0393 = mini \u0393i and \u03b3 = maxi \u03b3i.\nSince we keep at most H samples in a skeleton set, most of the cluster points are not in this set. The error made by the algorithm on a new point v is defined as follows: Suppose v comes from the core Ri, and gets assigned to a cluster that contains points from other cores as well, or there exists another cluster that also contains points fromRi. Note that it is a strict definition of error as in an online setting transient overclustering is expected due to lack of enough\ndata in the early phase. We say that the algorithm reaches the saturation phase when each skeleton set reaches size H . We are ready to state the main results of our analysis regarding the MergeOnlySOC version of the algorithm.\nTheorem 3.1. Assume that we are given a dataset constructed according to the (R\u0303, p\u0303)-model with k cores with outliers. Cores are \u2206-separable. Assume that each core of the cluster is (s, \u22064 )-coverable. Let n be the number of all the points seen by the algorithm after the saturation phase has been reached. Then with probability at least 1\u2212 4 for hinit = \u2126(max( \u03b3 \u03933 log( sk ), 1 \u03932 log( nsk ))), r = \u2206 4 and \u03b1 = \u03934 the SOC algorithm will not merge clusters containing points from different cores in the saturation phase if they were not merged earlier.\nTheorem 3.1 gives upper bounds on the minimal number of skeleton points per cluster ensuring that MergeOnlySOC does not undercluster. As we will see in the experimental section, this bound in practice is much lower. We also have the following.\nTheorem 3.2. Under the assumptions from Theorem 3.1, with probability at least 1 \u2212 , the SOC algorithm will not make any errors on points coming from coreRi after m = 17 2\u03932 log( 3s ) points from the corresponding cluster Ci have been seen in the saturation phase.\nTheorem 3.2 says that under a reasonable assumption regarding the number of initial skeleton points per cluster and after the short initial subphase of the saturation phase, algorithm MergeOnlySOC classifies correctly all points coming from cores. In other words, we obtain the upper bound on the rate of convergence of the number of clusters produced by MergeOnlySOC to the groundtruth value.\nThe proofs of Theorem 3.1 and Theorem 3.2 will be given in the Appendix. Below we give a very short introduction and present a useful lemma that we will rely on later."}, {"heading": "3.1. Merging Lemma", "text": "Since both theorems consider the saturation phase of the algorithm, in the theoretical analysis whenever we will talk about the algorithm we will in fact mean its saturation phase. Without loss of generality we can also assume in our theoretical analysis that each skeleton point has weight one. Indeed, a point v that has weight wv > 0 may be equivalently treated as a collection of wv skeleton points of weight one (see: description of the algorithm). Let us formulate the following lemma:\nLemma 3.1. Let us assume that at time t the algorithm merges two clusters: P1 and P2 such that P1 contains a point from Ri and P2 contains a point from Rj for some i 6= j. Then either: at least \u03b1h of all skeleton points of P1 at time t are outliers or: at least \u03b1h of all skeleton points of P2 at time t are outliers.\nProof. The lemma follows from the definition of \u2206, according to which, any two points taken from different cores are at least \u2206 distance apart. If two clusters: P1, P2 are merged at time t then there exists a data point v (a merger) and a ball B(v, \u22062 ) such that B(v, \u2206 2 ) contains at least \u03b1h skeleton points of P1 and at least \u03b1h skeleton points of P2 (see: Fig. 1). But B(v, \u22062 ) cannot contain points from different cores since they are \u2206-separable. Thus at least one of the clusters from: {P1, P2} has in B(v, \u22062 ) at least \u03b1h skeleton points that are outliers."}, {"heading": "4. Experiments", "text": "We evaluated the performance of the proposed SOC algorithm using four synthetic datasets as shown in Fig. 2 (left column). The sets contain data points in 20 dimensions. The first two dimensions were randomly drawn from predefined clusters, as shown in the figure, while the other 18 dimensions was random noise. For the data sets B1 and B2, 1000 data points were randomly drawn from each of the two banana shaped clusters for the first two dimensions, Then 1000 (for B1) and 2000 (for B2) outliers were randomly generated from a vicinity of the shapes, respectively. For the data sets L1 and L2, 500 data points were randomly drawn from each of the four letter shaped clusters. Then 500 (for L1) and 2500 (for L2) outliers were randomly generated from a vicinity of the shapes respectively. The values in the other 18 dimensions for all the data were Gaussian white noise with a standard deviation of 0.01. All the data points were then randomly permuted so that their orders in the data stream would not affect the results. Examples of the datasets were plotted in the first two dimensions and shown in Fig. 2 (left column). We used the MergeSplitSOC version of the algorithm since it provided faster convergence of the number of clusters under the same quality guarantees.\nWe compared the SOC method with several state-of-theart nonparametric clustering methods, i.e., DBScan (Ester et al., 1996), Leader-Follower algorithm (Duda et al., 2000), Doubling algorithm (Charikar et al., 1997), and DenStream (Cao et al., 2006). The clustering quality was quantitatively evaluated using the average clustering purity, which is defined as\np =\n\u2211K i=1 |Cdi | |Ci|\nK , (1)\nwhere K is the number of clusters, |Ci|is the number of points in cluster i, and |Cdi | is the number of points in cluster i with the dominant class label.\nFig. 2 shows the comparative results of the SoC method as well as several other methods. Fig. 3 shows the clustering purity of all the methods. The SOC method requires two parameters (\u03b1 and r). In all the experiments, we selected \u03b1 = 0.03 and r = 0.07. All the results were produced using the best choice of parameters for each method. We note that parameter tuning was not trivial because most of the methods require at least two parameters.\nResults showed that the SOC method was able to cluster the data well, even though it slightly over-clustered in the datasets B1 and B2. The Leader-Follower algorithm as well as streaming DBScan simply do not handle this type of data. SOC obtains similar results to these from DenStream algorithm (it produced slight overclustering, but obtained almost 100% purity). DBscan worked well on the banana sets, but it failed to cluster in L2, where the outliers overnumbered the true clusters. Doubling failed to work on the more noisy data sets (B2 and L2). For the other two datasets, the clustering purity was low, probably because of the noise in other 18 dimensions. Leader-Follower method worked fine for L1 and L2, but poorly for B1 and B2. It was mostly because of the nature of the method, and partly because of the noise in the other 18 dimensions. Standard variant of the Leader-Follower uses only a small number of centers per cluster and when the clusters are not contained in convex well-separable objects, the recognition is very poor. DenStream worked well on all the cases.\nOur method is faster than DenStream (the running times per point varied from 60 to 90 microseconds for SOC and were above 100 microseconds for DenStream). The reason is that DenStream is not a purely online approach and performs offline clustering periodically. That part is a bit expensive. DenStream has another serious drawback. The id of the cluster the newly coming point is assigned to is computed based on the most recent snapshot of the offline clustering that was produced, not on-fly. Thus the accuracy of the method depends heavily on the special parameter determining how much data distribution evolves over time. Since parameter tuning is always very nontrivial for\nthe density-based methods, this one extra parameter adds another layer of difficulty. We do not need this parameter in our approach.\nThe SOC method slightly overclustered in B1 and B2 because of the online nature of the algorithm and the presence of the outliers. Nontheless, it was able to correctly throw out the outliers and produced results with high purity. This is because, even though outliers can become part of the skeleton set of a cluster, they are typically replaced by the true cluster points eventually as the true cluster points have a higher density and arrive at a higher rate than the outliers.\nFig. 4 shows the skeleton points generated by SOC for two data sets. The maximum number of skeleton points used was only few hundred points. The number of skeleton points grows gradually as clusters become bigger. We set\nup the upper bound on the skeleton points per cluster as H = 400 but this number was not reached."}, {"heading": "5. Conclusions", "text": "We have presented a new truly online clustering algorithm which can recover arbitrary-shaped clusters in the presence of outliers from massive data streams. Each cluster is repre-\nsented efficiently by a skeleton set, which is continuously updated to dynamically adapt to the changing data distribution. The proposed technique is theoretically sound as well as fast and space-efficient in practice. It produced good-quality clusters in various experiments for nonconvex clusters. It outperforms several online approaches on many datasets and produces results similar to the most effective hybrid ones that combine online and offline steps (such as DenStream). In the future, we would like to investigate other methods for updating skeletons within given framework in the online fashion since this mechanism is crucial for the effectiveness of the presented approach. The other interesting area is the research on the maximal number of clusters that are created during the execution of the algorithm. A more precise bound will provide a more accurate theoretical estimate on the memory usage."}, {"heading": "6. Appendix", "text": ""}, {"heading": "6.1. Proof of Theorem 3.1", "text": "Proof. From Lemma 3.1 we conclude that in order to find an upper bound on the event that the algorithm will at some point merge \u201dwrong clusters\u201d, it suffices to find the upper bound on the probability that the algorithm will at some point produce a cluster with at least \u03b1h skeleton points that are outliers and at least one nonoutlier v. Denote this latter event by E . Denote by Ei the intersection of E with the event that v comes from the core Ri. Note that E = E1 + ...+ Ek.\nFor a set of points D we say that a point v dominates D if the following is true: At least one of the h random numbers assigned to v by the algorithm is smaller than all corresponding random numbers assigned to data points from D\\{v}. Denote the set of all data points that are outliers as O. Let us fix a core Ri and some constant W . Denote by t0, t1, ... time stamps at which first W , W + 1,... points from Ri \u222a O are collected. Let us first find a lower bound on the probability of the following event F : for every tj in every ball of the (s, \u22064 )-covering of Ri, there are at least \u03b1h points from Ri that are dominating the set of all the points collected up to time tj . Fix some ball B of the covering of Ri. By the definition of \u0393i, we know that on average (W + j)\u0393i points from Ri \u2229 B arrived up to time tj . By Azuma\u2019s inequality, we know that this number is tightly concentrated around its average, i.e., for every 1 > 0 the probability that for a fixed tj and fixed ball B this number is less than (W + j)(\u0393i \u2212 1) is at most e\u22122W 2 1 . In fact, by using a more general version of the Azuma\u2019s inequality, we can get rid of a fixed tj and have the same upper bound for all tjs simultanously. Consider the following event G: for every ball B up to time tj for every j = 0, 1, ... at least (W + j)(\u0393i \u2212 1) data points from that ball have arrived. Thus, using union bound over all the balls of the covering, we conclude that G happens with probability at least 1 \u2212 se\u22122W 21 . Now we analyze F conditioned on G. For any fixed tj , the average number of dominating points in the fixed ball B of the covering of Ri is at least h(\u0393i \u2212 1). Besides, as previously, one can easily note that the actual number is tightly concentrated around the average. Taking \u03b4 = \u0393i \u2212 1 \u2212 \u03b1 and using Azuma\u2019s inequality once again, we derive an upper bound of the form e\u22122h\u03b4 2\non the probability that a fixed ball B of the covering at some fixed time tj contains fewer dominat-\ning points than we assumed above. Using this and taking the union bound over all tj and all s balls of the covering we get: P(Fc) \u2264 se\u22122W 21 + (1 \u2212 se\u22122W 21)sne\u22122h\u03b42 , where X c stands for the complement of an event X . Let H denote an event that among first W points fromRi \u222a O there will be at most Wx outliers, where: x = \u03b3i(1 + 2) and 2 > 0 is some fixed constant. From Chernoff\u2019s inequality we get: P(Hc) \u2264 e\u2212 22 2+ 2 W\u03b3i . Now assume that H happens and that W points from Ri \u222a O have already arrived. Denote by I the following event: for all balls B of the covering of Ri at any time t\u2032 no \u03b1h skeleton points in any ball are outliers. Note that if I holds then after W points have been seen, at least \u03b1h \u2212Wx new points from B that are outliers must be seen up to time t\u2032. Fix again a ball B of the covering of Ri. Let us take next t points coming fromRi \u222aO for t = \u03b1h\u2212Wx,\u03b1h\u2212Wx+ 1, ... (we will take W in such a way that \u03b1h \u2212Wx > 0). For a fixed t the probability that out of those t points there are more than (1 + 2)t\u03b3i outliers is, by Chernoff\u2019s bound, at most e\u2212 22 2+ 2 t\u03b3i . Denote by J the following event: for all t \u2208 {\u03b1h \u2212 Wx,\u03b1h \u2212 Wx + 1, ...} there are at most (1 + 2)t\u03b3i outliers out of those t points. By the union bound we have: P(J c) \u2264 ne\u2212 22 2+ 2 t\u03b3i . Assume that J holds and fix t. The probability that after t new points have been seen, one of the outliers will become a skeleton point in a fixed ball B based on the fixed mth random number that was assigned to it by the algorithm is at most (1 + 2)\u03b3i. The probability that this will be the case for at least \u03b1h \u2212Wx outliers is at most ((1 + 2)\u03b3i)\u03b1h\u2212Wx. Thus, if we take the union bound we can conclude that P(Ic) \u2264 n(e\u2212 22 2+ 2 t\u03b3i + s((1 + 2)\u03b3i)\n\u03b1h\u2212Wx). Notice that Ei \u2286 Fc \u222a Hc \u222a Ic. Thus we obtain P(Ei) \u2264 P(Fc) + P(Hc) + P(Ic). We conclude that P(Ei) \u2264 se\u22122W 2 1 + (1\u2212 se\u22122W 21)sne\u22122h\u03b42 + e\u2212 22 2+ 2 W\u03b3i + n(e\u2212 22 2+ 2\nW\u03b3i + s((1 + 2)\u03b3i)\n\u03b1h\u2212Wx). Thus, taking the union bound over all the cores we obtain: P(E) \u2264 kse\u22122W 21 + ksne\u22122h\u03b42 + k(n + 1)e\u2212 22 2+ 2 W\u03b3i + kns((1 + 2)\u03b3i)\n\u03b1h\u2212Wx. If we fix: 1 = \u03b1 = \u0393i 4 and W = h\u0393i 16\u03b3i\n, then by bounding each term of the RHS expression in the last inequality with k , we obtain the lower bound on h as in the statement of the theorem. Since we have already noticed that it suffices to find an appropriate upper bound on P(E), this concludes the proof."}, {"heading": "6.2. Proof of Theorem 3.2", "text": "We will use notation from the proof of Theorem 3.1.\nProof. Fix some Ri. Note than we have already proved that an event F does not hold with probability at most se\u22122W 2 1 + (1 \u2212 se\u22122W 21)sne\u22122h\u03b42 . Also note that from\nthe definition of F we know that if F holds then the number of clusters computed by the algorithm and containing at least one point from Ri will not increase after the time when first W data points from the corresponding cluster Ci have been seen. Let us assume that F holds. Notice that by the time every ball of the covering gets at least one new data point, there will be just one cluster computed by the algorithm with points from the core Ri. When this is the case no further errors regarding new points from the core Ri will be made. From Azuma\u2019s inequality and the union bound we immediately get: the number of extra data points coming from Ci that need to be seen to populate every ball of the covering with at least one of them is more than T with probability at most se\u22122T\u0393 2 i . We conclude that with probability at most se\u22122W 2 1 + sne\u22122h\u03b4 2 + se\u22122T\u0393 2 i after T + W points of Ci have been already seen, the algorithm will still make mistakes on the new data points fromRi. If we now upper-bound every ingredient of the above sum by 3 and solve for W and T , then we get: W \u2265 8 \u03932 log( 3s ), T \u2265 12\u03932 log( 3s ). Taking the number of points h as in the previous theorem concludes the proof."}], "references": [{"title": "A framework for clustering evolving data streams", "author": ["Aggarwal", "Charu C", "Han", "Jiawei", "Wang", "Jianyong", "Yu", "Philip S"], "venue": "In VLDB, pp", "citeRegEx": "Aggarwal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Aggarwal et al\\.", "year": 2003}, {"title": "Streaming kmeans approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "Neural Processing Systems Conference,", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "Testing of clustering", "author": ["Alon", "Noga", "Dar", "Seannie", "Parnas", "Michal", "Ron", "Dana"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Alon et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2000}, {"title": "On density-based data streams clustering algorithms: A survey", "author": ["Amini", "Amineh", "Teh", "Ying Wah", "Saboohi", "Hadi"], "venue": "J. Comput. Sci. Technol.,", "citeRegEx": "Amini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2014}, {"title": "Approximate clustering via core-sets", "author": ["B\u0101doiu", "Mihai", "Har-Peled", "Sariel", "Indyk", "Piotr"], "venue": "In Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "B\u0101doiu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "B\u0101doiu et al\\.", "year": 2002}, {"title": "Fast modified global k-means algorithm for incremental cluster recognition", "author": ["A. Bagirov", "J. Ugon", "D. Webb"], "venue": "Pattern Recognition,", "citeRegEx": "Bagirov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bagirov et al\\.", "year": 2011}, {"title": "Density-based clustering over an evolving data stream with noise", "author": ["Cao", "Feng", "Ester", "Martin", "Qian", "Weining", "Zhou", "Aoying"], "venue": "In Proceedings of the Sixth SIAM International Conference on Data Mining, April", "citeRegEx": "Cao et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2006}, {"title": "Incremental clustering and dynamic information retrieval", "author": ["Charikar", "Moses", "Chekuri", "Chandra", "Feder", "Tom\u00e1s", "Motwani", "Rajeev"], "venue": "In Proceedings of the Twentyninth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Charikar et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 1997}, {"title": "Data stream clustering: A survey", "author": ["de Andrade Silva", "Jonathan", "Faria", "Elaine R", "Barros", "Rodrigo C", "Hruschka", "Eduardo R", "de Carvalho", "Andr\u00e9 Carlos Ponce Leon Ferreira", "Gama", "Jo\u00e3o"], "venue": "ACM Comput. Surv.,", "citeRegEx": "Silva et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2013}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["Ester", "Martin", "peter Kriegel", "S Hans", "Jrg", "Xu", "Xiaowei"], "venue": null, "citeRegEx": "Ester et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Ester et al\\.", "year": 1996}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["Gonzalez", "Teofilo F"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Gonzalez and F.,? \\Q1985\\E", "shortCiteRegEx": "Gonzalez and F.", "year": 1985}, {"title": "Clustering data streams: Theory and practice", "author": ["S. Guha", "A. Meyerson", "N. Mishra", "R. Motwani", "L. O\u2019Callaghan"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Guha et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2003}, {"title": "Cure: An efficient clustering algorithm for large databases", "author": ["Guha", "Sudipto", "Rastogi", "Rajeev", "Shim", "Kyuseok"], "venue": "Inf. Syst.,", "citeRegEx": "Guha et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2001}, {"title": "Approximate shape fitting via linearization", "author": ["Har-peled", "Sariel", "Varadarajan", "Kasturi R"], "venue": "Proc. 42nd Annu. IEEE Sympos. Found. Comput. Sci,", "citeRegEx": "Har.peled et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Har.peled et al\\.", "year": 2001}, {"title": "Online spectral clustering on network streams. Dissertation at the Department of Electrical Engineering and Computer Science, University of Kansas", "author": ["Y. Jia"], "venue": null, "citeRegEx": "Jia,? \\Q2012\\E", "shortCiteRegEx": "Jia", "year": 2012}, {"title": "Incremental kernel spectral clustering for online learning of non-stationary data", "author": ["R. Langone", "O. Agudelo", "Moor", "B. De", "J. Suykens"], "venue": null, "citeRegEx": "Langone et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Langone et al\\.", "year": 2014}, {"title": "Incremental spectral clustering by efficiently updating the eigen-system", "author": ["H. Ning", "W. Xu", "Y. Chi", "Y. Gong", "T. Huang"], "venue": "Pattern Recognition,", "citeRegEx": "Ning et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ning et al\\.", "year": 2010}, {"title": "Community detection in networks: The leader-follower algorithm", "author": ["Shah", "Devavrat", "Zaman", "Tauhid"], "venue": "CoRR, abs/1011.0774,", "citeRegEx": "Shah et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2010}, {"title": "Fast and accurate k-means for large datasets", "author": ["M. Shindler", "A. Wong", "A. Meyerson"], "venue": "Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems Conference,", "citeRegEx": "Shindler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shindler et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "Most of the existing online clustering algorithms that have theoretical guarantees fall under model-based techniques such as k-mean, k-median or k-medoid (Guha et al., 2003; Ailon et al., 2009; Shindler et al., 2011; Bagirov et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 1, "context": "Most of the existing online clustering algorithms that have theoretical guarantees fall under model-based techniques such as k-mean, k-median or k-medoid (Guha et al., 2003; Ailon et al., 2009; Shindler et al., 2011; Bagirov et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 18, "context": "Most of the existing online clustering algorithms that have theoretical guarantees fall under model-based techniques such as k-mean, k-median or k-medoid (Guha et al., 2003; Ailon et al., 2009; Shindler et al., 2011; Bagirov et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 5, "context": "Most of the existing online clustering algorithms that have theoretical guarantees fall under model-based techniques such as k-mean, k-median or k-medoid (Guha et al., 2003; Ailon et al., 2009; Shindler et al., 2011; Bagirov et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 9, "context": "Popular among them are DBScan (Ester et al., 1996), CluStream (Aggarwal et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 0, "context": ", 1996), CluStream (Aggarwal et al., 2003), and DenStream (Cao et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 6, "context": ", 2003), and DenStream (Cao et al., 2006).", "startOffset": 23, "endOffset": 41}, {"referenceID": 3, "context": "Recent surveys have described several variants of these algorithms (de Andrade Silva et al., 2013; Amini et al., 2014).", "startOffset": 67, "endOffset": 118}, {"referenceID": 7, "context": "Another popular method used in the context of incremental clustering is doubling algorithm (Charikar et al., 1997).", "startOffset": 91, "endOffset": 114}, {"referenceID": 12, "context": "two existing techniques: CURE algorithm (Guha et al., 2001), and core-set (B\u0101doiu et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 4, "context": ", 2001), and core-set (B\u0101doiu et al., 2002).", "startOffset": 22, "endOffset": 43}, {"referenceID": 2, "context": "The existing state-of-the-art algorithms that use the idea of the core-set (Gonzalez, 1985; Alon et al., 2000; Harpeled & Varadarajan, 2001; B\u0101doiu et al., 2002) are computationally too intensive to be useful for online clustering in practice.", "startOffset": 75, "endOffset": 161}, {"referenceID": 4, "context": "The existing state-of-the-art algorithms that use the idea of the core-set (Gonzalez, 1985; Alon et al., 2000; Harpeled & Varadarajan, 2001; B\u0101doiu et al., 2002) are computationally too intensive to be useful for online clustering in practice.", "startOffset": 75, "endOffset": 161}, {"referenceID": 4, "context": "For instance, the algorithm presented in (B\u0101doiu et al., 2002) needs to be rerun 2 log(n) times, where k is the size of the core-set an n is dataset size.", "startOffset": 41, "endOffset": 62}, {"referenceID": 16, "context": "Recently, there has been some work on incremental spectral clustering which essentially iteratively modifies the Graph Laplacian (Ning et al., 2010; Langone et al., 2014; Jia, 2012; Chia et al., 2009).", "startOffset": 129, "endOffset": 200}, {"referenceID": 15, "context": "Recently, there has been some work on incremental spectral clustering which essentially iteratively modifies the Graph Laplacian (Ning et al., 2010; Langone et al., 2014; Jia, 2012; Chia et al., 2009).", "startOffset": 129, "endOffset": 200}, {"referenceID": 14, "context": "Recently, there has been some work on incremental spectral clustering which essentially iteratively modifies the Graph Laplacian (Ning et al., 2010; Langone et al., 2014; Jia, 2012; Chia et al., 2009).", "startOffset": 129, "endOffset": 200}, {"referenceID": 9, "context": ", DBScan (Ester et al., 1996), Leader-Follower algorithm (Duda et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 7, "context": ", 2000), Doubling algorithm (Charikar et al., 1997), and DenStream (Cao et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 6, "context": ", 1997), and DenStream (Cao et al., 2006).", "startOffset": 23, "endOffset": 41}], "year": 2015, "abstractText": "We present a new fast online clustering algorithm that reliably recovers arbitrary-shaped data clusters in high throughout data streams. Unlike the existing state-of-the-art online clustering methods based on k-means or k-medoid, it does not make any restrictive generative assumptions. In addition, in contrast to existing nonparametric clustering techniques such as DBScan or DenStream, it gives provable theoretical guarantees. To achieve fast clustering, we propose to represent each cluster by a skeleton set which is updated continuously as new data is seen. A skeleton set consists of weighted samples from the data where weights encode local densities. The size of each skeleton set is adapted according to the cluster geometry. The proposed technique automatically detects the number of clusters and is robust to outliers. The algorithm works for the infinite data stream where more than one pass over the data is not feasible. We provide theoretical guarantees on the quality of the clustering and also demonstrate its advantage over the existing state-of-the-art on several datasets.", "creator": "LaTeX with hyperref package"}}}