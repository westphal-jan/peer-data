{"id": "1506.04147", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "On the Accuracy of Self-Normalized Log-Linear Models", "abstract": "flattop Calculation of labrum the lilandra log - normalizer yasuhara is a 1,818 major computational 1920/21 obstacle gandhism in rintaro applications of 3,202 log - higuera linear models with large output spaces. The par-5 problem of castra fast normalizer hashemian computation has peire therefore prasasti attracted significant enahoro attention economizer in ra\u00fal the swiftboat theoretical and applied afewerki machine biocultural learning literature. chung In humanitarianism this l\u00f6bau paper, 12-string we analyze kalavryta a recently proposed wecht technique baggara known as \" spider-woman self - prettily normalization \", hambledon which 156-man introduces blacktip a stucky regularization term in 283.1 training to rosenvinge penalize log lattisaw normalizers kucek for anurak deviating gardenias from zero. 1,667 This cynon makes fahr it possible rneth to roadtrips use zeitvogel unnormalized model scores 3.8 as chhote approximate aqualung probabilities. pistorius Empirical evidence chedadi suggests kimhae that self - normalization 170-million is 43.40 extremely connate effective, but stute a si\u00f4n theoretical turkish-german understanding single-step of fuelling why it should levere work, jeru and how paauwe generally it whately can palombo be 74,000 applied, gleneagle is largely lacking. We prove generalization bounds melloy on the take-down estimated variance nowland of potentiates normalizers and upper bounds rodolpho on the loss in accuracy peanuts due sloppiest to self - third-place normalization, lunokhod describe white-washed classes plusses of frayser input semeru distributions that taizu self - normalize easily, and sworn-in construct explicit examples prosperous of yongding high - variance ramalinga input yusaf distributions. valvoline Our theoretical results make bodoland predictions about sibir the anguish difficulty of fitting distributivity self - macdermots normalized fanfares models birckhead to several bignonia classes of loviisa distributions, and we conclude infrequency with 5-98 empirical validation of buzau these predictions.", "histories": [["v1", "Fri, 12 Jun 2015 20:00:29 GMT  (1017kb,D)", "http://arxiv.org/abs/1506.04147v1", null], ["v2", "Thu, 18 Jun 2015 15:22:50 GMT  (1017kb,D)", "http://arxiv.org/abs/1506.04147v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG stat.ME", "authors": ["jacob andreas", "maxim rabinovich", "michael i jordan", "dan klein"], "accepted": true, "id": "1506.04147"}, "pdf": {"name": "1506.04147.pdf", "metadata": {"source": "CRF", "title": "On the accuracy of self-normalized log-linear models", "authors": ["Jacob Andreas", "Maxim Rabinovich", "Dan Klein", "Michael I. Jordan"], "emails": ["jda@cs.berkeley.edu", "rabinovich@cs.berkeley.edu", "klein@cs.berkeley.edu", "jordan@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Log-linear models, a general class that includes conditional random fields (CRFs) and generalized linear models (GLMs), offer a flexible yet tractable approach modeling conditional probability distributions p(x|y) [1, 2]. When the set of possible y values is large, however, the computational cost of computing a normalizing constant for each x can be prohibitive\u2014involving a summation with many terms, a high-dimensional integral or an expensive dynamic program.\nThe machine translation community has recently described several procedures for training \u201cselfnormalized\u201d log-linear models [3, 4]. The goal of self-normalization is to choose model parameters that simultaneously yield accurate predictions and produce normalizers clustered around unity. Model scores can then be used as approximate surrogates for probabilities, obviating the computation normalizer computation.\nIn particular, given a model of the form\np\u03b7(y | x) = e\u03b7 TT (y, x)\u2212A(\u03b7, x) (1)\nwith A (\u03b7, x) = \u2211 y\u2208Y e\u03b7 TT (y, x) , (2)\nwe seek a setting of \u03b7 such that A(x, \u03b7) is close enough to zero (with high probability under p(x)) to be ignored.\n\u2217Authors contributed equally.\nar X\niv :1\n50 6.\n04 14\n7v 1\n[ st\nat .M\nL ]\n1 2\nJu n\n20 15\nThis paper aims to understand the theoretical properties of self-normalization. Empirical results have already demonstrated the efficacy of this approach\u2014for discrete models with many output classes, it appears that normalizer values can be made nearly constant without sacrificing too much predictive accuracy, providing dramatic efficiency increases at minimal performance cost.\nThe broad applicability of self-normalization makes it likely to spread to other large-scale applications of log-linear models, including structured prediction (with combinatorially many output classes) and regression (with continuous output spaces). But it is not obvious that we should expect such approaches to be successful: the number of inputs (if finite) can be on the order of millions, the geometry of the resulting input vectors x highly complex, and the class of functions A(\u03b7, x) associated with different inputs quite rich. To find to find a nontrivial parameter setting with A(\u03b7, x) roughly constant seems challenging enough; to require that the corresponding \u03b7 also lead to good classification results seems too much. And yet for many input distributions that arise in practice, it appears possible to choose \u03b7 to makeA(\u03b7, x) nearly constant without having to sacrifice classification accuracy.\nOur goal is to bridge the gap between theoretical intuition and practical experience. Previous work [5] bounds the sample complexity of self-normalizing training procedures for a restricted class of models, but leaves open the question of how self-normalization interacts with the predictive power of the learned model. This paper seeks to answer that question. We begin by generalizing the previously-studied model to a much more general class of distributions, including distributions with continuous support (Section 3). Next, we provide what we believe to be the first characterization of the interaction between self-normalization and model accuracy Section 4. This characterization is given from two perspectives:\n\u2022 a bound on the \u201clikelihood gap\u201d between self-normalized and unconstrained models \u2022 a conditional distribution provably hard to represent with a self-normalized model\nIn Figure 5, we present empirical evidence that these bounds correctly characterize the difficulty of self-normalization, and in the conclusion we survey a set of open problems that we believe merit further investigation."}, {"heading": "2 Problem background", "text": "The immediate motivation for this work is a procedure proposed to speed up decoding in a machine translation system with a neural-network language model [3]. The language model used is a standard feed-forward neural network, with a \u201csoftmax\u201d output layer that turns the network\u2019s predictions into a distribution over the vocabulary, where each probability is log-proportional to its output activation. It is observed that with a sufficiently large vocabulary, it becomes prohibitive to obtain probabilities from this model (which must be queried millions of times during decoding). To fix this, the language model is trained with the following objective:\nmax W \u2211 i [ N(yi|xi;W )\u2212 log \u2211 y\u2032 eN(y \u2032|xi;W ) \u2212 \u03b1 ( log \u2211 y\u2032 eN(y \u2032|xi;W ) )2] whereN(y|x;W ) is the response of output y in the neural net with weightsW given an input x. From a Lagrangian perspective, the extra penalty term simply confines the W to the set of \u201cempirically normalizing\u201d parameters, for which all log-normalizers are close (in squared error) to the origin. For a suitable choice of \u03b1, it is observed that the trained network is simultaneously accurate enough to produce good translations, and close enough to self-normalized that the raw scores N(yi|xi) can be used in place of log-probabilities without substantial further degradation in quality.\nWe seek to understand the observed success of these models in finding accurate, normalizing parameter settings. While it is possible to derive bounds of the kind we are interested in for general neural networks [6], in this paper we work with a simpler linear parameterization that we believe captures the interesting aspects of this problem. 1\n1It is possible to view a log-linear model as a single-layer network with a softmax output. More usefully, all of the results presented here apply directly to trained neural nets in which the last layer only is retrained to self-normalize [7]."}, {"heading": "Related work", "text": "The approach described at the beginning of this section is closely related to an alternative selfnormalization trick described based on noise-contrastive estimation (NCE) [8]. NCE is an alternative to direct optimization of likelihood, instead training a classifier to distinguish between true samples from the model, and \u201cnoise\u201d samples from some other distribution. The structure of the training objective makes it possible to replace explicit computation of each log-normalizer with an estimate. In traditional NCE, these values are treated as part of the parameter space, and estimated simultaneously with the model parameters; there exist guarantees that the normalizer estimates will eventually converge to their true values. It is instead possible to fix all of these estimates to one. In this case, empirical evidence suggests that the resulting model will also exhibit self-normalizing behavior [4].\nA host of other techniques exist for solving the computational problem posed by the log-normalizer. Many of these involve approximating the associated sum or integral using quadrature [9], herding [10], or Monte Carlo methods [11]. For the special case of discrete, finite output spaces, an alternative approach\u2014the hierarchical softmax\u2014is to replace the large sum in the normalizer with a series of binary decisions [12]. The output classes are arranged in a binary tree, and the probability of generating a particular output is the product of probabilities along the edges leading to it. This reduces the cost of computing the normalizer fromO(k) toO(log k). While this limits the set of distributions that can be learned, and still requires greater-than-constant time to compute normalizers, it appears to work well in practice. It cannot, however, be applied to problems with continuous output spaces."}, {"heading": "3 Self-normalizable distributions", "text": "We begin by providing a slightly more formal characterization of a general log-linear model:\nDefinition 1 (Log-linear models). Given a space of inputs X , a space of outputs Y , a measure \u00b5 on Y , a nonnegative function h : Y \u2192 R, and a function T : X \u00d7 Y \u2192 Rd that is \u00b5-measurable with respect to its second argument, we can define a log-linear model indexed by parameters \u03b7 \u2208 Rd, with the form\np\u03b7(y|x) = h(y)e\u03b7 >T (x,y)\u2212A(x,\u03b7) , (3)\nwhere\nA(x, \u03b7) \u2206 = log \u222b Y h(y)e\u03b7 >T (x,y) d\u00b5(y) . (4)\nIf A(x, \u03b7) \u2264 \u221e, then \u222b y p\u03b7(y|x) d\u00b5(y) = 1, and p\u03b7(y|x) is a probability density over Y .2\nWe next formalize our notion of a self-normalized model.\nDefinition 2 (Self-normalized models). The log-linear model p\u03b7(y|x) is self-normalized with respect to a set S \u2282 X if for all x \u2208 S, A(x, \u03b7) = 0. In this case we say that S is self-normalizable, and \u03b7 is self-normalizing w.r.t. S.\nAn example of a normalizable set is shown in Figure 1a, and we provide additional examples below:\n2Some readers may be more familiar with generalized linear models, which also describe exponential family distributions with a linear dependence on input. The presentation here is strictly more general, and has a few notational advantages: it makes explicit the dependence of A on x and \u03b7 but not y, and lets us avoid tedious bookkeeping involving natural and mean parameterizations. [13]\nExample. Suppose S = {log 2,\u2212 log 2} , Y = {\u22121, 1}\nT (x, y) = [xy, 1]\n\u03b7 = (1, log(2/5)) ."}, {"heading": "Then for either x \u2208 S,", "text": "A(x, \u03b7) = log(elog 2+log(2/5) + e\u2212 log 2+log(2/5))\n= log((2/5)(2 + 1/2))\n= 0 ,\nand \u03b7 is self-normalizing with respect to S.\nIt is also easy to choose parameters that do not result in a self-normalized distribution, and in fact to construct a target distribution which cannot be self-normalized:\nExample. Suppose\nX = {(1, 0), (0, 1), (1, 1)} Y = {\u22121, 1}\nT (x, y) = (x1y, x2y, 1)\nThen there is no \u03b7 such that A(x, \u03b7) = 0 for all x, and A(x, \u03b7) is constant if and only if \u03b7 = 0.\nAs previously motivated, downstream uses of these models may be robust to small errors resulting from improper normalization, so it would be useful to generalize this definition of normalizable distributions to distributions that are only approximately normalizable. Exact normalizability of the conditional distribution is a deterministic statement\u2014there either does or does not exist some x that violates the constraint. In Figure 1a, for example, it suffices to have a single x off of the indicated surface to make a set non-normalizable. Approximate normalizability, by contrast, is inherently a probabilistic statement, involving a distribution p(x) over inputs. Note carefully that\nwe are attempting to represent p(y|x) but have no representation of (or control over) p(x), and that approximate normalizability depends on p(x) but not p(y|x). Informally, if some input violates the self-normalization constraint by a large margin, but occurs only very infrequently, there is no problem; instead we are concerned with expected deviation. It is also at this stage that the distinction between penalization of the normalizer vs. log-normalizer becomes important. The normalizer is necessarily bounded below by zero (so overestimates might appear much worse than underestimates), while the log-normalizer is unbounded in both directions. For most applications we are concerned with log probabilities and log-odds ratios, for which an expected normalizer close to zero is just as bad as one close to infinity. Thus the log-normalizer is the natural choice of quantity to penalize. Definition 3 (Approximately self-normalized models). The log-linear distribution p\u03b7(y|x) is \u03b4approximately normalized with respect to a distribution p(x) over X if E[A(X, \u03b7)2] < \u03b42. In this case we say that p(x) is \u03b4-approximately self-normalizable, and \u03b7 is \u03b4-approximately self-normalizing.\nThe sets of \u03b4-approximately self-normalizing parameters for a fixed input distribution and feature function are depicted in Figure 1b. Unlike self-normalizable sets of inputs, self-normalizing and approximately self-normalizing sets of parameters may have complex geometry.\nThroughout this paper, we will assume that vectors of sufficient statistics T (x, y) have bounded `2 norm at most R, natural parameter vectors \u03b7 have `2 norm at most B (that is, they are Ivanovregularized), and that vectors of both kinds lie in Rd. Finally, we assume that all input vectors have a constant feature\u2014in particular, that x0 = 1 for every x (with corresponding weight \u03b70). 3\nThe first question we must answer is whether the problem of training self-normalized models is feasible at all\u2014that is, whether there exist any exactly self-normalizable data distributions p(x), or at least \u03b4-approximately self-normalizable distributions for small \u03b4. Section 3 already gave an example of an exactly normalizable distribution. In fact, there are large classes of both exactly and approximately normalizable distributions. Observation. Given some fixed \u03b7, consider the set S\u03b7 = {x \u2208 X : A(x, \u03b7) = 0}. Any distribution p(x) supported on S\u03b7 is normalizable. Additionally, every self-normalizable distribution is characterized by at least one such \u03b7.\nThis definition provides a simple geometric characterization of self-normalizable distributions. An example solution set is shown in Figure 1a. More generally, if y is discrete and T (x, y) consists of |Y| repetitions of a fixed feature function t(x) (as in Figure 1a), then we can write\nA(x, \u03b7) = log \u2211 y\u2208Y e\u03b7 > y t(x). (5)\nProvided \u03b7>y t(x) is convex in x for each \u03b7y , the level sets of A as a function of x form the boundaries of convex sets. In particular, exactly normalizable sets are always the boundaries of convex regions, as in the simple example Figure 1a.\nWe do not, in general, expect real-world datasets to be supported on the precise class of selfnormalizable surfaces. Nevertheless, it is very often observed that data of practical interest lie on other low-dimensional manifolds within their embedding feature spaces. Thus we can ask whether it is sufficient for a target distribution to be well-approximated by a self-normalizing one. We begin by constructing an appropriate measurement of the quality of this approximation. Definition 4 (Closeness). An input distribution p(x) is D-close to a set S if\nE [ inf x\u2217\u2208S sup y\u2208Y ||T (X, y)\u2212 T (x\u2217, y)||2 ] \u2264 D (6)\nIn other words, p(x) is D-close to S if a random sample from p is no more than a distance D from S in expectation. Now we can relate the quality of this approximation to the level of self-normalization achieved. Generalizing a result from [5], we have:\n3It will occasionally be instructive to consider the special case where X is the Boolean hypercube, and we will explicitly note where this assumption is made. Otherwise all results apply to general distributions, both continuous and discrete.\nProposition 1. Suppose p(x) is D-close to {x : A(x, \u03b7) = 1}. Then p(x) is BD-approximately self-normalizable (recalling that ||x||2 \u2264 B).\n(Proofs for this section may be found in Appendix A.)\nThe intuition here is that data distributions that place most of their mass in feature space close to normalizable sets are approximately normalizable on the same scale."}, {"heading": "4 Normalization and model accuracy", "text": "So far our discussion has concerned the problem of finding conditional distributions that selfnormalize, without any concern for how well they actually perform at modeling the data. Here the relationship between the approximately self-normalized distribution and the true distribution p(y|x) (which we have so far ignored) is essential. Indeed, if we are not concerned with making a good model it is always trivial to make a normalized one\u2014simply take \u03b7 = 0 and then scale \u03b70 appropriately! We ultimately desire both good self-normalization and good data likelihood, and in this section we characterize the tradeoff between maximizing data likelihood and satisfying a self-normalization constraint.\nWe achieve this characterization by measuring the likelihood gap between the classical maximum likelihood estimator, and the MLE subject to a self-normalization constraint. Specifically, given pairs ((x1, y1), (x2, y2), . . . , (xn, yn)), let `(\u03b7|x, y) = \u2211 i log p\u03b7(yi|xi). Then define\n\u03b7\u0302 = arg max \u03b7\n`(\u03b7|x, y) (7)\n\u03b7\u0302\u03b4 = arg max \u03b7:V (\u03b7)\u2264\u03b4\n`(\u03b7|x, y) (8)\n(where V (\u03b7) = 1n \u2211 iA(xi, \u03b7) 2).\nWe would like to obtain a bound on the likelihood gap, which we define as the quantity\n\u2206`(\u03b7\u0302, \u03b7\u0302\u03b4) = 1\nn (`(\u03b7\u0302|x, y)\u2212 `(\u03b7\u0302\u03b4|x, y)) . (9)\nWe claim: Theorem 2. Suppose Y has finite measure. Then asymptotically as n\u2192\u221e\n\u2206`(\u03b7\u0302, \u03b7\u0302\u03b4) \u2264 (\n1\u2212 \u03b4 R||\u03b7\u0302||2\n) EKL(p\u03b7(\u00b7|X) || Unif) . (10)\n(Proofs for this section may be found in Appendix B.)\nThis result lower-bounds the likelihood at \u03b7\u0302\u03b4 by explicitly constructing a scaled version of \u03b7\u0302 that satisfies the self-normalization constraint. Specifically, if \u03b7 is chosen so that normalizers are penalized for distance from log\u00b5(Y) (e.g. the logarithm of the number of classes in the finite case), then any increase in \u03b7 along the span of the data is guaranteed to increase the penalty. From here it is possible to choose an \u03b1 \u2208 (0, 1) such that \u03b1\u03b7\u0302 satisfies the constraint. The likelihood at \u03b1\u03b7\u0302 is necessarily less than `(\u03b7\u0302\u03b4|x, y), and can be used to obtain the desired lower bound. Thus at one extreme, distributions close to uniform can be self-normalized with little loss of likelihood. What about the other extreme\u2014distributions \u201cas far from uniform as possible\u201d? With suitable assumptions about the form of p\u03b7\u0302(y|x), we can use the same construction of a self-normalizing parameter to achieve an alternative characterization for distributions that are close to deterministic: Proposition 3. Suppose that X is a subset of the Boolean hypercube, Y is finite, and T (x, y) is the conjunction of each element of x with an indicator on the output class. Suppose additionally that in every input x, p\u03b7\u0302(y|x) makes a unique best prediction\u2014that is, for each x \u2208 X , there exists a unique y\u2217 \u2208 Y such that whenever y 6= y\u2217, \u03b7>T (x, y\u2217) > \u03b7>T (x, y). Then\n\u2206`(\u03b7\u0302, \u03b7\u0302\u03b4) \u2264 b ( ||\u03b7||2 \u2212 \u03b4\nR\n)2 e\u2212c\u03b4/R (11)\nfor distribution-dependent constants b and c.\nThis result is obtained by representing the constrained likelihood with a second-order Taylor expansion about the true MLE. All terms in the likelihood gap vanish except for the remainder; this can be upper-bounded by the ||\u03b7\u0302\u03b4||22 times the largest eigenvalue the feature covariance matrix at \u03b7\u0302\u03b4 , which in turn is bounded by e\u2212c\u03b4/R.\nThe favorable rate we obtain for this case indicates that \u201call-nonuniform\u201d distributions are also an easy class for self-normalization. Together with Theorem 2, this suggests that hard distributions must have some mixture of uniform and nonuniform predictions for different inputs. This is supported by the results in Section 4.\nThe next question is whether there is a corresponding lower bound; that is, whether there exist any conditional distributions for which all nearby distributions are provably hard to self-normalize. The existence of a direct analog of Theorem 2 remains an open problem, but we make progress by developing a general framework for analyzing normalizer variance.\nOne key issue is that while likelihoods are invariant to certain changes in the natural parameters, the log normalizers (and therefore their variance) is far from invariant. We therefore focus on equivalence classes of natural parameters, as defined below. Throughout, we will assume a fixed distribution p(x) on the inputs x. Definition 5 (Equivalence of parameterizations). Two natural parameter values \u03b7 and \u03b7\u2032 are said to be equivalent (with respect to an input distribution p(x)), denoted \u03b7 \u223c \u03b7\u2032 if\np\u03b7(y|X) = p\u03b7\u2032(y|X) a.s. p(x)\nWe can then define the optimal log normalizer variance for the distribution associated with a natural parameter value. Definition 6 (Optimal variance). We define the optimal log normalizer variance of the log-linear model associated with a natural parameter value \u03b7 by\nV \u2217(\u03b7) = inf \u03b7\u2032\u223c\u03b7 Varp(x) [A(X, \u03b7)] .\nWe now specialize to the case where Y is finite with |Y| = K and where T : Y \u00d7X \u2192 RKd satisfies T (k, x)k\u2032j = \u03b4kk\u2032xj .\nThis is an important special case that arises, for example, in multi-way logistic regression. In this setting, we can show that despite the fundamental non-identifiability of the model, the variance can still be shown to be high under any parameterization of the distribution. Theorem 4. Let X = {0, 1}d and let the input distribution p(x) be uniform on X . There exists an \u03b70 \u2208 RKd such that for \u03b7 = \u03b1\u03b70, \u03b1 > 0,\nV \u2217(\u03b7) \u2265 ||\u03b7||22\n32d(d\u2212 1) \u2212 4Ke\u2212\n\u221a 1\u2212 1\nd ||\u03b7||2\n2(d\u22121) ||\u03b7||2 ."}, {"heading": "5 Experiments", "text": "The high-level intuition behind the results in the preceding section can be summarized as follows: 1) for predictive distributions that are in expectation high-entropy or low-entropy, self-normalization results in a relatively small likelihood gap; 2) for mixtures of high- and low-entropy distributions, self-normalization may result in a large likelihood gap. More generally, we expect that an increased tolerance for normalizer variance will be associated with a decreased likelihood gap.\nIn this section we provide experimental confirmation of these predictions. We begin by generating a set of random sparse feature vectors, and an initial weight vector \u03b70. In order to produce a sequence of label distributions that smoothly interpolate between low-entropy and high-entropy, we introduce a temperature parameter \u03c4 , and for various settings of \u03c4 draw labels from p\u03c4\u03b7. We then fit a selfnormalized model to these training pairs. In addition to the synthetic data, we compare our results to empirical data [3] from a self-normalized language model.\nFigure 2a plots the tradeoff between the likelihood gap and the error in the normalizer, under various distributions (characterized by their KL from uniform). Here the tradeoff between self-normalization\n\u2206`\n0 \u00a0\n0.05 \u00a0\n0.1 \u00a0\n0.15 \u00a0\n0.2 \u00a0\n0 \u00a0 0.5 \u00a0 1 \u00a0 1.5 \u00a0\nKL=2.6 \u00a0\nKL=5.0 \u00a0\nLM \u00a0\n\u03b4\n(a) Normalization / likelihood tradeoff. As the normalization constraint \u03b4 is relaxed, the likelihood gap \u2206` decreases. Lines marked \u201cKL=\u201d are from synthetic data; the line marked \u201cLM\u201d is from [3].\n\u2206`\nEKL(p\u03b7||Unif) (b) Likelihood gap as a function of expected divergence from the uniform distribution. As predicted by theory, the likelihood gap increases, then decreases, as predictive distributions become more peaked.\nFigure 2: Experimental results\nand model accuracy can be seen\u2014as the normalization constraint is relaxed, the likelihood gap decreases.\nFigure 2b shows how the likelihood gap varies as a function of the quantity EKL(p\u03b7(\u00b7|X)||Unif). As predicted, it can be seen that both extremes of this quantity result in small likelihood gaps, while intermediate values result in large likelihood gaps."}, {"heading": "6 Conclusions", "text": "Motivated by the empirical success of self-normalizing parameter estimation procedures for log-linear models, we have attempted to establish a theoretical basis for the understanding of such procedures. We have characterized both self-normalizable distributions, by constructing provably easy examples, and self-normalizing training procedures, by bounding the loss of likelihood associated with selfnormalization.\nWhile we have addressed many of the important first-line theoretical questions around selfnormalization, this study of the problem is by no means complete. We hope this family of problems will attract further study in the larger machine learning community; toward that end, we provide the following list of open questions:\n1. How else can the approximately self-normalizable distributions be characterized? The class of approximately normalizable distributions we have described is unlikely to correspond perfectly to real-world data. We expect that Proposition 1 can be generalized to other parametric classes, and relaxed to accommodate spectral or sparsity conditions.\n2. Are the upper bounds in Theorem 2 or Proposition 3 tight? Our constructions involve relating the normalization constraint to the `2 norm of \u03b7, but in general some parameters can have very large norm and still give rise to almost-normalized distributions.\n3. Do corresponding lower bounds exist? While it is easy to construct of exactly selfnormalizable distributions (which suffer no loss of likelihood), we have empirical evidence that hard distributions also exist. It would be useful to lower-bound the loss of likelihood in terms of some simple property of the target distribution.\n4. Is the hard distribution in Theorem 4 stable? This is related to the previous question. The existence of high-variance distributions is less worrisome if such distributions are comparatively rare. If the variance lower bound falls off quickly as the given construction is perturbed, then the associated distribution may still be approximately self-normalizable with a good rate.\nWe have already seen that new theoretical insights in this domain can translate directly into practical applications. Thus, in addition to their inherent theoretical interest, answers to each of these questions\nmight be applied directly to the training of approximately self-normalized models in practice. We expect that self-normalization will find increasingly many applications, and we hope the results in this paper provide a first step toward a complete theoretical and empirical understanding of self-normalization in log-linear models."}, {"heading": "A Normalizable distributions", "text": "Proof of Proposition 1 (distributions close to normalizable sets are approximately normalizable).\nLet T (x, y) = T \u2217(x, y) + T\u2212(x, y), where T \u2217(x, y) = arg min T (x,y):x\u2208S ||T (X, y)\u2212 T (x, y)||2 .\nThen,\nE ( log (\u222b e\u03b7 >T (X,y) dy ))2 = E ( log (\u222b e\u03b7 >(T\u2217(X,y)+T\u2212(X,y)) dy ))2 \u2264 E ( log ( e\u03b7 >T\u0303 \u222b e\u03b7 >T\u2217(X,y) dy\n))2 for T\u0303 = arg max\nT (X,y)\n||\u03b7>T (X, y)||2,\n\u2264 E ( log ( e\u03b7 >T\u0303 ))2\n= (DB)2"}, {"heading": "B Normalization and likelihood", "text": ""}, {"heading": "B.1 General bound", "text": "Lemma 5. If ||\u03b7||2 \u2264 \u03b4/R, then p\u03b7(y|x) is \u03b4-approximately normalized about log\u00b5(Y). Proof. If \u222b e\u03b7 >T (X,y) d\u00b5(y) \u2265 log\u00b5(Y),(\nlog \u222b Y e\u03b7 >T (X,y) d\u00b5(y)\u2212 log\u00b5(Y) )2 \u2264 ( log \u222b Y e||\u03b7||2R d\u00b5(y)\u2212 log\u00b5(Y) )2 = ||\u03b7||22R2\n\u2264 \u03b42\nThe case where \u222b e\u03b7 >T (X,y) d\u00b5(y) \u2264 log\u00b5(Y) is analogous, instead replacing \u03b7>T (x, y) with \u2212||\u03b7||2R. The variance result follows from the fact that every log-partition is within \u03b4 of the mean.\nProof of Theorem 2 (loss of likelihood is bounded in terms of distance from uniform). Consider the likelihood evaluated at \u03b1\u03b7\u0302, where \u03b1 = \u03b4/R||\u03b7\u0302||2. We know that 0 \u2264 \u03b1 \u2264 1 (if \u03b4 > R\u03b7, then the MLE already satisfying the normalizing constraint). Additionally, p\u03b1\u03b7\u0302(y|x) is \u03b4-approximately normalized. (Both follow from Lemma 5.)\nThen,\n\u2206` = 1\nn \u2211 i [ (\u03b7\u0302>T (xi, yi)\u2212A(xi, \u03b7\u0302))\u2212 (\u03b1\u03b7\u0302>T (xi, yi)\u2212A(xi, \u03b1\u03b7\u0302)) ] = 1\nn \u2211 i [ (1\u2212 \u03b1)\u03b7\u0302>T (xi, yi)\u2212A(xi, \u03b7\u0302) +A(xi, \u03b1\u03b7\u0302) ] Because A(x, \u03b1\u03b7) is convex in \u03b1,\nA(xi, \u03b1\u03b7\u0302) \u2264 (1\u2212 \u03b1)A(xi,0) + \u03b1A(xi, \u03b7\u0302) = (1\u2212 \u03b1)\u00b5(Y) + \u03b1A(xi, \u03b7\u0302)\nThus,\n\u2206` = 1\nn \u2211 i [ (1\u2212 \u03b1)\u03b7\u0302>T (xi, yi)\u2212A(xi, \u03b7\u0302) + (1\u2212 \u03b1) log\u00b5(Y) + \u03b1A(xi, \u03b7\u0302) ] = (1\u2212 \u03b1) 1\nn \u2211 i [ \u03b7\u0302>T (xi, yi)\u2212A(xi, \u03b7\u0302) + log \u00b5(Y) ] = (1\u2212 \u03b1) 1\nn \u2211 i [log p\u03b7(y|x)\u2212 log Unif(y)]\n(1\u2212 \u03b1) EKL(p\u03b7(\u00b7|X) || Unif) \u2264 (\n1\u2212 \u03b4 R||\u03b7\u0302||2\n) EKL(p\u03b7(\u00b7|X) || Unif)"}, {"heading": "B.2 All-nonuniform bound", "text": "We make the following assumptions:\n\u2022 Labels y are discrete. That is, Y = {1, 2, . . . , k} for some k. \u2022 x \u2208 H(d). That is, each x is a {0, 1} indicator vector drawn from the Boolean hypercube in q dimensions.\n\u2022 Joint feature vectors T (x, y) are just the features of x conjoined with the label y. Then it is possible to think of \u03b7 as a sequence of vectors, one per class, and we can write \u03b7>T (x, y) = \u03b7>y x.\n\u2022 As in the body text, let all MLE predictions be nonuniform, and in particular let each \u03b7\u0302>y\u2217x\u2212 \u03b7\u0302>y x > c||\u03b7\u0302|| for y 6= y\u2217.\nLemma 6. For a fixed x, the maximum covariance between any two features xi and xj under the model evaluated at some \u03b7 in the direction of the MLE:\nCov[T (X,Y )i, T (X,Y )j |X = x] \u2264 2(k \u2212 1)e\u2212c\u03b4 (12)\nProof. If either i or j is not associated with the class y, or associated with a zero element of x, then the associated feature (and thus the covariance at (i, j)) is identically zero. Thus we assume that i and j are both associated with y and correspond to nonzero elements of x.\nCov[Ti, Tj |X = x] = \u2211 y p\u03b7(y|x)\u2212 p\u03b7(y|x)2\nSuppose y is the majority class. Then,\np\u03b7(y|x)\u2212 p\u03b7(y|x)2 = e\u03b7 > y x\u2211\ny\u2032 e \u03b7> y\u2032x \u2212 e 2\u03b7>y x(\u2211 y\u2032 e \u03b7> y\u2032x )2\n= e\u03b7 > y x (\u2211 y\u2032 e \u03b7> y\u2032x ) \u2212 e2\u03b7 > y x(\u2211\ny\u2032 e \u03b7> y\u2032x )2\n\u2264 e\u03b7 > y x (\u2211 y\u2032 e \u03b7> y\u2032x ) \u2212 e2\u03b7 > y x\ne2\u03b7 > y x = \u2211 y\u2032 6=y e(\u03b7 \u2032 y\u2212\u03b7y) >x\n\u2264 (k \u2212 1)e\u2212c||\u03b7||\nNow suppose y is not in the majority class. Then,\np\u03b7(y|x)\u2212 p\u03b7(y|x)2 \u2264 p(y|x)\n= e\u03b7 > y x\u2211\ny\u2032 e \u03b7> y\u2032x\n\u2264 e\u2212c||\u03b7||\nThus the covariance \u2211 y p\u03b7(y|x)\u2212 p\u03b7(y|x)2 \u2264 2(k \u2212 1)e\u2212c||\u03b7|||\nLemma 7. Suppose \u03b7 = \u03b2\u03b7\u0302 for some \u03b2 < 1. Then for a sequence of observations (x1, . . . , xn), under the model evaluated at \u03be, the largest eigenvalue of the feature covariance matrix\n1\nn \u2211 i [ E\u03be[TT >|X = xi]\u2212 (E\u03b8[T |X = xi])(E\u03be[T |X = xi])> ]\n(13)\nis at most q(k \u2212 1)e\u2212c\u03b2||\u03b7\u0302|| (14)\nProof. From Lemma 6, each entry in the covariance matrix is at most (k \u2212 1)e\u2212c||\u03b7|| = (k \u2212 1)e\u2212c\u03b2||\u03b7\u0302||. At most q features are nonzero active in any row of the matrix. Thus by Gershgorin\u2019s theorem, the maximum eigenvalue of each term in Equation 13 is q(k \u2212 1)e\u2212c\u03b2||\u03b7\u0302||, which is also an upper bound on the sum.\nProof of Proposition 3 (loss of likelihood goes as e\u2212\u03b4). As before, let us choose \u03b7\u0302\u03b4 = \u03b1\u03b7\u0302, with \u03b1 = \u03b4/R||\u03b7\u0302||2. We have already seen that this choice of parameter is normalizing. Taking a second-order Taylor expansion about \u03b7, we have\nlog p\u03b7\u0302\u03b4(y|x) = log p\u03b7(y|x) + (\u03b7\u0302\u03b4 \u2212 \u03b7\u0302)>\u2207 log p\u03b7\u0302(y|x) + (\u03b7\u0302\u03b4 \u2212 \u03b7\u0302)>\u2207\u2207> log p\u03be(y|x)(\u03b7\u0302\u03b4 \u2212 \u03b7\u0302) = log p\u03b7\u0302(y|x) + (\u03b7\u0302\u03b4 \u2212 \u03b7\u0302)>\u2207\u2207> log p\u03be(y|x)(\u03b7\u0302\u03b4 \u2212 \u03b7\u0302)\nwhere the first-order term vanishes because \u03b7\u0302 is the MLE. It is a standard result for exponential families that the Hessian in the second-order term is just Equation 13. Thus we can write\n\u2265 log p\u03b7\u0302(y|x)\u2212 ||\u03b7\u0302\u03b4 \u2212 \u03b7\u0302||2q(k \u2212 1)e\u2212c\u03b2||\u03b7||\n\u2265 log p\u03b7\u0302(y|x)\u2212 (1\u2212 \u03b1)2||\u03b7\u0302||2q(k \u2212 1)e\u2212c\u03b1||\u03b7||\n= log p\u03b7\u0302(y|x)\u2212 (||\u03b7\u0302|| \u2212 \u03b4/R)2q(k \u2212 1)e\u2212c\u03b4/R\nThe proposition follows.\nC Variance lower bound\nLet U0 = {\u03b2 \u2208 RKd : \u2203\u03b2\u0303 \u2208 Rd, \u03b2kj = \u03b2\u0303j , 1 \u2264 k \u2264 K, 1 \u2264 j \u2264 d}.\nLemma 8. If span (X ) = Rd, then equivalence of natural parameters is characterized by\n\u03b7 \u223c \u03b7\u2032 \u21d0\u21d2 \u03b7 \u2212 \u03b7\u2032 \u2208 U0.\nProof. For x \u2208 X , denote by P\u03b7(x) \u2208 \u2206K the distribution over Y . Now, suppose that \u03b7 \u223c \u03b7\u2032 and fix x \u2208 X . By the definition of equivalence, we have\nP\u03b7(x)k P\u03b7(x)k\u2032 = P\u03b7\u2032(x)k P\u03b7\u2032(x)k\u2032 ,\nwhich immediately implies (\u03b7k \u2212 \u03b7k\u2032)T x = (\u03b7\u2032k \u2212 \u03b7\u2032k\u2032) T x, whence [(\u03b7k \u2212 \u03b7\u2032k)\u2212 (\u03b7k\u2032 \u2212 \u03b7\u2032k\u2032)] T x = 0. Since this holds for all x \u2208 X and span(X ) = Rd, we get \u03b7k \u2212 \u03b7\u2032k = \u03b7k\u2032 \u2212 \u03b7\u2032k\u2032 .\nThat is, if we define \u03b2\u0303j = \u03b71j \u2212 \u03b7\u20321j , we get \u03b7kj \u2212 \u03b7\u2032kj = \u03b71d \u2212 \u03b7\u20321j = \u03b2\u0303j , and \u03b7 \u2212 \u03b7\u2032 \u2208 U0, as required.\nConversely, if \u03b7 \u2212 \u03b7\u2032 \u2208 U0, choose an appropriate \u03b2\u0303. We then get\n\u03b7Tk x = (\u03b7 \u2032) T x+ \u03b2\u0303Tx.\nIt follows that A(\u03b7\u2032, x) = A(\u03b7, x) + \u03b2\u0303Tx,\nso that \u03b7TT (k, x)\u2212A(\u03b7, x) = (\u03b7\u2032)T x+ \u03b2\u0303Tx\u2212 [ A(\u03b7\u2032, x) + \u03b2\u0303Tx ] = (\u03b7\u2032) T x\u2212A(\u03b7\u2032, x)\nand the claim follows.\nThe key tool we use to prove the theorem reinterprets V \u2217(\u03b7) as the norm of an orthogonal projection. We believe this may be of independent interest. To set it up, let S = L2 ( Q, RD ) be the Hilbert\nspace of square-integrable functions with respect to the input distribution p(x), define\nwj(x) = xj \u2212 Ep(x) [Xj ] and C = span (wj)1\u2264j\u2264d . We then have Lemma 9. Let A\u0303(\u03b7, x) = A(\u03b7, x)\u2212 Ep(x) [A(\u03b7, X)]. Then\nV \u2217(\u03b7) = \u2223\u2223\u2223\u2223\u2223\u2223A\u0303(\u03b7, \u00b7)\u2212\u03a0CA\u0303(\u03b7, \u00b7)\u2223\u2223\u2223\u2223\u2223\u22232\n2 .\nThe second key observation, which we again believe is of independent interest, is that under certain circumstances, we can completely replace the normalizer A(\u03b7, \u00b7) by maxy\u2208Y \u03b7TT (y, x). For this, we define\nE\u221e(\u03b7)(x) = max k \u03b7TT (k, x) = max k \u03b7Tk x\nand correspondingly let E\u0304\u221e(\u03b7) = Ep(x) [E\u221e(\u03b7)(x)].\nProof. By Lemma 8, we have\nV \u2217(\u03b7) = inf \u03b2\u2208Rd \u222b RKd [ A(\u03b7, x)\u2212 A\u0304(\u03b7)\u2212 ( \u03b2Tx\u2212 \u03b2TEp(x) [X] )]2 dp(x).\nBut now, we observe that this can be rewritten with the aid of the isomorphism Rd ' C defined by the identity\n\u03b2Tx\u2212 \u03b2TEp(x) [X] = \u2211 j \u03b2jwj(x)\nto read\nV \u2217(\u03b7) = inf f\u2208C \u222b Rd [ A(\u03b7, x)\u2212 A\u0304(\u03b7)\u2212 f ]2 dp(x) = \u2223\u2223\u2223\u2223\u2223\u2223A\u0303(\u03b7, \u00b7)\u2212\u03a0CA\u0303(\u03b7, \u00b7)\u2223\u2223\u2223\u2223\u2223\u22232 2 ,\nas required.\nLemma 10. Suppose for each x \u2208 X , there is a unique k\u2217 = k\u2217(x) such that k\u2217(x) = arg maxk \u03b7Tk x and such that for k 6= k\u2217, \u03b7Tk x \u2264 \u03b7Tk\u2217x\u2212\u2206 for some \u2206 > 0. Then\nsup x\u2208X \u2223\u2223A(\u03b7, x)\u2212 A\u0304(\u03b7)\u2212 [E\u221e(\u03b7)(x)\u2212 E\u0304\u221e(\u03b7)]\u2223\u2223 \u2264 Ke\u2212\u2206\u03b1. Proof. Denote by E\u0303\u221e the centered version of E\u221e. Using the identity 1 + t \u2264 et, we immediately see that\nE\u221e(\u03b1\u03b7)(x) \u2264 A(\u03b1\u03b7, x) = \u03b1E\u221e(\u03b7)(x)+log 1 + \u2211 k 6=k\u2217(x) e[\u03b7 T k x\u2212E\u221e(\u03b7)(x)]  \u2264 E\u221e(\u03b1\u03b7)(x)+Ke\u2212\u2206\u03b1. It follows that\nEp(x) [E\u221e(\u03b1\u03b7)(X)] \u2264 Ep(x) [A(\u03b1\u03b7, X)] \u2264 Ep(x) [E\u221e(\u03b1\u03b7)(X)] +Ke\u2212\u2206\u03b1.\nWe thus have \u2212Ke\u2212\u2206\u03b1 \u2264 A\u0303(\u03b1\u03b7, x)\u2212 E\u0303\u221e(\u03b1\u03b7)(x) \u2264 Ke\u2212\u2206\u03b1, x \u2208 X .\nThe claim follows.\nIf we let V \u2217E (\u03b7) = inf\n\u03b7\u2032\u223c\u03b7 Varp(x)\n[ E\u0303\u221e(\u03b7 \u2032, X) ] .\nCorollary 11. For \u03b1 > log 2K\u2206 , we have\nV \u2217(\u03b1\u03b7) \u2265 V \u2217E (\u03b7)\u03b12 \u2212 (1 + V \u2217E (\u03b7))\u03b1.\nProof. For this, observe first that if \u03b7\u2032 \u223c \u03b7, then A\u0303(\u03b7\u2032, x)2 \u2265 E\u0303\u221e(\u03b1\u03b7\u2032)(x)2 \u2212 2 \u2223\u2223\u2223E\u0303\u221e(\u03b1\u03b7\u2032)(x)\u2223\u2223\u2223 \u2223\u2223\u2223A\u0303(\u03b7\u2032, x)\u2212 E\u0303\u221e(\u03b7\u2032)(x)\u2223\u2223\u2223 .\nBy linearity of E\u221e(\u03b7\u2032) in its \u03b7 argument, and by Lemma 10, we therefore deduce A\u0303(\u03b7\u2032, x)2 \u2265 E\u0303\u221e(\u03b7\u2032)(x)2\u03b12 \u2212 2Ke\u2212\u2206\u03b1 \u2223\u2223\u2223E\u0303\u221e(\u03b7\u2032)(x)\u2223\u2223\u2223\u03b1.\nThen using the inequality Ep(x) [|f(X)|] \u2264 1 + Varp(x) [f(X)], valid for any f \u2208 L2 ( Q, RD ) with Ep(x) [f ] = 0, we thus deduce\nVarp(x) [A(\u03b1\u03b7 \u2032, X)] \u2265 Varp(x) [E\u221e(\u03b7\u2032)(X)]\u03b12 \u2212 2Ke\u2212\u2206\u03b1 ( 1 + Varp(x) [E\u221e(\u03b7 \u2032)(X)] ) \u03b1.\nTaking the infimum over both sides, we get\nV \u2217(\u03b7) \u2265 V \u2217E (\u03b7)\u2212 2Ke\u2212\u2206\u03b1 (1 + V \u2217E (\u03b7))\u03b1.\nWe are now prepared to give the explicit example. It is defined by \u03b7k = 0 if k > 2 and\n\u03b71j = { \u2212a if d = 1, a d\u22121 o.w.\n(15)\nand for all j, \u03b72j =\na\nd(d\u2212 1) , (16)\nwhere\na = \u221a 1\u2212 1\nd .\nFor convenience, also define b(x) = \u2211 d xd\nand observe that\nE\u221e(\u03b7)(x) = { ab(x) d(d\u22121) if xj = 1, ab(x) d\u22121 o.w. ,\nOur goal will be to prove that\n1 \u2265 V \u2217E (\u03b7) \u2265 1\n32d(d\u2212 1) .\nThe claim will then follow by the above corollary.\nTo see that V \u2217E (\u03b7) \u2264 1, we simply note that\nmax k \u2223\u2223\u03b7Tk x\u2223\u2223 \u2264 a < 1, whence Varp(x) [ \u03b7Tx ] \u2264 1 as well and we are done.\nThe other direction requires more work. To prove it, we first prove the following lemma Lemma 12. With \u03b7 defined as in (15)-(16), we have\ninf \u03b7\u2032\u223c\u03b7\nEp(x) [ E\u221e(\u03b7 \u2032)(X)2 ] \u2265 1\n16d(d\u2212 1) .\nProof. Suppose \u03b7k \u2212 \u03b7\u2032k = \u03b2 \u2208 Rd. We can then write\ninf \u03b7\u2032\u223c\u03b7\nEp(x) [ E\u221e(\u03b7 \u2032)(X)2 ]\n= inf \u03b2\u2208Rd\n1\n2d \u2211 x\u2208H \u2211 x\u2208H [ E\u221e(\u03b7)(x)\u2212 \u03b2Tx ]2 and we therefore define\nL(\u03b2) = \u2211 x\u2208H \u2211 x\u2208H [ E\u221e(\u03b7)(x)\u2212 \u03b2Tx ]2 =\n\u2211 x : x1=0\n[( \u03b21 + \u03b2\nTx\u2212 a d (d\u2212 1)\n)2 + ( ab(x)\nd\u2212 1 \u2212 \u03b2Tx\n)2] ,\nnoting that inf L = 2d \u00b7 inf\n\u03b7\u2032\u223c\u03b7 Ep(x)\n[ E\u221e(\u03b7 \u2032)(X)2 ] .\nWe therefore need to prove\nL \u2265 2 d\u22124\nd(d\u2212 1) .\nHolding \u03b22:d fixed, we note that the optimal setting of \u03b21 is given by\n\u03b21 = \u2212 1\n2 \u2211 j\u22652 \u03b2j + a d (d\u2212 1) .\nWe can therefore work with the objective\nL(\u03b2) = \u2211\nx : x1=0\n[( \u03b2Tx\u2212 \u03b2Tx\u00ac )2 4 + ( ab(x) d\u2212 1 \u2212 \u03b2Tx )2] ,\nwhere we have defined\nx\u00acj = { 0 if j = 1, 1\u2212 xj o.w.\nGrouping into {x, x\u00ac} pairs, we end up with\nL(\u03b22:d) = \u2211\nx : x1=x2=0\n[( \u03b2Tx\u2212 \u03b2Tx\u00ac )2 2 + ( ab(x) d\u2212 1 \u2212 \u03b2Tx )2 + ( ab(x\u00ac) d\u2212 1 \u2212 \u03b2Tx\u00ac )2]\nNow, supposing b(x) \u2264 d\u221212 \u2212 3 2 or b(x) \u2265 D\u22121 2 + 3 2 , we have\n|b(x\u00ac)\u2212 b(x)| = |d\u2212 1\u2212 2b(x)| \u2265 3.\nWe will bound the terms that satisfy this property. Indeed, supposing we fix such an x, at least one of the following must be true: either\nmax\n(( ab(x)\nd\u2212 1 \u2212 \u03b2Tx\n)2 , ( ab(x\u00ac)\nd\u2212 1 \u2212 \u03b2Tx\u00ac\n)2) \u2265 a 2\n(d\u2212 1)2 ,\nor ( \u03b2Tx\u2212 \u03b2Tx\u00ac )2 \u2265 a2 (d\u2212 1)2 . Indeed, suppose the first condition does not hold. Then necessarily\u2223\u2223\u2223\u2223ab(x)d\u2212 1 \u2212 \u03b2Tx \u2223\u2223\u2223\u2223 < ad\u2212 1 and \u2223\u2223\u2223\u2223ab(x\u00ac)d\u2212 1 \u2212 \u03b2Tx\u00ac \u2223\u2223\u2223\u2223 < ad\u2212 1 ,\nso that a (b(x)\u2212 1)\nd\u2212 1 \u2264 \u03b2Tx \u2264 a (b(x) + 1)\nd\u2212 1 and\na (b(x\u00ac)\u2212 1) d\u2212 1 \u2264 \u03b2Tx \u2264 a (b(x \u00ac) + 1) d\u2212 1 .\nNow, if b(x) \u2265 b(x\u00ac) + 3, this immediately implies\n\u03b2Tx\u2212 \u03b2Tx\u00ac \u2265 a d\u2212 1\nand, symmetrically, if b(x\u00ac) \u2265 b(x) + 3, we get\n\u03b2Tx\u00ac \u2212 \u03b2Tx \u2265 a d\u2212 1 .\nEither way, the second inequality holds, whence the claim. Since there are at least 2d\u22121 \u2212 3\u00b72 d\u221a\n3d 2 +1\n\u2265\n2d\u22122 choices of x satisfying the requirements of our line of reasoning, we get 2d\u22123 pairs, whence\nL(\u03b22:d) \u2265 2d\u22124a2\n(d\u2212 1)2 =\n2d\u22124\nd (d\u2212 1) ,\nas claimed.\nWe can apply this lemma to derive a variance bound, viz. Lemma 13. With \u03b7 as in (15)-(16), we have\nV \u2217E (\u03b7) \u2265 1\n32d(d\u2212 1) .\nProof. For this, observe that, with \u03b7\u2032 being the value corresponding to \u03b7\u2032k \u2212 \u03b7k = \u03b2, we have\nV \u2217E (\u03b7) = inf \u03b2\n1\n2d \u2211 x\u2208H E\u0303\u221e(\u03b7 \u2032)(x)2 \u2265 inf \u03b2 1 2d \u2211 x\u2208H : x1=1 E\u0303\u221e(\u03b7 \u2032)(x)2.\nApplying the previous result to the (D \u2212 1)-dimensional hypercube on which x1 = 1, we deduce\nV \u2217E (\u03b7) \u2265 1 2 \u00b7 1 16(d\u2212 1)(d\u2212 2) =\n1 32(d\u2212 1)(d\u2212 2) \u2265 1 32d(d\u2212 1) .\nProof of Theorem 4 from Lemma 13. Putting everything together, we see first that\nV \u2217(\u03b1\u03b7) \u2265 V \u2217E (\u03b7)\u03b12 \u2212 4e\u2212\u2206\u03b1\u03b1,\nwhere \u2206 = \u221a\n1\u2212 1d 2(d\u22121) . But then this implies\nV \u2217(\u03b1\u03b7) \u2265 \u03b1 2\n32d(d\u2212 1) \u2212 4e\u2212\u2206\u03b1\u03b1.\nOn the other hand, ||\u03b7||22 \u2264 2, so \u03b12 = ||\u03b1\u03b7||22 ||\u03b7||22 \u2265 ||\u03b1\u03b7|| 2 2 2 , whence\nV \u2217(\u03b1\u03b7) \u2265 ||\u03b1\u03b7||22\n64d(d\u2212 1) \u2212 4e\u2212\n\u221a 1\u2212 1\nd ||\u03b1\u03b7||2\n2(d\u22121) ||\u03b1\u03b7||2 ,\nwhich is the desired result."}], "references": [{"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Generalized linear models", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1989}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "When and why are log-linear models self-normalizing", "author": ["J. Andreas", "D. Klein"], "venue": "Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neural network learning: theoretical foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Journal of statistical planning and inference", "author": ["A. O\u2019Hagan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "An introduction to sequential Monte Carlo", "author": ["A. Doucet", "N. De Freitas", "N. Gordon"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Log-linear models, a general class that includes conditional random fields (CRFs) and generalized linear models (GLMs), offer a flexible yet tractable approach modeling conditional probability distributions p(x|y) [1, 2].", "startOffset": 214, "endOffset": 220}, {"referenceID": 1, "context": "Log-linear models, a general class that includes conditional random fields (CRFs) and generalized linear models (GLMs), offer a flexible yet tractable approach modeling conditional probability distributions p(x|y) [1, 2].", "startOffset": 214, "endOffset": 220}, {"referenceID": 2, "context": "The machine translation community has recently described several procedures for training \u201cselfnormalized\u201d log-linear models [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 3, "context": "The machine translation community has recently described several procedures for training \u201cselfnormalized\u201d log-linear models [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 4, "context": "Previous work [5] bounds the sample complexity of self-normalizing training procedures for a restricted class of models, but leaves open the question of how self-normalization interacts with the predictive power of the learned model.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "The immediate motivation for this work is a procedure proposed to speed up decoding in a machine translation system with a neural-network language model [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "More usefully, all of the results presented here apply directly to trained neural nets in which the last layer only is retrained to self-normalize [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "The approach described at the beginning of this section is closely related to an alternative selfnormalization trick described based on noise-contrastive estimation (NCE) [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 3, "context": "In this case, empirical evidence suggests that the resulting model will also exhibit self-normalizing behavior [4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "Many of these involve approximating the associated sum or integral using quadrature [9], herding [10], or Monte Carlo methods [11].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Many of these involve approximating the associated sum or integral using quadrature [9], herding [10], or Monte Carlo methods [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "Generalizing a result from [5], we have: It will occasionally be instructive to consider the special case where X is the Boolean hypercube, and we will explicitly note where this assumption is made.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "In addition to the synthetic data, we compare our results to empirical data [3] from a self-normalized language model.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Lines marked \u201cKL=\u201d are from synthetic data; the line marked \u201cLM\u201d is from [3].", "startOffset": 73, "endOffset": 76}], "year": 2017, "abstractText": "Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as \u201cself-normalization\u201d, which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.", "creator": "LaTeX with hyperref package"}}}