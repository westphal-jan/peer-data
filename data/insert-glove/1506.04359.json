{"id": "1506.04359", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms", "abstract": "68.95 This paper sefirot studies the fundus generalization performance 9x19mm of breakwell multi - anti-obesity class cuddling classification algorithms, 56.63 for which heedlessly we obtain, catucci for the maurito first +7.00 time, beeches a dabash data - brignoles dependent generalization mallige error cbot bound marrazzo with caball\u00e9 a sahoo logarithmic dependence saramacca on the marquesan class size, substantially scholar-official improving the 80-73 state - gambala of - kamoga the - art linear dependence amirian in phototrophic the existing motels data - ottery dependent jehan generalization analysis. non-sex The theoretical southie analysis dority motivates ahlenius us to 64.42 introduce a lro new chungcheongbuk-do multi - class downshifts classification machine zellweger based bajeux on $ \\ jaywalker ell_p $ - trimet norm nitty regularization, where przegl\u0105d the parameter $ nayinzira p $ nabet controls argot the complexity 4,346 of aeropuerto the d\u0142ugie corresponding mprs bounds. z\u00fclle We derive an shf efficient optimization r.s. algorithm jadavpur based on saddlebag Fenchel duality theory. arrivistes Benchmarks iodization on driedger several real - world datasets show huw that re-issued the eurotrip proposed gottscheerish algorithm can elemer achieve significant accuracy gains over the perspicacity state pintado of the art.", "histories": [["v1", "Sun, 14 Jun 2015 08:07:23 GMT  (23kb)", "http://arxiv.org/abs/1506.04359v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yunwen lei", "\u00fcr\u00fcn dogan", "alexander binder", "marius kloft"], "accepted": true, "id": "1506.04359"}, "pdf": {"name": "1506.04359.pdf", "metadata": {"source": "CRF", "title": "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms", "authors": ["Yunwen Lei", "\u00dcr\u00fcn Dogan", "Alexander Binder"], "emails": ["yunwen.lei@hotmail.com", "urundogan@gmail.com", "alexander.binder@tu-berlin.de", "kloft@hu-berlin.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n04 35\n9v 1\n[ cs\n.L G\n] 1"}, {"heading": "1 Introduction", "text": "Typical multi-class application domains such as natural language processing [1], information retrieval [2], image annotation [3] and web advertising [4] involve tens or hundreds of thousands of classes, and yet these datasets are still growing [5]. To handle such learning tasks, it is essential to build algorithms that scale favorably with respect to the number of classes. Over the past years, much progress in this respect has been achieved on the algorithmic side [4\u20137], including efficient stochastic gradient optimization strategies [8].\nAlthough also theoretical properties such as consistency [9\u201311] and finite-sample behavior [1, 12\u2013 15] have been studied, there still is a discrepancy between algorithms and theory in the sense that the corresponding theoretical bounds do often not scale well with respect to the number of classes. This discrepancy occurs the most strongly in research on data-dependent generalization bounds, that is, bounds that can measure generalization performance of prediction models purely from the training samples, and which thus are very appealing in model selection [16]. A crucial advantage of these bounds is that they can better capture the properties of the distribution that has generated the data, which can lead to tighter estimates [17] than conservative data-independent bounds.\nTo our best knowledge, for multi-class classification, the first data-dependent error bounds were given by [14]. These bounds exhibit a quadratic dependence on the class size and were used by [12] and [18] to derive bounds for kernel-based multi-class classification and multiple kernel learning\n\u2217yunwen.lei@hotmail.com \u2020urundogan@gmail.com \u2021alexander.binder@tu-berlin.de \u00a7kloft@hu-berlin.de\nproblems, respectively. More recently, [13] improve the quadratic dependence to a linear dependence by introducing a novel surrogate for the multi-class margin that is independent on the true realization of the class label.\nHowever, a heavy dependence on the class size, such as linear or quadratic, implies a poor generalization guarantee for large-scale multi-class classification problems with a massive number of classes. In this paper, we show data-dependent generalization bounds for multi-class classification problems that\u2014for the first time\u2014exhibit a sublinear dependence on the number of classes. Choosing appropriate regularization, this dependence can be as mild as logarithmic. We achieve these improved bounds via the use of Gaussian complexities, while previous bounds are based on a well-known structural result on Rademacher complexities for classes induced by the maximum operator. The proposed proof technique based on Gaussian complexities exploits potential coupling among different components of the multi-class classifier, while this fact is ignored by previous analyses.\nThe result shows that the generalization ability is strongly impacted by the employed regularization. Which motivates us to propose a new learning machine performing block-norm regularization over the multi-class components. As a natural choice we investigate here the application of the proven \u2113p norm [19]. This results in a novel \u2113p-norm multi-class support vector machine (SVM), which contains the classical model by Crammer & Singer [20] as a special case for p = 2. The bounds indicate that the parameter p crucially controls the complexity of the resulting prediction models.\nWe develop an efficient optimization algorithm for the proposed method based on its Fenchel dual representation. We empirically evaluate its effectiveness on several standard benchmarks for multiclass classification taken from various domains, where the proposed approach significantly outperforms the state-of-the-art method of [20] by up to 1%.\nThe remainder of this paper is structured as follows. Section 2 introduces the problem setting and presents the main theoretical results. Motivated by which we propose a new multi-class classification model in Section 3 and give an efficient optimization algorithm based on Fenchel duality theory. In Section 4 we evaluate the approach for the application of visual image recognition and on several standard benchmark datasets taken from various application domains. Section 5 concludes."}, {"heading": "2 Theory", "text": ""}, {"heading": "2.1 Problem Setting", "text": "This paper considers multi-class classification problems with c \u2265 2 classes. Let X denote the input space and Y = {1, 2, . . . , c} denote the output space. Assume that we are given a sequence of examples S = {(x1, y1), . . . , (xn, yn)} \u2208 (X \u00d7 Y)n, independently drawn according to a probability measure P defined on the sample space Z = X \u00d7 Y. Based on the training examples S, we wish to learn a prediction rule hz from a space H of hypothesis mapping from Z to R and use the mapping x \u2192 argmaxy\u2208Y hz(x, y) to predict. For any hypothesis h \u2208 H , the margin \u03c1h(x, y) of the function h at a labeled example (x, y) is \u03c1h(x, y) \u2192 h(x, y) \u2212 maxy\u2032 6=y h(x, y \u2032\n). The prediction rule h makes an error at (x, y) if \u03c1h(x, y) \u2264 0 and thus the expected risk incurred from using h for prediction is R(h) := E[1\u03c1h(x,y)\u22640]."}, {"heading": "2.2 Notation", "text": "Any function h : X \u00d7 Y \u2192 R can be equivalently represented by the function vector (h1, . . . , hc) with hj(x) = h(x, j), \u2200j = 1, . . . , c. We denote by H\u0303 := {\u03c1h(x, y) : h \u2208 H} the class of margin functions associated to H . Let k : X \u00d7 X \u2192 R be a mercer kernel with \u03c6(x) being the associated feature map, i.e., k(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009. We denote by \u2016 \u00b7 \u2016\u2217 the dual norm of \u2016 \u00b7 \u2016, i.e., \u2016w\u2016\u2217 = sup\u2016w\u0304\u2016\u22641\u3008w, w\u0304\u3009. For a convex function f , we denote by f\u2217 its Fenchel conjugate, i.e., f\u2217(v) := supw[\u3008w, v\u3009 \u2212 f(w)]. For any w = (w1, . . . ,wc) we define the \u21132,p-norm by \u2016w\u20162,p = [ \u2211c j=1 \u2016wj\u2016 p 2]\n1/p. For any p \u2265 1, we denote by p\u2217 the dual exponent of p satisfying 1/p+ 1/p\u2217 = 1 and p\u0304 := p(2\u2212 p)\u22121. In the remainder of the paper, we require the following definitions.\nDefinition 1 (Strong Convexity). A function f : X \u2192 R is said to be \u03b2-strongly convex w.r.t. a norm\n\u2016 \u00b7 \u2016 iff \u2200x, y \u2208 X and \u2200\u03b1 \u2208 (0, 1), we have\nf(\u03b1x+ (1 \u2212 \u03b1)y) \u2264 \u03b1f(x) + (1\u2212 \u03b1)f(y)\u2212 \u03b2 2 \u03b1(1 \u2212 \u03b1)\u2016x\u2212 y\u20162.\nDefinition 2 (Regular Loss). We call \u2113 a L-regular loss if it satisfies the following properties:\n(i) \u2113(t) bounds the 0-1 loss from above: \u2113(t) \u2265 1t\u22640; (ii) \u2113 is L-Lipschitz in the sense |\u2113(t1)\u2212 \u2113(t2)| \u2264 L|t1 \u2212 t2|; (iii) \u2113(t) is decreasing and it has a zero point c\u2113, i.e., \u2113(c\u2113) = 0.\nSome examples of L-regular loss functions include the hinge \u2113h(t) = (1\u2212 t)+ and the margin loss \u2113\u03c1(t) = 1t\u22640 + (1 \u2212 t\u03c1\u22121)10<t\u2264\u03c1, \u03c1 > 0. (1)"}, {"heading": "2.3 Main results", "text": "Our discussion on data-dependent generalization error bounds is based on the established methodology of Rademacher and Gaussian complexities [21].\nDefinition 3 (Rademacher and Gaussian Complexity). Let H be a family of real-valued functions defined on Z and S = (z1, . . . , zn) a fixed sample of size n with elements in Z. Then, the empirical Rademacher and Gaussian complexities of H with respect to the sample S are defined by\nRS(H) = E\u03c3 [ sup h\u2208H 1 n n\u2211\ni=1\n\u03c3ih(zi) ] , GS(H) = Eg [ sup h\u2208H 1 n n\u2211\ni=1\ngih(zi) ] ,\nwhere \u03c31, . . . , \u03c3n are independent random variables with equal probability taking values +1 or \u22121, and g1, . . . , gn are independent N(0, 1) random variables.\nNote that we have the following comparison inequality relating Rademacher and Gaussian complexities [22]:\nRS(H) \u2264 \u221a \u03c0\n2 GS(H) \u2264 3\n\u221a \u03c0\n2\n\u221a lognRS(H). (2)\nExisting work on data-dependent generalization bounds for multi-class classifiers [12\u201314, 18] build on the following structural result on Rademacher complexities (e.g., [12], Lemma 8.1):\nRS(max{h1, . . . , hc} : hj \u2208 Hj , j = 1, . . . , c) \u2264 c\u2211\nj=1\nRS(Hj), (3)\nwhere H1, . . . , Hc are c hypothesis sets. This result is crucial for the standard generalization analysis of multi-class classification since the definition of margin involves the maximum operator, which is removed by the above lemma, but at the expense of a linear dependency on the number of classes. In the following we show that this linear dependency is suboptimal because (3) does not take into account the coupling among different classes. For example, a common regularizer used in multi-class classification algorithms is r(h) = \u2211c j=1 \u2016hj\u201622 [20], for which the components h1, . . . , hc are correlated via a \u2016 \u00b7 \u20162,2 regularizer, and the bound Eq. (3) ignoring this correlation would not be effective in this case [12\u201314, 18].\nAs a remedy, we here introduce a new structural complexity result on function classes induced by general classes via the maximum operator, while allowing to preserve the correlations among different components meanwhile. Instead of considering the Rademacher complexity, Lemma 4 concerns the structural relationship on Gaussian complexities since it is based on a comparison result among different Gaussian processes.\nLemma 4 (Structural result on Gaussian complexity). Let H be a class of functions defined on X \u00d7Y with Y = {1, . . . , c}. Let g1, . . . , gnc be independent N(0, 1) distributed random variables. Then, for any sample S = {x1, . . . , xn} of size n, we have\nGS ( {max{h1, . . . , hc} : h \u2208 H} ) \u2264 1\nn Eg sup\nh\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ihj(xi), (4)\nwhere Eg denotes the expectation w.r.t. to the Gaussian variables g1, . . . , gnc.\nThe proof of Lemma 5 is given in Supplementary Material A. Equipped with Lemma 4, we are now able to present a general data-dependent margin-based generalization bound. The proof of the following results (Theorem 5, Theorem 7 and Corollary 8) is given in Supplementary Material B.\nTheorem 5 (Data-dependent generalization bound for multi-class classification). Let H \u2282 RX\u00d7Y be a hypothesis class with Y = {1, . . . , c}. Let \u2113 be a L-regular loss function and denote B\u2113 := sup(x,y),h \u2113(\u03c1h(x, y)). Suppose that the examples S = {(x1, y1), . . . , (xn, yn)} are independently drawn from a probability measure defined on X \u00d7Y. Then, for any \u03b4 > 0, with probability at least 1\u2212 \u03b4, the following multi-class classification generalization bound holds for any h \u2208 H:\nR(h) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1h(xi, yi)) + 2L\n\u221a 2\u03c0 n Eg sup\nh\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ihj(xi) + 3B\u2113 \u221a log 2\u03b4 2n ,\nwhere g1, . . . , gnc are independent N(0, 1) distributed random variables.\nRemark 6. Under the same condition of Theorem 5, [12] derive the following data-dependent generalization bound:\nR(h) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1h(xi, yi)) + 4Lc\nn RS(\u03a01(H)) + 3B\u2113 \u221a log 2\u03b4 2n ,\nwhere \u03a01(H) := {x \u2192 h(x, y) : y \u2208 Y, h \u2208 H}. This linear dependence on c is due to the use of Eq. (3). For comparison, Theorem 5 implies that the dependence on the class size is governed by the term \u2211n i=1 \u2211c j=1 g(j\u22121)n+ihj(xi), an advantage of which is that the components h1, . . . , hc are jointly coupled. As we will see, this allows us to derive an improved result having a favorable dependence on c, when a constraint is imposed on (h1, . . . , hc).\nThe following Theorem 7 applies the general result in Theorem 5 to kernel-based methods. The hypothesis space is defined by imposing a constraint with a general strongly convex function.\nTheorem 7 (Data-dependent generalization bound for kernel-based multi-class learning algorithms). Suppose that the hypothesis space is defined by\nH := Hf,\u039b = {hw = (\u3008w1, \u03c6(x)\u3009, . . . , \u3008wc, \u03c6(x)\u3009) : f(w) \u2264 \u039b}, where f is a \u03b2-strongly convex function w.r.t. a norm \u2016 \u00b7 \u2016 defined on H satisfying f\u2217(0) = 0. Let \u2113 be a L-regular loss function and denote B\u2113 := sup(x,y),h \u2113(\u03c1h(x, y)). Let g1, . . . , gnc be independent N(0, 1) distributed random variables. Then, for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 we have\nR(hw) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1hw(xi, yi))+ 4L\nn \u221a\u221a\u221a\u221a\u03c0\u039b \u03b2 Eg n\u2211\ni=1\n\u2016(gi\u03c6(xi), gn+i\u03c6(xi), . . . , g(c\u22121)n+i\u03c6(xi))\u20162\u2217+3B\u2113 \u221a log 2\u03b4 2n .\nWe now consider the following specific hypothesis spaces using a \u2016 \u00b7 \u20162,p constraint: Hp,\u039b := {hw = (\u3008w1, \u03c6(x)\u3009, . . . , \u3008wc, \u03c6(x)\u3009) : \u2016w\u20162,p \u2264 \u039b}, 1 \u2264 p \u2264 2. (5)\nCorollary 8 (\u2113p-norm multi-class SVM generalization bound). Let \u2113 be a L-regular loss function and denote B\u2113 := sup(x,y),h \u2113(\u03c1h(x, y)). Then, with probability at least 1 \u2212 \u03b4, for any hw \u2208 Hp,\u039b the generalization error R(hw) can be upper bounded by:\n1\nn\nn\u2211\ni=1\n\u2113(\u03c1hw(xi, yi)) + 3B\u2113 \u221a log 2\u03b4 2n + 2L\u039b n \u221a\u221a\u221a\u221a n\u2211\ni=1\nk(xi, xi)\u00d7 {\u221a e(4 log c)1+ 1\n2 log c , if p \u2264 2 log c2 log c\u22121 ,( 2p p\u22121 )2\u2212 1 p c p\u22121 p , otherwise.\nRemark 9. The bounds in Corollary 8 enjoy a mild dependence on the number of classes. The dependence is polynomial with exponent p\u22121p for 2 log c 2 log c\u22121 < p \u2264 2 and becomes logarithmic if 1 \u2264 p \u2264 2 log c2 log c\u22121 . Which is substantially milder than the quadratic dependence established in [12, 14, 18] and the linear dependence established in [13]. Our generalization bound is data-dependent and shows clearly how the margin would affect the generalization performance (when \u2113 is the margin loss \u2113\u03c1): a large margin \u03c1 would increase the empirical error while decrease the model\u2019s complexity, and vice versa."}, {"heading": "2.4 Comparison of the Achieved Bounds to the State of the Art", "text": "Related work on data-independent bounds. The large body of theoretical work on multiclass learning considers data-independent bounds. Based on the \u2113\u221e-covering number bound of linear operators, [15] obtain a generalization bound exhibiting a linear dependence on the class size, which is improved by [9] to a radical dependence of the form O(n\u2212 1 2 (log 3 2 n) \u221a c \u03c1 ). Under conditions analogous to Corollary 8, [23] derive a class-size independent generalization guarantee. However, their bound is based on a delicate definition of margin, which is why it is commonly not used in the mainstream multi-class literature. [1] derive the following generalization bound\nE [1 p log ( 1 + \u2211\ny\u0303 6=y ep(\u03c1\u2212\u3008w\u0302y\u2212w\u0302y\u0303,\u03c6(x)\u3009)\n)] \u2264 inf\nw\u2208H [1 p log ( 1 + \u2211\ny\u0303 6=y ep(\u03c1\u2212\u3008wy\u2212wy\u0303,\u03c6(x)\u3009)\n)\n+ \u03bbn\n2(n+ 1) \u2016w\u201622,2\n] + 2 supx\u2208X k(x, x)\n\u03bbn , (6)\nwhere \u03c1 is a margin condition, p > 0 a scaling factor, and \u03bb a regularization parameter. Eq. (6) is class-size independent, yet Corollary 8 shows superiority in the following aspects: first, for SVMs (i.e., margin loss \u2113\u03c1), our bound consists of an empirical error ( 1 n \u2211n i=1 \u2113\u03c1(\u03c1hw(xi, yi))) and a complexity term divided by the margin value (note that L = 1/\u03c1 in Corollary 8). When the margin is large (which is often desirable) [14], the last term in the bound given by Corollary 8 becomes small, while\u2014on the contrary\u2014-the bound (6) is an increasing function of \u03c1, which is undesirable. Secondly, Theorem 7 applies to general loss functions, expressed through a strongly convex function over a general hypothesis space, while the bound (6) only applies to a specific regularization algorithm. Lastly, all the above mentioned results are conservative data-independent estimates.\nRelated work on data-dependent bounds. The techniques used in above mentioned papers do not straightforward translate to data-dependent bounds, which is the type of bounds in the focus of the present work. The investigation of these was initiated, to our best knowledge, by [14]: with the structural complexity bound (3) for function classes induced via the maximal operator, [14] derive a margin bound admitting a quadratic dependency on the number of classes. [12] use these results in [14] to study the generalization performance of multi-class SVMs, where the components h1, . . . , hc are coupled with an \u2016 \u00b7 \u20162,p, p \u2265 1 constraint. Due to the usage of the suboptimal Eq. (3), [12] obtain a margin bound growing quadratically w.r.t. the number of classes. [18] develop a new multi-class classification algorithm based on a natural notion called the multi-class margin of a kernel. [18] also present a novel multi-class Rademacher complexity margin bound based on Eq. (3), and the bound also depends quadratically on the class size. More recently, [13] give a refined Rademacher complexity bound for multi-class classification with a linear dependence on the class size. The key reason for this improvement is the introduction of \u03c1\u03b8,h := miny\u2032\u2208Y [h(x, y)\u2212 h(x, y \u2032\n) + \u03b81y\u2032=y] bounding margin \u03c1h from below, and since the maximum operation in \u03c1\u03b8,h is applied to the set Y rather than the subset Y \u2212 {yi} for \u03c1h, one needs not to consider the random realization of yi. We also use this trick in our proof of Theorem 5. However, [13] failed to improve this linear dependence to a logarithmic dependence, as we achieved in Corollary 8, due to the use of the suboptimal structural result (3)."}, {"heading": "3 Algorithms", "text": "Motivated by the generalization analysis given in Section 2, we now present a new multi-class learning algorithm, based on performing empirical risk minimization in the hypothesis space (5). This corresponds to the following \u2113p-norm multi-class SVM (p \u2265 1):\nProblem 10 (Primal problem: \u2113p-norm multi-class SVM).\nmin w\n1\n2\n[ c\u2211\nj=1\n\u2016wj\u2016p2 ] 2 p + C n\u2211\ni=1\n\u2113(ti),\ns.t. ti = \u3008wyi , \u03c6(xi)\u3009 \u2212max y 6=yi\n\u3008wy, \u03c6(xi)\u3009, (P)\nFor p = 2 we recover the seminal multi-class algorithm by Crammer & Singer [20], which is thus a special case of the proposed formulation. An advantage of the proposed approach over [20] can be that, as shown in Corollary 8, the dependence of the generalization performance on the class size becomes milder as p decreases to 1."}, {"heading": "3.1 Dual problems", "text": "Since the optimization problem (P) is convex, we can derive the associated dual problem for the construction of efficient optimization algorithms. The derivation of the following dual problem is deferred to Supplementary Material C. For a matrix \u03b1 \u2208 Rn\u00d7c, we denote by \u03b1i the ith row. Denote by ej the j-th unit vector in R c and 1 the vector in Rc with all components being zero.\nProblem 11 (Completely dualized problem for general loss functions). The Lagrangian dual problem of (10) is:\nsup \u03b1\u2208Rn\u00d7c \u2212 1 2\n[ c\u2211\nj=1\n\u2225\u2225 n\u2211\ni=1\n\u03b1ij\u03c6(xi) \u2225\u2225 pp\u22121 2 ] 2(p\u22121) p \u2212 C n\u2211\ni=1\n\u2113\u2217(\u2212\u03b1iyi C )\ns.t. \u03b1ij \u2264 0 \u2227 \u03b1i \u00b7 1 = 0, \u2200j 6= yi, i = 1, . . . , n. (D)\nTheorem 12 (Representer theorem). For any dual variable \u03b1 \u2208 Rn\u00d7c, the associated primal variable w = (w1, . . . ,wc) minimizing the Lagrangian saddle problem can be represented by:\nwj = [ c\u2211\nj\u0303=1\n\u2016 n\u2211\ni=1\n\u03b1ij\u0303\u03c6(xi)\u2016 p\u2217 2\n] 2 p\u2217 \u22121\u2225\u2225 n\u2211\ni=1\n\u03b1ij\u03c6(xi) \u2225\u2225p\u2217\u22122 2 [ n\u2211\ni=1\n\u03b1ij\u03c6(xi) ] .\nFor the hinge loss \u2113h(t) = (1\u2212t)+, we know its Fenchel-Legendre conjugate is \u2113\u2217h(t) = t if \u22121 \u2264 t \u2264 0 and \u221e elsewise. Hence \u2113\u2217h(\u2212 \u03b1iyi C ) = \u2212 \u03b1iyi C if \u22121 \u2264 \u2212 \u03b1iyi C \u2264 0 and \u221e elsewise. Now we have the following dual problem for the hinge loss function:\nProblem 13 (Completely dualized problem for the hinge loss (\u2113p-norm multi-class SVM)).\nsup \u03b1\u2208Rn\u00d7c \u2212 1 2\n[ c\u2211\nj=1\n\u2225\u2225 n\u2211\ni=1\n\u03b1ij\u03c6(xi) \u2225\u2225 pp\u22121 2\n] 2(p\u22121) p + n\u2211\ni=1\n\u03b1iyi\ns.t. \u03b1i \u2264 eyi \u00b7 C \u2227 \u03b1i \u00b7 1 = 0, \u2200i = 1, . . . , n. (7)"}, {"heading": "3.2 Optimization Algorithms", "text": "The dual problems (D) and (7) are not quadratic programs for p 6= 2, and thus generally not easy to solve. To circumvent this difficulty, we rewrite Problem 10 as the following equivalent problem:\nmin w,\u03b2\nc\u2211\nj=1\n\u2016wj\u201622 2\u03b2j + C n\u2211\ni=1\n\u2113(ti)\ns.t. ti \u2264 \u3008wyi , \u03c6(xi)\u3009 \u2212 \u3008wy, \u03c6(xi)\u3009, y 6= yi, i = 1, . . . , n, \u2016\u03b2\u2016p\u0304 \u2264 1, p\u0304 = p(2\u2212 p)\u22121, \u03b2j \u2265 0.\n(8)\nThe class weights \u03b21, . . . , \u03b2c in Eq. (8) play a similar role as the kernel weights in \u2113p-norm multiple kernel learning (MKL) algorithms [19]. The equivalence between problem (P) and Eq. (8) follows directly from Lemma 26 in [24], which shows that the optimal \u03b2 = (\u03b21, . . . , \u03b2c) in Eq. (8) can be explicitly represented in closed form. Motivated by the recent work on \u2113p-norm MKL, we propose to solve the problem (8) via alternately optimizing w and \u03b2. As we will show, given temporarily fixed \u03b2, the optimization of w reduces to a standard multi-class classification problem. Furthermore, the update of \u03b2, given fixed w, can be achieved via an analytic formula.\nProblem 14 (Partially dualized problem for a general loss). For fixed \u03b2, the partial dual problem for the sub-optimization problem (8) w.r.t. w is\nsup \u03b1\u2208Rn\u00d7c \u2212 1 2\nc\u2211\nj=1\n\u03b2j \u2225\u2225 n\u2211\ni=1\n\u03b1ij\u03c6(xi) \u2225\u22252 2 \u2212 C n\u2211\ni=1\n\u2113\u2217(\u2212\u03b1iyi C )\ns.t. \u03b1ij \u2264 0 \u2227 \u03b1i \u00b7 1 = 0, \u2200j 6= yi, i = 1, . . . , n. (9)\nThe primal variable w minimizing the associated Lagrangian saddle problem is\nwj = \u03b2j\nn\u2211\ni=1\n\u03b1ij\u03c6(xi). (10)\nWe defer the proof to Supplementary Material C. Analogous to Problem 13, we have the following partial dual problem for the hinge loss.\nProblem 15 (Partially dualized problem for the hinge loss (\u2113p-norm multi-class SVM)).\nsup \u03b1\u2208Rn\u00d7c f(\u03b1) := \u22121 2\nc\u2211\nj=1\n\u03b2j \u2225\u2225 n\u2211\ni=1\n\u03b1ij\u03c6(xi) \u2225\u22252 2 + n\u2211\ni=1\n\u03b1iyi\ns.t. \u03b1i \u2264 eyi \u00b7 C \u2227 \u03b1i \u00b7 1 = 0, \u2200i = 1, . . . , n. (11)\nThe Problems 14 and 15 are quadratic, so we can use the dual coordinate ascent algorithm [25] to very efficiently solve them for the case of linear kernels. To this end, we need to compute the gradient and solve the restricted problem of optimizing only one \u03b1i, \u2200i, keeping all other dual variables fixed [25]. The gradient of f can be exactly represented by w:\n\u2202f\n\u2202\u03b1ij = \u2212\u03b2j\nn\u2211\ni\u0303=1\n\u03b1i\u0303jk(xi, xi\u0303) + 1yi=j = 1yi=j \u2212 \u3008wj , \u03c6(xi)\u3009. (12)\nSuppose the additive change to be applied to the current \u03b1i is \u03b4\u03b1i, then\nf(\u03b11, . . . , \u03b1i\u22121, \u03b1i + \u03b4\u03b1i, \u03b1i+1, . . . , \u03b1n)\n= \u2212 c\u2211\nj=1\n\u03b2j\nn\u2211\ni\u0303=1\n\u03b1i\u0303j(\u03b1ij + \u03b4\u03b1ij)k(xi, xi\u0303)\u2212 1\n2\nc\u2211\nj=1\n\u03b2j [\u03b4\u03b1ij ] 2k(xi, xi) + \u03b4\u03b1iyi + const\n=\nc\u2211\nj=1\n\u2202f\n\u2202\u03b1ij \u03b4\u03b1ij \u2212\n1\n2\nc\u2211\nj=1\n\u03b2jk(xi, xi)[\u03b4\u03b1ij ] 2 + const.\nTherefore, the sub-problem of optimizing \u03b4\u03b1i is given by\nmax \u03b4\u03b1i \u2212 1 2\nc\u2211\nj=1\n\u03b2jk(xi, xi)[\u03b4\u03b1ij ] 2 +\nc\u2211\nj=1\n\u2202f\n\u2202\u03b1ij \u03b4\u03b1ij\ns.t. \u03b4\u03b1i \u2264 eyi \u00b7 C \u2212\u03b1i \u2227 \u03b4\u03b1i \u00b7 1 = 0. (13)\nWe now consider the subproblem of updating class weights \u03b2 with temporarily fixed w, for which we have the following analytic solution. The proof is deferred to the Supplementary Material C.1.\nProposition 16. (Solving the subproblem with respect to the class weights) Given fixed wj, the minimal \u03b2j optimizing the problem (8) is attained at\n\u03b2j = \u2016wj\u20162\u2212p2 ( c\u2211\nj\u0303=1\n\u2016wj\u0303\u2016p2 ) p\u22122 p . (14)\nThe update of \u03b2j based on Eq. (14) requires calculating \u2016wj\u201622, which can be easily fulfilled by recalling the representation established in Eq. (10).\nThe resulting training algorithm for the proposed \u2113p-norm multi-class SVM is given Algorithm 1. The algorithm alternates between solving a multi-class SVM problem for fixed class weights (Line 3) and updating the class weights in a closed-form manner (Line 5). Recall that Problem 11 establishes a completely dualized problem, which can be used as a sound stopping and evaluation criterion for the optimization algorithm.\nAlgorithm 1: Training algorithm for \u2113p-norm multi-class classification.\ninput: examples {(xi, yi)ni=1} and the kernel k. 1 initialize \u03b2j = p\u0304 \u221a 1/c,wj = 0 for all j = 1, . . . , c 2 while Optimality conditions are not satisfied do 3 optimize the multi-class classification problem (9) 4 compute \u2016wj\u201622 for all j = 1, . . . , c, according to Eq. (10) 5 update \u03b2j for all j = 1, . . . , c, according to Eq. (14) 6 end"}, {"heading": "4 Empirical Analysis", "text": "We implemented the proposed \u2113p-norm multi-class SVM algorithm (Algorithm 1) in C++ and solved the involved MC-SVM problem using dual coordinate ascent [25]. We experiment on three benchmark datasets: the Sector dataset studied in [26], the News 20 dataset collected and originally used for text classification by [27], and the Rcv1 dataset collected by [28]. Table 1 gives a description of these datasets.\nWe compare with the classical multi-class classification algorithm proposed by Crammer & Singer [20], which constitutes strong baseline for these datasets [25]. We employ a 5-fold cross validation on the training set to tune the regularization parameter C by grid search over the set {2\u221212, 2\u221211, . . . , 212} and the parameter p from the interval [1.2, 1.25, . . . , 10]. For the parameter p we first use a larger grid of step size 0.5 and then a finer grid of step size 0.1 around the optimum. Note that the model parameters are tuned separately for each training set and only based on the training set, not the test set. We repeat the experiments 10 times, and report in Table 2 on the average accuracy and standard deviations attained on the test set.\nWe observe that the proposed \u2113p-normMC-SVM consistently outperforms the method by Crammer & Singer [20] on all considered datasets. Specifically, our method attains 0.31% accuracy gain on Sector, 1.07% accuracy gain on News 20, and 0.53% accuracy gain on Rcv1. These promising results indicate that the proposed \u2113p-norm multiclass SVM could further lift the state of the art in multi-class classification, even in real-world applications beyond the ones studied in this paper."}, {"heading": "5 Conclusion", "text": "Motivated by the ever growing size of multi-class datasets in real-world applications such as image annotation and web advertising, which involve tens or hundreds of thousands of classes, we studied the influence of the class size on the generalization behavior of multi-class classifiers. We focus here on data-dependent generalization bounds enjoying the ability to capture the properties of the distribution that has generated the data. Of independent interest, for hypothesis classes that are given as a maximum over base classes, we developed a new structural result on Gaussian complexities that is able to preserve the coupling among different components, while the existing structural results ignore this coupling and may yield suboptimal generalization bounds. We applied the new structural result to study learning rates for multi-class classifiers, and derived, for the first time, a data-dependent bound with a logarithmic dependence on the class size, which substantially outperforms the linear dependence in the state-of-the-art data-dependent generalization bounds.\nMotivated by the theoretical analysis, we proposed a novel \u2113p-norm regularized multi-class support vector machine, where the parameter p controls the complexity of the corresponding bounds. This class of algorithms contains the classical model by Crammer & Singer [20] as a special case for p = 2. We developed an effective optimization algorithm based on the Fenchel dual representation. For several standard benchmarks for multi-class classification taken from various domains, the proposed approach surpassed the state-of-the-art method of Crammer & Singer [20], by up to 1%.\nAn exciting future direction will be to derive a data-dependent bound that is completely independent of the class size (even overcoming the mild logarithmic dependence of our bounds). To this end, we will study more powerful structural results than Lemma 4 for controlling complexities of function classes induced via the maximum operator. As a good starting point to this end, we will consider \u2113\u221e-covering numbers."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "A Proofs on Structural Results on Gaussian Complexity", "text": "Our discussion on complexity bound is based on the following comparison results among different Gaussian processes.\nLemma A.1 (Theorem 1 in [29]). Let {X\u03b8 : \u03b8 \u2208 \u0398} and {Y\u03b8 : \u03b8 \u2208 \u0398} be two non-zero real-valued Gaussian processes indexed by the same countable set \u0398 and suppose that\nE[(Y\u03b8 \u2212Y\u03b8\u0304)2] \u2264 E[(X\u03b8 \u2212 X\u03b8\u0304)2], \u2200\u03b8, \u03b8\u0304 \u2208 \u0398. (A.1)\nThen, E[sup\n\u03b8 Y\u03b8] \u2264 E[sup \u03b8 X\u03b8].\nProof of Lemma 4. Define two Gaussian processes indexed by H (for any h \u2208 H , we use here the equivalent representation h = (h1, . . . , hc)):\nXh :=\nn\u2211\ni=1\n[ gimax{h1(xi), h2(xi), . . . , hc(xi)} ] ,\nYh := n\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ihj(xi), \u2200h \u2208 H.\nFor any h = (h1, . . . , hc), h\u0304 = (h\u03041, . . . , h\u0304c) \u2208 H , the independence of the gi and the equalities Eg2i = 1 imply that\nE[(Xh \u2212 Xh\u0304)2] = n\u2211\ni=1\n[ max{h1(xi), . . . , hc(xi)} \u2212max{h\u03041(xi), . . . , h\u0304c(xi)} ]2\nE[(Yh \u2212Yh\u0304)2] = n\u2211\ni=1\n[ (h1(xi)\u2212 h\u03041(xi))2 + \u00b7 \u00b7 \u00b7+ (hc(xi)\u2212 h\u0304c(xi))2 ] .\nFor any a = (a1, . . . , ac), b = (b1, . . . , bc) \u2208 Rc, it can be directly checked that\n|max{a1, . . . , ac} \u2212max{b1, . . . , bc}| \u2264 max{|a1 \u2212 b1|, . . . , |ac \u2212 bc|} \u2264 c\u2211\ni=1\n|ai \u2212 bi|.\nApplying the above inequality with a = (h1(xi), . . . , hc(xi)), b = (h\u03041(xi), . . . , h\u0304c(xi)), i = 1, . . . , n, yields directly the following bounds relating the increments of the two Gaussian processes Xh,Yh:\nE[(Xh \u2212 Xh\u0304)2] \u2264 n\u2211\ni=1\nmax{|h1(xi)\u2212 h\u03041(xi)|, . . . , |hc(xi)\u2212 h\u0304c(xi)|}2\n=\nn\u2211\ni=1\nmax{|h1(xi)\u2212 h\u03041(xi)|2, . . . , |hc(xi)\u2212 h\u0304c(xi)|2}\n\u2264 n\u2211\ni=1\nc\u2211\nj=1\n|hj(xi)\u2212 h\u0304j(xi)|2 = E[(Yh \u2212Yh\u0304)2], \u2200h, h\u0304 \u2208 H.\nThat is, the condition (A.1) holds and therefore Lemma A.1 can be applied here to yield the stated result.\nThe following lemma gives a general Gaussian complexity bound for hypothesis spaces used in multi-class classification.\nLemma A.2 (Gaussian complexity of multi-class hypothesis spaces). Let H be a class of functions defined on X \u00d7 Y with Y = {1, . . . , c}. Let S = {(x1, y1), . . . , (xn, yn)} be a sequence of examples. Let g1, . . . , gnc be independent N(0, 1) distributed random variables. Then the empirical Gaussian complexity of H can be controlled by:\nGS(H) \u2264 1\nn Eg sup\nh\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ihj(xi).\nProof. Define two Gaussian processes indexed by H :\nXh :=\nn\u2211\ni=1\ngihyi(xi), Yh :=\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ihj(xi), \u2200h \u2208 H.\nFor any h, h\u0304 \u2208 H , it is obvious that\nE[(Xh \u2212 Xh\u0304)2] = n\u2211\ni=1\n[hyi(xi)\u2212 h\u0304yi(xi)]2\n\u2264 n\u2211\ni=1\n[ (h1(xi)\u2212 h\u03041(xi))2 + \u00b7 \u00b7 \u00b7+ (hc(xi)\u2212 h\u0304c(xi))2 ]\n= E[(Yh \u2212Yh\u0304)2].\nNow the stated inequality follows directly from Lemma A.1."}, {"heading": "B Proofs on Generalization bounds For Multi-class Classifiers", "text": ""}, {"heading": "B.1 Proof of Theorem 5", "text": "Proof of Theorem 5. For any \u03b8 > 0, introduce the following function bounding \u03c1h(x, y) from below:\n\u03c1\u03b8,h(x, y) = h(x, y)\u2212max y\u2032\u2208Y [h(x, y \u2032 )\u2212 \u03b81y\u2032=y] = min y\u2032\u2208Y [h(x, y)\u2212 h(x, y\u2032) + \u03b81y\u2032=y].\nIt can be checked that \u03c1\u03b8,h(x, y) = min(\u03c1h(x, y), \u03b8). Introduce two function classes derived from \u03c1\u03b8,h:\nH\u0303\u03b8 = {\u03c1\u03b8,h(x, y) : h \u2208 H}, H\u0303\u03b8 = {\u2113(\u03c1\u03b8,h(x, y)) : h \u2208 H}.\nAccording to the definition of L-regular loss function and the relationship \u03c1\u03b8,h \u2264 \u03c1h, we have\nR(h) = E[1\u03c1h(X,Y ) \u2264 0] \u2264 E[1\u03c1\u03b8,h(X,Y ) \u2264 0] \u2264 E[\u2113(\u03c1\u03b8,h(X,Y ))],\nwhich, together with McDiarmid inequality [30], yields the following inequality\nR(h) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1\u03b8,h(xi, yi)) + 2RS(H\u0303\u03b8) + 3B\u2113 \u221a log 2\u03b4 2n , \u2200h \u2208 H (B.1)\nwith probability at least 1\u2212 \u03b4. For the fixed parameter \u03b8 = c\u2113, we observe that \u03c1\u03b8,h(x, y) = min(\u03c1h(x, y), c\u2113). If \u03c1h(x, y) > c\u2113, the definition of L-regular loss implies that\n\u2113(\u03c1\u03b8,h(x, y)) = \u2113(c\u2113) = 0 = \u2113(\u03c1h(x, y)).\nOtherwise, we have \u03c1\u03b8,h(x, y) = \u03c1h(x, y). Therefore, for any (x, y) we have \u2113(\u03c1\u03b8,h(x, y)) = \u2113(\u03c1h(x, y)), which, coupled with the Lipschitz property of \u2113 and Eq. (B.1), yields the following inequality with probability at least 1\u2212 \u03b4:\nR(h) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1h(xi, yi)) + 2LRS(H\u0303\u03b8) + 3B\u2113 \u221a log 2\u03b4 2n , \u2200h \u2208 H. (B.2)\nThe Rademacher complexity of H\u0303\u03b8 satisfies the following inequality:\nRS(H\u0303\u03b8) = 1\nn E\u03c3 [ sup h\u2208H n\u2211\ni=1\n\u03c3i ( h(xi, yi)\u2212max\ny\u2208Y (h(xi, y)\u2212 \u03b81y=yi)\n)]\n\u2264 1 n E\u03c3[ sup\nh\u2208H\nn\u2211\ni=1\n\u03c3ih(xi, yi)] + 1\nn E\u03c3 [ sup h\u2208H n\u2211\ni=1\n\u03c3i max y\u2208Y\n(h(xi, y)\u2212 \u03b81y=yi) ]\n\u2264 \u221a \u03c0\n2 GS(H) +\n1\nn\n\u221a \u03c0\n2 Eg [ sup h\u2208H n\u2211\ni=1\ngimax(h1(xi)\u2212 \u03b81yi=1, . . . , hc(xi)\u2212 \u03b81yi=c) ] ,\n(B.3)\nwhere the last step follows from the relationship between Gaussian and Rademacher processes expressed in Eq. (2). Furthermore, according to Lemma 4, the last term of the above inequality can be addressed by\nEg[ sup h\u2208H\nn\u2211\ni=1\ngimax{h1(xi)\u2212 \u03b81yi=1, h2(xi)\u2212 \u03b81yi=2, . . . , hc(xi)\u2212 \u03b81yi=c}]\nLem. 4 \u2264 Eg sup\nh\u2208H\nn\u2211\ni=1\n[ gi(h1(xi)\u2212 \u03b81yi=1) + gn+i(h2(xi)\u2212 \u03b81yi=2) + \u00b7 \u00b7 \u00b7+ g(c\u22121)n+i(hc(xi)\u2212 \u03b81yi=c) ]\n= Eg sup h\u2208H\nn\u2211\ni=1\n[ gih1(xi) + gn+ih2(xi) + \u00b7 \u00b7 \u00b7+ g(c\u22121)n+ihc(xi) ]\n\u2212 Eg n\u2211\ni=1\n[gi\u03b81yi=1 + \u00b7 \u00b7 \u00b7+ g(c\u22121)n+i\u03b81yi=c]\n= Eg sup h\u2208H\nn\u2211\ni=1\n[ gih1(xi) + gn+ih2(xi) + \u00b7 \u00b7 \u00b7+ g(c\u22121)n+ihc(xi) ] .\nWith this inequality and using Lemma A.2 to tackle GS(H), we immediately derive the following bound on RS(H\u0303\u03b8):\nRS(H\u0303\u03b8) \u2264 \u221a 2\u03c0\nn Eg sup\nh\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ihj(xi).\nPutting this Rademacher complexity bound back into Eq. (B.2), we obtain the stated result."}, {"heading": "B.2 Proof of Theorem 7", "text": "To apply Theorem 5, we need to control the term suph\u2208H \u2211n i=1 \u2211c j=1 g(j\u22121)n+ihj(xi), which we\ntackle by the following lemma due to [31].\nLemma B.1 (Corollary 4 in [31]). If f is \u03b2-strongly convex w.r.t. \u2016 \u00b7 \u2016 and f\u2217(0) = 0, then, for any sequence v1, . . . , vn and for any \u00b5 we have\nn\u2211\ni=1\n\u3008vi, \u00b5\u3009 \u2212 f(\u00b5) \u2264 n\u2211\ni=1\n\u3008\u25bdf\u2217(v1:i\u22121, vi) + 1\n2\u03b2\nn\u2211\ni=1\n\u2016vi\u20162\u2217,\nwhere v1:i denotes the sum \u2211i j=1 vj.\nProof of Theorem 7. For the hypothesis space H and any \u03bb > 0, applying Lemma B.1 with \u00b5 =\n(w1, . . . ,wc) and vi = \u03bb(gi\u03c6(xi), gn+i\u03c6(xi), . . . , g(c\u22121)n+i\u03c6(xi)), we have\n\u03bb sup hw\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ih w j (xi) = sup hw\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+i\u3008wj , \u03bb\u03c6(xi)\u3009\n= sup hw\u2208H\nn\u2211\ni=1\n\u3008(w1, . . . ,wc), (\u03bbgi\u03c6(xi), \u03bbgn+i\u03c6(xi), . . . , \u03bbg(c\u22121)n+i\u03c6(xi))\u3009\n\u2264 sup hw\u2208H f(w1, . . . ,wc) +\nn\u2211\ni=1\n\u3008\u25bdf\u2217(v1:i\u22121), vi\u3009+ \u03bb2\n2\u03b2\nn\u2211\ni=1\n\u2016(gi\u03c6(xi), gn+i\u03c6(xi), . . . , g(c\u22121)n+i\u03c6(xi))\u20162\u2217.\nTaking expectation on both sides w.r.t. the Gaussian variables g1, . . . , gnc, the term \u2211n\ni=1\u3008\u25bdf\u2217(v1:i\u22121), vi\u3009 vanishes, and therefore we obtain\nEg sup hw\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ih w j (xi) \u2264 \u039b\n\u03bb +\n\u03bb\n2\u03b2\nn\u2211\ni=1\nEg\u2016(gi\u03c6(xi), gn+i\u03c6(xi), . . . , g(c\u22121)n+i\u03c6(xi))\u20162\u2217.\nChoosing \u03bb = \u221a\n2\u03b2\u039b\u2211 n i=1 Eg\u2016(gi\u03c6(xi),gn+i\u03c6(xi),...,g(c\u22121)n+i\u03c6(xi))\u20162\u2217 , the above inequality translates to\nEg sup hw\u2208H\nn\u2211\ni=1\nc\u2211\nj=1\ng(j\u22121)n+ih w j (xi) \u2264 \u221a\u221a\u221a\u221a2\u039b \u03b2 n\u2211\ni=1\nEg\u2016(gi\u03c6(xi), gn+i\u03c6(xi), . . . , g(c\u22121)n+i\u03c6(xi))\u20162\u2217.\nPutting the above complexity bound into Theorem 5, we obtain the stated result.\nB.3 Proof on \u2113p-norm Multi-class Classification Generalization bounds (Corollary 8)\nThe following simple lemma controls the p-th moment of a N(0, 1) distributed random variable. We give the proof here for completeness.\nLemma B.2. Let g be N(0, 1) distributed. For any p > 0, the p-th moment of g can be bounded by\n[E|g|p] 1p \u2264 (2p) 12+ 1p .\nProof. Let \u2200n \u2208 N+ : \u0393(n) = (n \u2212 1)! be the Gamma function. The p-th moment of a N(0, 1) distributed random variable can be exactly expressed via Gamma function [32]:\nE|g|p = 2 p 2 \u221a \u03c0 \u0393 (p+ 1 2 ) \u2264 2 p 2 \u221a \u03c0 \u0393 ( \u2308p+ 1 2 \u2309 )\n= 2\np 2\n\u221a \u03c0 \u2308p\u2212 1 2 \u2309! \u2264 2\np 2\n\u221a \u03c0\n\u221a 2\u03c0\u2308p\u2212 1\n2 \u2309\u2308 p\u221212 \u2309+ 12\n\u2264 (2p) p2+1,\nwhere in the above deduction we have used Stirling\u2019s approximation [33]:\nn! \u2264 \u221a 2\u03c0nn+ 1 2 e\u2212n+1/(12n).\nProof of Corollary 8. Let g1, . . . , gnc be independent N(0, 1) distributed random variables. Denote by \u03c4s = [E|g1|s] 1 s the sth moment of a N(0, 1) distributed random variable. Let q be any number satisfying p \u2264 q \u2264 2. Introduce the function fq(w) := 12\u2016w\u201622,q. Any hw \u2208 Hq,\u039b satisfies the inequality\nfq(w) = 1\n2 \u2016w\u201622,q \u2264\n1 2 \u039b2.\nSince fq(w) is 1/q \u2217-strongly convex w.r.t. the norm \u2016 \u00b7\u20162,q, and the dual norm of \u2016 \u00b7\u20162,q is \u2016 \u00b7\u20162,q\u2217 [34], the summation of the squared dual norm in Theorem 7 can be rewritten as follows:\nn\u2211\ni=1\nEg\u2016(gi\u03c6(xi), . . . , g(c\u22121)n+i\u03c6(xi))\u201622,q\u2217 = n\u2211\ni=1\nEg\n[ c\u2211\nj=1\n\u2016g(j\u22121)n+i\u03c6(xi)\u2016q \u2217\n2\n] 2 q\u2217\n=\nn\u2211\ni=1\nEg\n[ c\u2211\nj=1\n|g(j\u22121)n+i|q \u2217] 2 q\u2217 k(xi, xi)\nsymmetry = Eg\n[ c\u2211\nj=1\n|gj |q \u2217] 2 q\u2217 n\u2211\ni=1\nk(xi, xi)\nJensen \u2264 c 2q\u2217 \u03c42q\u2217\nn\u2211\ni=1\nk(xi, xi).\nFrom which Theorem 7 immediately implies the following bounds, with probability at least 1\u2212 \u03b4 and for any hw \u2208 Hq,\u039b:\nR(hw) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1hw(xi, yi)) + 4L\u039bc1/q\n\u2217\n\u03c4q\u2217\nn\n\u221a\u221a\u221a\u221a\u03c0q\u2217 2 n\u2211\ni=1\nk(xi, xi) + 3B\u2113 \u221a log 2\u03b4 2n .\nFrom the trivial inequality \u2016w\u20162,p \u2265 \u2016w\u20162,q, we immediately conclude Hp,\u039b \u2282 Hq,\u039b. Therefore, for any hw \u2208 Hp,\u039b, we have\nR(hw) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1hw(xi, yi)) + inf p\u2264q\u22642\n4L\u039bc1/q \u2217\n\u03c4q\u2217\nn\n\u221a\u221a\u221a\u221a\u03c0q\u2217 2 n\u2211\ni=1\nk(xi, xi) + 3B\u2113 \u221a log 2\u03b4 2n .\nIt can be directly checked that the function t \u2192 \u221a tc1/t is decreasing along the interval (0, 2 log c) and increasing along the interval (2 log c,\u221e). Therefore, the above generalization bound satisfies the inequality\nR(hw) \u2264 1 n\nn\u2211\ni=1\n\u2113(\u03c1hw(xi, yi)) + 3B\u2113 \u221a log 2\u03b4 2n +\nL\u039b\nn\n\u221a\u221a\u221a\u221a8 n\u2211\ni=1\nk(xi, xi)\u00d7 {\u221a\n2e log c\u03c42 log c, if p \u2264 2 log c2 log c\u22121 , c p\u22121 p \u03c4 p\np\u22121\n\u221a p\np\u22121 , otherwise.\nApplying Lemma B.2 to bound the moments of Gaussian variables, the stated result follows immediately."}, {"heading": "C Proofs on the Dual Problems", "text": ""}, {"heading": "C.1 Equivalent Representation of \u2113p-norm Multi-class Classification", "text": "The equivalence between Problem (P) and Eq. (8) follows directly from the following lemma due to [24].\nLemma C.1 ([24]). Let ai \u2265 0, i \u2208 Nd and 1 \u2264 r < \u221e. Then\nmin \u03b7:\u03b7i\u22650,\n\u2211 i\u2208Nd \u03b7r i \u22641\n\u2211\ni\u2208Nd\nai \u03b7i =\n( \u2211\ni\u2208Nd a\nr r+1 i\n)1+ 1 r\nand the minimum is attained at\n\u03b7i = a\n1 r+1\ni(\u2211 k\u2208Nd a r r+1 k ) 1 r .\nProof of Proposition 16. Fixing w, the sub-optimization of Eq. (8) w.r.t. \u03b2 is\nmin \u03b2\nc\u2211\nj=1\n\u2016wj\u201622 2\u03b2j\ns.t. \u2016\u03b2\u2016p\u0304 \u2264 1, p\u0304 = p(2\u2212 p)\u22121, \u03b2j \u2265 0.\nThe stated result now follows directly by applying Lemma C.1 with r = p\u0304 and \u03b1j = \u2016wj\u201622."}, {"heading": "C.2 Derivation of the Completely Dualized Problem (Problem 11)", "text": "Derivation of Problem 11. Problem (P) translates to the following equivalent problem\nmin w\n1\n2\n[ c\u2211\nj=1\n\u2016wj\u2016p2 ] 2 p + C n\u2211\ni=1\n\u2113(ti)\ns.t. ti \u2264 \u3008wyi , \u03c6(xi)\u3009 \u2212 \u3008wy, \u03c6(xi)\u3009, y 6= yi, i = 1, . . . , n. (C.1)\nThe Lagrangian of the above convex optimization problem is\nL = 1 2\n[ c\u2211\nj=1\n\u2016wj\u2016p2 ] 2 p + C n\u2211\ni=1\n\u2113(ti) + n\u2211\ni=1\n\u2211 j 6=yi \u03b1\u0303ij ( ti + \u3008wj , \u03c6(xi)\u3009 \u2212 \u3008wyi , \u03c6(xi)\u3009 ) ,\nwith Lagrangian variables 0 \u2264 \u03b1\u0303 \u2208 Rn\u00d7(c\u22121). For the last term of the Lagrangian, we have the following identity:\nn\u2211\ni=1\n\u2211 j 6=yi \u03b1\u0303ij\u3008wj \u2212wyi , \u03c6(xi)\u3009 = n\u2211 i=1 \u2211 j 6=yi \u03b1\u0303ij\u3008wj , \u03c6(xi)\u3009 \u2212 n\u2211 i=1 \u2211\nj\u0303 6=yi\n\u03b1\u0303ij\u0303\u3008wyi , \u03c6(xi)\u3009\n=\nc\u2211\nj=1\n\u3008wj , \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u3009 \u2212\nc\u2211\nj=1\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u3008wj , \u03c6(xi)\u3009\n=\nc\u2211\nj=1\n\u2329 wj , \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) \u232a .\n(C.2)\nWith this identity, the Lagrangian translates to\nL = 1 2\n[ c\u2211\nj=1\n\u2016wj\u2016p2 ] 2 p + c\u2211\nj=1\n\u3008wj , \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi)\u3009+\nC\nn\u2211\ni=1\n[\u2113(ti) + 1\nC\n\u2211\nj\u0303 6=yi\n\u03b1\u0303ij\u0303ti]. (C.3)\nAccording to the definition of Fenchel conjugate function, it holds that\ninf w,t L = \u2212 sup w\n[ \u2212 1\n2\n[ c\u2211\nj=1\n\u2016wj\u2016p2 ] 2 p \u2212 c\u2211\nj=1\n\u3008wj , \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi)\u3009 ]\n\u2212 C n\u2211\ni=1\nsup ti\n[\u2212\u2113(ti)\u2212 \u2211\nj 6=yi\n1 C \u03b1\u0303ijti]\n= \u2212 [1 2 \u2225\u2225\u2225 ( \u2212 \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi) +\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) )c j=1 \u2225\u2225\u2225 2 2,p ]\u2217\n\u2212 C n\u2211\ni=1\n\u2113\u2217 ( \u2212 1\nC\n\u2211 j 6=yi \u03b1\u0303ij )\n= \u22121 2\n\u2225\u2225\u2225 ( \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) )c j=1 \u2225\u2225\u2225 2\n2, p p\u22121\n\u2212 C n\u2211\ni=1\n\u2113\u2217 ( \u2212 1\nC\n\u2211 j 6=yi \u03b1\u0303ij ) ,\n(C.4)\nwhere in the last step of the above deduction we have used the identity: ( 1 2\u2016 \u00b7 \u20162 )\u2217 = 12\u2016 \u00b7 \u20162\u2217 and the fact that the dual norm of \u2016 \u00b7 \u20162,p is \u2016 \u00b7 \u20162, p p\u22121 . Consequently, the dual problem becomes\nsup \u03b1\u0303\u2208Rn\u00d7(c\u22121) \u2212 1 2\n[ c\u2211\nj=1\n\u2225\u2225 \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) \u2225\u2225 pp\u22121 2 ] 2(p\u22121) p \u2212 C n\u2211\ni=1\n\u2113\u2217 ( \u2212 1\nC\n\u2211 j 6=yi \u03b1\u0303ij ) ,\ns.t. \u03b1\u0303 \u2265 0. Introducing \u03b1 \u2208 Rn\u00d7c via the substitution:\n\u03b1ij =\n{ \u2212\u03b1\u0303ij if j 6= yi\u2211\nj\u0303 6=yi \u03b1\u0303ij\u0303 if j = yi, (C.5)\nwe have \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) = \u2212 \u2211\ni:yi 6=j \u03b1ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u03b1ij\u03c6(xi), (C.6)\nfrom which the stated dual problem follows directly."}, {"heading": "C.3 Proof of the Representer Theorem (Theorem 12)", "text": "Let H1, . . . , Hc be c Hilbert spaces and p \u2265 1. Define the function gp(v1, . . . , vc) : H1\u00d7\u00b7 \u00b7 \u00b7\u00d7Hc \u2192 R by\ngp(v1, . . . , vc) = 1\n2 \u2016(v1, . . . , vc)\u201622,p, p \u2265 1.\nLemma C.2. The gradient of gp is\n\u2202gp(v1, . . . , vc) \u2202vj = [ c\u2211\nj\u0303=1\n\u2016vj\u0303\u2016 p 2 ] 2 p \u22121\u2016vj\u2016p\u221222 vj .\nProof. By the chain rule, we have\n\u2202gp(v1, . . . , vc)\n\u2202vj =\n1\np\n[ c\u2211\nj\u0303=1\n\u2016vj\u0303\u2016 p 2\n] 2 p \u22121 \u2202\u3008vj , vj\u3009 p 2\n\u2202vj\n= 1\n2\n[ c\u2211\nj\u0303=1\n\u2016vj\u0303\u2016 p 2\n] 2 p \u22121 \u2202\u3008vj , vj\u3009\n\u2202vj \u3008vj , vj\u3009\np 2\u22121\n= [ c\u2211\nj\u0303=1\n\u2016vj\u0303\u2016 p 2 ] 2 p \u22121\u2016vj\u2016p\u221222 vj .\nProof of Representer Theorem (Theorem 12). In our derivation of the dual problem (see Eq. (C.4)), the variable w should meet the optimality in the sense that\nw = argmax v \u22121 2\n[ c\u2211\nj=1\n\u2016vj\u2016p2 ] 2 p + c\u2211\nj=1\n\u3008vj , n\u2211\ni=1\n\u03b1ij\u03c6(xi)\u3009.\nSince (\u25bdf)\u22121 = \u25bdf\u2217 for any convex function f , and the Fenchel-conjugate of gp is gp\u2217 , we obtain the following representation of w: w = \u25bdg\u22121p ( n\u2211\ni=1\n\u03b1i1\u03c6(xi), . . . ,\nn\u2211\ni=1\n\u03b1ic\u03c6(xi) )\n= \u25bdgp\u2217 ( n\u2211\ni=1\n\u03b1i1\u03c6(xi), . . . ,\nn\u2211\ni=1\n\u03b1ic\u03c6(xi) )\n= [ c\u2211\nj=1\n\u2016 n\u2211\ni=1\n\u03b1ij\u03c6(xi)\u2016p \u2217\n2\n] 2 p\u2217 \u22121(\u2225\u2225 n\u2211\ni=1\n\u03b1i1\u03c6(xi) \u2225\u2225p\u2217\u22122 2 [ n\u2211\ni=1\n\u03b1i1\u03c6(xi) ] , . . . \u2225\u2225 n\u2211\ni=1\n\u03b1ic\u03c6(xi) \u2225\u2225p\u2217\u22122 2 [ n\u2211\ni=1\n\u03b1ic\u03c6(xi) ]) .\nThat is,\nwj = [ c\u2211\nj\u0303=1\n\u2016 n\u2211\ni=1\n\u03b1ij\u0303\u03c6(xi)\u2016 p\u2217 2\n] 2 p\u2217 \u22121\u2225\u2225 n\u2211\ni=1\n\u03b1ij\u03c6(xi) \u2225\u2225p\u2217\u22122 2 [ n\u2211\ni=1\n\u03b1ij\u03c6(xi) ] ."}, {"heading": "C.4 Derivation of Partially Dualized Problem (Problem 14)", "text": "Derivation of Problem 14. The Lagrangian of the problem (8) w.r.t. w is\nL = c\u2211\nj=1\n\u2016wj\u201622 2\u03b2j + C\nn\u2211\ni=1\n\u2113(ti) +\nn\u2211\ni=1\n\u2211 j 6=yi \u03b1\u0303ij ( ti + \u3008wj , \u03c6(xi)\u3009 \u2212 \u3008wyi , \u03c6(xi)\u3009 ) ,\nwith Lagrangian variables 0 \u2264 \u03b1\u0303 \u2208 Rn\u00d7(c\u22121). According to the identity (C.2), the Lagrangian translates to\nL = c\u2211\nj=1\n\u2016wj\u201622 2\u03b2j +\nc\u2211\nj=1\n\u3008wj , \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi) \u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi)\u3009 + C n\u2211\ni=1\n[\u2113(ti) + 1\nC\n\u2211\nj\u0303 6=yi\n\u03b1\u0303ij\u0303ti]. (C.7)\nAccording to the definition of Fenchel conjugate function, it holds that\ninf w,t\nL = \u2212 c\u2211\nj=1\n[ 1 \u03b2j sup wj [ \u2212 1 2 \u2016wj\u201622 \u2212 \u2329 wj , \u03b2j ( \u2211 i:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212 \u2211 i:yi=j \u2211 j\u0303 6=j \u03b1\u0303ij\u0303\u03c6(xi) )\u232a]]\n\u2212 C n\u2211\ni=1\nsup ti\n[\u2212\u2113(ti)\u2212 \u2211\nj 6=yi\n1 C \u03b1\u0303ijti]\n= \u2212 c\u2211\nj=1\n[ 1 \u03b2j [1 2 \u2225\u2225\u03b2j ( \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) )\u2225\u22252\n2\n]\u2217] \u2212 C n\u2211\ni=1\n\u2113\u2217 ( \u2212 1\nC\n\u2211 j 6=yi \u03b1\u0303ij )\n= \u22121 2\nc\u2211\nj=1\n\u03b2j \u2225\u2225\u2225 \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) \u2225\u2225\u2225 2\n2 \u2212 C\nn\u2211\ni=1\n\u2113\u2217 ( \u2212 1\nC\n\u2211 j 6=yi \u03b1\u0303ij ) ,\nwhere in the last step of the above deduction we have used the identity: ( 1 2\u2016 \u00b7 \u20162 )\u2217 = 12\u2016 \u00b7 \u20162\u2217 and the fact that the dual norm of \u2016 \u00b7 \u20162,2 is itself. Consequently, the dual problem becomes\nsup \u03b1\u0303\u2208Rn\u00d7(c\u22121) \u2212 1 2\nc\u2211\nj=1\n\u03b2j \u2225\u2225\u2225 \u2211\ni:yi 6=j \u03b1\u0303ij\u03c6(xi)\u2212\n\u2211\ni:yi=j\n\u2211\nj\u0303 6=j\n\u03b1\u0303ij\u0303\u03c6(xi) \u2225\u2225\u2225 2\n2 \u2212 C\nn\u2211\ni=1\n\u2113\u2217 ( \u2212 1\nC\n\u2211 j 6=yi \u03b1\u0303ij ) ,\ns.t. \u03b1\u0303 \u2265 0.\nIntroducing \u03b1 \u2208 Rn\u00d7c as in Eq. (C.5) and noticing the identity (C.6), the above dual problem becomes\nsup \u03b1\u2208Rn\u00d7c \u2212 1 2\nc\u2211\nj=1\n\u03b2j \u2225\u2225 n\u2211\ni=1\n\u03b1ij\u03c6(xi) \u2225\u22252 2 \u2212 C n\u2211\ni=1\n\u2113\u2217(\u2212\u03b1iyi C )\ns.t.\nc\u2211\nj=1\n\u03b1ij = 0, \u2200i = 1, 2, . . . , n,\n\u03b1ij \u2264 0, j 6= yi, \u2200i = 1, . . . , n.\n(C.8)\nNote that in the above derivation of the dual problem, the variable w should meet the optimality in the sense that\nw = argmax v \u22121 2\nc\u2211\nj=1\n\u2016vj\u201622 + c\u2211\nj=1\n\u03b2j\u3008vj , n\u2211\ni=1\n\u03b1ij\u03c6(xi)\u3009.\nThe representer theorem stated in Problem 14 follows directly from this optimization condition."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>This paper studies the generalization performance of multi-class classification algorithms, for<lb>which we obtain\u2014for the first time\u2014a data-dependent generalization error bound with a logarith-<lb>mic dependence on the class size, substantially improving the state-of-the-art linear dependence<lb>in the existing data-dependent generalization analysis. The theoretical analysis motivates us to<lb>introduce a new multi-class classification machine based on lp-norm regularization, where the<lb>parameter p controls the complexity of the corresponding bounds. We derive an efficient opti-<lb>mization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets<lb>show that the proposed algorithm can achieve significant accuracy gains over the state of the art.", "creator": "LaTeX with hyperref package"}}}