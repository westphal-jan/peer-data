{"id": "1506.05198", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "SAT-based Analysis of Large Real-world Feature Models is Easy", "abstract": "legge Modern stylet conflict - quechan driven clause - dependents learning (rispoli CDCL) huser Boolean gwendal SAT solvers midges provide efficient ckgm automatic analysis of real - world feature haryatna models (reh FM) of mahasiddhas systems embryonal ranging jokers from zebu cars to operating filefish systems. newcomers It is well - proteg\u00e9 known that bupa solver - based schachzeitung analysis goyo of tamsui real - right-footed world choicepoint FMs scale very roundell well vorbe even holism though 55.2 SAT instances schwieger obtained 809 from ollen such 10n FMs krumer are large, and even-par the corresponding lofland analysis thebus problems rath are known 2,512 to fouchet be boreotrophon NP - complete. To better walkeri understand xiangtang why SAT 10-play solvers are so effective, rakestraw we systematically delegitimized studied pragmatism many leskinen syntactic and ebv semantic characteristics 70-mph of a gravgaard representative brazda set of large real - kamino world FMs. acura We discovered dramatico that lezgin a key nicolae reason why pottsville large real - world bonnano FMs 13.08 are montilla easy - scoparia to - analyze liberalizes is that the vast interna majority swannell of the heythrop variables 39-32 in xacobeo these zahler models are unrestricted, 3,167 i. 13-county e. , the monoline models are satisfiable welf for both true dusapin and false ctm assignments to 26.04 such variables under the current adiwinoto partial hauenstein assignment. Given drugstore.com this discovery officiers and potocki our l\u00ea understanding of .68 CDCL SAT gastronome solvers, schemers we catalogo show genego that eufemiano solvers mediator can easily mabbitt find heise satisfying th\u00e9oden assignments dono for such psychopomp models butin without eucamptognathus too many backtracks relative raaphorst to the landsmannschaft model size, crucified explaining vyacheslavovich why hyperplastic solvers scale bakradze so well. Further analysis showed that 26,700 the crumpled presence squally of unrestricted variables in these greybull real - world lactulose models tibicen can be 87.88 attributed gullion to their friskiness high - degree stone-throwing of bromus variability. l'hebdo Additionally, we vahid experimented kendujhar with eleccion a series of ossuaries well - woodling known non - cf3 backtracking goyt simplifications booted that ush are stowaway particularly effective in solving FMs. third-person The remaining variables / svt1 clauses jaruga after simplifications, 1009 called the 107-105 core, are fulbert so few dispenza that hovingham they are hidetsugu easily diamnds solved 44s even normalcy with heliconius backtracking, thilak further flambards strengthening beno our ticktock conclusions.", "histories": [["v1", "Wed, 17 Jun 2015 04:25:39 GMT  (176kb)", "http://arxiv.org/abs/1506.05198v1", null], ["v2", "Thu, 18 Jun 2015 02:57:03 GMT  (223kb)", "http://arxiv.org/abs/1506.05198v2", null], ["v3", "Wed, 29 Jul 2015 02:54:29 GMT  (276kb)", "http://arxiv.org/abs/1506.05198v3", null]], "reviews": [], "SUBJECTS": "cs.SE cs.AI", "authors": ["jia hui liang", "vijay ganesh", "venkatesh raman", "krzysztof czarnecki"], "accepted": false, "id": "1506.05198"}, "pdf": {"name": "1506.05198.pdf", "metadata": {"source": "CRF", "title": "SAT-based Analysis of Large Real-world Feature Models is Easy", "authors": ["Jia Hui Liang", "Vijay Ganesh", "Krzysztof Czarnecki", "Venkatesh Raman"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n05 19\n8v 1\n[ cs\n.S E\n] 1\n7 Ju\nn 20\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nSPLC 2015, July 20 - 24, 2015, Nashville, TN, USA c\u00a9 2015 ACM. ISBN 978-1-4503-3613-0/15/07. . . $15.00\nDOI: http://dx.doi.org/10.1145/2791060.2791070\nCCS Concepts \u2022Software and its engineering \u2192 Software product lines;\nKeywords SAT-Based Analysis; Feature Model"}, {"heading": "1. INTRODUCTION", "text": "Feature models (FM) are widely used to represent the variablility and commonality in product lines and reusable software, first introduced in 1990 [14]. Feature models define all the valid feature configurations of a product line, whether it be a car or software. The process of feature modeling helps to ensure that relevant features are reusable, and unused features are removed to lower the complexity of the system under design [9]. Promoting the reuse of features shortens the time of delivering new products and lowers the cost of production. Such models can be automatically analyzed using conflict-driven clause-learning (CDCL) Boolean SAT solvers to detect inconsistencies or errors in the design process of a product, thus lowering the cost of production.\nModern CDCL Boolean SAT solvers are known to efficiently solve many large real-world SAT instances obtained from a variety of domains such as software testing, program analysis, and hardware verification. More recently, inspired by the success of SAT solvers in other domains, many researchers proposed the use of solvers to analyze feature models [2, 3, 4]. Subsequently, solvers have been widely applied to perform all manner of analysis on feature models, and have proven to be surprisingly effective even though the kind of feature model analysis discussed here is an NPcomplete problem. Furthermore, real-world FMs tend to be very large, often running into hundreds of thousands of clauses and tens of thousands of variables. This state of affairs has perplexed practitioners and theoreticians alike. Problem Statement: Hence, the problem we address in this paper is \u201cWhy is CDCL SAT-based analysis so effective in analyzing large real-world FMs?\u201d Contributions: Here we describe the contributions made in this paper.\n1. We found that the overwhelming number of variables occuring in real-world FMs (or its equivalent Boolean formula) that we studied are unrestricted. We say that a variable v in a Boolean formula \u03c6 given partial assignment S is unrestricted if there exist two satisfying ex-\ntensions of S (i.e., extensions of the partial assignment of S to all variables in \u03c6 such that \u03c6 is satisfiable), one with v = true and the other with v = false. Intuitively, an unrestricted variable does not cause modern CDCL SAT solvers to backtrack because under the given partial assignment, the solver cannot assign the unrestricted variable to a wrong value. This is important because if the number of backtracks a solver performs remains small relative to the size of inputs, then the solver is likely to scale very well for such class of inputs. Indeed, this is exactly what we observed in our experiments, CDCL SAT solvers perform very few, near-constant, number of backtracks even as the size of FMs increase. Prior to our findings, there was no known reason to believe that a majority of variables in real-world FMs are unrestricted. Note that in CDCL SAT solvers, the number of backtracks can be worst-case exponential in the number of Boolean variables. Hence, our finding of the link between large number of unrestricted variables in real-world FMs and near-constant number of backtracks by CDCL solvers in solving such instances goes to the heart of the question posed in the paper. Modern CDCL SAT solvers contain many features that improve their performance. We found that switching off these features, excluding Boolean constraint propagation (BCP) and backjumping, had no negative impact on performance while solving FMs. This observation is consistent with the fact that the vast majority of variables in real-world FMs are unrestricted.\n2. We also investigated the possible source of unrestricted variables in real-world FMs. We observed that the large percentage of unrestricted variables in real-world FMs is attributable to the high variability in such models. We say that an FM has high variability if a large number of features occuring in such a model are optional, i.e., one can obtain a valid product configuration irrespective of whether such features are selected. Indeed, this observation is consistent with the fact that FMs capture all features of a product line, whereas deriving a valid product from such models only require relatively few features to be present.\n3. We implemented numerous well-known non-backtracking simplifications from SAT literature. These techniques are invoked as a pre-processing step, prior to calling the solver. Often these simplifications completely solve such instances. There are few instances where these simplifications did not completely solve the input FMs, and instead returned a very small simplified formula, we call a core. These cores tend to consist largely of Horn clauses, that are subsequently easily solved by solvers with nearconstant number of backtracks in the worst-case. The point of these experiments was not to suggest new techniques, but to further test our hypothesis that SAT-based analysis of FM is easy by demonstrating the effectiveness of polynomial-time simplifications on the FM inputs. The main simplifications are based on resolution which is the basis of CDCL solvers. Efficient implementations of such simplification techniques are part of many modern CDCL SAT solvers such as Lingeling or Glucose.\n4. Our findings are consistent with the idea of backdoors posited by theorists to explain the power of SAT solvers [10, 29, 25, 20] on real-world SAT instances. We provide mul-\ntiple theorems to strengthen our hypothesis that SATbased analysis of FM is easy by connecting our findings with the theory of backdoors.\n5. Following previous work by Pohl et al. [22], we performed experiments to see if the treewidth of graphs of the formulas correlates with solver running time. We found that for large real-world FMs the correlation is weak.\n6. We also developed a technique for generating hard artificial FMs to better understand the different characteristics between easy, large, real-world FMs vs. hard, small, artificial ones.\nPrior to our discovery, it was not obvious why SAT-based analysis is so effective for real-world FMs. Our findings provide strong evidence supporting the thesis presented in the paper. We believe the questions we posed are important to better understand the conditions under which SAT solvers perform efficiently, guiding future research in technique development. Our findings are especially relevant for variability aware static analysis, which require making thousands of SAT queries on the FM. Further, it is possible that SAT solvers do not perform well on feature models for some specific future real-world problems; having a theory linking SAT solving methods and classes of models will be helpful to identify such challenges and their solutions. Additionally, we want to emphasize that all the FMs in our experiments are obtained from a diverse set of large real-world applications [6, 7], including a large FM based off the Linux kernel configuration model."}, {"heading": "2. BACKGROUND", "text": "This section provides the necessary background on FMs and the use of SAT solvers in analyzing them."}, {"heading": "2.1 Feature Models (FM)", "text": "Structurally, a FM looks like a tree where each node represents a distinct feature. The terms node and feature will be used interchangeably. Child nodes have two flavours: mandatory (the child feature is present if and only if the parent feature is present) and optional (if the parent feature is present then the child feature is optional, otherwise it is absent). Parent nodes can also restrict its children with feature groups: or (if the parent feature is present, then at least one of its child features is present) and alternative (if the parent feature is present, then exactly one of its child features is present). These are the structural constraints between the child and the parent.\nStructural constraints are often not enough to enforce the integrity of the model, in which case cross-tree constraints are necessary. Cross-tree constraints have no restrictions like structural constraints do, and can apply to any feature regardless of their position in the tree-part of the model. For this paper, cross-tree constraints are formulas in propositional logic where features are the variables in the formulas. Two examples of common cross-tree constraints are A \u2192 B (A requires B) and A \u2192 \u00acB (A excludes B)."}, {"heading": "2.2 SAT-based Analysis of Feature Models", "text": "The goal of SAT-based analysis of FMs is to find an assignment to the features such that the structural and cross-tree constraints are satisfied. It turns out that there is a natural\nreduction from feature models to SAT [2]. Each feature is mapped to a Boolean variable, the variable is true/false if the feature is selected/deselected. The structural and crosstree constraints are encoded as propositional logic formulas. The SAT solver can answer questions like whether the feature model encodes no products. The solver can also be adapted to handle product configuration: given a set of features that must be present and another set of features that must be absent, the solver will find a product that satsifies the request or answer that no such product exists. Optimization is also possible such as finding the product with the highest performance, although for this we need optimization solvers, multiple calls to a SAT solver, and/or bit-blasting the feature model attributes\u2019 integer values into a Boolean formula. Dead features, features that cannot exist in any valid products, can also be detected using solvers. More generally, SAT solvers provide a variety of possibilities for automated analysis of FMs, where manual analysis may be infeasible. Many specialized solvers [12, 13, 27] for FM analysis have been built that use SAT solvers as a backend. It is but natural to ask why SAT-based analysis tools scale so well and are so effective in diverse kinds of analysis of large real-world FMs. This question has been studied with randomly generated FMs based on realistic parameters [18, 23, 22] where all the instances were easily solved by a modern SAT solver. In this paper, we are studying large real-world FMs to explain why they are easy for SAT solvers."}, {"heading": "3. EXPERIMENTS AND RESULTS", "text": "In this section, we describe the experiments that we conducted to better understand the effectiveness of SAT-based analysis of FMs. We assume the reader is familiar with the translation of FMs to Boolean formulas in conjunctive normal form (CNF), which is explained in many papers [6, 7]."}, {"heading": "3.1 Experimental Setup and Benchmarks", "text": "All the experiments were performed on 3 different comparable systems whose specs are as follows: Linux 64 bit machines with 2.8 GHz processors and 32 GB of RAM.\nTable 1 lists 15 real-world feature models translated to CNF from a paper by Berger et al. [8]. The number of variables in these models range from 544 to 62470, and the number of clauses range from 1020 to 343944. Three of the models, the ones named 2.6.*, represent the x86 architecture as abstracted in the Linux kernel. A clause is binary if it\ncontains exactly 2 literals. If every clause is binary, then the satisfiability problem is called 2-SAT and it is solvable in polynomial time. A clause is Horn/anti-Horn if it contains at most one positive/negative literal. If every clause is Horn, then the satisfiability problem is called Horn-satisfiability and it is also solvable in polynomial time (likewise for antiHorn). Binary/Horn/anti-Horn clauses account for many of the clauses, but not overwhelmingly. Lots of Horn and anti-Horn clauses does not necessarily imply a problem is easy. For example, every clause in 3-SAT is either Horn or anti-Horn yet we currently do not know how to solve 3-SAT efficiently in general.\nThe real-world models we consider are also very complex at the syntactic level. For example, 5814 features in the Linux models declare a feature constraint, which, on average, refers to three other features. As shown by Berger et al. [8], these real-world feature models have significantly different characteristics than those of the randomly generated models used in previous studies [18, 22]. In particular, Mendonca et al. [18] generated models with cross-tree constraint ratio (CTCR) of maximum 30%, i.e., the percentage of features participating in cross-tree constraints. They also showed that models with higher CTCR tend to be harder. The real-world feature models we use have much higher CTCR, ranging from 46% to 96% [8]. Hence, given the semantics of these real-world FMs, not to mention their size, it was not a priori obvious at all that such models would be easy for modern CDCL solvers to analyze and solve."}, {"heading": "3.2 Experiment: How easy are real-world FMs", "text": "Figure 1 shows the solving times of the real-world feature models with the Sat4j solver [16] (version 2.3.4). With the default settings, the hardest feature model took a little over a second to solve. Clearly, the size of these feature models is not a problem for Sat4j. Modern SAT solvers have (at least) 4 main features that contribute to its performance: conflict-driven clause learning with backjumping, random search restarts, Boolean constraint propagation (BCP) using lazy data structures, and conflict-based adaptive branching called VSIDS [15]. Many modern solvers also implement pre-processing simplification techniques. Sat4j implements all these features except pre-processing simplifications. Figure 1 shows the running time after turning off 3 of these features, but the running times did not suffer significantly. BCP is surprisingly effective for real-world feature models.\nThe efficiency even without state-of-the-art techniques suggests that real-world FMs are easy to solve."}, {"heading": "3.3 Experiment: Hard artificial FMs", "text": "The question we posed in this experiment was whether it is possible to construct hard artificial FMs, and if so what would their structural features be. Indeed, we were able to construct small feature models that are very hard for SAT solvers. The procedure we used to generate such models is as follows:\n1. First, we randomly generate a small and hard CNF formula. One such method is to generate random 3-SAT with a clause density of 4.25. It is well-known that such random instances are hard for a CDCL SAT solver to solve [1]. We then used such generated problems as the cross-tree constraints for our hard FMs.\n2. Second, we generate a small tree with only optional features. The variables that occur in the cross-tree constraints from the first step are the leaves of the tree.\nThe key idea in generating such FMs is that for any pair of variables in the cross-tree constraints, the tree does not impose any constraints between the two. The problem then is as hard as the cross-tree constraints because a solution for the feature model is a solution for the cross-tree constraints and vice-versa. Using this technique, we can create small feature models that are hard to solve. Unlike these hard artifical FMs, the cross-tree constraints in large real-world FMs are evidently easy to solve. We also noted that the proportion of unrestricted variables in hard artificial FMs is relatively small."}, {"heading": "3.4 Experiment: Variability in real-world FMs", "text": "Feature models capture variability, and we believe that the search for valid product configuration is easier as variability increases. We hypothesized that finding a solution is easy, when the solver has numerous solutions to choose from. We ran the feature models with sharpSAT [28], a tool for counting the exact number of solutions. The results are in Table 2. We found that real-world FMs display very high variability, i.e., have lots of solutions.\nVariability is the reason feature models exist. The feature groups and optional features increase the variability in the model, and the results in Table 2 suggests the variability grows exponentially with the size of the model measured in number of variables. High variability in real-world FMs\nshould have structural properties that make them easy to solve."}, {"heading": "3.5 Experiment: Why solvers perform very few backtracks for real-world FMs", "text": "The goal of this experiment was to ascertain why solvers make so few backtracks while solving real-world FMs. First, observe that when a solver needs to make new decision, it must guess the correct assignment for the decision variablein-question under the current partial assignment in order to avoid backtracking. Also observe that if a decision variable is unrestricted then either a true or a false assignment to such a variable would lead to a satisfying assigment. In other words, a preponderance of unrestricted variable in an input formula to a solver implies that the solver will likely make few mistakes in assigning values to decision variables and thus perform very few backtracks during solving.\nGiven the above line of reasoning, we designed an experiment that would increase our confidence in our hypothesis that a vast majority of the variables in real-world FMs are unrestricted, and that the presence of these large number of unrestricted variables explains why SAT solvers perform very few backtracks while solving real-world FMs. The experiment is as follows: whenever the solver branches on a decision or backtracks (i.e., when the partial assignment changes), we take a snapshot of the solver\u2019s state. We want to examine how many unassigned variables are unrestricted/restricted. This requires copying the state of the snapshot into a new solver and checking if there are indeed solutions in both assignment branches of the variable under the current partial assignment, in which case the variablein-question is unrestricted. If only true branch (resp. false branch) has a solution, then it is a postively restricted (resp. negatively restricted) variable. If neither branch has a solution, then the current partial assignment is unsatisfiable.\nFigure 2 shows how the unrestricted variables for one Linux-based feature model change over the course of the search. Time is measured by the number of decisions and conflicts. The y-axis shows the unassigned variables under the current partial assignment. The red area denotes unrestricted variables. If the solver branches on a variable in the red region, the solver will remain on the right track to finding a solution. The blue/green area denotes restricted variables. If the solver branches on a variable in the blue/green region, the solver must assign the variable the correct value. 1 The\n1If the solver picks a random assignment for a restricted variable, then the solver will make a correct choice 50% of the time. In practice, SAT solvers bias towards false so neg-\npink area means the current partial assignment is unsatisfiable and the solver will need to backtrack eventually. The 15 real-world feature models have very large red regions.\nA large amount of unrestricted variables suggests that the instance is easy to solve. When the decision heuristic branches on an unrestricted variable it does not matter which branch to take, the partial assignment will remain satisfiable either way. Feature models offer enough flexibility such that the SAT solvers rarely run into dead ends."}, {"heading": "3.6 Experiment: Simplifications", "text": "We hypothesized that since the vast majority of the variables are unrestricted they can be easily simplified away. Furthermore, the remaining variables/clauses, we call a core, would be small enough such that even a brute-force approach could solve it easily. In point of fact, most, but not all, modern solvers have both pre- and in-processing simplification techniques already built-in. The term in-processing refers to simplification techniques that are called in the inner loop of the SAT solver, whereas pre-processing techniques are typically called only once at the start of a SAT solving run.\nThe goal of these experiments was not to suggest a new set of techniques to analyze FMs, but rather to reinforce our findings for the effectiveness of SAT-based analysis of real-world FMs.\nWe implemented a number of standard simplifications that are particularly suited for eliminating variables. These simplifications were implemented as a preprocessing step to the Sat4j solver. Note that the Sat4j solver does not come packaged with variable elimination techniques, and hence we had to implement them as a pre-processor. Briefly, a simplification is a transformation on the Boolean formula where the output formula is smaller and equisatisfiable to the input formula. We carefully chose simplifications that run in time polynomial in the size of the input Boolean formula.\nWhat we found was that indeed the simplifications were effective in eliminating more than 99% of the variables from real-world FMs. In 11 out of 15 instances from our bench-\natively restricted variables might be preferable to positively restricted variables. The bias is often configurable.\nmarks, the simplifications completely solved the instance without resorting to calling the backend solver. In the remaining cases, the cores were very small (at most 53 variables) and were largely Horn clauses that were easily solved by Sat4j. The simplifications we implemented are described below:\nEquivalent Variable Substitution: If x =\u21d2 y and y =\u21d2 x where x and y are variables, then replace x and y with a fresh variable z. The idea behind this technique is to coalesce the variables since effectively x = y. This simplification step is useful for mandatory features that produce bidirectional implications in the CNF translation.\nSubsumption: If C1 \u2282 C2 where C1 and C2 are clauses, then C2 is subsumed by C1. Remove all subsumed clauses. The idea here is that subsumed clauses are trivially redundant. Initially, no clauses are subsumed in the real-world feature models. After other simplifications, some clauses will become subsumed.\nSelf-Subsuming Resolution: If C \u2228 x and C \u2228 \u00acx \u2228 D where C and D are clauses and x is a literal, then replace with C\u2228x and C\u2228D. The idea here is that if x = true then C \u2228 \u00acx \u2228D reduces to C \u2228D. If x = false then C = true in which case both C \u2228 \u00acx \u2228D and C \u2228D are satisfied. In either case, C \u2228 \u00acx \u2228D is equal to C \u2228D, hence the clause can be shortened by removing the variable x.\nSome features in the 3 Linux models are tristate: include the feature compiled statically, include the feature as a dynamically loadable module, or exclude the feature. Tristate features require two Boolean variables to encode. This simplification step is particularly useful for tristate feature models. For example, the feature A is encoded using Boolean variables a and a\u2032. Table 3 shows how to interpret the values of this pair of variables.\nTo restrict the possibilities to one of these 3 combinations, the translation adds the clause a\u2228\u00aca\u2032. Since the interpretation of the feature requires both variables, they often appear together in clauses. Any other clause that contains a \u2228 \u00aca\u2032 will be removed by subsumption. Any other clause that contains a\u2228 a\u2032 or \u00aca\u2228\u00aca\u2032 will be shortened by self-subsuming resolution.\nVariable Elimination: Let T be the set of all clauses that contain a variable and its negation. Let x be a variable. Sx is the set of clauses where the variable x appears only positively. Sx is the set of clauses where the variable x appears only negatively. The variable x is eliminated by replacing the clauses Sx and Sx with:\n{C1\u2228C2 | (x\u2228C1) \u2208 Sx, (\u00acx\u2228C2) \u2208 Sx, (C1\u2228C2) /\u2208 T} The idea here is to proactively apply an inference rule from propositional logic called the resolution rule. This simplification step is called variable elimination because the variable x no longer appears in the resulting formula. This rule is only applied to variables where the number of output clauses is less than or equal to the number of input clauses.\nThis simplification step is very effective for pure literals. A variable is pure if it appears either only positively or only negatively in the input formula. If a variable is pure, then variable elimination will eliminate that variable and every clause containing that variable. 37.8% of variables in the 15 real-world feature models from Table 1 are pure. More variables can become pure as the formula is simplified.\nAsymmetric Branching: For a clause x\u2228C, where x is a literal and C is the remainder of the clause, temporarily add the constraint \u00acC. If a call to BCP returns unsatisfiable, then learn the clause C. The new learnt clause subsumes the original clause x\u2228C so remove the original clause. Otherwise, no changes.\nRCheck: For a clause C, temporarily replace the clause with the constraint \u00acC. If a call to BCP returns unsatisfiable, then the other clauses imply C so the clause is redundant. Remove C from the formula. Otherwise, no changes. The idea is to remove all clauses that are implied modulo BCP. Implied clauses can be useful for a SAT solver to prune the search space, for example learnt clauses, but they complicate analysis.\nBCP: The last simplification step is to apply BCP: if a clause of length k has k \u2212 1 of its literals assigned to false, then assign the last literal to true.\n3.6.1 Fixed Point We repeat the simplifications a maximum of 5 times or\nstop when a fixed point is reached. The simplifications can only shorten the size of the input formula with the exception of variable elimination. It is unclear how many passes of simplification are necessary to reach a fixed point in the worst-case with variable elimination in the mix. The upper limit of 5 passes is to guarantee a polynomial (in the size of the input formula) number of passes. For the models from Table 1, 2 to 3 passes are enough to reach a fixed point using the current implementation built on top of MiniSat\u2019s simplification routine. Each additional pass, in practice, experiences diminishing returns and 5 passes should be sufficient to simplify real-world feature models. The remaining formula after the 5 passes of simplifications is the core.\nThe worst-case execution time of the simplifications is\npolynomial in the size of the input formula. We used a standard encoding of FMs into Boolean formulas that are polynomially larger than the FMs in the number of features.\n3.6.2 Simplified Feature Models Table 4 shows the simplified feature models. Simplifica-\ntion alone is able to solve 11 of the models without resorting a backend solver. The remaining 4 are shrunk drastically in terms of variables and clauses by simplification.\nAt least 80% of the clauses in the simplified models are Horn. Horn-satisfiability can be solved in polynomial time. The algorithm works by applying BCP, and the Horn-formula is unsatisfiable if and only if the empty clause is derived. A similar algorithm for anti-Horn-satisfiability also exists. BCP is the engine for solving Horn-satisfiability in polynomial time and modern SAT solvers implement BCP, hence we hypothesize that Boolean formulas with \u2264 53 variables and \u2265 80% Horn clauses are easy for SAT solvers to solve.\nFigure 3 shows how Horn clauses make solving easier. Note that for 3-SAT, every clause is either Horn xor antiHorn, hence the symmetry in the graph. Every point in the graph is the average running time of 100 randomly generated 3-SAT instances. The instances were generated with 200 variables, which is larger than the simplified coreboot model. The running times where the Horn clauses exceed 80% is very low. Clause learning, adaptive branching, and random restarts are not necessary for this case. Although we originally suspected the core might be hard for its size, the core itself turned out to be easy as well."}, {"heading": "3.7 Experiment: Treewidth of real-world FMs", "text": "Experiments by Pohl et al. [22] show treewidth of the CNF representation of randomly-generated FMs to be strongly correlated with their corresponding solving times. We repeat the experiment on the real-world FMs to see if the correlation exists. In our experiments, treewidth is computed by finding a lower and upper bound because the exact treewidth computation is too expensive to compute. The longer the calculation runs, the tighter the bounds are. We used the same algorithm and package used by Pohl et al. We gave the algorithm a timeout of 3600 seconds and 24 GB of heap memory, up from 1000 seconds and 12 GB in the original experiment by Pohl et al. Figure 4 shows the results. The computation failed to place any upper bound on 9 FMs, and we omit these results because we do not know how close the\nL + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = >? @ A B CD EF\nT GH I JK M N\ncomputed lower bounds are to the exact answer. For the 6 models in the table, the lower and upper bound are close, and hence close to the exact answer.\nWe used Spearman\u2019s rank correlation, the same correlation method as in the original experiment, to correlate the lower bound and time. We found the correlation between the lower bound and time to be 0.257. We get the same number when correlating between the upper bound and time (Spearman\u2019s rank correlation is based on the rank and the lower and upper bound rank the models in the same order). Our results show a significantly poorer correlation than previous work on randomly-generated FMs. Although our sample is small, research on treewidth in real-world SAT instances (not FMs) have also found similar results, i.e., that treewidth of input Boolean formulas is not strongly correlated with running time of solvers and is not indicative of an instance\u2019s hardness [17]."}, {"heading": "3.8 Interpretation of Results", "text": "It is clear from our experiments that the vast majority of variables in real-world FMs are unrestricted and can be solved/analyzed by appropriate simplification or BCP in time polynomial in the number of variables of the corre-\nsponding SAT instance. What remains after BCP or simplification is a very small set of clauses (small relative to the number of variables in the input SAT formula), that we call the core, that can be solved by a CDCL solver with very few backtracks."}, {"heading": "4. FINDING RESTRICTED VARIABLES AND CONNECTIONS TO BACKDOORS", "text": "As defined in the introduction, we call a variable v of a satisfiable CNF formula \u03c6 unrestricted with respect to a partial assignment S, if S can be completed to a satisfying assignment to \u03c6 with v = 0 and a satisfying assignment with v = 1. We call a variable unrestricted (without reference to any partial assignment) if there exists a partial assignment S for which the variable is unrestricted and we call it restricted otherwise.\nNote that if a CDCL algorithm picks the partial assignment S correctly, then it will not backtrack on an unrestricted variable, and we believe that this is what is happening in the solvers when solving feature models, as there are overwhelmingly many unrestricted variables.\nHere we find interesting connections between this parame-\nter (of the number of restricted variables) and the well studied parameter of the number of backdoor variables. Let (C) be a class of CNF formulas. Recall [29, 11] that a strong (C) backdoor of a Boolean formula F is a set B of variables such that every assignment to the set B of variables (and simplification of F with the assignment) results in a formula in (C). A weak (C) backdoor of F is a set B of variables such that there exists an assignment to the variables of B such that the resulting simplified formula is satisfiable and is in (C). We observe the following interesting connections for the set of unrestricted variables with backdoors.\nLet E be the empty formula which is trivially satisfiable. Let S be the class of all satisfiable CNF formulas. Then we show the following.\nTheorem 4.0.1. A CNF formula F has at most k restricted variables if and only if it has a weak E backdoor of size at most k and if and only if it has a strong S backdoor of size at least n\u2212 k.\nBy a reduction from the Hitting Set problem, it can be shown that finding weak E backdoor of size at most k is not only NP -complete, but complete for the parameterized complexity class W [2] for general CNF formulas. However it is fixed-parameter tractable (but still NP -complete) for d-CNF formulas where each clause length is bounded by a fixed constant d. As we could not find a theorem for this in the backdoor literature [11, 20, 25] we give a theorem for completeness.\nTheorem 4.0.2. Determining if a given formula on n variables has a weak E backdoor of size at most k is W [2]- complete for general formulas, and is fixed-parameter tractable with an O\u2217(dk)2 algorithm for d-CNF formulas.\nBy Theorem 4.0.1, it follows that\nCorollary 4.0.1. Determining if a given formula on n variables, has at most k restricted variables or at least n\u2212 k strong S backdoor set is W [2]-complete for general formulas, and is fixed-parameter tractable with an O\u2217(dk) algorithm for d-CNF formulas.\nThe proofs are omitted due to lack of space, and are available in the tech report."}, {"heading": "5. THREATS TO VALIDITY OF EXPERIMENTAL METHODOLOGY AND RESULTS", "text": "In this Section we address threats to validity of our experimental methodology and results.\nValidity of FMs used for the Experiments: The studied collection of FMs are large real-world feature models [8], primarily based on software product lines. This collection includes all large real-world models that are publicly available. Further, systems software, the domain of these models, is known to produce some of the most complex configuration constraints [5]. After taking into account the complexity of these models, we believe that these findings will likely hold for many other large real-world models.\n2The O\u2217() notation ignores polynomial factors, i.e., by O\u2217(dk) we mean O(dknc) for some constant c. This is standard in parametrized complexity.\nValidity of Experimental Methodology: The unrestricted variables experiment takes the partial assignments we encounter during the run of the solver with default parameters. The partial assignments could vary if the branching heuristic initiated with a random seed. Ideally, we would like to show that unrestricted variables are plentiful under any partial assignment but this would require an infeasible amount of effort to prove. Having said that, there is no reason to believe that the number of unrestricted variables would change drastically for a different random seed. In fact, once we realized that real-world FMs have large number of unrestricted variables, we were able to analytically show that these unrestricted variables occur in such formulas due to the high variability of FMs. This high variability in FMs is a fundamental property of such models, and is independent of translation to SAT or random seed chosen.\nCorrectness of Mapping of FMs into SAT: The analyses rely on the correctness of mapping of the Kconfig and CDL models to propositional logic. We rely on existing work that reverse engineered the semantics of these languages from documentation and configuration tools and specified the semantics formally [7, 8].\nValidity of Choice of Solvers: All our experiments were performed using the Sat4j solver [16]. An important concern is whether these results would apply to other SAT solvers and analysis tools built using them. Our choice of Sat4j was driven by the fact that it is one of the simplest CDCL SAT solver publicly available, and this simplicity makes it easier to draw correct conclusions. Note that most other SAT solvers used in FM analysis are not only CDCL in the same vein as Sat4j, but also implement efficient variants of simplification techniques discussed above. Hence, we believe that our results would apply equally well to other solvers used in FM analysis. Finally, we did not consider CSP or BDD-based solvers since CDCL solvers are demonstrably better for FM analysis."}, {"heading": "6. RELATED WORK", "text": "Using constraint solvers to analyze feature models has a long tradition. Benavides et al. [3] give a comprehensive survey of techniques for analyzing feature models, such as checking consistency and detecting dead features, and for supporting configuration, such as propagating feature selections. These techniques use a range of solvers, including SAT and CSP solvers. Batory [2] was first to suggest the use of SAT solvers to support feature model analyses.\nSeveral authors have investigated the efficiency of different kinds of solvers for feature model analyses. Benavides et al. [4] compare the performance of SAT, CSP, and BDD solvers on two sample analyses on randomly generate feature models with up to 300 features. They show that BDD solvers are prone to exponential growth in memory usage on larger models, while SAT solvers achieve good runtime performance in the experiments. Pohl et al. [21] run similar comparison on feature models from the SPLOT collection, including a wider set of solver implementations. SPLOT models are relatively small\u2014the largest one has less than 300 features\u2014and are derived from academic works. Their results show that SAT solvers work well also for SPLOT models, although they detect some performance variation for Cvs. Java-based SAT solvers; they also confirm the tractability challenges for BDD solvers. Mendonca et al. [19] achieve\nscalability of BDD-based analyses to randomly generated feature models with up to 2000 features by optimizing the translation from the models to BDDs using variable ordering heuristics based on the feature hierarchy. Further, Mendonca et al. [18] show that SAT-based analyses scale to randomly generated feature models with up to 10000 features. All this previous work shows that SAT solvers perform well on small realistic feature models (SPLOT collection) and large randomly generated feature models. Although SAT solvers have been successfully applied to analyze the feature model of the Linux kernel [26], we are unaware of prior work systematically studying the performance of SAT solvers on large (1000+ features) real-world feature models, which is what we focus on in this paper.\nPrevious work has also investigated the hardness of the SAT instances derived from feature models, aiming at more general insights into the tractability of SAT-based analyses. Mendonca et al. [18] have studied the SAT solving behavior of randomly generated 3-SAT feature models. A 3-SAT feature model is one whose cross-tree constraints are 3-SAT formulas. While random 3-SAT formulas become hard when their clause density approaches 4.25 (so-called phase transition), this work shows experimentally that 3-SAT feature models remain easy across all clause densities of their corresponding 3-SAT formulas. Our work is different since it investigates the hardness of large real-world feature models rather than random ones. Even though random feature models are easy on average, specific instances may still be hard; for example, a tree of optional features with a hard 3-SAT cross-tree formula over the tree leaves is, albeit contrived, a hard feature model. Segura et al. [23] treat finding hard feature models as an optimization problem. They apply evolutionary algorithms to generate feature models of a given size that maximize solving time or memory use. Their experiments show that relatively small feature models can become intractable in terms of memory use for BDDs; however, the approach did not generate feature models that would be hard for SAT solvers. Again, this work only considers synthetically generated feature models, rather than real-world ones.\nFinally, Pohl et al. [22] propose graph width measures, such as treewidth, applied to graph representations of the CNF derived from a feature model as a upper bound of the complexity of analyses on the model. Their work evaluates this idea on a set of randomly generated feature models with up to 1000 features using SAT, CSP, and BDD solvers; they also repeat the experiment on the SPLOT collection. They find a significant correlation between certain treewidth measures on the incidence graph and the running time for most of the solvers. Further, the authors note that it is still unclear why SPLOT models are easier than generated ones, and whether that observation would hold for largescale real-world feature models. Our work addresses this gap by investigating the hardness of large-scale real-world feature models and providing an explanation why they are easy. Moreover, we were not able to identify any correlation between the treewidth and easiness of the SAT instances derived from large real-world models, which calls for more work to find effective hardness measures for feature models.\nLarge real-world models have become available to researchers only recently. While some papers hint at the existence of very large models in industry [6], these models are typically highly confidential. Sincero [24] was first to observe that\nthe definition of the Linux kernel build configuration, expressed in the Kconfig language, can be viewed as a feature model. Berger et al. [7] identify the Component Configuration Language (CDL) in eCos, an open-source real-time operating system, as additional feature modeling language and subsequently [8] create and analyze a collection of large real-world feature models from twelve open-source projects in the software systems domain. We use this collection as a basis for our work."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we provided an explanation, with strong experimental support, for the scalability of SAT solvers on large real-world FMs. The explanation is that the overwhelming majority of the variables in real-world FMs are unrestricted, and solvers tend not to backtrack in the presence of such variables. We argue that the reason for the presence of large number of unrestricted variables in real-world FMs has to do with high variability in such models. We also found that if we switch off all the heuristics in modern SAT solvers, except Boolean constant propagation (BCP) and backjumping (no clause-learning), then the solver does not suffer any deterioration in performance while solving FMs. Moreover, we ran a set of simplifications with substantial reduction to the size of the instances. In fact, a majority of the models were solved outright with these polynomial-time simplifications. These experiment further bolster our thesis that most variables in real-world FMs are unrestriced that can be eliminated by BCP or appropriate simplifications, and do not cause SAT solvers to perform expensive backtracks. Finally, we note that variables/clauses that are not eliminated through BCP or simplification, namely the core, are so few and mostly Horn that backtracking solvers can easily solve them. This observation is consistent with the idea of weak backdoors posited by theorists to explain the efficiency of CDCL solvers over certain classes of real-world SAT instances.\nVisit the following URL to download further details of the experiments we performed and additional associated data: https://github.com/JLiangWaterloo/fmeasy."}, {"heading": "8. ACKNOWLEDGEMENT", "text": "We thank Saket Saurabh for pointing out the connection to the dual of strong backdoors in Theorem 4.0.1."}, {"heading": "9. REFERENCES", "text": "[1] A. Balint, A. Belov, M. Ja\u0308rvisalo, and C. Sinz.\nOverview and analysis of the SAT Challenge 2012 solver competition. Artificial Intelligence, 223:120\u2013155, 2015.\n[2] D. Batory. Feature models, grammars, and propositional formulas. In Proceedings of the 9th International Conference on Software Product Lines, SPLC\u201905, pages 7\u201320, Berlin, Heidelberg, 2005. Springer-Verlag.\n[3] D. Benavides, S. Segura, and A. Ruiz-Corte\u0301s. Automated analysis of feature models 20 years later: A literature review. Information Systems, 35(6):615 \u2013 636, 2010.\n[4] D. Benavides, S. Segura, P. Trinidad, and A. Ruiz-Corte\u0301s. A first step towards a framework for\nthe automated analysis of feature models. In Managing Variability for Software Product Lines: Working With Variability Mechanisms, 2006.\n[5] T. Berger, R.-H. Pfeiffer, R. Tartler, S. Dienst, K. Czarnecki, A. Wasowski, and S. She. Variability mechanisms in software ecosystems. Information and Software Technology, 56(11):1520\u20131535, 2014.\n[6] T. Berger, R. Rublack, D. Nair, J. M. Atlee, M. Becker, K. Czarnecki, and A. Wasowski. A survey of variability modeling in industrial practice. In Proceedings of the Seventh International Workshop on Variability Modelling of Software-intensive Systems, VaMoS \u201913, pages 7:1\u20137:8, New York, NY, USA, 2013. ACM.\n[7] T. Berger, S. She, R. Lotufo, A. Wasowski, and K. Czarnecki. Variability modeling in the real: A perspective from the operating systems domain. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering, ASE \u201910, pages 73\u201382, New York, NY, USA, 2010. ACM.\n[8] T. Berger, S. She, R. Lotufo, A. Wasowski, and K. Czarnecki. A study of variability models and languages in the systems software domain. IEEE Trans. Softw. Eng., 39(12):1611\u20131640, Dec. 2013.\n[9] K. Czarnecki and U. W. Eisenecker. Generative Programming: Methods, Tools, and Applications. ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 2000.\n[10] B. Dilkina, C. Gomes, and A. Sabharwal. Backdoors in the context of learning. In O. Kullmann, editor, Theory and Applications of Satisfiability Testing - SAT 2009, volume 5584 of Lecture Notes in Computer Science, pages 73\u201379. Springer Berlin Heidelberg, 2009.\n[11] S. Gaspers and S. Szeider. Backdoors to satisfaction. In The Multivariate Algorithmic Revolution and Beyond, pages 287\u2013317. springer, 2012.\n[12] R. Gheyi, T. Massoni, and P. Borba. A theory for feature models in alloy. pages 71\u201380, Portland, USA, November 2006.\n[13] M. Janota. Do SAT solvers make good configurators? In SPLC (2), pages 191\u2013195, 2008.\n[14] K. C. Kang, S. G. Cohen, J. A. Hess, W. E. Novak, and A. S. Peterson. Feature-oriented domain analysis (FODA) feasibility study. Technical report, Carnegie-Mellon University Software Engineering Institute, November 1990.\n[15] H. Katebi, K. A. Sakallah, and J. a. P. Marques-Silva. Empirical study of the anatomy of modern SAT solvers. In Proceedings of the 14th International Conference on Theory and Application of Satisfiability Testing, SAT\u201911, pages 343\u2013356, Berlin, Heidelberg, 2011. Springer-Verlag.\n[16] D. Le Berre, A. Parrain, M. Baron, J. Bourgeois, Y. Irrilo, F. Fontaine, F. Laihem, O. Roussel, and L. Sais. Sat4j-a satisfiability library for java, 2006.\n[17] R. Mateescu. Treewidth in industrial SAT benchmarks. Technical report, Microsoft Research, 2011.\n[18] M. Mendonca, A. Wasowski, and K. Czarnecki. SAT-based analysis of feature models is easy. In Proceedings of the 13th International Software Product\nLine Conference, SPLC \u201909, pages 231\u2013240, Pittsburgh, PA, USA, 2009. Carnegie Mellon University.\n[19] M. Mendonca, A. Wasowski, K. Czarnecki, and D. Cowan. Efficient compilation techniques for large scale feature models. In Proceedings of the 7th International Conference on Generative Programming and Component Engineering, GPCE \u201908, pages 13\u201322, New York, NY, USA, 2008. ACM.\n[20] N. Misra, S. Ordyniak, V. Raman, and S. Szeider. Upper and lower bounds for weak backdoor set detection. In Theory and Applications of Satisfiability Testing\u2013SAT 2013, pages 394\u2013402. Springer, 2013.\n[21] R. Pohl, K. Lauenroth, and K. Pohl. A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models. In Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE \u201911, pages 313\u2013322, Washington, DC, USA, 2011. IEEE Computer Society.\n[22] R. Pohl, V. Stricker, and K. Pohl. Measuring the structural complexity of feature models. In Automated Software Engineering (ASE), 2013 IEEE/ACM 28th International Conference on, pages 454\u2013464, Nov 2013.\n[23] S. Segura, J. A. Parejo, R. M. Hierons, D. Benavides, and A. R. Corte\u0301s. Automated generation of computationally hard feature models using evolutionary algorithms. Expert Syst. Appl., 41(8):3975\u20133992, 2014.\n[24] J. Sincero, H. Schirmeier, W. Schro\u0308der-Preikschat, and O. Spinczyk. Is the linux kernel a software product line. In Proc. SPLC Workshop on Open Source Software and Product Lines, 2007.\n[25] S. Szeider. Backdoor sets for DLL subsolvers. Journal of Automated Reasoning, 35(1-3):73\u201388, 2005.\n[26] R. Tartler, D. Lohmann, J. Sincero, and W. Schro\u0308der-Preikschat. Feature consistency in compile-time-configurable system software: Facing the linux 10,000 feature problem. In Proceedings of the Sixth Conference on Computer Systems, EuroSys \u201911, pages 47\u201360, New York, NY, USA, 2011. ACM.\n[27] T. Thum, D. Batory, and C. Kastner. Reasoning about edits to feature models. In Proceedings of the 31st International Conference on Software Engineering, ICSE \u201909, pages 254\u2013264, Washington, DC, USA, 2009. IEEE Computer Society.\n[28] M. Thurley. sharpsat: Counting models with advanced component caching and implicit bcp. In Proceedings of the 9th International Conference on Theory and Applications of Satisfiability Testing, SAT\u201906, pages 424\u2013429, Berlin, Heidelberg, 2006. Springer-Verlag.\n[29] R. Williams, C. P. Gomes, and B. Selman. Backdoors to typical case complexity. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI\u201903, pages 1173\u20131178, San Francisco, CA, USA, 2003. Morgan Kaufmann Publishers Inc."}], "references": [{"title": "Overview and analysis of the SAT Challenge 2012 solver competition", "author": ["A. Balint", "A. Belov", "M. J\u00e4rvisalo", "C. Sinz"], "venue": "Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Feature models, grammars, and propositional formulas", "author": ["D. Batory"], "venue": "In Proceedings of the 9th International Conference on Software Product Lines,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Automated analysis of feature models 20 years later: A literature review", "author": ["D. Benavides", "S. Segura", "A. Ruiz-Cort\u00e9s"], "venue": "Information Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Ruiz-Cort\u00e9s. A first step towards a framework for  the automated analysis of feature models", "author": ["D. Benavides", "S. Segura", "P. Trinidad"], "venue": "In Managing Variability for Software Product Lines: Working With Variability Mechanisms,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Variability mechanisms in software ecosystems", "author": ["T. Berger", "R.-H. Pfeiffer", "R. Tartler", "S. Dienst", "K. Czarnecki", "A. Wasowski", "S. She"], "venue": "Information and Software Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A survey of variability modeling in industrial practice", "author": ["T. Berger", "R. Rublack", "D. Nair", "J.M. Atlee", "M. Becker", "K. Czarnecki", "A. Wasowski"], "venue": "In Proceedings of the Seventh International Workshop on Variability Modelling of Software-intensive Systems, VaMoS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Variability modeling in the real: A perspective from the operating systems domain", "author": ["T. Berger", "S. She", "R. Lotufo", "A. Wasowski", "K. Czarnecki"], "venue": "In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A study of variability models and languages in the systems software domain", "author": ["T. Berger", "S. She", "R. Lotufo", "A. Wasowski", "K. Czarnecki"], "venue": "IEEE Trans. Softw. Eng.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Generative Programming: Methods, Tools, and Applications", "author": ["K. Czarnecki", "U.W. Eisenecker"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Backdoors in the context of learning", "author": ["B. Dilkina", "C. Gomes", "A. Sabharwal"], "venue": "Theory and Applications of Satisfiability Testing - SAT 2009,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Backdoors to satisfaction. In The Multivariate Algorithmic Revolution and Beyond, pages 287\u2013317", "author": ["S. Gaspers", "S. Szeider"], "venue": "springer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A theory for feature models in alloy", "author": ["R. Gheyi", "T. Massoni", "P. Borba"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Feature-oriented domain analysis (FODA) feasibility study", "author": ["K.C. Kang", "S.G. Cohen", "J.A. Hess", "W.E. Novak", "A.S. Peterson"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1990}, {"title": "Empirical study of the anatomy of modern SAT solvers", "author": ["H. Katebi", "K.A. Sakallah", "J. a. P. Marques-Silva"], "venue": "In Proceedings of the 14th International Conference on Theory and Application of Satisfiability Testing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Sat4j-a satisfiability library for java", "author": ["D. Le Berre", "A. Parrain", "M. Baron", "J. Bourgeois", "Y. Irrilo", "F. Fontaine", "F. Laihem", "O. Roussel", "L. Sais"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Treewidth in industrial SAT benchmarks", "author": ["R. Mateescu"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "SAT-based analysis of feature models is easy", "author": ["M. Mendonca", "A. Wasowski", "K. Czarnecki"], "venue": "In Proceedings of the 13th International Software Product  Line Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Efficient compilation techniques for large scale feature models", "author": ["M. Mendonca", "A. Wasowski", "K. Czarnecki", "D. Cowan"], "venue": "In Proceedings of the 7th International Conference on Generative Programming and Component Engineering,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Upper and lower bounds for weak backdoor set detection", "author": ["N. Misra", "S. Ordyniak", "V. Raman", "S. Szeider"], "venue": "In Theory and Applications of Satisfiability Testing\u2013SAT", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models", "author": ["R. Pohl", "K. Lauenroth", "K. Pohl"], "venue": "In Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Measuring the structural complexity of feature models", "author": ["R. Pohl", "V. Stricker", "K. Pohl"], "venue": "In Automated Software Engineering (ASE),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Automated generation of computationally hard feature models using evolutionary algorithms", "author": ["S. Segura", "J.A. Parejo", "R.M. Hierons", "D. Benavides", "A.R. Cort\u00e9s"], "venue": "Expert Syst. Appl.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Is the linux kernel a software product line", "author": ["J. Sincero", "H. Schirmeier", "W. Schr\u00f6der-Preikschat", "O. Spinczyk"], "venue": "In Proc. SPLC Workshop on Open Source Software and Product Lines,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Backdoor sets for DLL subsolvers", "author": ["S. Szeider"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Feature consistency in compile-time-configurable system software: Facing the linux 10,000 feature problem", "author": ["R. Tartler", "D. Lohmann", "J. Sincero", "W. Schr\u00f6der-Preikschat"], "venue": "In Proceedings of the Sixth Conference on Computer Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Reasoning about edits to feature models", "author": ["T. Thum", "D. Batory", "C. Kastner"], "venue": "In Proceedings of the 31st International Conference on Software Engineering,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "sharpsat: Counting models with advanced component caching and implicit bcp", "author": ["M. Thurley"], "venue": "In Proceedings of the 9th International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Backdoors to typical case complexity", "author": ["R. Williams", "C.P. Gomes", "B. Selman"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}], "referenceMentions": [{"referenceID": 12, "context": "Feature models (FM) are widely used to represent the variablility and commonality in product lines and reusable software, first introduced in 1990 [14].", "startOffset": 147, "endOffset": 151}, {"referenceID": 8, "context": "The process of feature modeling helps to ensure that relevant features are reusable, and unused features are removed to lower the complexity of the system under design [9].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "More recently, inspired by the success of SAT solvers in other domains, many researchers proposed the use of solvers to analyze feature models [2, 3, 4].", "startOffset": 143, "endOffset": 152}, {"referenceID": 2, "context": "More recently, inspired by the success of SAT solvers in other domains, many researchers proposed the use of solvers to analyze feature models [2, 3, 4].", "startOffset": 143, "endOffset": 152}, {"referenceID": 3, "context": "More recently, inspired by the success of SAT solvers in other domains, many researchers proposed the use of solvers to analyze feature models [2, 3, 4].", "startOffset": 143, "endOffset": 152}, {"referenceID": 9, "context": "Our findings are consistent with the idea of backdoors posited by theorists to explain the power of SAT solvers [10, 29, 25, 20] on real-world SAT instances.", "startOffset": 112, "endOffset": 128}, {"referenceID": 27, "context": "Our findings are consistent with the idea of backdoors posited by theorists to explain the power of SAT solvers [10, 29, 25, 20] on real-world SAT instances.", "startOffset": 112, "endOffset": 128}, {"referenceID": 23, "context": "Our findings are consistent with the idea of backdoors posited by theorists to explain the power of SAT solvers [10, 29, 25, 20] on real-world SAT instances.", "startOffset": 112, "endOffset": 128}, {"referenceID": 18, "context": "Our findings are consistent with the idea of backdoors posited by theorists to explain the power of SAT solvers [10, 29, 25, 20] on real-world SAT instances.", "startOffset": 112, "endOffset": 128}, {"referenceID": 20, "context": "[22], we performed experiments to see if the treewidth of graphs of the formulas correlates with solver running time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Additionally, we want to emphasize that all the FMs in our experiments are obtained from a diverse set of large real-world applications [6, 7], including a large FM based off the Linux kernel configuration model.", "startOffset": 136, "endOffset": 142}, {"referenceID": 6, "context": "Additionally, we want to emphasize that all the FMs in our experiments are obtained from a diverse set of large real-world applications [6, 7], including a large FM based off the Linux kernel configuration model.", "startOffset": 136, "endOffset": 142}, {"referenceID": 1, "context": "reduction from feature models to SAT [2].", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "Many specialized solvers [12, 13, 27] for FM analysis have been built that use SAT solvers as a backend.", "startOffset": 25, "endOffset": 37}, {"referenceID": 25, "context": "Many specialized solvers [12, 13, 27] for FM analysis have been built that use SAT solvers as a backend.", "startOffset": 25, "endOffset": 37}, {"referenceID": 5, "context": "We assume the reader is familiar with the translation of FMs to Boolean formulas in conjunctive normal form (CNF), which is explained in many papers [6, 7].", "startOffset": 149, "endOffset": 155}, {"referenceID": 6, "context": "We assume the reader is familiar with the translation of FMs to Boolean formulas in conjunctive normal form (CNF), which is explained in many papers [6, 7].", "startOffset": 149, "endOffset": 155}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], these real-world feature models have significantly different characteristics than those of the randomly gener-", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "ated models used in previous studies [18, 22].", "startOffset": 37, "endOffset": 45}, {"referenceID": 20, "context": "ated models used in previous studies [18, 22].", "startOffset": 37, "endOffset": 45}, {"referenceID": 16, "context": "[18] generated models with cross-tree constraint ratio (CTCR) of maximum 30%, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "higher CTCR, ranging from 46% to 96% [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 14, "context": "Figure 1 shows the solving times of the real-world feature models with the Sat4j solver [16] (version 2.", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "ing lazy data structures, and conflict-based adaptive branching called VSIDS [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "It is well-known that such random instances are hard for a CDCL SAT solver to solve [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "We ran the feature models with sharpSAT [28], a tool for counting the exact number of solutions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "[22] show treewidth of the CNF representation of randomly-generated FMs to be strongly correlated with their corresponding solving times.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": ", that treewidth of input Boolean formulas is not strongly correlated with running time of solvers and is not indicative of an instance\u2019s hardness [17].", "startOffset": 147, "endOffset": 151}, {"referenceID": 27, "context": "Recall [29, 11] that a strong (C) backdoor of a Boolean formula F is a set B of variables such that every assignment to the set B of variables (and simplification of F with the assignment) results in a formula in (C).", "startOffset": 7, "endOffset": 15}, {"referenceID": 10, "context": "Recall [29, 11] that a strong (C) backdoor of a Boolean formula F is a set B of variables such that every assignment to the set B of variables (and simplification of F with the assignment) results in a formula in (C).", "startOffset": 7, "endOffset": 15}, {"referenceID": 1, "context": "By a reduction from the Hitting Set problem, it can be shown that finding weak E backdoor of size at most k is not only NP -complete, but complete for the parameterized complexity class W [2] for general CNF formulas.", "startOffset": 188, "endOffset": 191}, {"referenceID": 10, "context": "As we could not find a theorem for this in the backdoor literature [11, 20, 25] we give a theorem for completeness.", "startOffset": 67, "endOffset": 79}, {"referenceID": 18, "context": "As we could not find a theorem for this in the backdoor literature [11, 20, 25] we give a theorem for completeness.", "startOffset": 67, "endOffset": 79}, {"referenceID": 23, "context": "As we could not find a theorem for this in the backdoor literature [11, 20, 25] we give a theorem for completeness.", "startOffset": 67, "endOffset": 79}, {"referenceID": 1, "context": "Determining if a given formula on n variables has a weak E backdoor of size at most k is W [2]complete for general formulas, and is fixed-parameter tractable with an O(d) algorithm for d-CNF formulas.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Determining if a given formula on n variables, has at most k restricted variables or at least n\u2212 k strong S backdoor set is W [2]-complete for general formulas, and is fixed-parameter tractable with an O(d) algorithm for d-CNF formulas.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "Validity of FMs used for the Experiments: The studied collection of FMs are large real-world feature models [8], primarily based on software product lines.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "is known to produce some of the most complex configuration constraints [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "We rely on existing work that reverse engineered the semantics of these languages from documentation and configuration tools and specified the semantics formally [7, 8].", "startOffset": 162, "endOffset": 168}, {"referenceID": 7, "context": "We rely on existing work that reverse engineered the semantics of these languages from documentation and configuration tools and specified the semantics formally [7, 8].", "startOffset": 162, "endOffset": 168}, {"referenceID": 14, "context": "Validity of Choice of Solvers: All our experiments were performed using the Sat4j solver [16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "[3] give a comprehensive survey of techniques for analyzing feature models, such as checking consistency and detecting dead features, and for supporting configuration, such as propagating feature selections.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Batory [2] was first to suggest the use of SAT solvers to support feature model analyses.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "[4] compare the performance of SAT, CSP, and BDD solvers on two sample analyses on randomly generate feature models with up to 300 features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[21] run similar comparison on feature models from the SPLOT collection,", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] achieve", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] show that SAT-based analyses scale to randomly generated feature models with up to 10000 features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Although SAT solvers have been successfully applied to analyze the feature model of the Linux kernel [26], we are unaware of prior work systematically studying the performance of SAT solvers on large (1000+ features) real-world feature models, which is", "startOffset": 101, "endOffset": 105}, {"referenceID": 16, "context": "[18] have studied the SAT solving behavior of randomly generated 3-SAT feature models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] treat finding hard feature models as an optimization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] propose graph width measures, such as treewidth, applied to graph representations of the CNF derived from a feature model as a upper bound of the complexity of analyses on the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "While some papers hint at the existence of very large models in industry [6], these models are typically highly confidential.", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "Sincero [24] was first to observe that the definition of the Linux kernel build configuration, ex-", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "[7] identify the Component Configuration Language (CDL) in eCos, an open-source real-time operating system, as additional feature modeling language and subsequently [8] create and analyze a collection of large real-world feature models from twelve open-source projects in the software systems domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] identify the Component Configuration Language (CDL) in eCos, an open-source real-time operating system, as additional feature modeling language and subsequently [8] create and analyze a collection of large real-world feature models from twelve open-source projects in the software systems domain.", "startOffset": 165, "endOffset": 168}], "year": 2017, "abstractText": "Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-toanalyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known nonbacktracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions. We explain the connection between our findings and backdoors, an idea posited by theorists to explain the power of SAT solvers. This connection strengthens our hypothesis that SAT-based analysis of FMs is easy. In contrast to our findings, previous research characterizes the difficulty of analyzing randomly-generated FMs in terms of treewidth. Our experiments suggest that the difficulty of analyzing realworld FMs cannot be explained in terms of treewidth. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SPLC 2015, July 20 24, 2015, Nashville, TN, USA c \u00a9 2015 ACM. ISBN 978-1-4503-3613-0/15/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2791060.2791070 CCS Concepts \u2022Software and its engineering \u2192 Software product lines;", "creator": "LaTeX with hyperref package"}}}