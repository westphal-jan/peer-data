{"id": "1506.05424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization", "abstract": "dormammu This paper introduces a jingwen high - performance meny hybrid algorithm, menshevik called ernouf Hybrid hasrat Hypervolume crutch Maximization Algorithm (H2MA ), for multi - u.n.-monitored objective osteogenesis optimization lousy that alternates metamorphosed between exploring burntwood the decision clariant space febs and w\u00f6rth exploiting 52-0 the barlinnie already turani obtained non - domestic dominated solutions. The proposal boli is munting centered wistron on jockeys maximizing breakfast the hesler hypervolume sitarist indicator, 1205 thus swithun converting kulakov the 1895-1897 multi - objective nicolai problem wila into a single - objective one. 14-second The exploitation 1,787 employs rammed gradient - based methods, roundheads but considering a heyward single candidate efficient solution at a crouched time, to overcome idrisov limitations blackhill associated crichlow with zongmin population - marinid based approaches premeditated and 7.7385 also saxicola to goffredo allow an easy control uncontentious of the tearle number of solutions provided. 44.88 There is housebuilders an b\u00fclach interchange 1975-77 between two lescarbot steps. sclerotic The spartak first step is nipah a jianchuan deterministic local comt\u00e9 exploration, endowed alper with chorske an roomies automatic malayan procedure gaer to detect domhnall stagnation. When bang stagnation gord is kalema detected, leontyev the despair search is switched to a dermot second step game-winning characterized by a stochastic eichendorff global 746,000 exploration using an televisi evolutionary algorithm. autobiographies Using five ZDT mark-to-market benchmarks with 30 malgieri variables, the luay performance catalanes of shamanov the 10.05 new algorithm manolescu is antipodes compared to osai state - of - pantai the - compliments art hardesty algorithms zaller for 52-47 multi - alignments objective optimization, more specifically 54.0 NSGA - strelitz II, quillen SPEA2, universit\u00e9 and SMS - gielen EMOA. shatranj The solutions found shakespearean by isospin the buf H2MA guide to macabre higher hypervolume punishers and four-fold smaller distance inul to uranium-233 the true deker Pareto frontier u2019ve with significantly less function backboard evaluations, elaina even when chamberings the gradient fellow is 2hrs estimated lozenge-shaped numerically. callinicus Furthermore, gd although only g\u00f3zd continuous knut decision pressurising spaces have shaktoi been kuqa considered japw here, discrete moderato decision parsons spaces could enya also tocs have kroner been hochfilzen treated, shosha replacing gradient - based 97.54 search by ovens hill - halcro climbing. transportacion Finally, a thorough explanation betrays is tapit provided 3,083 to krewson support the tssa expressive aruppukottai gain in performance .583 that was vidarbha achieved.", "histories": [["v1", "Wed, 17 Jun 2015 18:52:18 GMT  (271kb)", "http://arxiv.org/abs/1506.05424v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["conrado silva miranda", "fernando jos\\'e von zuben"], "accepted": false, "id": "1506.05424"}, "pdf": {"name": "1506.05424.pdf", "metadata": {"source": "CRF", "title": "Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization", "authors": ["Conrado S. Miranda", "Fernando J. Von Zuben"], "emails": ["conrado@dca.fee.unicamp.br", "vonzuben@dca.fee.unicamp.br"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n05 42\n4v 1\n[ cs\n.N E\n] 1\n7 Ju\nn 20\n15 1\nIndex Terms\u2014Exploration-exploitation algorithm; Gradientbased optimization; Hypervolume maximization; Multi-objective optimization.\nI. INTRODUCTION\nMULTI-OBJECTIVE optimization (MOO) is a general-ization of the standard single-objective optimization to problems where multiple criteria are defined and they conflict with each other [1]. In this case, there can be multiple optimal solutions with different trade-offs between the objectives. Since the optimal set can be continuous, an MOO problem is given by finding samples from the optimal set, called Pareto set. However, we also wish that the projection of the obtained samples of the Pareto set into the objective space be welldistributed along the Pareto frontier, which is the counterpart for the Pareto set, so that the solutions present more diverse trade-offs.\nThe current state-of-the-art for MOO relies on the use of evolutionary algorithms for finding the desired samples [2]. One of these algorithms is the NSGA-II [3], which performs non-dominance sorting, thus ordering the proposed solutions according to their relative dominance degree, and dividing the\nC. S. Miranda and F. J. Von Zuben are with the School of Electrical and Computer Engineering, University of Campinas (Unicamp), Brazil. E-mail: {conrado,vonzuben}@dca.fee.unicamp.br\nsolution set in subsequent frontiers of non-dominated solutions. NSGA-II also uses crowding distance, which measures how close the nearby solutions are, to maintain diversity in the objective space. Another well-known algorithm is the SPEA2 [4], where the solutions have a selective pressure to move towards the Pareto frontier and also to stay away from each other.\nThese algorithms are based on heuristics to define what characterizes a good set of solutions. However, the hypervolume indicator [5] defines a metric of performance for a set of solutions, thus allowing a direct comparison of multiple distinct sets of solutions [6], with higher values indicating possible better quality. The hypervolume is maximal at the Pareto frontier and increases if the samples are better distributed along the frontier [7]. Due to these properties, it represents a good candidate to be maximized in MOO, being explicitly explored in the SMS-EMOA [8], where solutions that contribute the least to the hypervolume are discarded.\nOn the other hand, local search methods have been successful in single-objective optimization (SOO) due to their efficiency in finding a local optimum for some problems [9], [10], so that research has been performed to try to adapt these methods for MOO problems. For instance, [11] defined a method for finding all minimizing directions in a MOO problem, but the proposed algorithm achieved low performance on usual benchmark functions.\nAlternatively, instead of adapting the single-objective methods to work on MOO problems, we can create a SOO problem associated with the MOO one, such that a good solution for the single-objective case is a good solution for the multi-objective case. Since the hypervolume is able to describe how good a population is, based on a single indicator, the MOO problem can be converted into the maximization of the population\u2019s hypervolume.\nBased on this idea, [12] proposed a method to compute the hypervolume\u2019s gradient for a given population, so that the optimal search direction for each individual could be established. However, [13] showed that adjusting the population through integration of the hypervolume\u2019s gradient not always work, with some initially non-dominated points becoming dominated and others changing very little over the integration.\nIn this paper, we introduce an algorithm for maximizing the hypervolume by optimizing one point at a time, instead of adjusting a whole population at once. The algorithm alternates between exploring the space for non-dominated solutions and, when they are found, exploiting them using local search methods to maximize the populations\u2019 hypervolume when only this active point can be moved. Therefore, once the hypervolume has converged, which is guaranteed to happen\n2 because the problem is bounded, the point is fixed in all further iterations. We found that this restriction is enough to overcome the issues presented in [13] when using the hypervolume\u2019s gradient. The proposed algorithm, called Hybrid Hypervolume Maximization Algorithm (H2MA), is a hybrid one, since it is composed of global exploration and local exploitation procedures, properly managed to be executed alternately.\nResults over the ZDT benchmark [14] show that the new algorithm performs better than the state-of-the-art evolutionary algorithms, both in terms of total hypervolume and distance to the Pareto frontier. Moreover, the algorithm was able to work deterministically in most of the benchmark problems, which makes it less susceptible to variations due to random number generation. Due to the high quality of the solutions found in less function evaluations than what is achieved by the current state-of-the-art, we consider that the new algorithm is a viable choice for solving MOO problems. Moreover, since a single solution is introduced at a time, the user is able to stop the algorithm when the desired number of solutions is found, while evolutionary algorithms must evolve the whole population at the same time.\nThis paper is organized as follows. Section II introduces the concepts of multi-objective optimization required, including the hypervolume indicator, and discusses the problems with the gradient-based approach for hypervolume maximization introduced in [13]. Section III provides the details of the new H2MA algorithm, and Section IV shows the comparison with the state-of-the-art algorithms. Finally, Section V summarizes the results and discusses future research direction."}, {"heading": "II. MULTI-OBJECTIVE OPTIMIZATION AND THE", "text": "HYPERVOLUME INDICATOR\nA multi-objective optimization problem is described by its decision space X and a set of objective functions fi(x) : X \u2192 Yi, i \u2208 {1, . . . ,M}, where Yi \u2286 R is the associated objective space for each objective function [15]. Due to the symmetry between maximization and minimization, only the minimization problem is considered here. Each point x in the decision space has a counterpart in the objective space Y = Y1\u00d7\u00b7 \u00b7 \u00b7\u00d7YM given by y = f(x) = (f1(x), . . . , fM (x)).\nSince there are multiple objectives, a new operator for comparing solutions must be used, since the conventional \u201cless than\u201d operator < can only compare two numbers. This operator is denoted the dominance operator and is defined as follows.\nDefinition 1 (Dominance). Let y and y\u2032 be points in Y , the objective space. Then y dominates y\u2032, denoted y \u227a y\u2032, if yi < y\u2032 i for all i.\nFrom this definition, a point y that dominates another point y\u2032 is better than y\u2032 in all objectives. Thus, there is no reason someone would choose y\u2032 over y, and it can be discarded, as occurs in many multi-objective optimization algorithms [15]. Note that there are other definitions of the dominance operator [6], where one considers the inequality \u2264 instead of the strict inequality < used here. However, equality in some of the coordinates may be an issue when using the\nhypervolume indicator, such as when taking its derivative [12]. This is why the strict version is used in this work.\nUsing the dominance, we can define the set of points characterized by the fact that no other point can have better performance in all objectives.\nDefinition 2 (Pareto Set and Frontier). The Pareto set is defined by the set of all points in the decision space that are not dominated by any other point in the decision space, when using the objectives. That is, the Pareto set is given by P = {x \u2208 X | \u2204x\u2032 \u2208 X : f(x\u2032) \u227a f(x)}. The Pareto frontier is the associated set in the objective space, given by F = {f(x) | x \u2208 P}."}, {"heading": "A. The Hypervolume Indicator", "text": "In order to define the hypervolume indicator [5], we must first define the Nadir point, which is a point in the objective space that is dominated by every point in a set.\nDefinition 3 (Nadir Point). Let X = {x1, . . . , xN} \u2208 XN be a set of points in the decision space. Let z \u2208 RM be a point in the objective space. Then z is a valid Nadir point if, for all x \u2208 X and i \u2208 {1, . . . ,M}, we have that fi(x) < zi. Using Definition 1, this can be written as f(x) \u227a z.\nAgain, it is possible to allow equality in the definition of the Nadir point, just like in the definition of dominance. However, when equality is allowed, it is possible for some point to have a null hypervolume, which can guide to undesired decisions when using the hypervolume as a performance metric, since such points would not contribute to the hypervolume and would be replaced by other points. Using the definition of a Nadir point, we can define the hypervolume for a set of points.\nDefinition 4 (Hypervolume). Let X = {x1, . . . , xN} \u2208 XN be a set of points in the decision space. Let z \u2208 RM be a valid Nadir point in the objective space. Then the hypervolume can be defined as:\nH(X ; z) =\n\u222b\nRM\n1[\u2203x \u2208 X : f(x) \u227a y \u227a z]dy, (1)\nwhere 1[\u00b7] is the indicator function.\nThe hypervolume measures how much of the objective space is dominated by a current set X and dominates the Nadir point z. Fig. 1 shows an example of the hypervolume for a set of three non-dominated points. For each point, the shaded region represents the area dominated by the given point, with colors combining when there is overlap."}, {"heading": "B. Gradient of the Hypervolume", "text": "As stated earlier, since the hypervolume provides such a good indicator of performance in multi-objective problems, it can be used to transform the multi-objective problem into a single-objective one, characterized by the maximization of the hypervolume.\nAlthough such approach proved to be successful when using evolutionary algorithms as the optimization method [8], the same did not happen when using the hypervolume\u2019s gradient\n3 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\ny1\ny 2\nFigure 1: Example of hypervolume. The non-dominated solutions in the objective space are shown in black circles, and the Nadir point is shown in the black square. For each nondominated solution, the region between it and the Nadir point is filled, with colors combining when there is overlap, and the total hypervolume is given by the area of the shaded regions. Best viewed in color.\nto perform the optimization [13]. However, it is well-known that gradient methods have been successful in single-objective optimization [9], [10], thus suggesting that they should be a reasonable choice for multi-objective optimization devoted to maximizing the hypervolume, since the hypervolume operator is well-defined almost everywhere in the objective space.\nThe hypervolume\u2019s gradient for a set of points was introduced in [12], and it can be used to compute the optimal direction in which a given point should move to increase the hypervolume associated with the current set of non-dominated solutions. Although the hypervolume is not a continuously differentiable function of its arguments, since dominated points do not contribute to the hypervolume and thus have null gradient, the gradient can be computed whenever any two points have different values for all objectives.\nBased on this motivation, [13] used the hypervolume\u2019s gradient as a guide for adjusting a set of points by numerical integration, that is, performing a small step in the direction pointed by the gradient. Even though the algorithm was able to achieve the Pareto set in some cases, it failed to converge to efficient points when some points got stuck along the iterative process, either because their gradients became very small or because they became dominated by other points. Once dominated, these points do not contribute to the hypervolume and remain fixed. This causes a major issue to using the hypervolume gradient in practice, since dominated points can be discarded, because there is no possibility to revert them to non-dominated points anymore, and the points with small gradients remain almost stagnant.\nIf we analyze Eq. (1), we can see that points at the border in the objective space are the only ones that can fill some portions of the objective space. On the other hand, points that are not at the border have less influence in the hypervolume, since part of the area dominated by them is also dominated by some other points. In the analysis presented in [13], it is clear that the cases where some points got stuck had higher gradients for the border points in the objective space, which led to the dominance or decrease of contribution of some or\nall central points. To make this idea clearer, consider the example in Fig. 1. If the point located at (0.75, 0.25) decreases its value on the second objective, it can increase the population\u2019s hypervolume. Moreover, it is the only point that can do so without competition for that portion of the space, since it is the point with the largest value for the first objective. The same holds for the point at (0.25, 0.75) and the first objective.\nHowever the point located at (0.5, 0.5) has to compete with the other two points to be the sole contributor for some regions. Therefore, its effect on the hypervolume is smaller, which leads to a smaller gradient. Furthermore, if less area is dominated by the middle point alone, which can occur during the points adjustment as the middle one moves less, then its influence becomes even smaller and it can become dominated.\nIt is important to highlight that this behavior does not happen always, but can occur along the iterative process, as shown in [13]. This leads to the base hypothesis for the algorithm developed in this paper: when using the hypervolume\u2019s gradient for optimization, the competition for increasing the hypervolume among points should be avoided."}, {"heading": "III. HYBRID HYPERVOLUME MAXIMIZATION ALGORITHM", "text": "From the discussion in Section II-B, one can see that the major problem when optimizing the hypervolume directly using its gradient may be the competition among points. Therefore, our proposed algorithm optimizes a single solution at a time, avoiding this competition.\nTheoretically, the algorithm can be described by choosing a new point that maximizes the hypervolume when taking into account the previous points, such that its recurring equation can be written as:\nxt = argmax x\u2208X H(Xt\u22121\u222a{x}), Xt = Xt\u22121\u222a{xt}, t \u2208 N, (2)\nwhere the initial set is given by X\u22121 = {}. Since a single point is being optimized at a time, the optimization becomes simpler and, as we will show in Section IV, requires less function evaluations. Moreover, one could argue that maintaining the previous set fixed reduces the flexibility allowed in comparison with a set where all the points are being concurrently adjusted. Although this may be true, we will also show in Section IV that the proposed algorithm performs well despite this loss of flexibility.\nThe algorithm described in Eq. (2) is theoretically ideal, since finding the maximum is hard in practice. Therefore, the actual algorithm proposed is shown in Fig. 2. This algorithm performs exploration of the objective space until a new solution that is not dominated by the previous candidate solutions is found. When it happens, the hypervolume of the whole set is larger than the hypervolume when considering only previous candidate solutions.\nThe new candidate solution is then exploited to maximize the total hypervolume and, after convergence, is then added to the existing set. It is important to highlight that the exploitation phase cannot make the solution become dominated, since that would reduce the hypervolume in comparison with the initial condition. Therefore, the problem of points becoming\n4 Input: Objectives f Input: Design space X Input: Nadir point z Output: Set of candidate solutions X\nfunction HYBRIDGREEDYOPTIMIZER(f,X , z) Regions,X \u2190 CREATEINITIALREGION(f,X ) while not stop condition and |Regions| > 0 do R \u2190 Regions.pop() \u22b2 Removes the region with the largest volume\nx0 \u2190 EXPLOREDETERMINISTIC(f,X , R,X) if x0 is valid then\nx \u2190 EXPLOIT(f,X , x0, X, z) NewRegions \u2190 CREATEREGIONS(R,x, f ) Regions \u2190 Regions \u222aNewRegions X \u2190 X \u222a {x}\nend if end while while not stop condition do\nx0 \u2190 EXPLORESTOCHASTIC(f,X , X) x \u2190 EXPLOIT(f,X , x0, X, z) X \u2190 X \u222a {x}\nend while return X end function\ndominated during the exploitation is avoided. Furthermore, the exploitation is a traditional single-objective optimization, so that gradient methods can be used if the decision set X is continuous or hill-climbing methods can be used for discrete X .\nOnce finished the exploitation, the algorithm begins the exploration phase again. The exploration can be deterministic, based on regions of the objective space defined by previous solutions, or stochastic, where a stochastic algorithm, such as an evolutionary algorithm, is used to find the new candidate. When a non-dominated candidate is found, the algorithm turns to exploitation again.\nWe highlight that the deterministic exploitation algorithm proposed is based on the definition of these regions, but other deterministic methods can be used. However, the algorithm must be able to establish when it is not able to provide\nInput: Objectives f Input: Design space X Output: Set of candidate solutions X Output: Initial exploration region R\nfunction CREATEINITIALREGION(f,X ) X \u2190 {} x0 \u2190 X .mean \u22b2 Gets the average candidate for i = 1, . . . , |f | do\nx \u2190 MINIMIZE(fi, x0,X ) X \u2190 X \u222a {x}\nend for R \u2190 CREATEREGION(X,f ) return {R}, X\nend function\nfurther improvements, so that the change to the stochastic global exploration can be made. In the algorithm shown in Fig. 2, regions that do not provide a valid initial condition are discarded without creating new regions, so that eventually the algorithm can switch to the stochastic global exploration.\nThe algorithm for deterministic exploration is shown in Fig. 3. It combines the points used to create a given region in order to produce an initial condition and tries to minimize the distance between its objective value and a reference point. Once a non-dominated point is found, it is returned for exploitation. Although this simple optimization provided good results without requiring many function evaluations, other methods can be used to perform this exploration. Alternatively, one can also perform a stochastic exploration instead of a deterministic one, but this may have negative effects on the performance if the information provided by the output (region R) is not used, since a global search would be required.\nThe first region is created by finding points that minimize each objective separately, as shown in Fig. 4. This establishes that the initial region will have a number of candidate solutions associated with it equal to the number of objectives, so that the solutions are at the border of the region.\nWhen new regions are created after exploitation, we ignore the solutions that created the region, one at a time, and replace it with the proposed new solution, as shown in Fig. 5, to create a new region. This guarantees that the number of solutions for each region is kept equal to the number of objectives.\nFinally, Fig. 6 shows how a region is created. If a region\n5 0 0.2 0.4 0.6 0.8 1 1.2 0 0.2 0.4 0.6 0.8 1 f(x1) f(x2) f(x12) f(x\u2032) y 12\nf1(\u00b7)\nf 2 (\u00b7 )\nR\n(a) Deterministic exploration\n0 0.2 0.4 0.6 0.8 1 1.2 0\n0.2\n0.4\n0.6\n0.8\n1\nf(x1)\nf(x2)\nf(x\u2032)\nf(x\u2217)\nH\u2217\nH \u2032\nf1(\u00b7)\nf 2 (\u00b7 )\nR1\nR2\n(b) Exploitation\nFigure 7: Deterministic exploration and exploitation steps of the new algorithm in an example problem. The Pareto frontier is shown in the blue line, and the regions used by the deterministic exploration are shown in yellow.\ndoes not have a volume, then at least one objective for two solutions is the same. Although we could allow such region to exist without modifying the rest of the algorithm, these regions tend to not provide good candidates for exploitation and delay the change to stochastic global exploration. Furthermore, one can even prohibit regions with volume smaller than some known constant, as they probably will not provide good exploitation points, and the change to stochastic global exploration happens earlier.\nFig. 7 shows a step of the algorithm in an example problem with two objectives. The deterministic exploration receives a region R, composed of the points x1 and x2. The mean of the points that compose the region is given by x12 = (x1+x2)/2 and its evaluation in the objective space is shown in Fig. 7a. The mean objective of the points that compose the region is also computed and is shown as y\n12 = (f(x1)+f(x2))/2. The\ndeterministic exploration is then defined by the problem\nmin x\u2208X \u2016f(x)\u2212 y 12 \u2016, (3)\nwhich uses x12 as the initial condition for the optimization. Since y\n12 is guaranteed to be non-dominated by f(x1) and\nf(x2), this should guide the search to the non-dominated region of the space.\nWhile performing this optimization, some intermediary points are evaluated, either while computing the numeric gradient or after performing a gradient descent step. The deterministic exploration stops as soon as a non-dominated point is found, which is given by f(x\u2032) in the example in Fig. 7a. Note that this example shows f(x12) as being dominated by f(x1) and f(x2), but it can also be non-dominated. In this case, x\u2032 = x12 and no optimization step for the problem in Eq. (3) is performed. Supposing no non-dominated point f(x\u2032) is found during the deterministic exploration, the region is simply discarded, without performing an exploitation step.\nUsing the point x\u2032, whose f(x\u2032) is non-dominated, provided by the deterministic or stochastic exploration, the exploitation is performed. Fig. 7b shows the hypervolume contributions for the initial point x\u2032 and the optimal point x\u2217, which maximizes the total hypervolume as in Eq. (2). Since x\u2032 is non-dominated, its hypervolume contribution H \u2032 is positive and the hypervolume gradient relative to the objectives is non-zero. After finding x\u2217 and if x\u2032 was provided by the deterministic exploration, new regions must be created to allow further exploration. Therefore, according to Fig. 5, the regions R1 = (x1, x\n\u2217) and R2 = (x2, x\u2217) are created for further exploration.\nThis finalizes a step of the algorithm, which is repeated until the given stop condition is not met. As at most one point is found by each step, the stop condition can be defined based on the number of desired points.\nNote that all the methods used in this algorithm assume that the optimization, either for exploitation or for minimizing one objective alone, requires an initial condition. This is true for hill climbing or gradient methods, but the algorithm can easily be modified if the optimization does not require it."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "To compare the algorithm proposed in Section III, called Hybrid Hypervolume Maximization Algorithm (H2MA), with\n6 \u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n15\n16\n17\n18\n19\n20\n21\n22\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nH\n(a) Hypervolume\n\u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n\u22123.5\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nlo g 1 0\nP\n(b) P-distance. Zero values not shown.\nFigure 8: 0th, 25th, 50th, 75th, and 100th percentiles every 2000 evaluations for the all algorithms on ZDT1.\nthe existing algorithms, the ZDT family of functions [14] was chosen. These functions define a common benchmark set in the multi-objective optimization literature, since they define a wide range of problems to test different characteristics of the optimization algorithm. All functions defined in [14] have a continuous decision space X , except for the ZDT5 which has a binary space. In this paper, only the continuous test functions were used to evaluate the performance of the new algorithm, and their equations are shown in the appendix.\nTable I provides a summary of the evaluation functions, their decision spaces, and the Nadir points used to compute the hypervolume. The Nadir points are defined by upper bounds of the objectives, which guarantees that the hypervolume computation is always valid, plus one, since not adding an extra value would mean that points at the border of the frontier would have no contribution to the hypervolume and would be avoided. In all instances, a total of n = 30 variables were considered, as common in the literature. The evolutionary algorithms\u2019 and evaluation functions\u2019 implementations were given by the PaGMO library [16].\nWe compare our algorithm with existing state-of-the-art multi-objective optimization algorithms, namely NSGA-II [3], SPEA2 [4], and SMS-EMOA [8]. All of them used a population size of 100 individuals. Tests have shown that this size is able to provide a good performance due to balance between exploration of the space and exploitation of the individuals, with much less individuals not providing good exploration and much more not providing good exploitation. The SMS-EMOA\ncan use two methods for selecting points in dominated fronts: the least hypervolume contribution or the domination count. Both methods were tested, with labels SMS1 and SMS2, respectively, in the following figures. Note that this method only applies for the dominated fronts, since the domination count is zero for all points in the non-dominated front and the least contributor method must be used. Furthermore, the SMS-EMOA algorithm\u2019s performance presented in this paper uses a dynamic Nadir point, which is found by adding one to the maximum over all points in each objective, since using the Nadir points presented in Table I created a very high selective pressure, which in turn led to poor exploration and performance.\nSince the decision space and objectives are continuous, the exploitation and deterministic exploration methods may resort to a gradient-based algorithm. In this paper, we used the LBFGS-B method implemented in the library SciPy [17], which is able to handle the bounds of X and is very efficient to find a local optimum. As the other algorithms being compared are evolutionary algorithms, which can only access the objective functions by evaluating them at given points, the gradient for the L-BFGS-B is computed numerically to avoid an unfair advantage in favor of our algorithm.\nFor the stochastic global exploration, we used an evolutionary algorithm with non-dominance sorting and removal based on the number of dominating points. The population had a minimum size of 20 and was filled with the given set of previous solutions X . If less than 20 points were provided, the others were created by randomly sampling the decision space X uniformly. Once a new point is introduced to the non-dominated front, it is returned for exploitation because it increases the hypervolume when added to the previous solutions X . The size of this population was chosen experimentally to provide a good enough exploration of the space toward the initial conditions for the exploitation. This size is smaller than the population size for the pure evolutionary algorithms\n7 \u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nH\n(a) Hypervolume\n\u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n\u22123.5\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nlo g 1 0\nP\n(b) P-distance. Zero values not shown.\nFigure 9: 0th, 25th, 50th, 75th, and 100th percentiles every 2000 evaluations for the all algorithms on ZDT2.\n\u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n16\n17\n18\n19\n20\n21\n22\n23\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nH\n(a) Hypervolume\n\u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nlo g 1 0\nP\n(b) P-distance. Zero values not shown.\nFigure 10: 0th, 25th, 50th, 75th, and 100th percentiles every 2000 evaluations for the all algorithms on ZDT3.\nbecause the pure evolutionary algorithm need diversity to explore and exploit all of its population, but the stochastic part of the H2MA is already initialized with good and diverse candidate solutions provided by the exploitation procedure, reducing its exploration requirements.\nBesides computing the solutions\u2019 hypervolume, which is the metric that the H2MA is trying to maximize and that provides a good method for comparing solutions, we can compute the distance between the achieved objectives and the Pareto frontier, since the Pareto frontiers for the ZDT functions are known. This defines the P-distance, which is zero, or close to zero due to numerical issues, for points at the frontier.\nFigs. 8, 9, 10, 11, and 12 present the results for the problems ZDT1, ZDT2, ZDT3, ZDT4, and ZDT6, respectively. A maximum of 20000 function evaluations was considered, and the graphs show the 0th, 25th, 50th, 75th, and 100th\npercentiles for each performance indicator over 100 runs of the algorithms. Since the P-distance is shown in log-scale, some values obtained by our proposal are absent or partially present, because they have produced zero P-distance.\nFrom ZDT1 to ZDT4, the H2MA never ran out of regions to explore, so the stochastic exploration was not used and all runs have the same performance. For the function ZDT6, the first objective, given by Eq. (8a), causes some problems to the deterministic exploration.\nDuring the creation of the first region for this problem, the mean point is used as initial condition for optimizing each objective, as shown in Fig. 4. However, the first objective for ZDT6 has null derivative when x1 = 0.5. In this case, even traditional higher-order methods would not help, since the first non-zero derivative of f1(x) is the sixth. As the first objective does not change in this case and it also has local minima that\n8 \u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nH\n(a) Hypervolume\n\u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n\u221212\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nlo g 1 0\nP\n(b) P-distance. Zero values not shown.\nFigure 11: 0th, 25th, 50th, 75th, and 100th percentiles every 2000 evaluations for the all algorithms on ZDT4.\n\u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n4\n6\n8\n10\n12\n14\n16\n18\n20\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nH\n(a) Hypervolume\n\u22120.5 0 0.5 1 1.5 2 2.5\nx 10 4\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nNSGA\u2212II SPEA2 SMS1 SMS2 H2MA\n# evaluations\nlo g 1 0\nP\n(b) P-distance. Zero values not shown.\nFigure 12: 0th, 25th, 50th, 75th, and 100th percentiles every 2000 evaluations for the all algorithms on ZDT6.\nare very hard to overcome, the algorithm quickly switches to using stochastic exploration. Once new regions have candidate points, the algorithm is able to exploit them.\nBesides this issue in the deterministic exploration of the problem ZDT6, the local minima of the first objective makes some candidate solutions be sub-optimal, increasing the Pdistance as shown in Fig. 12b. Nonetheless, the achieved Pdistance is better than the evolutionary algorithms and the 75th percentile is zero. Moreover, Figs. 8b, 9b, 10b, and 11b show that the candidate solutions are always on the Pareto frontier for the problems ZDT1 to ZDT4. This allows the user to stop the optimization at any number of evaluations, even with very few function evaluations, and have a reasonable expectation that the solutions found are efficient.\nWhen we evaluate the hypervolume indicator, we see that, for the problems ZDT1, ZDT2, ZDT4, and ZDT6, the per-\nformance of the H2MA is much better, even for the last one using stochastic exploration. Moreover, the H2MA\u2019s worst hypervolume was always better than the best hypervolume for all evolutionary algorithms and it was able to get closer to the maximum hypervolume possible with relatively few function evaluations, being a strong indication of its efficiency.\nFor the problem ZDT3, whose hypervolume performance is shown in Fig. 10a, the H2MA was generally better than the evolutionary algorithms. The Pareto frontier for ZDT3 is composed of disconnected sets of points, which was created to test the algorithm\u2019s ability to deal with discontinuous frontiers. Since the exploitation algorithm used for the results is gradient-based, it is not able to properly handle discontinuous functions, which is the case of the hypervolume on discontinuous frontiers. However, the deterministic exploration method is able to find points whose exploitation lay on the\n9 0 100 200 300 400 500 600 700 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 4 ZDT1 ZDT2 ZDT3 ZDT4\nAnalytic\nN um\ner ic\nFigure 13: Comparison between the number of function evaluations required to achieve the same hypervolume using numeric or analytic gradient. The dotted line represents a 30- fold improvement.\ndifferent parts of the Pareto frontier, providing the expected diversity.\nFig. 13 shows a comparison between the number of function evaluations required by the numeric and the analytic gradient to achieve the same hypervolume on the problems ZDT1 to ZDT4. The analytic method for computing the hypervolume\u2019s gradient is described in [12]. The comparison for ZDT6 is not shown due to its different scale, since many function evaluations are used in the global stochastic exploration because the deterministic exploration fails to find regions.\nAs expected, using the analytic gradient causes a 30-fold improvement in comparison to the numeric gradient, since the number of decision variables is 30. However, the gain is not linear. This can be explained by the difference in behavior during the deterministic exploration: the first nondominated point found is used to perform the exploitation, even if this point was found during the computation of the numeric gradient. For ZDT1 and ZDT4, this causes the new points found by the numeric gradient to be very close to the original points, reducing its performance and increasing the improvement of using the analytic gradient.\nMoreover, a similar effect makes the ZDT3 performance to have a lower improvement when using the analytic gradient. Since the Pareto frontier for ZDT3 is discontinuous and this causes a discontinuity in the hypervolume, these large changes can be seen by the numeric gradient because small changes in the variables can have large effects on the hypervolume, pulling the solution if the difference is significant, while the analytic gradient is not able to provide such knowledge. Nonetheless, the analytic gradient presents at least a 15-fold improvement over the numeric one over the ZDT3."}, {"heading": "A. Analysis of the H2MA\u2019s performance", "text": "As shown in Section IV, the proposed H2MA is able to surpass the state-of-the-art in multi-objective optimization, based on evolutionary algorithms. Therefore, it is important to analyze the algorithm and to discuss why this improvement happened.\nEvolutionary algorithms perform a guided exploration, with new individuals created based on existing high-performing individuals, which allows them to escape local minima but reduces the convergence speed. On the other hand, traditional optimization algorithms tend to find local minima quickly, but the optimal point achieved depends on the minima\u2019s regions of attraction.\nThese two kind of algorithms have complementary natures, which makes them good candidates for creating a hybrid algorithm: the evolutionary algorithm explores the space and provides initial conditions for the local optimization, which then finds minima quickly. Although this does create better results, it only explains the performance on the ZDT6 problem, since the other problems did not enter the stochastic phase.\nIn order to understand the algorithm\u2019s behavior, we must keep in mind that each new point added by the algorithm is solving a very different problem. Since the previous points that are considered during the hypervolume optimization change as more points are added, the objective surface for each new point is different from the previous ones and takes into account the already achieved portion of the hypervolume. To visualize this, supposed that the hypervolume\u2019s gradient is defined over previously found points and one previous solution is used as the initial condition for the gradient-based exploitation to find a new point to be added to the solution set. Although the initial condition was a local optimum for a previous problem, it is not a local optimum to the current problem, because any small change that creates a non-dominated point will improve the total hypervolume. Therefore, we do not need to worry about the new optimization converging to a previous solution point because the problem landscape is different and different local minima will be found, increasing the total hypervolume. The deterministic exploration is only required because the hypervolume\u2019s gradient is not defined at the border of the hypervolume, so a new independent point must be found.\nThis explains the performance improvement over ZDT1 to ZDT4, because every added point improves the hypervolume as much as it can do locally, so that an improvement is guaranteed to happen. Evolutionary algorithms, on the other hand, use function evaluations without guarantees of improvement of the total hypervolume, since dominated solutions can be found.\nMoreover, although a local optimum found during exploitation may not be an efficient solution due to irregularities in the objective surface, the experiments show that this is not the case most of the time, since the P-distance of the solutions found are generally zero. This result is expected, since the hypervolume is maximal when computed over points in the Pareto set, and the performance on all ZDT problems provide support to this claim.\nWe must highlight that we are not saying that evolutionary algorithms should not be used at all, but that they should\n10\nbe applied whenever traditional optimization methods are not able to solve the problem. This is the case of the ZDT6, for instance, where an evolutionary algorithm was required to provide initial conditions for the exploitation. We consider very important to have alternative methods that are better on a subset of the problems and to use them when a problem from such subset is present. This is exactly what the H2MA does: when the traditional optimization is not able to find an answer, which indicates that the problem is outside of the subset with which it can deal, an evolutionary algorithm, which is able to handle a superset class of problems, is used until the problem becomes part of the subset again, establishing a switching behavior that takes advantage of both algorithms."}, {"heading": "V. CONCLUSION", "text": "This paper proposed the Hybrid Hypervolume Maximization Algorithm (H2MA) for multi-objective optimization, which tries to maximize the hypervolume one point at a time. It first tries to perform deterministic local exploration and, when it gets stuck, it switches to stochastic global exploration using an evolutionary algorithm. The optimization algorithm used during deterministic optimization is problem-dependent and can be given by a gradient-based method, when the decision space is continuous, or a hill-climbing method, when the decision space is discrete. Here we have explored solely continuous decision spaces.\nThe algorithm was compared with state-of-the-art algorithms for multi-objective optimization, namely NSGA-II, SPEA2, and SMS-EMOA on the ZDT problems. Despite using numeric gradient for the objective functions, which increases the number of function calls, the algorithm consistently provided a higher hypervolume for the same number of function evaluations when compared to the aforementioned evolutionary algorithms. Only for the ZDT3 the performance was slightly reduced due to the discontinuous nature of the Pareto frontier, which causes a discontinuity in the hypervolume, not properly handled by gradient-based methods.\nMoreover, for all problems except for ZDT6, all the solutions found by the algorithm were over the Pareto frontier, which makes them efficient solutions. For the ZDT6, the median case also had all solutions over the Pareto frontier, but the use of the stochastic exploration not always guided to a solution at the Pareto frontier. Nonetheless, the obtained solutions were better than those provided by the evolutionary algorithms. Moreover, the solutions provided for ZDT1 to ZDT4 achieved high performance using only the deterministic part of the algorithm.\nEvolutionary algorithms usually have better performance when their populations are larger, so that diverse individuals can be selected for crossover. However, most of the time people do not require many options, so the H2MA presents itself as an alternative choice for finding a good set of solutions at a lower computational cost in most problems, although it does not limit the computational burden and the number of points found. If the problem has more reasonable objectives than ZDT6, which was designed with an extreme case in mind, we can expect that many points will be found by the\ndeterministic mechanisms, which makes the algorithm more reliable. Moreover, the solutions found should be efficient, which is characterized by a low P-distance, and diverse on the objectives, which is characterized by a larger hypervolume when only efficient solutions are considered.\nFuture work should focus on using surrogates to reduce the number of evaluations [18], [19], [20]. Although the H2MA is very efficient on its evaluations, the numeric gradient may consume lots of evaluations and be unreliable for complicated functions, as their implementation can cause numerical errors larger than the step used. Using a surrogate, the gradient can be determined directly and less evaluations are required.\nAnother important research problem is to find a new algorithm for computing the hypervolume, since existing algorithms are mainly focused on computing the hypervolume given a set of points [21]. Since the solution set is constructed one solution at a time in the H2MA, a recursive algorithm that computes the hypervolume of X\u222a{x} given the hypervolume of X should reduce the computing requirement.\nAPPENDIX\nZDT1:\nf1(x) = x1 (4a)\nf2(x) = g(x)h(f1(x), g(x)) (4b)\ng(x) = 1 + 9\nn\u2212 1\nn \u2211\ni=2\nxi (4c)\nh(f1(x), g(x)) = 1\u2212 \u221a f1(x)/g(x) (4d)\nZDT2:\nf1(x) = x1 (5a)\nf2(x) = g(x)h(f1(x), g(x)) (5b)\ng(x) = 1 + 9\nn\u2212 1\nn \u2211\ni=2\nxi (5c)\nh(f1(x), g(x)) = 1\u2212 (f1(x)/g(x)) 2 (5d)\nZDT3:\nf1(x) = x1 (6a)\nf2(x) = g(x)h(f1(x), g(x)) (6b)\ng(x) = 1 + 9\nn\u2212 1\nn \u2211\ni=2\nxi (6c)\nh(f1(x), g(x)) = 1\u2212\n\u221a\nf1(x)\ng(x) \u2212 sin(10\u03c0f1(x))\nf1(x)\ng(x) (6d)\nZDT4:\nf1(x) = x1 (7a)\nf2(x) = g(x)h(f1(x), g(x)) (7b)\ng(x) = 1 + 10(n\u2212 1) + n \u2211\ni=2\n(x2 i \u2212 10 cos(4\u03c0xi))\n(7c)\nh(f1(x), g(x)) = 1\u2212 \u221a f1(x)/g(x) (7d)\n11\nZDT6:\nf1(x) = 1\u2212 exp(\u22124x1) sin 6(6\u03c0x1) (8a)\nf2(x) = g(x)h(f1(x), g(x)) (8b)\ng(x) = 1 + 9\n(\nn \u2211\ni=2\nxi n\u2212 1\n)0.25\n(8c)\nh(f1(x), g(x)) = 1\u2212 (f1(x)/g(x)) 2 (8d)"}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank CNPq for the financial support."}], "references": [{"title": "Nonlinear Multiobjective Optimization", "author": ["K. Miettinen"], "venue": "Springer US,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Multi-Objective Optimization Using Evolutionary Algorithms", "author": ["K. Deb"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["K. Deb", "A. Pratap", "S. Agarwal", "T.A.M.T. Meyarivan"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 6, no. 2, pp. 182\u2013197, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "SPEA2: Improving the strength Pareto evolutionary algorithm", "author": ["E. Zitzler", "M. Laumanns", "L. Thiele"], "venue": "2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "The hypervolume indicator revisited: On the design of Pareto-compliant indicators via weighted integration", "author": ["E. Zitzler", "D. Brockhoff", "L. Thiele"], "venue": "Evolutionary multi-criterion optimization. Springer, 2007, pp. 862\u2013876.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Performance Assessment of Multiobjective Optimizers: An Analysis and Review", "author": ["E. Zitzler", "L. Thiele", "M. Laumanns", "C.M. Fonseca", "V.G. Da Fonseca"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 7, no. 2, pp. 117\u2013132, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Theory of the hypervolume indicator: optimal \u03bc-distributions and the choice of the reference point", "author": ["A. Auger", "J. Bader", "D. Brockhoff", "E. Zitzler"], "venue": "Proceedings of the tenth ACM SIGEVO workshop on Foundations of genetic algorithms. ACM, 2009, pp. 87\u2013102.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "SMS-EMOA: Multiobjective selection based on dominated hypervolume", "author": ["N. Beume", "B. Naujoks", "M. Emmerich"], "venue": "European Journal of Operational Research, vol. 181, no. 3, pp. 1653\u20131669, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "On gradients and hybrid evolutionary algorithms for real-valued multiobjective optimization", "author": ["P.A.N. Bosman"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 16, no. 1, pp. 51\u201369, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Time Complexity and Zeros of the Hypervolume Indicator Gradient Field", "author": ["M. Emmerich", "A. Deutz"], "venue": "EVOLVE-A Bridge between Probability, Set Oriented Numerics, and Evolutionary Computation III. Springer, 2014, pp. 169\u2013193.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Hypervolume Maximization via Set Based Newton\u2019s Method", "author": ["V.A.S. Hern\u00e1ndez", "O. Sch\u00fctze", "M. Emmerich"], "venue": "EVOLVE-A Bridge between Probability, Set Oriented Numerics, and Evolutionary Computation V. Springer, 2014, pp. 15\u201328.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of Multiobjective Evolutionary Algorithms: Empirical Results", "author": ["E. Zitzler", "K. Deb", "L. Thiele"], "venue": "Evolutionary Computation, vol. 8, no. 2, pp. 173\u2013195, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-objective optimization", "author": ["K. Deb"], "venue": "Search methodologies. Springer, 2014, pp. 403\u2013449.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A global optimisation toolbox for massively parallel engineering optimisation", "author": ["F. Biscani", "D. Izzo", "C.H. Yam"], "venue": "arXiv preprint arXiv:1004.3824, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "SciPy: Open source scientific tools for Python", "author": ["E. Jones", "T. Oliphant", "P. Peterson"], "venue": "2001\u2013, [Online; accessed 2015-05-10]. [Online]. Available: http://www.scipy.org/", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "A Comprehensive Survey of Fitness Approximation in Evolutionary Computation", "author": ["Y. Jin"], "venue": "Soft computing, vol. 9, no. 1, pp. 3\u201312, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Meta-Modeling in Multiobjective Optimization", "author": ["J. Knowles", "H. Nakayama"], "venue": "Multiobjective Optimization. Springer, 2008, pp. 245\u2013284.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-objective Optimization Using Surrogates", "author": ["I. Voutchkov", "A. Keane"], "venue": "Computational Intelligence in Optimization. Springer, 2010, pp. 155\u2013175.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "MULTI-OBJECTIVE optimization (MOO) is a generalization of the standard single-objective optimization to problems where multiple criteria are defined and they conflict with each other [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 1, "context": "The current state-of-the-art for MOO relies on the use of evolutionary algorithms for finding the desired samples [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "One of these algorithms is the NSGA-II [3], which performs non-dominance sorting, thus ordering the proposed solutions according to their relative dominance degree, and dividing the", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Another well-known algorithm is the SPEA2 [4], where the solutions have a selective pressure to move towards the Pareto frontier and also to stay away from each other.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "However, the hypervolume indicator [5] defines a metric of performance for a set of solutions, thus allowing a direct comparison of multiple distinct sets of solutions [6], with higher values indicating possible better quality.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "However, the hypervolume indicator [5] defines a metric of performance for a set of solutions, thus allowing a direct comparison of multiple distinct sets of solutions [6], with higher values indicating possible better quality.", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "The hypervolume is maximal at the Pareto frontier and increases if the samples are better distributed along the frontier [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "Due to these properties, it represents a good candidate to be maximized in MOO, being explicitly explored in the SMS-EMOA [8], where solutions that contribute the least to the hypervolume are discarded.", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "For instance, [11] defined a method for finding all minimizing directions in a MOO problem, but the proposed algorithm achieved low performance on usual benchmark functions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "Based on this idea, [12] proposed a method to compute the hypervolume\u2019s gradient for a given population, so that the optimal search direction for each individual could be established.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "However, [13] showed that adjusting the population through integration of the hypervolume\u2019s gradient not always work, with some initially non-dominated points becoming dominated and others changing very little over the integration.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "We found that this restriction is enough to overcome the issues presented in [13] when using the hypervolume\u2019s gradient.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Results over the ZDT benchmark [14] show that the new algorithm performs better than the state-of-the-art evolutionary algorithms, both in terms of total hypervolume and distance to the Pareto frontier.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "Section II introduces the concepts of multi-objective optimization required, including the hypervolume indicator, and discusses the problems with the gradient-based approach for hypervolume maximization introduced in [13].", "startOffset": 217, "endOffset": 221}, {"referenceID": 12, "context": ",M}, where Yi \u2286 R is the associated objective space for each objective function [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Thus, there is no reason someone would choose y over y, and it can be discarded, as occurs in many multi-objective optimization algorithms [15].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "Note that there are other definitions of the dominance operator [6], where one considers the inequality \u2264 instead of the strict inequality < used here.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "However, equality in some of the coordinates may be an issue when using the hypervolume indicator, such as when taking its derivative [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 4, "context": "In order to define the hypervolume indicator [5], we must first define the Nadir point, which is a point in the objective space that is dominated by every point in a set.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Although such approach proved to be successful when using evolutionary algorithms as the optimization method [8], the same did not happen when using the hypervolume\u2019s gradient", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "to perform the optimization [13].", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "The hypervolume\u2019s gradient for a set of points was introduced in [12], and it can be used to compute the optimal direction in which a given point should move to increase the hypervolume associated with the current set of non-dominated solutions.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "Based on this motivation, [13] used the hypervolume\u2019s gradient as a guide for adjusting a set of points by numerical integration, that is, performing a small step in the direction pointed by the gradient.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "In the analysis presented in [13], it is clear that the cases where some points got stuck had higher gradients for the border points in the objective space, which led to the dominance or decrease of contribution of some or all central points.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "It is important to highlight that this behavior does not happen always, but can occur along the iterative process, as shown in [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "(4) [0, 1] (2, 11) ZDT2 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(5) [0, 1] (2, 11) ZDT3 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(6) [0, 1] (2, 11) ZDT4 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(7) [0, 1]\u00d7 [\u22125, 5]n\u22121 (2, 2 + 50(n \u2212 1)) ZDT6 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(8) [0, 1] (2, 11)", "startOffset": 4, "endOffset": 10}, {"referenceID": 11, "context": "the existing algorithms, the ZDT family of functions [14] was chosen.", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "All functions defined in [14] have a continuous decision space X , except for the ZDT5 which has a binary space.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "The evolutionary algorithms\u2019 and evaluation functions\u2019 implementations were given by the PaGMO library [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "We compare our algorithm with existing state-of-the-art multi-objective optimization algorithms, namely NSGA-II [3], SPEA2 [4], and SMS-EMOA [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "We compare our algorithm with existing state-of-the-art multi-objective optimization algorithms, namely NSGA-II [3], SPEA2 [4], and SMS-EMOA [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "We compare our algorithm with existing state-of-the-art multi-objective optimization algorithms, namely NSGA-II [3], SPEA2 [4], and SMS-EMOA [8].", "startOffset": 141, "endOffset": 144}, {"referenceID": 14, "context": "In this paper, we used the LBFGS-B method implemented in the library SciPy [17], which is able to handle the bounds of X and is very efficient to find a local optimum.", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "The analytic method for computing the hypervolume\u2019s gradient is described in [12].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Future work should focus on using surrogates to reduce the number of evaluations [18], [19], [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "Future work should focus on using surrogates to reduce the number of evaluations [18], [19], [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Future work should focus on using surrogates to reduce the number of evaluations [18], [19], [20].", "startOffset": 93, "endOffset": 97}], "year": 2015, "abstractText": "This paper introduces a high-performance hybrid algorithm, called Hybrid Hypervolume Maximization Algorithm (H2MA), for multi-objective optimization that alternates between exploring the decision space and exploiting the already obtained non-dominated solutions. The proposal is centered on maximizing the hypervolume indicator, thus converting the multi-objective problem into a single-objective one. The exploitation employs gradient-based methods, but considering a single candidate efficient solution at a time, to overcome limitations associated with population-based approaches and also to allow an easy control of the number of solutions provided. There is an interchange between two steps. The first step is a deterministic local exploration, endowed with an automatic procedure to detect stagnation. When stagnation is detected, the search is switched to a second step characterized by a stochastic global exploration using an evolutionary algorithm. Using five ZDT benchmarks with 30 variables, the performance of the new algorithm is compared to state-of-the-art algorithms for multi-objective optimization, more specifically NSGA-II, SPEA2, and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume and smaller distance to the true Pareto frontier with significantly less function evaluations, even when the gradient is estimated numerically. Furthermore, although only continuous decision spaces have been considered here, discrete decision spaces could also have been treated, replacing gradient-based search by hill-climbing. Finally, a thorough explanation is provided to support the expressive gain in performance that was achieved.", "creator": "LaTeX with hyperref package"}}}