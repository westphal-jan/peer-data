{"id": "1506.06081", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements", "abstract": "We simple propose neg a stators simple, wordpress scalable, rooivalk and bhattacharjee fast dolphins gradient social-emotional descent algorithm seersucker to chretien optimize execute a seiyaku nonconvex surman objective for kalix the virtually rank kalabari minimization problem directx and rodger a thurgarton closely related family klokot of reagans semidefinite extant programs. procrastinators With $ fiber-based O (r ^ 2 \\ khoshkeh kappa ^ digg 2 nunno n \\ log tcpa n) $ random measurements of avatars a blechnum positive semidefinite $ n \\ times ioi n $ matrix wew of rank $ astp r $ eie and nosotros condition number $ \\ kappa $, our ecclesiastic method kumars is guaranteed khumalo to campionato converge rhb linearly vibration to the formalise global 3,844 optimum.", "histories": [["v1", "Fri, 19 Jun 2015 16:41:08 GMT  (94kb,D)", "https://arxiv.org/abs/1506.06081v1", null], ["v2", "Thu, 8 Oct 2015 22:46:11 GMT  (95kb,D)", "http://arxiv.org/abs/1506.06081v2", "Sample complexity updated. Accepted to NIPS 2015"], ["v3", "Thu, 24 Mar 2016 16:27:51 GMT  (95kb,D)", "http://arxiv.org/abs/1506.06081v3", "Fix a minor error in Appendix E"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qinqing zheng", "john d lafferty"], "accepted": true, "id": "1506.06081"}, "pdf": {"name": "1506.06081.pdf", "metadata": {"source": "CRF", "title": "A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements", "authors": ["Qinqing Zheng", "John Lafferty"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Semidefinite programming has become a key optimization tool in many areas of applied mathematics, signal processing and machine learning. SDPs often arise naturally from the problem structure, or are derived as surrogate optimizations that are relaxations of difficult combinatorial problems [7, 1, 8]. In spite of the importance of SDPs in principle\u2014promising efficient algorithms with polynomial runtime guarantees\u2014it is widely recognized that current optimization algorithms based on interior point methods can handle only relatively small problems. Thus, a considerable gap exists between the theory and applicability of SDP formulations. Scalable algorithms for semidefinite programming, and closely related families of nonconvex programs more generally, are greatly needed.\nA parallel development is the surprising effectiveness of simple classical procedures such as gradient descent for large scale problems, as explored in the recent machine learning literature. In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6]. In this paper we build on this work to develop first-order algorithms for solving the rank minimization problem under random measurements and a closely related family of semidefinite programs. Our algorithms are efficient and scalable, and we prove that they attain linear convergence to the global optimum under natural assumptions.\nThe affine rank minimization problem is to find a matrix X? \u2208 Rn\u00d7p of minimum rank satisfying constraints A(X?) = b, where A : Rn\u00d7p \u2212\u2192 Rm is an affine transformation. The underde-\nar X\niv :1\n50 6.\n06 08\n1v 3\n[ st\nat .M\nL ]\n2 4\nM ar\ntermined case where m np is of particular interest, and can be formulated as the optimization\nmin X\u2208Rn\u00d7p\nrank(X)\nsubject to A(X) = b. (1)\nThis problem is a direct generalization of compressed sensing, and subsumes many machine learning problems such as image compression, low rank matrix completion and low-dimensional metric embedding [18, 12]. While the problem is natural and has many applications, the optimization is nonconvex and challenging to solve. Without conditions on the transformation A or the minimum rank solution X?, it is generally NP hard [15].\nExisting methods, such as nuclear norm relaxation [18], singular value projection (SVP) [11], and alternating least squares (AltMinSense) [12], assume that a certain restricted isometry property (RIP) holds for A. In the random measurement setting, this essentially means that at least O(r(n + p) log(n + p)) measurements are available, where r = rank(X?) [18]. In this work, we assume that (i) X? is positive semidefinite and (ii) A : Rn\u00d7n \u2212\u2192 Rm is defined as A(X)i = tr(AiX), where each Ai is a random n\u00d7n symmetric matrix from the Gaussian Orthogonal Ensemble (GOE), with (Ai)jj \u223c N (0, 2) and (Ai)jk \u223c N (0, 1) for j 6= k. Our goal is thus to solve the optimization\nmin X 0\nrank(X)\nsubject to tr(AiX) = bi, i = 1, . . . ,m. (2)\nIn addition to the wide applicability of affine rank minimization, the problem is also closely connected to a class of semidefinite programs. In Section 2, we show that the minimizer of a particular class of SDP can be obtained by a linear transformation of X?. Thus, efficient algorithms for problem (2) can be applied in this setting as well.\nNoting that a rank-r solution X? to (2) can be decomposed as X? = Z?Z?> where Z? \u2208 Rn\u00d7r, our approach is based on minimizing the squared residual\nf(Z) = 1\n4m \u2225\u2225A(ZZ>)\u2212 b\u2225\u22252 = 1 4m m\u2211 i=1 ( tr(Z>AiZ)\u2212 bi )2 .\nWhile this is a nonconvex function, we take motivation from recent work for phase retrieval by Cande\u0300s et al. [6], and develop a gradient descent algorithm for optimizing f(Z), using a carefully constructed initialization and step size. Our main contributions concerning this algorithm are as follows.\n\u2022 We prove that withO(r3n log n) constraints our gradient descent scheme can exactly recover X? with high probability. Empirical experiments show that this bound may potentially be improved to O(rn log n).\n\u2022 We show that our method converges linearly, and has lower computational cost compared with previous methods.\n\u2022 We carry out a detailed comparison of rank minimization algorithms, and demonstrate that when the measurement matricesAi are sparse, our gradient method significantly outperforms alternative approaches.\nIn Section 3 we briefly review related work. In Section 4 we discuss the gradient scheme in detail. Our main analytical results are presented in Section 5, with detailed proofs contained in the supplementary material. Our experimental results are presented in Section 6, and we conclude with a brief discussion of future work in Section 7."}, {"heading": "2 Semidefinite Programming and Rank Minimization", "text": "Before reviewing related work and presenting our algorithm, we pause to explain the connection between semidefinite programming and rank minimization. This connection enables our scalable gradient descent algorithm to be applied and analyzed for certain classes of SDPs.\nConsider a standard form semidefinite program\nmin X\u0303 0\ntr(C\u0303X\u0303)\nsubject to tr(A\u0303iX\u0303) = bi, i = 1, . . . ,m (3)\nwhere C\u0303, A\u03031, . . . , A\u0303m \u2208 Sn. If C\u0303 is positive definite, then we can write C\u0303 = LL> where L \u2208 Rn\u00d7n is invertible. It follows that the minimum of problem (3) is the same as\nmin X 0\ntr(X)\nsubject to tr(AiX) = bi, i = 1, . . . ,m (4)\nwhere Ai = L\u22121A\u0303iL\u22121 >. In particular, minimizers X\u0303\u2217 of (3) are obtained from minimizers X\u2217 of (4) via the transformation X\u0303\u2217 = L\u22121 > X\u2217L\u22121.\nSince X is positive semidefinite, tr(X) is equal to \u2016X\u2016\u2217. Hence, problem (4) is the nuclear norm relaxation of problem (2). Next, we characterize the specific cases where X\u2217 = X?, so that the SDP and rank minimization solutions coincide. The following result is from Recht et al. [18].\nTheorem 1. Let A : Rn\u00d7n \u2212\u2192 Rm be a linear map. For every integer k with 1 \u2264 k \u2264 n, define the k-restricted isometry constant to be the smallest value \u03b4k such that\n(1\u2212 \u03b4k) \u2016X\u2016F \u2264 \u2016A(X)\u2016 \u2264 (1 + \u03b4k) \u2016X\u2016F\nholds for any matrix X of rank at most k. Suppose that there exists a rank r matrix X? such that A(X?) = b. If \u03b42r < 1, then X? is the only matrix of rank at most r satisfying A(X) = b. Furthermore, if \u03b45r < 1/10, then X? can be attained by minimizing \u2016X\u2016\u2217 over the affine subset.\nIn other words, since \u03b42r \u2264 \u03b45r, if \u03b45r < 1/10 holds for the transformation A and one finds a matrix X of rank r satisfying the affine constraint, then X must be positive semidefinite. Hence, one can ignore the semidefinite constraint X 0 when solving the rank minimization (2). The resulting problem then can be exactly solved by nuclear norm relaxation. Since the minimum rank solution is positive semidefinite, it then coincides with the solution of the SDP (4), which is a constrained nuclear norm optimization.\nThe observation that one can ignore the semidefinite constraint justifies our experimental comparison with methods such as nuclear norm relaxation, SVP, and AltMinSense, described in the following section."}, {"heading": "3 Related Work", "text": "Burer and Monteiro [4] proposed a general approach for solving semidefinite programs using factored, nonconvex optimization, giving mostly experimental support for the convergence of the algorithms. The first nontrivial guarantee for solving affine rank minimization problem is given by Recht et al. [18], based on replacing the rank function by the convex surrogate nuclear norm, as already mentioned in the previous section. While this is a convex problem, solving it in practice is nontrivial, and a variety of methods have been developed for efficient nuclear norm minimization. The most popular algorithms are proximal methods that perform singular value thresholding [5] at every iteration. While effective for small problem instances, the computational expense of the SVD prevents the method from being useful for large scale problems.\nRecently, Jain et al. [11] proposed a projected gradient descent algorithm SVP (Singular Value Projection) that solves\nmin X\u2208Rn\u00d7p\n\u2016A(X)\u2212 b\u20162\nsubject to rank(X) \u2264 r,\nwhere \u2016\u00b7\u2016 is the `2 vector norm and r is the input rank. In the (t+1)th iteration, SVP updatesX t+1 as the best rank r approximation to the gradient updateX t\u2212\u00b5A>(A(X t)\u2212b), which is constructed from the SVD. If rank(X?) = r, then SVP can recover X? under a similar RIP condition as the nuclear norm heuristic, and enjoys a linear numerical rate of convergence. Yet SVP suffers from the expensive per-iteration SVD for large problem instances.\nSubsequent work of Jain et al. [12] proposes an alternating least squares algorithm AltMinSense that avoids the per-iteration SVD. AltMinSense factorizes X into two factors U \u2208 Rn\u00d7r, V \u2208 Rp\u00d7r such thatX = UV > and minimizes the squared residual \u2225\u2225A(UV >)\u2212 b\u2225\u22252 by updating U and V alternately. Each update is a least squares problem. The authors show that the iterates obtained by AltMinSense converge to X? linearly under a RIP condition. However, the least squares problems are often ill-conditioned, it is difficult to observe AltMinSense converging to X? in practice.\nAs described above, considerable progress has been made on algorithms for rank minimization and certain semidefinite programming problems. Yet truly efficient, scalable and provably convergent algorithms have not yet been obtained. In the specific setting that X? is positive semidefinite, our algorithm exploits this structure to achieve these goals. We note that recent and independent\nwork of Tu et al. [21] proposes a hybrid algorithm called Procrustes Flow (PF), which uses a few iterations of SVP as initialization, and then applies gradient descent."}, {"heading": "4 A Gradient Descent Algorithm for Rank Minimization", "text": "Our method is described in Algorithm 1. It is parallel to the Wirtinger Flow (WF) algorithm for phase retrieval [6], to recover a complex vector x \u2208 Cn given the squared magnitudes of its linear measurements bi = |\u3008ai, x\u3009|2, i \u2208 [m], where a1, . . . , am \u2208 Cn. Cande\u0300s et al. [6] propose a first-order method to minimize the sum of squared residuals\nfWF(z) = n\u2211 i=1 ( |\u3008ai, z\u3009|2 \u2212 bi )2 . (5)\nThe authors establish the convergence of WF to the global optimum\u2014given sufficient measurements, the iterates of WF converge linearly to x up to a global phase, with high probability.\nIf z and the ais are real-valued, the function fWF(z) can be expressed as\nfWF(z) = n\u2211 i=1 ( z>aia > i z \u2212 x>aia>i x )2 ,\nwhich is a special case of f(Z) whereAi = aia>i and each of Z andX ? are rank one. See Figure 1a for an illustration; Figure 1b shows the convergence rate of our method. Our methods and results are thus generalizations of Wirtinger flow for phase retrieval.\nBefore turning to the presentation of our technical results in the following section, we present some intuition and remarks about how and why this algorithm works. For simplicity, let us assume that the rank is specified correctly.\nInitialization is of course crucial in nonconvex optimization, as many local minima may be present. To obtain a sufficiently accurate initialization, we use a spectral method, similar to those used in [17, 6]. The starting point is the observation that a linear combination of the constraint values and matrices yields an unbiased estimate of the solution.\nLemma 1. Let M = 1 m \u2211m i=1 biAi. Then 1 2 E(M) = X?, where the expectation is with respect to the randomness in the measurement matrices Ai.\nBased on this fact, let X? = U?\u03a3U?> be the eigenvalue decomposition of X?, where U? = [u?1, . . . , u ? r] and \u03a3 = diag(\u03c31, . . . , \u03c3r) such that \u03c31 \u2265 . . . \u2265 \u03c3r are the nonzero eigenvalues of X?. Let Z? = U?\u03a3 1 2 . Clearly, u?s = z ? s/ \u2016z?s\u2016 is the top sth eigenvector of E(M) associated with eigenvalue 2 \u2016z?s\u2016 2. Therefore, we initialize according to z0s = \u221a |\u03bbs| 2 vs where (vs, \u03bbs) is the top sth eigenpair of M . For sufficiently large m, it is reasonable to expect that Z0 is close to Z?; this is confirmed by concentration of measure arguments.\nCertain key properties of f(Z) will be seen to yield a linear rate of convergence. In the analysis of convex functions, Nesterov [16] shows that for unconstrained optimization, the gradient descent\nscheme with sufficiently small step size will converge linearly to the optimum if the objective function is strongly convex and has a Lipschitz continuous gradient. However, these two properties are global and do not hold for our objective function f(Z). Nevertheless, we expect that similar conditions hold for the local area near Z?. If so, then if we start close enough to Z?, we can achieve the global optimum.\nIn our subsequent analysis, we establish the convergence of Algorithm 1 with a constant step size of the form \u00b5/ \u2016Z?\u20162F , where \u00b5 is a small constant. Since \u2016Z?\u2016F is unknown, we replace it by \u2016Z0\u2016F ."}, {"heading": "5 Convergence Analysis", "text": "In this section we present our main result analyzing the gradient descent algorithm, and give a sketch of the proof. To begin, note that the symmetric decomposition of X? is not unique, since X? = (Z?U)(Z?U)> for any r \u00d7 r orthonormal matrix U . Thus, the solution set is\nS = { Z\u0303 \u2208 Rn\u00d7r | Z\u0303 = Z?U for some U with UU> = U>U = I } .\nNote that \u2016Z\u0303\u20162F = \u2016X?\u2016\u2217 for any Z\u0303 \u2208 S. We define the distance to the optimal solution in terms of this set.\nDefinition 1. Define the distance between Z and Z? as\nd(Z,Z?) = min UU>=U>U=I \u2016Z \u2212 Z?U\u2016F = min Z\u0303\u2208S\n\u2225\u2225Z \u2212 Z\u0303\u2225\u2225 F .\nAlgorithm 1: Gradient descent for rank minimization input: {Ai, bi}mi=1, r, \u00b5 initialization\nSet (v1, \u03bb1), . . . , (vr, \u03bbr) to the top r eigenpairs of 1m \u2211m i=1 biAi s.t. |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbr|\nZ0 = [z01 , . . . , z 0 r ] where z 0 s = \u221a |\u03bbs| 2 \u00b7 vs, s \u2208 [r]\nk \u2190 0 repeat\n\u2207f(Zk) = 1 m m\u2211 i=1 ( tr(Zk>AiZk)\u2212 bi ) AiZ k\nZk+1 = Zk \u2212 \u00b5\u2211r s=1 |\u03bbs|/2 \u2207f(Zk)\nk \u2190 k + 1 until convergence; output: X\u0302 = ZkZk>\nOur main result for exact recovery is stated below, assuming that the rank is correctly specified. Since the true rank is typically unknown in practice, one can start from a very low rank and gradually increase it.\nTheorem 2. Let the condition number \u03ba = \u03c31/\u03c3r denote the ratio of the largest to the smallest nonzero eigenvalues of X?. There exists a universal constant c0 such that if m \u2265 c0\u03ba2r3n log n, with high probability the initialization Z0 satisfies\nd(Z0, Z?) \u2264 \u221a 3\n16 \u03c3r. (6)\nMoreover, there exists a universal constant c1 such that when using constant step size \u00b5/ \u2016Z?\u20162F with \u00b5 \u2264 c1\n\u03ban and initial value Z0 obeying (6), the kth step of Algorithm 1 satisfies d(Zk, Z?) \u2264 \u221a 3\n16 \u03c3r\n( 1\u2212 \u00b5\n12\u03bar )k/2 with high probability.\nWe now outline the proof, giving full details in the supplementary material. The proof has four main steps. The first step is to give a regularity condition under which the algorithm converges linearly if we start close enough to Z?. This provides a local regularity property that is similar to the Nesterov [16] criteria that the objective function is strongly convex and has a Lipschitz continuous gradient.\nDefinition 2. Let Z = arg minZ\u0303\u2208S \u2225\u2225Z \u2212 Z\u0303\u2225\u2225 F denote the matrix closest to Z in the solution set. We say that f satisfies the regularity condition RC(\u03b5, \u03b1, \u03b2) if there exist constants \u03b1, \u03b2 such that for any Z satisfying d(Z,Z?) \u2264 \u03b5, we have\n\u3008\u2207f(Z), Z \u2212Z\u3009 \u2265 1 \u03b1 \u03c3r \u2225\u2225Z \u2212Z\u2225\u22252 F +\n1\n\u03b2 \u2016Z?\u20162F \u2016\u2207f(Z)\u20162F .\nUsing this regularity condition, we show that the iterative step of the algorithm moves closer to the optimum, if the current iterate is sufficiently close.\nTheorem 3. Consider the update Zk+1 = Zk \u2212 \u00b5 \u2016Z?\u20162F \u2207f(Zk). If f satisfies RC(\u03b5, \u03b1, \u03b2), d(Zk, Z?) \u2264 \u03b5, and 0 < \u00b5 < min(\u03b1/2, 2/\u03b2), then\nd(Zk+1, Z?) \u2264 \u221a\n1\u2212 2\u00b5 \u03b1\u03bar d(Zk, Z?).\nIn the next step of the proof, we condition on two events that will be shown to hold with high probability using concentration results. Let \u03b4 denote a small value to be specified later.\nA1 For any u \u2208 Rn such that \u2016u\u2016 \u2264 \u221a\u03c31,\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (u>Aiu)Ai \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r .\nA2 For any Z\u0303 \u2208 S, \u2225\u2225\u2225\u2225\u2225\u22022f(Z\u0303)\u2202z\u0303s\u2202z\u0303>k \u2212 E [ \u22022f(Z\u0303) \u2202z\u0303s\u2202z\u0303>k ]\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r , for all s, k \u2208 [r]. Here the expectations are with respect to the random measurement matrices. Under these assumptions, we can show that the objective satisfies the regularity condition with high probability.\nTheorem 4. Suppose that A1 and A2 hold. If \u03b4 \u2264 1 16 \u03c3r, then f satisfies the regularity condition RC( \u221a\n3 16 \u03c3r, 24, 513\u03ban) with probability at least 1\u2212mCe\u2212\u03c1n, where C, \u03c1 are universal constants.\nNext we show that under A1, a good initialization can be found.\nTheorem 5. Suppose that A1 holds. Let {vs, \u03bbs}rs=1 be the top r eigenpairs of M = 1 m m\u2211 i=1 biAi\nsuch that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbr|. Let Z0 = [z1, . . . , zr] where zs = \u221a |\u03bbs| 2 \u00b7 vs, s \u2208 [r]. If \u03b4 \u2264 \u03c3r4\u221ar , then\nd(Z0, Z?) \u2264 \u221a\n3\u03c3r/16.\nFinally, we show that conditioning on A1 and A2 is valid since these events have high probability as long as m is sufficiently large.\nTheorem 6. If the number of samples m \u2265 42 min(\u03b42/r2\u03c321, \u03b4/r\u03c31) n log n, then for any u \u2208 Rn\nsatisfying \u2016u\u2016 \u2264 \u221a\u03c31, \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (u>Aiu)Ai \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r holds with probability at least 1\u2212mCe\u2212\u03c1n \u2212 2 n2 , where C and \u03c1 are universal constants.\nTheorem 7. For any x \u2208 Rn, if m \u2265 128 min(\u03b42/4r2\u03c321, \u03b4/2r\u03c31) n log n, then for any Z\u0303 \u2208 S\n\u2225\u2225\u2225\u2225\u2225\u22022f(Z\u0303)\u2202z\u0303s\u2202z\u0303>k \u2212 E [ \u22022f(Z\u0303) \u2202z\u0303s\u2202z\u0303>k ]\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r , for all s, k \u2208 [r], with probability at least 1\u2212 6me\u2212n \u2212 4\nn2 . Note that since we need \u03b4 \u2264 min (\n1 16 , 1 4 \u221a r ) \u03c3r, we have \u03b4r\u03c31 \u2264 1, and the number of measure-\nments required by our algorithm scales as O(r3\u03ba2n log n), while only O(r2\u03ba2n log n) samples are required by the regularity condition. We conjecture this bound could be further improved to be O(rn log n); this is supported by the experimental results presented below.\nRecently, Tu et al. [21] establish a tighter O(r2\u03ba2n) bound overall. Specifically, when only one single SVP step is used in preprocessing, the initialization of PF is also the spectral decomposition of 1\n2 M . The authors show that O(r2\u03ba2n) measurements are sufficient for the initial solution to\nsatisfy d(Z0, Z?) \u2264 O(\u221a\u03c3r) with high probability, and demonstrate an O(rn) sample complexity for the regularity condition."}, {"heading": "6 Experiments", "text": "In this section we report the results of experiments on synthetic datasets. We compare our gradient descent algorithm with nuclear norm relaxation, SVP and AltMinSense for which we drop the positive semidefiniteness constraint, as justified by the observation in Section 2. We use ADMM for the nuclear norm minimization, based on the algorithm for the mixture approach in Tomioka et al. [19]; see Appendix G. For simplicity, we assume that AltMinSense, SVP and the gradient scheme know the true rank. Krylov subspace techniques such as the Lanczos method could be used compute the partial eigendecomposition; we use the randomized algorithm of Halko et al. [9] to compute the low rank SVD. All methods are implemented in MATLAB and the experiments were run on a MacBook Pro with a 2.5GHz Intel Core i7 processor and 16 GB memory."}, {"heading": "6.1 Computational Complexity", "text": "It is instructive to compare the per-iteration cost of the different approaches; see Table 1. Suppose that the density (fraction of nonzero entries) of each Ai is \u03c1. For AltMinSense, the cost of solving the least squares problem is O(mn2r2 + n3r3 + mn2r\u03c1). The other three methods have O(mn2\u03c1) cost to compute the affine transformation. For the nuclear norm approach, the O(n3) cost is from the SVD and the O(m2) cost is due to the update of the dual variables. The gradient scheme requires 2n2r operations to compute ZkZk> and to multiply Zk by n\u00d7 n matrix to obtain the gradient. SVP needs O(n2r) operations to compute the top r singular vectors. However, in practice this partial SVD is more expensive than the 2n2r cost required for the matrix multiplies in the gradient scheme.\nClearly, AltMinSense is the least efficient. For the other approaches, in the dense case (\u03c1 large), the affine transformation dominates the computation. Our method removes the overhead caused by the SVD. In the sparse case (\u03c1 small), the other parts dominate and our method enjoys a low cost."}, {"heading": "6.2 Runtime Comparison", "text": "We conduct experiments for both dense and sparse measurement matrices. AltMinSense is indeed slow, so we do not include it here.\nIn the first scenario, we randomly generate a 400 \u00d7 400 rank-2 matrix X? = xx> + yy> where x, y \u223c N (0, I). We also generate m = 6n matrices A1, . . . , Am from the GOE, and then take b = A(X?). We report the relative error measured in the Frobenius norm defined as \u2016X\u0302 \u2212X?\u2016F/\u2016X?\u2016F . For the nuclear norm approach, we set the regularization parameter to \u03bb = 10\u22125. We test three values \u03b7 = 10, 100, 200 for the penalty parameter and select \u03b7 = 100 as it leads to the fastest convergence. Similarly, for SVP we evaluate the three values 5\u00d710\u22125, 10\u22124, 2\u00d710\u22124 for the step size, and select 10\u22124 as the largest for which SVP converges. For our approach, we test the three values 0.6, 0.8, 1.0 for \u00b5 and select 0.8 in the same way.\nIn the second scenario, we use a more general and practical setting. We randomly generate a rank-2 matrix X? \u2208 R600\u00d7600 as before. We generate m = 7n sparse Ais whose entries are i.i.d. Bernoulli:\n(Ai)jk = { 1 with probability \u03c1, 0 with probability 1\u2212 \u03c1,\nwhere we use \u03c1 = 0.001. For all the methods we use the same strategies as before to select parameters. For the nuclear norm approach, we try three values \u03b7 = 10, 100, 200 and select \u03b7 = 100. For SVP, we test the three values 5 \u00d7 10\u22123, 2 \u00d7 10\u22123, 10\u22123 for the step size and select 10\u22123. For the gradient algorithm, we check the three values 0.8, 1, 1.5 for \u00b5 and choose 1.\nThe results are shown in Figures 2a and 2b. In the dense case, our method is faster than the nuclear norm approach and slightly outperforms SVP. In the sparse case, it is significantly faster than the other approaches."}, {"heading": "6.3 Sample Complexity", "text": "We also evaluate the number of measurements required by each method to exactly recover X?, which we refer to as the sample complexity. We randomly generate the true matrix X? \u2208 Rn\u00d7n and compute the solutions of each method given m measurements, where the Ais are randomly drawn from the GOE. A solution with relative error below 10\u22125 is considered to be successful. We run 40 trials and compute the empirical probability of successful recovery.\nWe consider cases where n = 60 or 100 and X? is of rank one or two. The results are shown in Figure 2c. For SVP and our approach, the phase transitions happen around m = 1.5n when X? is rank-1 and m = 2.5n when X? is rank-2. This scaling is close to the number of degrees of freedom in each case; this confirms that the sample complexity scales linearly with the rank r. The phase transition for the nuclear norm approach occurs later. The results suggest that the sample complexity of our method should also scale as O(rn log n) as for SVP and the nuclear norm approach [11, 18]."}, {"heading": "7 Conclusion", "text": "We connect a special case of affine rank minimization to a class of semidefinite programs with random constraints. Building on a recently proposed first-order algorithm for phase retrieval [6], we develop a gradient descent procedure for rank minimization and establish convergence to the optimal solution with O(r3n log n) measurements. We conjecture that O(rn log n) measurements are sufficient for the method to converge, and that the conditions on the sampling matrices Ai can be significantly weakened. More broadly, the technique used in this paper\u2014factoring the semidefinite matrix variable, recasting the convex optimization as a nonconvex optimization, and applying first-order algorithms\u2014first proposed by Burer and Monteiro [4], may be effective for a much wider class of SDPs, and deserves further study."}, {"heading": "Acknowledgements", "text": "Research supported in part by NSF grant IIS-1116730 and ONR grant N00014-12-1-0762. The authors thank Afonso Bandeira, Ryota Tomioka and the authors of Tu et al. [21] for helpful comments on this work."}, {"heading": "A Proof of Lemma 1", "text": "Let A = (aij) be a random matrix that is GOE distributed; thus aij \u223c N (0, 1) for i 6= j and aii \u223c N (0, 2). We have E(M) = \u2211r s=1 E((z?s\n>Az?s)A). Hence, it suffices to show that E((x>Ax)A) = 2xx> for any x \u2208 Rn. The (i, j) entry of (x>Ax)A has expected value\nE((x>Ax)aij) = E (\u2211 k \u2211 l xkxlaklaij ) = \u2211 k \u2211 l xkxlE(aklaij)\n= \u2211 k \u2211 l xkxl \u00b7 { 0 if (k, l) 6= (i, j) \u2227 (k, l) 6= (j, i) E(a2kl) otherwise\n= { 2xixjE(a2ij) if i 6= j x2iE(a2ii) otherwise\n= { 2xixj if i 6= j, 2x2i otherwise,\nwhere we use that the variance of aii is 2 and the variance of aij is 1 for any i 6= j. In matrix form, this is E((x>Ax)A) = 2xx>.\nB Ingredients We first present some technical lemmas that will be needed later. Recall Definition 2 that for any Z, Z = arg minZ\u0303\u2208S \u2225\u2225Z \u2212 Z\u0303\u2225\u2225 F\n. Let H = Z \u2212Z . The sth column of Z, Z , Z?, H are denoted by zs, z\u0304s, z?s , hs respectively. We shall use the following formulas for the gradient and second order partial derivatives:\n\u2207f(Z) = 1 m m\u2211 i=1 ( tr(H>AiH) + 2 tr(Z>AiH) ) (AiH + AiZ),\n\u22022f(Z) \u2202zs\u2202z>s = 1 m m\u2211 i=1 ( 2Aizsz > s A > i + ( tr(Z>AiZ)\u2212 bi ) Ai ) , \u2200s \u2208 [r],\n\u22022f(Z) \u2202zs\u2202z>k = 1 m m\u2211 i=1 2Aizsz > k A > i , \u2200s, k \u2208 [r] such that s 6= k.\nThe next ingredient we need is the expectation of the second order partial derivatives with respect to the random measurement matrices.\nLemma 2. Let A = (aij) be a GOE distributed random matrix. For any two fixed vectors x and y, we have E [AxyA] = x>yI + yx>.\nProof. The expectation of (i, j) entry of Axy>A is\nE[(Axy>A)ij] = E (\u2211 k l aikajkxkyl ) .\nIf i = j, then we have\nE[(Axy>A)ii] = E (\u2211 k a2ikxkyk ) = \u2211 k xkyk + xiyi,\nsince Var(a2ii) = 2 and Var(a2ik) = 1 if k 6= i. On the other hand, if i 6= j, then\nE[(Axy>A)ij] = E (\u2211 kl aikajlxkyl ) = E(a2ijxjyi) = xjyi.\nTherefore, E(Axy>A) = x>yI + yx>. Lemma 3. For all s \u2208 [r], it holds that E [ \u22022f(Z)\n\u2202zs\u2202z>s\n] = 2 \u2016zs\u20162 I + 2zsz>s + 2ZZ> \u2212 2X? and\nE [ \u22022f(Z)\n\u2202zs\u2202z>k\n] = 2z>s zkI + 2zkz > s for all k \u2208 [r] such that k 6= s, where the expectation is over the\nrandom measurement matrices.\nProof. The case where k 6= s is a direct result of Lemma 2. For the other case, let A = (aij) be a GOE distributed random matrix. It follows from Lemma 1 that\nE [ \u22022f(Z)\n\u2202zs\u2202z>s\n] = 2E(Azsz>s A) + 2ZZ> \u2212 2X?.\nBy Lemma 2, we have E(Azsz>s A) = \u2016zs\u2016 2 I + zsz > s .\nSubstituting this back into the above equation, we obtain the lemma.\nWe next recall a concentration result for the operator (spectral) norm of the random measurement matrices.\nLemma 4. (Ledoux and Rider [14, Theorem 1]) There exists two absolute constants C and \u03c1 = 1\u221a 8C such that with probability at least 1\u2212 Ce\u2212\u03c1n,\n\u2016Ai\u2016 \u2264 3 \u221a n.\nA tighter upper bound is actually given in the Tracy-Widow law: w.h.p. \u2016Ai\u2016 = O(2 \u221a n+n1/6).\nCorollary 1. With probability at least 1 \u2212mCe\u2212\u03c1n, the average of the squared operator norm of the random measurement matrices is upper bounded by 9n.\nProof. Applying a union bound we have\nP\n( 1\nm m\u2211 i=1 \u2016Ai\u20162 \u2264 9n\n) \u2265 P ( \u2200i, \u2016Ai\u2016 \u2264 3 \u221a n )\n\u2265 1\u2212 m\u2211 i=1 P ( \u2016Ai\u2016 > 3 \u221a n ) \u2265 1\u2212mCe\u2212\u03c1n,\nwhere we use Lemma 4 in the last line.\nThe following two technical lemmas are important tools for us. Define the set\nE(\u03b5) = {Z | d(Z,Z?) \u2264 \u03b5} .\nLemma 5. Suppose that A1 holds: \u2225\u2225 1 m \u2211m i=1(u >Aiu)Ai \u2212 2uu> \u2225\u2225 \u2264 \u03b4 r , for any u such that \u2016u\u2016 \u2264 \u221a \u03c31. If \u03b4 \u2264 116\u03c3r, then for any Z \u2208 E (\u221a 3 16 \u03c3r ) it holds that\n2 \u2225\u2225HH>\u2225\u22252\nF \u2212 \u03b4 \u2016H\u20162F \u2264\n1\nm m\u2211 i=1 tr(H>AiH)2 \u2264 \u03b4 \u2016H\u20162F + 2 \u2225\u2225HH>\u2225\u22252 F .\nProof. Let hs be the sth column of H . Since maxs\u2208[r] \u2016hs\u20162 \u2264 \u2016H\u2016F \u2264 \u221a 3 16 \u03c3r \u2264 \u221a \u03c31, it follows from the assumption of the lemma that\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (h>s Aihs)Ai \u2212 2hsh>s\n\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r , s = 1, . . . , r. By the triangle inequality, we have\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 r\u2211 s=1 (h>s Aihs)Ai \u2212 2 r\u2211 s=1 hsh > s\n\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4 and consequently\n\u2212\u03b4 \u2016hs\u20162 \u2264 h>s\n( 1\nm m\u2211 i=1 tr(H>AiH)Ai \u2212 2HH> ) h>s \u2264 \u03b4 \u2016hs\u2016 2 , s = 1, . . . , r,\nwhere we replace r\u2211 s=1 h>s Aihs by tr(H >AiH) and \u2211r s=1 hsh > s by HH >. Taking the sum of the above inequalities, we obtain\n\u2212\u03b4 \u2016H\u20162F \u2264 1\nm m\u2211 i=1 tr(H>AiH)2 \u2212 2 tr(H>HH>H) \u2264 \u03b4 \u2016H\u20162F .\nNote that tr(H>HH>H) = \u2225\u2225HH>\u2225\u22252\nF . Therefore,\n2 \u2225\u2225HH>\u2225\u22252\nF \u2212 \u03b4 \u2016H\u20162F \u2264\n1\nm m\u2211 i=1 tr(H>AiH)2 \u2264 \u03b4 \u2016H\u20162F + 2 \u2225\u2225HH>\u2225\u22252 F .\nLemma 6. Suppose that A2 holds: for any Z\u0303 such that Z\u0303Z\u0303> = X? we have\u2225\u2225\u2225\u2225\u2225\u22022f(Z\u0303)\u2202z\u0303s\u2202z\u0303>k \u2212 E [ \u22022f(Z\u0303) \u2202z\u0303s\u2202z\u0303>k ]\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r , s, k = 1, . . . , r. (7) Then(\n\u03c3r \u2212 \u03b4\n2\n) \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F \u2264 1 m m\u2211 i=1 tr(H>AiZ)2 \u2264 ( \u03c31 + \u03b4 2 ) \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F .\nProof. Our goal is to bound 1 m m\u2211 i=1 tr(H>AiZ)2. This can be expanded as\n1\nm m\u2211 i=1 ( r\u2211 s=1 (h>s Aiz\u0304s) )2 = 1 m m\u2211 i=1 r\u2211 s=1 (h>s Aixs) 2 + 1 m m\u2211 i=1 \u2211 s<k 2(h>s Aixs)(h > k Aixk).\nWe first bound the sum of the quadratic terms. For any s \u2208 [r], we have\n\u22022f(Z) \u2202z\u0304s\u2202z\u0304>s = 1 m m\u2211 i=1 2Aiz\u0304sz\u0304 > s Ai,\nE [ \u22022f(Z)\n\u2202z\u0304s\u2202z\u0304>s\n] = 2 \u2016z\u0304s\u20162 I + 2z\u0304sz\u0304>s .\nIt follows from assumption (7) that for any s \u2208 [r],\n\u2212\u03b4 r \u2016hs\u20162 \u2264 1 m m\u2211 i=1 2(h>s Aiz\u0304s) 2 \u2212 2 \u2016z\u0304s\u20162 \u2016hs\u20162 \u2212 2(h>s z\u0304s)2 \u2264 \u03b4 r \u2016hs\u20162 .\nTaking the sum of above inequalities, we obtain\n\u2212 \u03b4 2r r\u2211 s=1 \u2016hs\u20162 \u2264 1 m m\u2211 i=1 r\u2211 s=1 (h>s Aiz\u0304s) 2 \u2212 r\u2211 s=1 \u2016z\u0304s\u20162 \u2016hs\u20162 \u2212 r\u2211 s=1 (h>s z\u0304s) 2 \u2264 \u03b4 2r r\u2211 s=1 \u2016hs\u20162 . (8)\nSimilarly, we bound the sum of the cross terms. For any fixed s, k such that s 6= k, we have\n\u22022f(Z) \u2202z\u0304s\u2202z\u0304>k = 1 m f(Z) m\u2211 i=1 2Aiz\u0304sz\u0304 > k Ai,\nE [ \u22022f(Z)\n\u2202z\u0304s\u2202z\u0304>k\n] = 2z\u0304>s z\u0304kI + 2z\u0304kz\u0304 > s ,\nand consequently\n\u2212\u03b4 r \u2211 s<k \u2016hs\u2016 \u2016hk\u2016 \u2264 1 m m\u2211 i=1 \u2211 s<k 2(h>s Aiz\u0304s)(h > k Aiz\u0304k)\u2212 2 \u2211 s<k z\u0304>s z\u0304kh > s hk \u2212 2 \u2211 s<k h>s z\u0304kz\u0304 > s hk (9)\n\u2264 \u03b4 r \u2211 s<k \u2016hs\u2016 \u2016hk\u2016 .\nWe combine equations (9) and (8) to get\n\u2212 \u03b4 2r \u2211 sk \u2016hs\u2016 \u2016hk\u2016 \u2264 1 m m\u2211 i=1 tr(H>AiZ)2\u2212 \u2211 sk z\u0304>s z\u0304kh > s hk\u2212 \u2211 sk h>s z\u0304kz\u0304 > s hk \u2264 \u03b4 2r \u2211 sk \u2016hs\u2016 \u2016hk\u2016 . (10) Note that \u2211 sk h > s z\u0304kz\u0304 > s hk = tr(H >ZH>Z), \u2211 sk z\u0304 > s z\u0304kh > s hk = \u2225\u2225ZH>\u2225\u22252 F and\n\u2211 sk \u2016hs\u2016 \u2016hk\u2016 = ( r\u2211 s=1 \u2016hs\u2016 )2 \u2264 r r\u2211 s=1 \u2016hs\u20162 = r \u2016H\u20162F .\nBy Lemma 7, tr(H>ZH>Z) = \u2225\u2225H>Z\u2225\u22252\nF . Replacing those terms in equation (10) gives us\n\u2212\u03b4 2 \u2016H\u20162F + \u2225\u2225ZH>\u2225\u22252 F + \u2225\u2225H>Z\u2225\u22252 F \u2264 1 m m\u2211 i=1 tr(H>AiZ)2 \u2264 \u03b4 2 \u2016H\u20162F + \u2225\u2225ZH>\u2225\u22252 F + \u2225\u2225H>Z\u2225\u22252 F .\nFinally, we obtain the claim by noticing that\n\u221a \u03c3r \u2016H\u2016F \u2264 \u2225\u2225ZH>\u2225\u2225 F \u2264 \u221a \u03c31 \u2016H\u2016F ,\nwhere \u221a \u03c31 = \u03c3max(Z) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3min(Z) = \u221a \u03c3r are the singular values ofZ .\nLemma 7. tr(H>ZH>Z) = \u2225\u2225H>Z\u2225\u22252\nF .\nProof. Let U\u0304 = arg minUU>=U>U=I \u2016Z \u2212 Z?U\u2016 2 F = arg maxUU>=U>U=I\u3008U,Z? >Z\u3009. Note that \u3008A,B\u3009 \u2264 \u2016A\u2016\u2217 \u2016B\u2016 for any matrices A,B that are of the same size. The equality holds when B = UAV > A whereA = UA\u03a3AV > A is the SVD ofA. Hence, U\u0304 = U\u0303 V\u0303\n> where U\u0303 S\u0303V\u0303 > is the SVD of Z?>Z; Z = Z?U\u0304 . Therefore, Z>Z = Z>Z?U\u0304 = V\u0303 S\u0303V\u0303 > is symmetric and positive semidefinite. Thus, H>Z = Z>Z \u2212Z>Z is also symmetric. This implies that tr(H>ZH>Z) = \u2225\u2225H>Z\u2225\u22252 F ."}, {"heading": "C Linear Convergence", "text": "Proof of Theorem 3\nLet Hk = Zk \u2212Zk. Then we have that \u2225\u2225Zk+1 \u2212Zk\u2225\u22252 F = \u2225\u2225\u2225\u2225\u2225Zk \u2212 \u00b5\u2016Z?\u20162F\u2207f(Zk)\u2212Zk \u2225\u2225\u2225\u2225\u2225 2\nF = \u2225\u2225Hk\u2225\u22252\nF +\n\u00b52\n\u2016Z?\u20164F \u2225\u2225\u2207f(Zk)\u2225\u22252 F \u2212 2\u00b5 \u2016Z?\u20162F \u3008\u2207f(Zk), Hk\u3009\n\u2264 \u2225\u2225Hk\u2225\u22252\nF +\n\u00b52\n\u2016Z?\u20164F \u2225\u2225\u2207f(Zk)\u2225\u22252 F \u2212 2\u00b5 \u2016Z?\u20162F\n( 1\n\u03b1 \u03c3r \u2225\u2225Hk\u2225\u22252 F +\n1\n\u03b2 \u2016Z?\u20162F\n\u2225\u2225\u2207f(Zk)\u2225\u22252 F\n)\n= ( 1\u2212 2\u00b5\n\u03b1 \u00b7 \u03c3r\u2211r\ns=1 \u03c3s\n)\u2225\u2225Hk\u2225\u22252 F\n+ \u00b5(\u00b5\u2212 2/\u03b2) \u2016Z?\u20164F\n\u2225\u2225\u2207f(Zk)\u2225\u22252 F\n\u2264 (\n1\u2212 2\u00b5 \u03b1 \u00b7 \u03c3r r\u03c31\n)\u2225\u2225Hk\u2225\u22252 F\n= ( 1\u2212 2\u00b5\n\u03b1\u03bar\n) d(Zk, Z?)2,\nwhere we use the definition of RC(\u03b5, \u03b1, \u03b2) in the third line, \u2016Z?\u20162F = \u2016X?\u2016\u2217 = \u2211r\ns=1 \u03c3s in the third to last line and 0 < \u00b5 < min {\u03b1/2, 2/\u03b2} in the second to last line. Therefore,\nd(Zk+1, Z?) = min Z\u0303\u2208S \u2225\u2225\u2225Zk+1 \u2212 Z\u0303\u2225\u2225\u22252 F \u2264 \u221a 1\u2212 2\u00b5 \u03b1\u03bar d(Zk, Z?)."}, {"heading": "D Regularity Condition", "text": "As mentioned before, Nesterov [16, Theorem 2.1.11] shows that the gradient scheme converges linearly under a condition similar to the regularity condition, which is satisfied if the function is strongly convex and has a Lipschitz continuous gradient (strongly smooth). In order to prove Theorem 4, we show that with high probability the function f satisfies the local curvature condition, which is analogous to strong convexity, and the local smoothness condition, which is analogous to strong smoothness.\nC1 Local Curvature Condition There exists a constant C1 such that for any Z satisfying d(Z,Z?) \u2264 \u221a 3 16 \u03c3r,\n\u3008\u2207f(Z), Z \u2212Z\u3009 \u2265 C1 \u2225\u2225Z \u2212Z\u2225\u22252 F + \u2225\u2225(Z \u2212Z)>Z\u2225\u22252 F .\nC2 Local Smoothness Condition There exist constants C2, C3 such that for any Z satisfying d(Z,Z?) \u2264 \u221a 3 16 \u03c3r,\n\u2016\u2207f(Z)\u20162F \u2264 C2 \u2225\u2225Z \u2212Z\u2225\u22252 F + C3 \u2225\u2225(Z \u2212Z)>Z\u2225\u22252 F .\nD.1 Proof of the Local Curvature Condition\n\u3008\u2207f(Z), H\u3009 =\np2\ufe37 \ufe38\ufe38 \ufe37 2\nm m\u2211 i=1 tr(H>AiZ)2 +\nq2\ufe37 \ufe38\ufe38 \ufe37 1\nm m\u2211 i=1 tr(H>AiH)2 + 3 m m\u2211 i=1 tr(H>AiZ) tr(H>AiH)\n\u2265 p2 + q2 \u2212 3 m \u221a\u221a\u221a\u221a m\u2211 i=1 tr(H>AiZ)2 \u221a\u221a\u221a\u221a m\u2211 i=1 tr(H>AiH)2\n= p2 + q2 \u2212 3\u221a 2 p\ufe37 \ufe38\ufe38 \ufe37\u221a\u221a\u221a\u221a 2 m m\u2211 i=1 tr(H>AiZ)2 q\ufe37 \ufe38\ufe38 \ufe37\u221a\u221a\u221a\u221a 1 m m\u2211 i=1 tr(H>AiH)2\n= ( p\u2212 3\n2 \u221a 2 q\n)2 \u2212 1\n8 q2 \u2265 ( p2\n2 \u2212 9 8 q2 ) \u2212 1 8 q2\n= p2 2 \u2212 5 4 q2 = 1 m m\u2211 i=1 tr(H>AiZ)2 \u2212 5 4 1 m \u2211 i tr(H>AiH)2\n\u2265 ( \u03c3r \u2212 \u03b4\n2\n) \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F \u2212 5\u03b4 4 \u2016H\u20162F \u2212 5 2 \u2225\u2225HH>\u2225\u22252 F\n\u2265 ( \u03c3r \u2212 5\n2 \u2016H\u20162F \u2212\n7 4 \u03b4\n) \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F .\nwhere we use Cauchy-Schwarz inequality in the 2nd line, the inequality (a\u2212 b)2 \u2265 a2 2 \u2212 b2 in the 5th line, Lemma 5 and 6 in the 7th line, and the fact that \u2225\u2225HH>\u2225\u2225\nF \u2264 \u2016H\u20162F in the 8th line. Since \u2016H\u2016F \u2264 \u221a 3 16 \u03c3r and \u03b4 \u2264 116\u03c3r, we have\n\u3008\u2207f(Z), H\u3009 \u2265 27 64 \u03c3r \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F . (11)\nD.2 Proof of the Local Smoothness Condition We need to upper bound \u2016\u2207f(Z)\u20162F = max\u2016W\u2016F=1 |\u3008\u2207f(Z),W \u3009| 2. It suffices to show that for\nany W \u2208 Rn\u00d7R of unit Frobenius norm, |\u3008\u2207f(Z),W \u3009|2 is upper bounded if Z \u2208 E (\u221a\n3 16 \u03c3r\n) .\nSince (a+ b+ c+ d)2 \u2264 4(a2 + b2 + c2 + d2), we have\n|\u3008\u2207f(Z),W \u3009|2 =\n( 1\nm m\u2211 i=1 ( tr(H>AiH) + 2 tr(H>AiZ) ) ( tr(W>AiH) + tr(W>AiZ)\n))2\n=\n( 1\nm m\u2211 i=1 tr(H>AiH) tr(W>AiH) + 2 tr(H>AiZ) tr(W>AiH)\n+ tr(H>AiH) tr(W>AiZ) + 2 tr(H>AiZ) tr(W>AiZ) )2\n\u2264 4\n( 1\nm m\u2211 i=1 tr(H>AiH) tr(W>AiH)\n)2 + 4 ( 2\nm m\u2211 i=1 tr(H>AiZ) tr(W>AiH)\n)2\n+ 4\n( 1\nm m\u2211 i=1 tr(H>AiH) tr(W>AiZ)\n)2 + 4 ( 2\nm m\u2211 i=1 tr(H>AiZ) tr(W>AiZ)\n)2 .\nThe first term in the righthand side can be upper bounded as\n4\n( 1\nm m\u2211 i=1 tr(H>AiH) tr(W>AiH)\n)2 \u2264 4 ( 1\nm m\u2211 i=1\ntr(H>AiH)2 )( 1\nm m\u2211 i=1\ntr(W>AiH)2 )\n\u2264 4 ( 2 \u2016H\u20164F + \u03b4 \u2016H\u2016 2 F )( 1 m m\u2211 i=1 \u2016W\u20162F \u2016AiH\u2016 2 F )\n= 4 ( 2 \u2016H\u20164F + \u03b4 \u2016H\u2016 2 F )( 1 m m\u2211 i=1 \u2016AiH\u20162F )\n\u2264 4 ( 2 \u2016H\u20164F + \u03b4 \u2016H\u2016 2 F )( 1 m m\u2211 i=1 \u2016Ai\u20162 \u2016H\u20162F ) \u2264 36n \u2016H\u20162F ( 2 \u2016H\u20164F + \u03b4 \u2016H\u2016 2 F ) ,\nwhere we use the Cauchy-Schwarz inequality in the first and second line, Lemma 5 and \u2225\u2225HH>\u2225\u2225\nF \u2264\n\u2016H\u20162F in the third line and Corollary 1 in the last line. The other three terms are bounded similarly. For the second term, we have\n4\n( 2\nm m\u2211 i=1 tr(H>AiZ) tr(W>AiH)\n)2 \u2264 16 ( 1\nm m\u2211 i=1\ntr(H>AiZ)2 )( 1\nm m\u2211 i=1\ntr(W>AiH)2 )\n\u2264 36n \u2016H\u20162F ( (4\u03c31 + 2\u03b4) \u2016H\u20162F + 4 \u2225\u2225H>Z\u2225\u22252\nF\n) ,\nwhere we use Lemma 6 and 1. The third term is bounded as\n4\n( 1\nm m\u2211 i=1 tr(H>AiH) tr(W>AiZ)\n)2 \u2264 4 ( 1\nm m\u2211 i=1\ntr(H>AiH)2 )( 1\nm m\u2211 i=1\ntr(W>AiZ)2 )\n\u2264 36n \u2225\u2225Z\u2225\u22252\nF\n( 2 \u2016H\u20164F + \u03b4 \u2016H\u2016 2 F ) ,\nand the fourth term is bounded as\n4\n( 2\nm m\u2211 i=1 tr(H>AiZ) tr(W>AiZ)\n)2 \u2264 16 ( 1\nm m\u2211 i=1\ntr(H>AiZ)2 )( 1\nm m\u2211 i=1 (W>AiZ) 2 ) \u2264 36n \u2225\u2225Z\u2225\u22252 F ( (4\u03c31 + 2\u03b4) \u2016H\u20162F + 4 \u2225\u2225H>Z\u2225\u22252 F ) .\nPutting these inequalities together, we have \u2016\u2207f(Z)\u20162F \u2264 36n (\u2225\u2225Z\u2225\u22252 F + \u2016H\u20162F )( 2 \u2016H\u20164F + (4\u03c31 + 3\u03b4) \u2016H\u2016 2 F + 4 \u2225\u2225H>Z\u2225\u22252 F ) .\nHence, \u2016\u2207f(Z)\u20162F\n144n (\u2225\u2225Z\u2225\u22252\nF + \u2016H\u20162F\n) \u2264 (\u03c31 + 1 2 \u2016H\u20162F + 3 4 \u03b4 ) \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F .\nSince \u2016H\u2016F \u2264 \u221a 3 16 \u03c3r and \u03b4 \u2264 116\u03c3r, we have\n\u2016\u2207f(Z)\u20162 144n (\u2225\u2225Z\u2225\u22252\nF + (3/16)\u03c3r\n) \u2264 (\u03c31 + 9 64 \u03c3r ) \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F .\nD.3 Proof of the Regularity Condition Now we combine the curvature and the smoothness conditions. For any \u03b3 \u2208 (\n0, \u03c31 \u03c3r\n) , it holds that\n\u03b3 \u03c3r \u03c31 \u00b7 \u2016\u2207f(Z)\u2016 2 F 144n (\u2225\u2225Z\u2225\u22252\nF + (3/16)\u03c3r\n) \u2264 \u03b3 \u03c3r \u03c31 \u00b7 ( \u03c31 + 9 64 \u03c3r ) \u2016H\u20162F + \u2225\u2225H>Z\u2225\u22252 F . (12)\nCombining equation (11) and (12), we obtain\n\u3008\u2207f(Z), H\u3009 \u2265 ( 27\n64 \u2212 \u03b3 \u2212 \u03b3 \u03c3r\n\u03c31\n9\n64\n) \u03c3r \u2016H\u20162F + \u03b3\n\u03c3r \u03c31 \u00b7 \u2016\u2207f(Z)\u2016 2 F 144n( \u2225\u2225Z\u2225\u22252\nF + (3/16)\u03c3r) \u2265 ( 27\n64 \u2212 73 64 \u03b3\n) \u03c3r \u2016H\u20162F + \u03b3\n\u03c3r \u03c31 \u00b7 \u2016\u2207f(Z)\u2016 2 F 144n( \u2225\u2225Z\u2225\u22252\nF + (3/16)\u03c3r)\n.\nIf we take \u03b3 = 1 3 , then\n\u3008\u2207f(Z), H\u3009 \u2265 1 24 \u03c3r \u2016H\u20162F + \u03c3r \u03c31 \u00b7 \u2016\u2207f(Z)\u2016 2 F 3 \u00b7 144n (\u2225\u2225Z\u2225\u22252\nF + (3/16)\u03c3r ) \u2265 1\n24 \u03c3r \u2016H\u20162F +\n\u03c3r/\u03c31\n513n \u2016Z?\u20162F \u2016\u2207f(Z)\u20162F ,\nwhere we use \u2225\u2225Z\u2225\u22252\nF = \u2016Z?\u20162F = \u2016X?\u2016\u2217 \u2265 \u03c3r. Thus we have\n\u3008\u2207f(Z), H\u3009 \u2265 1 \u03b1 \u03c3r \u2016H\u20162F +\n1\n\u03b2 \u2016Z?\u20162F \u2016\u2207f(Z)\u20162F\nfor \u03b1 \u2265 24 and \u03b2 \u2265 \u03c31 \u03c3r \u00b7 513n.\nE Initialization Proof of Theorem 5\nBy assumption, we have\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (z?s >Aiz ? s)Ai \u2212 2z?sz?s > \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r , s \u2208 [r]. Hence,\n\u2016M \u2212 2X?\u2016 = \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 r\u2211 s=1 (z?s >Aiz ? s)Ai \u2212 2 r\u2211 s=1 z?sz ? s T \u2225\u2225\u2225\u2225\u2225 \u2264 r\u2211 s=1 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (z?s >Aiz ? s)Ai \u2212 2z?sz?s > \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4. (13)\nLet \u03bb\u20321 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb\u2032n be the eigenvalues of M . By Weyl\u2019s theorem, we have\n|\u03bb\u2032s \u2212 2\u03c3s| \u2264 \u03b4, s \u2208 [n].\nSince \u03b4 < \u03c3r, it is easy to see \u03bb\u20321 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb\u2032r > \u03b4 and |\u03bb\u2032s| \u2264 \u03b4, s = r + 1, . . . , n. Hence, \u03bbs = \u03bb\u2032s, s \u2208 [r], and Z0Z0> is the best rank r approximation of 1\n2 M . Therefore,\u2225\u2225\u2225Z0Z0> \u2212 Z?Z?>\u2225\u2225\u2225\nF \u2264 \u221a\n2r \u2225\u2225\u2225Z0Z0> \u2212 Z?Z?>\u2225\u2225\u2225\n= \u221a 2r \u2225\u2225\u2225\u2225Z0Z0> \u2212 12M + 12M \u2212 Z?Z?> \u2225\u2225\u2225\u2225 \u2264 \u221a 2r (\u2225\u2225\u2225\u2225Z0Z0> \u2212 12M \u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u222512M \u2212 Z?Z?>\n\u2225\u2225\u2225\u2225) \u2264 \u221a 2r\u03b4,\nwhere we used \u2016A\u2016F \u2264 \u221a rank(A) \u2016A\u2016 in first line, the fact \u2225\u2225\u2225Z0Z0> \u2212 12M\u2225\u2225\u2225 = 12 |\u03bbr+1| \u2264 12\u03b4 and inequality (13) in the last line. Let H = Z0 \u2212 Z0. We want to bound d(Z0, Z?)2 = \u2016H\u20162F . According to the discussion in Lemma 7, H>Z0 is symmetric and Z0>Z0 is positive semidefinite.\nThe following step closely follows [21]. It holds that\u2225\u2225\u2225Z0Z0> \u2212 Z?Z?>\u2225\u2225\u22252 F = \u2225\u2225\u2225Z0Z0> \u2212Z0Z0>\u2225\u2225\u22252 F\n= \u2225\u2225\u2225HZ0> +Z0H> +HH>\u2225\u2225\u22252\nF = tr ( Z0H>HZ0 > +HZ0 > HZ0 > +HH>Z0 >\n+Z0H>Z0H> +HZ0 > Z0H +HH>Z0H>\n+Z0H>HH> +HZ0 > HH> +HH>HH> ) = tr ( (H>H)2 + 2(H>Z0)2 + 2(H>H)(Z0 > Z0) + 4(H>H)(H>Z0)\n) = tr (( H>H + \u221a 2H>Z0 )2 + (4\u2212 2 \u221a 2)(H>H)(H>Z0) + 2(H>H)(Z0 > Z0)\n) \u2265 tr ( (4\u2212 2 \u221a 2)(H>H)(H>Z0) + 2(H>H)(Z>Z)\n) = tr ( (4\u2212 2 \u221a 2)(H>H)(Z0 > Z0) ) + tr ( (2 \u221a 2\u2212 2)(H>H)(Z>Z) ) ,\nwhere in the fourth line we used the property that the trace is invariant under cyclic permutations and H>Z0 = Z0 > H .\nSince Z0>Z0 is positive semidefinite, tr((H>H)(Z0>Z0)) is nonnegative. Hence,\u2225\u2225\u2225Z0Z0> \u2212 Z?Z?>\u2225\u2225\u22252 F \u2265 (2 \u221a 2\u2212 2) tr ( (H>H)(Z>Z)\n) = (2 \u221a 2\u2212 2) \u2225\u2225HZ>\u2225\u22252 F \u2265 (2 \u221a\n2\u2212 2) \u2016H\u20162F \u03c3r = (2 \u221a 2\u2212 2)\u03c3rd(Z0, Z?)2.\nIf \u03b4 \u2264 \u03c3r 4 \u221a r , then\nd(Z0, Z?)2 \u2264 \u2225\u2225Z0Z0 \u2212 Z?Z?>\u2225\u22252 F\n(2 \u221a 2\u2212 2)\u03c3r \u2264 2r\u03b4\n2\n(2 \u221a 2\u2212 2)\u03c3r \u2264 3 16 \u03c3r."}, {"heading": "F Sample Complexity", "text": "In this section, we verify that our assumptions hold with high probability if m \u2265 cn log n, where c is a constant that depends on \u03b4, r, and \u03ba. Our proof relies on the following concentration inequality.\nTheorem 8. (Matrix Bernstein Inequality [20]) Let S1, . . . , Sm be independent random matrices with dimension n \u00d7 n. Assume that E(Si) = 0 and \u2016Si\u2016 \u2264 L, for all i \u2208 [m]. Let \u03bd2 =\nmax {\u2225\u2225\u2211m i=1 E(SiS>i ) \u2225\u2225 ,\u2225\u2225\u2211mi=1 E(S>i Si)\u2225\u2225}. Then for all \u03b4 \u2265 0, P\n(\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 Si \u2225\u2225\u2225\u2225\u2225 \u2265 \u03b4 ) \u2264 2n exp ( \u2212m2\u03b42 \u03bd2 + Lm\u03b4/3 ) .\nWe first give a technical lemma that we will use later.\nLemma 8. Let A = (aij) be a random matrix drawn from GOE. Let S = a11A \u2212 2e1e>1 . There exist absolute constants C, \u03c1 such that with probability at least 1\u2212 Ce\u2212\u03c1n, we have\n\u2016S\u2016 \u2264 18n.\nProof. Let A\u0303 = A \u2212 a11e1e>1 . S = a11A\u0303 + (a211 \u2212 2)e1e>1 . Note that a11 and A\u0303 are independent, hence \u2016S\u2016 \u2264 |a11|\u2016A\u0303\u2016 + |a211 \u2212 2|. Besides, since a11 \u223c N (0, 2), we can see that a211/2 is \u03c72 distributed.\nFirst we bound the operator norm of A\u0303. We rewrite \u2016A\u0303\u2016 as\n\u2016A\u0303\u2016 = max \u2016u\u2016=1 |u>A\u0303u| = max \u2016u\u2016=1 |u>Du\u2212 du21| \u2264 \u2016D\u2016+ |d|,\nwhere D = A\u0303+ de1e>1 , d \u223c N (0, 2). As D is GOE distributed, by Lemma 4,\nP ( \u2016D\u2016 > 3 \u221a n ) \u2264 C \u2032e\u2212\u03c1\u2032n, (14)\nwhere C \u2032 and \u03c1\u2032 are absolute constants. Using the Gaussian tail inequality, we have\nP ( |d| > 2 \u221a n ) \u2264 2e\u2212n. (15)\nCombining inequalities (14) and (15), we have P ( \u2016A\u0303\u2016 > 5 \u221a n ) \u2264 P ( \u2016D\u2016 > 3 \u221a n \u2228 |d| > 2 \u221a n ) \u2264 C \u2032e\u2212\u03c1\u2032n + 2e\u2212n, (16)\nwhere the last inequality follows from the union bound. Next we bound the deviation of the \u03c72 term. By the corollary of Lemma 1 in Laurent and Massart [13], we have\nP(|a211 \u2212 2| > 4( \u221a n+ n)) \u2264 2e\u2212n. (17)\nSince a11 is identically distributed as d, inequality (15) holds for a11 as well. Namely, P (|a11| > 2 \u221a n) \u2264 2e\u2212n. Combining this with inequalities (17), (16), we have\nP ( \u2016S\u2016 \u2264 14n+ 4 \u221a n ) \u2265 1\u2212 6e\u2212n \u2212 C \u2032e\u2212\u03c1\u2032n.\nFinally, the statement is obtained by choosing proper C, \u03c1, and using \u221a n \u2264 n.\nF.1 Proof of Theorem 6 Proof. It is equivalent to show that for any unit vector u, with high probability,\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (u>Aiu)Ai \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r\u03c31 . If P is an orthonormal matrix, then\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 ( (Pu)>Ai(Pu) ) Ai \u2212 2(Pu)(Pu)> \u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 ( u>(P>AiP )uAi ) \u2212 2Puu>P>\n\u2225\u2225\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 u>(P>AiP )uP >AiP \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 u>A\u0303iuA\u0303i \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 ,\nwhere in the second line we use unitary invariance of the operator norm, and in the last line we denote P>AiP by A\u0303i. Since the GOE is invariant under orthogonal conjugation, A\u0303i and Ai are identically distributed. Hence, it suffices to prove the claim when u = e1, i.e.\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 a (i) 11Ai \u2212 2e1e>1\n\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40, where a(i)11 is the (1, 1) entry of Ai and \u03b40 = \u03b4 r\u03c31 .\nTo show this, we apply Theorem 8, where Si = a (i) 11Ai \u2212 2e1e>1 . This requires that the operator norm of Si is bounded, for each i. We address this by noticing that with high probability \u2016Si\u2016 \u2264 18n, \u2200i. To be precise, by Lemma 8 there exist constants C, \u03c1, such that\nP (\u2016Si\u2016 > 18n) \u2264 Ce\u2212\u03c1n, i = 1, . . . ,m.\nTaking the union bound over all the Sis leads to P (\nmax i \u2016Si\u2016 > 18n\n) \u2264 mCe\u2212\u03c1n. (18)\nNext, we calculate \u03bd2 = \u2016 \u2211m\ni=1 E(S2i )\u2016 = m \u2016E(S21)\u2016. Let A = (aij) denote A1, S denote S1. We have E(S2) = E(a112A2)\u2212 4e1e>1 , and(\na211A 2 ) 11 = a411 + n\u2211 k=2 a211a 2 1k,\n( a211A 2 ) ii = a211 ( a2ii + n\u2211 k 6=i a2ik ) , \u2200i 6= 1,\n( a211A 2 ) ij = a211 n\u2211 k=1 aikajk, \u2200i 6= j.\nIt is easy to see that E(a211A2) = diag(2n+10, 2n+2, . . . , 2n+2). Consequently, \u03bd2 = (2n+6)m. By Theorem 8, if m \u2265 42\nmin(\u03b420 ,\u03b40) \u00b7 n log n, then\nP (\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 Si \u2225\u2225\u2225\u2225\u2225 \u2265 \u03b40 ) \u2264 2n exp ( \u2212m\u03b420 2n(1 + 3\u03b40) + 6 ) \u2264 2n exp ( \u2212m\u03b420\n2n(4 + 3\u03b40) ) \u2264 2n exp ( \u2212m\u03b420\n14n \u00b7max(1, \u03b40) ) \u2264 2 n2 .\n(19)\nCombining inequalities (18) and (19), we conclude that\nP (\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 a (i) 11Ai \u2212 2e1e>1 \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40 ) \u2265 1\u2212mCe\u2212\u03c1n \u2212 2 n2 .\nF.2 Proof of Theorem 7 The formulation of the second order partial derivatives and their expectations is given in Appendix B.\nIt is easy to see that for anyZ \u2208 S, maxs\u2208[r] \u2016z\u0304r\u2016 \u2264 \u221a \u03c31. Thus it is sufficient to prove that for\nany two unitary vector u and y with high probability it holds that\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2Aiuy >Ai \u2212 2u>yI \u2212 2yu> \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4r\u03c31 . We can decompose y as y = \u03b2u + \u03b2\u22a5u\u22a5 for a certain unit vector u\u22a5 that is orthogonal to u, where \u03b22 + \u03b22\u22a5 = 1. Let \u03b40 = \u03b4\n2r\u03c31 . It suffices to prove the following two claims.\n(i) For any unitary vector u, with high probability\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2Aiuu >Ai \u2212 2I \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40. (ii) For any two orthogonal unit vectors u and u\u22a5, with high probability\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2Aiuu > \u22a5Ai \u2212 2u\u22a5u> \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40.\nProof of (i)\nIf P is an orthonormal matrix, then\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2AiPuu >PAi \u2212 2I \u2212 2Puu>P> \u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2P>AiPuu >P>AiP \u2212 2I \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2A\u0303iuu >A\u0303i \u2212 2I \u2212 2uu> \u2225\u2225\u2225\u2225\u2225 , where A\u0303i and Ai have the same distribution. Hence we only need to prove the case where u = e1:\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2v(i)v(i) > \u2212 2I \u2212 2e1e>1\n\u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40, where v(i) = Aie1 is the first column of Ai.\nLet Si = 2(v(i)v(i) > \u2212 I \u2212 e1e>1 ). To apply Theorem 8, we need to show that with high probability \u2016Si\u2016 is bounded for each i and calculate \u03bd2 = \u2016 \u2211n\ni=1 E(S2i )\u2016 = m \u2016E(S21)\u2016. Let S, v, A denote S1, v(1), and A(1) respectively. It is easy to see that\n\u2016S\u2016 \u2264 2 \u2016v\u20162 + 4 = 2(w + a211) + 4, where w = \u2211n k=2 a 2 1k. As a11 \u223c N (0, 2), a1k \u223c N (0, 1) for k 6= 1, we can see that a211/2 and w are \u03c72 distributed with degrees of freedom 1 and n \u2212 1, respectively. Using the \u03c72 tail bound, we have\nP ( a211/2 > 2( \u221a n+ n) + 1 ) \u2264 e\u2212n,\nP (w > 5n\u2212 1) \u2264 e\u2212n, k = 2, . . . , n. It follows from the union bound that\nP (\u2016S\u2016 > 26n+ 6) \u2264 2e\u2212n, and consequently\nP (\nmax i \u2016Si\u2016 > 26n+ 6\n) \u2264 2me\u2212n. (20)\nTo calculate \u03bd2, we expand E(S2) as E(S2) = 4E ( (vv>)2 ) \u2212 4(I + e1e>1 )2\n= 4E ( \u2016v\u20162 vv> ) \u2212 4(I + 3e1e>1 ).\nSome simple calculations show that( \u2016v\u20162 vv> ) 11 = v1 4 + n\u2211 k=2 vk 2v1\n2,( \u2016v\u20162 vv> ) jj = v1 2vj 2 + vj 4 + \u2211 k 6=1,j vk 2vj 2, j = 2, . . . , n,\n( \u2016v\u20162 vv> ) jl = n\u2211 k=1 vk 2vjvl, j < l.\nAs v1 \u223c N (0, 2), vj \u223c N (0, 1) for j 6= 1,\nE ( \u2016v\u20162 vv> ) 11 = 2n+ 10,\nE ( \u2016v\u20162 vv> ) jj = n+ 3, j = 2, . . . , n,\nE ( \u2016v\u20162 vv> ) jl = 0, j < l.\nHence, E(S2) = diag(8n+ 24, 4n+ 8, . . . , 4n+ 8) and thus \u03bd2 = m(8n+ 24). If m \u2265 (128/min(\u03b420, \u03b40))n log n, then by applying Theorem 8 we can see\nP (\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2v(i)v(i) > \u2212 2I \u2212 2e1e>1 \u2225\u2225\u2225\u2225\u2225 > \u03b40 ) \u2264 2n exp ( \u2212m\u03b420 8n+ 24 + (26 3 n+ 2)\u03b40 ) \u2264 2n exp ( \u2212m\u03b420\n(128/3)nmax(1, \u03b40) ) \u2264 2 n2 .\n(21)\nCombining inequalities (21) and (20) leads to\nP (\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2v(i)v(i) > \u2212 2I \u2212 2e1e>1 \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40 ) \u2265 1\u2212 2me\u2212n \u2212 2 n2 .\nProof of (ii)\nWe only need to prove the case where u = e1 and u\u22a5 = e2 due to the same reason above. That is,\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2v(i)q(i) > \u2212 2e2e>1 \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40, where v(i) and q(i) are the first and second columns of Ai.\nAs before, let Si = 2(v(i)q(i) > \u2212 e2e>1 ) and let S, v, q, A denote S1, v(1), q(1) and A(1) respectively. From the proof of (i), we can see that with probability at least 1 \u2212 4e\u2212n both \u2016v\u2016 and \u2016q\u2016 are no larger than \u221a 13n+ 1. Since \u2016S\u2016 \u2264 2 \u2016v\u2016 \u2016q\u2016+ 2, we have\nP (\nmax i \u2016Si\u2016 \u2265 26n+ 4\n) \u2264 4me\u2212n.\nNext, we calculate \u03bd2 = mmax {\u2225\u2225E(SS>)\u2225\u2225 ,\u2225\u2225E(S>S)\u2225\u2225}.\nE(SS>) = 4E(\u2016q\u20162)E(vv>) + 4e2e>2 .\nE(S>S) = 4E(\u2016v\u20162)E(qq>) + 4e1e>1 .\nSome simple calculation shows that E(\u2016v\u20162) = E(\u2016q\u20162) = n + 1, E(vv>) = I + e1e>1 and E(qq>) = I + e2e>2 . Hence,\nE(SS>) = 4(n+ 1)I + 4(n+ 1)e1e>1 + 4e2e>2 ,\nE(S>S) = 4(n+ 1)I + 4(n+ 1)e2e>2 + 4e1e>1 ,\nand \u03bd2 = 8(n+ 1)m. If m \u2265 78 min(\u03b420 ,\u03b40) n log n, then by applying Theorem 8 we have\nP (\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2v(i)q(i) > \u2212 2e1e>2 \u2225\u2225\u2225\u2225\u2225 > \u03b40 ) \u2264 2n exp ( \u2212m\u03b420 8n+ 8 + (26n+4 3 )\u03b40 ) \u2264 2n exp ( \u2212m\u03b420\n26nmax(1, \u03b40) ) \u2264 2 n2 .\n(22)\nThis means,\nP (\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 2v(i)q(i) > \u2212 2e1e>2 \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b40 ) \u2265 1\u2212 4me\u2212n \u2212 2 n2 ."}, {"heading": "G ADMM for Nuclear Norm Minimization", "text": "We reformulate the nuclear norm minimizing problem as\nmin X\u2208Rn\u00d7n\n1\n2\u03bb \u2016A(X)\u2212 b\u20162 + \u2016X\u2016\u2217 , (23)\nwhere \u03bb > 0 is the regularization parameter. \u03bb\u2192 0 will enforce the minimizer X\u2217nuc satisfying the affine constraint A(X\u2217nuc) = b.\nWe apply ADMM to the dual problem of (23):\nmin \u03b1\u2208Rm,V \u2208Rn\u00d7n\n\u03bb 2 \u2016\u03b1\u20162 \u2212 \u03b1>b\nsubject to \u2016V \u2016 \u2264 1 A>(\u03b1) = V,\n(24)\nwhere we introduce an auxiliary variable V to make this problem equality constrained. The augmented Lagrangian of problem (24) can be written as\nL\u03b7(\u03b1,X) = \u03bb\n2 \u2016\u03b1\u20162 \u2212 \u03b1>b+ 1\u2016\u00b7\u2016\u22641(V ) + \u3008X,A>(\u03b1)\u2212 V \u3009+\n\u03b7\n2\n\u2225\u2225A>(\u03b1)\u2212 V \u2225\u22252 F ,\nwhere X is the multiplier, \u03b7 is the penalty parameter, and 1\u2016\u00b7\u2016\u22641 is the indicator function of the unit spectral norm ball i.e. 1\u2016\u00b7\u2016\u22641(V ) equals 0 if \u2016V \u2016 \u2264 1 and +\u221e otherwise.\nLet vec(\u00b7) denote the vectorization of a matrix, whose inverse mapping is denoted by mat(\u00b7). We can rewrite the transformations asA(X) = Avec(X) andA>(\u03b1) = mat(A>\u03b1) = \u2211m i=1 \u03b1iAi, where A is a m\u00d7 n2 matrix whose ith row is vec(Ai)>. The ADMM starts from initialization (\u03b10, V 0, X0) and updates the three variables alternately. The updates can be computed in close forms:\n\u03b1k+1 = (\u03bbI + \u03b7AA>)\u22121 ( b+ Avec ( \u03b7V k \u2212Xk )) ,\nV k+1 = proj ( m\u2211\ni=1\n\u03b1k+1i Ai +X k/\u03b7\n) ,\nXk+1 = Xk + \u03b7 ( m\u2211 i=1 \u03b1k+1i Ai \u2212 V k+1 ) ,\nwhere proj(\u00b7) is the projection onto the unit spectral norm ball. Let X = U\u03a3V > be the singular value decomposition of X ,\nproj(X) = U min(\u03a3, 1)V >.\nIn fact, the update of V can be combined with other steps without being computed explicitly. One only has to iterate the following two steps:\n\u03b1k+1 = (\u03bbI + \u03b7AA>)\u22121 ( b+ Avec ( \u03b7 \u2211 i=1 \u03b1kiAi +X k\u22121 \u2212 2Xk )) ,\nXk+1 = prox\u03b7\n( \u03b7 m\u2211 i=1 \u03b1k+1i Ai +X k ) ,\nwhere prox\u03b7(\u00b7) is the singular value soft-thresholding operator defined as\nprox\u03b7(X) = U max(\u03a3\u2212 \u03b7, 0)V >.\nThe sequence of multipliers { Xk }\nconverges to the primal solution of (23). To speed up the update of \u03b1, the Cholesky decomposition of \u03bbI + \u03b7AA> is precomputed in our implementation."}], "references": [{"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Arash A. Amini", "Martin J. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression", "author": ["Francis Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["Francis Bach", "Eric Moulines"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["Samuel Burer", "Renato DC Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1956}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1407.1065,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G. Lanckriet"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X. Goemans", "David P. Williamson"], "venue": "Journal of the ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic variational inference", "author": ["Matt Hoffman", "David M. Blei", "Chong Wang", "John Paisley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Guaranteed rank minimization via singular value projection", "author": ["Prateek Jain", "Raghu Meka", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "Annals of Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Small deviations for beta ensembles", "author": ["Michel Ledoux", "Brian Rider"], "venue": "Electron. J. Probab.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Rank minimization via online learning", "author": ["Raghu Meka", "Prateek Jain", "Constantine Caramanis", "Inderjit S Dhillon"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo"], "venue": "SIAM review,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Estimation of low-rank tensors via convex optimization", "author": ["Ryota Tomioka", "Kohei Hayashi", "Hisashi Kashima"], "venue": "arXiv preprint arXiv:1010.0789,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": "arXiv preprint arXiv:1501.01571,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "SDPs often arise naturally from the problem structure, or are derived as surrogate optimizations that are relaxations of difficult combinatorial problems [7, 1, 8].", "startOffset": 154, "endOffset": 163}, {"referenceID": 0, "context": "SDPs often arise naturally from the problem structure, or are derived as surrogate optimizations that are relaxations of difficult combinatorial problems [7, 1, 8].", "startOffset": 154, "endOffset": 163}, {"referenceID": 7, "context": "SDPs often arise naturally from the problem structure, or are derived as surrogate optimizations that are relaxations of difficult combinatorial problems [7, 1, 8].", "startOffset": 154, "endOffset": 163}, {"referenceID": 2, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 1, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 9, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 5, "context": "In many areas of machine learning and signal processing such as classification, deep learning, and phase retrieval, gradient descent methods, in particular first order stochastic optimization, have led to remarkably efficient algorithms that can attack very large scale problems [3, 2, 10, 6].", "startOffset": 279, "endOffset": 292}, {"referenceID": 17, "context": "This problem is a direct generalization of compressed sensing, and subsumes many machine learning problems such as image compression, low rank matrix completion and low-dimensional metric embedding [18, 12].", "startOffset": 198, "endOffset": 206}, {"referenceID": 11, "context": "This problem is a direct generalization of compressed sensing, and subsumes many machine learning problems such as image compression, low rank matrix completion and low-dimensional metric embedding [18, 12].", "startOffset": 198, "endOffset": 206}, {"referenceID": 14, "context": "Without conditions on the transformation A or the minimum rank solution X, it is generally NP hard [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "Existing methods, such as nuclear norm relaxation [18], singular value projection (SVP) [11], and alternating least squares (AltMinSense) [12], assume that a certain restricted isometry property (RIP) holds for A.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "Existing methods, such as nuclear norm relaxation [18], singular value projection (SVP) [11], and alternating least squares (AltMinSense) [12], assume that a certain restricted isometry property (RIP) holds for A.", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "Existing methods, such as nuclear norm relaxation [18], singular value projection (SVP) [11], and alternating least squares (AltMinSense) [12], assume that a certain restricted isometry property (RIP) holds for A.", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "In the random measurement setting, this essentially means that at least O(r(n + p) log(n + p)) measurements are available, where r = rank(X) [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "[6], and develop a gradient descent algorithm for optimizing f(Z), using a carefully constructed initialization and step size.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "3 Related Work Burer and Monteiro [4] proposed a general approach for solving semidefinite programs using factored, nonconvex optimization, giving mostly experimental support for the convergence of the algorithms.", "startOffset": 34, "endOffset": 37}, {"referenceID": 17, "context": "[18], based on replacing the rank function by the convex surrogate nuclear norm, as already mentioned in the previous section.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The most popular algorithms are proximal methods that perform singular value thresholding [5] at every iteration.", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "[11] proposed a projected gradient descent algorithm SVP (Singular Value Projection) that solves min X\u2208Rn\u00d7p \u2016A(X)\u2212 b\u2016 subject to rank(X) \u2264 r, where \u2016\u00b7\u2016 is the `2 vector norm and r is the input rank.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposes an alternating least squares algorithm AltMinSense that avoids the per-iteration SVD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "It is parallel to the Wirtinger Flow (WF) algorithm for phase retrieval [6], to recover a complex vector x \u2208 C given the squared magnitudes of its linear measurements bi = |\u3008ai, x\u3009|, i \u2208 [m], where a1, .", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "[6] propose a first-order method to minimize the sum of squared residuals", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "To obtain a sufficiently accurate initialization, we use a spectral method, similar to those used in [17, 6].", "startOffset": 101, "endOffset": 108}, {"referenceID": 5, "context": "To obtain a sufficiently accurate initialization, we use a spectral method, similar to those used in [17, 6].", "startOffset": 101, "endOffset": 108}, {"referenceID": 15, "context": "In the analysis of convex functions, Nesterov [16] shows that for unconstrained optimization, the gradient descent", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "The underlying truth is Z = [1, 1]>.", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "The underlying truth is Z = [1, 1]>.", "startOffset": 28, "endOffset": 34}, {"referenceID": 15, "context": "This provides a local regularity property that is similar to the Nesterov [16] criteria that the objective function is strongly convex and has a Lipschitz continuous gradient.", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "[19]; see Appendix G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] to compute the low rank SVD.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The results suggest that the sample complexity of our method should also scale as O(rn log n) as for SVP and the nuclear norm approach [11, 18].", "startOffset": 135, "endOffset": 143}, {"referenceID": 17, "context": "The results suggest that the sample complexity of our method should also scale as O(rn log n) as for SVP and the nuclear norm approach [11, 18].", "startOffset": 135, "endOffset": 143}, {"referenceID": 5, "context": "Building on a recently proposed first-order algorithm for phase retrieval [6], we develop a gradient descent procedure for rank minimization and establish convergence to the optimal solution with O(rn log n) measurements.", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "More broadly, the technique used in this paper\u2014factoring the semidefinite matrix variable, recasting the convex optimization as a nonconvex optimization, and applying first-order algorithms\u2014first proposed by Burer and Monteiro [4], may be effective for a much wider class of SDPs, and deserves further study.", "startOffset": 227, "endOffset": 230}, {"referenceID": 0, "context": "References [1] Arash A.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Francis Bach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Francis Bach and Eric Moulines.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Samuel Burer and Renato DC Monteiro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Emmanuel Cand\u00e8s, Xiaodong Li, and Mahdi Soltanolkotabi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Michel X.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Matt Hoffman, David M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Prateek Jain, Raghu Meka, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Beatrice Laurent and Pascal Massart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Michel Ledoux and Brian Rider.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Raghu Meka, Prateek Jain, Constantine Caramanis, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Yurii Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Ryota Tomioka, Kohei Hayashi, and Hisashi Kashima.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "(Matrix Bernstein Inequality [20]) Let S1, .", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "By the corollary of Lemma 1 in Laurent and Massart [13], we have P(|a11 \u2212 2| > 4( \u221a n+ n)) \u2264 2e\u2212n.", "startOffset": 51, "endOffset": 55}], "year": 2016, "abstractText": "We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. WithO(r3\u03ba2n log n) random measurements of a positive semidefinite n\u00d7nmatrix of rank r and condition number \u03ba, our method is guaranteed to converge linearly to the global optimum.", "creator": "LaTeX with hyperref package"}}}