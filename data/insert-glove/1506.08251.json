{"id": "1506.08251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2015", "title": "Occam's Gates", "abstract": "nasira We present surpassing a youlus complimentary zerby objective for diaochan training recurrent neural theocrats networks (RNN) with gating themsleves units luzhin that mounds helps follow with 33.85 regularization bettles and interpretability of the trained ingrate model. Attention - natallia based vampire-like RNN models antyukh have shown 47.95 success front-drive in many perforating difficult sequence to sequence classification problems with cipher long magyar and short term barchas dependencies, air-defense however introverted these models are prone to overfitting. ujazdowski In this rovell paper, airsoft we 84-page describe okechukwu how to bogdanich regularize these models through agbar an L1 panha penalty lourinh\u00e3 on single-disc the activation kayleigh of d'arras the gating mccrery units, publicnytimes.com and fishergate show kwhs that kamte this scalfaro technique reduces leitzinger overfitting unzen on brabourne a delgaudio variety colourful of okkalapa tasks playwright while also providing hudna to us a pulso human - kabaka interpretable visualization of severin the altdorf inputs tackler used by 77.59 the network. These sarno tasks spira include non-spanish sentiment puy-en-velay analysis, ree paraphrase recognition, and boyer-ahmad question answering.", "histories": [["v1", "Sat, 27 Jun 2015 03:03:10 GMT  (385kb,D)", "http://arxiv.org/abs/1506.08251v1", "In review at NIPS"]], "COMMENTS": "In review at NIPS", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jonathan raiman", "szymon sidor"], "accepted": false, "id": "1506.08251"}, "pdf": {"name": "1506.08251.pdf", "metadata": {"source": "CRF", "title": "Occam\u2019s Gates", "authors": ["Jonathan Raiman", "Szymon Sidor"], "emails": ["jraiman@mit.edu", "sidor@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].\nWhile there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition. The gates modulating the network\u2019s attention in these networks serve a dual purpose: first they allow control of the information flow, and second, and perhaps more crucially, the gates communicate problem structure by ensuring that specific groups of neurons activate or go dormant jointly. For instance, in the case of prediction from a sequence of words, it is expected to find that certain words are predictive while others not; if this word sequence is projected using an embedding matrix into word vectors, then by the same logic all the dimensions of superfluous words\u2019 vectors should be wiped out entirely.\nIntuitively, this Occam\u2019s Razor observation can be translated into considering that the activation of gating units should be as sparse as possible when not all the words or information units are necessary. The main focus of this paper is to show how to enforce sparsity on gating units by adding an unsupervised training objective: the sum of the activations of the gating units gi weighed by a hyper-parameter \u03bbsparse that controls the tradeoff between the original objective function J and the sparsity criterion:\nJ\u2217 = J + \u03bbsparse \u00b7 \u2211 i gi.\nIn this work, we show that enforcing gate sparsity improves generalization in RNNs while also providing useful visualisations of the problem, and evaluate this approach on three different problems."}, {"heading": "2 Related Work", "text": "The work we are presenting is closely related to two areas of Machine Learning research: RNN regularization and attention-based models.\nar X\niv :1\n50 6.\n08 25\n1v 1\n[ cs\n.L G\n] 2\n7 Ju\nn 20\n15"}, {"heading": "2.1 RNN Regularization", "text": "RNN regularization has recently been shown to be achievable using Dropout [11] by regularizing a subset of the recurrent connections in deep RNNs [12, 13]. Previously, it was shown that weight decay regularization only provided small improvement [5] and dropout noise was detrimental when applied to all connections due to the compounding of errors over time [14]. In this work, we show that this problem can also be solved using a deterministic approach by penalizing gate activations from deep RNNs. As a result, RNNs can now benefit from multiple regularization techniques in varying architectures."}, {"heading": "2.2 Attention-Based Models", "text": "In recent years, there has been a wealth of evidence that attention-based techniques can improve the performance of machine learning models. Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].\nIn certain cases the models are trained with supervision on the gates [1, 16], however in many cases there is no supervised data for the attentional component. Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation [10], using reinforcement learning [9] to optimize a visual tracking strategy [3], or leaving this part semi-supervised through the primary objective. Our work resembles the observation prior of [10], where we favor input gates being closed and penalize deviation with a penalty of our choosing. Similarly to the annealed Dropout from [18], we also consider a gradual increase in the sparsity penalty during training to encourage early exploration."}, {"heading": "3 Problem Statement", "text": "A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23]. The main problem we are trying to solve in this paper is improving generalization performance when performing these types of classical or structured prediction tasks using RNNs. In sections below we describe three different sequence classification problems used to evaluate our approach."}, {"heading": "3.1 Sentiment Analysis", "text": "The central problem in sentiment analysis is correctly identifying and extracting the attitude or emotional tone of a speaker in the context of a particular topic or domain.\nHere we consider predicting the sentiment expressed in the Stanford Sentiment Treebank (SST) [24], a collection of 11,855 sentences extracted from movie reviews. This dataset is made up of the sentiment annotations from 5 classes: {terrible, bad, neutral, good, terrific}, for the 215,154 unique sub-phrases obtained after parsing each sentence using the Stanford Parser. In our work we do not make use of the parse trees, and instead treat each sub-phrase as a labeled sequence of words."}, {"heading": "3.2 Paraphrase Recognition", "text": "In Paraphrase Recognition the problem is it to predict how semantically similar two phrases are from 0 to 1. This task can either be seen as regression or binary classification, and the goal is measured as the Pearson correlation with human annotations or recalling correct paraphrase pairs.\nHere we focus on paraphrase detection on the SemEval 2014 shared task 1 dataset [25] which includes 9927 sentence pairs in a 4500/500/4927 train/dev/test split. Each sentence is annotated with a score c \u2208 [1, 5], with 5 indicating the pair is a paraphrase, and 1 that the pair is unrelated. We additionally train using paraphrase pairs from the wikianswers paraphrase corpus [26]."}, {"heading": "3.3 Question Answering", "text": "Facebook AI Research recently proposed a set of 20 tasks designed to be \u201cprerequisites\u201d for any system \u201ccapable of conversing with human\u201d [27]. The dataset for each task is a set of stories each composed of many facts, with some marked as relevant, a question and the correct answer.\nDaniel and Sandra journeyed to the office. Then they went to the garden. Sandra and John travelled to the kitchen. After that they moved to the hallway. Where is Daniel? A: garden\nThe football fits in the suitcase. The suitcase fits in the cupboard. The box of chocolates is smaller than the football. Will the box of chocolates fit in the suitcase? A:yes\nThe tasks are synthetic and lack noisy nature of real-world natural processing, which makes them easy to solve with hand engineered systems, however the open question is how to create a model capable of solving these tasks without any manual feature engineering for particular problems."}, {"heading": "4 Approach", "text": "In order to improve RNN performance over unseen data apply Occam\u2019s Razor over our training data by finding in each example a minimal set of useful inputs over time. To achieve this property we apply gates to the different observations of the input sequence to allow the network to keep or erase a timestep\u2019s input. For instance, in a sentiment classification problem, gates would ideally fire only for emotionally loaded words, and stay dormant otherwise.\nBecause our approach relies on gates, we make the assumption that the vector input at each time-step is an inseparable information unit, like a word, image, or fact. If this assumption holds, then when we force the network to reduce its gate usage by penalizing the sum of those activations, we will obtain a solution in a local optima where gates are less often active, which should generalize better.\nWe formalise our approach by describing how we enforce sparsity on the gate activations for a variety of RNNs. Then we introduce the RNNs considered for the different tasks in this paper. Finally we explain the sparsity-enforcing objective function and our different annealing regimens during training."}, {"heading": "4.1 Gated LSTMs", "text": "In our work we make extensive use of Long-Short Term Memory networks [28], a popular RNN architecture specifically designed to capture long range dependencies and alleviate training difficulties [29]. Since their introduction in 1995, many variants have been proposed [30], however for the purposes of this research we found that the vanilla version from [30] worked best.\nWhile LSTMs are capable of selectively remembering or forget parts of their memory and input, they lack the ability to transform uniformly their input. We extend LSTMs to include an additional gate, goccam, that uniformly multiplies all the inputs simultaneously. In Table 1 we present equations for the Gated-LSTM, with the differences with the regular LSTM highlighted in red. We use the following denotations: \u03c3(\u00b7) for the logistic sigmoid function, Wi,z,f,o and Ri,z,f,o for matrices, and~bz,i,f,o for vectors.\nThe gating function fgate(\u00b7) can take various forms. Two examples we consider are linear function of the input ~xt and a second order gate capable of capturing higher-order interaction:\nflinear(~xt,~ht\u22121) = \u03c3(~p T \u00b7 ~xt + ~qT \u00b7 ~ht\u22121 + b)\nfquad(~xt,~ht\u22121) = \u03c3(~h T \u00b7W \u00b7 ~xT + ~pT \u00b7 ~xt + ~qT \u00b7 ~ht\u22121 + b).\nAdditionally, we consider Gated Stacked LSTMs, a variant of Stacked LSTMs [31, 5, 20], where the input the lowest LSTM is gated using the hidden state from the topmost LSTM of the previous timestep. The equation for this modification is as follows, with l \u2208 {1, lmax}, the LSTM level:\ngoccam = fgate(~xt,~h(lmax,t\u22121))."}, {"heading": "4.2 Hierarchical Gated LSTMs", "text": "In this section we introduce Hierarchical Gated LSTM (HG-LSTM), a gated attention model that uses Gated LSTMs as a central building block. In the previous section we introduced Gated LSTMs that are able to selectively ignore or include the entire input at a timestep, however for many tasks where the information presented can be subdivided into larger chunks such as sentences, paragraphs, or episodes, a similar gating procedure could be applied to these higher levels of abstraction. For example to find the answer to question about a story in the bAbI dataset, such a model would benefit from being selective about which words and facts to listen to.\nHG-LSTM consists of two submodels: a Fact model and High-Level model (HL model), which are both Gated LSTMs. Figure 1 presents the architecture. Every word in a fact sequence is projected using an embedding matrix and processed by the Fact model. The final hidden state of the Fact model for each fact is then passed to the HL model as an input vector. We consider the final hidden state of the HL model after reading each fact representation to be a the high-level representation for the entire sequence of facts. The hierarchy of the submodels explicitly leverages the problem structure, and allows fine grain attention control at two levels of abstraction."}, {"heading": "4.3 Sparsity Penalty", "text": "The original training objective J is augmented with the sparsity penalty and the resulting objective is optimized through gradient descent. The penalty is constructed by summing the activations of the gates presented in 4.1, and weighing them by a parameter \u03bbsparse chosen through hyperparameter search:\nJ\u2217 = J + \u03bbsparse \u00b7 n\u2211\ni=1\ngoccam,i."}, {"heading": "4.4 Training Regimens", "text": "Our approach\u2019s ultimate goal is to preserve network expressivity while making it robust against changes in the input. However, forcing sparsity too soon can do more harm than good: a greedy and locally optimal solution is forcing all gates to be closed. To prevent this from happening we encourage early exploration by progressively increasing the sparsity penalty, \u03bbsparse. We investigated 2 different annealing regimens: a linear and a quadratic increase up to \u03bbmax at training epoch Tmax,\nas shown below with e the training epoch:\n\u03bbsparse(e) =  \u03bbmax flat regimen min{(e/Tmax) \u00b7 \u03bbmax, \u03bbmax} linear regimen min{(e/Tmax)2 \u00b7 \u03bbmax, \u03bbmax} quadratic regimen"}, {"heading": "5 Experiments", "text": "The code needed to run the experiments in this paper are available online at https://www. github.com/JonathanRaiman/Dali 1."}, {"heading": "5.1 Sentiment Analysis", "text": "For this problem our model is a Gated LSTM that reads each sequence of words sequentially, and uses the last hidden vector as input to a softmax linear classifier, and our target is to minimize the Kullback-Leibler divergence with the correct label along with the sparsity penalty.\nWe project each word using an embedding matrix into a 100 dimensional vector, and keep only the words that appear at least twice in our training data, with the remaining words replaced with a special unknown word, <UNK>. We train 3 different models with hidden sizes 25, 50, 150, and apply Dropout [11, 12] with probability p = 0.3 to the non recurrent connections of the LSTM. All models are trained using Adadelta [32] with \u03c1 = 0.95, and we perform early stopping when the accuracy stops increasing on the validation set."}, {"heading": "5.2 Paraphrase Detection", "text": "For paraphrase prediction we also employ Gated LSTMs with the final Softmax layer removed. Each sentence in a pair is fed to a separate LSTM and our objective is to minimize the squared difference between the true similarity t of the sentences and the dot product of the two LSTMs\u2019 final hidden states ~h1,~h2:\nJ = min\n{( ~hT1~\u0307h2 |~h1||\u0307~h2| \u2212 t )2} + \u03bbsparse \u00b7 ( \u2211n i=1 g1,i + \u2211n i=1 g2,i)\ninstead of a softmax linear classifier, we instead use the last hidden state of the LSTM."}, {"heading": "5.3 Facebook\u2019s bAbI dataset", "text": "For this problem we use an HG-LSTM to compute the high level representation of each story. The HG-LSTM takes a question, followed by the sequence of facts, and the final hidden state of the HG-LSTM is fed as input to an LSTM decoder that produces the answer sequentially and ends its prediction with an <EOS> symbol [5, 22].\nWe use separate a Gated-LSTM for question and facts when creating representations for the HighLevel model in the HG-LSTM. To make the question influence the High Level\u2019s input gates we average the embeddings of the words in the question and concatenate this with the fact representation and the current hidden state of the High Level model.\nOur error function is the sum of three separate objectives: Eprediction = \u2211 w\u2208Y \u2211 w\u0304 6=w max(\u03b3 \u2212 s(w) + s(w\u0304), 0)\nEfact = \u2211 i\u2208F log(gi) + \u00b5unsupporting \u2211 i/\u2208F log(1\u2212 gi)\nEword = \u2211 f\u2208F \u2211 w\u2208f |gw|\nPrediction error Eprediction defined as margin loss on every word of the output,where Y is a target sequence of words, s(w) is a score a particular word and \u03b3 is margin. We found that it significantly\n1The project is currently under heavy development, do not hesitate to ask the authors for help!\ndecreases training time compared to cross entropy error while achieving similar results. For fact selection errorEfact a set of supporting facts S is known, therefore rather than using sparsity penalty, we used cross entropy error between expected (1 for f \u2208 S and 0 otherwise) and actual gate activation. F is set of fact indexes, gi is activation of gate for fact i. The \u00b5unsupporting coefficient was introduced because authors reasoned that false negatives are potentially more harmful than false negatives for network learning process. Finally Eword is a L1 sparsity penalty for all the word gates in fact model. Symbol gw denotes gate activation for a particular word in a particular fact. We combine the errors into a single objective:\nE = Eprediction + \u03bbfactEfact + \u03bbwordEword\nOur precise parameters for the experiment were as follows: all word embeddings have 50 dimensions, we used Dropout with p = 0.5 in the High Level model and p = 0.3 for Question and Fact models. The Fact model has a hidden size of 30, while the High level model is a Gated StackedLSTM with 6 layers and a hidden size of 20. All the gates used are second order, fquad(\u00b7). We use the first 1000 examples for training as suggested in [27], and reserve 20% for validation. Our model is trained using AdaDelta [32], with \u03c1 = 0.95, and a minibatch size of 50. We perform early stopping when the validation score stops increasing."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Effects on performance", "text": "Occam\u2019s gates improve generalization on sentiment analysis (fig. 2), paraphrase prediction (fig. 4), and for the majority of bAbI question answering problems (fig. 6, Table 2). This effect is especially visible as model size increases (fig. 2, fig. 4). We find that without a sparsity penalty increasing model size has smaller effect, however using sparsity we manage to achieve 5% improvement on sentiment analysis and 18% on paraphrase prediction recall. Additionally for three arg. relations bAbI problem it increases the accuracy by 14%. We observe greater improvements on this task than the other two; notably, this task has longer sentences, and thus word gating is more present.\nMoreover, the sparsity annealing methods described in section 4.4 show improvements over a static objective function (fig. 3, fig. 5). In particular, the linear regimen improves the result by 1% for sentiment analysis, and by 7% for recall on paraphrase prediction.\nFinally, we observed that the HG-LSTM model significantly improves performance over the LSTM baseline from [27]. As visible in table 2, this model improves scores on 17 out of 20 problems. Moreover, HG-LSTMs with no penalties, \u03bbword = \u03bbfact = 0, yields worse results than those with penalties for the majority of the problems (17 out of 20 tasks). Our best results are achieved by using mixture of both fact detection penalisation and word sparsity (7 out of 20 task). The HGLSTM performs worse than Memory Networks (MemNN), however our model appears to be less computationally costly since we do not require branch and bound search to select supporting facts.\n0.0 1e-05 0.0001 0.0005 0.001 0.01 0.1 memory sparsity\n15 0\n50 25\nhi dd\nen\n37 40 39 41 41 40 42\n38 39 40 40 39 40 40\n37 38 38 39 38 39 40\nF1 score for training regimen vs. sparsity penalty \u03bb\n37\n38\n39\n40\n41\nFigure 3: Effect of sparsity regimen and sparsity penalty \u03bb on SST Root Accuracy."}, {"heading": "6.2 Interpretability", "text": "Ability to interpret the calculation carried out by Machine Learning models is crucial for advancing research. Especially for Neural Network models there are no well established methods for under-\n0.0 1e-05 0.0001 0.0005 0.001 0.01 0.1 memory sparsity\nsq ua\nre lin\nea r\nfla t\nsp ar\nsi ty\nr eg\nim en\n0.79 0.87 0.86 0.86 0.85 0.83 0.83\n0.79 0.81 0.89 0.77 0.86 0.76 0.95\n0.79 0.86 0.78 0.88 0.81 0.84 0.82\n0.80\n0.84\n0.88\n0.92\nFigure 5: Effect of sparsity regimen and penalty \u03bb on Paraphrase prediction.\n5. 0\n1. 0\n0. 1\n0. 01\n0. 0\nFa ct\nse le\nct io\nn\n40 38 43 33 23\n44 43 43 31 24\n46 42 40 30 24\n46 44 41 33 23\n45 45 47 24 24\nstanding its capabilities, although attempts have been made, e.g Hinton Diagrams [33]. We claim that Occam\u2019s Razors provide some insights into the way network operates on it\u2019s hidden state."}, {"heading": "6.2.1 Error analysis", "text": "Diagnosing and identifying the root cause of errors during model design is critical for finding with new research directions and making improvements. We believe using Occam\u2019s gates can help researchers gain insight into their network\u2019s workings. To support this claim let us consider an example from bAbi dataset where gates provide a visual indication of progress.\nIn Figure 8 we notice that the model upon reaching a validation accuracy of 20% is not yet capable of distinguishing important information from noise. At 60% accuracy it can now highlights the relevant facts, but the gates on words are not yet compelling. At 100% accuracy fact and word gates work in unison: the network activates for fact with the relevant person and words that contain location information. We hypothesize that LSTMs without gates can pick out the correct person and place, but Occam\u2019s gates help them ignore facts about persons irrelevant to the question."}, {"heading": "6.2.2 Relevancy detection", "text": "We argue that Occam\u2019s gates allow one to judge which pieces of information are relevant to a problem. To illustrate this claim we show two examples, both of which emerged when training the system on a paraphrase detection problem with a Character model Gated LSTM (Char Gated LSTM). Figure 9 supports the belief that the model makes use word boundaries, and figure 10 suggests that the network can ignore repetititve or superfluous characters.\nFigure 9: Char Gated LSTM, gate action shown with yellow highlighter. Model discovers tokenisation.\nFigure 10: Char Gated LSTM, gate action shown with yellow highlighter. Model focuses on upper case characters and ignores repeats."}, {"heading": "7 Conclusion", "text": "In this paper, we investigated the use of a complimentary objective function that forces attentionbased RNNs to be selective about their inputs. We showed on three different tasks that our approach improves generalization and interpretability of the trained models with respect their counterparts that do not use sparsity penalties. Additionally, to encourage early exploration and preserve sparsity, we designed an annealing objective function that provides benefits over a standard one.\nFinally, we introduced Hierarchical-Gated LSTM, a new model that performs significantly better than regular Stacked LSTMs; this network combines attentional and hierarchical components, and reasons at several levels of abstraction. Future work includes investigation of this model family, which shows promise towards advancing the state of the art."}], "references": [{"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Yin Zheng", "Richard S Zemel", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": "International Journal of Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "arXiv preprint arXiv:1312.4569,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "On fast dropout and its applicability to recurrent networks", "author": ["Justin Bayer", "Christian Osendorfer", "Daniela Korhammer", "Nutan Chen", "Sebastian Urban", "Patrick van der Smagt"], "venue": "arXiv preprint arXiv:1311.0701,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning generative models with visual attention", "author": ["Yichuan Tang", "Nitish Srivastava", "Ruslan R Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "The ibm 2015 english conversational telephone speech recognition system", "author": ["George Saon", "Hong-Kwang J Kuo", "Steven Rennie", "Michael Picheny"], "venue": "arXiv preprint arXiv:1505.05899,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "CoRR, abs/1505.08075,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Towards AI-complete question answering: A set of prerequisite toy", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "tasks. CoRR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "diagrams: Viewing connection strengths in neural networks", "author": ["Frederick J Bremner", "Stephen J Gotts", "Dina L Denham. Hinton"], "venue": "Behavior Research Methods, Instruments, & Computers,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 137, "endOffset": 146}, {"referenceID": 1, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 137, "endOffset": 146}, {"referenceID": 2, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 137, "endOffset": 146}, {"referenceID": 3, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 164, "endOffset": 170}, {"referenceID": 4, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 164, "endOffset": 170}, {"referenceID": 5, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 7, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 280, "endOffset": 286}, {"referenceID": 8, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 280, "endOffset": 286}, {"referenceID": 1, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 230, "endOffset": 236}, {"referenceID": 0, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 230, "endOffset": 236}, {"referenceID": 3, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 252, "endOffset": 258}, {"referenceID": 4, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 252, "endOffset": 258}, {"referenceID": 9, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 281, "endOffset": 285}, {"referenceID": 10, "context": "1 RNN Regularization RNN regularization has recently been shown to be achievable using Dropout [11] by regularizing a subset of the recurrent connections in deep RNNs [12, 13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "1 RNN Regularization RNN regularization has recently been shown to be achievable using Dropout [11] by regularizing a subset of the recurrent connections in deep RNNs [12, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 12, "context": "1 RNN Regularization RNN regularization has recently been shown to be achievable using Dropout [11] by regularizing a subset of the recurrent connections in deep RNNs [12, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 4, "context": "Previously, it was shown that weight decay regularization only provided small improvement [5] and dropout noise was detrimental when applied to all connections due to the compounding of errors over time [14].", "startOffset": 90, "endOffset": 93}, {"referenceID": 13, "context": "Previously, it was shown that weight decay regularization only provided small improvement [5] and dropout noise was detrimental when applied to all connections due to the compounding of errors over time [14].", "startOffset": 203, "endOffset": 207}, {"referenceID": 3, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 14, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 2, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 0, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 1, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 9, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 7, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 199, "endOffset": 210}, {"referenceID": 0, "context": "In certain cases the models are trained with supervision on the gates [1, 16], however in many cases there is no supervised data for the attentional component.", "startOffset": 70, "endOffset": 77}, {"referenceID": 9, "context": "Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation [10], using reinforcement learning [9] to optimize a visual tracking strategy [3], or leaving this part semi-supervised through the primary objective.", "startOffset": 186, "endOffset": 190}, {"referenceID": 8, "context": "Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation [10], using reinforcement learning [9] to optimize a visual tracking strategy [3], or leaving this part semi-supervised through the primary objective.", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation [10], using reinforcement learning [9] to optimize a visual tracking strategy [3], or leaving this part semi-supervised through the primary objective.", "startOffset": 264, "endOffset": 267}, {"referenceID": 9, "context": "Our work resembles the observation prior of [10], where we favor input gates being closed and penalize deviation with a penalty of our choosing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Similarly to the annealed Dropout from [18], we also consider a gradual increase in the sparsity penalty during training to encourage early exploration.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 143, "endOffset": 153}, {"referenceID": 7, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 143, "endOffset": 153}, {"referenceID": 16, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 143, "endOffset": 153}, {"referenceID": 17, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 191, "endOffset": 199}, {"referenceID": 18, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 191, "endOffset": 199}, {"referenceID": 3, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 218, "endOffset": 221}, {"referenceID": 19, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 265, "endOffset": 273}, {"referenceID": 20, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 265, "endOffset": 273}, {"referenceID": 21, "context": "Here we consider predicting the sentiment expressed in the Stanford Sentiment Treebank (SST) [24], a collection of 11,855 sentences extracted from movie reviews.", "startOffset": 93, "endOffset": 97}, {"referenceID": 22, "context": "Here we focus on paraphrase detection on the SemEval 2014 shared task 1 dataset [25] which includes 9927 sentence pairs in a 4500/500/4927 train/dev/test split.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "Each sentence is annotated with a score c \u2208 [1, 5], with 5 indicating the pair is a paraphrase, and 1 that the pair is unrelated.", "startOffset": 44, "endOffset": 50}, {"referenceID": 4, "context": "Each sentence is annotated with a score c \u2208 [1, 5], with 5 indicating the pair is a paraphrase, and 1 that the pair is unrelated.", "startOffset": 44, "endOffset": 50}, {"referenceID": 23, "context": "3 Question Answering Facebook AI Research recently proposed a set of 20 tasks designed to be \u201cprerequisites\u201d for any system \u201ccapable of conversing with human\u201d [27].", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "1 Gated LSTMs In our work we make extensive use of Long-Short Term Memory networks [28], a popular RNN architecture specifically designed to capture long range dependencies and alleviate training difficulties [29].", "startOffset": 83, "endOffset": 87}, {"referenceID": 25, "context": "1 Gated LSTMs In our work we make extensive use of Long-Short Term Memory networks [28], a popular RNN architecture specifically designed to capture long range dependencies and alleviate training difficulties [29].", "startOffset": 209, "endOffset": 213}, {"referenceID": 26, "context": "Since their introduction in 1995, many variants have been proposed [30], however for the purposes of this research we found that the vanilla version from [30] worked best.", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "Since their introduction in 1995, many variants have been proposed [30], however for the purposes of this research we found that the vanilla version from [30] worked best.", "startOffset": 154, "endOffset": 158}, {"referenceID": 27, "context": "Additionally, we consider Gated Stacked LSTMs, a variant of Stacked LSTMs [31, 5, 20], where the input the lowest LSTM is gated using the hidden state from the topmost LSTM of the previous timestep.", "startOffset": 74, "endOffset": 85}, {"referenceID": 4, "context": "Additionally, we consider Gated Stacked LSTMs, a variant of Stacked LSTMs [31, 5, 20], where the input the lowest LSTM is gated using the hidden state from the topmost LSTM of the previous timestep.", "startOffset": 74, "endOffset": 85}, {"referenceID": 17, "context": "Additionally, we consider Gated Stacked LSTMs, a variant of Stacked LSTMs [31, 5, 20], where the input the lowest LSTM is gated using the hidden state from the topmost LSTM of the previous timestep.", "startOffset": 74, "endOffset": 85}, {"referenceID": 10, "context": "We train 3 different models with hidden sizes 25, 50, 150, and apply Dropout [11, 12] with probability p = 0.", "startOffset": 77, "endOffset": 85}, {"referenceID": 11, "context": "We train 3 different models with hidden sizes 25, 50, 150, and apply Dropout [11, 12] with probability p = 0.", "startOffset": 77, "endOffset": 85}, {"referenceID": 28, "context": "All models are trained using Adadelta [32] with \u03c1 = 0.", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "The HG-LSTM takes a question, followed by the sequence of facts, and the final hidden state of the HG-LSTM is fed as input to an LSTM decoder that produces the answer sequentially and ends its prediction with an <EOS> symbol [5, 22].", "startOffset": 225, "endOffset": 232}, {"referenceID": 19, "context": "The HG-LSTM takes a question, followed by the sequence of facts, and the final hidden state of the HG-LSTM is fed as input to an LSTM decoder that produces the answer sequentially and ends its prediction with an <EOS> symbol [5, 22].", "startOffset": 225, "endOffset": 232}, {"referenceID": 23, "context": "We use the first 1000 examples for training as suggested in [27], and reserve 20% for validation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Our model is trained using AdaDelta [32], with \u03c1 = 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "Finally, we observed that the HG-LSTM model significantly improves performance over the LSTM baseline from [27].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "g Hinton Diagrams [33].", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "Table 2: Comparison of test accuracy on bAbI dataset from [27] with different models.", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "Models are (left to right): LSTM baseline from [27], followed HG-LSTM with: no penalty, word sparsity penalty only, fact selection penalty only and both.", "startOffset": 47, "endOffset": 51}], "year": 2015, "abstractText": "We present a complimentary objective for training recurrent neural networks (RNN) with gating units that helps with regularization and interpretability of the trained model. Attention-based RNN models have shown success in many difficult sequence to sequence classification problems with long and short term dependencies, however these models are prone to overfitting. In this paper, we describe how to regularize these models through an L1 penalty on the activation of the gating units, and show that this technique reduces overfitting on a variety of tasks while also providing to us a human-interpretable visualization of the inputs used by the network. These tasks include sentiment analysis, paraphrase recognition, and question answering.", "creator": "LaTeX with hyperref package"}}}