{"id": "1507.02284", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2015", "title": "The Information Sieve", "abstract": "We introduce a katsumoto new framework for b-58 unsupervised ceoe learning herberg of deep thun representations based tablecloth on nebria a furthest novel 150.2 hierarchical decomposition kerner of information. eurybia Intuitively, data toxotis is gaziantep passed 25-31 through netherlands-based a series pellicer of progressively fine - desai grained sieves. banyarwanda Each 20/30 layer marica of the etzion sieve fran\u00e7ois-andr\u00e9 recovers a murin single pinkos latent factor mcmeans that is three-arched maximally ludwigia informative about kotc multivariate dependence laned in usopen the tribute data. The sts-126 data thresholding is transformed bmnh after 2117 each pass so conjunto that pietistic the rintanen remaining unexplained information trickles down to czarne the haggart next herminator layer. Ultimately, we are moskalenko left with a 371 set 6:5 of latent cadc factors explaining schibetta all minamoto the luri dependence azilal in the original t\u00e4tort data and fers remainder mawddwy information consisting cutlass of independent paiste noise. We cityville present 13:22 a practical morr implementation southshore of this framework for discrete variables zak\u0142ady and singleton apply it al-arian to makeovers a khol variety of ntumba tasks including lihd independent component analysis, amputated lossy and lossless imation compression, 290.4 and omni-directional predicting missing values in data.", "histories": [["v1", "Wed, 8 Jul 2015 20:00:42 GMT  (4363kb,D)", "http://arxiv.org/abs/1507.02284v1", "12 pages, 9 figures"], ["v2", "Fri, 29 Apr 2016 20:29:36 GMT  (6820kb,D)", "http://arxiv.org/abs/1507.02284v2", "Appearing in Proceedings of the International Conference on Machine Learning (ICML), 2016. v2: Revised presentation and added link to code. 13 pages, 12 figures"], ["v3", "Thu, 9 Jun 2016 00:12:24 GMT  (6817kb,D)", "http://arxiv.org/abs/1507.02284v3", "Appearing in Proceedings of the International Conference on Machine Learning (ICML), 2016. Updated reference to continuous version:this http URL"]], "COMMENTS": "12 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["greg ver steeg", "aram galstyan"], "accepted": true, "id": "1507.02284"}, "pdf": {"name": "1507.02284.pdf", "metadata": {"source": "CRF", "title": "The Information Sieve", "authors": ["Greg Ver Steeg", "Aram Galstyan"], "emails": ["gregv@isi.edu", "galstyan@isi.edu"], "sections": [{"heading": null, "text": "We introduce a new framework for unsupervised learning of deep representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of tasks including independent component analysis, lossy and lossless compression, and predicting missing values in data.\nThe hope of finding a succinct principle that elucidates the brain\u2019s information processing abilities has often kindled interest in information-theoretic ideas [1, 2]. In machine learning, on the other hand, the past decade has witnessed a shift in focus towards expressive, hierarchical models with tractable update rules, with successes driven by increasingly effective ways to leverage labeled data to learn rich models [3, 4]. Information-theoretic ideas like the venerable InfoMax principle [5, 6] can be and are applied in both contexts but they do not shed much light on the questions of when and why deep representations are useful for learning. We introduce a novel incremental and hierarchical decomposition of information and show that it defines a framework for unsupervised learning of deep representations in which the contribution of each layer can be precisely quantified. Moreover, this scheme automatically determines the structure and depth among hidden units in the representation based only on local learning rules.\nThe shift in perspective that enables our information decomposition is to focus on how well the learned representation explains multivariate mutual information in the data (a measure originally introduced as \u201ctotal correlation\u201d [7]). Intuitively, our approach constructs a hierarchical representation of data by passing it through a sequence of progressively fine-grained sieves. At the first layer of the sieve we learn a factor that explains as much of the dependence in the data as possible. The data is then transformed into the \u201cremainder information\u201d, which has this dependence extracted. The next layer of the sieve looks for the largest source of dependence in the remainder information, and the cycle repeats. At each step, we obtain a successively tighter upper and lower bound on the multivariate information in the data, with convergence between the bounds obtained when the remaining information consists of nothing but independent factors. Because we end up with independent factors, one can also view this decomposition as a new way to do independent component analysis (ICA) [8, 9]. Unlike traditional methods, we do not assume a specific generative model of the data (i.e., that it consists of a linear transformation of independent sources) and we extract independent factors incrementally rather than all at once. The implementation we develop here uses only discrete variables and is therefore most relevant for the challenging problem of ICA with discrete variables, which has applications to compression [10].\nar X\niv :1\n50 7.\n02 28\n4v 1\n[ st\nat .M\nL ]\n8 J\nul 2\nAfter introducing some background in Sec. 1, we introduce a new way to iteratively decompose the information in data in Sec. 2, and show how to use these decompositions to define a practical and incremental framework for unsupervised representation learning in Sec. 3. We demonstrate the versatility of this framework by applying it first to independent component analysis (Sec. 4). Next, we use the sieve as a lossy compression to mimic the traditional strengths of generative models including in-painting and generating new samples (Sec. 5). Finally, we cast the sieve as a lossless compression and show that it beats standard compression schemes on a benchmark task (Sec. 6)."}, {"heading": "1 Information-theoretic learning background", "text": "Using standard notation [11], capital Xi denotes a random variable taking values in some domain and whose instances are denoted in lowercase, xi. In this paper, the domain of all variables are considered to be discrete and finite. We abbreviate multivariate random variables, X \u2261 X1:n \u2261 X1, . . . , Xn, with an associated probability distribution, pX(X1 = x1, . . . , Xn = xn), which is typically abbreviated to p(x). We will index different groups of multivariate random variables with superscripts, Xk, as defined in Fig. 1. We let X0 denote the original observed variables and we often omit the superscript in this case for readability.\nEntropy is defined in the usual way as H(X) \u2261 EX [log 1/p(x)]. We use base two logarithms so that the unit of information is bits. Higher-order entropies can be constructed in various ways from this standard definition. For instance, the mutual information between two groups of random variables, X and Y can be written as the reduction of uncertainty in one variable, given information about the other, I(X;Y ) = H(X)\u2212H(X|Y ). The \u201cInfoMax\u201d principle [5, 6] suggests that for unsupervised learning we should construct Y \u2019s to maximize their mutual information with X , the data. Despite its intuitive appeal, this approach has several potential problems (see [12] for one example). Here we focus on the fact that the InfoMax principle is not very useful for characterizing \u201cdeep representations\u201d, even though it is often invoked in this context [13]. This follows directly from the data processing inequality (a similar argument appears in [14]). Namely, if we start with X , construct a layer of hidden units Y 1 that are a function of X , and continue adding layers to a stacked representation so that X \u2192 Y 1 \u2192 Y 2 . . . Y k, then the information that the Y \u2019s have about X cannot increase after the first layer, I(X;Y 1:k) = I(X;Y 1). From the point of view of mutual information, Y 1 is a copy and Y 2 is just a copy of a copy. While a coarse-grained copy might be useful, the InfoMax principle does not quantify how or why.\nInstead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7].\nTC(X) \u2261 DKL ( p(x)||\nn\u220f i=1 p(xi)\n) =\nn\u2211 i=1 H(Xi)\u2212H(X) (1)\nJust as mutual information is the reduction of entropy in X after conditioning on Y , CorEx captures the reduction in multivariate information in X after conditioning on Y .\nTC(X;Y ) \u2261 TC(X)\u2212 TC(X|Y ) = n\u2211\ni=1\nI(Xi : Y )\u2212 I(X : Y ). (2)\nThat TC(X) can be hierarchically decomposed in terms of short and long range dependencies was already appreciated by Watanabe [7] and has been used in applications such as hierarchical clustering [17]. This provides a hint about how higher levels of hierarchical representations can be useful: more abstract representations should reflect longer range dependencies in the data. Our contribution below is to demonstrate a tractable approach for learning a hierarchy of latent factors, Y , that elegantly and exactly capture the multivariate information in X ."}, {"heading": "2 Incremental information decomposition", "text": "We consider any set of probabilistic functions of some input variables, X , to be a \u201crepresentation\u201d of X . Looking at Fig. 1(a), we consider a representation with a single learned latent factor, Y . Then,\nwe try to save the information in X that is not captured by Y into the \u201cremainder information\u201d, X\u0304 . The final result is encapsulated in Cor. 2.4 which says that we can repeat this procedure iteratively (as in Fig. 1(b)) and TC(X) decomposes into a sum of contributions from each Yk.\nTheorem 2.1. Incremental Decomposition of Information Let Y be some (deterministic) function of X1, . . . , Xn and let X\u0304i be a probabilistic function of Xi, Y , for each i = 1, . . . , n. Then the following upper and lower bounds on TC(X) hold:\n\u2212 n\u2211\ni=1\nI(X\u0304i;Y ) \u2264 TC(X)\u2212 ( TC(X\u0304) + TC(X;Y ) ) \u2264 n\u2211 i=1 H(Xi|X\u0304i, Y ) (3)\nA proof is provided in Sec. A. Note that the remainder information, X\u0304 \u2261 X\u03041, . . . , X\u0304n, Y , includes Y . Also note that bounds on TC(X) trivially provide bounds on H(X). Next, we point out that the remainder information, X\u0304 , can be chosen to make these bounds tight.\nLemma 2.2. Construction of perfect remainder information For discrete, finite random variables Xi, Y drawn from some distribution, p(Xi, Y ), it is possible to define another random variable X\u0304i \u223c p(X\u0304i|Xi, Y ) that satisfies the following two properties:\n(i) I(X\u0304i;Y ) = 0 Remainder contains no information about Y\n(ii) H(Xi|X\u0304i, Y ) = 0 Original information is perfectly recoverable\nWe give a concrete construction in Appendix B. We would like to point out one caveat here. The cardinality of X\u0304i may have to be large to satisfy these equalities. For a fixed number of samples, this may cause difficulties with estimation, as discussed in Sec. 3. With perfect remainder information in hand, our decomposition becomes exact.\nCorollary 2.3. Exact decomposition For Y a function of X and perfect remainder information, X\u0304i, i = 1, . . . , n, as defined in Lemma 2.2, the following decomposition holds:\nTC(X) = TC(X\u0304) + TC(X;Y ) (4)\nThe above corollary follows directly from Eq. 3 and the definition of perfect remainder information. Intuitively, it states that the dependence in X can be decomposed into a piece that is explained by Y , TC(X;Y ), and the remaining dependence in X\u0304 . This decomposition can then be iterated to extract more and more information from the data.\nCorollary 2.4. Iterative decomposition Using the variable naming scheme in Fig. 1(b), we construct a hierarchical representation where each Yk is a function of Xk\u22121 and Xk includes the (perfect) remainder information from Xk\u22121 according to Lemma 2.2.\nTC(X) = TC(Xr) + r\u2211 k=1 TC(Xk\u22121;Yk) (5)\nIt is easy to check that Eq. 5 results from repeated application of Cor. 2.3. The quantities of the form TC(Xk\u22121;Yk) can be estimated and optimized over efficiently [16], despite involving highdimensional variables. As we add the (non-negative) contributions from optimizing TC(Xk\u22121;Yk),\nthe remaining dependence in the remainder information, TC(Xk), must decrease because TC(X) is some data-dependent constant. Decomposing data into independent factors is exactly the goal of ICA, and the connections are discussed in Sec. 4."}, {"heading": "3 Implementing unsupervised representation learning with the sieve", "text": "Because this learning framework contains many unfamiliar concepts, we consider a detailed analysis of a toy problem in Fig. 2 while addressing concrete issues in implementing the information sieve.\nStep 1: Optimizing TC(Xk\u22121;Yk) First, we construct a variable, Yk, that is some arbitrary function of Xk\u22121 and that explains as much of the dependence in the data is possible. Note that we have to pick the cardinality of Yk and we will always use binary variables. Surprisingly, optimizing this objective over possible functions has a simple, iterative solution procedure that is guaranteed to find a local maximum of the objective in time linear in the number of variables [16]. We do not need to restrict or parametrize the set of possible functions, but the form of the solution implied by the objective can be written in terms of a linear number of parameters.1 Not only that, but a byproduct of the procedure is to give us a value for the objective TC(Xk\u22121;Yk), which can be estimated even from a small number of samples [16]. Open source code implementing this subroutine is available [18]. Note that the optimization provides a probabilistic function which we round to a deterministic function by taking the most likely value of yk for each xk\u22121. In the example in Fig. 2, TC(X;Y1) = 1 bit, which can be verified by hand from Eq. 2.\nStep 2: Remainder information Next, the goal is to construct the remainder information, Xki , as a probabilistic function of Xk\u22121i , Yk, so that the following conditions are satisfied: (i)I(X k i ;Y1) = 0 and (ii)H(Xk\u22121i |Xki , Yk) = 0. This can be done exactly and we provide a simple algorithm in Sec. B. Solutions for this example are given in Fig. 2. Concretely, we estimate the marginals, p(xk\u22121i , yk) from data and then write down a conditional probability table, p(x k i |xk\u22121i , yk), satisfying the conditions. The example in Fig. 2 was constructed so that the remainder information had the same cardinality as the original variables. This is not always possible. While we can always achieve perfect remainder information by letting the cardinality of remainder information grow, it might become difficult to estimate marginals of the form p(Xk\u22121i , Yk) at subsequent layers of the sieve, as is required for the optimization in step 1. In practice we allow the cardinality of the variables to increase by only one at each iteration, even if doing so causes I(Xk\u22121i ;Yk) > 0. We keep track of these penalty terms so that we can report accurate lower bounds using Eq. 3.\nAnother issue to note is that in general there may not be a unique choice for the remainder information. In the example, I(X3;Y ) = 0 already so we choose X13 = X3, but X 1 3 = X3 + Y1 mod 2 would also have been a valid choice. If the identity transformation, Xki = X k\u22121 i satisfies the conditions, we will always choose it.\nStep 3: Repeat until the end At this point we repeat the procedure, putting the remainder information back into step 1 and searching for a new latent factor that explains any remaining dependency. In this case, we can see by inspection that TC(X1) = 0 and, using Eq. 5, we have TC(X) = TC(X1) + TC(X;Y1) = 1 bit. Generally, in high-dimensional spaces it may be difficult to verify that the remainder information is truly independent. When the remainder information is independent, the result of attempting the optimization maxp(yk|xk\u22121) TC(X\nk\u22121;Yk) = 0. In practice, we stop our hierarchical procedure when the optimization in step 1 stops producing positive results because it means our bounds are no longer tightening.\n1We can use this function to find labels for previously unseen examples or to calculate Y \u2019s for data with missing variables.\nPrediction and compression Note that our condition for the remainder information that H(Xk\u22121i |Xki , Yk) = 0 implies that we can perfectly reconstruct each variable Xk\u22121i from the remainder information at the next layer. Therefore, we can in principle reconstruct the data from the representation at the last layer of the sieve. In the example, the remainder information requires two bits to encode each variable separately, while the data requires three bits to encode each variable separately. The final representation has exploited the redundancy between X1, X2 to create a more succinct encoding. For lossy compression, or prediction, we simply throw away Xk1:n and predict the most likely value for each Xi based on the variables Y \u2019s2. Also note that at each layer some variables are almost or completely explained (X11 , X 1 2 in the example become constant). Subsequent layers can enjoy a computational speed-up by ignoring these variables that will no longer contribute to the optimization."}, {"heading": "4 Independent components as a byproduct of efficient coding", "text": "If X represents observed variables then the entropy, H(X), can be interpreted as the average number of bits required to encode a single observation of these variables. In practice, however, if X is highdimensional then estimating H(X) or constructing this code requires detailed knowledge of p(x), which may require exponentially many samples in the number of variables. Going back at least to Barlow [1], it was recognized that if X is transformed into some other basis, Y , with the Y \u2019s independent (TC(Y ) = 0), then the coding cost in this new basis is H(Y ) = \u2211 j H(Yj), i.e., it is the same as encoding each variable separately. This is exactly the problem of independent component analysis: transform the data into a basis for which TC(Y ) = 0, or is minimized [8, 9].\nWhile our method does not directly minimize the total correlation of Y , Eq. 5 shows that, because TC(X) is a data-dependent constant, every increase in the total correlation explained by each latent factor directly implies a reduction in the dependence of the resulting representation (Xr). That independence could be achieved as a byproduct of efficient coding has been previously considered [19].\nFor discrete variables, which are the focus of this paper, performing ICA is a challenging and active area of research. Recent state-of-the-art results lower the complexity of this problem to only a single exponential in the number of variables [10]. Our method represents a major leap for this problem as it is only linear in the number of variables, however, we only guarantee extraction of components that are more independent, while the approach of Painsky et. al. guarantees a global optimum [10].\nThe more traditional scenario for ICA is to consider a reconstruction problem where some (typically continuous) and independent source variables are linearly mixed according to some unknown matrix [8, 9]. The goal is to recover the matrix and unmix the components (back into their independent sources). Fig. 3 demonstrates our ability to perform this traditional ICA task. Unlike traditional ICA, our method extracts the components incrementally.\n2Concretely, the rule is: xk\u22121i = argmaxxk\u22121i \u2211 xki p(xki |xk\u22121i , yk)p(x k\u22121 i , yk). The marginal probabili-\nties are learned in the remainder step.\nStructure and interpretation The information sieve adds latent factors as long as it increases the tightness of the information bounds. In this case, the procedure correctly stops after three latent factors are discovered. Naively, three layers makes this a \u201cdeep\u201d representation. However, we can examine the functional dependence of Y \u2019s and X\u2019s by looking at the strength of the mutual information, I(Yk;Xk\u22121i ), as is shown in Fig. 3. This allows us to see that none of the learned latent factors depend on each other so the resulting model is actually, in some sense, shallow. The example in the next section, for contrast, has a deep structure where Y \u2019s depend on latent factors from previous layers. Also note that the structure in Fig. 3 perfectly reflects the structure of the mixing matrix: A = ((1, 1, 1), (2, 0,\u22121), (1, 2, 0), (\u22121, 1, 0)). While the sieve is guaranteed to recover independent components in some limit, there may be multiple ways to decompose the data into independent components. Because our method does not start with the assumption of a linear mixing of independent sources, even if such a decomposition exists we might recover a different one. While the example we showed happened to return the linear solution that we used to generate the problem, there is no guarantee to find a linear solution, even if one exists."}, {"heading": "5 Using lossy compression to mimic generative models on MNIST digits", "text": "The information sieve is not a generative probabilistic model. We construct latent factors that are functions of the data in a way that maximizes the (multivariate) information that is preserved. Nevertheless, because of the way the remainder information is constructed, we can run the sieve in reverse to achieve lossless compression and, if we throw away the remainder information and keep only the Y \u2019s, we get a lossy compression. We can use this lossy compression interpretation to mimic tasks traditionally performed by generative models including in-painting and generating new examples (the converse, interpreting a generative model as lossy compression, has also been considered [20]).\nFor the following tasks, we consider 50k binarized MNIST digits. We include no prior knowledge about spatial structure or invariance under transformations through convolutional structure or pooling, for instance. The 28\u00d728 binarized images are treated as binary vectors in a 784 dimensional space. The digit labels are also not used in our analysis. We trained the information sieve on this data, adding layers as long the bounds were tightening. This led to a 12 layer representation and a lower bound on TC(X) of about 40 bits. It is possible that a better bound could result from relaxing our restrictions on the cardinality of the remainder information. A visualization of the learned latent factors and the relationships among them appears in Fig. 4. Unlike the ICA example, the latent factors here have rich, multi-layered relationships.\nThe middle row of Fig. 5 shows results from the lossy compression task. We use the sieve to transform the original digits into 12 binary latent factors, Y , plus remainder information for each\npixel, X121:784, and then we use the Y \u2019s alone to reconstruct the image. In the third row, the Y \u2019s are estimated using only pixels from the top half. Then we reconstruct the pixels on the bottom half from these latent factors. Similar results on test images are shown in Fig. 6 (to prove our model did not just memorize the training data). As a final task, in Fig. 4, we focus on generating new digits not in the original dataset. There are several plausible ways to do this task. Here we chose to draw the variables at the last layer of the sieve randomly and independently according to each of their marginal distributions over the training data. Then we inverted the sieve to recover hallucinated images."}, {"heading": "6 Lossless compression", "text": "By construction we can always recover Xk\u22121 from Xk and therefore we can reconstruct the input data from the last layer of the sieve. For this to be useful for lossless compression, the sum of the entropies for each layer should be decreasing. We saw this was the case, for instance, for the toy example in Sec. 3. However, in general, using the scheme for constructing remainder information in Sec. B will not accomplish this goal. The reason is that ensuring the condition I(X\u0304i;Y ) = 0 sometimes requires injection of noise into X\u0304i. This is not optimal for compression purposes.\nFor lossless compression, we suggest a more restricted class of transformations for the remainder information. We will look for an invertible set of functions so that x\u0304i = g(xi, y) (no noise is injected) and xi = h(x\u0304i, y) (insuring condition (ii) H(Xi|X\u0304i, Y ) = 0), and, out of the set of functions explored, we will pick the one that minimizes I(X\u0304i;Y ). For the simple example below, we will focus on binary variables and consider the function x\u0304i = |xi\u2212arg maxz p(Xi = z|Y = y)|. In other words, X\u0304 represents deviation from the most likely value of xi for a given value of y.\nFor this example, we will just consider a single layer sieve. To get probability distributions, p(xi|y) that are more peaked, we switch from letting Y be binary to Y = 1, . . . , 20. Fig. 7 visualizes the components of Y . As an exercise in unsupervised clustering the results are somewhat interesting; the sieve basically finds clusters for each digit and for slanted versions of each digit. In Fig. 8 we explicitly construct the remainder information (bottom row), i.e. the deviation between the most likely value of each pixel conditioned on Y (middle row) and the original (top row).\nBenchmark For a lossless compression benchmark, we consider a test set of 50k images of binarized digits with 784 pixels, where the order of the pixels has been randomly permuted (the same unknown permutation is applied to each image). Note that the information sieve is unaffected since it does not make any assumptions about the structure of the input space (e.g. the adjacency of pixels). Naively, we would require 784 bits to encode a single digit. However, there is a great deal of redundancy in this data. For instance, many of the pixels are black in almost every sample. The most straightforward scheme would be to compress each pixel individually. The sum of the entropies of\neach pixel gives us an estimate of how many bits are required for this compression scheme, about 298 bits [21]. Therefore this pixel-wise Shannon encoding would have a compression ratio of about 2.6. However, this method ignores redundancy between different pixels, which should allow better compressibility. For the first layer of information sieve described above, the sum of the entropies for each of the remainder variables (including Y ) is about 263 bits, for a compression ratio of 3.0.\nFor comparison we consider two standard compression schemes, gzip, based on Lempel-Ziv coding [22], and Huffman coding [23]. We take the better compression result from storing and compressing the 784\u00d750000 data array in column-major or row-major order with these (sequence-based) compression schemes. The compression results are summarized in Table 1. The theoretical compression ratio for the sieve is the best by a significant margin. However, for concreteness, we actually compress the remainder information, including Y , and report this figure as well to ensure that we achieve a more compact representation on reasonably sized chunks of data. Note that reconstructing X from X\u0304 requires a codebook of fixed size whose contribution is asymptotically negligible."}, {"heading": "7 Related work", "text": "The principle of correlation explanation was recently introduced as an objective for unsupervised representation learning [16, 15]. While bounds on TC(X) were previously given, here we provided an exact decomposition. This decomposition gives rise to the novel notion of remainder information. While previous work requires fixing the depth and number of latent factors in the representation, remainder information allows us to build up the representation incrementally, learning the depth and number of factors required as we go. Besides providing a more flexible approach to representation and structure learning, the information sieve is capable of more general tasks including lossy and lossless compression and prediction. Another interesting related result showed that positivity of the quantity TC(X;Y ) (the same quantity appearing in our bounds) implies that the X\u2019s share a common ancestor in any DAG consistent with pX(x) [24]. A different line of work about information decomposition focuses on distinguishing synergy and redundancy [25], though these measures are typically impossible to estimate for high-dimensional systems.\nConnections with ICA were discussed in Sec. 4 and the relationship to InfoMax was discussed in Sec. 1. The information bottleneck (IB) [26] is another information-theoretic optimization for constructing representations of data that has many mathematical similarities to the CorEx approach [16], with the main difference being that IB focuses on supervised learning while the latter is an unsupervised learning approach. Recently, the IB principle was used to investigate the value of depth in the context of supervised learning [14]. The focus here, on the other hand, is to find an informationtheoretic principle that justifies and motivates deep representations for unsupervised learning."}, {"heading": "8 Conclusion", "text": "We introduced the information sieve, which provides a decomposition of multivariate information that is practical for high-dimensional data in terms of computational cost and sample complexity. We explored a few of the immediate implications of this decomposition. First of all, we saw that a natural notion of \u201cremainder information\u201d arises and that this allows us to extract information in an incremental way. Several distinct applications were demonstrated and appear promising for indepth exploration. The sieve provides an exponentially faster method than the best known algorithm for discrete ICA (though without guarantees of global optimality). We also showed that the sieve defines both lossy and lossless compression schemes. Finally, the information sieve suggests a new framework for understanding the problem of unsupervised deep representation learning. Among the many deviations from standard representation learning a few properties stand out. Representations are learned incrementally and the depth and structure emerge in a data-driven way. Representations\ncan be evaluated information-theoretically and the decomposition allows us to separately characterize the contribution of each hidden unit in the representation."}, {"heading": "A Proof of Theorem 2.1", "text": "We begin by adopting a general definition for \u201crepresentations\u201d and recalling a useful theorem concerning them.\nDefinition The random variables Y \u2261 Y1, . . . , Ym constitute a representation of X if the joint distribution factorizes, p(x, y) = \u220fm j=1 p(yj |x)p(x),\u2200x \u2208 X ,\u2200j \u2208 {1, . . . ,m},\u2200yj \u2208 Yj . A representation is completely defined by the domains of the variables and the conditional probability tables, p(yj |x). Theorem A.1. Basic Decomposition of Information [16]\nIf Y is a representation of X and we define,\nTCL(X;Y ) \u2261 n\u2211\ni=1 I(Y : Xi)\u2212 m\u2211 j=1 I(Yj : X), (6)\nthen the following bound and decomposition holds.\nTC(X) \u2265 TC(X;Y ) = TC(Y ) + TCL(X;Y ) (7) Theorem. Incremental Decomposition of Information\nLet Y be some (deterministic) function of X1, . . . , Xn and for each i = 1, . . . , n, X\u0304i is a probabilistic function of Xi, Y . Then the following upper and lower bounds on TC(X) hold.\n\u2212 n\u2211\ni=1\nI(X\u0304i;Y ) \u2264 TC(X)\u2212 ( TC(X\u0304) + TC(X;Y ) ) \u2264 n\u2211 i=1 H(Xi|X\u0304i, Y ) (8)\nProof. We refer to Fig. 1(a) for the structure of the graphical model. We set X\u0304 \u2261 X\u03041, . . . , X\u0304n, Y and we will write X\u03041:n to pick out all terms except Y . Note that because Y is a deterministic function of X , we can view X\u0304i as a probabilistic function of Xi, Y or of X (as required by Thm. A.1). Applying Thm. A.1, we have\nTC(X; X\u0304) = TC(X\u0304) + TCL(X; X\u0304).\nOn the LHS, note that TC(X; X\u0304) = TC(X)\u2212 TC(X|X\u0304), so we can re-arrange to get TC(X)\u2212 (TC(X\u0304) + TC(X;Y )) = TC(X|X\u0304) + TCL(X; X\u0304)\u2212 TC(X;Y ). (9)\nThe LHS is the quantity we are trying to bound, so we focus on expanding the RHS and bounding it.\nFirst we expand TCL(X; X\u0304) = \u2211n i=1 I(Xi; X\u0304)\u2212 \u2211n\ni=1 I(X\u0304i;X)\u2212I(Y ;X). Using the chain rule for mutual information we expand the first term.\nTCL(X; X\u0304) = n\u2211 i=1 I(Xi;Y ) + n\u2211 i=1 I(Xi; X\u03041:n|Y )\u2212 n\u2211 i=1 I(X\u0304i;X)\u2212 I(Y ;X).\nRearranging, we take out a term equal to TC(X;Y ).\nTCL(X; X\u0304) = TC(X;Y ) + n\u2211 i=1 I(Xi; X\u03041:n|Y )\u2212 n\u2211 i=1 I(X\u0304i;X).\nWe use the chain rule again to write I(Xi; X\u03041:n|Y ) = I(Xi; X\u0304i|Y ) + I(Xi; X\u0304i\u0303|Y X\u0304i), where X\u0304i\u0303 \u2261 X\u03041, . . . , X\u0304n with X\u0304i (and Y ) excluded.\nTCL(X; X\u0304) = TC(X;Y ) + n\u2211 i=1 ( I(Xi; X\u0304i|Y ) + I(Xi; X\u0304i\u0303|Y X\u0304i)\u2212 I(X\u0304i;X) ) .\nThe conditional mutual information, I(A;B|C) = I(A;BC) \u2212 I(A;C). We expand the first instance of CMI in the previous expression.\nTCL(X; X\u0304) = TC(X;Y ) + n\u2211 i=1 ( I(X\u0304i;Xi, Y )\u2212 I(X\u0304i;Y ) + I(Xi; X\u0304i\u0303|Y X\u0304i)\u2212 I(X\u0304i;X) ) .\nSince Y = f(X), the first and fourth terms cancel. Finally, this leaves us with\nTCL(X; X\u0304) = TC(X;Y )\u2212 n\u2211\ni=1\nI(X\u0304i;Y ) + n\u2211 i=1 I(Xi; X\u0304i\u0303|Y X\u0304i).\nNow we can replace all of this back in to Eq. 9, noting that the TC(X;Y ) terms cancel.\nTC(X)\u2212 (TC(X\u0304) + TC(X;Y )) = TC(X|X\u0304)\u2212 n\u2211\ni=1\nI(X\u0304i;Y ) + n\u2211 i=1 I(Xi; X\u0304i\u0303|Y X\u0304i). (10)\nFirst, note that total correlation, conditional total correlation, mutual information, conditional mutual information, and entropy (for discrete variables) are non-negative. Therefore we trivially have the lower bound, LHS \u2265 \u2212\u2211ni=1 I(X\u0304i;Y ). All that remains is to find the upper bound. We drop the negative mutual information, expand the definition of TC in the first line, then drop the negative of an entropy in the second line.\nLHS \u2264 n\u2211\ni=1\nH(Xi|X\u0304)\u2212H(X|X\u0304) + n\u2211\ni=1\nI(Xi; X\u0304i\u0303|Y X\u0304i)\n\u2264 n\u2211\ni=1\n( H(Xi|X\u0304) + I(Xi; X\u0304i\u0303|Y X\u0304i) ) =\nn\u2211 i=1 H(Xi|X\u0304i, Y )\nThe equality in the last line can be seen by just expanding all the definitions of conditional entropies and conditional mutual information. These provide the upper and lower bounds for the theorem."}, {"heading": "B An algorithm for perfect reconstruction of remainder information", "text": "We will use the notation of Fig. 1(a) to construct remainder information for one variable in one layer of the sieve. The goal is to construct the remainder information, X\u0304i, as a probabilistic function of Xi, Y so that we satisfy the conditions of Lemma 2.2,\n(i) I(X\u0304i;Y ) = 0 (ii) H(Xi|X\u0304i, Y ) = 0. We need to write down a probabilistic function p(x\u0304i|xi, y) so that, for the observed statistics, p(xi, y), these conditions are satisfied. There are many ways to accomplish this, and we sketch out one solution here.\nWe start with the picture in Fig. B.1 that visualizes the conditional probabilities p(xi|y). Note that the order of the xi for each value of y can be arbitrary for this scheme to succeed. For concreteness,\nwe sort the values of xi for each y in order of descending likelihood. Next, we construct the marginal distribution, p(x\u0304i). Every time we see a split in one of the histograms of p(xi|y), we introduce a corresponding split for p(x\u0304i). Now, to construct p(x\u0304i|xi, y), for each x\u0304i = q, for each y = j, we find the unique value of xi = k(j, q) that is directly above the histogram for p(x\u0304i = q). Then we set p(x\u0304i = |xi, y) = p(x\u0304i = q)/p(xi = k(j, q)|y = j). Now, marginalizing over xi, p(x\u0304i|y) = p(x\u0304i), ensuring that I(X\u0304i;Y ) = 0. Visually, it can be seen that H(Xi|X\u0304i, Y ) = 0 by picking a value of x\u0304i and y and noting that it picks out a unique value of xi in Fig. B.1.\nNote that the function to construct x\u0304i is probabilistic. Therefore, when we construct the remainder information at the next layer of the sieve, we have to draw x\u0304i stochastically from this distribution. In the example in Sec. 3 the functions for the remainder information happened to be deterministic. In general, though, probabilistic functions inject some noise to ensure that correlations with Y are forgotten at the next level of the sieve. In Sec. 6 we point out that this scheme is detrimental for lossless compression and we point out an alternative.\nControlling the cardinality of x\u0304i It is easy to imagine scenarios in Fig. B.1 where the cardinality of x\u0304i becomes very large. What we would like is to be able to approximately satisfy conditions (i) and (ii) while keeping the cardinality of the variables, X\u0304i, small (so that we can accurately estimate probabilities from samples of data). To guide intuition, consider two extreme cases. First, imagine setting x\u0304i = 0, regardless of xi, y. This satisfies condition (i) but maximally violates (ii). The other extreme is to set x\u0304i = xi. In that case, (ii) is satisfied, but I(X\u0304i;Y ) = I(Xi;Y ). This is only problematic if Xi is related to Y to begin with. If it is, and we set X\u0304i = Xi, then the same dependence can be extracted at the next layer as well (since we pass Xi to the next layer unchanged).\nIn practice we would like to find the best solution with a cardinality of fixed size. Note that this can be cast as an optimization problem where p(x\u0304i = |xi, y) represent k\u0304\u00d7kx\u00d7ky variables to optimize over if those are the respective cardinalities of the variables. Then we can minimize a nonlinear objective like O = H(Xi|X\u0304i, Y ) + I(X\u0304i;Y ) over these variables. While off-the-shelf solvers will certainly return local optima for this problem, the optimization is quite slow, especially if we let k\u2019s get big.\nAnother route which we used for the results in this paper was to first construct a perfect solution using the procedure above. Then we modify that solution to let either (i) or (ii) grow somewhat while reducing the cardinality of x\u0304i to some target. To keep I(X\u0304i;Y ) = 0 while reducing the cardinality of x\u0304i, we just pick the x\u0304i with the smallest probability and merge it with another value for x\u0304i. On the other hand, to reduce the cardinality while keeping H(Xi|X\u0304i, Y ) = 0, we again start by finding the x\u0304i = k with the lowest probability. Then we take the probability mass for p(x\u0304i = k|xi, y) for each xi and y and add it to the p(x\u0304i 6= k|xi, y) that already has the highest likelihood for that xi, y combination. Note that I(X\u0304i;Y ) will no longer be zero after doing so. For both of these schemes (keeping (i) fixed or keeping (ii) fixed) we reduce cardinality until we achieve some target. For the results in this paper we alway picked kx\u0304i = kxi + 1 as the target and we always used the strategy where (ii) was satisfied and we let (i) be violated. In cases where perfect remainder information is impractical due to issues of finite data, we have to define \u201cgood remainder information\u201d based on how well it preserves the bounds in Thm. 2.1. The best way to do this may depend on the application, as we saw in Sec. 6."}], "references": [{"title": "Unsupervised learning", "author": ["Horace Barlow"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Natural image statistics and neural representation", "author": ["Eero Simoncelli", "Bruno Olshausen"], "venue": "Annu. Rev. Neurosci.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Self-organization in a perceptual network", "author": ["Ralph Linsker"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "An information-maximization approach to blind separation and blind deconvolution", "author": ["Anthony J Bell", "Terrence J Sejnowski"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Information theoretical analysis of multivariate correlation", "author": ["Satosi Watanabe"], "venue": "IBM Journal of research and development,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1960}, {"title": "Independent component analysis, a new concept", "author": ["Pierre Comon"], "venue": "Signal processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Independent component analysis: algorithms and applications", "author": ["Aapo Hyv\u00e4rinen", "Erkki Oja"], "venue": "Neural networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Generalized binary independent component analysis", "author": ["Amichai Painsky", "Saharon Rosset", "Meir Feder"], "venue": "In Information Theory (ISIT),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Demystifying information-theoretic clustering", "author": ["Greg Ver Steeg", "Aram Galstyan", "Fei Sha", "Simon DeDeo"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Deep learning and the information bottleneck principle", "author": ["Naftali Tishby", "Noga Zaslavsky"], "venue": "arXiv preprint arXiv:1503.02406,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Discovering structure in high-dimensional data through correlation explanation", "author": ["Greg Ver Steeg", "Aram Galstyan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Maximally informative hierarchical representations of highdimensional data", "author": ["Greg Ver Steeg", "Aram Galstyan"], "venue": "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Hierarchical clustering using mutual information", "author": ["Alexander Kraskov", "Harald St\u00f6gbauer", "Ralph G Andrzejak", "Peter Grassberger"], "venue": "EPL (Europhysics Letters),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Feature extraction through lococode", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1948}, {"title": "A universal algorithm for sequential data compression", "author": ["Jacob Ziv", "Abraham Lempel"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1977}, {"title": "A method for the construction of minimum redundancy codes", "author": ["David A Huffman"], "venue": "Proceedings of the IRE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1952}, {"title": "Information-theoretic inference of common", "author": ["Bastian Steudel", "Nihat Ay"], "venue": "ancestors. Entropy,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Nonnegative decomposition of multivariate information", "author": ["P.L. Williams", "R.D. Beer"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The hope of finding a succinct principle that elucidates the brain\u2019s information processing abilities has often kindled interest in information-theoretic ideas [1, 2].", "startOffset": 160, "endOffset": 166}, {"referenceID": 1, "context": "The hope of finding a succinct principle that elucidates the brain\u2019s information processing abilities has often kindled interest in information-theoretic ideas [1, 2].", "startOffset": 160, "endOffset": 166}, {"referenceID": 2, "context": "In machine learning, on the other hand, the past decade has witnessed a shift in focus towards expressive, hierarchical models with tractable update rules, with successes driven by increasingly effective ways to leverage labeled data to learn rich models [3, 4].", "startOffset": 255, "endOffset": 261}, {"referenceID": 3, "context": "In machine learning, on the other hand, the past decade has witnessed a shift in focus towards expressive, hierarchical models with tractable update rules, with successes driven by increasingly effective ways to leverage labeled data to learn rich models [3, 4].", "startOffset": 255, "endOffset": 261}, {"referenceID": 4, "context": "Information-theoretic ideas like the venerable InfoMax principle [5, 6] can be and are applied in both contexts but they do not shed much light on the questions of when and why deep representations are useful for learning.", "startOffset": 65, "endOffset": 71}, {"referenceID": 5, "context": "Information-theoretic ideas like the venerable InfoMax principle [5, 6] can be and are applied in both contexts but they do not shed much light on the questions of when and why deep representations are useful for learning.", "startOffset": 65, "endOffset": 71}, {"referenceID": 6, "context": "The shift in perspective that enables our information decomposition is to focus on how well the learned representation explains multivariate mutual information in the data (a measure originally introduced as \u201ctotal correlation\u201d [7]).", "startOffset": 228, "endOffset": 231}, {"referenceID": 7, "context": "Because we end up with independent factors, one can also view this decomposition as a new way to do independent component analysis (ICA) [8, 9].", "startOffset": 137, "endOffset": 143}, {"referenceID": 8, "context": "Because we end up with independent factors, one can also view this decomposition as a new way to do independent component analysis (ICA) [8, 9].", "startOffset": 137, "endOffset": 143}, {"referenceID": 9, "context": "The implementation we develop here uses only discrete variables and is therefore most relevant for the challenging problem of ICA with discrete variables, which has applications to compression [10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 4, "context": "The \u201cInfoMax\u201d principle [5, 6] suggests that for unsupervised learning we should construct Y \u2019s to maximize their mutual information with X , the data.", "startOffset": 24, "endOffset": 30}, {"referenceID": 5, "context": "The \u201cInfoMax\u201d principle [5, 6] suggests that for unsupervised learning we should construct Y \u2019s to maximize their mutual information with X , the data.", "startOffset": 24, "endOffset": 30}, {"referenceID": 10, "context": "Despite its intuitive appeal, this approach has several potential problems (see [12] for one example).", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "Here we focus on the fact that the InfoMax principle is not very useful for characterizing \u201cdeep representations\u201d, even though it is often invoked in this context [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "This follows directly from the data processing inequality (a similar argument appears in [14]).", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7].", "startOffset": 106, "endOffset": 114}, {"referenceID": 14, "context": "Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7].", "startOffset": 106, "endOffset": 114}, {"referenceID": 6, "context": "Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7].", "startOffset": 286, "endOffset": 289}, {"referenceID": 6, "context": "That TC(X) can be hierarchically decomposed in terms of short and long range dependencies was already appreciated by Watanabe [7] and has been used in applications such as hierarchical clustering [17].", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "That TC(X) can be hierarchically decomposed in terms of short and long range dependencies was already appreciated by Watanabe [7] and has been used in applications such as hierarchical clustering [17].", "startOffset": 196, "endOffset": 200}, {"referenceID": 14, "context": "The quantities of the form TC(X;Yk) can be estimated and optimized over efficiently [16], despite involving highdimensional variables.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "Surprisingly, optimizing this objective over possible functions has a simple, iterative solution procedure that is guaranteed to find a local maximum of the objective in time linear in the number of variables [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "1 Not only that, but a byproduct of the procedure is to give us a value for the objective TC(X;Yk), which can be estimated even from a small number of samples [16].", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": "Note that PCA fails to recover the sources for this example while ICA (the FastICA algorithm [9]) also succeeds.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Going back at least to Barlow [1], it was recognized that if X is transformed into some other basis, Y , with the Y \u2019s independent (TC(Y ) = 0), then the coding cost in this new basis is H(Y ) = \u2211 j H(Yj), i.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "This is exactly the problem of independent component analysis: transform the data into a basis for which TC(Y ) = 0, or is minimized [8, 9].", "startOffset": 133, "endOffset": 139}, {"referenceID": 8, "context": "This is exactly the problem of independent component analysis: transform the data into a basis for which TC(Y ) = 0, or is minimized [8, 9].", "startOffset": 133, "endOffset": 139}, {"referenceID": 16, "context": "That independence could be achieved as a byproduct of efficient coding has been previously considered [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "Recent state-of-the-art results lower the complexity of this problem to only a single exponential in the number of variables [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "guarantees a global optimum [10].", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "The more traditional scenario for ICA is to consider a reconstruction problem where some (typically continuous) and independent source variables are linearly mixed according to some unknown matrix [8, 9].", "startOffset": 197, "endOffset": 203}, {"referenceID": 8, "context": "The more traditional scenario for ICA is to consider a reconstruction problem where some (typically continuous) and independent source variables are linearly mixed according to some unknown matrix [8, 9].", "startOffset": 197, "endOffset": 203}, {"referenceID": 17, "context": "We can use this lossy compression interpretation to mimic tasks traditionally performed by generative models including in-painting and generating new examples (the converse, interpreting a generative model as lossy compression, has also been considered [20]).", "startOffset": 253, "endOffset": 257}, {"referenceID": 18, "context": "each pixel gives us an estimate of how many bits are required for this compression scheme, about 298 bits [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "Measure Naive Shannon gzip [22] Huffman [23] Sieve Compression ratio 2.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Measure Naive Shannon gzip [22] Huffman [23] Sieve Compression ratio 2.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "For comparison we consider two standard compression schemes, gzip, based on Lempel-Ziv coding [22], and Huffman coding [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "For comparison we consider two standard compression schemes, gzip, based on Lempel-Ziv coding [22], and Huffman coding [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "The principle of correlation explanation was recently introduced as an objective for unsupervised representation learning [16, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 13, "context": "The principle of correlation explanation was recently introduced as an objective for unsupervised representation learning [16, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "Another interesting related result showed that positivity of the quantity TC(X;Y ) (the same quantity appearing in our bounds) implies that the X\u2019s share a common ancestor in any DAG consistent with pX(x) [24].", "startOffset": 205, "endOffset": 209}, {"referenceID": 22, "context": "A different line of work about information decomposition focuses on distinguishing synergy and redundancy [25], though these measures are typically impossible to estimate for high-dimensional systems.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "The information bottleneck (IB) [26] is another information-theoretic optimization for constructing representations of data that has many mathematical similarities to the CorEx approach [16], with the main difference being that IB focuses on supervised learning while the latter is an unsupervised learning approach.", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Recently, the IB principle was used to investigate the value of depth in the context of supervised learning [14].", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "We introduce a new framework for unsupervised learning of deep representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of tasks including independent component analysis, lossy and lossless compression, and predicting missing values in data. The hope of finding a succinct principle that elucidates the brain\u2019s information processing abilities has often kindled interest in information-theoretic ideas [1, 2]. In machine learning, on the other hand, the past decade has witnessed a shift in focus towards expressive, hierarchical models with tractable update rules, with successes driven by increasingly effective ways to leverage labeled data to learn rich models [3, 4]. Information-theoretic ideas like the venerable InfoMax principle [5, 6] can be and are applied in both contexts but they do not shed much light on the questions of when and why deep representations are useful for learning. We introduce a novel incremental and hierarchical decomposition of information and show that it defines a framework for unsupervised learning of deep representations in which the contribution of each layer can be precisely quantified. Moreover, this scheme automatically determines the structure and depth among hidden units in the representation based only on local learning rules. The shift in perspective that enables our information decomposition is to focus on how well the learned representation explains multivariate mutual information in the data (a measure originally introduced as \u201ctotal correlation\u201d [7]). Intuitively, our approach constructs a hierarchical representation of data by passing it through a sequence of progressively fine-grained sieves. At the first layer of the sieve we learn a factor that explains as much of the dependence in the data as possible. The data is then transformed into the \u201cremainder information\u201d, which has this dependence extracted. The next layer of the sieve looks for the largest source of dependence in the remainder information, and the cycle repeats. At each step, we obtain a successively tighter upper and lower bound on the multivariate information in the data, with convergence between the bounds obtained when the remaining information consists of nothing but independent factors. Because we end up with independent factors, one can also view this decomposition as a new way to do independent component analysis (ICA) [8, 9]. Unlike traditional methods, we do not assume a specific generative model of the data (i.e., that it consists of a linear transformation of independent sources) and we extract independent factors incrementally rather than all at once. The implementation we develop here uses only discrete variables and is therefore most relevant for the challenging problem of ICA with discrete variables, which has applications to compression [10]. 1 ar X iv :1 50 7. 02 28 4v 1 [ st at .M L ] 8 J ul 2 01 5 After introducing some background in Sec. 1, we introduce a new way to iteratively decompose the information in data in Sec. 2, and show how to use these decompositions to define a practical and incremental framework for unsupervised representation learning in Sec. 3. We demonstrate the versatility of this framework by applying it first to independent component analysis (Sec. 4). Next, we use the sieve as a lossy compression to mimic the traditional strengths of generative models including in-painting and generating new samples (Sec. 5). Finally, we cast the sieve as a lossless compression and show that it beats standard compression schemes on a benchmark task (Sec. 6). 1 Information-theoretic learning background Using standard notation [11], capital Xi denotes a random variable taking values in some domain and whose instances are denoted in lowercase, xi. In this paper, the domain of all variables are considered to be discrete and finite. We abbreviate multivariate random variables, X \u2261 X1:n \u2261 X1, . . . , Xn, with an associated probability distribution, pX(X1 = x1, . . . , Xn = xn), which is typically abbreviated to p(x). We will index different groups of multivariate random variables with superscripts, X, as defined in Fig. 1. We let X denote the original observed variables and we often omit the superscript in this case for readability. Entropy is defined in the usual way as H(X) \u2261 EX [log 1/p(x)]. We use base two logarithms so that the unit of information is bits. Higher-order entropies can be constructed in various ways from this standard definition. For instance, the mutual information between two groups of random variables, X and Y can be written as the reduction of uncertainty in one variable, given information about the other, I(X;Y ) = H(X)\u2212H(X|Y ). The \u201cInfoMax\u201d principle [5, 6] suggests that for unsupervised learning we should construct Y \u2019s to maximize their mutual information with X , the data. Despite its intuitive appeal, this approach has several potential problems (see [12] for one example). Here we focus on the fact that the InfoMax principle is not very useful for characterizing \u201cdeep representations\u201d, even though it is often invoked in this context [13]. This follows directly from the data processing inequality (a similar argument appears in [14]). Namely, if we start with X , construct a layer of hidden units Y 1 that are a function of X , and continue adding layers to a stacked representation so that X \u2192 Y 1 \u2192 Y 2 . . . Y , then the information that the Y \u2019s have about X cannot increase after the first layer, I(X;Y ) = I(X;Y ). From the point of view of mutual information, Y 1 is a copy and Y 2 is just a copy of a copy. While a coarse-grained copy might be useful, the InfoMax principle does not quantify how or why. Instead of maximizing I(X;Y ), the recently introduced principle of total Correlation Explanation (CorEx) [15, 16] suggests to construct Y \u2019s that explain the multivariate dependence in X according to a multivariate measure of mutual information first introduced as \u201ctotal correlation\u201d [7]. TC(X) \u2261 DKL ( p(x)|| n \u220f", "creator": "LaTeX with hyperref package"}}}