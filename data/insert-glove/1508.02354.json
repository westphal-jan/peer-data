{"id": "1508.02354", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2015", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning", "abstract": "tertiaries Deep compositional models of meaning jako acting jesuits on distributional marling representations weightiness of words in order section23 to produce vectors of brcic larger lius text 19:13 constituents dubnicoff are ratnakara evolving to 7:14 a highfather popular helgoland area sgu of warrender NLP research. eritreans We chinamasa detail a compositional cathedral-like distributional jinghu framework based on a mercantilism rich chucked form rebrov of word agf embeddings whampoa that slichter aims 109.70 at fjetland facilitating the visualised interactions 10,650 between syngnathidae words walk-in in carnley the context of a sentence. Embeddings nashville and composition intramural layers wobbly are lupine jointly zhu learned guandong against a faulder generic objective previsions that 192d enhances the vectors horsefly with syntactic desker information from the melander surrounding belgrano context. Furthermore, each word m37 is barr\u00f3n associated with a number windheim of paru senses, stalactites the most plausible of twinsburg which mascot is selected dynamically during maquina the denigrates composition aleh process. We evaluate zargar the produced vectors qualitatively scroll and quantitatively segawa with arnoldi positive results. bogomil At the djindjic sentence level, quiescence the effectiveness mahtab of five-story the bayville framework is 78.20 demonstrated 80.36 on transgas the MSRPar compensators task, for arbor which 1,076 we report crate results 2.5-4 within 1.76 the anguilla state - of - the - art range.", "histories": [["v1", "Mon, 10 Aug 2015 19:04:18 GMT  (205kb,D)", "https://arxiv.org/abs/1508.02354v1", "Accepted for presentation at EMNLP 2015"], ["v2", "Thu, 13 Aug 2015 12:50:43 GMT  (205kb,D)", "http://arxiv.org/abs/1508.02354v2", "Accepted for presentation at EMNLP 2015"]], "COMMENTS": "Accepted for presentation at EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["jianpeng cheng 0002", "dimitri kartsaklis"], "accepted": true, "id": "1508.02354"}, "pdf": {"name": "1508.02354.pdf", "metadata": {"source": "CRF", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning", "authors": ["Jianpeng Cheng", "Dimitri Kartsaklis"], "emails": ["jianpeng.cheng@stcatz.oxon.org", "d.kartsaklis@qmul.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Representing the meaning of words by using their distributional behaviour in a large text corpus is a well-established technique in NLP research that has been proved useful in numerous tasks. In a distributional model of meaning, the semantic representation of a word is given as a vector in some high dimensional vector space, obtained either by explicitly collecting co-occurrence statistics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture (Collobert and Weston, 2008; Mikolov et al., 2013).\nRegardless their method of construction, distributional models of meaning do not scale up to\nlarger text constituents such as phrases or sentences, since the uniqueness of multi-word expressions would inevitably lead to data sparsity problems, thus to unreliable vectorial representations. The problem is usually addressed by the provision of a compositional function, the purpose of which is to prepare a vectorial representation for a phrase or sentence by combining the vectors of the words therein. While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences for a variety of tasks (Socher et al., 2012; Kalchbrenner et al., 2014).\nThe mutual interaction of distributional word vectors by a means of a compositional model provides many opportunities for interesting research, the majority of which still remains to be explored. One such direction is to investigate in what way lexical ambiguity affects the compositional process. In fact, recent work has shown that shallow multi-linear compositional models that explicitly handle extreme cases of lexical ambiguity in a step prior to composition present consistently better performance than their \u201cambiguous\u201d counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). A first attempt to test these observations in a deep compositional setting has been presented by Cheng et al. (2014) with promising results.\nFurthermore, a second important question relates to the very nature of the word embeddings used in the context of a compositional model. In a setting of this form, word vectors are not any more just a means for discriminating words based on their underlying semantic relationships; the main goal of a word vector is to contribute to a bigger whole\u2014a task in which syntax, along with semantics, also plays a very important role. It is a central point of this paper, therefore, that in a compositional distributional model of meaning word vectors should be injected with information that reflects their syntactical roles in the training corpus.\nar X\niv :1\n50 8.\n02 35\n4v 2\n[ cs\n.C L\n] 1\n3 A\nug 2\n01 5\nThe purpose of this work is to improve the current practice in deep compositional models of meaning in relation to both the compositional process itself and the quality of the word embeddings used therein. We propose an architecture for jointly training a compositional model and a set of word embeddings, in a way that imposes dynamic word sense induction for each word during the learning process. Note that this is in contrast with recent work in multi-sense neural word embeddings (Neelakantan et al., 2014), in which the word senses are learned without any compositional considerations in mind.\nFurthermore, we make the word embeddings syntax-aware by introducing a variation of the hinge loss objective function of Collobert and Weston (2008), in which the goal is not only to predict the occurrence of a target word in a context, but to also predict the position of the word within that context. A qualitative analysis shows that our vectors reflect both semantic and syntactic features in a concise way.\nIn all current deep compositional distributional settings, the word embeddings are internal parameters of the model with no use for any other purpose than the task for which they were specifically trained. In this work, one of our main considerations is that the joint training step should be generic enough to not be tied in any particular task. In this way the word embeddings and the derived compositional model can be learned on data much more diverse than any task-specific dataset, reflecting a wider range of linguistic features. Indeed, experimental evaluation shows that the produced word embeddings can serve as a high quality general-purpose semantic word space, presenting performance on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012) competitive to and even better of the performance of well-established neural word embeddings sets.\nFinally, we propose a dynamic disambiguation framework for a number of existing deep compositional models of meaning, in which the multisense word embeddings and the compositional model of the original training step are further refined according to the purposes of a specific task at hand. In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005). An interesting aspect at the sideline of the paraphrase detection experiment is that, in contrast to mainstream approaches that mainly rely on simple forms of clas-\nsifiers, we approach the problem by following a siamese architecture (Bromley et al., 1993)."}, {"heading": "2 Background and related work", "text": ""}, {"heading": "2.1 Distributional models of meaning", "text": "Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings. Traditional approaches for constructing a word space rely on simple counting: a word is represented by a vector of numbers (usually smoothed by the application of some function such as point-wise mutual information) which show how frequently this word co-occurs with other possible context words in a corpus of text.\nIn contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model). Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al., 2014)."}, {"heading": "2.2 Syntactic awareness", "text": "Since the main purpose of distributional models until now was to measure the semantic relatedness of words, relatively little effort has been put into making word vectors aware of information regarding the syntactic role under which a word occurs in a sentence. In some cases the vectors are POStag specific, so that \u2018book\u2019 as noun and \u2018book\u2019 as verb are represented by different vectors (Kartsaklis and Sadrzadeh, 2013). Furthermore, word spaces in which the context of a target word is determined by means of grammatical dependencies (Pado\u0301 and Lapata, 2007) are more effective in capturing syntactic relations than approaches based on simple word proximity.\nFor word embeddings trained in neural settings, syntactic information is not usually taken explicitly into account, with some notable exceptions. At the lexical level, Levy and Goldberg (2014) propose an extension of the skip-gram model\nbased on grammatical dependencies. Following a different approach, Mnih and Kavukcuoglu (2013) weight the vector of each context word depending on its distance from the target word. With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words.\nAs we will see in Section 3, the current paper offers an appealing alternative to those approaches that does not depend on grammatical relations or types of any form."}, {"heading": "2.3 Compositionality in distributional models", "text": "The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al., 2010). In this latter work relational words (such as verbs or adjectives) are represented as multi-linear maps acting on vectors representing their arguments (nouns and noun phrases). In general, the above models are shallow in the sense that they do not have functional parameters and the output is produced by the direct interaction of the inputs; yet they have been shown to capture the compositional meaning of sentences to an adequate degree.\nThe idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popularized by Socher and colleagues (Socher et al., 2011a; Socher et al., 2012). The compositional architecture used in these works is that of a recursive neural network (RecNN) (Socher et al., 2011b), where the words get composed by following a parse tree. A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010). Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al., 1998), the main characteristic of which is that it acts on small overlapping parts of the input vectors. In all the above models, the word embeddings and the weights of the compositional layers are optimized against a task-specific objective function.\nIn Section 3 we will show how to remove the restriction of a supervised setting, introduc-\ning a generic objective that can be trained on any general-purpose text corpus. While we focus on recursive and recurrent neural network architectures, the general ideas we will discuss are in principle model-independent."}, {"heading": "2.4 Disambiguation in composition", "text": "Regardless of the way they address composition, all the models of Section 2.3 rely on ambiguous word spaces, in which every meaning of a polysemous word is merged into a single vector. Especially for cases of homonymy (such as \u2018bank\u2019, \u2018organ\u2019 and so on), where the same word is used to describe two or more completely unrelated concepts, this approach is problematic: the semantic representation of the word becomes the average of all senses, inadequate to express any of them in a reliable way.\nTo address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). This idea has been tested on algebraic and tensor-based compositional functions with very positive results. Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014). This latter work clearly suggests that explicitly dealing with lexical ambiguity in a deep compositional setting is an idea that is worth to be further explored. While treating disambiguation as only a preprocessing step is a strategy less than optimal for a neural setting, one would expect that the benefits should be greater for an architecture in which the disambiguation takes place in a dynamic fashion during training.\nWe are now ready to start detailing a compositional model that takes into account the above considerations. The issue of lexical ambiguity is covered in Section 4; Section 3 below deals with generic training and syntactic awareness."}, {"heading": "3 Syntax-based generic training", "text": "We propose a novel architecture for learning word embeddings and a compositional model to use them in a single step. The learning takes places in the context of a RecNN (or an RNN), and both word embeddings and parameters of the compositional layer are optimized against a generic objective function that uses a hinge loss function.\nFigure 1 shows the general form of recursive and recurrent neural networks. In architectures of this form, a compositional layer is applied on each pair of inputs x1 and x2 in the following way:\np = g(Wx[1:2] + b) (1)\nwhere x[1:2] denotes the concatenation of the two vectors, g is a non-linear function, and W,b are the parameters of the model. In the RecNN case, the compositional process continues recursively by following a parse tree until a vector for the whole sentence or phrase is produced; on the other hand, an RNN assumes that a sentence is generated in a left-to-right fashion, taking into consideration no dependencies other than word adjacency.\nWe amend the above setting by introducing a novel layer on the top of the compositional one, which scores the linguistic plausibility of the composed sentence or phrase vector with regard to both syntax and semantics. Following Collobert and Weston (2008), we convert the unsupervised learning problem to a supervised one by corrupting training sentences. Specifically, for each sentence s we create two sets of negative examples. In the first set, S\u2032, the target word within a given context is replaced by a random word; as in the original C&W paper, this set is used to enforce semantic coherence in the word vectors. Syntactic coherence is enforced by a second set of negative examples, S\u2032\u2032, in which the words of the context have been randomly shuffled. The objective function is defined in terms of the following hinge losses:\u2211\ns\u2208S \u2211 s\u2032\u2208S\u2032 max(0,m\u2212 f(s) + f(s\u2032)) (2)\n\u2211 s\u2208S \u2211 s\u2032\u2032\u2208S\u2032\u2032 max(0,m\u2212 f(s) + f(s\u2032\u2032)) (3)\nwhere S is the set of sentences, f the compositional layer, and m a margin we wish to retain between the scores of the positive training examples and the negative ones. During training, all parameters in the scoring layer, the compositional layers and word representations are jointly updated by error back-propagation. As output, we get both general-purpose syntax-aware word representations and weights for the corresponding compositional model."}, {"heading": "4 From words to senses", "text": "We now extend our model to address lexical ambiguity. We achieve that by applying a gated architecture, similar to the one used in the multi-sense model of Neelakantan et al. (2014), but advancing the main idea to the compositional setting detailed in Section 3.\nWe assume a fixed number of n senses per word.1 Each word is associated with a main vector (obtained for example by using an existing vector set, or by simply applying the process of Section 3 in a separate step), as well as with n vectors denoting cluster centroids and an equal number of sense vectors. Both cluster centroids and sense vectors are randomly initialized in the beginning of the process. For each word wt in a training sentence, we prepare a context vector by averaging the main vectors of all other words in the same context. This context vector is compared with the cluster centroids of wt by cosine similarity, and the sense corresponding to the closest cluster is selected as the most representative of wt in the current context. The selected cluster centroid is updated by the addition of the context vector, and the associated sense vector is passed as input to the compositional layer. The selected sense vectors for each word in the sentence are updated by backpropagation, based on the objectives of Equations 2 and 3. The overall architecture of our model, as described in this and the previous section, is illustrated in Figure 2."}, {"heading": "5 Task-specific dynamic disambiguation", "text": "The model of Figure 2 decouples the training of word vectors and compositional parameters from\n1Note that in principle the fixed number of senses assumption is not necessary; Neelakantan et al. (2014), for example, present a version of their model in which new senses are added dynamically when appropriate.\nmain (ambiguous)\nvectors\nsense vectors\ngate\ncompositional layer\nphrase vector plausibility layer\ncompositional layer sentence vector plausibility layer\nFigure 2: Training of syntax-aware multi-sense embeddings in the context of a RecNN.\na specific task, and as a consequence from any task-specific training dataset. However, note that by replacing the plausibility layer with a classifier trained for some task at hand, you get a taskspecific network that transparently trains multisense word embeddings and applies dynamic disambiguation on the fly. While this idea of a singlestep direct training seems appealing, one consideration is that the task-specific dataset used for the training will not probably reflect the linguistic variety that is required to exploit the expressiveness of the setting in its full. Additionally, in many cases the size of datasets tied to specific tasks is prohibiting for training a deep architecture.\nIt is a merit of this proposal that, in cases like these, it is possible for one to train the generic model of Figure 2 on any large corpus of text, and then use the produced word vectors and compositional weights to initialize the parameters of a more specific version of the architecture. As a result, the trained parameters will be further refined according to the task-specific objective. Figure 3 illustrates the generic case of a compositional framework applying dynamic disambiguation. Note that here sense selection takes place by a soft-max layer, which can be directly optimized on the task objective."}, {"heading": "6 A siamese network for paraphrase detection", "text": "We will test the dynamic disambiguation framework of Section 5 in a paraphrase detection task. A paraphrase is a restatement of the meaning of a\nsentence using different words and/or syntax. The goal of a paraphrase detection model, thus, is to examine two sentences and decide if they express the same meaning.\nWhile the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014). While siamese networks have been also used in the past for NLP purposes (for example, by Yih et al. (2011)), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection.\nIn our model, two networks sharing the same parameters are used to compute the vectorial representations of two sentences, the paraphrase relation of which we wish to detect; this is achieved by employing a cost function that compares the two vectors. There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al., 2014). The L2 norm variation is capable of handling differences in the magnitude of the vectors. Formally, the cost function is defined as:\nEf =\n{ 1 2 \u2016f(s1)\u2212 f(s2)\u201622 , if y = 1\n1 2 max(0,m\u2212 \u2016f(s1)\u2212 f(s2)\u20162) 2, o.w.\nwhere s1, s2 are the input sentences, f the compositional layer (so f(s1) and f(s2) refer to sentence vectors), and y = 1 denotes a paraphrase relationship between the sentences; m stands for the margin, a hyper-parameter chosen in advance. On\nthe other hand, the cost function based on cosine similarity handles only directional differences, as follows:\nEf = 1\n2 (y \u2212 \u03c3(wd+ b))2 (4)\nwhere d = f(s1)\u00b7f(s2)\u2016f(s1)\u20162\u2016f(s2)\u20162 is the cosine similarity of the two sentence vectors, w and b are the scaling and shifting parameters to be optimized, \u03c3 is the sigmoid function and y is the label. In the experiments that will follow in Section 7.4, both of these cost functions are evaluated. The overall architecture is shown in Figure 4.\nIn Section 7.4 we will use the pre-trained vectors and compositional weights for deriving sentence representations that will be subsequently fed to the siamese network. When the dynamic disambiguation framework is used, the sense vectors of the words are updated during training so that the sense selection process is gradually refined."}, {"heading": "7 Experiments", "text": "We evaluate the quality of the compositional word vectors and the proposed deep compositional framework in the tasks of word similarity and paraphrase detection, respectively."}, {"heading": "7.1 Model pre-training", "text": "In all experiments the word representations and compositional models are pre-trained on the British National Corpus (BNC), a general-purpose text corpus that contains 6 million sentences of written and spoken English. For comparison we train two sets of word vectors and compositional models, one ambiguous and one multi-sense (fix-\ning 3 senses per word). The dimension of the embeddings is set to 300.\nAs our compositional architectures we use a RecNN and an RNN. In the RecNN case, the words are composed by following the result of an external parser, while for the RNN the composition takes place in sequence from left to right. To avoid the exploding or vanishing gradient problem (Bengio et al., 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). During the training of each model, we minimize the hinge loss in Equations 2 and 3. The plausibility layer is implemented as a 2-layer network, with 150 units at the hidden layer, and is applied at each individual node (as opposed to a single application at the sentence level). All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (\u03bb = 0.03, initial \u03b1 = 0.05)."}, {"heading": "7.2 Qualitative evaluation of the word vectors", "text": "As a first step, we qualitatively evaluate the trained word embeddings by examining the nearest neighbours lists of a few selected words. We compare the results with those produced by the skipgram model (SG) of Mikolov et al. (2013) and the language model (CW) of Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense). The results in Table 1 show clearly that our model tends to group words that are both semantically and syntactically related; for example, and in contrast with the compared models which group words only at the semantic level, our model is able to retain tenses, numbers (singulars and plurals), and gerunds.\nThe observed behaviour is comparable to that of embedding models with objective functions conditioned on grammatical relations between words; Levy and Goldberg (2014), for example, present a similar table for their dependency-based extension of the skip-gram model. The advantage of our approach against such models is twofold: firstly, the word embeddings are accompanied by a generic compositional model that can be used for creating sentence representations independently of any specific task; and secondly, the training is quite forgiving to data sparsity problems that in general a dependency-based approach would intensify (since context words are paired with the grammatical relations they occur with the target word). As a result, a small corpus such as the BNC is sufficient for producing high quality syntax-aware word embeddings."}, {"heading": "7.3 Word similarity", "text": "We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012). The dataset contains 2,003 pairs of words and the contexts they occur in. We can therefore make use of the contextual information in order to select the most appropriate sense for each ambiguous word. Similarly to Neelakantan et al. (2014), we use three different metrics: globalSim measures the similarity between two ambiguous word vectors; localSim selects a single sense for each word based on the context and computes the similarity between the two sense vectors; avgSim represents each word as a weighted average of all senses in the given context and computes the similarity between the two weighted sense vectors.\nWe compute and report the Spearman\u2019s correlation between the embedding similarities and human judgments (Table 2). In addition to the skipgram and Collobert and Weston models, we also compare against the CBOW model (Mikolov et al., 2013) and the multi-sense skip-gram (MSSG) model of Neelakantan et al. (2014).\nAmong all methods, only the MSSG model and ours are capable of learning multi-prototype word representations. Our embeddings show top performance for localSim and avgSim measures, and performance competitive to that of MSSG and SG for globalSim, both of which use a hierarchical\nsoft-max as their objective function. Compared to the original C&W model, our version presents an improvement of 4.6%\u2014a clear indication for the effectiveness of the proposed learning method and the enhanced objective."}, {"heading": "7.4 Paraphrase detection", "text": "In the last set of experiments, the proposed compositional distributional framework is evaluated on the Microsoft Research Paraphrase Corpus (MSRPC) (Dolan and Brockett, 2005), which contains 5,800 pairs of sentences. This is a binary classification task, with labels provided by human annotators. We apply the siamese network detailed in Section 6.\nWhile MSRPC is one of the most used datasets for evaluating paraphrase detection models, its size is prohibitory for any attempt of training a deep architecture. Therefore, for our training we rely on a much larger external dataset, the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). The PPDB contains more than 220 million paraphrase pairs, of which 73 million are phrasal paraphrases and 140 million are paraphrase patterns that capture syntactic transformations of sentences. We use these phrase- and sentence-level paraphrase pairs as additional training contexts to fine-tune the generic compositional model parameters and word embeddings and to train the baseline models. The original training set of the MSRPC is used as validation set for deciding hyperparameters, such as the margin of the error function and the number of training epochs.\nThe evaluations were conducted on various aspects, and the models are gradually refined to demonstrate performance within the state-of-theart range.\nComparison of the two error functions In the first evaluation, we compare the two error functions of the siamese network using only ambigu-\nous vectors. As we can see in Table 3, the cosine error function consistently outperforms the L2 norm-based one for both compositional models, providing a yet another confirmation of the already well-established fact that similarity in semantic vector spaces is better reflected by lengthinvariant measures.\nEffectiveness of disambiguation We now proceed to compare the effectiveness of the two compositional models when using ambiguous vectors and multi-sense vectors, respectively. Our error function is set to cosine similarity, following the results of the previous evaluation. When dynamic disambiguation is applied, we test two methods of selecting sense vectors: in the hard case the vector of the most plausible sense is selected, while in the soft case a new vector is prepared as the weighted average of all sense vectors according to probabilities returned by the soft-max layer (see Figure 3). As a baseline we use a simple compositional model based on vector addition.\nThe dynamic disambiguation models and the additive baseline are compared with variations that use a simple prior disambiguation step applied on the word vectors. This is achieved by first selecting for each word the sense vector that is the closest to the average of all other word vectors in the same sentence, and then composing the selected sense vectors without further considerations regarding ambiguity. The baseline model and the prior disambiguation variants are trained as separate logistic regression classifiers. The results are shown in Table 4.\nOverall, disambiguated vectors work better than the ambiguous ones, with the improvement to be more significant for the additive model; there, a simple prior disambiguation step produces 1.4% gains. For the deep compositional models, simple\nprior disambiguation is still helpful with small improvements, a result which is consistent with the findings of Cheng et al. (2014). The small gains of the prior disambiguation models over the ambiguous models clearly show that deep architectures are quite capable of performing this elementary form of sense selection intrinsically, as part of the learning process itself. However, the situation changes when the dynamic disambiguation framework is used, where the gains over the ambiguous version become more significant. Comparing the two ways of dynamic disambiguation (hard method and soft method), the numbers that the soft method gives are slightly higher, producing a total gain of 1.1% over the ambiguous version for the RecNN case.2\nNote that, at this stage, the advantage of using the dynamic disambiguation framework over simple prior disambiguation is still small (0.7% for the case of RecNN). We seek the reason behind this in the recursive nature of our architecture, which tends to progressively \u201chide\u201d local features of word vectors, thus diminishing the effect of the fine-tuned sense vectors produced by the dynamic disambiguation mechanism. The next section discusses the problem and provides a solution.\nThe role of pooling One of the problems of the recursive and recurrent compositional architectures, especially in grammars with strict branching structure such as in English, is that any given composition is usually the product of a terminal and a non-terminal; i.e. a single word can contribute to the meaning of a sentence to the same extent as the rest of a sentence on its whole, as below:\n[[kids]NP [play ball games in the park]VP]S\nIn the above case, the contribution of the words within the verb phrase to the final sentence representation will be faded out due to the recursive composition mechanism. Inspired by related work in computer vision (Sun et al., 2014), we attempt to alleviate this problem by introducing an average pooling layer at the sense vector level and adding the resulting vector to the sentence representation. By doing this we expect that the new sentence vector will reflect local features from all words in the sentence that can help in the classification in a more direct way. The results for the new deep architectures are shown in Table 5, where we see substantial improvements for both deep nets. More importantly, the effect of dynamic\n2For all subsequent experiments, the reported results are based on the soft selection method.\ndisambiguation now becomes more significant, as expected by our analysis.\nTable 5 also includes results for two models trained in a single step, with word and sense vectors randomly initialized at the beginning of the process. We see that, despite the large size of the training set, the results are much lower than the ones obtained when using the pre-training step. This demonstrates the importance of the initial training on a general-purpose corpus: the resulting vectors reflect linguistic information that, although not obtainable from the task-specific training, can make great difference in the result of the classification.\nCross-model comparison In this section we propose a method to further improve the performance of our models, and we present an evaluation against some of the previously reported results.\nWe notice that using distributional properties alone cannot capture efficiently subtle aspects of a sentence, for example numbers or human names. However, even small differences on those aspects between two sentences can lead to a different classification result. Therefore, we train (using the MSPRC training data) an additional logistic regression classifier which is based not only on the embeddings similarity, but also on a few handengineered features. We then ensemble the new classifier (C1) with the original one. In terms of feature selection, we follow Socher et al. (2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same). In Table 6 we report results of the original model and the ensembled model, and we compare with the performance of other existing models.\nIn all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far\nfor the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3"}, {"heading": "8 Conclusion and future work", "text": "The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the syntax-aware, multi-sense word vectors and the dynamic compositional disambiguation framework in which they are used was demonstrated by appropriate tasks at the lexical and sentence level, respectively, with very positive results. As an aside, we also demonstrated the benefits of a siamese architecture in the context of a paraphrase detection task. While the architectures tested in this work were limited to a RecNN and an RNN, the ideas we presented are in principle directly applicable to any kind of deep network. As a future step, we aim to test the proposed models on a convolutional compositional architecture, similar to that of Kalchbrenner et al. (2014)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the three anonymous reviewers for their useful comments, as well as Nal Kalchbrenner and Ed Grefenstette for early discussions and suggestions on the paper, and Simon S\u030custer for comments on the final draft. Dimitri Kartsaklis gratefully acknowledges financial support by AFOSR.\n3Source: ACL Wiki (http://www.aclweb.org/aclwiki), August 2015.\n4Code in Python/Theano and the word embeddings can be found at https://github.com/cheng6076."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah."], "venue": "International Journal of Pattern Recognition", "citeRegEx": "Bromley et al\\.,? 1993", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Investigating the role of prior disambiguation in deep-learning compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis", "Edward Grefenstette."], "venue": "2nd Workshop of Learning Semantics, NIPS 2014, Montreal, Canada,", "citeRegEx": "Cheng et al\\.,? 2014", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Mathematical foundations for a distributional compositional model of meaning", "author": ["B Coecke", "M Sadrzadeh", "S Clark."], "venue": "lambek festschrift. Linguistic Analysis, 36:345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Paraphrase identification as probabilistic quasi-synchronous recognition", "author": ["Dipanjan Das", "Noah A Smith."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-", "citeRegEx": "Das and Smith.,? 2009", "shortCiteRegEx": "Das and Smith.", "year": 2009}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["W.B. Dolan", "C. Brockett."], "venue": "Third International Workshop on Paraphrasing (IWP2005).", "citeRegEx": "Dolan and Brockett.,? 2005", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "A semantic similarity approach to paraphrase detection", "author": ["Samuel Fernando", "Mark Stevenson."], "venue": "Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics, pages 45\u201352. Citeseer.", "citeRegEx": "Fernando and Stevenson.,? 2008", "shortCiteRegEx": "Fernando and Stevenson.", "year": 2008}, {"title": "Ppdb: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "HLT-NAACL, pages 758\u2013764.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun."], "venue": "Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pages 1735\u20131742. IEEE.", "citeRegEx": "Hadsell et al\\.,? 2006", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 2014 Conference on Empirical Methods", "citeRegEx": "Hashimoto et al\\.,? 2014", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 894\u2013904,", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Semantic similarity of short texts", "author": ["Aminul Islam", "Diana Inkpen."], "venue": "Recent Advances in Natural Language Processing V, 309:227\u2013236.", "citeRegEx": "Islam and Inkpen.,? 2009", "shortCiteRegEx": "Islam and Inkpen.", "year": 2009}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891\u2013896, Seattle, Washington, USA, Oc-", "citeRegEx": "Ji and Eisenstein.,? 2013", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590\u20131601, Seattle, Wash-", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2013", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2013}, {"title": "Separating disambiguation from composition in distributional semantics", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "Proceedings of 17th Conference on Natural Language Learning (CoNLL), pages 114\u2013123, Sofia, Bulgaria,", "citeRegEx": "Kartsaklis et al\\.,? 2013", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2013}, {"title": "Resolving lexical ambiguity in tensor regression models of meaning", "author": ["Dimitri Kartsaklis", "Nal Kalchbrenner", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol. 2: Short Pa-", "citeRegEx": "Kartsaklis et al\\.,? 2014", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308, Baltimore, Maryland, June. Association", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava."], "venue": "AAAI, volume 6, pages 775\u2013780.", "citeRegEx": "Mihalcea et al\\.,? 2006", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of ACL-08: HLT, pages 236\u2013244, Columbus, Ohio, June. Association for Computational Linguistics.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Advances in Neural Information Processing Systems, pages 2265\u20132273.", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Dependency-based Construction of Semantic Space Models", "author": ["S. Pad\u00f3", "M. Lapata."], "venue": "Computational Linguistics, 33(2):161\u2013199.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2007", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2007}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive distributed representations", "author": ["Jordan B Pollack."], "venue": "Artificial Intelligence, 46(1):77\u2013105.", "citeRegEx": "Pollack.,? 1990", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Paraphrase recognition via dissimilarity significance classification", "author": ["Long Qiu", "Min-Yen Kan", "Tat-Seng Chua."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 18\u201326. Association for Compu-", "citeRegEx": "Qiu et al\\.,? 2006", "shortCiteRegEx": "Qiu et al\\.", "year": 2006}, {"title": "Dynamic and static prototype vectors for semantic composition", "author": ["Siva Reddy", "Ioannis P Klapaftis", "Diana McCarthy", "Suresh Manandhar."], "venue": "IJCNLP, pages 705\u2013713.", "citeRegEx": "Reddy et al\\.,? 2011", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Paraphrase identification with lexicosyntactic graph subsumption", "author": ["Vasile Rus", "Philip M McCarthy", "Mihai C Lintean", "Danielle S McNamara", "Arthur C Graesser."], "venue": "FLAIRS conference, pages 201\u2013206.", "citeRegEx": "Rus et al\\.,? 2008", "shortCiteRegEx": "Rus et al\\.", "year": 2008}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng."], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng."], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "Ng. A."], "venue": "Conference on Empirical Methods in Natural Language Processing 2012.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Yi Sun", "Yuheng Chen", "Xiaogang Wang", "Xiaoou Tang."], "venue": "Advances in Neural Information Processing Systems, pages 1988\u2013 1996.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Using dependency-based features to take the para-farce out of paraphrase", "author": ["Stephen Wan", "Mark Dras", "Robert Dale", "C\u00e9cile Paris."], "venue": "Proceedings of the Australasian Language Technology Workshop, volume 2006.", "citeRegEx": "Wan et al\\.,? 2006", "shortCiteRegEx": "Wan et al\\.", "year": 2006}, {"title": "Learning discriminative projections for text similarity measures", "author": ["Wen-tau Yih", "Kristina Toutanova", "John C. Platt", "Christopher Meek."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247\u2013256, Port-", "citeRegEx": "Yih et al\\.,? 2011", "shortCiteRegEx": "Yih et al\\.", "year": 2011}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "tics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 206, "endOffset": 256}, {"referenceID": 29, "context": "tics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture (Collobert and Weston, 2008; Mikolov et al., 2013).", "startOffset": 206, "endOffset": 256}, {"referenceID": 42, "context": "While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences for a variety of tasks (Socher et al., 2012; Kalchbrenner et al., 2014).", "startOffset": 225, "endOffset": 273}, {"referenceID": 20, "context": "While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences for a variety of tasks (Socher et al., 2012; Kalchbrenner et al., 2014).", "startOffset": 225, "endOffset": 273}, {"referenceID": 21, "context": "ter performance than their \u201cambiguous\u201d counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 52, "endOffset": 109}, {"referenceID": 23, "context": "ter performance than their \u201cambiguous\u201d counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 52, "endOffset": 109}, {"referenceID": 5, "context": "A first attempt to test these observations in a deep compositional setting has been presented by Cheng et al. (2014) with promising results.", "startOffset": 97, "endOffset": 117}, {"referenceID": 33, "context": "Note that this is in contrast with recent work in multi-sense neural word embeddings (Neelakantan et al., 2014), in which the word senses are learned without any compositional considerations in mind.", "startOffset": 85, "endOffset": 111}, {"referenceID": 7, "context": "Furthermore, we make the word embeddings syntax-aware by introducing a variation of the hinge loss objective function of Collobert and Weston (2008), in which the goal is not only to predict the occurrence of a target word in a context, but to also predict the position of the word within that context.", "startOffset": 121, "endOffset": 149}, {"referenceID": 17, "context": "Indeed, experimental evaluation shows that the produced word embeddings can serve as a high quality general-purpose semantic word space, presenting performance on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012) competitive to and even better of the performance of well-established neural word embeddings sets.", "startOffset": 221, "endOffset": 241}, {"referenceID": 9, "context": "In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005).", "startOffset": 146, "endOffset": 172}, {"referenceID": 4, "context": "An interesting aspect at the sideline of the paraphrase detection experiment is that, in contrast to mainstream approaches that mainly rely on simple forms of classifiers, we approach the problem by following a siamese architecture (Bromley et al., 1993).", "startOffset": 232, "endOffset": 254}, {"referenceID": 13, "context": "Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings.", "startOffset": 70, "endOffset": 84}, {"referenceID": 2, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 7, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 29, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 35, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 156, "endOffset": 252}, {"referenceID": 0, "context": "Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al.", "startOffset": 154, "endOffset": 175}, {"referenceID": 0, "context": "In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model).", "startOffset": 157, "endOffset": 416}, {"referenceID": 21, "context": "In some cases the vectors are POStag specific, so that \u2018book\u2019 as noun and \u2018book\u2019 as verb are represented by different vectors (Kartsaklis and Sadrzadeh, 2013).", "startOffset": 126, "endOffset": 158}, {"referenceID": 34, "context": "Furthermore, word spaces in which the context of a target word is determined by means of grammatical dependencies (Pad\u00f3 and Lapata, 2007) are more effective in capturing syntactic relations than approaches based on simple word proximity.", "startOffset": 114, "endOffset": 137}, {"referenceID": 25, "context": "At the lexical level, Levy and Goldberg (2014) propose an extension of the skip-gram model", "startOffset": 22, "endOffset": 47}, {"referenceID": 29, "context": "Following a different approach, Mnih and Kavukcuoglu (2013) weight the vector of each context word depending on its distance from the target word.", "startOffset": 32, "endOffset": 60}, {"referenceID": 14, "context": "With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words.", "startOffset": 71, "endOffset": 95}, {"referenceID": 14, "context": "With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words.", "startOffset": 71, "endOffset": 201}, {"referenceID": 30, "context": "The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al.", "startOffset": 219, "endOffset": 246}, {"referenceID": 6, "context": "The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al., 2010).", "startOffset": 266, "endOffset": 287}, {"referenceID": 36, "context": "The idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popularized by Socher and colleagues", "startOffset": 111, "endOffset": 126}, {"referenceID": 40, "context": "(Socher et al., 2011a; Socher et al., 2012).", "startOffset": 0, "endOffset": 43}, {"referenceID": 42, "context": "(Socher et al., 2011a; Socher et al., 2012).", "startOffset": 0, "endOffset": 43}, {"referenceID": 41, "context": "(Socher et al., 2011b), where the words get composed by following a parse tree.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010).", "startOffset": 153, "endOffset": 175}, {"referenceID": 20, "context": "Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al.", "startOffset": 30, "endOffset": 57}, {"referenceID": 24, "context": ", 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al., 1998), the main characteristic of which is that it acts on", "startOffset": 99, "endOffset": 119}, {"referenceID": 38, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 22, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 21, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 23, "context": "To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014).", "startOffset": 218, "endOffset": 320}, {"referenceID": 5, "context": "Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014).", "startOffset": 155, "endOffset": 175}, {"referenceID": 7, "context": "Following Collobert and Weston (2008), we convert the unsupervised learning problem to a supervised one by corrupting training sentences.", "startOffset": 10, "endOffset": 38}, {"referenceID": 33, "context": "We achieve that by applying a gated architecture, similar to the one used in the multi-sense model of Neelakantan et al. (2014), but advancing the main idea to the compositional setting detailed in Section 3.", "startOffset": 102, "endOffset": 128}, {"referenceID": 33, "context": "Note that in principle the fixed number of senses assumption is not necessary; Neelakantan et al. (2014), for example, present a version of their model in which new senses are added dynamically when appropriate.", "startOffset": 79, "endOffset": 105}, {"referenceID": 4, "context": "While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al.", "startOffset": 236, "endOffset": 258}, {"referenceID": 12, "context": ", 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014).", "startOffset": 69, "endOffset": 109}, {"referenceID": 43, "context": ", 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014).", "startOffset": 69, "endOffset": 109}, {"referenceID": 12, "context": "There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al.", "startOffset": 78, "endOffset": 118}, {"referenceID": 43, "context": "There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al.", "startOffset": 78, "endOffset": 118}, {"referenceID": 32, "context": ", 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al., 2014).", "startOffset": 51, "endOffset": 92}, {"referenceID": 43, "context": ", 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al., 2014).", "startOffset": 51, "endOffset": 92}, {"referenceID": 4, "context": "While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014). While siamese networks have been also used in the past for NLP purposes (for example, by Yih et al. (2011)), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection.", "startOffset": 237, "endOffset": 469}, {"referenceID": 1, "context": "To avoid the exploding or vanishing gradient problem (Bengio et al., 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997).", "startOffset": 53, "endOffset": 74}, {"referenceID": 16, "context": ", 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997).", "startOffset": 78, "endOffset": 112}, {"referenceID": 46, "context": "All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (\u03bb = 0.", "startOffset": 57, "endOffset": 71}, {"referenceID": 27, "context": "We compare the results with those produced by the skipgram model (SG) of Mikolov et al. (2013) and the language model (CW) of Collobert and Weston (2008).", "startOffset": 73, "endOffset": 95}, {"referenceID": 7, "context": "(2013) and the language model (CW) of Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense).", "startOffset": 38, "endOffset": 66}, {"referenceID": 25, "context": "The observed behaviour is comparable to that of embedding models with objective functions conditioned on grammatical relations between words; Levy and Goldberg (2014), for example, present a similar table for their dependency-based extension of the skip-gram model.", "startOffset": 142, "endOffset": 167}, {"referenceID": 17, "context": "We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012). The dataset contains 2,003 pairs of words and the contexts they occur in.", "startOffset": 123, "endOffset": 143}, {"referenceID": 17, "context": "We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012). The dataset contains 2,003 pairs of words and the contexts they occur in. We can therefore make use of the contextual information in order to select the most appropriate sense for each ambiguous word. Similarly to Neelakantan et al. (2014), we use three different metrics: globalSim measures the similarity between two ambiguous word vectors; localSim selects a single sense for each word based on the context and computes the similarity between the two sense vectors; avgSim represents each word as a weighted average of all senses in the given context and computes the similarity between the two weighted sense vectors.", "startOffset": 123, "endOffset": 384}, {"referenceID": 29, "context": "In addition to the skipgram and Collobert and Weston models, we also compare against the CBOW model (Mikolov et al., 2013) and the multi-sense skip-gram (MSSG) model of Neelakantan et al.", "startOffset": 100, "endOffset": 122}, {"referenceID": 7, "context": "In addition to the skipgram and Collobert and Weston models, we also compare against the CBOW model (Mikolov et al., 2013) and the multi-sense skip-gram (MSSG) model of Neelakantan et al. (2014).", "startOffset": 32, "endOffset": 195}, {"referenceID": 9, "context": "In the last set of experiments, the proposed compositional distributional framework is evaluated on the Microsoft Research Paraphrase Corpus (MSRPC) (Dolan and Brockett, 2005), which contains 5,800 pairs of sentences.", "startOffset": 149, "endOffset": 175}, {"referenceID": 11, "context": "Therefore, for our training we rely on a much larger external dataset, the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013).", "startOffset": 102, "endOffset": 129}, {"referenceID": 5, "context": "For the deep compositional models, simple prior disambiguation is still helpful with small improvements, a result which is consistent with the findings of Cheng et al. (2014). The small gains of the prior disambiguation models over the ambiguous models clearly show that deep architectures are quite capable of performing this elementary form of sense selection intrinsically, as part of the learning process itself.", "startOffset": 155, "endOffset": 175}, {"referenceID": 43, "context": "Inspired by related work in computer vision (Sun et al., 2014), we attempt to alleviate this problem by introducing an average pooling layer at the sense vector level and adding the resulting vector to the sentence representation.", "startOffset": 44, "endOffset": 62}, {"referenceID": 39, "context": "In terms of feature selection, we follow Socher et al. (2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same).", "startOffset": 41, "endOffset": 63}, {"referenceID": 3, "context": "(2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same).", "startOffset": 12, "endOffset": 37}, {"referenceID": 22, "context": "Pu bl is he d re su lts Mihalcea et al. (2006) 70.", "startOffset": 24, "endOffset": 47}, {"referenceID": 22, "context": "Pu bl is he d re su lts Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.", "startOffset": 24, "endOffset": 75}, {"referenceID": 22, "context": "Pu bl is he d re su lts Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.", "startOffset": 24, "endOffset": 103}, {"referenceID": 16, "context": "6 Islam and Inkpen (2009) 72.", "startOffset": 2, "endOffset": 26}, {"referenceID": 9, "context": "3 Fernando and Stevenson (2008) 74.", "startOffset": 2, "endOffset": 32}, {"referenceID": 9, "context": "3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.", "startOffset": 2, "endOffset": 60}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.", "startOffset": 2, "endOffset": 23}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.", "startOffset": 2, "endOffset": 55}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.", "startOffset": 2, "endOffset": 87}, {"referenceID": 8, "context": "0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.", "startOffset": 2, "endOffset": 122}, {"referenceID": 19, "context": "6 difference in Fscore from the first (Ji and Eisenstein, 2013).", "startOffset": 38, "endOffset": 63}, {"referenceID": 20, "context": "models on a convolutional compositional architecture, similar to that of Kalchbrenner et al. (2014).", "startOffset": 73, "endOffset": 100}], "year": 2015, "abstractText": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.", "creator": "TeX"}}}