{"id": "1508.04562", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections", "abstract": "Weak c-3 topic correlation across document khudiram collections kineret with glasslike different numbers sch\u00fctz of roadworks topics in individual cantering collections 117.56 presents nojima challenges lodge for babylonians existing 1,873 cross - 53,400 collection 24-bit topic models. This paper introduces two harriman probabilistic nl topic models, bayleaf Correlated gunports LDA (1974-1975 C - LDA) kohler and kanout\u00e9 Correlated HDP (C - HDP ). wijk These sanpaolo address ambassador-at-large problems kohls that can kidjo arise pignatelli when geospatial analyzing large, asymmetric, newbridge and over-expression potentially bovee weakly - 0430 related interwest collections. Topic correlations bagels in weakly - related representativeness collections andar typically g\u00f6kt\u00fcrk lie in vence the roundheads tail sancar of the androgen topic annabella distribution, rivulets where pizzaro they 0-for-7 would be overlooked seedings by mic.smith models unable pennate to fit unobservant large numbers of topics. lavrenti To efficiently zedler model this erle long tail for fetu'u large - brunson scale unpatrolled analysis, our ickler models implement pettingill a sparer parallel sampling algorithm 128.50 based on 80-20 the Metropolis - Hastings ihi and galligan alias methods (Yuan marunouchi et al. , stress-energy 2015 ). tuppence The westinghouse models sivana are starforce first evaluated gitis on synthetic data, cramphorn generated slivinski to simulate various ma\u0142y collection - level nitzarim asymmetries. mant We t\u00e2rgu then al-jazeera present a buncefield case study buoyant of modeling bn.com over dignam 300k documents normalized in hiscott collections tweeters of mascarene sciences and korps humanities baha'is research 2,768 from JSTOR.", "histories": [["v1", "Wed, 19 Aug 2015 08:30:37 GMT  (748kb,D)", "http://arxiv.org/abs/1508.04562v1", "EMNLP 2015"]], "COMMENTS": "EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jingwei zhang", "aaron gerow", "jaan altosaar", "james evans", "richard jean so"], "accepted": true, "id": "1508.04562"}, "pdf": {"name": "1508.04562.pdf", "metadata": {"source": "CRF", "title": "Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections", "authors": ["Jingwei Zhang", "Aaron Gerow", "Jaan Altosaar", "James Evans", "Richard Jean So"], "emails": ["jz2541@columbia.edu", "gerow@uchicago.edu", "jevans@uchicago.edu", "altosaar@princeton.edu", "richardjeanso@uchicago.edu"], "sections": [{"heading": "1 Introduction", "text": "Comparing large text collections is a critical task for the curation and analysis of human cultural history. Achievements of research and scholarship are most accessible through textual artifacts, which are increasingly available in digital archives. Text-based research, often undertaken by humanists, historians, lexicographers, and cor-\npus linguists, explores patterns of words in documents across time-periods and distinct collections of text. Here, we introduce two new topic models designed to compare large collections, Correlated LDA (C-LDA) and Correlated HDP (C-HDP), which are sensitive to document-topic asymmetry (where collections have different topic distributions) and topic-word asymmetry (where a single topic has different word distributions in each collection). These models seek to address terminological questions, such as how a topic on physics is articulated distinctively in scientific compared to humanistic research. Accommodating potential collection-level asymmetries is particularly important when researchers seek to analyze collections with little prior knowledge about shared or collection-specific topic structure. Our models extend existing cross-collection approaches to accommodate these asymmetries and implement an efficient parallel sampling algorithm enabling users to examine the long tail of topics in particularly large collections.\nUsing topic models for comparative text mining was introduced by Zhai et al. (2004), who developed the ccMix model which extended pLSA (Hofmann, 1999). Later work by Paul and Girju (2009) developed ccLDA, which adopted the hierarchical Bayes framework of Latent Dirichlet Allocation or LDA (Blei et al., 2003). These models account for topic-word asymmetry by assuming variation in the vocabularies of topics is due to collection-level differences. Nevertheless, they require the same topics to be present in each collection. These models are useful for comparing collections under specific assumptions, but cannot accommodate collection-topic asymmetry (which ar X\niv :1\n50 8.\n04 56\n2v 1\n[ cs\n.C L\n] 1\n9 A\nug 2\n01 5\narises in collections that do not share every topic or that have different numbers of topics). In situations where collections do not share all topics, the results often include junk, mixed, or sparse topics, making them difficult to interpret (Paul and Girju, 2009). Such asymmetries make it difficult to use models like ccLDA and ccMix when little is known about collections in advance. This motivates our efforts to model variation in the long tail of topic distributions, where correlations are more likely to appear when collections are weakly related.\nC-LDA and C-HDP extend ccLDA (Paul and Girju, 2009) to accommodate collection-topic level asymmetries, particularly by allowing noncommon topics to appear in each collection. This added flexibility allows our models to discover topic correlations across arbitrary collections with different numbers of topics, even when there are few (or unknown) numbers of common topics. To demonstrate the effectiveness of our models, we evaluate them on synthetic data and show that they outperform related models such as ccLDA and differential topic models (Chen et al., 2014). We then fit C-LDA to two large collections of humanities and sciences documents from JSTOR. Such historical analyses of text would be intractable without an efficient sampler. An optimized sampler is required in such situations because common topics in weakly-correlated collections are usually found in the tail of the document-topic distribution of a sufficiently large set of topics. To make this feasible on large datasets such as JSTOR, we employ a parallelized Metropolis-Hastings (Kronmal and Peterson Jr, 1979) and alias-table sampling framework, adapted from LightLDA (Yuan et al., 2015). These optimizations, which achieve O(1) amortized sampling time per token, allow our models to be fit to large corpora with up to thousands of topics in a matter of hours \u2014 an order of magnitude speed-up from ccLDA.\nAfter reviewing work related to topic modeling across collections, section 3 describes C-LDA and C-HDP, and then details their technical relationship to existing models. Section 5 introduces the synthetic data and part of the JSTOR corpus used in our evaluations. We then compare our models\u2019 performances to other models in terms of heldout perplexity and a measure of distinguishability. The final results section exemplifies the use of C-LDA in a qualitative analysis of humanities\nand sciences research. We conclude with a brief discussion of the strengths of C-LDA and C-HDP, and outline directions for future work and applications."}, {"heading": "2 Related Work", "text": "Our models seek to enable users to compare large collections that may only be weakly correlated and that may contain different numbers of topics. While topic models could be fit to separate collections to make post-hoc comparisons (Denny et al., 2014; Yang et al., 2011), our goal is to account for both document-topic asymmetry and topic-word asymmetry \u201cin-model\u201d. In short, we seek to model the correlation between arbitrary collections. Prioritizing in-model solutions for document-topic asymmetry has been explored elsewhere, such as in hierarchical Dirichlet processes (HDP), which use an additional level to account for collection variations in document-topic distributions (Teh et al., 2006).\nOne method designed to model topic-word asymmetry is ccMix (Zhai et al., 2004), which models the generative probability of a word in topic z from collection c as a mixture of shared and collection-specific distributions \u03b8z:\np(w) = \u03bbc p(w|\u03b8z) + (1\u2212 \u03bbc) p(w|\u03b8z,c)\nwhere \u03b8z,c is collection-specific and \u03bbc controls the mixing between shared and collection-specific topics. ccLDA extends ccMix to the LDA framework and adds a beta prior over \u03bbc that reduces sensitivity to input parameters (Paul and Girju, 2009). Another approach, differential topic models (Chen et al., 2014), is based on hierarchical Bayesian models over topic-word distributions. This method uses the transformed Pitman-Yor process (TPYP) to model topic-word distributions in each collection, with shared common base measures. As (Paul and Girju, 2009) note, ccLDA cannot accommodate a topic if it is not common across collections \u2014 an assumption made by ccMix, ccLDA and the TPYP. In a situation where a topic is found in only one collection, it would either dominate the shared topic portion (resulting in a noisy, collection-specific portion), or it would appear as a mixed topic, revealing two sets of unrelated words (Newman et al., 2010b). C-LDA ameliorates this situation by allowing the number of common and non-common topics to be specified separately and by efficiently sampling the tail\nof the document-topic distribution, allowing users to examine less prominent regions of the topic space. C-HDP also grants collections documenttopic independence using a hierarchical structure to model the differences between collections.\nDue to increased demand for scalable topic model implementations, there has been a proliferation of optimized methods for efficient inference, such as SparseLDA (Yao et al., 2009) and AliasLDA (Li et al., 2014). AliasLDA achieves O(Kd) complexity by using the MetropolisHastings-Walker algorithm and an alias table to sample topic-word distributions in O(1) time. Although this strategy introduces temporal staleness in the updates of sufficient statistics, the lag is overcome by more iterations, and converges significantly faster. A similar technique by Yuan et al. (2015), LightLDA, employs cycle-based Metropolis Hastings mixing with alias tables for both document-topic and topic-word distributions. Despite introducing lag in the sufficient statistics, this method achieves O(1) amortized sampling complexity and results in even faster convergence than AliasLDA. In addition to being fully parallelized, C-LDA adopts this sampling framework to make comparing large collections more tractable for large numbers of topics. Our models\u2019 efficient sampling methods allow users to fit large numbers of topics to big datasets where variation might not be observed in sub-sampled datasets or models with fewer topics."}, {"heading": "3 The Models", "text": ""}, {"heading": "3.1 Correlated LDA", "text": "In ccLDA (and ccMix), each topic has shared and collection-specific components for each collection. C-LDA extends ccLDA to make it more robust with respect to topic asymmetries between collections (Figure 1a). The crucial extension is that by allowing each collection to define a set of non-common topics in addition to common topics, the model removes an assumption imposed by ccLDA and other inter-collection models, namely that collections have the same number of topics. As a result, C-LDA is suitable for collections without a large proportion of common topics, and can also reduce noise (discussed in Section 2). To achieve this, C-LDA assumes document d in collection c has a multinomial document-topic distribution \u03b8 with an asymmetric Dirichlet prior for Kc topics, where the first K\u2205 are common\nacross collections. It is also possible to introduce a tree structure into the model that uses a binomial distribution to decide whether a word was drawn from common or non-common topics. This yields collection-specific background topics by using a binomial distribution instead of a multinomial. However, we prefer the simpler, non-tree version because background topics are unnecessary when using an asymmetric \u03b1 prior (Wallach et al., 2009a).\nThe generative process for C-LDA is as follows:\n1. Sample a distribution \u03c6k (shared component) from Dir(\u03b2) and a distribution \u03c3k from Beta(\u03b41, \u03b42) for each common topic k \u2208 {1, . . . ,K\u2205};\n2. For each collection c, sample a distribution \u03c6ck (collectionspecific component) from Dir(\u03b2) for each common topic k \u2208 {1, . . . ,K\u2205} and non-common topic k \u2208 {K\u2205 + 1, . . . ,Kc};\n3. For each document d in c, sample a distribution \u03b8 from Dir(\u03b1c);\n4. For each word wi in d:\n(a) Sample a topic zi \u2208 {1, . . . ,Kc} from Multi(\u03b8); (b) If zi \u2264 K\u2205, sample yi from Binomial(\u03c3zi); (c) Sample wi from Multi(\u03c6\u03bezi), where\n\u03be = { null , zi \u2264 K\u2205 and yi = 0; c , otherwise.\nNote that to capture common topics, K\u2205 should be set such that \u2203 c where Kc = K\u2205. Otherwise, words sampled as a non-common topic will not have information about non-common topics in other collections. Then a \u201ccommon-topic word\u201d is found among non-common topics in all collections (a local minima) and it will take a long time to stabilize as a common topic. To avoid this, when determining the number of topics for sampling, the number of non-common topics for the collection with the smallest number of total topics should be zero. After inference, to distinguish common and non-common topics in this collection, we model \u03c3 independently by assuming collections have the same mixing ratio for common topics. With this reasonable assumption and an asymmetric \u03b1, common topics become sparse enough that some \u03c3 distributions reduce nearly to 0, distinguishing them as non-common topics. Although this may seem counterintuitive, it does not negatively affect results.\nThree kinds of collection-level imbalance can confound inter-collection topic models: 1) in the numbers of topics between collections, 2) in the numbers of documents between collections, and 3) in the document-topic distributions. Each of\nthese can cause topics in different collections to have significantly different numbers of words assigned to the same topic. In this way, a topic can be dominated by the collection comprising most of its words. C-LDA addresses imbalances in the document-topic distributions between collections by estimating \u03b1. For imbalance in the number of topics and documents, C-LDA mimics document over-sampling in the Gibbs sampler using a different unit-value in the word count table for each collection. Specifically, a unit \u03b7c is chosen for each collection such that the average equivalent number of assigned words per-topic ( \u2211 d\u2208c \u03b7cNd/Kc, where Nd is the length of document d) is equal. This process both increases the topic quality (in terms of collection balance) in the resulting heldout perplexity of the model."}, {"heading": "3.2 Correlated HDP", "text": "To alleviate C-LDA\u2019s requirement that \u2203 c such that Kc = K\u2205, we introduce a variant of the model, the correlated hierarchical Dirichlet process (C-HDP), that uses a 3-level hierarchical Dirichlet process (Teh et al., 2006). The generative process for C-HDP is the same as C-LDA shown above, except that here we assume a word\u2019s topic, z, is generated by a hierarchical Dirichlet process:\nG0|\u03b3,H \u223c DP(\u03b3,H) Gc|\u03b10, G0 \u223c DP(\u03b10, G0) Gd|\u03b11, Gc \u223c DP(\u03b11, Gc)\nz|Gd \u223c Gd where G0 is a base measure for each collectionlevel Dirichlet process, and Gc are base measures of document-level Dirichlet processes in each collection (Figure 1b). Thus, documents from the\nsame collection will have similar topic distributions compared to those from other collections, and collections are allowed to have distinct sets of topics due to the use of HDP."}, {"heading": "4 Inference", "text": ""}, {"heading": "4.1 Posterior Inference in C-LDA", "text": "C-LDA can be trained using collapsed Gibbs sampling with \u03c6, \u03b8, and \u03c3 integrated out. Given the status assignments of other words, the sampling distribution for word wi is given by:\np(yi, zi|w,y\u2212i, z\u2212i, \u03b4, \u03b1, \u03b2) \u221d (N(d, zi) + \u03b1c,zi)\ufe38 \ufe37\ufe37 \ufe38\nqd\n\u00d7  N(yi, zi) + \u03b4yi N(zi) + \u2211 k \u03b4k \u00d7 N(wi, yi, zi, \u03b6) + \u03b2 N(yi, zi, \u03b6) + V \u03b2 zi \u2264 K\u2205 N(wi, zi, c) + \u03b2\nN(zi, c) + V \u03b2 zi > K \u2205\ufe38 \ufe37\ufe37 \ufe38 qw\n(1)\nwhere \u03b6 = { \u2217 yi = 0 c yi = 1\n,N(\u00b7 \u00b7 \u00b7 ) is the number of status assignments for (\u00b7 \u00b7 \u00b7 ), not including wi.\nInference in C-LDA employs two optimizations: a parallelized sampler and an efficient sampling algorithm (Algorithm 1). We use the parallel schema in (Smola and Narayanamurthy, 2010; Lu et al., 2013) which applies atomic updates to the sufficient statistics to avoid race conditions. The key idea behind the optimized sampler is the combination of alias tables and the MetropolisHastings method (MH), adapted from (Yuan et al., 2015; Li et al., 2014). Metropolis-Hastings is a Markov chain Monte Carlo method that uses a proposal distribution to approximate the true distribu-\nAlgorithm 1 Sampling in C-LDA repeat\nfor all documents {d} in parallel do for words {w} in d do\nz \u2190 CYCLEMH(p, qw, qd, z) sample y given z\nAtomic update sufficient statics Estimate \u03b1\nuntil convergence\nprocedure CYCLEMH(p, qw, qd, z) for i = 1 to N do\nif i is even then proposal q \u2190 qw else proposal q \u2190 qd sample z\u2032 \u223c ALIASTABLE(q) if RandUnif(1) < min(1, p(z\n\u2032)q(z) p(z)q(z\u2032) ) then\nz \u2190 z\u2032return z\ntion when exact sampling is difficult. In a complimentary way, Walker\u2019s alias method (2004) allows one to effectively sample from a discrete distribution by using an alias table, constructed in O(K) time, from which we can sample in O(1). Thus, reusing the samplerK times as the proposal distribution for Metropolis-Hastings yields O(1) amortized sampling time per-token.\nNotice that in Eq. 1, the sampling distribution is the product of a single document-dependent term qd and a single word-dependent term qw. After burn-in, both terms will be sparse (without the smoothing factor). It is therefore reasonable to use qd and qw as cycle proposals (Yuan et al., 2015), alternating them in each Metropolis-Hastings step. Our experiments show that the primary drawback of this method \u2014 stale sufficient statistics \u2014 does not empirically affect convergence. Our implementation uses proposal distributions qw and qd, with y marginalized out. After the MetropolisHastings steps, y is sampled to update z, to reduce the size of the alias tables, yielding even faster convergence.\nLastly, the use of an asymmetric \u03b1 allows CLDA to discover correlations between less dominant topics across collections (Wallach et al., 2009a). We use Minka\u2019s fixed-point method, with a gamma hyper-prior to optimize \u03b1c for each collection separately (Wallach, 2008). All other hyperparameters were fixed during inference."}, {"heading": "4.2 Posterior Inference in C-HDP", "text": "C-HDP uses the block sampling algorithm described in (Chen et al., 2011), which is based on\nthe Chinese restaurant process metaphor. Here, rather than tracking all assignments (as the samplers given in (Teh et al., 2006)), table indicators are used to track only the start of new tables, which allows us to adopt the same sampling framework as C-LDA. In the Chinese restaurant process, each Dirichlet process in the hierarchical structure is represented as a restaurant with an infinite number of tables, each serving the same dish. New customers can either join a table with existing customers, or start a new table. If a new table is chosen, a proxy customer will be sent to the parent restaurant to determine the dish served to that table.\nIn the block sampler, indicators are used to denote a customer creating a table (or tables) up to level u (0 as the root, 1 for collection level, and 2 for the document level), and u = \u2205 indicates no table has been created. For example, when a customer creates a table at the collection level, and the proxy customer in the collection level creates a table at the root level, u is 0. With this metaphor, let nlz be the number of customers (including their proxies) served dish z at restaurant l, and let tlz be the number of tables serving dish z at restaurant l (l = 0 for root, l = c for collection level or l = d for document level), with N0 = \u2211 z n0z and\nNc = \u2211\nz ncz . By the chain rule, the conditional probability of the state assignments for wi, given all others, is p(yi, zi, ui|w,y\u2212i,z\u2212i,u\u2212i, . . .) \u221d N(y, z) + \u03b4y N(z) + \u2211 k \u03b4k \u00d7 N(w, y, z, \u03b6) + \u03b2 N(y, z, \u03b6) + V \u03b2\n\u00d7  \u03b3\u03b10 \u03b3+N0\nu = 0\n\u03b10 \u03b3+N0\nS ncz+1 tcz+1\nS ncz tcz\nS ndz+1 tdz+1\nS ndz tdz\nn20z(tcz+1)(tdz+1)\n(n0z+1)(ncz+1)(ndz+1) u = 1\nS ncz+1 tcz\nS ncz tcz\nS ndz+1 tdz+1\nS ndz tdz\n(tdz+1)(ncz\u2212tcz+1) (ncz+1)(ndz+1)\nu = 2\n\u03b10+N1 \u03b11\nS ndz+1 tdz\nS ndz tdz\nndz\u2212tdz+1 ndz+1\nu = \u2205\nHere, Snt is the Stirling number, the ratios of which can be efficiently precomputed (Buntine and Hutter, 2010). The concentration parameters \u03b3, \u03b10, and \u03b11 can be sampled using the auxiliary variable method (Teh et al., 2006).\nNote that because conditional probability has the same separability as C-LDA (to give term qw and qd), the same sampling framework can be used with two alterations: 1) when a new topic is created or removed at the root, collection, or document level, the related alias tables must be reset, which makes the sampling slightly slower\nthan O(1), and 2) while the document alias table samples z and u simultaneously, after sampling z from the word alias table u must be sampled using tlc/nlz (Chen et al., 2011). Parallelizing C-HDP requires an additional empirical method of merging new topics between threads (Newman et al., 2009), which is outside of the scope of this work. Our implementation of both models, C-LDA and C-HDP, are open-sourced online 1."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Model Comparison", "text": "We use perplexity on held-out documents to evaluate the performance of C-LDA and C-HDP. In all experiments, the gamma prior for \u03b1 in C-LDA was set to (1, 1), and (5, 0.1), (5, 0.1), (0.1, 0.1) for \u03b3, \u03b10, \u03b11 respectively in C-HDP. In the hold-out procedure, 20% of documents were randomly selected as test data. LDA, C-LDA and ccLDA were run for 1,000 iterations and C-HDP and the TIIvariant of TPYP for 1,500 iterations (unless otherwise noted), all of which converged to a state where change in perplexity was less than 1% for ten consecutive iterations.\nPerplexity was calculated from the marginal likelihood of a held-out document p(w|\u03a6, \u03b1), estimated using the \u201cleft-to-right\u201d method (Wallach et al., 2009b). Because it is difficult to validate real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks (AlSumait et al., 2009; Wallach et al., 2009b; Kucukelbir and Blei, 2014)."}, {"heading": "5.1.1 Topic Correlation", "text": "C-LDA is unique in the amount of freedom it allows when setting the number of topics for col-\n1https://github.com/iceboal/correlated-lda\nlections. To assess the models\u2019 performances with various topic correlations in a fair setting, we generated two collections of synthetic data by following the generative process (varying the number of topics) and measured the models\u2019 perplexities against the ground truth parameters. In each experiment, two collections were generated, each with 1,000 documents containing 50 words each, over a vocabulary of 3,000. \u03b2 and \u03b4 were fixed at 0.01 and 1.0 respectively, and \u03b1 was asymmetrically defined as 1/(i+ \u221a Kc) for i \u2208 [0,Kc \u2212 1].\nCompletely shared topics The assumptions imposed by ccLDA and TPYP effectively make them a special case of our model where K\u2205 = K1 = K2 = . . .. To compare results, data was generated such that all numbers of topics were equal to K \u2208 [10, 90]. Additionally, all models were configured to use this ground truth parameter when training. Not surprisingly, ccLDA, C-LDA, and CHDP have almost the same perplexity with respect to K because their structure is the same when all topics are shared (Figure 2a).\nAsymmetric numbers of topics To explore the effect of asymmetry in the number of topics, data was generated such that one collection had K1 \u2208 [20, 60] topics while a second had a fixedK2 = 40 topics. The number of shared topics was set to K\u2205 = 20. The parameters for C-LDA and C-HDP (initial values) were set to ground truths, and, to retain a fair comparison, versions of ccLDA and TPYP were fit with both K = K1 and K = K2.\nWe find that ccLDA performs nearly as well as C-LDA and C-HDP when there is more symmetry between collection, namely when K1 \u2248 K2 (Figure 2b). TPYP, on the other hand, performs well with more topics (2 \u00d7 max(K1,K2) where the ground truth is K1 & K2). In contrast, C-LDA\nand C-HDP perform more consistently than other models across varying degrees of asymmetry.\nPartially-shared topics When collections have the same number of topics, C-LDA, C-HDP and ccLDA exhibit adequate flexibility, resulting in similar perplexities. When collections have increasingly few common topics, however, common and non-common topics from ccLDA are considerably less distinguishable than those from CLDA. To evaluate the models\u2019 abilities in such situations, data was generated for two collections having K1 = K2 = 50 topics, but with the shared number of topics K\u2205 \u2208 [5, 45]. We also set \u03b4(0) = \u03b4(1) = 5, and for comparison to ccLDA we used K = 50.\nTo measure this distinguishability, we examine the inferred \u03c3. Recall that \u03c3 indicates what percentage of a common topic is shared. When a topic is actually non-common, the value of \u03c3 should be small. We sort \u03c3k for k \u2208 [1,K] in reverse and use\n\u03c3\u0304common = 1 K\u2205 \u2211K\u2205 k=1 \u03c3k\n\u03c3\u0304non-common = 1 K\u2212K\u2205 \u2211K k=K\u2205+1 \u03c3k\n(2)\nas measures of how well common and noncommon topics were learned2. \u03c3\u0304common is the average of the K\u2205 largest \u03c3 values, and \u03c3\u0304non-common is the average of the rest. When \u03b4(0) = \u03b4(1) in the synthetic data, \u03c3 in the common portion should be 0.5, whereas it should be 0 in the non-common part. Figure 3 shows that C-LDA better distinguishes between common and non-common topics, especially whenK\u2205 is small. This allows noncommon topics to be separated from the results by examining the value of \u03c3. C-HDP has similar performance but larger \u03c3 values. In ccLDA, all topics are shared between collections which means that common and non-common topics are mixed. As expected, ccLDA performs similarly when all topics are common across collections."}, {"heading": "5.2 Semantic Coherence", "text": "Semantic coherence is a corpus-based metric of the quality of a topic, defined as the average pairwise similarity of the top n words (Newman et al., 2010a; Mimno et al., 2011). A PMI-based form of coherence, which has been found to be the best\n2TPYP is not comparable using this metric, but its hierarchical structure will cause topics to mix naturally.\nproxy human judgements of topic quality, is defined for a topic k as:\nC(k) = 2 n(n\u2212 1) n\u2211\n(wi,wj)\u2208k i<j\nlog D(wi, wj) + 1\nD(wi)D(wj)\nwhere D(\u00b7) computes the document cooccurrence. To accommodate coherence with common topics in C-LDA that have shared and collection-specific components we define mutual coherence, MC(k), as\nMC(k) = 1\nn2\nn\u2211 wi\u2208shared,\nwj\u2208collection-specific\nlog D(wi, wj) + 1\nD(wi)D(wj)\nso that for each collection, C(k) (2n words) is equal to C(k, shared) + C(k, collection-specific) + MC(k). Table 1 shows the semantic coherence of topics fit with ccLDA and C-LDA. We used a 10% sample of JSTOR due to the limited speed of ccLDA, using 50 (common) topics for ccLDA / CLDA, and 250 non-common humanities topics for C-LDA. Although these settings are different for the models, the science topics are still comparable because they both have 50 topics. We found that C-LDA provides improved coherence in nearly all situations."}, {"heading": "5.2.1 Inference Efficiency", "text": "To compare the model efficiency, we timed runs on a sample of 5,036 documents from JSTOR (introduced in the next section) with a 20% heldout and set K = K1 = K2 = 200 run on a commodity computer with four cores and 16GB of memory. Figure 4a shows the perplexity over\ntime and iterations. The inference algorithm introduces some staleness, which yields slower convergence in the first 200 iterations. This effect, however, is outweighed in both C-LDA and C-HDP by the increased sampling speed. With 8 threads, CLDA not only converges faster, but yields lower perplexity, likely due to threads introducing additional stochasticity."}, {"heading": "5.3 Performance on JSTOR", "text": "To compare our models against slower models, we sampled 2,465 documents from JSTOR, withholding 20% as testing set. We fit a model with 100 common and 50 non-common initial topics using C-HDP, which produced 272 root topics after 2,000 iterations.The perplexity scores are roughly the same when C-LDA uses the same average number of topics per collection (Figure 4b), except when numbers of topics are very asymmetric. Our model begins to outperform ccLDA after 80 topics. C-HDP did not, however, out-perform C-LDA despite the original HDP outperforming LDA. This could be do to the fact that the hierarchical structure of C-HDP is considerably different than the typical 2-level HDP. Held-out perplexity on real data provides a quantitative evaluation of our models\u2019 performance in a real-world setting. However, the goal of our models is to enable a deeper analysis of large, weakly-related corpora, which we next discuss."}, {"heading": "5.4 Qualitative Analysis", "text": "Our models are designed to enable researchers to compare collections of text in a way that is scalable and sensitive to collection-level asymmetries. To demonstrate that C-LDA can fill this role, we fit a model to the entire JSTOR sciences and humanities collections with 100 science topics and 1000 humanities topics (to reveal the less popular science-related topics in the humanities), and \u03b2 = 0.01, \u03b4 = 1.0. JSTOR includes books and journal publications in over 9 million documents across nearly 3 thousand journals. We used the journal Science to represent a collection of scientific research and 76 humanist journals to represent humanities research3. Words were lemmatized, and the most and least frequent words discarded. The final humanities collection contained 149,734 documents and the sciences collection had 160,680 documents, with a combined vocabulary of 21,513 unique words. Together, these collections typify a real-world situation where there is likely some, but not overwhelming correlation.\nThe results indicate that the sciences and humanities share several topics. Both exhibit an interest in a \u201cnon-human\u201d theme (common topic #2; Table 2). This topic is quite similar in both collections (pig and monkey for science documents; bird and gorilla for humanities documents), while their shared component forms a cohesive topic (animal,\n3The list is available at http://j.mp/humanities-txt.\nspecie, and monkey). This kind of correlation is also evident in topic #23, about physics. While the science documents clearly represent research in particle physics, it is interesting to find the topic is also represented by humanist research focused on cultural representations of science. This reflects a growing interest in science and technology studies that has gained recent traction in the humanities. Despite their differences, both collections engage with a similar theme, seen in the shared component with words like particle, energy and atom.\nThe results also indicate that while sciences and humanities documents can share themes, they often diverge in how they are discussed. For example, common topic #21 could be identified as economic or capitalist, but in the collection-specific components, the two disciplines differ in their articulatation. Science uses terms like price and market, indicating an acceptance of free-market capitalism (especially as it affects the practice of science), while the humanities, which has long been critical of free-market capitalism, uses terms like rural and community, highlighting cultural facets of modern economics. These results provide evidence about how ideas move between the sciences and humanities \u2014 a phenomenon that constitutes a growing area of research for historians (Galison, 2003; Canales, 2015). C-LDA provides empirical, measurable, and reproducible evidence of the shared research between these disciplines, as well as how concepts are articulated."}, {"heading": "6 Discussion", "text": "Our models provide a robust way to explore large and potentially weakly-related text collections without imposing assumptions about the data. Like ccLDA and TPYP, our models account for topic-word variation at the collection level. The models accommodate asymmetry in\nthe numbers of topics (set in C-LDA, fit in CHDP) and provide an efficient inference method which allows them to fit data with large values for K, which can help find correlations in less prevalent topics. Our primary contribution is our models\u2019 ability to accommodate asymmetries between arbitrary collections. JSTOR, the world\u2019s largest digital collection of humanities research, was an ideal application setting given the size, asymmetry, and comprehensiveness of the humanities collection. As we show, humanities and science research exhibit asymmetries with regard to vocabulary and topic structure \u2014 asymmetries that would be systematically overlooked using existing models. By characterizing common topics as mixtures of shared and collection-specific components, we can capture a kind of topic-level homophily, where similar themes are articulated in different ways due to word-, document-, and collection-level variation. Future work on these models could explore methods to fit non-common topics for both collections. In general, C-LDA and C-HDP can be used whenever documents are sampled from ostensibly different populations, where the nature of the difference is unknown."}, {"heading": "Acknowledgements", "text": "Thanks to David Blei for advice on applications of the model. This work contains analysis of private, or otherwise restricted data, made available to James Evans and Eamon Duede by ITHAKA (JSTOR), the opinions of whom are not represented in this paper. Jaan Altosaar acknowledges support from the Natural Sciences and Engineering Research Council of Canada. This work was supported by a grant from the Templeton Foundation to the Metaknowledge Research Network and by grant #1158803 from the National Science Foundation."}], "references": [{"title": "Topic significance ranking of LDA generative models", "author": ["Loulwah AlSumait", "Daniel Barbar\u00e1", "James Gentle", "Carlotta Domeniconi."], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 67\u201382. Springer.", "citeRegEx": "AlSumait et al\\.,? 2009", "shortCiteRegEx": "AlSumait et al\\.", "year": 2009}, {"title": "Latent Dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "Journal of Machine Learning Research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A Bayesian view of the Poisson-Dirichlet process", "author": ["Wray Buntine", "Marcus Hutter."], "venue": "arXiv preprint arXiv:1007.0296.", "citeRegEx": "Buntine and Hutter.,? 2010", "shortCiteRegEx": "Buntine and Hutter.", "year": 2010}, {"title": "The Physicist and the Philosopher: Einstein, Bergson, and the Debate that Changed our Understanding of Time", "author": ["Jimena Canales."], "venue": "Princeton University Press, Princeton, NJ.", "citeRegEx": "Canales.,? 2015", "shortCiteRegEx": "Canales.", "year": 2015}, {"title": "Sampling table configurations for the hierarchical Poisson-Dirichlet process", "author": ["Changyou Chen", "Lan Du", "Wray Buntine."], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 296\u2013 311. Springer.", "citeRegEx": "Chen et al\\.,? 2011", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Differential topic models", "author": ["Changyou Chen", "Wray Buntine", "Nan Ding", "Lexing Xie", "Lan Du."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Modeling email network content and structure", "author": ["M. Denny", "J. ben Aaron", "H. Wallach", "B. Desmarais."], "venue": "the 72nd Annual Midwest Political Science Association Conference, 2014; the Northeast Political Methodology Meeting, 2014; the 7th Annual Po-", "citeRegEx": "Denny et al\\.,? 2014", "shortCiteRegEx": "Denny et al\\.", "year": 2014}, {"title": "Poincar\u2019s Maps", "author": ["Peter Galison."], "venue": "Norton, New York, NY.", "citeRegEx": "Galison.,? 2003", "shortCiteRegEx": "Galison.", "year": 2003}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann."], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357.", "citeRegEx": "Hofmann.,? 1999", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "On the alias method for generating random variables from a discrete distribution", "author": ["Richard A Kronmal", "Arthur V Peterson Jr."], "venue": "The American Statistician, 33(4):214\u2013218.", "citeRegEx": "Kronmal and Jr.,? 1979", "shortCiteRegEx": "Kronmal and Jr.", "year": 1979}, {"title": "Profile predictive inference", "author": ["Alp Kucukelbir", "David M Blei."], "venue": "arXiv preprint arXiv:1411.0292.", "citeRegEx": "Kucukelbir and Blei.,? 2014", "shortCiteRegEx": "Kucukelbir and Blei.", "year": 2014}, {"title": "Reducing the sampling complexity of topic models", "author": ["Aaron Q Li", "Amr Ahmed", "Sujith Ravi", "Alexander J Smola."], "venue": "Proceedings of the 20th ACM SIGKDD international conference on knowledge discovery and data mining, pages 891\u2013900.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Accelerating topic model training on a single machine", "author": ["Mian Lu", "Ge Bai", "Qiong Luo", "Jie Tang", "Jiuxin Zhao."], "venue": "Web Technologies and Applications, pages 184\u2013195. Springer.", "citeRegEx": "Lu et al\\.,? 2013", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Fast generation of discrete random variables", "author": ["George Marsaglia", "Wai Wan Tsang", "Jingbo Wang."], "venue": "Journal of Statistical Software, 11:1\u20138.", "citeRegEx": "Marsaglia et al\\.,? 2004", "shortCiteRegEx": "Marsaglia et al\\.", "year": 2004}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna M Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 262\u2013", "citeRegEx": "Mimno et al\\.,? 2011", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "Distributed algorithms for topic models", "author": ["David Newman", "Arthur Asuncion", "Padhraic Smyth", "Max Welling."], "venue": "The Journal of Machine Learning Research, 10:1801\u20131828.", "citeRegEx": "Newman et al\\.,? 2009", "shortCiteRegEx": "Newman et al\\.", "year": 2009}, {"title": "Automatic evaluation of topic coherence", "author": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Newman et al\\.,? 2010a", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Evaluating topic models for digital libraries", "author": ["David Newman", "Youn Noh", "Edmund Talley", "Sarvnaz Karimi", "Timothy Baldwin."], "venue": "Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL \u201910, pages 215\u2013224. ACM.", "citeRegEx": "Newman et al\\.,? 2010b", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Cross-cultural analysis of blogs and forums with mixed-collection topic models", "author": ["Michael Paul", "Roxana Girju."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3, pages 1408\u20131417.", "citeRegEx": "Paul and Girju.,? 2009", "shortCiteRegEx": "Paul and Girju.", "year": 2009}, {"title": "An architecture for parallel topic models", "author": ["Alexander Smola", "Shravan Narayanamurthy."], "venue": "Proceedings of the VLDB Endowment, 3(1-2):703\u2013710.", "citeRegEx": "Smola and Narayanamurthy.,? 2010", "shortCiteRegEx": "Smola and Narayanamurthy.", "year": 2010}, {"title": "Hierarchical Dirichlet processes", "author": ["Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei."], "venue": "Journal of the american statistical association, 101(476).", "citeRegEx": "Teh et al\\.,? 2006", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Rethinking LDA: Why priors matter", "author": ["Hanna M Wallach", "David Mimno", "Andrew Mccallum."], "venue": "Advances in Neural Information Processing Systems, pages 1973\u20131981.", "citeRegEx": "Wallach et al\\.,? 2009a", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Evaluation methods for topic models", "author": ["Hanna M. Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1105\u20131112, New York, NY,", "citeRegEx": "Wallach et al\\.,? 2009b", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Structured topic models for language", "author": ["Hanna M Wallach."], "venue": "Unpublished doctoral dissertation, Univ. of Cambridge.", "citeRegEx": "Wallach.,? 2008", "shortCiteRegEx": "Wallach.", "year": 2008}, {"title": "Topic modeling on historical newspapers", "author": ["Tze-I Yang", "Andrew J Torget", "Rada Mihalcea."], "venue": "Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 96\u2013104.", "citeRegEx": "Yang et al\\.,? 2011", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["Limin Yao", "David Mimno", "Andrew McCallum."], "venue": "Proceedings of the 15th ACM SIGKDD international conference on knowledge discovery and data mining,", "citeRegEx": "Yao et al\\.,? 2009", "shortCiteRegEx": "Yao et al\\.", "year": 2009}, {"title": "Lightlda: Big topic models on modest computer clusters", "author": ["Jinhui Yuan", "Fei Gao", "Qirong Ho", "Wei Dai", "Jinliang Wei", "Xun Zheng", "Eric Po Xing", "Tie-Yan Liu", "Wei-Ying Ma."], "venue": "Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "Yuan et al\\.,? 2015", "shortCiteRegEx": "Yuan et al\\.", "year": 2015}, {"title": "A cross-collection mixture model for comparative text mining", "author": ["ChengXiang Zhai", "Atulya Velivelli", "Bei Yu."], "venue": "Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743\u2013748.", "citeRegEx": "Zhai et al\\.,? 2004", "shortCiteRegEx": "Zhai et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 26, "context": "To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015).", "startOffset": 164, "endOffset": 183}, {"referenceID": 8, "context": "(2004), who developed the ccMix model which extended pLSA (Hofmann, 1999).", "startOffset": 58, "endOffset": 73}, {"referenceID": 1, "context": "Later work by Paul and Girju (2009) developed ccLDA, which adopted the hierarchical Bayes framework of Latent Dirichlet Allocation or LDA (Blei et al., 2003).", "startOffset": 138, "endOffset": 157}, {"referenceID": 24, "context": "Using topic models for comparative text mining was introduced by Zhai et al. (2004), who developed the ccMix model which extended pLSA (Hofmann, 1999).", "startOffset": 65, "endOffset": 84}, {"referenceID": 7, "context": "(2004), who developed the ccMix model which extended pLSA (Hofmann, 1999). Later work by Paul and Girju (2009) developed ccLDA, which adopted the hierarchical Bayes framework of Latent Dirichlet Allocation or LDA (Blei et al.", "startOffset": 59, "endOffset": 111}, {"referenceID": 18, "context": "In situations where collections do not share all topics, the results often include junk, mixed, or sparse topics, making them difficult to interpret (Paul and Girju, 2009).", "startOffset": 149, "endOffset": 171}, {"referenceID": 18, "context": "C-LDA and C-HDP extend ccLDA (Paul and Girju, 2009) to accommodate collection-topic level asymmetries, particularly by allowing noncommon topics to appear in each collection.", "startOffset": 29, "endOffset": 51}, {"referenceID": 5, "context": "To demonstrate the effectiveness of our models, we evaluate them on synthetic data and show that they outperform related models such as ccLDA and differential topic models (Chen et al., 2014).", "startOffset": 172, "endOffset": 191}, {"referenceID": 26, "context": "To make this feasible on large datasets such as JSTOR, we employ a parallelized Metropolis-Hastings (Kronmal and Peterson Jr, 1979) and alias-table sampling framework, adapted from LightLDA (Yuan et al., 2015).", "startOffset": 190, "endOffset": 209}, {"referenceID": 6, "context": "While topic models could be fit to separate collections to make post-hoc comparisons (Denny et al., 2014; Yang et al., 2011), our goal is to account for both document-topic asymmetry and topic-word asymmetry \u201cin-model\u201d.", "startOffset": 85, "endOffset": 124}, {"referenceID": 24, "context": "While topic models could be fit to separate collections to make post-hoc comparisons (Denny et al., 2014; Yang et al., 2011), our goal is to account for both document-topic asymmetry and topic-word asymmetry \u201cin-model\u201d.", "startOffset": 85, "endOffset": 124}, {"referenceID": 20, "context": "Prioritizing in-model solutions for document-topic asymmetry has been explored elsewhere, such as in hierarchical Dirichlet processes (HDP), which use an additional level to account for collection variations in document-topic distributions (Teh et al., 2006).", "startOffset": 240, "endOffset": 258}, {"referenceID": 27, "context": "One method designed to model topic-word asymmetry is ccMix (Zhai et al., 2004), which models the generative probability of a word in topic z from collection c as a mixture of shared and collection-specific distributions \u03b8z:", "startOffset": 59, "endOffset": 78}, {"referenceID": 18, "context": "ccLDA extends ccMix to the LDA framework and adds a beta prior over \u03bbc that reduces sensitivity to input parameters (Paul and Girju, 2009).", "startOffset": 116, "endOffset": 138}, {"referenceID": 5, "context": "Another approach, differential topic models (Chen et al., 2014), is based on hierarchical Bayesian models over topic-word distributions.", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": "As (Paul and Girju, 2009) note, ccLDA cannot accommodate a topic if it is not common across collections \u2014 an assumption made by ccMix, ccLDA and the TPYP.", "startOffset": 3, "endOffset": 25}, {"referenceID": 17, "context": "In a situation where a topic is found in only one collection, it would either dominate the shared topic portion (resulting in a noisy, collection-specific portion), or it would appear as a mixed topic, revealing two sets of unrelated words (Newman et al., 2010b).", "startOffset": 240, "endOffset": 262}, {"referenceID": 25, "context": "Due to increased demand for scalable topic model implementations, there has been a proliferation of optimized methods for efficient inference, such as SparseLDA (Yao et al., 2009) and AliasLDA (Li et al.", "startOffset": 161, "endOffset": 179}, {"referenceID": 11, "context": ", 2009) and AliasLDA (Li et al., 2014).", "startOffset": 21, "endOffset": 38}, {"referenceID": 11, "context": ", 2009) and AliasLDA (Li et al., 2014). AliasLDA achieves O(Kd) complexity by using the MetropolisHastings-Walker algorithm and an alias table to sample topic-word distributions in O(1) time. Although this strategy introduces temporal staleness in the updates of sufficient statistics, the lag is overcome by more iterations, and converges significantly faster. A similar technique by Yuan et al. (2015), LightLDA, employs cycle-based Metropolis Hastings mixing with alias tables for both document-topic and topic-word distributions.", "startOffset": 22, "endOffset": 404}, {"referenceID": 21, "context": "However, we prefer the simpler, non-tree version because background topics are unnecessary when using an asymmetric \u03b1 prior (Wallach et al., 2009a).", "startOffset": 124, "endOffset": 147}, {"referenceID": 20, "context": "To alleviate C-LDA\u2019s requirement that \u2203 c such that Kc = K\u2205, we introduce a variant of the model, the correlated hierarchical Dirichlet process (C-HDP), that uses a 3-level hierarchical Dirichlet process (Teh et al., 2006).", "startOffset": 204, "endOffset": 222}, {"referenceID": 19, "context": "We use the parallel schema in (Smola and Narayanamurthy, 2010; Lu et al., 2013) which applies atomic updates to the sufficient statistics to avoid race conditions.", "startOffset": 30, "endOffset": 79}, {"referenceID": 12, "context": "We use the parallel schema in (Smola and Narayanamurthy, 2010; Lu et al., 2013) which applies atomic updates to the sufficient statistics to avoid race conditions.", "startOffset": 30, "endOffset": 79}, {"referenceID": 26, "context": "The key idea behind the optimized sampler is the combination of alias tables and the MetropolisHastings method (MH), adapted from (Yuan et al., 2015; Li et al., 2014).", "startOffset": 130, "endOffset": 166}, {"referenceID": 11, "context": "The key idea behind the optimized sampler is the combination of alias tables and the MetropolisHastings method (MH), adapted from (Yuan et al., 2015; Li et al., 2014).", "startOffset": 130, "endOffset": 166}, {"referenceID": 26, "context": "It is therefore reasonable to use qd and qw as cycle proposals (Yuan et al., 2015), alternating them in each Metropolis-Hastings step.", "startOffset": 63, "endOffset": 82}, {"referenceID": 21, "context": "Lastly, the use of an asymmetric \u03b1 allows CLDA to discover correlations between less dominant topics across collections (Wallach et al., 2009a).", "startOffset": 120, "endOffset": 143}, {"referenceID": 23, "context": "We use Minka\u2019s fixed-point method, with a gamma hyper-prior to optimize \u03b1c for each collection separately (Wallach, 2008).", "startOffset": 106, "endOffset": 121}, {"referenceID": 4, "context": "C-HDP uses the block sampling algorithm described in (Chen et al., 2011), which is based on the Chinese restaurant process metaphor.", "startOffset": 53, "endOffset": 72}, {"referenceID": 20, "context": "Here, rather than tracking all assignments (as the samplers given in (Teh et al., 2006)), table indicators are used to track only the start of new tables, which allows us to adopt the same sampling framework as C-LDA.", "startOffset": 69, "endOffset": 87}, {"referenceID": 2, "context": "Here, Sn t is the Stirling number, the ratios of which can be efficiently precomputed (Buntine and Hutter, 2010).", "startOffset": 86, "endOffset": 112}, {"referenceID": 20, "context": "The concentration parameters \u03b3, \u03b10, and \u03b11 can be sampled using the auxiliary variable method (Teh et al., 2006).", "startOffset": 94, "endOffset": 112}, {"referenceID": 4, "context": "than O(1), and 2) while the document alias table samples z and u simultaneously, after sampling z from the word alias table u must be sampled using tlc/nlz (Chen et al., 2011).", "startOffset": 156, "endOffset": 175}, {"referenceID": 15, "context": "Parallelizing C-HDP requires an additional empirical method of merging new topics between threads (Newman et al., 2009), which is outside of the scope of this work.", "startOffset": 98, "endOffset": 119}, {"referenceID": 22, "context": "Perplexity was calculated from the marginal likelihood of a held-out document p(w|\u03a6, \u03b1), estimated using the \u201cleft-to-right\u201d method (Wallach et al., 2009b).", "startOffset": 132, "endOffset": 155}, {"referenceID": 0, "context": "Because it is difficult to validate real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks (AlSumait et al., 2009; Wallach et al., 2009b; Kucukelbir and Blei, 2014).", "startOffset": 166, "endOffset": 239}, {"referenceID": 22, "context": "Because it is difficult to validate real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks (AlSumait et al., 2009; Wallach et al., 2009b; Kucukelbir and Blei, 2014).", "startOffset": 166, "endOffset": 239}, {"referenceID": 10, "context": "Because it is difficult to validate real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks (AlSumait et al., 2009; Wallach et al., 2009b; Kucukelbir and Blei, 2014).", "startOffset": 166, "endOffset": 239}, {"referenceID": 16, "context": "Semantic coherence is a corpus-based metric of the quality of a topic, defined as the average pairwise similarity of the top n words (Newman et al., 2010a; Mimno et al., 2011).", "startOffset": 133, "endOffset": 175}, {"referenceID": 14, "context": "Semantic coherence is a corpus-based metric of the quality of a topic, defined as the average pairwise similarity of the top n words (Newman et al., 2010a; Mimno et al., 2011).", "startOffset": 133, "endOffset": 175}, {"referenceID": 7, "context": "These results provide evidence about how ideas move between the sciences and humanities \u2014 a phenomenon that constitutes a growing area of research for historians (Galison, 2003; Canales, 2015).", "startOffset": 162, "endOffset": 192}, {"referenceID": 3, "context": "These results provide evidence about how ideas move between the sciences and humanities \u2014 a phenomenon that constitutes a growing area of research for historians (Galison, 2003; Canales, 2015).", "startOffset": 162, "endOffset": 192}], "year": 2015, "abstractText": "Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models. This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (CHDP). These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections. Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to fit large numbers of topics. To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015). The models are first evaluated on synthetic data, generated to simulate various collection-level asymmetries. We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR.", "creator": "LaTeX with hyperref package"}}}