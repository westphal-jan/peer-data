{"id": "1508.05051", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models", "abstract": "Neural feh\u00e9r networks 2,455 have anti-social been buchbinder shown temporale to figes improve performance mirro across a a-4p range of northwich natural - catalpa language fermanagh tasks. frolicking However, designing shelled and spender training them can ulzheimer be syringae complicated. thought Frequently, researchers resort to espalier repeated khatemi experimentation to pick optimal settings. freguesia In pizzarelli this esb paper, tablawy we richelle address the rohsh issue of polene choosing strategy the annemarie correct chanvit number of naito units paliano in stt hidden layers. nazarova We dealin introduce a anouilh method sainte-h\u00e9l\u00e8ne for automatically 6.17 adjusting network size by pruning out hidden critic units 67.26 through $ \\ ell_ {\\ judas infty, loot 1} $ and $ \\ ell_ {antiheroes 2, non-active 1} $ yitzhak regularization. We auping apply time-outs this method to rottenness language chartism modeling unfeminine and numb3rs demonstrate ryozo its ability malate to combo correctly choose the number 38.60 of manakara hidden units while maintaining perplexity. We sheltering also 14.66 include these models humberto in aghaj a machine translation langar decoder yashiro and dairylea show that nil these west-north-west smaller inconceivable neural fleitz models maintain zenit-3sl the karle significant chamlong improvements 1980-89 of scuttling their savater unpruned versions.", "histories": [["v1", "Thu, 20 Aug 2015 17:21:50 GMT  (475kb,D)", "http://arxiv.org/abs/1508.05051v1", "EMNLP 2015"]], "COMMENTS": "EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton murray", "david chiang"], "accepted": true, "id": "1508.05051"}, "pdf": {"name": "1508.05051.pdf", "metadata": {"source": "META", "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models", "authors": ["Kenton Murray", "David Chiang"], "emails": ["kmurray4@nd.edu", "dchiang@nd.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural networks have proven to be highly effective at many tasks in natural language. For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014). However, neural networks can be complicated to design and train well. Many decisions need to be made, and performance can be highly dependent on making them correctly. Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments. In this paper, we focus on the choice of the sizes of hidden layers. We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that\nthey can be removed from the network. Thus, after training with more units than necessary, a network is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use. Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity. The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes. In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements. The result is that fewer experiments are needed to obtain models that perform well and are correctly sized."}, {"heading": "2 Background", "text": "Language models are often used in natural language processing tasks involving generation of text. For instance, in machine translation, the language model helps to output fluent translations, and in speech recognition, the language model helps to disambiguate among possible utterances. Current language models are usually n-gram models, which look at the previous (n\u2212 1) words to predict the nth word in a sequence, based on (smoothed) counts of n-grams collected from training data. These models are simple but very effective in improving the performance of natural language systems. However, n-gram models suffer from some limitations, such as data sparsity and memory usage. As an alternative, researchers have begun exploring the use of neural networks for language modeling. For modeling n-grams, the most common approach is the feedforward network of Bengio et ar X iv :1\n50 8.\n05 05\n1v 1\n[ cs\n.C L\n] 2\n0 A\nug 2\n01 5\nal. (2003), shown in Figure 1. Each node represents a unit or \u201cneuron,\u201d which has a real valued activation. The units are organized into real-vector valued layers. The activations at each layer are computed as follows. (We assume n = 3; the generalization is easy.) The two preceding words, w1, w2, are mapped into lowerdimensional word embeddings,\nx1 = A:w1 x2 = A:w2\nthen passed through two hidden layers,\ny = f(B1x1 +B2x2 + b)\nz = f(Cy + c)\nwhere f is an elementwise nonlinear activation (or transfer) function. Commonly used activation functions are the hyperbolic tangent, logistic function, and rectified linear units, to name a few. Finally, the result is mapped via a softmax to an output probability distribution,\nP (wn | w1 \u00b7 \u00b7 \u00b7wn\u22121) \u221d exp([Dz+ d]wn).\nThe parameters of the model are A, B1, B2, b, C, c, D, and d, which are learned by minimizing the negative log-likelihood of the the training data using stochastic gradient descent (also known as backpropagation) or variants. Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation. In this paper, we use and extend their implementation."}, {"heading": "3 Methods", "text": "Our method is focused on the challenge of choosing the number of units in the hidden layers of a feed-forward neural network. The networks used for different tasks require different numbers of units, and the layers in a single network also require different numbers of units. Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfitting. It can also slow down computations with the network, which can be a major concern for many applications such as integrating neural language models into a machine translation decoder. Our method starts out with a large number of units in each layer and then jointly trains the network while pruning out individual units when possible. The goal is to end up with a trained network\nthat also has the optimal number of units in each layer. We do this by adding a regularizer to the objective function. For simplicity, consider a single layer without bias, y = f(Wx). Let L(W) be the negative log-likelihood of the model. Instead of minimizing L(W) alone, we want to minimize L(W) + \u03bbR(W), where R(W) is a convex regularizer. The \ufffd1 norm, R(W) = \ufffdW\ufffd1 =\ufffd\ni,j |Wij |, is a common choice for pushing parameters to zero, which can be useful for preventing overfitting and reducing model size. However, we are interested not only in reducing the number of parameters but the number of units. To do this, we need a different regularizer. We assume activation functions that satisfy f(0) = 0, such as the hyperbolic tangent or rectified linear unit (f(x) = max{0, x}). Then, if we push the incoming weights of a unit yi to zero, that is, Wij = 0 for all j (as well as the bias, if any: bi = 0), then yi = f(0) = 0 is independent of the previous layers and contributes nothing to subsequent layers. So the unit can be removed without affecting the network at all. Therefore, we need a regularizer that pushes all the incoming connection weights to a unit together towards zero. Here, we experiment with two, the \ufffd2,1 norm and the \ufffd\u221e,1 norm.1 The \ufffd2,1 norm on a ma-\n1In the notation \ufffdp,q , the subscript p corresponds to the norm over each group of parameters, and q corresponds to the norm over the group norms. Contrary to more common usage, in this paper, the groups are rows, not columns.\ntrixW is\nR(W) = \ufffd\ni\n\ufffdWi:\ufffd2 = \ufffd\ni\n \ufffd\nj\nW 2ij\n  1 2\n. (1)\n(If there are biases bi, they should be included as well.) This puts equal pressure on each row, but within each row, the larger values contribute more, and therefore there is more pressure on larger values towards zero. The \ufffd\u221e,1 norm is\nR(W) = \ufffd\ni\n\ufffdWi:\ufffd\u221e = \ufffd\ni\nmax j\n|Wij |. (2)\nAgain, this puts equal pressure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s). Figure 2 visualizes the sparsity-inducing behavior of the two regularizers on a single row. Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero."}, {"heading": "4 Optimization", "text": "However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply. The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness."}, {"heading": "4.1 Proximal gradient method", "text": "Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014). Our objective function can be split into two parts, a convex and differentiable part (L) and a\nconvex but non-differentiable part (\u03bbR). In proximal gradient descent, we alternate between improving L alone and \u03bbR alone. Let u be the parameter values from the previous iteration. We compute new parameter values w using:\nv\u2190 u\u2212 \u03b7\u2207L(u) (3)\nw\u2190 arg max w\n\ufffd 1\n2\u03b7 \ufffdw \u2212 v\ufffd2 + \u03bbR(w)\n\ufffd (4)\nand repeat until convergence. The first update is just a standard gradient descent update on L; the second is known as the proximal operator for \u03bbR and in many cases has a closed-form solution. In the rest of this section, we provide some justification for this method, and in Sections 4.2 and 4.3 we show how to compute the proximal operator for the \ufffd2 and \ufffd\u221e norms. We can think of the gradient descent update (3) on L as follows. Approximate L around u by the tangent plane,\nL\u0304(v) = L(u) +\u2207L(u)(v \u2212 u) (5)\nand move v to minimize L\u0304, but don\u2019t move it too far from u; that is, minimize\nF (v) = 1\n2\u03b7 \ufffdv \u2212 u\ufffd2 + L\u0304(v).\nSetting partial derivatives to zero, we get\n\u2202F \u2202v = 1 \u03b7 (v \u2212 u) +\u2207L(u) = 0\nv = u\u2212 \u03b7\u2207L(u).\nBy a similar strategy, we can derive the second step (4). Again we want to move w to minimize the objective function, but don\u2019t want to move it too far from u; that is, we want to minimize:\nG(w) = 1\n2\u03b7 \ufffdw \u2212 u\ufffd2 + L\u0304(w) + \u03bbR(w).\nNote that we have not approximated R by a tangent plane. We can simplify this by substituting in (3). The first term becomes\n1 2\u03b7 \ufffdw \u2212 u\ufffd2 = 1 2\u03b7 \ufffdw \u2212 v \u2212 \u03b7\u2207L(u)\ufffd2\n= 1\n2\u03b7 \ufffdw \u2212 v\ufffd2 \u2212\u2207L(u)(w \u2212 v)\n+ \u03b7\n2 \ufffd\u2207L(u)\ufffd2\nand the second term becomes\nL\u0304(w) = L(u) +\u2207L(u)(w \u2212 u) = L(u) +\u2207L(u)(w \u2212 v \u2212 \u03b7\u2207L(u)).\nThe \u2207L(u)(w \u2212 v) terms cancel out, and we can ignore terms not involving w, giving\nG(w) = 1\n2\u03b7 \ufffdw \u2212 v\ufffd2 + \u03bbR(w) + const.\nwhich is minimized by the update (4). Thus, we have split the optimization step into two easier steps: first, do the update for L (3), then do the update for \u03bbR (4). The latter can often be done exactly (without approximating R by a tangent plane). We show next how to do this for the \ufffd2 and \ufffd\u221e norms."}, {"heading": "4.2 \ufffd2 and \ufffd2,1 regularization", "text": "Since the \ufffd2,1 norm on matrices (1) is separable into the \ufffd2 norm of each row, we can treat each row separately. Thus, for simplicity, assume that we have a single row and want to minimize\nG(w) = 1\n2\u03b7 \ufffdw \u2212 v\ufffd2 + \u03bb\ufffdw\ufffd+ const.\nThe minimum is either at w = 0 (the tip of the cone) or where the partial derivatives are zero (Figure 3):\n\u2202G \u2202w = 1 \u03b7 (w \u2212 v) + \u03bb w\ufffdw\ufffd = 0.\nClearly,w and vmust have the same direction and differ only in magnitude, that is, w = \u03b1 v\ufffdv\ufffd . Substituting this into the above equation, we get the solution\n\u03b1 = \ufffdv\ufffd \u2212 \u03b7\u03bb.\nTherefore the update is\nw = \u03b1 v\n\ufffdv\ufffd \u03b1 = max(0, \ufffdv\ufffd \u2212 \u03b7\u03bb)."}, {"heading": "4.3 \ufffd\u221e and \ufffd\u221e,1 regularization", "text": "As above, since the \ufffd\u221e,1 norm on matrices (2) is separable into the \ufffd\u221e norm of each row, we can treat each row separately; thus, we want to minimize\nG(w) = 1\n2\u03b7 \ufffdw \u2212 v\ufffd2 + \u03bbmax j |xj |+ const.\nIntuitively, the solution can be characterized as: Decrease all of the maximal |xj | until the total decrease reaches \u03b7\u03bb or all the xj are zero. See Figure 4. If we pre-sort the |xj | in nonincreasing order, it\u2019s easy to see how to compute this: for \u03c1 = 1, . . . , n, see if there is a value \u03be \u2264 x\u03c1 such that decreasing all the x1, . . . , x\u03c1 to \u03be amounts to a total decrease of \u03b7\u03bb. The largest \u03c1 for which this is possible gives the correct solution. But this situation seems similar to another optimization problem, projection onto the \ufffd1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting. In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012). Intuitively, the \ufffd1 projection of v is exactly what is cut out by the \ufffd\u221e proximal operator, and vice versa (Figure 4). Duchi et al.\u2019s algorithm modified for the present problem is shown as Algorithm 1. It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value \u03be such that the total decrease is \u03b4 (line 8). If so, it recursively searches the right side; if not, the\nleft side. At the conclusion of the algorithm, \u03c1 is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) \u2013 the only difference from Duchi et al.\u2019s algorithm. This algorithm is asymptotically faster than that of Quattoni et al. (2009). They reformulate \ufffd\u221e,1 regularization as a constrained optimization problem (in which the \ufffd\u221e,1 norm is bounded by \u00b5) and provide a solution inO(n log n) time. The method shown here is simpler and faster because it can work on each row separately.\nAlgorithm 1 Linear-time algorithm for the proximal operator of the \ufffd\u221e norm. 1: procedure UPDATE(w, \u03b4) 2: lo, hi\u2190 1, n 3: s\u2190 0 4: while lo \u2264 hi do 5: select md randomly from lo, . . . , hi 6: \u03c1\u2190 PARTITION(w, lo,md, hi) 7: \u03be \u2190 1\u03c1 \ufffd s+ \ufffd\u03c1 i=lo |xi|\u2212 \u03b4 \ufffd\n8: if \u03be \u2264 |x\u03c1| then 9: s\u2190 s+\ufffd\u03c1i=lo |xi| 10: lo\u2190 \u03c1+ 1 11: else 12: hi\u2190 \u03c1\u2212 1 13: \u03c1\u2190 hi 14: \u03be \u2190 1\u03c1 (s\u2212 \u03b4) 15: for i\u2190 1, . . . , n do 16: xi \u2190 min(max(xi,\u2212\u03be), \u03be) 17: procedure PARTITION(w, lo,md, hi) 18: swap xlo and xmd 19: i\u2190 lo+ 1 20: for j \u2190 lo+ 1, . . . , hi do 21: if xj \u2265 xlo then 22: swap xi and xj 23: i\u2190 i+ 1 24: swap xlo and xi\u22121 25: return i\u2212 1"}, {"heading": "5 Experiments", "text": "We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.2 We use a vocabulary size of 100k and word embeddings with 50 dimensions. We use two hidden layers of rectified linear units (Nair and Hinton, 2010).\n2These extensions have been contributed to the NPLM project.\nWe train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5. After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens. For both corpora, we hold out a validation set of 5,000 tokens. We train each model for 10 iterations over the training data. Our experiments break down into three parts. First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings. Second, we take a closer look at how the model evolves through the training process. Finally, we explore the downstream impact of our method on a statistical phrase-based machine translation system."}, {"heading": "5.1 Evaluating perplexity and network size", "text": "We first look at the impact that the \ufffd\u221e,1 regularizer has on the perplexity of our validation set. The main results are shown in Table 1. For \u03bb \u2264 0.01, the regularizer seems to have little impact: no hidden units are pruned, and perplexity is also not affected. For \u03bb = 1, on the other hand, most hidden units are pruned \u2013 apparently too many, since perplexity is worse. But for \u03bb = 0.1, we see that we are able to prune out many hidden units: up to half of the first layer, with little impact on perplexity. We found this to be consistent across all our experiments, varying n-gram size, initial hidden layer size, and vocabulary size. Table 2 shows the same information for 5-gram models trained on the larger Gigaword AFP corpus. These numbers look very similar to those on Europarl: again \u03bb = 0.1 works best, and, counter to expectation, even the final number of units is similar. Table 3 shows the result of varying the vocabulary size: again \u03bb = 0.1 works best, and, although it is not shown in the table, we also found that the final number of units did not depend strongly on the vocabulary size. Table 4 shows results using the \ufffd2,1 norm (Europarl corpus, 5-grams, 100k vocabulary). Since this is a different regularizer, there isn\u2019t any reason to expect that \u03bb behaves the same way, and indeed, a smaller value of \u03bb seems to work best."}, {"heading": "5.2 A closer look at training", "text": "We also studied the evolution of the network over the training process to gain some insights into how the method works. The first question we want to\nanswer is whether the method is simply removing units, or converging on an optimal number of units. Figure 5 suggests that it is a little of both: if we start with too many units (900 or 1000), the method converges to the same number regardless of how many extra units there were initially. But if we start with a smaller number of units, the method still prunes away about 50 units. Next, we look at the behavior over time of different regularization strengths \u03bb. We found that not only does \u03bb = 1 prune out too many units, it does so at the very first iteration (Figure 6, above), perhaps prematurely. By contrast, the \u03bb = 0.1 run prunes out units gradually. By plotting these curves together with perplexity (Figure 6, below), we can see that the \u03bb = 0.1 run is fitting the model and pruning it at the same time, which seems preferable to fitting without any pruning (\u03bb =\n0.01) or pruning first and then fitting (\u03bb = 1). We can also visualize the weight matrix itself over time (Figure 7), for \u03bb = 0.1. It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune."}, {"heading": "5.3 Evaluating on machine translation", "text": "We also looked at the impact of our method on statistical machine translation systems. We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext. We augmented this system with neural LMs trained on the Europarl data and the Gigaword AFP data. Based on the results from the perplexity experiments, we looked at models both built with a \u03bb = 0.1 regularizer, and without regularization (\u03bb = 0). We built our system using the newscommentary dataset v8. We tuned our model using newstest13 and evaluated using newstest14. After standard cleaning and tokenization, there were 155k parallel sentences in the newscommentary dataset, and 3,000 sentences each for the tuning and test sets. Table 5 shows that the addition of a neural LM helps substantially over the baseline, with improvements of up to 2 BLEU. Using the Europarl model, the BLEU scores obtained without and with regularization were not significantly different (p \u2265 0.05), consistent with the negligible perplexity difference between these models. On the Gigaword AFP model, regularization did decrease the BLEU score by 0.3, consistent with the small perplexity increase of the regularized model. The decrease is statistically significant, but small compared with the overall benefit of adding a neural LM."}, {"heading": "6 Related Work", "text": "Researchers have been exploring the use of neural networks for language modeling for a long time. Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams. Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder. Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features. Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures. RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011). Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit. In future work, we plan on exploring how our method could improve these more complicated neural models as well.\nAutomatically limiting the size of neural networks is an old idea. The \u201cOptimal Brain Damage\u201d (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter. The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned. The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously. Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both \ufffd22 and \ufffd0 regularization. Our method develops on this idea by using a mixed norm to prune units, rather than parameters. Srivastava et al. introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations. However, at the end of training, all units are reactivated, as the goal of dropout is to reduce overfitting, not to reduce network size. Thus, dropout and our method seem to be complementary."}, {"heading": "7 Conclusion", "text": "We have presented a method for auto-sizing a neural network during training by removing units using a \ufffd\u221e,1 regularizer. This regularizer drives a unit\u2019s input weights as a group down to zero, allowing the unit to be pruned. We can thus prune units out of our network during training with minimal impact to held-out perplexity or downstream performance of a machine translation system. Our results showed empirically that the choice\nof a regularization coefficient of 0.1 was robust to initial configuration parameters of initial network size, vocabulary size, n-gram order, and training corpus. Furthermore, imposing a single regularizer on the objective function can tune all of the hidden layers of a network with one setting. This reduces the need to conduct expensive, multi-dimensional grid searches in order to determine optimal sizes. We have demonstrated the power and efficacy of this method on a feed-forward neural network for language modeling though experiments on perplexity and machine translation. However, this method is general enough that it should be applicable to other domains, both inside natural language processing and outside. As neural models become more pervasive in natural language processing, the ability to auto-size networks for fast experimentation and quick exploration will become increasingly important."}, {"heading": "Acknowledgments", "text": "We would like to thank Tomer Levinboim, Antonios Anastasopoulos, and Ashish Vaswani for their helpful discussions, as well as the reviewers for their assistance and feedback."}], "references": [{"title": "Optimization with sparsity-inducing penalties", "author": ["Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski."], "venue": "Foundations and Trends in Machine Learning, 4(1):1\u2013106.", "citeRegEx": "Bach et al\\.,? 2012", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "OxLM: A neural language modelling framework for machine translation", "author": ["Paul Baltescu", "Phil Blunsom", "Hieu Hoang."], "venue": "Prague Bulletin of Mathematical Linguistics, 102(1):81\u201392.", "citeRegEx": "Baltescu et al\\.,? 2014", "shortCiteRegEx": "Baltescu et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "Proc. ACL, pages 1370\u20131380.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer."], "venue": "J. Machine Learning Research, 10:2899\u20132934.", "citeRegEx": "Duchi and Singer.,? 2009", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Efficient projections onto the \ufffd1-ball for learning in high dimensions", "author": ["John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra."], "venue": "Proc. ICML, pages 272\u2013279.", "citeRegEx": "Duchi et al\\.,? 2008", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla", "Richard E. Howard", "Lawrence D. Jackel."], "venue": "Proc. NIPS, volume 2, pages 598\u2013 605.", "citeRegEx": "LeCun et al\\.,? 1989", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "RNNLM recurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "Jan Cernocky."], "venue": "Proc. ASRU, pages 196\u2013201.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Proc. ICML, pages 1751\u20131758.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Rectified linear units improve Restricted Boltzmann Machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proc. ICML, pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Steven J. Nowland", "Geoffrey E. Hinton."], "venue": "Neural Computation, 4:473\u2013493.", "citeRegEx": "Nowland and Hinton.,? 1992", "shortCiteRegEx": "Nowland and Hinton.", "year": 1992}, {"title": "Proximal algorithms", "author": ["Neal Parikh", "Stephen Boyd."], "venue": "Foundations and Trends in Optimization, 1(3):127\u2013239.", "citeRegEx": "Parikh and Boyd.,? 2014", "shortCiteRegEx": "Parikh and Boyd.", "year": 2014}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["Ariadna Quattoni", "Xavier Carreras", "Michael Collins", "Trevor Darrell."], "venue": "Proc. ICML, pages 857\u2013 864.", "citeRegEx": "Quattoni et al\\.,? 2009", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Sequential neural text compression", "author": ["Jurgen Schmidhuber", "Stefan Heil."], "venue": "IEEE Transactions on Neural Networks, 7:142\u2013146.", "citeRegEx": "Schmidhuber and Heil.,? 1996", "shortCiteRegEx": "Schmidhuber and Heil.", "year": 1996}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "J. Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schl\u00fcter."], "venue": "Trans. Audio, Speech, and Language, 23(3):517\u2013529.", "citeRegEx": "Sundermeyer et al\\.,? 2015", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "Proc. EMNLP, pages 1387\u20131392.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Can artificial neural networks learn language models? In Proc", "author": ["Wei Xu", "Alexander I. Rudnicky."], "venue": "International Conference on Statistical Language Processing, pages M1\u201313.", "citeRegEx": "Xu and Rudnicky.,? 2000", "shortCiteRegEx": "Xu and Rudnicky.", "year": 2000}], "referenceMentions": [{"referenceID": 16, "context": "For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).", "startOffset": 124, "endOffset": 167}, {"referenceID": 3, "context": "For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).", "startOffset": 124, "endOffset": 167}, {"referenceID": 2, "context": "Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity.", "startOffset": 37, "endOffset": 58}, {"referenceID": 16, "context": "Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Figure 1: Neural probabilistic language model (Bengio et al., 2003), adapted from Vaswani et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 2, "context": "Figure 1: Neural probabilistic language model (Bengio et al., 2003), adapted from Vaswani et al. (2013).", "startOffset": 47, "endOffset": 104}, {"referenceID": 5, "context": "The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.", "startOffset": 53, "endOffset": 97}, {"referenceID": 4, "context": "The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.", "startOffset": 53, "endOffset": 97}, {"referenceID": 11, "context": "Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014).", "startOffset": 125, "endOffset": 148}, {"referenceID": 4, "context": "In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).", "startOffset": 121, "endOffset": 164}, {"referenceID": 0, "context": "In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).", "startOffset": 121, "endOffset": 164}, {"referenceID": 3, "context": "But this situation seems similar to another optimization problem, projection onto the \ufffd1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "At the conclusion of the algorithm, \u03c1 is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) \u2013 the only difference from Duchi et al.\u2019s algorithm. This algorithm is asymptotically faster than that of Quattoni et al. (2009). They reformulate \ufffd\u221e,1 regularization as a constrained optimization problem (in which the \ufffd\u221e,1 norm is bounded by \u03bc) and provide a solution inO(n log n) time.", "startOffset": 171, "endOffset": 273}, {"referenceID": 9, "context": "We use two hidden layers of rectified linear units (Nair and Hinton, 2010).", "startOffset": 51, "endOffset": 74}, {"referenceID": 15, "context": "We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.", "startOffset": 69, "endOffset": 91}, {"referenceID": 10, "context": "Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams.", "startOffset": 0, "endOffset": 145}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE).", "startOffset": 0, "endOffset": 157}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al.", "startOffset": 0, "endOffset": 283}, {"referenceID": 1, "context": "Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder.", "startOffset": 0, "endOffset": 347}, {"referenceID": 1, "context": "Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011).", "startOffset": 154, "endOffset": 176}, {"referenceID": 6, "context": "The \u201cOptimal Brain Damage\u201d (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter.", "startOffset": 43, "endOffset": 63}, {"referenceID": 14, "context": "introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations.", "startOffset": 100, "endOffset": 125}, {"referenceID": 6, "context": "RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011). Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit.", "startOffset": 155, "endOffset": 204}, {"referenceID": 6, "context": "The \u201cOptimal Brain Damage\u201d (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter. The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned. The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously. Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both \ufffd2 and \ufffd0 regularization.", "startOffset": 44, "endOffset": 486}], "year": 2015, "abstractText": "Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through \ufffd\u221e,1 and \ufffd2,1 regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.", "creator": "LaTeX with hyperref package"}}}