{"id": "1509.01346", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Deep Broad Learning - Big Models for Big Data", "abstract": "triple-play Deep macau learning benzedrine has byrdcliffe demonstrated charan the machiguenga power of similac detailed hadnott modeling gropper of prods complex high - s600 order (multivariate) interactions in data. bentworth For nikpai some culp learning biveson tasks there is swaroopam power in smalltalk learning prejudicing models injuries that town are not only 10.5:1 Deep kuniko but inundations also 77.11 Broad. By Broad, mamikonian we shiplock mean models that incorporate kulasegaran evidence takoyaki from joubert large mastandrea numbers of thermopylae features. tetroxide This cessnas is of pramudya especial value pimpong in applications where fgd many egion different features 2346 and osid combinations of features all smilon carry small urged amounts rb4 of chemoprevention information about the ragale class. multi-tracked The moayyed most accurate viraat models will 250m integrate all that kittinger information. In 11-25 this paper, we propose koltanowski an algorithm for Deep liaqat Broad al-tawhid Learning neuveville called DBL. The inanities proposed embolisms algorithm cucaracha has recessing a fotopoulos tunable parameter $ n $, saltation that jansch specifies majungasaurus the weybourne depth luckett of the model. deusen It walks provides straightforward rainforest paths spl towards kreisau out - caddo of - core peltasts learning 84.80 for pharnabazus large data. tvp1 We demonstrate that percina DBL learns models from daymond large quantities graspop of data shouf with 115.31 accuracy hungered that is highly competitive unattributed with the state - jinyang of - the - vilanch art.", "histories": [["v1", "Fri, 4 Sep 2015 06:01:11 GMT  (3259kb,D)", "http://arxiv.org/abs/1509.01346v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nayyar a zaidi", "geoffrey i webb", "mark j carman", "francois petitjean"], "accepted": false, "id": "1509.01346"}, "pdf": {"name": "1509.01346.pdf", "metadata": {"source": "CRF", "title": "Deep Broad Learning - Big Models for Big Data", "authors": ["Nayyar A. Zaidi", "Geoffrey I. Webb", "Mark J. Carman", "Francois Petitjean"], "emails": ["nayyar.zaidi@monash.edu", "geoff.webb@monash.edu", "mark.carman@monash.edu", "francois.petitjean@monash.edu"], "sections": [{"heading": null, "text": "Keywords: Classification, Big Data, Deep Learning, Broad Learning, DiscriminativeGenerative Learning, Logistic Regression, Extended Logistic Regression"}, {"heading": "1. Introduction", "text": "The rapid growth in data quantity (Ganz and Reinsel, 2012) makes it increasingly difficult for machine learning to extract maximum value from current data stores. Most state-ofthe-art learning algorithms were developed in the context of small datasets. However, the amount of information present in big data is typically much greater than that present in small quantities of data. As a result, big data can support the creation of very detailed models that encode complex higher-order multivariate distributions, whereas, for small data, very detailed models will tend to overfit and should be avoided (Brain and Webb, 2002; Martinez et al., 2015). We highlight this phenomenon in Figure 1. We know that the error of most classifiers decreases as they are provided with more data. This can be observed in Figure 1 where the variation in error-rate of two classifiers is plotted with increasing quantities of training data on the poker-hand dataset (Frank and Asuncion, 2010). One is a low-bias high-variance learner (KDB k = 5, taking into account quintic features, (Sahami, 1996)) and the other is a low-variance high-bias learner (naive Bayes, a linear classifier). For small quantities of data, the low-variance learner achieves the lowest error. However,\nc\u00a92015 Nayyar A. Zaidi and Francois Petitjean and Geoffrey I. Webb.\nar X\niv :1\n50 9.\n01 34\n6v 1\n[ cs\n.L G\n] 4\nS ep\nas the data quantity increases, the low-bias learner comes to achieve the lower error as it can better model higher-order distributions from which the data might be sampled.\nThe capacity to model different types of interactions among variables in the data is a major determinant of a learner\u2019s bias. The greater the capacity of a learner to model differing distributions, the lower its bias will tend to be. However, many learners have limited capacity to model complex higher-order interactions.\nDeep learning1 has demonstrated some remarkable successes through its capacity to create detailed (deep) models of complex multivariate interactions in structured data (e.g., data in computer vision, speech recognition, bioinformatics, etc.). Deep learning can be characterized in several different ways. But the underlying theme is that of learning higherorder interactions among features using a cascade of many layers. This process is known as \u2018feature extraction\u2019 and can be un-supervised as it leverages the structure within the data to create new features. Higher-order features are created from lower-order features creating a hierarchical structure. We conjecture that the deeper the model the higher the order of interactions that are captured in the data and the lower the bias that the model exhibits.\nWe argue that in many domains there is value in creating models that are broad as well as deep. For example, when using web browsing history, or social network likes, or when analyzing text, it is often the case that each feature provides only extremely small amounts of information about the target class. It is only by combining very large amounts of this micro-evidence that reliable classification is possible.\nWe call a model broad if it utilizes large numbers of variables. We call a model deep and broad if it captures many complex interactions each between numerous variables. For example, typical linear classifiers such as Logistic Regression (LR) and Naive Bayes (NB) are Broad Learners, in that they utilize all variables. However, these models are not deep, as\n1. Deep neural networks, convolutional deep neural networks, deep belief networks, etc.\nthey do not directly model interactions between variables. In contrast, Logistic Regression2 with cubic features (LR3) (Langford et al., 2007) and Averaged 2-Dependence Estimators (A2DE) (Webb et al., 2011; Zaidi and Webb, 2012), both of which consider all combinations of 3 variables, are both Deep and Broad. The parameters of the former are fit discriminatively through computationally intensive gradient descent-based search, while the parameters of the latter are fit generatively using computationally efficient maximum-likelihood estimation. This efficient estimation of A2DE parameters makes it computationally wellsuitable for big data. In contrast, we argue that LR3\u2019s discriminative parameterization can more closely fit the data than A2DE, making it lower bias and hence likely to have lower error when trained on large training sets. However, the computation required to optimize the parameters for LR3 becomes computationally intensive even on moderate dimensional data.\nRecently, it has been shown that it is possible to form a hybrid generative-discriminate learner that exploits the strengths of both naive Bayes (NB) and Logistic Regression (LR) by creating a weighted variant of NB in which the weights are optimized using discriminative minimization of conditional log-likelihood (Zaidi et al., 2013, 2014). From one perspective, the resulting learner can be viewed as using weights to alleviate the attribute independence assumption of NB. From another perspective it can be seen to use the maximum likelihood parameterization of NB to pre-condition the discriminative search of LR. The result is a learner that learns models that are exactly equivalent to LR, but does so much more efficiently.\nIn this work, we show how to achieve the same result with LRn, creating a hybrid generative-discriminative learner named DBLn for categorical data that learns equivalent deep broad models to those of LRn, but does so more efficiently. We further demonstrate that the resulting models have low bias and have very low error on large quantities of data. However, to create this hybrid learner we must first create an efficient generative counterpart to LRn.\nIn short, the contributions of this work are:\n\u2022 developing an efficient generative counter-part to LRn, named Averaged n-Join Estimators (AnJE),\n\u2022 developing DBLn, a hybrid of LRn and AnJE,\n\u2022 demonstrating that DBLn has equivalent error to LRn, but is more efficient,\n\u2022 demonstrating that DBLn has low error on large data."}, {"heading": "2. Notation", "text": "We seek to assign a value y \u2208 \u2126Y = {y1, . . . yC} of the class variable Y , to a given example x = (x1, . . . , xa), where the xi are value assignments for the a attributes A = {X1, . . . , Xa}. We define (A n ) as the set of all subsets of A of size n, where each subset in the set is denoted\n2. Logistic Regression taking into account all n-level features is denoted by LRn, e.g., LR2, LR3, LR4, etc. takes into account all quadratic, cubic, quartic, etc. features.\nas \u03b1: ( A n ) = {\u03b1 \u2286 A : |\u03b1| = n}.\nWe use x\u03b1 to denote the set of values taken by attributes in the subset \u03b1 for any data object x.\nLR for categorical data learns a weight for every attribute value per class. Therefore, for LR, we denote, \u03b2y to be the weight associated with class y, and \u03b2y,i,xi to be the weight associated with attribute i taking value xi with class label y. For LR\nn, \u03b2y,\u03b1,x\u03b1 specifies the weight associated with class y and attribute subset \u03b1 taking value x\u03b1. The equivalent weights for DBLn are denoted by wy, wy,i,xi and wy,\u03b1,x\u03b1 .\nThe probability of attribute i taking value xi given class y is denoted by P(xi | y). Similarly, probability of attribute subset \u03b1, taking value x\u03b1 is denoted by P(x\u03b1|y)."}, {"heading": "3. Using generative models to precondition discriminative learning", "text": "There is a direct equivalence between a weighted NB and LR (Zaidi et al., 2013, 2014). We write LR for categorical features as:\nPLR(y |x) = exp ( \u03b2y + a\u2211 i=1 \u03b2y,i,xi \u2212 log \u2211 c\u2208\u2126Y exp ( \u03b2c + a\u2211 j=1 \u03b2c,j,xj )) (1)\nand NB as:\nPNB(y |x) = P(y) \u220fa i=1 P(xi |y)\u2211\nc\u2208\u2126Y P(c) \u220fa i=1 P(xi |c) .\nOne can add the weights in NB to alleviate the attribute independence assumption, resulting in the WANBIA-C formulation, that can be written as:\nPW(y |x) = P(y)wy\n\u220fa i=1 P(xi |y)\nwy,i,xi\u2211 c\u2208\u2126Y P(c) wc \u220fa j=1 P(xi |c) wc,j,xj\n= exp ( wy log P(y) + a\u2211 i=1 wy,i,xi log P(xi |y)\u2212\nlog \u2211 c\u2208\u2126Y exp ( wc log P(c) + a\u2211 j=1 wc,j,xj log P(xj|c) )) . (2)\nWhen conditional log likelihood (CLL) is maximized for LR and weighted NB using Equation 1 and 2 respectively, we get an equivalence such that \u03b2c \u221d wc log P(c) and \u03b2c,i,xi \u221d wc,i,xi log P(xi |c). Thus, WANBIA-C and LR generate equivalent models. While it might seem less efficient to use WANBIA-C which has twice the number of parameters of LR, the probability estimates are learned very efficiently using maximum likelihood estimation, and provide useful information about the classification task that in practice serve to effectively precondition the search for the parameterization of weights to maximize conditional log likelihood."}, {"heading": "4. Deep Broad Learner (DBL)", "text": "In order to create an efficient and effective low-bias learner, we want to perform the same trick that is used by WANBIA-C for LR with higher-order categorical features. We define LRn as:\nPLRn(y |x) = exp ( \u03b2y + \u2211 \u03b1\u2208(An) \u03b2y,\u03b1,x\u03b1 )\n\u2211 c\u2208\u2126Y exp ( \u03b2c + \u2211 \u03b1\u2217\u2208(An) \u03b2c,\u03b1\u2217,x\u03b1\u2217 ) . (3)\nWe do not include lower-order terms. For example, if n = 2 we do not include terms for \u03b2y,i,xi as well as for \u03b2y,i,xi,j,xj , because doing so does not increase the space of distinct distributions that can be modeled but does increase the number of parameters that must be optimized.\nTo precondition this model using generative learning, we need a generative model of the form\nP(y |x) = P(y)\n\u220f \u03b1\u2208(An)\nP(x\u03b1 |y)\u220f c\u2208\u2126Y ( P(c) \u220f \u03b1\u2217\u2208(An) P(x\u03b1\u2217 |c) ) (4)\n= exp ( log P(y) + \u2211 \u03b1\u2208(An) log P(x\u03b1 |y)\u2212\nlog \u2211 c\u2208\u2126Y exp ( log P(c) + \u2211\n\u03b1\u2217\u2208(An)\nlog P(x\u03b1\u2217 |c) )) . (5)\nThe only existing generative model of this form is a log-linear model, which requires computationally expensive conditional log-likelihood optimization and consequently would not be efficient to employ. It is not possible to create a Bayesian network of this form as it would require that P(xi, xj) be independent of P(xi, xk). However, we can use a variant of the AnDE (Webb et al., 2011, 2005) approach of averaging many Bayesian networks. Unlike AnDE, we cannot use the arithmetic mean, as we require a product of terms in Equation 4 rather than a sum, so we must instead use a geometric mean."}, {"heading": "4.1 Averaged n-Join Estimators (AnJE)", "text": "Let P be a partition of the attributes A. By assuming independence only between the sets of attributes A \u2208 P one obtains an n-joint estimator:\nPAnJE(x |y) = \u220f \u03b1\u2208P P(x\u03b1 |y).\nFor example, if there are four attributes X1, X2 , X3 and X4 that are partitioned into the sets {X1, X2} and {X3, X4} then by assuming conditional independence between the sets we obtain PAnJE(x1, x2, x3, x4 |y) = P(x1, x2 |y)P(x3, x4 |y). Let \u03a8An be the set of all partitions of A such that \u2200P\u2208\u03a8An \u2200\u03b1\u2208P |\u03b1| = n. For convenience we assume that |A| is a multiple of n. Let \u03a5AN be a subset of \u03a8An that includes each set of n attributes once,\n\u03a5AN \u2286 \u03a8An : \u2200\u03b1\u2208(An) \u2223\u2223{P \u2208 \u03a5AN : \u03b1 \u2208 P}\u2223\u2223 = 1.\nThe AnJE model is the geometric mean of the set of n-joint estimators for the partitions Q \u2208 \u03a5AN .\nThe AnJE estimate of conditional likelihood on a per-datum-basis can be written as:\nPAnJE(y |x) \u221d P(y)PAnJE(x|y) \u221d P(y) \u220f\n\u03b1\u2208(An)\nP(x\u03b1 |y) (n\u22121)!(a\u2212n) (a\u22121) . (6)\nThis is derived as follows. Each P is of size s = a/n. There are ( a n ) attribute-value n-tuples. Each must occur in exactly one partition, so the number of partitions must be\np =\n( a\nn\n) /s =\n(a\u2212 1)! (n\u2212 1)!(a\u2212 n)! . (7)\nThe geometric mean of all the AnJE models is thus\nPAnJE(x |y) = p \u221a\u221a\u221a\u221a \u220f \u03b1\u2208(xa) P(x\u03b1 |y),\n= \u220f \u03b1\u2208(xa) P(x\u03b1 |y)(n\u22121)!(a\u2212n)!/(a\u22121)!. (8)\nUsing Equation 6, we can write the log of P(y |x) as:\nlog PAnJE(y |x) \u221d log P(y) + (n\u22121)!(a\u2212n) (a\u22121)! \u2211 \u03b1\u2208(xa) log P(x\u03b1 |y). (9)\n4.2 DBLn\nIt can be seen that AnJE is a simple model that places the weight defined in Equation 7 on all feature subsets in the ensemble. The main advantage of this weighting scheme is that it requires no optimization, making AnJE learning extremely efficient. All that is required for training is to calculate the counts from the data. However, the disadvantage AnJE is its inability to perform any form of discriminative learning. Our proposed algorithm, DBLn uses AnJE to precondition LRn by placing weights on all probabilities in Equation 4 and learning these weights by optimizing the conditional-likelihood3. One can re-write AnJE models with this parameterization as:\nPDBL(y |x) = exp ( wy log P(y) + \u2211 \u03b1\u2208(An) wy,\u03b1,x\u03b1 log P(x\u03b1 |y)\u2212\nlog \u2211 c\u2208\u2126Y exp ( wc log P(c) + \u2211 \u03b1\u2217\u2208(An) wc,\u03b1\u2217,x\u03b1\u2217 log P(x\u03b1\u2217 |c) )) . (10)\n3. One can initialize these weights with weights in Equation 7 for faster convergence.\nNote that we can compute the likelihood and class-prior probabilities using either MLE or MAP. Therefore, we can write Equation 10 as:\nlog PDBL(y |x) =wy log \u03c0y + \u2211 \u03b1\u2208(An) wy,\u03b1,x\u03b1 log \u03b8x\u03b1 |y\u2212\nlog \u2211 c\u2208\u2126Y exp ( wc log \u03c0c + \u2211 \u03b1\u2217\u2208(An) wc,\u03b1\u2217,x\u03b1\u2217 log \u03b8x\u03b1\u2217 |c ) . (11)\nAssuming a Dirichlet prior, a MAP estimate of P(y) is \u03c0y which equals:\n#y +m/|Y| t+m ,\nwhere #y is the number of instances in the dataset with class y and t is the total number of instances, and m is the smoothing parameter. We will set m = 1 in this work. Similarly, a MAP estimate of P(x\u03b1 |y) is \u03b8x\u03b1|c which equals:\n#x\u03b1,y +m/|x\u03b1| #y +m ,\nwhere #x\u03b1,y is the number of instances in the dataset with class y and attribute values x\u03b1. DBLn computes weights by optimizing CLL. Therefore, one can compute the gradient of Equation 11 with-respect-to weights and rely on gradient descent based methods to find the optimal value of these weights. Since we do not want to be stuck in local minimums, a natural question to ask is whether the resulting objective function is convex Boyd and Vandenberghe (2008). It turns out that the objective function of DBLn is indeed convex. Roos et al. (2005) proved that an objective function of the form \u2211 x\u2208D log PB(y|x), optimized by any conditional Bayesian network model is convex if and only if the structure G of the Bayesian network B is perfect, that is, all the nodes in G are moral nodes. DBLn is a geometric mean of several sub-models where each sub-model models b anc interactions each conditioned on the class attribute. Each sub-model has a structure that is perfect. Since, the product of two convex objective function leads to a convex function, one can see that DBLn\u2019s optimization function will also lead to a convex objective function.\nLet us first calculate the gradient of Equation 11 with-respect-to weights associated with \u03c0y. We can write:\n\u2202 log P(y |x) \u2202wy\n= 1y log \u03c0y \u2212 \u03c0 wy y log \u03c0y\n\u220f \u03b1\u2208(An)\n\u03b8 wy,\u03b1,x\u03b1 x\u03b1 |y\u2211 c\u2208\u2126Y \u03c0 wc c \u220f \u03b1\u2217\u2208(An) \u03b8 wc,\u03b1\u2217,x\u03b1\u2217 x\u2217\u03b1 |c\n= (1y \u2212 P(y|x)) log \u03c0y, (12)\nwhere 1y denotes an indicator function that is 1 if derivative is taken with-respect-to class y and 0 otherwise. Computing the gradient with-respect-to weights associated with \u03b8x\u03b1|y gives:\n\u2202 log P(y |x) \u2202wy,\u03b1,x\u03b1\n= 1y1\u03b1 log \u03b8x\u03b1|y \u2212 \u03c0 wy y \u220f \u03b1\u2208(An) \u03b8 wy,\u03b1,x\u03b1 x\u03b1 |y 1\u03b1 log \u03b8x\u03b1|y\u2211\nc\u2208\u2126Y \u03c0 wc c \u220f \u03b1\u2217\u2208(An) \u03b8 wc,\u03b1\u2217,x\u03b1\u2217 x\u03b1\u2217 |c\n= (1y \u2212 P(y|x))1\u03b1 log \u03b8x\u03b1|y, (13)\nwhere 1\u03b1 and 1y denotes an indicator function that is 1 if the derivative is taken withrespect-to attribute set \u03b1 (respectively, class y) and 0 otherwise."}, {"heading": "4.3 Alternative Parameterization", "text": "Let us reparameterize DBLn such that:\n\u03b2y = wy log \u03c0y, and \u03b2y,\u03b1,x\u03b1 = wy,\u03b1,x\u03b1 log \u03b8x\u03b1 |y. (14)\nNow, we can re-write Equation 11 as:\nlog PLR(y |x) = \u03b2y + \u2211 \u03b1\u2208(An) \u03b2y,\u03b1,x\u03b1 \u2212 log \u2211 c\u2208\u2126Y exp ( \u03b2c + \u2211 \u03b1\u2217\u2208(An) \u03b2c,\u03b1\u2217,x\u03b1\u2217 ) . (15)\nIt can be seen that this leads to Equation 3. We call this parameterization LRn. Like DBLn, LRn also leads to a convex optimization problem, and, therefore, its parameters can also be optimized by simple gradient decent based algorithms. Let us compute the gradient of objective function in Equation 15 with-respect-to \u03b2y. In this case, we can write:\n\u2202 log P(y |x) \u2202\u03b2y = (1\u2212 P(y|x)). (16)\nSimilarly, computing gradient with-respect-to \u03b2\u03b1|c, we can write:\n\u2202 log P(y |x) \u2202\u03b2y,\u03b1,x\u03b1 = (1\u2212 P(y|x))1\u03b1. (17)\n4.4 Comparative analysis of DBLn and LRn\nIt can be seen that the two models are actually equivalent and each is a re-parameterization of the other. However, there are subtle distinctions between the two.. The most important distinction is the utilization of MAP or MLE probabilities in DBLn. Therefore, DBLn is a two step learning algorithm:\n\u2022 Step 1 is the optimization of log-likelihood of the data (log P(y,x)) to obtain the estimates of the prior and likelihood probabilities. One can view this step as of generative learning.\n\u2022 Step 2 is the introduction of weights on these probabilities and learning of these weights by maximizing CLL (P(y |x)) objective function. This step can be interpreted as discriminative learning.\nDBLn employs generative-discriminative learning as opposed to only discriminative learning by LRn.\nOne can expect a similar bias-variance profile and a very similar classification performance as both models will converge to a similar point in the optimization space, the only difference in the final parameterization being due to recursive descent being terminated before absolute optimization. However, the rate of convergence of the two models can be\nvery different. Zaidi et al. (2014) show that for NB, such DBLn style parameterization with generative-discriminative learning can greatly speed-up convergence relative to only discriminative training. Note, discriminative training with NB as the graphical model is vanilla LR. We expect to see the same trend in the convergence performance of DBLn and LRn.\nAnother distinction between the two models becomes explicit if a regularization penalty is added to the objective function. One can see that in case of DBLn, optimizing parameters towards 1 will effectively pull parameters back towards the generative training estimates. For smaller datasets, one can expect to obtain better performance by using a large regularization parameter and pulling estimates back towards 1. However, one cannot do this for LRn. Therefore, DBLn models can very elegantly combine generative discriminative parameters.\nAn analysis of the gradient of DBLn in Equation 12 and 13 and that of LRn in Equation 16 and 17 also reveals an interesting comparison. We can write DBLn\u2019s gradients in terms of LRn\u2019s gradient as follows:\n\u2202 log P(y |x) \u2202wy = \u2202 log P(y |x) \u2202\u03b2y log \u03c0y, \u2202 log P(y |x) \u2202wy,\u03b1,x\u03b1 = \u2202 log P(y |x) \u2202\u03b2y,\u03b1,x\u03b1 log \u03b8x\u03b1|y.\nIt can be seen that DBLn has the effect of re-scaling LRn\u2019s gradient by the log of the conditional probabilities. We conjecture that such re-scaling has the effect of pre-conditioning the parameter space and, therefore, will lead to faster convergence."}, {"heading": "5. Related Work", "text": "Averaged n-Dependent Estimators (AnDE) is the inspiration for AnJE. An AnDE model is the arithmetic mean of all Bayesian Network Classifiers in each of which all attributes depend on the class and the some n attributes. A simple depiction of A1DE in graphical form in shown in Figure 2. There are ( a n ) possible combination of attributes that can be\nused as parents, producing ( a n ) sub-models which are combined by averaging.\nAnDE and AnJE both use simple generative learning, merely the counting the relevant sufficient statistics from the data. Second, both have only one tweaking parameter: n \u2013 that controls the bias-variance trade-off. Higher values of n leads to low bias and high variance and vice-versa.\nIt is important not to confuse the equivalence (in terms of the level of interactions they model) of AnJE and AnDE models. That is, the following holds:\nf(A2JE) = f(A1DE),\nf(A3JE) = f(A2DE),\n... = ...\nf(AnJE) = f(A(n-1)DE),\nwhere f(.) is a function that returns the number of interactions that the algorithm models. Thus, an AnJE model uses the same core statistics as an A(n-1)DE model. At training\ntime, AnJE and A(n-1)DE must learn the same information from the data. However, at classification time, each of these statistics is accessed once by AnJE and n times by A(n1)DE, making AnJE more efficient. However, as we will show, it turns out that AnJE\u2019s use of the geometric mean results in a more biased estimator than than the arithmetic mean used by AnDE. As a result, in practice, an AnJE model is less accurate than the equivalent AnDE model.\nHowever, due to the use of arithmetic mean by AnDE, its weighted version would be much more difficult to optimize than AnJE, as transformed to log space it does not admit to a simple linear model.\nA work relevant to DBLn is that of Greiner et al. (2004); Greiner and Zhou (2002). The proposed technique in these papers named ELR has a number of similar traits with DBLn. For example, the parameters associated with a Bayesian network classifier (naive Bayes and TAN) are learned by optimizing the CLL. Both ELR and DBLn can be viewed as feature engineering frameworks. An ELR (let us say with TAN structure) model is a subset of DBL2 models. The comparison of DBLn with ELR is not the goal of this work. But in our preliminary results, DBLn produce models of much lower bias that ELR (TAN). Modelling higher-order interactions is also an issue with ELR. One could learn a Bayesian network structure and create features based on that and then use ELR. But several restrictions needs to be imposed on the structure, that is, it has to fulfill the property of perfectness, to make sure that it leads to a convex optimization problem. With DBLn, as we discussed in Section 4.2, there are no restrictions. Need less to say, ELR is neither broad nor deep. Some related ideas to ELR are also explored in Pernkopf and Bilmes (2005); Pernkopf and Wohlmayr (2009); Su et al. (2008).\nSeveral"}, {"heading": "6. Experiments", "text": "In this section, we compare and analyze the performance of our proposed algorithms and related methods on 77 natural domains from the UCI repository of machine learning (Frank and Asuncion, 2010). The experiments are conducted on the datasets described in Table 1.\nThere are a total of 77 datasets, 40 datasets with less than 1000 instances, 21 datasets with instances between 1000 and 10000, and 16 datasets with more than 10000 instances. There are 8 datasets with over 100000 instances. These datasets are shown in bold font in Table 1.\nEach algorithm is tested on each dataset using 5 rounds of 2-fold cross validation4. We compare four different metrics, i.e., 0-1 Loss, RMSE, Bias and Variance5. We report Win-Draw-Loss (W-D-L) results when comparing the 0-1 Loss, RMSE, bias and variance of two models. A two-tail binomial sign test is used to determine the significance of the results. Results are considered significant if p \u2264 0.05.\nThe datasets in Table 1 are divided into two categories. We call the following datasets Big \u2013 KDDCup, Poker-hand, USCensus1990, Covertype, MITFaceSetB, MITFaceSetA, Census-income, Localization. All remaining datasets are denoted as Little in the results. Due to their size, experiments for most of the Big datasets had to be performed in a heterogeneous environment (grid computing) for which CPU wall-clock times are not commensurable. In consequence, when comparing classification and training time, the following 9 datasets constitutes Big category \u2013 Localization, Connect-4, Shuttle, Adult, Letter-recog, Magic, Nursery, Sign, Pendigits.\n4. Exception is MITFaceSetA, MITFaceSetB and Kddcup where results are reported with 2 rounds of 2-fold cross validation. 5. As discussed in Section 1, the reason for performing bias/variance estimation is that it provides insights into how the learning algorithm will perform with varying amount of data. We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002).\nWhen comparing average results across Little and Big datasets, we normalize the results with respect to DBL2 and present a geometric mean.\nNumeric attributes are discretized by using the Minimum Description Length (MDL) discretization method (Fayyad and Irani, 1992). A missing value is treated as a separate attribute value and taken into account exactly like other values.\nWe employed L-BFGS quasi-Newton methods (Zhu et al., 1997) for solving the optimization6.\nWe used a Random Forest that is an ensemble of 100 decision trees Breiman (2001).\nBoth DBLn and LRn are L2 regularized. The regularization constant C is not tuned and is set to 10\u22122 for all experiments.\nThe detailed 0-1 Loss and RMSE results on Big datasets are also given in Appendix A.\n6.1 DBLn vs. AnJE\nA W-D-L comparison of the 0-1 Loss, RMSE, bias and variance of DBLn and AnJE on Little datasets is shown in Table 2. We compare DBL2 with A2JE and DBL3 with A3JE only. It can be seen that DBLn has significantly lower bias but significantly higher variance. The 0-1 Loss and RMSE results are not in favour of any algorithm. However, on Big datasets, DBLn wins on 7 out of 8 datasets in terms of both RMSE and 0-1 Loss. The results are not significant since p value of 0.070 is greater than our set threshold of 0.05. One can infer that DBLn successfully reduces the bias of AnJE, at the expense of increasing its variance.\nNormalized 0-1 Loss and RMSE results for both models are shown in Figure 3. It can be seen that DBLn has a lower averaged 0-1 Loss and RMSE than AnJE. This difference is substantial when comparing on Big datasets. The training and classification time of AnJE is, however, substantially lower than DBLn as can be seen from Figure 4. This is to be expected as DBLn adds discriminative training to AnJE and uses twice the number of parameters at classification time.\n6. The original L-BFGS implementation of (Byrd et al., 1995) from http://users.eecs.northwestern. edu/~nocedal/lbfgsb.html is used.\n6.2 DBLn vs. AnDE\nA W-D-L comparison for 0-1 Loss, RMSE, bias and variance results of the two DBLn models relative to the corresponding AnDE models are presented in Table 3. We compare DBL2 with A1DE and DBL3 with A2DE only. It can be seen that DBLn has significantly lower bias and significantly higher variance variance than AnDE models. Recently, AnDE models have been proposed as a fast and effective Bayesian classifiers when learning from large quantities of data (Zaidi and Webb, 2012). These bias-variance results make DBLn a suitable alternative to AnDE when dealing with big data. The 0-1 Loss and RMSE results (with exception of RMSE comparison of DBL3 vs. A2DE) are similar.\nNormalized 0-1 Loss and RMSE are shown in Figure 5. It can be seen that the DBLn\nmodels have lower 0-1 Loss and RMSE than the corresponding AnDE models.\nA comparison of the training time of DBLn and AnDE is given in Figure 6. As expected, due to its additional discriminative learning, DBLn requires substantially more training time than AnDE. However, AnDE does not share such a consistent advantage with respect to classification time, the relativities depending on the dimensionality of the data. For highdimensional data the large number of permutations of attributes that AnDE must consider results in greater computation.\n6.3 DBLn vs. LRn\nIn this section, we will compare the two DBLn models with their equivalent LRn models. As discussed before, we expect to see similar bias-variance profile and a similar classification performance as the two models are re-parameterization of each other.\nWe compare the two parameterizations in terms of the scatter of their 0-1 Loss and RMSE values on Little datasets in Figure 7, 9 respectively, and on Big datasets in Figure 8, 10 respectively. It can be seen that the two parameterizations (with an exception of one dataset, that is: wall-following) have a similar spread of 0-1 Loss and RMSE values for both n = 2 and n = 3.\nThe comparative scatter of the number of iterations each parameterization takes to converge is shown in Figure 11 and 12 for Little and Big datasets respectively. It can be\nseen that the number of iterations for DBLn are far fewer than LRn. With a similar spread of 0-1 Loss and RMSE values, it is very encouraging to see that DBLn converges in far fewer iterations. The number of iterations to converge plays a major part in determining an algorithm\u2019s training time. The training time of the two parameterizations is shown in Figure 13 and 14 for Little and Big datasets, respectively. It can be seen that DBLn models are much faster than equivalent LRn models.\nA comparison of rate of convergence of Negative-Log-Likelihood (NLL) of DBL2 and LR2 parameterization on some sample datasets is shown in Figure 15. It can be seen that, DBL2 has a steeper curve, asymptoting to its global minimum much faster. For example, on almost all datasets, one can see that DBL2 follows a steeper, hence more desirable, path toward convergence. This is extremely advantageous when learning from very few iterations (for example, when learning using Stochastic Gradient Descent based optimization) and, therefore, is a desirable property for scalable learning. A similar trend can be seen in Figure 16 for DBL3 and LR3.\nFinally, let us present some comparison results about the speed of convergence of DBLn vs. LRn as we increase n. In Figure 17, we compare the convergence for n = 1, n = 2\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\n10 4\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 4\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5 Adult\nDBL 2 LR 2"}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2 10 3 10 4 10 5\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 4\n-10\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n8 Census-income\nDBL 2 LR2\nNo. of Iterations\n10 0 10 1 10 2 10 3 10 4\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 5\n-6\n-5\n-4\n-3\n-2\n-1\n0 Covtype\nDBL 2 LR2\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 4\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0 Letter-recog\nDBL 2 LR 2\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\n10 4\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 4\n-18\n-16\n-14\n-12\n-10\n-8\n-6\n-4\n-2\n0 Localization\nDBL 2 LR 2\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\n10 4\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-5000\n-4000\n-3000\n-2000\n-1000\n0\n1000\n2000\n3000\n4000 Magic\nDBL 2 LR 2\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-12000\n-10000\n-8000\n-6000\n-4000\n-2000\n0 Nursery\nDBL 2 LR 2"}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-7000\n-6000\n-5000\n-4000\n-3000\n-2000\n-1000\n0 Optdigits\nDBL 2 LR2\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-14000\n-12000\n-10000\n-8000\n-6000\n-4000\n-2000\n0 Pendigits\nDBL 2 LR 2\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-4500\n-4000\n-3500\n-3000\n-2500\n-2000\n-1500\n-1000\n-500\n0 Page-blocks\nDBL 2 LR 2\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 5\n-12\n-10\n-8\n-6\n-4\n-2\n0 Poker-hand\nDBL 2 LR 2\nNo. of Iterations\n10 0 10 1 10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-6000\n-5000\n-4000\n-3000\n-2000\n-1000\n0 Satellite\nDBL 2 LR2"}, {"heading": "No. of Iterations", "text": "and n = 3 on the sample Localization dataset. It can be seen that the improvement that DBLn provides over LRn gets better as we go to deeper structures, i.e., as n becomes larger. The similar behaviour was observed for several datasets and, although studying rates\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\n10 4\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 4\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n2 Adult\nDBL 3 LR 3"}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 5\n-1\n0\n1\n2\n3\n4 Census-income\nDBL 3 LR3\nNo. of Iterations\n10 0 10 1 10 2 10 3\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 5\n-6\n-5\n-4\n-3\n-2\n-1\n0 Covtype\nDBL 3 LR3\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 4\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0 Letter-recog\nDBL 3 LR 3\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 5\n-2\n-1.5\n-1\n-0.5\n0 Localization\nDBL 3 LR 3\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\n10 4\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-6000\n-4000\n-2000\n0\n2000\n4000\n6000 Magic\nDBL 3 LR 3\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-12000\n-10000\n-8000\n-6000\n-4000\n-2000\n0 Nursery\nDBL 3 LR 3"}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-7000\n-6000\n-5000\n-4000\n-3000\n-2000\n-1000\n0 Optdigits\nDBL 3 LR3\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-14000\n-12000\n-10000\n-8000\n-6000\n-4000\n-2000\n0 Pendigits\nDBL 3 LR 3\nNo. of Iterations\n10 0\n10 1\n10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-4500\n-4000\n-3500\n-3000\n-2500\n-2000\n-1500\n-1000\n-500\n0 Page-blocks\nDBL 3 LR 3\nNo. of Iterations\n10 0\n10 1\n10 2\n10 3\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n# 10 5\n-12\n-10\n-8\n-6\n-4\n-2\n0 Poker-hand\nDBL 3 LR 3\nNo. of Iterations\n10 0 10 1 10 2\nN e\ng a\nti v\ne L\no g\n-L ik\ne li\nh o\no d\n-6000\n-5000\n-4000\n-3000\n-2000\n-1000\n0 Satellite\nDBL 3 LR3"}, {"heading": "No. of Iterations", "text": "of convergence is a complicated matter and is outside the scope of this work, we anticipate this phenomenon to be an interesting venue of investigation for future work."}, {"heading": "No. of Iterations", "text": "6.4 DBLn vs. Random Forest\nThe two DBLn models are compared in terms of W-D-L of 0-1 Loss, RMSE, bias and variance with Random Forest in Table 4. On Little datasets, it can be seen that DBLn has significantly lower bias than RF. The variance of DBL3 is significantly higher than RF, whereas, difference in the variance is not significant for DBL2 and RF. 0-1 Loss results of DBLn and RF are similar. However, RF has better RMSE results than DBLn on Little datasets. On Big datasets, DBLn wins on majority of datasets in terms of 0-1 Loss and RMSE.\nThe averaged 0-1 Loss and RMSE results are given in Figure 18. It can be seen that DBL2, DBL3 and RF have similar 0-1 Loss and RMSE across Little datasets. However, on Big datasets, the lower bias of DBLn results in much lower error than RF in terms of both 0-1 Loss and RMSE. These averaged results also corroborate with the W-D-L results in Table 4, showing DBLn to be a less biased model than RF.\nThe comparison of training and classification time of DBLn and RF is given in Figure 19. It can be seen that DBLn models are worst than RF in terms of the training time but better in terms of classification time."}, {"heading": "7. Conclusion and Future Work", "text": "We have presented an algorithm for deep broad learning. DBL consists of parameters that are learned using both generative and discriminative training. To obtain the generative parameterization for DB, we first developed AnJE, a generative counter-part of higherorder logistic regression. We showed that DBLn and LRn learn equivalent models, but that\nDBLn is able to exploit the information gained generatively to effectively precondition the optimization process. DBLn converges in fewer iterations, leading to its global minimum much more rapidly, resulting in faster training time. We also compared DBLn with the equivalent AnJE and AnDE models and showed that DBLn has lower bias than both AnJE and AnDE models. We compared DBLn with state of the art classifier Random Forest and showed that DBLn models are indeed lower biased than RF and on bigger datasets DBLn often obtains lower 0-1 loss than RF.\nThere are a number of exciting new directions for future work.\n\u2022 We have showed that DBLn is a low bias classifier with minimal tuning parameters and has the ability to handle multiple classes. The obvious extension is to make it out-of-core. We argue that DBLn is greatly suited for stochastic gradient descent based methods as it can converge to global minimum very quickly.\n\u2022 It may be desirable to utilize a hierarchical DBL, such that hDBLn = {DBL1 . . .DBLn}, incorporating all the parameters up till n. This may be useful for smoothing the parameters. For example, if a certain interaction does not occur in the training data, at classification time one can resort to lower values of n.\n\u2022 In this work, we have constrained the values of n to two and three. Scaling-up DBLn to higher values of n is greatly desirable. One can exploit the fact that many inter-\nactions at higher values of n will not occur in the data and hence can develop sparse implementations of DBLn models.\n\u2022 Exploring other objective functions such as Mean-Squared-Error or Hinge Loss can result in improving the performance and has been left as a future work.\n\u2022 The preliminary version of DBL that we have developed is restricted to categorical data and hence requires that numeric data be discretized. While our results show that this is often highly competitive with random forest using local cut-points, on some datasets it is not. In consequence, there is much scope for investigation of deep broad techniques for numeric data.\n\u2022 DBL presents a credible path towards deep broad learning for big data. We have demonstrated very competitive error on big data and expect future refinements to deliver even more efficient and effective outcomes."}, {"heading": "8. Code and Datasets", "text": "Code with running instructions can be download from https://www.dropbox.com/sh/ iw33mgcku9m2quc/AABXwYewVtm0mVE6KoyMPEVFa?dl=0."}, {"heading": "9. Acknowledgments", "text": "This research has been supported by the Australian Research Council (ARC) under grants DP140100087, DP120100553, DP140100087 and Asian Office of Aerospace Research and Development, Air Force Office of Scientific Research under contracts FA2386-12-1-4030, FA2386-15-1-4017 and FA2386-15-1-4007."}, {"heading": "Appendix A. Detailed Results", "text": "In this appendix, we compare the 0-1 Loss and RMSE results of DBLn, AnDE and RF. The goal here is to assess the performance of each model on Big datasets. Therefore, results on 8 big datasets are reported only in Table 5 and 6 for 0-1 Loss and RMSE respectively. We also compare results with AnJE. Note A1JE is naive Bayes. Also DBL1 results are also compared. Note, DBL1 is WANBIA-C (Zaidi et al., 2013).\nThe best results are shown in bold font."}], "references": [{"title": "Convex Optimization", "author": ["S Boyd", "L Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2008\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2008}, {"title": "Structural extension to logistic regression: Discriminative", "author": ["ics.uci.edu/ml. J. Ganz", "D. Reinsel"], "venue": "The Digital Universe Study,", "citeRegEx": "Ganz and Reinsel.,? \\Q2012\\E", "shortCiteRegEx": "Ganz and Reinsel.", "year": 2012}, {"title": "Discriminative versus generative parameter and structure learning of bayesian network classifiers", "author": ["F. Pernkopf", "J. Bilmes"], "venue": "In ICML,", "citeRegEx": "Pernkopf and Bilmes.,? \\Q2005\\E", "shortCiteRegEx": "Pernkopf and Bilmes.", "year": 2005}, {"title": "On discriminative parameter learning of bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr"], "venue": "In ECML PKDD,", "citeRegEx": "Pernkopf and Wohlmayr.,? \\Q2009\\E", "shortCiteRegEx": "Pernkopf and Wohlmayr.", "year": 2009}, {"title": "On discriminative Bayesian network classifiers and logistic regression", "author": ["T Roos", "H Wettig", "P Gr\u00fcnwald", "P Myllym\u00e4ki", "H Tirri"], "venue": "Machine Learning,", "citeRegEx": "Roos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roos et al\\.", "year": 2005}, {"title": "Learning limited dependence Bayesian classifiers", "author": ["M Sahami"], "venue": "In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Sahami.,? \\Q1996\\E", "shortCiteRegEx": "Sahami.", "year": 1996}, {"title": "Discriminative parameter learning for bayesian networks", "author": ["J. Su", "H. Zhang", "C. Ling", "S. Matwin"], "venue": "In ICML,", "citeRegEx": "Su et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Su et al\\.", "year": 2008}, {"title": "Not so naive Bayes: Averaged one-dependence estimators", "author": ["G I. Webb", "J Boughton", "Z Wang"], "venue": "Machine Learning,", "citeRegEx": "Webb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2005}, {"title": "Learning by extrapolation from marginal to full-multivariate probability distributions: decreasingly naive Bayesian classification", "author": ["Geoffrey I. Webb", "Janice Boughton", "Fei Zheng", "Kai Ming Ting", "Houssam Salem"], "venue": "Machine Learning,", "citeRegEx": "Webb et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2011}, {"title": "Fast and efficient single pass Bayesian learning", "author": ["N.A. Zaidi", "G.I. Webb"], "venue": "In Proceedings of the 17th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD),", "citeRegEx": "Zaidi and Webb.,? \\Q2012\\E", "shortCiteRegEx": "Zaidi and Webb.", "year": 2012}, {"title": "Alleviating naive Bayes attribute independence assumption by attribute weighting", "author": ["N.A. Zaidi", "J. Cerquides", "M. J Carman", "G.I. Webb"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zaidi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zaidi et al\\.", "year": 2013}, {"title": "Naive-bayes inspired effective pre-conditioners for speeding-up logistic regression", "author": ["N.A. Zaidi", "M. J Carman", "J. Cerquides", "G.I. Webb"], "venue": "In IEEE International Conference on Data Mining,", "citeRegEx": "Zaidi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaidi et al\\.", "year": 2014}, {"title": "LBFGSB, fortran routines for large scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "J. Nocedal"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "Introduction The rapid growth in data quantity (Ganz and Reinsel, 2012) makes it increasingly difficult for machine learning to extract maximum value from current data stores.", "startOffset": 47, "endOffset": 71}, {"referenceID": 5, "context": "One is a low-bias high-variance learner (KDB k = 5, taking into account quintic features, (Sahami, 1996)) and the other is a low-variance high-bias learner (naive Bayes, a linear classifier).", "startOffset": 90, "endOffset": 104}, {"referenceID": 8, "context": ", 2007) and Averaged 2-Dependence Estimators (A2DE) (Webb et al., 2011; Zaidi and Webb, 2012), both of which consider all combinations of 3 variables, are both Deep and Broad.", "startOffset": 52, "endOffset": 93}, {"referenceID": 9, "context": ", 2007) and Averaged 2-Dependence Estimators (A2DE) (Webb et al., 2011; Zaidi and Webb, 2012), both of which consider all combinations of 3 variables, are both Deep and Broad.", "startOffset": 52, "endOffset": 93}, {"referenceID": 0, "context": "Since we do not want to be stuck in local minimums, a natural question to ask is whether the resulting objective function is convex Boyd and Vandenberghe (2008). It turns out that the objective function of DBL is indeed convex.", "startOffset": 132, "endOffset": 161}, {"referenceID": 0, "context": "Since we do not want to be stuck in local minimums, a natural question to ask is whether the resulting objective function is convex Boyd and Vandenberghe (2008). It turns out that the objective function of DBL is indeed convex. Roos et al. (2005) proved that an objective function of the form \u2211 x\u2208D log PB(y|x), optimized by any conditional Bayesian network model is convex if and only if the structure G of the Bayesian network B is perfect, that is, all the nodes in G are moral nodes.", "startOffset": 132, "endOffset": 247}, {"referenceID": 10, "context": "Zaidi et al. (2014) show that for NB, such DBL style parameterization with generative-discriminative learning can greatly speed-up convergence relative to only discriminative training.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Some related ideas to ELR are also explored in Pernkopf and Bilmes (2005); Pernkopf and Wohlmayr (2009); Su et al.", "startOffset": 47, "endOffset": 74}, {"referenceID": 2, "context": "Some related ideas to ELR are also explored in Pernkopf and Bilmes (2005); Pernkopf and Wohlmayr (2009); Su et al.", "startOffset": 47, "endOffset": 104}, {"referenceID": 2, "context": "Some related ideas to ELR are also explored in Pernkopf and Bilmes (2005); Pernkopf and Wohlmayr (2009); Su et al. (2008). Several", "startOffset": 47, "endOffset": 122}, {"referenceID": 12, "context": "We employed L-BFGS quasi-Newton methods (Zhu et al., 1997) for solving the optimization6.", "startOffset": 40, "endOffset": 58}, {"referenceID": 12, "context": "We employed L-BFGS quasi-Newton methods (Zhu et al., 1997) for solving the optimization6. We used a Random Forest that is an ensemble of 100 decision trees Breiman (2001). Both DBL and LR are L2 regularized.", "startOffset": 41, "endOffset": 171}, {"referenceID": 9, "context": "Recently, AnDE models have been proposed as a fast and effective Bayesian classifiers when learning from large quantities of data (Zaidi and Webb, 2012).", "startOffset": 130, "endOffset": 152}], "year": 2015, "abstractText": "Deep learning has demonstrated the power of detailed modeling of complex high-order (multivariate) interactions in data. For some learning tasks there is power in learning models that are not only Deep but also Broad. By Broad, we mean models that incorporate evidence from large numbers of features. This is of especial value in applications where many different features and combinations of features all carry small amounts of information about the class. The most accurate models will integrate all that information. In this paper, we propose an algorithm for Deep Broad Learning called DBL. The proposed algorithm has a tunable parameter n, that specifies the depth of the model. It provides straightforward paths towards out-of-core learning for large data. We demonstrate that DBL learns models from large quantities of data with accuracy that is highly competitive with the state-ofthe-art.", "creator": "LaTeX with hyperref package"}}}