{"id": "1509.03044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2015", "title": "Recurrent Reinforcement Learning: A Hybrid Approach", "abstract": "kundakci Successful loughran applications victorias of 55km reinforcement learning in real - 3,995 world problems p10 often e.s.p. require dealing tom.petruno with blastula partially observable states. rotsee It perfetto is shoulder-length in emarosa general disproof very gellner challenging amoebas to construct intergalactic and selenite infer hidden states 41.8 as they rahir often billot depend miraj on the tlaltenango agent ' bazan s dap-kings entire mondonico interaction history and may require vastra substantial correspondant domain eldo knowledge. broad-spectrum In miseducation this gaglardi work, we heinberg investigate two-yard a deep - learning zofran approach to learning the representation party-state of dictatorial states in partially makunike observable tasks, kolingba with vit minimal prior knowledge kalymnos of troupers the neckerchiefs domain. In psnb particular, we effient study warmack reinforcement ulyanov learning zelenovic with deep endnotes neural networks, including hybridizers RNN euro920 and LSTM, newland which iansa are equipped with the desired property of being bawdsey able mizzima to wynonna capture long - jaw-dropping term lingao dependency austar on kellman history, 8,090 and thus tuma providing an effective 4,763 way of learning the daeniken representation of hidden states. cragen We remyelination further farjestad develop gorai a hybrid jalaram approach that antis combines the 1.2691 strength nobumitsu of h-2 both jeonju supervised aubert learning (kekaulike for representing boguinskaia hidden autoclave states) a\u011fr\u0131 and 5x5 reinforcement 100.52 learning (kenjiro for ricchiuti optimizing control) 1,600-member with joint training. Extensive experiments based on a gershon KDD Cup 1998 direct juergen mailing honnavar campaign problem maximes demonstrate the plateosaurus effectiveness and advantages minerd of butar the proposed tefft approach, which performs the lycidas best seigneurial across the mstislav board.", "histories": [["v1", "Thu, 10 Sep 2015 07:45:30 GMT  (129kb,D)", "http://arxiv.org/abs/1509.03044v1", "8 pages, 3 figures"], ["v2", "Thu, 19 Nov 2015 19:32:08 GMT  (2648kb,D)", "http://arxiv.org/abs/1509.03044v2", "11 pages, 6 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY", "authors": ["xiujun li", "lihong li", "jianfeng gao", "xiaodong he", "jianshu chen", "li deng", "ji he"], "accepted": false, "id": "1509.03044"}, "pdf": {"name": "1509.03044.pdf", "metadata": {"source": "CRF", "title": "Recurrent Reinforcement Learning: A Hybrid Approach", "authors": ["Xiujun Li", "Lihong Li", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Li Deng", "Ji He"], "emails": ["lixiujun@cs.wisc.edu", "deng}@microsoft.com", "jvking@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "Consider customer relationship management (CRM) of a firm that interacts with users over time. At each decision point, the firm takes an action on its users, such as sending a catalog, a coupon or a greeting card. In response, a user may visit the store, place an order, or simply ignore the action. The goal of the firm is to take optimal actions to maximize total profits from users. In marketing, it is well established that actions taken by the firm can have a long-term effect on user response in the future, implying that myopic optimization of profit is usually sub-optimal. Instead, the lifetime value (LTV) of users is a more desired metric of interest (Dwyer 1997). With LTV as the objective, CRM can be naturally formulated as a reinforcement-learning (RL) problem (Sutton and Barto 1998) where the immediate profit is used as reward and LTV as long-term value function. A similar motivation was used in a recent application of RL to advertising (Theocharous, Thomas, and Ghavamzadeh 2015).\nLike many other real-world problems, e.g., robotics and human-computer interaction applications, CRM is challenging partly because of the partial observability of a user\u2019s\nCopyright c\u00a9 2015, Microsoft Corporation. All rights reserved.\n(Markovian) state. Roughly speaking, a user\u2019s state summarizes her entire interaction history with the firm: conditioned on the state and future actions, future response of the user is independent of the interaction history. In practice, constructing and measuring such a state is difficult in complex problems like CRM. Popular choices such as the Recency-Frequency-Monetary value model (details of which are given in experiments) arguably capture only partial information of a real user state. The problem of state inference therefore becomes critical when applying RL to these non-Markovian problems.\nThe most common approach to dealing with partially observable states in reinforcement learning is to use a partially observable Markov decision process, or POMDP (Kaelbling, Littman, and Cassandra 1998), which is found successful in a few domains (Pineau, Gordon, and Thrun 2003; Williams and Young 2007). However, defining hidden states in a POMDP requires substantial domain knowledge, while such knowledge is not always available (or hard to obtain) for many complex, real-world tasks.\nIn this work, inspired by recent success of deep reinforcement learning (Mnih et al. 2015), we investigate the use of deep neural networks to capture and infer hidden states in an automatic way. As opposed to POMDP-based approaches, deep learning holds the promise of automatically finding appropriate representations for a given problem, which can be difficult for a human expert, see Deng and Yu (2014) for an extensive survey of successful applications, thus avoiding the laborious and challenging step of designing hidden states. Our \u201crecipe\u201d for using deep learning to tackle complex tasks like CRM can be summarized as follows:\n\u2022 First, unlike Mnih et al. (2015), we employ recurrent neural networks (RNN) and long short-term memory (LSTM) (Hochreiter and Schmidhuber 1997) models to learn the representation of states for RL. Since these recurrent models can aggregate partial information in the past, and can capture long-term dependencies in the sequential information, their performance is expected be superior to the contextual-window-based approach, which was used in the DQN model of Mnih et al. (2015).\n\u2022 Second, in order to best leverage supervision signals in the training data, we propose a new, hybrid approach that combines the strength of both supervised learning and re-\nar X\niv :1\n50 9.\n03 04\n4v 1\n[ cs\n.L G\n] 1\n0 Se\np 20\n15\ninforcement learning. Furthermore, the model in our hybrid approach is jointly learned using stochastic gradient descent (SGD) as follows. In each iteration, the representation of hidden states is first learned/inferred using supervision signals (next observation and immediate reward) in the training data, turning the POMDP into a MDP. Then, the Q-function is optimized using DQN that takes the learned hidden states as input. The superiority of the hybrid approach is validated in extensive experiments on a benchmark dataset.\nThe rest of the paper is organized as follows. Section 2 provides background information and related work on (deep) reinforcement learning and customer relationship management. Section 3 describes three different approaches (supervised learning, reinforcement learning, and the new hybrid) to the CRM task. Section 4 presents our experiments, evaluation methodology and results. We finally conclude the paper with future directions in Section 5."}, {"heading": "2 Background and Related Work", "text": ""}, {"heading": "2.1 Reinforcement Learning", "text": "In reinforcement learning, an agent uses observation and rewards to learn a (near-)optimal policy for an environment that maximizes the expected total reward. Formally, in discrete steps t = 1, 2, 3, . . ., the agent receives an observation ot \u2208 O, takes an action at \u2208 A, and receives a real-valued reward rt \u2208 R, where O, A and R are the sets of observations, actions and rewards, respectively. Let ht = (o1, a1, r1, . . . , ot\u22121, at\u22121, rt\u22121, ot) be an interaction history up to step t. The agent may select actions according to a policy \u03c0. The task of reinforcement learning is for the agent to learn an optimal action-selection policy \u03c0 such that the discounted cumulative reward, R = \u2211 t \u03b3\nt\u22121rt, is maximized, for a given discount factor \u03b3 \u2208 (0, 1).\nIn the case of MDPs where observations are states, there exist a variety of RL algorithms to effectively find the optimal policy (Sutton and Barto 1998) that depends solely on ot. Often, ot is denoted as st in MDPs. One of the most fundamental results in MDPs is the Bellman equation:\nQ\u2217(st, at) = E[rt + \u03b3max a Q\u2217(st+1, a)] ,\nwhich relates the immediate reward rt to the optimal Qfunction Q\u2217(s, a), defined as the discounted cumulative reward obtained by taking action a in state s and then following an optimal policy thereafter. The celebrated Q-learning algorithm and its variants (Sutton and Barto 1998) can be used to learn the optimal Q-function from data, under certain assumptions, by repeated applications of a stochastic approximation update rule on observed transitions (s, a, r, s\u2032):\nQ(s, a)\u2190 Q(s, a)+\u03b7(r+\u03b3 argmax a\u2032\nQ(s\u2032, a\u2032)\u2212Q(s, a)) ,\nwhere \u03b7 is a decaying step-sise parameter. Once Q \u2248 Q\u2217, the greedy policy, \u03c0Q, with respect to Q is near-optimal:\n\u03c0Q(s) := argmax a Q(s, a) .\nA large family of RL algorithms find near-optimal policies by approximating Q\u2217.\nIn the case of partially observable MDPs or POMDPs (Kaelbling, Littman, and Cassandra 1998), ot provides partial information about the underlying, unobserved state st. Let O(ot|st) be the (emission) probability of observing ot in state st, and b(s1) be the distribution of the start state, one may apply Bayes rule repeatedly to compute the posterior distribution of hidden states at any step t, b(st). It is well-known that b(st), also known as the belief state, is a sufficient statistics summarizing ht. Finally, while POMDP-based approaches may be built on the well-developed formal foundations on Markov processes and dynamic programming, other solutions based on trees or history windows may provide more flexible and effective alternative in certain cases (Lin 1993; McCallum 1995). In this paper, we use RNN/LSTM to represent and learn hidden states in a CRM task."}, {"heading": "2.2 Deep (Reinforcement) Learning", "text": "Recently, deep learning has seen exciting successes in solving reinforcement-learning problems. Most prominent is the recent use of a deep Q-network (DQN) in Q-learning to solve a large number of Atari games (Mnih et al. 2015), although neural networks have been used in some of the classic RL applications like TD-Gammon (Tesauro 1995).\nFor partially observable environments, deep learning may also be used to construct and track hidden states, even though the hidden states in a deep network may not be readily interpretable. For example, Deng et al. (2013) apply a deep network to track belief states in a spoken dialogue system. Earlier examples are applications of recurrent neural networks (LSTM and RNN) to control problems (Bakker 2002; Lin 1993). More recently, DQN was extended to deep recurrent Q-Learning for POMDPs on Atari games (Hausknecht and Stone 2015), where a recurrent Long Short-Term Memory (LSTM) model is used to encode the long interaction history. A similar LSTM-based approach is proposed to solve two text games to learn a state representation from observations (Narasimhan, Kulkarni, and Barzilay 2015). LSTM is also used to build a world model to make long-term predictions in complex Atari games (Oh et al. 2015).\nIn all approaches above, a neural network is used to represent a Q-function, and we call these models RL-RNN and RL-LSTM etc., depending on the network architecture used. The Q-function can be expressed as Q(s, a; \u03b8) where \u03b8 are the parameters in the network to be optimized. These networks are typically trained with a parametric version of Qlearning on observed transitions (s, a, r, s\u2032),\n\u03b8 \u2190 \u03b8 + \u03b7 ( r + \u03b3max\na\u2032 Q(s\u2032, a\u2032)\u2212Q(s, a)\n) \u2207\u03b8Q(s, a; \u03b8) .\nThe difference between DQN and RL-RNN/RL-LSTM is that RNN/LSTM can capture longer history information for state representation. In our work, in addition to using DQN, RL-RNN and RL-LSTM, we propose a new, hybrid model which combines supervised learning and reinforcement learning. During training, we use the supervised signals to learn the state representation, then jointly train DQN to approximate the Q-function."}, {"heading": "2.3 Customer Relationship Management", "text": "In general, customer relationship management (CRM) refers to \u201cthe practice of analyzing and using marketing databases and leveraging communication technologies to determine corporate practices and methods that maximize the lifetime value of each customer to the firm\u201d (Kumar and Reinartz 2012). Central to the definition of CRM is the notion of lifetime value (LTV) of a customer (Dwyer 1997), in contrast to short-term, or myopic, measures of customer value.\nIn the past, data mining has been applied to CRM (Berry and Linoff 2004) in a number of important applications, such as segmenting customers according to their behavior, identifying potentially risky customers, etc. In these cases, data mining provides valuable insights (via, e.g., clustering and classification) to support business decisions: \u201cdata mining suggests, businesses decide\u201d (Berry and Linoff 2004, page 7). Our work, in contrast, tries to close the decisionmaking loop: we aim to develop machine-learned models that directly take actions to maximize LTV of customers.\nWe take a reinforcement-learning approach, instead of a data-mining or supervised-learning one, to learn a decision-making policy from data. Pednault, Abe, and Zadrozny (2002) consider cost-effective decision making using the batch version of Q-learning (and a variant called Sarsa), with the Q-functions represented by piecewise linear functions. A similar approach is taken by Silver et al. (2013), although they focus on the online setting with the RL model interacting with simulated customers and use a linear Q function. In contrast, we use the much more flexible function approximator of deep neural networks to fit the Q function. It allows us to learn an RL model that outperforms a strong algorithm based on linear function approximation. More importantly, it provides an effective way to deal with hidden states that are not considered in these previous works.\nThe closest work to ours is the recent application of DQN to CRM by Tkachenko (2015). The same data is used in our experiments and by Pednault, Abe, and Zadrozny (2002) as well. The present paper differs from earlier work in a number of substantial ways. First, we focus on the challenge of non-Markovian CRM problems, overcoming the suboptimality associated with the earlier direct use of DQNs. Second, we employ state-of-the-art deep learning models (nonlinear function approximation) that try to capture hidden states, and develop a novel, hybrid model combining the strengths of supervised and reinforcement learning. Finally, we adopt a different evaluation methodology that is more appropriate for RL tasks, while the one in Tkachenko (2015) is fundamentally flawed. Details of these distinctions are made clearer in the experiment section."}, {"heading": "3 Models", "text": "Recall that the our goal is to learn the optimal Q-function from a sequence of (or sequences of) interaction histories in the form of (o1, a1, r1, o2, a2, r2, . . .). Once a good approximation of the optimal Q-function is obtained, a near-optimal policy can be readily defined that selects actions greedily. Here, we describe three families of approaches. The first is a supervised-learning (SL) baseline that ignores long-term\neffect on customers, therefore effectively optimizes shortterm (myopic) customer value. The second uses neural networks to represent the Q-function, including DQN, RLRNN and RL-LSTM. They represent state-of-the-art baselines that have found many recent successes in a variety of problems. The third is the hybrid approach we propose in this work, in which supervised learning is used to infer and track hidden states and reinforcement learning is used to optimize control in order to maximize long-term reward."}, {"heading": "3.1 Supervised Learning", "text": "The first baseline is to treat the problem as supervised learning, in which one tries to predict which action leads to higher expected (immediate) reward given the interaction history so far. In our experiments, we formulated the problem as regression with the raw reward signal as target. For each transition tuple (o, a, r, o\u2032) from training data, we tried to learn the regression of r given observation o and possibly the history that led to this transition, depending on which network model is used. This reduction results in standard deep learning models with mean squared error as the loss function for training.\nSeveral network architectures are used, with and without built-in modeling of long-term dependency: \u2022 Multi-layer (deep) neural networks (DNN) breaks\nan interaction history into individual transitions, {(ot, at, rt, ot+1)}t=1,2,.... The network is learned to predict rt based on (ot, at), for rt > \u03c4 . In our experiments, we found \u03c4 = 0 to work best empirically. At test time, the model, denoted R\u0302, takes current observation o as input, and selects actions greedily according to its reward predictions: argmaxa R\u0302(o, a).\n\u2022 RNN and LSTM can model long-term dependency in a customer\u2019s interaction history. As shown in Figure 1 for the case of RNN, the interaction history can no longer be decomposed into separate transitions like DNN. At step t, the model is updated using the observation ot and the current internal history summary, h\u0303t\u22121, which is maintained recursively in RNN. At test time, the model selects actions in a similar fashion, based on both the current observation and the current internal history summary. The case for LSTM is similar."}, {"heading": "3.2 Reinforcement Learning", "text": "Supervised-learning models given in the previous subsection only considers immediate rewards. In contrast, reinforcement learning takes future rewards into account and aims to\noptimize total long-term reward directly. This makes reinforcement learning particularly suitable for tasks like CRM when one tries to optimize LTV of customers.\nThe first deep RL baseline is DQN (Mnih et al. 2015), where we treat ot as state st,1 and optimize network parameters to obtain an approximate Q-function, Q(s, a). Once a good Q-network is learned, it can be used to select actions in a greedy fashion: \u03c0Q(s) := argmaxaQ(s, a).\nThe second and third deep reinforcement learning baselines, RL-RNN and RL-LSTM, are similar to DQN, but are expected to handle partial observability by explicitly modeling long-term dependencies of future rewards on history (Bakker 2002; Hausknecht and Stone 2015; Lin 1993; Narasimhan, Kulkarni, and Barzilay 2015). Similar to RNN and LSTM in supervised-learning models, the Q-network now is a function of the current observation ot and the current internal history summary h\u0303t\u22121. Again, the internal history summary h\u0303t is updated recursively as time goes on. Actions are selected greedily after the Q-network is learned.\nIn practice, training DQN or RL-RNN/RL-LSTM may be quite unstable, due to dependence of transition tuples in an interaction history. One variant, as used in previous work (Mnih et al. 2015), is to use two Q-networks: one network (the \u201ctarget network\u201d) is used to define the target value in Q-learning updates, r + \u03b3maxa\u2032 Q(s\u2032, a\u2032; \u03b8), while the other is used for parameter updates. When the latter network\u2019s parameter converges, it becomes the target network, and the process repeats until convergence. We use the same two-network implementation of DQN, RL-RNN and RLLSTM in the experiments."}, {"heading": "3.3 SL+RL Hybrid Models", "text": "RL models are trained to maximize long-term rewards. In contrast, SL models can be optimized to predict observations and immediate rewards, thus having the potential to better represent and infer hidden states. With such complementary strengths, it is beneficial to take a hybrid approach, which uses SL for hidden-state representation learning and RL for policy learning. Moreover, these two components should not be optimized separately: ideally, the SL component should learn an internal state representation that allows the RL component to maximize long-term reward.\n1Contextual-window-based method can be used to capture a fixed-window interaction history for ot, but does not work well in our simulation settings.\nTo this end, we propose a new family of hybrid models combining supervised learning and reinforcement learning, trained in a joint fashion: the SL component can be an RNN or LSTM; the RL component is a DQN. The resulting models are called SL-RNN+RL-DQN (Figure 3) and SLLSTD+RL-DQN, respectively.\nFor training the hybrid model, we use a joint supervisedreinforced approach. First, we train an RNN (or LSTM), which learns hidden states from signals including next observations and immediate rewards. Then, the learned hidden states are the input to DQN, which learns Q-function of a near-optimal policy. These two training steps are interleaved in each SGD iteration.\nThe difference between these models and RL-RNN (or RL-LSTM) is that, during training, the supervised signals are used to learn the state information, are back-propagated to the head of RNN/LSTM, while the RL signals are only back-propagated to the hidden layers of RNN for DQN training, do not involve in the RNN training."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 DataSet", "text": "In this paper, we use the 1998 KDD Cup direct mailing dataset2, which has been used in the RL literature (Marivate 2015; Pednault, Abe, and Zadrozny 2002) It was collected by a non-profit organization, PVA, who provides programs and services for US veterans with spinal cord injuries or disease. PVA raises money via direct mailing campaigns. The dataset contains a record for every donor who received the mailing and did not make a donation in the 12 months before that. For each of them, it is recorded whether and how much they donated as a response to the campaigns. Apart from that, data is given about the previous and current mailing campaign, as well as personal information and the giving history of each lapsed donor. The training data is collected for 23 distinct periods for 95, 412 donors, resulting in over 2M transition tuples. Each donor\u2019s interaction history can be viewed as a time series of 23 steps.\nDirect mailing campaigns are a typical CRM task, where the decision is on what type of email to send, in order to\n2https://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html\nmaximize long-term profit (or cumulative donation in the case of PVA). In the dataset, we found 12 actions, including 11 mailing types and 1 inaction (corresponding to nonresponse in the dataset). The resulting data for each client is a sequence (o1, a1, r1, . . . , o22, a22, r22, o23), where: \u2022 ot is the current observation of the client. As in previ-\nous work (Tkachenko 2015), it is a 5-dimensional vector consisting of (1) how recently the donor donated last, (2) how frequently she donates, (3) her average donation amount, (4) how many times PVA sends her a mail in the last six months, and (5) how many times PVA has sent her mails. The first three correspond to the well-known Recency-Frequency-Monetary value model in CRM, and the other two are application specific.\n\u2022 at is one of the 12 actions taken by PVA. \u2022 rt is the immediate reward received after taken action at.\nIn the raw dataset, the reward is the amount of donation in dollars, ranges from $0 to $1000."}, {"heading": "4.2 Evaluation Methodology", "text": "Reliable evaluation has been a challenge in reinforcement learning when no simulator (like those used in Atari games (Mnih et al. 2015)) is available. The approach of Tkachenko (Tkachenko 2015) proceeds as follows. After a model is optimized using training data, it is run on test data to select actions in every step. The test data is then partitioned into two subsets: the SAME set consists of transitions where the model\u2019s selected action is the same as the action in the data; all other transitions are in the DEVIATED set. Clearly, the partition into SAME and DEVIATED is model dependent. Finally, a model is considered better if its average reward in the corresponding SAME set is higher.\nWhile the procedure above might sound intuitive and appealing, it is fundamentally flawed as a way to evaluate models that select actions. First of all, it focuses on myopic rewards, and does not reflect the capability of RL models whose very aim is to optimize (long-term) LTV of customers. The second problem is more subtle but equally severe: the evaluation is really about finding correlation (between actions and rewards in data) rather than causation (whether higher rewards can be obtained if an RL model is used to select actions). The distinction between correlation and causation can lead to many paradoxes. For instance, imagine a model that learns how PVA selected actions when collecting this dataset. It can then select actions based on whether the client is generous (using information encoded in the observation vector): it selects the same action as PVA if and only if the client is generous. This way, the model is able to \u201cgame\u201d the evaluation protocol of (Tkachenko 2015) by enforcing its SAME set to contain only generous clients who tend to donate more. However, such a cherry-picking model is not expected to do anything better than the datacollection policy of PVA.\nGiven these important drawbacks, we adopt a different evaluation that is common in the RL literature: we use the dataset to build a simulator, and rely on the simulator to generate synthetic CRM interaction sequences for training and evaluating different action-selection models. While building\na model is in generally nontrivial, this data presents a factored structure: at any step t, given at and rt, the five components in the observation vectors evolve (from ot to ot+1) independently. A similar approach was also taken in previous work (Pednault, Abe, and Zadrozny 2002). More specifically, at step t, the simulator takes the observed history ht and action at as input to predict: \u2022 next observation ot+1: the 5-dimensional observation is\ndiscrete in this problem, and individual dimensions evolve independently of each other. We therefore build an observation probability table for each observation dimension, and the sample next observations using these tables.\n\u2022 reward rt: in the experiments, we build two reward functions. One is based on standard linear least-squares regression (LR), which predicts a reward r give the current observation o, assuming no long-term dependence of rewards on the history. In this case, observation o is a state. The second is based on an RNN, trained to predict reward r using its internal history summary h\u0303t. This simulator thus represents a more realistic scenario that allows hidden states and long-term effects on customers (Netzer, Lattin, and Srinivasan 2008)."}, {"heading": "4.3 Experiment Setup", "text": "We found smaller data are enough to yield strong policies, therefore only use a random subset of donors of the entire data for experiments. We tried four data sizes of varying number of donors, each having 23 steps, so that the total number of transitions is {50K, 100K, 200K, 500K}. The data were then split into training, validation, and test sets with proportions 4:1:1.\nTo generate training data, we started with the initial observation vector of donors in the training set, and ran one of the following data-collection policies to select actions: \u2022 Uniformly random policy (U): at any step, the action is\nchosen uniformly at random from the set of actions. \u2022 Probability-matching policy (M): at any step, the action is\nchosen with probability p(a), the frequency that a occurs in raw data.\n\u2022 Real policy (R): the action is the real action recorded in the raw data. Hence, the simulator is used only to regenerate reward. Table 1 summarizes all choices along three dimensions in our experiment setup. In all experiments, we fixed \u03b3 = 0.9 and compared the average per-step reward (donation) of the learned policies. We ran each setting at least 5 times and report the average results."}, {"heading": "4.4 Results", "text": "A Linear-Parametric Baseline All RL models we study here try to approximate the Q-function with different\nnetwork architectures. A natural baseline of such valuefunction-based approaches is to use linear function approximation, which is perhaps the most commonly used in the literature (Sutton and Barto 1998). Here, we used least-squares policy iteration (LSPI) (Lagoudakis and Parr 2003), a popular and strong baseline whose Q-function is expressed as a set of weights {wa; a \u2208 A}, such that Q(o, a) = \u3008wa, o\u3009.\nTable 2 shows the average per-step reward of policies learned by LSPI and random policy under LR simulator. So LSPI did learn a policy that was better than a random policy for all three scenarios.\nWith the LSPI baseline, we are now ready to present and compare the models based on neural networks. We will see they all outperform LSPI substantially.3 Three sets of experiments were done, as summarized in Table 3. Each experiment focuses on a particular aspect and is discussed in detail in the following subsections.\nExperiment E1 This experiment is to investigate how hidden states affect relative performance of various models.\nIf the LR simulator is used, the problem becomes an MDP with observations o being the states. Since the number of observations is not too large, we ran exact value iteration to compare the optimal policy, which resulted in an average reward of roughly $7.14, an upper bound for all policies.\nTable 4 summarizes results of all deep supervised learning and reinforcement learning models. The best deep SL model is RNN (or LSTM), with an average reward of $6.48. All deep RL models performed almost the same, with average rewards around $7.06, which is quite close to the optimal value. A couple of observations are in place. First, RL models can indeed outperform SL models in terms of long-term rewards, as expected. Second, there is no difference between all RL models, because there is no hidden states when LR simulator is used to generate reward.\n3Although it is possible to hand-engineer good features so that linear function approximation works well, neural networks are found to be able to learn strong features automatically in a variety of domains. We therefore did not do feature engineering in LSPI. A comparison to LSPI is not the focus of our work, either.\nHowever, if an RNN model is used to generate reward, there are interesting differences between reinforcement learning models, as shown in Table 5. Recall that with the RNN simulator, rewards are a function of the current observation as well as history up to that step. In other words, the model must be able to infer and track such hidden states in order to maximize cumulative rewards. From the table, we can see that all RL models still outperform SL models, as expected. Furthermore, there is a clear advantage of RL+RNN and RL+LSTM over DQN, since the latter ignores history information. Finally, Supervised LSTM/RNN + Reinforced DQN are better than Reinforcement learning with RNN (RL-RNN) and Reinforcement learning with LSTM (RL-LSTM).\nExperiment E2 The second set of experiments is to investigate how the data-collection policy affects model performance. It is well-known that proper exploration is necessary to learn a good policy, and we examine how it affects our models empirically. From Table 6, when actions were selected by U and M, qualitatively similar results can be obtained as before. However, when actions were chosen by R, all models\u2019 performance decays, and reinforcement learning\nmodels are hurt much more that they are inferior to supervised learning models.\nAn examination of the raw data revealed that the actual data-collection policy by PVA seemed to be deterministic: the same action is applied to all donors at the same step. Therefore, little or no exploration exists in this dataset. Reinforcement learning algorithms tend to be more sensitive to lack of exploration than supervised learning algorithms, because the former in a sense try to predict what will happen far into the future. The less exploration, the more error is introduced into the projection of an RL algorithm, which is consistent with what is shown in Table 6.\nExperiment E3 In the last set of experiments, we varied data sizes to see how it affects each model\u2019s performance. From Table 7, we saw similar results with a wide range of data sizes. It indicates our models are data-efficient, and the benefits over SL or RL models are consistent with different data sizes."}, {"heading": "5 Conclusions", "text": "In this work, we have studied how to use deep learning models, for both supervised learning and reinforcement learning, to solve a CRM task, which is typical of real-world nonMarkovian problems. In particular, we investigated how to utilize supervised signals in training data to learn hiddenstate representations, and then jointly train a deep Q-network (using reinforcement learning) to optimize the control for maximizing long-term rewards. Through a large-scale experimental analysis under different settings, we showed that: (1) Deep RL is more effective than SL for optimizing lifetime values; (2) RL with RNN/LSTM models is a promising approach to solving non-Markovian tasks with longterm dependencies; (3) It is promising to use memory networks models to learn hidden-state representations in a supervised learning manner, with DQN jointly trained for nonMarkovian tasks. Beyond the analysis, our experimental results demonstrate the promise for deep reinforcement learning on specific CRM tasks.\nThe promising results suggest multiple interesting directions for future work. One idea is to explore the use of RNN/LSTM in model-based or policy-based RL, as opposed to the value-function-based approaches this work focuses on. Those alternatives may be more appropriate solutions in\ncertain application domains. Another important direction is to capture latent structures of actions, in order to facilitate generalization across different actions. Doing so also allows handling newly emerged actions that are common in many applications.\nAcknowledgments We thank Nan Jiang for helpful discussions on building the simulator in our experiments."}], "references": [{"title": "G", "author": ["M.J. Berry", "Linoff"], "venue": "S.", "citeRegEx": "Berry and Linoff 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Yu", "author": ["L. Deng"], "venue": "D.", "citeRegEx": "Deng and Yu 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Recent advances in deep learning for speech research at Microsoft", "author": ["Deng"], "venue": "In In Acoustics, Speech and Signal Processing 2013 (ICASSP-13)", "citeRegEx": "Deng,? \\Q2013\\E", "shortCiteRegEx": "Deng", "year": 2013}, {"title": "F", "author": ["Dwyer"], "venue": "R.", "citeRegEx": "Dwyer 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Stone", "author": ["M. Hausknecht"], "venue": "P.", "citeRegEx": "Hausknecht and Stone 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "A", "author": ["L.P. Kaelbling", "M.L. Littman", "Cassandra"], "venue": "R.", "citeRegEx": "Kaelbling. Littman. and Cassandra 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "and Reinartz", "author": ["V. Kumar"], "venue": "W.", "citeRegEx": "Kumar and Reinartz 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Parr", "author": ["M.G. Lagoudakis"], "venue": "R.", "citeRegEx": "Lagoudakis and Parr 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcement Learning for Robots using Neural Networks", "author": ["Lin", "L.-J"], "venue": "Ph.D. Dissertation,", "citeRegEx": "Lin and L..J.,? \\Q1993\\E", "shortCiteRegEx": "Lin and L..J.", "year": 1993}, {"title": "V", "author": ["Marivate"], "venue": "N.", "citeRegEx": "Marivate 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "Fidjeland"], "venue": "K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D.", "citeRegEx": "Mnih et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Kulkarni Narasimhan", "K. Barzilay 2015] Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-15)", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "J", "author": ["Netzer, O.", "Lattin"], "venue": "M.; and Srinivasan, V.", "citeRegEx": "Netzer. Lattin. and Srinivasan 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Action-conditional video prediction using deep networks in Atari games. arXiv preprint arXiv:1507.06527", "author": ["Oh"], "venue": null, "citeRegEx": "Oh,? \\Q2015\\E", "shortCiteRegEx": "Oh", "year": 2015}, {"title": "E", "author": ["Pednault"], "venue": "P. D.; Abe, N.; and Zadrozny, B.", "citeRegEx": "Pednault. Abe. and Zadrozny 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "G", "author": ["Pineau, J.", "Gordon"], "venue": "J.; and Thrun, S.", "citeRegEx": "Pineau. Gordon. and Thrun 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Concurrent reinforcement learning from customer interactions", "author": ["Silver"], "venue": "In Proceedings of the Thirtieth International Conference on Machine Learning", "citeRegEx": "Silver,? \\Q2013\\E", "shortCiteRegEx": "Silver", "year": 2013}, {"title": "A", "author": ["R.S. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "P", "author": ["Theocharous, G.", "Thomas"], "venue": "S.; and Ghavamzadeh, M.", "citeRegEx": "Theocharous. Thomas. and Ghavamzadeh 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["J.D. Williams", "Young"], "venue": "J.", "citeRegEx": "Williams and Young 2007", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [], "year": 2015, "abstractText": "Successful applications of reinforcement learning in realworld problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent\u2019s entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we study reinforcement learning with deep neural networks, including RNN and LSTM, which are equipped with the desired property of being able to capture long-term dependency on history, and thus providing an effective way of learning the representation of hidden states. We further develop a hybrid approach that combines the strength of both supervised learning (for representing hidden states) and reinforcement learning (for optimizing control) with joint training. Extensive experiments based on a KDD Cup 1998 direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best across the board.", "creator": "LaTeX with hyperref package"}}}