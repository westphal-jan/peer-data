{"id": "1509.05172", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2015", "title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis", "abstract": "We reticular consider univox the grohmann off - mclaughlin policy lepist\u00f6 evaluation problem pungo in sprockets Markov ear decision urartians processes hauss with 64.19 function motherwell approximation. word-initial We stewartville propose descubrio a generalization of boraginaceae the dahuri recently qabalan introduced \\ 1up emph {biekert emphatic somerton temporal differences} (ETD) algorithm \\ fixated citep {lleva SuttonMW15 }, blethyn which encompasses the shen original ETD ($ \\ groupies lambda $ ), as well as canterbury several other off - policy satin evaluation athanasiadis algorithms as special grada\u0161\u010devi\u0107 cases. We mediados call 53.09 this framework \\ palanggoon ETD, heavy where three-man our shangela introduced parameter $ \\ avicenna beta $ macoto controls snares the carducci decay rate premolars of acero an 47-minute importance - 12-team sampling emanate term. stolice We study rubric conditions fruitful under blaen which aglianico the projected fixed - point equation gardyne underlying \\ kelu ETD \\ counterinsurgents involves shefqet a j.gordon contraction operator, 29-april allowing us jsat to liveliest present loving-kindness the neuromodulation first 118.08 asymptotic uriminzokkiri error twt bounds (roy.rivenburg bias) for \\ ETD. bokeem Our results mouthing show that gholi the pre-arranged original ogunbiyi ETD algorithm leotardo always involves djakarta a amrullah contraction operator, and shrapnel its bias diblasio is romel bounded. stacked Moreover, by fukin controlling $ \\ sansa beta $, polsby our proposed jellacic generalization geach allows soldats trading - wicky off wacken bias for syndey variance rheumatic reduction, thereby cavuto achieving a lymphadenopathy lower total error.", "histories": [["v1", "Thu, 17 Sep 2015 09:03:35 GMT  (219kb)", "https://arxiv.org/abs/1509.05172v1", "arXiv admin note: text overlap witharXiv:1508.03411"], ["v2", "Fri, 27 Nov 2015 07:17:55 GMT  (214kb)", "http://arxiv.org/abs/1509.05172v2", "arXiv admin note: text overlap witharXiv:1508.03411"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1508.03411", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["assaf hallak", "aviv tamar", "r\u00e9mi munos", "shie mannor"], "accepted": true, "id": "1509.05172"}, "pdf": {"name": "1509.05172.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n05 17\n2v 2\n[ st\nat .M\nL ]\n2 7"}, {"heading": "1 Introduction", "text": "In Reinforcement Learning (RL; Sutton and Barto 1998), policy-evaluation refers to the problem of evaluating the value function \u2013 a mapping from states to their longterm discounted return under a given policy, using sampled observations of the system dynamics and reward. Policy-evaluation is important both for assessing the quality of a policy, but also as a sub-procedure for policy optimization.\nFor systems with large or continuous state-spaces, an exact computation of the value function is often impossible. Instead, an approximate value-function is sought using various function-approximation techniques (a.k.a. approximate dynamic-programming; Bertsekas 2012). In this approach, the parameters of the value-function approximation are tuned using machine-learning inspired methods, often based on temporaldifferences (TD;Sutton and Barto 1998).\nThe source generating the sampled data divides policy evaluation into two cases. In the on-policy case, the samples are generated by the target-policy \u2013 the policy under evaluation; In the off-policy setting, a different behavior-policy generates the data. In\nthe on-policy setting, TD methods are well understood, with classic convergence guarantees and approximation-error bounds, based on a contraction property of the projected Bellman operator underlying TD (Bertsekas and Tsitsiklis, 1996). These bounds guarantee that the asymptotic error, or bias, of the algorithm is contained. For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995).\nThe standard error-bounds may be shown to hold for an importance-sampling TD method (IS-TD), as proposed by Precup, Sutton, and Dasgupta (2001). However, this method is known to suffer from a high variance of its importance-sampling estimator, limiting its practicality.\nLately, Sutton, Mahmood, and White (2015) proposed the emphatic TD (ETD) algorithm: a modification of the TD idea, which converges off-policy (Yu, 2015), and has a reduced variance compared to IS-TD. This variance reduction is achieved by incorporating a certain decay factor over the importance-sampling ratio. However, to the best of our knowledge, there are no results that bound the bias of ETD. Thus, while ETD is assured to converge, it is not known how good its limit actually is.\nIn this paper, we propose the ETD(\u03bb, \u03b2) framework \u2013 a modification of the ETD(\u03bb) algorithm, where the decay rate of the importance-sampling ratio, \u03b2, is a free parameter, and \u03bb is the same bootstrapping parameter employed in TD(\u03bb) and ETD(\u03bb). By varying the decay rate, one can smoothly transition between the IS-TD algorithm, through ETD, to the standard TD algorithm.\nWe investigate the bias of ETD(\u03bb, \u03b2), by studying the conditions under which its underlying projected Bellman operator is a contraction. We show that the original ETD possesses a contraction property, and present the first error bounds for ETD and ETD(\u03bb, \u03b2). In addition, our error bound reveals that the decay rate parameter balances between the bias and variance of the learning procedure. In particular, we show that selecting a decay equal to the discount factor as in the original ETD may be suboptimal in terms of the mean-squared error.\nThe main contributions of this work are therefore a unification of several off-policy TD algorithms under the ETD(\u03bb, \u03b2) framework, and a new error analysis that reveals the bias-variance trade-off between them.\nRelated Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al., 2009; Liu et al., 2015). These algorithms are guaranteed to converge, however, their asymptotic error can be bounded only when the target and behavior policies are similar (Bertsekas and Yu, 2009), or when their induced transition matrices satisfy a certain matrix-inequality suggested by Kolter (2011), which limits the discrepancy between the target and behavior policies. When these conditions are not satisfied, the error may be arbitrarily large (Kolter, 2011). In contrast, the approximation-error bounds in this paper hold for general target and behavior policies."}, {"heading": "2 Preliminaries", "text": "We consider an MDP M = (S,A, P,R, \u03b3), where S is the state space, A is the action space, P is the transition probability matrix, R is the reward function, and \u03b3 \u2208 [0, 1) is the discount factor.\nGiven a target policy \u03c0 mapping states to a distribution over actions, our goal is to evaluate the value function:\nV \u03c0(s) . = E\u03c0\n[\n\u221e \u2211\nt=0\nR(st, at)\n\u2223 \u2223 \u2223 \u2223 \u2223 s0 = s ] .\nLinear temporal difference methods (Sutton and Barto, 1998) approximate the value function by\nV \u03c0(s) \u2248 \u03b8\u22a4\u03d5(s),\nwhere \u03d5(s) \u2208 Rn are state features, and \u03b8 \u2208 Rn are weights, and use sampling to find a suitable \u03b8. Let \u00b5 denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u00b5(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u00b5(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u00b5 and \u03c0 are such that \u03c1t is well-defined1 for all t.\nLet T denote the Bellman operator for policy \u03c0, given by\nT (V ) . = R+ \u03b3PV,\nwhere R and P are the reward vector and transition matrix induced by policy \u03c0, and let \u03a6 denote a matrix whose columns are the feature vectors for all states. Let d\u00b5 and d\u03c0 denote the stationary distributions over states induced by the policies \u00b5 and \u03c0, respectively. For some d \u2208 R|S| satisfying d > 0 element-wise, we denote by \u03a0d a projection to the subspace spanned by \u03d5(s) with respect to the d-weighted Euclideannorm.\nFor \u03bb = 0, the ETD(0, \u03b2) (Sutton, Mahmood, and White, 2015) algorithm seeks to find a good approximation of the value function by iteratively updating the weight vector \u03b8:\n\u03b8t+1 = \u03b8t + \u03b1Ft\u03c1t(Rt+1 + \u03b3\u03b8 \u22a4 t \u03d5t+1 \u2212 \u03b8 \u22a4 t \u03d5t)\u03d5t\nFt = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1, (1)\nwhere Ft is a decaying trace of the importance-sampling ratios, and \u03b2 \u2208 (0, 1) controls the decay rate.\nRemark 1. The algorithm of Sutton, Mahmood, and White (2015) selects the decay rate equal to the discount factor, i.e., \u03b2 = \u03b3. Here, we provide more freedom in choosing the decay rate. As our analysis reveals, the decay rate controls a bias-variance trade-off of ETD, therefore this freedom is important. Moreover, we note that for \u03b2 = 0, we obtain the standard TD in an off-policy setting Yu (2012), and when \u03b2 = 1 we obtain the full importance-sampling TD algorithm Precup, Sutton, and Dasgupta (2001).\n1Namely, if \u00b5(a|s) = 0 then \u03c0(a|s) = 0 for all s \u2208 S.\nRemark 2. The ETD(0, \u03b3) algorithm of Sutton, Mahmood, and White (2015) also includes a state-dependent emphasis weight i(s), and a state-dependent discount factor \u03b3(s). Here, we analyze the case of a uniform weight i(s) = 1 and constant discount factor \u03b3 for all states. While our analysis can be extended to their more general setting, the insights from the analysis remain the same, and for the purpose of clarity we chose to focus on this simpler setting.\nAn important term in our analysis is the emphatic weight vector f , defined by\nf\u22a4 = d\u22a4\u00b5 (I \u2212 \u03b2P ) \u22121. (2)\nIt can be shown (Sutton, Mahmood, and White, 2015; Yu, 2015), that ETD(0, \u03b2) converges to \u03b8\u2217 - a solution of the following projected fixed point equation:\nV = \u03a0fTV, V \u2208 R |S|. (3)\nFor the fixed point equation (3), a contraction property of \u03a0fT is important for guaranteeing both a unique solution, and a bias bound (Bertsekas and Tsitsiklis, 1996).\nIt is well known that T is a \u03b3-contraction with respect to the d\u03c0-weighted Euclidean norm (Bertsekas and Tsitsiklis, 1996), and by definition \u03a0f is a non-expansion in f - norm, however, it is not immediate that the composed operator \u03a0fT is a contraction in any norm. Indeed, for the TD(0) algorithm (Sutton and Barto 1998; corresponding to the \u03b2 = 0 case in our setting), a similar representation as a projected Bellman operator holds, but it may be shown that in the off-policy setting the algorithm might diverge (Baird, 1995). In the next section, we study the contraction properties of \u03a0fT , and provide corresponding bias bounds.\n3 Bias of ETD(0, \u03b2)\nIn this section we study the bias of the ETD(0, \u03b2) algorithm. Let us first introduce the following measure of discrepancy between the target and behavior policies:\n\u03ba . = min\ns\nd\u00b5(s)\nf(s) .\nLemma 1. The measure \u03ba obtains values ranging from \u03ba = 0 (when there is a state visited by the target policy, but not the behavior policy), to \u03ba = 1 \u2212 \u03b2 (when the two policies are identical).\nThe technical proof is given in the supplementary material. The following theorem shows that for ETD(0, \u03b2) with a suitable \u03b2, the projected Bellman operator \u03a0fT is indeed a contraction.\nTheorem 1. For \u03b2 > \u03b32(1\u2212\u03ba), the projected Bellman operator\u03a0fT is a \u221a \u03b32\n\u03b2 (1\u2212 \u03ba)-\ncontraction with respect to the Euclidean f -weighted norm, namely, \u2200v1, v2 \u2208 R|S|:\n\u2016\u03a0fTv1 \u2212\u03a0fTv2\u2016f \u2264\n\u221a\n\u03b32\n\u03b2 (1\u2212 \u03ba)\u2016v1 \u2212 v2\u2016f .\nProof. Let F = diag(f). We have\n\u2016v\u20162f \u2212 \u03b2 \u2016Pv\u2016 2 f = v \u22a4Fv \u2212 \u03b2v\u22a4P\u22a4FPv\n\u2265(a) v\u22a4Fv \u2212 \u03b2v\u22a4diag(f\u22a4P )v\n= v\u22a4[F \u2212 \u03b2diag(f\u22a4P )]v\n= v\u22a4 [ diag ( f\u22a4(I \u2212 \u03b2P ) )] v\n=(b) v\u22a4diag(d\u00b5)v = \u2016v\u2016 2 d\u00b5 ,\nwhere (a) follows from Jensen inequality:\nv\u22a4P\u22a4FPv = \u2211\ns\nf(s)( \u2211\ns\u2032\nP (s\u2032|s)v(s\u2032))2\n\u2264 \u2211\ns\nf(s) \u2211\ns\u2032\nP (s\u2032|s)v2(s\u2032)\n= \u2211\ns\u2032\nv2(s\u2032) \u2211\ns\nf(s)P (s\u2032|s)\n= v\u22a4diag(f\u22a4P )v,\nand (b) is by the definition of f in (2). Notice that for every v:\n\u2016v\u2016 2 d\u00b5 = \u2211\ns\nd\u00b5(s)v 2(s) \u2265\n\u2211\ns\n\u03baf(s)v2(s) = \u03ba \u2016v\u2016 2 f\nTherefore:\n\u2016v\u2016 2 f \u2265 \u03b2 \u2016Pv\u2016 2 f + \u2016v\u2016 2 d\u00b5 \u2265 \u03b2 \u2016Pv\u2016 2 f + \u03ba \u2016v\u2016 2 f ,\n\u21d2 \u03b2 \u2016Pv\u2016 2 f \u2264 (1\u2212 \u03ba) \u2016v\u2016 2 f\nand:\n\u2016Tv1 \u2212 Tv2\u2016 2 f = \u2016\u03b3P (v1 \u2212 v2)\u2016 2 f\n= \u03b32 \u2016P (v1 \u2212 v2)\u2016 2 f \u2264 \u03b32\n\u03b2 (1\u2212 \u03ba) \u2016v1 \u2212 v2\u2016\n2 f .\nHence, T is a \u221a \u03b32\n\u03b2 (1\u2212 \u03ba)-contraction. Since \u03a0f is a non-expansion in the f -weighted\nnorm (Bertsekas and Tsitsiklis, 1996), \u03a0fT is a \u221a \u03b32\n\u03b2 (1 \u2212 \u03ba)-contraction as well.\nRecall that for the original ETD algorithm (Sutton, Mahmood, and White, 2015), we have that \u03b2 = \u03b3, and the contraction modulus is \u221a\n\u03b3(1\u2212 \u03ba) < 1, thus the contraction of \u03a0fT always holds.\nAlso note that in the on-policy case, the behavior and target policies are equal, and according to Lemma 1 we have 1 \u2212 \u03ba = \u03b2. In this case, the contraction modulus in Theorem 1 is \u03b3, similar to the result for on-policy TD Bertsekas and Tsitsiklis (1996).\nWe remark that Kolter (2011) also used a measure of discrepancy between the behavior and the target policy to bound the TD-error. However, Kolter (2011) considered the standard TD algorithm, for which a contraction could be guaranteed only for a class of behavior policies that satisfy a certain matrix inequality criterion. Our results show that for ETD(0, \u03b2) with a suitable \u03b2, a contraction is guaranteed for general behavior policies. We now show in an example that our contraction modulus bounds are tight.\nExample 1. Consider an MDP with two states: Left and Right. In each state there are two identical actions leading to either Left or Right deterministically. The behavior policy will choose Right with probability \u03b5, and the target policy will choose Left with probability \u03b5, hence 1\u2212 \u03ba \u2248 1. Calculating the quantities of interest:\nP =\n(\n\u03b5 1\u2212 \u03b5 \u03b5 1\u2212 \u03b5\n)\n, d\u00b5 = (1\u2212 \u03b5, \u03b5)\nf = 1\n1\u2212 \u03b2 (1 + 2\u03b5\u03b2 \u2212 \u03b5\u2212 \u03b2,\u22122\u03b5\u03b2 + \u03b5+ \u03b2)\n\u22a4 .\nSo for v = (0, 1)\u22a4:\n\u2016v\u2016 2 f =\n\u03b5+ \u03b2 \u2212 2\u03b5\u03b2\n1\u2212 \u03b2 , \u2016Pv\u2016\n2 f =\n(1\u2212 \u03b5)2\n1\u2212 \u03b2 ,\nand for small \u03b5 we obtain that \u2016\u03b3Pv\u2016 2 \u2016v\u20162f \u2248 \u03b3 2 \u03b2 .\nAn immediate consequence of Theorem 1 is the following error bound, based on Lemma 6.9 of Bertsekas and Tsitsiklis (1996):\nCorollary 1. We have\n\u2225 \u2225\u03a6\u22a4\u03b8\u2217 \u2212 V \u03c0 \u2225 \u2225\nf \u2264\n1 \u221a\n1\u2212 \u03b3 2\n\u03b2 (1\u2212 \u03ba)\n\u2016\u03a0fV \u03c0 \u2212 V \u03c0\u2016\nf ,\n\u2225 \u2225\u03a6\u22a4\u03b8\u2217 \u2212 V \u03c0 \u2225 \u2225\nd\u00b5 \u2264\n1 \u221a\n\u03b3 ( 1\u2212 \u03b3 2\n\u03b2 (1\u2212 \u03ba)\n)\n\u2016\u03a0fV \u03c0 \u2212 V \u03c0\u2016\nf .\nUp to the weights in the norm, the error \u2016\u03a0fV \u03c0 \u2212 V \u03c0\u2016f is the best approximation we can hope for, within the capability of the linear approximation architecture. Corollary 1 guarantees that we are not too far away from it.\nNotice that the error \u2225 \u2225\u03a6\u22a4\u03b8\u2217 \u2212 V \u03c0 \u2225 \u2225\nd\u00b5 uses a measure d\u00b5 which is independent of\nthe target policy; This could be useful in further analysis of a policy iteration algorithm, which iteratively improves the target policy using samples from a single behavior policy. Such an analysis may proceed similarly to that in Munos (2003) for the on-policy case."}, {"heading": "3.1 Numerical Illustration", "text": "We illustrate the importance of the ETD(0, \u03b2) bias bound in a numerical example. Consider the 2-state MDP example of Kolter (2011), with transition matrix P = (1/2)1 (where 1 is an all 1 matrix), discount factor \u03b3 = 0.99, and value function V = [1, 1.05]\u22a4 (with R = (I\u2212\u03b3P )V ). The features are \u03a6 = [1, 1.05+\u03b5]\u22a4, with \u03b5 = 0.001. Clearly, in this example we have d\u03c0 = [0.5, 0.5]. The behavior policy is chosen such that d\u00b5 = [p, 1\u2212 p].\nIn Figure 1 we plot the mean-squared error \u2225 \u2225\u03a6\u22a4\u03b8\u2217 \u2212 V \u03c0 \u2225 \u2225\nd\u03c0 , where \u03b8\u2217 is either\nthe fixed point of the standard TD equation V = \u03a0d\u00b5TV , or the ETD(0, \u03b2) fixed point of (3), with \u03b2 = \u03b3. We also show the optimal error \u2016\u03a0d\u03c0V \u2212 V\n\u03c0\u2016d\u03c0 achievable with these features. Note that, as observed by Kolter (2011), for certain behavior policies the bias of standard TD is infinite. This means that algorithms that converge to this fixed point, such as the GTD algorithm (Sutton et al., 2009), are hopeless in such cases. The ETD algorithm, on the other hand, has a bounded bias for all behavior policies.\n4 The Bias-Variance Trade-Off of ETD(0, \u03b2)\nFrom the results in Corollary 1, it is clear that increasing the decay rate \u03b2 decreases the bias bound. Indeed, for the case \u03b2 = 1 we obtain the importance sampling TD algorithm (Precup, Sutton, and Dasgupta, 2001), which is known to have a bias bound similar to on-policy TD. However, as recognized by Precup, Sutton, and Dasgupta (2001) and Sutton, Mahmood, and White (2015), the importance sampling ratio Ft suffers from a high variance, which increases with \u03b2. The quantityFt is important as it appears as a multiplicative factor in the definition of the ETD learning rule, so its amplitude directly impacts the stability of the algorithm. In fact, the asymptotic variance of Ft may be infinite, as we show in the following example:\nExample 2. Consider the same MDP given in Example 1, only now the behavior policy chooses Left or Right with probability 0.5, and the target policy chooses always Right. For ETD(0, \u03b2) with \u03b2 \u2208 [0, 1), we have that when St = Left then Ft = 1 (since \u03c1t\u22121 = 0). When St = Right, Ft may take several values depending on how many steps, \u03c4(t),\nwas the last transition from Left to Right, i.e. \u03c4(t) def = min{i \u2265 0 : St\u2212i = Left}. We can write this value as F \u03c4(t) where:\nF \u03c4 . =\n\u03c4 \u2211\ni=0\n(2\u03b2)i = (2\u03b2)\u03c4+1 \u2212 1\n2\u03b2 \u2212 1 ,\nif 2\u03b2 6= 1. Let us assume that 2\u03b2 > 1 since interesting cases happen when \u03b2 is close to 1.\nLet\u2019s compute Ft\u2019s average over time: Following the stationary distribution of the behavior policy, St = Left with probability 1/2. Now, conditioned on St = Right (which happens with probability 1/2), we have \u03c4(t) = i with probability 2\u2212i\u22121. Thus the average (over time) value of Ft is\nEFt = 1\n2\n\u221e \u2211\ni=0\n2\u2212i\u22121F i =\n\u2211\ni \u03b2 i+1 \u2212 1\n2(2\u03b2 \u2212 1) =\n1\n2(1\u2212 \u03b2) .\nThus Ft amplifies the TD update by a factor of 12(1\u2212\u03b2) in average. Unfortunately, the actual values of the (random variable) Ft does not concentrate around its expectation, and actually Ft does not even have a finite variance. Indeed the average (over time) of F 2t is\nEF 2t = 1\n4\n\u221e \u2211\ni=0\n2\u2212i(F i)2 =\n\u2211 i 2 \u2212i ( (2\u03b2)i+1 \u2212 1 )2\n4(2\u03b2 \u2212 1)2 = \u221e,\nas soon as 2\u03b22 \u2265 1.\nSo although ETD(0, \u03b2) converges almost surely (as shown by Yu 2015), the variance of the estimate may be infinite, which suggests a prohibitively slow convergence rate.\nIn the following proposition we characterize the dependence of the variance of Ft on \u03b2.\nProposition 1. Define the mismatch matrix P\u0303\u00b5,\u03c0 such that [P\u0303\u00b5,\u03c0]s\u0304s = \u2211 a p(s|s\u0304, a\u0304) \u03c02(a|s\u0304) \u00b5(a|s\u0304) and write \u03b1(\u00b5, \u03c0) the largest magnitude of its eigenvalues. Then for any \u03b2 < 1/ \u221a\n\u03b1(\u00b5, \u03c0) the average variance of Ft (conditioned on any state) is finite, and\nE\u00b5 [Var[Ft|St = s]] \u2264 \u03b22\n1\u2212 \u03b2\n 2 + (1 + \u03b2)\n\u2225 \u2225 \u2225P\u0303\u00b5,\u03c0 \u2225 \u2225 \u2225\n\u221e\n1\u2212 \u03b22 \u2225 \u2225\n\u2225P\u0303\u00b5,\u03c0\n\u2225 \u2225 \u2225\n\u221e\n\n ,\nwhere \u2225 \u2225\n\u2225 P\u0303\u00b5,\u03c0\n\u2225 \u2225 \u2225\n\u221e is the l\u221e-induced norm which is the maximum absolute row sum of\nthe matrix.\nProof. (Partial) Following the same derivation that Sutton, Mahmood, and White (2015)\nused to prove that f(s) = d\u00b5(s) limt\u2192\u221e E[Ft|St = s], we have\nq(s) . = d\u00b5(s) lim\nt\u2192\u221e E[F 2t |St = s]\n= d\u00b5(s) lim t\u2192\u221e\nE[(1 + \u03c1t\u22121\u03b2Ft\u22121) 2|St = s]\n= d\u00b5(s) lim t\u2192\u221e\nE[1 + 2\u03c1t\u22121\u03b2Ft\u22121 + \u03c1 2 t\u22121\u03b2 2F 2t\u22121|St = s].\nFor the first summand, we get d\u00b5(s). For the second summand, we get:\n2\u03b2d\u00b5(s) lim t\u2192\u221e\nE[\u03c1t\u22121Ft\u22121|St = s] = 2\u03b2 \u2211\ns\u0304\n[P\u03c0]s\u0304sf(s\u0304).\nThe third summand equals\n\u03b22 \u2211\ns\u0304,a\u0304\nd\u00b5(s\u0304)\u00b5(a\u0304|s\u0304)p(s|s\u0304, a\u0304) \u03c02(a\u0304|s\u0304)\n\u00b52(a\u0304|s\u0304) lim t\u2192\u221e\nE[F 2t\u22121|St\u22121 = s\u0304]\n= \u03b22 \u2211\ns\u0304,a\u0304\np(s|s\u0304, a\u0304) \u03c02(a\u0304|s\u0304)\n\u00b5(a\u0304|s\u0304) q(s\u0304) = \u03b22\n\u2211\ns\u0304\n[P\u0303\u00b5,\u03c0 ]s\u0304sq(s\u0304).\nHence q = d\u00b5 + 2\u03b2P\u22a4\u03c0 f + \u03b2 2P\u0303\u22a4\u00b5,\u03c0q. Thus for any \u03b2 < 1/\n\u221a\n\u03b1(\u00b5, \u03c0), all eigenval-\nues of the matrix \u03b22P\u0303\u22a4\u00b5,\u03c0 have magnitude smaller than 1, and the vector q has finite components. The rest of the proof is very technical and is given in Lemma 2 in the supplementary material.\nProposition 1 and Corollary 1 show that the decay rate \u03b2 acts as an implicit tradeoff parameter between the bias and variance in ETD. For large \u03b2, we have a low bias but suffer from a high variance (possibly infinite if \u03b2 \u2265 1/ \u221a\n\u03bb(\u00b5, \u03c0)), and vice versa for small \u03b2. Notice that for the on-policy case, \u03bb(\u00b5, \u03c0) = 1 thus for any \u03b2 < 1 the variance is finite.\nOriginally, ETD(0, \u03b2) was introduced with \u03b2 = \u03b3, and from our perspective, it may be seen as a specific choice for the bias-variance trade-off. However, there is no intrinsic reason to choose \u03b2 = \u03b3, and other choices may be preferred in practice, depending on the nature of the problem. In the following numerical example, we investigate the biasvariance dependence on \u03b2, and show that the optimal \u03b2 in term of mean-squared error may be quite different from \u03b3."}, {"heading": "4.1 Numerical Illustration", "text": "We revisit the 2-state MDP described in Section 3.1, with \u03b3 = 0.9, \u03b5 = 0.2 and p = 0.95. For these parameter settings, the error of standard TD is 42.55 (p was chosen to be close to a point of infinite bias for these parameters).\nIn Figure 2 we plot the mean-squared error \u2225 \u2225\u03a6\u22a4\u03b8\u2217 \u2212 V \u03c0 \u2225 \u2225\nd\u03c0 , where \u03b8\u2217 was ob-\ntained by running ETD(0, \u03b2) with a step size \u03b1 = 0.001 for 10, 000 iterations, and averaging the results over 10, 000 different runs.\nFirst of all, note that for all \u03b2, the error is smaller by two orders of magnitude than that of standard TD. Thus, algorithms that converge to the standard TD fixed point such as GTD Sutton et al. (2009) are significantly outperformed by ETD(0, \u03b2) in this case. Second, note the dependence of the error on \u03b2, demonstrating the bias-variance tradeoff discussed above. Finally, note that the minimal error is obtained for \u03b3 = 0.8, and is considerably smaller than that of the original ETD with \u03b2 = \u03b3 = 0.9."}, {"heading": "5 Contraction Property for ETD(\u03bb, \u03b2)", "text": "We now extend our results to incorporate eligibility traces, in the style of the ETD(\u03bb) algorithm (Sutton, Mahmood, and White, 2015), and show similar contraction properties and error bounds.\nThe ETD(\u03bb, \u03b2) algorithm iteratively updates the weight vector \u03b8 according to\n\u03b8t+1 := \u03b8t + \u03b1(Rt+1 + \u03b3\u03b8 \u22a4 t \u03d5t+1 \u2212 \u03b8 \u22a4 t \u03d5t)et\net = \u03c1t(\u03b3\u03bbet\u22121 +Mt\u03d5t), e\u22121 = 0\nMt = \u03bb+ (1\u2212 \u03bb)Ft\nFt = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1,\nwhere et is the eligibility trace (Sutton, Mahmood, and White, 2015). In this case, we define the emphatic weight vector m by\nm\u22a4 = d\u22a4\u00b5 (I \u2212 P \u03bb,\u03b2)\u22121, (4)\nwhere P a,b for some a, b \u2208 R denotes the following matrix:\nP a,b = I \u2212 (I \u2212 baP )\u22121(I \u2212 bP ).\nThe Bellman operator for general \u03bb and \u03b3 is given by:\nT (\u03bb)(V ) = (I \u2212 \u03b3\u03bbP )\u22121R + P\u03bb,\u03b3V, V \u2208 R|S|.\nFor \u03bb = 0 we have P\u03bb,\u03b2\u03c0 = \u03b2P , P \u03bb,\u03b3 \u03c0 = \u03b3P , and m = f so we recover the definitions of ETD(0, \u03b2). Recall that our goal is to estimate the value function V \u03c0. Thus, we would like to know how well the ETD(\u03bb, \u03b2) solution approximatesV \u03c0. Mahmood et al. (2015) show that, under suitable step-size conditions, ETD converges to some \u03b8\u2217 that is a solution of the projected fixed-point equation:\n\u03b8\u22a4\u03a6 = \u03a0mT (\u03bb)(\u03b8\u22a4\u03a6).\nIn their analysis, however, Mahmood et al. (2015) did not show how well the solution \u03a6\u22a4\u03b8\u2217 approximates V \u03c0. Next, we establish that the projected Bellman operator \u03a0mT (\u03bb) is a contraction. This result will then allow us to bound the error \u2225 \u2225\u03a6\u22a4\u03b8\u2217 \u2212 V \u03c0 \u2225 \u2225\nm .\nTheorem 2. \u03a0mT (\u03bb) is an \u03c9-contraction with respect to the Euclidean m-weighted norm where:\n\u03b2 \u2265 \u03b3 : \u03c9 =\n\u221a\n\u03b32(1 + \u03bb\u03b2)2(1\u2212 \u03bb) \u03b2(1 + \u03b3\u03bb)2(1\u2212 \u03bb\u03b2) ,\n\u03b2 \u2264 \u03b3 : \u03c9 =\n\u221a\n\u03b32(1\u2212 \u03b2\u03bb)(1 \u2212 \u03bb)\n\u03b2(1\u2212 \u03b3\u03bb)2 .\n(5)\nProof. (sketch) The proof is almost identical to the proof of Theorem 1, only now we cannot apply Jensen\u2019s inequality directly, since the rows of P\u03bb,\u03b2 do not sum to 1. However:\nP\u03bb,\u03b21 = ( I \u2212 (I \u2212 \u03b2\u03bbP )\u22121(I \u2212 \u03b2P ) ) 1 = \u03b61,\nwhere \u03b6 = \u03b2(1\u2212\u03bb)1\u2212\u03bb\u03b2 . Notice that each entry of P \u03bb,\u03b2 is positive. Therefore P\n\u03bb,\u03b2 \u03b6 will hold\nfor Jensen\u2019s inequality. Let M = diag(m), we have\n\u2016v\u2016 2 m \u2212\n1\n\u03b6\n\u2225 \u2225P\u03bb,\u03b2v \u2225 \u2225\n2 m = v\u22a4Mv \u2212 \u03b6v\u22a4\nP\u03bb,\u03b2\n\u03b6\n\u22a4\nM P\u03bb,\u03b2\n\u03b6 v\n\u2265(a) v\u22a4Mv \u2212 \u03b2v\u22a4diag(m\u22a4 P\u03bb,\u03b2\n\u03b6 )v\n= v\u22a4[M \u2212 diag(m\u22a4P\u03bb,\u03b2)]v\n= v\u22a4 [ diag ( m\u22a4(I \u2212 P\u03bb,\u03b2) )] v\n=(b) v\u22a4diag(d\u00b5)v = \u2016v\u2016 2 d\u00b5 ,\nwhere (a) follows from the Jensen inequality and (b) from Equation (4). Therefore:\n\u2016v\u2016 2 m \u2265\n1\n\u03b6\n\u2225 \u2225P\u03bb,\u03b2v \u2225 \u2225\n2 m + \u2016v\u2016 2 d\u00b5 \u2265 1\n\u03b6\n\u2225 \u2225P\u03bb,\u03b2v \u2225 \u2225\n2 m ,\nand: \u2225\n\u2225 \u2225T (\u03bb)v1 \u2212 T (\u03bb)v2\n\u2225 \u2225 \u2225 2\nm =\n\u2225 \u2225P\u03bb,\u03b3(v1 \u2212 v2) \u2225 \u2225\n2\nm\n(Case A: \u03b2 \u2265 \u03b3) \u2264\n\u2225 \u2225 \u2225 \u2225 \u03b3(1 + \u03b2\u03bb)\n\u03b2(1 + \u03b3\u03bb) P\u03bb,\u03b2(v1 \u2212 v2)\n\u2225 \u2225 \u2225 \u2225 2\nm\n\u2264 \u03b32(1 + \u03bb\u03b2)2(1\u2212 \u03bb)\n\u03b2(1 + \u03b3\u03bb)2(1 \u2212 \u03bb\u03b2) \u2016v1 \u2212 v2\u2016\n2 m ,\n(Case B: \u03b2 \u2264 \u03b3) \u2264\n\u2225 \u2225 \u2225 \u2225 \u03b3(1\u2212 \u03b2\u03bb)\n\u03b2(1 \u2212 \u03b3\u03bb) P\u03bb,\u03b2(v1 \u2212 v2)\n\u2225 \u2225 \u2225 \u2225 2\nm\n\u2264 \u03b32(1\u2212 \u03b2\u03bb)(1 \u2212 \u03bb)\n\u03b2(1 \u2212 \u03b3\u03bb)2 \u2016v1 \u2212 v2\u2016\n2 m .\nThe inequalities depending on the two cases originate from the fact that the two matrices P\u03bb,\u03b2 , P\u03bb,\u03b3 are polynomials of the same matrix P\u03c0 , and mathematical manipulation on the corresponding eigenvalues decomposition of (v1 \u2212 v2). The details are given in Lemma 3 of the supplementary material.\nNow, for a proper choice of \u03b2, the operator T (\u03bb) is a contraction, and since \u03a0m is a non-expansion in the m-weighted norm, \u03a0mT (\u03bb) is a contraction as well.\nIn Figure 3 we illustrate the dependence of the contraction moduli bound on \u03bb and \u03b2. In particular, for \u03bb \u2192 1, the contraction modulus diminishes to 0. Thus, for large enough \u03bb, a contraction can always be guaranteed (this can also be shown mathematically from the contraction results of Theorem 2). We remark that a similar result for standard TD(\u03bb) was established by Yu 2012. However, as is well-known (Bertsekas, 2012), increasing \u03bb also increases the variance of the algorithm, and we therefore obtain a bias-variance trade-off in \u03bb as well as \u03b2. Finally, note that for \u03b2 = \u03b3, the contraction modulus equals \u221a\n\u03b3(1\u2212\u03bb) 1\u2212\u03b3\u03bb , and that for \u03bb = 0 the result is the same as in Theorem\n1."}, {"heading": "6 Conclusion", "text": "In this work we unified several off-policy TD algorithms under the ETD(\u03bb, \u03b2) framework, which flexibly manages the bias and variance of the algorithm by controlling the decay-rate of the importance-sampling ratio. From this perspective, we showed that several different methods proposed in the literature are special instances of this biasvariance selection.\nOur main contribution is an error analysis of ETD(\u03bb, \u03b2) that quantifies the biasvariance trade-off. In particular, we showed that the recently proposed ETD algorithm of Sutton, Mahmood, and White (2015) has bounded bias for general behavior and target policies, and that by controlling the decay-rate in the ETD(\u03bb, \u03b2) algorithm, an improved performance may be obtained by reducing the variance of the algorithm while still maintaining a reasonable bias.\nPossible future extensions of our work includes finite-time bounds for off-policy ETD(\u03bb, \u03b2), an error propagation analysis of off-policy policy improvement, and solving the bias-variance trade-off adaptively from data."}, {"heading": "A Proof of Lemma 1", "text": "Notice that \u03ba obtains non-negative values since d\u00b5(s), f(s) \u2265 0. Now, if there is a state s visited by the target policy, but not the behavior policy, this means that d\u00b5(s) = 0, and that there is some t such that [d\u22a4\u00b5P t \u03c0](s) > 0, and by definition f(s) \u2265 [\u03b2 td\u22a4\u00b5P t \u03c0 ](s), so we can get \u03ba = 0. Next, we prove the upper bound on \u03ba. Notice that f(s) \u2265 0, and that \u2211\ns f(s) = 1/(1 \u2212 \u03b2). Hence, if d\u00b5 6= (1 \u2212 \u03b2)f , then there must exist some s such that d\u00b5(s) < (1\u2212 \u03b2)f(s) so \u03ba < 1 \u2212 \u03b2. Now, when d\u00b5 = d\u03c0, by definition d\u00b5 = (1\u2212 \u03b2)f and we obtain this upper bound."}, {"heading": "B Technical Part of Proposition 1", "text": "Lemma 2. The following is true:\n\u2211\ns\nd\u00b5(s) lim t\u2192\u221e\nVar[Ft|St = s] \u2264 \u03b22\n1\u2212 \u03b2\n 2 + (1 + \u03b2)\n\u2225 \u2225 \u2225P\u0303\u00b5,\u03c0 \u2225 \u2225 \u2225\n\u221e\n1\u2212 \u03b22 \u2225 \u2225\n\u2225P\u0303\u00b5,\u03c0\n\u2225 \u2225 \u2225\n\u221e\n\n .\nProof. Notice that:\nf\u22a4 = d\u22a4\u00b5 (I \u2212 \u03b2P\u03c0) \u22121 \u2265(cw) d\u22a4\u00b5 + \u03b2d \u22a4 \u00b5 P\u03c0,\nso: \u2211\ns\nd\u00b5(s) lim t\u2192\u221e\nVar[Ft|St = s] = q \u22a41 \u2212 f\u22a4D\u22121\u00b5 f\n\u2264(a) d\u22a4\u00b5 1 + 2\u03b2f \u22a4P\u03c01\n+ (d\u22a4\u00b5 + 2\u03b2f \u22a4P\u03c0)\u03b2 2P\u0303\u00b5,\u03c0(I \u2212 \u03b2 2P\u0303\u00b5,\u03c0) \u221211 \u2212 (d\u00b5 + \u03b2P \u22a4 \u03c0 d\u00b5) \u22a4D\u22121\u00b5 (d\u00b5 + \u03b2P \u22a4 \u03c0 d\u00b5)\n\u2264(b) (1 + 2\u03b2\n1\u2212 \u03b2 )\u2212 (1 + 2\u03b2)\n+ \u2225 \u2225\n\u2225(d\u22a4\u00b5 + 2\u03b2f \u22a4P\u03c0)\u03b2 2P\u0303\u00b5,\u03c0(I \u2212 \u03b2 2P\u0303\u00b5,\u03c0)\n\u22121 \u2225 \u2225\n\u2225\n1\n\u2264(c) 2\u03b22\n1\u2212 \u03b2 + \u03b22\n\u2225 \u2225d\u22a4\u00b5 + 2\u03b2f \u22a4P\u03c0 \u2225 \u2225\n1\n\u2225 \u2225 \u2225P\u0303\u00b5,\u03c0(I \u2212 \u03b2 2P\u0303\u00b5,\u03c0) \u22121 \u2225 \u2225 \u2225\n\u221e\n\u2264(d) \u03b22\n1\u2212 \u03b2\n 2 + (1 + \u03b2)\n\u2225 \u2225 \u2225P\u0303\u00b5,\u03c0 \u2225 \u2225 \u2225\n\u221e\n1\u2212 \u03b22 \u2225 \u2225\n\u2225P\u0303\u00b5,\u03c0\n\u2225 \u2225 \u2225\n\u221e\n\n .\nWhere (a) comes from the inequality on f , (b) also removes the negative summand \u03b22d\u22a4\u00b5 P\u03c0D \u22121 \u00b5 P \u22a4 \u03c0 d\u00b5, and swaps sum with l1 norm (all coordinates are non-negative), (c) and (d) are from the sub-multiplicative property of induced norms (the l\u221e norm originates from the transpose).\nC Norm Inequality between P\u03bb,\u03b2\u03c0 and P \u03bb,\u03b3 \u03c0\nLemma 3. If \u03b2 \u2265 \u03b3:\n\u2225 \u2225P\u03bb,\u03b3\u03c0 v \u2225 \u2225\n2 m \u2264\n\u2225 \u2225 \u2225 \u2225 \u03b3(1 + \u03b2\u03bb)\n\u03b2(1 + \u03b3\u03bb) P\u03bb,\u03b2\u03c0 v\n\u2225 \u2225 \u2225 \u2225 2\nm\n, (6)\nand if \u03b2 \u2264 \u03b3: \u2225\n\u2225P\u03bb,\u03b3\u03c0 v \u2225 \u2225\n2 m \u2264\n\u2225 \u2225 \u2225 \u2225 \u03b3(1\u2212 \u03b2\u03bb)\n\u03b2(1 \u2212 \u03b3\u03bb) P\u03bb,\u03b2\u03c0 v\n\u2225 \u2225 \u2225 \u2225 2\nm\n, (7)\nProof. Mark the orthonormal eigenvectors w.r.t. m, and corresponding eigenvalues of P\u03c0 by uj, tj respectively (tj may be a complex number, this decomposition exists over C almost surely). Notice that since P\u03bb,\u03b2\u03c0 , P \u03bb,\u03b3 \u03c0 are polynomials of P\u03c0 they have the same eigenvectors, with the eigenvalues l\u03b2j := \u03b2tj(1\u2212\u03bb) 1\u2212\u03b2\u03bbtj , l\u03b3j := \u03b3tj(1\u2212\u03bb) 1\u2212\u03b3\u03bbtj\ncorrespondingly. Hence, we can write the first norm as follows:\n\u2225 \u2225P\u03bb,\u03b3\u03c0 v \u2225 \u2225\n2 m =\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 P\u03bb,\u03b3\u03c0 \u2211\nj\n< uj, v > uj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nm\n=\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\n< uj, v > P \u03bb,\u03b3 \u03c0 uj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nm\n=\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj\n< uj, v > l \u03b3 j uj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nm\n= \u2211\nj\n\u2225 \u2225< uj , v > l \u03b3 j uj \u2225 \u2225 2\nm\n= \u2211\nj\n|< uj , v >| 2 \u2223 \u2223l\u03b3j \u2223 \u2223 2 \u2016uj\u2016 2 m .\n(8)\nAnd similarly for \u03b2: \u2225\n\u2225P\u03bb,\u03b2\u03c0 v \u2225 \u2225\n2 m = \u2211\nj\n|< uj , v >| 2 \u2223 \u2223 \u2223l \u03b2 j \u2223 \u2223 \u2223 2 \u2016uj\u2016 2 m . (9)\nSo if we can find a constant \u03b1 such that:\n\u2200j : \u2223 \u2223l\u03b3j \u2223 \u2223 2 \u2264 \u03b12\n\u2223 \u2223 \u2223l \u03b2 j \u2223 \u2223 \u2223 2 , (10)\nthen could swap \u2225 \u2225P\u03bb,\u03b3\u03c0 v \u2225 \u2225 2 m \u2264 \u2225 \u2225\u03b1P\u03bb,\u03b3\u03c0 v \u2225 \u2225 2 m . The expression we want to maximize is:\n\u2223 \u2223l\u03b3j \u2223 \u2223 2 \u2223 \u2223 \u2223l \u03b2 j \u2223 \u2223 \u2223 2 = \u03b32(1\u2212 \u03b2\u03bbtj)(1 \u2212 \u03b2\u03bbt \u2217 j ) \u03b22(1\u2212 \u03b3\u03bbtj)(1 \u2212 \u03b3\u03bbt\u2217j )\n= \u03b32(1\u2212 \u03b2\u03bbtj \u2212 \u03b2\u03bbt \u2217 j + \u03b2 2\u03bb2 |tj | 2 )\n\u03b22(1\u2212 \u03b3\u03bbtj \u2212 \u03b3\u03bbt\u2217j + \u03b3 2\u03bb2 |tj |\n2) .\n(11)\nTaking the derivative with respect to Re(tj), Im(tj), shows that there are no extrema points inside the ball |tj | \u2264 1 (we know the eigenvalues are inside this ball since they belong to a stochastic matrix), which means we can look at the boundary of this ball |tj | = 1 to find the maximum value. Since now we get dependence only on Re(tj), the maximum must be on tj = \u00b11:\nmax t:|tj |\u22641\n\u2223 \u2223l\u03b3j \u2223 \u2223 2 \u2223 \u2223 \u2223l \u03b2 j \u2223 \u2223 \u2223 2 = \u03b32(1\u00b1 \u03b2\u03bb)2 \u03b22(1 \u00b1 \u03b3\u03bb)2 , (12)\nwhere when \u03b2 \u2265 \u03b3 the plus is larger and vice versa."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "ICML.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Neuro-Dynamic Programming", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Projected equation methods for approximate solution of large linear systems", "author": ["D. Bertsekas", "H. Yu"], "venue": "Journal of Computational and Applied Mathematics 227(1):27\u201350.", "citeRegEx": "Bertsekas and Yu,? 2009", "shortCiteRegEx": "Bertsekas and Yu", "year": 2009}, {"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D. Bertsekas"], "venue": "Athena Scientific, 4th edition.", "citeRegEx": "Bertsekas,? 2012", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "The fixed points of off-policy TD", "author": ["J.Z. Kolter"], "venue": "NIPS.", "citeRegEx": "Kolter,? 2011", "shortCiteRegEx": "Kolter", "year": 2011}, {"title": "Finite-sample analysis of proximal gradient td algorithms", "author": ["B. Liu", "J. Liu", "M. Ghavamzadeh", "S. Mahadevan", "M. Petrik"], "venue": "UAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Emphatic TemporalDifference Learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "arXiv:1507.01569.", "citeRegEx": "Mahmood et al\\.,? 2015", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Error bounds for approximate policy iteration", "author": ["R. Munos"], "venue": "ICML.", "citeRegEx": "Munos,? 2003", "shortCiteRegEx": "Munos", "year": 2003}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "ICML.", "citeRegEx": "Precup et al\\.,? 2001", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A. Barto"], "venue": "Cambridge Univ Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "ICML.", "citeRegEx": "Sutton et al\\.,? 2009", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "arXiv:1503.04269.", "citeRegEx": "Sutton et al\\.,? 2015", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Least squares temporal difference methods: An analysis under general conditions", "author": ["H. Yu"], "venue": "SIAM Journal on Control and Optimization 50(6):3310\u20133343.", "citeRegEx": "Yu,? 2012", "shortCiteRegEx": "Yu", "year": 2012}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "COLT.", "citeRegEx": "Yu,? 2015", "shortCiteRegEx": "Yu", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "1 Introduction In Reinforcement Learning (RL; Sutton and Barto 1998), policy-evaluation refers to the problem of evaluating the value function \u2013 a mapping from states to their longterm discounted return under a given policy, using sampled observations of the system dynamics and reward.", "startOffset": 41, "endOffset": 68}, {"referenceID": 3, "context": "Instead, an approximate value-function is sought using various function-approximation techniques (a.k.a. approximate dynamic-programming; Bertsekas 2012).", "startOffset": 97, "endOffset": 153}, {"referenceID": 9, "context": "In this approach, the parameters of the value-function approximation are tuned using machine-learning inspired methods, often based on temporaldifferences (TD;Sutton and Barto 1998).", "startOffset": 155, "endOffset": 181}, {"referenceID": 1, "context": "the on-policy setting, TD methods are well understood, with classic convergence guarantees and approximation-error bounds, based on a contraction property of the projected Bellman operator underlying TD (Bertsekas and Tsitsiklis, 1996).", "startOffset": 203, "endOffset": 235}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995).", "startOffset": 167, "endOffset": 180}, {"referenceID": 13, "context": "Lately, Sutton, Mahmood, and White (2015) proposed the emphatic TD (ETD) algorithm: a modification of the TD idea, which converges off-policy (Yu, 2015), and has a reduced variance compared to IS-TD.", "startOffset": 142, "endOffset": 152}, {"referenceID": 12, "context": "Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al.", "startOffset": 159, "endOffset": 169}, {"referenceID": 10, "context": "Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al., 2009; Liu et al., 2015).", "startOffset": 193, "endOffset": 232}, {"referenceID": 5, "context": "Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al., 2009; Liu et al., 2015).", "startOffset": 193, "endOffset": 232}, {"referenceID": 2, "context": "These algorithms are guaranteed to converge, however, their asymptotic error can be bounded only when the target and behavior policies are similar (Bertsekas and Yu, 2009), or when their induced transition matrices satisfy a certain matrix-inequality suggested by Kolter (2011), which limits the discrepancy between the target and behavior policies.", "startOffset": 147, "endOffset": 171}, {"referenceID": 4, "context": "When these conditions are not satisfied, the error may be arbitrarily large (Kolter, 2011).", "startOffset": 76, "endOffset": 90}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995). The standard error-bounds may be shown to hold for an importance-sampling TD method (IS-TD), as proposed by Precup, Sutton, and Dasgupta (2001). However, this method is known to suffer from a high variance of its importance-sampling estimator, limiting its practicality.", "startOffset": 168, "endOffset": 326}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995). The standard error-bounds may be shown to hold for an importance-sampling TD method (IS-TD), as proposed by Precup, Sutton, and Dasgupta (2001). However, this method is known to suffer from a high variance of its importance-sampling estimator, limiting its practicality. Lately, Sutton, Mahmood, and White (2015) proposed the emphatic TD (ETD) algorithm: a modification of the TD idea, which converges off-policy (Yu, 2015), and has a reduced variance compared to IS-TD.", "startOffset": 168, "endOffset": 495}, {"referenceID": 0, "context": "For the off-policy case, however, standard TD methods no longer maintain this contraction property, the error bounds do not hold, and these methods might even diverge (Baird, 1995). The standard error-bounds may be shown to hold for an importance-sampling TD method (IS-TD), as proposed by Precup, Sutton, and Dasgupta (2001). However, this method is known to suffer from a high variance of its importance-sampling estimator, limiting its practicality. Lately, Sutton, Mahmood, and White (2015) proposed the emphatic TD (ETD) algorithm: a modification of the TD idea, which converges off-policy (Yu, 2015), and has a reduced variance compared to IS-TD. This variance reduction is achieved by incorporating a certain decay factor over the importance-sampling ratio. However, to the best of our knowledge, there are no results that bound the bias of ETD. Thus, while ETD is assured to converge, it is not known how good its limit actually is. In this paper, we propose the ETD(\u03bb, \u03b2) framework \u2013 a modification of the ETD(\u03bb) algorithm, where the decay rate of the importance-sampling ratio, \u03b2, is a free parameter, and \u03bb is the same bootstrapping parameter employed in TD(\u03bb) and ETD(\u03bb). By varying the decay rate, one can smoothly transition between the IS-TD algorithm, through ETD, to the standard TD algorithm. We investigate the bias of ETD(\u03bb, \u03b2), by studying the conditions under which its underlying projected Bellman operator is a contraction. We show that the original ETD possesses a contraction property, and present the first error bounds for ETD and ETD(\u03bb, \u03b2). In addition, our error bound reveals that the decay rate parameter balances between the bias and variance of the learning procedure. In particular, we show that selecting a decay equal to the discount factor as in the original ETD may be suboptimal in terms of the mean-squared error. The main contributions of this work are therefore a unification of several off-policy TD algorithms under the ETD(\u03bb, \u03b2) framework, and a new error analysis that reveals the bias-variance trade-off between them. Related Work: In recent years, several different off-policy policy-evaluation algorithms have been studied, such as importance-sampling based least-squares TD (Yu, 2012), and gradient-based TD (Sutton et al., 2009; Liu et al., 2015). These algorithms are guaranteed to converge, however, their asymptotic error can be bounded only when the target and behavior policies are similar (Bertsekas and Yu, 2009), or when their induced transition matrices satisfy a certain matrix-inequality suggested by Kolter (2011), which limits the discrepancy between the target and behavior policies.", "startOffset": 168, "endOffset": 2578}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8.", "startOffset": 35, "endOffset": 59}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t.", "startOffset": 36, "endOffset": 444}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t. Let T denote the Bellman operator for policy \u03c0, given by T (V ) . = R+ \u03b3PV, where R and P are the reward vector and transition matrix induced by policy \u03c0, and let \u03a6 denote a matrix whose columns are the feature vectors for all states. Let d\u03bc and d\u03c0 denote the stationary distributions over states induced by the policies \u03bc and \u03c0, respectively. For some d \u2208 R satisfying d > 0 element-wise, we denote by \u03a0d a projection to the subspace spanned by \u03c6(s) with respect to the d-weighted Euclideannorm. For \u03bb = 0, the ETD(0, \u03b2) (Sutton, Mahmood, and White, 2015) algorithm seeks to find a good approximation of the value function by iteratively updating the weight vector \u03b8: \u03b8t+1 = \u03b8t + \u03b1Ft\u03c1t(Rt+1 + \u03b3\u03b8 \u22a4 t \u03c6t+1 \u2212 \u03b8 \u22a4 t \u03c6t)\u03c6t Ft = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1, (1) where Ft is a decaying trace of the importance-sampling ratios, and \u03b2 \u2208 (0, 1) controls the decay rate. Remark 1. The algorithm of Sutton, Mahmood, and White (2015) selects the decay rate equal to the discount factor, i.", "startOffset": 36, "endOffset": 1420}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t. Let T denote the Bellman operator for policy \u03c0, given by T (V ) . = R+ \u03b3PV, where R and P are the reward vector and transition matrix induced by policy \u03c0, and let \u03a6 denote a matrix whose columns are the feature vectors for all states. Let d\u03bc and d\u03c0 denote the stationary distributions over states induced by the policies \u03bc and \u03c0, respectively. For some d \u2208 R satisfying d > 0 element-wise, we denote by \u03a0d a projection to the subspace spanned by \u03c6(s) with respect to the d-weighted Euclideannorm. For \u03bb = 0, the ETD(0, \u03b2) (Sutton, Mahmood, and White, 2015) algorithm seeks to find a good approximation of the value function by iteratively updating the weight vector \u03b8: \u03b8t+1 = \u03b8t + \u03b1Ft\u03c1t(Rt+1 + \u03b3\u03b8 \u22a4 t \u03c6t+1 \u2212 \u03b8 \u22a4 t \u03c6t)\u03c6t Ft = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1, (1) where Ft is a decaying trace of the importance-sampling ratios, and \u03b2 \u2208 (0, 1) controls the decay rate. Remark 1. The algorithm of Sutton, Mahmood, and White (2015) selects the decay rate equal to the discount factor, i.e., \u03b2 = \u03b3. Here, we provide more freedom in choosing the decay rate. As our analysis reveals, the decay rate controls a bias-variance trade-off of ETD, therefore this freedom is important. Moreover, we note that for \u03b2 = 0, we obtain the standard TD in an off-policy setting Yu (2012), and when \u03b2 = 1 we obtain the full importance-sampling TD algorithm Precup, Sutton, and Dasgupta (2001).", "startOffset": 36, "endOffset": 1759}, {"referenceID": 9, "context": "Linear temporal difference methods (Sutton and Barto, 1998) approximate the value function by V (s) \u2248 \u03b8\u03c6(s), where \u03c6(s) \u2208 R are state features, and \u03b8 \u2208 R are weights, and use sampling to find a suitable \u03b8. Let \u03bc denote a behavior policy that generates the samples s0, a0, s1, a1, . . . according to at \u223c \u03bc(\u00b7|st) and st+1 \u223c P (\u00b7|st, at). We denote by \u03c1t the ratio \u03c0(at|st)/\u03bc(at|st), and we assume, similarly to Sutton, Mahmood, and White (2015), that \u03bc and \u03c0 are such that \u03c1t is well-defined1 for all t. Let T denote the Bellman operator for policy \u03c0, given by T (V ) . = R+ \u03b3PV, where R and P are the reward vector and transition matrix induced by policy \u03c0, and let \u03a6 denote a matrix whose columns are the feature vectors for all states. Let d\u03bc and d\u03c0 denote the stationary distributions over states induced by the policies \u03bc and \u03c0, respectively. For some d \u2208 R satisfying d > 0 element-wise, we denote by \u03a0d a projection to the subspace spanned by \u03c6(s) with respect to the d-weighted Euclideannorm. For \u03bb = 0, the ETD(0, \u03b2) (Sutton, Mahmood, and White, 2015) algorithm seeks to find a good approximation of the value function by iteratively updating the weight vector \u03b8: \u03b8t+1 = \u03b8t + \u03b1Ft\u03c1t(Rt+1 + \u03b3\u03b8 \u22a4 t \u03c6t+1 \u2212 \u03b8 \u22a4 t \u03c6t)\u03c6t Ft = \u03b2\u03c1t\u22121Ft\u22121 + 1, F0 = 1, (1) where Ft is a decaying trace of the importance-sampling ratios, and \u03b2 \u2208 (0, 1) controls the decay rate. Remark 1. The algorithm of Sutton, Mahmood, and White (2015) selects the decay rate equal to the discount factor, i.e., \u03b2 = \u03b3. Here, we provide more freedom in choosing the decay rate. As our analysis reveals, the decay rate controls a bias-variance trade-off of ETD, therefore this freedom is important. Moreover, we note that for \u03b2 = 0, we obtain the standard TD in an off-policy setting Yu (2012), and when \u03b2 = 1 we obtain the full importance-sampling TD algorithm Precup, Sutton, and Dasgupta (2001). 1Namely, if \u03bc(a|s) = 0 then \u03c0(a|s) = 0 for all s \u2208 S.", "startOffset": 36, "endOffset": 1863}, {"referenceID": 13, "context": "(2) It can be shown (Sutton, Mahmood, and White, 2015; Yu, 2015), that ETD(0, \u03b2) converges to \u03b8 - a solution of the following projected fixed point equation: V = \u03a0fTV, V \u2208 R .", "startOffset": 20, "endOffset": 64}, {"referenceID": 1, "context": "(3) For the fixed point equation (3), a contraction property of \u03a0fT is important for guaranteeing both a unique solution, and a bias bound (Bertsekas and Tsitsiklis, 1996).", "startOffset": 139, "endOffset": 171}, {"referenceID": 1, "context": "It is well known that T is a \u03b3-contraction with respect to the d\u03c0-weighted Euclidean norm (Bertsekas and Tsitsiklis, 1996), and by definition \u03a0f is a non-expansion in f norm, however, it is not immediate that the composed operator \u03a0fT is a contraction in any norm.", "startOffset": 90, "endOffset": 122}, {"referenceID": 0, "context": "Indeed, for the TD(0) algorithm (Sutton and Barto 1998; corresponding to the \u03b2 = 0 case in our setting), a similar representation as a projected Bellman operator holds, but it may be shown that in the off-policy setting the algorithm might diverge (Baird, 1995).", "startOffset": 248, "endOffset": 261}, {"referenceID": 1, "context": "Since \u03a0f is a non-expansion in the f -weighted norm (Bertsekas and Tsitsiklis, 1996), \u03a0fT is a \u221a \u03b3 \u03b2 (1 \u2212 \u03ba)-contraction as well.", "startOffset": 52, "endOffset": 84}, {"referenceID": 1, "context": "In this case, the contraction modulus in Theorem 1 is \u03b3, similar to the result for on-policy TD Bertsekas and Tsitsiklis (1996). We remark that Kolter (2011) also used a measure of discrepancy between the behavior and the target policy to bound the TD-error.", "startOffset": 96, "endOffset": 128}, {"referenceID": 1, "context": "In this case, the contraction modulus in Theorem 1 is \u03b3, similar to the result for on-policy TD Bertsekas and Tsitsiklis (1996). We remark that Kolter (2011) also used a measure of discrepancy between the behavior and the target policy to bound the TD-error.", "startOffset": 96, "endOffset": 158}, {"referenceID": 1, "context": "In this case, the contraction modulus in Theorem 1 is \u03b3, similar to the result for on-policy TD Bertsekas and Tsitsiklis (1996). We remark that Kolter (2011) also used a measure of discrepancy between the behavior and the target policy to bound the TD-error. However, Kolter (2011) considered the standard TD algorithm, for which a contraction could be guaranteed only for a class of behavior policies that satisfy a certain matrix inequality criterion.", "startOffset": 96, "endOffset": 282}, {"referenceID": 1, "context": "9 of Bertsekas and Tsitsiklis (1996): Corollary 1.", "startOffset": 5, "endOffset": 37}, {"referenceID": 7, "context": "Such an analysis may proceed similarly to that in Munos (2003) for the on-policy case.", "startOffset": 50, "endOffset": 63}, {"referenceID": 4, "context": "Consider the 2-state MDP example of Kolter (2011), with transition matrix P = (1/2)1 (where 1 is an all 1 matrix), discount factor \u03b3 = 0.", "startOffset": 36, "endOffset": 50}, {"referenceID": 10, "context": "This means that algorithms that converge to this fixed point, such as the GTD algorithm (Sutton et al., 2009), are hopeless in such cases.", "startOffset": 88, "endOffset": 109}, {"referenceID": 4, "context": "Note that, as observed by Kolter (2011), for certain behavior policies the bias of standard TD is infinite.", "startOffset": 26, "endOffset": 40}, {"referenceID": 4, "context": "Note that, as observed by Kolter (2011), for certain behavior policies the bias of standard TD is infinite. This means that algorithms that converge to this fixed point, such as the GTD algorithm (Sutton et al., 2009), are hopeless in such cases. The ETD algorithm, on the other hand, has a bounded bias for all behavior policies. 4 The Bias-Variance Trade-Off of ETD(0, \u03b2) From the results in Corollary 1, it is clear that increasing the decay rate \u03b2 decreases the bias bound. Indeed, for the case \u03b2 = 1 we obtain the importance sampling TD algorithm (Precup, Sutton, and Dasgupta, 2001), which is known to have a bias bound similar to on-policy TD. However, as recognized by Precup, Sutton, and Dasgupta (2001) and Sutton, Mahmood, and White (2015), the importance sampling ratio Ft suffers from a high variance, which increases with \u03b2.", "startOffset": 26, "endOffset": 713}, {"referenceID": 4, "context": "Note that, as observed by Kolter (2011), for certain behavior policies the bias of standard TD is infinite. This means that algorithms that converge to this fixed point, such as the GTD algorithm (Sutton et al., 2009), are hopeless in such cases. The ETD algorithm, on the other hand, has a bounded bias for all behavior policies. 4 The Bias-Variance Trade-Off of ETD(0, \u03b2) From the results in Corollary 1, it is clear that increasing the decay rate \u03b2 decreases the bias bound. Indeed, for the case \u03b2 = 1 we obtain the importance sampling TD algorithm (Precup, Sutton, and Dasgupta, 2001), which is known to have a bias bound similar to on-policy TD. However, as recognized by Precup, Sutton, and Dasgupta (2001) and Sutton, Mahmood, and White (2015), the importance sampling ratio Ft suffers from a high variance, which increases with \u03b2.", "startOffset": 26, "endOffset": 751}, {"referenceID": 10, "context": "Thus, algorithms that converge to the standard TD fixed point such as GTD Sutton et al. (2009) are significantly outperformed by ETD(0, \u03b2) in this case.", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "Mahmood et al. (2015) show that, under suitable step-size conditions, ETD converges to some \u03b8 that is a solution of the projected fixed-point equation: \u03b8\u03a6 = \u03a0mT (\u03b8\u03a6).", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Mahmood et al. (2015) show that, under suitable step-size conditions, ETD converges to some \u03b8 that is a solution of the projected fixed-point equation: \u03b8\u03a6 = \u03a0mT (\u03b8\u03a6). In their analysis, however, Mahmood et al. (2015) did not show how well the solution \u03a6\u03b8 approximates V .", "startOffset": 0, "endOffset": 217}, {"referenceID": 3, "context": "However, as is well-known (Bertsekas, 2012), increasing \u03bb also increases the variance of the algorithm, and we therefore obtain a bias-variance trade-off in \u03bb as well as \u03b2.", "startOffset": 26, "endOffset": 43}, {"referenceID": 3, "context": "However, as is well-known (Bertsekas, 2012), increasing \u03bb also increases the variance of the algorithm, and we therefore obtain a bias-variance trade-off in \u03bb as well as \u03b2. Finally, note that for \u03b2 = \u03b3, the contraction modulus equals \u221a \u03b3(1\u2212\u03bb) 1\u2212\u03b3\u03bb , and that for \u03bb = 0 the result is the same as in Theorem 1. 6 Conclusion In this work we unified several off-policy TD algorithms under the ETD(\u03bb, \u03b2) framework, which flexibly manages the bias and variance of the algorithm by controlling the decay-rate of the importance-sampling ratio. From this perspective, we showed that several different methods proposed in the literature are special instances of this biasvariance selection. Our main contribution is an error analysis of ETD(\u03bb, \u03b2) that quantifies the biasvariance trade-off. In particular, we showed that the recently proposed ETD algorithm of Sutton, Mahmood, and White (2015) has bounded bias for general behavior and target policies, and that by controlling the decay-rate in the ETD(\u03bb, \u03b2) algorithm, an improved performance may be obtained by reducing the variance of the algorithm while still maintaining a reasonable bias.", "startOffset": 27, "endOffset": 884}], "year": 2015, "abstractText": "We consider the off-policy evaluation problem in Markov decision processes with function approximation. We propose a generalization of the recently introduced emphatic temporal differences (ETD) algorithm (Sutton, Mahmood, and White, 2015), which encompasses the original ETD(\u03bb), as well as several other off-policy evaluation algorithms as special cases. We call this framework ETD(\u03bb, \u03b2), where our introduced parameter \u03b2 controls the decay rate of an importancesampling term. We study conditions under which the projected fixedpoint equation underlying ETD(\u03bb, \u03b2) involves a contraction operator, allowing us to present the first asymptotic error bounds (bias) for ETD(\u03bb, \u03b2). Our results show that the original ETD algorithm always involves a contraction operator, and its bias is bounded. Moreover, by controlling \u03b2, our proposed generalization allows trading-off bias for variance reduction, thereby achieving a lower total error.", "creator": "LaTeX with hyperref package"}}}