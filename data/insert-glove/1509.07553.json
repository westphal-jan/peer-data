{"id": "1509.07553", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Linear-Time Learning on Distributions with Approximate Kernel Embeddings", "abstract": "adelita Many interesting machine gebel learning problems homebuyer are best posed pinnacle by tizzy considering neverfail instances quiros that open-topped are distributions, or sample sets drachman drawn from distributions. mahlerian Previous eckfeldt work devoted albany to hennagan machine stepashin learning afp tasks torturously with vliegen distributional inputs has done rohp so through pairwise kernel archil evaluations between pdfs (or sample sets ). krin While mlitt such an tiraspol approach is wind fine cheerier for smaller yuthasak datasets, 1.3970 the bachir computation wissenschaft of winamp an $ N \\ times N $ documentation Gram bouygues matrix is underpinnings prohibitive teacup in large beholder datasets. aphelion Recent scalable 118.79 estimators ekka that work over altuna pdfs levell have ulusoy done so entreaty only chalem with dunois kernels that vallbona use Euclidean 5,723 metrics, retyped like sli the $ udb L_2 $ distance. lgr However, run-away there are feron a lancia myriad doje of 0-10-1 other useful prowess metrics kerrigan available, such as total comparision variation, 5,285 Hellinger 527,000 distance, and miklikova the Jensen - Shannon divergence. This work kawerau develops sickout the grandfield first viesturs random practice-oriented features spithill for clarki pdfs gandhian whose dot schulich product approximates susukino kernels using these bhera non - massocca Euclidean metrics, waasland-beveren allowing estimators claussen using such chuar kernels huia to mccaugheys scale to large datasets by working in mosisili a ohka primal space, scoffs without guitar-like computing coma large 8.1 Gram inaba matrices. We provide adjunction an analysis txe of the laksanawisit approximation error in using renew our proposed mormaers random pronounced features asymptotic and show ppk empirically the hipo quality of nagma our vermeersch approximation razorsharks both stopwatch in estimating a Gram algerian matrix and shihmen in kingaroy solving learning baureihe tasks guajardo in schaal real - apostoli world 109.79 and mausoleum synthetic data.", "histories": [["v1", "Thu, 24 Sep 2015 22:26:02 GMT  (593kb,D)", "http://arxiv.org/abs/1509.07553v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dougal j sutherland", "junier b oliva", "barnab\u00e1s p\u00f3czos", "jeff g schneider"], "accepted": true, "id": "1509.07553"}, "pdf": {"name": "1509.07553.pdf", "metadata": {"source": "CRF", "title": "Linear-time Learning on Distributions with Approximate Kernel Embeddings", "authors": ["Dougal J. Sutherland", "Junier B. Oliva", "Barnab\u00e1s P\u00f3czos", "Jeff Schneider"], "emails": ["dsutherl@cs.cmu.edu", "joliva@cs.cmu.edu", "bapoczos@cs.cmu.edu", "schneide@cs.cmu.edu"], "sections": [{"heading": "Introduction", "text": "As machine learning matures, focus has shifted towards datasets with richer, more complex instances. For example, a great deal of effort has been devoted to learning functions on vectors of a large fixed dimension. While complex static vector instances are useful in a myriad of applications, many machine learning problems are more naturally posed by considering instances that are distributions, or sets drawn from distributions. Political scientists can learn a function from community demographics to vote percentages to understand who supports a candidate (Flaxman, Wang, and Smola 2015). The mass of dark matter halos can be inferred from the velocity of galaxies in a cluster (Ntampaka et al. 2015). Expensive expectation propagation messages can be sped up by learning a \u201cjust-in-time\u201d regression model (Jitkrittum et al. 2015). All of these applications are aided by working directly over sets drawn from the distribution of interest, rather than having to develop a per-problem ad-hoc set of summary statistics.\n\u2217These two authors contributed equally.\nDistributions are inherently infinite-dimensional objects, since in general they require an infinite number of parameters for their exact representation. Hence, it is not immediate how to extend traditional finite vector technique machine learning techniques to distributional instances. However, recent work has provided various approaches for dealing with distributional data in a nonparametric fashion. For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing (Po\u0301czos et al. 2012a; Oliva, Po\u0301czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; Po\u0301czos et al. 2012b). A major shortcoming of both approaches is that they require computing N kernel evaluations per prediction, where N is the number of training instances in a dataset. Often, this implies that one must compute a N \u00d7 N Gram matrix of pairwise kernel evaluations. Such approaches fail to scale to datasets where the number of instances N is very large. Another shortcoming of these approaches is that they are often based on Euclidean metrics, either working over a linear kernel, or one based on the L2 distance over distributions. While such kernels are useful in certain applications, better performance can sometimes be obtained by considering non-Euclidean based kernels. To this end, Po\u0301czos et al. (2012b) use a kernel based on Re\u0301nyi divergences; however, this kernel is not positive semi-definite (PSD), leading to even higher computational cost and other practical issues.\nThis work addresses these major shortcomings by developing an embedding of random features for distributions. The dot product of the random features for two distributions will approximate kernels based on various distances between densities (see Figure 1). With this technique, we can approximate kernels based on total variation, Hellinger, and Jensen-Shannon divergences, among others. Since there is then no need to compute a Gram matrix, one will be able to use these kernels while still scaling to datasets with a"}, {"heading": "K( , ) \u2248 z( )Tz( )", "text": "with random features of sample sets \u03c7i iid\u223c pi, \u03c7j iid\u223c pj .\nlarge number of instances using primal-space techniques. We provide an approximation bound for the embeddings, and demonstrate the efficacy of the embeddings on both realworld and synthetic data. To the best of our knowledge, this work provides the first non-discretized embedding for nonL2 kernels for probability density functions."}, {"heading": "Related Work", "text": "The two main lines of relevant research are the development of kernels on probability distributions and explicit approximate embeddings for scalable kernel learning.\nLearning on distributions In computer vision, the popular \u201cbag of words\u201d model (Leung and Malik 2001) represents a distribution by quantizing it onto codewords (usually by running k-means on all, or many, of the input points from all sets), then compares those histograms with some kernel (often exponentiated \u03c72).\nAnother approach estimates a distance between distributions, often the L2 distance or Kullback-Leibler (KL) divergence, parametrically (Jaakkola and Haussler 1998; Moreno, Ho, and Vasconcelos 2003; Jebara, Kondor, and Howard 2004) or nonparametrically (Sricharan, Wei, and Hero 2013; Krishnamurthy et al. 2014). The distance can then be used in kernel smoothing (Po\u0301czos et al. 2012a; Oliva, Po\u0301czos, and Schneider 2013) or Mercer kernels (Moreno, Ho, and Vasconcelos 2003; Kondor and Jebara 2003; Jebara, Kondor, and Howard 2004; Po\u0301czos et al. 2012b).\nThese approaches can be powerful, but usually require computing an N \u00d7 N matrix of kernel evaluations, which can be infeasible for large datasets. Using these distances in Mercer kernels faces an additional challenge, which is that the estimated Gram matrix may not be PSD, due to estimation error or because some divergences in fact do not induce a PSD kernel. In general this must be remedied by altering the Gram matrix a \u201cnearby\u201d PSD one. Typical approaches involve eigendecomposing the Gram matrix, which usually costs O(N3) computation and also presents challenges for traditional inductive learning, where the test points are not known at training time (Chen et al. 2009).\nOne way to alleviate the scaling problem is the Nystro\u0308m extension (Williams and Seeger 2001), in which some columns of the Gram matrix are used to estimate the remainder. In practice, one frequently must compute many columns, and methods to make the result PSD are known only for mildly-indefinite kernels (Belongie et al. 2002).\nAnother approach is to represent a distribution by its mean RKHS embedding under some kernel k. The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD) (Gretton et al. 2009; Muandet et al. 2012; Szabo\u0301 et al. 2015). When k is the common RBF kernel, the MMK estimate is proportional to an L2 inner product between Gaussian kernel density estimates.\nApproximate embeddings Recent interest in approximate kernel embeddings was spurred by the \u201crandom kitchen sink\u201d (RKS) embedding (Rahimi and Recht 2007),\nwhich approximates shift-invariant kernelsK on R` by sampling their Fourier transform.\nA related line of work considers additive kernels, of the form K(x, y) = \u2211` j=1 \u03ba(xj , yj), usually defined on R`\u22650 (e.g. histograms). Maji and Berg (2009) construct an embedding for the intersection kernel \u2211` j=1 min(xj , yj) via step functions. Vedaldi and Zisserman (2010) consider any homogeneous \u03ba, so that \u03ba(tx, ty) = t \u03ba(x, y), which also allows them to embed histogram kernels such as the additive \u03c72 kernel and Jensen-Shannon divergence. Their embedding uses the same fundamental result of Fuglede (2005) as ours; we expand to the continuous rather than the discrete case. Vempati et al. (2010) later apply RKS embeddings to obtain generalized RBF kernels (1).\nFor embeddings of kernels on input spaces other than R`, the RKS embedding extends naturally to locally compact abelian groups (Li, Ionescu, and Sminchisescu 2010). Oliva et al. (2014) embedded an estimate of the L2 distance between continuous densities via orthonormal basis functions. An embedding for the base kernel k also gives a simple embedding for the mean map kernel (Flaxman, Wang, and Smola 2015; Jitkrittum et al. 2015; Lopez-Paz et al. 2015; Sutherland and Schneider 2015).\nEmbedding Information Theoretic Kernels For a broad class of distributional distances d, including many common and useful information theoretic divergences, we consider generalized RBF kernels of the form\nK(p, q) = exp ( \u2212 1\n2\u03c32 d2(p, q)\n) . (1)\nWe will construct features z(A(\u00b7)) such that K(p, q) \u2248 z(A(p))Tz(A(q)) as follows:\n1. We define a random function \u03c8 such that d(p, q) \u2248 \u2016\u03c8(p) \u2212 \u03c8(q)\u2016, where \u03c8(p) is a function from [0, 1]` to R2M . Thus the metric space of densities with distance d is approximately embedded into the metric space of 2M - dimensional L2 functions.\n2. We use orthonormal basis functions to approximately embed smooth L2 functions into finite vectors in R|V |. Combined with the previous step, we obtain features A(p) \u2208 R2M |V | such that d is approximated by Euclidean distances between the A features.\n3. We use the RKS embedding z(\u00b7) so that inner products between z(A(\u00b7)) features, in RD, approximate K(p, q).\nWe can thus approximate the powerful kernel K without needing to compute an expensive N \u00d7N Gram matrix."}, {"heading": "Homogeneous Density Distances (HDDs)", "text": "We consider kernels based on metrics which we term homogeneous density distances (HDDs):\nd2(p, q) = \u222b [0,1]` \u03ba(p(x), q(x)) dx, (2)\nwhere \u03ba(x, y) : R+ \u00d7 R+ \u2192 R+ is a negative-type kernel, i.e. a squared Hilbertian metric, and \u03ba(tx, ty) = t\u03ba(x, y)\nfor all t > 0. Table 1 shows a few important instances. Note that we assume the support of the distributions is contained within [0, 1]`.\nWe then use these distances in a generalized RBF kernel (1). d is a Hilbertian metric (Fuglede 2005), so K is positive definite (Haasdonk and Bahlmann 2004). Note we use the\u221a\nTV metric, even though TV is itself a metric."}, {"heading": "Embedding HDDs into L2", "text": "Fuglede (2005) shows that \u03ba corresponds to a bounded measure \u00b5(\u03bb), as in Table 1, with\n\u03ba(x, y) = \u222b R\u22650 |x 1 2+i\u03bb \u2212 y 1 2+i\u03bb|2 d\u00b5(\u03bb). (3)\nLet Z = \u00b5(R\u22650) and c\u03bb = (\u2212 12 + i\u03bb)/( 1 2 + i\u03bb); then\n\u03ba(x, y) = E\u03bb\u223c \u00b5Z |g\u03bb(x)\u2212 g\u03bb(y)| 2\nwhere g\u03bb(x) = \u221a Zc\u03bb(x 1 2+i\u03bb \u2212 1).\nWe can approximate the expectation with an empirical mean. Let \u03bbj iid\u223c \u00b5Z for j \u2208 {1, . . . ,M}; then\n\u03ba(x, y) \u2248 1 M M\u2211 j=1 |g\u03bbj (x)\u2212 g\u03bbj (y)|2.\nHence, using R, I to denote the real and imaginary parts:\nd2(p, q) = \u222b [0,1]` \u03ba(p(x), q(x)) dx\n= \u222b [0,1]` E\u03bb\u223c \u00b5Z |g\u03bb(p(x))\u2212 g\u03bb(q(x))| 2 dx\n\u2248 1 M M\u2211 j=1 \u222b [0,1]` ( ( R(g\u03bbj (p(x)))\u2212R(g\u03bbj (q(x))) )2 + ( I(g\u03bbj (p(x)))\u2212 I(g\u03bbj (q(x))) )2 ) dx\n= \u2016\u03c8(p)\u2212 \u03c8(q)\u20162, (4) where [\u03c8(p)](x) is given by\n1\u221a M\n( pR\u03bb1(x), . . . , p R \u03bbM (x), p I \u03bb1(x), . . . , p I \u03bbM (x) ) ,\ndefining pR\u03bbj (x) = R(g\u03bbj (p(x))), p I \u03bbj (x) = I(g\u03bbj (p(x))). Hence, the HDD between densities p and q is approximately the L2 distance from \u03c8(p) to \u03c8(q), where \u03c8 maps a function f : [0, 1]` 7\u2192 R to a vector-valued function \u03c8(f) : [0, 1]` 7\u2192 R2M of \u03bb functions. M can typically be quite small, since the kernel it approximates is one-dimensional."}, {"heading": "Finite Embeddings of L2", "text": "If densities p and q are smooth, then the L2 metric between the p\u03bb and q\u03bb functions may be well approximated using projections to basis functions. Suppose that {\u03d5i}i\u2208Z is an orthonormal basis for L2([0, 1]); then we can construct an orthonormal basis for L2([0, 1]`) by the tensor product:\n{\u03d5\u03b1}\u03b1\u2208Z` where \u03d5\u03b1(x) = \u220f\u0300 i=1 \u03d5\u03b1i(xi), x \u2208 [0, 1]`,\n\u2200f \u2208 L2([0, 1]`), f(x) = \u2211 \u03b1\u2208Z` a\u03b1(f)\u03d5\u03b1(x)\nand a\u03b1(f) = \u3008\u03d5\u03b1, f\u3009 = \u222b [0,1]`\n\u03d5\u03b1(t) f(t) dt \u2208 R. Let V \u2282 Z` be an appropriately chosen finite set of indices. If f, f \u2032 \u2208 L2([0, 1]\n`) are smooth and ~a(f) = (a\u03b11(f), . . . , a\u03b1|V |(f)), then \u2016f\u2212f \u2032\u20162 \u2248 \u2016~a(f)\u2212~a(f \u2032)\u20162. Thus we can approximate d2 as the squared distance between finite vectors:\nd2(p, q) \u2248 \u2016\u03c8(p)\u2212 \u03c8(q)\u20162\n\u2248 1 M M\u2211 j=1 \u2016~a(pR\u03bbj )\u2212 ~a(q R \u03bbj )\u2016 2 + \u2016~a(pI\u03bbj )\u2212 ~a(q I \u03bbj )\u2016 2\n= \u2016A(p)\u2212A(q)\u20162 (5)\nwhere A : L2([0, 1]`) \u2192 R2M |V | concatenates the ~a features for each \u03bb function. That is, A(p) is given by\n1\u221a M\n( ~a(pR\u03bb1), . . . ,~a(p R \u03bbM ),~a(p I \u03bb1), . . . ,~a(p I \u03bbM ) ) . (6)\nWe will discuss how to estimate ~a(pR\u03bb ), ~a(p I \u03bb) shortly."}, {"heading": "Embedding RBF Kernels into RD", "text": "The A features approximate the HDD (2) in R2M |V |; thus applying the RKS embedding (Rahimi and Recht 2007) to the A features will approximate our generalized RBF kernel (1). The RKS embedding is1 z : Rm \u2192 RD such that for fixed {\u03c9i}D/2i=1 iid\u223c N (0, \u03c3\u22122Im) and for each x, y \u2208 Rm:\nz(x)Tz(y) \u2248 exp ( \u2212 12\u03c32 \u2016x\u2212 y\u2016 2 ) , where\nz(x) = \u221a\n2 D ( sin(\u03c9T1 x), cos(\u03c9 T 1 x), . . . ) . (7)\nThus we can approximate the HDD kernel (1) as:\nK(p, q) = exp ( \u2212 1\n2\u03c32 d2(p, q) ) \u2248 exp ( \u2212 1\n2\u03c32 \u2016A(p)\u2212A(q)\u20162 ) \u2248 z(A(p))Tz(A(q)). (8)\n1There are two versions of the embedding in common use, but this one is preferred (Sutherland and Schneider 2015)."}, {"heading": "Finite Sample Estimates", "text": "Our final approximation for HDD kernels (8) depends on integrals of densities p and q. In practice, we are unlikely to directly observe an input density, but even given a pdf p, the integrals that make up the elements of A(p) are not readily computable. We thus first estimate the density as p\u0302, e.g. with kernel density estimation (KDE), and estimateA(p) asA(p\u0302). Recall that the elements of A(p\u0302) are:\na\u03b1(p\u0302 S \u03bbj ) = \u222b [0,1]` \u03d5\u03b1(t) p\u0302 S \u03bbj (t) dt (9)\nwhere j \u2208 {1, . . . ,M}, S \u2208 {R, I}, \u03b1 \u2208 V . In lower dimensions, we can approximate (9) with simple Monte Carlo numerical integration. Choosing {ui}nei=1 iid\u223c Unif([0, 1]`):\na\u0302\u03b1(p\u0302 S \u03bbj ) =\n1\nne ne\u2211 i=1 \u03d5\u03b1(ui) p\u0302 S \u03bbj (ui), (10)\nobtaining A\u0302(p\u0302). We note that in high dimensions, one may use any high-dimensional density estimation scheme (e.g. Lafferty, Liu, and Wasserman 2012) and estimate (9) with MCMC techniques (e.g. Hoffman and Gelman 2014)."}, {"heading": "Summary and Complexity", "text": "The algorithm for computing features {z(A(pi))}Ni=1 for a set of distributions {pi}Ni=1, given sample sets {\u03c7i}Ni=1 where \u03c7i = {X(i)j \u2208 [0, 1]`} ni j=1 iid\u223c pi, is thus:\n1. Draw M scalars \u03bbj iid\u223c \u00b5Z and D/2 vectors \u03c9r iid\u223c N (0, \u03c3\u22122I2M |V |), in O(M |V |D) time.\n2. For each of the N input distributions i: (a) Compute a kernel density estimate from \u03c7i, p\u0302i(uj) for\neach uj in (10), in O(nine) time. (b) Compute A\u0302(p\u0302i) using a numerical integration estimate\nas in (10), in O(M |V |ne) time. (c) Get the RKS features, z(A\u0302(p\u0302i)), in O(M |V |D) time.\nSupposing each ni n, this process takes a total of O (Nnne +NM |V |ne +NM |V |D) time. Taking |V | to be asymptotically O(n), ne = O(D), and M = O(1) for simplicity, this is O(NnD) time, compared to O(N2n log n+N3) for the methods of Po\u0301czos et al. (2012b) and O(N2n2) for Muandet et al. (2012)."}, {"heading": "Theory", "text": "We bound Pr (\u2223\u2223\u2223K(p, q)\u2212 z(A\u0302(p\u0302))Tz(A\u0302(q\u0302))\u2223\u2223\u2223 \u2265 \u03b5) for two fixed densities p and q by considering each source of error: kernel density estimation (\u03b5KDE); approximating \u00b5(\u03bb) with M samples (\u03b5\u03bb); truncating the tails of the projection coefficients (\u03b5tail); Monte Carlo integration (\u03b5int); and the RKS embedding (\u03b5RKS).\nWe need some smoothness assumptions on p and q: that they are members of a periodic Ho\u0308lder class \u03a3per(\u03b2, L\u03b2), that they are bounded below by \u03c1\u2217 and above by \u03c1\u2217, and that their kernel density estimates are in \u03a3per(\u03b3\u0302, L\u0302) with probability at least 1\u2212 \u03b4. We use a suitable form of kernel density\nestimation, to obtain a uniform error bound with a rate based on the function C\u22121 (Gine\u0301 and Guillou 2002). We use the Fourier basis and choose V = {\u03b1 \u2208 Z` | \u2211` j=1|\u03b1j |2s \u2264 t} for parameters 0 < s < \u03b3\u0302, t > 0. Then, for any \u03b5RKS + 1\u03c3k \u221a e\n(\u03b5KDE + \u03b5\u03bb + \u03b5tail + \u03b5int) \u2264 \u03b5, the probability of the error exceeding \u03b5 is at most:\n2 exp ( \u2212D\u03b52RKS ) + 2 exp ( \u2212M\u03b54\u03bb/(8Z2) ) + \u03b4\n+ 2C\u22121 ( \u03b54KDEn 2\u03b2/(2\u03b2+`)\n4 log n\n) + 2M ( 1\u2212 \u00b5 ( [0, utail) )) + 8M |V | exp \u2212 12ne (\u221a 1 + \u03b52int/(8 |V |Z)\u2212 1\u221a \u03c1\u2217 + 1\n)2 where utail = \u221a max ( 0, \u03c1\u2217t\n8M`L\u03022 4\u03b3\u0302\u22124s 4\u03b3\u0302 \u03b52tail \u2212 1 4\n) .\nThe bound decreases when the function is smoother (larger \u03b2, \u03b3\u0302; smaller L\u0302) or lower-dimensional (`), or when we observe more samples (n). Using more projection coefficients (higher t or smaller s, giving higher |V |) improves the approximation but makes numerical integration more difficult. Likewise, taking more samples from \u00b5 (higher M ) improves that approximation, but increases the number of functions to be approximated and numerically integrated.\nFor the proof and further details, see the appendix."}, {"heading": "Numerical Experiments", "text": "Throughout these experiments we use M = 5, |V | = 10` (selected as rules of thumb; larger values did not improve performance), and use a validation set (10% of the training set) to choose bandwidths for KDE and the RBF kernel as well as model regularization parameters. Except in the scene classification experiments, the histogram methods used 10 bins per dimension; performance with other values was not better. The KL estimator used the fourth nearest neighbor.\nWe evaluate RBF kernels based on various distances. First, we try our JS, Hellinger, and TV embeddings. We compare to L2 kernels as in Oliva et al. (2014): exp ( \u2212 12\u03c32 \u2016p\u2212 q\u2016 2 2 ) \u2248 z(~a(p\u0302))Tz(~a(q\u0302)) (L2). We also try the MMD distance (Muandet et al. 2012) with approximate kernel embeddings: exp ( \u2212 12\u03c32 M\u0302MD(p, q) ) \u2248 z (z\u0304(p\u0302))T z (z\u0304(q\u0302)), where z\u0304 is\nthe mean embedding z\u0304(p\u0302) = 1n \u2211n i=1 z(Xi) (MMD). We further compare to RKS with histogram JS embeddings (Vempati et al. 2010) (Hist JS); we also tried \u03c72 embeddings, but their performance was quite similar. We finally try the full Gram matrix approach of Po\u0301czos et al. (2012b) with the KL estimator of Wang, Kulkarni, and Verdu\u0301 (2009) in an RBF kernel (KL), as did Ntampaka et al. (2015)."}, {"heading": "Gram Matrix Estimation", "text": "We first illustrate that our embedding, using the parameter selections as above, can approximate the Jensen-Shanon kernel well. We compare three different approaches to estimating K(pi, pj) = exp(\u2212 12\u03c32 JS(pi, pj)). Each approach uses kernel density estimates p\u0302i. The estimates are compared on a dataset of N = 50 random GMM distributions {pi}Ni=1 and\nsamples of size n = 2 500: \u03c7i = {X(i)j \u2208 [0, 1]2}nj=1 iid\u223c pi. See the appendix for more details. The first approach approximates JS based on empirical estimates of entropies E log p\u0302i. The second approach estimates JS as the Euclidean distance of vectors of projection coefficients (5) : JSpc(pi, pj) = \u2016A\u0302(p\u0302i)\u2212 A\u0302(p\u0302j)\u20162. For these first two approaches we compute the pairwise kernel evaluations in the Gram matix as Gentij = exp(\u2212 12\u03c32 JSent(pi, pj)), and Gpcij = exp(\u2212 12\u03c32 JSpc(pi, pj)) using their respective approximations for JS. Lastly, we directly estimate the JS kernel with dot products of our random features (8): Grksij = z(A\u0302(p\u0302i)) Tz(A\u0302(p\u0302j)), with D = 7 000.\nFigure 2 shows the N2 true pairwise kernel values versus the aforementioned estimates. Quantitatively, the entropy method obtained a squared correlation to the true kernel value of R2ent = 0.981; using the A features with an exact kernel yielded R2pc = 0.974; adding RKS embeddings gave\nR2rks = 0.966. Thus our method\u2019s estimates are nearly as good as direct estimation via entropies, while allowing us to work in primal space and avoid N \u00d7N Gram matrices."}, {"heading": "Estimating the Number of Mixture Components", "text": "We will now illustrate the efficacy of HDD random features in a regression task, following Oliva et al. (2014): estimate the number of components from a mixture of truncated Gaussians. We generate the distributions as follows: Draw the number of components Yi for the ith distribution as Yi \u223c Unif{1, . . . , 10}. For each component select a mean \u00b5(i)k \u223c Unif[\u22125, 5]2 and covariance \u03a3(i)k = a (i) k A (i) k A (i)T k + B (i) k , where a \u223c Unif[1, 4], A(i)k (u, v) \u223c Unif[\u22121, 1], and B (i) k is a diagonal 2 \u00d7 2 matrix with B(i)k (u, u) \u223c Unif[0, 1]. Then weight each component equally in the mixture. Given a sample \u03c7i, we predict the number of components Yi. An example distribution and sample are shown in Figure 3; predicting the number of components is difficult even for humans.\nFigure 4 presents results for predicting with ridge regression the number of mixture components Yi, given a varying number of sample sets \u03c7i, with |\u03c7i| \u2208 {200, 800}; we use\nD = 5 000. The HDD-based kernels achieve substantially lower error than the L2 and MMD kernels. They also outperform the histogram kernel, especially with |\u03c7i| = 200, and the KL kernel. Note that fitting mixtures with EM and selecting a number of components using AIC (Akiake 1973) or BIC (Schwarz 1978) performed much worse than regression; only AIC with |\u03c7i| = 800 outperformed a constant predictor of 5.5. Linear versions of the L2 and MMD kernels were also no better than the constant predictor.\nThe HDD embeddings were more computationally expensive than the other embeddings, but much less expensive than the KL kernel, which grows at least quadratically in the number of distributions. Note that the histogram embeddings used an optimized C implementation (Vedaldi and Fulkerson 2008), as did the KL kernel2, while the HDD embeddings used a simple Matlab implementation."}, {"heading": "Image Classification", "text": "As another example of the performance of our embeddings, we now attempt to classify images based on their distributions of pixel values. We took the \u201ccat\u201d and \u201cdog\u201d classes from the CIFAR-10 dataset (Krizhevsky and Hinton 2009), and represented each 32 \u00d7 32 image by a set of triples (x, y, v), where x and y are the position of each pixel in the image and v the pixel value after converting to grayscale. The horizontal reflection of the image was also included, so each sample set \u03c7i \u2282 R3 had |\u03c7i| = 2 048. This is certainly not the best representation for these images; rather, we wish to show that given this simple representation, our HDD kernels perform well relative to the other options.\nWe used the same kernels as above in an SVM classifier from LIBLINEAR (Fan et al. 2008, for the embeddings) or LIBSVM (Chang and Lin 2011, for the KL kernel), with D = 7 000. Figure 5 shows computation time and accuracy on the standard test set (of size 2K) with 2.5K, 5K, and 10K training images. Our JS and Hellinger embedding approximately match the histogram JS embedding in accuracy here, while our TV embedding beats histogram JS; all outperform L2 and MMD. We could only run the KL kernel for the 2.5K training set size; its accuracy was comparable to the HDD and histogram embeddings, at far higher computational cost."}, {"heading": "Scene Classification", "text": "Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n\u00d7h\u00d7w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how Po\u0301czos et al. (2012b) and Muandet et al. (2012) used SIFT features from image patches. Wu, Gao, and Liu (2015) set accuracy records on several scene classification datasets with a particular ad-hoc method of extracting features from distributions (D3); we compare to our more principled alternatives.\n2 https://github.com/dougalsutherland/skl-groups/\nRMSE 1.35 1.4 1.45 1.5 1.55\nT im\ne (\nc p\nu -h\no u\nrs )\n10 0\n10 1\n10 2\n10 3\nHellingerJS\nTV\nL2 MMD\nKL with kNN\nHist JS\n(a) Samples of size 200. RMSE\n1.35 1.4 1.45 1.5 1.55\nT im\ne (\nc p u -h\no u rs\n)\n10 0\n10 1\n10 2\n10 3\nHellinger JS\nTV\nL2 MMD\nKL with kNN\nHist JS\n(b) Samples of size 800.\nFigure 4: Error and computation time for estimating the number of mixture components. The three points on each line correspond to training set sizes of 4K, 8K, and 16K; error is on the fixed test set of size 2K. Note the logarithmic scale on the time axis. The KL kernel for |\u03c7i| = 800 with 16K training sets was too slow to run. AIC-based predictions achieved RMSEs of 2.7 (for 200 samples) and 2.3 (for 800); BIC errors were 3.8 and 2.7; a constant predictor of 5.5 had RMSE of 2.8.\nError 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38\nT im\ne (\nc p u -h\no u rs\n)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nL2\nHist JS MMD\nJS HellingerTV\nKL\nFigure 5: Misclassification rate and computation time for classifying CIFAR-10 cats versus dogs. The three points on each line show training set sizes of 2.5K, 5K, and 10K; error is on the fixed test set of size 2K. Note the linear scale for time. The KL kernel was too slow to run for 5K or 10K training points.\n87%\n88%\n89%\n90%\n91%\n92%\n93%\nD3 L2 HDDs JS TV Hel\nMMD\n1 2 \u03c3 \u03c3 2\u03c3\nHist JS\n10 25 50\nFigure 6: Mean and standard deviation of accuracies on the Scene-15 dataset in 10 random splits. The left, black lines use A\u0302(\u00b7) features; the right, blue lines show z(A\u0302(\u00b7)) features. MMD methods vary bandwidth are relative to \u03c3, the median of pairwise distances; histogram methods vary the number of bins.\nWe consider the Scene-15 dataset (Lazebnik, Schmid, and Ponce 2006), which contains 4 485 natural images in 15 location categories, and follow Wu, Gao, and Liu in extracting features from the last convolutional layer of the imagenet-vgg-verydeep-16 model (Simonyan and Zisserman 2015). We replace that layer\u2019s rectified linear activations with sigmoid squashing to [0, 1].3 hw ranges from 400 to 1 000. There are 512 filter dimensions; we concatenate features A\u0302(p\u0302i) extracted from each independently.\nWe train on the standard for this dataset of 100 images from each class (1500 total) and test on the remainder; Figure 6 shows results. We do not add any spatial information to the model; still, we match the best prior published performance of 91.59\u00b1 0.48, which trained on over 7 million external images (Zhou et al. 2014). Adding spatial information\n3We used piecewise-linear weights before the sigmoid function such that 0 maps to 0.5, the 90th percentile of the positive observations maps to 0.9, and the 10th percentile of the negative observations to 0.1, for each filter.\nbrought the D3 method slightly above 92% accuracy; their best hybrid method obtained 92.9%. Using these features, however, our methods match or beat MMD and substantially outperform D3, L2, and the histogram embeddings."}, {"heading": "Discussion", "text": "This work presents the first nonlinear embedding of density functions for quickly computing HDD-based kernels, including kernels based on the popular total variation, Hellinger and Jensen-Shanon divergences. While such divergences have shown good empirical results in the comparison of densities, nonparametric uses of kernels with these divergences previously necessitated the computation of a large N \u00d7 N Gram matrix, prohibiting their use in large datasets. Our embeddings allow one to work in a primal space while using information theoretic kernels. We analyze the approximation error of our embeddings, and illustrate their quality on several synthetic and real-world datasets."}, {"heading": "Acknowledgements", "text": "This work was funded in part by NSF grant IIS1247658 and by DARPA grant FA87501220324. DJS is also supported by a Sandia Campus Executive Program fellowship."}, {"heading": "Appendices", "text": ""}, {"heading": "Gram Matrix Estimation", "text": "We illustrate our embedding\u2019s ability to approximate the Jensen-Shanon divergence. In the examples below the densities considered are mixtures of five equally weighted truncated spherical Gaussians on [0, 1]2. That is,\npi(x) = 1\n5 5\u2211 j=1 Nt(mij , diag(s2ij))\nwhere mij iid\u223c Unif([0, 1]2), sij iid\u223c Unif([0.05, 0.15]2) and Nt(m, s) is the distribution of a Gaussian truncated on [0, 1]2 with mean parameter m and covariance matrix parameter s. We work over the sample set {\u03c7i}Ni=1, where \u03c7i = {X (i) j \u2208 [0, 1]2}nj=1 iid\u223c pi, n = 2500, N = 50.\nWe compare three different approaches to estimating K(pi, pj) = exp(\u2212 12\u03c32 JS(pi, pj)). Each approach uses density estimates p\u0302i, which are computed using kernel density estimation. The first approach is based on estimating JS using empirical estimates of entropies:\nJS(pi, pj) = \u2212 12Epi [ log ( 1\npi(x)\n)] \u2212 12Epj [ log ( 1\npj(x) )] + E 1\n2pi+ 1 2pj\n[ log ( 2\npi(x) + pj(x)\n)]\n\u2248 \u221212 dn/2e\u2211 m=1 log\n( 1\np\u0302i(X (i) m )\n) \u2212 12 dn/2e\u2211 m=1 log ( 1 p\u0302j(X (j) m ) )\n+ 12 dn/2e\u2211 m=1 log\n( 2\np\u0302i(X (i) m ) + p\u0302j(X (i) m )\n)\n+ 12 dn/2e\u2211 m=1 log\n( 2\np\u0302i(X (j) m ) + p\u0302j(X (k) m ) ) = JSent(pi, pj),\nwhere density estimates p\u0302i above are based on points {X(i)m }nm=dn/2e+1 to avoid biasing the empirical means. The second approach estimates JS as the Euclidean distance of vectors of projection coefficients:\nJS(pi, pj) \u2248 \u2016A\u0302(p\u0302i)\u2212 A\u0302(p\u0302j)\u20162 = JSpc(pi, pj),\nwhere here the density estimates p\u0302i are based on the entire set of points \u03c7i. We build a Gram matrix for each of these approaches by setting Gentij = exp(\u2212 12\u03c32 JSent(pi, pj)) and Gpcij = exp(\u2212 1 2\u03c32\nJSpc(pi, pj)). Lastly, we directly estimate the JS kernel with random features:\nGrksij = z(A\u0302(p\u0302i)) Tz(A\u0302(p\u0302j)).\nWe compare the effectiveness of each approach by computing the R2 score of the estimates produced versus a true JS\nkernel value computed through numerically integrating the true densities (see Figure 7 and Table 2). The RBF values estimated with our random features produce estimates that are nearly as good as directly estimating JS divergences through entropies, whilst allowing us to work over a primal space and thus avoid computing a N \u00d7N Gram matrix for learning tasks."}, {"heading": "Proofs", "text": "We will now prove the bound on the error probability of our embedding Pr\n(\u2223\u2223\u2223K(p, q)\u2212 z(A\u0302(p\u0302))Tz(A\u0302(q\u0302))\u2223\u2223\u2223 \u2265 \u03b5) for fixed densities p and q.\nSetup We will need a few assumptions on the densities:\n1. p and q are bounded above and below: for x \u2208 [0, 1]`, 0 < \u03c1\u2217 \u2264 p(x), q(x) \u2264 \u03c1\u2217 <\u221e.\n2. p, q \u2208 \u03a3(\u03b2, L\u03b2) for some \u03b2, L\u03b2 > 0. \u03a3(\u03b2, L) refers to the Ho\u0308lder class of functions f whose partial derivatives up to order b\u03b2c are continuous and whose rth partial derivatives, where r is a multi-index of order b\u03b2c, satisfy |Drf(x)\u2212Drf(y)| \u2264 L\u2016x \u2212 y\u2016\u03b2 . Here b\u03b2c is the greatest integer strictly less than \u03b2.\n3. p, q are periodic.\nThese are fairly standard smoothness assumptions in the nonparametric estimation literature.\nLet \u03b3 = min(\u03b2, 1). If \u03b2 > 1, then p, q \u2208 \u03a3(1, L\u03b3) for some L\u03b3 ; otherwise, clearly p, q \u2208 \u03a3(\u03b2, L\u03b2). Then, from assumption 3, p, q \u2208 \u03a3per(\u03b3, L\u03b3), the periodic Ho\u0308lder class. We\u2019ll need this to establish the Sobolev ellipsoid containing p and q.\nWe will use kernel density estimation with a bounded, continuous kernel so that the bound of Gine\u0301 and Guillou (2002) applies, with bandwidth h n\u22121/(2\u03b2+`) logn, and truncating density estimates to [\u03c1\u2217, \u03c1\u2217].\nWe also use the Fourier basis \u03d5\u03b1 = exp ( 2i\u03c0\u03b1Tx ) , and de-\nfine V as the set of indices \u03b1 s.t. \u2211` j=1|\u03b1j |\n2s \u2264 t for parameters 0 < s \u2264 1, t > 0 to be discussed later.\nDecomposition Let r\u03c3(\u2206) = exp ( \u2212\u22062/(2\u03c32) ) . Then\u2223\u2223\u2223K(p, q)\u2212 z(A\u0302(p\u0302))T z(A\u0302(q\u0302))\u2223\u2223\u2223 \u2264\u2223\u2223K(p, q)\u2212 r\u03c3k (\u2016A\u0302(p\u0302)\u2212 A\u0302(q\u0302)\u2016)\u2223\u2223\n+ \u2223\u2223\u2223r\u03c3k (\u2016A\u0302(p\u0302)\u2212 A\u0302(q\u0302)\u2016)\u2212 z(A\u0302(p\u0302))T z(A\u0302(q\u0302))\u2223\u2223\u2223 .\nThe latter term was bounded by Rahimi and Recht (2007). For the former, note that r\u03c3 is 1\u03c3\u221ae -Lipschitz, so the first term is at most 1 \u03c3k \u221a e \u2223\u2223d(p, q)\u2212 \u2016A\u0302(p\u0302)\u2212 A\u0302(q\u0302)\u2016\u2223\u2223. Breaking this up with the triangle inequality:\u2223\u2223d(p, q)\u2212 \u2016A\u0302(p\u0302)\u2212 A\u0302(q\u0302)\u2016\u2223\u2223 \u2264\n|d(p, q)\u2212 d(p\u0302, q\u0302)|+ |d(p\u0302, q\u0302)\u2212 \u2016\u03c8(p\u0302)\u2212 \u03c8(q\u0302)\u2016| + |\u2016\u03c8(p\u0302)\u2212 \u03c8(q\u0302)\u2016 \u2212 \u2016A(p\u0302)\u2212A(q\u0302)\u2016|\n+ \u2223\u2223\u2016A(p\u0302)\u2212A(q\u0302)\u2016 \u2212 \u2016A\u0302(p\u0302)\u2212 A\u0302(q\u0302)\u2016\u2223\u2223 . (11)\nEstimation error Recall that d is a metric, so the reverse triangle inequality allows us to address the first term with\n|d(p, q)\u2212 d(p\u0302, q\u0302)| \u2264 d(p, p\u0302) + d(q, q\u0302).\nFor d2 the total variation, squared Hellinger, or Jensen-Shannon HDDs, we have that d2(p, q\u0302) \u2264 TV(p, p\u0302) (Lin 1991). Moreover, as the distributions are supported on [0, 1]`, TV(p, p\u0302) = 1 2 \u2016p\u2212 p\u0302\u20161 \u2264 1 2 \u2016p\u2212 p\u0302\u2016\u221e.\nIt is a consequence of Gine\u0301 and Guillou (2002) that, for any \u03b4 > 0, Pr ( \u2016p\u2212 p\u0302\u2016\u221e > \u221a C\u03b4 logn\nn\u03b2/(2\u03b2+`)\n) < \u03b4 for some C\u03b4\ndepending on the kernel. Thus Pr (|d(p, q)\u2212 d(p\u0302, q\u0302)| \u2265 \u03b5) < 2C\u22121 ( \u03b54n2\u03b2/(2\u03b2+`)\n4 logn\n) , where CC\u22121(x) = x.\n\u03bb approximation The second term of (11), the approximation error due to sampling \u03bbs, admits a simple Hoeffd-\ning bound. Note that \u2225\u2225\u2225p\u0302R\u03bb \u2212 q\u0302R\u03bb \u2225\u2225\u22252 + \u2225\u2225\u2225p\u0302I\u03bb \u2212 q\u0302I\u03bb\u2225\u2225\u22252, viewed as\na random variable in \u03bb only, has expectation d2(p\u0302, q\u0302) and is bounded by [0, 4Z] (where Z = \u222b R\u22650 d\u00b5(\u03bb)): write it as\nZ \u222b |p\u0302(x) 1 2+i\u03bb \u2212 q\u0302(x)\n1 2+i\u03bb|2 dx, expand the square, and use\u222b \u221a\np\u0302(x)q\u0302(x)dx \u2264 1 (via Cauchy-Schwarz). For nonnegative random variables X and Y , Pr (|X \u2212 Y | \u2265 \u03b5) \u2264 Pr (\u2223\u2223X2 \u2212 Y 2\u2223\u2223 \u2265 \u03b52), so we have that Pr (|\u2016\u03c8(p\u0302)\u2212 \u03c8(q\u0302)\u2016 \u2212 d(p\u0302, q\u0302)| \u2265 \u03b5) is at most 2 exp(\u2212M\u03b54/(8Z2)).\nTail truncation error The third term of (11), the error due to truncating the tail projection coefficients of the pS\u03bb functions, requires a little more machinery. First note that\u2223\u2223\u2223\u2016\u03c8(p\u0302)\u2212 \u03c8(q\u0302)\u20162 \u2212 \u2016A(p\u0302)\u2212A(q\u0302)\u20162\u2223\u2223\u2223 is at most\nM\u2211 j=1 \u2211 S=R,I \u2211 \u03b1/\u2208V \u2223\u2223\u2223a\u03b1(p\u0302S\u03bb \u2212 q\u0302S\u03bb )\u2223\u2223\u22232 . (12) Let W(s, L) be the Sobolev ellipsoid of functions\u2211 \u03b1\u2208Z` a\u03b1\u03d5\u03b1 such that \u2211 \u03b1\u2208Z` (\u2211` j=1|\u03b1j | 2s ) |a\u03b1|2 \u2264 L, where \u03d5 is still the Fourier basis. Then Lemma 14 of Krishnamurthy et al. (2014) shows that \u03a3per(\u03b3, L\u03b3) \u2286 W(s, L\u2032) for any 0 < s < \u03b3 and L\u2032 = `L2\u03b3(2\u03c0)\u22122b\u03b3c 4 \u03b3 4\u03b3\u22124s .\nSo, suppose that p\u0302, q\u0302 \u2208 \u03a3per(\u03b3\u0302, L\u0302) with probability at least\n1\u2212 \u03b4. Since x 7\u2192 x 1 2+i\u03bb is\n\u221a 1+4\u03bb2\n2 \u221a \u03c1\u2217 -Lipschitz on [\u03c1\u2217,\u221e), p\u0302S\u03bb \u2208\n\u03a3per ( \u03b3\u0302, 12 \u221a 1 + 4\u03bb2 L\u0302 \u03c1 \u2212 12\u2217 ) and so p\u0302S\u03bb \u2212 q\u0302 S \u03bb is in W(s, (1 +\n4\u03bb2)L\u0302\u2032) for s < \u03b3\u0302 and L\u0302\u2032 = `L\u03022\u03c1\u22121\u2217 /(1\u2212 4s\u2212\u03b3\u0302). Recall that we chose V to be the set of \u03b1 \u2208 Z` such\nthat \u2211` j=1|\u03b1j | 2s \u2264 t. Thus \u2211 \u03b1/\u2208V |a\u03b1(p\u0302 S \u03bb \u2212 q\u0302 S \u03bb )|\n2 \u2264\u2211 \u03b1/\u2208V |a\u03b1(p\u0302 S \u03bb \u2212 q\u0302 S \u03bb )| 2 (\u2211` j=1|\u03b1j | 2s ) /t \u2264 (1 + 4\u03bb2)L\u0302\u2032/t.\nThe tail error term is therefore at least \u03b5 with probability no more than \u03b4 + 2 \u2211M j=1 Pr ( (1 + 4\u03bb2j )L\u0302 \u2032/t \u2265 \u03b52/(2M) )\n. The latter probability, of course, depends on the choice of HDD d. Letting \u03b6 = t\u03b52/(8ML\u0302\u2032) \u2212 14 , it is 1 if \u03b6 < 0 and 1\u2212\u00b5 ( [0, \u221a \u03b6] ) /Z otherwise. If \u03b6 \u2265 0, squared Hellinger\u2019s probability is 0, and total variation\u2019s is 2\u03c0 arctan( \u221a \u03b6). A closed form for the cumulative distribution function for the Jensen-Shannon measure is unfortunately unknown.\nNumerical integration error The final term of (11) also bears a Hoeffding bound. Define the projection coefficient difference \u2206S\u03bb,\u03b1(p, q) = a\u03b1,\u03bb(p S \u03bb) \u2212 a\u03b1(q S \u03bb ), and \u2206\u0302 similarly but\nwith a\u0302. Then \u2223\u2223\u2223\u2016A(p\u0302)\u2212A(q\u0302)\u20162 \u2212 \u2225\u2225A\u0302(p\u0302)\u2212 A\u0302(q\u0302)\u2225\u22252\u2223\u2223\u2223 is at most\nM\u2211 j=1 \u2211 S=R,I \u2211 \u03b1\u2208V \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2206S\u03b1,\u03bbj (p\u0302, q\u0302)\u2223\u2223\u22232 \u2212 \u2223\u2223\u2223\u2206\u0302S\u03b1,\u03bbj (p\u0302, q\u0302)\u2223\u2223\u22232\u2223\u2223\u2223\u2223 . (13) Letting \u0302(p) = a\u03b1(p\u0302S\u03bb) \u2212 a\u0302\u03b1(p\u0302 S \u03bb), each summand is at\nmost (\u0302(p) + \u0302(q))2 + 2 \u2223\u2223\u2223\u2206S\u03bb,\u03b1(p\u0302, q\u0302)\u2223\u2223\u2223 (\u0302(p) + \u0302(q)). Also,\u2223\u2223\u2223\u2206S\u03b1,\u03bb(p\u0302, q\u0302)\u2223\u2223\u2223 \u2264 2\u221aZ, using Cauchy-Schwarz on the integral and \u2016\u03d5\u03b1\u20162 = 1. Thus each summand in (13) can be more than \u03b5 only if one of the \u0302s is more than \u221a Z + \u03b5/4\u2212 \u221a Z.\nNow, using (10), a\u0302\u03b1(p\u0302S\u03bb) is an empirical mean of ne independent terms, each with absolute value bounded by ( \u221a \u03c1\u2217 + 1) maxx|\u03d5\u03b1(x)| = \u221a \u03c1\u2217 + 1.\nThus, using a Hoeffding bound on the \u0302s, we get that Pr (\u2223\u2223\u2016A(p\u0302)\u2212A(q\u0302)\u20162 \u2212 \u2016A\u0302(p\u0302)\u2212 A\u0302(q\u0302)\u20162\u2223\u2223 \u2265 \u03b5) is no more than\n8MS exp ( \u2212 ne (\u221a Z+\u03b52/(8S)\u2212 \u221a Z )2\n2Z( \u221a \u03c1\u2217+1)2\n) .\nFinal bound Combining the bounds for the decomposition (11) with the pointwise rate for RKS features, we get:\nPr (\u2223\u2223\u2223K(p, q)\u2212 z(A\u0302(p\u0302)Tz(A\u0302(q\u0302))\u2223\u2223\u2223 \u2265 \u03b5) \u2264\n2 exp ( \u2212D\u03b52RKS ) + 2C\u22121 ( \u03b54KDEn 2\u03b2/(2\u03b2+`)\n4 logn ) + 2 exp ( \u2212M\u03b54\u03bb/(8Z 2) )\n+ \u03b4 + 2M ( 1\u2212 \u00b5 [ 0, \u221a max ( 0, \u03c1\u2217t\u03b52tail 8M`L\u03022 4\u03b3\u0302 \u2212 4s 4\u03b3\u0302 \u2212 1 4 )))\n+ 8M |V | exp \u2212 12ne  \u221a 1 + \u03b52int/(8 |V |Z)\u2212 1 \u221a \u03c1\u2217 + 1 2  (14)\nfor any \u03b5RKS + 1\u03c3k \u221a e (\u03b5KDE + \u03b5\u03bb + \u03b5tail + \u03b5int) \u2264 \u03b5."}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akiake"], "venue": "2nd Int. Symp. on Inf. Theory.", "citeRegEx": "Akiake,? 1973", "shortCiteRegEx": "Akiake", "year": 1973}, {"title": "Spectral partitioning with indefinite kernels using the Nystr\u00f6m extension", "author": ["S. Belongie", "C. Fowlkes", "F. Chung", "J. Malik"], "venue": "ECCV.", "citeRegEx": "Belongie et al\\.,? 2002", "shortCiteRegEx": "Belongie et al\\.", "year": 2002}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C.", "Lin", "C.-J."], "venue": "ACM Trans. Intell. Syst. Technol. 2(3):1\u2013", "citeRegEx": "Chang et al\\.,? 2011", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Similarity-based classification: Concepts and algorithms", "author": ["Y. Chen", "E.K. Garcia", "M.R. Gupta", "A. Rahimi", "L. Cazzanti"], "venue": "JMLR 10:747\u2013776.", "citeRegEx": "Chen et al\\.,? 2009", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "JMLR 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Who supported Obama in 2012? Ecological inference through distribution regression", "author": ["S.R. Flaxman", "Y.-x. Wang", "A.J. Smola"], "venue": "KDD, 289\u2013298.", "citeRegEx": "Flaxman et al\\.,? 2015", "shortCiteRegEx": "Flaxman et al\\.", "year": 2015}, {"title": "Spirals in Hilbert space: With an application in information theory", "author": ["B. Fuglede"], "venue": "Exposition. Math. 23(1):23\u201345.", "citeRegEx": "Fuglede,? 2005", "shortCiteRegEx": "Fuglede", "year": 2005}, {"title": "Rates of strong uniform consistency for multivariate kernel density estimators", "author": ["E. Gin\u00e9", "A. Guillou"], "venue": "Ann. Inst. H. Poincar\u00e9 Probab. Statist. 38(6):907\u2013921.", "citeRegEx": "Gin\u00e9 and Guillou,? 2002", "shortCiteRegEx": "Gin\u00e9 and Guillou", "year": 2002}, {"title": "A fast, consistent kernel two-sample test", "author": ["A. Gretton", "K. Fukumizu", "Z. Harchaoui", "B.K. Sriperumbudur"], "venue": "NIPS.", "citeRegEx": "Gretton et al\\.,? 2009", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Learning with distance substitution kernels", "author": ["B. Haasdonk", "C. Bahlmann"], "venue": "Pattern Recognition: 26th DAGM Symposium, 220\u2013227.", "citeRegEx": "Haasdonk and Bahlmann,? 2004", "shortCiteRegEx": "Haasdonk and Bahlmann", "year": 2004}, {"title": "The No-U-Turn Sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo", "author": ["M.D. Hoffman", "A. Gelman"], "venue": "JMLR 15(1):1593\u20131623.", "citeRegEx": "Hoffman and Gelman,? 2014", "shortCiteRegEx": "Hoffman and Gelman", "year": 2014}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "NIPS.", "citeRegEx": "Jaakkola and Haussler,? 1998", "shortCiteRegEx": "Jaakkola and Haussler", "year": 1998}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "JMLR 5:819\u2013844.", "citeRegEx": "Jebara et al\\.,? 2004", "shortCiteRegEx": "Jebara et al\\.", "year": 2004}, {"title": "Kernelbased just-in-time learning for passing expectation propagation messages", "author": ["W. Jitkrittum", "A. Gretton", "N. Heess", "S. Eslami", "B. Lakshminarayanan", "D. Sejdinovic", "Z. Szab\u00f3"], "venue": "UAI.", "citeRegEx": "Jitkrittum et al\\.,? 2015", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2015}, {"title": "A kernel between sets of vectors", "author": ["R. Kondor", "T. Jebara"], "venue": "ICML.", "citeRegEx": "Kondor and Jebara,? 2003", "shortCiteRegEx": "Kondor and Jebara", "year": 2003}, {"title": "Nonparametric estimation of R\u00e9nyi divergence and friends", "author": ["A. Krishnamurthy", "K. Kandasamy", "B. Poczos", "L. Wasserman"], "venue": "ICML.", "citeRegEx": "Krishnamurthy et al\\.,? 2014", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "University of Toronto, Tech. Rep.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Sparse nonparametric graphical models", "author": ["J. Lafferty", "H. Liu", "L. Wasserman"], "venue": "Statistical Science 27(4):519\u2013537.", "citeRegEx": "Lafferty et al\\.,? 2012", "shortCiteRegEx": "Lafferty et al\\.", "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR.", "citeRegEx": "Lazebnik et al\\.,? 2006", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Representing and recognizing the visual appearance of materials using three-dimensional textons", "author": ["T. Leung", "J. Malik"], "venue": "IJCV 43.", "citeRegEx": "Leung and Malik,? 2001", "shortCiteRegEx": "Leung and Malik", "year": 2001}, {"title": "Random Fourier approximations for skewed multiplicative histogram kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "Pattern Recognition: DAGM, 262\u2013271.", "citeRegEx": "Li et al\\.,? 2010", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Divergence measures based on the Shannon entropy", "author": ["J. Lin"], "venue": "IEEE Trans. Inf. Theory 37(1):145\u2013151.", "citeRegEx": "Lin,? 1991", "shortCiteRegEx": "Lin", "year": 1991}, {"title": "Towards a learning theory of causation", "author": ["D. Lopez-Paz", "K. Muandet", "B. Sch\u00f6lkopf", "I. Tolstikhin"], "venue": "ICML.", "citeRegEx": "Lopez.Paz et al\\.,? 2015", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2015}, {"title": "Max-margin additive classifiers for detection", "author": ["S. Maji", "A.C. Berg"], "venue": "ICCV.", "citeRegEx": "Maji and Berg,? 2009", "shortCiteRegEx": "Maji and Berg", "year": 2009}, {"title": "A KullbackLeibler divergence based kernel for SVM classification in multimedia applications", "author": ["P.J. Moreno", "P.P. Ho", "N. Vasconcelos"], "venue": "NIPS.", "citeRegEx": "Moreno et al\\.,? 2003", "shortCiteRegEx": "Moreno et al\\.", "year": 2003}, {"title": "Learning from distributions via support measure machines", "author": ["K. Muandet", "K. Fukumizu", "F. Dinuzzo", "B. Sch\u00f6lkopf"], "venue": "NIPS.", "citeRegEx": "Muandet et al\\.,? 2012", "shortCiteRegEx": "Muandet et al\\.", "year": 2012}, {"title": "A machine learning approach for dynamical mass measurements of galaxy clusters", "author": ["M. Ntampaka", "H. Trac", "D.J. Sutherland", "N. Battaglia", "B. P\u00f3czos", "J. Schneider"], "venue": "The Astrophysical Journal 803(2):50.", "citeRegEx": "Ntampaka et al\\.,? 2015", "shortCiteRegEx": "Ntampaka et al\\.", "year": 2015}, {"title": "Fast distribution to real regression", "author": ["J.B. Oliva", "W. Neiswanger", "B. P\u00f3czos", "J. Schneider", "E. Xing"], "venue": "AISTATS.", "citeRegEx": "Oliva et al\\.,? 2014", "shortCiteRegEx": "Oliva et al\\.", "year": 2014}, {"title": "Distribution to distribution regression", "author": ["J.B. Oliva", "B. P\u00f3czos", "J. Schneider"], "venue": "ICML.", "citeRegEx": "Oliva et al\\.,? 2013", "shortCiteRegEx": "Oliva et al\\.", "year": 2013}, {"title": "Distribution-free distribution regression", "author": ["B. P\u00f3czos", "A. Rinaldo", "A. Singh", "L. Wasserman"], "venue": "AISTATS.", "citeRegEx": "P\u00f3czos et al\\.,? 2012a", "shortCiteRegEx": "P\u00f3czos et al\\.", "year": 2012}, {"title": "Nonparametric kernel estimators for image classification", "author": ["B. P\u00f3czos", "L. Xiong", "D.J. Sutherland", "J. Schneider"], "venue": "CVPR.", "citeRegEx": "P\u00f3czos et al\\.,? 2012b", "shortCiteRegEx": "P\u00f3czos et al\\.", "year": 2012}, {"title": "Random features for largescale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS.", "citeRegEx": "Rahimi and Recht,? 2007", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Statist. 6(2):461\u2013464.", "citeRegEx": "Schwarz,? 1978", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR.", "citeRegEx": "Simonyan and Zisserman,? 2015", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Ensemble estimators for multivariate entropy estimation", "author": ["K. Sricharan", "D. Wei", "Hero, III, A.O."], "venue": "IEEE Trans. Inf. Theory 59:4374\u20134388.", "citeRegEx": "Sricharan et al\\.,? 2013", "shortCiteRegEx": "Sricharan et al\\.", "year": 2013}, {"title": "On the error of random Fourier features", "author": ["D.J. Sutherland", "J. Schneider"], "venue": "UAI.", "citeRegEx": "Sutherland and Schneider,? 2015", "shortCiteRegEx": "Sutherland and Schneider", "year": 2015}, {"title": "Two-stage sampled learning theory on distributions", "author": ["Z. Szab\u00f3", "A. Gretton", "B. P\u00f3czos", "B. Sriperumbudur"], "venue": "AISTATS.", "citeRegEx": "Szab\u00f3 et al\\.,? 2015", "shortCiteRegEx": "Szab\u00f3 et al\\.", "year": 2015}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org/.", "citeRegEx": "Vedaldi and Fulkerson,? 2008", "shortCiteRegEx": "Vedaldi and Fulkerson", "year": 2008}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "CVPR.", "citeRegEx": "Vedaldi and Zisserman,? 2010", "shortCiteRegEx": "Vedaldi and Zisserman", "year": 2010}, {"title": "Generalized RBF feature maps for efficient detection", "author": ["S. Vempati", "A. Vedaldi", "A. Zisserman", "C.V. Jawahar"], "venue": "British Machine Vision Conference.", "citeRegEx": "Vempati et al\\.,? 2010", "shortCiteRegEx": "Vempati et al\\.", "year": 2010}, {"title": "Divergence estimation for multidimensional densities via k-nearest-neighbor distances", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "IEEE Trans. Inf. Theory 55(5):2392\u20132405.", "citeRegEx": "Wang et al\\.,? 2009", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "NIPS.", "citeRegEx": "Williams and Seeger,? 2001", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "Visual recognition using directional distribution distance", "author": ["J. Wu", "B.-B. Gao", "G. Liu"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Learning deep features for scene recognition using Places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS.", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "The mass of dark matter halos can be inferred from the velocity of galaxies in a cluster (Ntampaka et al. 2015).", "startOffset": 89, "endOffset": 111}, {"referenceID": 13, "context": "Expensive expectation propagation messages can be sped up by learning a \u201cjust-in-time\u201d regression model (Jitkrittum et al. 2015).", "startOffset": 104, "endOffset": 128}, {"referenceID": 29, "context": "For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al.", "startOffset": 124, "endOffset": 180}, {"referenceID": 25, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b).", "startOffset": 102, "endOffset": 144}, {"referenceID": 30, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b).", "startOffset": 102, "endOffset": 144}, {"referenceID": 13, "context": "Expensive expectation propagation messages can be sped up by learning a \u201cjust-in-time\u201d regression model (Jitkrittum et al. 2015). All of these applications are aided by working directly over sets drawn from the distribution of interest, rather than having to develop a per-problem ad-hoc set of summary statistics. \u2217These two authors contributed equally. Distributions are inherently infinite-dimensional objects, since in general they require an infinite number of parameters for their exact representation. Hence, it is not immediate how to extend traditional finite vector technique machine learning techniques to distributional instances. However, recent work has provided various approaches for dealing with distributional data in a nonparametric fashion. For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b). A major shortcoming of both approaches is that they require computing N kernel evaluations per prediction, where N is the number of training instances in a dataset. Often, this implies that one must compute a N \u00d7 N Gram matrix of pairwise kernel evaluations. Such approaches fail to scale to datasets where the number of instances N is very large. Another shortcoming of these approaches is that they are often based on Euclidean metrics, either working over a linear kernel, or one based on the L2 distance over distributions. While such kernels are useful in certain applications, better performance can sometimes be obtained by considering non-Euclidean based kernels. To this end, P\u00f3czos et al. (2012b) use a kernel based on R\u00e9nyi divergences; however, this kernel is not positive semi-definite (PSD), leading to even higher computational cost and other practical issues.", "startOffset": 105, "endOffset": 1753}, {"referenceID": 19, "context": "Learning on distributions In computer vision, the popular \u201cbag of words\u201d model (Leung and Malik 2001) represents a distribution by quantizing it onto codewords (usually by running k-means on all, or many, of the input points from all sets), then compares those histograms with some kernel (often exponentiated \u03c7).", "startOffset": 79, "endOffset": 101}, {"referenceID": 11, "context": "Another approach estimates a distance between distributions, often the L2 distance or Kullback-Leibler (KL) divergence, parametrically (Jaakkola and Haussler 1998; Moreno, Ho, and Vasconcelos 2003; Jebara, Kondor, and Howard 2004) or nonparametrically (Sricharan, Wei, and Hero 2013; Krishnamurthy et al.", "startOffset": 135, "endOffset": 230}, {"referenceID": 15, "context": "Another approach estimates a distance between distributions, often the L2 distance or Kullback-Leibler (KL) divergence, parametrically (Jaakkola and Haussler 1998; Moreno, Ho, and Vasconcelos 2003; Jebara, Kondor, and Howard 2004) or nonparametrically (Sricharan, Wei, and Hero 2013; Krishnamurthy et al. 2014).", "startOffset": 252, "endOffset": 310}, {"referenceID": 29, "context": "The distance can then be used in kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013) or Mercer kernels (Moreno, Ho, and Vasconcelos 2003; Kondor and Jebara 2003; Jebara, Kondor, and Howard 2004; P\u00f3czos et al.", "startOffset": 50, "endOffset": 106}, {"referenceID": 14, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013) or Mercer kernels (Moreno, Ho, and Vasconcelos 2003; Kondor and Jebara 2003; Jebara, Kondor, and Howard 2004; P\u00f3czos et al. 2012b).", "startOffset": 60, "endOffset": 172}, {"referenceID": 30, "context": "2012a; Oliva, P\u00f3czos, and Schneider 2013) or Mercer kernels (Moreno, Ho, and Vasconcelos 2003; Kondor and Jebara 2003; Jebara, Kondor, and Howard 2004; P\u00f3czos et al. 2012b).", "startOffset": 60, "endOffset": 172}, {"referenceID": 3, "context": "Typical approaches involve eigendecomposing the Gram matrix, which usually costs O(N) computation and also presents challenges for traditional inductive learning, where the test points are not known at training time (Chen et al. 2009).", "startOffset": 216, "endOffset": 234}, {"referenceID": 41, "context": "One way to alleviate the scaling problem is the Nystr\u00f6m extension (Williams and Seeger 2001), in which some columns of the Gram matrix are used to estimate the remainder.", "startOffset": 66, "endOffset": 92}, {"referenceID": 1, "context": "In practice, one frequently must compute many columns, and methods to make the result PSD are known only for mildly-indefinite kernels (Belongie et al. 2002).", "startOffset": 135, "endOffset": 157}, {"referenceID": 8, "context": "The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD) (Gretton et al. 2009; Muandet et al. 2012; Szab\u00f3 et al. 2015).", "startOffset": 114, "endOffset": 175}, {"referenceID": 25, "context": "The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD) (Gretton et al. 2009; Muandet et al. 2012; Szab\u00f3 et al. 2015).", "startOffset": 114, "endOffset": 175}, {"referenceID": 36, "context": "The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD) (Gretton et al. 2009; Muandet et al. 2012; Szab\u00f3 et al. 2015).", "startOffset": 114, "endOffset": 175}, {"referenceID": 31, "context": "Approximate embeddings Recent interest in approximate kernel embeddings was spurred by the \u201crandom kitchen sink\u201d (RKS) embedding (Rahimi and Recht 2007), which approximates shift-invariant kernelsK on R by sampling their Fourier transform.", "startOffset": 129, "endOffset": 152}, {"referenceID": 13, "context": "An embedding for the base kernel k also gives a simple embedding for the mean map kernel (Flaxman, Wang, and Smola 2015; Jitkrittum et al. 2015; Lopez-Paz et al. 2015; Sutherland and Schneider 2015).", "startOffset": 89, "endOffset": 198}, {"referenceID": 22, "context": "An embedding for the base kernel k also gives a simple embedding for the mean map kernel (Flaxman, Wang, and Smola 2015; Jitkrittum et al. 2015; Lopez-Paz et al. 2015; Sutherland and Schneider 2015).", "startOffset": 89, "endOffset": 198}, {"referenceID": 35, "context": "An embedding for the base kernel k also gives a simple embedding for the mean map kernel (Flaxman, Wang, and Smola 2015; Jitkrittum et al. 2015; Lopez-Paz et al. 2015; Sutherland and Schneider 2015).", "startOffset": 89, "endOffset": 198}, {"referenceID": 19, "context": "Approximate embeddings Recent interest in approximate kernel embeddings was spurred by the \u201crandom kitchen sink\u201d (RKS) embedding (Rahimi and Recht 2007), which approximates shift-invariant kernelsK on R by sampling their Fourier transform. A related line of work considers additive kernels, of the form K(x, y) = \u2211` j=1 \u03ba(xj , yj), usually defined on R\u22650 (e.g. histograms). Maji and Berg (2009) construct an embedding for the intersection kernel \u2211` j=1 min(xj , yj) via step functions.", "startOffset": 210, "endOffset": 395}, {"referenceID": 19, "context": "Approximate embeddings Recent interest in approximate kernel embeddings was spurred by the \u201crandom kitchen sink\u201d (RKS) embedding (Rahimi and Recht 2007), which approximates shift-invariant kernelsK on R by sampling their Fourier transform. A related line of work considers additive kernels, of the form K(x, y) = \u2211` j=1 \u03ba(xj , yj), usually defined on R\u22650 (e.g. histograms). Maji and Berg (2009) construct an embedding for the intersection kernel \u2211` j=1 min(xj , yj) via step functions. Vedaldi and Zisserman (2010) consider any homogeneous \u03ba, so that \u03ba(tx, ty) = t \u03ba(x, y), which also allows them to embed histogram kernels such as the additive \u03c7 kernel and Jensen-Shannon divergence.", "startOffset": 210, "endOffset": 515}, {"referenceID": 6, "context": "Their embedding uses the same fundamental result of Fuglede (2005) as ours; we expand to the continuous rather than the discrete case.", "startOffset": 52, "endOffset": 67}, {"referenceID": 6, "context": "Their embedding uses the same fundamental result of Fuglede (2005) as ours; we expand to the continuous rather than the discrete case. Vempati et al. (2010) later apply RKS embeddings to obtain generalized RBF kernels (1).", "startOffset": 52, "endOffset": 157}, {"referenceID": 6, "context": "Their embedding uses the same fundamental result of Fuglede (2005) as ours; we expand to the continuous rather than the discrete case. Vempati et al. (2010) later apply RKS embeddings to obtain generalized RBF kernels (1). For embeddings of kernels on input spaces other than R, the RKS embedding extends naturally to locally compact abelian groups (Li, Ionescu, and Sminchisescu 2010). Oliva et al. (2014) embedded an estimate of the L2 distance between continuous densities via orthonormal basis functions.", "startOffset": 52, "endOffset": 407}, {"referenceID": 6, "context": "d is a Hilbertian metric (Fuglede 2005), so K is positive definite (Haasdonk and Bahlmann 2004).", "startOffset": 25, "endOffset": 39}, {"referenceID": 9, "context": "d is a Hilbertian metric (Fuglede 2005), so K is positive definite (Haasdonk and Bahlmann 2004).", "startOffset": 67, "endOffset": 95}, {"referenceID": 6, "context": "Embedding HDDs into L2 Fuglede (2005) shows that \u03ba corresponds to a bounded measure \u03bc(\u03bb), as in Table 1, with", "startOffset": 23, "endOffset": 38}, {"referenceID": 31, "context": "Embedding RBF Kernels into RD The A features approximate the HDD (2) in R |V |; thus applying the RKS embedding (Rahimi and Recht 2007) to the A features will approximate our generalized RBF kernel (1).", "startOffset": 112, "endOffset": 135}, {"referenceID": 35, "context": "There are two versions of the embedding in common use, but this one is preferred (Sutherland and Schneider 2015).", "startOffset": 81, "endOffset": 112}, {"referenceID": 28, "context": "Taking |V | to be asymptotically O(n), ne = O(D), and M = O(1) for simplicity, this is O(NnD) time, compared to O(Nn log n+N) for the methods of P\u00f3czos et al. (2012b) and O(Nn) for Muandet et al.", "startOffset": 145, "endOffset": 167}, {"referenceID": 25, "context": "(2012b) and O(Nn) for Muandet et al. (2012).", "startOffset": 22, "endOffset": 44}, {"referenceID": 7, "context": "We use a suitable form of kernel density estimation, to obtain a uniform error bound with a rate based on the function C\u22121 (Gin\u00e9 and Guillou 2002).", "startOffset": 123, "endOffset": 146}, {"referenceID": 25, "context": "We also try the MMD distance (Muandet et al. 2012) with approximate kernel embeddings:", "startOffset": 29, "endOffset": 50}, {"referenceID": 21, "context": "First, we try our JS, Hellinger, and TV embeddings. We compare to L2 kernels as in Oliva et al. (2014): exp ( \u2212 1 2\u03c32 \u2016p\u2212 q\u2016 2 2 ) \u2248 z(~a(p\u0302))z(~a(q\u0302)) (L2).", "startOffset": 25, "endOffset": 103}, {"referenceID": 39, "context": "We further compare to RKS with histogram JS embeddings (Vempati et al. 2010) (Hist JS); we also tried \u03c7 embeddings, but their performance was quite similar.", "startOffset": 55, "endOffset": 76}, {"referenceID": 28, "context": "We finally try the full Gram matrix approach of P\u00f3czos et al. (2012b) with the KL estimator of Wang, Kulkarni, and Verd\u00fa (2009) in an RBF kernel (KL), as did Ntampaka et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 28, "context": "We finally try the full Gram matrix approach of P\u00f3czos et al. (2012b) with the KL estimator of Wang, Kulkarni, and Verd\u00fa (2009) in an RBF kernel (KL), as did Ntampaka et al.", "startOffset": 48, "endOffset": 128}, {"referenceID": 26, "context": "(2012b) with the KL estimator of Wang, Kulkarni, and Verd\u00fa (2009) in an RBF kernel (KL), as did Ntampaka et al. (2015).", "startOffset": 96, "endOffset": 119}, {"referenceID": 27, "context": "Estimating the Number of Mixture Components We will now illustrate the efficacy of HDD random features in a regression task, following Oliva et al. (2014): estimate the number of components from a mixture of truncated Gaussians.", "startOffset": 135, "endOffset": 155}, {"referenceID": 0, "context": "Note that fitting mixtures with EM and selecting a number of components using AIC (Akiake 1973) or BIC (Schwarz 1978) performed much worse than regression; only AIC with |\u03c7i| = 800 outperformed a constant predictor of 5.", "startOffset": 82, "endOffset": 95}, {"referenceID": 32, "context": "Note that fitting mixtures with EM and selecting a number of components using AIC (Akiake 1973) or BIC (Schwarz 1978) performed much worse than regression; only AIC with |\u03c7i| = 800 outperformed a constant predictor of 5.", "startOffset": 103, "endOffset": 117}, {"referenceID": 37, "context": "Note that the histogram embeddings used an optimized C implementation (Vedaldi and Fulkerson 2008), as did the KL kernel2, while the HDD embeddings used a simple Matlab implementation.", "startOffset": 70, "endOffset": 98}, {"referenceID": 16, "context": "We took the \u201ccat\u201d and \u201cdog\u201d classes from the CIFAR-10 dataset (Krizhevsky and Hinton 2009), and represented each 32 \u00d7 32 image by a set of triples (x, y, v), where x and y are the position of each pixel in the image and v the pixel value after converting to grayscale.", "startOffset": 62, "endOffset": 90}, {"referenceID": 21, "context": "Scene Classification Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n\u00d7h\u00d7w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how P\u00f3czos et al. (2012b) and Muandet et al.", "startOffset": 137, "endOffset": 540}, {"referenceID": 21, "context": "Scene Classification Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n\u00d7h\u00d7w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how P\u00f3czos et al. (2012b) and Muandet et al. (2012) used SIFT features from image patches.", "startOffset": 137, "endOffset": 566}, {"referenceID": 21, "context": "Scene Classification Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n\u00d7h\u00d7w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how P\u00f3czos et al. (2012b) and Muandet et al. (2012) used SIFT features from image patches. Wu, Gao, and Liu (2015) set accuracy records on several scene classification datasets with a particular ad-hoc method of extracting features from distributions (D3); we compare to our more principled alternatives.", "startOffset": 137, "endOffset": 629}, {"referenceID": 33, "context": "We consider the Scene-15 dataset (Lazebnik, Schmid, and Ponce 2006), which contains 4 485 natural images in 15 location categories, and follow Wu, Gao, and Liu in extracting features from the last convolutional layer of the imagenet-vgg-verydeep-16 model (Simonyan and Zisserman 2015).", "startOffset": 255, "endOffset": 284}, {"referenceID": 43, "context": "48, which trained on over 7 million external images (Zhou et al. 2014).", "startOffset": 52, "endOffset": 70}], "year": 2015, "abstractText": "Many interesting machine learning problems are best posed by considering instances that are distributions, or sample sets drawn from distributions. Previous work devoted to machine learning tasks with distributional inputs has done so through pairwise kernel evaluations between pdfs (or sample sets). While such an approach is fine for smaller datasets, the computation of an N \u00d7N Gram matrix is prohibitive in large datasets. Recent scalable estimators that work over pdfs have done so only with kernels that use Euclidean metrics, like the L2 distance. However, there are a myriad of other useful metrics available, such as total variation, Hellinger distance, and the Jensen-Shannon divergence. This work develops the first random features for pdfs whose dot product approximates kernels using these non-Euclidean metrics, allowing estimators using such kernels to scale to large datasets by working in a primal space, without computing large Gram matrices. We provide an analysis of the approximation error in using our proposed random features and show empirically the quality of our approximation both in estimating a Gram matrix and in solving learning tasks in real-world and synthetic data. Introduction As machine learning matures, focus has shifted towards datasets with richer, more complex instances. For example, a great deal of effort has been devoted to learning functions on vectors of a large fixed dimension. While complex static vector instances are useful in a myriad of applications, many machine learning problems are more naturally posed by considering instances that are distributions, or sets drawn from distributions. Political scientists can learn a function from community demographics to vote percentages to understand who supports a candidate (Flaxman, Wang, and Smola 2015). The mass of dark matter halos can be inferred from the velocity of galaxies in a cluster (Ntampaka et al. 2015). Expensive expectation propagation messages can be sped up by learning a \u201cjust-in-time\u201d regression model (Jitkrittum et al. 2015). All of these applications are aided by working directly over sets drawn from the distribution of interest, rather than having to develop a per-problem ad-hoc set of summary statistics. \u2217These two authors contributed equally. Distributions are inherently infinite-dimensional objects, since in general they require an infinite number of parameters for their exact representation. Hence, it is not immediate how to extend traditional finite vector technique machine learning techniques to distributional instances. However, recent work has provided various approaches for dealing with distributional data in a nonparametric fashion. For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing (P\u00f3czos et al. 2012a; Oliva, P\u00f3czos, and Schneider 2013), and many learning tasks can be solved with RKHS approaches (Muandet et al. 2012; P\u00f3czos et al. 2012b). A major shortcoming of both approaches is that they require computing N kernel evaluations per prediction, where N is the number of training instances in a dataset. Often, this implies that one must compute a N \u00d7 N Gram matrix of pairwise kernel evaluations. Such approaches fail to scale to datasets where the number of instances N is very large. Another shortcoming of these approaches is that they are often based on Euclidean metrics, either working over a linear kernel, or one based on the L2 distance over distributions. While such kernels are useful in certain applications, better performance can sometimes be obtained by considering non-Euclidean based kernels. To this end, P\u00f3czos et al. (2012b) use a kernel based on R\u00e9nyi divergences; however, this kernel is not positive semi-definite (PSD), leading to even higher computational cost and other practical issues. This work addresses these major shortcomings by developing an embedding of random features for distributions. The dot product of the random features for two distributions will approximate kernels based on various distances between densities (see Figure 1). With this technique, we can approximate kernels based on total variation, Hellinger, and Jensen-Shannon divergences, among others. Since there is then no need to compute a Gram matrix, one will be able to use these kernels while still scaling to datasets with a", "creator": "TeX"}}}