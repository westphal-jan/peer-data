{"id": "1509.09011", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring", "abstract": "souvenir Partial entomologique monitoring kuang is a kinzler general model for sequential learning leeson with 20-time limited ingram feedback formalized as excoriated a akie game mal\u00ebsi between two players. bco In this kelian game, kpcs the compels learner rosecroft chooses unemotionally an action sea-bed and at baya the same time ibragimova the opponent stossel chooses 108-member an outcome, plantarum then morgul the bechard learner enclosing suffers a 170th loss multiuse and kambi receives a alenichev feedback sirius/xm signal. The bazoft goal yona of the bushrod learner is to cantones minimize the ammi total comping loss. In this 63.59 paper, grigori we study monico partial bilos monitoring part-of-speech with finite nimis actions and stochastic cercano outcomes. We redness derive a logarithmic superwoman distribution - dependent 6,000-strong regret 90.02 lower bound correlated that nambiar defines sauros the udea hardness ailleurs of the reserves problem. halaf Inspired by half-size the borden DMED algorithm (ybarra Honda and Takemura, zavodskoi 2010) for the domaine multi - kidron armed gasps bandit razafimahaleo problem, karitani we wasif propose 61-50 PM - stockbrokers DMED, an algorithm cromemco that minimizes the distribution - dependent regret. PM - DMED significantly oberkassel outperforms plumtree state - swamithoppe of - 22-31 the - shaqaqi art algorithms dalman in mederake numerical gang-raped experiments. clamart To show husen the optimality trips of PM - hygrocybe DMED alfonso with british-irish respect crystallex to unlikeliest the regret strategems bound, freudenstadt we chea slightly manuputty modify jaan the algorithm deans by leschly introducing a haikus hinge reichstadt function (aylett PM - DMED - Hinge ). meromictic Then, we derive intertie an erj-190 asymptotically 6.2-million optimal interland regret upper tiflis bound kgs of PM - DMED - Hinge aliye that matches the zx lower bound.", "histories": [["v1", "Wed, 30 Sep 2015 04:36:40 GMT  (1407kb)", "http://arxiv.org/abs/1509.09011v1", "24 pages, to appear in NIPS2015"]], "COMMENTS": "24 pages, to appear in NIPS2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["junpei komiyama", "junya honda", "hiroshi nakagawa"], "accepted": true, "id": "1509.09011"}, "pdf": {"name": "1509.09011.pdf", "metadata": {"source": "CRF", "title": "Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring", "authors": ["Junpei Komiyama", "Junya Honda"], "emails": ["junpei@komiyama.info", "honda@stat.t.u-tokyo.ac.jp", "nakagawa@dl.itc.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n09 01\n1v 1\n[ st\nat .M\nL ]\nPartial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PMDMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound."}, {"heading": "1 Introduction", "text": "Partial monitoring is a general framework for sequential decision making problems with imperfect feedback. Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.\nPartial monitoring is formalized as a repeated game played by two players called a learner and an opponent. At each round, the learner chooses an action, and at the same time the opponent chooses an outcome. Then, the learner observes a feedback signal from a given set of symbols and suffers some loss, both of which are deterministic functions of the selected action and outcome.\nThe goal of the learner is to find the optimal action that minimizes his/her cumulative loss. Alternatively, we can define the regret as the difference between the cumulative losses of the learner and the single optimal action, and minimization of the loss is equivalent to minimization of the regret. A learner with a small regret balances exploration (acquisition of information about the strategy of the opponent) and exploitation (utilization of information). The rate of regret indicates how fast the learner adapts to the problem: a linear regret indicates the inability of the learner to find the optimal action, whereas a sublinear regret indicates that the learner can approach the optimal action given sufficiently large time steps.\nThe study of partial monitoring is classified into two settings with respect to the assumption on the outcomes. On one hand, in the stochastic setting, the opponent chooses an outcome distribution before the game starts, and an outcome at each round is an i.i.d. sample from the distribution. On the other hand, in the adversarial setting, the opponent chooses the outcomes to maximize the regret of the learner. In this paper, we study the former setting."}, {"heading": "1.1 Related work", "text": "The paper by Piccolboni and Schindelhauer [8] is one of the first to study the regret of the finite partial monitoring problem. They proposed the FeedExp3 algorithm, which attains O(T 3/4) minimax regret on some problems. This bound was later improved by Cesa-Bianchi et al. [9] to O(T 2/3), who also showed an instance in which the bound is optimal. Since then, most literature on partial monitoring has dealt with the minimax regret, which is the worst-case regret over all possible opponent\u2019s strategies. Barto\u0301k et al. [10] classified the partial monitoring problems into four categories in terms of the minimax regret: a trivial problem with zero regret, an easy problem with \u0398\u0303( \u221a T ) regret1, a hard problem with \u0398(T 2/3) regret, and a hopeless problem with \u0398(T ) regret. This shows that the class of the partial monitoring problems is not limited to the bandit sort but also includes larger classes of problems, such as dynamic pricing. Since then, several algorithms with a O\u0303( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13]. Among them, the Bayes-update Partial Monitoring (BPM) algorithm [13] is state-of-the-art in the sense of empirical performance.\nDistribution-dependent and minimax regret: we focus on the distribution-dependent regret that depends on the strategy of the opponent. While the minimax regret in partial monitoring has been extensively studied, little has been known on distribution-dependent regret in partial monitoring. To the authors\u2019 knowledge, the only paper focusing on the distribution-dependent regret in finite discrete partial monitoring is the one by Barto\u0301k et al. [11], which derivedO(logT ) distribution-dependent regret for easy problems. In contrast to this situation, much more interest in the distribution-dependent regret has been shown in the field of multi-armed bandit problems. Upper confidence bound (UCB), the most well-known algorithm for the multi-armed bandits, has a distribution-dependent regret bound [2, 14], and algorithms that minimize the distribution-dependent regret (e.g., KL-UCB) has been shown to perform better than ones that minimize the minimax regret (e.g., MOSS), even in instances in which the distributions are hard to distinguish (e.g., Scenario 2 in Garivier et al. [15]). Therefore, in the field of partial monitoring, we can expect that an algorithm that minimizes the distribution-dependent regret would perform better than the existing ones.\nContribution: the contributions of this paper lie in the following three aspects. First, we derive the regret lower bound: in some special classes of partial monitoring (e.g., multi-armed bandits), an O(logT ) regret lower bound is known to be achievable. In this paper, we further extend this lower bound to obtain a regret lower bound for general partial monitoring problems. Second, we propose an algorithm called Partial Monitoring DMED (PM-DMED). We also introduce a slightly modified version of this algorithm (PM-DMED-Hinge) and derive its regret bound. PM-DMED-Hinge is the first algorithm with a logarithmic regret bound for hard problems. Moreover, for both easy and hard problems, it is the first algorithm with the optimal constant factor on the leading logarithmic term. Third, performances of PM-DMED and existing algorithms are compared in numerical experiments. Here, the partial monitoring problems consisted of three specific instances of varying difficulty. In all instances, PM-DMED significantly outperformed the existing methods when a number of rounds is large. The regret of PM-DMED on these problems quickly approached the theoretical lower bound."}, {"heading": "2 Problem Setup", "text": "This paper studies the finite stochastic partial monitoring problem with N actions, M outcomes, and A symbols. An instance of the partial monitoring game is defined by a loss matrix L = (li,j) \u2208 R\nN\u00d7M and a feedback matrix H = (hi,j) \u2208 [A]N\u00d7M , where [A] = {1, 2, . . . , A}. At the beginning, the learner is informed of L and H . At each round t = 1, 2, . . . , T , a learner selects an action i(t) \u2208 [N ], and at the same time an opponent selects an outcome j(t) \u2208 [M ]. The learner\n1Note that \u0398\u0303 ignores a polylog factor.\nsuffers loss li(t),j(t), which he/she cannot observe: the only information the learner receives is the signal hi(t),j(t) \u2208 [A]. We consider a stochastic opponent whose strategy for selecting outcomes is governed by the opponent\u2019s strategy p\u2217 \u2208 PM , where PM is a set of probability distributions over an M -ary outcome. The outcome j(t) of each round is an i.i.d. sample from p\u2217.\nThe goal of the learner is to minimize the cumulative loss over T rounds. Let the optimal action be the one that minimizes the loss in expectation, that is, i\u2217 = arg mini\u2208[N ] L \u22a4 i p\n\u2217, where Li is the i-th row of L. Assume that i\u2217 is unique. Without loss of generality, we can assume that i\u2217 = 1. Let \u2206i = (Li\u2212L1)\u22a4p\u2217 \u2208 [0,\u221e) and Ni(t) be the number of rounds before the t-th in which action i is selected. The performance of the algorithm is measured by the (pseudo) regret,\nRegret(T ) =\nT\u2211\nt=1\n\u2206i(t) = \u2211\ni\u2208[N ]\n\u2206iNi(T + 1),\nwhich is the difference between the expected loss of the learner and the optimal action 1. It is easy to see that minimizing the loss is\nequivalent to minimizing the regret. The expectation of the regret measures the performance of an algorithm that the learner uses.\nFor each action i \u2208 [N ], let Ci be the set of opponent strategies for which action i is optimal: Ci = {q \u2208 PM : \u2200j 6=i(Li \u2212 Lj)\u22a4q \u2264 0}.\nWe call Ci the optimality cell of action i. Each optimality cell is a convex closed polytope. Furthermore, we call the set of optimality cells {C1, . . . , CN} the cell decomposition as shown in Figure 1. Let Cci = PM \\ Ci be the set of strategies with which action i is not optimal. The signal matrix Si \u2208 {0, 1}A\u00d7M of action i is defined as (Si)k,j = 1[hi,j = k], where 1[X ] = 1 if X is true and 0 otherwise. The signal matrix defined here is slightly different from the one in the previous papers (e.g., Barto\u0301k et al. [10]) in which the number of rows of Si is the number of the different symbols in the i-th row of H . The advantage in using the definition here is that, Sip\n\u2217 \u2208 RA is a probability distribution over symbols that the algorithm observes when it selects an action i. Examples of signal matrices are shown in Section 5. An instance of partial monitoring is globally observable if for all pairs i, j of actions, Li \u2212 Lj \u2208 \u2295k\u2208[N ]ImS\u22a4k . In this paper, we exclusively deal with globally observable instances: in view of the minimax regret, this includes trivial, easy, and hard problems."}, {"heading": "3 Regret Lower Bound", "text": "A good algorithm should work well against any opponent\u2019s strategy. We extend this idea by introducing the notion of strong consistency: a partial monitoring algorithm is strongly consistent if it satisfies E[Regret(T )] = o(T a) for any a > 0 and p \u2208 PM given L and H . In the context of the multi-armed bandit problem, Lai and Robbins [2] derived the regret lower bound of a strongly consistent algorithm: an algorithm must select each arm i until its number of draws Ni(t) satisfies log t . Ni(t)d(\u03b8i\u2016\u03b81), where d(\u03b8i\u2016\u03b81) is the KL divergence between the two one-parameter distributions from which the rewards of action i and the optimal action are generated. Analogously, in the partial monitoring problem, we can define the minimum number of observations.\nLemma 1. For sufficiently large T , a strongly consistent algorithm satisfies:\n\u2200q\u2208Cc1 \u2211\ni\u2208[N ]\nE[Ni(T )]D(p \u2217 i \u2016Siq) \u2265 logT \u2212 o(logT ),\nwhere p\u2217i = Sip \u2217 and D(p\u2016q) = \u2211i(p)i log ((p)i/(q)i) is the KL divergence between two discrete distributions, in which we define 0 log 0/0 = 0.\nLemma 1 can be interpreted as follows: for each round t, consistency requires the algorithm to make sure that the possible risk that action i 6= 1 is optimal is smaller than 1/t. Large deviation principle [16] states that, the probability that an opponent with strategy q behaves like p\u2217 is\nroughly exp (\u2212\u2211iNi(t)D(p\u2217i \u2016Siq)). Therefore, we need to continue exploration of the actions until \u2211 iNi(t)D(p \u2217 i \u2016Siq) \u223c log t holds for any q \u2208 Cc1 to reduce the risk to exp (\u2212 log t) = 1/t.\nThe proof of Lemma 1 is in Appendix B in the supplementary material. Based on the technique used in Lai and Robbins [2], the proof considers a modified game in which another action i 6= 1 is optimal. The difficulty in proving the lower bound in partial monitoring lies in that, the feedback structure can be quite complex: for example, to confirm the superiority of action 1 over 2, one might need to use the feedback from action 3 /\u2208 {1, 2}. Still, we can derive the lower bound by utilizing the consistency of the algorithm in the original and modified games.\nWe next derive a lower bound on the regret based on Lemma 1. Note that, the expectation of the regret can be expressed as E[Regret(T )] = \u2211 i6=1 E[Ni(t)](Li \u2212 L1)\u22a4p\u2217. Let\nRj({pi}) = { {ri}i6=j \u2208 [0,\u221e)N\u22121 : inf\nq\u2208cl(Ccj ):pj=Sjq\n\u2211\ni\nriD(pi\u2016Siq) \u2265 1 } ,\nwhere cl(\u00b7) denotes a closure. Moreover, let\nC\u2217j (p, {pi}) = inf ri\u2208Rj({pi})\n\u2211\ni6=j\nri(Li \u2212 Lj)\u22a4p ,\nthe optimal solution of which is\nR\u2217j (p, {pi}) = { {ri}i6=j \u2208 Rj({pi}) : \u2211\ni6=j\nri(Li \u2212 Lj)\u22a4p = C\u2217j (p, {pi}) } .\nThe value C\u22171 (p \u2217, {p\u2217i }) logT is the possible minimum regret for observations such that the minimum divergence of p\u2217 from any q \u2208 Cc1 is larger than log T . Using Lemma 1 yields the following regret lower bound:\nTheorem 2. The regret of a strongly consistent algorithm is lower bounded as:\nE[Regret(T )] \u2265 C\u22171 (p\u2217, {p\u2217i }) logT \u2212 o(logT ).\nFrom this theorem, we can naturally measure the harshness of the instance byC\u22171 (p \u2217, {p\u2217i }), whereas the past studies (e.g., Vanchinathan et al. [13]) ambiguously define the harshness as the closeness to the boundary of the cells. Furthermore, we show in Lemma 5 in the Appendix that C\u22171 (p\n\u2217, {p\u2217i }) = O(N/\u2016p\u2217 \u2212 Cc1\u20162M ): the regret bound has at most quadratic dependence on \u2016p\u2217 \u2212 Cc1\u2016M , which is defined in Appendix D as the closeness of p\u2217 to the boundary of the optimal cell."}, {"heading": "4 PM-DMED Algorithm", "text": "In this section, we describe the partial monitoring deterministic minimum empirical divergence (PMDMED) algorithm, which is inspired by DMED [17] for solving the multi-armed bandit problem. Let p\u0302i(t) \u2208 [0, 1]A be the empirical distribution of the symbols under the selection of action i. Namely, the k-th element of p\u0302i(t) is ( \u2211t\u22121 t\u2032=1 1[i(t \u2032) = i\u2229hi(t\u2032),j(t\u2032) = k])/( \u2211t\u22121 t\u2032=1 1[i(t \u2032) = i]). We sometimes omit t from p\u0302i when it is clear from the context. Let the empirical divergence of q \u2208 PM be \u2211 i\u2208[N ] Ni(t)D(p\u0302i(t)\u2016Siq), the exponential of which can be considered as a likelihood that q is the opponent\u2019s strategy.\nThe main routine of PM-DMED is in Algorithm 1. At each loop, the actions in the current list ZC are selected once. The list for the actions in the next loop ZN is determined by the subroutine in Algorithm 2. The subroutine checks whether the empirical divergence of each point q \u2208 Cc1 is larger than log t or not (Eq. (3)). If it is large enough, it exploits the current information by selecting i\u0302(t), the optimal action based on the estimation p\u0302(t) that minimizes the empirical divergence. Otherwise, it selects the actions with the number of observations below the minimum requirement for making the empirical divergence of each suboptimal point q \u2208 Cc1 larger than log t. Unlike the N -armed bandit problem in which a reward is associated with an action, in the partial monitoring problem, actions, outcomes, and feedback signals can be intricately related. Therefore, we need to solve a non-trivial optimization to run PM-DMED. Later in Section 5, we discuss a practical implementation of the optimization.\nAlgorithm 1 Main routine of PM-DMED and PM-DMED-Hinge\n1: Initialization: select each action once. 2: ZC , ZR \u2190 [N ], ZN \u2190 \u2205. 3: while t \u2264 T do 4: for i(t) \u2208 ZC in an arbitrarily fixed order do 5: Select i(t), and receive feedback. 6: ZR \u2190 ZR \\ {i(t)}. 7: Add actions to ZN in accordance with{\nAlgorithm 2 (PM-DMED) Algorithm 3 (PM-DMED-Hinge) .\n8: t \u2190 t+ 1. 9: end for\n10: ZC , ZR \u2190 ZN , ZN \u2190 \u2205. 11: end while\nAlgorithm 2 PM-DMED subroutine for adding actions to ZN (without duplication).\n1: Parameter: c > 0. 2: Compute an arbitrary p\u0302(t) such that\np\u0302(t)\u2208 arg min q\n\u2211\ni\nNi(t)D(p\u0302i(t)\u2016Siq) (1)\nand let i\u0302(t) = arg mini L \u22a4 i p\u0302(t).\n3: If i\u0302(t) /\u2208 ZR then put i\u0302(t) into ZN . 4: If there are actions i /\u2208 ZR such that\nNi(t) < c \u221a log t (2)\nthen put them into ZN . 5: If\n{Ni(t)/ log t}i6=i\u0302(t) /\u2208 Ri\u0302(t)({p\u0302i(t)}) (3) then compute some\n{r\u2217i }i6=i\u0302(t) \u2208 R\u2217i\u0302(t)(p\u0302(t), {p\u0302i(t)}) (4)\nand put all actions i such that i /\u2208 ZR and r\u2217i > Ni(t)/ log t into ZN .\nNecessity of \u221a logT exploration: PM-DMED tries to observe each action to some extent (Eq. (2)), which is necessary for the following reason: consider a four-state game characterized by\nL =  \n0 1 1 0 10 1 0 0 10 0 1 0 11 11 11 11\n  , H =  \n1 1 1 1 1 2 2 3 1 2 2 3 1 1 2 2\n  , and p\u2217 = (0.1, 0.2, 0.3, 0.4)\u22a4.\nThe optimal action here is action 1, which does not yield any useful information. By using action 2, one receives three kinds of symbols from which one can estimate (p\u2217)1, (p\u2217)2 + (p\u2217)3, and (p\u2217)4, where (p\u2217)j is the j-th component of p\u2217. From this, an algorithm can find that (p\u2217)1 is not very small and thus the expected loss of actions 2 and 3 is larger than that of action 1. Since the feedback of actions 2 and 3 are the same, one may also use action 3 in the same manner. However, the loss per observation is 1.2 and 1.3 for actions 2 and 3, respectively, and thus it is better to use action 2. This difference comes from the fact that (p\u2217)2 = 0.2 < 0.3 = (p\u2217)3. Since an algorithm does not know p\u2217 beforehand, it needs to observe action 4, the only source for distinguishing (p\u2217)2 from (p\u2217)3. Yet, an optimal algorithm cannot select it more than \u2126(logT ) times because it affects the O(logT ) factor in the regret. In fact, O((logT )a) observations of action 4 with some a > 0 are sufficient to be convinced that (p\u2217)2 < (p\u2217)3 with probability 1 \u2212 o(1/T poly(a)). For this reason, PM-DMED selects each action \u221a log t times."}, {"heading": "5 Experiment", "text": "Following Barto\u0301k et al. [11], we compared the performances of algorithms in three different games: the four-state game (Section 4), a three-state game and dynamic pricing. Experiments on the N - armed bandit game was also done, and the result is shown in Appendix C.1.\nThe three-state game, which is classified as easy in terms of the minimax regret, is characterized by:\nL = ( 1 1 0 0 1 1 1 0 1 ) and H = ( 1 2 2 2 1 2 2 2 1 ) .\nThe signal matrices of this game are,\nS1 = ( 1 0 0 0 1 1 ) , S2 = ( 0 1 0 1 0 1 ) , and S3 = ( 0 0 1 1 1 0 ) .\nDynamic pricing, which is classified as hard in terms of the minimax regret, is a game that models a repeated auction between a seller (learner) and a buyer (opponent). At each round, the seller sets a price for a product, and at the same time, the buyer secretly sets a maximum price he is willing to pay. The signal is \u201cbuy\u201d or \u201cno-buy\u201d, and the seller\u2019s loss is either a given constant (no-buy) or the difference between the buyer\u2019s and the seller\u2019s prices (buy). The loss and feedback matrices are:\nL =   0 1 . . . N \u2212 1 c 0 . . . N \u2212 2 ... . . . . . .\n... c . . . c 0\n  and H =   2 2 . . . 2 1 2 . . . 2 ... . . . . . .\n... 1 . . . 1 2\n  ,\nwhere signals 1 and 2 correspond to no-buy and buy. The signal matrix of action i is\nSi = ( i\u22121\ufe37 \ufe38\ufe38 \ufe37 1 . . . 1 0 . . . 0 M\u2212i+1\ufe37 \ufe38\ufe38 \ufe37 0 . . . 0 1 . . . 1 ) .\nFollowing Barto\u0301k et al. [11], we set N = 5,M = 5, and c = 2.\nIn our experiments with the three-state game and dynamic pricing, we tested three settings regarding the harshness of the opponent: at the beginning of a simulation, we sampled 1,000 points uniformly at random from PM , then sorted them by C\u22171 (p\u2217, {p\u2217i }). We chose the top 10%, 50%, and 90% harshest ones as the opponent\u2019s strategy in the harsh, intermediate, and benign settings, respectively.\nWe compared Random, FeedExp3 [8], CBP [11] with \u03b1 = 1.01, BPM-LEAST, BPM-TS [13], and PM-DMED with c = 1. Random is a naive algorithm that selects an action uniformly random. FeedExp3 requires a matrix G such that H\u22a4G = L\u22a4, and thus one cannot apply it to the four-state game. CBP is an algorithm of logarithmic regret for easy games. The parameters \u03b7 and f(t) of CBP were set in accordance with Theorem 1 in their paper. BPM-LEAST is a Bayesian algorithm with O\u0303( \u221a T ) regret for easy games, and BPM-TS is a heuristic of state-of-the-art performance. The priors of two BPMs were set to be uninformative to avoid a misspecification, as recommended in their paper.\nAlgorithm 3 PM-DMED-Hinge subroutine for adding actions to ZN (without duplication).\n1: Parameters: c > 0, f(n) = bn\u22121/2 for b > 0, \u03b1(t) = a/(log log t) for a > 0. 2: Compute arbitrary p\u0302(t) which satisfies\np\u0302(t) \u2208 arg min q\n\u2211\ni\nNi(t)(D(p\u0302i(t)\u2016Siq)\u2212 f(Ni(t)))+ (5)\nand let i\u0302(t) = arg mini L \u22a4 i p\u0302(t).\n3: If i\u0302(t) /\u2208 ZR then put i\u0302(t) into ZN . 4: If\np\u0302(t) /\u2208 Ci\u0302(t),\u03b1(t) (6) or there exists an action i such that\nD(p\u0302i(t)\u2016Sip\u0302(t)) > f(Ni(t)) (7) then put all actions i /\u2208 ZR into ZN .\n5: If there are actions i such that Ni(t) < c \u221a log t (8)\nthen put the actions not in ZR into ZN . 6: If\n{Ni(t)/ log t}i6=i\u0302(t) /\u2208 Ri\u0302(t)({p\u0302i(t), f(Ni(t))}) (9) then compute some\n{r\u2217i }i6=i\u0302(t) \u2208 R\u2217i\u0302(t)(p\u0302(t), {p\u0302i(t), f(Ni(t))}) (10)\nand put all actions such that i /\u2208 ZR and r\u2217i > Ni(t)/ log t into ZN . If such r\u2217i is infeasible then put all action i /\u2208 ZR into ZN .\nThe computation of p\u0302(t) in (1) and the evaluation of the condition in (3) involve convex optimizations, which were done with Ipopt [18]. Moreover, obtaining {r\u2217i } in (4) is classified as a linear semi-infinite programming (LSIP) problem, a linear programming (LP) with finitely many variables and infinitely many constraints. Following the optimization of BPM-LEAST [13], we resorted to a finite sample approximation and used the Gurobi LP solver [19] in computing {r\u2217i }: at each round, we sampled 1,000 points from PM , and relaxed the constraints on the samples. To speed up the computation, we skipped these optimizations in most rounds with large t and used the result of the last computation. The computation of the coefficient C\u22171 (p\n\u2217, {p\u2217i }) of the regret lower bound (Theorem 2) is also an LSIP, which was approximated by 100,000 sample points from Cc1. The experimental results are shown in Figure 2. In the four-state game and the other two games with an easy or intermediate opponent, PM-DMED outperforms the other algorithms when the number of rounds is large. In particular, in the dynamic pricing game with an intermediate opponent, the regret of PM-DMED at T = 106 is ten times smaller than those of the other algorithms. Even in the harsh setting in which the minimax regret matters, PM-DMED has some advantage over all algorithms except for BPM-TS. With sufficiently large T , the slope of an optimal algorithm should converge to LB. In all games and settings, the slope of PM-DMED converges to LB, which is empirical evidence of the optimality of PM-DMED."}, {"heading": "6 Theoretical Analysis", "text": "Section 5 shows that the empirical performance of PM-DMED is very close to the regret lower bound in Theorem 2. Although the authors conjecture that PM-DMED is optimal, it is hard to analyze PM-DMED. The technically hardest part arises from the case in which the divergence of each action is small but not yet fully converged. To circumvent this difficulty, we can introduce a discount factor. Let\nRj({pi, \u03b4i})= { {ri}i6=j \u2208 [0,\u221e)N\u22121 : inf\nq\u2208cl(Ccj ):D(pj\u2016Sjq)\u2264\u03b4j\n\u2211\ni\nri(D(pi\u2016Siq)\u2212\u03b4i)+ \u2265 1 } , (11)\nwhere (X)+ = max(X, 0). Note that Rj({pi, \u03b4i}) in (11) is a natural generalization of Rj({pi}) in Section 4 in the sense that Rj({pi, 0}) = Rj({pi}). Event {Ni(t)/ log t}i6=1 \u2208 R1({p\u0302i(t), \u03b4i}) means that the number of observations {Ni(t)} is enough to ensure that the \u201c{\u03b4i}-discounted\u201d empirical divergence of each q \u2208 Cc1 is larger than log t. Analogous to Rj({pi, \u03b4i}), we define\nC\u2217j (p, {pi, \u03b4i}) = inf {ri}i6=j\u2208Rj({pi,\u03b4i}))\n\u2211\ni6=j\nri(Lj \u2212 Li)\u22a4p\nand its optimal solution by\nR\u2217j (p, {pi, \u03b4i}) = { {ri}i6=j \u2208 Rj({pi, \u03b4i}) : \u2211\ni6=j\nri(Lj \u2212 Li)\u22a4p = C\u2217j (p, {pi, \u03b4i}) } .\nWe also define Ci,\u03b1 = {p \u2208 PM : L\u22a4i p + \u03b1 \u2264 minj 6=i L\u22a4j p}, the optimal region of action i with margin. PM-DMED-Hinge shares the main routine of Algorithm 1 with PM-DMED and lists the next actions by Algorithm 3. Unlike PM-DMED, it (i) discounts f(Ni(t)) from the empirical divergenceD(p\u0302i(t)\u2016Siq). Moreover, (ii) when p\u0302(t) is close to the cell boundary, it encourages more exploration to identify the cell it belongs to by Eq. (6).\nTheorem 3. Assume the following regularity conditions hold for p\u2217. (1) R\u22171(p, {pi, \u03b4i}) is unique at p = p\u2217, pi = Sip\u2217, \u03b4i = 0. Moreover, (2) for S\u03b4 = {q : D(p\u22171\u2016S1q) \u2264 \u03b4}, it holds that cl(int(Cc1) \u2229 S\u03b4) = cl(cl(Cc1) \u2229 S\u03b4) for all \u03b4 \u2265 0 in some neighborhood of \u03b4 = 0, where cl(\u00b7) and int(\u00b7) denote the closure and the interior, respectively. Then,\nE[Regret(T )] \u2264 C\u22171 (p\u2217, {p\u2217i }) logT + o(logT ) .\nWe prove this theorem in Appendix D. Recall that R\u22171(p, {p\u0302i(t), \u03b4i}) is the set of optimal solutions of an LSIP. In this problem, KKT conditions and the duality theorem apply as in the case of finite constraints; thus, we can check whether Condition 1 holds or not for each p\u2217 (see, e.g., Ito et al. [20] and references therein). Condition 2 holds in most cases, and an example of an exceptional case is shown in Appendix A.\nTheorem 3 states that PM-DMED-Hinge has a regret upper bound that matches the lower bound of Theorem 2.\nCorollary 4. (Optimality in the N -armed bandit problem) In the N -armed Bernoulli bandit problem, the regularity conditions in Theorem 3 always hold. Moreover, the coefficient of the leading logarithmic term in the regret bound of the partial monitoring problem is equal to the bound given in Lai and Robbins [2]. Namely, C\u22171 (p \u2217, {p\u2217i }) = \u2211N\ni6=1(\u2206i/d(\u00b5i\u2016\u00b51)), where d(p\u2016q) = p log (p/q) + (1\u2212 p) log ((1 \u2212 p)/(1\u2212 q)) is the KL-divergence between Bernoulli distributions. Corollary 4, which is proven in Appendix C, states that PM-DMED-Hinge attains the optimal regret of the N -armed bandit if we run it on an N -armed bandit game represented as partial monitoring.\nAsymptotic analysis: it is Theorem 6 where we lose the finite-time property. This theorem shows the continuity of the optimal solution set R\u22171(p, {pi, \u03b4i}) of C\u2217j (p, {pj}), which does not mention how close R\u22171(p, {pi, \u03b4i}) is to R\u22171(p\u2217, {p\u2217i , 0}) if max{\u2016p\u2212p\u2217\u2016M ,maxi \u2016pi\u2212p\u2217i \u2016M ,maxi \u03b4i} \u2264 \u03b4 for given \u03b4. To obtain an explicit bound, we need sensitivity analysis, the theory of the robustness of the optimal value and the solution for small deviations of its parameters (see e.g., Fiacco [21]). In particular, the optimal solution of partial monitoring involves an infinite number of constraints, which makes the analysis quite hard. For this reason, we will not perform a finite-time analysis. Note that, the N -armed bandit problem is a special instance in which we can avoid solving the above optimization and a finite-time optimal bound is known.\nNecessity of the discount factor: we are not sure whether discount factor f(n) in PM-DMEDHinge is necessary or not. We also empirically tested PM-DMED-Hinge: although it is better than the other algorithms in many settings, such as dynamic pricing with an intermediate opponent, it is far worse than PM-DMED. We found that our implementation, which uses the Ipopt nonlinear optimization solver, was sometimes inaccurate at optimizing (5): there were some cases in which the true p\u2217 satisfies \u2200i\u2208[N ]D(p\u0302i(t)\u2016Sip\u2217) \u2212 f(Ni(t)) = 0, while the solution p\u0302(t) we obtained had non-zero hinge values. In this case, the algorithm lists all actions from (7), which degrades performance. Determining whether the discount factor is essential or not is our future work."}, {"heading": "Acknowledgements", "text": "The authors gratefully acknowledge the advice of Kentaro Minami and sincerely thank the anonymous reviewers for their useful comments. This work was supported in part by JSPS KAKENHI Grant Number 15J09850 and 26106506."}, {"heading": "A Case in which Condition 2 Does Not Hold", "text": "Figure 3 is an example that Theorem 3 does not cover. The dotted line is {q : p\u22171 = S1q}, which (accidentally) coincides with a line that makes the convex polytope of Cc1. In this case, Condition 2 does not hold because int(Cc1) \u2229 S0 = \u2205 whereas cl(Cc1) \u2229 S0 6= \u2205 (two starred points), which means that a slight modification of p\u2217 changes the set of cells that intersects with the dotted line discontinuously. We exclude these unusual cases for the ease of analysis.\nThe authors consider that it is quite hard to give the optimal regret bound without such regularity conditions. In fact, many regularity conditions are assumed in Graves and Lai [22], where another generalization of the bandit problem is considered and the regret lower bound is expressed in terms of LSIP. In this paper, the regularity conditions are much simplified by the continuity argument in Theorem 6 but it remains an open problem to fully remove them."}, {"heading": "B Proof: Regret Lower Bound", "text": "In this section, we prove Lemma 1 and Theorem 2.\nProof of Lemma 1. The technique here is mostly inspired from Theorem 1 in Lai and Robbins [2]. The use of a \u221a T term is inspired from Kaufmann et al. [23]. Let p\u2032 \u2208 int(Cc1) and i\u2032 6= 1 be the optimal action under the opponent\u2019s strategy p\u2032. We consider a modified partial monitoring game with its opponent\u2019s strategy is p\u2032.\nNotation: Let X\u0302mi \u2208 [A] is the signal of the m-th observation of action i. Let\nK\u0302Li(n) =\nn\u2211\nm=1\nlog\n( (Sip\n\u2217)X\u0302mi (Sip\u2032)X\u0302mi\n) ,\nand K\u0302L = \u2211\ni\u2208[N ] K\u0302Li(Ni(T )). Let P \u2032 and E\u2032 be the probability and the expectation with respect\nto the modified game, respectively. Then, for any event E ,\nP \u2032[E ] = E [ 1[E ] exp ( \u2212K\u0302L )] (12)\nholds. Now, let us define the following events:\nD1 =    \u2211\ni\u2208[N ]\nNi(T )D(Sip \u2217\u2016Sip\u2032) < (1\u2212 \u01eb) logT,Ni\u2032(T ) < \u221a T    ,\nD2 = { K\u0302L \u2264 ( 1\u2212 \u01eb\n2\n) logT } ,\nD12 = D1 \u2229 D2, D1\\2 = D1 \u2229 Dc2.\nFirst step (Pr[D12] = o(1)): from (12), P \u2032[D12] \u2265 E [ 1[D12] exp ( \u2212 ( 1\u2212 \u01eb\n2\n) logT )] = T\u2212(1\u2212\u01eb/2)Pr[D12].\nBy using this we have\nPr[D12] \u2264 T (1\u2212\u01eb/2)P\u2032[D12] \u2264 T (1\u2212\u01eb/2)P\u2032 [ Ni\u2032(T ) < \u221a T ]\n= T (1\u2212\u01eb/2)P\u2032 [ T \u2212Ni\u2032(T ) > T \u2212 \u221a T ]\n\u2264 T (1\u2212\u01eb/2)E \u2032[T \u2212Ni\u2032(T )] T \u2212 \u221a T\n(by the Markov inequality). (13)\nSince this algorithm is strongly consistent, E\u2032[T \u2212Ni\u2032(T )] \u2192 o(T a) for any a > 0. Therefore, the RHS of the last line of (13) is o(T a\u2212\u01eb/2), which, by choosing sufficiently small a, converges to zero as T \u2192 \u221e. In summary, Pr[D12] = o(1). Second step (Pr[D1\\2] = o(1)): we have Pr[D1\\2]\n= Pr\n  \u2211\ni\u2208[N ]\nNi(T )D(Sip \u2217\u2016Sip\u2032) < (1 \u2212 \u01eb) logT,Ni\u2032(T ) < \u221a T , \u2211\ni\u2208[N ]\nK\u0302Li(Ni(T )) > ( 1\u2212 \u01eb\n2\n) logT   .\nNote that\nmax 1\u2264n\u2264N K\u0302Li(n) = max 1\u2264n\u2264N\nn\u2211\nm=1\nlog\n( (Sip\n\u2217)X\u0302mi (Sip\u2032)X\u0302mi\n) ,\nis the maximum of the sum of positive-mean random variables, and thus converges to is average (c.f., Lemma 10.5 in [24]). Namely,\nlim N\u2192\u221e max 1\u2264n\u2264N\nK\u0302Li(n)\nN \u2192 D(Sip\u2217\u2016Sip\u2032)\nalmost surely. Therefore,\nlim T\u2192\u221e\nmax{Ni(T )}\u2208NN , \u2211 i\u2208[N ] Ni(T )D(Sip \u2217\u2016Sip\u2032)<(1\u2212\u01eb) log T \u2211 i\u2208[N ] K\u0302Li(Ni(T ))\nlogT \u2192 1\u2212 \u01eb\nalmost surely. By using this fact and 1\u2212 \u01eb/2 > 1\u2212 \u01eb, we have\nPr   max {Ni(T )}\u2208NN , \u2211 i\u2208[N ] Ni(T )D(Sip \u2217\u2016Sip\u2032)<(1\u2212\u01eb) log T \u2211\ni\u2208[N ]\nK\u0302Li(Ni(T )) > ( 1\u2212 \u01eb\n2\n) logT   = o(1).\nIn summary, we obtain Pr [ D1\\2 ] = o(1).\nLast step: we here have\nD1 =    \u2211\ni\u2208[N ]\nNi(T )D(Sip \u2217\u2016Sip\u2032) < (1\u2212 \u01eb) logT    \u2229 { Ni\u2032(T ) < \u221a T }\n\u2287    \u2211\ni\u2208[N ]\nNi(T )D(Sip \u2217\u2016Sip\u2032) + (1 \u2212 \u01eb) logT\u221a T Ni\u2032(T ) < (1\u2212 \u01eb) logT    ,\nwhere we used the fact that {A < C} \u2229 {B < C} \u2287 {A+ B < C} for A,B > 0 in the last line. Note that, by using the result of the previous steps, Pr[D1] = Pr[D12]+Pr[D1\\2] = o(1). By using the complementary of this fact,\nPr\n  \u2211\ni\u2208[N ]\nNi(T )D(Sip \u2217\u2016Sip\u2032) + (1 \u2212 \u01eb) logT\u221a T Ni\u2032(T ) \u2265 (1\u2212 \u01eb) logT\n  \u2265 Pr[Dc1] = 1\u2212 o(1).\nUsing the Markov inequality yields\nE\n  \u2211\ni\u2208[N ]\nNi(T )D(Sip \u2217\u2016Sip\u2032) + (1 \u2212 \u01eb) logT\u221a T Ni\u2032(T )\n  \u2265 (1\u2212 \u01eb)(1 \u2212 o(1)) logT. (14)\nBecause E[Ni\u2032 (T )] is subpolynomial as a function of T due to the consistency, the second term in LHS of (14) is o(1) and thus negligible. Lemma 1 follows from the fact that (14) holds for sufficiently small \u01eb and arbitrary p\u2032 \u2208 int(Cc1).\nProof of Theorem 2. Assume that there exists \u03b4 > 0 and a sequence T1 < T2 < T3 < \u00b7 \u00b7 \u00b7 such that for all t E[Regret(Tt)] < (1 \u2212 \u03b4)C\u22171 (p\u2217, {p\u2217i }) logTt , that is,\n\u2211\ni6=1\nE[Ni(Tt)]\n(1\u2212 \u03b4) log Tt (Li \u2212 L1)\u22a4p\u2217 < C\u22171 (p\u2217, {p\u2217i }) .\nFrom the definition of C\u22171 , there exists q \u2032 t \u2208 {q \u2208 cl(Cc1) : p\u22171 = Sjq} =: S such that\n\u2211\ni6=1\nE[Ni(Tt)]\n(1\u2212 \u03b4) logTt D(p\u2217i \u2016Siq\u2032t) < 1 .\nSince S is compact, there exists a subsequence t0 < t1 < \u00b7 \u00b7 \u00b7 such that limu\u2192\u221e q\u2032tu = q\u2032 for some q\u2032 \u2208 S. Therefore from the lower semicontinuity of the divergence we obtain\n1 \u2265 \u2211\ni6=1\nlim inf u\u2192\u221e\nE[Ni(Tt)]\n(1\u2212 \u03b4) logTt D(pi\u2016Siq\u2032tu)\n\u2265 \u2211\ni6=1\nlim inf t\u2192\u221e\nE[Ni(Tt)]\n(1\u2212 \u03b4) logTt D(pi\u2016Siq\u2032)\n= \u2211\ni\nlim inf t\u2192\u221e\nE[Ni(Tt)]\n(1\u2212 \u03b4) logTt D(pi\u2016Siq\u2032) ,\nwhich contradicts Lemma 1.\nC The N-armed Bandit Problem as Partial Monitoring\nIn Section 6, we have introduced PM-DMED-Hinge, an asymptotically optimal algorithm for partial monitoring. In this appendix, we prove that this algorithm also has an optimal regret bound of the N -armed bandit problem when we run it on an N -armed bandit game represented as an instance of partial monitoring.\nIn the N -armed bandit problem, the learner selects one of N actions (arms) and receives a corresponding reward. This problem can be considered as a special case of partial monitoring in which the learner directly observes the loss matrix. For example, three-armed Bernoulli bandit can be represented by the following loss and feedback matrices, and the strategy:\nL = H = ( 2 1 2 1 2 1 2 1 2 2 1 1 2 2 1 1 2 2 2 2 1 1 1 1 ) , and p\u2217 =   (1\u2212 \u00b51)(1\u2212 \u00b52)(1\u2212 \u00b53) \u00b51(1\u2212 \u00b52)(1\u2212 \u00b53) (1\u2212 \u00b51)\u00b52(1\u2212 \u00b53) \u00b51\u00b52(1\u2212 \u00b53) (1\u2212 \u00b51)(1 \u2212 \u00b52)\u00b53\n\u00b51(1\u2212 \u00b52)\u00b53 (1\u2212 \u00b51)\u00b52\u00b53\n\u00b51\u00b52\u00b53\n  , (15)\nwhere \u00b51, \u00b52, and \u00b53 are the expected rewards of the actions. Signals 1 and 2 correspond to the rewards of 1 and 0 generated by the selected arm, respectively. More generally, N -armed Bernoulli\nbandit is represented as an instance of partial monitoring in which the loss and feedback matrices are the same N \u00d7 2N matrix\nli,j = hi,j = 1[(j \u2212 1 mod 2i) < 2i\u22121] + 1, where mod denotes the modulo operation. This problem is associated with N parameters \u00b51, \u00b52, . . . , \u00b5N that correspond to the expected rewards of the actions. For the ease of analysis, we assume {\u00b5i} are in (0, 1) and different from each other. Without loss of generality, we assume 1 > \u00b51 > \u00b52 > \u00b7 \u00b7 \u00b7 > \u00b5N > 0, and thus action 1 is the optimal action. The opponent\u2019s strategy is\np\u2217j = \u220f\ni\u2208[N ]\n(\u00b5i + (1\u2212 2\u00b5i)1[(j \u2212 1 mod 2i) < 2i\u22121]) .\nNote that \u00b5i = (Sip\u2217)1.\nProof of Corollary 4. In the following, we prove that the regularity conditions in Theorem 3 are always satisfied in the case of the N -armed bandit. During the proof we also show that C\u22171 (p\n\u2217, {p\u2217i }) is equal to the optimal constant factor of Lai and Robbins [2].\nBecause signal 1 corresponds to the reward of 1, we can define \u00b5\u0302i(q) = (Siq)1, and thus\nCi = {q \u2208 PM : \u2200i\u2032 6=i \u00b5\u0302i(q) \u2265 \u00b5\u0302i\u2032 (q)}. First, we show the uniqueness of R\u22171(p, {pi, \u03b4i}) at p = p\u2217, {pi} = Sip\u2217, \u03b4i = 0. It is easy to check\nD(p\u2217i \u2016Siq) = d(\u00b5\u0302i(p\u2217)\u2016\u00b5\u0302i(q)) = d(\u00b5i\u2016\u00b5\u0302i(q)), where d(a\u2016b) is the KL divergence between two Bernoulli distributions with parameters a and b. Then\nR1({p\u2217i }) = { {ri}i6=1 \u2208 [0,\u221e)N\u22121 : inf\nq\u2208cl(Cc1):p \u2217 i =S1q\n\u2211\ni\nriD(p \u2217 i \u2016Siq) \u2265 1\n}\n= { {ri}i6=1 \u2208 [0,\u221e)N\u22121 : inf\nq\u2208cl(Cc1):\u00b51=\u00b5\u03021(q)\n\u2211\ni\nriD(p \u2217 i \u2016Siq) \u2265 1\n}\n= { {ri}i6=1 : ri \u2265\n1\nd(\u00b5i\u2016\u00b51)\n} , (16)\nwhere the last inequality follows from the fact that\n{q \u2208 cl(Cc1) : \u00b5\u03021(q) = \u00b51} = {q \u2208 PM : \u00b5\u03021(q) = \u00b51, \u2203i6=1\u00b5\u0302i(q) \u2265 \u00b51}. By Eq. (16), the regret minimizing solution is\nC\u22171 (p \u2217, {p\u2217i }) =\n\u2211\ni6=1\n\u2206i d(\u00b5i\u2016\u00b51) ,\nand\nR\u22171(p\u2217, {p\u2217i }) = { {ri}i6=1 : ri =\n1\nd(\u00b5i\u2016\u00b51)\n} ,\nwhich is unique.\nSecond, we show that cl(int(Cc1) \u2229 S\u03b4) = cl(cl(Cc1) \u2229 S\u03b4) for sufficiently small \u03b4 \u2265 0. Note that, cl(Cc1) \u2229 S\u03b4 = {q \u2208 PM : \u2203i\u2032 6=1 \u00b5\u03021(q) \u2264 \u00b5\u0302i\u2032(q), d(\u00b51\u2016\u00b5\u03021(q)) \u2264 \u03b4}\nand int(Cc1) \u2229 S\u03b4 = {q \u2208 PM : \u2203i\u2032 6=1 \u00b5\u03021(q) < \u00b5\u0302i\u2032(q), d(\u00b51\u2016\u00b5\u03021(q)) \u2264 \u03b4}.\nTo prove cl(Cc1) \u2229 S\u03b4 \u2282 cl(int(Cc1) \u2229 S\u03b4), (17)\nit suffices to show that, an open ball centered at any position in\n{q \u2208 PM : \u2203i\u2032 6=1 \u00b5\u03021(q) = \u00b5\u0302i\u2032(q), d(\u00b51\u2016\u00b5\u03021(q)) \u2264 \u03b4} \u2283 (cl(Cc1) \u2229 S\u03b4) \\ (int(Cc1) \u2229 S\u03b4)\ncontains a point in int(Cc1) \u2229 S\u03b4. This holds because we can make a slight move towards the direction of increasing \u00b5\u0302i\u2032 : we can always find q\u2032 in an open ball centered at q such that \u00b5\u0302i\u2032(q\n\u2032) > \u00b5\u0302i\u2032 (q) and \u00b5\u03021(q\u2032) = \u00b5\u03021(q) because of (i) the fact that there always exists q \u2208 PM such that {q \u2208 PM , \u2200i\u2208[N ]\u00b5\u0302i(q) = \u00b5i} for arbitrary {\u00b5i} \u2208 (0, 1)N and (ii) the continuity of the \u00b5\u0302i operator. Therefore, any open ball centered at q \u2208 cl(Cc1) \u2229 S\u03b4 contains an element of int(Cc1) \u2229 S\u03b4, by which we obtain (17). By using (17), we have\ncl(cl(Cc1) \u2229 S\u03b4) \u2282 cl(cl(int(Cc1) \u2229 S\u03b4)) = cl(int(Cc1) \u2229 S\u03b4), (18) where we used the fact that cl(cl(X)) = cl(X). Combining (18) with the fact that cl(cl(Cc1)\u2229S\u03b4) \u2283 cl(int(Cc1) \u2229 S\u03b4) yields cl(cl(Cc1) \u2229 S\u03b4) = cl(int(Cc1) \u2229 S\u03b4). Therefore, in the N -armed Bernoulli bandit problem, the regularity conditions are always satisfied and C\u22171 (p\n\u2217, {p\u2217i }) matches the optimal coefficient of the logarithmic regret bound. From Theorem 3, if we run PM-DMED-Hinge in this game, its expected regret is asymptotically optimal in view of the N -armed bandit problem.\nC.1 Experiment\nWe also assessed the performance of PM-DMED and other algorithms in solving the three-armed Bernoulli bandit game defined by (15) with parameters \u00b51 = 0.4, \u00b52 = 0.3, and \u00b53 = 0.2. The settings of the algorithms are the same as that of the main paper. The results of simulations are shown in Figure 4. LB-Theory is the regret lower bound of Lai and Robbins [2], that is, \u2211 i6=1 \u2206i log t d(\u00b5i\u2016\u00b51)\n. The slope of PM-DMED quickly approaches that of LB-Theory, which is empirical evidence that PM-DMED has optimal performance in N -armed bandits."}, {"heading": "D Optimality of PM-DMED-Hinge", "text": "In this appendix we prove Theorem 3. First we define distances among distributions. For distributions pi, p\u2032i \u2208 PA of symbols we use the total variation distance\n\u2016pi \u2212 p\u2032i\u2016 = 1\n2\nA\u2211\na=1\n|(pi)a \u2212 (p\u2032i)a| .\nFor distributions p, p\u2032 \u2208 PM of outcomes, we identify p with the set {p\u2032 : \u2200i, Sip = Sip\u2032} and define\n\u2016p\u2212 p\u2032\u2016M = max i \u2016Sip\u2212 Sip\u2032\u2016.\nFor Q \u2282 PM we define \u2016p\u2212Q\u2016M = inf\np\u2032\u2208Q \u2016p\u2212 p\u2032\u2016M .\nIn the following, we use Pinsker\u2019s inequality given below many times.\nD(pi\u2016qi) \u2265 2\u2016pi \u2212 qi\u20162 .\nLet\n\u03c1i,L = sup \u03bb>0\n1 \u03bb min x\u2208Ci,\u03bb \u2016x\u2212 Cci \u2016M ,\n\u03bdi,L = sup \u03bb>0\n1 \u03bb max x\u2208Cci,\u03bb \u2016x\u2212 Cci \u2016M .\nNote that these two constants are positive from the global observability.\nD.1 Properties of regret lower bound\nIn this section, we give Lemma 5 and Theorem 6 that are about the functions C\u2217j (p, {pi, \u03b4i}) and R\u2217j (p, {pi, \u03b4i}). In the following, we always consider these functions on p \u2208 PM , pi \u2208 {Sip : supp(p) \u2282 supp(p\u2217)} and \u03b4i \u2265 0, where supp(\u00b7) denotes the support of the distribution. We define\nLmax = max i\u2032,j\u2032 li\u2032,j\u2032 .\nLemma 5. Let p \u2208 Cj,\u03b1 and {pi, \u03b4i} be satisfying \u2016pi \u2212 Sip\u2016 \u2264 \u03b1\u03c1j,L/2 and \u03b4i \u2264 (\u03b1\u03c1j,L)2/4 for all i. Then\nC\u2217j (p, {pi, \u03b4i}) \u2264 4NLmax (\u03b1\u03c1j,L)2 . (19)\nFurtheremore, Rj({pi, \u03b4i}) is nonempty and\nR\u2217j (p, {pi, \u03b4i}) \u2282 [ 0,\n4NLmax (\u03c1j,L)2\u03b13\n]N\u22121 .\nProof of Lemma 5. Since \u2016p\u2212 Cc1\u2016M \u2265 \u03b1\u03c1j,L, there exists i = i(q) for any q \u2208 Cc1 such that \u2016Siq \u2212 Sip\u2016 \u2265 \u03b1\u03c1j,L .\nFor this i we have\nD(pi\u2016Siq)\u2212 \u03b4i \u2265 2\u2016pi \u2212 Siq\u20162 \u2212 \u03b4i \u2265 2(\u2016Siq \u2212 Sip\u2016 \u2212 \u2016pi \u2212 Sip\u2016)2+ \u2212 \u03b4i \u2265 (\u03b1\u03c1j,L)2/2\u2212 \u03b4i \u2265 (\u03b1\u03c1j,L)2/4 .\nThus, by letting ri = 4/(\u03b1\u03c1j,L\u03b1)2 for all i 6= j we have {ri}i6=j \u2208 Rj({pi, \u03b4i}) ,\nwhich implies (19). On the other hand it holds for any {r\u2217i }i6=j \u2208 R\u2217j (p, {pi, \u03b4i}) from p \u2208 Cj,\u03b1 that\nC\u2217j (p, {pi, \u03b4i}) = \u2211\ni6=j\nr\u2217i L \u22a4 i p \u2265 max i6=j r\u2217i \u03b1\nand therefore we have\nmax i6=j r\u2217i \u2264 4NLmax (\u03c1j,L)2\u03b13 .\nTheorem 6. Assume that the regularity conditions in Theorem 3 hold. Then the point-to-set map R\u22171(p, {pi, \u03b4i}) is (i) nonempty near p = p\u2217, pi = Sip\u2217, \u03b4i = 0 and (ii) continuous at p = p\u2217, pi = Sip \u2217, \u03b4i = 0.\nSee Hogan [25] for definitions of terms such as continuity of point-to-set maps.\nProof of Theorem 6. Define\nR\u03041({pi, \u03b4i}) =   {ri}i6=1 \u2208 [0, \u03be] N\u22121 : inf q\u2208cl(Cc1):D(p1\u2016S1q)\u2264\u03b41 \u2211\ni6=1\nri(D(pi\u2016Siq)\u2212 \u03b4i)+ \u2265 1   \nfor\n\u03be = 4NLmax\n(\u03c11,L)2(maxi6=1 L\u22a4i p \u2217 \u2212 L\u22a41 p\u2217)3\n.\nNote that p\u2217 \u2208 C1,\u03b1 for \u03b1 \u2264 maxi6=1 L\u22a4i p\u2217 \u2212 L\u22a41 p\u2217. From Lemma 5, near p = p\u2217, pi = Sip\u2217, \u03b4i = 0,\nR\u03041({pi, \u03b4i}) \u2283 R\u22171(p, {pi, \u03b4i}) and\nC\u22171 (p, {pi, \u03b4i}) = inf {ri}i6=1\u2208R\u03041({pi,\u03b4i})\n\u2211\ni6=1\nri(Li \u2212 L1)\u22a4p\nhold. Since the function \u2211\ni\nri(D(pi\u2016Siq)\u2212 \u03b4i)+\nis continuous in {ri}, R\u03041({pi, \u03b4i}) is a closed set and therefore R\u22171(p, {pi, \u03b4i}) is nonempty near p = p\u2217, pi = Sip \u2217, \u03b4i = 0.\nFrom the continuity of D(pi\u2016Siq) at any q such that D(pi\u2016Siq) < \u221e, we have\ninf q\u2208cl(Cc1)\u2229S\u03b41\n\u2211\ni6=1\nri(D(pi\u2016Siq)\u2212 \u03b4i)+ = inf q\u2208cl(cl(Cc1)\u2229S\u03b41 )\n\u2211\ni6=1\nri(D(pi\u2016Siq)\u2212 \u03b4i)+\n= inf q\u2208cl(int(Cc1)\u2229S\u03b41)\n\u2211\ni6=1\nri(D(pi\u2016Siq)\u2212 \u03b4i)+\n= inf q\u2208int(Cc1)\u2229S\u03b41\n\u2211\ni6=1\nri(D(pi\u2016Siq)\u2212 \u03b4i)+ .\nThus, we have\nR\u03041({pi, \u03b4i}) = { {ri}i6=1 \u2208 [0, \u03be]N\u22121 : inf\nq\u2208int(Cc1):D(p1\u2016S1q)\u2264\u03b41\n\u2211\ni\nri(D(pi\u2016Siq)\u2212 \u03b4i)+ \u2265 1 } .\n(20)\nSince the objective function \u2211\ni6=j ri(Li \u2212 Lj)\u22a4p is continuous in {ri} and p, and (20) is compact, now it suffices to show that (20) is continuous in {pi, \u03b4i} at {Sip\u2217, 0} to prove the theorem from [25, Corollary 8.1].\nFirst we show that R\u03041({pi, \u03b4i}) is closed at {Sip\u2217, 0}. Consider {r(m)i }i6=1 \u2208 R\u03041({p (m) i , \u03b4 (m) i }) for a sequence {p(m)i , \u03b4 (m) i }i which converges to {Sip\u2217, 0}i as m \u2192 \u221e. We show that {ri}i6=1 \u2208 R\u03041({Sip\u2217, 0}) if r(m)i \u2192 ri as m \u2192 \u221e.\nTake an arbitrary q \u2208 int(Cc1) such that D(S1p\u2217\u2016S1q) = 0. Since \u2016S1p\u2217 \u2212 p(m)1 \u2016 \u2192 0 and p1 \u2208 {S1p : supp(p) \u2282 supp(p\u2217)}, there exists p\u0303(m) such that p(m)1 = S1p\u0303(m) and \u2016p\u2217 \u2212 p\u0303(m)\u2016M \u2192 0.\nThus, from q \u2208 int(Cc1), it holds for sufficiently large m that q(m) = q \u2212 p\u2217 + p\u0303(m) \u2208 int(Cc1). For this q(m) we have\nD(p (m) 1 \u2016S1q(m)) \u2264 D(S1p\u0303(m)\u2016S1(q \u2212 p\u2217 + p\u0303(m))) = 0 \u2264 \u03b41 .\nthat is, q(m) \u2208 int(Cc1) \u2229 S\u03b41 . Therefore for sufficiently large m we have \u2211\ni\nr (m) i (D(pi\u2016Siq(m))\u2212 \u03b4 (m) i )+ \u2265 0 .\nand, letting m \u2192 \u221e, \u2211\ni\nriD(pi\u2016Siq) \u2265 0 .\nThis means that {ri}i6=1 \u2208 R\u03041({pi, \u03b4i}), that is, R\u03041({pi, \u03b4i}) is closed at {Sip\u2217, 0}. Next we show that R\u03041({pi, \u03b4i}) is open at {Sip\u2217, 0}. Consider {ri}i6=1 \u2208 R\u03041({Sip\u2217, 0}) and a sequence {p(m)i , \u03b4 (m) i }i which converges to {Sip\u2217, 0}i as m \u2192 \u221e. We show that there exists a sequence {r(m)i }i6=1 \u2208 R\u03041({p (m) i , \u03b4 (m) i }) such that r (m) i \u2192 ri.\nConsider the optimal value function\nv({p(m)i , \u03b4 (m) i }) = inf\nq\u2208cl(Cc1)\u2229S\u03b41\n\u2211\ni\nri(D(p (m) i \u2016Siq)\u2212 \u03b4 (m) i )+ . (21)\nSince the feasible region of (21) is closed at pi = Sip\u2217, \u03b4i = 0 and the objective function of (21) is lower semicontinuous in q, {pi, \u03b4i} we see that v({p(m)i , \u03b4 (m) i }) is lower semicontinuous from [25, Theorem 2]. Therefore, for any \u01eb > 0 there exists m0 > 0 such that for all m \u2265 m0\nv({p(m)i , \u03b4 (m) i }) \u2265 (1 \u2212 \u01eb)v({Sip\u2217, 0}) \u2265 1\nsince v({Sip\u2217, 0}) \u2265 1 from ri \u2208 R\u03041({Sip\u2217, 0}). Thus, by letting r(m)i := ri/(1\u2212 \u01eb) we have\ninf v\u2208cl(Cc1)\u2229S\u03b41\n\u2211\ni\nr (m) i (D(p (m) i \u2016Siq(m))\u2212 \u03b4 (m) i )+ \u2265 1 ,\nthat is, {r(m)i }i6=1 \u2208 R\u03041({p (m) i , \u03b4 (m) i }).\nD.2 Regret analysis of PM-DMED-Hinge\nLet p\u0302i,n \u2208 [0, 1]A be the empirical distribution of the symbols from the action i when the action i is selected n times. Then we have p\u0302i(t) = p\u0302i,Ni(t). Let Pi,ni(u) = Pr[D(p\u0302i,ni\u2016Sip\u2217) \u2265 u]. Then, from the large deviation bound on discrete distributions (Theorem 11.2.1 in Cover and Thomas [26]), we have\nPi,ni(u) \u2264 (ni + 1)Ae\u2212niu . (22)\nWe also define\nH({pi, ni}) = {i \u2282 [N ] : D(pi\u2016Sip\u2217)\u2212 f(ni) > 0}.\nFor\n0 < \u03b4 \u2264 \u2016p\u2217 \u2212 Cc1\u20162M/8 (23)\ndefine events\nA(t) = {p\u0302(t) \u2208 C1} A\u2032(t) = {p\u0302(t) \u2208 C1,\u03b1(t)} B(t) = \u22c2\ni\n{\u2016p\u0302i(t)\u2212 Sip\u2217\u2016 \u2264 \u221a \u03b4}\nC(t) = {i\u0302(t) /\u2208 H({p\u0302i(t), Ni(t)}), H({p\u0302i(t), Ni(t)}) 6= \u2205}\n= { D(p\u0302i\u0302(t)(t)\u2016Si\u0302(t)p\u2217) \u2264 f(Ni\u0302(t)(t)), \u22c3\ni\n{D(p\u0302i(t)\u2016Sip\u2217) > f(Ni(t))} }\nD(t) = \u22c2\ni\n{D(p\u0302i(t)\u2016Sip\u0302(t)) \u2264 f(Ni(t))}\nE(t) = { max\ni f(Ni(t)) \u2264 min\n{ 2\u03b4, (\u03c11,L\u03b1(t)) 2/4 } , (24)\nmin i\nNi(t) \u2265 max{c \u221a log t, (log logT )1/3}, 2\u03bd1,L\u03b1(t) \u2264 \u2016p\u2217 \u2212 Cc1\u2016M } ,\nwhere we write {T , U} instead of {T \u2229 U} for events T and U .\nProof of theorem 3. Since A\u2032(t) \u2282 A(t), the whole sample space is covered by {A\u2032(t),B(t)} \u222a {A\u2032(t),Bc(t)} \u222a {A(t), (A\u2032(t))c} \u222a {Ac(t), C(t)} \u222a {Ac(t), Cc(t)} \u2282 {A\u2032(t),B(t),D(t), E(t)} \u222a {A\u2032(t),Bc(t),D(t), E(t)} \u222a {A(t), (A\u2032(t))c,D(t), E(t)} \u222a {Ac(t), C(t)}\n\u222a {Ac(t), Cc(t),D(t), E(t)} \u222a Dc(t) \u222a Ec(t) . (25)\nLet Ji(t) denote the event that action i is newly added into the list LN at the t-th round and J \u2032i(t) \u2282 Ji(t) denote the event that Ji(t) occurred by Step 6 of Algorithm 3. Note that if {A\u2032(t), D(t), E(t)} occurred then Ji(t) is equivalent to J \u2032i(t). Combining this fact with (25) we can bound the regret as\nRegret(T ) \u2264 \u2211\ni6=1\n\u2206i\nT\u2211\nt=1\n1 [Ji(t)] +N\n\u2264 \u2211\ni6=1\n\u2206i\nT\u2211\nt=1\n( 1 [J \u2032i(t),A\u2032(t), B(t), D(t), E(t)] + 1 [J \u2032i(t),A(t), Bc(t), D(t), E(t)]\n+ 1 [Ji(t),A(t), (A\u2032(t))c,D(t), E(t)] + 1 [Ji(t),Ac(t), Cc(t),D(t), E(t)] + 1 [Ji(t),Dc(t) \u222a Ec(t)] ) +  \u2211\ni6=1\n\u2206i\n  T\u2211\nt=1\n1 [Ac(t), C(t)] +N .\nThe following Lemmas 7\u201313 bound the expectation of each term and complete the proof.\nLemma 7. Let {r\u2217i }i6=1 be the unique member of R\u2217j (p\u2217, {Sip\u2217, 0}). Then there exists \u01eb\u03b4 > 0 such that lim\u03b4\u21920 \u01eb\u03b4 = 0 and for all i 6= 1\nT\u2211\nt=1\n1 [J \u2032i(t), A\u2032(t), B(t), D(t), E(t)] \u2264 (1 + \u01eb\u03b4)r\u2217i logT + 1 .\nLemma 8.\nE\n[ T\u2211\nt=1\n1 [J \u2032i(t), A\u2032(t), Bc(t), D(t), E(t)] ] = o(logT ) .\nLemma 9.\nE\n[ T\u2211\nt=1\n1 [Ac(t), C(t)] ] = O(1) .\nLemma 10.\nE\n[ T\u2211\nt=1\n1 [Ji(t), A(t), (A\u2032(t))c, D(t), E(t)] ] = O(1) .\nLemma 11.\nE\n[ T\u2211\nt=1\n1 [Ji(t), Ac(t), Cc(t), D(t), E(t)] ] = O(1) .\nLemma 12.\nE\n[ T\u2211\nt=1\n1 [Ji(t), Dc(t)] ] = O(1) .\nLemma 13.\nT\u2211\nt=1\n1 [Ji(t), Ec(t)] = o(logT ) .\nProof of Lemma 7. From D(t) we have \u2211\ni\nNi(t)(D(p\u0302i(t)\u2016Sip\u0302(t))\u2212 f(Ni(t)))+ = 0 . (26)\nHere assume that \u2016p\u0302(t)\u2212 p\u2217\u2016M > 2 \u221a \u03b4. Then\nmax i D(p\u0302i(t)\u2016Sip\u0302(t)) \u2265 2max i \u2016p\u0302i(t)\u2212 Sip\u0302(t)\u20162 (by Pinsker\u2019s inequality)\n\u2265 2max i (\u2016Sip\u2217 \u2212 Sip\u0302(t)\u2016 \u2212 \u2016Sip\u2217 \u2212 p\u0302i(t)\u2016)2+ \u2265 2max\ni (\u2016Sip\u2217 \u2212 Sip\u0302(t)\u2016 \u2212\n\u221a \u03b4)2+ (by definition of B(t))\n> 2\u03b4\n\u2265 f(Ni(t)) , (by definition of E(t))\nwhich contradicts (26) and we obtain \u2016p\u0302(t) \u2212 p\u2217\u2016M \u2264 2 \u221a \u03b4. Furthermore, from B(t) and E(t) we have \u22c2\ni\n{\u2016p\u0302i(t)\u2212 Sip\u2217\u2016 \u2264 \u221a \u03b4} and \u22c2\ni\n{f(Ni(t)) \u2264 2\u03b4} ,\nrespectively. Since R\u22171(p, {pi, \u03b4i}) is continuous at p = p\u2217, pi = Sip\u2217, \u03b4i = 0 from Theorem 6, ri \u2264 (1 + \u01eb\u03b4)r\u2217i for all {ri}i6=1 \u2208 R\u2217i\u0302(t)(p\u0302(t), {p\u0302i(t), f(Ni(t))}) where r \u2217 i is the unique member of R\u22171(p\u2217, {Sip\u2217, 0}) and we used the fact that A\u2032(t) implies i\u0302(t) = 1. We complete the proof by\nT\u2211\nt=1\n1 [J \u2032i(t), A\u2032(t), B(t), D(t), E(t)]\n= T\u2211\nn=1\n1\n[ T\u22c3\nt=1\n{J \u2032i(t), A\u2032(t), B(t), D(t), E(t), Ni(t) = n} ]\n\u2264 T\u2211\nn=1\n1\n[ T\u22c3\nt=1\n{n/ log t \u2264 (1 + \u01eb\u03b4)r\u2217i } ]\n\u2264 (1 + \u01eb\u03b4)r\u2217i logT + 1 .\nProof of Lemma 8. First, we obtain from D(t) and E(t) that f(Ni(t)) \u2264 (\u03c11,L\u03b1(t))2/4 and\n\u2016p\u0302i(t)\u2212 Sip\u0302(t)\u2016 \u2264 \u221a D(p\u0302i(t)\u2016Sip\u0302(t))/2\n\u2264 \u221a f(Ni(t))/2 \u2264 \u03c11,L\u03b1(t)/ \u221a 8 .\nTherefore, from Lemma 5, it holds for any {r\u2217i }i6=1 \u2208 R\u2217j (p\u0302(t), {p\u0302i(t), f(Ni(t))}) that\nr\u2217i \u2264 4NLmax\n(\u03c11,L)2(\u03b1(t))3\n\u2264 4NLmax (\u03c11,L)2(\u03b1(T ))3 .\nNow we have\nE\n[ \u221e\u2211\nt=1\n1 [J \u2032i(t), A\u2032(t), Bc(t), D(t), E(t)] ]\n\u2264 E [ \u221e\u2211\nt=1\n1\n[ Ni(t)\nlogT < 4NLmax (\u03c11,L)2(\u03b1(T ))3\n, Bc(t), E(t) ]]\n\u2264 ( 4NLmax logT\n(\u03c11,L)2(\u03b1(T ))3 + 1\n) Pr [ T\u22c3\nt=1\n{Bc(t), E(t)} ] . (27)\nHere, note that\nBc(t) \u2282 \u22c3\ni\n{\u2016p\u0302i(t)\u2212 Sip\u2217\u2016 \u2265 \u221a \u03b4}\n\u2282 \u22c3\ni\n{D(p\u0302i(t)\u2016Sip\u2217) \u2265 2\u03b4} .\nSince Ni(t) \u2265 (log logT )1/3 holds under event E(t), we can bound the probability in (27) as\nPr\n[ T\u22c3\nt=1\n{Bc(t), E(t)} ]\n\u2264 \u2211\ni\n\u221e\u2211\nni=(log log T )1/3\nPr[D(p\u0302i,ni\u2016Sip\u2217) \u2265 2\u03b4]\n\u2264 N \u221e\u2211\nn=(log log T )1/3\n(n+ 1)Ae\u22122n\u03b4 (by (22))\n= e\u2212\u0398((log log T ) 1/3)\nand combining this with (27) we have\nE\n[ \u221e\u2211\nt=1\n1 [J \u2032i(t), A\u2032(t), Bc(t), D(t), E(t)] ]\n\u2264 O ( (logT )(log logT )3 ) e\u2212\u0398((log log T ) 1/3)\n= o(logT ) .\nProof of Lemma 9. Let G \u2208 2[N ] \\ \u2205 and {ni}i\u2208G \u2208 N|G| be arbitrary. Consider the case that \u2211\ni\u2208G\nni(D(p\u0302i,ni\u2016Sip\u2217)\u2212 f(ni))+ < x . (28)\nfor some x > 0. Then under events t \u2265 ex, \u22c2i\u2208G{Ni(t) = ni}, Ac(t), C(t) and H({p\u0302i(t), f(ni)}) = G we have\nmin p\u2208Cc\ni\u0302(t) :D(p\u0302i\u0302(t)(t)\u2016Si\u0302(t)p)\u2264f(Ni\u0302(t)(t))\n\u2211\ni\nNi(t)(D(p\u0302i(t)\u2016Sip)\u2212 f(Ni(t)))+\n\u2264 \u2211\ni\nni(D(p\u0302i(t)\u2016Sip\u2217)\u2212 f(ni))+ < x \u2264 log t ,\nwhich implies that the condition (9) is satisfied. On the other hand from (10), {r\u2217i } satisfies \u2211\ni\u2208G\n(r\u2217i log t)(D(p\u0302i(t)\u2016Sip)\u2212 f(ni))+ \u2265 log t . (29)\nEqs. (28) and (29) imply that there exists at least one i \u2208 G such that r\u2217i log t > Ni(t) = ni. This action is selected within N rounds and therefore Ni(t\u2032) = ni never holds for all t\u2032 \u2265 t+N . Thus, under the condition (28) it holds that\n\u2211\nt\n1 [ Ac(t), C(t), H({p\u0302i(t), Ni(t)}) = G, \u22c2\ni\u2208G\n{Ni(t) = ni} ] \u2264 ex +N .\nBy using this inequality we have\n\u221e\u2211\nt=1\n1 [Ac(t), C(t)]\n\u2264 \u2211\nG\u22082[N ]\\\u2205\n\u2211\n{ni}i\u2208G\u2208N|G|\n\u221e\u2211\nt=1\n1 [ Ac(t), C(t), H({p\u0302i(t), Ni(t)}) = G, \u22c2\ni\u2208G\n{Ni(t) = ni} ]\n\u2264 \u2211\nG\u22082[N ]\\\u2205\n\u2211\n{ni}i\u2208G\u2208N|G|\n1\n[\u22c2\ni\u2208G\n{D(p\u0302i,ni\u2016Sip\u2217) \u2265 f(ni)} ]( exp (\u2211\ni\u2208G\nni(D(p\u0302i,ni\u2016Sip\u2217)\u2212 f(ni)) ) +N ) .\n(30)\nLet Di = sup\nn {ess supD(p\u0302i,n\u2016Sip\u2217)} = \u2212 log min j:(Sip\u2217)j>0 (Sip \u2217)j ,\nwhere (Sip\u2217)j is the j-th component of Sip\u2217. Then,\nE\n  \u2211\n{ni}i\u2208G\u2208N|G|\n1\n[\u22c2\ni\u2208G\n{D(p\u0302i,ni\u2016Sip\u2217) \u2265 f(ni)} ]( exp (\u2211\ni\u2208G\nni(D(p\u0302i,ni\u2016Sip\u2217)\u2212 f(ni)) ) +N ) \n\u2264 \u2211\n{ni}i\u2208G\u2208N|G|\n(\u220f\ni\u2208G\n\u222b Di f(ni) eni(ui\u2212f(ni))d(\u2212Pi,ni(ui)) +N \u220f\ni\u2208G\n(ni + 1) Ae\u2212nif(ni) ) .\nThe first integral is bounded as \u222b Di f(ni) eni(ui\u2212f(ni))d(\u2212Pi(ui))\n= [ \u2212eni(ui\u2212f(ni))Pi(ui) ]Di f(ni) +\n\u222b Di\nf(ni)\nnie ni(ui\u2212f(ni))Pi(ui)dui (integration by parts)\n\u2264 (ni + 1)Ae\u2212nif(ni) + \u222b Di f(ni) ni(ni + 1) Ae\u2212nif(ni)dui\n\u2264 (1 + niDi)(ni + 1)Ae\u2212nif(ni) . (31)\nPutting (30)\u2013(31) together we have\nE\n[ \u221e\u2211\nt=1\n1 [Ac(t), C(t)] ]\n\u2264 \u2211\nG\u22082[N ]\\\u2205\n\u2211\n{ni}i\u2208G\u2208N|G|\n(\u220f\ni\u2208G\n(1 + niDi)(ni + 1) Ae\u2212nif(ni) +N\n\u220f\ni\u2208G\n(ni + 1) |A|e\u2212nif(ni)\n)\n\u2264 (N + 1) \u2211\nG\u22082[N ]\\\u2205\n\u220f\ni\u2208G\n\u2211\nni\u2208N\n(1 + niDi)(ni + 1) Ae\u2212nif(ni)\n= O(1) . (\nby nif(ni) = \u0398(n 1/2 i )\n)\nProof of Lemma 10. Because A(t), (A\u2032(t))c and E(t) imply \u2016p\u2217 \u2212 p\u0302(t)\u2016M \u2265 sup\np\u2208Cc1\n{\u2016p\u2217 \u2212 p\u2016M \u2212 \u2016p\u0302(t)\u2212 p\u2016M}\n\u2265 sup p\u2208Cc1 {\u2016p\u2217 \u2212 Cc1\u2016M \u2212 \u2016p\u0302(t)\u2212 p\u2016M} \u2265 \u2016p\u2217 \u2212 Cc1\u2016M \u2212 \u03bd1,L\u03b1(t) \u2265 \u2016p\u2217 \u2212 Cc1\u2016M/2 ,\n(A\u2032(t))c, D(t) and E(t) imply max\ni D(p\u0302i(t)\u2016Sip\u2217) \u2265 2max i \u2016p\u0302i(t)\u2212 Sip\u2217\u20162\n\u2265 2max i (\u2016Sip\u2217 \u2212 Sip\u0302(t)\u2016 \u2212 \u2016p\u0302i(t)\u2212 Sip\u0302(t)\u2016)2+\n\u2265 2max i\n( \u2016Sip\u2217 \u2212 Sip\u0302(t)\u2016 \u2212 \u221a f(Ni(t))/4 )2 +\n\u2265 2(\u2016p\u2217 \u2212 Cc1\u2016M/2\u2212 \u221a \u03b4/2)2+\n\u2265 \u2016p\u2217 \u2212 Cc1\u20162M/8 . (by (23)) On the other hand, event {Ji(t), Ac(t), A\u2032(t), minj Nj(t) = n} occurs for at most twice since all actions are put into the list if {Ac(t), A\u2032(t)} occurred. Thus, we have\nE\n[\u2211\nn\n1 [Ji(t), A(t), (A\u2032(t))c, D(t), E(t)] ]\n\u2264 2E [\u2211\nn\n1\n[\u22c3\nt\n{A(t), (A\u2032(t))c, D(t), E(t), min j\nNj(t) = n} ]]\n\u2264 2 \u2211\nn\nPr  max\nj D(p\u0302j(t)\u2016Sjp\u2217) \u2265 \u2016p\u2217 \u2212 Cc1\u20162M/8,\n\u22c2\nj\n{Nj(t) \u2265 n}\n \n\u2264 2N \u2211\nn\n(n+ 1)Ae\u2212n\u2016p \u2217\u2212Cc1\u2016 2 M/8 (by (22))\n= O(1) .\nProof of Lemma 11. Recall that\nCc(t) =   {D(p\u0302i\u0302(t)(t)\u2016Si\u0302(t)p \u2217) > f(Ni\u0302(t)(t))} \u222a \u22c2\nj\n{D(p\u0302j(t)\u2016Sjp\u2217) \u2264 f(Nj(t))}    .\nHere   A c(t), D(t), E(t), \u22c2\nj\n{D(p\u0302j(t)\u2016Sjp\u2217) \u2264 f(Nj(t))}    (32)\ncannot occur since (32) implies that\n\u2016p\u0302(t)\u2212 p\u2217\u2016M = max j \u2016Sj p\u0302(t)\u2212 Sjp\u2217\u2016\n\u2264 max j (\u2016Sj p\u0302(t)\u2212 p\u0302j(t)\u2016+ \u2016p\u0302j(t)\u2212 Sjp\u2217\u2016)\n\u2264 max j\n(\u221a D(p\u0302j(t)\u2016Sj p\u0302(t))/2 + \u221a D(p\u0302j(t)\u2016Sjp\u2217)/2 )\n\u2264 \u221a 2max\nj f(Nj(t)) (by D(t))\n\u2264 2 \u221a \u03b4 (by (24)) \u2264 \u2016p\u2217 \u2212 Cc1\u2016M/ \u221a 2 (by (23)) ,\nwhich contradicts p\u0302(t) \u2208 Cc1. On the other hand, 1 [ Ji(n), i\u0302(t) = j, D(p\u0302j(t)\u2016Sjp\u2217) > f(Nj(t)), Nj(t) = nj ] occurs for at most\ntwice since i\u0302(t) is put into the list under this event. Thus, we have\nE\n[ \u221e\u2211\nt=1\n1 [Ji(t), Ac(t), Cc(t), D(t), E(t)] ] \u2264 2 \u2211\nj\n\u221e\u2211\nn=1\nPr[D(p\u0302j, n\u2016Sjp\u2217) > f(n)]\n\u2264 2 \u2211\nj\n\u221e\u2211\nn=1\n(n+ 1)Ae\u2212nf(n) (by (22))\n= O(1) .\nProof of Lemma 12. Dc(t) implies\n0 < min p\n\u2211\nj\nNj(t)(D(p\u0302j(t)\u2016Sjp)\u2212 f(Nj(t)))+\n\u2264 \u2211\nj\nNj(t)(D(p\u0302j(t)\u2016Sjp\u2217)\u2212 f(Nj(t)))+\nand therefore \u22c3\nj\n{D(p\u0302j(t)\u2016Sjp\u2217) \u2265 f(Nj(t))} .\nNote that {Ji(t), Dc(t), Nj(t) = n} occurs for at most twice because all actions are put into the list if Dc(t) occurred. Thus, we have\nE\n[\u2211\nn\n1 [Ji(t), Dc(t)] ]\n\u2264 2E\n \u2211\nj\n\u2211\nn\n1 [D(p\u0302j,n\u2016Sjp\u2217) \u2265 f(n)]\n \n\u2264 2 \u2211\nj\n\u2211\nn\n(n+ 1)Ae\u2212nf(n) (by (22))\n= O(1) .\nProof of Lemma 13. First, we have\nEc(t) = { max\ni f(Ni(t)) > min\n{ 2\u03b4, (\u03c11,L\u03b1(t)) 2/4 }\n\u222a min i\nNi(t) < max{c \u221a log t, (log logT )1/3} \u222a 2\u03bd1,L\u03b1(t) > \u2016p\u2217 \u2212 Cc1\u2016M\n}\n\u2282 { f(min\ni Ni(t)) > min\n{ 2\u03b4, (\u03c11,L\u03b1(t)) 2/4 }\n\u222a min i\nNi(t) < c \u221a log t \u222a c \u221a log t < (log logT )1/3 \u222a 2\u03bd1,L\u03b1(t) > \u2016p\u2217 \u2212 Cc1\u2016M\n}\n\u2282 {\nb\u221a c \u221a log t\n> min { 2\u03b4, (\u03c11,L\u03b1(t)) 2/4 } \u222a min\ni Ni(t) < c\n\u221a log t\n\u222a t < e (log log T ) 2/3 c \u222a 2a/ log log t > \u2016p\u2217 \u2212 Cc1\u2016M/\u03c11,L }\n= { t < e b4 16\u03b44c2 \u222a (log t) 1/4\nlog log t <\nb\na \u221a c\u03c11,L \u222a min i\nNi(t) < c \u221a log t\n\u222a t < e (log log T ) 2/3 c \u222a t < ee2a\u03c11,L/\u2016p \u2217\u2212Cc1\u2016M } .\nFrom limt\u2192\u221e(log t)1/4/ log log t = \u221e and e (log log T )2/3 c = o(elog log T ) = o(logT ) we have\nT\u2211\nt=1\n1 [Ji(t), Ec(t)]\n= \u2211\nj\nT\u2211\nt=1\n1 [ Ji(t), Nj(t) < c \u221a log t ] + o(logT ) .\nBy (8), event {Ji(t), Nj(t) < c \u221a log t, Nj(t) = n} occurs for at most twice and therefore\nT\u2211\nt=1\n1 [Ji(t), Ec(t)]\n\u2264 2 \u2211\nj\nT\u2211\nn=1\n1\n[ T\u22c3\nt=1\n{n < c \u221a log t} ] + o(logT )\n= o(logT ) ."}], "references": [{"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Inf. Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1985}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert D. Kleinberg", "Frank Thomson Leighton"], "venue": "In FOCS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Optimal allocation strategies for the dark pool problem", "author": ["Alekh Agarwal", "Peter L. Bartlett", "Max Dama"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Minimizing regret with label efficient prediction", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML, pages 928\u2013936,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In COLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In COLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Regret minimization under partial monitoring", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "Math. Oper. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Minimax regret of finite partial-monitoring games in stochastic environments", "author": ["G\u00e1bor Bart\u00f3k", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "An adaptive algorithm for finite stochastic partial monitoring", "author": ["G\u00e1bor Bart\u00f3k", "Navid Zolghadr", "Csaba Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A near-optimal algorithm for finite partial-monitoring games against adversarial opponents", "author": ["G\u00e1bor Bart\u00f3k"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Efficient partial monitoring with prior information", "author": ["Hastagiri P. Vanchinathan", "G\u00e1bor Bart\u00f3k", "Andreas Krause"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Aur\u00e9lien Garivier", "Olivier Capp\u00e9"], "venue": "In COLT,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Large deviations techniques and applications", "author": ["Amir Dembo", "Ofer Zeitouni"], "venue": "Applications of mathematics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "An Asymptotically Optimal Bandit Algorithm for Bounded Support Models", "author": ["Junya Honda", "Akimichi Takemura"], "venue": "In COLT,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A dual parametrization method for convex semi-infinite programming", "author": ["S. Ito", "Y. Liu", "K.L. Teo"], "venue": "Annals of Operations Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Introduction to sensitivity and stability analysis in nonlinear programming", "author": ["Anthony V. Fiacco"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1983}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled Markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM J. Contr. and Opt.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "On the complexity of A/B testing", "author": ["Emilie Kaufmann", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier"], "venue": "In COLT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Bandits Games and Clustering Foundations", "author": ["S\u00e9bastien Bubeck"], "venue": "Theses, Universite\u0301 des Sciences et Technologie de Lille - Lille I,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Point-to-set maps in mathematical programming", "author": ["William W. Hogan"], "venue": "SIAM Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1973}], "referenceMentions": [{"referenceID": 0, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 251, "endOffset": 257}, {"referenceID": 6, "context": "Many classes of problems, including prediction with expert advice [1], the multi-armed bandit problem [2], dynamic pricing [3], the dark pool problem [4], label efficient prediction [5], and linear and convex optimization with full or bandit feedback [6, 7] can be modeled as an instance of partial monitoring.", "startOffset": 251, "endOffset": 257}, {"referenceID": 7, "context": "1 Related work The paper by Piccolboni and Schindelhauer [8] is one of the first to study the regret of the finite partial monitoring problem.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "[9] to O(T ), who also showed an instance in which the bound is optimal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] classified the partial monitoring problems into four categories in terms of the minimax regret: a trivial problem with zero regret, an easy problem with \u0398\u0303( \u221a T ) regret1, a hard problem with \u0398(T ) regret, and a hopeless problem with \u0398(T ) regret.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13].", "startOffset": 97, "endOffset": 109}, {"referenceID": 11, "context": "Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13].", "startOffset": 97, "endOffset": 109}, {"referenceID": 12, "context": "Since then, several algorithms with a \u00d5( \u221a T ) regret bound for easy problems have been proposed [11, 12, 13].", "startOffset": 97, "endOffset": 109}, {"referenceID": 12, "context": "Among them, the Bayes-update Partial Monitoring (BPM) algorithm [13] is state-of-the-art in the sense of empirical performance.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "[11], which derivedO(logT ) distribution-dependent regret for easy problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Upper confidence bound (UCB), the most well-known algorithm for the multi-armed bandits, has a distribution-dependent regret bound [2, 14], and algorithms that minimize the distribution-dependent regret (e.", "startOffset": 131, "endOffset": 138}, {"referenceID": 13, "context": "Upper confidence bound (UCB), the most well-known algorithm for the multi-armed bandits, has a distribution-dependent regret bound [2, 14], and algorithms that minimize the distribution-dependent regret (e.", "startOffset": 131, "endOffset": 138}, {"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10]) in which the number of rows of Si is the number of the different symbols in the i-th row of H .", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In the context of the multi-armed bandit problem, Lai and Robbins [2] derived the regret lower bound of a strongly consistent algorithm: an algorithm must select each arm i until its number of draws Ni(t) satisfies log t .", "startOffset": 66, "endOffset": 69}, {"referenceID": 15, "context": "Large deviation principle [16] states that, the probability that an opponent with strategy q behaves like p is", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "Based on the technique used in Lai and Robbins [2], the proof considers a modified game in which another action i 6= 1 is optimal.", "startOffset": 47, "endOffset": 50}, {"referenceID": 12, "context": "[13]) ambiguously define the harshness as the closeness to the boundary of the cells.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "4 PM-DMED Algorithm In this section, we describe the partial monitoring deterministic minimum empirical divergence (PMDMED) algorithm, which is inspired by DMED [17] for solving the multi-armed bandit problem.", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "Let p\u0302i(t) \u2208 [0, 1] be the empirical distribution of the symbols under the selection of action i.", "startOffset": 13, "endOffset": 19}, {"referenceID": 10, "context": "[11], we compared the performances of algorithms in three different games: the four-state game (Section 4), a three-state game and dynamic pricing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], we set N = 5,M = 5, and c = 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We compared Random, FeedExp3 [8], CBP [11] with \u03b1 = 1.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "We compared Random, FeedExp3 [8], CBP [11] with \u03b1 = 1.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "01, BPM-LEAST, BPM-TS [13], and PM-DMED with c = 1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "Following the optimization of BPM-LEAST [13], we resorted to a finite sample approximation and used the Gurobi LP solver [19] in computing {r\u2217 i }: at each round, we sampled 1,000 points from PM , and relaxed the constraints on the samples.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "[20] and references therein).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Moreover, the coefficient of the leading logarithmic term in the regret bound of the partial monitoring problem is equal to the bound given in Lai and Robbins [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 18, "context": ", Fiacco [21]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "References [1] Nick Littlestone and Manfred K.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Robert D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Alekh Agarwal, Peter L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Nicol\u00f2 Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Martin Zinkevich.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Varsha Dani, Thomas P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Antonio Piccolboni and Christian Schindelhauer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Nicol\u00f2 Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] G\u00e1bor Bart\u00f3k, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] G\u00e1bor Bart\u00f3k, Navid Zolghadr, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] G\u00e1bor Bart\u00f3k.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Hastagiri P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Peter Auer, Nicol\u00f3 Cesa-bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Aur\u00e9lien Garivier and Olivier Capp\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Amir Dembo and Ofer Zeitouni.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Junya Honda and Akimichi Takemura.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Anthony V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] S\u00e9bastien Bubeck.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] William W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In fact, many regularity conditions are assumed in Graves and Lai [22], where another generalization of the bandit problem is considered and the regret lower bound is expressed in terms of LSIP.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "The technique here is mostly inspired from Theorem 1 in Lai and Robbins [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 20, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "5 in [24]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "During the proof we also show that C 1 (p , {pi }) is equal to the optimal constant factor of Lai and Robbins [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "LBTheory is the asymptotic regret lower bound of Lai and Robbins [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "LB-Theory is the regret lower bound of Lai and Robbins [2], that is, \u2211 i6=1 \u2206i log t d(\u03bci\u2016\u03bc1) .", "startOffset": 55, "endOffset": 58}, {"referenceID": 22, "context": "See Hogan [25] for definitions of terms such as continuity of point-to-set maps.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "2 Regret analysis of PM-DMED-Hinge Let p\u0302i,n \u2208 [0, 1] be the empirical distribution of the symbols from the action i when the action i is selected n times.", "startOffset": 47, "endOffset": 53}], "year": 2015, "abstractText": "Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PMDMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}