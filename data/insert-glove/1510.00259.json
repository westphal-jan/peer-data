{"id": "1510.00259", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2015", "title": "A Generative Model of Words and Relationships from Multiple Sources", "abstract": "Neural jebal Language birir Models are bhanj a powerful tool metrocards to meaningfully embed words hyppolite into sabar semantic 04-dnp vector parthenium spaces. However, learning allvine vector space models of 33-point language generally 12.75 relies on 30-index the scramble availability 75-basis of abundant booing and epicatechin diverse tsangpo training telefilms examples. general-major In highly specialized domains jixian this 98.61 requirement may not be met sas due ebonite to noturus difficulties in 3,014 obtaining sidefooted a a-changin large rens corpus, or the ksfo limited sr-2 range sigourney of pauker expression in uh-1 average sangamam usage. Prior knowledge spinotti about kewal entities in the sorest language often exists in spatted a scelsi knowledge cyclocross base misterioso or ontology. easton We 14:28 propose igloolik a buckeystown generative publicos model urge which ahady allows for modeling and supercomputer transfering 14.34 semantic weepies information in vector spaces by combining diverse bechet data santosa sources. neo-colonialism We generalize 19-19 the karey concept of co - occurrence kodungalloor from distributional hidetada semantics zichron to 7-zip include other types of relations between entities, kosheh evidence rdx for which kingdon can billingsley come turbo-prop from a knowledge artemev base (coloring such as WordNet loadmaster or UMLS ). nominations Our shorish model lub defines iduna a eruption probability distribution obingo over presento triplets 2253 consisting of word muzahem pairs with relations. Through masaryk stochastic escheated maximum likelihood u.n.-mediated we froid learn hasanpur a representation of cantatore these words as elements schairer of vin\u00edcius a greats vector space and honker model the savige relations equilibrate as severe affine transformations. We diborane demonstrate the hockeytown effectiveness of our aaba generative vestido approach by outperforming recent models heeter on a knowledge - thura base completion phrenological task 93.95 and sovietskaya demonstrating its i-jafria ability refugia to marselisborg profit from and1 the use 82.88 of palustre partially observed sloughed or fully unobserved darvin data coiffures entries. lanzone Our model 76.70 is capable of asks operating sixteen-year-old semi - supervised, where word pairs kaikan with no mcdivitt known lupien relation are xiaohan used d'automne as training 396,000 data. penta We further demonstrate mommies the h\u00f8nefoss usefulness empresa of leymarie learning nasality from different ascencio data barnardiston sources fuzzy with overlapping vocabularies.", "histories": [["v1", "Thu, 1 Oct 2015 14:42:19 GMT  (2404kb)", "https://arxiv.org/abs/1510.00259v1", "7 pages, 5 figures"], ["v2", "Thu, 3 Dec 2015 17:08:28 GMT  (125kb,D)", "http://arxiv.org/abs/1510.00259v2", "8 pages, 5 figures; incorporated feedback from reviewers; to appear in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence 2016"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["stephanie l hyland", "theofanis karaletsos", "gunnar r\u00e4tsch"], "accepted": true, "id": "1510.00259"}, "pdf": {"name": "1510.00259.pdf", "metadata": {"source": "META", "title": "A Generative Model of Words and Relationships from Multiple Sources", "authors": ["Stephanie L. Hylanda", "Theofanis Karaletsosa", "Gunnar R\u00e4tscha"], "emails": ["gunnar}@ratschlab.org"], "sections": [{"heading": null, "text": "Introduction1 A deep problem in natural language processing is to model the semantic relatedness of words, drawing on evidence from text and spoken language, as well as knowledge graphs such as ontologies. A successful modelling approach is to obtain an embedding of words into a metric space such that semantic relatedness is reflected by closeness in this space. One paradigm for obtaining this embedding is the neural language model (Bengio et al. 2003), which traditionally draws on local co-occurence statistics from sequences of words (sentences) to obtain an encoding of words as vectors in a space whose geometry respects linguistic and semantic features. The core concept behind this procedure is the distributional hypothesis of language; see Sahlgren (2008), that semantics can be inferred by examining the context of a word. This relies on the availability of a large corpus of diverse sentences, such that a word\u2019s typical context can be accurately estimated.\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1A preliminary version of this work appeared at the International Workshop on Embeddings and Semantics at SEPLN 2015 (Hyland, Karaletsos, and Ra\u0308tsch 2015).\nIn the age of web-scale data, there is abundant training data available for such models in the case of generic language. For specialised language domains this may not be true. For example, medical text data (Liu et al. 2015) often contains protected health information, necessitating access restrictions and potentially limiting corpus size to that obtainable from a single institution, resulting in a corpus with less than tens of millions of sentences, not billions as in (for example) Google n-grams. In addition to this, specialised domains expect certain prior knowledge from the reader. A doctor may never mention that anastrazole is a aromatase inhibitor (a type of cancer drug), for example, because they communicate sparsely, assuming the reader shares their training in this terminology. In such cases, it is likely that even larger quantities of data are required, but the sensitive nature of such data makes this difficult to attain.\nFortunately, such specialised disciplines often create expressive ontologies, in the form of annotated relationships between terms (denoted by underlines). These may be semantic, such as dog is a type of animal, or derived from domain-specific knowledge, such as anemia is an associated disease of leukemia. (This is a relationship found in the medical ontology system UMLS; see Bodenreider, 2004). We observe that these relationships can be thought of as additional contexts from which co-occurrence statistics can be drawn; the set of diseases associated with leukemia arguably share a common context, even if they may not cooccur in a sentence (see Figure 1).\nWe would like to use this structured information to improve the quality of learned embeddings, to use their information content to regularize the embedding space in cases of low data abundance while obtaining an explicit representation of these relationships in a vector space. We tackle this by assuming that each relationship is an operator which transforms words in a relationship-specific way. Intuitively, the action of these operators is to distort the shape of the space, effectively allowing words to have multiple representations without requiring a full set of parameters for each possible sense.\nThe intended effect on the underlying (untransformed) embedding is twofold: to encourage a solution which is more sensitive to the domain than would be achieved using only unstructured information and to use heterogeneous sources of information to compensate for sparsity of data. In addition\nar X\niv :1\n51 0.\n00 25\n9v 2\n[ cs\n.C L\n] 3\nD ec\n2 01\n5\nto this, since relationship operators can act on any part of the space, by learning these functions we can apply them to any word regardless of its source, allowing for link prediction on new entities in a knowledge base.\nWhile we do not attempt to model higher-order language structure such as grammar and syntax, we consider a generative model in which the distance between terms in the embedded space describes the probability of their co-occurrence under a given relationship. Through this, we learn the joint distribution of all pairs in all relationships, and can ask questions such as \u2018What is the probability of anemia appearing in a sentence with imatinib2?\u2019, or \u2018What is the probability that anemia is a disease associated with leukemia?\u2019 This introduces flexibility for subsequent analyses that require a generative approach.\nThis paper is laid out as follows: In Related Work, we describe relevant prior work concerning embedding words and relationships and place our contributions in context. In Modelling, we describe in detail the probabilistic model and our inference and learning strategy, including a link to code. In Experiments, we show an array of experimental results to quantitatively demonstrate different aspects of the model on datasets using WordNet and Wikipedia sources in supervised, semi-supervised and unsupervised settings, before summarising our findings in the Discussion section."}, {"heading": "Related Work", "text": "The task of finding continuous representation for elements of language has been explored in great detail in recent and\n2Imatinib is a tyrosine-kinase inhibitor used in the treatment of chronic myelogenous leukemia.\nless-recent years. Bengio et al. (2003) described a neural architecture to predict the next word in a sequence, using distributed representations to overcome the curse of dimensionality. Since then, much work has been devoted to obtaining, understanding, and applying these distributed language representations. One such model is word2vec of Mikolov et al. (2013), which more explicitly relies on the distributional hypothesis of semantics by attempting to predict the surrounding context of a word, either as a set of neighbouring words (the skip-gram model) or as an average of its environment (continuous bag of words). We note later in the model section that the idealised version of skip-gram word2vec is a special case of our model with one relationship; appears in a sentence with. In practice, word2vec uses a distinct objective function, replacing the full softmax with an approximation intended to avoid computing a normalising factor. We retain a probabilistic interpretation by approximating gradients of the partition function, allowing us to follow the true model gradient while maintaining tractability. Furthermore, learning a joint distribution facilitates imputation and generation of data, dealing with missing data and making predictions using the model itself. We note that a generative approach to language was also explored by Andreas and Ghahramani (2013), but does not concern relationships.\nRelational data can also be used to learn distributed representations of entities in knowledge graphs, entities which may correspond to or can be mapped to words. A general approach is to implicitly embed the graph structure through vertex embeddings and rules (or transformations) for traversing it. Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al. (2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al. 2013). These approaches are uniquely concerned with relational data however, and do not consider distributional semantics from free text. Faruqui et al. (2015) and Johansson and Nieto Pin\u0303a (2015) describe methods to modify pre-existing word embeddings to align them with evidence derived from a knowledge base, although their models do not learn representations de novo.\nSimilar in spirit to our work is Weston et al. (2013), where entities belonging to a structured database are identified in unstructured (free) text in order to obtain embeddings useful for relation prediction. However, they learn separate scoring functions for each data source. This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al. (2014a). In these cases, separate objectives are used to incorporate different data sources, combining (in the case of Xu et al. (2014)) the skip-gram objective from Mikolov et al. (2013) and the TransE ob-\njective of Bordes et al. (2013). Our method uses a single energy function over the joint space of word pairs with relationships, combining the \u2018distributional objective\u2019 with that of relational data by considering free-text co-occurrences as another type of relationship.\nWe have mentioned several approaches to integrating graphs into embedding procedures. While these graphs have been derived from knowledge bases or ontologies, other forms of graphs have also been exploited in related efforts, for example using constituency trees to obtain sentence-level embeddings (Tai, Socher, and Manning 2015).\nThe motivation for our work is similar in spirit to multitask and transfer learning (for instance, Caruana (1997), Evgeniou and Pontil (2004), or Widmer and Ra\u0308tsch (2012)). In transfer learning one takes advantage of data related to a similar, typically supervised, learning task with the aim of improving the accuracy of a specific learning task. In our case, we have the unsupervised learning task of embedding words and relationships into a vector space and would like to use data from another task to improve the learned embeddings, here word co-occurrence relationships. This may be understood as a case of unsupervised transfer learning, which we tackle using a principled generative model.\nFinally, we note that a recent extension of word2vec to full sentences (Jernite, Rush, and Sontag 2015) using a fast generative model exceeds the scope of our model in terms of sentence modeling, but does not explicitly model latent relationships or tackle transfer learning from heterogeneous data sources."}, {"heading": "Probabilistic Modelling of Words and Relationships", "text": "We consider a probability distribution over triplets (S,R, T ) where S is the source word of the (possibly directional) relationshipR and T is the target word. Note that while we refer to \u2018words\u2019, S and T could represent any entity between which a relationship may hold without altering our mathematical formulation, and so could refer to multiple-word entities (such as UMLS Concept Unique Identifiers) or even non-lexical objects. Without loss of generality, we proceed to refer to them as words. Following Mikolov et al. (2013), we learn two representations for each word: cs represents word s when it appears as a source, and vt for word t appearing as a target.3 Relationships act by altering cs through their action on the vector space (cs 7\u2192 GRcs). By allowing GR to be an arbitrary affine transformation, we combine the bilinear form of Socher et al. (2013) with translation operators of Bordes et al. (2013).\nThe joint model is given by a Boltzmann probability den-\n3Goldberg and Levy (2014) provide a motivation for using two representations for each word. We can extend this by observing that words with similar v representations share a paradigmatic relationship in that they may be exchangeable in sentences, but do not tend to co-occur. Conversely, words s and t with cs \u2248 vt have a syntagmatic relationship and tend to co-occur (e.g. Sahlgren (2008)). Thus, we seek to enforce syntagmatic relationships and through transitivity obtain paradigmatic relationships of v vectors.\nsity function,\nP (S,R, T |\u0398) = 1Z(\u0398)e \u2212E(S,R,T |\u0398)\n= e \u2212E(S,R,T |\u0398)\u2211\ns,r,t e \u2212E(s,r,t|\u0398) (1)\nHere, the partition function is the normalisation factor over the joint posterior space captured by the model parameters Z(\u0398) = \u2211 s,r,t e\n\u2212E(s,r,t|\u0398). The parameters \u0398 in this case are the representations of all words (both as sources and targets) and relationship matrices; \u0398 = {ci, Gr,vj , }r\u2208relationshipsi,j\u2208vocabulary. If we choose an energy function\nE(S,R, T |\u0398) = \u2212vT \u00b7GRcS (2) we observe that the |R| = 1, GR = I case recovers the original softmax objective described in Mikolov et al. (2013), so the idealised word2vec model is a special case of our model.\nThis energy function is problematic however, as it can be trivially minimised by making the norms of all vectors tend to infinity. While the partition function provides a global regularizer, we find that it is not sufficient to avoid norm growth during training. We therefore use as our energy function the negative cosine similarity, which does not suffer this weakness;4\nE(S,R, T |\u0398) = \u2212 vT \u00b7GRcS \u2016vT \u2016\u2016GRcS\u2016\n(3)\nThis is also a natural choice, as cosine similarity is the standard method for evaluating word vector similarities. Energy minimisation is therefore equal to finding an embedding in which the angle between related entities is minimised in an appropriately transformed relational space. It would be simple to define a more complex energy function (using perhaps splines) by choosing a different functional representation for R, but we focus in this work on the affine case.\nInference and Learning We estimate our parameters \u0398 from data using stochastic maximum likelihood on the joint probability distribution. The maximum likelihood estimator is:\n\u0398\u2217 = argmax P (D|\u0398) = argmax N\u220f n P ((S,R, T )n|\u0398)\n(4) Considering the log-likelihood at a single training example (S,R, T ) and taking the derivative with respect to parameters, we obtain: \u2202 logP (S,R, T |\u0398)\n\u2202\u0398i =\n\u2202\n\u2202\u0398i [\u2212E(S,R, T |\u0398)]\n\u2212 \u2202 \u2202\u0398i [ log \u2211 s,r,t e\u2212E(S,R,T |\u0398) ] (5)\n4We also considered an alternate, more symmetric energy function using the Frobenius norm of G;\nE(S,R, T |\u0398) = \u2212 vT \u00b7GRcS\u2016vT \u2016\u2016GR\u2016F \u2016cS\u2016 However, we found no clear empirical advantage to this choice.\nGiven a smooth energy function the first term is easily obtained, but the second term is problematic. This term, derived from the partition function Z(\u0398), is intractable to evaluate in practice owing to its double sum over the size of the vocabulary (potentially O(105)). In order to circumvent this intractability we resort to techniques used to train Restricted Boltzmann Machines and use stochastic maximum likelihood, also known as persistent contrastive divergence (PCD); (Tieleman 2008). In contrastive divergence, the gradient of the partition function is estimated using samples drawn from the model distribution seeded at the current training example (Hinton 2002). However, many rounds of sampling may be required to obtain good samples. PCD retains a persistent Markov chain of model samples across gradient evaluations, assuming that the underlying distribution changes slowly enough to allow the Markov chain chain to mix. We use Gibbs sampling by iteratively using the conditional distributions of all variables (S, R, and T , see below) to obtain model samples.\nIn particular, we draw S, R and T from the conditional probability distributions:\nP (S|r, t; \u0398) = e \u2212E(S,r,t|\u0398)\u2211\ns\u2032 e \u2212E(s\u2032,r,t|\u0398)\nP (R|s, t; \u0398) = e \u2212E(s,R,t|\u0398)\u2211\nr\u2032 e \u2212E(s,r\u2032,t|\u0398)\nP (T |s, r; \u0398) = e \u2212E(s,r,T |\u0398)\u2211 t\u2032 e \u2212E(s,r,t\u2032|\u0398)\n(6)\nThereby, we can estimate the gradient of Z(\u0398) at the cost of these evaluations, which are linear in the size of the vocabulary.\nUsing this, following the objective from (5) further simplifies to a contrastive objective given a batch ofB data samples and M model samples (each model sample obtained from an independent, persistent Markov chain):\n\u2202P (D|\u0398) \u2202\u0398i ' 1 M M\u2211 m=1 [ \u2202E((S,R, T )m|\u0398) \u2202\u0398i ]\n\u2212 1 B B\u2211 b=1 [ \u2202E((S,R, T )b|\u0398) \u2202\u0398i ] (7) Interestingly, the model can gracefully deal with missing elements in observed triplets (for instance missing observed relationships). Learning is achieved by considering the partially observed triple as a superposition of all possible completions of that triple, each weighted by its conditional probability given the observed elements, using (6). This produces a gradient which is a weighted sum.\nIn the fully-observed case (which we sometimes call supervised in an abuse of terminology), the weighting is simply a spike on the observed state. Similarly, the model can predict missing values as a simple inference step. These properties make having a joint distribution very attractive in practical use, offsetting the conceptual difficulty of training. In our experiments, we exploit these properties to do principled semi-supervised and unsupervised learning with par-\ntially observed or unobserved relationships without needing an external noise distribution or further assumptions.\nImplementation We provide the algorithm in Python (https://github.com/corcra/bf2). Since most of its runtime takes place in vector operations, we are developing a GPU-optimised version. We use Adam (Kingma and Ba 2015) to adapt learning rates and improve numerical stability. We used the recommended hyperparameters from this paper; \u03bb = 1 \u2212 10\u22128, = 1 \u2212 10\u22128, \u03b21 = 0.9, \u03b22 = 0.999. Unless otherwise stated, hyperparameters specific to our model were: dimension d = 100, batch size of B = 100, learning rate for all parameter types of \u03b1 = 0.001, and three rounds of Gibbs sampling to obtain model samples."}, {"heading": "Experiments", "text": "We will proceed to explore the model in five settings. First, an entity vector embedding problem on WordNet which consists of fully observed triplets of words and relationships. In the second case we demonstrate the power of the semisupervised extension of the algorithm on the same task. We then show that a) adding relationship data can lead to better embeddings and b) that adding unstructured text can lead to better relationship predictions. Finally, we demonstrate that the algorithm can also identify latent relationships that lead to better word embeddings.\nData As structured data, we use the WordNet dataset described by Socher et al. (2013), available at http:// stanford.io/1IENOYH. This contains 38,588 words and 11 types of relationships. Training data consists of true triples such as (feeling, has instance, pride).\nWe derived an additional version of this dataset by stripping sense IDs from the words, which reduced the vocabulary to 33,330 words. We note that this procedure likely makes prediction on this data more difficult, as every word receives only one representation. We did this in order to produce an aligned vocabulary with our unstructured data source, taken to be English Wikipedia (https://dumps. wikimedia.org/, August 2014). We extracted text using WikiExtractor (http://bit.ly/1Imz1WJ). We greedily identified WordNet 2-grams in the Wikipedia text. Two words were considered in a sentence context if they appeared within a five word window. Only pairs for which both words appeared in the WordNet vocabulary were included. We drew from a pool of 112,581 training triples in WordNet with 11 relationships, and 8,206,304 triples from Wikipedia (heavily sub-sampled, see experiments). To check that our choice to strip sense IDs was valid, we also created a version of the Wikipedia dataset where each word was tagged with its most common sense from the WordNet training corpus. We found that this did not significantly impact our results, so we chose to continue with the sense-stripped version, preferring to collapse some WordNet identities over assigning possibly-incorrect senses to words in Wikipedia.\nWordNet Prediction Task We used our model to solve the basic prediction task described in Socher et al. (2013). In this case, the model must differentiate true and false triples, where false triples are obtained by corrupting the T entry in the triple, e.g. (S,R, T ) \u2192 (S,R, T\u0303 ) (where (S,R, T\u0303 ) doesn\u2019t appear in the training data). The \u2018truth\u2019 of a triple is evaluated by its energy E(S,R, T ), with a relationshipspecific cut-off chosen by maximizing accuracy on a validation set (this is an equivalent procedure to the task as initially described). By learning explicit representations of each of the 38,588 entities in WordNet, our approach most closely follows the \u2018Entity Vector\u2019 task in Socher et al. This is to be contrasted with the \u2018Word Vector\u2019 task, where a representation is learned for each word, and entity representations are obtained by averaging their word vectors. We elected not to perform this task because we are not confident that composition into phrases through averaging is well-justified. Using the validation set to select an early stopping point at 66 epochs, we obtain a test set accuracy of 78.2% with an AUROC of 85.6%. The \u2018Neural Tensor Model\u2019 (NTN) described in Socher et al. (2013) achieves an accuracy of around 70% on this task, although we note that the simpler Bilinear model also described in Socher et al. (2013) achieves 74% and is closer to the energy function we employ. The improved performance exhibited by this simpler Bilinear model was also noted by Yang et al. (2015). Other baselines reported by Socher et al. were a single layer model without an interaction term, a Hadamard model (Bordes et al. 2012) and the model of Bordes et al. (2011) which learns separate left and right relationship operators for each element of the triple. These were outperformed by the Bilinear and NTN models, see Figure 4 in Socher et al. (2013) for further details. Hence, our model outperforms the two previous methods by more than 4%.\nAs a preliminary test of our model, we also considered the FreeBase task described by Socher et al. (2013). Initial testing yielded an accuracy of 85.7%, which is comparable to the result of their best-performing model (NTN) of about 87%. We chose not to further explore this dataset however, because its entities are mostly proper nouns and thus seemed unlikely to benefit from additional semantic data.\nSemi-supervised Learning for WordNet We next tested the semi-supervised learning capabilities of our algorithm (see Inference and Learning). For this we consider the same task as before, but omit some label information in the training set and instead use posterior probabilities during the inference. For this we trained our algorithm with a subset of the training data (total 112,581 examples) and measured the accuracy of classifying into true and false relationships as before. The fully-observed case used only a subset of fully-observed data (varying amounts as indicated on the x-axis). For semi-supervised learning, we also used the remaining data, but masking the type of the relationship between pairs. In Figure 2 we report the accuracy for different labelled/unlabelled fractions of otherwise the same dataset. We find that the semi-supervised method consistently performs better than the fully observed method for all analysed\ntraining set sizes. In this and the previous experiment, one Markov chain was used for PCD and a l2 regulariser on GR parameters with weight 0.01.\nAdding Unstructured Data to a Relationship Prediction Task To test how unstructured text data may improve a prediction task when structured data is scarce, we augmented a subsampled set of triples from WordNet with 10,000 examples from Wikipedia and varied the weight \u03ba associated with their gradients during learning. The task is then to predict whether or not a given triple (S,R, T ) is a true example from WordNet, as described previously. Figure 3 shows accuracy on this task as \u03ba and the amount of structured data vary. To find the improvement associated with unstructured data, we compared accuracy at \u03ba = 0 with \u03ba = \u03ba\u2217 (where \u03ba\u2217 gave the highest accuracy on the validation set; marked with \u2217). We find that including free text data quite consistently improves the classification accuracy, particularly when structured data is scarce.\nIn this experiment and all following, we used five Markov chains for PCD and a l2 regulariser on all parameters with weight 0.001.\nRelationship Data for Improved Embeddings In this case, we assume unstructured text data is restricted, and vary the quantity of structured data. To evaluate the untrans-\nformed embeddings, we use them as the inputs to a supervised multi-class classifier. The task for a given (S,R, T ) triple is to predict R given the vector formed by concatenating cS and vT . We use a random forest classifier trained on the WordNet validation set using five-fold cross-validation.\nTo avoid testing on the training data (since the embeddings are obtained using the WordNet training set), we perform this procedure once for each relationship (11 times - excluding appears in sentence with), each time removing from the training data all triples containing that relationship. Figure 4 shows the F1 score of the multi-class classifier on the left-out relationship for different combinations of data set sizes. We see that for most relationships, including more unstructured data improves the embeddings (measured by performance on this task). We also trained word2vec (Mikolov et al. 2013) on a much larger Wikipedia-only dataset (4,145,372 sentences) and trained a classifier on its vectors; results are shown as black lines. We see that our approach yields a consistently higher F1 score, suggesting that even data about unrelated relationships provides information to produce vectors that are semantically richer overall.\nThese results illustrate that embeddings learned from limited free text data can be improved by additional, unrelated relationship data.\nUnsupervised Learning Of Relationships In our final experiment, we explore the ability of the model to learn embeddings from co-occurrence data alone, without specifying\nhas part, part of, member meronym, synset domain topic, type of; relationships similar to, domain topic were omitted for technical reasons). We used eight of the relationships together with the Wikipedia data to learn representations that are then used in a subsequent supervised learning task to predict the remaining ninth relationship based on the representations using random forests. Black lines denote results from word2vec trained on a Wikipedia-only dataset with 4,145,372 sentences.\nthe relationships it should use. When using the model with just one relationship (trivially the identity), the model effectively reverts to word2vec. However, if we add a budget of relationships (in our experiments we use 1, 3, 5, 7, 11), the model has additional parameters available to learn affine transformations of the space which can differentiate how distances and meaning interact for the word embeddings without fixing this a priori. Our intuition is that we want to test whether textual context alone has substructure that we can capture with latent variables. We generate a training set of one million word co-occurrences from Wikipedia (using a window size of 5 and restricting to words appearing in the WordNet dataset, as described earlier), and train different models for each number of latent relationships. Inspired by earlier experiments testing the utility of supplanting WordNet training data with Wikipedia examples, we decide to test the ability of a model purely trained on Wikipedia to learn word and relationship representations which are predictive of WordNet triplets, without having seen any data from WordNet. As a baseline we start with |R| = 1 to test how well word embeddings from context alone can perform, indicated by the leftmost bar in Figure 5. We then proceed to train models with more latent relationships. We observe that, especially for some relationship prediction tasks, including this flexibility in the model produces a noticeable increase in F1 score on this task. Since we evaluate the embeddings alone, this effect must be due to a shift in the content of these vectors, and cannot be explained by the additional parameters introduced by the latent relationships. We note that a consistent explanation for this phenomenon is that the model discovers contextual subclasses which are indica-\ntive of WordNet-type relationships. This observation opens doors to further explorations of the hypothesis regarding contextual subclasses and unsupervised relationships learning from different types of co-occurrence data.\nWe note that we did not perform an exhaustive search of the hyperparameter space; better settings may yet exist and will be sought in future work. Nonetheless, although the absolute improvement in F1 score yielded by this method is modest, we are encouraged by the model\u2019s ability to exploit latent variables in this way."}, {"heading": "Discussion", "text": "We have presented a probabilistic generative model of words and relationships between them. By estimating the parameters of this model through stochastic gradient descent, we obtain vector and matrix representations of these words and relationships respectively. To make learning tractable, we use persistent contrastive divergence with Gibbs sampling between entity types (S, R, T ) to approximate gradients of the partition function. Our model uses an energy function which contains the idealised word2vec model as a special case. By augmenting the embedding space and considering relationships as arbitrary affine transformations, we combine benefits of previous models. In addition, our formulation as a generative model is distinct and allows a more flexible use, especially in the missing data, semi- and unsupervised setting. Motivated by domain-settings in which structured or unstructured data may be scarce, we illustrated how a model that combines both data sources can improve the quality of embeddings, supporting other findings in this direction.\nA promising field of exploration for future work is a more detailed treatment of relationships, perhaps general-\nising from affine transformations to include nonlinear maps. Our choice of cosine similarity in the energy function can also be developed, as we note that this function is insensitive to very small deviations in angle, and may therefore produce looser clusters of synonyms. Nonlinearity could also be introduced in the energy, using for example splines. Furthermore, we intend to encode the capacity for richer transfer of structured information from sources such as graphs as prior knowledge into the model. Our current model can take advantage of local properties of graphs to that purpose, but has no explicit encoding for nested and distributed relationships.\nA limitation of our model is its conceptual inability to embed whole sentences (which has been tackled by averaging vectors in other work, but requires deeper investigation). Recurrent or more complex neural language models offer many avenues to pursue as extensions for our model to tackle this. A particularly interesting direction to achieve that would be a combination with work such as (Jernite, Rush, and Sontag 2015), which could in principle be integrated with our model to include relationships.\nThe intended future application of this model is exploratory semantic data analysis in domain-specific pools of knowledge. We can do so by combining prior knowledge with unstructured information to infer, for example, new edges in knowledge graphs. A promising such field is medical language processing, retrospective exploratory data analysis may boost our understanding of the complex relational mechanisms inherent in multimodal observations, and specific medical knowledge in the form of (for example) the UMLS can be used as a strong regulariser. Indeed, initial experiments combining clinical text notes with relational data between UMLS concepts from SemMedDB (Kilicoglu et al. 2012) have demonsrated the utility of this combined approach to predict the functional relationship between medical concepts, for example, cisplatin treats diabetes. We are in the process of expanding this investigation."}, {"heading": "Acknowledgments", "text": "This work was funded by the Memorial Hospital and the Sloan Kettering Institute (MSKCC; to G.R.). Additional support for S.L.H. was provided by the Tri-Institutional Training Program in Computational Biology and Medicine."}], "references": [{"title": "A generative model of vector space semantics", "author": ["J. Andreas", "Z. Ghahramani"], "venue": "Association for Computational Linguistics (ACL) 91.", "citeRegEx": "Andreas and Ghahramani,? 2013", "shortCiteRegEx": "Andreas and Ghahramani", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res. 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The unified medical language system (umls): integrating biomedical terminology", "author": ["O. Bodenreider"], "venue": "Nucleic Acids Research 32:D267\u2013D270.", "citeRegEx": "Bodenreider,? 2004", "shortCiteRegEx": "Bodenreider", "year": 2004}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "Conference on Artificial Intelligence.", "citeRegEx": "Bordes et al\\.,? 2011", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 127\u2013135.", "citeRegEx": "Bordes et al\\.,? 2012", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Machine Learning 28(1):41 \u2013 75.", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "International Conference on Knowledge Discovery and Data Mining, 109\u2013117.", "citeRegEx": "Evgeniou and Pontil,? 2004", "shortCiteRegEx": "Evgeniou and Pontil", "year": 2004}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Association for Computational", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Incorporating both distributional and relational semantics in word representations", "author": ["D. Fried", "K. Duh"], "venue": "Workshop Contribution at the International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "Fried and Duh,? 2014", "shortCiteRegEx": "Fried and Duh", "year": 2014}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Y. Goldberg", "O. Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy", "year": 2014}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation 14(8):1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "A generative model of words and relationships from multiple sources", "author": ["S.L. Hyland", "T. Karaletsos", "G. R\u00e4tsch"], "venue": "International Workshop on Embeddings and Semantics, 16\u201320.", "citeRegEx": "Hyland et al\\.,? 2015", "shortCiteRegEx": "Hyland et al\\.", "year": 2015}, {"title": "A fast variational approach for learning markov random field language models", "author": ["Y. Jernite", "A.M. Rush", "D. Sontag"], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Jernite et al\\.,? 2015", "shortCiteRegEx": "Jernite et al\\.", "year": 2015}, {"title": "Embedding a semantic network in a word space", "author": ["R. Johansson", "L. Nieto Pi\u00f1a"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Human Language Technologies, 1428\u20131433. Association for Computa-", "citeRegEx": "Johansson and Pi\u00f1a,? 2015", "shortCiteRegEx": "Johansson and Pi\u00f1a", "year": 2015}, {"title": "Semmeddb: a pubmed-scale repository of biomedical semantic predications", "author": ["H. Kilicoglu", "D. Shin", "M. Fiszman", "G. Rosemblat", "T.C. Rindflesch"], "venue": "Bioinformatics 28(23):3158\u20133160.", "citeRegEx": "Kilicoglu et al\\.,? 2012", "shortCiteRegEx": "Kilicoglu et al\\.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2015.", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of AAAI.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Exploiting task-oriented resources to learn word embeddings for clinical abbreviation expansion", "author": ["Y. Liu", "T. Ge", "K. Mathews", "H. Ji", "D. McGuinness"], "venue": "Proceedings of BioNLP 15, 92\u201397. Beijing, China: Association for Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems (NIPS), 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The distributional hypothesis", "author": ["M. Sahlgren"], "venue": "Italian Journal of Linguistics 20(1):33\u201353.", "citeRegEx": "Sahlgren,? 2008", "shortCiteRegEx": "Sahlgren", "year": 2008}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems (NIPS), 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K. Tai", "R. Socher", "C.D. Manning"], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Tieleman,? 2008", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "Knowledge graph and text jointly embedding", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 1591\u20131601.", "citeRegEx": "Wang et al\\.,? 2014a", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014b", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["J. Weston", "A. Bordes", "O. Yakhnenko", "N. Usunier"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 1366\u20131371.", "citeRegEx": "Weston et al\\.,? 2013", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Multitask Learning in Computational Biology", "author": ["C. Widmer", "G. R\u00e4tsch"], "venue": "JMLR W&CP. ICML 2011 Unsupervised and Transfer Learning Workshop. 27:207\u2013216.", "citeRegEx": "Widmer and R\u00e4tsch,? 2012", "shortCiteRegEx": "Widmer and R\u00e4tsch", "year": 2012}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "Proceedings of the 23rd ACM International Conference on Information and Knowledge Management, 1219\u20131228. ACM.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["B. Yang", "W. Yih", "X. He", "J. Gao", "L. Deng"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2015.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["M. Yu", "M. Dredze"], "venue": "Association for Computational Linguistics (ACL), 545\u2013550.", "citeRegEx": "Yu and Dredze,? 2014", "shortCiteRegEx": "Yu and Dredze", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "One paradigm for obtaining this embedding is the neural language model (Bengio et al. 2003), which traditionally draws on local co-occurence statistics from sequences of words (sentences) to obtain an encoding of words as vectors in a space whose geometry respects linguistic and semantic features.", "startOffset": 71, "endOffset": 91}, {"referenceID": 1, "context": "One paradigm for obtaining this embedding is the neural language model (Bengio et al. 2003), which traditionally draws on local co-occurence statistics from sequences of words (sentences) to obtain an encoding of words as vectors in a space whose geometry respects linguistic and semantic features. The core concept behind this procedure is the distributional hypothesis of language; see Sahlgren (2008), that semantics can be inferred by examining the context of a word.", "startOffset": 72, "endOffset": 404}, {"referenceID": 19, "context": "For example, medical text data (Liu et al. 2015) often contains protected health information, necessitating access restrictions and potentially limiting corpus size to that obtainable from a single institution, resulting in a corpus with less than tens of millions of sentences, not billions as in (for example) Google n-grams.", "startOffset": 31, "endOffset": 48}, {"referenceID": 0, "context": "Bengio et al. (2003) described a neural architecture to predict the next word in a sequence, using distributed representations to overcome the curse of dimensionality.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2003) described a neural architecture to predict the next word in a sequence, using distributed representations to overcome the curse of dimensionality. Since then, much work has been devoted to obtaining, understanding, and applying these distributed language representations. One such model is word2vec of Mikolov et al. (2013), which more explicitly relies on the distributional hypothesis of semantics by attempting to predict the surrounding context of a word, either as a set of neighbouring words (the skip-gram model) or as an average of its environment (continuous bag of words).", "startOffset": 0, "endOffset": 345}, {"referenceID": 0, "context": "We note that a generative approach to language was also explored by Andreas and Ghahramani (2013), but does not concern relationships.", "startOffset": 68, "endOffset": 98}, {"referenceID": 20, "context": "(2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al. 2013).", "startOffset": 208, "endOffset": 229}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities.", "startOffset": 0, "endOffset": 189}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al.", "startOffset": 0, "endOffset": 389}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al.", "startOffset": 0, "endOffset": 433}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al.", "startOffset": 0, "endOffset": 452}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al. (2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al.", "startOffset": 0, "endOffset": 475}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al. (2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al. 2013). These approaches are uniquely concerned with relational data however, and do not consider distributional semantics from free text. Faruqui et al. (2015) and Johansson and Nieto Pi\u00f1a (2015) describe methods to modify pre-existing word embeddings to align them with evidence derived from a knowledge base, although their models do not learn representations de novo.", "startOffset": 0, "endOffset": 852}, {"referenceID": 3, "context": "Bordes et al. (2011) scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. Socher et al. (2013) describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of Bordes et al. (2013) (and extensions such as Wang et al. (2014b), Fan et al. (2014), and Lin et al. (2015)) represents relationships as translations, motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning (Mikolov et al. 2013). These approaches are uniquely concerned with relational data however, and do not consider distributional semantics from free text. Faruqui et al. (2015) and Johansson and Nieto Pi\u00f1a (2015) describe methods to modify pre-existing word embeddings to align them with evidence derived from a knowledge base, although their models do not learn representations de novo.", "startOffset": 0, "endOffset": 888}, {"referenceID": 23, "context": "Similar in spirit to our work is Weston et al. (2013), where entities belonging to a structured database are identified in unstructured (free) text in order to obtain embeddings useful for relation prediction.", "startOffset": 33, "endOffset": 54}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al.", "startOffset": 34, "endOffset": 73}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al.", "startOffset": 34, "endOffset": 95}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al. (2014a). In these cases, separate objectives are used to incorporate different data sources, combining (in the case of Xu et al.", "startOffset": 34, "endOffset": 120}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al. (2014a). In these cases, separate objectives are used to incorporate different data sources, combining (in the case of Xu et al. (2014)) the skip-gram objective from Mikolov et al.", "startOffset": 34, "endOffset": 248}, {"referenceID": 10, "context": "This approach is also employed by Fried and Duh (2014), Xu et al. (2014), Yu and Dredze (2014), and Wang et al. (2014a). In these cases, separate objectives are used to incorporate different data sources, combining (in the case of Xu et al. (2014)) the skip-gram objective from Mikolov et al. (2013) and the TransE ob-", "startOffset": 34, "endOffset": 300}, {"referenceID": 3, "context": "jective of Bordes et al. (2013). Our method uses a single energy function over the joint space of word pairs with relationships, combining the \u2018distributional objective\u2019 with that of relational data by considering free-text co-occurrences as another type of relationship.", "startOffset": 11, "endOffset": 32}, {"referenceID": 6, "context": "The motivation for our work is similar in spirit to multitask and transfer learning (for instance, Caruana (1997), Evgeniou and Pontil (2004), or Widmer and R\u00e4tsch (2012)).", "startOffset": 99, "endOffset": 114}, {"referenceID": 6, "context": "The motivation for our work is similar in spirit to multitask and transfer learning (for instance, Caruana (1997), Evgeniou and Pontil (2004), or Widmer and R\u00e4tsch (2012)).", "startOffset": 99, "endOffset": 142}, {"referenceID": 6, "context": "The motivation for our work is similar in spirit to multitask and transfer learning (for instance, Caruana (1997), Evgeniou and Pontil (2004), or Widmer and R\u00e4tsch (2012)).", "startOffset": 99, "endOffset": 171}, {"referenceID": 17, "context": "Following Mikolov et al. (2013), we learn two representations for each word: cs represents word s when it appears as a source, and vt for word t appearing as a target.", "startOffset": 10, "endOffset": 32}, {"referenceID": 17, "context": "Following Mikolov et al. (2013), we learn two representations for each word: cs represents word s when it appears as a source, and vt for word t appearing as a target.3 Relationships act by altering cs through their action on the vector space (cs 7\u2192 GRcs). By allowing GR to be an arbitrary affine transformation, we combine the bilinear form of Socher et al. (2013) with translation operators of Bordes et al.", "startOffset": 10, "endOffset": 367}, {"referenceID": 3, "context": "(2013) with translation operators of Bordes et al. (2013).", "startOffset": 37, "endOffset": 58}, {"referenceID": 21, "context": "Sahlgren (2008)).", "startOffset": 0, "endOffset": 16}, {"referenceID": 20, "context": "we observe that the |R| = 1, GR = I case recovers the original softmax objective described in Mikolov et al. (2013), so the idealised word2vec model is a special case of our model.", "startOffset": 94, "endOffset": 116}, {"referenceID": 24, "context": "In order to circumvent this intractability we resort to techniques used to train Restricted Boltzmann Machines and use stochastic maximum likelihood, also known as persistent contrastive divergence (PCD); (Tieleman 2008).", "startOffset": 205, "endOffset": 220}, {"referenceID": 12, "context": "In contrastive divergence, the gradient of the partition function is estimated using samples drawn from the model distribution seeded at the current training example (Hinton 2002).", "startOffset": 166, "endOffset": 179}, {"referenceID": 17, "context": "We use Adam (Kingma and Ba 2015) to adapt learning rates and improve numerical stability.", "startOffset": 12, "endOffset": 32}, {"referenceID": 22, "context": "Data As structured data, we use the WordNet dataset described by Socher et al. (2013), available at http:// stanford.", "startOffset": 65, "endOffset": 86}, {"referenceID": 22, "context": "WordNet Prediction Task We used our model to solve the basic prediction task described in Socher et al. (2013). In this case, the model must differentiate true and false triples, where false triples are obtained by corrupting the T entry", "startOffset": 90, "endOffset": 111}, {"referenceID": 4, "context": "were a single layer model without an interaction term, a Hadamard model (Bordes et al. 2012) and the model of Bordes et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 19, "context": "By learning explicit representations of each of the 38,588 entities in WordNet, our approach most closely follows the \u2018Entity Vector\u2019 task in Socher et al. This is to be contrasted with the \u2018Word Vector\u2019 task, where a representation is learned for each word, and entity representations are obtained by averaging their word vectors. We elected not to perform this task because we are not confident that composition into phrases through averaging is well-justified. Using the validation set to select an early stopping point at 66 epochs, we obtain a test set accuracy of 78.2% with an AUROC of 85.6%. The \u2018Neural Tensor Model\u2019 (NTN) described in Socher et al. (2013) achieves an accuracy of around 70% on this task, although we note that the simpler Bilinear model also described in Socher et al.", "startOffset": 142, "endOffset": 666}, {"referenceID": 19, "context": "By learning explicit representations of each of the 38,588 entities in WordNet, our approach most closely follows the \u2018Entity Vector\u2019 task in Socher et al. This is to be contrasted with the \u2018Word Vector\u2019 task, where a representation is learned for each word, and entity representations are obtained by averaging their word vectors. We elected not to perform this task because we are not confident that composition into phrases through averaging is well-justified. Using the validation set to select an early stopping point at 66 epochs, we obtain a test set accuracy of 78.2% with an AUROC of 85.6%. The \u2018Neural Tensor Model\u2019 (NTN) described in Socher et al. (2013) achieves an accuracy of around 70% on this task, although we note that the simpler Bilinear model also described in Socher et al. (2013) achieves 74% and is closer to the energy function we employ.", "startOffset": 142, "endOffset": 803}, {"referenceID": 19, "context": "By learning explicit representations of each of the 38,588 entities in WordNet, our approach most closely follows the \u2018Entity Vector\u2019 task in Socher et al. This is to be contrasted with the \u2018Word Vector\u2019 task, where a representation is learned for each word, and entity representations are obtained by averaging their word vectors. We elected not to perform this task because we are not confident that composition into phrases through averaging is well-justified. Using the validation set to select an early stopping point at 66 epochs, we obtain a test set accuracy of 78.2% with an AUROC of 85.6%. The \u2018Neural Tensor Model\u2019 (NTN) described in Socher et al. (2013) achieves an accuracy of around 70% on this task, although we note that the simpler Bilinear model also described in Socher et al. (2013) achieves 74% and is closer to the energy function we employ. The improved performance exhibited by this simpler Bilinear model was also noted by Yang et al. (2015). Other baselines reported by Socher et al.", "startOffset": 142, "endOffset": 967}, {"referenceID": 3, "context": "were a single layer model without an interaction term, a Hadamard model (Bordes et al. 2012) and the model of Bordes et al. (2011) which learns separate left and right relationship operators for each element of the triple.", "startOffset": 73, "endOffset": 131}, {"referenceID": 3, "context": "were a single layer model without an interaction term, a Hadamard model (Bordes et al. 2012) and the model of Bordes et al. (2011) which learns separate left and right relationship operators for each element of the triple. These were outperformed by the Bilinear and NTN models, see Figure 4 in Socher et al. (2013) for further details.", "startOffset": 73, "endOffset": 316}, {"referenceID": 22, "context": "As a preliminary test of our model, we also considered the FreeBase task described by Socher et al. (2013). Initial testing yielded an accuracy of 85.", "startOffset": 86, "endOffset": 107}, {"referenceID": 22, "context": "Figure 2: Semi-supervised learning improves learned embeddings: We tested the semi-supervised extension of our approach on the entity relationship learning task described in Socher et al. (2013) and previous subsection.", "startOffset": 174, "endOffset": 195}, {"referenceID": 20, "context": "We also trained word2vec (Mikolov et al. 2013) on a much larger Wikipedia-only dataset (4,145,372 sentences) and trained a classifier on its vectors; results are shown as black lines.", "startOffset": 25, "endOffset": 46}, {"referenceID": 16, "context": "Indeed, initial experiments combining clinical text notes with relational data between UMLS concepts from SemMedDB (Kilicoglu et al. 2012) have demonsrated the utility of this combined approach to predict the functional relationship between medical concepts, for example, cisplatin", "startOffset": 115, "endOffset": 138}], "year": 2015, "abstractText": "Neural language models are a powerful tool to embed words into semantic vector spaces. However, learning such models generally relies on the availability of abundant and diverse training examples. In highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus, or the limited range of expression in average use. Such domains may encode prior knowledge about entities in a knowledge base or ontology. We propose a generative model which integrates evidence from diverse data sources, enabling the sharing of semantic information. We achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words, which we model as affine transformations on the embedding space. We demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels. We further demonstrate the usefulness of learning from different data sources with overlapping vocabularies.", "creator": "TeX"}}}