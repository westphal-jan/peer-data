{"id": "1510.01025", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2015", "title": "Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods", "abstract": "plutarco A big-screen fundamental outgoings class of matrix optimization 17,917 problems kitada that scissurella arise refrains in prospera many areas of science and mejorado engineering hacz\u00f3w is dally that of vranken quadratic cell optimization with orthogonality nebel constraints. junpei Such problems can be solved using line - merchants search eliteserien methods teague on reghaia the Stiefel manifold, which kurir are known to 5-disc converge carotenes globally anstee under asda mild dpmne conditions. To magellanic determine the gyldendal convergence rate 20-31 of schoener these lenzing methods, pavanello we give an byck explicit estimate heartbreaks of the zeph exponent hieroglyphic in a consulship Lojasiewicz revans inequality benet for floribunda the (kittikachorn non - convex) unwanted set of sgm critical points of coceres the aforementioned regnery class of problems. By combining a.f.c such an dicks estimate bookers with known arguments, we are matanzima able to establish 3,590 the baboquivari linear convergence lingpa of kingscourt a 02:45 large class of 12-metre line - big search shengliang methods. bbtv A 1274 key cockatoo step itemizing in our plain-clothes proof pimms is to establish a local dillons error vehicle-mounted bound iraan for the 30.0 set librettist of critical mustaqbal points, which cattleman may 1206 be of scepanovic independent interest.", "histories": [["v1", "Mon, 5 Oct 2015 04:14:22 GMT  (122kb)", "http://arxiv.org/abs/1510.01025v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG cs.NA math.NA", "authors": ["huikang liu", "weijie wu", "anthony man-cho so"], "accepted": true, "id": "1510.01025"}, "pdf": {"name": "1510.01025.pdf", "metadata": {"source": "CRF", "title": "Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods", "authors": ["Huikang Liu", "Weijie Wu", "Anthony Man\u2013Cho"], "emails": ["hkliu@se.cuhk.edu.hk", "wwu@se.cuhk.edu.hk", "manchoso@se.cuhk.edu.hk"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n01 02\n5v 1\n[ m\nat h.\nO C"}, {"heading": "1 Introduction", "text": "Quadratic optimization problems with orthogonality constraints constitute an important class of matrix optimization problems that have found applications in areas such as combinatorial optimization, data mining, dynamical systems, multivariate statistical analysis, and signal processing, just to mention a few (see, e.g., [6, 13, 3, 8, 10, 16, 21, 25]). A prototypical form of such problems is\nmin X\u2208St(m,n)\n{ F (X) = tr ( XTAXB )} , (1)\nwhere St(m,n) = { X \u2208 Rm\u00d7n | XTX = In }\n(with m \u2265 n and In being the n\u00d7n identity matrix) is the compact Stiefel manifold and A \u2208 Sm, B \u2208 Sn are given symmetric matrices. Despite its simplicity, Problem (1) already has many applications, a most prominent of which is Principal\n\u2217Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E\u2013mail: hkliu@se.cuhk.edu.hk\n\u2020Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E\u2013mail: wwu@se.cuhk.edu.hk\n\u2021Department of Systems Engineering and Engineering Management, and, by courtesy, CUHK\u2013BGI Innovation Institute of Trans\u2013omics, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E\u2013mail: manchoso@se.cuhk.edu.hk\nComponent Analysis (PCA). One of the algorithmic approaches for solving (1) is to apply linesearch methods on the manifold St(m,n). The update formula of this family of methods takes the form\nXk+1 = R (Xk, \u03b1k\u03bek) for k = 0, 1, . . . , (2)\nwhere \u03b1k \u2265 0 is the step size, \u03bek is a search direction in the tangent space to St(m,n) at Xk, and R(Xk, \u00b7) is a so-called retraction that maps a vector in the tangent space to St(m,n) at Xk into a point on St(m,n). In particular, the iterates produced by (2) are all feasible for Problem (1). Naturally, the choice of step sizes, search directions and the retraction will affect the convergence and efficiency of the resulting method. For the general problem of optimizing a smooth function over the Stiefel manifold (which includes Problem (1) as a special case), various choices have been proposed over the years, and the convergence properties of the resulting methods are relatively well understood; see, e.g., [1, 3, 4, 24, 7]. However, very little is known about the convergence rates of these methods, even when they are applied to the much more structured problem (1). Part of the difficulty is due to the fact that optimization problems over the Stiefel manifold are non-convex in general. This implies that much of the existing analysis machinery, which heavily exploits convexity, cannot be applied to such problems. Currently, convergence rates of linesearch methods for solving Problem (1) are established only under quite restrictive conditions. For instance, Absil et al. [3, Theorem 4.6.3] showed that when n = 1 and B = In = 1 (and hence Problem (1) corresponds to minimizing the Rayleigh quotient on the unit sphere in Rm), a certain line-search method will converge linearly to an eigenvector corresponding to the smallest eigenvalue \u03bb of A, provided that \u03bb is simple. More recently, Shamir [19] developed a stochastic line-search method for Problem (1) when n = 1, B = In = 1, and A is negative semidefinite. He showed that if the smallest eigenvalue \u03bb is simple and certain boundedness assumptions hold, then his proposed method converges linearly to an eigenvector corresponding to \u03bb. However, it is not clear how to extend the above results to handle the case where n > 1 and/or the multiplicity of \u03bb is greater than one. On another front, Smith [20] showed that when used to optimize a smooth function over a Riemannian manifold, the method of steepest descent will converge linearly to a critical point if the function is strongly convex on the manifold. However, such a notion of convexity is much stronger than that on the Euclidean space. In particular, it is known that every smooth function that is convex on a compact Riemannian manifold (such as the Stiefel manifold) is constant [5]. Therefore, one cannot hope to obtain linear convergence results for Problem (1) using the convexity-based approach in [20]. Recently, there have been some endeavors to analyze the convergence rates of line-search methods for solving optimization problems over embedded submanifolds using the so-called Lojasiewicz inequality ; see, e.g., [2, 14, 17]. Although such an approach is extremely powerful, it has a severe limitation; namely, the exponent in the Lojasiewicz inequality is often hard to determine explicitly. Without the knowledge of such exponent, one cannot determine the exact rate of convergence of a given method. As it turns out, the Lojasiewicz exponent for general polynomial systems is known (see, e.g., [11]) and can in principle be applied to Problem (1). However, the exponent depends on the dimensions of the problem and leads only to very weak convergence rate results.\nIn view of the above discussion, our main contribution of this paper is to give a significantly sharper estimate of the Lojasiewicz exponent for the non-convex problem (1). In particular, it is independent of the dimensions of the problem. We achieve this by establishing a local Lipschitzian error bound for the (non-convex) set of critical points of Problem (1), which may be of independent interest. By combining our estimate of the Lojasiewicz exponent with a\nwell-established analysis framework in the literature [17], we conclude that a host of line-search methods for solving Problem (1) converge linearly to a critical point. It should be noted that our convergence result does not require any restriction on the eigenvalues of A and B. Thus, it is qualitatively different from those in [3, 19]. Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.\nBesides the notations introduced earlier, we shall use On to denote the set of n\u00d7n orthogonal matrices (in particular, we have On = St(n, n)); Diag(x1, . . . , xn) to denote the diagonal matrix with x1, . . . , xn on the diagonal; BlkDiag(A1, . . . , An) to denote the block diagonal matrix whose diagonal blocks are A1, . . . , An. Given a matrix Y \u2208 R\nm\u00d7n and a non-empty closed set X \u2282 R m\u00d7n, we shall use dist(Y,X ) to denote the distance of Y to X ; i.e., dist(Y,X ) = minX\u2208X \u2016X \u2212 Y \u2016F . Other notations are standard."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 First-Order Optimality Condition and Descent Directions", "text": "To begin, let us introduce some basic definitions and concepts. We view St(m,n) as an embedded submanifold of Rm\u00d7n with the inherited Riemannian metric \u3008\u00b7, \u00b7\u3009 given by \u3008X,Y \u3009 = tr ( XTY )\n. For any X \u2208 St(m,n), the tangent space to St(m,n) at X is given by T (X) = {\nY \u2208 Rm\u00d7n | XTY + Y TX = 0 } . The gradient of F (X) = tr ( XTAXB )\nis \u2207F (X) = 2AXB, and its orthogonal projection onto T (X) is given by\ngradF (X) = ( Im \u2212XX T ) \u2207F (X) + 1\n2 X\n( XT\u2207F (X)\u2212\u2207F (X)TX )\n= 2AXB \u2212XXTAXB \u2212XBXTAX.\nLet X = {X \u2208 St(m,n) | gradF (X) = 0} be the set of critical points of Problem (1). The following proposition gives a characterization of X :\nProposition 1 Let X \u2208 St(m,n) be given. Then, the following are equivalent:\n(i) gradF (X) = 0.\n(ii) \u2207F (X)\u2212X\u2207F (X)TX = 0.\n(iii) For any \u03c1 > 0, D\u03c1(X) = \u2207F (X)\u2212X ( 2\u03c1\u2207F (X)TX + (1\u2212 2\u03c1)XT\u2207F (X) ) = 0.\nProof The equivalence between (ii) and (iii) is established in [7, Lemma 2.1]. To prove the equivalence between (i) and (ii), observe that\ngradF (X) =\n(\nIm \u2212 1\n2 XXT\n)\n\u2207F (X)\u2212 1\n2 X\u2207F (X)TX\n=\n(\nIm \u2212 1\n2 XXT\n)\n( \u2207F (X)\u2212X\u2207F (X)TX ) .\nNow, it remains to note that Im \u2212 (1/2)XX T is invertible. \u2294\u2293 It is easy to verify that D\u03c1(X) \u2208 T (X) for any \u03c1 > 0. Moreover, as shown in [7, Lemma 3.1], \u2212D\u03c1(X) is a descent direction at X \u2208 St(m,n) for any \u03c1 > 0. Hence, in the sequel, we shall focus on line-search methods that use \u2212D\u03c1(\u00b7) as the search direction."}, {"heading": "2.2 Retraction", "text": "Another ingredient in line-search methods for optimizing over St(m,n) is a retraction:\nDefinition 1 (Retraction) A map R : \u22c3\nX\u2208St(m,n){X} \u00d7 T (X) \u2192 St(m,n) will be called a retraction, if for any fixed X \u2208 St(m,n) and \u03be \u2208 T (X) it holds that \u03be 7\u2192 R(X, \u03be) is continuous on T (X), and for all X \u2208 St(m,n),\nlim T (X)\u220b\u03be\u21920 \u2016R(X, \u03be)\u2212 (X + \u03be)\u2016F \u2016\u03be\u2016F = 0. (3)\nVarious smooth retractions on the Stiefel manifold have been proposed in the literature. These include the polar decomposition-based retraction, the QR-decomposition-based retraction, the Cayley transform, and the Riemannian exponential mapping. We refer the reader to [3, 9] for details of these retractions. In Section 4, we shall conduct numerical experiments with these four retractions."}, {"heading": "2.3 Step Sizes", "text": "To complete the specification of a line-search method, it remains to choose the step sizes. This is done in the following:\nDefinition 2 (Armijo Point) Let \u03b3 > 0, \u03b2, c \u2208 (0, 1) be given constants. The number\n\u03b1 = max { \u03b2n\u03b3 | n \u2265 0, F (R (X,\u2212\u03b2n\u03b3D\u03c1(X)))\u2212 F (X) \u2264 \u2212c\u03b2 n\u03b3\u2207F (X)TD\u03c1(X) }\n(4)\nis called the Armijo point at X \u2208 St(m,n) with parameters (\u03b3, \u03b2, c).\nSince the smooth retraction (3) is a first-order approximation, the left hand side approximate the first-order derivative along \u2212\u03b2n\u03b3D\u03c1 when m is large enough. Consequently, the Armijo point exists. We refer the reader to [17] for details.\nWe summarize the line-search method in Algorithm 1.\nAlgorithm 1 Line-Search Method on the Stiefel manifold\nRequire: Select X0 \u2208 St(m,n), \u03c1 > 0, \u03b2, c \u2208 (0, 1). 1: for k = 0, 1, 2, . . . do 2: Calculate the descent direction \u2212D\u03c1(Xk) at Xk. 3: Choose \u03b2\u0304k \u2265 1 and find the Armijo point \u03b1k at Xk with parameters (\u03b2\u0304k, \u03b2, c). 4: Set Xk+1 = R (Xk,\u2212\u03b1kD\u03c1(Xk)). 5: end for"}, {"heading": "2.4 Convergence Analysis Framework for the Line-Search Method", "text": "To analyze the convergence properties of Algorithm 1, we adopt the framework introduced in [17]. It has been shown in [17, Corollary 2.9] that Algorithm 1 has the following properties:\n\u2022 (Primary Descent) There exists a constant \u03c3 > 0 such that for all k large enough,\nF (Xk+1)\u2212 F (Xk) \u2264 \u2212\u03c3 \u2016D\u03c1(Xk)\u2016F \u2016Xk+1 \u2212Xk\u2016F .\n\u2022 (Stationarity) For all k large enough,\n\u2016D\u03c1(Xk)\u2016F = 0 =\u21d2 Xk+1 = Xk.\nMoreover, we show in the appendix that Algorithm 1 has the following property:\nProposition 2 (Asymptotic Small Step Size Safeguard) There exists a constant \u03ba > 0 such that for all k large enough,\n\u2016Xk+1 \u2212Xk\u2016F \u2265 \u03ba \u2016D\u03c1(Xk)\u2016F . (5)\nThus, by [17, Theorem 2.3], in order to establish the linear convergence of Algorithm 1 to a critical point of Problem (1), it remains to prove the following theorem:\nTheorem 1 ( Lojasiewicz Inequality for Quadratic Optimization with Orthogonality Constraints) There exist constants \u03b4, \u03b7 > 0 such that for all X \u2208 St(m,n) and X\u2217 \u2208 X with \u2016X \u2212X\u2217\u2016F \u2264 \u03b4,\n|F (X) \u2212 F (X\u2217)|1/2 \u2264 \u03b7 \u2016D\u03c1(X)\u2016F .\nThe proof of Theorem 1 is based on the following two results:\nTheorem 2 (Local Error Bound for Quadratic Optimization with Orthogonality Constraints) There exist constants \u03b4, \u03b7 > 0 such that\ndist(X,X ) \u2264 \u03b7\u2016D\u03c1(X)\u2016F whenever X \u2208 St(m,n) and dist(X,X ) \u2264 \u03b4.\nWe defer the proof of Theorem 2 to Section 3.\nProposition 3 (2-Ho\u0308lder Continuity of F ) There exists a constant \u03b7 > 0 such that for all X \u2208 St(m,n) and X\u2217 \u2208 X ,\n|F (X) \u2212 F (X\u2217)| \u2264 \u03b7\u2016X \u2212X\u2217\u20162F .\nProof Observe that F , when viewed as a function on Rm\u00d7n, is continuously differentiable with Lipschitz continuous gradient. Thus, we have\n|F (X) \u2212 F (X\u2217)\u2212 \u3008\u2207F (X\u2217),X \u2212X\u2217\u3009| \u2264 L\n2 \u2016X \u2212X\u2217\u20162F , (6)\nwhere L > 0 is the Lipschitz constant of \u2207F ; see, e.g., [15]. Now, by Proposition 1, we have \u2207F (X\u2217) = X\u2217\u2207F (X\u2217)TX\u2217. This implies that\n\u3008\u2207F (X\u2217),X \u2212X\u2217\u3009 = \u2329 X\u2217\u2207F (X\u2217)TX\u2217,X \u2212X\u2217 \u232a = \u2329 \u2207F (X\u2217)TX\u2217, (X\u2217)TX \u2212 In \u232a . (7)\nOn the other hand,\n\u2329\n\u2207F (X\u2217)TX\u2217, In \u2212X TX\u2217\n\u232a = \u2329 (X\u2217)T\u2207F (X\u2217), (X\u2217)TX\u2217 \u2212XTX\u2217 \u232a\n= \u2329 X\u2217\u2207F (X\u2217)TX\u2217,X\u2217 \u2212X \u232a = \u2212\u3008\u2207F (X\u2217),X \u2212X\u2217\u3009. (8)\nUpon adding (7) and (8) and using the fact that (X \u2212X\u2217)T (X\u2212X\u2217) = 2In\u2212 (X \u2217)TX\u2212XTX\u2217, we obtain 2\u3008\u2207F (X\u2217),X \u2212X\u2217\u3009 = \u2212 \u2329 \u2207F (X\u2217)TX\u2217, (X \u2212X\u2217)T (X \u2212X\u2217) \u232a ,\nor equivalently,\n|\u3008\u2207F (X\u2217),X \u2212X\u2217\u3009| \u2264 1\n2\n\u2225 \u2225\u2207F (X\u2217)TX\u2217 \u2225 \u2225 F \u2016X \u2212X\u2217\u20162F .\nThis, together with (6), yields the desired inequality with \u03b7 = ( L+ \u2225 \u2225\u2207F (X\u2217)TX\u2217 \u2225 \u2225\nF\n)\n/2. \u2294\u2293 To complete the proof of Theorem 1, let X \u2208 St(m,n) and X\u2217 \u2208 X be such that dist(X,X ) = \u2016X \u2212X\u2217\u2016F \u2264 \u03b4, where \u03b4 > 0 is given by Theorem 2. Then, by Proposition 3, we obtain\n|F (X) \u2212 F (X\u2217)| \u2264 \u03b71\u2016X \u2212X \u2217\u20162F = \u03b71 \u00b7 dist(X,X ) 2 \u2264 \u03b71\u03b72\u2016D\u03c1(X)\u2016 2 F\nfor some constants \u03b71, \u03b72 > 0, as desired."}, {"heading": "3 Proof of Theorem 2", "text": "We now prove Theorem 2, which is the main result of this paper. The proof can be divided into four steps."}, {"heading": "3.1 Preliminary Observations", "text": "Let A = UA\u03a3AU T A and B = UB\u03a3BU T B be spectral decompositions of A and B, respectively. It is straightforward to verify that tr ( XTAXB ) = tr ( X\u0304T\u03a3AX\u0304\u03a3B )\n, where X\u0304 = UTAXUB \u2208 St(m,n). Thus, we may assume without loss of generality that\nA = Diag(a1, . . . , am) \u2208 S m and B = Diag(b1, . . . , bn) \u2208 S n,\nwhere a1 \u2265 a2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 am and b1 \u2265 b2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 bn. By Proposition 1, we can write\nX = { X \u2208 St(m,n) | AXB \u2212XBXTAX = 0 } . (9)\nNow, it can be verified that\nD\u03c1(X) = ( Im \u2212 (1\u2212 2\u03c1)XX T ) ( \u2207F (X)\u2212X\u2207F (X)TX ) .\nSince \u03c1 > 0, we see that Im \u2212 (1\u2212 2\u03c1)XX T is invertible and\n\u2225 \u2225\u2207F (X)\u2212X\u2207F (X)TX \u2225 \u2225\nF \u2264\n\u2225 \u2225 \u2225 ( Im \u2212 (1\u2212 2\u03c1)XX T )\u22121 \u2225 \u2225 \u2225 \u00b7 \u2016D\u03c1(X)\u2016F \u2264 1\n2\u03c1 \u2016D\u03c1(X)\u2016F .\nIn particular, in order to prove Theorem 2, it suffices to prove the following:\nTheorem 2\u2019. There exist constants \u03b4, \u03b7 > 0 such that\ndist(X,X ) \u2264 \u03b7 \u2225 \u2225AXB \u2212XBXTAX \u2225 \u2225\nF whenever X \u2208 St(m,n) and dist(X,X ) \u2264 \u03b4."}, {"heading": "3.2 Characterizing the Set of Critical Points when B has Full Rank", "text": "Consider first the case where B has full rank; i.e., bi 6= 0 for i = 1, . . . , n. Let nA and nB be the number of distinct eigenvalues of A and B, respectively. Then, there exist indices s0, s1, . . . , snA and t0, t1, . . . , tnB such that 0 = s0 < s1 < \u00b7 \u00b7 \u00b7 < snA = m and 0 = t0 < t1 < \u00b7 \u00b7 \u00b7 < tnB = n, and\nas0+1 = \u00b7 \u00b7 \u00b7 = as1 > as1+1 = \u00b7 \u00b7 \u00b7 = as2 > \u00b7 \u00b7 \u00b7 > asnA\u22121+1 = \u00b7 \u00b7 \u00b7 = asnA , bt0+1 = \u00b7 \u00b7 \u00b7 = bt1 > bt1+1 = \u00b7 \u00b7 \u00b7 = bt2 > \u00b7 \u00b7 \u00b7 > btnB\u22121+1 = \u00b7 \u00b7 \u00b7 = btnB .\nLet U1, . . . , UnA and V1, . . . , VnB be the eigenspaces of A andB, respectively. Note that dim(Ui) = si \u2212 si\u22121 for i = 1, . . . , nA and dim(Vj) = tj \u2212 tj\u22121 for j = 1, . . . , nB. Furthermore, let\nH =\n{\n(h1, . . . , hnA)\n\u2223 \u2223 \u2223 \u2223 \u2223 nA \u2211\ni=1\nhi = n, hi \u2208 {0, 1, . . . , si \u2212 si\u22121} for i = 1, . . . , nA\n}\nand {ei} m i=1 be the standard basis of R m. Given any h = (h1, . . . , hnA) \u2208 H, define\nEi(h) = [esi\u22121+1 \u00b7 \u00b7 \u00b7 esi\u22121+hi ] \u2208 R m\u00d7hi for i = 1, . . . , nA,\nE(h) = [E1(h) \u00b7 \u00b7 \u00b7 EnA(h)] \u2208 R m\u00d7n. (10)\nWe then have the following characterization of the set X of critical points of Problem (1), whose proof can be found in the appendix:\nProposition 4 The following holds:\nX = {X \u2208 St(m,n) | X = BlkDiag(P1, . . . , PnA) \u00b7 E(h) \u00b7 BlkDiag(Q1, . . . , QnB )\nfor some Pi \u2208 O si\u2212si\u22121 (i = 1, . . . , nA), Qj \u2208 O tj\u2212tj\u22121 (j = 1, . . . , nB), and h \u2208 H }\n. (11)\nRemarks. (i) Essentially, Proposition 4 states that every X \u2208 X can be factorized as X = PQ, where P \u2208 St(m,n) and Q \u2208 On, and the columns of P (resp. Q) are the eigenvectors of A (resp. B). Indeed, observe that for i = 1, . . . , nA, the (si\u22121 + 1)-st to si-th columns of BlkDiag(P1, . . . , PnA) form an orthonormal basis of Ui. Similarly, for j = 1, . . . , nB , the (tj\u22121 + 1)-st to tj-th columns of BlkDiag(Q1, . . . , QnB ) form an orthonormal basis of Vj . To specify which n of the m eigenvectors of A are chosen to form P , we use the matrix E(h), where h = (h1, . . . , hnA) \u2208 H and hi is the number of eigenvectors chosen from the eigenspace Ui.\n(ii) A result similar to Proposition 4 has appeared in [3, Section 4.8.2]. However, the proof therein contains a small gap. Specifically, from the properties that B is diagonal and commutes with XTAX, it is claimed in [3, Section 4.8.2] that XTAX is also diagonal. However, this is not true unless the diagonal entries of B are all distinct.\nProposition 4 suggests that we can partition X into disjoint subsets {Xh}h\u2208H, where\nXh = {X \u2208 St(m,n) | X = BlkDiag(P1, . . . , PnA) \u00b7 E(h) \u00b7 BlkDiag(Q1, . . . , QnB )\nfor some Pi \u2208 O si\u2212si\u22121 (i = 1, . . . , nA), Qj \u2208 O tj\u2212tj\u22121 (j = 1, . . . , nB) } .\nConsequently, in order to prove Theorem 2\u2019, it suffices to bound dist(X,Xh) for any X \u2208 St(m,n) and h \u2208 H."}, {"heading": "3.3 Estimating the Distance to the Set of Critical Points", "text": "Let X \u2208 St(m,n) and h = (h1, . . . , hnA) \u2208 H be arbitrary. By definition,\ndist(X,Xh) = min { \u2016X \u2212 BlkDiag (P1, . . . , PnA) \u00b7E(h) \u00b7 BlkDiag (Q1, . . . , QnB )\u2016F |\nPi \u2208 O si\u2212si\u22121 for i = 1, . . . , nA; Qj \u2208 O tj\u2212tj\u22121 for j = 1, . . . , nB } . (12)\nLet ( P \u22171 , . . . , P \u2217 nA , Q \u2217 1, . . . , Q \u2217 nB ) be an optimal solution to (12). Upon letting\nP \u2217 = BlkDiag ( P \u22171 , . . . , P \u2217 nA ) \u2208 Om, Q\u2217 = BlkDiag ( Q\u22171, . . . , Q \u2217 nB ) \u2208 On,\nand X\u0304 = (P \u2217)TX(Q\u2217)T , it is clear that dist2(X,Xh) = \u2225 \u2225X\u0304 \u2212 E(h) \u2225 \u2225 2\nF . To bound this quantity,\nconsider the decompositions\nX\u0304 = [ X\u03041 \u00b7 \u00b7 \u00b7 X\u0304nB ] and E(h) = [ E\u03041(h) \u00b7 \u00b7 \u00b7 E\u0304nB(h) ] , (13)\nwhere X\u0304j , E\u0304j(h) \u2208 R m\u00d7(tj\u2212tj\u22121). We then have the following result, whose proof can be found in the appendix:\nProposition 5 For j = 1, . . . , nB and k = 1, . . . ,m, denote the k-th row of X\u0304j and E\u0304j(h) by [\nX\u0304j ]\nk and\n[ E\u0304j(h) ]\nk , respectively. Suppose that dist(X,Xh) \u2264 \u03b4 for some \u03b4 \u2208 (0, 1). Then,\ndist2(X,Xh) =\nnB \u2211\nj=1\n\u2211\nk\u2208Ij\n\u0398 ( \u2225 \u2225 [ X\u0304j ]\nk\n\u2225 \u2225 2\n2\n)\n,\nwhere Ij = { k \u2208 {1, . . . ,m} : [ E\u0304j(h) ]\nk = 0\n}\n.\nTo establish the desired error bound, we need to link \u2225 \u2225AXB \u2212XBXTAX \u2225 \u2225\nF to the bound on\ndist2(X,Xh) in Proposition 5. This is achieved in two steps. First, we prove the following result:\nProposition 6 Consider the decomposition of X\u0304 in (13). Then,\n\u2225 \u2225AXB \u2212XBXTAX \u2225 \u2225\n2 F = \u2126\n\n\nnB \u2211\nj=1\n\u2225 \u2225AX\u0304j \u2212 X\u0304jX\u0304 T j AX\u0304j \u2225 \u2225\n2 F\n\n .\nIn view of Proposition 6, we then proceed to prove the following bound:\nProposition 7 Suppose that dist(X,Xh) \u2264 \u03b4 for some \u03b4 \u2208 (0, 1). Then,\nnB \u2211\nj=1\n\u2225 \u2225AX\u0304j \u2212 X\u0304jX\u0304 T j AX\u0304j \u2225 \u2225\n2 F =\nnB \u2211\nj=1\n\u2211\nk\u2208Ij\n\u2126 ( \u2225 \u2225 [ X\u0304j ]\nk\n\u2225 \u2225 2\n2\n)\n.\nThe proofs of Propositions 6 and 7 can be found in the appendix. Now, observe that whenever X \u2208 St(m,n) and dist(X,X ) \u2264 \u03b4, then there exists an h \u2208 H such that dist(X,Xh) \u2264 \u03b4. Hence, by combining Propositions 5, 6, and 7, we obtain Theorem 2\u2019."}, {"heading": "3.4 Removing the Full Rank Assumption on B", "text": "Consider now the case where B does not have full rank. Without loss of generality, we assume that B = BlkDiag(B\u0304,0), where B\u0304 = Diag(b1, . . . , bp) \u2208 S\np has full rank. Then, using (9), it can be shown that\nX = { X = [X1 X2] \u2208 St(m,n) | X1 \u2208 R m\u00d7p,X2 \u2208 R m\u00d7(n\u2212p), AX1B\u0304 \u2212X1B\u0304X T 1 AX1 = 0 } .\nIt follows that for any X = [X1 X2] \u2208 St(m,n) with X1 \u2208 R m\u00d7p and X2 \u2208 R m\u00d7(n\u2212p), we have dist(X,X ) = dist(X1, X\u0304 ), where\nX\u0304 = { X \u2208 St(m, p) | AXB\u0304 \u2212XB\u0304XTAX = 0 } .\nBy our previous result, there exist constants \u03b4, \u03b7 > 0 such that\ndist(X1, X\u0304 ) \u2264 \u03b7 \u2225 \u2225AXB\u0304 \u2212XB\u0304XTAX \u2225 \u2225\nF\nwhenever X1 \u2208 St(m, p) and dist(X1, X\u0304 ) \u2264 \u03b4. To complete the proof, it remains to observe that\n\u2225 \u2225AXB \u2212XBXTAX \u2225 \u2225\n2 F = \u2225 \u2225AX1B\u0304 \u2212X1B\u0304X T 1 AX1 \u2225 \u2225 2 F + \u2225 \u2225X1B\u0304X T 1 AX2 \u2225 \u2225 2 F\n= \u2225 \u2225AX1B\u0304 \u2212X1B\u0304X T 1 AX1 \u2225 \u2225\n2 F + \u2225 \u2225XT2 ( AX1B\u0304 \u2212X1B\u0304X T 1 AX1 ) XT1 \u2225 \u2225 2 F\n= \u0398 ( \u2225\n\u2225AX1B\u0304 \u2212X1B\u0304X T 1 AX1\n\u2225 \u2225\n2 F\n)\n."}, {"heading": "4 Numerical Experiments", "text": "In this section, we perform numerical experiments to investigate the convergence rate of the retracted line-search algorithm for problem (1) on synthetic datasets. As we shall see, the results consistency with the theoretical analysis in previous sections. In particular, we consider the four retractions mentioned above.\nFirst, we generate our diagonal matrices A \u2208 Sm and B \u2208 Sn, whose diagonal elements are sampled randomly from the uniform distribution. The starting point X0 is chosen from the uniform distribution and get the orthonormal basis for the range of X0 to keep the feasibility. In the setting of Armijo point, we fix \u03b3 = 1, \u03b2 = 0.5 and c = 0.001. We stop the algorithm when F (Xk)\u2212 F (Xk+1) < 10\n\u22128. In practical computations, the orthogonality constraint may be violated after several iterations, which is mainly due to numerical errors incurred in the multiplication. In the numerical experiments, we follow the technique introduced in [7] and use (XTX)\u22121 to control feasibility error.\nFigure (1) illustrates the convergence performance of the four retractions with the relative \u201cThin\u201d matrix: (1(a))m = 20, n = 10, (1(b)) m = 30, n = 10, (1(c)) m = 100, n = 10. Figure (2) illustrates the convergence performance with the relative \u201cFat\u201d matrix: (2(a)) m = 20, n = 15, (2(b)) m = 50, n = 40, (2(c)) m = 100, n = 80. It can be seen that as long as the iterates are close enough to the optimal set, both the objective values and the solutions converge linearly."}, {"heading": "5 Conclusion", "text": "In this paper, we gave an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of Problem (1). Such an estimate was obtained by establishing a local error bound for the aforementioned set of critical points. Together with known arguments, our result implies the linear convergence of a large class of line-search methods on the Stiefel manifold. An interesting future direction would be to extend our techniques to analyze the convergence rates of first-order methods for solving structured non-convex optimization problems.\nAppendix"}, {"heading": "A Proof of Proposition 2", "text": "The proof of Proposition 2 is based on the following lemma:\nLemma 1 The Armijo points {\u03b1k}k\u22650 satisfy limk\u21920 \u03b1k \u2016D\u03c1(Xk)\u2016F = 0.\nProof The Armijo point exists in each step, which guarantees a sufficient decrease. We add all the decrease together and the sum must be finite, since there is a lower bound on the function value; i.e.\n+\u221e \u2211\nk=0\nc\u03b1k\u2016D\u03c1(Xk)\u2016 2 F < +\u221e,\nwhich implies that lim k\u21920 \u03b1k\u2016D\u03c1(Xk)\u2016 2 F = 0. Here, all the Armijo points have an upper bound \u03b3. Thus,\nlim k\u21920\n\u03b12k\u2016D\u03c1(Xk)\u2016 2 \u2264 lim\nk\u21920 \u03b3\u03b1k\u2016D\u03c1(Xk)\u2016\n2 = 0.\nThus, we have lim k\u21920 \u03b1k\u2016D\u03c1(Xk)\u2016 = 0, as desired. \u2294\u2293 Proof of Proposition 2. By construction of the algorithm, with Vk = \u2212\u03b1kD\u03c1(Xk), we have\nXk+1 \u2212Xk = R(Xk, Vk)\u2212Xk = R(Xk, Vk)\u2212 (Xk + Vk)\u2212 Vk,\nwhich implies that\n\u2016Xk+1 \u2212Xk\u2016F \u2265 \u2016Vk\u2016F \u2212 \u2016R(Xk, Vk)\u2212 (Xk + Vk)\u2016F .\nWe divide by \u2016Vk\u2016F on both sides to obtain\n\u2016Xk+1 \u2212Xk\u2016F \u2016Vk\u2016F \u2265 1\u2212 \u2016R(Xk, Vk)\u2212 (Xk + Vk)\u2016F \u2016Vk\u2016F .\nIt follows that\nlim k\u2192\u221e \u2016Xk+1 \u2212Xk\u2016F \u2016Vk\u2016F \u2265 1\u2212 lim \u2016Vk\u2016F\u21920 \u2016R(Xk, Vk)\u2212 (Xk + Vk)\u2016F \u2016Vk\u2016F .\nAccording to the definition of smooth retraction (3), the last term is equal to 0. Thus,\nlim k\u2192\u221e \u2016Xk+1 \u2212Xk\u2016F \u2016Vk\u2016F \u2265 1.\nTherefore, there exists a large enough k to make sure (5) hold if we choose 1/2."}, {"heading": "B Proof of Proposition 4", "text": "Let X \u2208 X be arbitrary. Using (9) and the fact that XTX = In, we have X TAXB = BXTAX. Since both XTAX and B are symmetric, this implies that XTAX and B are simultaneously diagonalizable. In particular, there exist orthogonal matrices Qj \u2208 O\ntj\u2212tj\u22121 and diagonal matrices \u03a3j \u2208 S\ntj\u2212tj\u22121 , where j = 1, . . . , nB , such that the columns of BlkDiag(Q1, . . . , QnB) are the eigenvectors of B, and that\nXTAX = BlkDiag ( QT1 \u03a31Q1, . . . , Q T nB \u03a3nBQnB ) . (14)\nNow, using (9) again, we have ( AX \u2212XXTAX )\nB = 0. Since B has full rank and hence invertible, this yields AX = XXTAX. Upon letting Y = X \u00b7BlkDiag (\nQT1 , . . . , Q T nB\n)\n\u2208 St(m,n) and using (14), we obtain AY = Y \u00b7 BlkDiag(\u03a31, . . . ,\u03a3nB). As \u03a31, . . . ,\u03a3nB are diagonal, this implies that each of the n columns of Y is an eigenvector of A. To see that X can be expressed in the form given on the right-hand side of (11), it remains to note that A has m eigenvectors in total, and that any set of m eigenvectors of A can be expressed as BlkDiag(P1, . . . , PnA) for some Pi \u2208 O\nsi\u2212si\u22121 , where i = 1, . . . , nA. The converse is rather easy to verify. Hence, the proof is completed."}, {"heading": "C Proof of Proposition 5", "text": "Using (12) and (13), it can be verified that\ndist2(X,Xh) = \u2225 \u2225X\u0304 \u2212 E(h) \u2225 \u2225 2\nF\n= min { \u2225\n\u2225X\u0304 \u2212 E(h) \u00b7 BlkDiag(Q1, . . . , QnB ) \u2225 \u2225 2\nF\n\u2223 \u2223 \u2223 Qj \u2208 O tj\u2212tj\u22121 for j = 1, . . . , nB }\n=\nnB \u2211\nj=1\nmin { \u2225 \u2225X\u0304j \u2212 E\u0304j(h)Qj \u2225 \u2225 2\nF\n\u2223 \u2223 \u2223 Qj \u2208 O tj\u2212tj\u22121 } .\nFrom the definitions of E(h) in (10) and E\u0304j(h) in (13), we see that up to a rearrangement\nof the rows, E\u0304j(h) takes the form E\u0304j(h) =\n[\nItj\u2212tj\u22121 0\n]\n. Thus, to obtain the desired bound on\ndist2(X,Xh), it remains to prove the following:\nLemma 2 Let S =\n[\nS1 S2\n]\n\u2208 St(p, q) be given, with S1 \u2208 R q\u00d7q and S2 \u2208 R (p\u2212q)\u00d7q. Consider the\nfollowing problem:\nv\u2217 = min\n{\n\u2225 \u2225 \u2225 \u2225 S \u2212 [ Iq 0 ] X \u2225 \u2225 \u2225 \u2225\n2\nF\n\u2223 \u2223 \u2223 \u2223 \u2223 X \u2208 Oq } .\nSuppose that v\u2217 < 1. Then, we have v\u2217 = \u0398 ( \u2016S2\u2016 2 F ) .\nProof Since \u2225\n\u2225 \u2225 \u2225 S \u2212\n[\nIq 0\n]\nX\n\u2225 \u2225 \u2225 \u2225\n2\nF\n= \u2016S1 \u2212X\u2016 2 F + \u2016S2\u2016 2 F ,\nit suffices to consider the problem\nmin { \u2016S1 \u2212X\u2016 2 F | X \u2208 O q } . (15)\nProblem (15) is an instance of the orthogonal Procrustes problem, whose optimal solution is given by X\u2217 = UV T , where S1 = U\u03a3V\nT is the singular value decomposition of S1 [18]. It follows that\nv\u2217 = \u2016\u03a3\u2212 Iq\u2016 2 F + \u2016S2\u2016 2 F .\nNow, since S \u2208 St(p, q), we have STS = ST1 S1 + S T 2 S2 = Iq, or equivalently,\n\u03a32 + V TST2 S2V = Iq.\nThis implies that 0 \u03a3 Iq and\nIq \u2212 \u03a3 = (Iq +\u03a3) \u22121\n( V TST2 S2V ) .\nIt follows that 1\n4 \u2016S2\u2016\n4 F + \u2016S2\u2016 2 F \u2264 v \u2217 \u2264 \u2016S2\u2016 4 F + \u2016S2\u2016 2 F .\nThis, together with the fact that \u2016S2\u2016 2 F \u2264 v \u2217 < 1, yields v\u2217 = \u0398 ( \u2016S2\u2016 2 F ) , as desired. \u2294\u2293"}, {"heading": "D Proof of Proposition 6", "text": "Recall that\nP \u2217 = BlkDiag ( P \u22171 , . . . , P \u2217 nA ) \u2208 Om, Q\u2217 = BlkDiag ( Q\u22171, . . . , Q \u2217 nB ) \u2208 On, X\u0304 = (P \u2217)TX(Q\u2217)T .\nUpon observing that AP \u2217 = P \u2217A, BQ\u2217 = Q\u2217B, B = BlkDiag (\nbt1It1\u2212t0 , . . . , btnB ItnB\u2212tnB\u22121\n)\nand using (13), we compute\n\u2225 \u2225AXB \u2212XBXTAX \u2225 \u2225\n2 F = \u2225 \u2225AP \u2217X\u0304Q\u2217B \u2212 P \u2217X\u0304Q\u2217B(Q\u2217)T X\u0304T (P \u2217)TAP \u2217X\u0304Q\u2217 \u2225 \u2225 2 F\n= \u2225 \u2225P \u2217 ( AX\u0304B \u2212 X\u0304BX\u0304TAX\u0304 ) Q\u2217 \u2225 \u2225\n2 F\n= \u2225 \u2225AX\u0304B \u2212 X\u0304BX\u0304TAX\u0304 \u2225 \u2225\n2 F\n=\nnB \u2211\nj=1\n\u2225 \u2225 \u2225 \u2225 \u2225 btjAX\u0304j \u2212 nB \u2211\nk=1\nbtkX\u0304k ( X\u0304Tk AX\u0304j )\n\u2225 \u2225 \u2225 \u2225 \u2225 2\nF\n. (16)\nNow, observe that the columns of X\u0304 are orthonormal and span an n-dimensional subspace L. In particular, for j = 1, . . . , nB , each column of AX\u0304j can be decomposed as u+ v, where u is a linear combination of the columns of X\u0304 and v \u2208 L\u22a5, the orthogonal complement of L. In view of the structure of X\u0304 in (13), this leads to\nAX\u0304j =\nnB \u2211\nk=1\nX\u0304k ( X\u0304Tk AX\u0304j ) + Tj,\nwhere Tj \u2208 R m\u00d7(tj\u2212tj\u22121) is formed by projecting the columns of AX\u0304j onto L \u22a5. Hence,\n\u2225 \u2225 \u2225 \u2225 \u2225 btjAX\u0304j \u2212 nB \u2211\nk=1\nbtkX\u0304k ( X\u0304Tk AX\u0304j )\n\u2225 \u2225 \u2225 \u2225 \u2225 2\nF\n= \u2211\nk 6=j\n(btj \u2212 btk) 2 \u2225 \u2225X\u0304k ( X\u0304Tk AX\u0304j )\u2225 \u2225\n2 F + b2tj\u2016Tj\u2016 2 F\n= \u2126\n\n\n\u2211\nk 6=j\n\u2225 \u2225X\u0304k ( X\u0304Tk AX\u0304j )\u2225 \u2225\n2 F + \u2016Tj\u2016 2 F\n\n (17)\n= \u2126 ( \u2225\n\u2225AX\u0304j \u2212 X\u0304jX\u0304 T j AX\u0304j\n\u2225 \u2225\n2 F\n)\n,\nwhere (17) follows from the fact that btj 6= btk whenever j 6= k and btj 6= 0 since B is assumed to have full rank. By combining the above with (16), the proof is completed."}, {"heading": "E Proof of Proposition 7", "text": "Consider a fixed j \u2208 {1, . . . , nB}. Let x\u0304k be the k-th column of X\u0304j and (x\u0304k)\u03b1 be the \u03b1-th entry of x\u0304k, where k = 1, . . . , tj \u2212 tj\u22121 and \u03b1 = 1, . . . ,m. Since dist(X,Xh) = \u2225 \u2225X\u0304 \u2212 E(h) \u2225 \u2225\nF \u2264 \u03b4, using\nthe definition of E(h) in (10), we have\n(x\u0304k)\u03b1 =\n{\n1 +O(\u03b4) if \u03b1 = \u03c0(k), O(\u03b4) otherwise,\nwhere \u03c0(k) is the coordinate of the k-th column of E\u0304j(h) that equals 1. Since \u03c0(k) 6= \u03c0(\u2113) whenever k 6= \u2113, it follows that\nx\u0304TkAx\u0304\u2113 =\n{\na\u03c0(k) +O(\u03b4) if k = \u2113,\nO(\u03b4) otherwise.\nNow, let \u2206k be the k-th column of AX\u0304j \u2212 X\u0304jX\u0304 T j AX\u0304j , where k = 1, . . . , tj \u2212 tj\u22121. Then,\n\u2206k = Ax\u0304k \u2212\ntj\u2212tj\u22121 \u2211\n\u2113=1\nx\u0304\u2113 ( x\u0304T\u2113 Ax\u0304k ) = ( A\u2212 a\u03c0(k)Im ) x\u0304k \u2212O(\u03b4) \u00b7\n\n\ntj\u2212tj\u22121 \u2211\n\u2113=1\nx\u0304\u2113\n\n .\nLet \u03a0Ij be the projector onto the coordinates in Ij. By Proposition 5 and the assumption that dist(X,Xh) \u2264 \u03b4, we have\ntj\u2212tj\u22121 \u2211\n\u2113=1\n\u2225 \u2225\u03a0Ij (x\u0304\u2113) \u2225 \u2225 2 2 =\n\u2211\nk\u2208Ij\n\u2225 \u2225 [ X\u0304j ]\nk\n\u2225 \u2225 2\n2 = O(\u03b4).\nHence,\n\u2225 \u2225\u03a0Ij(\u2206k) \u2225 \u2225 2 \u2265 \u2225 \u2225\u03a0Ij (( A\u2212 a\u03c0(k)Im ) x\u0304k )\u2225 \u2225 2 \u2212O(\u03b4) \u00b7\n\n\ntj\u2212tj\u22121 \u2211\n\u2113=1\n\u2225 \u2225\u03a0Ij (x\u0304\u2113) \u2225 \u2225\n2\n\n\n\u2265 \u2225 \u2225\u03a0Ij (( A\u2212 a\u03c0(k)Im ) x\u0304k )\u2225 \u2225 2 \u2212O(\u03b42). (18)\nLet i\u2032 \u2208 {0, 1, . . . , nA \u2212 1} be such that si\u2032 + 1 \u2264 \u03c0(k) \u2264 si\u2032+1. Then, we have\n\u2225 \u2225\u03a0Ij (( A\u2212 a\u03c0(k)Im ) x\u0304k )\u2225 \u2225 2 2 =\n\u2211\ni 6=i\u2032\n\u2211\n\u03b1\u2208Ij\u2229{si+1,...,si+1}\n(( asi+1 \u2212 a\u03c0(k) ) (x\u0304k)\u03b1 )2\n= \u2211\ni 6=i\u2032\n\u2211\n\u03b1\u2208Ij\u2229{si+1,...,si+1}\n\u2126 ( (x\u0304k) 2 \u03b1 )\n= \u2126 ( \u2225\n\u2225\u03a0Ij (x\u0304k) \u2225 \u2225 2\n2\n)\n\u2212O\n(\n\u2225 \u2225 \u2225 \u03a0Ij\u2229{si\u2032+1,...,si\u2032+1}(x\u0304k) \u2225 \u2225 \u2225 2\n2\n)\n. (19)\nTo bound the term \u2225 \u2225\n\u2225 \u03a0Ij\u2229{si\u2032+1,...,si\u2032+1}(x\u0304k)\n\u2225 \u2225 \u2225 2\n2 , we proceed as follows. Let Y = X(Q\u2217)T \u2208 St(m,n)\nand decompose it as\nY =\n\n  Y11 \u00b7 \u00b7 \u00b7 Y1nA ... . . . ...\nYnA1 \u00b7 \u00b7 \u00b7 YnAnA\n\n  ,\nwhere Yii \u2208 R (si\u2212si\u22121)\u00d7hi , for i = 1, . . . , nA. Observe that\ndist2(X,Xh) = min { \u2016Y \u2212 BlkDiag(P1, . . . , PnA) \u00b7 E(h)\u2016 2 F | Pi \u2208 O si\u2212si\u22121 for i = 1, . . . , nA }\n= \u2211\n1\u2264i 6=j\u2264nA\n\u2016Yij\u2016 2 F +\nnA \u2211\ni=1\nmin\n{\n\u2225 \u2225 \u2225 \u2225 Yii \u2212 Pi [ Ihi 0 ]\u2225 \u2225 \u2225 \u2225\n2\nF\n\u2223 \u2223 \u2223 \u2223 \u2223 Pi \u2208 O si\u2212si\u22121 } . (20)\nThe following lemma establishes a bound on the second term in (20):\nLemma 3 For i = 1, . . . , nA, let\nv\u2217i = min\n{\n\u2225 \u2225 \u2225 \u2225 Yii \u2212 Pi [ Ihi 0 ]\u2225 \u2225 \u2225 \u2225 2\nF\n\u2223 \u2223 \u2223 \u2223 \u2223 Pi \u2208 O si\u2212si\u22121 } . (21)\nThen, we have\nv\u2217i = \u0398\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj 6=i\nY Tji Yji\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nF\n\n .\nLet us defer the proof of Lemma 3 to the end of this section. Together with (20), Lemma 3 implies that\ndist2(X,Xh) = \u2211\n1\u2264i 6=j\u2264nA\n\u2016Yij\u2016 2 F +\nnA \u2211\ni=1\n\u0398\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj 6=i\nY Tji Yji\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nF\n\n .\nSince dist(X,Xh) \u2264 \u03b4 for some \u03b4 \u2208 (0, 1), we have \u2211\n1\u2264i 6=j\u2264nA \u2016Yij\u2016 2 F = O(\u03b4 2). This implies that\nv\u2217i = O\n\n\n\n\n\u2211\nj 6=i\n\u2016Yji\u2016 2 F\n\n\n2\n = O(\u03b44)\nfor i = 1, . . . , nA. Now, decompose X\u0304 = (P \u2217)TY as\n\n  X\u030411 \u00b7 \u00b7 \u00b7 X\u03041nA ... . . . ...\nX\u0304nA1 \u00b7 \u00b7 \u00b7 X\u0304nAnA\n\n  ,\nwhere X\u0304ii = (P \u2217 i ) TYii \u2208 R (si\u2212si\u22121)\u00d7hi for i = 1, . . . , nA. Note that for i = 1, . . . , nA, we have\nv\u2217i =\n\u2225 \u2225 \u2225 \u2225 X\u0304ii \u2212 [ Ihi 0 ]\u2225 \u2225 \u2225 \u2225 2\nF\n.\nMoreover, observe that \u03a0Ij\u2229{si\u2032+1,...,si\u2032+1}(x\u0304k) is part of X\u0304i\u2032+1,i\u2032+1 and does not intersect the diagonal of the top hi\u2032+1 \u00d7 hi\u2032+1 block of X\u0304i\u2032+1,i\u2032+1. Thus, by Lemma 3,\n\u2225 \u2225 \u2225\u03a0Ij\u2229{si\u2032+1,...,si\u2032+1}(x\u0304k) \u2225 \u2225 \u2225 2 2 \u2264 v\u2217i\u2032+1 = O(\u03b4 4).\nTogether with (18) and (19), this yields\n\u2225 \u2225\u03a0Ij (\u2206k) \u2225 \u2225 2 2 \u2265 \u2126\n(\n\u2225 \u2225\u03a0Ij (x\u0304k) \u2225 \u2225 2\n2\n)\n\u2212O(\u03b43).\nIt follows that\n\u2225 \u2225AX\u0304j \u2212 X\u0304jX\u0304 T j AX\u0304j \u2225 \u2225\n2 F =\ntj\u2212tj\u22121 \u2211\nk=1\n\u2016(\u2206k)\u2016 2 2\n\u2265\ntj\u2212tj\u22121 \u2211\nk=1\n\u2225 \u2225\u03a0Ij(\u2206k) \u2225 \u2225 2\n2\n\u2265\ntj\u2212tj\u22121 \u2211\nk=1\n\u2126 ( \u2225\n\u2225\u03a0Ij (x\u0304k) \u2225 \u2225 2\n2\n)\n\u2212O(\u03b43)\n= \u2211\nk\u2208Ij\n\u2126 ( \u2225 \u2225 [ X\u0304j ]\nk\n\u2225 \u2225 2\n2\n)\n\u2212O(\u03b43).\nUpon summing over j = 1, . . . , nB and using Proposition 5, we obtain the desired bound. To complete the proof, it remains to prove Lemma 3. Proof of Lemma 3. Consider a fixed i \u2208 {1, . . . , nA}. Note that Problem (21) is again an instance of the orthogonal Procrustes problem. Hence, by the result in [18], an optimal solution to Problem (21) is given by\nP \u2217i = Hi\n[\nW Ti 0 0 Isi\u2212si\u22121\u2212hi\n]\n,\nwhere Yii = Hi\n[\n\u03a3i 0\n]\nW Ti is a singular value decomposition of Yii with Hi \u2208 O si\u2212si\u22121 , Wi \u2208 O hi ,\nand \u03a3i \u2208 S hi being diagonal. It follows from (21) that\nv\u2217i =\n\u2225 \u2225 \u2225 \u2225 Yii \u2212 P \u2217 i [ Ihi 0 ]\u2225 \u2225 \u2225 \u2225 2\nF\n= \u2016\u03a3i \u2212 Ihi\u2016 2 F .\nNow, since Y \u2208 St(m,n), we have\nY Tii Yii + \u2211\nj 6=i\nY Tji Yji = Wi\u03a3 2 iW T i +\n\u2211\nj 6=i\nY Tji Yji = Ihi ,\nor equivalently,\n\u03a32i +W T i\n\n\n\u2211\nj 6=i\nY Tji Yji\n\nWi = Ihi .\nBy following the arguments in the proof of Lemma 2, we conclude that\n\u2016\u03a3i \u2212 Ihi\u2016 2 F = \u0398\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj 6=i\nY Tji Yji\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nF\n\n ,\nas desired."}], "references": [{"title": "Steepest Descent Algorithms for Optimization under Unitary Matrix Constraint", "author": ["T.E. Abrudan", "J. Eriksson", "V. Koivunen"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Convergence of the Iterates of Descent Methods for Analytic Cost Functions", "author": ["P.-A. Absil", "R. Mahony", "B. Andrews"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Projection\u2013Like Retractions on Matrix Manifolds", "author": ["P.-A. Absil", "J. Malick"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Manifolds of Negative Curvature", "author": ["R.L. Bishop", "B. O\u2019Neill"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1969}, {"title": "Extrema of Sums of Heterogeneous Quadratic Forms", "author": ["M. Bolla", "G. Michaletzky", "G. Tusn\u00e1dy", "M. Ziermann"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A Framework of Constraint Preserving Update Schemes for Optimization on Stiefel Manifold. Accepted for publication", "author": ["B. Jiang", "Y.-H. Dai"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Generalized Power Method for Sparse Principal Component Analysis", "author": ["M. Journ\u00e9e", "Yu. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Empirical Arithmetic Averaging over the Compact Stiefel Manifold", "author": ["T. Kaneko", "S. Fiori", "T. Tanaka"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Trace Optimization and Eigenproblems in Dimension Reduction Methods", "author": ["E. Kokiopoulou", "J. Chen", "Y. Saad"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "New Fractional Error Bounds for Polynomial Systems with Applications to H\u00f6lderian Stability in Optimization and Spectral Theory of Tensors", "author": ["G. Li", "B.S. Mordukhovich", "T.S.  Pha.m"], "venue": "Accepted for publication in Mathematical Programming, Series A,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Error Bounds and Convergence Analysis of Feasible Descent Methods: A General Approach", "author": ["Z.-Q. Luo", "P. Tseng"], "venue": "Annals of Operations Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Optimization Algorithms Exploiting Unitary Constraints", "author": ["J.H. Manton"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Convergence to Equilibrium for Discretizations of Gradient", "author": ["B. Merlet", "T.N. Nguyen"], "venue": "Like Flows on Riemannian Manifolds. Differential and Integral Equations,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Yu. Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Numerical Methods for Large Eigenvalue Problems", "author": ["Y. Saad"], "venue": "Classics in Applied Mathematics. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania, revised edition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Convergence Results for Projected Line\u2013Search Methods on Varieties of Low\u2013Rank Matrices via Lojasiewicz Inequality", "author": ["R. Schneider", "A. Uschmajew"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A Generalized Solution of the Orthogonal", "author": ["P.H. Sch\u00f6nemann"], "venue": "Procrustes Problem. Psychometrika,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1966}, {"title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "author": ["O. Shamir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Optimization Techniques on Riemannian Manifolds", "author": ["S.T. Smith"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Moment Inequalities for Sums of Random Matrices and Their Applications in Optimization", "author": ["A.M.-C. So"], "venue": "Mathematical Programming, Series A,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Non\u2013Asymptotic Convergence Analysis of Inexact Gradient Methods for Machine Learning Without Strong Convexity", "author": ["A.M.-C. So"], "venue": "Preprint, available at http://www.se.cuhk.edu.hk/~manchoso/papers/inexact_GM_conv.pdf,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Approximation Accuracy, Gradient Methods, and Error Bound for Structured Convex Optimization", "author": ["P. Tseng"], "venue": "Mathematical Programming, Series B,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A Feasible Method for Optimization with Orthogonality Constraints", "author": ["Z. Wen", "W. Yin"], "venue": "Mathematical Programming, Series A,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Adaptive Canonical Correlation Analysis Based on Matrix Manifolds", "author": ["F. Yger", "M. Berar", "G. Gasso", "A. Rakotomamonjy"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "l1,p\u2013Norm Regularization: Error Bounds and Convergence Rate Analysis of First\u2013Order Methods", "author": ["Z. Zhou", "Q. Zhang", "A.M.-C. So"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 12, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 2, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 7, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 9, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 15, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 20, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 24, "context": ", [6, 13, 3, 8, 10, 16, 21, 25]).", "startOffset": 2, "endOffset": 31}, {"referenceID": 0, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 2, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 3, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 23, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 6, "context": ", [1, 3, 4, 24, 7].", "startOffset": 2, "endOffset": 18}, {"referenceID": 18, "context": "More recently, Shamir [19] developed a stochastic line-search method for Problem (1) when n = 1, B = In = 1, and A is negative semidefinite.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "On another front, Smith [20] showed that when used to optimize a smooth function over a Riemannian manifold, the method of steepest descent will converge linearly to a critical point if the function is strongly convex on the manifold.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "In particular, it is known that every smooth function that is convex on a compact Riemannian manifold (such as the Stiefel manifold) is constant [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 19, "context": "Therefore, one cannot hope to obtain linear convergence results for Problem (1) using the convexity-based approach in [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 1, "context": ", [2, 14, 17].", "startOffset": 2, "endOffset": 13}, {"referenceID": 13, "context": ", [2, 14, 17].", "startOffset": 2, "endOffset": 13}, {"referenceID": 16, "context": ", [2, 14, 17].", "startOffset": 2, "endOffset": 13}, {"referenceID": 10, "context": ", [11]) and can in principle be applied to Problem (1).", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "well-established analysis framework in the literature [17], we conclude that a host of line-search methods for solving Problem (1) converge linearly to a critical point.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "Thus, it is qualitatively different from those in [3, 19].", "startOffset": 50, "endOffset": 57}, {"referenceID": 18, "context": "Thus, it is qualitatively different from those in [3, 19].", "startOffset": 50, "endOffset": 57}, {"referenceID": 11, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 22, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 21, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 25, "context": "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.", "startOffset": 52, "endOffset": 68}, {"referenceID": 2, "context": "We refer the reader to [3, 9] for details of these retractions.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "We refer the reader to [3, 9] for details of these retractions.", "startOffset": 23, "endOffset": 29}, {"referenceID": 16, "context": "We refer the reader to [17] for details.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "4 Convergence Analysis Framework for the Line-Search Method To analyze the convergence properties of Algorithm 1, we adopt the framework introduced in [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": ", [15].", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "In the numerical experiments, we follow the technique introduced in [7] and use (XTX)\u22121 to control feasibility error.", "startOffset": 68, "endOffset": 71}, {"referenceID": 17, "context": "(15) Problem (15) is an instance of the orthogonal Procrustes problem, whose optimal solution is given by X\u2217 = UV T , where S1 = U\u03a3V T is the singular value decomposition of S1 [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 17, "context": "Hence, by the result in [18], an optimal solution to Problem (21) is given by P \u2217 i = Hi [ W T i 0 0 Isi\u2212si\u22121\u2212hi ]", "startOffset": 24, "endOffset": 28}], "year": 2015, "abstractText": "A fundamental class of matrix optimization problems that arise in many areas of science and engineering is that of quadratic optimization with orthogonality constraints. Such problems can be solved using line-search methods on the Stiefel manifold, which are known to converge globally under mild conditions. To determine the convergence rate of these methods, we give an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of the aforementioned class of problems. By combining such an estimate with known arguments, we are able to establish the linear convergence of a large class of line-search methods. A key step in our proof is to establish a local error bound for the set of critical points, which may be of independent interest.", "creator": "LaTeX with hyperref package"}}}