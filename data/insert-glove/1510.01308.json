{"id": "1510.01308", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2015", "title": "Tight Variational Bounds via Random Projections and I-Projections", "abstract": "kleinsmith Information projections electron-rich are chilka the key fbh building block siphoned of raouf variational inference papathemelis algorithms and venables are hpe used to approximate antv a target ruminating probabilistic banki model by projecting dudnik it bashiri onto a family of tractable siga distributions. In yosu general, yogurts there is pistolesi no guarantee jehona on chaparro the quilting quality shortens of heraclitus the thakore approximation obtained. To ogre overcome this issue, koff we demagogues introduce a alleanza new class of unconditionally random reimut projections kour to reduce kib the amoeba dimensionality 1.5700 and hence protuberances the artamonov complexity qsm of the anansie original marrabenta model. In the spirit of random babas projections, the projection surrounds preserves (mirecki with high population probability) key http://www.nyse.com properties ma\u1e29alleh-ye of the en-masse target distribution. warwickshire We show that villarreal information projections can be fotbollf\u00f6rbund combined santofimio with untargeted random kankaras projections to obtain provable obanikoro guarantees on meteoritics the quality inwardly of contagions the district approximation obtained, regardless raistlin of mathieson the complexity of the 10-0 original busova\u010da model. We brokeback demonstrate empirically qionglai that sumarlin augmenting mean stinger field with a bentvueghels random ilic projection step dramatically improves sebestyen partition unfulfilled function mary-anne and 61.23 marginal probability estimates, gr\u00fcnberg both weltanschauung on .0212 synthetic 1499 and berhalter real drafters world data.", "histories": [["v1", "Mon, 5 Oct 2015 19:53:22 GMT  (138kb,D)", "http://arxiv.org/abs/1510.01308v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lun-kai hsu", "tudor achim", "stefano ermon"], "accepted": false, "id": "1510.01308"}, "pdf": {"name": "1510.01308.pdf", "metadata": {"source": "CRF", "title": "Tight Variational Bounds via Random Projections and I-Projections", "authors": ["Lun-Kai Hsu", "Tudor Achim", "Stefano Ermon"], "emails": ["luffykai@cs.stanford.edu", "tachim@cs.stanford.edu", "ermon@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Probabilistic inference is a core problem in machine learning, physics, and statistics [1]. Probabilistic inference methods are needed for training, evaluating, and making predictions with probabilistic models [2]. Developing scalable and accurate inference techniques is the key computational bottleneck towards deploying large-scale statistical models, but exact inference is known to be computationally intractable. The root cause is the curse of dimensionality \u2013 the number of possible scenarios to consider grows exponentially in the number of variables, and in continuous domains, the volume grows exponentially in the number of dimensions [3]. Approximate techniques are therefore almost always used in practice [2].\nSampling-based techniques and variational approaches are the two main paradigms for approximate inference [4, 5]. Sampling-based approaches attempt to approximate intractable, high-dimensional distributions using a (small) collection of representative samples [6, 7, 8]. Unfortunately, it is usually no easier to obtain such samples than it is to solve the original probabilistic inference problem [9]. Variational approaches, on the other hand, approximate an intractable distribution using a family of tractable ones. Finding the best approximation, also known as computing an I-Projection onto the family, is the key ingredient in all variational inference algorithms. In general, there is no guarantee on the quality of the approximation obtained. Intuitively, if the target model is too complex with respect to the family used, then the approximation will be poor.\nTo overcome this issue, we introduce a new class of random projections [10, 11, 12]. These projections take as input a probabilistic model and randomly perturb it, reducing its degrees of freedom. The projections can be computed efficiently and they reduce the effective dimensionality and\nLKH and TA contributed equally to this paper.\nar X\niv :1\n51 0.\n01 30\n8v 1\n[ cs\n.L G\n] 5\nO ct\ncomplexity of the target model. Our key result is that the randomly projected model can then be approximated with I-projections onto simple families of distributions such as mean field with provable guarantees on the accuracy, regardless of the complexity of the original model. Crucially, in the spirit of random projections for dimensionality reduction, the random projections affect key properties of the target distribution (such as the partition function) in a highly predictable way. The I-projection of the projected model can therefore be used to accurately recover properties of the original model with high probability.\nWe demonstrate the effectiveness of our approach by using mean field augmented with random projections to estimate marginals and the log partition function on models of synthetic and realworld data, empirically showing large improvements on both tasks."}, {"heading": "2 Preliminaries", "text": "Let p(x) = 1Z \u220f \u03b1 \u03c8\u03b1({x}\u03b1) be a probability distribution over n binary variables x \u2208 {0, 1}n specified by an undirected graphical model 1 [1]. We further assume that p(x) is a member of an exponential family of distributions parameterized by \u03b8 \u2208 Rd and with sufficient statistics \u03c6(x) [5], i.e., p(x) = exp(\u03b8\u00b7\u03c6(x))Z . The constant Z = \u2211 x\u2208{0,1}n \u220f \u03b1\u2208I \u03c8\u03b1({x}\u03b1) is known as the partition function and ensures that the probability distribution is properly normalized. Computing the partition function is one of the key computational challenges in probabilistic reasoning and statistical machine learning, as it is needed to evaluate likelihoods and compare competing models of data. This computation is known to be intractable (#-P hard) in the worst-case, as the sum is defined over an exponentially large number of terms [13, 1]. We focus on variational approaches for approximating the partition function."}, {"heading": "2.1 Variational Inference and I-projections", "text": "The key idea of variational inference approaches is to approximate the intractable probability distribution p(x) with one that is more tractable. The approach is to define a family Q of tractable distributions and then look for a distribution in this family that minimizes a notion of divergence from p. Typically, the Kullback-Leibler divergence DKL(q||p) is used, which is defined as follows\nDKL(q||p) = \u2211 x q(x) log q(x) p(x) = \u2211 x q(x) log q(x)\u2212 \u03b8 \u00b7 \u2211 x q(x)\u03c6(x) + logZ (1)\nA distribution q\u2217 \u2208 Q that minimizes this divergence, q\u2217 = argminq\u2208QDKL(q||p), is called an information projection (I-projection) onto Q. Intuitively, q\u2217 is the \u201cclosest\u201d distribution to p among all the distributions inQ. Typically, one choosesQ to be a family of tractable distributions for which inference is tractable, i.e., such that (1) can be evaluated efficiently. The simplest choice, which removes all conditional dependencies, is to letQ be the set of fully factored probability distributions over X , namelyQMF = {q(x)|q(x) = \u220f i qi(xi)}. This is known as the mean field approximation. Even when Q is tractable, computing an I-projection, i.e., minimizing the KL divergence, is a nonconvex optimization problem which can be difficult to solve.\nSince the KL-divergence is non-negative, equation (1) shows that any distribution q \u2208 Q provides a lower bound on the value of the partition function\nlogZ \u2265 max q\u2208Q \u2212 \u2211 x q(x) log q(x) + \u03b8 \u00b7 \u2211 x q(x)\u03c6(x) (2)\nThe distribution q\u2217 that minimizesDKL(q||p) is also the distribution that provides the tightest lower bound on the partition function by maximizing the RHS of equation (2). The larger the set Q is, the better q\u2217 can approximate p and the tighter the bound becomes. If Q is so large that p \u2208 Q, then minq\u2208QDKL(q||p) = 0, because when q\u2217 = p, DKL(q\u2217||p) = 0. In general, however, there is no guarantee on the tightness of bound (2) even if the optimization can be solved exactly.\n1 We restrict ourselves to binary variables for the ease of exposition. Our approach applies more generally to discrete graphical models."}, {"heading": "2.2 Random Projections", "text": "We introduce a different class of random projections that we will use for probabilistic inference. Let P be the set of all probability distributions over {0, 1}n. We introduce a family of operators RmA,b : P \u2192 P , where m \u2208 [0, n], A \u2208 {0, 1}m\u00d7n, and b \u2208 {0, 1}m. RmA,b \u2208 R maps p(x) = 1 Z \u220f \u03b1 \u03c8\u03b1({x}\u03b1) to a new probability distribution RmA,b(p) restricted to {x : Ax = b mod 2} whose probability mass function is proportional to p. Formally,\nRmA,b(p)(x) = 1\nZ(A, b) \u220f \u03b1 \u03c8\u03b1({x}\u03b1)\nIn other words, for all x \u2208 {x|Ax = b mod 2} RmA,b(p) is proportional to the original p(x), and the new normalization constant is\nZ(A, b) = \u2211\nx|Ax=b mod 2 \u220f \u03b1 \u03c8\u03b1({x}\u03b1)\nThese operators are clearly idempotent and can thus be interpreted as projections.\nWhen the parameters A, b are chosen randomly, the operator RmA,b can be seen as a random projection. We consider random projections obtained by choosing A \u2208 {0, 1}m\u00d7n and b \u2208 {0, 1}m independently and uniformly at random, i.e., choosing each entry by sampling an independent unbiased Bernoulli random variable. This can be shown to implement a strongly universal hash function [10, 11]. Intuitively, the projection randomly subsamples the original space, selecting configurations x \u2208 {0, 1}n pairwise independently with probability 2\u2212m. It can be shown [12, 14] that\nE[Z(A, b)] = 2\u2212mZ\nwhere the expectation is over the random choices of A, b, and that V ar [Z(A, b)] = 1 2m ( 1\u2212 12m )\u2211 x ( \u220f \u03b1 \u03c8\u03b1({x}\u03b1))\n2. As we will formalize later, this random projection simplifies the model without losing too much information because it affects the partition function in a highly predictable way (knowing the expectation and the variance is sufficient to achieve high probability bounds).\nTo gain some intuition on the effect of the random projection, we can rewrite the linear system Ax = b mod 2 in reduced row-echelon form [12]. Assuming A is full-rank, we have that C = [Im|A\u2032] where Im is the m\u00d7m identity matrix. The system Ax = b is mathematically equivalent to Cx = b\u2032. For notational simplicity, we continue to use b instead of b\u2032. We can equivalently rewrite the constraints Ax = b mod 2 as the following set of constraints\nx1 = n\u2295 i=m+1 c1ixi \u2295 b1, \u00b7 \u00b7 \u00b7 , xm = n\u2295 i=m+1 cmixi \u2295 bm\nwhere \u2295 denotes the exclusive-or (XOR) operator. Thus, the random projection reduces the degrees of freedom of the model by m, as the first m variables are completely determined by the last n \u2212 m. For later development it will also be convenient to rewrite these linear equations modulo 2 as polynomial equations by changing variables from {0, 1} to {\u22121, 1}:\n(1\u2212 2x1) = n\u220f\ni=m+1\n(1\u2212 2C1ixi)(1\u2212 2b1), \u00b7 \u00b7 \u00b7 , (1\u2212 2xm) = n\u220f\ni=m+1\n(1\u2212 2Cmixi)(1\u2212 2bm)\n(3)"}, {"heading": "3 Combining Random Projections with I-Projections", "text": "Given an intractable target distribution p and a candidate set of (tractable) distributions Q, there are two main issues with variational approximation techniques: (i) p can be far from the approximating familyQ in the sense that even the optimal q\u2217 = argminq\u2208QDKL(q\u2016p) can have a large divergence DKL(q\n\u2217\u2016p) and therefore yield a poor lower bound in Eq. (2), and (ii) the variational problem in Eq. (2) is non-convex and thus difficult to solve exactly in high dimensions. Our key idea is to address\n(i) by using the random projections introduced in the previous section to \u201csimplify\u201d p, producing a projection RmA,b(p) that provably is closer to Q. Crucially, because of the statistical properties of the random projection used, (variational) inferences on the randomly projected model RmA,b(p) reveal useful information about the original distribution p. Randomization plays a key role in our approach, boosting the power of variational inference. In fact, it is known that #-P problems (e.g., computing the partition function) can be approximated in BPPNP, i.e. in bounded error probabilistic polyonomial time by an algorithm that has access to a NP-oracle [15, 16, 17]. Randomness appears to be crucial, and the ability to solve difficult (NP-equivalent) optimization problems, such as the one in in Eq. (2), does not appear to be sufficient. By leveraging randomization, we are able to boost variational inference approaches (such as mean field), obtaining formal approximation guarantees for general target probability distributions p. A pictorial representation is given in Figure 1."}, {"heading": "3.1 Provably Tight Variational Bounds on the Partition Function", "text": "Let D = {\u03b4x0|x0 \u2208 X} denote the set of degenerate probability distributions over {0, 1}n, i.e. probability distributions that place all the probability mass on a single configuration. There are 2n such probability distributions and the entropy of each is zero. Given any probability distribution p, its projection on D, i.e., argminq\u2208DDKL(q||p), is given by a distribution that places all the probability on argmaxx\u2208X log p(x). Thus computing the I-projection on D is equivalent to solving a Most Probable Explanation query [1] which is NP-equivalent in the worst-case.\nLetQ \u2287 D be a family of probability distributions that contains D. Our key result is that we can get provably tight bounds on the partition function Z by taking an I-projection onto Q after a suitable random projection. This is formalized by the following theorem:\nTheorem 1. Let Ai,t \u2208 {0, 1}i\u00d7n iid\u223c Bernoulli( 12 ) and b i,t \u2208 {0, 1}i iid\u223c Bernoulli( 12 ) for i \u2208 [0, n] and t \u2208 [1, T ]. Let Q be a family of distributions such that D \u2286 Q. Let\n\u03b3i,t = exp ( max q\u2208Q \u03b8 \u00b7 \u2211\nx:Ai,tx=bi,t\nq(x)\u03c6(x)\u2212 \u2211\nx:Ai,tx=bi,t\nq(x) log q(x) ) (4)\nbe the optimal solutions for the projected variational inference problems. Then for all i \u2208 [0, n] and for all T \u2265 1 the rescaled variational solution is a lower bound for Z in expectation\nE\n[ 1\nT T\u2211 t=1 \u03b3i,t2i\n] = E[\u03b3i,t2i] \u2264 Z\nThere also exists an m such that for any \u03b4 > 0 and positive constant \u03b1 \u2264 0.0042, if T \u2265 1 \u03b1 (log(n/\u03b4)) then with probability at least (1\u2212 2\u03b4)\n1\nT T\u2211 t=1 \u03b3m,t2m \u2265 Z 64(n+ 1)\n(5)\n4Z \u2265Median ( \u03b3m,1, \u00b7 \u00b7 \u00b7 , \u03b3m,T ) 2m \u2265 Z\n32(n+ 1) (6)\nProof. See Appendix.\nThis proves that appropriately rescaled variational lower bounds obtained on the randomly projected models (aggregated through median or mean) are within a factor n of the true value Z, where n is the number of variables in the model. This is an improvement on prior variational approximations which can either be unboundedly suboptimal or provide guarantees that hold only in expectation [18]; in contrast, our bounds are tight and require a relatively small number of samples proportional to log n/\u03b4. The proof, reported in the appendix for space reasons, relies on the following technical result which can be seen as a variational interpretation of Theorem 1 from [17] and is of independent interest: Theorem 2. For any \u03b4 > 0, and positive constant \u03b1 \u2264 0.0042, let T \u2265 1\u03b1 (log(n/\u03b4)). Let A\ni,t \u2208 {0, 1}i\u00d7n iid\u223c Bernoulli( 12 ) and b i,t \u2208 {0, 1}i iid\u223c Bernoulli( 12 ) for i \u2208 [0, n] and t \u2208 [1, T ]. Let\n\u03b4i,t = min q\u2208D DKL(q||RiAi,t,bi,t(p))\nThen with probability at least (1\u2212 \u03b4) n\u2211 i=0 exp ( Median ( \u2212\u03b4i,1 + logZ(Ai,1, bi,1), \u00b7 \u00b7 \u00b7 ,\u2212\u03b4i,T + logZ(Ai,T , bi,T ) )) 2i\u22121 (7)\nis a 32-approximation to Z.\nThe proof can be found in the appendix. Intuitively, Theorem 2 states that one can always find a small number of degenerate distributions (which can be equivalently thought of as special states that can be discovered through random projections and KL-divergence minimization) that are with high probability representative of the original probabilistic model, regardless of how complex the model is. Theorem 1 extends this idea to more general families of distributions such as Mean Field."}, {"heading": "3.2 Solving Randomly Projected Variational Inference Problems", "text": "To apply the results from Theorem 1 we must choose a tractable approximating family D \u2286 Q for the I-projection part and integrate our random projections into the optimization procedure. We focus on mean field (Q = QMF ) as our approximating family, but the results can be easily extended to structured mean field [19]. For simplicity of exposition we consider only probabilistic models p with unary and binary factors (e.g. Ising models, restricted Boltzmann machines). That is, p(x) = exp(\u03b8 \u00b7 \u03c6(x))/Z, where \u03c6(x) are single node and pairwise edge indicator variables. Recall that our projectionRmA,b(p) constrains the distribution p to {x|Ax = b mod 2}. The projected variational optimization problem (4) is therefore\nlogZ(A, b) \u2265 max q\n\u03b8 \u00b7 \u2211\nx|Ax=b mod 2\nq(x)\u03c6(x)\u2212 \u2211\nx|Ax=b mod 2\nq(x) log q(x)\nOr, equivalently,\nlogZ(A, b) \u2265 max \u00b5\n\u03b8 \u00b7 \u00b5+ n\u2211\ni=m+1\nH(\u00b5i) (8)\nwhere \u00b5 is the vector of singleton and pairwise marginals of q(x) and H(\u00b5i) is the entropy of a Bernoulli random variable with parameter \u00b5i. To solve this optimization problem efficiently we need a clever way to take into account the parity constraints, for running traditional mean field with message passing as in [18] would fail in the normalization step because of the presence of hard parity constraints. The key idea is to consider the equivalent row-reduced representation of the constraints from (3) and define\nq(x1, \u00b7 \u00b7 \u00b7 , xn) = n\u220f\ni=m+1\nqi(xi) m\u220f k=1 1\n{ (1\u2212 2xk) =\nn\u220f i=m+1 (1\u2212 2Ckixi)(1\u2212 2bk)\n}\nwhere we have a set of independent \u201cfree variables\u201d (wlog., the last n\u2212m) and a set of \u201cconstrained variables\u201d (the first m) that are always set as to satisfy the parity constraints. Since the variables x1, . . . , xm are fully determined by xm+1, . . . , xn, we see that the marginals \u00b51, . . . , \u00b5m are also determined by \u00b5m+1, . . . , \u00b5n, as shown by the following proposition: Proposition 1. The singleton and pairwise marginals in (8) can be computed as follows:\nSingleton marginals: for k \u2208 [m+ 1, n], \u00b5k = Eq [xk] = qk(1). For k \u2208 [1,m],\n\u00b5k = ( 1\u2212 (1\u2212 2bk)\nn\u220f i=m+1 (1\u2212 2Cki\u00b5i)\n) /2\nPairwise marginals: for k, ` \u2208 [m+ 1, n], \u00b5kl = Eq[xkx`] = \u00b5k\u00b5`. For k \u2208 [m+ 1, n], ` \u2208 [1,m]\n\u00b5kl =\n{ \u00b5k 1 2 (1 + (1\u2212 2bl) \u220fn i 6=k,i=m+1(1\u2212 2Cli\u00b5i)) if Clk = 1\n\u00b5k\u00b5l otherwise\nFor k \u2208 [1,m], ` \u2208 [1,m]\n\u00b5kl = 1\n4 (1 + (1\u2212 2bk)(1\u2212 2bl) n\u220f i=m+1 (1\u2212 \u00b5i(2Cki + 2Cli \u2212 4CkiCli))\n\u2212 (1\u2212 2bk) n\u220f\ni=m+1\n(1\u2212 2Cki\u00b5i)\u2212 (1\u2212 2bl) n\u220f\ni=m+1\n(1\u2212 2Cli\u00b5i))\nThe derivation is found in the appendix. We can therefore maximize the lower bound in (8) by optimizing only over the \u201cfree marginals\u201d \u00b5m+1, . . . , \u00b5n, as the remaining one are completely determined per Proposition 1. Compared to a traditional mean field variational approximation, we have a problem with a smaller number of variables, but with additional non-convex constraints."}, {"heading": "4 Algorithm: Mean Field with Random Projections", "text": "Theorem 1 guarantees that the approximation to Z has a tight lower bound only if we are able to find globally optimal solutions for (8). However, nontrivial variational inference problems (2) are nonconvex in general even without any random projections and even whenQ is simple, e.g.,Q = QMF . We do not explicitly handle this nonconvexity, but nevertheless we show empirically that we can vastly improve on mean field lower bounds. The key insight for our optimization procedure is that the objective function is still coordinate-wise concave, like in a traditional mean-field approximation: Proposition 2. The objective function \u03b8 \u00b7 \u00b5+ \u2211n i=m+1H(\u00b5i) in (8) is concave with respect to any particular free marginal \u00b5m+1, . . . , \u00b5n.\nProof. By inspection, all the marginals in Proposition 1 are linear with respect to any specific free marginal \u00b5m+1, . . . , \u00b5n. Since the entropy term is concave, the RHS in (8) is concave in each free marginal \u00b5m+1, . . . , \u00b5n.\nSince (8) is concave in each variable we devise a coordinate-wise ascent algorithm, called Mean Field with Random Projections (MFRP), for maximizing the lower bound in (8) over the free marginals defined in Proposition 1. Starting from a random initialization, we iterate over each free marginal \u00b5k and maximize (8) with the rest of the free marginals fixed by setting the gradient with respect to \u00b5k equal to 0 and solving for \u00b5k. The closed form expressions we use are reported in the appendix. Because the overall optimization problem is not concave the algorithm may converge at a local maximum; therefore, we use J random initializations and use the best lower bound found across the J runs of the ascent algorithm. For a given m, we repeat this procedure T times and return the median across the runs. Each coordinate ascent step for free marginal \u00b5i takes O(m+ n+ |Ecc|(n\u2212m)) steps in expectation where Ecc is the number of variables co-occurring in a parity constraint. Recomputing the constrained marginals takes O(m(n\u2212m)) steps. The algorithm returns the maximum of MFRP(p(x),m) over m \u2208 [0, n]. If MFRP finds a global optimum, then Theorem 1 guarantees it is a tight lower bound for logZ with high probability. Since MFRP uses coordinate-wise ascent we cannot certify global optimality; however, our experiments show large improvements in the lower bound when compared to existing variational methods.\nAlgorithm 1 MFRP(p(x) \u221d \u220f \u03b1 \u03c8\u03b1({x}\u03b1),m)\nfor t = 1, \u00b7 \u00b7 \u00b7 , T do . Do T random projections Generate parity bits b(t) iid\u223c Bernoulli( 12 ) m . Generate random projection Rm A(t),b(t)\nGenerate matrix A(t) iid\u223c Bernoulli( 12 ) m\u00d7n Row reduce A,b and permute to yield C = [I|A\u2032] and b . Compute constraints Z\u0303(t) \u2190 0 for j = 1, \u00b7 \u00b7 \u00b7 , J do . Try different initializations\nInitialize \u00b5(j,t) iid\u223c Unif(0, 1)n for l = 1, \u00b7 \u00b7 \u00b7 ,m do . Compute constrained marginals\n\u00b5(j,t) \u2190 ( 1\u2212 \u220fn i=m+1(1\u2212 2Cli\u00b5 (j,t) i )(1\u2212 2bl) ) /2\nend for while not converged do . Stop when the increment is small or timeout\nfor k = m+ 1, \u00b7 \u00b7 \u00b7 , n do . Coordinate ascent over free marginals \u00b5 (j,t) k \u2190 argmax\u00b5k \u03b8 \u00b7 \u00b5(j,t) + \u2211n i=m+1H(\u00b5 (j,t) i )\nfor l = 1, \u00b7 \u00b7 \u00b7 ,m do . Update constrained marginals \u00b5 (j,t) l \u2190 ( 1\u2212 \u220fn i=m+1(1\u2212 2Cli\u00b5 (j,t) i )(1\u2212 2bl) ) /2\nend for end for\nend while end for Z\u0303(t) \u2190 maxj exp(\u03b8 \u00b7 \u00b5(j,t) + \u2211n i=m+1H(\u00b5 (j,t) i )) . Pick best over initializations\nend for Return 2mMedian ( Z\u0303(1), . . . , Z\u0303(T ) ) . Aggregate across projections"}, {"heading": "5 Experiments", "text": "We investigate MFRP\u2019s empirical performance on Ising models and on Restricted Boltzmann Machines. In particular, we are interested in the log partition function estimates and in the quality of the marginal estimates. Where applicable, exact ground truth estimates are obtained with the libDAI implementation of Junction Tree [20]. Upper bounds are calculated with Tree-Reweighted Belief Propagation (TRW-BP) [21], also implemented in libDAI. All methods are compared to mean field (MF) optimized with coordinate-wise ascent and random restarts."}, {"heading": "5.1 Ising Models", "text": "We consider n\u00d7n binary grid Ising models with variables xi \u2208 {\u22121, 1} and potentials \u03c8ij(xi, xj) = exp(wijxixj + fixi + fjxj). In particular, we look at mixed models where the wij\u2019s are drawn uniformly from [\u221210, 10] and the fi\u2019s uniformly from [\u22121, 1]. Figure 2 compares the log partition function estimates from MF, Junction Tree, MFRP, and TRWBP. For each grid size, we generated five different grids and computed the mean field estimate for each as a baseline lower bound. For each of the five grids we also computed the best MFRP lower bound over m \u2208 [0, 20] with T = 5 trials each. For comparison we include the exact log partition calculation from Junction Tree up to n = 20 and the TRW-BP upper bounds for all n. We plot the mean and standard error bars of the log ratio of each estimate over mean field for each method over the five grids. Note that for large grid sizes, the lower bound provided by MFRP is hundreds of orders of magnitude better than than those found by mean field.\nFinally, we consider the empirical runtime of the method for varying grid sizes n and number of constraints m in Figure 3. As expected, the runtime for mean field grows linearly in the number of variables in the graph (quadratically with n) and there is a linear slowdown as constraints are added to the optimization."}, {"heading": "5.2 Restricted Boltzmann Machines", "text": "We train Restricted Boltzmann Machines (RBMs) [22] using Contrastive Divergence (CD) [23, 24] on the MNIST hand-written digits dataset. In an RBM there is a layer of nh hidden binary variables h = h1, \u00b7 \u00b7 \u00b7 , hnh and a layer of nv binary visible units v = v1, \u00b7 \u00b7 \u00b7 , vnv . The joint probability distribution is given by P (h, v) = 1Z exp(b\n\u2032v + c\u2032h + h\u2032Wv). We use nh \u2208 {100, 200} hidden units and nv = 28 \u00d7 28 = 784 visible units. We learn the parameters b, c,W using CD-k for k \u2208 {1, 5, 15}, where k denotes the number of Gibbs sampling steps used in the inference phase, with 15 training epochs and minibatches of size 20.\nWe then use MF and MFRP to estimate the log partition function and also consider the aggregate marginals of the sub-problems. For most of the cases we see a clear improvement in both the log partition lower bounds and in the marginals, with the marginal for No. Hidden Nodes = 100, k = 15 similar visually to an average over all digits in the MNIST dataset."}, {"heading": "6 Conclusions", "text": "We introduced a new, general approach to variational inference that combines random projections with I-projections to obtain provably tight lower bounds for the log partition function. Our approach is the first to leverage universal hash functions and their properties in a variational sense. We demonstrated the effectiveness of this idea by extending mean field with random projections and empirically showed a large improvement in the partition function lower bounds and marginals obtained on both synthetic and real world data. Natural extensions to the approach include applications to other\nvariational methods, like the Bethe approximation, and the use of more powerful global optimization techniques instead of the coordinate-wise ascent currently used."}, {"heading": "A Appendix : Proofs", "text": "Proof of Theorem 2.\nmin q\u2208D\nDKL(q||RiAi,t,bi,t(p)) =\nmin q\u2208D \u2211 x|Ai,tx=bi,t mod 2 q(x) log q(x)\u2212 \u03b8 \u00b7 \u2211 x|Ai,tx=bi,t mod 2 q(x)\u03c6(x) + logZ(Ai,t, bi,t) =\nmin q\u2208D \u2212\u03b8 \u00b7 \u2211 x|Ai,tx=bi,t mod 2 q(x)\u03c6(x) + logZ(Ai,t, bi,t) =\n\u2212 max x|Ai,tx=bi,t mod 2 \u03b8\u03c6(x) + logZ(Ai,t, bi,t)\nTherefore \u2212min\nq\u2208D DKL(q||RiAi,t,bi,t(p)) + logZ(A i,t, bi,t) = max x|Ai,tx=bi,t mod 2 \u03b8\u03c6(x)\nSubstituting into Eq. (7) we can rewrite as n\u2211\ni=0\nexp ( Median ( max\nx|Ai,1x=bi,1 mod 2 \u03b8\u03c6(x), \u00b7 \u00b7 \u00b7 , max x|Ai,T x=bi,T mod 2 \u03b8\u03c6(x)\n)) 2i\u22121 =\nn\u2211 i=0 Median ( exp ( max x|Ai,1x=bi,1 mod 2 \u03b8\u03c6(x) ) , \u00b7 \u00b7 \u00b7 , exp ( max x|Ai,T x=bi,T mod 2 \u03b8\u03c6(x) )) 2i\u22121 =\nn\u2211 i=0 Median ( max x|Ai,1x=bi,1 mod 2 exp(\u03b8\u03c6(x)), \u00b7 \u00b7 \u00b7 , max x|Ai,T x=bi,T mod 2 exp(\u03b8\u03c6(x)) ) 2i\u22121\nThe result then follows directly from Theorem 1 from [17].\nProof of Theorem 1. For the first part, we have the following relationship on the expectation:\nE[Z(Ai,t, bi,t)] = 2\u2212iZ\nFrom the non-negativity of the KL divergence we have that for any q \u2208 Q logZ(Ai,t, bi,t) \u2265 \u03b8 \u00b7 \u2211\nx:Ai,tx=bi,t\nq(x)\u03c6(x)\u2212 \u2211\nx:Ai,tx=bi,t\nq(x) log q(x)\nlogZ(Ai,t, bi,t) \u2265 max q\u2208Q\n{ \u03b8 \u00b7\n\u2211 x:Ai,tx=bi,t q(x)\u03c6(x)\u2212 \u2211 x:Ai,tx=bi,t q(x) log q(x)\n}\nZ(Ai,t, bi,t) \u2265 exp ( max q\u2208Q { \u03b8 \u00b7 \u2211 x:Ai,tx=bi,t q(x)\u03c6(x)\u2212 \u2211 x:Ai,tx=bi,t q(x) log q(x) }) , \u03b3i,t (9)\nWe take the expectation of both sides to yield\nZ 2i = E\n[ Z(Ai,t, b) ] \u2265 E [ exp ( max q\u2208Q { \u03b8 \u00b7 \u2211 x:Ai,tx=b q(x)\u03c6(x)\u2212 \u2211 x:Ai,tx=b q(x) log q(x) })] = E[\u03b3i,t]\nFor the second part. Since the conditions of Theorem 2 are satisfied, we know that equation (7) holds with probability at least 1 \u2212 \u03b4. From (7), since the terms in the sum are non-negative we have that the maximum element is at least 1/(n+ 1) of the sum:\nmax i exp\n( Median ( \u2212min\nq\u2208D DKL(q||RiAi,1,bi,1(p)) + logZ(A i,1, bi,1), \u00b7 \u00b7 \u00b7 ,\n\u2212min q\u2208D DKL(q||RiAi,T ,bi,T (p)) + logZ(A i,T , bi,T )\n)) 2i\u22121 \u2265 1\n32 Z\n1\nn+ 1\nTherefore there exists m such that\nMedian ( \u2212min\nq\u2208D DKL(q||RmAm,1,bm,1(p)) + logZ(A m,1, bm,1), \u00b7 \u00b7 \u00b7 ,\n\u2212min q\u2208D DKL(q||RmAm,T ,bm,T (p)) + logZ(A m,T , bm,T )\n) + (m\u2212 1) log 2 \u2265 \u2212 log 32 + logZ \u2212 log(n+ 1)\nWe also have min q\u2208Q DKL(q||RA,b(p)) \u2264 min q\u2208D DKL(q||RA,b(p))\nbecause D \u2286 Q. Thus\nMedian ( \u2212min\nq\u2208Q DKL(q||RmAm,1,bm,1(p)) + logZ(A m,1, bm,1), \u00b7 \u00b7 \u00b7 ,\n\u2212min q\u2208Q DKL(q||RmAm,T ,bm,T (p)) + logZ(A m,T , bm,T )\n) + (m\u2212 1) log 2 \u2265 \u2212 log 32 + logZ \u2212 log(n+ 1)\nFrom the definition of DKL, we have DKL(q||RmA,b(p)) = \u2212 \u2211\nx:Ax=b mod 2\nq(x)\u03c6(x) + \u2211\nx:Ax=b mod 2\nq(x) log q(x) + logZ(A, b)\nPluggin in we get Median ( log \u03b3m,1, \u00b7 \u00b7 \u00b7 , log \u03b3m,T ) + (m\u2212 1) log 2 \u2265 \u2212 log 32\u2212 log(n+ 1) + logZ\nand also Median ( log \u03b3m,1, \u00b7 \u00b7 \u00b7 , log \u03b3m,T ) +m log 2 \u2265 \u2212 log 32\u2212 log(n+ 1) + logZ\nwith probability at least 1\u2212 \u03b4.\nMedian ( \u03b3m,1, \u00b7 \u00b7 \u00b7 , \u03b3m,T ) 2m \u2265 Z\n32(n+ 1)\nSince the terms are non zero,\n1\nT T\u2211 t=1 \u03b3m,t \u2265 1 2 Median ( \u03b3m,1, \u00b7 \u00b7 \u00b7 , \u03b3m,T ) therefore with probability at least 1\u2212 \u03b4\n1\nT T\u2211 t=1 \u03b3m,t2m \u2265 Z 64(n+ 1)\nFrom Markov\u2019s inequality\nP [ Z(Ai,t, bi,t) \u2265 cE[Z(Ai,t, bi,t)] ] \u2264 1 c\nP [ Z(Ai,t, bi,t)2i \u2265 cZ ] \u2264 1 c\nTherefore since Z(Ai,t, bi,t) \u2265 \u03b3i,t from (9), setting c = 4 and i = m we get\nP [ \u03b3m,t2m \u2265 4Z ] \u2264 1\n4\nFrom Chernoff\u2019s inequality, P [ 4Z \u2265Median ( \u03b3m,1, \u00b7 \u00b7 \u00b7 , \u03b3m,T ) 2m ] \u2265 1\u2212 \u03b4\nand the claim follows from union bound.\nProof of Proposition 1. For singleton marginals, when k \u2208 [m + 1, n], xk is a free variable and thus \u00b5k = Eq[xk] = qk(1). When k \u2208 [1,m],\n(1\u2212 2xk) = (1\u2212 2bk) n\u220f\ni=m+1\n(1\u2212 2Ckixi)\nTake the expectation on both side and since xi for i \u2208 [m+ 1, n] are free (independent) variables, we have\n(1\u2212 2\u00b5k) = (1\u2212 2bk) n\u220f\ni=m+1\n(1\u2212 2Cki\u00b5i)\nThat is,\n\u00b5k = ( 1\u2212 (1\u2212 2bk)\nn\u220f i=m+1 (1\u2212 2Cki\u00b5i)\n) /2\nFor the binary marginal \u00b5kl, there are three cases: both xk, xl are free variables; one is free and the other is constrained; both are constrained. For the first case, k, ` \u2208 [m+ 1, n], they are independent and thus\n\u00b5kl = Eq[xkx`] = \u00b5k\u00b5`\nFor the second case, k \u2208 [m+ 1, n], ` \u2208 [1,m].\n\u00b5kl = Pr[xk = 1, xl = 1] = \u2211\nxm+1, ..., xn, xk = 1 \u22121 = (1\u2212 2xl) = (1\u2212 2bl) \u220fn i=m+1(1\u2212 2Clixi)\nn\u220f i=m+1 qi(xi)\nWhen Clk = 1, 1\u2212 2Clkxk = \u22121,\n\u00b5kl = \u00b5k \u00b7 \u2211\nxm+1, ...xk\u22121, xk+1, ..., xn, 1 = (1\u2212 2bl) \u220fn i=m+1,i 6=k(1\u2212 2Clixi)\nn\u220f i=m+1,i 6=k qi(xi)\nLet\u2019s introduce a new binary variable, u, satisfying the constraint\n(2u\u2212 1) = (1\u2212 2bl) n\u220f\ni6=k,i=m+1\n(1\u2212 2Clixi)\nThen the above summation is over xm+1, ...xk\u22121, xk+1, ..., xn, u such that u = 1. The probability of u being 1 is the expected value of u. Therefore,\n\u00b5kl = \u00b5kE[u] = \u00b5k 1\n2 (1 + (1\u2212 2bl) n\u220f i 6=k,i=m+1 (1\u2212 2Cli\u00b5i))\nWhen Clk = 0, xl is independent of xk, so \u00b5kl = \u00b5k\u00b5l For the last case, k, ` \u2208 [1,m].\n(1\u2212 2xk)(1\u2212 2x`) = (1\u2212 2bk)(1\u2212 2b`) n\u220f\ni=m+1\n(1\u2212 2Ckixi) n\u220f\ni=m+1\n(1\u2212 2C`ixi)\nTaking the expected value of both side\n1\u2212 2\u00b5l \u2212 2\u00b5k + 4\u00b5kl = (1\u2212 2bl)(1\u2212 2bk) n\u220f\ni=m+1\nE[1\u2212 xi(2Cki + 2Cli) + 4CkiClix2i ]\n\u00b5kl = 1\n4 (\u22121 + 2\u00b5k + 2\u00b5l + (1\u2212 2bk)(1\u2212 2bl) n\u220f i=m+1 (1\u2212 \u00b5i(2Cki + 2Cli \u2212 4CkiCli)))\nPlug in the result of \u00b5k, \u00b5l:\n\u00b5kl = 1\n4 (1 + (1\u2212 2bk)(1\u2212 2bl) n\u220f i=m+1 (1\u2212 \u00b5i(2Cki + 2Cli \u2212 4CkiCli))\n\u2212 (1\u2212 2bk) n\u220f\ni=m+1\n(1\u2212 2Cki\u00b5i)\u2212 (1\u2212 2bl) n\u220f\ni=m+1\n(1\u2212 2Cli\u00b5i))\nProposition 3 (The gradients for coordinate ascent). Assuming we are taking the gradient with respect to \u00b5k, where k \u2208 [m+ 1, n]. 1. Unary term \u00b5k\n\u2202\u00b5k \u2202\u00b5k = 1\nAnd thus \u2202H(\u00b5k)\n\u2202\u00b5k =\n\u2202\n\u2202\u00b5k \u2212 (\u00b5k log(\u00b5k) + (1\u2212 \u00b5k) log(1\u2212 \u00b5k)) = log( 1\u2212 \u00b5k \u00b5k )\n2. Unary term \u00b5l, l \u2265 m+ 1, l 6= k \u2202\u00b5l \u2202\u00b5k = 0\n\u2202H(\u00b5l)\n\u2202\u00b5k = 0\n3. Unary term \u00b5l, l \u2264 m \u2202\u00b5l \u2202\u00b5k = \u2202 \u2202\u00b5k 1 2 (1\u2212 (1\u2212 2bl) n\u220f i=m+1 (1\u2212 2Cli\u00b5i)) = (1\u2212 2bl)Clk n\u220f i=m+1,i6=k (1\u2212 2Cli\u00b5i)\n4. Binary term, \u00b5kl, l \u2265 m+ 1 \u2202\n\u2202\u00b5k \u00b5kl = \u00b5l\n5. Binary term, \u00b5pl, both p, l \u2265 m+ 1, p 6= k, l 6= k \u2202\n\u2202\u00b5k \u00b5pl = 0"}, {"heading": "6. Binary term, \u00b5kl, l \u2264 m", "text": "When Clk = 0, \u00b5kl = \u00b5k\u00b5l and its derivative is\n\u00b5l + \u00b5k\u00b5 \u2032 l = \u00b5l =\n1 2 (1\u2212 (1\u2212 2bl) n\u220f i=m+1,i 6=k (1\u2212 2Cli\u00b5i))\nWhen Clk = 1, \u00b5kl = \u00b5k 12 (1 + (1\u2212 2bl) \u220fn i 6=k,i=m+1(1\u2212 2Cli\u00b5i)). The derivative is\n1 2 (1 + (1\u2212 2bl) n\u220f i 6=k,i=m+1 (1\u2212 2Cli\u00b5i))"}, {"heading": "7. Binary term, \u00b5pl, where p \u2265 m+ 1, p 6= k, l \u2264 m", "text": "When Clp = 0, \u00b5pl = \u00b5p\u00b5l and its derivative is\n\u00b5p\u00b5 \u2032 l When Clp = 1, \u00b5pl = \u00b5p 12 (1 + (1\u2212 2bl) \u220fn i 6=p,i=m+1(1\u2212 2Cli\u00b5i)). The derivative is\n\u2212\u00b5pCkl(1\u2212 2bl) n\u220f\ni6=k,i6=p,i=m+1\n(1\u2212 2Cli\u00b5i))\n8 Binary term \u00b5pl, where both p, l \u2264 m \u2202\n\u2202\u00b5k \u00b5pl =\n\u2202\n\u2202\u00b5k\n1 4 (1\u2212 (1\u2212 2bp) n\u220f i=m+1 (1\u2212 2Cpi\u00b5i)\n\u2212 (1\u2212 2bl) n\u220f\ni=m+1\n(1\u2212 2Cli\u00b5i)\n+ (1\u2212 2bp)(1\u2212 2bl) n\u220f\ni=m+1\n(1\u2212 (2Cpi + 2Cli \u2212 4cpiCli)\u00b5i))\n\u2202\n\u2202\u00b5k \u00b5pl = (1\u2212 2bp)Cpk 2 n\u220f i=m+1,i 6=k (1\u2212 2Cpi\u00b5i) (= \u00b5\u2032p/2)\n+ (1\u2212 2bl)Clk\n2\nn\u220f i=m+1,i 6=k (1\u2212 2Cpi\u00b5i) (= \u00b5\u2032l/2)\n\u2212 (1\u2212 2bp)(1\u2212 2bl)(2Cpk + 2Clk \u2212 4CpkClk) 4 n\u220f i=m+1,i 6=k (1\u2212 \u00b5i(2Cpi + 2Cli \u2212 4CpiCli))\nAll gradients except the entropy one are independent of \u00b5k, so whole gradient can be expressed as\nc+ log( 1\u2212 \u00b5k \u00b5k ),\nwhere c is a constant with respect to \u00b5k. Therefore, the coordinate ascent step for \u00b5k is to set it to 1\n1 + exp(\u2212c)"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>Information projections are the key building block of variational inference algo-<lb>rithms and are used to approximate a target probabilistic model by projecting it<lb>onto a family of tractable distributions. In general, there is no guarantee on the<lb>quality of the approximation obtained. To overcome this issue, we introduce a<lb>new class of random projections to reduce the dimensionality and hence the com-<lb>plexity of the original model. In the spirit of random projections, the projection<lb>preserves (with high probability) key properties of the target distribution. We show<lb>that information projections can be combined with random projections to obtain<lb>provable guarantees on the quality of the approximation obtained, regardless of<lb>the complexity of the original model. We demonstrate empirically that augmenting<lb>mean field with a random projection step dramatically improves partition function<lb>and marginal probability estimates, both on synthetic and real world data.", "creator": "LaTeX with hyperref package"}}}