{"id": "1510.01443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "A Waveform Representation Framework for High-quality Statistical Parametric Speech Synthesis", "abstract": "32.19 State - jozsef of - the - art wedgwood statistical parametric cadix speech shochiku synthesis (6-for-11 SPSS) co-cathedral generally uses a vocoder to superstitions represent yachiyo speech vp2 signals putins and parameterize limited-time them tiaret into features konami for pallmeyer subsequent modeling. anti-heroes Magnitude nyk\u00e4nen spectrum has been transcriptionist a nakanishi dominant feature over the 1979-1984 years. Although pretzels perceptual studies 726,000 have hukam shown that ambessa phase gorgodze spectrum is 903 essential 700-acre to the formula quality ktwv of apatite synthesized speech, mangles it socreds is lethargic often ignored ghilarducci by using a minimum solvability phase sendas filter perrysville during recalcitrant synthesis and the susan speech royd quality tyrolian suffers. montreaux To bypass torx this bottleneck in universum vocoded hourihan speech, lidge this paper proposes estripeau a orchiectomy phase - serrao embedded meteoric waveform representation framework and 2,152 establishes kont a magnitude - damodar phase joint tapit modeling flintheart platform all-midwest for abm high - quality munawar SPSS. Our brounstein experiments foretz on waveform fangshan reconstruction katagiri show shatto that the bahnbetriebswerk performance sugisaki is better 83.23 than bollard that bidaya of broadmeadows the dealogic widely - 6-7 used 44-23 STRAIGHT. rychnov Furthermore, the r\u00e6der proposed modeling and synthesis platform outperforms a leading - and-5 edge, vocoded, reprimands deep bidirectional raphaelites long rnr short - mexia term memory recurrent neural sasseville network (64.42 DBLSTM - RNN) - hetauda based sonakshi baseline system in russian-born various objective tshering evaluation labienus metrics conducted.", "histories": [["v1", "Tue, 6 Oct 2015 06:12:31 GMT  (553kb,D)", "http://arxiv.org/abs/1510.01443v1", "accepted and will appear in APSIPA2015; keywords: speech synthesis, LSTM-RNN, vocoder, phase, waveform, modeling"]], "COMMENTS": "accepted and will appear in APSIPA2015; keywords: speech synthesis, LSTM-RNN, vocoder, phase, waveform, modeling", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["bo fan", "siu wa lee", "xiaohai tian", "lei xie", "minghui dong"], "accepted": false, "id": "1510.01443"}, "pdf": {"name": "1510.01443.pdf", "metadata": {"source": "CRF", "title": "A Waveform Representation Framework for High-quality Statistical Parametric Speech Synthesis", "authors": ["Bo Fan", "Siu Wa Lee", "Xiaohai Tian", "Lei Xie", "Minghui Dong"], "emails": ["bofan@nwpu-aslp.org,", "lxie@nwpu-aslp.org,", "swylee@i2r.a-star.edu.sg,", "mhdong@i2r.a-star.edu.sg,", "xhtian@ntu.edu.sg"], "sections": [{"heading": null, "text": "A Waveform Representation Framework for High-quality Statistical Parametric Speech Synthesis\nBo Fan\u2217, Siu Wa Lee\u2020, Xiaohai Tian\u2021\u00a7, Lei Xie\u2217 and Minghui Dong\u2020 \u2217 School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China \u2020 Human Language Technology Department, Institute for Infocomm Research, Singapore \u2021 School of Computer Engineering, Nanyang Technological University (NTU), Singapore\n\u00a7 Joint NTU-UBC Research Center of Excellence in Active Living for the Elderly, NTU, Singapore Email: {bofan,lxie}@nwpu-aslp.org, {swylee,mhdong}@i2r.a-star.edu.sg, xhtian@ntu.edu.sg\nAbstract\u2014State-of-the-art statistical parametric speech synthesis (SPSS) generally uses a vocoder to represent speech signals and parameterize them into features for subsequent modeling. Magnitude spectrum has been a dominant feature over the years. Although perceptual studies have shown that phase spectrum is essential to the quality of synthesized speech, it is often ignored by using a minimum phase filter during synthesis and the speech quality suffers. To bypass this bottleneck in vocoded speech, this paper proposes a phase-embedded waveform representation framework and establishes a magnitude-phase joint modeling platform for high-quality SPSS. Our experiments on waveform reconstruction show that the performance is better than that of the widely-used STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms a leading-edge, vocoded, deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based baseline system in various objective evaluation metrics conducted.\nI. INTRODUCTION\nStatistical parametric speech synthesis (SPSS) has been increasingly popular due to its compact and flexible representation of voice characteristics [1]. Conventionally, in an SPSS system, we firstly extract parametric representations of speech including spectral and excitation parameters from a speech database and then model them with a set of models [2]. Several statistical generative models have been applied to SPSS successfully, e.g., hidden Markov model (HMM)-based SPSS [2], deep neural network (DNN)-based SPSS [3] and deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based SPSS [4].\nTo parameterize speech signals into features for subsequent synthesis processes, vocoder has been typically used. It is based on the source-filter model [5], which assumes a stationary speech segment is generated by passing a sound source through a vocal tract filter. By using a vocoder, the resultant speech features are regular and suitable for modeling. However, in [6], their subjective listening test shows clear degradation of quality in vocoded speech. It further indicates that the source and filter parameters have to be jointly modelled for high-quality synthesis. Besides, to assure interframe coherence [7], a minimum phase hypothesis [7] has been used in most vocoders, which ignores the natural mixedphase characteristics of speech signals, resulting in apparent degradation of the speech waveform quality.\nMore and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14]. Paliwal et al. [15] have investigated the relative importance of short-time magnitude and phase spectra on speech perception through human perception listening test. Results show that phase spectrum clearly contributes to the speech intelligibility. Sometimes its contribution is as much as the magnitude spectrum. Koutsogiannaki et al. [16] have proposed the phase distortion deviation feature, enabling to capture voice irregularities and highlights the importance of the phase spectrum in voice quality assessment. These two works indicate that phase information is important for both human perception and voice quality assessment. Combining phase spectrum with magnitude spectrum in frequency domain is equivalent to the speech waveform in time-domain. Therefore, the phase information is focused in our speech waveform representation framework.\nThere are some approaches of waveform representation directly in the time domain. Time domain pitch-synchronous overlap-add (TD-PSOLA) [17] performs pitch-synchronous analysis, modification and synthesis. During synthesis, speech frames are summed up. The quality of the reconstructed waveform with typical pitch or timing modification is similar to that of the original waveform. Multi-band re-synthesis pitch synchronous overlap add (MBR-PSOLA) [18] comments TD-PSOLA with three mismatches: phase mismatch, pitch mismatch, spectral envelope mismatch. It further suggests to solve these mismatches by re-synthesizing voiced parts of the speech database with constant phase and constant pitch. The artificial processing in MBR-PSOLA decreases the quality of speech and leads to buzzy sound [19]. Alternatively, there are a few recent works for SPSS directly in the time domain. Tokuda et al. [20] have proposed an approach to model cepstral coefficients to approximate the speech waveform. In their framework, periodic, voiced components have not been properly generated yet. In [21], complex cepstrum has been used to embed phase information for hidden semi-Markov models (HSMM) speech modelling.\nIn this paper, we propose a phase-embedded waveform representation framework, and establish a magnitude-phase\nar X\niv :1\n51 0.\n01 44\n3v 1\n[ cs\n.S D\n] 6\nO ct\n2 01\n5\njoint modeling platform for SPSS. This work uses glottalsynchronous overlap add approach for speech analysis and synthesis where glottal closure instants (GCIs) are employed. GCIs refer to the moments of most significant excitation that occur at the level of the vocal folds during each glottal period [22]. Short-term segments are defined as any two consecutive GCI periods. In order to produce smooth trajectories of our features which are required in SPSS, we design a cost function with a global smoothness constraint. The GCI locations selected are finally determined by conducting dynamic programming over a list of probable GCI candidates. Consequently, these segments will be very regular with stable magnitude and matched phase spectrum. With this waveform representation framework, the bottleneck suffered from vocoded speech is thus bypassed. This framework is hence capable of delivering better quality speech over the vocoded speech. Then we propose an approach for magnitude-phase joint spectrum modeling. Full spectrum is used in this framework, which is in line with the satisfactory performance in recent deep learningbased TTS [23]. To leverage on the modeling power of deep learning, we use DBLSTM-RNN to learn magnitude and phase spectrum simultaneously. Bidirectional recurrent connections can fully exploit the speech contextual information in both forward and backward directions. With purpose-built memory cells to store information, the long short-term memory (LSTM) architecture does better in finding and taking advantage of the long range context."}, {"heading": "II. TD-PSOLA", "text": "Time domain pitch-synchronous overlap add (TD-PSOLA) is used for pitch and timing modification of speech signals [17], [24]. It is also popular for concatenation-based TTS. As no source-filter decomposition or vocoding is performed, the quality of resultant speech after analysis and reconstruction is highly similar to the original speech.\nGiven an arbitrary speech waveform signal x(n), TDPSOLA is carried out in the time domain. It first decomposes x(n) into a sequence of overlapping, pitch-synchronized segments. Each segment xs(n) lasts for two pitch periods, running from a pitch period before and another pitch period after the segment centre. Then a window function hs(n), such as hanning window, will be applied to each segment. Assuming S denotes the total number of the segments, where s = 1, 2, ..., S,\nxs(n) = hs(n)x(n) (1)\nhs(n) is non-zero during the above two-pitch period. This is how xs(n) is extracted for voiced speech; for unvoiced speech, the segment length is set to a constant. Any modification in pitch or timing can then be performed on these extracted segments. Finally, modified segments are overlapped and added to produce the speech output [24].\nAlthough TD-PSOLA generates pitch- and timing-modified output signals with satisfactory speech quality, using TDPSOLA in speech synthesis where statistical averaging, modeling or signal modification are common, is not sufficient. This\nis because matched attributes on phase and pitch are needed [18]."}, {"heading": "III. WAVEFORM REPRESENTATION FRAMEWORK", "text": "In this work, a glottal-synchronous based waveform representation framework is proposed for speech modelling. Similar to TD-PSOLA, glottal closure instants (GCIs) represent both the pitch contours and the boundaries of individual cycles of speech. Existing GCI detection approaches generally estimate the GCI locations in a local manner, ignoring the resultant trajectories of various acoustic attributes, i.e. segment length (representing fundamental frequency (F0)), magnitude and phase spectrum, exhibited in the utterance. As smooth trajectories of these attributes are necessary for SPSS, we revise a state-of-the-art GCI detection approach, so as to facilitate satisfactory modelling of these attributes."}, {"heading": "A. System Overview", "text": "The proposed framework, as shown in Fig. 1, consists of two parts: analysis and synthesis. In the analysis stage, given an arbitrary waveform, firstly, the GCI locations are detected by the following revised GCI detection module. Then, the waveform is decomposed into overlapping short-term segments. Each segment is defined by any two consecutive GCI periods. Finally, segment lengths, magnitude and phase spectrum are used to represent these segments.\nIn the synthesis stage, given corresponding segment lengths, magnitude and phase spectrum, we convert them into overlapping short-term segments. Then, the waveform is reconstructed using the similar technique as TD-PSOLA [17]."}, {"heading": "B. Glottal Closure Instant Detection", "text": "The GCI positions determine the features including segment lengths, magnitude and phase spectrum. Thus, the GCI detection method is of great importance.\nAmong the present GCIs detection techniques, the Speech Event Detection using the Residual Excitation And a Meanbased Signal (SEDREAMS) algorithm [25] is widely used. In\n[26], SEDREAMS was shown to have the highest robustness and reliability. During the detection, SEDREAMS outputs only one GCI location for each GCI segment [25]. This is a local estimation process, without considering the GCI detection results in the neighborhood. However, SPSS requires smooth trajectories of speech features, which are defined once GCI locations are determined. By considering lists of probable GCI candidates and estimating the optimal GCI locations in a global manner, the trajectories of these features are stabilized.\nBased on SEDREAMS, our modified GCI detection method contains the following steps:\na) Given a waveform x(n) (Fig. 2(a)), calculate the moving average signal (Fig. 2(b)). b) Determine the intervals for possible GCI locations1 (Fig. 2(c)). c) M candidates are chosen, based on the top M highest linear predictive coding (LPC) residual values in the LPC residual signal (Fig. 2(d)), as the possible GCI locations in each interval. Suppose there are N intervals, the k-th candidate of i-th interval denoted as gi,k. d) Transfer all the possible segment lengths into F0. For the i-th segment, the j-th F0 is expressed as\nF0i,j = Fs/(g(i+1),s \u2212 gi,t), (2)\nwhere Fs is the sampling frequency, i = 1, 2, ..., N , j = 1, 2, ...,M \u00d7M , s = 1, 2, ...,M and t = 1, 2, ...,M . e) Given the reference F0ref, the optimal segment lengths are determined by dynamic programming with the following constraint,\nE = argmin j \u2211N i=1 \u2225\u2225F0ref \u2212 F0i,j\u2225\u2225 . (3) f) Finally, the GCI locations are deduced accordingly (Fig.\n2(e)). In our implementation, M is five and the reference F0 is extracted by STRAIGHT [27]. STRAIGHT is robust for F0 tracking and can generate a highly accurate and smooth F0 trajectory. The F0 trajectory extracted from STRAIGHT is robust The dynamic programming process is implemented by the Viterbi algorithm. In a voiced segment, the pitch located in the middle is more stable compared to the rest. Consequently, Viterbi search starts at this middle position to both ends.\nA comparison of F0 trajectory between our GCI detection and SEDREAMS is depicted in Fig. 3. From Fig. 3(a), it is observed that the F0 given by our GCI detection is smoother than the one from SEDREAMS. And from Fig. 3(b), it is clear that our GCI detection approach removes some abnormal jumps (around the 247-th frame) of the F0 trajectory occurred in the SEDREAMS."}, {"heading": "IV. WAVEFORM MODELING", "text": "State-of-the-art SPSS usually models the magnitude spectrum of speech signals and discards the phase spectrum.\n1For the detailed implementations of the moving average filter and interval determination, please refer to [25]\nDuring synthesis, a vocoder based on minimum-phase or zerophase filter is often used together with the generated magnitude spectra to produce the synthesized output. Nevertheless, phase spectrum has been recently found to be essential for speech perception. The speech quality of vocoded outputs are found to be degraded from the original speech recordings [6]. This may shed light on SPSS, where speech waveform with phase information in addition to the existing magnitude spectrum, is modeled.\nIn our work, speech signals are modeled by the corresponding magnitude and phase spectra, without the use of a vocoder. Consequently, reconstruction of speech waveform is facilitated. We use a recently-emerging learning technique, DBLSTM-RNN, to jointly model the two spectra. DBLSTMRNN is well-suited for learning sequential events apart from long time lags of unknown size [28]. Promising performance in various speech applications is observed [29], [4].\nOur joint model of magnitude and phase is constructed\nas follows. We employ line spectrum pair (LSP) as the feature representation of magnitude spectrum. LSP, being an alternative LPC spectral representation, is robust and suitable for interpolation and modeling [30], [31].\nFor phase spectrum, we propose to use the dynamic phase spectrum for this waveform learning TTS framework. It is also called group delay: the group delay \u03c4k(n) at time n and frequency bin k is calculated as the frequency derivative of the instantaneous phase \u03b8k(n), i.e.\n\u03c4k(n) = \u03b8k(n)\u2212 \u03b8k\u22121(n). (4)\nTo enable reconstruction of the phase spectrum after DBLSTM-RNN modeling, the instantaneous phase at the first frequency bin is kept, together with the group delays of the remaining frequency bins. In other words, our phase representation consists of \u03b81(n), \u03c42(n), \u03c43(n), ..., \u03c4K(n), where K is the total number of frequency bins.\nThis group-delay-based phase representation is found to be stable and facilitates statistical modeling in subsequent TTS process, as shown in Fig. 4. Comparing the spectra of static phase and dynamic phase, the distribution of the dynamic phase often exhibits a smaller range. Comparing the log magnitude spectrum with the dynamic phase spectrum, patterns of voiced and unvoiced portions are consistent and spectral patterns of individual speech sounds are quite similar in the log magnitude spectrum and the dynamic phase spectrum. This is important and useful for our joint modeling. On the contrary, there is no clear difference in the static phase spectrum for individual speech sounds. When moving along the time-axis, the static phase spectra look like the same."}, {"heading": "V. EXPERIMENTS", "text": "We conducted two experiments to assess the efficacy of our waveform representation framework. In the experiment on waveform reconstruction, objective and subjective evaluations were carried out to compare the performance between our framework and other three vocoders: STRAIGHT, TandemSTRAIGHT [32] and AHOCoder [33] respectively. As we know, STRAIGHT is a very popular vocoder used for speech analysis and reconstruction, and Tandem-STRAIGHT is the upgrade version of STRAIGHT. AHOCoder is reported to\nbe of similar quality compared with STRAIGHT. In the experiment on waveform modeling, we trained a text-to-speech (TTS) system based on our framework and also a baseline TTS system [4] as a comparison. This baseline is a leading-edge approach based on DBLSTM-RNN and generates high-quality synthesized speech. It uses STRAIGHT as its vocoder.\nA corpus with 4,936 Chinese utterances (around 6 hours) spoken by a native male speaker in a neutral style was used in our experiments. Speech waveform signals are sampled at 16kHz. The contextual labels are both phonetically and prosodically rich, including quin-phone, prosody, tone and syllable information. For TTS systems, the training, validation and test data consist of 3,949, 494 and 493 utterances, respectively."}, {"heading": "A. Experiment on Waveform Reconstruction", "text": "Speech waveform in the test set of the corpus was analyzed and re-synthesized using our waveform representation framework and the three vocoders. The reconstructed speech waveform was then used for objective and subjective evaluations.\n1) Objective Evaluation: In the objective evaluation, we calculated the root mean square error (RMSE) between the reconstructed and original speech waveform signals in the voiced parts (RMSE voiced), the unvoiced parts (RMSE unvoiced) and the entire waveform (RMSE), respectively. The results are shown in Table I. These voiced/unvoiced results from our framework and the three vocoders generally represent the performance on vowels/consonants respectively.\nThe objective evaluation result shows that the performance of our framework is much better than that of the three vocoders\nespecially in the voiced parts. The short-term segments are extracted at a constant rate in the unvoiced parts from our framework which is similar to STRAIGHT. Taking the waveform in Fig. 5 around the 5000-th sample as an example, the absolute difference between (a) and (b) is very close to that between (a) and (c). In the voiced parts, our framework performs much better than STRAIGHT does. It is because our framework retains the full phase spectrum, while STRAIGHT discards it and uses a minimum-phase setting instead. We can see clearly from Fig. 5 that the absolute difference between (a) and (b) is much smaller than that between (a) and (c) in the voiced parts.\n2) Subjective Evaluation: 20 pairs of speech waveform are randomly selected from the reconstructed waveforms. Then a group of 20 subjects were asked to perform the ABX preference test. We put the original waveform into X, while we put the waveform reconstructed using our framework and each of the three vocoders into A and B randomly. Each subject was asked to answer which one(A or B) is more similar to X. The third option Neutral means the subject has no preference on A or B. The ABX result is shown in Fig. 6. We can clearly see that the reconstructed speech waveform using our framework is significantly preferred as compared with all of the three vocoders."}, {"heading": "B. Experiment on Waveform Modeling", "text": "In the baseline DBLSTM-RNN-based TTS [4], STRAIGHT is used to vocode the speech waveform by a 25-ms moving window, and shifted every 5-ms. The generated magnitude spectrum from STRAIGHT was converted into LSP. The dimensionality of the input contextual label is 427. The output feature contains voiced/unvoiced flag (1 dimension), log F0 (1 dimension), LSP (40 dimensions) and gain (1 dimension), totally 43 dimensions. As suggested in [4], a neural network with two BLSTM layers sitting on two feed forward layers with 256\nnodes in each layer is employed to train the DBLSTM-RNNbased TTS.\nFor our TTS system, features were extracted from the shortterm segments specified by GCI locations. The format of the input label is the same as the baseline. The segment length is transformed into F0. The output feature comprises several components: voice/unvoiced flag (1 dimension), log F0 (1 dimension), LSP (40 dimensions), gain (1 dimension) and dynamic phase feature (257 dimensions), totally 300 dimensions. The same network topology as baseline is used to train our TTS system.\nTo evaluate the performance of these two TTS systems, five metrics are used for objective evaluation:\n\u2022 RMSE F0: root mean square error in F0 estimation; \u2022 Voiced/unvoiced (V/U) error rate; \u2022 Log spectral distance (LSD):\nLSD(Sp,Sg) =\u221a\u221a\u221a\u221a 1 N N\u2211 j=1 ( Ms\u2211 k=1 [10log10 sp(j, k)\u2212 10log10 sg(j, k)]2),\n(5)\nwhere Sp and Sg are the predicted and ground-truth magnitude spectrum, respectively. N is the total number of frames in the voiced parts and Ms refers to the dimensionality of magnitude spectrum. sp(j, k) is the the k-th value of magnitude in j-th frame;\n\u2022 Mel cepstral distance (MCD):\nMCD(cp, cg) = 10\nln 10 \u221a\u221a\u221a\u221a2 Mc\u2211 k=1 [cp(k)\u2212 cg(k)]2, (6)\nwhere cp and cg are the predicted and ground-truth Mel cepstrum coefficient vectors, respectively, and Mc refers to the dimensionality of Mel cepstrum coefficients;\n\u2022 Dynamic phase distance (DPD):\nDPD(dp,dg) = \u221a\u221a\u221a\u221aMd\u2211 k=1 [dp(k)\u2212 dg(k)]2, (7)\nwhere dp and dg are the predicted and ground-truth dynamic phase feature vectors, respectively, and Md refers to the dimensionality of the dynamic phase feature.\nThe synthesized speech waveform from the labels in the test set uses the ground-truth durations. These five metrics are calculated at the GCIs level, i.e., the short-term segments are specified by the GCIs locations. In order to make the systems comparable, GCI detection is required for all speech waveforms synthesized from any system under comparison. And after the GCI detection, it should be aligned to the ground-truth GCIs by finding out the closest one.\nThe objective evaluation result is shown in Table II. It shows that our TTS system is better than the baseline in terms of all the five metrics. In particular, for DPD, the average absolute difference in one frequency bin is about 0.70rad in our TTS system while 0.91rad for the baseline TTS system."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "This paper proposed a glottal-synchronous based waveform representation framework for high-quality statistical parametric speech synthesis. Speech signal was represented by magnitude and phase full-spectral components, without the the use of a vocoder. We revised the SEDREAMS GCI detection approach to improve the feature stability for statistical modelling. Both objective and subjective evaluations were conducted to assess the reconstruction performance of our framework. Results indicate that, comparing to the reconstructed signal obtained by three popular vocoders, the proposed framework achieves promising results in RMSE in time domain speech waveform and preference score.\nWe also proposed a platform for speech modelling. DBLSTM-RNN is applied to jointly model the corresponding magnitude and phase spectra, and group delay-based phase representation is used to facilitate statistical modelling. Objective results show that, the TTS system based on the proposed framework generates the features, specifically the phase feature, with lower distortion as compared with a vocoder based\nsystem. Further works include studying the speech quality of synthesized speech and the associated factors and experiments on subjective evaluation."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the National Natural Science Foundation of China (61175018 and 61571363)."}], "references": [{"title": "Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis", "author": ["T. Yoshimura", "K. Tokuda", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proc. Eurospeech, 1999, pp. 2347\u20132350.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 7962\u20137966.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F. Soong"], "venue": "Proc. Interspeech, 2014, pp. 1964\u20131968.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic Theory of Speech Production", "author": ["G. Fant"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1960}, {"title": "Investigating source and filter contributions, and their interaction, to statistical parametric speech synthesis", "author": ["T. Merritt", "T. Raitio", "S. King"], "venue": "Proc. Interspeech, 2014, pp. 1509\u20131513.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "On the use of a sinusoidal model for speech synthesis in textto-speech", "author": ["M. Crespo", "P. Velasco", "L. Serrano", "J. Sardina"], "venue": "Progress in Speech Synthesis, pp. 57\u201370. Springer, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "The effect of group delay spectrum on timbre", "author": ["H. Banno", "K. Takeda", "F. Itakura"], "venue": "Acoustical Science and Technology, vol. 23, no. 1, pp. 1\u20139, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Phase minimization for glottal model estimation", "author": ["G. Degottex", "A. Roebel", "X. Rodet"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1080\u20131090, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative reconstruction of speech from short-time Fourier transform phase and magnitude spectra", "author": ["L. Alsteris", "K. Paliwal"], "venue": "Computer Speech & Language, vol. 21, no. 1, pp. 174\u2013186, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Using phase spectrum information for improved speech recognition performance", "author": ["R. Schluter", "H. Ney"], "venue": "Proc. ICASSP. IEEE, 2001, vol. 1, pp. 133\u2013136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "On the importance of phase in human speech recognition", "author": ["G. Shi", "M. Shanechi", "P. Aarabi"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 14, no. 5, pp. 1867\u20131874, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1867}, {"title": "Squared error as a measure of perceived phase distortion", "author": ["H. Pobloth", "W. Kleijn"], "venue": "The Journal of the Acoustical Society of America, vol. 114, no. 2, pp. 1081\u2013 1094, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Dominance spectrum based v/uv classification and F0 estimation", "author": ["T. Nakatani", "T. Irino", "P. Zolfaghari"], "venue": "Proc. Eurospeech, 2003, pp. 2313\u20132316.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Usefulness of phase spectrum in human speech perception", "author": ["K. Paliwal", "L. Alsteris"], "venue": "Proc. Eurospeech, 2003, pp. 2117\u20132120.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "The importance of phase on voice quality assessment", "author": ["M. Koutsogiannaki", "O. Simantiraki", "G. Degottex", "Y. Stylianou"], "venue": "Proc. Interspeech, 2014, pp. 1653\u20131657.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones", "author": ["E. Moulines", "F. Charpentier"], "venue": "Speech communication, vol. 9, no. 5, pp. 453\u2013467, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "MBR-PSOLA: Text-to-speech synthesis based on an MBE re-synthesis of the segments database", "author": ["T. Dutoit", "H. Leich"], "venue": "Speech Communication, vol. 13, no. 3, pp. 435\u2013440, 1993.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Removing linear phase mismatches in concatenative speech synthesis", "author": ["Y. Stylianou"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 3, pp. 232\u2013 239, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis", "author": ["K. Tokuda", "H. Zen"], "venue": "Proc. ICASSP, 2015, pp. 4215\u2013 4219.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Complex cepstrum as phase information in statistical parametric speech synthesis", "author": ["R. Maia", "M. Akamine", "M. Gales"], "venue": "Proc. ICASSP. IEEE, 2012, pp. 4581\u2013 4584.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Determination of instants of significant excitation in speech using group delay function", "author": ["R. Smits", "B. Yegnanarayana"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 3, no. 5, pp. 325\u2013333, 1995.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis", "author": ["Z. Ling", "L. Deng", "D. Yu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Text-to-Speech Synthesis, United Kingdom", "author": ["P. Taylor"], "venue": "University of Cambridge,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Glottal closure and opening instant detection from speech signals", "author": ["T. Drugman", "T. Dutoit"], "venue": "Proc. Interspeech, 2009, pp. 2891\u20132894.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Detection of glottal closure instants from speech signals: a quantitative review", "author": ["T. Drugman", "M. Thomas", "J. Gudnason", "P. Naylor", "T. Dutoit"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 3, pp. 994\u20131006, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Restructuring speech representations using a pitch adaptive time-frequency smoothing and an instantaneousfrequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A. De Cheveigne"], "venue": "Speech communication, vol. 27, no. 3, pp. 187\u2013207, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Supervised Sequence Labelling with Recurrent", "author": ["A. Graves"], "venue": "Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Line spectrum pair (LSP) and speech data compression", "author": ["F.K. Soong", "B.-H. Juang"], "venue": "Proc. ICASSP. IEEE, 1984, pp. 37\u201340.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1984}, {"title": "Line spectrum representation of linear predictor coefficients of speech signals", "author": ["F. Itakura"], "venue": "The Journal of the Acoustical Society of America, vol. 57, pp. S35, 1975.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1975}, {"title": "TANDEM-STRAIGHT: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, F0, and aperiodicity estimation", "author": ["H. Kawahara", "M. Morise", "T. Takahashi", "R. Nisimura", "T. Irino", "H. Banno"], "venue": "Proc. ICASSP, 2008, pp. 3933\u20133936.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Harmonics plus noise model based vocoder for statistical parametric speech synthesis", "author": ["D. Erro", "I. Sainz", "E. Navas", "I. Hernaez"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 8, no. 2, pp. 184\u2013194, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Statistical parametric speech synthesis (SPSS) has been increasingly popular due to its compact and flexible representation of voice characteristics [1].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "Conventionally, in an SPSS system, we firstly extract parametric representations of speech including spectral and excitation parameters from a speech database and then model them with a set of models [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 1, "context": ", hidden Markov model (HMM)-based SPSS [2], deep neural network (DNN)-based SPSS [3] and deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based SPSS [4].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": ", hidden Markov model (HMM)-based SPSS [2], deep neural network (DNN)-based SPSS [3] and deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based SPSS [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": ", hidden Markov model (HMM)-based SPSS [2], deep neural network (DNN)-based SPSS [3] and deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based SPSS [4].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "It is based on the source-filter model [5], which assumes a stationary speech segment is generated by passing a sound source through a vocal tract filter.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "However, in [6], their subjective listening test shows clear degradation of quality in vocoded speech.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Besides, to assure interframe coherence [7], a minimum phase hypothesis [7] has been used in most vocoders, which ignores the natural mixedphase characteristics of speech signals, resulting in apparent degradation of the speech waveform quality.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "Besides, to assure interframe coherence [7], a minimum phase hypothesis [7] has been used in most vocoders, which ignores the natural mixedphase characteristics of speech signals, resulting in apparent degradation of the speech waveform quality.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 140, "endOffset": 146}, {"referenceID": 8, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 140, "endOffset": 146}, {"referenceID": 9, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 215, "endOffset": 223}, {"referenceID": 11, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 215, "endOffset": 223}, {"referenceID": 12, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 239, "endOffset": 243}, {"referenceID": 13, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 265, "endOffset": 269}, {"referenceID": 14, "context": "[15] have investigated the relative importance of short-time magnitude and phase spectra on speech perception through human perception listening test.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] have proposed the phase distortion deviation feature, enabling to capture voice irregularities and highlights the importance of the phase spectrum in voice quality assessment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Time domain pitch-synchronous overlap-add (TD-PSOLA) [17] performs pitch-synchronous analysis, modification and synthesis.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "Multi-band re-synthesis pitch synchronous overlap add (MBR-PSOLA) [18] comments TD-PSOLA with three mismatches: phase mismatch, pitch mismatch, spectral envelope mismatch.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "The artificial processing in MBR-PSOLA decreases the quality of speech and leads to buzzy sound [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "[20] have proposed an approach to model cepstral coefficients to approximate the speech waveform.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21], complex cepstrum has been used to embed phase information for hidden semi-Markov models (HSMM) speech modelling.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GCIs refer to the moments of most significant excitation that occur at the level of the vocal folds during each glottal period [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "Full spectrum is used in this framework, which is in line with the satisfactory performance in recent deep learningbased TTS [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Time domain pitch-synchronous overlap add (TD-PSOLA) is used for pitch and timing modification of speech signals [17], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "Time domain pitch-synchronous overlap add (TD-PSOLA) is used for pitch and timing modification of speech signals [17], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "Finally, modified segments are overlapped and added to produce the speech output [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "is because matched attributes on phase and pitch are needed [18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Then, the waveform is reconstructed using the similar technique as TD-PSOLA [17].", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Among the present GCIs detection techniques, the Speech Event Detection using the Residual Excitation And a Meanbased Signal (SEDREAMS) algorithm [25] is widely used.", "startOffset": 146, "endOffset": 150}, {"referenceID": 25, "context": "[26], SEDREAMS was shown to have the highest robustness and reliability.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "During the detection, SEDREAMS outputs only one GCI location for each GCI segment [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "In our implementation, M is five and the reference F0 is extracted by STRAIGHT [27].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "1For the detailed implementations of the moving average filter and interval determination, please refer to [25] 3200 3400 3600 3800 4000 4200 4400 4600 4800 -1 0 1 (a)", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "The speech quality of vocoded outputs are found to be degraded from the original speech recordings [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 27, "context": "DBLSTMRNN is well-suited for learning sequential events apart from long time lags of unknown size [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "Promising performance in various speech applications is observed [29], [4].", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "Promising performance in various speech applications is observed [29], [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 29, "context": "LSP, being an alternative LPC spectral representation, is robust and suitable for interpolation and modeling [30], [31].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "LSP, being an alternative LPC spectral representation, is robust and suitable for interpolation and modeling [30], [31].", "startOffset": 115, "endOffset": 119}, {"referenceID": 31, "context": "In the experiment on waveform reconstruction, objective and subjective evaluations were carried out to compare the performance between our framework and other three vocoders: STRAIGHT, TandemSTRAIGHT [32] and AHOCoder [33] respectively.", "startOffset": 200, "endOffset": 204}, {"referenceID": 32, "context": "In the experiment on waveform reconstruction, objective and subjective evaluations were carried out to compare the performance between our framework and other three vocoders: STRAIGHT, TandemSTRAIGHT [32] and AHOCoder [33] respectively.", "startOffset": 218, "endOffset": 222}, {"referenceID": 3, "context": "In the experiment on waveform modeling, we trained a text-to-speech (TTS) system based on our framework and also a baseline TTS system [4] as a comparison.", "startOffset": 135, "endOffset": 138}, {"referenceID": 26, "context": "031 STRAIGHT [27] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "152 Tandem-STRAIGHT [32] 0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "156 AHOCoder [33] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "In the baseline DBLSTM-RNN-based TTS [4], STRAIGHT is used to vocode the speech waveform by a 25-ms moving window, and shifted every 5-ms.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "As suggested in [4], a neural network with two BLSTM layers sitting on two feed forward layers with 256 37.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Measures Methods Our TTS system Baseline [4]", "startOffset": 41, "endOffset": 44}], "year": 2015, "abstractText": "State-of-the-art statistical parametric speech synthesis (SPSS) generally uses a vocoder to represent speech signals and parameterize them into features for subsequent modeling. Magnitude spectrum has been a dominant feature over the years. Although perceptual studies have shown that phase spectrum is essential to the quality of synthesized speech, it is often ignored by using a minimum phase filter during synthesis and the speech quality suffers. To bypass this bottleneck in vocoded speech, this paper proposes a phase-embedded waveform representation framework and establishes a magnitude-phase joint modeling platform for high-quality SPSS. Our experiments on waveform reconstruction show that the performance is better than that of the widely-used STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms a leading-edge, vocoded, deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based baseline system in various objective evaluation metrics conducted.", "creator": "LaTeX with hyperref package"}}}