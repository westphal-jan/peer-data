{"id": "1510.01570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "Analyzer and generator for Pali", "abstract": "This goseong work describes witherow a system that performs portuguese-based morphological coletta analysis kfs and riverton generation julii of faqiryar Pali dherynia words. The emka system baruch works with regular inflectional paradigms spellemannprisen and a 56.30 lexical ifv database. ph. The generator arterburn is used to build respective a hanger collection 141,300 of miklosko inflected wharington and derived filippini words, which in turn is saivite used eckart by the analyzer. Generating existentialist and 5.00 storing fethi morphological neelix forms 138.5 along 511,000 with 8-4 the corresponding dorner morphological information 44.14 allows pohjola for efficient leonidovich and lyndeborough simple ryman look hilltops up by barbarina the palapa analyzer. Indeed, chasin by defries looking ramsbury up a word and beguin extracting linkin the attached szilard morphological milborne information, the analyzer madlib does not have to compute goodner this information. particularity As oom we must, however, cra assume the lexical srimad database vuelta to florida-based be incomplete, imperviousness the system can shaziman also emap work bck without the 5-by-7 dictionary component, using ummc a assumed rule - rachel based approach.", "histories": [["v1", "Tue, 6 Oct 2015 13:33:46 GMT  (1185kb)", "http://arxiv.org/abs/1510.01570v1", "Bachelor Thesis"]], "COMMENTS": "Bachelor Thesis", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david alfter"], "accepted": false, "id": "1510.01570"}, "pdf": {"name": "1510.01570.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Reinhard K\u00f6hler"], "emails": ["s2daalft@uni-trier.de"], "sections": [{"heading": null, "text": "This work describes a system that performs morphological analysis and generation of\nPali words. The system works with regular inflectional paradigms and a lexical database. The generator is used to build a collection of inflected and derived words, which in turn is used by the analyzer. Generating and storing morphological forms along with the corresponding morphological information allows for efficient and simple look up by the analyzer. Indeed, by looking up a word and extracting the attached morphological information, the analyzer does not have to compute this information. As we must, however, assume the lexical database to be incomplete, the system can also work without the dictionary component, using a rule-based approach.\nTable of Contents\n1. Introduction ..................................................................................................................................... 1\n1.1. What is Pali ? ............................................................................................................................ 1\n1.2. Sanskrit .................................................................................................................................... 1\n1.3. Motivation................................................................................................................................ 2\n2. Challenges........................................................................................................................................ 3\n3. Lexical Database............................................................................................................................... 4\n4. Generator ........................................................................................................................................ 4\n4.1. Rationale .................................................................................................................................. 4\n4.2. Approach .................................................................................................................................. 5\n4.3. Problems .................................................................................................................................. 5\n5. Analyzer ........................................................................................................................................... 6\n5.1. Dictionary-based approach ....................................................................................................... 6\n5.2. Rule-based approach ................................................................................................................ 7\n6. Lemmatizer ...................................................................................................................................... 7\n7. Sandhi Splitter .................................................................................................................................. 8\n7.1. Sandhi ...................................................................................................................................... 8\n7.2. Preliminary notes ..................................................................................................................... 9\n7.2.1. Capturing groups and back-references .............................................................................. 9\n7.2.2. Constants ........................................................................................................................ 10\n7.2.3. Operations ...................................................................................................................... 10\n7.3. Approach ................................................................................................................................ 11\n7.3.1. Reversing rules ............................................................................................................... 11\n7.3.2. Splitting compounds ....................................................................................................... 14\n7.3.3. Merging compounds ....................................................................................................... 15\n8. Stemmer ........................................................................................................................................ 16\n9. Technical documentation ............................................................................................................... 16\n9.4. Data format and conventions ................................................................................................. 19\n9.4.1. Input ............................................................................................................................... 19\n9.4.2. Output ............................................................................................................................ 21\n9.5. Paradigm ................................................................................................................................ 22\n9.6. Generator ............................................................................................................................... 22\n9.6.1. Word class guesser ......................................................................................................... 22\n9.6.2. Strategies ........................................................................................................................ 23\n9.7. Analyzer ................................................................................................................................. 23\n9.8. Lemmatizer ............................................................................................................................ 24\n9.9. Sandhi splitter ........................................................................................................................ 24\n9.10. Sandhi merger .................................................................................................................... 24\n9.11. Stemmer ............................................................................................................................. 25\n9.12. API ...................................................................................................................................... 25\n10. Future work................................................................................................................................ 25\n11. Conclusion.................................................................................................................................. 26\n13. Appendix .................................................................................................................................... 28\n13.1. Rule reversal ....................................................................................................................... 28\n13.2. Simple program .................................................................................................................. 29\n13.3. User manual ....................................................................................................................... 30\n1\n1. Introduction\n1.1. What is Pali ?\nPali (also written P\u0101li, Pa\u1e37i or P\u0101\u1e37i) is a dead language from the group of Middle Indo-\nAryan languages (Burrow, 1955: 2). Despite its status as dead language, Pali is still widely studied because many of the early Buddhist scriptures were written in Pali (Bloch, 1970: 8). It is also said that Buddha himself spoke Pali or a closely related dialect (Pali Text Society; Thera, 1953: 9).\nPali is an agglutinating and fusional language (Aikhenvald, 2007: 4). Besides a base\nmeaning expressed by a stem or root, further information is expressed by adding affixes (Aikhenvald, 2007:4). However, in contrast to purely agglutinating languages, affixes in fusional languages cannot be split into separate meaning-bearing units; there are no clear-cut boundaries between the morphemes (Aikhenvald, 2007:4). Rather, a single affix expresses multiple morphemes, such as case, number and gender (Aikhenvald, 2007:4).\nIn the word devo \u2018a god\u2019 for example, the lemma is deva with the ending -o expressing\nNOUN, DECLENSION TYPE A, SINGULAR, MASCULINE, NOMINATIVE.\n1.2. Sanskrit\nPali is related to the more widely studied Indo-Aryan language Sanskrit (Duroiselle, 1921:\n1). While Sanskrit was taught to members of the upper social class, Pali belonged to a group of vernacular dialects (Burrow, 1955: 36, 50). Buddha allegedly rejected Sanskrit as a language for his teachings because of its status as learned language, language of the upper classes, as opposed to vernacular dialects, spoken by the majority of people (Burrow, 1995: 58).\nMuch of the available literature on Pali grammar expect the reader to have at least some\nknowledge of Sanskrit, as Sanskrit is often used not only to trace the etymology of Pali words, but also to explain the grammar itself (Collins, 2006: ix). Duroiselle, in his Practical Grammar of the Pali Language, tries to abstain from using Sanskrit as an explanation for Pali, as he intended his grammar for students who do not know any Sanskrit (1921: 2). Even so, he concedes to sometimes using Sanskrit examples to explain seemingly idiosyncratic word formations in Pali (Duroiselle, 1921: 2).\nGiven the relationship between Pali and Sanskrit, quite some problems arising from the\nwish to computationally process Sanskrit are also valid for Pali. In the same vein, some linguistic methods and solutions for Sanskrit can be transferred to Pali. Some of the problems listed by Huet are the representation of phonemes, suitable transliteration schemes for program-intern\n2\nprocessing and sandhi, sound changes occurring at morpheme boundaries (2005; 2009, as cited in Kulkarni and Shukl, 2009: 1-2).\nThe representation of phonemes did not pose any problems. However, due to the\nwidespread use of Pali, it was also written in a myriad of scripts. We have, in this work, opted for a diacritical Unicode notation, based on the notation used by the dictionary of the Pali Text Society, on which the lexical database used in this work is based. Handling Sandhi is another important problem, and, while not exhaustive, has also been addressed in this work.\n1.3. Motivation\nCurrently, there are no natural language processing (NLP) systems available for Pali.\nThus, analyzing Pali texts is restricted to manual analysis. Even though there are some programs that allow the user to specify a word and generate morphological forms (see, for example the Pali Lookup System by Aukana Trust1), there currently is, to the best of our knowledge, no system available that covers all the functionality presented in this work.\nHaving access to morphological information is important for all natural language\nprocessing systems (Kulkarni and Shuckl, 2009: 6). Many NLP tasks require the input to be at least Part-of-Speech-tagged. Indeed, many NLP systems are realized as pipelines, with the output from one module being the input for the next module. We are working on the start of this pipeline. Most systems for morphological analysis work on pre-tagged words (see, for example Lezius, Rapp and Wettler, 1998), rely on large lexical databases or a combination of both (see, for example Perera and Witte, 2005). Indeed, using morphological information in such systems can greatly increase accuracy (Silva, 2007: 42).\nWe can, however, not work with tagged words, since there are no reliable taggers\navailable for Pali. Furthermore, the relation between morphological analyzers and taggers seems to be a circular one: morphological analyzers work best with tagged words, and taggers work best with morphologically analyzed words. Tagging and morphological analysis are two different NLP tasks but they support each other. One solution to this circular problem would be to use a tagged corpus and train a tagger on it. Unfortunately, there are virtually no tagged corpora available for Pali; manual tagging is a time-consuming and inefficient solution (Perera and Witte, 2005: 1).\nIn addition, the Pali-English dictionary by the Pali Text Society, the basis for the lexical\ndatabase, presents some further problems. First of all, the dictionary does not present its information in an easily accessible way. This dictionary can be regarded as the standard, since\n1 Available from http://metta.lk/pali-utils/\n3\nthere are only a handful of dictionaries for Pali. We would like to computationally process this corpus. The notation used in the dictionary is not always consistent, and even though the desired information is present, it is difficult to extract this information computationally. Another problem is that we cannot assume the dictionary to be complete. Lastly, we cannot assume the dictionary to be error-free. It is thus not recommended to rely solely on the dictionary.\nThe aim of this work thus is to assist the user by providing tools for the morphological\nanalysis of Pali. These modules should be useable by themselves or as a basis for higher-level systems such as Part-of-Speech taggers.\n2. Challenges\nMost grammars dealing with Pali refer to Sanskrit. These grammars expect the reader to\nhave knowledge of Sanskrit and deduce Pali words from Sanskrit words and vice versa. Even without a presupposed knowledge of Sanskrit, performing morphological analysis and devising a system capable of automatically processing a language requires a certain level of understanding. As we had not been exposed to Pali prior to this work, we had to work ourselves into the language as quickly as possible.\nAnother challenge is the lack of literature on Pali. Working with as little, sometimes\ncontradictory or incomplete, resources can be problematic. As Sanskrit presents similar problems as Pali with regard to morphology, using literature about Sanskrit seems to be a sensible choice. However, Sanskrit is written in Devanagari, an Indian script. Subsequently, literature dealing with the morphological analysis of Sanskrit does so using this script, which further complicates matters.\nAs a singularity, no new texts in Pali emerge. Therefore, the vocabulary of Pali does not\nchange, as is the case with living languages. Given these circumstances, the occurrence of outof-vocabulary words would be expected to be relatively low. The problem is that we have a noncomputer-readable dictionary of yet unknown quality. We are in the process of processing this dictionary and making it computer-readable. However, the quality of the data would have to be checked.\nCompounding is frequent and complex in Pali, following similar rules as Sanskrit.\nFurthermore, it is improbable that each and every single possible morphological form permissible in the language has at some point been written down. Therefore, assuming only the attested forms as correct is not a viable choice. We must allow all possible forms, including nonexistent and possible erroneous forms when generating morphological forms.\n4\nThe system here presented could theoretically work without a dictionary. The ultimate\ngoal, however, is to continually improve the dictionary. With this goal in mind, dictionary functionality has been incorporated into the morphological analyzer and generator.\n3. Lexical Database\nThe modules in this system access a lexical database. The implementation and\nspecification of the database are not part of this work. The database holds different collections of words, also referred to as dictionaries. The collections of importance in this work are the wordforms collection, containing all occurring tokens from the corpus it was compiled from; the lemma collection, which contains only lemmata and, if available, word class information; and the generated collection, which contains data computed by this system\u2019s generator.\n4. Generator\nThe core module of this work is the generator. The generator returns all possible regular\ninflected forms of a lemma. Section 4.1 explains the importance of the generator for this work. Section 4.2 gives a more detailed description of the approach taken. Finally, section 4.3 highlights some problems encountered.\n4.1. Rationale\nThe approach taken in this work is as follows: given a set of lemmata, generate and store\nall inflected and derived forms of each lemma. Storing these generated forms in a separate dictionary inside the lexical database (generated collection), we can, at runtime, simply look up each encountered word during lemmatization. If we can find the encountered form inside the generated dictionary, the lemma can simply be retrieved by lookup. It must however be borne in mind that the set of lemmata (lemma collection) is in all probability not exhaustive. As such, the lemmatizer will have to rely on additional methods. This point will be elaborated on further in section 6 (Lemmatizer).\nFurthermore, if, for any given word, the syntactic category (noun, verb,\u2026) cannot be\ndetermined with certainty, the generator could be used. Indeed, generating all forms under the assumption that the current word is a noun or a verb and checking the occurrence of these forms against the database of actually occurring forms (wordforms collection) can be helpful. For example, if we have to choose between verb and noun: if many forms generated according to the nominal paradigm can be found, and forms generated according to the verbal paradigm cannot be found or occur with low frequency, it is more probable that the current word belongs to the nominal category. This technique could be used by other programs using this system.\n5\n4.2. Approach\nThe generator works with paradigms. These paradigms are data models and were\nmanually compiled from different works (mainly Duroiselle, 1921 and Collins, 2006) and then represented in XML format. A paradigm as data model is a hierarchical structure with the innermost node representing regular morphological endings, and all other traversed nodes to reach this node the morphological information encoded by this node. This data is later read and processed to yield paradigms as program data structure.\nLet us consider the following excerpt from the pronominal paradigm model:\n<pronoun>\n<personal>\n<number type=\"singular\">\n<person type=\"1\">\n<case type=\"nominative\">\n<ending>aha\u1e43</ending>\n</case> <case type=\"accusative\">\n<ending>ma\u1e43</ending>\n</case> ...\n</person>\n</number>\n</personal>\n</pronoun>\nWe have two innermost nodes, namely <ending>aha\u1e43</ending> and <ending>ma\u1e43</ending>. To reach the first node (aha\u1e43), we have to traverse the nodes pronoun, personal, number, person, case. This node thus expresses the morphological information: personal pronoun, first person singular nominative. The node ma\u1e43 expresses: personal pronoun, first person singular accusative.\nGenerally, the generator takes a lemma and a paradigm data structure and generates all\nforms according to the paradigm by first deriving the word-class-specific root of the word and then combining the root with the morphemes specified by the paradigm. In special cases, such as the pronominal paradigm, the generator determines the exact paradigm(s) the lemma belongs to (i.e. personal pronouns) and simply returns all forms from this paradigm.\n4.3. Problems\nOne problem of the generator is the lengthy and extensive computation it has to\nperform. If we want our system to be exhaustive, we have to allow for over-generation. If for example, a word does not have attached morphological information, the theoretical worst case scenario would consist of generating all forms according to the nominal, adjectival, verbal and\n6\nnumeral paradigm. In practice, the worst case scenario encompasses only the nominal, adjectival and numeral paradigms. If we assume the nominal paradigm to have two numbers, three genders, eight cases and only one ending per case, we arrive at 76 possibilities. If we assume the adjectival paradigm to have two numbers, three genders, eight cases and only one ending per case, we arrive at 76 possibilities. If we assume the numeral paradigm to have two numbers, no gender, eight cases and only one ending per case, we arrive at 16 possibilities. If we assume the prefix number to be at 24, with prefixes being attachable to all generated words, we arrive at a total of ( ) generated word forms from one single word. This case does occur with lemmata ending in a with no further information attached. In reality, however, there often is more than one case per ending and the total of generated words is in excess of 5000 for a word without morphological information.\nAnother problem is that the paradigms encode regular inflectional forms. Even though\nsome irregular nouns and numerals have been added as separate temporary files, the generator will have to be extended with more functionality relating to irregular inflections.\n5. Analyzer\nThe analyzer returns morphological information about incoming words, including the\npossible lemma(ta). There are two ways to analyze a word: dictionary- based and rule-based. Dictionary -based analysis consists in simply looking up the word in the dictionary of generated word forms. If a match is found, the corresponding entry is returned. If there is no corresponding entry in the dictionary, the analyzer falls back to the rule-based analysis. Section 5.1 will succinctly explain the dictionary-based lookup and section 5.2 will give an overview over the rule-based approach.\n5.1. Dictionary-based approach\nIf we presuppose the dictionary collection of generated forms to be completed (see\nabove), the dictionary will contain entries, such as (line breaks and indentation added for reading ease):\n{ \"word\":\"dev\u0101ya\", \"grammar\":\n{ \"morphology\":\n{ \"lemma\":\"deva\", \"information\":\n{ \"paradigm\":\"noun\",\n7\n\"gender\":\"masculine\", \"number\":\"singular\", \"case\":\"dative\", \"declension\":\"a\" }\n}\n}\n}\nIf we want to analyze the word dev\u0101ya, we look up the word in the dictionary. In this example, we assume the word to be in the dictionary. We can then simply retrieve this entry, which contains the morphological analysis, as returned by the generator. If the dictionary lookup fails, the analyzer falls back to the rule based approach.\n5.2. Rule-based approach\nGiven the grammatical regularity of Pali, we opted for a rule-based default approach.\nThe analyzer takes a word and, if available, the word\u2019s word class. As in most cases, however, no Part-of-speech information can be supplied, a dedicated module responsible for guessing the word class(es) of a word has been created. Thus, if no information about the word class of a word can be supplied, the word\u2019s word class is guessed.\nThe analyzer then checks whether the word is a pronoun or an irregular word. Irregular\nword files have been included for offline functionality. However, this information should eventually be incorporated into the dictionary. If a match is found at this point, the morphological information is retrieved by look up and returned.\nOtherwise, the analyzer tries to separate prefixes from the word and identifies the stem\nand ending of the word. Based on this putative stem and the word class information, whether given or guessed, the analyzer derives the lemma. The morphological information corresponds to the identified ending\u2019s morphological information. The full morphological analysis returns a lemma, morphological information and a separation of morphemes. The separation of morphemes is not available for pronouns and irregular words.\n6. Lemmatizer"}, {"heading": "In contrast with Nov\u00e1k\u2019s terminology, the lemmatizer\u2019s function is to return the lemma", "text": "and word class of a given word, excluding additional morphological information (Nov\u00e1k, 2004: 65). The functionality of what Nov\u00e1k calls lemmatizer is taken over by our morphological analyzer. However, this is but a terminological difference.\nAs we have seen in the previous section, the morphological analyzer already derives\nlemmata. This functionality is strongly dependent on morphological information, and cannot\n8\neasily be isolated and moved from the analyzer to the lemmatizer. Therefore, calling the lemmatizer\u2019s lemmatize function in offline context results in the analyzer being invoked. The lemmatizer then extracts the lemma and word class of the result and returns this information.\nIn contrast, calling lemmatize in online context results in the word being looked up in\nthe dictionary of generated word forms. As each of these entries contains the lemma from which it was derived, the lemma can simply be retrieved. If no matching entry can be found, the lemmatizer, as the analyzer, falls back to the offline method.\n7. Sandhi Splitter\n7.1. Sandhi\nWhen words in Pali are combined, they often undergo phonological changes broadly\nknown as euphony (Duroiselle, 1921: 6). More specifically, these sound changes are referred to as sandhi, the same term used as in Sanskrit (Duroiselle, 1921: 6). The word sandhi itself means union (Duroiselle, 1921: 6). Not unsurprisingly, Sanskrit sandhi rules can be mapped to Pali sandhi rules via another set of rules, and vice versa. However, the set of sandhi rules for Pali is smaller than the set of rules for Sanskrit.\nThere are different types of sandhi. Duroiselle divides them into vowel sandhi, mixed\nsandhi and niggah\u012bta sandhi (1921: 6). To give a full overview over the sandhi rules would exceed the scope of this work, therefore I will give but a few examples to illustrate the different types of sandhi.\nVowel sandhi are sound changes that take place when two vowel sounds meet (Duroiselle, 1921: 6). One subtype of vowel sandhi is elision. In this case, either the first or the second vowel sound is elided (Duroiselle, 1921: 7).\najja + upposatho = ajjuposatho (elision of first vowel sound)\ncakkhu + indriya\u1e43 = cakkhundriya\u1e43 (elision of second vowel sound)\nMixed sandhi are sound changes that take place when a word ending in a vowel is followed by a word beginning with a consonant (Duroiselle, 1921: 12). One subtype is consonantal reduplication. In this case, the consonant is doubled.\npa + kamo = pakkamo\nvi + payutto = vippayuto\nFinally, niggah\u012bta sandhi takes place when a word ending with a niggah\u012bta (\u1e43) is followed by a word starting with either a vowel or a consonant (Duroiselle, 1921: 14). The niggah\u012bta may\n9\nremain unchanged, is assimilated by the following letter, assimilates the following letter, is elided, leads to elision of the following letter or a combination of these processes.\nta\u1e43 + patto = ta\u1e43 patto (niggah\u012bta remains unchanged)\nsa\u1e43 + mato = sammato (assimilation of niggah\u012bta to m)\nsa\u1e43 + yogo = sa\u00f1\u00f1ogo (assimilation of y to niggah\u012bta and change from \u1e43\u1e43 to \u00f1\u00f1)\nki\u1e43 + iti = kinti (elision of i and assimilation of niggah\u012bta to t)\nAs can be seen from the examples given, undoing these sound changes is not a trivial task. To illustrate some of the difficulties, let us look at elisions. The nature of the elided sound cannot be determined with certainty, or in other words, we cannot say which sound was elided. Furthermore, there is no straightforward way of identifying places where elision could have taken place. Therefore, undoing elisions amounts to generating all possible combinations for all potential elisions.\n7.2. Preliminary notes\nThe rules for the Sandhi Splitter described in this work were extracted from Duroiselle\u2019s\nA Practical Grammar of the Pali Language (1921). In the first step, the rules, as listed by Duroiselle, were manually extracted. This step yielded rules like \u201cA vowel followed by another vowel results in the elision of the first vowel\u201d. The next step consisted in rewriting these rules in a computer-interpretable way. I have opted for a regular expression syntax with backreferences, constants and operations. The above mentioned rule thus becomes \u201c(VOWEL) (VOWEL):$2\u201d. To better understand the structure of these rules, I will shortly digress and give a succinct explanation of the constructs used.\n7.2.1. Capturing groups and back-references\nIn regular expressions, a capturing group is a group of characters that is treated as a\nsingle unit and that is saved in memory for later retrieval, either directly or by back-reference. Capturing groups are typically written by enclosing an expression in parentheses. A backreference is a reference to such a capturing group. Back-references are typically numbered, the number referring to the group, which almost always corresponds to the group\u2019s position in the regular expression. Given the following regular expression:\n(A|The) (cat) lives (here)\nThere are three capturing groups: (A|The), (cat) and (here). \u201clives\u201d is not a capturing group, since it is not enclosed in parentheses. This expression will recognize the sentences \u201cA cat lives here\u201d and \u201cThe cat lives here\u201d. When the regular expression is evaluated against either one of\n10\nthese sentences (it will fail against other sentences), the following back-references will be created:\n$1: A/The*\n$2: cat\n$3: here\n* Back-reference $1 refers to either \u201cA\u201d or \u201cThe\u201d, depending on which one was encountered in the sentence the expression was evaluated against.\n7.2.2. Constants\nBased on the extracted sandhi rules, constants have been introduced for recurring sets\nof characters. Doing this greatly enhances readability and maintenance of the rules and also helps formulating the rules in a more succinct manner. These constants are defined in a separate file using a specific prefix notation. It should be noted that the prefix notation is unrelated to regular expressions and is merely a construct introduced by me to easily identify non-rules. During program execution, the constants will be replaced by the corresponding set of characters. As an example, vowels are often found in rules. Therefore, we have a constant named VOWEL, defined as:\n=VOWEL:a,i,u,e,o,\u0101,\u012b,\u016b\nThe prefix equals (=) indicates that this is a constant definition. The first part of the definition contains the name of the constant (VOWEL), the colon separates the first part of the definition from the second part of the definition. The second part of the definition indicates the corresponding set of characters, each character separated from the next character by a comma. As an example of increased readability and succinctness, let us look at the definition of CONSONANT:\n=CONSONANT:k,c,\u1e6d,t,p,kh,ch,\u1e6dh,th,ph,g,j,\u1e0d,d,b,gh,jh,\u1e0dh,dh,bh,y,r,\u1e37,v,h,s,\u1e45,\u00f1,\u1e47,n,m,\u1e43\nIt should be clear that using constants not only improves readability and maintenance, but it also reduces the probability of errors in rules.\n7.2.3. Operations\nSome sandhi rules are of the form \u201cIf a long vowel follows a consonant, shorten the\nvowel\u201d or \u201cIf a non-nasal sound follows a mute sound, elide the mute sound and reduplicate the non-nasal sound\u201d. An elegant way of rewriting these rules in a computer-interpretable way seems to be:\n11\n(CONSONANT) (LONG_VOWEL):$1+short($2)\n(MUTE) (NON_NASAL):+duplicate($2)\nThat is by using operations. In these cases, the operations would be \u201cshort(x)\u201d to shorten a (long vowel) sound and \u201cduplicate(x)\u201d to reduplicate a consonant. In the prefix notation, operations are introduced by plus (+). The operations are declared in the same file as the constants, but the exact implementation of the operations has to be declared inside the program, since reading partial program code from a file and integrating it into a program is overly difficult.\n7.3. Approach\nTo come back to the example given at the beginning of this chapter, it should now be\nclear what\n(VOWEL) (VOWEL):$2\ndoes. VOWEL is replaced by the corresponding set of characters when the program is run, and whichever characters were matched, the second group is taken as replacement for the two vowels. To illustrate this, let us consider some possible matches. The expression would match the following expressions:\ne o\nu i\n\u0101 \u016b\nAll of these expressions consist of (VOWEL) (VOWEL). The back-reference $2 would be o, i and \u016b respectively.\nHowever, the rules, as given by Duroiselle, are applicable only in one direction, namely\nthe direction of merging, that is two or more sounds are merged. On the other hand, sandhi splitting is required to perform the exact opposite operation. Therefore, a system to reverse the rules had to be devised.\n7.3.1. Reversing rules\nManually reversing the rules is not a viable option in most cases. Not all rules present\nthe same degree of difficulty with regard to reversal.\nAtomic rules are easiest to reverse. Atomic rules are rules that do not contain back-\nreferences, operations or expandables. Expandables are expressions stating alternatives. These expressions can be expanded to yield a number of separate rules, each with one of the given\n12\nalternatives. This point will be elaborated on in the next paragraph. The following rule is an atomic rule:\ne a:aya\nReversing this rule is accomplished by simply reversing the left-hand-side and the right-handside of the rule, yielding:\naya:e a\nRules containing expandables are more tricky to reverse. Let us consider the following rule:\n(a|\u0101) (i|\u012b):e\nSimply reversing this rule would yield:\ne:(a|\u0101) (i|\u012b)\nThis would correspond to the rule \u201cIf you encounter the letter \u2018e\u2019, replace it by either a or \u0101 followed by either i or \u012b\u201d. The right-hand-side of this rule thus expresses alternatives. The rule cannot be directly applied as is and has to be expanded into a separate set of rules by resolving the expandables. The astute reader will have noticed that capturing groups on the left hand side of a rule become expandables on the right hand side of the rule. For each expandable, we will create n separate rules, each with one of the n alternatives stated by the expandable. In a first step, this would yield two rules:\ne:a (i|\u012b) e:\u0101 (i|\u012b)\nAs the newly created rules still contain expandables, we will resolve the remaining expandables. This yields:\ne:a i e:a \u012b e:\u0101 i e:\u0101 \u012b\nThis new set of rules contains only atomic rules. We thus expanded one rule into a set of four rules.\nRules containing back-references need special attention as well. Let us consider the rule:\n(VOWEL) (iha|agga):$1t$2\nSimply reversing it would yield:\n13\n$1t$2:(VOWEL) (iha|agga)\nThe problem here is that the back-references cannot reference to anything, since there are no capturing groups preceding the back-references. They are thus meaningless. To solve this problem, we have to identify all back-references and the groups the back-references refer to. We then have to swap these reference\u2013back-reference pairs before reversing the rule. The rule then becomes:\n(VOWEL)t(iha|agga):$1 $2\nFinally, rules containing operations need special treatment. Let us consider the rule:\n(DENTAL) (CONSONANT):+duplicate($2)\nReversing this rule after having swapped the references\u2013back-references yields:\n+duplicate((CONSONANT)):(DENTAL) $2\nThis rule is problematic in more than one way. First of all, the back-reference $2 cannot reference anything because there is at most one capturing group: (CONSONANT). This group being an argument to the operation, it can be debated whether this can be considered a capturing group or not. However, this is not of importance here. Secondly, the rule states \u201cIf you encounter the operation \u2018duplicate\u2019 with a CONSONANT\u2026\u201d. However, the operation itself will never be encountered; the result of the operation will be encountered. To solve this problem, we again create a set of separate rules from this rule: First, the argument of the function, if it is a constant, is replaced by the corresponding set of characters. Then, for each character c in the set of characters, the function is called with the character c and the (invalid) back-reference is replaced by the character c. The complete output of this step is too voluminous to list here. It should be sufficient to give a short illustrative example of this step\u2019s output. For the character t, we invoke the function duplicate(t) which yields tt, and we replace the back reference $2 by t. We then take the next character p and proceed as described, and so on.\ntt:(DENTAL) t pp:(DENTAL) p\nThe rule still contains the expandable (DENTAL), which will further bloat the result list. The complete result of this rule\u2019s reversal can be found in the appendix (13.1 Rule reversal).\nAll these steps have been automated and the results have been manually checked and\ncorrected to ensure the quality and correctness of the rules. Some rules that rely on information not accessible at this point were excluded. Such rules are of the form \u201ci or u followed by a noun in the vocative case\u201d. Part-of-speech information is still virtually non-existent at this point and\n14\nwould require preprocessing by a POS-Tagger. It is possible that such information will become available at a later stage.\n7.3.2. Splitting compounds\nThe module responsible for splitting compound words into their constituents operates\non the basis of the above mentioned reversed rules. The module takes a word and first identifies which rules can be applied at which positions inside the word. It does so by comparing the rules against the word and position at hand, iterating through the word from left to right. The information from this step is saved in a table.\nTo illustrate this procedure, let us consider a hypothetical word consisting of the letters abcdef. Let the hypothetical rules RULE 1 and RULE 2 be as follows:\nb:x y (RULE 1)\nd:w v (RULE 2)\nThe algorithm starts at position 0, which corresponds first letter of the word. Since no rule of the form\na:x y\ncan be found, the algorithm proceeds to the next position. At position 1, we encounter the letter b. RULE 1 states that b should be replaced by x y. Thus a table entry is created with the current position and the relevant rule. Since there are no further rules applicable at position 1, the algorithm proceeds to the next position. The next table entry is created for position 3. All further positions do not result in table entries, since no applicable rules can be found. The table thus contains these entries:\nPosition Rule"}, {"heading": "1 RULE 1", "text": ""}, {"heading": "3 RULE 2", "text": "The algorithm then steps through each entry of the table and generates a result by applying the rule specified by the entry at the specified position in the word. Each result self-validates itself. Self-validation results in the result becoming invalid if it contains words made up of improbable letter combinations. When querying the confidence of a split, the result calculates its confidence of the split by looking up each resulting word in the dictionary. Confidence c is simply expressed as :\n15\nwith N being the total number of words in the result after splitting and n being the number of words from N found in the dictionary.\nAt this moment, the Sandhi Splitter can only split a word once. However, it is desirable to\nextend this behavior to split a word more than once. We have therefore introduced a parameter depth. Depending on the parameter depth, the table is traversed again with the current result list. Indeed, depth specifies the maximal number of splits that should be executed, which correlates with the expected number of constituents in the compound. The introduction of this parameter seems indispensable, as in most cases the number of constituents of a compound is not known. Conceptually, a depth of zero would simply return the received word, as no splitting at all would have to be performed. At depth 1, the result of the above mentioned hypothetical rules applied to the hypothetical word would yield:\nax ycdef (RESULT 1)\nabcw vef (RESULT 2)\nIndeed, at depth 1, each rule in the table is applied once, resulting in at most one split, or in other words two separate words. At depth 2, the result would be:\nax ycdef (RESULT 1)\nabcw vef (RESULT 2)\nax ycw vef (RESULT 3)\nThe results RESULT 1 and RESULT 2 are the same as with depth 1. However, the table entries are again applied to these results. RULE 1 however is not applied to RESULT 1, as RESULT 1 already resulted from the application of RULE 1. Similarly, RULE 2 is not applied to RESULT 2. After updating the positions to reflect the correct position inside the results, applying RULE 1 to RESULT 2 yields RESULT 3. Applying RULE 2 to RESULT 1 also yields RESULT 3. Since the result is the same in both cases, it is only retained once. In this example, any depth greater than 2 will yield the same result as depth 2.\n7.3.3. Merging compounds\nThe module responsible for merging two or more words into one word according to the\nrules of Sandhi operates on the basis of the above mentioned rules. The module takes a list of words and performs pair-wise merging. Starting with the first pair, the module identifies applicable rules, if any, and applies these rules to yield one or more merged words. If no applicable rule can be found, the words are left unmerged. If more than two words are specified, the module merges the remaining words after the first pair with the results of the first pair one by one.\n16\n8. Stemmer\nA simple stemmer has been included as well. The stemmer simply removes all endings\nfrom a word, returning the stem of a word. The stem is not identical to the lemma of a word. Stemming is much faster than lemmatizing; in most cases, the result is not a grammatically valid word though. Still, the reduction of words to a common stem might prove useful for some future applications.\n9. Technical documentation\nThe system consists of different parts. The following diagram gives a broad overview of\nthe main modules.\nEllipses stand for data. Rectangles stand for modules. Solid arrow heads denote an information flow from-to. Hollow arrow heads denote an optional information flow. Gray fields denote temporary, soon to-be-deprecated fields. A broken line stands for an anticipated permanent relocation of information from-to.\nThe implementation and specification of the lexical database are not part of this work.\nThe lexical database is used to store different collections of words. The database is realized as a JSON-based database. A dedicated data management system has been created to facilitate the access to the database (to appear).\n17\nThe diagram on the following page gives a more detailed picture of the interaction\nbetween the modules. The lexical database section has been left out to improve readability. Ellipses stand for data. Rectangles stand for modules. Hexagons stand for a collection of classes. For example, Strategy stands all Strategy classes (AdjectiveStrategy, NounStrategy, NumeralStrategy, NullStrategy,\u2026).\nArrows in this diagram represent a uses-relation.\n18\n19\n9.4. Data format and conventions\n9.4.1. Input\n9.4.1.1. Grammar files\nThe grammar files are specified in XML format. The structure is hierarchical, with\nterminal nodes (innermost nodes) standing for particular morphs (word endings) or concrete word forms. The order of non-terminal nodes is not important. However, it must be ensured that all nodes traversed in order to reach a terminal node express that node\u2019s morphological information. In the following excerpt, we have two innermost nodes, namely <ending>aha\u1e43 </ending> and <ending>ma\u1e43</ending>. To reach the first node (aha\u1e43), we have to traverse the nodes pronoun, personal, number, person, case. This node thus expresses the morphological information: personal pronoun, first person singular nominative. The node ma\u1e43 expresses: personal pronoun, first person singular accusative.\n<pronoun>\n<personal>\n<number type=\"singular\">\n<person type=\"1\">\n<case type=\"nominative\">\n<ending>aha\u1e43</ending>\n</case> <case type=\"accusative\">\n<ending>ma\u1e43</ending>\n</case> ...\n</person>\n</number>\n</personal>\n</pronoun>\nIf a node has an attribute, it is specified via the generic attribute name type.\n9.4.1.2. Temporary irregular files\nThe temporary irregular declension files are simple text files, with each line representing one entry. The following line is an example of an irregular declension file line.\neko{paradigm=numeral, number=singular, gender=masculine, case=nominative}\nThe first word is the morphological form, followed by the information encoded as key-value pairs. The key-value pairs are enclosed in curly braces. The key from a key-value pair is separated from the corresponding value by an equals sign. Key-value pairs are commaseparated.\n20\n9.4.1.3. Sandhi files\nSandhi files are separated into rule files and dictionary files. Rule files contain one rule\nper line. Rules consist of a left hand side and a right hand side, separated by a colon. Both left and right hand side of a rule may contain literals and operations. Additionally, the left hand side may contain groups of literals, constants and regular expression elements. The right hand side may contain back-references.\nLiterals are characters that are to be matched as-is. Groups of literals are literals separated by the pipe character and enclosed in parentheses:\n(j|c)\nOperations start with a plus sign, followed by the function name, followed by the operation argument enclosed in parentheses. The operations have to be declared in the dictionary file, but have to be implemented as functions in the program itself. In the dictionary file, the definition of an operation is the plus sign, the operation\u2019s name, an opening parenthesis, the argument, a closing parenthesis:\n+long(x)\nConstants are named sets of characters. The constant name is written in all-caps. The equivalent set of characters has to be declared in the dictionary file. In the dictionary file, the definition of a constant is an equals sign, followed by the constant\u2019s name, followed by a colon, followed by the set of characters:\n=VOWEL:a,i,u,e,o,\u0101,\u012b,\u016b\nWhen used in rules, constants are not prefixed with an equals sign. When used in rules, constants must always be enclosed in parentheses.\nRegular expression elements include the start of line ^ and the end of line $.\nBack-references can be used to refer to a prior match by number. Back-references can only be used on the right hand side of a rule to refer to a grouping expression on the left hand side of the rule. Grouping expressions are enclosed in parentheses. The first grouping expression is the first expression enclosed in parentheses when reading from left to right. Back-references are written as the dollar sign followed by the number of the expression. Numbering of the expressions starts with 1. The number cannot be greater than the number of grouping expressions.\nThe dictionary file(s) contain the constant definitions and operation definitions.\n21\nAll files may include end-of-line comments. Comments start with the hash character and extend to the end of the line. In-line comments are not supported; each comment must begin at the beginning of a line.\nThis convention was chosen because such rules, after some transformations, can be\nused by Java\u2019s replaceAll method, which takes a regular expression as first parameter, specifying the part to replace, and an expression as second parameter, specifying the expression to replace the first part with. The second parameter may contain back-references as declared by this convention.\n9.4.2. Output\nThe lemmatizer, analyzer and generator return a List of JSON objects; a JSON object is an object with properties. Properties are expressed as key-value pairs. Property values can be queried by property name/key. All returned objects contain a key:\n- \u201cword\u201d, the value being the morphological form of a word.\n- \u201cgrammar.morphology.lemma\u201d, the value being the lemma of \u201cword\u201d\n- \u201cgrammar.morphology.information\u201d, the value being a list of key-value pairs that express the word\u2019s morphological information\nJSON object was chosen as output format because it can directly be inserted into the lexical database. Furthermore, JSON object as a key-value map presents itself as an easily accessible model.\nInternally, the program represents words that have been constructed by the morphological generator in objects of type ConstructedWord. Such constructed words are collection and then later converted to a suitable format for inserting into the dictionary. The latter is done by using the class DictWord provided by the library that provides a simple API to access the dictionary. ConstructedWord contains a feature set (key-value mappings) and the fields lemma, word and stem. The structure of a ConstructedWord is flat, compared to the nested DictWord structure.\nThe sandhi merger returns a List of String. The stemmer returns a String.\nThe sandhi splitter returns a List of SplitResult. SplitResult is a List of String containing the result of splitting a word, a list of SandhiTableEntries containing information about which rule has been applied at which position, and the confidence of the split result. SplitResult could have been converted to DictWord. However, the calculation of the confidence is done in a lazy manner. Calculating confidence is a lengthy computation and will only be performed when the confidence is required. Converting all SplitResults to DictWords would require the computation of every result\u2019s confidence.\n22\n9.5. Paradigm\nA Paradigm saves the information read from the grammar file for program-intern\nprocessing. The information from the grammar file is parsed into a morphological structure. Terminal nodes from the grammar file are saved as Morph objects, containing Occurrence information if present. If there is more than one terminal node at one level, the nodes are saved as a list of Morph objects inside a Morpheme object. Otherwise, the Morpheme object only contains a single Morph. The Morpheme object also saves the morphological information, that is all non-terminal nodes traversed so far, in the form of a FeatureSet containing a list of Feature objects (key-value pairs). Paradigm contains a list of Morpheme objects.\nOccurrence objects represent information about the Morph it is attached to; Occurrence\nobjects may contain information about how a Morph objects influences the context it occurs in.\n9.6. Generator\nThe generator takes a lemma as String, the lemma\u2019s word class as String and an optional list of options as input. Options are used for instance to specify the lemma\u2019s gender in the case of nouns.\nIf word class is not specified, the generator uses a word class guesser to guess the possible word classes. For lemmata, the word class depends solely on the ending of the lemma.\nThe generator uses a strategy manager to get the word class specific declension strategies and applies them to the lemma.\n9.6.1. Word class guesser\nThe word class guesser takes a word as String as input.\nThe word class guesser is responsible for returning possible word classes given a word of unknown word class. There are two different implementations of the guessing algorithm:\n- If the word in question is known to be a lemma, the word class solely depends on the\ncurrent ending of the word. The algorithms checks known lemma endings against the ending of the word at hand and returns all applicable word classes. - If the word in question is not a lemma, the word class guesser checks all paradigms,\ntrying to identify word class specific suffixes on the word. The algorithms counts the number of possible identified suffixes per word class paradigm. The algorithms weighs the counts, so that longer identified suffixes get more weight than shorter suffixes. The result is a weighted list of word classes. The word class with the most weight is, in most cases, the correct word class. Based on a pruning parameter, the list of possible word classes is cut off if the difference between the current weighted word class frequency\n23\nand the weighted frequency of the following word class is less than the pruning parameter. A pruning parameter of 10 has been found to perform reasonably well.\n9.6.2. Strategies\nStrategies are responsible for deriving the word class specific stem of a word and combining the stem with the word class specific endings. A strategy retrieves the relevant paradigms and applies all relevant paradigms to the stem.\nThe GeneralDeclensionStrategy is a general strategy. It takes a lemma, a paradigm and a rule describing how to derive the stem from the lemma.\nThe AdjectiveStrategy, NounStrategy, NumeralStrategy and VerbStrategy are pre-configured classes that select the correct parameters for a given input and call the GeneralDeclensionStrategy with these parameters. The AffixStrategy is responsible for returning all possible combinations of prefixes and suffixes with a given word.\nThe GeneralDeclensionStrategy checks the validity of the generated forms using a validator. The validator uses simple, general rules to determine whether a word is valid or not. If a word is deemed invalid, the declension strategy tries to merge the stem and ending resulting in the invalid word using sandhi rules.\n9.7. Analyzer\nThe analyzer takes a word as String or DictWord and an optional list of options as input. Options are used to specify the word class of the input.\nIn offline context, the analyzer guesses the word class if it was not provided. The\nanalyzer then checks whether the word is any form of a pronoun. If this is the case, the analyzer constructs a new analysis from the pronoun form and the attached morphological information and adds this analysis to the output list. The analyzer then checks whether the word is irregular. If this is the case, the analyzer constructs a new analysis from the morphological information attached to the irregular form and adds this analysis to the output list. The analyzer then tries to identify prefixes. For each guessed word class, the analyzer then tries to identify suffixes and paradigm endings. The analyzer then determines the boundary between the stem and the attached ending. If the ending contains declension information, this information is used in conjunction with the word class and putative identified stem to derive the word class specific lemma. The analyzer then constructs a new analysis using all the gathered information and adds this analysis to the output list. The analyzer then returns the output list.\nThe analyzer contains two overloaded methods for analyzing: analyze for offline context and analyzeWithDictionary for online context. These methods are overloaded to work with String and DictWord input.\n24\nIn online context, the analyzer checks whether the input word is in the dictionary of\ngenerated word forms. If this is the case, the analyzer retrieves and returns all relevant entries. Otherwise, the analyzer falls back to offline mode.\n9.8. Lemmatizer\nThe lemmatizer takes a word as String or DictWord as input.\nIn offline context, the lemmatizer calls the analyzer and extracts the relevant information.\nIn online context, the lemmatizer checks whether the input word is in the dictionary of lemma forms. If this is the case, the lemmatizer retrieves and returns this word wrapped as a singleton list of DictWord. Otherwise, the lemmatizer falls back to offline mode.\nThe lemmatizer contains two overloaded methods for lemmatizing: lemmatize for offline context and lemmatizeWithDictionary for online context. These methods are overloaded to work with String and DictWord input.\n9.9. Sandhi splitter\nThe sandhi splitter takes a word as String and a depth as integer as input.\nThe sandhi splitter takes a word and first identifies which rules can be applied at which positions inside the word. It does so by comparing the rules against the word and position at hand, iterating through the word from left to right. The information from this step is saved in a table.\nThe algorithm then steps through each entry of the table and generates a result by applying the rule specified by the entry at the specified position in the word. Each result self-validates itself. Self-validation results in the result becoming invalid if it contains words made up of improbable letter combinations. Each result has a confidence. Confidence is only calculated when required. Confidence is expressed as the number of words of the split found in the dictionary divided by the total number of words in the split.\nDepending on the parameter depth, the table is traversed again with the current result list. Indeed, depth specifies the maximal number of splits that should be executed, which correlates with the expected number of constituents in the compound. The introduction of this parameter seems indispensable, as in most cases the number of constituents of a compound is not known.\n9.10. Sandhi merger\nThe sandhi merger takes two or more words as String array as input.\nThe sandhi merger merges two or more words. Internally, the words are merged pair-wise. The merger takes two words and identifies rules that are applicable to the word boundaries created\n25\nby the ending of word 1 and the start of word 2 and merges these two words. The results of this merge operation are merged with the remaining words in the same manner, using each word from the result list as word 1 and the next of the remaining words as word 2, creating a new result list. This process continues until no words remain.\n9.11. Stemmer\nThe stemmer takes a word as String as input.\nA simple stemmer has been included as well. The stemmer simply removes all endings from a word, returning the stem of a word. The stem is not identical to the lemma of a word. Stemming is much faster than lemmatizing; in most cases, the result is not a grammatically valid word though. The stemmer retrieves all paradigm endings, discarding all other attached information. The stemmer recursively strips off endings from a word until the word contains no strippable endings anymore. The remaining stem is returned.\n9.12. API\nA high-level application programming interface (API) has been implemented to provide easy access to the main functions of the system. The API contains static methods that call the relevant system modules. To use the Pali NLP system via the API, simply use PaliNLP from the package de.unitrier.daalft.pali.\nFor example, to lemmatize, call:\nPaliNLP.lemmatize(\u201cgavassa\u201d);\nAlternatively, you can call the modules directly. The main modules are Lemmatizer, MorphologyAnalyzer, MorphologyGenerator, NaiveStemmer and SandhiManager. For example, to generate morphological forms using the MorphologyGenerator, call:\nMorphologyGenerator.generate(\u201cgo\u201d);\nFor a complete example of a simple program using the PaliNLP system, see Appendix 13.2.\n10. Future work\nIn the future, it would be desirable to further increase the functionality and accuracy of\nthe presented system. Some areas could be improved by the inclusion of metathesis, dropping of syllables and epenthesis. Though some regular cases of metathesis and epenthesis are covered by the modules, not every case of metathesis or epenthesis could be covered. Another are that could need improvement is the Sandhi splitter. Compound words can only be split once\n26\nby the Sandhi splitter at the moment. Even though this might be sufficient to resolve the majority of compounds, a more exhaustive splitting module will probably be necessary.\nAnother desirable evolution would be the development of a POS-Tagger for Pali, using\nthe presented system as a basis. As mentioned earlier, POS-Tagging benefits from morphological information, and morphological analyzers benefit from POS-Taggers. This interdependency could be used to incrementally improve both systems as well as the quality and completeness of the dictionary.\nLastly, using n-grams to check the validity of words could be implemented and\nevaluated. The current system uses simple, general rules to determine whether a word is valid or not. However, using n-grams could possibly yield more accurate results and graded results, since anagram analysis would allow to check words at a finer granularity and assign a probability to the validity, correlating with the frequency of the constituting n-grams.\n11. Conclusion\nThe presented system is a first step in the direction of the morphological analysis of Pali.\nThe system is already functional, proving the concept to be viable and promising. As stated in the section 9, there is still some fine-tuning than can and should be done to improve this system, especially in the area of irregular declensions. Nonetheless, the system at the current stage of development should be able to process the majority of Pali words.\nThe system also uses a rather unconventional paradigm-based rule system to derive\nmorphological information. Most morphological analyzers are built by using tools to derive a finite state transducer from a set of grammatical rules. This approach could have been applied in this case; it would have presupposed a better knowledge of the language though. However, given the regularity and immutability of Pali words, the rule-based approach seems reasonable.\nAdditionally, the incorporation of a lexical database allows for fast look up instead of a\nlengthy computation. The ultimate goal would be a system relying solely on the database. However, this would require the database to be reliable. Improving the quality of the lexical database would not only be reliant on the presented system, but also on a POS tagging system developed at a later stage. Still, this system can contribute to improving the existing lexical database.\nAs we have seen, Sandhi compounds still pose a problem. The presented system has\nmade a first step towards resolving Sandhi-merged compounds, yielding a module that merges words according to the rules of Sandhi as well. However, more work needs to be done in order to be able to accurately split compounds.\n27\n12. Sources\nAikhenvald, Alexandra Y. \u201cTypological distinctions in word-formation.\u201d Grammatical categories and the\nlexicon 3.2 (2007): 1\u201364. Print.\nBloch, Jules. Formation of the Marathi Language: Motilal Banarsidass, 2008. Print.\nBurrow, Thomas. The Sanskrit Language: Faber and Faber, 1955. Print.\nCollins, Steven. A Pali grammar for students. Chiang Mai: Silkworm Books, 2006. Print.\nDuroiselle, Charles. A practical grammar of the P\u0101li language. 3rd ed. Rangoon: British Burma Press, 1921.\nBuddhaNet eBooks. Web. 1997.\nKulkarni, Amba, and Devanand Shukla. \"Sanskrit morphological analyser: Some issues.\" Bh. K Festschrift\nvolume by LSI (2009).\nPali Text Society. Web.\nPerera, Praharshana, and Ren\u00e9 Witte, ed. A self-learning context-aware lemmatizer for German. October\n2005. Vancouver: Proceedings of Human Language Technology Conference and Conference on\nEmpirical Methods in Natural Language Processing (HLT/EMNLP): Association for Computational\nLinguistics, 2005. Print.\nLezius, Wolfgang, Reinhard Rapp, and Manfred Wettler. \u201cA Freely Available Morphological Analyzer,\nDisambiguator and Context Sensitive Lemmatizer for German.\u201d ACL '98 Proceedings of the 36th Annual\nMeeting of the Association for Computational Linguistics and 17th International Conference on\nComputational Linguistics - Volume 2 (1998). Print.\nNov\u00e1k, Attila. \"Creating a Morphological Analyzer and Generator for the Komi language.\" First Steps in\nLanguage Documentation for Minority Languages (2004): 64.\nShopen, Timothy, ed. Grammatical categories and the lexicon. 2nd ed. Cambridge: Cambridge Univ. Press,\n2007. Print. Language typology and syntactic description 3.\nSilva, Jo\u00e3o. \"Shallow processing of Portuguese: From sentence chunking to nominal lemmatization.\"\n(2007).\nThera, N\u0101rada. An elementary P\u0101\u1e37i course. 2nd ed. Colombo: Associated Newspapers of Ceylon, 1953.\nBuddhaNet eBooks. Web. N.d.\n28\n13. Appendix\n13.1. Rule reversal\nReversing and resolving (DENTAL) (CONSONANT):+duplicate($2)\nkk:t k kk:th k kk:d k kk:dh k kk:n k kk:l k kk:s k cc:t c cc:th c cc:d c cc:dh c cc:n c cc:l c cc:s c \u1e6d\u1e6d:t \u1e6d \u1e6d\u1e6d:th \u1e6d \u1e6d\u1e6d:d \u1e6d \u1e6d\u1e6d:dh \u1e6d \u1e6d\u1e6d:n \u1e6d \u1e6d\u1e6d:l \u1e6d \u1e6d\u1e6d:s \u1e6d tt:t t tt:th t tt:d t tt:dh t tt:n t tt:l t tt:s t pp:t p pp:th p pp:d p pp:dh p pp:n p pp:l p pp:s p kkh:t kh kkh:th kh kkh:d kh kkh:dh kh kkh:n kh kkh:l kh kkh:s kh cch:t ch cch:th ch cch:d ch\ncch:dh ch cch:n ch cch:l ch cch:s ch \u1e6d\u1e6dh:t \u1e6dh \u1e6d\u1e6dh:th \u1e6dh \u1e6d\u1e6dh:d \u1e6dh \u1e6d\u1e6dh:dh \u1e6dh \u1e6d\u1e6dh:n \u1e6dh \u1e6d\u1e6dh:l \u1e6dh \u1e6d\u1e6dh:s \u1e6dh tth:t th tth:th th tth:d th tth:dh th tth:n th tth:l th tth:s th pph:t ph pph:th ph pph:d ph pph:dh ph pph:n ph pph:l ph pph:s ph gg:t g gg:th g gg:d g gg:dh g gg:n g gg:l g gg:s g jj:t j jj:th j jj:d j jj:dh j jj:n j jj:l j jj:s j \u1e0d\u1e0d:t \u1e0d \u1e0d\u1e0d:th \u1e0d \u1e0d\u1e0d:d \u1e0d \u1e0d\u1e0d:dh \u1e0d \u1e0d\u1e0d:n \u1e0d \u1e0d\u1e0d:l \u1e0d\n\u1e0d\u1e0d:s \u1e0d dd:t d dd:th d dd:d d dd:dh d dd:n d dd:l d dd:s d bb:t b bb:th b bb:d b bb:dh b bb:n b bb:l b bb:s b ggh:t gh ggh:th gh ggh:d gh ggh:dh gh ggh:n gh ggh:l gh ggh:s gh jjh:t jh jjh:th jh jjh:d jh jjh:dh jh jjh:n jh jjh:l jh jjh:s jh \u1e0d\u1e0dh:t \u1e0dh \u1e0d\u1e0dh:th \u1e0dh \u1e0d\u1e0dh:d \u1e0dh \u1e0d\u1e0dh:dh \u1e0dh \u1e0d\u1e0dh:n \u1e0dh \u1e0d\u1e0dh:l \u1e0dh \u1e0d\u1e0dh:s \u1e0dh ddh:t dh ddh:th dh ddh:d dh ddh:dh dh ddh:n dh ddh:l dh ddh:s dh bbh:t bh bbh:th bh\nbbh:d bh bbh:dh bh bbh:n bh bbh:l bh bbh:s bh yy:t y yy:th y yy:d y yy:dh y yy:n y yy:l y yy:s y rr:t r rr:th r rr:d r rr:dh r rr:n r rr:l r rr:s r \u1e37\u1e37:t \u1e37 \u1e37\u1e37:th \u1e37 \u1e37\u1e37:d \u1e37 \u1e37\u1e37:dh \u1e37 \u1e37\u1e37:n \u1e37 \u1e37\u1e37:l \u1e37 \u1e37\u1e37:s \u1e37 vv:t v vv:th v vv:d v vv:dh v vv:n v vv:l v vv:s v hh:t h hh:th h hh:d h hh:dh h hh:n h hh:l h hh:s h ss:t s ss:th s ss:d s ss:dh s ss:n s\nss:l s ss:s s \u1e45\u1e45:t \u1e45 \u1e45\u1e45:th \u1e45 \u1e45\u1e45:d \u1e45 \u1e45\u1e45:dh \u1e45 \u1e45\u1e45:n \u1e45 \u1e45\u1e45:l \u1e45 \u1e45\u1e45:s \u1e45 \u00f1\u00f1:t \u00f1 \u00f1\u00f1:th \u00f1 \u00f1\u00f1:d \u00f1 \u00f1\u00f1:dh \u00f1 \u00f1\u00f1:n \u00f1 \u00f1\u00f1:l \u00f1 \u00f1\u00f1:s \u00f1 \u1e47\u1e47:t \u1e47 \u1e47\u1e47:th \u1e47 \u1e47\u1e47:d \u1e47 \u1e47\u1e47:dh \u1e47 \u1e47\u1e47:n \u1e47 \u1e47\u1e47:l \u1e47 \u1e47\u1e47:s \u1e47 nn:t n nn:th n nn:d n nn:dh n nn:n n nn:l n nn:s n mm:t m mm:th m mm:d m mm:dh m mm:n m mm:l m mm:s m \u1e43\u1e43:t \u1e43 \u1e43\u1e43:th \u1e43 \u1e43\u1e43:d \u1e43 \u1e43\u1e43:dh \u1e43 \u1e43\u1e43:n \u1e43 \u1e43\u1e43:l \u1e43 \u1e43\u1e43:s \u1e43\n29\n13.2. Simple program\nimport java.util.List;\nimport lu.cl.dictclient.DictWord; import de.unitrier.daalft.pali.PaliNLP; import de.unitrier.daalft.pali.phonology.element.SplitResult;\npublic class Demo {\npublic void run (String word) {\nString stem = PaliNLP.stem(word); List<DictWord> lemmata = PaliNLP.lemmatize(word); List<DictWord> analyses = PaliNLP.analyze(word); System.out.println(\"The stem of \" + word + \" is \" + stem); for (DictWord lemma : lemmata)\nSystem.out.println(\"A possible lemma of \" + word + \" is \" +\nlemma.toString());\nfor (DictWord analysis : analyses)\nSystem.out.println(\"A possible analysis of \" + word + \" is \" +\nanalysis.toString());\n}\npublic void generate (String lemma) {\n// expects word class as second parameter List<DictWord> forms = PaliNLP.generate(lemma, null); for (DictWord form : forms) {\nSystem.out.println(\"A possible form of \" + lemma + \" is \" +\nform.toString());\n}\n}\npublic void split (String word) {\n// expects splitting depth as second parameter // only depth 1 supported at the moment List<SplitResult> split = PaliNLP.split(word, 1); for (SplitResult result : split) {\nSystem.out.println(\"A possible split of \" + word + \" is \" +\nresult.toString());\n}\n}\npublic void merge (String... words) {\nList<String> mergedWords = PaliNLP.merge(words); for (String mergedWord : mergedWords) {\nSystem.out.println(\"A possible merge is \" + mergedWord);\n}\n}\npublic static void main (String[] args) {\nDemo demo = new Demo(); demo.run(\"gavassa\"); demo.generate(\"go\"); demo.split(\"sakideva\"); demo.merge(\"saki\", \"eva\");\n}\n}\n30\n13.3. User manual\n1. Getting started\nImportant: Please make sure that you have Java 7 installed before using this program. The program will not work with a prior version of Java.\nYou should have received a CD-ROM containing the Pali NLP system. Copy the contents of this CD-ROM to your computer (for example to C:\\PaliNLP). You can skip the next step (2. Compiling source code).\nAlternatively, the source code can be cloned from https://github.com/daalft/PaliNLP/. For help on how to clone a repository from github, please see the github manual.\n2. Compiling source code\nBefore compiling, make sure that you have the Java Software Development Kit (Java SDK) 7 installed. Before compiling, make sure that you have Apache Ant installed. For help on how to install Apache Ant, please see the Apache Ant manual.\nOpen a new shell window/command-line window and navigate to the path containing the\nsource code (the path containing the src and data folders). Run the command\nant\nAfter a successful build, run the command\nant jar\nIf the operation succeeds, you will find a jar file and four platform dependent scripts\n(PaliConsole.bat, PaliGUI.bat, PaliConsole.sh, PaliGUI.sh).\n3. Console mode\nTo start the console mode, double-click the PaliConsole script that corresponds to your system. On Unix systems, launch PaliConsole.sh; on Windows systems, launch PaliConsole.bat. If neither of these works on your system, set the classpath to include the created jar and the packages\n31\n2013-01-19_LibDictionaryClientRecompiled.jar Jackson-annotations-2.2.3.jar Jackson-core-2.2.3.jar Jackson-databind-2.2.3.jar from the folder data/extlib/. Launch the program by calling de.unitrier.daalft.pali.PaliConsole.\nFor example, if we are on the path containing the created jar file PaliNLP-20XXYYZZ, on a Windows system (spaces are indicated for clarity):\njava[SPACE]-cp[SPACE].\\PaliNLP20XXYYZZ.jar;.\\data\\extlib\\jackson-databind2.2.3.jar;.\\data\\extlib\\jackson-core2.2.3.jar;.\\data\\extlib\\jackson-annotations2.2.3.jar;.\\data\\extlib\\2014-0119_LibDictionaryClientRecompiled.jar[SPACE]de.unitrier.daalft.pal i.PaliConsole\nwith 20XXYYZZ corresponding to the timestamp of the jar (for example PaliNLP-20140126.jar).\nThe PaliNLP console should open. The console mode has been specifically written to provide access to the PaliNLP system via the console.\nEach command in the console should be followed by [ENTER]. For example, the instruction\nType\nlemma\nmeans that you should enter the word \u2018lemma\u2019, then press the [ENTER] key.\n4. GUI mode\nTo start the graphical user interface (GUI) mode, double-click the PaliGUI script that corresponds to your system. On Unix systems, launch PaliGUI.sh; on Windows systems, launch PaliGUI.bat. If neither of these works on your system, set the classpath to include the created jar and the packages\n2013-01-19_LibDictionaryClientRecompiled.jar Jackson-annotations-2.2.3.jar Jackson-core-2.2.3.jar Jackson-databind-2.2.3.jar\n32\nfrom the folder data/extlib/. Launch the program by calling de.unitrier.daalft.pali.PaliGUI. For example, if we are on the path containing the created jar file PaliNLP-20XXYYZZ, on a Windows system (spaces are indicated for clarity):\njava[SPACE]-cp[SPACE].\\PaliNLP20XXYYZZ.jar;.\\data\\extlib\\jackson-databind2.2.3.jar;.\\data\\extlib\\jackson-core2.2.3.jar;.\\data\\extlib\\jackson-annotations2.2.3.jar;.\\data\\extlib\\2014-0119_LibDictionaryClientRecompiled.jar[SPACE]de.unitrier.daalft.pal i.PaliGUI\nwith 20XXYYZZ corresponding to the timestamp of the jar (for example PaliNLP-20140126.jar).\nThe PaliNLP GUI window should open. The GUI mode has been specifically written to provide access to the PaliNLP system via a graphical user interface.\n5. Lemmatizer"}, {"heading": "In the console:", "text": "If you have activated any mode other than the Analyzer, type\nchmod\nto return to the mode selection.\nType\nlemma\nEnter a word, for example:\nbuddhassa\nYou will get a list of possible lemmata. You can also specify the word class of a word by\nappending it to the word, using a colon as separator:\nbuddhassa:noun"}, {"heading": "In the GUI:", "text": "Enter one or more words in the input field. Multiple words should be separated by spaces.\nClick the Lemmatize button. You will get a list of possible lemmata. You can also specify the word class of the word(s) by appending it to the word(s), using a colon as separator.\n33\n6. Stemmer"}, {"heading": "In the console:", "text": "If you have activated any mode other than the Stemmer, type\nchmod\nto return to the mode selection.\nType\nstem\nEnter a word, for example:\ngavena\nYou will get the word stem. You can also specify the word class of a word by appending it to the word, using a colon as separator. However, this information does not influence the result."}, {"heading": "In the GUI:", "text": "Enter one or more words in the input field. Multiple words should be separated by spaces.\nClick the Stem button. You will get the word stem. You can also specify the word class of the word(s) by appending it to the word(s), using a colon as separator. However, this information does not influence the result.\n7. Analyzer"}, {"heading": "In the console:", "text": "If you have activated any mode other than the Analyzer, type\nchmod\nto return to the mode selection.\nType\nana\nEnter a word, for example:\ngavena\n34\nYou will get a list of possible analyses."}, {"heading": "In the GUI:", "text": "Enter one or more words in the input field. Multiple words should be separated by spaces.\nClick the Analyze button. You will get possible analyses. You can also specify the word class of the word(s) by appending it to the word(s), using a colon as separator.\n8. Generator\nPlease note that generating word forms may take some time to complete."}, {"heading": "In the console:", "text": "If you have activated any mode other than the Generator, type\nchmod\nto return to the mode selection.\nType\ngen\nEnter a word, for example:\ngo\nYou will get a list of morphological forms."}, {"heading": "In the GUI:", "text": "Enter one or more words in the input field. Multiple words should be separated by spaces.\nClick the Generate button. You will get a list of morphological forms. You can also specify the word class of the word(s) by appending it to the word(s), using a colon as separator.\n35\n9. Sandhi\n9.4. Splitting\nPlease not that splitting can take some time to complete."}, {"heading": "In the console:", "text": "If you have activated any mode other than the Sandhi Splitter, type\nchmod\nto return to the mode selection.\nType\nss\nEnter a word, for example:\nsakideva\nYou will get a list of possible splits."}, {"heading": "In the GUI:", "text": "Enter one or more words in the input field. Multiple words should be separated by spaces.\nClick the Split button. You will get a list of possible splits. You can also specify the word class of the word(s) by appending it to the word(s), using a colon as separator. However, this information does not influence the result.\n9.5. Merging"}, {"heading": "In the console:", "text": "If you have activated any mode other than the Sandhi Merger, type\nchmod\nto return to the mode selection.\nType\nsm\nEnter two or more words separated by spaces, for example:\n36\nsaki eva\nYou will get a list of possible merges. Please not that it is not possible to specify word classes\nwhen using the Sandhi Merger."}, {"heading": "In the GUI:", "text": "Enter two or more words in the input field. Multiple words should be separated by spaces.\nClick the Merge button. You will get a list of possible merges. Please not that it is not possible to specify word classes when using the Sandhi Merger.\n37\nERKL\u00c4RUNG ZUR BACHELORARBEIT\nHiermit erkl\u00e4re ich, dass ich die Bachelorarbeit selbst\u00e4ndig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt und die aus fremden Quellen direkt oder indirekt \u00fcbernommenen Gedanken als solche kenntlich gemacht habe. Die Arbeit habe ich bisher keinem anderen Pr\u00fcfungsamt in gleicher oder vergleichbarer Form vorgelegt. Sie wurde bisher nicht ver\u00f6ffentlicht.\n__________________________________ ____________________________________ Datum Unterschrift"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "This work describes a system that performs morphological analysis and generation of Pali words. The system works with regular inflectional paradigms and a lexical database. The generator is used to build a collection of inflected and derived words, which in turn is used by the analyzer. Generating and storing morphological forms along with the corresponding morphological information allows for efficient and simple look up by the analyzer. Indeed, by looking up a word and extracting the attached morphological information, the analyzer does not have to compute this information. As we must, however, assume the lexical database to be incomplete, the system can also work without the dictionary component, using a rule-based approach.", "creator": "Microsoft\u00ae Word 2010"}}}