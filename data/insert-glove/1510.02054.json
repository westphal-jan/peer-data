{"id": "1510.02054", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations", "abstract": "Deep talhelm CCA is a wier recently proposed deep elilan neural network swapo extension retargeting to the traditional canonical 99.999 correlation analysis (CCA ), and power-base has been successful yonge for municipal multi - view lightkeepers representation deflate learning dcm in several consulates domains. atenulf However, stochastic optimization pentangelo of love-struck the deep gums CCA objective lyoto is sarli not straightforward, kathu because cherubini it does debden not laverton decouple chiki over cattolica training wargrave examples. Previous gudalur optimizers for immortals deep sujatmiko CCA are belaga either casta\u00f1eda batch - 10-cent based kevorkian algorithms 717-646-4822 or zhiyuan stochastic optimization using large minibatches, luepnitz which abarca can have high glorantha memory consumption. In this sacramentary paper, we tackle ahirudin the problem of teniamos stochastic optimization boulogne-billancourt for tajiks deep iredell CCA apds with small minibatches, garson based on cassandras an iterative solution 60.58 to bisto the rh\u00f6n CCA factfile objective, gurongi and playback show embody that zahar we 20-degree can 1h41 achieve as koho good performance tri\u00e2ngulo as skiers previous optimizers adivasis and thus alleviate mahira the istrati memory girolami requirement.", "histories": [["v1", "Wed, 7 Oct 2015 18:32:41 GMT  (112kb)", "http://arxiv.org/abs/1510.02054v1", "in 2015 Annual Allerton Conference on Communication, Control and Computing"]], "COMMENTS": "in 2015 Annual Allerton Conference on Communication, Control and Computing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "raman arora", "karen livescu", "nathan srebro"], "accepted": false, "id": "1510.02054"}, "pdf": {"name": "1510.02054.pdf", "metadata": {"source": "CRF", "title": "Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations", "authors": ["Weiran Wang"], "emails": ["weiranwang@ttic.edu,", "klivescu@ttic.edu,", "nati@ttic.edu,", "arora@cs.jhu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n02 05\n4v 1\n[ cs\n.L G\n] 7\nO ct\n2 01\nI. INTRODUCTION\nStochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5]. SGD is particularly well-suited for large-scale machine learning problems because it is extremely simple and easy to implement, it often achieves better generalization (test) performance (which is the focus of machine learning research) than sophisticated batch algorithms, and it usually achieves large error reduction very quickly in a small number of passes over the training set [6]. One intuitive explanation for the empirical success of stochastic gradient descent for large data is that it makes better use of data redundancy, with an extreme example given by [2]: If the training set consists of 10 copies of the same set of examples, then computing an estimate of the gradient over one single copy is 10 times more efficient than computing the full gradient over the entire training set, while achieving the same optimization progress in the following gradient descent step.\nAt the same time, \u201cmulti-view\u201d data are becoming increasingly available, and methods based on canonical correlation analysis (CCA) [7] that use such data to learn representations (features) form an active research area. The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such\nThis research was supported by the NSF grants IIS-1546482 and IIS1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.\nas words + context [19] or document text + text of inbound hyperlinks [20]. The presence of multiple information sources presents an opportunity to learn better representations (features) by analyzing multiple views simultaneously. Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].\nIn contrast with most DNN-based methods, the objective of deep CCA couples together all of the training examples due to its whitening constraint, making stochastic optimization challenging. Previous optimizers for this model are batch-based, e.g., limited-memory BFGS (L-BFGS) [27] as in [21], or stochastic optimization with large minibatches [22], because it is difficult to obtain an accurate estimate of the gradient with a small subset of the training examples (again due to the whitening constraint). As a result, these approaches have high memory complexity and may not be practical for large DNN models with hundreds of millions of weight parameters (common with web-scale data [28]), or if one would like to run the training procedure on GPUs which are equipped with faster but smaller (more expensive) memory than CPUs. In such cases there is not enough memory to save all intermediate hidden activations of the batch/large minibatch used in error backpropagation.\nIn this paper, we tackle this problem with two key ideas. First, we reformulate the CCA solution with orthogonal iterations, and embed the DNN parameter training in the orthogonal iterations with a nonlinear least squares regression objective, which naturally decouples over training examples. Second, we use adaptive estimates of the covariances used by the CCA whitening constraints and carry out whitening only for the minibatch used at each step to obtain training signals for the DNNs. This results in a stochastic optimization algorithm that can operate on small minibatches and thus consume little memory. Empirically, the new stochastic optimization algorithm performs as well as previous optimizers in terms of convergence speed, even when using small minibatches with which the previous stochastic approach makes no training progress.\nIn the following sections, we briefly introduce deep CCA\nand discuss the difficulties in training it (Section II); motivate and propose our new algorithm (Section III); describe related work (Section IV); and present experimental results comparing different optimizers (Section V)."}, {"heading": "II. DEEP CCA", "text": "Notation In the multi-view feature learning setting, we have access to paired observations from two views, denoted (x1,y1), . . . , (xN ,yN ), where N is the training set size, xi \u2208 R\nDx and yi \u2208 RDy for i = 1, . . . , N . We also denote the data matrices for View 1 and View 2 X = [x1, . . . ,xN ] and Y = [y1, . . . ,yN ], respectively. We use bold-face letters, e.g. f , to denote mappings implemented by DNNs, with a corresponding set of learnable parameters, denoted, e.g., Wf . The dimensionality of the learned features is denoted L.\nDeep CCA (DCCA) [21] extends (linear) CCA [7] by extracting dx- and dy-dimensional nonlinear features with two DNNs f and g for views 1 and 2 respectively, such that the canonical correlation (measured by CCA) between the DNN outputs is maximized, as illustrated in Fig. 1. The goal of the final CCA is to find L \u2264 min(dx, dy) pairs of linear projection vectors U \u2208 Rdx\u00d7L and V \u2208 Rdy\u00d7L such that the projections of each view (a.k.a. canonical variables, [7]) are maximally correlated with their counterparts in the other view, constrained such that the dimensions in the representation are uncorrelated with each other. Formally, the DCCA objective can be written as1\nmax Wf ,Wg,U,V\ntr ( U\u22a4FG\u22a4V )\n(1)\ns.t. U\u22a4FF\u22a4U = V\u22a4GG\u22a4V = I,\nwhere F = f(X) = [f(x1), . . . , f(xN )] \u2208 Rdx\u00d7N and G = g(Y) = [g(y1), . . . ,g(yN )] \u2208 R\ndy\u00d7N . We assume that F and G are centered at the origin for notational simplicity; if they are not, we can center them as a preprocessing operation. Notice that if we use the original input data without further feature extraction, i.e. F = X and G = Y, then we recover the CCA objective. In DCCA, the final features (projections) are\nf\u0303(x) = U\u22a4f(x) and g\u0303(y) = V\u22a4g(y). (2)\n1In this paper, we use the scaled covariance matrices (scaled by N ) so that the dimensions of the projection are orthonormal and comply with the custom of orthogonal iterations.\nWe observe that the last CCA step with linear projection mappings U and V can be considered as adding a linear layer on top of the feature extraction networks f and g respectively. In the following, we sometimes refer to the concatenated networks f\u0303 and g\u0303 as defined in (2), with Wf\u0303 = {Wf ,U} and Wg\u0303 = {Wg,V}. 2\nLet \u03a3fg = FG\u22a4, \u03a3ff = FF\u22a4 and \u03a3gg = GG\u22a4 be the (scaled) cross- and auto-covariance matrices of the featuremapped data in the two views. It is well-known that, when f and g are fixed, the last CCA step in (1) has a closed form solution as follows. Define \u03a3\u0303fg = \u03a3 \u2212 1 2 ff \u03a3fg\u03a3 \u2212 1 2\ngg , and let \u03a3\u0303fg = U\u0303\u039bV\u0303\n\u22a4 be its rank-L singular value decomposition (SVD), where \u039b contains the singular values \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3L \u2265 0 on its diagonal. Then the optimum of (1) is achieved by (U,V) = (\u03a3 \u2212 1 2\nff U\u0303,\u03a3 \u2212 1 2 gg V\u0303), and the optimal\nobjective value (the total canonical correlation) is \u2211L\nj=1 \u03c3j . By switching max(\u00b7) with \u2212min\u2212(\u00b7), and adding 1/2 times the constraints, it is straightforward to show that (1) is equivalent to the following:\nmin Wf ,Wg,U,V\n1\n2\n\u2225 \u2225U\u22a4F\u2212V\u22a4G \u2225 \u2225 2\nF (3)\ns.t. (U\u22a4F)(U\u22a4F)\u22a4 = (V\u22a4G)(V\u22a4G)\u22a4 = I.\nIn other words, CCA minimizes the squared difference between the projections of the two views, subject to the whitening constraints. This alternative formulation of CCA will also shed light on our proposed algorithm for DCCA.\nThe DCCA objective (1) differs from typical DNN regression or classification training objectives. Typically, the objectives are unconstrained and can be written as the expectation (or sum) of error functions (e.g., squared loss or cross entropy) incurred at each training example. This property naturally suggests stochastic gradient descent (SGD) for optimization, where one iteratively generates random unbiased estimates of the gradient based on one or a few training examples (a minibatch) and takes a small step in the opposite direction. However, the objective in (1) can not be written as an unconstrained sum of errors. The difficulty lies in the fact that the training examples are coupled through the auto-covariance matrices (in the constraints), which can not be reliably estimated with only a small amount of data.\nWhen introducing deep CCA, [21] used the L-BFGS algorithm for optimization. To compute the gradients of the objective with respect to (Wf ,Wg), one first computes the gradients3 with respect to (F,G) as\n\u2202 \u2211L\nj=1 \u03c3j\n\u2202F = 2\u2206ffF+\u2206fgG, (4)\nwith \u2206ff = \u2212 1\n2 \u03a3\n\u22121/2 ff U\u0303\u039bU\u0303 \u22a4\u03a3\u22121/2ff\n\u2206fg = \u03a3 \u22121/2 ff U\u0303V\u0303 \u22a4\u03a3\u22121/2gg\n2In principle there is no need for the final linear layer; we could define DCCA such that the correlation objective and constraints are imposed on the final nonlinear layer. However, the linearity of the final layer is crucial for algorithmic implementations such as ours.\n3Technically we are computing subgradients as the \u201csum of singular values\u201d (trace norm) is not a differentiable function of the matrix.\nAlgorithm 1 CCA projections via alternating least squares. Input: Data matrices F \u2208 Rdx\u00d7N , G \u2208 Rdy\u00d7N . Initialization U\u03030 \u2208 Rdx\u00d7L s.t. U\u0303\u22a40 U\u03030 = I.\nA0 \u2190 U\u0303 \u22a4 0 \u03a3\n\u2212 1 2 ff F\nfor t = 1, 2, . . . , T do Bt \u2190 At\u22121G\u22a4 ( GG\u22a4 )\u22121 G\nBt \u2190 ( BtB \u22a4 t )\u2212 1 2 Bt At \u2190 BtF \u22a4 (FF\u22a4 )\u22121 F At \u2190 ( AtA \u22a4 t )\u2212 1 2 At\nend for Output: AT /BT are the CCA projections of view 1/2.\nwhere \u03a3\u0303fg = U\u0303\u039bV\u0303\u22a4 is the SVD of \u03a3\u0303fg as in the closedform solution to CCA, and \u2202\n\u2211L j=1 \u03c3j/\u2202G has an analogous\nexpression. One can then compute the gradients with respect to Wf and Wg via the standard backpropagation procedure [29]. From the gradient formulas, it is clear that the key to optimizing DCCA is the SVD of \u03a3\u0303fg; various nonlinear optimization techniques can be used here once the gradient is computed. In practice, however, batch optimization is undesirable for applications with large training sets or large DNN architectures, as each gradient step computed on the entire training set can be expensive in both memory and time.\nLater, it was observed by [22] that stochastic optimization still works well even for the DCCA objective, as long as larger minibatches are used to estimate the covariances and \u03a3\u0303fg when computing the gradient with (4). More precisely, the authors find that learning plateaus at a poor objective value if the minibatch is too small, but fast convergence and better generalization than batch algorithms can be obtained once the minibatch size is larger than some threshold, presumably because a large minibatch contains enough information to estimate the covariances and therefore the gradient accurately enough (the threshold of minibatch size varies for different datasets because they have different levels of data redundancy). Theoretically, the necessity of using large minibatches in this approach can also be established. Let the empirical estimate of \u03a3\u0303fg using a minibatch of n samples be \u03a3\u0302 (n) fg . It can be shown that the expectation of \u03a3\u0302 (n)\nfg does not equal the true \u03a3\u0303fg computed using the entire dataset, mainly due to the nonlinearities in the matrix inversion and multiplication operations in computing \u03a3\u0303fg, and the nonlinearity in the \u201csum of singular values\u201d (trace norm) of \u03a3\u0303fg; moreover, the spectral norm of the error \u2225 \u2225 \u2225 \u03a3\u0302 (n) fg \u2212 \u03a3\u0303fg \u2225 \u2225 \u2225 decays slowly as 1\u221a n\n. Consequently, the gradient estimated on a minibatch using (4) does not equal the true gradient of the objective in expectation, indicating that the stochastic approach of [22] does not qualify as a stochastic gradient descent method for the DCCA objective."}, {"heading": "III. OUR ALGORITHM", "text": ""}, {"heading": "A. An iterative solution to linear CCA", "text": "Our solution to (1) is inspired by the iterative solution for finding the linear CCA projections (U\u22a4F,V\u22a4G) for inputs\n(F,G), as shown in Algorithm 1. This algorithm computes the top-L singular vectors (U\u0303, V\u0303) of \u03a3\u0303fg via orthogonal iterations [30]. An essentially identical algorithm (named alternating least squares for reasons that will soon become evident) appears in [31, Algorithm 5.2] and according to the authors the idea goes back to J. Von Neumann. A similar algorithm is also recently used by [32, Algorithm 1] for large scale linear CCA with high-dimensional sparse inputs, although their algorithm does not implement the whitening operations At \u2190 ( AtA \u22a4 t )\u2212 1 2 At and Bt \u2190 ( BtB \u22a4 t )\u2212 1 2 Bt or they use the QR decomposition instead. The convergence of Algorithm 1 is characterized by the following theorem, which parallels [32, Theorem 1].\nTheorem 1: Let the singular values of \u03a3\u0303fg be\n\u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3L > \u03c3L+1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3min(dx,dy)\nand suppose U\u0303\u22a40 U\u0303 is nonsingular. Then the output (AT ,BT ) of Algorithm 1 converges to the CCA projections as T \u2192 \u221e.\nProof: We focus on showing that AT converges to the view 1 projection; the proof for BT is similar.\nFirst recall that \u03a3\u0303fg = U\u0303\u039bV\u0303\u22a4 is the rank-L SVD of\n\u03a3 \u2212 1 2 ff \u03a3fg\u03a3 \u2212 1 2 gg , and thus U\u0303 contains the top-L eigenvectors of \u03a3\u0303fg\u03a3\u0303 \u22a4 fg = U\u0303\u039b 2U\u0303\u22a4.\nSince the operation ( AA\u22a4 )\u2212 1\n2 A extracts an orthonormal basis of the row space of A, at iteration t we can write\nAt\u22121G \u22a4 (GG\u22a4 )\u22121 G = PtBt\nBtF \u22a4 (FF\u22a4 )\u22121 F = QtAt\nwhere Pt \u2208 RL\u00d7L and Qt \u2208 RL\u00d7L are nonsingular coefficient matrices (as the initialization U\u03030 is nonsingular) for representing the left-hand side matrices in their row space basis. Combining the above two equations gives the following recursion at iteration t:\nAt\u22121G \u22a4 (GG\u22a4 )\u22121 GF\u22a4 ( FF\u22a4 )\u22121 F = PtQtAt.\nBy induction, it can be shown that by the end of iteration t we have\nA0\n(\nG\u22a4 ( GG\u22a4 )\u22121 GF\u22a4 ( FF\u22a4 )\u22121 F )t\n= OtAt.\nwhere Ot = P1Q1 . . .PtQt \u2208 RL\u00d7L is nonsingular. Plugging in the definition of A0, this equation reduces to\nU\u0303\u22a40\n( \u03a3\u0303fg\u03a3\u0303 \u22a4 fg )t \u03a3 \u2212 1 2 ff F = OtAt. (5)\nIt is then clear that At can be written as\nAt = U\u0303 \u22a4 t \u03a3\n\u2212 1 2 ff F\nwith\nU\u0303t = ( \u03a3\u0303fg\u03a3\u0303 \u22a4 fg )t U\u03030O \u22121 t \u2208 R dx\u00d7L.\nAnd since At has orthonormal rows, we have\nI = AtA \u22a4 t = U\u0303 \u22a4 t \u03a3\n\u2212 1 2\nff (FF \u22a4)\u03a3\n\u2212 1 2\nff U\u0303t = U\u0303 \u22a4 t U\u0303t,\nindicating that U\u0303t has orthonormal columns.\nAs a result, we consider the algorithm as working implicitly in the space of {U\u0303t \u2208 Rdx\u00d7L, t = 0, . . . , T }, and have\n(\u03a3\u0303fg\u03a3\u0303 \u22a4 fg) T U\u03030 = OT U\u0303T . (6)\nFollowing the argument of [30, Theorem 8.2.2]) for orthogonal iterations, under the assumptions of our theorem, the column space of U\u0303T converges to that of U\u0303, the topL eigenvectors of \u03a3\u0303fg\u03a3\u0303 \u22a4 fg, with a linear convergence rate depending on the ratio \u03c3L+1/\u03c3L. In view of the relationship between U\u0303T and At, we conclude that AT converges to the view 1 CCA projection as T \u2192 \u221e.\nIt is interesting to note that, besides the whitening op-\nerations ( AtA \u22a4 t )\u2212 1 2 At, the other basic operations in each iteration of Algorithm 1 are of the form\nAt \u2190 BtF \u22a4 (FF\u22a4 )\u22121 F (7)\nwhich is solving a linear least squares (regression) problem with input F and target output Bt satisfying BtB\u22a4t = I, i.e.,\nmin Ut\n\u2225 \u2225U\u22a4t F\u2212Bt \u2225 \u2225 2 F .\nBy setting the gradient of this unconstrained objective to zero, we obtain Ut = (FF\u22a4)\u22121FB\u22a4t and so the optimal projection U\u22a4t F coincides with the update (7).\nFor [32], the advantage of the alternating least squares formulation over the exact solution to CCA is that it does not need to form the high-dimensional (nonsparse) matrix \u03a3\u0303fg; instead it directly operates on the projections, which are much smaller in size, and one can solve the least squares problems using iterative algorithms that require only sparse matrix-vector multiplications."}, {"heading": "B. Extension to DCCA", "text": "Our intuition for adapting Algorithm 1 to DCCA is as follows. During DCCA optimization, the DNN weights (Wf ,Wg) are updated frequently and thus the outputs (f(X),g(Y)), which are also the inputs to the last CCA step, also change upon each weight update. Therefore, the last CCA step needs to adapt to the fast evolving input data distribution. On the other hand, if we are updating the CCA weights (U,V) based on a small minibatch of data (as happens in stochastic optimization), it is intuitively wasteful to solve (U,V) to optimality rather than to make a simple update based on the minibatch. Moreover, the objective of this \u201csimple update\u201d can be used to derive a gradient estimate for (Wf ,Wg).\nIn view of Algorithm 1, it is a natural choice to embed the optimization of (f ,g) into the iterative solution to linear CCA. Instead of solving the regression problem F \u2192 Bt exactly with At \u2190 BtF\u22a4 ( FF\u22a4 )\u22121\nF, we try to solve the problem X \u2192 Bt on a minibatch with a gradient descent step on (Wf ,U) jointly (recall F = f(X) is a function of Wf ). Notice that this regression objective is unconstrained and decouples over training samples, so an unbiased gradient estimate for this problem can be easily derived through standard backpropagation using minibatches (however, this\ngradient estimate may not be unbiased for the original DCCA objective; see discussion in Section IV).\nThe less trivial part of Algorithm 1 to implement in\nDCCA is the whitening operation ( AtA \u22a4 t )\u2212 1 2 At, which needs At \u2208 RL\u00d7N , the projections of all training samples. We would like to avoid the exact computation of At as it requires feeding forward the entire training set X with the updated Wf\u0303 , and the computational cost of this operation is as high as (half of) the cost of evaluating the batch gradient (the latter requires both the forward and backward passes). We bypass this difficulty by noting that the only portion of At needed is the updated projection of the minibatch used in the subsequent view 2 regression problem X \u2192 At (corresponding to the step Bt+1 \u2190 AtG\u22a4 ( GG\u22a4 )\u22121 G in Algorithm 1). Therefore, if we have an estimate of the covariance \u03a3t\nf\u0303 f\u0303 := AtA \u22a4 t without feeding forward the entire\ntraining set, we can estimate the updated projection for this minibatch only. Specifically, we estimate this quantity by4\n\u03a3t f\u0303 f\u0303 \u2190 \u03c1\u03a3t\u22121 f\u0303 f\u0303 + (1\u2212 \u03c1) N |b| f\u0303 (Xb)f\u0303 (Xb) \u22a4, (8)\nwhere \u03c1 \u2208 [0, 1], Xb denotes a minibatch of data with index set b, and |b| denotes the size (number of samples) of this minibatch. The time constant \u03c1 controls how much the previous covariance estimate is kept in the update; a larger \u03c1 indicates forgetting the \u201cmemory\u201d more slowly. Assuming that the parameters do not change much from time t\u2212 1 to t, then \u03a3t\u22121\nf\u0303 f\u0303 will be close to \u03a3t f\u0303 f\u0303 , and incorporating it helps\nto reduce the variance from the term f\u0303(Xb)f\u0303 (Xb)\u22a4 when |b| \u226a N . The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection. We note that the memory cost of \u03a3t\nf\u0303 f\u0303 \u2208 RL\u00d7L\nis small as we look for low-dimensional projections (small L) in practice. These advantages validate our choice of whitening operations over the more commonly used QR decomposition used by [32].\nWe give the resulting nonlinear orthogonal iterations procedure (NOI) for DCCA in Algorithm 2. Now adaptive whitening is used to obtain suitable target outputs of the regression problems for computing derivatives (\u2202Wf\u0303 , \u2202Wg\u0303), and we no longer maintain the whitened projections of the entire training set at each iteration. Therefore, by the end of the algorithm, (f\u0303 (X), g\u0303(Y)) may not satisfy the whitening constraints of (1). One may use an additional CCA step on (f\u0303 (X), g\u0303(Y)) to obtain a feasible solution of the original problem if desired, and this amounts to linear transforms in R\nL which do not change the canonical correlations between the projections for both the training and test sets. In practice, we adaptively estimate the mean of f\u0303(X) and g\u0303(Y) with an update formula similar to that of (8) and center the samples accordingly before estimating the covariances and computing the target outputs. We also use momentum in\n4We add a small value \u01eb > 0 to the diagonal of the covariance estimates in our implementation for numerical stability.\nAlgorithm 2 Nonlinear orthogonal iterations (NOI) for DCCA. Input: Data matrix X \u2208 RDx\u00d7N , Y \u2208 RDy\u00d7N . Initializa-\ntion (Wf\u0303 ,Wg\u0303), time constant \u03c1, learning rate \u03b7. Randomly choose a minibatch (Xb0 ,Yb0) \u03a3f\u0303 f\u0303 \u2190 N b0 \u2211 i\u2208b0 f\u0303 (xi)f\u0303 (xi) \u22a4, \u03a3g\u0303g\u0303 \u2190 N b0 \u2211 i\u2208b0 g\u0303(yi)g\u0303(yi) \u22a4 for t = 1, 2, . . . , T do Randomly choose a minibatch (Xbt ,Ybt) \u03a3f\u0303 f\u0303 \u2190 \u03c1\u03a3f\u0303 f\u0303 + (1\u2212 \u03c1) N |bt| \u2211 i\u2208bt f\u0303(xi)f\u0303 (xi) \u22a4\n\u03a3g\u0303g\u0303 \u2190 \u03c1\u03a3g\u0303g\u0303 + (1 \u2212 \u03c1) N |bt|\n\u2211 i\u2208bt g\u0303(yi)g\u0303(yi) \u22a4\nCompute the gradient \u2202Wf\u0303 of the objective\nmin W\nf\u0303\n1\n|bt|\n\u2211\ni\u2208bt\n\u2225 \u2225 \u2225 f\u0303(xi)\u2212\u03a3 \u2212 1 2 g\u0303g\u0303 g\u0303(yi) \u2225 \u2225 \u2225\n2\nCompute the gradient \u2202Wg\u0303 of the objective\nmin Wg\u0303\n1\n|bt|\n\u2211\ni\u2208bt\n\u2225 \u2225 \u2225 g\u0303(yi)\u2212\u03a3 \u2212 1 2\nf\u0303 f\u0303 f\u0303(xi)\n\u2225 \u2225 \u2225\n2\nWf\u0303 \u2190 Wf\u0303 \u2212 \u03b7\u2202Wf\u0303 , Wg\u0303 \u2190 Wg\u0303 \u2212 \u03b7\u2202Wg\u0303. end for\nOutput: The updated (Wf\u0303 ,Wg\u0303).\nthe stochastic gradient steps for the nonlinear least squares problems as is commonly used in the deep learning community [34]. Overall, Algorithm 2 is intuitively quite simple: It alternates between adaptive covariance estimation/whitening and stochastic gradient steps over (a stochastic version of) the least squares objectives, without any involved gradient computation."}, {"heading": "IV. RELATED WORK", "text": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46]. However, as pointed out by [42], the CCA objective is more challenging due to the whitening constraints.\nRecently, [38] proposed an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the whitening constraints. However, the goal of their algorithm is anomaly detection rather than optimizing the canonical correlation objective for a given dataset. Based on the alternating least squares formulation of CCA (Algorithm 1), [32] propose an iterative solution of CCA for very high-dimensional and sparse input features, and the key idea is to solve the high dimensional least squares problems with randomized PCA and (batch) gradient descent.\nUpon the submission of this paper, we have become aware of the very recent publication of [47], which extends [32] by solving the linear least squares problems with (stochastic) gradient descent. We notice that a specical case of our algorithm (\u03c1 = 0) is equivalent to theirs for linear CCA. To see this, we give the linear CCA version of our algorithm\nAlgorithm 3 CCA via gradient descent over least squares. Input: Data matrix F \u2208 Rdx\u00d7N , G \u2208 Rdy\u00d7N . Initialization u0 \u2208 R\ndx , v0 \u2208 Rdy . Learning rate \u03b7. for t = 1, 2, . . . , T do ut \u2190 ut\u22121 \u2212 \u03b7F(F\u22a4ut\u22121 \u2212 1\n\u2016v\u22a4t\u22121G\u2016 G\u22a4vt\u22121)\nvt \u2190 vt\u22121 \u2212 \u03b7G(G\u22a4vt\u22121 \u2212 1 \u2016u\u22a4t\u22121F\u2016 F\u22a4ut\u22121)\nend for u \u2190 uT\n\u2016u\u22a4T F\u2016 , v \u2190 vT\n\u2016v\u22a4T G\u2016 Output: u/v are the CCA directions of view 1/2.\n(for a one-dimensional projection, to be consistent with the notation of [47]) in Algorithm 3, where we take a batch gradient descent step over the least squares objectives in each iteration. This algorithm is equivalent to Algorithm 3 of [47].5 Though intuitively very simple, the analysis of this algorithm is challenging. In [47] it is shown that the solution to the CCA objective is a fixed point of this algorithm, but no global convergence property is given. We also notice that the gradients used in this algorithm are derived from the alternating least squares problems\nmin u\n\u2225 \u2225 \u2225 \u2225 u\u22a4F\u2212 v\u22a4G\n\u2016v\u22a4G\u2016\n\u2225 \u2225 \u2225 \u2225\n2\nF\nand min v\n\u2225 \u2225 \u2225 \u2225 v\u22a4G\u2212 u\u22a4F\n\u2016u\u22a4F\u2016\n\u2225 \u2225 \u2225 \u2225\n2\nF\n,\nwhile the true CCA objective can be written as\nmin u,v\n\u2225 \u2225 \u2225 \u2225 u\u22a4F\n\u2016u\u22a4F\u2016 \u2212\nv\u22a4G\n\u2016v\u22a4G\u2016\n\u2225 \u2225 \u2225 \u2225\n2\nF\n.\nThis shows that Algorithm 3 is not implementing gradient descent over the CCA objective.\nWhen extending Algorithm 3 to stochastic optimization, we observe the key differences between their algorithm and ours as follows. Due to the evolving (Wf ,Wg), the last CCA step in the DCCA model is dealing with different (f(X),g(Y)) and covariance structures in different iterates, even though the original inputs (X,Y) are the same; this motivates the adaptive estimate of covariances in (8). In the whitening steps of [47], however, the covariances are estimated using only the current minibatch at each iterate, without consideration of the remaining training samples or previous estimates, which corresponds to \u03c1 \u2192 0 in our estimate. [47] also suggests using a minibatch size of the order O(L), the dimensionality of the covariance matrices to be estimated, in order to obtain a high-accuracy estimate for whitening. As we will show in the experiments, in both CCA and DCCA, it is important to incorporate the previous covariance estimates (\u03c1 \u2192 1) at each step to reduce the variance, especially when small minibatches are used. Based on the above analysis for batch gradient descent, solving the least squares problem with stochastic gradient descent is\n5Although Algorithm 3 of [47] maintains two copies\u2014the normalized and the unnormalized versions\u2014of the weight parameters, we observe that the sole purpose of the normalized version in the intermediate iterations is to provide whitened target output for the least squares problems; our version of the algorithm eliminates this copy and the normalized version can be retrieved by a whitening step at the end.\nnot implementing stochastic gradient descent over the CCA objective. Nonetheless, as shown in the experiments, this stochastic approach works remarkably well and can match the performance of batch optimization, for both linear and nonlinear CCA, and is thus worth careful analysis.\nFinally, we remark that other possible approaches for solving (1) exist. Since the difficulty lies in the whitening constraints, one can relax the constraints and solve the Lagrangian formulation repeatedly with updated Lagrangian multipliers, as done by [25]; or one can introduce auxiliary variables and apply the quadratic penalty method [48], as done by [49]. The advantage of such approaches is that there exists no coupling of all training samples when optimizing the primal variables (the DNN weight parameters) and thus one can easily apply SGD there, but one also needs to deal with the Lagrange multipliers or to set a schedule for the quadratic penalty parameter (which is non-trivial) and alternately optimize over two sets of variables repeatedly in order to obtain a solution of the original constrained problem."}, {"heading": "V. EXPERIMENTS", "text": ""}, {"heading": "A. Experimental setup", "text": "We now demonstrate the NOI algorithm on the two realworld datasets used by [21] when introducing DCCA. The first dataset is a subset of the University of Wisconsin XRay Microbeam corpus [50], which consists of simultaneously recorded acoustic and articulatory measurements during speech. Following [21], [22], the acoustic view inputs are 39D Mel-frequency cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2018JW11\u2019. The second dataset consists of left/right halves of the images in the MNIST dataset [51], and so the input of each view consists of 28 \u00d7 14 grayscale images. We do not tune neural network architectures as it is out of the scope of this paper. Instead, we use DNN architectures similar to those used by [21] with ReLU activations [52], and we achieve better generalization performance with these architectures mainly due to better optimization. The statistics of each dataset and the chosen DNN architectures (widths of input layer-hidden layers-output layer) are given in Table I. The projection dimensionality L is set to 112/50 for JW11/MNIST respectively as in [21]; these are also the maximum possible total canonical correlations for the two datasets.\nWe compare three optimization approaches: full batch optimization by L-BFGS [21], using the implementation of [53] which includes a good line-search procedure; stochastic\noptimization with large minibatches [22], denoted STOL; and our algorithm, denoted NOI. We create training/tuning/test splits for each dataset and measure the total canonical correlations on the test sets (measured by linear CCA on the projections) for different optimization methods. Hyperparameters of each algorithm, including \u03c1 for NOI, minibatch size n = |b1| = |b2| , . . . , learning rate \u03b7 and momentum \u00b5 for both STOL and NOI, are chosen by grid search on the tuning set. All methods use the same random initialization for DNN weight parameters. We set the maximum number of iterations to 300 for L-BFGS and number of epochs (one pass over the training set) to 50 for STOL and NOI."}, {"heading": "B. Effect of minibatch size n", "text": "In the first set of experiments, we vary the minibatch size n of NOI over {10, 20, 50, 100}, while tuning \u03c1, \u03b7 and \u00b5. Learning curves (objective value vs. number of epochs) on the tuning set for each n with the corresponding optimal hyperparameters are shown in Fig. 2. For comparison, we also show the learning curves of STOL with n = 100 and n = 500, while \u03b7 and \u00b5 are also tuned by grid search. We observe that STOL performs very well at n = 500 (with the performance on MNIST being somewhat better due to higher data redundancy), but it can not achieve much progress in the objective over the random initialization with n = 100, for the reasons described earlier. In contrast, NOI achieves very competitive performance with various small minibatch sizes, with fast improvement in objective during the first few iterations, although larger n tends to achieve slightly higher correlation on tuning/test sets eventually. Total canonical correlations on the test sets are given in Table II, showing that we achieve better results than [21] with similar DNN architectures."}, {"heading": "C. Effect of time constant \u03c1", "text": "In the second set of experiments, we demonstrate the importance of \u03c1 in NOI for different minibatch sizes. The total canonical correlations achieved by NOI on the tuning set for \u03c1 = {0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.99, 0.999, 0.9999} are shown in Fig. 3, while other hyper-parameters are set to their optimal values. We confirm that for relatively large n, NOI works reasonably well with \u03c1 = 0 (so we are using the same covariance estimate/whitening as [47]). But also as expected, when n is small, it is beneficial to incorporate the\nTABLE II TOTAL TEST SET CANONICAL CORRELATION OBTAINED BY DIFFERENT ALGORITHMS.\ndataset L-BFGS STOL NOI\nn = 100 n = 500 n = 10 n = 20 n = 50 n = 100\nJW11 78.7 33.0 86.7 83.6 86.9 87.9 89.1 MNIST 47.0 26.1 47.0 45.9 46.4 46.4 46.4\nprevious estimate of the covariance because the covariance information contained in each small minibatch is noisy. Also, as \u03c1 becomes too close to 1, the covariance estimates are not adapted to the DNN outputs and the performance of NOI degrades. Moreover, we observe that the optimal \u03c1 value seems different for each n."}, {"heading": "D. Pure stochastic optimization for CCA", "text": "Finally, we carry out pure stochastic optimization (n = 1) for linear CCA on the MNIST dataset. Notice that linear CCA is a special case of DCCA with (f\u0303 , g\u0303) both being single-layer linear networks (although we have used small weight-decay terms for the weights, leading to a slightly different objective than that of CCA). Total canonical correlations achieved by STOL with n = 500 and by NOI (50 training epochs) on the training set with different \u03c1 values are shown in Fig. 4. The objective of the random initialization and the closed-form solution (by SVD) are also shown for comparison. NOI could not improve over the random initialization without memory (\u03c1 = 0, corresponding to the\nalgorithm of [47]), but gets very close to the optimal solution and matches the objective obtained by the previous large minibatch approach when \u03c1 \u2192 1. This result demonstrates the importance of our adaptive estimate (8) also for CCA."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we have proposed a stochastic optimization algorithm NOI for training DCCA which updates the DNN weights based on small minibatches and performs competitively to previous optimizers.\nOne direction for future work is to better understand the convergence properties of NOI, which presents several difficulties. First, we note that convergence of the alternating least squares formulation of CCA (Algorithm 1, or rather orthogonal iterations) is usually stated as the angle between the estimated subspace and the ground-truth subspace converging to zero. In the stochastic optimization setting, we need to relate this measure of progress (or some other measure) to the nonlinear least squares problems we are trying to solve in the NOI iterations. As discussed in Section IV, even the convergence of the linear CCA version of NOI with batch gradient descent is not well understood [47]. Second, the use of memory in estimating covariances (8) complicates the analysis and ideally we would like to come up with ways of determining the time constant \u03c1.\nWe have also tried using the same form of adaptive covariance estimates in both views for the STOL approach for computing the gradients (4), but its performance with small minibatches is much worse than that of NOI. Presumably this is because the gradient computation of STOL suffers from noise in both views which are further combined through various nonlinear operations, whereas the noise in the gradient computation of NOI only comes from the output target (due to inexact whitening), and as a result NOI is more tolerant to the noise resulting from using small minibatches. This deserves further analysis as well."}], "references": [{"title": "Stochastic gradient learning in neural networks", "author": ["L. Bottou"], "venue": "Proc. Neuron\u0131\u0302mes, 1991.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade, ser. Lecture Notes in Computer Science, vol. 1524, 1998, pp. 9\u201350.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Advanced Lectures on Machine Learning, ser. Lecture Notes in Artificial Intelligence 3176, 2004, pp. 146\u2013168.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proc. of the 21st Int. Conf. Machine Learning (ICML\u201904), 2004, pp. 919\u2013926.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey", "author": ["D.P. Bertsekas"], "venue": "Optimization for Machine Learning, S. Sra, S. Nowozin, and S. J. Wright, Eds. MIT Press, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 20, 2008, pp. 161\u2013168.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, no. 3/4, pp. 321\u2013377, 1936.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1936}, {"title": "Pixels that sound", "author": ["E. Kidron", "Y.Y. Schechner", "M. Elad"], "venue": "Proc. of the 2005 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201905), 2005, pp. 88\u201395.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiview clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "Proc. of the 26th Int. Conf. Machine Learning (ICML\u201909), 2009, pp. 129\u2013136.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["R. Arora", "K. Livescu"], "venue": "Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913), 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation, vol. 16, no. 12, pp. 2639\u20132664, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "F.-F. Li"], "venue": "Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201910), 2010, pp. 966\u2013973.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, vol. 47, pp. 853\u2013899, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring a semantic representation of text via cross-language correlation analysis", "author": ["A. Vinokourov", "N. Cristianini", "J. Shawe-Taylor"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 15, 2003, pp. 1497\u20131504.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2008), 2008, pp. 771\u2013779.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["S. Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V. Raykar", "A. Saha"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 27, 2014, pp. 1853\u20131861.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "Proceedings of EACL, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "The 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015), 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. 2014 Conference on Empirical Methods in Natural Language Processing, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-view clustering", "author": ["S. Bickel", "T. Scheffer"], "venue": "Proc. of the 4th IEEE Int. Conf. Data Mining (ICDM\u201904), 2004, pp. 19\u201326.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), 2013, pp. 1247\u20131255.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["\u2014\u2014"], "venue": "Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), 2015, pp. 1083\u20131092.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep correlation for matching images and text", "author": ["F. Yan", "K. Mikolajczyk"], "venue": "Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201915), 2015, pp. 3441\u20133450.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "Int. J. Neural Syst., vol. 10, no. 5, pp. 365\u2013377, 2000.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1\u201348, 2002.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Updating quasi-Newton matrices with limited storage", "author": ["J. Nocedal"], "venue": "Math. Comp., vol. 35, no. 151, pp. 773\u2013782, 1980.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1980}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 25, 2012, pp. 1232\u20131240.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, pp. 533\u2013 536, 1986.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1986}, {"title": "Matrix Computations, 3rd ed", "author": ["G.H. Golub", "C.F. van Loan"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Linear Algebra for Signal Processing, ser. The IMA Volumes in Mathematics and its Applications", "author": ["G.H. Golub", "H. Zha"], "venue": "ch. The Canonical Correlations of Matrix Pairs and their Numerical Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1995}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Y. Lu", "D.P. Foster"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 27, 2014, pp. 91\u201399.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics, vol. 4, no. 5, pp. 1\u201317, 1964.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1964}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), 2013, pp. 1139\u20131147.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "No more pesky learning rate", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), 2013, pp. 343\u2013351.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast low-rank modifications of the thin singular value decomposition", "author": ["M. Brand"], "venue": "Linear Algebra Appl., vol. 415, no. 1, pp. 20\u201330, 2006.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Data stream anomaly detection through principal subspace tracking", "author": ["P.H. dos Santos Teixeira", "R.L. Milidi\u00fa"], "venue": "Proceedings of the 2010 ACM Symposium on Applied Computing (SAC\u201910), 2010, pp. 1609\u20131616.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive canonical correlation analysis based on matrix manifolds", "author": ["F. Yger", "M. Berar", "G. Gasso", "A. Rakotomamonjy"], "venue": "Proc. of the 29th Int. Conf. Machine Learning (ICML 2012), 2012, pp. 1071\u2013 1078.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "A method of stochastic approximation for the determination of the least eigenvalue of a symmetric matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics, vol. 9, no. 6, pp. 189\u2013195, 1969.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1969}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["E. Oja", "J. Karhunen"], "venue": "J. Math. Anal. Appl., vol. 106, no. 1, pp. 69\u201384, 1985.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1985}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["M.K. Warmuth", "D. Kuzmin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2287\u20132320, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "50th Annual Allerton Conference on Communication, Control, and Computing, Montcello, IL, Oct. 1\u20135 2012, pp. 861\u2013868.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 26, 2013, pp. 1815\u20131823.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Memory limited, streaming PCA", "author": ["I. Mitliagkas", "C. Caramanis", "P. Jain"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 26, 2013, pp. 2886\u20132894.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "The fast convergence of incremental PCA", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 26, 2013, pp. 3174\u20133182.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), 2015, pp. 144\u2013152.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Z. Ma", "Y. Lu", "D. Foster"], "venue": "Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), 2015, pp. 169\u2013178.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed optimization of deeply nested systems", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "W. Wang"], "venue": "Proc. of the 17th Int. Workshop on Artificial Intelligence and Statistics (AISTATS 2014), Reykjavik, Iceland, Apr. 22 \u2013 Apr. 25 2014, pp. 10\u201319.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "X-Ray Microbeam Speech Production", "author": ["J.R. Westbury"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proc. of the 27th Int. Conf. Machine Learning (ICML 2010), 2010, pp. 807\u2013814.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "minFunc", "author": ["M. Schmidt"], "venue": "2012, code available at http://www.cs.ubc.ca/ \u0303schmidtm/Software/minFunc.html.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "SGD is particularly well-suited for large-scale machine learning problems because it is extremely simple and easy to implement, it often achieves better generalization (test) performance (which is the focus of machine learning research) than sophisticated batch algorithms, and it usually achieves large error reduction very quickly in a small number of passes over the training set [6].", "startOffset": 383, "endOffset": 386}, {"referenceID": 1, "context": "One intuitive explanation for the empirical success of stochastic gradient descent for large data is that it makes better use of data redundancy, with an extreme example given by [2]: If the training set consists of 10 copies of the same set of examples, then computing an estimate of the gradient over one single copy is 10 times more efficient than computing the full gradient over the entire training set, while achieving the same optimization progress in the following gradient descent step.", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "At the same time, \u201cmulti-view\u201d data are becoming increasingly available, and methods based on canonical correlation analysis (CCA) [7] that use such data to learn representations (features) form an active research area.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 217, "endOffset": 221}, {"referenceID": 17, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 223, "endOffset": 227}, {"referenceID": 18, "context": "as words + context [19] or document text + text of inbound hyperlinks [20].", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "as words + context [19] or document text + text of inbound hyperlinks [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 249, "endOffset": 253}, {"referenceID": 22, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 255, "endOffset": 259}, {"referenceID": 23, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 261, "endOffset": 265}, {"referenceID": 24, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 345, "endOffset": 349}, {"referenceID": 25, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 351, "endOffset": 355}, {"referenceID": 10, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 357, "endOffset": 361}, {"referenceID": 26, "context": ", limited-memory BFGS (L-BFGS) [27] as in [21], or stochastic optimization with large minibatches [22], because it is difficult to obtain an accurate estimate of the gradient with a small subset of the training examples (again due to the whitening constraint).", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": ", limited-memory BFGS (L-BFGS) [27] as in [21], or stochastic optimization with large minibatches [22], because it is difficult to obtain an accurate estimate of the gradient with a small subset of the training examples (again due to the whitening constraint).", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": ", limited-memory BFGS (L-BFGS) [27] as in [21], or stochastic optimization with large minibatches [22], because it is difficult to obtain an accurate estimate of the gradient with a small subset of the training examples (again due to the whitening constraint).", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "As a result, these approaches have high memory complexity and may not be practical for large DNN models with hundreds of millions of weight parameters (common with web-scale data [28]), or if one would like to run the training procedure on GPUs which are equipped with faster but smaller (more expensive) memory than CPUs.", "startOffset": 179, "endOffset": 183}, {"referenceID": 20, "context": "Deep CCA (DCCA) [21] extends (linear) CCA [7] by extracting dx- and dy-dimensional nonlinear features with two DNNs f and g for views 1 and 2 respectively, such that the canonical correlation (measured by CCA) between the DNN outputs is maximized, as illustrated in Fig.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "Deep CCA (DCCA) [21] extends (linear) CCA [7] by extracting dx- and dy-dimensional nonlinear features with two DNNs f and g for views 1 and 2 respectively, such that the canonical correlation (measured by CCA) between the DNN outputs is maximized, as illustrated in Fig.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "canonical variables, [7]) are maximally correlated with their counterparts in the other view, constrained such that the dimensions in the representation are uncorrelated with each other.", "startOffset": 21, "endOffset": 24}, {"referenceID": 20, "context": "When introducing deep CCA, [21] used the L-BFGS algorithm for optimization.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "One can then compute the gradients with respect to Wf and Wg via the standard backpropagation procedure [29].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Later, it was observed by [22] that stochastic optimization still works well even for the DCCA objective, as long as larger minibatches are used to estimate the covariances and \u03a3\u0303fg when computing the gradient with (4).", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "on a minibatch using (4) does not equal the true gradient of the objective in expectation, indicating that the stochastic approach of [22] does not qualify as a stochastic gradient descent method for the DCCA objective.", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "This algorithm computes the top-L singular vectors (\u0168, \u1e7c) of \u03a3\u0303fg via orthogonal iterations [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "For [32], the advantage of the alternating least squares formulation over the exact solution to CCA is that it does not need to form the high-dimensional (nonsparse) matrix \u03a3\u0303fg; instead it directly operates on the projections, which are much smaller in size, and one can solve the least squares problems using iterative algorithms that require only sparse matrix-vector multiplications.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "where \u03c1 \u2208 [0, 1], Xb denotes a minibatch of data with index set b, and |b| denotes the size (number of samples) of this minibatch.", "startOffset": 10, "endOffset": 16}, {"referenceID": 32, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 138, "endOffset": 142}, {"referenceID": 34, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 144, "endOffset": 148}, {"referenceID": 35, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 170, "endOffset": 174}, {"referenceID": 36, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 176, "endOffset": 180}, {"referenceID": 37, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 182, "endOffset": 186}, {"referenceID": 31, "context": "These advantages validate our choice of whitening operations over the more commonly used QR decomposition used by [32].", "startOffset": 114, "endOffset": 118}, {"referenceID": 33, "context": "the stochastic gradient steps for the nonlinear least squares problems as is commonly used in the deep learning community [34].", "startOffset": 122, "endOffset": 126}, {"referenceID": 38, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 174, "endOffset": 178}, {"referenceID": 39, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 180, "endOffset": 184}, {"referenceID": 40, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 186, "endOffset": 190}, {"referenceID": 41, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 192, "endOffset": 196}, {"referenceID": 42, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 198, "endOffset": 202}, {"referenceID": 43, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 204, "endOffset": 208}, {"referenceID": 44, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 210, "endOffset": 214}, {"referenceID": 45, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 216, "endOffset": 220}, {"referenceID": 41, "context": "However, as pointed out by [42], the CCA objective is more challenging due to the whitening constraints.", "startOffset": 27, "endOffset": 31}, {"referenceID": 37, "context": "Recently, [38] proposed an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the whitening constraints.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "Based on the alternating least squares formulation of CCA (Algorithm 1), [32] propose an iterative solution of CCA for very high-dimensional and sparse input features, and the key idea is to solve the high dimensional least squares problems with randomized PCA and (batch) gradient descent.", "startOffset": 73, "endOffset": 77}, {"referenceID": 46, "context": "Upon the submission of this paper, we have become aware of the very recent publication of [47], which extends [32] by solving the linear least squares problems with (stochastic) gradient descent.", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "Upon the submission of this paper, we have become aware of the very recent publication of [47], which extends [32] by solving the linear least squares problems with (stochastic) gradient descent.", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "(for a one-dimensional projection, to be consistent with the notation of [47]) in Algorithm 3, where we take a batch gradient descent step over the least squares objectives in each iteration.", "startOffset": 73, "endOffset": 77}, {"referenceID": 46, "context": "This algorithm is equivalent to Algorithm 3 of [47].", "startOffset": 47, "endOffset": 51}, {"referenceID": 46, "context": "In [47] it is shown that the solution to the CCA objective is a fixed point of this algorithm, but no global convergence property is given.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "In the whitening steps of [47], however, the covariances are estimated using only the current minibatch at each iterate, without consideration of the remaining training samples or previous estimates, which corresponds to \u03c1 \u2192 0 in our estimate.", "startOffset": 26, "endOffset": 30}, {"referenceID": 46, "context": "[47] also suggests using a minibatch size of the order O(L), the dimensionality of the covariance matrices to be estimated, in order to obtain a high-accuracy estimate for whitening.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "5Although Algorithm 3 of [47] maintains two copies\u2014the normalized and the unnormalized versions\u2014of the weight parameters, we observe that the sole purpose of the normalized version in the intermediate iterations is to provide whitened target output for the least squares problems; our version of the algorithm eliminates this copy and the normalized version can be retrieved by a whitening step at the end.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Since the difficulty lies in the whitening constraints, one can relax the constraints and solve the Lagrangian formulation repeatedly with updated Lagrangian multipliers, as done by [25]; or one can introduce auxiliary variables and apply the quadratic penalty method [48], as done by [49].", "startOffset": 182, "endOffset": 186}, {"referenceID": 47, "context": "Since the difficulty lies in the whitening constraints, one can relax the constraints and solve the Lagrangian formulation repeatedly with updated Lagrangian multipliers, as done by [25]; or one can introduce auxiliary variables and apply the quadratic penalty method [48], as done by [49].", "startOffset": 285, "endOffset": 289}, {"referenceID": 20, "context": "We now demonstrate the NOI algorithm on the two realworld datasets used by [21] when introducing DCCA.", "startOffset": 75, "endOffset": 79}, {"referenceID": 48, "context": "The first dataset is a subset of the University of Wisconsin XRay Microbeam corpus [50], which consists of simultaneously recorded acoustic and articulatory measurements during speech.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Following [21], [22], the acoustic view inputs are 39D Mel-frequency cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2018JW11\u2019.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "Following [21], [22], the acoustic view inputs are 39D Mel-frequency cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2018JW11\u2019.", "startOffset": 16, "endOffset": 20}, {"referenceID": 49, "context": "The second dataset consists of left/right halves of the images in the MNIST dataset [51], and so the input of each view consists of 28 \u00d7 14 grayscale images.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Instead, we use DNN architectures similar to those used by [21] with ReLU activations [52], and we achieve better generalization performance with these architectures mainly due to better optimization.", "startOffset": 59, "endOffset": 63}, {"referenceID": 50, "context": "Instead, we use DNN architectures similar to those used by [21] with ReLU activations [52], and we achieve better generalization performance with these architectures mainly due to better optimization.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "The projection dimensionality L is set to 112/50 for JW11/MNIST respectively as in [21]; these are also the maximum possible total canonical correlations for the two datasets.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "We compare three optimization approaches: full batch optimization by L-BFGS [21], using the implementation of [53] which includes a good line-search procedure; stochastic JW11 MNIST", "startOffset": 76, "endOffset": 80}, {"referenceID": 51, "context": "We compare three optimization approaches: full batch optimization by L-BFGS [21], using the implementation of [53] which includes a good line-search procedure; stochastic JW11 MNIST", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "optimization with large minibatches [22], denoted STOL; and our algorithm, denoted NOI.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "Total canonical correlations on the test sets are given in Table II, showing that we achieve better results than [21] with similar DNN architectures.", "startOffset": 113, "endOffset": 117}, {"referenceID": 46, "context": "We confirm that for relatively large n, NOI works reasonably well with \u03c1 = 0 (so we are using the same covariance estimate/whitening as [47]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 46, "context": "NOI could not improve over the random initialization without memory (\u03c1 = 0, corresponding to the algorithm of [47]), but gets very close to the optimal solution and matches the objective obtained by the previous large minibatch approach when \u03c1 \u2192 1.", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "As discussed in Section IV, even the convergence of the linear CCA version of NOI with batch gradient descent is not well understood [47].", "startOffset": 133, "endOffset": 137}], "year": 2015, "abstractText": "Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.", "creator": "LaTeX with hyperref package"}}}