{"id": "1510.02777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Early Inference in Energy-Based Models Approximates Back-Propagation", "abstract": "seelye We massamba show that near-infrared Langevin MCMC unexpired inference in ecureuil an transfers energy - leonello based model german-led with latent schlepper variables telefono has the rows property pkcs that the early then-general steps overfly of inference, starting lucretius from a stationary point, correspond dmitrieva to propagating volt error gradients s\u0101r\u012b into playfield internal 390-7862 layers, pre-processing similarly to mic.smith back - guinot propagation. puccinellia The error shiurim that chimoio is mroue back - propagated sperry is with respect hyperexcitability to visible ivison units sleepyhead that have hemmingson received phthalates an 17.11 outside driving angulatus force pushing 3-to-4 them quakerism away timeframes from fast-pitch the stationary 26-nation point. Back - oboi propagated error waif gradients correspond to aristocracy temporal derivatives of hp-ux the activation of wasps hidden units. This bakkal observation impatience could ropner be an mt-32 element of gar\u00e7ons a rosendall theory egghead.com for reconfiguring explaining how tarbut brains perform credit geometria assignment in deep anoxic hierarchies prentiss as efficiently as back - gotshal propagation cashmere does. In this theory, the continuous - paraje valued latent kadu variables tour-de-force correspond slicks to mozaffarian averaged hoster voltage potential (across 3,051 time, 2-for-18 spikes, differ and 1927 possibly neurons jazz in ozaukee the nederrijn same minicolumn ), and cabdrivers neural clifty computation shadix corresponds ski-jump to approximate inference and periodista error 1934-1935 back - aute propagation beynon at the bleda same sainted time.", "histories": [["v1", "Fri, 9 Oct 2015 19:21:32 GMT  (18kb)", "http://arxiv.org/abs/1510.02777v1", "arXiv admin note: text overlap witharXiv:1509.05936"], ["v2", "Sun, 7 Feb 2016 20:24:38 GMT  (19kb)", "http://arxiv.org/abs/1510.02777v2", "arXiv admin note: text overlap witharXiv:1509.05936"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1509.05936", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "asja fischer"], "accepted": false, "id": "1510.02777"}, "pdf": {"name": "1510.02777.pdf", "metadata": {"source": "CRF", "title": "Early Inference in Energy-Based Models Approximates Back-Propagation", "authors": ["Yoshua Bengio"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n02 77\n7v 1\n[ cs\n.L G\n] 9\nO ct\n2 01"}, {"heading": "1. Introduction", "text": "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.e., moving towards configurations that better \u201cexplain\u201d the observed sensory data. We can think of the configuration of internal neurons (hidden units or latent variables) as an \u201cexplanation\u201d for the observed sensory data.\nUnder this hypothesis, when an unexpected signal arrives at visible neurons1, the other neurons will change their state (from one that is near a stochastic equilibrium) so as to better reflect that change in input. If this error signal is\n1in contact with the outside world, sensory neurons or internal neurons that directly reflect the sensory signal\nslowly driving visible neurons towards the externally observed value, this creates a perturbation for the whole network. In this paper, we consider what happens early in this process, when the perturbation due to the incorrect prediction of visible neurons is propagated towards inner areas of the brain (hidden units and hidden layers). We show how the propagation of this perturbation is mathematically equivalent to the propagation of activations gradients by the back-propagation algorithm in a deep neural network.\nThis result assumes that the latent variable is continuousvalued (unlike in the Boltzmann machine), making the system an energy-based model, defined by an energy function over both visible and hidden units. It also assumes that the neurons are leaky integrator with noise injected, such that neural computation corresponds to gradually going down the energy while adding noise, which corresponds to running Langevin Monte-Carlo Markov chain (MCMC) as inference mechanism.\nThis work is only a stepping stone. Several points still need to be elucidated before a complete theory of learning, inference and credit assignment is elaborated that is both biologically plausible and makes sense from a machine learning point of view for whole networks (with global optimization of the whole network and not being limited to learning of visible neurons that receive a target). In particular, energy based models require symmetry of connections, but note that hidden units in the model need not correspond exactly to actual neurons in the brain (it could be groups of neurons in a cortical microcircuit, for example). It remains to be shown how a form of symmetry could arise from the learning procedure itself. Another interesting question is the learning algorithm itself. As shown in this paper, synaptic changes proportional to the stochastic gradient with respect to the prediction error could be achieved if the synaptic updates are similar to those resulting from minimizing the local objective function introduced by Bengio et al. (2015b) in order to mimic spike-timing dependent plasticity (STDP).\n1"}, {"heading": "2. Neural computation does inference: going down the energy", "text": "We consider the hypothesis that a central interpretation of neural computation (what neurons do, on a short time scale at which weights can be considered fixed) is that it consists in performing iterative inference. Iterative inference means that the hidden units h of the network are gradually changed towards configurations that are more probable, with the given sensory input x and according to the current \u201cmodel of the world\u201d associated with the parameters of the model. In other words, they are approximately moving towards configurations more probable under P (h|x), and eventually sampling from P (h|x).\nBefore making the connection between Boltzmann machines or energy-based models and back-propagation, let us see in more mathematical detail how neural computation could be interpreted as inference."}, {"heading": "2.1. Leaky integrator neuron as Langevin MCMC", "text": "For this purpose, consider the classical leaky integrator neural equation. Let st represent the state of the system at time t, a vector with one element per unit, where st,i is a real valued quantity associated with the i-th unit, corresponding to a time-integrated voltage potential.\nLet us denote xt for the visible units, a subset of the elements of st (i.e., the externally driven input) and ht for the hidden units, i.e., st = (xt, ht). Let f be the function that computes the new value of the complete state st given its previous value, with f = (fx, fh) to denote the parts of f that respectively outputs the predictions on the clamped (visible) units and on the unclamped (hidden) units. The time evolution of the unclamped units is assumed to follow a leaky integration equation, i.e.,\nht+1 = fh(st, \u03b7t) = ht + \u01eb(Rh(s\u0303t)\u2212 ht) (1)\nwhere R(s) = (Rx(s), Rh(s)) represents the networkgenerated pressure on neurons, i.e., Ri(s) is what the rest of the network asks neuron i to move towards, and the corrupted state s\u0303 is results from synaptic noise and spiking effects. Here we roughly model this corruption by simple additive noise:\ns\u0303t = st + \u03b7t , (2)\nand we see that the above equation corresponds to the discretization of a differential equation\n\u03c4h\u0307 = Rh(s+ \u03b7)\u2212 h\nwhich brings h exponentially fast towards the \u201ctarget\u201d value Rh(s), along with the random walk movements brought by the noise \u03b7. We assume that Ri(s) is a weighted sum of input signals coming from the neurons connected\ninto neuron i, although the derivation below does not depend on the specific form of R, only on the assumption that it corresponds to an energy gradient, an idea developed below."}, {"heading": "2.2. Machine Learning Interpretation", "text": "R(s\u0303t) in Eq. 1 represents a guess for a new configuration, with R(s\u0303t) \u2212 st a noisy direction of movement. A noisefree direction would be R(st) \u2212 st, but injecting noise is important in order to find not just a single local mode of P (h|x) but explore the full distribution.\nWe now draw an interesting link with recent work on unsupervised learning using denoising auto-encoders and denoising score matching (Vincent, 2011; Alain and Bengio, 2013). If R(s) is the linear combination of input rates \u03c1(s), the above papers make a link between R(s)\u2212 s and the energy of a probabilistic model P (s) \u221d e\u2212E(s) with energy function E(s), i.e., they find that\nR(s)\u2212 s \u221d \u2202 logP (s)\n\u2202s = \u2212\n\u2202E(s)\n\u2202s . (3)\nWith this interpretation, the leaky integration neural computation of Eq. 1 seems to follow a Langevin Monte-Carlo Markov chain (Andrieu et al., 2003):\nst+1 = st + \u01eb(R(s\u0303t)\u2212 st)\n= st + \u01eb(R(s\u0303t)\u2212 s\u0303t + s\u0303t \u2212 st)\n= st + \u01eb(\u2212 \u2202E(s\u0303t)\n\u2202s\u0303t + \u03b7t) (4)\nwhere for the last line we used Eqs. 3, 2, and we see that we are going down the gradient from s\u0303t. Hence from the point of view of the noisy states s\u0303, we see that the update equation corresponds to\ns\u0303t+1 \u2212 \u03b7t+1 = s\u0303t \u2212 \u03b7t \u2212 \u01eb \u2202E(s\u0303t)\n\u2202s\u0303t + \u01eb\u03b7t\ns\u0303t+1 = s\u0303t \u2212 \u01eb \u2202E(s\u0303t)\n\u2202s\u0303t + \u03b7t+1 \u2212 (1 \u2212 \u01eb)\u03b7t (5)\nwhich we recognize as going down the gradient of the energy with \u201clearning rate\u201d \u01eb and adding \u201cnoise\u201d \u03b7t+1 \u2212 (1\u2212 \u01eb)\u03b7t."}, {"heading": "2.3. A Possible Energy Function", "text": "To fix ideas and illustrate the possibility of a driving function R corresponding to the gradient of an energy function, we propose the following energy function, closely related to the Boltzmann machine energy function, but with continuous non-linearities inserted.\nE(s) = \u2211\ni\ns2i 2 \u2212 1 2 \u2211\ni6=j\nWi,j\u03c1(si)\u03c1(sj)\u2212 \u2211\ni\nbi\u03c1(si)\n(6)\nwhere Wi,j is the weight between unit j to unit i, \u03c1 is the neural non-linearity, some kind of monotonic bounded function which outputs a value between 0 and 1 corresponding to a firing rate. With this energy function, the driving function R would be\nR(s) = s\u2212 \u2202E(s)\n\u2202s\nRi(s) = \u03c1 \u2032(si)\n\nbi + \u2211\nj\nWi,j\u03c1(sj)\n\n . (7)\nTo obtain this, we have assumed that Wi,j = Wj,i. Otherwise, we would get that Ri(s) = \u03c1\u2032(si) ( bi + \u2211 j 1 2 (Wi,j +Wj,i)\u03c1(sj) ) , which would be equivalent.\nThis formula for the driving input R is similar to the usual weighted sum of firing rates, except for the novel factor \u03c1\u2032(si), which would suggest that when a neuron is saturated (either being shut off or firing at the maximal rate), the external inputs have no impact on its state. The only term that remains in the neural update equation (Eq. 1) is the one that drives the state towards 0, i.e., bringing it out of the saturation region and back into a regime where the neuron is sensitive to the outside feedback, so long as \u03c1(0) is not a saturated value. This idea is developed further below.\n2.3.1. Fixed Point Behavior\nIn particular, it is interesting to note what happens around a fixed point of the state dynamics.\n\u2202E(s)\n\u2202s = 0 \u21d2 R(s) = s \u21d2 f(s, 0) = s (8)\nwhich means that\nsi = \u03c1 \u2032(si)(bi +\n\u2211\nj\nWi,j\u03c1(sj)). (9)\nLet us consider the hypothesis where the unit is saturated, i.e., \u03c1\u2032(si) \u2248 0. In that case, Ri(s) = 0 and the neural update becomes\nst+1,i = (1\u2212 \u01eb)st,i\nwhich converges towards si = 0. If the origin corresponds to a region where the derivative is significantly nonzero, |\u03c1\u2032(0)| > 0, we get that when \u03c1\u2032(si) = 0, the network cannot be at a fixed point. Otherwise the state would move towards 0 and thus it could not have been at a fixed point."}, {"heading": "3. Link to Back-propagation", "text": "We are now ready to present the main result of this paper, i.e., a link between neural computation as inference in an energy-based model and back-propagation of prediction error gradients."}, {"heading": "3.1. Propagation of Pertubations", "text": "Consider what happens when a network such as described above sits near equilibrium, i.e., near a fixed point as per Eq. 8. At that point, as per that equation, the average gradient of the energy is 0 and weight updates are also 0 in average.\nTo make the link to supervised back-propagation simpler, let us consider two kinds of visible units: input units x and output units y, and s = (x, y, h). Suppose that we start by letting the network settle to a fixed point with x clamped to the observed input values. Then we obtain an output y\u0302 at the fixed point s\u0302, where Ry(s\u0302) = y\u0302, Rh(s\u0302) = h\u0302. Equivalently, we have that\n\u2202E(s\u0302)\n\u2202h\u0302 = 0\n\u2202E(s\u0302)\n\u2202y\u0302 = 0 (10)\ni.e., the \u201cfree\u201d units (hidden and output) have settled to a value that is in agreement with the clamped input units.\nNow suppose that a target value y is observed and gradually drives the output units from their fixed point value y\u0302, towards y. This happens because the output units are also leaky integrator neurons, meaning that their state gradually changes based on the input they receive, in direction of the driving signal (now y rather than Ry(s\u0302)). Let us denote\n\u2206y = \u01eb(y \u2212 y\u0302) (11)\nthat initial change of y\u0302 when going from time step 0 to time step 1 (following the neural update equation 1, but with Ry(s\u0302) replaced by y). That would push the global state s\u0302 away from the equilibrium where it was sitting, and into a region where \u2202E(s)\n\u2202s is non-zero. Initially, the only part\nof the STDP objective function (Eq. 22) which would be non-zero would be due to the prediction error\nC = 1\n2 ||y\u0302 \u2212 y||2, (12)\nwhich corresponding to the mismatch between the prediction Ry(s\u0302) and the target value y driving the output units, or equivalently\nC = 1\n2 ||Ry(s\u0302)\u2212 y||\n2 = 1\n2\u01eb2 ||\u2206y||2, (13)\nbecause at equilibrium Ry(s\u0302) = y\u0302. Note that\n\u2206y = \u2212\u01eb \u2202C\n\u2202y\u0302 , (14)\nwhere \u01eb can be seen as a learning rate if we were trying to do SGD on y\u0302 directly.\nNow, how would the rest of the network react to this external perturbation? Each hidden neuron would approximately move in the direction of the gradient of C, but only those (call them h1) that are directly connected to the output would initially feel the pressure to minimize C. That perturbation (in the form of a volley of additional spikes, for real neurons) would then travel to the next circle of neurons, those directly connected to h1 but not to the output, etc.\nLet us look at this in more detail. Consider a typical multi-layer architecture with connections between the output layer (yielding value y\u0302 at equilibrium) and the top hidden layer (yielding value h\u03021), between h\u03021 and h\u03022, etc. The change \u2206y would propagate to h\u03021 via the neural update, which, when we ignore the effect of the injected noise, would yield a change in h\u03021\n\u2206h1 = \u01eb(Rh1((x, y\u0302 +\u2206y, h\u0302))\u2212 h\u03021).\nWith \u2206y small (arising out of our assumption that the visible units only gradually move towards their target), we can approximate the above by taking the Taylor expansion of R around y\u0302,\nRh1(x, (y\u0302 +\u2206y, h)) = h\u03021 + \u2202Rh1(s\u0302)\n\u2202y\u0302 \u2206y + o(\u01eb)\nexploiting Rh1(s\u0302) = h\u03021 at the fixed point, yielding\n\u2206h1 = \u01eb \u2202Rh1(s\u0302)\n\u2202y\u0302 \u2206y + o(\u01eb2)\nHence we have that\n\u2206h1 = \u2212\u01eb 2 \u2202Rh1(s\u0302)\n\u2202y\u0302\n\u2202C \u2202y\u0302 + o(\u01eb2). (15)\nIn order to obtain backprop, what we would like to get, though is not \u2202Rh1 (s\u0302)\n\u2202y\u0302 \u2206y but \u2202Ry(s\u0302)\n\u2202h\u03021 \u2206y since that would\ncorrespond to an application of the chain rule and we would have \u2206h1 \u221d \u2202C\n\u2202h\u03021 . The good news is that this equality is\ntrue because R is a first derivative of a function related to the energy function:\nR(s) = s\u2212 \u2202E(s)\n\u2202s =\n\u2202L(s)\n\u2202s (16)\nwhere\nL(s) = 1\n2 ||s||2 \u2212 E(s) (17)\nand Ry(s\u0302) = \u2202L(s\u0302) \u2202y\u0302 , Rh1(s\u0302) = \u2202L(s\u0302)\n\u2202h\u03021 , so that\n\u2202Rh1(s\u0302)\n\u2202y\u0302 and\n\u2202Ry(s\u0302)\n\u2202h\u03021 are cross-derivatives of L. As we know that cross\nderivatives are symmetric,\n\u2202Rh1(s\u0302)\n\u2202y\u0302 =\n\u22022L\n\u2202y\u0302\u2202h\u03021 =\n(\n\u22022L\n\u2202h\u03021\u2202y\u0302\n)T\n= \u2202Ry(s\u0302)\n\u2202h\u03021\nT\n. (18)\nNow note that since at the fixed point y\u0302 = Ry(s\u0302)\n\u2202Ry(s\u0302)\n\u2202h\u03021 =\n\u2202y\u0302\n\u2202h\u03021\nand we are ready to exploit that to rewrite \u2206h1 (Eq. 15) in a form that equates it with a backpropagated gradient:\n\u2206h1 = \u2212\u01eb 2 \u2202Ry(s\u0302)\n\u2202h\u03021\nT \u2202C\n\u2202y\u0302 + o(\u01eb2)\n= \u2212\u01eb2 \u2202y\u0302\n\u2202h\u03021\nT \u2202C\n\u2202y\u0302 + o(\u01eb2)\n= \u2212\u01eb2 \u2202C\n\u2202h\u03021 + o(\u01eb2)\n(19)\nSimilarly, the perturbation \u2206h1 will be transmitted at the next time step to the units h\u03022 that are directly connected to h\u03021 (but not to y\u0302), and yield\n\u2206h2 = \u01eb \u2202h\u03021\n\u2202h\u03022\nT\n\u2206h1 + o(\u01eb 3)\n\u2206h2 = \u2212\u01eb 3 \u2202h\u03021\n\u2202h\u03022\nT \u2202C\n\u2202h\u03021 + o(\u01eb3)\n\u2206h2 = \u2212\u01eb 3 \u2202C\n\u2202h\u03022 + o(\u01eb3). (20)"}, {"heading": "3.2. Stochastic Gradient Descent Weight Update", "text": "The above result is consistent with and inspired by the idea previously proposed by Hinton (2007) that temporal change can encode back-propagated gradients. What would it take for the \u2206h\u0302k at layer k to turn into a stochastic gradient descent (SGD) weight update with respect to the prediction error ||y \u2212 y\u0302||2? Since the state change s\u0307 represents the gradient of the prediction error with respect to s, SGD on Wi,j would require the weight change \u2206Wi,j being proportional to the rate of change of the state of the post-synaptic neuron, s\u0307i and proportional to the firing rate of the pre-synaptic neuron, \u03c1(sj), i.e., to \u2202si\u2202Wi,j :\n\u2206Wi,j \u221d s\u0307i\u03c1(sj) . (21)\nIt turns out that such a learning rule allows to simulate the relationship between spike-timing and synaptic change according to the STDP (spike-timing dependent plasticity), as shown by Bengio et al. (2015a). It arises as a stochastic gradient step in a predictive objective function\nJSTDP = 1\n2 ||f(st\u22121, \u03b7t\u22121)\u2212 st+1||\n2 , (22)\nwhere \u03b7 is the injected noise and f computed the predicted new state st, given st\u22121.\nThus, with this STDP-compatible learning rule, the change in weights due to the initial perturbation would be proportional to the back-propagation update, since it corresponds to \u2206h \u2202h\n\u2202W . However, note the multiplicative factors \u01ebk+2\nfor units h\u0302k at layer k, that make the initial changes much slower for the more remote layers. This is because the leaky integration neurons have not had time to integrate the information yet, so practically it will take on the order of the time constant times k for the change in h\u0302k to become significant, unless we adjust the per-layer learning rates accordingly.\nAlthough we see that the proposed neural dynamics and weight updates will behave approximately like backpropagation, there are differences, especially when we consider what happens after more time steps. But maybe the most important take-home message from this link with back-propagation is the following. We know that backpropagation works extremely well to train both supervised and unsupervised networks. We see here that backpropagation essentially corresponds to a variational update when the inference is infinitesimal, i.e., we only allow a single step of inference corresponding to small moves in the direction of reducing the energy function."}, {"heading": "4. Related work, contributions and future work", "text": "An important inspiration for this work is the idea proposed by Hinton (2007) that brains could implement backpropagation by using temporal derivatives to represent activation gradients, and the suggestion that combining this assumption with STDP would yield SGD on the synaptic weights.\nThe idea of neural computation corresponding to a form of stochastic relaxation towards lower energy configurations is of course very old, for example with the Boltzmann machine (Hinton and Sejnowski, 1986) and its Gibbs sampling procedure. For more recent work in this direction, see also (Berkes et al., 2011). What differs here from the Boltzmann machine is that we consider the state space to be continuous (associated with the expected voltage potential, integrating out the random effects due to spikes), rather than discrete, and that we consider very small steps (going down the gradient of the energy), which is more like a Langevin MCMC, rather than allowing each neuron to stochastically jump with higher probabibility to its optimal state, given its neighbors configuration, which is what Gibbs sampling does.\nThere are of course many other papers on theoretical interpretations of STDP, and the reader can find many references in Markram et al. (2012), but more work is needed to explore the connection of STDP to an unsupervised learn-\ning objective that could be used to train not just a single layer network (like PCA and traditional Hebbian updates) but also a deep unsupervised model. Many approaches (Fiete and Seung, 2006; Rezende and Gerstner, 2014) rely on variants of the REINFORCE algorithm (Williams, 1992) to estimate the gradient of a global objective function (basically by correlating stochastic variations at each neuron with the changes in the global objective). Although this principle is simple, it is not clear that it will scale to very large networks due to the linear growth of the variance of the estimator with the number of neurons. It is therefore tempting to explore other avenues, and we hope that the building blocks introduced here and the links made with energy-based approaches with variational inference for unsupervised learning can form useful material for a more efficient unsupervised learning principle for deep networks that is also consistent with STDP.\nA practical outcome of this work is a prediction that synaptic weight changes should vanish when the postsynaptic firing rate remains constant, as seen by inspection of Eq. 21. It would clearly be interesting to test this prediction in actual biological experiments.\nMuch remains to be done to obtain a complete probabilistic theory of unsupervised learning that is consistent with STDP, but we believe that we have put interesting ingredients in place. One aspect that requires a lot more development is how the proposed STDP objective function helps to fit the sensory observations x. If, as hypothesized above, neural computation is approximately doing inference (e.g. Langevin MCMC), then each step of inference, in average, brings us towards an equally likely or even more likely configuration of h, given x, according to the model. Hence each step is approximately pointing down the energy of P (h|x). Now, in an EM or variational EM context such as discussed in Neal and Hinton (1999); Kingma and Welling (2014); Bengio et al. (2015b), with x fixed, the distribution we want to model and consider as a target towards which parameters should be updated is precisely the joint of h \u223c P (h|x) and x \u223c the training data, which we now call Q(h, x) (the inference distribution), following the above papers. By minimizing a predictive criterion such as JSTDP , we conjecture that the model parameters move in the direction that makes the model more consistent with Q(h, x), which is what is required to maximize the variational EM bound on the data likelihood P (x). The idea is that we change the inference process so that it would reach its final state faster, which corresponds to a configuration of h that fits well the observed x.\nAnother open question is how to reconcile the need for symmetric weights when we introduce an energy function and the fact that Wi,j and Wj,i are stored at two physically different places in biological neurons. An encouraging ob-\nservation is that earlier work on auto-encoders empirically showed that even when the forward and backward weights are not tied, they tend to converge to symmetric values, and in the linear case the minimization of reconstruction error automatically yields symmetric weights (Vincent et al., 2010)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Benjamin Scellier, Asja Fischer, Thomas Mesnard, Saizheng Zhang, Yuhuai Wu, Dong-Hyun Lee, Jyri Kivinen, Jorg Bornschein, Roland Memisevic and Tim Lillicrap for feedback and discussions, as well as NSERC, CIFAR, Samsung and Canada Research Chairs for funding."}], "references": [{"title": "What regularized autoencoders learn from the data generating distribution", "author": ["G. Alain", "Y. Bengio"], "venue": "ICLR\u20192013. also arXiv report 1211.4246.", "citeRegEx": "Alain and Bengio,? 2013", "shortCiteRegEx": "Alain and Bengio", "year": 2013}, {"title": "An introduction to MCMC for machine learning", "author": ["C. Andrieu", "N. de Freitas", "A. Doucet", "M. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "An objective function for stdp", "author": ["Y. Bengio", "T. Mesnard", "A. Fischer", "S. Zhang", "Y. Wu"], "venue": "arXiv:1509.05936.", "citeRegEx": "Bengio et al\\.,? 2015a", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Towards biologically plausible deep learning", "author": ["Y. Bengio", "Lee", "D.-H.", "J. Bornschein", "Z. Lin"], "venue": "arXiv:1502.04156.", "citeRegEx": "Bengio et al\\.,? 2015b", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment", "author": ["P. Berkes", "G. Orban", "M. Lengyel", "J. Fiser"], "venue": "Science, 331, 83\u2013\u201387.", "citeRegEx": "Berkes et al\\.,? 2011", "shortCiteRegEx": "Berkes et al\\.", "year": 2011}, {"title": "Gradient learning in spiking neural networks by dynamic perturbations of conductances", "author": ["I.R. Fiete", "H.S. Seung"], "venue": "Physical Review Letters, 97(4).", "citeRegEx": "Fiete and Seung,? 2006", "shortCiteRegEx": "Fiete and Seung", "year": 2006}, {"title": "Free-energy and the brain", "author": ["K.J. Friston", "K.E. Stephan"], "venue": "Synthese, 159, 417\u2013\u2013458.", "citeRegEx": "Friston and Stephan,? 2007", "shortCiteRegEx": "Friston and Stephan", "year": 2007}, {"title": "How to do backpropagation in a brain", "author": ["G.E. Hinton"], "venue": "Invited talk at the NIPS\u20192007 Deep Learning Workshop.", "citeRegEx": "Hinton,? 2007", "shortCiteRegEx": "Hinton", "year": 2007}, {"title": "Learning and relearning in Boltzmann machines", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, pages 282\u2013317. MIT Press,", "citeRegEx": "Hinton and Sejnowski,? 1986", "shortCiteRegEx": "Hinton and Sejnowski", "year": 1986}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Welling,? 2014", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Spiketiming-dependent plasticity: A comprehensive overview", "author": ["H. Markram", "W. Gerstner", "P. Sj\u00f6str\u00f6m"], "venue": "Frontiers in synaptic plasticity, 4(2).", "citeRegEx": "Markram et al\\.,? 2012", "shortCiteRegEx": "Markram et al\\.", "year": 2012}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": "M. I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA.", "citeRegEx": "Neal and Hinton,? 1999", "shortCiteRegEx": "Neal and Hinton", "year": 1999}, {"title": "Stochastic variational learning in recurrent spiking networks", "author": ["D.J. Rezende", "W. Gerstner"], "venue": "Frontiers in Computational Neuroscience, 8(38).", "citeRegEx": "Rezende and Gerstner,? 2014", "shortCiteRegEx": "Rezende and Gerstner", "year": 2014}, {"title": "A connection between score matching and denoising autoencoders", "author": ["P. Vincent"], "venue": "Neural Computation, 23(7).", "citeRegEx": "Vincent,? 2011", "shortCiteRegEx": "Vincent", "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "J. Machine Learning Res., 11.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Simple statistical gradientfollowing algorithms connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning, 8, 229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}], "referenceMentions": [{"referenceID": 8, "context": "Introduction It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 53, "endOffset": 129}, {"referenceID": 6, "context": "Introduction It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 53, "endOffset": 129}, {"referenceID": 4, "context": "Introduction It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 53, "endOffset": 129}, {"referenceID": 2, "context": "As shown in this paper, synaptic changes proportional to the stochastic gradient with respect to the prediction error could be achieved if the synaptic updates are similar to those resulting from minimizing the local objective function introduced by Bengio et al. (2015b) in order to mimic spike-timing dependent plasticity (STDP).", "startOffset": 250, "endOffset": 272}, {"referenceID": 13, "context": "We now draw an interesting link with recent work on unsupervised learning using denoising auto-encoders and denoising score matching (Vincent, 2011; Alain and Bengio, 2013).", "startOffset": 133, "endOffset": 172}, {"referenceID": 0, "context": "We now draw an interesting link with recent work on unsupervised learning using denoising auto-encoders and denoising score matching (Vincent, 2011; Alain and Bengio, 2013).", "startOffset": 133, "endOffset": 172}, {"referenceID": 1, "context": "1 seems to follow a Langevin Monte-Carlo Markov chain (Andrieu et al., 2003):", "startOffset": 54, "endOffset": 76}, {"referenceID": 7, "context": "The above result is consistent with and inspired by the idea previously proposed by Hinton (2007) that temporal change can encode back-propagated gradients.", "startOffset": 84, "endOffset": 98}, {"referenceID": 2, "context": "It turns out that such a learning rule allows to simulate the relationship between spike-timing and synaptic change according to the STDP (spike-timing dependent plasticity), as shown by Bengio et al. (2015a). It arises as a stochastic gradient step in a predictive objective function", "startOffset": 187, "endOffset": 209}, {"referenceID": 7, "context": "Related work, contributions and future work An important inspiration for this work is the idea proposed by Hinton (2007) that brains could implement backpropagation by using temporal derivatives to represent activation gradients, and the suggestion that combining this assumption with STDP would yield SGD on the synaptic weights.", "startOffset": 107, "endOffset": 121}, {"referenceID": 8, "context": "The idea of neural computation corresponding to a form of stochastic relaxation towards lower energy configurations is of course very old, for example with the Boltzmann machine (Hinton and Sejnowski, 1986) and its Gibbs sampling procedure.", "startOffset": 178, "endOffset": 206}, {"referenceID": 4, "context": "For more recent work in this direction, see also (Berkes et al., 2011).", "startOffset": 49, "endOffset": 70}, {"referenceID": 5, "context": "Many approaches (Fiete and Seung, 2006; Rezende and Gerstner, 2014) rely on variants of the REINFORCE algorithm (Williams, 1992) to estimate the gradient of a global objective function (basically by correlating stochastic variations at each neuron with the changes in the global objective).", "startOffset": 16, "endOffset": 67}, {"referenceID": 12, "context": "Many approaches (Fiete and Seung, 2006; Rezende and Gerstner, 2014) rely on variants of the REINFORCE algorithm (Williams, 1992) to estimate the gradient of a global objective function (basically by correlating stochastic variations at each neuron with the changes in the global objective).", "startOffset": 16, "endOffset": 67}, {"referenceID": 15, "context": "Many approaches (Fiete and Seung, 2006; Rezende and Gerstner, 2014) rely on variants of the REINFORCE algorithm (Williams, 1992) to estimate the gradient of a global objective function (basically by correlating stochastic variations at each neuron with the changes in the global objective).", "startOffset": 112, "endOffset": 128}, {"referenceID": 9, "context": "There are of course many other papers on theoretical interpretations of STDP, and the reader can find many references in Markram et al. (2012), but more work is needed to explore the connection of STDP to an unsupervised learning objective that could be used to train not just a single layer network (like PCA and traditional Hebbian updates) but also a deep unsupervised model.", "startOffset": 121, "endOffset": 143}, {"referenceID": 5, "context": "Now, in an EM or variational EM context such as discussed in Neal and Hinton (1999); Kingma and Welling (2014); Bengio et al.", "startOffset": 70, "endOffset": 84}, {"referenceID": 5, "context": "Now, in an EM or variational EM context such as discussed in Neal and Hinton (1999); Kingma and Welling (2014); Bengio et al.", "startOffset": 70, "endOffset": 111}, {"referenceID": 2, "context": "Now, in an EM or variational EM context such as discussed in Neal and Hinton (1999); Kingma and Welling (2014); Bengio et al. (2015b), with x fixed, the distribution we want to model and consider as a target towards which parameters should be updated is precisely the joint of h \u223c P (h|x) and x \u223c the training data, which we now call Q(h, x) (the inference distribution), following the above papers.", "startOffset": 112, "endOffset": 134}, {"referenceID": 14, "context": "servation is that earlier work on auto-encoders empirically showed that even when the forward and backward weights are not tied, they tend to converge to symmetric values, and in the linear case the minimization of reconstruction error automatically yields symmetric weights (Vincent et al., 2010).", "startOffset": 275, "endOffset": 297}], "year": 2017, "abstractText": "We show that Langevin MCMC inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similarly to back-propagation. The error that is backpropagated is with respect to visible units that have received an outside driving force pushing them away from the stationary point. Backpropagated error gradients correspond to temporal derivatives of the activation of hidden units. This observation could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as backpropagation does. In this theory, the continuousvalued latent variables correspond to averaged voltage potential (across time, spikes, and possibly neurons in the same minicolumn), and neural computation corresponds to approximate inference and error back-propagation at the same time.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}