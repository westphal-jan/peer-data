{"id": "1510.02847", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Active Learning from Weak and Strong Labelers", "abstract": "leupold An active protsyuk learner hapoalim is raposo given rochon a pta hypothesis class, shokei a large set of unlabeled 360th examples and the ciccarelli ability to interactively exploded query labels to dady an gher oracle of biffo a subset niimi of these t\u0142uszcz examples; the delphin goal of blachman the eldik learner globonews is to learn daghlian a hypothesis in marcouiller the class that fits applicability the data wakeman well guksu by making as tori few sabzevar label disrespectful queries brindle as twinnings possible.", "histories": [["v1", "Fri, 9 Oct 2015 23:15:40 GMT  (38kb)", "https://arxiv.org/abs/1510.02847v1", "To appear in NIPS 2015"], ["v2", "Fri, 16 Oct 2015 01:06:27 GMT  (37kb)", "http://arxiv.org/abs/1510.02847v2", "To appear in NIPS 2015"]], "COMMENTS": "To appear in NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["chicheng zhang", "kamalika chaudhuri"], "accepted": true, "id": "1510.02847"}, "pdf": {"name": "1510.02847.pdf", "metadata": {"source": "CRF", "title": "Active Learning from Weak and Strong Labelers", "authors": ["Chicheng Zhang"], "emails": ["chz038@eng.ucsd.edu", "kamalika@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n02 84\n7v 2\n[ cs\n.L G\n] 1\n6 O\nThis work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone."}, {"heading": "1 Introduction", "text": "An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively make label queries to an oracle on a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few oracle queries as possible.\nAs labeling examples is a tedious task for any one person, many applications of active learning involve synthesizing labels from multiple experts who may have slightly different labeling patterns. While a body of recent empirical work [28, 29, 30, 26, 27, 12] has developed methods for combining labels from multiple experts, little is known on the theory of actively learning with labels from multiple annotators. For example, what kind of assumptions are needed for methods that use labels from multiple sources to work, when these methods are statistically consistent, and when they can yield benefits over plain active learning are all open questions.\nThis work addresses these questions in the context of active learning from strong and weak labelers. Specifically, in addition to unlabeled data and the usual labeling oracle in standard active learning, we have an extra weak labeler. The labeling oracle is a gold standard \u2013 an expert on the problem domain \u2013 and it provides high quality but expensive labels. The weak labeler is cheap, but may provide incorrect labels\n\u2217chz038@eng.ucsd.edu \u2020kamalika@cs.ucsd.edu\non some inputs. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier in a hypothesis class whose error with respect to the data labeled by the oracle is low, while exploiting the weak labeler to reduce the number of queries made to this oracle. Observe that in our model the weak labeler can be incorrect anywhere, and does not necessarily provide uniformly noisy labels everywhere, as was assumed by some previous works [8, 24].\nA plausible approach in this framework is to learn a difference classifier to predict where the weak labeler differs from the oracle, and then use a standard active learning algorithm which queries the weak labeler when this difference classifier predicts agreement. Our first key observation is that this approach is statistically inconsistent; false negative errors (that predict no difference when O and W differ) lead to biased annotation for the target classification task. We address this problem by learning instead a costsensitive difference classifier that ensures that false negative errors rarely occur. Our second key observation is that as existing active learning algorithms usually query labels in localized regions of space, it is sufficient to train the difference classifier restricted to this region and still maintain consistency. This process leads to significant label savings. Combining these two ideas, we get an algorithm that is provably statistically consistent and that works under the assumption that there is a good difference classifier with low false negative error.\nWe analyze the label complexity of our algorithm as measured by the number of label requests to the labeling oracle. In general we cannot expect any consistent algorithm to provide label savings under all circumstances, and indeed our worst case asymptotic label complexity is the same as that of active learning using the oracle alone. Our analysis characterizes when we can achieve label savings, and we show that this happens for example if the weak labeler agrees with the labeling oracle for some fraction of the examples close to the decision boundary. Moreover, when the target classification task is agnostic, the number of labels required to learn the difference classifier is of a lower order than the number of labels required for active learning; thus in realistic cases, learning the difference classifier adds only a small overhead to the total label requirement, and overall we get label savings over using the oracle alone.\nRelated Work. There has been a considerable amount of empirical work on active learning where multiple annotators can provide labels for the unlabeled examples. One line of work assumes a generative model for each annotator\u2019s labels. The learning algorithm learns the parameters of the individual labelers, and uses them to decide which labeler to query for each example. [29, 30, 13] consider separate logistic regression models for each annotator, while [20, 19] assume that each annotator\u2019s labels are corrupted with a different amount of random classification noise. A second line of work [12, 16] that includes Pro-Active Learning, assumes that each labeler is an expert over an unknown subset of categories, and uses data to measure the class-wise expertise in order to optimally place label queries. In general, it is not known under what conditions these algorithms are statistically consistent, particularly when the modeling assumptions do not strictly hold, and under what conditions they provide label savings over regular active learning.\n[25], the first theoretical work to consider this problem, consider a model where the weak labeler is more likely to provide incorrect labels in heterogeneous regions of space where similar examples have different labels. Their formalization is orthogonal to ours \u2013 while theirs is more natural in a non-parametric setting, ours is more natural for fitting classifiers in a hypothesis class. In a NIPS 2014 Workshop paper, [21] have also considered learning from strong and weak labelers; unlike ours, their work is in the online selective sampling setting, and applies only to linear classifiers and robust regression. [11] study learning from multiple teachers in the online selective sampling setting in a model where different labelers have different regions of expertise.\nFinally, there is a large body of theoretical work [1, 9, 10, 14, 31, 2, 4] on learning a binary classifier based on interactive label queries made to a single labeler. In the realizable case, [22, 9] show that a generalization of binary search provides an exponential improvement in label complexity over passive learning. The problem is more challenging, however, in the more realistic agnostic case, where such approaches lead to inconsistency. The two styles of algorithms for agnostic active learning are disagreement-based active learning (DBAL) [1, 10, 14, 4] and the more recent margin-based or confidence-based active learning [2, 31]. Our algorithm builds on recent work in DBAL [4, 15]."}, {"heading": "2 Preliminaries", "text": "The Model. We begin with a general framework for actively learning from weak and strong labelers. In the standard active learning setting, we are given unlabelled data drawn from a distribution U over an input space X , a label space Y = {\u22121,1}, a hypothesis class H , and a labeling oracle O to which we can make interactive queries.\nIn our setting, we additionally have access to a weak labeling oracle W which we can query interactively. Querying W is significantly cheaper than querying O; however, querying W generates a label yW drawn from a conditional distribution PW (yW |x) which is not the same as the conditional distribution PO(yO|x) of O.\nLet D be the data distribution over labelled examples such that: PD(x,y) = PU(x)PO(y|x). Our goal is to learn a classifier h in the hypothesis class H such that with probability \u2265 1\u2212\u03b4 over the sample, we have: PD(h(x) 6= y)\u2264 minh\u2032\u2208H PD(h\u2032(x) 6= y)+ \u03b5 , while making as few (interactive) queries to O as possible.\nObserve that in this model W may disagree with the oracle O anywhere in the input space; this is unlike previous frameworks [8, 24] where labels assigned by the weak labeler are corrupted by random classification noise with a higher variance than the labeling oracle. We believe this feature makes our model more realistic.\nSecond, unlike [25], mistakes made by the weak labeler do not have to be close to the decision boundary. This keeps the model general and simple, and allows greater flexibility to weak labelers. Our analysis shows that if W is largely incorrect close to the decision boundary, then our algorithm will automatically make more queries to O in its later stages.\nFinally note that O is allowed to be non-realizable with respect to the target hypothesis class H .\nBackground on Active Learning Algorithms. The standard active learning setting is very similar to ours, the only difference being that we have access to the weak oracle W . There has been a long line of work on active learning [1, 7, 9, 14, 2, 10, 4, 31]. Our algorithms are based on a style called disagreement-based active learning (DBAL). The main idea is as follows. Based on the examples seen so far, the algorithm maintains a candidate set Vt of classifiers in H that is guaranteed with high probability to contain h\u2217, the classifier in H with the lowest error. Given a randomly drawn unlabeled example xt , if all classifiers in Vt agree on its label, then this label is inferred; observe that with high probability, this inferred label is h\u2217(xt). Otherwise, xt is said to be in the disagreement region of Vt , and the algorithm queries O for its label. Vt is updated based on xt and its label, and algorithm continues.\nRecent works in DBAL [10, 4] have observed that it is possible to determine if an xt is in the disagreement region of Vt without explicitly maintaining Vt . Instead, a labelled dataset St is maintained; the labels of the examples in St are obtained by either querying the oracle or direct inference. To determine whether an xt lies in the disagreement region of Vt , two constrained ERM procedures are performed; empirical risk is minimized over St while constraining the classifier to output the label of xt as 1 and \u22121 respectively. If\nthese two classifiers have similar training errors, then xt lies in the disagreement region of Vt ; otherwise the algorithm infers a label for xt that agrees with the label assigned by h\u2217.\nMore Definitions and Notation. The error of a classifier h under a labelled data distribution Q is defined as: errQ(h) = P(x,y)\u223cQ(h(x) 6= y); we use the notation err(h,S) to denote its empirical error on a labelled data set S. We use the notation h\u2217 to denote the classifier with the lowest error under D and \u03bd to denote its error errD(h\u2217), where D is the target labelled data distribution.\nOur active learning algorithm implicitly maintains a (1\u2212 \u03b4 )-confidence set for h\u2217 throughout the algorithm. Given a set S of labelled examples, a set of classifiers V (S) \u2286 H is said to be a (1\u2212 \u03b4 )-confidence set for h\u2217 with respect to S if h\u2217 \u2208V with probability \u2265 1\u2212\u03b4 over S.\nThe disagreement between two classifiers h1 and h2 under an unlabelled data distribution U , denoted by \u03c1U(h1,h2), is Px\u223cU(h1(x) 6= h2(x)). Observe that the disagreements under U form a pseudometric over H . We use BU(h,r) to denote a ball of radius r centered around h in this metric. The disagreement region of a set V of classifiers, denoted by DIS(V ), is the set of all examples x \u2208 X such that there exist two classifiers h1 and h2 in V for which h1(x) 6= h2(x)."}, {"heading": "3 Algorithm", "text": "Our main algorithm is a standard single-annotator DBAL algorithm with a major modification: when the DBAL algorithm makes a label query, we use an extra sub-routine to decide whether this query should be made to the oracle or the weak labeler, and make it accordingly. How do we make this decision? We try to predict if weak labeler differs from the oracle on this example; if so, query the oracle, otherwise, query the weak labeler.\nKey Idea 1: Cost Sensitive Difference Classifier. How do we predict if the weak labeler differs from the oracle? A plausible approach is to learn a difference classifier hd f in a hypothesis class H d f to determine if there is a difference. Our first key observation is when the region where O and W differ cannot be perfectly modeled by H d f , the resulting active learning algorithm is statistically inconsistent. Any false negative errors (that is, incorrectly predicting no difference) made by difference classifier leads to biased annotation for the target classification task, which in turn leads to inconsistency. We address this problem by instead learning a cost-sensitive difference classifier and we assume that a classifier with low false negative error exists in H d f . While training, we constrain the false negative error of the difference classifier to be low, and minimize the number of predicted positives (or disagreements between W and O) subject to this constraint. This ensures that the annotated data used by the active learning algorithm has diminishing bias, thus ensuring consistency.\nKey Idea 2: Localized Difference Classifier Training. Unfortunately, even with cost-sensitive training, directly learning a difference classifier accurately is expensive. If d\u2032 is the VC-dimension of the difference hypothesis class H d f , to learn a target classifier to excess error \u03b5 , we need a difference classifier with false negative error O(\u03b5), which, from standard generalization theory, requires O\u0303(d\u2032/\u03b5) labels [6, 23]! Our second key observation is that we can save on labels by training the difference classifier in a localized manner \u2013 because the DBAL algorithm that builds the target classifier only makes label queries in the disagreement region of the current confidence set for h\u2217. Therefore we train the difference classifier only on this region and still maintain consistency. Additionally this provides label savings because while training the target\nclassifier to excess error \u03b5 , we need to train a difference classifier with only O\u0303(d\u2032\u03c6k/\u03b5) labels where \u03c6k is the probability mass of this disagreement region. The localized training process leads to an additional technical challenge: as the confidence set for h\u2217 is updated, its disagreement region changes. We address this through an epoch-based DBAL algorithm, where the confidence set is updated and a fresh difference classifier is trained in each epoch.\nMain Algorithm. Our main algorithm (Algorithm 1) combines these two key ideas, and like [4], implicitly maintains the (1\u2212 \u03b4 )-confidence set for h\u2217 by through a labeled dataset S\u0302k. In epoch k, the target excess error is \u03b5k \u2248 12k , and the goal of Algorithm 1 is to generate a labeled dataset S\u0302k that implicitly represents a (1\u2212\u03b4k)-confidence set on h\u2217. Additionally, S\u0302k has the property that the empirical risk minimizer over it has excess error \u2264 \u03b5k.\nA naive way to generate such an S\u0302k is by drawing O\u0303(d/\u03b52k ) labeled examples, where d is the VC dimension of H . Our goal, however, is to generate S\u0302k using a much smaller number of label queries, which is accomplished by Algorithm 3. This is done in two ways. First, like standard DBAL, we infer the label of any x that lies outside the disagreement region of the current confidence set for h\u2217. Algorithm 4 identifies whether an x lies in this region. Second, for any x in the disagreement region, we determine whether O and W agree on x using a difference classifier; if there is agreement, we query W , else we query O. The difference classifier used to determine agreement is retrained in the beginning of each epoch by Algorithm 2, which ensures that the annotation has low bias.\nThe algorithms use a constrained ERM procedure CONS-LEARN. Given a hypothesis class H , a labeled dataset S and a set of constraining examples C, CONS-LEARNH(C,S) returns a classifier in H that minimizes the empirical error on S subject to h(xi) = yi for each (xi,yi) \u2208C.\nIdentifying the Disagreement Region. Algorithm 4 identifies if an unlabeled example x lies in the disagreement region of the current (1\u2212 \u03b4 )-confidence set for h\u2217; recall that this confidence set is implicitly maintained through S\u0302k. The identification is based on two ERM queries. Let h\u0302 be the empirical risk minimizer on the current labeled dataset S\u0302k\u22121, and h\u0302\u2032 be the empirical risk minimizer on S\u0302k\u22121 under the constraint that h\u0302\u2032(x) =\u2212h\u0302(x). If the training errors of h\u0302 and h\u0302\u2032 are very different, then, all classifiers with training error close to that of h\u0302 assign the same label to x, and x lies outside the current disagreement region.\nTraining the Difference Classifier. Algorithm 2 trains a difference classifier on a random set of examples which lies in the disagreement region of the current confidence set for h\u2217. The training process is costsensitive, and is similar to [17, 18, 6, 23]. A hard bound is imposed on the false-negative error, which translates to a bound on the annotation bias for the target task. The number of positives (i.e., the number of examples where W and O differ) is minimized subject to this constraint; this amounts to (approximately) minimizing the fraction of queries made to O.\nThe number of labeled examples used in training is large enough to ensure false negative error O(\u03b5k/\u03c6k) over the disagreement region of the current confidence set; here \u03c6k is the probability mass of this disagreement region under U . This ensures that the overall annotation bias introduced by this procedure in the target task is at most O(\u03b5k). As \u03c6k is small and typically diminishes with k, this requires less labels than training the difference classifier globally which would have required O\u0303(d\u2032/\u03b5k) queries to O.\nAlgorithm 1 Active Learning Algorithm from Weak and Strong Labelers 1: Input: Unlabeled distribution U , target excess error \u03b5 , confidence \u03b4 , labeling oracle O, weak oracle W ,\nhypothesis class H , hypothesis class for difference classifier H d f . 2: Output: Classifier h\u0302 in H . 3: Initialize: initial error \u03b50 = 1, confidence \u03b40 = \u03b4/4. Total number of epochs k0 = \u2308log 1\u03b5 \u2309. 4: Initial number of examples n0 = O( 1\u03b520 (d ln 1\u03b520 + ln 1\u03b40 )). 5: Draw a fresh sample and query O for its labels S\u03020 = {(x1,y1), . . . ,(xn0 ,yn0)}. Let \u03c30 = \u03c3(n0,\u03b40). 6: for k = 1,2, . . . ,k0 do 7: Set target excess error \u03b5k = 2\u2212k, confidence \u03b4k = \u03b4/4(k+1)2. 8: # Train Difference Classifier 9: h\u0302d fk \u2190 Call Algorithm 2 with inputs unlabeled distribution U , oracles W and O, target excess error\n\u03b5k, confidence \u03b4k/2, previously labeled dataset S\u0302k\u22121. 10: # Adaptive Active Learning using Difference Classifier 11: \u03c3k, S\u0302k \u2190 Call Algorithm 3 with inputs unlabeled distribution U , oracles W and O, difference classi-\nfier h\u0302d fk , target excess error \u03b5k, confidence \u03b4k/2, previously labeled dataset S\u0302k\u22121. 12: end for 13: return h\u0302 \u2190 CONS-LEARNH ( /0, S\u0302k0 ).\nAlgorithm 2 Training Algorithm for Difference Classifier\n1: Input: Unlabeled distribution U , oracles W and O, target error \u03b5 , hypothesis class H d f , confidence \u03b4 , previous labeled dataset T\u0302 . 2: Output: Difference classifier h\u0302d f . 3: Let p\u0302 be an estimate of Px\u223cU(in disagr region(T\u0302 , 3\u03b52 ,x) = 1), obtained by calling Algorithm 5 with\nfailure probability \u03b4/3. 1 4: Let U \u2032 = /0, i = 1, and\nm = 64 \u00b71024p\u0302\n\u03b5 (d\u2032 ln 512 \u00b71024p\u0302 \u03b5 + ln 72 \u03b4 ) (1)\n5: repeat 6: Draw an example xi from U . 7: if in disagr region(T\u0302 , 3\u03b52 ,xi) = 1 then # xi is inside the disagreement region 8: query both W and O for labels to get yi,W and yi,O. 9: end if\n10: U \u2032 =U \u2032\u222a{(xi,yi,O,yi,W )} 11: i = i+1 12: until |U \u2032|= m 13: Learn a classifier h\u0302d f \u2208 H d f based on the following empirical risk minimizer:\nh\u0302d f = argminhd f\u2208H d f m\n\u2211 i=1\n1(hd f (xi) = +1), s.t. m\n\u2211 i=1 1(hd f (xi) =\u22121\u2227 yi,O 6= yi,W )\u2264 m\u03b5/256p\u0302 (2)\n14: return h\u0302d f .\n1Note that if in Algorithm 5, the upper confidence bound of Px\u223cU (in disagr region(T\u0302 , 3\u03b52 ,x) = 1) is lower than \u03b5/64, then we can halt Algorithm 2 and return an arbitrary hd f in H d f . Using this hd f will still guarantee the correctness of Algorithm 1.\nAdaptive Active Learning using the Difference Classifier. Finally, Algorithm 3 is our main active learning procedure, which generates a labeled dataset S\u0302k that is implicitly used to maintain a tighter (1\u2212 \u03b4 )- confidence set for h\u2217. Specifically, Algorithm 3 generates a S\u0302k such that the set Vk defined as:\nVk = {h : err(h, S\u0302k)\u2212 min h\u0302k\u2208H err(h\u0302k, S\u0302k)\u2264 3\u03b5k/4}\nhas the property that:\n{h : errD(h)\u2212 errD(h\u2217)\u2264 \u03b5k/2} \u2286Vk \u2286 {h : errD(h)\u2212 errD(h\u2217)\u2264 \u03b5k}\nThis is achieved by labeling, through inference or query, a large enough sample of unlabeled data drawn from U . Labels are obtained from three sources - direct inference (if x lies outside the disagreement region as identified by Algorithm 4), querying O (if the difference classifier predicts a difference), and querying W . How large should the sample be to reach the target excess error? If errD(h\u2217) = \u03bd , then achieving an excess error of \u03b5 requires O\u0303(d\u03bd/\u03b52k ) samples, where d is the VC dimension of the hypothesis class. As \u03bd is unknown in advance, we use a doubling procedure in lines 4-14 to iteratively determine the sample size.\nAlgorithm 3 Adaptive Active Learning using Difference Classifier\n1: Input: Unlabeled data distribution U , oracles W and O, difference classifier hd f , target excess error \u03b5 , confidence \u03b4 , previous labeled dataset T\u0302 . 2: Output: Parameter \u03c3 , labeled dataset S\u0302. 3: Let h\u0302 = CONS-LEARNH ( /0, T\u0302 ). 4: for t = 1,2, . . . , do 5: Let \u03b4 t = \u03b4/t(t +1). Define: \u03c3(2t ,\u03b4 t) = 82t (2d ln 2e2t d + ln 24 \u03b4 t ). 6: Draw 2t examples from U to form St,U . 7: for each x \u2208 St,U do: 8: if in disagr region(T\u0302 , 3\u03b52 ,x) = 0 then # x is inside the agreement region 9: Add (x, h\u0302(x)) to S\u0302t .\n10: else # x is inside the disagreement region 11: If hd f (x) = +1, query O for the label y of x, otherwise query W . Add (x,y) to S\u0302t . 12: end if 13: end for 14: Train h\u0302t \u2190 CONS-LEARNH ( /0, S\u0302t). 15: if \u03c3(2t ,\u03b4 t)+ \u221a\n\u03c3(2t ,\u03b4 t)err(h\u0302t , S\u0302t)\u2264 \u03b5/512 then 16: t0 \u2190 t, break 17: end if 18: end for 19: return \u03c3 \u2190 \u03c3(2t0 ,\u03b4 t0), S\u0302 \u2190 S\u0302t0 ."}, {"heading": "4 Performance Guarantees", "text": "We now examine the performance of our algorithm, which is measured by the number of label queries made to the oracle O. Additionally we require our algorithm to be statistically consistent, which means that the true error of the output classifier should converge to the true error of the best classifier in H on the data distribution D.\nAlgorithm 4 in disagr region(S\u0302,\u03c4 ,x): Test if x is in the disagreement region of current confidence set 1: Input: labeled dataset S\u0302, rejection threshold \u03c4 , unlabeled example x. 2: Output: 1 if x in the disagreement region of current confidence set, 0 otherwise. 3: Train h\u0302 \u2190 CONS-LEARNH ({ /0, S\u0302}). 4: Train h\u0302\u2032x \u2190 CONS-LEARNH ({(x,\u2212h\u0302(x))}, S\u0302}). 5: if err(h\u0302\u2032x, S\u0302)\u2212 err(h\u0302, S\u0302)> \u03c4 then # x is in the agreement region 6: return 0 7: else # x is in the disagreement region 8: return 1 9: end if\nSince our framework is very general, we cannot expect any statistically consistent algorithm to achieve label savings over using O alone under all circumstances. For example, if labels provided by W are the complete opposite of O, no algorithm will achieve both consistency and label savings. We next provide an assumption under which Algorithm 1 works and yields label savings.\nAssumption. The following assumption states that difference hypothesis class contains a good cost-sensitive predictor of when O and W differ in the disagreement region of BU(h\u2217,r); a predictor is good if it has low false-negative error and predicts a positive label with low frequency. If there is no such predictor, then we cannot expect an algorithm similar to ours to achieve label savings.\nAssumption 1. Let D be the joint distribution: PD(x,yO,yW ) = PU(x)PW (yW |x)PO(yO|x). For any r,\u03b7 > 0, there exists an hd f\u03b7 ,r \u2208 H d f with the following properties:\nPD(h d f \u03b7 ,r(x) =\u22121,x \u2208 DIS(BU(h\u2217,r)),yO 6= yW )\u2264 \u03b7 (3)\nPD(h d f \u03b7 ,r(x) = 1,x \u2208 DIS(BU(h\u2217,r)))\u2264 \u03b1(r,\u03b7) (4)\nNote that (3), which states there is a hd f \u2208 H d f with low false-negative error, is minimally restrictive, and is trivially satisfied if H d f includes the constant classifier that always predicts 1. Theorem shows that (3) is sufficient to ensure statistical consistency.\n(4) in addition states that the number of positives predicted by the classifier hd f\u03b7 ,r is upper bounded by \u03b1(r,\u03b7). Note \u03b1(r,\u03b7) \u2264 PU(DIS(BU(h\u2217,r))) always; performance gain is obtained when \u03b1(r,\u03b7) is lower, which happens when the difference classifier predicts agreement on a significant portion of DIS(BU(h\u2217,r)).\nConsistency. Provided Assumption 1 holds, we next show that Algorithm 1 is statistically consistent. Establishing consistency is non-trivial for our algorithm as the output classifier is trained on labels from both O and W .\nTheorem 1 (Consistency). Let h\u2217 be the classifier that minimizes the error with respect to D. If Assumption 1 holds, then with probability \u2265 1\u2212\u03b4 , the classifier h\u0302 output by Algorithm 1 satisfies: errD(h\u0302)\u2264 errD(h\u2217)+\u03b5 .\nLabel Complexity. The label complexity of standard DBAL is measured in terms of the disagreement coefficient. The disagreement coefficient \u03b8(r) at scale r is defined as: \u03b8(r) = suph\u2208H supr\u2032\u2265r PU (DIS(BU (h,r\u2032)) r\u2032 ; intuitively, this measures the rate of shrinkage of the disagreement region with the radius of the ball BU(h,r) for any h in H . It was shown by [10] that the label complexity of DBAL for target excess generalization\nerror \u03b5 is O\u0303(d\u03b8(2\u03bd + \u03b5)(1+ \u03bd2\u03b52 )) where the O\u0303 notation hides factors logarithmic in 1/\u03b5 and 1/\u03b4 . In contrast, the label complexity of our algorithm can be stated in Theorem 2. Here we use the O\u0303 notation for convenience; we have the same dependence on log1/\u03b5 and log 1/\u03b4 as the bounds for DBAL.\nTheorem 2 (Label Complexity). Let d be the VC dimension of H and let d\u2032 be the VC dimension of H d f . If Assumption 1 holds, and if the error of the best classifier in H on D is \u03bd , then with probability \u2265 1\u2212\u03b4 , the following hold:\n1. The number of label queries made by Algorithm 1 to the oracle O in epoch k at most:\nmk = O\u0303 (d(2\u03bd + \u03b5k\u22121)(\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k\u22121/1024)+ \u03b5k\u22121)\n\u03b52k + d\u2032P(DIS(BU(h\u2217,2\u03bd + \u03b5k\u22121))) \u03b5k )\n(5)\n2. The total number of label queries made by Algorithm 1 to the oracle O is at most:\nO\u0303 (\nsup r\u2265\u03b5 \u03b1(2\u03bd + r,r/1024)+ r 2\u03bd + r \u00b7d ( \u03bd2 \u03b52 +1 ) +\u03b8(2\u03bd + \u03b5)d\u2032 (\u03bd \u03b5 +1 ))\n(6)"}, {"heading": "4.1 Discussion", "text": "The first terms in (5) and (6) represent the labels needed to learn the target classifier, and second terms represent the overhead in learning the difference classifier.\nIn the realistic agnostic case (where \u03bd > 0), as \u03b5 \u2192 0, the second terms are lower order compared to the label complexity of DBAL. Thus even if d\u2032 is somewhat larger than d, fitting the difference classifier does not incur an asymptotically high overhead in the more realistic agnostic case. In the realizable case, when d\u2032 \u2248 d, the second terms are of the same order as the first; therefore we should use a simpler difference hypothesis class H d f in this case. We believe that the lower order overhead term comes from the fact that there exists a classifier in H d f whose false negative error is very low.\nComparing Theorem 2 with the corresponding results for DBAL, we observe that instead of \u03b8(2\u03bd + \u03b5), we have the term supr\u2265\u03b5 \u03b1(2\u03bd+r,r/1024) 2\u03bd+r . Since supr\u2265\u03b5 \u03b1(2\u03bd+r,r/1024) 2\u03bd+r \u2264 \u03b8(2\u03bd + \u03b5), the worst case asymptotic label complexity is the same as that of standard DBAL. This label complexity may be considerably better however if supr\u2265\u03b5 \u03b1(2\u03bd+r,r/1024) 2\u03bd+r is less than the disagreement coefficient. As we expect, this will happen when the region of difference between W and O restricted to the disagreement regions is relatively small, and this region is well-modeled by the difference hypothesis class H d f .\nAn interesting case is when the weak labeler differs from O close to the decision boundary and agrees with O away from this boundary. In this case, any consistent algorithm should switch to querying O close to the decision boundary. Indeed in earlier epochs, \u03b1 is low, and our algorithm obtains a good difference classifier and achieves label savings. In later epochs, \u03b1 is high, the difference classifiers always predict a difference and the label complexity of the later epochs of our algorithm is the same order as DBAL. In practice, if we suspect that we are in this case, we can switch to plain active learning once \u03b5k is small enough.\nCase Study: Linear Classfication under Uniform Distribution. We provide a simple example where our algorithm provides a better asymptotic label complexity than DBAL. Let H be the class of homogeneous linear separators on the d-dimensional unit ball and let H d f = {h\u2206h\u2032 : h,h\u2032 \u2208 H }. Furthermore, let U be the uniform distribution over the unit ball.\nSuppose that O is a deterministic labeler such that errD(h\u2217) = \u03bd > 0. Moreover, suppose that W is such that there exists a difference classifier h\u0304d f with false negative error 0 for which PU(h\u0304d f (x) = 1) \u2264 g.\nAdditionally, we assume that g = o( \u221a\nd\u03bd); observe that this is not a strict assumption on H d f , as \u03bd could be as much as a constant. Figure 1 shows an example in d = 2 that satisfies these assumptions. In this case, as \u03b5 \u2192 0, Theorem 2 gives the following label complexity bound.\nCorollary 1. With probability \u2265 1\u2212 \u03b4 , the number of label queries made to oracle O by Algorithm 1 is O\u0303 (\nd max( g\u03bd ,1)( \u03bd2 \u03b52 +1)+d 3/2 ( 1+ \u03bd\u03b5 )\n)\n, where the O\u0303 notation hides factors logarithmic in 1/\u03b5 and 1/\u03b4 .\nAs g = o( \u221a\nd\u03bd), this improves over the label complexity of DBAL, which is O\u0303(d3/2(1+ \u03bd2\u03b52 )).\nLearning with respect to Data labeled by both O and W . Finally, an interesting variant of our model is to measure error relative to data labeled by a mixture of O and W \u2013 say, (1\u2212\u03b2 )O+\u03b2W for some 0 < \u03b2 < 1. Similar measures have been considered in the domain adaptation literature [5].\nWe can also analyze this case using simple modifications to our algorithm and analysis. The results are presented in Corollary 2, which suggests that the number of label queries to O in this case is roughly 1\u2212\u03b2 times the label complexity in Theorem 2.\nLet O\u2032 be the oracle which, on input x, queries O for its label w.p 1\u2212\u03b2 and queries W w.p \u03b2 . Let D\u2032 be the distribution: PD\u2032(x,y) = PU(x)PO\u2032(y|x), h\u2032 = argminh\u2208H errD\u2032(h) be the classifier in H that minimizes error over D\u2032, and \u03bd \u2032 = errD\u2032(h\u2032) be its true error. Moreover, suppose that Assumption 1 holds with respect to oracles O\u2032 and W with \u03b1 = \u03b1 \u2032(r,\u03b7) in (4). Then, the modified Algorithm 1 that simulates O\u2032 by random queries to O has the following properties.\nCorollary 2. With probability \u2265 1\u22122\u03b4 ,\n1. the classifier h\u0302 output by (the modified) Algorithm 1 satisfies: errD\u2032(h\u0302)\u2264 errD\u2032(h\u2032)+ \u03b5 .\n2. the total number of label queries made by this algorithm to O is at most:\nO\u0303 ( (1\u2212\u03b2 ) (\nsup r\u2265\u03b5 \u03b1 \u2032(2\u03bd \u2032+ r,r/1024)+ r 2\u03bd \u2032+ r \u00b7d ( \u03bd \u20322 \u03b52 +1 ) +\u03b8(2\u03bd \u2032+ \u03b5)d\u2032 ( \u03bd \u2032 \u03b5 +1 ) ))\nConclusion. In this paper, we take a step towards a theoretical understanding of active learning from multiple annotators through a learning theoretic formalization for learning from weak and strong labelers. Our work shows that multiple annotators can be successfully combined to do active learning in a statistically\nconsistent manner under a general setting with few assumptions; moreover, under reasonable conditions, this kind of learning can provide label savings over plain active learning.\nAn avenue for future work is to explore a more general setting where we have multiple labelers with expertise on different regions of the input space. Can we combine inputs from such labelers in a statistically consistent manner? Second, our algorithm is intended for a setting where W is biased, and performs suboptimally when the label generated by W is a random corruption of the label provided by O. How can we account for both random noise and bias in active learning from weak and strong labelers?"}, {"heading": "Acknowledgements", "text": "We thank NSF under IIS 1162581 for research support and Jennifer Dy for introducing us to the problem of active learning from multiple labelers."}, {"heading": "A Notation", "text": ""}, {"heading": "A.1 Basic Definitions and Notation", "text": "Here we do a brief recap of notation. We assume that we are given a target hypothesis class H of VC dimension d, and a difference hypothesis class H d f of VC dimension d\u2032.\nWe are given access to an unlabeled distribution U and two labeling oracles O and W . Querying O (resp. W ) with an unlabeled data point xi generates a label yi,O (resp. yi,W ) which is drawn from the distribution PO(y|xi) (resp. PW (y|xi)). In general these two distributions are different. We use the notation D to denote the joint distribution over examples and labels from O and W :\nPD(x,yO,yW ) = PU(x)PO(yO|x)PW (yW |x)\nOur goal in this paper is to learn a classifier in H which has low error with respect to the data distribution D described as: PD(x,y) = PU(x)PO(y|x) and our goal is use queries to W to reduce the number of queries to O. We use yO to denote the labels returned by O, yW to denote the labels returned by W .\nThe error of a classifier h under a labeled data distribution Q is defined as: errQ(h) = P(x,y)\u223cQ(h(x) 6= y); we use the notation err(h,S) to denote its empirical error on a labeled data set S. We use the notation h\u2217 to denote the classifier with the lowest error under D. Define the excess error of h with respect to distribution D as errD(h)\u2212 errD(h\u2217). For a set Z, we occasionally abuse notation and use Z to also denote the uniform distribution over the elements of Z.\nConfidence Sets and Disagreement Region. Our active learning algorithm will maintain a (1 \u2212 \u03b4 )- confidence set for h\u2217 throughout the algorithm. A set of classifiers V \u2286 H produced by a (possibly randomized) algorithm is said to be a (1\u2212 \u03b4 )-confidence set for h\u2217 if h\u2217 \u2208 V with probability \u2265 1\u2212 \u03b4 ; here the probability is over the randomness of the algorithm as well as the choice of all labeled and unlabeled examples drawn by it.\nGiven two classifiers h1 and h2 the disagreement between h1 and h2 under an unlabeled data distribution U , denoted by \u03c1U(h1,h2), is Px\u223cU(h1(x) 6= h2(x)). Given an unlabeled dataset S, the empirical disagreement of h1 and h2 on S is denoted by \u03c1S(h1,h2). Observe that the disagreements under U form a pseudometric over H . We use BU(h,r) to denote a ball of radius r centered around h in this metric. The disagreement region of a set V of classifiers, denoted by DIS(V ), is the set of all examples x \u2208 X such that there exist two classifiers h1 and h2 in V for which h1(x) 6= h2(x).\nDisagreement Region. We denote the disagreement region of a disagreement ball of radius r centered around h\u2217 by\n\u2206(r) := DIS(B(h\u2217,r)) (7)\nConcentration Inequalities. Suppose Z is a dataset consisting of n iid samples from a distribution D. We will use the following result, which is obtained from a standard application of the normalized VC inequality. With probability 1\u2212\u03b4 over the random draw of Z, for all h,h\u2032 \u2208 H ,\n|(err(h,Z)\u2212 err(h\u2032,Z))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 min ( \u221a \u03c3(n,\u03b4 )\u03c1Z(h,h\u2032)+\u03c3(n,\u03b4 ), \u221a \u03c3(n,\u03b4 )\u03c1D(h,h\u2032)+\u03c3(n,\u03b4 ) )\n(8)\n|(err(h,Z)\u2212 errD(h)| \u2264 min ( \u221a \u03c3(n,\u03b4 )err(h,Z)+\u03c3(n,\u03b4 ), \u221a \u03c3(n,\u03b4 )errD(h)+\u03c3(n,\u03b4 ) )\n(9)\nwhere d is the VC dimension of H and the notation \u03c3(n,\u03b4 ) is defined as:\n\u03c3(n,\u03b4 ) = 8 n (2d ln 2en d + ln 24 \u03b4 ) (10)\nEquation (8) loosely implies the following equation:\n|(err(h,Z)\u2212 err(h\u2032,Z))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 \u221a 4\u03c3(n,\u03b4 ) (11)\nThe following is a consequence of standard Chernoff bounds. Let X1, . . . ,Xn be iid Bernoulli random variables with mean p. If p\u0302 = \u2211i Xi/n, then with probabiliy 1\u2212\u03b4 ,\n|p\u0302\u2212 p| \u2264 min( \u221a p\u03b3(n,\u03b4 )+ \u03b3(n,\u03b4 ), \u221a p\u0302\u03b3(n,\u03b4 )+ \u03b3(n,\u03b4 )) (12)\nwhere the notation \u03b3(n,\u03b4 ) is defined as:\n\u03b3(n,\u03b4 ) = 4 n ln 2 \u03b4\n(13)\nEquation (12) loosely implies the following equation:\n|p\u0302\u2212 p| \u2264 \u221a 4\u03b3(n,\u03b4 ) (14)\nUsing the notation we just introduced, we can rephrase Assumption 1 as follows. For any r,\u03b7 > 0, there exists an hd f\u03b7 ,r \u2208 H d f with the following properties:\nPD(h d f \u03b7 ,r(x) =\u22121,x \u2208 \u2206(r),yO 6= yW )\u2264 \u03b7\nPD(h d f \u03b7 ,r(x) = 1,x \u2208 \u2206(r))\u2264 \u03b1(r,\u03b7)\nWe end with an useful fact about \u03c3(n,\u03b4 ).\nFact 1. The minimum n such that \u03c3(n,\u03b4/(log n(log n+1)))\u2264 \u03b5 is at most\n64 \u03b5 (d ln 512 \u03b5 + ln 24 \u03b4 )"}, {"heading": "A.2 Adaptive Procedure for Estimating Probability Mass", "text": "For completeness, we describe in Algorithm 5 a standard doubling procedure for estimating the bias of a coin within a constant factor. This procedure is used by Algorithm 2 to estimate the probability mass of the disagreement region of the current confidence set based on unlabeled examples drawn from U .\nLemma 1. Suppose p > 0 and Algorithm 5 is run with failure probability \u03b4 . Then with probability 1\u2212 \u03b4 , (1) the output p\u0302 is such that p\u0302 \u2264 p \u2264 2p\u0302. (2) The total number of calls to O is at most O( 1p2 ln 1 \u03b4 p).\nProof. Consider the event\nE = { for all i \u2208 N, |p\u0302i \u2212 p| \u2264\n\u221a\n4ln 2\u00b72 i\n\u03b4 2i }\nAlgorithm 5 Adaptive Procedure for Estimating the Bias of a Coin 1: Input: failure probability \u03b4 , an oracle O which returns iid Bernoulli random variables with unknown\nbias p. 2: Output: p\u0302, an estimate of bias p such that p\u0302 \u2264 p \u2264 2p\u0302 with probability \u2265 1\u2212\u03b4 . 3: for i = 1,2, . . . do 4: Call the oracle O 2i times to get empirical frequency p\u0302i.\n5: if\n\u221a\n4ln 4\u00b72 i\n\u03b4 2i \u2264 p\u0302i/3 then return p\u0302 = 2p\u0302i 3\n6: end if 7: end for\nBy Equation (14) and union bound, P(E)\u2265 1\u2212\u03b4 . On event E , we claim that if i is large enough that\n4\n\u221a\n4ln 4\u00b72 i\n\u03b4 2i \u2264 p (15)\nthen the condition in line 5 will be met. Indeed, this implies\n\u221a\n4ln 4\u00b72 i\n\u03b4 2i\n\u2264 p\u2212\n\u221a\n4ln 4\u00b72 i\n\u03b4 2i\n3 \u2264 p\u0302i 3\nDefine i0 as the smallest number i such that Equation (15) is true. Then by algebra, 2i0 = O( 1p2 ln 1 \u03b4 p). Hence the number of calls to oracle O is at most 1+2+ . . .+2i0 = O( 1p2 ln 1\n\u03b4 p). Consider the smallest i\u2217 such that the condition in line 5 is met. We have that\n\u221a\n4ln 4\u00b72 i\u2217\n\u03b4 2i\u2217 \u2264 p\u0302i\u2217/3\nBy the definition of E , |p\u2212 p\u0302i\u2217 | \u2264 p\u0302i\u2217/3\nthat is, 2p\u0302i\u2217/3 \u2264 p \u2264 4p\u0302i\u2217/3, implying p\u0302 \u2264 p \u2264 2p\u0302."}, {"heading": "A.3 Notations on Datasets", "text": "Without loss of generality, assume the examples drawn throughout Algorithm 1 have distinct feature values x, since this happens with probability 1 under mild assumptions.\nAlgorithm 1 uses a mixture of three kinds of labeled data to learn a target classifier \u2013 labels obtained from querying O, labels inferred by the algorithm, and labels obtained from querying W . To analyze the effect of these three kinds of labeled data, we need to introduce some notation.\nRecall that we define the joint distribution D over examples and labels both from O and W as follows:\nPD(x,yO,yW ) = PU(x)PO(yO|x)PW (yW |x)\nwhere given an example x, the labels generated by O and W are conditionally independent.\nA dataset S\u0302 with empirical error minimizer h\u0302 and a rejection threshold \u03c4 define a implicit confidence set for h\u2217 as follows: V (S\u0302,\u03c4) = {h : err(h, S\u0302)\u2212 err(h\u0302, S\u0302)\u2264 \u03c4} At the beginning of epoch k, we have S\u0302k\u22121. h\u0302k\u22121 is defined as the empirical error minimizer of S\u0302k\u22121. The disagreement region of the implicit confidence set at epoch k, Rk\u22121 is defined as Rk\u22121 := DIS(V (S\u0302k\u22121,3\u03b5k/2)). Algorithm 4 in disagr region(S\u0302k\u22121,3\u03b5k/2,x) provides a test deciding if an unlabeled example x is inside Rk\u22121 in epoch k. (See Lemma 6.)\nDefine Ak to be the distribution D conditioned on the set {(x,yO,yW ) : x \u2208 Rk\u22121}. At epoch k, Algorithm 2 has inputs distribution U , oracles W and O, target false negative error \u03b5 = \u03b5k/128, hypothesis class H d f , confidence \u03b4 = \u03b4k/2, previous labeled dataset S\u0302k\u22121, and outputs a difference classfier h\u0302d fk . By the setting of m in Equation (1), Algorithm 2 first computes p\u0302k using unlabeled examples drawn from U , which is an estimator of PD (x \u2208 Rk\u22121). Then it draws a subsample of size\nmk,1 = 64 \u00b71024p\u0302k\n\u03b5k (d ln 512 \u00b71024p\u0302k \u03b5k + ln 144 \u03b4k ) (16)\niid from Ak. We call the resulting dataset A \u2032k . At epoch k, Algorithm 3 performs adaptive subsampling to refine the implicit (1\u2212 \u03b4 )-confidence set. For each round t, it subsamples U to get an unlabeled dataset St,Uk of size 2 t . Define the corresponding (hypothetical) dataset with labels queried from both W and O as S tk . S t k, the (hypothetical) dataset with labels queried from O, is defined as:\nStk = {(x,yO)|(x,yO,yW ) \u2208 S tk}\nIn addition to obtaining labels from O, the algorithm obtains labels in two other ways. First, if an x \u2208 X \\Rk\u22121, then its label is safely inferred and with high probability, this inferred label h\u0302k\u22121(x) is equal to h\u2217(x). Second, if an x lies in Rk\u22121 but if the difference classifier h\u0302 d f k predicts agreement between O and W , then its label is obtained by querying W . The actual dataset S\u0302tk generated by Algorithm 3 is defined as:\nS\u0302tk = {(x, h\u0302k\u22121(x))|(x,yO,yW ) \u2208 S tk ,x /\u2208 Rk\u22121}\u222a{(x,yO)|(x,yO,yW ) \u2208 S tk ,x \u2208 Rk\u22121, h\u0302d fk (x) = +1} \u222a{(x,yW )|(x,yO,yW ) \u2208 S tk ,x \u2208 Rk\u22121, h\u0302d fk (x) =\u22121}\nWe use D\u0302k to denote the labeled data distribution as follows:\nPD\u0302k (x,y) = PU(x)PQ\u0302k(y|x)\nPQ\u0302k (y|x) =\n\n \n  I(h\u0302k\u22121(x) = y), x /\u2208 Rk\u22121 PO(y|x), x \u2208 Rk\u22121, h\u0302d fk (x) = +1 PW (y|x), x \u2208 Rk\u22121, h\u0302d fk (x) =\u22121\nTherefore, S\u0302tk can be seen as a sample of size 2 t drawn iid from D\u0302k.\nObserve that h\u0302tk is obtained by training an ERM classifier over S\u0302 t k, and \u03b4 t k = \u03b4k/2t(t +1). Suppose Algorithm 3 stops at iteration t0(k), then the final dataset returned is S\u0302k = S\u0302 t0(k) k , with a total\nnumber of mk,2 label requests to O. We define Sk = S t0(k) k , Sk = S t0(k) k and \u03c3k = \u03c3(2 t0(k),\u03b4 t0(k)k ).\nFor k = 0, we define the notation S\u0302k differently. S\u03020 is the dataset drawn iid at random from D, with labels queried entirely to O. For notational convenience, define S0 = S\u03020. \u03c30 is defined as \u03c30 = \u03c3(n0,\u03b40), where \u03c3(\u00b7, \u00b7) is defined by Equation (10) and n0 is defined as:\nn0 = (64 \u00b710242)(2d ln(512 \u00b710242)+ ln 96 \u03b4 )\nRecall that h\u0302k = argminh\u2208H err(h, S\u0302k) is the empirical error minimizer with respect to the dataset S\u0302k. Note that the empirical distance \u03c1Z(\u00b7, \u00b7) does not depend on the labels in dataset Z, therefore, \u03c1S\u0302k(h,h\n\u2032) = \u03c1Sk(h,h\u2032). We will use them interchangably throughout."}, {"heading": "A.4 Events", "text": "Recall that \u03b4k = \u03b4/(4(k+1)2),\u03b5k = 2\u2212k. Define\nhd fk = h d f 2\u03bd+\u03b5k\u22121,\u03b5k/512\nwhere the notation hd fr,\u03b7 is introduced in Assumption 1. We begin by defining some events that we will condition on later in the proof, and showing that these events occur with high probability. Define event\nE1k := {\nPD(x \u2208 Rk\u22121)/2 \u2264 p\u0302k \u2264 PD (x \u2208 Rk\u22121),\nand For all hd f \u2208 H d f , |PA \u2032k (h d f (x) =\u22121,yO 6= yW )\u2212PAk(hd f (x) =\u22121,yO 6= yW )| \u2264 \u03b5k\n1024PD (x \u2208 Rk\u22121)\n+\n\u221a\nmin(PAk(h d f (x) =\u22121,yO 6= yW ),PA \u2032k (hd f (x) =\u22121,yO 6= yW )) \u03b5k 1024PD (x \u2208 Rk\u22121)\nand |PA \u2032k (h d f (x) = +1)\u2212PAk(hd f (x) = +1)|\n\u2264 \u221a\nmin(PAk(h d f (x) = +1),PA \u2032k (h\nd f (x) = +1)) \u03b5k\n1024PD (x \u2208 Rk\u22121) + \u03b5k 1024PD (x \u2208 Rk\u22121) }\nFact 2. P(E1k )\u2265 1\u2212\u03b4k/2.\nDefine event\nE2k = {\nFor all t \u2208 N, for all h,h\u2032 \u2208 H ,\n|(err(h,Stk)\u2212 err(h\u2032,Stk))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 \u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)\u03c1Stk(h,h \u2032)\nand err(h, S\u0302tk)\u2212 errD\u0302k(h) \u2264 \u03c3(2 t ,\u03b4 tk)+\n\u221a\n\u03c3(2t ,\u03b4 tk)errD\u0302k(h)\nand PS tk (h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u2212PD(h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\n\u2264 \u221a\n\u03b3(2t ,\u03b4 tk)PS tk (h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)+ \u03b3(2t ,\u03b4 tk)\nand PS tk (h\u0302 d f k (x) =\u22121\u2229 x \u2208 Rk\u22121)\u2264 2(PD (h\u0302 d f k (x) =\u22121,x \u2208 Rk\u22121)+ \u03b3(2t ,\u03b4 tk))\n}\nFact 3. P(E2k )\u2265 1\u2212\u03b4k/2.\nWe will also use the following definitions of events in our proof. Define event F0 as\nF0 = { for all h,h\u2032 \u2208H , |(err(h,S0)\u2212err(h\u2032,S0))\u2212(errD(h)\u2212errD(h\u2032))| \u2264\u03c3(n0,\u03b40)+ \u221a \u03c3(n0,\u03b40)\u03c1S0(h,h\u2032) }\nFor k \u2208 {1,2, . . . ,k0}, event Fk is defined inductively as\nFk = Fk\u22121 \u2229 (E1k \u2229E2k )\nFact 4. For k \u2208 {0,1, . . . ,k0}, P(Fk)\u2265 1\u2212\u03b40 \u2212\u03b41 \u2212 . . .\u2212\u03b4k. Specifically, P(Fk0)\u2265 1\u2212\u03b4 .\nThe proofs of Facts 2, 3 and 4 are provided in Appendix E."}, {"heading": "B Proof Outline and Main Lemmas", "text": "The main idea of the proof is to maintain the following three invariants on the outputs of Algorithm 1 in each epoch. We prove that these invariants hold simultaneously for each epoch with high probability by induction over the epochs. Throughout, for k \u2265 1, the end of epoch k refers to the end of execution of line 13 of Algorithm 1 at iteration k. The end of epoch 0 refers to the end of execution of line 5 in Algorithm 1.\nInvariant 1 states that if we replace the inferred labels and labels obtained from W in S\u0302k by those obtained from O (thus getting the dataset Sk), then the excess errors of classifiers in H will not decrease by much. Invariant 1 (Approximate Favorable Bias). Let h be any classifier in H , and h\u2032 be another classifier in H with excess error on D no greater than \u03b5k. Then, at the end of epoch k, we have:\nerr(h,Sk)\u2212 err(h\u2032,Sk)\u2264 err(h, S\u0302k)\u2212 err(h\u2032, S\u0302k)+ \u03b5k/16\nInvariant 2 establishes that in epoch k, Algorithm 3 selects enough examples so as to ensure that concentration of empirical errors of classifiers in H on Sk to their true errors.\nInvariant 2 (Concentration). At the end of epoch k, S\u0302k, Sk and \u03c3k are such that: 1. For any pair of classifiers h,h\u2032 \u2208 H , it holds that:\n|(err(h,Sk)\u2212 err(h\u2032,Sk))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 \u03c3k + \u221a \u03c3k\u03c1Sk(h,h\u2032) (17)\n2. The dataset S\u0302k has the following property:\n\u03c3k + \u221a \u03c3kerr(h\u0302k, S\u0302k)\u2264 \u03b5k/512 (18)\nFinally, Invariant 3 ensures that the difference classifier produced in epoch k has low false negative error on the disagreement region of the (1\u2212\u03b4 ) confidence set at epoch k. Invariant 3 (Difference Classifier). At epoch k, the difference classifier output by Algorithm 2 is such that\nPD(h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u2264 \u03b5k/64 (19)\nPD(h\u0302 d f k (x) = +1,x \u2208 Rk\u22121)\u2264 6(\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k/1024) (20)\nWe will show the following property about the three invariants. Its proof is deferred to Subsection B.4.\nLemma 2. There is a numerical constant c0 > 0 such that the following holds. The collection of events {Fk}k0k=0 is such that for k \u2208 {0,1, . . . ,k0}: (1) If k = 0, then on event Fk, at epoch k,\n(1.1) Invariants 1,2 hold. (1.2) The number of label requests to O is at most m0 \u2264 c0(d + ln 1\u03b4 ).\n(2) If k \u2265 1, then on event Fk, at epoch k, (2.1) Invariants 1,2,3 hold. (2.2) the number of label requests to O is at most\nmk \u2264 c0 ((\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/1024)+ \u03b5k)(\u03bd + \u03b5k)\n\u03b52k d(ln2 1 \u03b5k + ln2 1 \u03b4k )+ PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) \u03b5k (d\u2032 ln 1 \u03b5k + ln 1 \u03b4k ) )"}, {"heading": "B.1 Active Label Inference and Identifying the Disagreement Region", "text": "We begin by proving some lemmas about Algorithm 4 which identifies if an example lies in the disagreement region of the current confidence set. This is done by using a constrained ERM oracle CONS-LEARNH(\u00b7, \u00b7) using ideas similar to [10, 15, 3, 4].\nLemma 3. When given as input a dataset S\u0302, a threshold \u03c4 > 0, an unlabeled example x, Algorithm 4 in disagr region returns 1 if and only if x lies inside DIS(V (S\u0302,\u03c4)).\nProof. (\u21d2) If Algorithm 4 returns 1, then we have found a classifier h\u0302\u2032x such that (1) h\u0302x(x) =\u2212h\u0302(x), and (2) err(h\u0302\u2032x, S\u0302)\u2212 err(h\u0302, S\u0302)\u2264 \u03c4 , i.e. h\u0302\u2032x \u2208V (S\u0302,\u03c4). Therefore, x is in DIS(V (S\u0302,\u03c4)). (\u21d0) If x is in DIS(V (S\u0302,\u03c4)), then there exists a classifier h\u2208H such that (1) h(x) =\u2212h\u0302(x) and (2) err(h, S\u0302)\u2212 err(h\u0302, S\u0302)\u2264 \u03c4 . Hence by definition of h\u0302\u2032x, err(h\u0302\u2032x, S\u0302)\u2212 err(h\u0302, S\u0302)\u2264 \u03c4 . Thus, Algorithm 4 returns 1.\nWe now provide some lemmas about the behavior of Algorithm 4 called at epoch k.\nLemma 4. Suppose Invariants 1 and 2 hold at the end of epoch k\u2212 1. If h \u2208 H is such that errD(h) \u2264 errD(h\u2217)+ \u03b5k\u22121/2, then\nerr(h, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121)\u2264 3\u03b5k\u22121/4\nProof. If h \u2208 H has excess error at most \u03b5k\u22121/2 with respect to D, then,\nerr(h, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121) \u2264 err(h,Sk\u22121)\u2212 err(h\u0302k\u22121,Sk\u22121)+ \u03b5k\u22121/16 \u2264 errD(h)\u2212 errD(h\u0302k\u22121)+\u03c3k\u22121 + \u221a \u03c3k\u22121\u03c1Sk\u22121(h, h\u0302k\u22121)+ \u03b5k\u22121/16 \u2264 \u03b5k\u22121/2+\u03c3k\u22121 + \u221a \u03c3k\u22121\u03c1Sk\u22121(h, h\u0302k\u22121)+ \u03b5k\u22121/16 \u2264 9\u03b5k\u22121/16+\u03c3k\u22121 + \u221a \u03c3k\u22121err(h, S\u0302k\u22121)+ \u221a \u03c3k\u22121err(h\u0302k\u22121, S\u0302k\u22121)\n\u2264 9\u03b5k\u22121/16+\u03c3k\u22121 + \u221a \u03c3k\u22121err(h, S\u0302k\u22121)+ \u221a \u03c3k\u22121(err(h\u0302k\u22121, S\u0302k\u22121)+9\u03b5k\u22121/16)\nWhere the first inequality follows from Invariant 1, the second inequality from Equation (17) of Invariant 2, the third inequality from the assumption that h has excess error at most \u03b5k\u22121/2, and the fourth inequality from the triangle inequality, the fifth inequality is by adding a nonnegative number in the last term. Continuing,\nerr(h, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121)\n\u2264 9\u03b5k\u22121/16+4\u03c3k\u22121 +2 \u221a \u03c3k\u22121(err(h\u0302k\u22121, S\u0302k\u22121)+9\u03b5k\u22121/16) \u2264 9\u03b5k\u22121/16+4\u03c3k\u22121 +2 \u221a \u03c3k\u22121err(h\u0302k\u22121, S\u0302k\u22121)+2 \u221a\n\u03b5k\u22121/512 \u00b79\u03b5k\u22121/16 \u2264 9\u03b5k\u22121/16+ \u03b5k\u22121/32+2 \u221a\n\u03b5k\u22121/512 \u00b79\u03b5k\u22121/16 \u2264 3\u03b5k\u22121/4\nWhere the first inequality is by simple algebra (by letting D = err(h, S\u0302k\u22121), E = err(h\u0302k\u22121, S\u0302k\u22121)+9\u03b5k\u22121/16, F = \u03c3k\u22121 in D \u2264 E +F + \u221a DF + \u221a EF \u21d2 D \u2264 E +4F +2 \u221a EF), the second inequality is from \u221a A+B \u2264\u221a\nA+ \u221a\nB and \u03c3k\u22121 \u2264 \u03b5k\u22121/512 which utilizes Equation (18) of Invariant 2, the third inequality is again by Equation (18) of Invariant 2, the fourth inequality is by algebra.\nLemma 5. Suppose Invariants 1 and 2 hold at the end of epoch k\u22121. Then,\nerrD(h\u0302k\u22121)\u2212 errD(h\u2217)\u2264 \u03b5k\u22121/8\nProof. By Lemma 4, we know that since h\u2217 has excess error 0 with respect to D,\nerr(h\u2217, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121)\u2264 3\u03b5k\u22121/4 (21)\nTherefore,\nerrD(h\u0302k\u22121)\u2212 errD(h\u2217)\n\u2264 err(h\u0302k\u22121,Sk\u22121)\u2212 err(h\u2217,Sk\u22121)+\u03c3k\u22121 + \u221a \u03c3k\u22121\u03c1Sk\u22121(h\u0302k\u22121,h\u2217) \u2264 err(h\u0302k\u22121, S\u0302k\u22121)\u2212 err(h\u2217, S\u0302k\u22121)+\u03c3k\u22121 + \u221a \u03c3k\u22121\u03c1Sk\u22121(h\u0302k\u22121,h\u2217)+ \u03b5k\u22121/16 \u2264 \u03b5k\u22121/16+\u03c3k\u22121 + \u221a \u03c3k\u22121(err(h\u0302k\u22121, S\u0302k\u22121)+ err(h\u2217, S\u0302k\u22121)) \u2264 \u03b5k\u22121/16+\u03c3k\u22121 + \u221a \u03c3k\u22121(2err(h\u0302k\u22121, S\u0302k\u22121)+3\u03b5k\u22121/4) \u2264 \u03b5k\u22121/16+\u03c3k\u22121 + \u221a 2\u03c3k\u22121err(h\u0302k\u22121, S\u0302k\u22121)+ \u221a\n\u03b5k\u22121/512 \u00b73\u03b5k\u22121/4 \u2264 \u03b5k\u22121/8\nwhere the first inequality is from Equation (17) of Invariant 2, the second inequality uses Invariant 1, the third inequality follows from the optimality of h\u0302k\u22121 and triangle inequality, the fourth inequality uses Equation (21), the fifth inequality uses the fact that \u221a A+B \u2264 \u221a A+ \u221a B and \u03c3k\u22121 \u2264 \u03b5k\u22121/512, which is from Equation (18) of Invariant 2, the last inequality again utilizes the Equation (18) of Invariant 2.\nLemma 6. Suppose Invariants 1, 2, and 3 hold in epoch k\u22121 conditioned on event Fk\u22121. Then conditioned on event Fk\u22121, the implicit confidence set Vk\u22121 =V (S\u0302k\u22121,3\u03b5k/2) is such that: (1) If h \u2208 H satisfies errD(h)\u2212 errD(h\u2217)\u2264 \u03b5k, then h is in Vk\u22121. (2) If h \u2208 H is in Vk\u22121, then errD(h)\u2212 errD(h\u2217)\u2264 \u03b5k\u22121. Hence Vk\u22121 \u2286 BU(h\u2217,2\u03bd + \u03b5k\u22121). (3) Algorithm 4, in disagr region, when run on inputs dataset S\u0302k\u22121, threshold 3\u03b5k/2, unlabeled example x, returns 1 if and only if x is in Rk\u22121.\nProof. (1) Let h be a classifier with errD(h)\u2212 errD(h\u2217) \u2264 \u03b5k = \u03b5k\u22121/2. Then, by Lemma 4, one has err(h, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121)\u2264 3\u03b5k\u22121/4 = 3\u03b5k/2. Hence, h is in Vk\u22121. (2) Fix any h in Vk\u22121, by definition of Vk\u22121,\nerr(h, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121)\u2264 3\u03b5k/2 = 3\u03b5k\u22121/4 (22)\nRecall that from Lemma 5, errD(h\u0302k\u22121)\u2212 errD(h\u2217)\u2264 \u03b5k\u22121/8\nThus for classifier h, applying Invariant 1 by taking h\u2032 := h\u0302k\u22121, we get\nerr(h,Sk\u22121)\u2212 err(h\u0302k\u22121,Sk\u22121)\u2264 err(h, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121)+ \u03b5k\u22121/32 (23)\nTherefore,\nerrD(h)\u2212 errD(h\u0302k\u22121)\n\u2264 err(h,Sk\u22121)\u2212 err(h\u0302k\u22121,Sk\u22121)+\u03c3k\u22121 + \u221a \u03c3k\u22121\u03c1Sk\u22121(h, h\u0302k\u22121) \u2264 err(h,Sk\u22121)\u2212 err(h\u0302k\u22121,Sk\u22121)+\u03c3k\u22121 + \u221a \u03c3k\u22121(err(h, S\u0302k\u22121)+ err(h\u0302k\u22121, S\u0302k\u22121)) \u2264 err(h, S\u0302k\u22121)\u2212 err(h\u0302k\u22121, S\u0302k\u22121)+\u03c3k\u22121 + \u221a \u03c3k\u22121(err(h, S\u0302k\u22121)+ err(h\u0302k\u22121, S\u0302k\u22121))+ \u03b5k\u22121/16 \u2264 13\u03b5k\u22121/16+\u03c3k\u22121 + \u221a \u03c3k\u22121(2err(h\u0302k\u22121, S\u0302k\u22121)+3\u03b5k\u22121/4) \u2264 13\u03b5k\u22121/16+\u03c3k\u22121 + \u221a 2\u03c3k\u22121err(h\u0302k\u22121, S\u0302k\u22121)+ \u221a\n\u03b5k\u22121/512 \u00b73\u03b5k\u22121/4 \u2264 7\u03b5k\u22121/8\nwhere the first inequality is from Equation (17) of Invariant 2, the second inequality uses the fact that \u03c1S\u0302k\u22121(h,h\n\u2032) = \u03c1Sk\u22121(h,h\u2032)\u2264 err(h, S\u0302k\u22121)+err(h\u2032, S\u0302k\u22121) for h,h\u2032 \u2208H , the third inequality uses Equation (23); the fourth inequality is from Equation (22); the fifth inequality is from the fact that \u221a A+B \u2264 \u221a A+ \u221a B and \u03c3k\u22121 \u2264 \u03b5k\u22121/512, which is from Equation (18) of Invariant 2, the last inequality again follows from Equation (18) of Invariant 2 and algebra. In conjunction with the fact that errD(h\u0302k\u22121)\u2212 errD(h\u2217)\u2264 \u03b5k\u22121/8, this implies\nerrD(h)\u2212 errD(h\u2217)\u2264 \u03b5k\u22121\nBy triangle inequality, \u03c1(h,h\u2217)\u2264 2\u03bd + \u03b5k\u22121, hence h \u2208 BU(h\u2217,2\u03bd + \u03b5k\u22121). In summary Vk\u22121 \u2286 BU(h\u2217,2\u03bd + \u03b5k\u22121). (3) Follows directly from Lemma 3 and the fact that Rk\u22121 = DIS(Vk\u22121)."}, {"heading": "B.2 Training the Difference Classifier", "text": "Recall that \u2206(r) = DIS(BU(h\u2217,r)) is the disagreement region of the disagreement ball centered around h\u2217 with radius r.\nLemma 7 (Difference Classifier Invariant). There is a numerical constant c1 > 0 such that the following holds. Suppose that Invariants 1 and 2 hold at the end of epoch k \u2212 1 conditioned on event Fk\u22121 and that Algorithm 2 has inputs unlabeled data distribution U, oracle O, \u03b5 = \u03b5k/128, hypothesis class H d f , \u03b4 = \u03b4k/2, previous labeled dataset S\u0302k\u22121. Then conditioned on event Fk, (1) h\u0302d fk , the output of Algorithm 2, maintains Invariant 3. (2)(Label Complexity: Part 1.) The number of label queries made to O is at most\nmk,1 \u2264 c1 ( PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) \u03b5k (d\u2032 ln 1 \u03b5k + ln 1 \u03b4k ) )\nProof. (1) Recall that Fk = Fk\u22121 \u2229E1k \u2229E2k , where E1k , E2k are defined in Subsection A.4. Suppose event Fk happens.\nProof of Equation (19). Recall that h\u0302d fk is the optimal solution of optimization problem (2). We have by feasibility and the fact that on event E3k , 2p\u0302k \u2265 PD(x \u2208 Rk\u22121),\nPA \u2032k (h\u0302d fk (x) =\u22121,yO 6= yW )\u2264 \u03b5k 256p\u0302k \u2264 \u03b5k 128PD (x \u2208 Rk\u22121)\nBy definition of event E2k , this implies\nPAk(h\u0302 d f k (x) =\u22121,yO 6= yW )\n\u2264 PA \u2032k (h\u0302 d f k (x) =\u22121,yO 6= yW )+\n\u221a\nPA \u2032k (h\u0302d fk (x) =\u22121,yO 6= yW ) \u03b5k 1024PD (x \u2208 Rk\u22121) + \u03b5k\n1024PD (x \u2208 Rk\u22121) \u2264 \u03b5k\n64PD (x \u2208 Rk\u22121)\nIndicating\nPD(h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u2264 \u03b5k 64\nProof of Equation (20). By definition of hd fk in Subsection A.4, h d f k is such that:\nPD (h d f k (x) = +1,x \u2208 \u2206(2\u03bd + \u03b5k\u22121))\u2264 \u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)\nPD(h d f k (x) =\u22121,yO 6= yW ,x \u2208 \u2206(2\u03bd + \u03b5k\u22121))\u2264 \u03b5k/512\nBy item (2) of Lemma 6, we have Rk\u22121 \u2286 DIS(BU(h\u2217,2\u03bd + \u03b5k\u22121)), thus\nPD(h d f k (x) = +1,x \u2208 Rk\u22121)\u2264 \u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512) (24)\nPD (h d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u2264 \u03b5k/512 (25)\nEquation (25) implies that\nPAk(h d f k (x) =\u22121,yO 6= yW )\u2264 \u03b5k 512PD (x \u2208 Rk\u22121)\n(26)\nRecall that A \u2032k is the dataset subsampled from Ak in line 3 of Algorithm 2. By definition of event E 1 k , we have that for hd fk ,\nPA \u2032k (hd fk (x) =\u22121,yO 6= yW )\n\u2264 PAk(h d f k (x) =\u22121,yO 6= yW )+\n\u221a\nPAk(h d f k (x) =\u22121,yO 6= yW ) \u03b5k 1024PD (x \u2208 Rk\u22121) + \u03b5k\n1024PD (x \u2208 Rk\u22121) \u2264 \u03b5k\n256PD (x \u2208 Rk\u22121) \u2264 \u03b5k 256p\u0302k\nwhere the second inequality is from Equation (26), and the last inequality is from the fact that p\u0302k \u2264 PD(x \u2208 Rk\u22121). Hence, h d f k is a feasible solution to the optimization problem (2). Thus,\nPAk(h\u0302 d f k (x) = +1)\n\u2264 PA \u2032k (h\u0302 d f k (x) = +1)+\n\u221a\nPA \u2032k (h\u0302d fk (x) = +1) \u03b5k 1024PD (x \u2208 Rk\u22121) + \u03b5k\n1024PD (x \u2208 Rk\u22121) \u2264 2(PA \u2032k (h\u0302 d f k (x) = +1)+ \u03b5k 1024PD (x \u2208 Rk\u22121) )\n\u2264 2(PA \u2032k (h d f k (x) = +1)+ \u03b5k 1024PD (x \u2208 Rk\u22121) )\n\u2264 2((PAk(h d f k (x) = +1)+\n\u221a\nPAk(h d f k (x) = +1) \u03b5k 1024PD (x \u2208 Rk\u22121) + \u03b5k 1024PD (x \u2208 Rk\u22121) )+ \u03b5k 1024PD (x \u2208 Rk\u22121) )\n\u2264 6(PAk(h d f k (x) = +1)+ \u03b5k 1024PD (x \u2208 Rk\u22121) )\nwhere the first inequality is by definition of event E1k , the second inequality is by algebra, the third inequality is by optimality of h\u0302d fk in (2), PA \u2032k (h\u0302 d f k (x) = +1)\u2264 PA \u2032k (h d f k (x) = +1), the fourth inequality is by definition of event E1k , the fifth inequality is by algebra. Therefore,\nPD(h\u0302 d f k (x)=+1,x\u2208Rk\u22121)\u2264 6(PD (h d f k (x)=+1,x\u2208Rk\u22121)+\u03b5k/1024)\u2264 6(\u03b1(2\u03bd +\u03b5k\u22121,\u03b5k/512)+\u03b5k/1024) (27) where the second inequality follows from Equation (24). This establishes the correctness of Invariant 3. (2) The number of label requests to O follows from line 3 of Algorithm 2 (see Equation (16)). That is, we can choose c1 large enough (independently of k), such that\nmk,1 \u2264 c1 ( PD(x \u2208 Rk\u22121) \u03b5k (d\u2032 ln 1 \u03b5k + ln 1 \u03b4k ) ) \u2264 c1 ( PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) \u03b5k (d\u2032 ln 1 \u03b5k + ln 1 \u03b4k ) )\nwhere in the second step we use the fact that on event Fk, by item (2) of Lemma 6, Rk\u22121 \u2286 DIS(BU(h\u2217,2\u03bd + \u03b5k\u22121)), thus PD(x \u2208 Rk\u22121)\u2264 PD (x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) = PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121))."}, {"heading": "B.3 Adaptive Subsampling", "text": "Lemma 8. There is a numerical constant c2 > 0 such that the following holds. Suppose Invariants 1, 2, and 3 hold in epoch k\u2212 1 on event Fk\u22121; Algorithm 3 receives inputs unlabeled distribution U, classifier h\u0302k\u22121, difference classifier h\u0302d f = h\u0302 d f k , target excess error \u03b5 = \u03b5k, confidence \u03b4 = \u03b4k/2, previous labeled dataset S\u0302k\u22121. Then on event Fk, (1) S\u0302k, the output of Algorithm 3, maintains Invariants 1 and 2. (2) (Label Complexity: Part 2.) The number of label queries to O in Algorithm 3 is at most:\nmk,2 \u2264 c2 ((\u03bd + \u03b5k)(\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k) \u03b52k \u00b7d(ln2 1 \u03b5k + ln2 1 \u03b4k ) )\nProof. (1) Recall that Fk = Fk\u22121 \u2229E1k \u2229E2k , where E1k , E2k are defined in Subsection A.4. Suppose event Fk happens.\nProof of Invariant 1. We consider a pair of classifiers h,h\u2032 \u2208 H , where h is an arbitrary classifier in H and h\u2032 has excess error at most \u03b5k.\nAt iteration t = t0(k) of Algorithm 3, the breaking criteron in line 14 is met, i.e.\n\u03c3(2t0(k),\u03b4 t0(k)k )+ \u221a \u03c3(2t0(k),\u03b4 t0(k)k )err(h\u0302t0(k), S\u0302 t0(k) k )\u2264 \u03b5k/512 (28)\nFirst we expand the definition of err(h,Sk) and err(h, S\u0302k) respectively:\nerr(h,Sk)=PSk(h\u0302 d f k (x)=+1,h(x) 6= yO,x\u2208Rk\u22121)+PSk(h\u0302 d f k (x)=\u22121,h(x) 6= yO,x\u2208Rk\u22121)+PSk(h(x) 6= yO,x /\u2208Rk\u22121)\nerr(h, S\u0302k)=PSk(h\u0302 d f k (x)=+1,h(x) 6= yO,x\u2208Rk\u22121)+PSk(h\u0302 d f k (x)=\u22121,h(x) 6= yW ,x\u2208Rk\u22121)+PSk(h(x) 6= h\u2217(x),x /\u2208Rk\u22121)\nwhere we use the fact that by Lemma 6, for all examples x /\u2208 Rk\u22121, h\u0302k\u22121(x) = h\u2217(x). We next show that PSk(h\u0302 d f k (x) = \u22121,h(x) 6= yO,x \u2208 Rk\u22121) is close to PSk(h\u0302 d f k (x) =\u22121,h(x) 6= yW ,x \u2208\nRk\u22121). From Lemma 7, we know that conditioned on event Fk,\nPD(h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u2264 \u03b5k/64\nIn the meantime, from Equation (28), \u03b3(2t0(k),\u03b4 t0(k)k ) \u2264 \u03c3(2t0(k),\u03b4 t0(k) k ) \u2264 \u03b5k/512. Recall that Sk = S t0(k) k . Therefore, by definition of E2k ,\nPSk(h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\n\u2264 PD(h\u0302d fk (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)+ \u221a PD (h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u03b3(2t0(k),\u03b4 t0(k) k )+ \u03b3(2 t0(k),\u03b4 t0(k)k ) \u2264 PD(h\u0302d fk (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)+ \u221a PD (h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u03b5k/512+ \u03b5k/512 \u2264 \u03b5k/32\nBy triangle inequality, for all classifier h0 \u2208 H ,\n|PSk(h\u0302 d f k (x) =\u22121,h0(x) 6= yO,x \u2208 Rk\u22121)\u2212PSk(h\u0302 d f k (x) =\u22121,h0(x) 6= yW ,x \u2208 Rk\u22121)| \u2264 \u03b5k/32 (29)\nSpecifically for h and h\u2032, Equation (29) hold:\n|PSk(h\u0302 d f k (x) =\u22121,h(x) 6= yO,x \u2208 Rk\u22121)\u2212PSk(h\u0302 d f k (x) =\u22121,h(x) 6= yW ,x \u2208 Rk\u22121)| \u2264 \u03b5k/32\n|PSk(h\u0302 d f k (x) =\u22121,h\u2032(x) 6= yO,x \u2208 Rk\u22121)\u2212PSk(h\u0302 d f k (x) =\u22121,h\u2032(x) 6= yW ,x \u2208 Rk\u22121)| \u2264 \u03b5k/32\nCombining, we get:\n(PSk(h\u0302 d f k (x) =\u22121,h(x) 6= yW ,x \u2208 Rk\u22121)\u2212PSk(h\u0302 d f k (x) =\u22121,h\u2032(x) 6= yW ,x \u2208 Rk\u22121)) (30)\n\u2212 (PSk(h\u0302 d f k (x) =\u22121,h(x) 6= yO,x \u2208 Rk\u22121)\u2212PSk(h\u0302 d f k (x) =\u22121,h\u2032(x) 6= yO,x \u2208 Rk\u22121))\u2264 \u03b5k/16\nWe now show the labels inferred in the region X \\Rk\u22121 is \u201cfavorable\u201d to the classifiers whose excess error is at most \u03b5k/2. By triangle inequality,\nPSk(h(x) 6= yO,x /\u2208 Rk\u22121)\u2212PSk(h\u2217(x) 6= yO,x /\u2208 Rk\u22121)\u2264 PSk(h(x) 6= h\u2217(x),x /\u2208 Rk\u22121)\nBy Lemma 6, since h\u2032 has excess error at most \u03b5k, h\u2032 agrees with h\u2217 on all x inside X \\Rk\u22121 on event Fk\u22121, hence PSk(h \u2032(x) 6= h\u2217(x),x /\u2208 Rk\u22121) = 0. This gives\nPSk(h(x) 6= yO,x /\u2208 Rk\u22121)\u2212PSk(h\u2032(x) 6= yO,x /\u2208 Rk\u22121) \u2264 PSk(h(x) 6= h\u2217(x),x /\u2208 Rk\u22121)\u2212PSk(h\u2032(x) 6= h\u2217(x),x /\u2208 Rk\u22121) (31)\nCombining Equations (30) and (31), we conclude that\nerr(h,Sk)\u2212 err(h\u2032,Sk)\u2264 err(h, S\u0302k)\u2212 err(h\u2032, S\u0302k)+ \u03b5k/16\nThis establishes the correctness of Invariant 1.\nProof of Invariant 2. Recall by definition of E2k the following concentration results hold for all t \u2208 N:\n|(err(h,Stk)\u2212 err(h\u2032,Stk))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 \u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)\u03c1Stk(h,h \u2032))\nIn particular, for iteration t0(k) we have\n|(err(h,St0(k)k )\u2212 err(h\u2032,S t0(k) k ))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 \u03c3(2t0(k),\u03b4 t0(k) k )+\n\u221a\n\u03c3(2t0(k),\u03b4 t0(k)k )\u03c1St0(k)k (h,h\u2032)\nRecall that S\u0302k = S\u0302 t0(k) k , h\u0302k = h\u0302 t0(k) k , and \u03c3k = \u03c3(2 t0(k),\u03b4 t0(k)k ), hence the above is equivalent to\n|(err(h,Sk)\u2212 err(h\u2032,Sk))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 \u03c3k + \u221a \u03c3k\u03c1Sk(h,h\u2032) (32)\nEquation (32) establishes the correctness of Equation (17) of Invariant 2. Equation (18) of Invariant 2 follows from Equation (28).\n(2) We define h\u0303k = argminh\u2208H errD\u0302k(h), and define \u03bd\u0303k to be errD\u0302k(h\u0303k). To prove the bound on the number of label requests, we first claim that if t is sufficiently large that\n\u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)\u03bd\u0303k \u2264 \u03b5k/1536 (33)\nthen the algorithm will satisfy the breaking criterion at line 14 of Algorithm 3, that is, for this value of t,\n\u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)err(h\u0302t , S\u0302 t k)\u2264 \u03b5k/512 (34)\nIndeed, by definition of E2k , if event Fk happens,\nerr(h\u0303k, S\u0302 t k)\n\u2264 errD\u0302k(h\u0303k)+\u03c3(2 t ,\u03b4 tk)+\n\u221a\n\u03c3(2t ,\u03b4 tk)errD\u0302k(h\u0303k)\n= \u03bd\u0303k +\u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)\u03bd\u0303k (35)\nTherefore,\n\u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)err(h\u0302 t k, S\u0302 t k)\n\u2264 \u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)err(h\u0303k, S\u0302 t k) \u2264 \u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)(2\u03bd\u0303k +2\u03c3(2t ,\u03b4 t k)) \u2264 3\u03c3(2t ,\u03b4 tk)+2 \u221a\n\u03c3(2t ,\u03b4 tk)\u03bd\u0303k \u2264 \u03b5k/512\nwhere the first inequality is from the optimality of h\u0302tk, the second inequality is from Equation (35), the third inequality is by algebra, the last inequality follows from Equation (33). The claim follows. Next, we solve for the minimum t that satisfies (33), which is an upper bound of t0(k). Fact 1 implies that there is a numerical constant c3 > 0 such that\n2t0(k) \u2264 c3 \u03bd\u0303k + \u03b5k\n\u03b52k (d ln 1 \u03b5k + ln 1 \u03b4k ))\nThus, there is a numerical constant c4 > 0 such that\nt0(k)\u2264 c4(lnd + ln 1 \u03b5k + ln ln 1 \u03b4k )\nHence, there is a numerical constant c5 > 0 (that does not depend on k) such that the following holds. If event Fk happens, then the number of label queries made by Algorithm 3 to O can be bounded as follows:\nmk,2 = t0(k)\n\u2211 t=1 |St,Uk \u2229{x : h\u0302 d f k (x) = +1}\u2229Rk\u22121|\n= t0(k)\n\u2211 t=1 2tPS tk (h\u0302 d f k (x) = +1,x \u2208 Rk\u22121)\n\u2264 t0(k)\n\u2211 t=1 2t(2PD (h\u0302 d f k (x) = +1,x \u2208 Rk\u22121)+2 \u00b74 ln 2\u03b4 tk 2t )\n\u2264 4 \u00b72t0(k)PD(h\u0302d fk (x) = +1,x \u2208 Rk\u22121)+8 \u00b7 t0(k) ln 2\n\u03b4 t0(k)k\n\u2264 c5 ( ( (\u03bd\u0303k + \u03b5k)PD (h\u0302\nd f k (x) = +1,x \u2208 Rk\u22121)\n\u03b52k +1) \u00b7d(ln2 1 \u03b5k + ln2 1 \u03b4k ) )\n\u2264 c5 ( ( (\u03bd\u0303k + \u03b5k) \u00b76(\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k/1024) \u03b52k +1) \u00b7d(ln2 1 \u03b5k + ln2 1 \u03b4k ) )\nwhere the second equality is from the fact that |St,Uk \u2229 {x : h\u0302 d f k (x) = \u22121}\u2229Rk\u22121| = |S t,U k | \u00b7PS tk (h\u0302 d f k (x) = \u22121,x \u2208 Rk\u22121), in conjunction with |St,Uk |= 2t ; the first inequality is by definition of E2k , the second and third inequality is from algebra that t0(k) ln 1\n\u03b4 t0(k)k \u2264 c5d(ln2 1\u03b5k + ln 2 1 \u03b4k ) for some constant c5 > 0, along with the\nchoice of c2, the fourth step is from Lemma 7 which states that Invariant 3 holds at epoch k. What remains to be argued is an upper bound on \u03bd\u0303k. Note that\n\u03bd\u0303k = min\nh\u2208H [PD(h\u0302\nd f k (x) =\u22121,h(x) 6= yW ,x \u2208 Rk\u22121)+PD(h\u0302 d f k (x) = +1,h(x) 6= yO,x \u2208 Rk\u22121)+PD(h(x) 6= h\u2217(x),x /\u2208 Rk\u22121)]\n\u2264 PD(h\u0302d fk (x) =\u22121,h\u2217(x) 6= yW ,x \u2208 Rk\u22121)+PD(h\u0302 d f k (x) = +1,h \u2217(x) 6= yO,x \u2208 Rk\u22121) \u2264 PD(h\u0302d fk (x) =\u22121,h\u2217(x) 6= yO,x \u2208 Rk\u22121)+PD(h\u0302 d f k (x) = +1,h\n\u2217(x) 6= yO,x \u2208 Rk\u22121)+ \u03b5k/64 \u2264 PD(h\u0302d fk (x) =\u22121,h\u2217(x) 6= yO,x \u2208 Rk\u22121)+PD(h\u0302 d f k (x) = +1,h\n\u2217(x) 6= yO,x \u2208 Rk\u22121)+PD(h(x) 6= yO,x /\u2208 Rk\u22121)+ \u03b5k/64 = \u03bd + \u03b5k/64\nwhere the first step is by definition of errD\u0302k(h), the second step is by the suboptimality of h \u2217, the third step is by Equation (29), the fourth step is by adding a positive term PD(h(x) 6= yO,x /\u2208 Rk\u22121), the fifth step is by definition of errD(h). Therefore, we conclude that there is a numerical constant c2 > 0, such that mk,2, the number of label requests to O in Algorithm 3 is at most\nc2 ((\u03bd + \u03b5k)(\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k) \u03b52k \u00b7d(ln2 1 \u03b5k + ln2 1 \u03b4k ) )"}, {"heading": "B.4 Putting It Together \u2013 Consistency and Label Complexity", "text": "Proof of Lemma 2. With foresight, pick c0 > 0 to be a large enough constant. We prove the result by induction.\nBase case. Consider k = 0. Recall that F0 is defined as\nF0 = { for all h,h\u2032 \u2208H , |(err(h,S0)\u2212err(h\u2032,S0))\u2212(errD(h)\u2212errD(h\u2032))| \u2264\u03c3(n0,\u03b40)+ \u221a \u03c3(n0,\u03b40)\u03c1S0(h,h\u2032) }\nNote that by definition in Subsection A.3, S\u03020 = S0. Therefore Invariant 1 trivially holds. When F0 happens, Equation (17) of Invariant 2 holds, and n0 is such that \u221a \u03c30 \u2264 \u03b50/1024, thus,\n\u03c30 + \u221a \u03c30err(h\u03020, S\u03020)\u2264 \u03b50/512\nwhich establishes the validity of Equation (18) of Invariant 2. Meanwhile, the number of label requests to O is\nn0 = 64 \u00b710242(d ln(512 \u00b710242)+ ln 96 \u03b4 ))\u2264 c0(d + ln 1 \u03b4 )\nInductive case. Suppose the claim holds for k\u2032 < k. The inductive hypothesis states that Invariants 1,2,3 hold in epoch k\u2212 1 on event Fk\u22121. By Lemma 7 and Lemma 8, Invariants 1,2,3 holds in epoch k on event\nFk. Suppose Fk happens. By Lemma 7, there is a numerical constant c1 > 0 such that the number of label queries in Algorithm 2 in line 12 is at most\nmk,1 \u2264 c1 ( PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) \u03b5k (d\u2032 ln 1 \u03b5k + ln 1 \u03b4k ) )\nMeanwhile, by Lemma 8, there is a numerical constant c2 > 0 such that the number of label queries in Algorithm 3 in line 14 is at most\nmk,2 \u2264 c2 ((\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k)(\u03bd + \u03b5k) \u03b52k \u00b7d(ln2 1 \u03b5k + ln2 1 \u03b4k ) )\nThus, the number of label requests in total at epoch k is at most\nmk = mk,1 +mk,2\n\u2264 c0 ( ( \u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k)(\u03bd + \u03b5k)\n\u03b52k d(ln2 1 \u03b5k + ln2 1 \u03b4k )+ PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) \u03b5k (d\u2032 ln 1 \u03b5k + ln 1 \u03b4k ) )\nThis completes the induction.\nTheorem 3 (Consistency). If Fk0 happens, then the classifier h\u0302 returned by Algorithm 1 is such that\nerrD(h\u0302)\u2212 errD(h\u2217)\u2264 \u03b5\nProof. By Lemma 2, Invariants 1, 2, 3 hold at epoch k0. Thus by Lemma 5,\nerrD(h\u0302)\u2212 errD(h\u2217) = errD(h\u0302k0)\u2212 errD(h\u2217)\u2264 \u03b5k0/8 \u2264 \u03b5\nProof of Theorem 1. This is an immediate consequence of Theorem 3.\nTheorem 4 (Label Complexity). If Fk0 happens, then the number of label queries made by Algorithm 1 to O is at most\nO\u0303((sup r\u2265\u03b5 \u03b1(2\u03bd + r,r/1024) 2\u03bd + r )d( \u03bd2 \u03b52 +1)+ (sup r\u2265\u03b5 PU(x \u2208 \u2206(2\u03bd + r)) 2\u03bd + r )d\u2032( \u03bd \u03b5 +1))\nProof. Conditioned on event Fk0 , we bound the sum \u2211 k0 k=0 mk.\nk0\n\u2211 k=0 mk\n\u2264 c0(d + ln 1 \u03b4 )+ c0\n( k0\n\u2211 k=1 (\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k)(\u03bd + \u03b5k) \u03b52k d(ln2 1 \u03b5k + ln2 1 \u03b4k )+ PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) \u03b5k (d\u2032 ln 1 \u03b5k + ln 1 \u03b4k ) )\n\u2264 c0(d + ln 1 \u03b4 )+ c0\n( k0\n\u2211 k=1 (\u03b1(2\u03bd + \u03b5k\u22121,\u03b5k/512)+ \u03b5k)(\u03bd + \u03b5k) \u03b52k d(3ln2 1 \u03b5 + 2ln2 1 \u03b4 )+ PU(x \u2208 \u2206(2\u03bd + \u03b5k\u22121)) \u03b5k (2d\u2032 ln 1 \u03b5 + ln 1 \u03b4 ) )\n\u2264 (sup r\u2265\u03b5 \u03b1(2\u03bd + r,r/1024)+ r 2\u03bd + r )d(3ln2 1 \u03b5 + 2ln2 1 \u03b4 )\nk0\n\u2211 k=0\n(\u03bd + \u03b5k)2\n\u03b52k + sup r\u2265\u03b5\nPU(x \u2208 \u2206(2\u03bd + r)) 2\u03bd + r (2d\u2032 ln 1 \u03b5 + ln 1 \u03b4 )\nk0\n\u2211 k=0 (\u03bd + \u03b5k) \u03b5k\n\u2264 O\u0303((sup r\u2265\u03b5 \u03b1(2\u03bd + r,r/1024)+ r 2\u03bd + r )d( \u03bd2 \u03b52 + 1)+ (sup r\u2265\u03b5 PU(x \u2208 \u2206(2\u03bd + r)) 2\u03bd + r )d\u2032( \u03bd \u03b5 + 1))\nwhere the first inequality is by Lemma 2, the second inequality is by noticing for all k \u2265 1, ln2 1\u03b5k + ln 2 1 \u03b4k \u2264 3ln2 1\u03b5 +2ln 2 1 \u03b4 and d \u2032 ln 1\u03b5k + ln 1 \u03b4k \u2264 2d \u2032 ln 1\u03b5 + ln 1 \u03b4 , the rest of the derivations follows from standard algebra.\nProof of Theorem 2. Item 1 is an immediate consequence of Lemma 2, whereas item 2 is a consequence of Theorem 4."}, {"heading": "C Case Study: Linear Classfication under Uniform Distribution over Unit Ball", "text": "We remind the reader the setting of our example in Section 4. H is the class of homogeneous linear separators on the d-dimensional unit ball and H d f is defined to be {h\u2206h\u2032 : h,h\u2032 \u2208 H }. Note that d\u2032 is at most 5d. Furthermore, U is the uniform distribution over the unit ball. O is a deterministic labeler such that errD(h\u2217) = \u03bd > 0, W is such that there exists a difference classifier h\u0304d f with false negative error 0 for which PrU(h\u0304d f (x) = 1)\u2264 g = o( \u221a d\u03bd). We prove the label complexity bound provided by Corollary 1.\nProof of Corollary 1. We claim that under the assumptions of Corollary 1, \u03b1(2\u03bd + r,r/1024) is at most g. Indeed, by taking hd f = h\u0304d f , observe that\nP(h\u0304d f (x) =\u22121,yW 6= yO,x \u2208 \u2206(2\u03bd + r))\u2264 P(h\u0304d f (x) =\u22121,yW 6= yO) = 0\nP(h\u0304d f (x) = +1,x \u2208 \u2206(2\u03bd + r))\u2264 g This shows that \u03b1(2\u03bd + r,0)\u2264 g. Hence, \u03b1(2\u03bd + r,r/1024) \u2264 \u03b1(2\u03bd + r,0)\u2264 g. Therefore,\nsup r:r\u2265\u03b5 \u03b1(2\u03bd + r,r/1024)+ r 2\u03bd + r \u2264 sup r\u2265\u03b5 g+ r \u03bd + r \u2264 max( g \u03bd ,1)\nRecall that the disagreement coefficient \u03b8(2\u03bd + r) \u2264 \u221a\nd for all r, and d\u2032 \u2264 5d. Thus, by Theorem 2, the number of label queries to O is at most\nO\u0303\n(\nd max( g \u03bd ,1)(\n\u03bd2 \u03b52 +1)+d3/2 ( 1+ \u03bd \u03b5 ) )"}, {"heading": "D Performance Guarantees for Learning with Respect to Data labeled by O and W", "text": "An interesting variant of our model is to consider learning from data labeled by a mixture of O and W . Let DW be the distribution over labeled examples determined by U and W , specifically, PDW (x,y) = PU(x)PW (y|x). Let D\u2032 be a mixture of D and DW , specifically D\u2032 = (1\u2212\u03b2 )D+\u03b2DW , for some parameter \u03b2 > 0. Define h\u2032 to be the best classifier with respect to D\u2032, and denote by \u03bd \u2032 the error of h\u2032 with respect to D\u2032.\nLet O\u2032 be the following mixture oracle. Given an example x, the label yO\u2032 is generated as follows. O\u2032 flips a coin with bias \u03b2 . If it comes up heads, it queries W for the label of x and returns the result; otherwise O is queried and the result returned. It is immediate that the conditional probability induced by O\u2032 is PO\u2032(y|x) = (1\u2212\u03b2 )PO(y|x)+\u03b2PW (y|x), and D\u2032(x,y) = PO\u2032(y|x)PU (x).\nAssumption 2. For any r,\u03b7 > 0, there exists an hd f\u03b7 ,r \u2208 H d f with the following properties:\nPD(h d f \u03b7 ,r(x) =\u22121,x \u2208 \u2206(r),yO\u2032 6= yW )\u2264 \u03b7\nPD (h d f \u03b7 ,r(x) = 1,x \u2208 \u2206(r))\u2264 \u03b1 \u2032(r,\u03b7)\nRecall that the disagreement coefficient \u03b8(r) at scale r is \u03b8(r) = suph\u2208H supr\u2032\u2265r PU (DIS(BU (h,r\u2032))\nr\u2032 , which only depends on the unlabeled data distribution U and does not depend on W or O.\nWe have the following corollary.\nCorollary 3 (Learning with respect to Mixture). Let d be the VC dimension of H and let d\u2032 be the VC dimension of H d f . If Assumption 2 holds, and if the error of the best classifier in H on D\u2032 is at most \u03bd \u2032. Algorithm 1 is run with inputs unlabeled distribution U, target excess error \u03b5 , confidence \u03b4 , labeling oracle O\u2032, weak oracle W, hypothesis class H , hypothesis class for difference classifier H d f , confidence \u03b4 . Then with probability \u2265 1\u22122\u03b4 , the following hold:\n1. the classifier h\u0302 output by Algorithm 1 satisfies: errD\u2032(h\u0302)\u2264 errD\u2032(h\u2032)+ \u03b5 .\n2. the total number of label queries made by Algorithm 1 to the oracle O is at most:\nO\u0303 ( (1\u2212\u03b2 ) (\nsup r\u2265\u03b5 \u03b1 \u2032(2\u03bd \u2032+ r,r/1024)+ r 2\u03bd \u2032+ r \u00b7d ( \u03bd \u20322 \u03b52 +1 ) +\u03b8(2\u03bd \u2032+ \u03b5)d\u2032 ( \u03bd \u2032 \u03b5 +1 ) ))\nProof Sketch. Consider running Algorithm 1 in the setting above. By Theorem 1 and Theorem 2, there is an event F such that P(F)\u2265 1\u2212\u03b4 , if event F happens, h\u0302, the classifier learned by Algorithm 1 is such that\nerrD\u2032(h\u0302)\u2264 errD\u2032(h\u2032)+ \u03b5\nBy Theorem 2, the number of label requests to O\u2032 is at most\nmO\u2032 = O\u0303 (\nsup r\u2265\u03b5 \u03b1 \u2032(2\u03bd \u2032+ r,r/1024)+ r 2\u03bd \u2032+ r \u00b7d ( \u03bd \u20322 \u03b52 +1 ) +\u03b8(2\u03bd \u2032+ \u03b5)d\u2032 ( \u03bd \u2032 \u03b5 +1 ) )\nSince O\u2032 is simulated by drawing a Bernoulli random variable Zi \u223c Ber(1\u2212\u03b2 ) in each call of O\u2032, if Zi = 1, then return O(x), otherwise return W (x). Define event\nH = { mO\u2032\n\u2211 i=1 Zi \u2264 2((1\u2212\u03b2 )mO\u2032 +4ln 2 \u03b4 )}\nby Chernoff bound, P(H)\u2265 1\u2212\u03b4 . Consider event J = F \u2229H , by union bound, P(J)\u2265 1\u22122\u03b4 . Conditioned on event J, the number of label requests to O is at most \u2211mO\u2032i=1 Zi, which is at most\nO\u0303 ( (1\u2212\u03b2 ) (\nsup r\u2265\u03b5 \u03b1 \u2032(2\u03bd \u2032+ r,r/1024)+ r 2\u03bd \u2032+ r \u00b7d ( \u03bd \u20322 \u03b52 +1 ) +\u03b8(2\u03bd \u2032+ \u03b5)d\u2032 ( \u03bd \u2032 \u03b5 +1 ) ))"}, {"heading": "E Remaining Proofs", "text": "Proof of Fact 2. (1) First by Lemma 1, PD(x \u2208 Rk\u22121)/2 \u2264 p\u0302k \u2264 PD(x \u2208 Rk\u22121) holds with probability 1\u2212 \u03b4k/6. Second, for each classifier hd f \u2208 H d f , define functions f 1hd f , and f 2hd f associated with it. Formally,\nf 1hd f (x,yO,yW ) = I(h d f (x) =\u22121,yO 6= yW )\nf 2hd f (x,yO,yW ) = I(h d f (x) = +1)\nConsider the function class F 1 = { f 1hd f : hd f \u2208 H d f }, F 2 = { f 2hd f : hd f \u2208 H d f }. Note that both F 1 and F 2 have VC dimension d\u2032, which is the same as H d f . We note that A \u2032k is a random sample of size mk drawn iid from Ak. The fact follows from normalized VC inequality on F 1 and F 2 and the choice of mk in Algorithm 2 called in epoch k, along with union bound.\nProof of Fact 3. For fixed t, we note that Stk is a random sample of size 2 t drawn iid from D. By Equation (8), for any fixed t \u2208 N,\nP\n(\nfor all h,h\u2032 \u2208 H , |(err(h,Stk)\u2212 err(h\u2032,Stk))\u2212 (errD(h)\u2212 errD(h\u2032))| \u2264 \u03c3(2t ,\u03b4 tk)+ \u221a \u03c3(2t ,\u03b4 tk)\u03c1Stk(h,h \u2032) ) \u2265 1\u2212 \u03b4 tk/8 (36)\nMeanwhile, for fixed t \u2208N, note that S\u0302tk is a random sample of size 2t drawn iid from D\u0302k. By Equation (8),\nP\n(\nfor all h,h\u2032 \u2208 H ,err(h, S\u0302tk)\u2212 errD\u0302k(h)\u2264 \u03c3(2 t ,\u03b4 tk)+\n\u221a \u03c3(2t ,\u03b4 tk)errD\u0302k(h) ) \u2265 1\u2212\u03b4 tk/8 (37)\nMoreover, for fixed t \u2208 N, note that S tk is a random sample of size 2t drawn iid from D . By Equation (12),\nP\n(\nPS tk (h\u0302d fk (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\u2264 PD(h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)\n+\n\u221a\n\u03b3(2t ,\u03b4 tk)PD (h\u0302 d f k (x) =\u22121,yO 6= yW ,x \u2208 Rk\u22121)+ \u03b3(2t ,\u03b4 tk)\n)\n\u2265 1\u2212\u03b4 tk/8 (38)\nFinally, for fixed t \u2208 N, note that S tk is a random sample of size 2t drawn iid from D . By Equation (12),\nP\n(\nPS tk (h\u0302d fk (x)=\u22121,x\u2208Rk\u22121)\u2264PD(h\u0302 d f k (x)=\u22121,x\u2208Rk\u22121)+\n\u221a\nPD(h\u0302 d f k (x) =\u22121,x \u2208 Rk\u22121)\u03b3(2t ,\u03b4 tk)+\u03b3(2t ,\u03b4 tk)\n)\n\u2265 1\u2212\u03b4 tk/8 (39)\nNote that by algebra,\nPD(h\u0302 d f k (x)=\u22121,x\u2208Rk\u22121)+\n\u221a\nPD (h\u0302 d f k (x) =\u22121,x \u2208 Rk\u22121)\u03b3(2t ,\u03b4 tk)+\u03b3(2t ,\u03b4 tk)\u2264 2(PD(h\u0302 d f k (x)=\u22121,x\u2208Rk\u22121)+\u03b3(2t ,\u03b4 tk))\nTherefore,\nP\n(\nPS tk (h\u0302d fk (x) =\u22121,x \u2208 Rk\u22121)\u2264 2(PD (h\u0302 d f k (x) =\u22121,x \u2208 Rk\u22121)+ \u03b3(2t ,\u03b4 tk))\n)\n\u2265 1\u2212\u03b4 tk/12 (40)\nThe proof follows by applying union bound over Equations (36), (37), (38) and (40) and t \u2208N.\nWe emphasize that S tk is chosen iid at random after h\u0302 d f k is determined, thus uniform convergence argu-\nment over H d f is not necessary for Equations (38) and (40).\nProof of Fact 4. By induction on k.\nBase Case. For k = 0, it follows directly from normalized VC inequality that P(F0)\u2265 1\u2212\u03b40.\nInductive Case. Assume P(Fk\u22121)\u2265 1\u2212\u03b40 \u2212 . . .\u2212\u03b4k\u22121 holds. By union bound,\nP(Fk)\u2265 P(Fk\u22121 \u2229E1k \u2229E2k )\u2265 P(Fk\u22121)\u2212\u03b4k/2\u2212\u03b4k/2 \u2265 P(Fk\u22121)\u2212\u03b4k\nHence, P(Fk)\u2265 1\u2212\u03b40 \u2212 . . .\u2212\u03b4k. This finishes the induction. In particular, P(Fk0)\u2265 1\u2212\u03b40 \u2212 . . .\u03b4k0 \u2265 1\u2212\u03b4 ."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible. This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.", "creator": "LaTeX with hyperref package"}}}