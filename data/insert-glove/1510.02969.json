{"id": "1510.02969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?", "abstract": "electrico Despite being the appearance - based classifier of beimel choice appalled in really recent basmati years, hkm relatively trunklines few cikampek works have examined pajero how much happend convolutional neural grodriguez@latimescolumnists.com networks (CNNs) impatient can coillte improve warchus performance witchfire on accepted expression recognition biloxi benchmarks and, julin more cchl importantly, examine what breac it is insomniacs they cementation actually learn. In dc-10 this work, 9th not only ingelsby do we 8:53 show glowingly that sportscast CNNs grosset can hemal achieve zelienople strong wachovia performance, brochette but we canceled also kvint introduce cachan an approach to 40-cent decipher micciche which paschalis portions voldemort of the face toned influence wielaard the CNN ' s mieth predictions. First, we aahs train shapovalov a sylvaticum zero - bias dowdow CNN unicode on 50.29 facial expression gopers data wriston and achieve, suncor to cantave our knowledge, yakov state - of - younes the - art decapitation performance glafcos on state-level two expression oberwetter recognition fotherby benchmarks: the extended Cohn - makeev Kanade (ambassador-at-large CK +) nonmotile dataset and the amination Toronto Face aiff Dataset (seif TFD ). causality We then 1914-1916 qualitatively xianbin analyze the talaash network burford by daylesford visualizing the hcg spatial patterns manjarin that diacritics maximally gpf excite choristers different neurons ateeq in 3,817 the kiram convolutional concur layers sheuserglobe.com and youlan show 970 how they 40.97 resemble viic Facial Action Units (FAUs ). Finally, we use premise the FAU labels risperidone provided doln\u00ed in kuchak the ary CK + waitomo dataset belul to verify kurumba that the bennet FAUs observed sequentially in upbraids our 48.28 filter sensualist visualizations indeed align katiyar with the subject ' s facial scaglione movements.", "histories": [["v1", "Sat, 10 Oct 2015 18:53:21 GMT  (1702kb,D)", "https://arxiv.org/abs/1510.02969v1", "Accepted at ICCV 2015 CV4AC Workshop"], ["v2", "Fri, 28 Oct 2016 06:12:07 GMT  (1394kb,D)", "http://arxiv.org/abs/1510.02969v2", "Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Table 1 and some minor typos"], ["v3", "Thu, 16 Mar 2017 03:07:21 GMT  (1393kb,D)", "http://arxiv.org/abs/1510.02969v3", "Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Tables 2 and 3"]], "COMMENTS": "Accepted at ICCV 2015 CV4AC Workshop", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["pooya khorrami", "tom le paine", "thomas s huang"], "accepted": false, "id": "1510.02969"}, "pdf": {"name": "1510.02969.pdf", "metadata": {"source": "CRF", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?", "authors": ["Pooya Khorrami", "Tom Le Paine", "Thomas S. Huang"], "emails": ["pkhorra2@illinois.edu", "paine1@illinois.edu", "t-huang1@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "Facial expressions provide a natural and compact way for humans to convey their emotional state to another party. Therefore, designing accurate facial expression recognition algorithms is crucial to the development of interactive computer systems in artificial intelligence. Extensive work in this area has found that only a small number of regions change as a human changes their expression and are located around the subject\u2019s eyes, nose and mouth. In [7], Paul Ekman proposed the Facial Action Coding System (FACS) which enumerated these regions and described how every facial expression can be described as the combination of multiple action units (AUs), each corresponding to a particular muscle group in the face. However, having a computer\naccurately learn the parts of the face that convey emotion has proven to be a non-trivial task.\nPrevious work in facial expression recognition can be split into two broad categories: AU-based/rule-based methods and appearance-based methods. AU-based methods [29, 30] would detect the presence of individual AUs explicitly and then classify a person\u2019s emotion based on the combinations originally proposed by Friesen and Ekman in [8]. Unfortunately, each AU detector required careful hand-engineering to ensure good performance. On the other hand, appearance-based methods [1, 2, 31, 33] modeled a person\u2019s expression from their general facial shape and texture.\nar X\niv :1\n51 0.\n02 96\n9v 3\n[ cs\n.C V\n] 1\n6 M\nar 2\nIn the last few years, many well-established problems in computer vision have greatly benefited from the rise of convolutional neural networks (CNNs) as an appearance-based classifier. Tasks such as object recognition [14], object detection [9], and face recognition [28] have seen huge boosts in performance on several accepted benchmarks. Unfortunately, other tasks such as facial expression recognition have not experienced performance gains of the same magnitude. Little work has been done to see how much deep CNNs can help on accepted expression recognition benchmarks.\nIn this paper, we seek the answer to the following questions: Can CNNs improve performance on emotion recognition datasets/baselines and what do they learn? We propose to do this by training a CNN on established facial expression datasets and then analyzing what they learn by visualizing the individual filters in the network. In this work, we apply the visualization techniques proposed by Zeiler and Fergus [32] and Springenberg et al. [25] where individual neurons in the network are excited and their corresponding spatial patterns are displayed in pixel space using a deconvolutional network. When visualizing these discriminative spatial patterns, we find that many of the filters are excited by regions in the face that corresponded to Facial Action Units (FAUs). A subset of these spatial patterns is shown in Figure 1.\nThus, the main contributions of this paper are as follows:\n1. We show that CNNs trained for the emotion recognition task learn features that correspond strongly with the FAUs proposed by Ekman [7]. We demonstrate this result by first visualizing the spatial patterns that maximally excite different filters in the convolutional layers of our networks, and then using the ground truth FAU labels to verify that the FAUs observed in the filter visualizations align with the subject\u2019s facial movements.\n2. We also show that our CNN model, based on works originally proposed by [20, 21], can achieve, to our knowledge, state-of-the-art performance on the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD)."}, {"heading": "2. Related Work", "text": "In most facial expression recognition systems, the main machinery matches quite nicely with the traditional machine learning pipeline. More specifically, a face image is passed to a classifier that tries to categorize it as one of several (typically 7) expression classes: 1. anger, 2. disgust, 3. fear, 4. neutral, 5. happy, 6. sad, and 7. surprise. In most cases, prior to being passed to the classifier, the face image is pre-processed and given to a feature extractor. Up until rather recently, most appearance-based expression recognition techniques relied on hand-crafted features, specifically\nGabor wavelets [1, 2], Haar features [31] and LBP features [33], in order to make representations of different expression classes more discriminative.\nFor some time, systems based on hand-crafted features were able to achieve impressive results on accepted expression recognition benchmarks such as the Japanese Female Facial Expression (JAFFE) database [19], the extended Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10]. However, the recent success of deep neural networks has caused many researchers to explore feature representations that are learned from data. Not surprisingly, almost all of the methods used some form of unsupervised pre-training/learning to initialize their models. We hypothesize this may be because the scarcity of labeled data prevented the authors from training a completely supervised model that did not experience heavy overfitting.\nIn [17], the authors trained a multi-layer boosted deep belief network (BDBN) and achieved state-of-the-art accuracy on the CK+ and JAFFE datasets. Meanwhile in [23], the authors used a convolutional contractive auto-encoder (CAE) as their underlying unsupervised model. They then performed a semi-supervised encoding function called Contractive Discriminant Analysis (CDA) to separate discriminative expression features from the unsupervised representation.\nA few works based on unsupervised deep learning have also tried to analyze the relationship between FAUs and the learned feature representations. In [15, 16], the authors learned a patch-based filter bank using K-means as their low-level feature. These features were then used to select receptive fields corresponding to specific FAU receptive fields which were subsequently passed to multi-layer restricted Boltzmann machines (RBMs) for classification. The FAU receptive fields were selected using a mutual information criterion between the image feature and the expression label. An earlier work by Susskind et al. [27], showed that the first layer features a deep belief network trained to generate facial expression images appeared to learn filters that were sensitive to face parts. We conduct a similar analysis except we use a CNN as our underlying model and we visualize the spatial patterns that excite higher-level neurons in the network.\nTo the authors\u2019 knowledge, the only works that previously applied CNNs to expression data were that of Kahou et al. [13, 12] and Jung et al. [11]. In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition. However, one key point is that these works dealt with emotion recognition of video / image sequence data and therefore, actively incorporated temporal data when computing their predictions.\n5\nIn contrast, our work deals with emotion recognition from a single image, and will focus on analyzing the features learned by the network. Thus, not only will we demonstrate the effectiveness of CNNs on existing emotion classification baselines but we will also qualitatively show that the network is able to learn patterns in the face images that correspond to Facial Action Units (FAUs)."}, {"heading": "3. Our Approach", "text": ""}, {"heading": "3.1. Network Architecture", "text": "For all of the experiments we present in this paper, we use a classic feed-forward convolutional neural network. The networks we use, shown visually in Figure 2 consist of three convolutional layers with 64, 128, and 256 filters, respectively, and with filter sizes of 5x5 followed by ReLU (Rectified Linear Unit) activation functions. Max pooling layers are placed after the first two convolutional layers while quadrant pooling [3] is applied after the third. The quadrant pooling layer is then followed by a full-connected layer with 300 hidden units and, finally, a softmax layer for classification. The softmax layer contains anywhere between 6-8 outputs corresponding to the number of expressions present in the training set.\nOne modification that we introduce to the classical configuration is that we ignore the biases of the convolutional layers. This idea was introduced first by Memisevic et al. in [20] for fully-connected networks and later extended by Paine et al. in [21] to convolutional layers. In our experiments, we found that ignoring the bias allowed our network to train very quickly while simultaneously reducing the number of parameters to learn."}, {"heading": "3.2. Network Training", "text": "When training our network, we train from scratch using stochastic gradient descent with a batch size of 64, mo-\nmentum set to 0.9, and a weight decay parameter of 1e-5. We use a constant learning rate of 0.01 and do not use any form of annealing. The parameters of each layer are randomly initialized by drawing from a Gaussian distribution with zero mean and standard deviation \u03c3 = kNFAN IN where NFAN IN is the number of input connections to each layer and k is drawn uniformly from the range: [0.2, 1.2].\nWe also use dropout and various forms of data augmentation to regularize our network and combat overfitting. We apply dropout to the fully-connected layer with a probability of 0.5 (i.e. each neuron\u2019s output is set to zero with probability 0.5). For data augmentation, we apply a random transformation to each input image consisting of: translations, horizontal flips, rotations, scaling, and pixel intensity augmentation. All of our models were trained using the anna software library 1."}, {"heading": "4. Experiments and Analysis", "text": "We use two facial expression datasets in our experiments: the extended Cohn-Kanade database (CK+) [18] and the Toronto Face Dataset (TFD) [26]. The CK+ database contains 327 image sequences, each of which is assigned one of 7 expression labels: anger, contempt, disgust, fear, happy, sad, and surprise. For fair comparison, we follow the protocol used by previous works [15, 17], and use the first frame of each sequence as a neutral frame in addition to the last three expressive frames to form our dataset. This leads to a total of 1308 images and 8 classes total. We then split the frames into 10 subject independent subsets in the manner presented by [15] and perform 10-fold cross-validation.\nTFD is an amalgamation of several facial expression datasets. It contains 4178 images annotated with one of 7 expression labels: anger, disgust, fear, happy, neutral, sad, and surprise. The labeled samples are divided into 5 folds,\n1https://github.com/ifp-uiuc/anna\neach containing a train, validation, and test set. We train all of our models using just the training set of each fold, pick the best performing model using each split\u2019s validation set, then we evaluate on each split\u2019s test set and average the results over all 5 folds.\nIn both datasets, the images are grayscale and are of size 96x96 pixels. In the case of TFD, the faces have already been detected and normalized such that all of the subjects\u2019 eyes are the same distance apart and have the same vertical coordinates. Meanwhile for the CK+ dataset, we simply detect the face in the 640x480 image and resize it to 96x96. The only other pre-processing we employ is patchwise mean subtraction and scaling to unit variance."}, {"heading": "4.1. Performance on Toronto Face Database (TFD)", "text": "First, we analyze the discriminative ability of the CNN by assessing its performance on the TFD dataset. Table 1 shows the recognition accuracy obtained when training a zero-bias CNN from a random initialization with no other regularization as well as CNNs that have dropout (D), data augmentation (A) or both (AD). We also include recognition accuracies from previous methods. From the results in Table 1, there are two main observations: (i) not surprisingly, regularization significantly boosts performance (ii) data augmentation improves performance over the regular CNN more than dropout (9.4% vs. 2.8%). Furthermore, when both dropout and data augmentation are used, our model is able to exceed the previous state-of-the-art performance on TFD by 3.6%."}, {"heading": "4.2. Performance on the Extended Cohn-Kanade", "text": "Dataset (CK+)\nWe now present our results on the CK+ dataset. The CK+ dataset usually contains eight labels (anger, contempt, disgust, fear, happy, neutral, sad, and surprise). However, many works [34, 24, 17] ignore the samples labeled as neutral or contempt, and only evaluate on the six basic emotions. Therefore, to ensure fair comparison, we trained two separate models. We present the eight class model results in Table 2 and the six class model results in Table 3. For\nthe eight class model, we conduct the same study we did on the TFD and we observe rather similar results. Once again, regularization appears to play a significant role in obtaining good performance. Data augmentation gives a significant boost in performance (16.4%) and when combined with dropout, leads to a 16.9% increase. For the eight class and six class models, we achieve state-of-the-art and near state-of-the-art accuracy respectively on the CK+ dataset."}, {"heading": "4.3. Visualization of higher-level neurons", "text": "Now, with a strong discriminative model in hand, we will analyze which facial regions the neural network identifies as the most discriminative when performing classification. To do this, we employ the visualization technique presented by Zeiler and Fergus in [32].\nFor each dataset, we consider the third convolutional layer and for each filter, we find the N images in the chosen split\u2019s training set that generated the strongest magnitude response. We then leave the strongest neuron high and set all other activations to zero and use the deconvolutional network to reconstruct the region in pixel space. For our experiments, we chose N=10 training images.\nWe further refine our reconstructions by employing a technique called \u201dGuided Backpropagation\u201d proposed by Springenberg et al. in [25]. \u201dGuided Backpropogation\u201d aims to improve the reconstructed spatial patterns by not solely relying on the masked activations given by the toplevel signal during deconvolution but by also incorporating knowledge of which activations were suppressed during the forward pass. Therefore, each layer\u2019s output during the deconvolution stage is masked twice: (i) once by the ReLU of\nthe deconvotional layer and (ii) again by the mask generated by the ReLU of the layer\u2019s matching convolutional layer in the forward pass.\nFirst, we will analyze patterns discovered in the Toronto Face Dataset (TFD). In Figure 3, we select 10 of the 256 filters in the third convolutional layer and for each filter, we present the spatial patterns of the top-10 images in the training set. From these images, the reader can see that several of the filters appear to be sensitive to regions that align with several of the Facial Actions Units such as: AU12: Lip Corner Puller (row 1), AU4: Brow Lowerer (row 4), and AU15: Lip Corner Depressor (row 9).\nNext, we display the patterns discovered in the CK+ dataset. In Figure 4, we, once again, select 10 of the 256 filters in the third convolutional layer and for each filter, we present the spatial patterns of the top-10 images in the training set. The reader will notice that the CK+ discriminative spatial patterns are very clearly defined and correspond nicely with Facial Action Units such as: AU12: Lip Corner Puller (rows 2, 6, and 9), AU9: Nose Wrinkler (row 3) and AU27: Mouth Stretch (row 8)."}, {"heading": "4.4. Finding Correspondences Between Filter Activations and the Ground Truth Facial Action", "text": "Units (FAUs)\nIn addition to categorical labels (anger, disgust, etc.), the CK+ dataset also contains labels that denote which FAUs are present in each image sequence. Using these labels, we now present a preliminary experiment to verify that the filter activations/spatial patterns learned by the CNN indeed match with the actual FAUs shown by the subject in the image. Our experiment aims to answer the following question: For a particular filter i, which FAU j has samples whose activation values most strongly differ from the activations of\nsamples that do not contain FAU j, and does that FAU accurately correspond with the visual spatial patterns that maximally excite filter i?\nGiven a training set of M images (X) and their corresponding FAU labels (Y ), let F`i(x) be the activations of sample x at layer ` for filter i. Since we are examining the 3rd convolutional layer in the network, we set ` = 3. Then, for each of the 10 filters visualized in Figure 4, we do the following:\n(i) We consider a particular FAU j and place the samples X that contain j in set S where: S = {xm | j \u2208 ym}, \u2200m \u2208 {1, ...,M}\n(ii) We then build a histogram of the maximum activations of the samples that contained FAU j: Qij(x) = P (F3i(x) | S), \u2200(x, y) \u2208 (X,Y )\n(iii) We then, similarly, build a distribution over maximum activations of the samples that do not contain FAU j: Rij(x) = P (F3i(x) | Sc), \u2200(x, y) \u2208 (X,Y )\n(iv) We compute the KL divergence between Qij(x) and Rij(x), DKL(Qij \u2016 Rij), and repeat the process for all of the other FAUs.\nFigure 5 shows the bar charts of the KL divergences computed for all of the FAUs for each of the 10 filters displayed in Figure 4. The FAU with the largest KL divergence value is denoted in red and its corresponding name is documented in Table 4 for each filter. From these results, we see that in the majority of the cases, the FAUs listed in Table 4 match the facial regions visualized in Figure 4. This means that the samples that appear to strongly influence the activations of these particular filters are indeed those that possess the AU shown in the corresponding filter visualizations. Thus, we show that certain neurons in the neural network implicitly learn to detect specific FAUs in face images when given a relatively \u201dloose\u201d supervisory signal (i.e. emotion type: anger, happy, sad, etc.).\nWhat is most encouraging is that these results appear to confirm our intuitions about how CNNs work as appearance-based classifiers. For instance, filter 2, 6, and 9 appear to be very sensitive to patterns that correspond to AU 12. This is not surprising as AU 12 (Lip Corner Puller) is almost always associated with smiles and from the visualizations in Figure 4, a subject often shows their teeth when smiling, a highly distinctive appearance cue. Similarly, for filter 8, it is not surprising that FAU 25 (Lips Part) and FAU 27 (Mouth Stretch) had the most different activation distributions given that the filter\u2019s spatial patterns corresponded to the \u201dO\u201d shape made by the mouth region in surprised faces, another visually salient cue."}, {"heading": "5. Conclusions", "text": "In this work, we showed both qualitatively and quantitatively that CNNs trained to do emotion recognition are indeed able to model high-level features that strongly correspond to FAUs. Qualitatively, we showed which portions of the face yielded the most discriminative information by visualizing the spatial patterns that maximally excited dif-\nferent filters in the convolutional layers of our learned networks. Meanwhile, quantitatively, we correlated the numerical activations of the visualized filters with the subject\u2019s actual facial movements using the FAU labels given in the CK+ dataset. Finally, we demonstrated how a zero-bias CNN can achieve state-of-the-art recognition accuracy on the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD)."}, {"heading": "Acknowledgments", "text": "This work was supported in part by MIT Lincoln Laboratory. The Tesla K40 GPU used for this research was donated by the NVIDIA Corporation. The authors would also like to thank Dr. Kevin Brady, Dr. Charlie Dagli, Professor Yun Fu, and Professor Usman Tariq for their insightful comments and suggestions with regards to this work."}], "references": [{"title": "Recognizing facial expression: machine learning and application to spontaneous behavior", "author": ["M.S. Bartlett", "G. Littlewort", "M. Frank", "C. Lainscsek", "I. Fasel", "J. Movellan"], "venue": "In CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Empath: A neural network that categorizes facial expressions", "author": ["M.N. Dailey", "G.W. Cottrell", "C. Padgett", "R. Adolphs"], "venue": "Journal of cognitive neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Emotion recognition in the wild challenge 2014: Baseline, data and protocol", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "K. Sikka", "T. Gedeon"], "venue": "In 16th ACM International Conference on Multimodal Interaction. ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Emotion recognition in the wild challenge", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "M. Wagner", "T. Gedeon"], "venue": "In Proceedings of the 15th ACM on International conference on multimodal interaction,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Facial action coding system", "author": ["P. Ekman", "W.V. Friesen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Emfacs-7: Emotional facial action coding system", "author": ["W.V. Friesen", "P. Ekman"], "venue": "Unpublished manuscript, University of California at San Francisco,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Deep temporal appearance-geometry network for facial expression recognition", "author": ["H. Jung", "S. Lee", "S. Park", "I. Lee", "C. Ahn", "J. Kim"], "venue": "arXiv preprint arXiv:1503.01532,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Emonets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulcehre", "V. Michalski", "K. Konda", "S. Jean", "P. Froumenty", "A. Courville", "P. Vincent"], "venue": "arXiv preprint arXiv:1503.01800,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "\u00c7. G\u00fcl\u00e7ehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio", "R.C. Ferrari"], "venue": "In ICMI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS, pages 1097\u20131105,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Au-aware deep networks for facial expression recognition", "author": ["M. Liu", "S. Li", "S. Shan", "X. Chen"], "venue": "In FG, pages", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Au-inspired deep networks for facial expression feature learning", "author": ["M. Liu", "S. Li", "S. Shan", "X. Chen"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Facial expression recognition via a boosted deep belief network", "author": ["P. Liu", "S. Han", "Z. Meng", "Y. Tong"], "venue": "In CVPR, pages 1805\u20131812,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["P. Lucey", "J.F. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews"], "venue": "In CVPRW,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Automatic classification of single facial", "author": ["M.J. Lyons", "J. Budynek", "S. Akamatsu"], "venue": "images. PAMI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["R. Memisevic", "K. Konda", "D. Krueger"], "venue": "stat, 1050:10,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["T.L. Paine", "P. Khorrami", "W. Han", "T.S. Huang"], "venue": "arXiv preprint arXiv:1412.6597,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "On deep generative models with applications to recognition", "author": ["M. Ranzato", "J. Susskind", "V. Mnih", "G. Hinton"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["S. Rifai", "Y. Bengio", "A. Courville", "P. Vincent", "M. Mirza"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Facial expression recognition based on local binary patterns: A comprehensive study", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "Image and Vision Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "The toronto face database", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton"], "venue": "Department of Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Generating facial expressions with deep belief nets", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton", "J.R. Movellan"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Recognizing upper face action units for facial expression analysis", "author": ["Y.-l. Tian", "T. Kanada", "J.F. Cohn"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Facial action unit recognition by exploiting their dynamic and semantic relationships", "author": ["Y. Tong", "W. Liao", "Q. Ji"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Haar features for facs au recognition", "author": ["J. Whitehill", "C.W. Omlin"], "venue": "In FGR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Dynamic texture recognition using local binary patterns with an application to facial expressions", "author": ["G. Zhao", "M. Pietikainen"], "venue": "PAMI, 29(6):915\u2013928,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning active facial patches for expression analysis", "author": ["L. Zhong", "Q. Liu", "P. Yang", "B. Liu", "J. Huang", "D.N. Metaxas"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "In [7], Paul Ekman proposed the Facial Action Coding System (FACS) which enumerated these regions and described how every facial expression can be described as the combination of multiple action units (AUs), each corresponding to a particular muscle group in the face.", "startOffset": 3, "endOffset": 6}, {"referenceID": 26, "context": "AU-based methods [29, 30] would detect the presence of individual AUs explicitly and then classify a person\u2019s emotion based on the combinations originally proposed by Friesen and Ekman in [8].", "startOffset": 17, "endOffset": 25}, {"referenceID": 27, "context": "AU-based methods [29, 30] would detect the presence of individual AUs explicitly and then classify a person\u2019s emotion based on the combinations originally proposed by Friesen and Ekman in [8].", "startOffset": 17, "endOffset": 25}, {"referenceID": 6, "context": "AU-based methods [29, 30] would detect the presence of individual AUs explicitly and then classify a person\u2019s emotion based on the combinations originally proposed by Friesen and Ekman in [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "On the other hand, appearance-based methods [1, 2, 31, 33] modeled a person\u2019s expression from their general facial shape and texture.", "startOffset": 44, "endOffset": 58}, {"referenceID": 28, "context": "On the other hand, appearance-based methods [1, 2, 31, 33] modeled a person\u2019s expression from their general facial shape and texture.", "startOffset": 44, "endOffset": 58}, {"referenceID": 30, "context": "On the other hand, appearance-based methods [1, 2, 31, 33] modeled a person\u2019s expression from their general facial shape and texture.", "startOffset": 44, "endOffset": 58}, {"referenceID": 11, "context": "Tasks such as object recognition [14], object detection [9], and face recognition [28] have seen huge boosts in performance on several accepted benchmarks.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "Tasks such as object recognition [14], object detection [9], and face recognition [28] have seen huge boosts in performance on several accepted benchmarks.", "startOffset": 56, "endOffset": 59}, {"referenceID": 25, "context": "Tasks such as object recognition [14], object detection [9], and face recognition [28] have seen huge boosts in performance on several accepted benchmarks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "In this work, we apply the visualization techniques proposed by Zeiler and Fergus [32] and Springenberg et al.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "[25] where individual neurons in the network are excited and their corresponding spatial patterns are displayed in pixel space using a deconvolutional network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We show that CNNs trained for the emotion recognition task learn features that correspond strongly with the FAUs proposed by Ekman [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 17, "context": "We also show that our CNN model, based on works originally proposed by [20, 21], can achieve, to our knowledge, state-of-the-art performance on the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD).", "startOffset": 71, "endOffset": 79}, {"referenceID": 18, "context": "We also show that our CNN model, based on works originally proposed by [20, 21], can achieve, to our knowledge, state-of-the-art performance on the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD).", "startOffset": 71, "endOffset": 79}, {"referenceID": 0, "context": "Up until rather recently, most appearance-based expression recognition techniques relied on hand-crafted features, specifically Gabor wavelets [1, 2], Haar features [31] and LBP features [33], in order to make representations of different expression classes more discriminative.", "startOffset": 143, "endOffset": 149}, {"referenceID": 28, "context": "Up until rather recently, most appearance-based expression recognition techniques relied on hand-crafted features, specifically Gabor wavelets [1, 2], Haar features [31] and LBP features [33], in order to make representations of different expression classes more discriminative.", "startOffset": 165, "endOffset": 169}, {"referenceID": 30, "context": "Up until rather recently, most appearance-based expression recognition techniques relied on hand-crafted features, specifically Gabor wavelets [1, 2], Haar features [31] and LBP features [33], in order to make representations of different expression classes more discriminative.", "startOffset": 187, "endOffset": 191}, {"referenceID": 16, "context": "For some time, systems based on hand-crafted features were able to achieve impressive results on accepted expression recognition benchmarks such as the Japanese Female Facial Expression (JAFFE) database [19], the extended Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10].", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": "For some time, systems based on hand-crafted features were able to achieve impressive results on accepted expression recognition benchmarks such as the Japanese Female Facial Expression (JAFFE) database [19], the extended Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10].", "startOffset": 248, "endOffset": 252}, {"referenceID": 14, "context": "In [17], the authors trained a multi-layer boosted deep belief network (BDBN) and achieved state-of-the-art accuracy on the CK+ and JAFFE datasets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Meanwhile in [23], the authors used a convolutional contractive auto-encoder (CAE) as their underlying unsupervised model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "In [15, 16], the authors learned a patch-based filter bank using K-means as their low-level feature.", "startOffset": 3, "endOffset": 11}, {"referenceID": 13, "context": "In [15, 16], the authors learned a patch-based filter bank using K-means as their low-level feature.", "startOffset": 3, "endOffset": 11}, {"referenceID": 24, "context": "[27], showed that the first layer features a deep belief network trained to generate facial expression images appeared to learn filters that were sensitive to face parts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13, 12] and Jung et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[13, 12] and Jung et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 3, "endOffset": 11}, {"referenceID": 4, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 146, "endOffset": 152}, {"referenceID": 3, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 146, "endOffset": 152}, {"referenceID": 8, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 162, "endOffset": 166}, {"referenceID": 1, "context": "Max pooling layers are placed after the first two convolutional layers while quadrant pooling [3] is applied after the third.", "startOffset": 94, "endOffset": 97}, {"referenceID": 17, "context": "in [20] for fully-connected networks and later extended by Paine et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "in [21] to convolutional layers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "We use two facial expression datasets in our experiments: the extended Cohn-Kanade database (CK+) [18] and the Toronto Face Dataset (TFD) [26].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "We use two facial expression datasets in our experiments: the extended Cohn-Kanade database (CK+) [18] and the Toronto Face Dataset (TFD) [26].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "For fair comparison, we follow the protocol used by previous works [15, 17], and use the first frame of each sequence as a neutral frame in addition to the last three expressive frames to form our dataset.", "startOffset": 67, "endOffset": 75}, {"referenceID": 14, "context": "For fair comparison, we follow the protocol used by previous works [15, 17], and use the first frame of each sequence as a neutral frame in addition to the last three expressive frames to form our dataset.", "startOffset": 67, "endOffset": 75}, {"referenceID": 12, "context": "We then split the frames into 10 subject independent subsets in the manner presented by [15] and perform 10-fold cross-validation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "Gabor+PCA [4] 80.", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Deep mPoT [22] 82.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "CDA [23] 85.", "startOffset": 4, "endOffset": 8}, {"referenceID": 31, "context": "However, many works [34, 24, 17] ignore the samples labeled as neutral or contempt, and only evaluate on the six basic emotions.", "startOffset": 20, "endOffset": 32}, {"referenceID": 21, "context": "However, many works [34, 24, 17] ignore the samples labeled as neutral or contempt, and only evaluate on the six basic emotions.", "startOffset": 20, "endOffset": 32}, {"referenceID": 14, "context": "However, many works [34, 24, 17] ignore the samples labeled as neutral or contempt, and only evaluate on the six basic emotions.", "startOffset": 20, "endOffset": 32}, {"referenceID": 12, "context": "AURF [15] 92.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "AUDN [16] 93.", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "CSPL [34] 89.", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "LBPSVM [24] 95.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "10% BDBN [17] 96.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "To do this, we employ the visualization technique presented by Zeiler and Fergus in [32].", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "in [25].", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN\u2019s predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject\u2019s facial movements.", "creator": "LaTeX with hyperref package"}}}