{"id": "1510.02983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "OmniGraph: Rich Representation and Graph Kernel Learning", "abstract": "OmniGraph, a commenters novel aama representation unusally to support b-boy a range tortellini of NLP spectator classification kuai tasks, fuchu integrates lexical items, kalafi syntactic bobblehead dependencies soaps and ,17 frame oratorian semantic parses into graphs. Feature narellan engineering marshevet is folded into the lazzaro learning rivals through convolution 4,149 graph kernel jarvie learning to lingwei explore madder different jiggins extents of miejska the graph. A kaindl high - dimensional k\u00f6nigsberg space of j\u014d features colruyt includes individual nodes as well as caginess complex inertia subgraphs. farber In 903 experiments on a konstantyn\u00f3w text - 1939-75 forecasting mccredie problem kamiki that soltys predicts tapert stock price change from best-seller news 4-month for chengguan company scheidler mentions, megler OmniGraph 1.5-metre beats plenderleith several benchmarks based u.s.-picked on bag - dhungel of - words, wandesforde syntactic dependencies, forster and semantic sentry trees. murtha The highly iniciar expressive haor features janel OmniGraph elenco discovers provide 17-nation insights puttin into pre-olympic the semantics across distinct impending market waisale sectors. eosinophilia To demonstrate the 5.62 method ' s artichoke generality, l'air we donleavy also report zhongliang its high faunus performance 68-69 results badagry on nebelhorn a fine - heihe grained sentiment corpus.", "histories": [["v1", "Sat, 10 Oct 2015 21:22:00 GMT  (1070kb,D)", "http://arxiv.org/abs/1510.02983v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["boyi xie", "rebecca j passonneau"], "accepted": false, "id": "1510.02983"}, "pdf": {"name": "1510.02983.pdf", "metadata": {"source": "CRF", "title": "OmniGraph: Rich Representation and Graph Kernel Learning", "authors": ["Boyi Xie", "Rebecca J. Passonneau"], "emails": ["xie@cs.columbia.edu", "becky@ccls.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "For diverse NLP classification tasks, such as sentiment and opinion mining, or text-forecasting, in which text documents are used to make predictions about measurable phenomena in the real world (Kogan et al., 2009), there is a need to generalize over words while simultaneously capturing relational and structural information. Feature engineering for NLP learning tasks can be laborintensive. We propose OmniGraph, a novel representation that supports a continuum of features from lexical items, to syntactic dependencies, to frame semantic features. Figure 1 illustrates a sentence, the structure of its graph, and a predictive subgraph feature our method discovers that captures semantic and syntactic dependencies (arrows), semantic role information for syntactic ar-\n\u201cThis milestone highlighted the Boeing KC-767\u2019s ability to perform refueling operations under all lighting conditions,\u201d said George Hildebrand, Boeing Japan program manager.\nFigure 1: The graph for the sentence appears with nodes and edges greyed out, apart from the colored subgraph, which is a predictive OminGraph feature consisting of frame names (rectangles), semantic roles (diamonds), and semantic and syntactic relations (arrows). It predicts a positive price change for Boeing (boldface).\nguments (diamonds), and generalizations over lexical items (semantic frame names, shown as rectangles). For machine learning with OmniGraph, we use graph kernels that allow the user to control how much of the graph is explored for similarity computation. We test this approach on an extremely challenging text-forecasting problem: a polarity classification task to predict the direction of price change for publicly traded companies based on news. We also report results on an entitydriven fine-grained sentiment corpus.\nThe ability to exploit deep semantic information in text, e.g. to distinguish the depicted scenarios and semantic roles of the entity mentions, motivates our study. We hypothesize that a general and uniform representation of linguistic information that combines multiple levels, such as semantic frames and roles, syntactic dependency structure and lexical items, can support challenging classification tasks for NLP problems. Consider the following three sentences from financial news articles.\n- \u201cThe accreditation renewal also underscores the quality of our work with Humana members, customers, clients, payors and health care providers by confirming our compliance with national standards for PBM services,\u201d said William Fleming, vice president of Humana Pharmacy Solutions.\n- \u201cThe testing program highlighted the abilities of the\nar X\niv :1\n51 0.\n02 98\n3v 1\n[ cs\n.C L\n] 1\n0 O\nct 2\n01 5\nNavy, Raytheon Missile Systems and NASA to effectively partner on this complicated program and deliver what would have been previously unobtainable data,\u201d said Don Nickison, chief of the NASA Ames Wind Tunnel operations division.\n- \u201cThe initiation of a dividend and the renewed share repurchase authorization underscore the board and management\u2019s confidence in Symantec\u2019s long-term business outlook and ability to generate significant free cash flow on a consistent basis,\u201d said Symantec\u2019s executive vice president and chief financial officer, James Beer. The sentences all describe a scenario in which a company executive makes a positive statement about the company\u2019s capabilities. Of note, the stock price of the three companies (in boldface) went up the next day. Four semantic frames from FrameNet (Baker et al., 1998), a linguistic resource that exemplifies Fillmore\u2019s frame semantics (Fillmore, 1976), capture the commonality of a statement (green) from an organization leader (blue) that conveys the importance (orange) of capabilities (brown). Further, within each sentence the frames have the same syntactic dependencies in the three sentences. The feature in Figure 1 captures the common meaning of these sentences, and is predictive in three distinct market sectors: industrials, health care and information technology."}, {"heading": "2 Related Work", "text": "Much recent work on the kinds of NLP classification tasks our experiments address, textforecasting and fine-grained sentiment, builds on linguistically informed features or knowledge. Kim and Hovy (2006) introduce fine-grained opinion mining, using semantic role labeling to mine triples of the source, target and content of opinions applied to online news. To similarly mine opinion triples, Sayeed et al. (2012) depend more on syntax, using a suffix-tree data structure to represent syntactic relationships. Instead of feature engineering, Yogatama and Smith (2014), develop structured regularization for BOW based on parse trees, topics and hierarchical word clusters to improve BOW for 3 classification tasks: topic, sentiment, and text-driven forecasting. Another approach to forecasting from text (Joshi et al., 2010) combines BOW and the names of dependency relations to engineer features for predicting movie revenue from reviews. They devote considerable effort to feature engineering, while our approach folds feature engineering into the learning.\nOmniGraph feature engineering is handled automatically by convolution graph kernels. Convolution kernels have been used in NLP to exploit structured information using trees for parsing and\ntagging (Collins and Duffy, 2001), text categorization (Lodhi et al., 2002), and question answering (Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti, 2006). To learn social networks, Agarwal et al. (2014) use partial tree kernels on a representation with frame semantic information (Fillmore, 1976). The tree representation in Xie et al. (2013) also incorporates frame semantics, and uses subtree and subset tree kernels for the same forecasting task we pursue. In contrast to their taskspecific representations, our more general OmniGraph can be used for many tasks. Rather than having to choose among many tree kernels, the graph kernels we use allow users to specify the size of the graph neighborhoods to explore.\nStudies of the effect of financial news on the market (Gerber et al., 2009; Gentzkow and Shapiro, 2010; Engelberg and Parsons, 2011) have been increasingly important since Tetlock (2007) investigated the role of media in the stock market. As mentioned in Wong et al. (2014), a better solution to the problem can help gain more insights to the long-lasting question in finance about how financial markets react to news (Fama, 1998; Chan, 2003). In general, the work in NLP that uses news to predict price does well if it achieves better than 50% accuracy (Lee et al., 2014; BarHaim et al., 2011; Creamer et al., 2013; Xie et al., 2013). Wong et al. (2014) report that even \u201ctextbook models\u201d that uses time series data have less than 51.5% prediction accuracy. Unlike many other domains, however, a higher than random accuracy can have great value in a high-volume trading strategy. Work in NLP and related areas (Devitt and Ahmad, 2007; Schumaker et al., 2012; Feldman et al., 2011; Zhang and Skiena, 2010) often treats stock price prediction from news as a sentiment classification problem. Xie et al. (2013) point out that this is consistent with the direction component of the three-part ADS model (Rydberg and Shephard, 2003). Their model was shown better than BOW alone on three market sectors, but there was no comparison to the majority baseline. In contrast, our OmniGraph outperforms the baseline in seven out of eight market sectors and beats BOW and two other benchmarks."}, {"heading": "3 Methods", "text": "We first introduce our data representation, then describe our learning methods. The nodes in an OmniGraph encode semantic and lexical content, and\nSentence: \u201cThe accreditation renewal underscores the quality of our work with Humana members,\u201d said Humana\u2019s president.\nFrame semantic parse: [\u201cThe accreditation renewal [underscoresConvey importance] [the [qualityCapability] of our work with Humana membersConvey importance.Message],\u201dStmt.Message] [saidStatement] [Humana\u2019s [presidentLeadership] Stmt.Speaker]. Dependency parse:\nthe edges encode semantic and syntactic dependency relations. In Section 3.1 we use an example to describe how to construct a one-sentence OmniGraph. Later we introduce the data instances for our learning task: OmniGraph forests that represent all the sentences in the news that mention a given company on a particular day. As in Xie et al. (2013), we refer to the company we make predictions about as the designated entity.\n3.1 OmniGraph Construction To construct a one-sentence OmniGraph, the sentence must first be assigned a frame semantic parse and a syntactic dependency parse. Section 4 describes the parsers we use and their performance. We use the example sentence in Figure 2 to illustrate how to construct its OmniGraph in Figure 3. (1) Create building blocks by converting each semantic frame into a subgraph. In frame-based semantic parsing, the scenarios in a sentence are identified as frames, and each frame is triggered by a frame target: the lexical item that evokes the frame. The frame name and frame target become OmniGraph nodes, along with the frame elements (semantic roles) that have been filled by sentential arguments. In Figure 2, nodes with the same color correspond to a frame, its target, and its elements. Four frames have been identified by the parser, and three frame elements: the MESSAGE\nelements of the Statement and Convey importance frames, the SPEAKER element of the Statement frame, and the Capability and Leadership frames. An edge connects a frame target and the frame it evokes (e.g. said and Statement), and a frame element and the frame it belongs to (e.g. MESSAGE and Statement). Figure 2 shows an actual parse for a sentence in our data; in a correct parse, the phrase accreditation renewal would fill the MEDIUM element of the Convey importance frame. We achieve good prediction performance despite such inaccuracies. (2) Add the dependency relations among frames. Frame semantic parsing identifies individual frames, but not the syntactic dependencies among frames. As shown in the dependency parse in Figure 2, we locate the frame targets, i.e. lexical items that evoke frames, and use the dependency relations among the frame targets to link the frames. In OmniGraph, a dependent node points to the node it depends on, as indicated by the red arrows in Figure 3. (3) Connect the designated entity to its semantic roles. For the learning task we present here, we make predictions about a designated entity, a publicly traded company. Company mentions are identified using pattern matching. All mentions of the designated entity become nodes, which are linked to the frame elements they fill, or partly fill. For example the designated entity Humana fills the SPEAKER role of the Statement frame, and also occurs in the MESSAGE element of the Statement frame and the MESSAGE element of the Convey importance frame. It thus has three out-degree edges. (4) Connect lexical items to frame elements they help fill. OmniGraph incorporates lexical information with one node for each lexical item in a constituent that fills a frame element. The nodes connect to the frame elements they fill. Some of\nthese edges are shown as grey dashed nodes and edges in Figure 3. Some edges are omitted for readability.\nIn sum, OmniGraph nodes are: 1) frame names (boxes), 2) frame targets (rounded boxes), 3) frame elements (diamonds), 4) other lexical items (dashed boxes), and 5) designated entities (ellipses). An edge connects: 1) a frame target to its frame; 2) a frame element to the frame it belongs to; 3) a designated entity to the frame element it fills; 4) from one frame to another where the target of the first frame is a dependent node in a dependency parse, and the target of the second frame is the dependent\u2019s head; and 5) a lexical item to the frame element it helps fill. Edge directions are exploited by the graph kernels we use."}, {"heading": "3.2 Weisfeiler-Lehman Graph Kernel", "text": "We selected the Weisfeiler-Lehman (WL) graph kernel (Shervashidze et al., 2011) for SVM learning because it has a lower computational complexity compared to other graph kernels, and because it can measure similarity between graphs for different neighborhood sizes. At each degree i of neighborhood, all nodes are relabeled with their neighborhoods, then graph similarity is measured. For example, to explore its first degree neighbors, the immediate neighborhood of the Designated Entity node in Figure 3 is used to relabel the node as {Designated Entity\u2192C.Message, S.Message, S.Speaker}. The WL kernel computation is based on the Weisfeiler-Lehman test of isomorphism (Weisfeiler and Lehman, 1968), which iteratively augments the node labels by the sorted set of their neighboring node labels, and compresses them into new short labels, called multisetlabels. Through neighbor augmentation, similarity between graphs is iteratively measured using dynamic programming.\nFigure 4 illustrates how to calculate the WL graph kernel between graphs G and G\u2032 for degrees of neighbor up to 1 (h=1). Iteration i=0 for de-\ngree of neighbor 0 (stepsize 0) compares only the nodes of the original graphs. Nodes with label 0 have one match; nodes with label 1 have two matches. This gives a total similarity of three. The neighborhoods for each node are then augmented to compute similarity when iteration i=1, which compares the nodes and their first degree neighbors. New labels (i.e. 3, 4, and 5) are assigned to represent each node and its first degree neighbors, and similarity of the relabeled graphs is 2. Therefore, kh=1(G,G\u2032) = k(G0, G\u20320)+k(G1, G\u20321) = 3+2 = 5."}, {"heading": "3.3 Node Edge Weighting Graph Kernel", "text": "The WL kernel is efficient at neighborhood augmentation but there is no distinction between different node types, and node augmentation gathers up all nodes for a given degree. The 1-degree WL feature for the Designated Entity (DE) node in Figure 3 is <DE\u2192Spkr,Msg,Msg>, i.e. DE fills a SPEAKER and two MESSAGE elements (one for the Statement frame and the other for the Convey importance frame). No credit for partial matching is given when this graph instance is compared to another instance where DE just fills the MESSAGE element of the Convey importance frame. To allow partial matching, and to take advantage of the type information of nodes and edges, we introduce a novel graph kernel: node edge weighting (NEW) graph kernel.\nLike WL, NEW also measures subgraph similarities through neighborhood augmentation. The kernel computation can be broken down into node kernels and edge kernels. Node and edge kernels are weighted Kronecker delta kernels (\u03b4(\u00b7, \u00b7)) that return whether the two objects being compared are identical. Define wFn for the weight of node n of feaure type F , node label L, and w<Ffr\u2192Fto> for the weight of edge e with from-node of feature type Ffr and to-node of feature type Fto. We have knode(n, n\n\u2032) = wFn \u00b7 \u03b4(Fn,Fn\u2032) \u00b7 \u03b4(Ln,Ln\u2032), and kedge(e, e\n\u2032) = w<Ffr\u2192Fto> \u00b7 \u03b4(Ffr,Ffr\u2032) \u00b7 \u03b4(Fto,Fto\u2032). Define kp(G,G\u2032) to be the basis kernel for p-degree neighborhood; the kernel between graph G and G\u2032 is computed by recursion as in Equation 1.\nkp(G,G\u2032) = \u2211\nall paths of length p\u2208G,G\u2032 knode(n\nG p , n G\u2032 p )\np\u22121\u220f i=1 kedge(e G i , e G\u2032 i )knode(n G i , n G\u2032 i ) (1)\nDynamic programming can be used to improve the efficiency. Each entry in the dynamic program-\nming table is a tuple of <G,G\u2032, nGi , nG \u2032\ni >, where nGi and nG\u2032i are nodes in graph G and G\u2032.\nThe toy example in Figure 5 illustrates how to calculate the NEW graph kernel between graphs G and G\u2032. As with the WL kernel, NEW compares different degrees of node neighborhoods up to p degrees of neighbors, and the final kernel is a sum of all basis kernels. For p=0, only the nodes of the original graphs are compared. Nodes with labels DE, Msg and ConImp all have one match. With node weights as shown, kp=0=0.3+0.7+0.9=1.9. For p=1, each node plus its one-degree neighbors are compared, and the relations between the nodes. Path DE\u2192 Msg has a match. With node and edge weighting, kp=1=0.3\u22170.4\u22170.7=0.084. For the same reason, kp=2=0.3\u22170.4\u22170.7\u22170.6\u22170.9=0.045. There is no match for three degrees of neighbors, kp=3=0.\nEach basis kernel that corresponds to different neighborhood sizes are then normalized by the maximum of the evaluation between each graph and itself. For each graph kernel kp(G,G\u2032) we have a normalized k\u0302p(G,G\u2032):\nk\u0302p(G,G\u2032) = kp(G,G\u2032)\nmax(kp(G,G), kp(G\u2032, G\u2032) (2)\nThis normalization ensures that a graph will always match itself with the highest value of 1 and other graphs with values between 0 and 1. The final kernel is an interpolation of basis kernels: k(G,G\u2032) = \u2211 p \u03b1pk\u0302 p(G,G\u2032) , where \u2211\np \u03b1p=1. Combining basis kernels is a common problem in machine learning and several multiple kernel learning techniques have been developed to allow benefits from multiple kernels (Smits and Jordaan, 2002; Bach et al., 2004)."}, {"heading": "4 Financial News Analytics", "text": "We test the performance of OmniGraph with WL and NEW kernels on a polarity task: to predict the direction of price change for 321 companies from eight market sectors of Standard & Poor\u2019s 500 index. On average, there are from 27 to 67 companies per sector. One of the biggest challenges of the financial domain is the unpredictability of the\nmarket. As noted above, use of NLP methods on news to predict price does well if it achieves better than random performance, as described in the Related Work. We rely on Student\u2019s T to test statistical significance of classification accuracy. We use the majority class label as a baseline, which ranges from 54% to 56%, depending on the market sector. Compared with three NLP benchmarks, only OmniGraph beats the baseline, and results are statistically significant."}, {"heading": "4.1 Experimental Setup", "text": "The experiments use Reuters news data from 2007 to 2013 for eight GICS1 sectors. Sentences that mention companies are extracted using highprecision, high-recall pattern matching on company name variants. A data instance for a company consists of an OmniGraph forest representing all the sentences that mention that company on a given day. On average, each data instance encodes from 4.11 to 7.18 sentences, and each company has an average total of from 605 to 858 sentences, depending on the sector. In work reported elsewhere, we found that we could expand the number of sentences per company using coreference by 15-30%, depending on the sector. The additional sentences did not, however, improve performance (Anon). Sentences that mention companies by name tend to occur early in news articles, and are apparently more predictive.\nA binary class label {-1, +1} indicates the direction of price change on the next day after the news associated to the data instance. The one-day delay of price response to news is due to (Tetlock et al., 2008). Only the instances with a price change of 2% are included in our polarity prediction task.\nSentences are parsed using the MST dependency parser (McDonald et al., 2005), which implements the Eisner algorithm (Eisner, 1996) for dependency parsing, and provides an efficient and robust performance. For frame semantic parsing, we use SEMAFOR (Das and Smith, 2011; Das and Smith, 2012), which generates state-of-the-art results on SemEval benchmark datasets.\nFor the learning, we found that no single stepsize performed best for a given company, much less the entire data set. We select the stepsize and weights of the basis kernels for NEW using grid search on 80% of the data, where we use leaveone-out cross validation. The selected parameters\n1Global Industry Classification Standard.\nfor a given company are then used to test the average prediction performance on the 20% of heldout data."}, {"heading": "4.2 Benchmark Methods", "text": "Three benchmark methods are reported for comparison with OmniGraph: (1) BOW-a vector space model that contains unigrams, bigrams, and trigrams. (2) DepTree-a tree space representation based on the dependency parses used to create OmniGraph. The root is the sentence entry, and dependency relation types, such as SUB, OBJ, VMOD, and the lexical items, are represented as tree nodes. (3) SemTreeFWD-a state-of-the-art representation for the price prediction task that is an enriched hybrid of vector and tree space (Xie et al., 2013). It includes semantic frames, lexical items, and part-of-speech-specific psycholinguistic features. Learning relies on Tree Kernel SVM (Moschitti, 2006).2"}, {"heading": "4.3 Features", "text": "OmniGraph with NEW kernel learning shows a strong impact of stepsize and weighting of nodes and edges. In a detailed analysis of the 26 companies in GICS 30 (Consumer Staples), a sector with average amounts of news, all but three companies have non-zero coefficients on two or more of the basis kernels. Two of the three outliers rely only on stepsize 1 and the third on stepsize 2. Thirteen companies combine two basis kernels and the remaining ten combine three. On average, only 9% of the features are non-relational (p=0). The other sectors have a similar trend.\nGrid search determines the stepsize, and also determines which node types to include during neighborhood augmentation; nodes are weighted 0 or 1. Figure 6 shows the proportion of GICS 30 companies that use each of seven node types.\n2Data provided by the authors.\nThe most important node types are frame names (FN) and frame elements (FE): more than 60% of the companies need them to obtain the best performance. The next most frequent node types are designated entities (DE) and other entities (OE), each used by 50% of companies. This result suggests that relations between companies are useful for price polarity. More than one third of the companies need the feature for dependencies between frames (FDEP), often involving complex sentences where multiples frames are evoked. The lexical item features (LI) have a contribution similar to FDEP. Note that depending on the stepsize, LI features from OmniGraph include lexical items (p=0), their dependencies and the frame elements they fill (p=1), and the frames to which the frame elements belong (p=2). Frame target (FT) is the least preferred feature."}, {"heading": "4.4 Results", "text": "Table 1 summarizes the average accuracy for all eight sectors of the majority class baseline, the three benchmarks, and the two OmniGraph models. Both versions of OmniGraph significantly outperform the three benchmarks. The cells with asterisks represent a difference from the baseline that is statistically significant. OmniGraphWL beats the baseline with statistical significance in six sectors, and OmniGraphNEW in seven. Note that none of the benchmarks outperforms the baseline.\nDespite the excellent performance of BOW for topical classification tasks, for this price prediction task it does poorly. Both DepTree and SemTreeFWD outperform BOW, which indicates that features derived from dependency syntax and semantic frame parsing improve performance. DepTree directly represents the dependency parse with both dependencies and words as nodes, without semantic information. The limitation of SemTree comes from its entity-centric representation \u2013 the root node is the designated entity. The semantic frames without DE mentions are discarded, and a heterogeneous combination of trees and vectors are used for learning. Between WL and NEW learning on OmniGraph, NEW produces the best results. We suspect this is due to the high granularity of the features it generates, and its flexibility in assigning different weights to nodes and edges, depending on the node and edge feature types."}, {"heading": "5 GoodFor/BadFor Corpus", "text": "To further test OmniGraph performance for entity driven text analytics, we used a recently introduced, publicly available dataset - the GoodFor/BadFor (gfbf ) Corpus3 (Deng et al., 2013), which is part of MPQA (Wiebe et al., 2005). gfbf has been annotated for two fine-grained sentiment judgments: 1) benefactive/malefactive event annotation, and 2) writer attitude. The benefactive/malefactive task asked annotators to identify the affected entity (the object) and the entity causing the event (the agent), and to label whether the agent and the event is benefactive or malefactive on the object. We treat the object as the designated entity. The writer attitude task asked annotators to identify the writer\u2019s attitude towards the agent and the object. We treat both the agent and the object as the designated entity in turn.\nWe use the percentage of the majority class as the baseline, and compare the same five methods as in our previous experiment. Table 2 summarizes the results. On the benefactive/malefactive task, BOW obtains a 10% improvement over the baseline. Structured representations significantly improve over BOW. SemTreeFWD, which incorporates the semantic frame features and a sentiment lexicon, improves the performance by another 5%. The dependency tree performance is similar to SemTreeFWD. OmniGraph with graph kernel learning (WL or NEW kernels) performs\n3http://mpqa.cs.pitt.edu/corpora/gfbf/\nmuch better. The writer attitude task is a more difficult one with a slightly lower baseline, and a much lower inter-annotator agreement (Deng et al., 2013). Dependency trees and semantic trees do not improve over BOW. Both versions of OmniGraph, however, have superior performance."}, {"heading": "6 Discussion", "text": "OmniGraph with graph kernel learning exhibits superior performance over vector and tree space models in both experiments. To understand what contributes to the predictive power of OmniGraph models, we use mutual information to rank features discovered by OmniGraph. Compared to the vector and tree representations, the graphstructured features are more expressive, and can be interpreted. Figure 7 presents six highly ranked features from our experiments. Features 1-3 are from the financial news analytics task. Feature 1 is a complex feature with frame names, frame elements, and the dependencies among frames. It generalizes over multiple sectors and predicts a positive change in price. It is the feature that corresponds to the example sentences in the Introduction. Feature 2 combines frame names, frame elements, a frame target, and two lexical items to capture an interesting pattern: referring to the former leader of a company predicts a negative price change. Feature 3 is a 2-degree neighbor subgraph that consists of three frames and their interdependencies. This feature represents that the designated entity experiences a change over a time period. The feature generalizes across many different wordings, and although the feature does not directly encode direction of change, it happens that this feature rarely occurs in a negative description. It is therefore an example of a positive sentiment feature that is detected without reliance on a sentiment lexicon or on explicit polarity information.\nFeatures 4-6 are from the gfbf experiment. Feature 4 is a top ranked predictor for the bene-\nfective/malefactive task, and it predicts a positive affect toward the object. It captures the relation between the Agent and the Object in an Assistance scenario where the Agent fills the HELPER role and the affected Object fills the BENEFITED PARTY role. Row 5 contains two features that are predictive in the writer attitude task. Recall that a writer can have different attitudes towards the Agent and the Object. Our approach is able to distinguish different roles of different entities of interest for the same sentence, and make separate predictions.\nAs seen above, OmniGraph is very good at modeling complex intra-sentence semantic relations. Inspired by the work of Galitsky (2014), who constructed dependency parse forests for paragraphs of text, one of our future directions is to extend OmniGraph to incorporate discourse information. An obvious choice would be to encode inter-sentential discourse relations as one or more new edge types to connect the OmniGraphs that correspond to distinct sentences."}, {"heading": "7 Conclusion", "text": "In this study, we have presented a novel graph-based representation \u2013 OmniGraph \u2013 with\nWeisfeiler-Lehman and node edge weighting graph kernel learning, for entity-driven semantic analysis of documents. This method exhibits superior performance in a text-forecasting task that uses financial news to predict the stock market performance of company mentions, and a finegrained sentiment task. OmniGraph\u2019s advantages stem from the use of semantic frames to generalize word meanings in a flexible and extensible graph structure, where rich relational linguistic information, such as dependencies among frames and lexical items, can be modeled and learned with graph kernels that make feature engineering part of the learning. The resulting graph features are able to reflect deeper semantic patterns beyond words, and to help provide insights into the problem domain. Here, we applied OmniGraph to two rather distinct problems to illustrate that it could potentially support a wide range of NLP classification problems. On top of OmniGraph\u2019s capability of modeling complex intra-sentence semantic relations, a future direction is to model inter-sentence relations through discourse structure to form a more linguistically informed document-level representation."}], "references": [{"title": "Frame semantic tree kernels for social network extraction from text", "author": ["Sriramkumar Balabsubramanian", "Anup Kotalwar", "Jiehan Zheng", "Owen Rambow"], "venue": "In Proceedings of the 14th Conference of the European", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["Bach et al.2004] Francis R. Bach", "Gert R.G. Lanckriet", "Michael I. Jordan"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2004}, {"title": "The Berkeley Framenet project", "author": ["Charles J. Fillmore", "John B. Lowe"], "venue": "In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics - Volume", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Identifying and following expert investors in stock microblogs", "author": ["Elad Dinur", "Ronen Feldman", "Moshe Fresko", "Guy Goldstein"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language", "citeRegEx": "Bar.Haim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bar.Haim et al\\.", "year": 2011}, {"title": "Stock price reaction to news and no-news: Drift and reversal after headlines", "author": ["Wesley S. Chan"], "venue": "Journal of Financial Economics,", "citeRegEx": "Chan.,? \\Q2003\\E", "shortCiteRegEx": "Chan.", "year": 2003}, {"title": "Convolution kernels for natural language", "author": ["Collins", "Duffy2001] Michael Collins", "Nigel Duffy"], "venue": "In Proceedings of the 14th Conference on Neural Information Processing Systems", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "News and sentiment analysis of the european market with a hybrid expert weighting algorithm", "author": ["Yong Ren", "Yasuaki Sacamoto", "Jeffrey V. Nickerson"], "venue": "In SocialCom\u201913,", "citeRegEx": "Creamer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Creamer et al\\.", "year": 2013}, {"title": "Semi-supervised frame-semantic parsing for unknown predicates", "author": ["Das", "Smith2011] Dipanjan Das", "Noah A. Smith"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Graph-based lexicon expansion with sparsity-inducing penalties. In HLT-NAACL, pages 677\u2013687", "author": ["Das", "Smith2012] Dipanjan Das", "Noah A. Smith"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Sentiment polarity identification in financial news: A cohesion-based approach", "author": ["Devitt", "Ahmad2007] Ann Devitt", "Khurshid Ahmad"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Devitt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Devitt et al\\.", "year": 2007}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M. Eisner"], "venue": "In Proceedings of the 16th Conference on Computational Linguistics - Volume 1,", "citeRegEx": "Eisner.,? \\Q1996\\E", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "The causal impact of media in financial markets", "author": ["Engelberg", "Parsons2011] Joseph Engelberg", "Christopher A. Parsons"], "venue": "Journal of Finance,", "citeRegEx": "Engelberg et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Engelberg et al\\.", "year": 2011}, {"title": "Market efficiency, long-term returns, and behavioral finance", "author": ["Eugene F. Fama"], "venue": "Journal of Financial Economics,", "citeRegEx": "Fama.,? \\Q1998\\E", "shortCiteRegEx": "Fama.", "year": 1998}, {"title": "The stock sonar - sentiment analysis of stocks based on a hybrid approach", "author": ["Benjamin Rosenfeld", "Roy Bar-Haim", "Moshe Fresko"], "venue": "In Proceedings of the Twenty-Third Conference on Innovative", "citeRegEx": "Feldman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2011}, {"title": "Learning parse structure of paragraphs and its applications in search", "author": ["Boris Galitsky"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "Galitsky.,? \\Q2014\\E", "shortCiteRegEx": "Galitsky.", "year": 2014}, {"title": "What drives media slant? evidence from u.s. daily newspapers", "author": ["Gentzkow", "Shapiro2010] M. Gentzkow", "J.M. Shapiro"], "venue": null, "citeRegEx": "Gentzkow et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gentzkow et al\\.", "year": 2010}, {"title": "Does the media matter? a field experiment measuring the effect of newspapers on voting behavior and political opinions", "author": ["Gerber et al.2009] A.S. Gerber", "D. Karlan", "D. Bergan"], "venue": "American Economic Journal: Applied Economics,", "citeRegEx": "Gerber et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gerber et al\\.", "year": 2009}, {"title": "Movie reviews and revenues: An experiment in text regression. In Human Language Technologies: The 2010", "author": ["Joshi et al.2010] Mahesh Joshi", "Dipanjan Das", "Kevin Gimpel", "Noah A. Smith"], "venue": null, "citeRegEx": "Joshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Extracting opinions, opinion holders, and topics expressed in online news media text", "author": ["Kim", "Hovy2006] Soo-Min Kim", "Eduard Hovy"], "venue": "In Proceedings of the Workshop on Sentiment and Subjectivity in Text,", "citeRegEx": "Kim et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2006}, {"title": "Predicting risk from financial reports with regression", "author": ["Kogan et al.2009] Shimon Kogan", "Dimitry Levin", "Bryan R. Routledge", "Jacob S. Sagi", "Noah A. Smith"], "venue": "In Proceedings of Human Language Technologies:", "citeRegEx": "Kogan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kogan et al\\.", "year": 2009}, {"title": "On the importance of text analysis for stock price prediction", "author": ["Lee et al.2014] Heeyoung Lee", "Mihai Surdeanu", "Bill Maccartney", "Dan Jurafsky"], "venue": "In LREC\u201914,", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Text classification using string kernels", "author": ["Lodhi et al.2002] Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lodhi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Making tree kernels practical for natural language learning", "author": ["Alessandro Moschitti"], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Moschitti.,? \\Q2006\\E", "shortCiteRegEx": "Moschitti.", "year": 2006}, {"title": "Dynamics of Trade-byTrade Price Movements: Decomposition and Models", "author": ["Rydberg", "Shephard2003] Tina H. Rydberg", "Neil Shephard"], "venue": "Journal of Financial Econometrics,", "citeRegEx": "Rydberg et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rydberg et al\\.", "year": 2003}, {"title": "Grammatical structures for word-level sentiment detection", "author": ["Jordan BoydGraber", "Bryan Rusk", "Amy Weinberg"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Sayeed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sayeed et al\\.", "year": 2012}, {"title": "Evaluating sentiment in financial news articles", "author": ["Yulei Zhang", "Chun-Neng Huang", "Hsinchun Chen"], "venue": "Decision Support Systems,", "citeRegEx": "Schumaker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schumaker et al\\.", "year": 2012}, {"title": "Weisfeilerlehman graph kernels", "author": ["Karsten M. Borgwardt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Borgwardt.,? \\Q2011\\E", "shortCiteRegEx": "Borgwardt.", "year": 2011}, {"title": "Improved svm regression using mixtures of kernels", "author": ["Smits", "Jordaan2002] G.F. Smits", "E.M. Jordaan"], "venue": "In Proceedings of 2002 International Joint Conference on Neural Networks,", "citeRegEx": "Smits et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Smits et al\\.", "year": 2002}, {"title": "Hierarchical directed acyclic graph kernel: Methods for structured natural language data", "author": ["Suzuki et al.2003] Jun Suzuki", "Tsutomu Hirao", "Yutaka Sasaki", "Eisaku Maeda"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computa-", "citeRegEx": "Suzuki et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2003}, {"title": "More than Words: Quantifying Language to Measure Firms", "author": ["Maytal SaarTsechansky", "Sofus Macskassy"], "venue": null, "citeRegEx": "Tetlock et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tetlock et al\\.", "year": 2008}, {"title": "Giving Content to Investor Sentiment: The Role of Media in the Stock Market", "author": ["Paul C. Tetlock"], "venue": null, "citeRegEx": "Tetlock.,? \\Q2007\\E", "shortCiteRegEx": "Tetlock.", "year": 2007}, {"title": "A reduction of graph to a canonical form and an algebra arising during this reduction", "author": ["Weisfeiler", "Lehman1968] B. Weisfeiler", "A.A. Lehman"], "venue": "Nauchno-Technicheskaya Informatsiya,", "citeRegEx": "Weisfeiler et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Weisfeiler et al\\.", "year": 1968}, {"title": "Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165\u2013210", "author": ["Wiebe et al.2005] Janyce Wiebe", "Theresa Wilson", "Claire Cardie"], "venue": null, "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Stock market prediction from WSJ: text mining via sparse matrix factorization", "author": ["Zhenming Liu", "Mung Chiang"], "venue": "IEEE International Conference on Data Mining,", "citeRegEx": "Wong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2014}, {"title": "Semantic frames to predict stock price movement", "author": ["Xie et al.2013] Boyi Xie", "Rebecca J. Passonneau", "Leon Wu", "Germ\u00e1n Creamer"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Xie et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2013}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Yogatama", "Smith2014] Dani Yogatama", "Noah A. Smith"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yogatama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2014}, {"title": "Question classification using support vector machines", "author": ["Zhang", "Lee2003] Dell Zhang", "Wee Sun Lee"], "venue": "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}, {"title": "Trading strategies to exploit blog and news sentiment", "author": ["Zhang", "Skiena2010] Wenbin Zhang", "Steven Skiena"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "For diverse NLP classification tasks, such as sentiment and opinion mining, or text-forecasting, in which text documents are used to make predictions about measurable phenomena in the real world (Kogan et al., 2009), there is a need to generalize over words while simultaneously capturing relational and structural information.", "startOffset": 195, "endOffset": 215}, {"referenceID": 2, "context": "Four semantic frames from FrameNet (Baker et al., 1998), a linguistic resource that exemplifies Fillmore\u2019s frame seman-", "startOffset": 35, "endOffset": 55}, {"referenceID": 17, "context": "Another approach to forecasting from text (Joshi et al., 2010) combines BOW and the names of dependency relations to engineer features for predicting movie revenue from reviews.", "startOffset": 42, "endOffset": 62}, {"referenceID": 24, "context": "To similarly mine opinion triples, Sayeed et al. (2012) depend more on syntax, using a suffix-tree data structure to represent syntactic relationships.", "startOffset": 35, "endOffset": 56}, {"referenceID": 24, "context": "To similarly mine opinion triples, Sayeed et al. (2012) depend more on syntax, using a suffix-tree data structure to represent syntactic relationships. Instead of feature engineering, Yogatama and Smith (2014), develop structured regularization for BOW based on parse trees, topics and hierarchical word clusters to improve BOW for 3 classification tasks: topic, sentiment, and text-driven forecasting.", "startOffset": 35, "endOffset": 210}, {"referenceID": 21, "context": "Convolution kernels have been used in NLP to exploit structured information using trees for parsing and tagging (Collins and Duffy, 2001), text categorization (Lodhi et al., 2002), and question answering", "startOffset": 159, "endOffset": 179}, {"referenceID": 29, "context": "(Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti, 2006).", "startOffset": 0, "endOffset": 59}, {"referenceID": 23, "context": "(Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti, 2006).", "startOffset": 0, "endOffset": 59}, {"referenceID": 0, "context": "To learn social networks, Agarwal et al. (2014) use partial tree kernels on a representation with frame semantic information (Fillmore, 1976).", "startOffset": 26, "endOffset": 48}, {"referenceID": 0, "context": "To learn social networks, Agarwal et al. (2014) use partial tree kernels on a representation with frame semantic information (Fillmore, 1976). The tree representation in Xie et al. (2013) also incorporates frame semantics, and uses sub-", "startOffset": 26, "endOffset": 188}, {"referenceID": 16, "context": "the market (Gerber et al., 2009; Gentzkow and Shapiro, 2010; Engelberg and Parsons, 2011) have been increasingly important since Tetlock (2007) investigated the role of media in the stock market.", "startOffset": 11, "endOffset": 89}, {"referenceID": 16, "context": "the market (Gerber et al., 2009; Gentzkow and Shapiro, 2010; Engelberg and Parsons, 2011) have been increasingly important since Tetlock (2007) investigated the role of media in the stock market.", "startOffset": 12, "endOffset": 144}, {"referenceID": 16, "context": "the market (Gerber et al., 2009; Gentzkow and Shapiro, 2010; Engelberg and Parsons, 2011) have been increasingly important since Tetlock (2007) investigated the role of media in the stock market. As mentioned in Wong et al. (2014), a better solution to the problem can help gain more in-", "startOffset": 12, "endOffset": 231}, {"referenceID": 12, "context": "sights to the long-lasting question in finance about how financial markets react to news (Fama, 1998; Chan, 2003).", "startOffset": 89, "endOffset": 113}, {"referenceID": 4, "context": "sights to the long-lasting question in finance about how financial markets react to news (Fama, 1998; Chan, 2003).", "startOffset": 89, "endOffset": 113}, {"referenceID": 6, "context": ", 2011; Creamer et al., 2013; Xie et al., 2013). Wong et al. (2014) report that even \u201ctextbook models\u201d that uses time series data have less than 51.", "startOffset": 8, "endOffset": 68}, {"referenceID": 26, "context": "Work in NLP and related areas (Devitt and Ahmad, 2007; Schumaker et al., 2012; Feldman et al., 2011; Zhang and Skiena, 2010) often treats stock price prediction from news as a sentiment classification problem.", "startOffset": 30, "endOffset": 124}, {"referenceID": 13, "context": "Work in NLP and related areas (Devitt and Ahmad, 2007; Schumaker et al., 2012; Feldman et al., 2011; Zhang and Skiena, 2010) often treats stock price prediction from news as a sentiment classification problem.", "startOffset": 30, "endOffset": 124}, {"referenceID": 13, "context": ", 2012; Feldman et al., 2011; Zhang and Skiena, 2010) often treats stock price prediction from news as a sentiment classification problem. Xie et al. (2013) point out that this is consistent with the direction component of the three-part ADS model (Rydberg and Shephard, 2003).", "startOffset": 8, "endOffset": 157}, {"referenceID": 35, "context": "As in Xie et al. (2013), we refer to the company we make predictions about as the designated entity.", "startOffset": 6, "endOffset": 24}, {"referenceID": 1, "context": "Combining basis kernels is a common problem in machine learning and several multiple kernel learning techniques have been developed to allow benefits from multiple kernels (Smits and Jordaan, 2002; Bach et al., 2004).", "startOffset": 172, "endOffset": 216}, {"referenceID": 30, "context": "The one-day delay of price response to news is due to (Tetlock et al., 2008).", "startOffset": 54, "endOffset": 76}, {"referenceID": 22, "context": "Sentences are parsed using the MST dependency parser (McDonald et al., 2005), which implements the Eisner algorithm (Eisner, 1996) for dependency parsing, and provides an efficient and robust performance.", "startOffset": 53, "endOffset": 76}, {"referenceID": 10, "context": ", 2005), which implements the Eisner algorithm (Eisner, 1996) for dependency parsing, and provides an efficient and robust performance.", "startOffset": 47, "endOffset": 61}, {"referenceID": 35, "context": "representation for the price prediction task that is an enriched hybrid of vector and tree space (Xie et al., 2013).", "startOffset": 97, "endOffset": 115}, {"referenceID": 23, "context": "Learning relies on Tree Kernel SVM (Moschitti, 2006).", "startOffset": 35, "endOffset": 52}, {"referenceID": 33, "context": "which is part of MPQA (Wiebe et al., 2005).", "startOffset": 22, "endOffset": 42}, {"referenceID": 14, "context": "Inspired by the work of Galitsky (2014), who constructed dependency parse forests for paragraphs of text, one of our future directions is to extend OmniGraph to incorporate discourse information.", "startOffset": 24, "endOffset": 40}], "year": 2015, "abstractText": "OmniGraph, a novel representation to support a range of NLP classification tasks, integrates lexical items, syntactic dependencies and frame semantic parses into graphs. Feature engineering is folded into the learning through convolution graph kernel learning to explore different extents of the graph. A high-dimensional space of features includes individual nodes to complex networks. In experiments on a text-forecasting problem that predicts stock price change from news for company mentions, OmniGraph beats several benchmarks based on bag-of-words, syntactic dependencies, and semantic trees. The highly expressive features OmniGraph discovers provide insights into the semantics across distinct market sectors. To demonstrate the method\u2019s generality, we also report its high performance results on a fine-grained sentiment corpus.", "creator": "LaTeX with hyperref package"}}}