{"id": "1510.03623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "Elastic regularization in restricted Boltzmann machines: Dealing with $p\\gg N$", "abstract": "llamazares Restricted Boltzmann meszaros machines (9/27 RBMs) conflictual are myroslav endowed with nembe the universal power azaz of shokhin modeling (mariavite binary) succi joint homestays distributions. Meanwhile, as pulte a tibbles result bordes of mazidi their lewenhaupt confining benzino network alers structure, training mahu RBMs confronts less juul difficulties (compared englander with more 232.1 complicated l.h. models, esquipulas e. g. , bia\u0142y Boltzmann kentwell machines) when shm dealing with approximation and 45.79 inference augury issues. However, teixeira in 1,499 certain mileti computational 3,053 biology rosprirodnadzor scenarios, ornithoptera such sympathising as the cancer icelandair data kegworth analysis, christabel employing coalmine RBMs libido to frn model data eyjafjallaj\u00f6kull features mexican-americans may showbread lose piegan its radcliff efficacy due detuned to forchheim the \" $ p \\ 53-44 gg N $ \" problem, benzine in kyats which destroying the untersuchungen number of features / predictors is non-performing much larger arboviruses than the shanley sample size. The \" $ sharafi p \\ gangliosides gg N $ \" spreading problem denfeld puts palatka the bias - convergencia variance trade - legendarium off in a razgrad more bexhill crucial potsie place when designing statistical amboyna learning tail-end methods. In this kyokutenho manuscript, furse we plutocratic try masferrer to 26.06 address meppen this problem driehaus by proposing stolt-nielsen a scrums novel oulart RBM model, called parme elastic brownless restricted tryscorers Boltzmann xyzzy machine (eRBM ), which rishtey incorporates drik the elastic regularization term into rustics the osio likelihood / kandie cost kadisha function. tehnika We zna provide several saladin theoretical 24-percent analysis on the superiority of cobram our blandy model. bransgrove Furthermore, two-fold attributed yakushev to power the classic nahida contrastive divergence (rassinier CD) sawicki algorithm, eRBMs eco-systems can otar be footwear trained efficiently. equational Our eavesdropped novel model is velikov a chatterer promising unterallg\u00e4u method 43.39 for future cancer concealer data analysis.", "histories": [["v1", "Tue, 13 Oct 2015 11:14:03 GMT  (15kb,D)", "http://arxiv.org/abs/1510.03623v1", "12 pages, 1 figure"], ["v2", "Wed, 21 Oct 2015 12:19:13 GMT  (0kb,I)", "http://arxiv.org/abs/1510.03623v2", "This paper has been withdrawn by the author due to a critical error"]], "COMMENTS": "12 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sai zhang"], "accepted": false, "id": "1510.03623"}, "pdf": {"name": "1510.03623.pdf", "metadata": {"source": "CRF", "title": "Elastic regularization in restricted Boltzmann machines: Dealing with p N", "authors": ["Sai Zhang"], "emails": ["zhangsai13@mails.tsinghua.edu.cn."], "sections": [{"heading": null, "text": "\u2217 Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China. E-mail: zhangsai13@mails.tsinghua.edu.cn.\nar X\niv :1\n51 0.\n03 62\n3v 1\n[ cs\n.L G\n] 1\n3 O\nct 2\n01 5"}, {"heading": "1 Introduction", "text": "In the past decade, with the advent of high-throughput experimental techniques, comprehensive efforts have been made to collect molecular profiling data (e.g., genomic, transcriptomic, epigenomic and proteomic) of tumor samples. The availability of these large amounts of cancer molecular profiles facilitates extensive cancer disease studies and further novel biological discoveries, in which appropriate computational and statistical tools are needed to perform the data analysis task [1\u20133], e.g., exploring data features (co-expression genes) and influential elements (genes or mRNAs), integrating various data types and predicting certain biological responses (survival time or cancer subtypes) we are interested in.\nHowever, it is not straightforward to apply existing machine learning methods in the cancer data analysis, mainly due to the specific properties of cancer molecular profiles: Unlike other data analysis tasks, such as image classification and automatic speech recognition, the dimension of cancer data features greatly outnumbers the sample size. For example, a typical cancer genomic dataset downloaded from The Cancer Genome Atlas (TCGA) [4] contains only hundreds (denoted by N) of samples, but each sample contains more than 10k (denoted by p) measurements of genomic profiling, such as gene expression and copy number variation. This well-known \u201cp N\u201d problem [5, 6] requires the designed model to be able to address the bias-variance trade-off issue [5] effectively: On one hand, as the feature dimension p is large, the model should be equipped with enough potential/complexity for discovering complicated statistical characteristics across the input features; one the other hand, we need the model to perform well in sensitivity/generalization since the observed sample size N is extremely limited compared to p, which will lead to overfitting easily. In statistics and machine learning fields, one popular principle to tackle the \u201cp N\u201d problem is variable selection [7], also referred to as feature selection, which integrates additional penalty/regularization terms into the original cost function, and performs estimation along with selecting pivotal features. LASSO [8] and its descendant elastic net [9] are two of the most classic methods realizing feature selection. Nevertheless, most of these models are linear in nature and lack the ability to model complicated statistical characteristics, especially when p is large.\nRestricted Boltzmann machines (RBMs) [10] have been widely studied and used in the machine learning fields. For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14]. In particular, RBMs have been proven to own the universal potential of approximating discrete distributions [15]. In addition, Hinton\u2019s training algorithm, i.e., the contrastive divergence (CD) algorithm [16], can be used to train RBMs efficiently. Both these aspects make the RBM an ideal candidate for modeling complex statistical characteristics of the input data. In fact, based on the original RBMs, various modifications have been made in different applications, for instance, Gaussian RBMs are proposed to model images [10], replicated softmax model are used to model word distributions and extract latent topics in the documents [17], RBMs can also be made to perform collaborative filtering tasks [18]. Note that in the bioinformatics field, Wang and Zeng have proposed an RBM-liked model to predict drug\u2013target interactions effectively [19]. Though RBMs have been applied successfully in numerous applications, for the cancer data analysis, it is far from making use of RBMs directly, mainly due to the aforementioned \u201cp N\u201d problem.\nThe goal of this manuscript is to solve the above problem, i.e., to fully exploit the modeling power of RBMs in the \u201cp N\u201d scenario, which fills the gap between the model complexity and sensitivity. More specifically, we develop a new RBM model, called elastic restricted Boltzmann machine (eRBM), which extends the traditional RBM model by adding an elastic regularization term to the likelihood function. Under the maximum likelihood estimation (MLE), our eRBMs can be trained using the standard CD method efficiently, with only a few modifications. We also provide several theoretical results showing that the regularized optimization problem of\neRBMs owns nice properties that satisfy our complexity-vs-generalization demand."}, {"heading": "2 Preliminaries", "text": "Restricted Boltzmann machines (RBMs, see Fig. 1) [10] are undirected graphical models (also referred to as Markov random fields) which describe the probability distributions of binary variables. Various generalizations [20] of RBMs make them effective to model different types of data, e.g., count vectors [17] and real-valued data [20]. As a preliminary, here we introduce the Bernoulli-Bernoulli RBMs (BBRBMs). In the following, we always denote BBRBM by RBM for short.\nAn RBM can be regarded as a bipartite graph: the visible layer consists of m visible units {V1, \u00b7 \u00b7 \u00b7 , Vm} to represent observed data/variables, and the dependency across them is depicted by n hidden units {H1, \u00b7 \u00b7 \u00b7 , Hn} which constitute the hidden layer. We note that both visible and hidden units are binary variables in a BBRBM. In particular, in an RBM, each visible unit is connected to every hidden unit, but there is no edge between each two units in the same layer. This means that the variables in the same layer are conditionally independent given the other layer.\nIn an RBM, let wij denote the real-valued weight associated with the edge between the visible variable Vi and the hidden variable Hj , and let bi and ci be the bias terms of Vi and Hi, respectively. We further use W, b and c to denote the vector representations of corresponding parameters. Then the joint probability mass function of the RBM can be defined by\np(v,h;\u03b8) := 1\nZ(\u03b8) exp(\u2212E(v,h;\u03b8)), (1)\nwhere (v,h) \u2208 {0, 1}m+n, \u03b8 := {b, c,W}, Z(\u03b8) is the partition function, and the energy function E : {0, 1}m+n \u2192 R is defined by\nE(v,h;\u03b8) := \u2212 m\u2211 i=1 n\u2211 j=1 wijvihj \u2212 m\u2211 i=1 bivi \u2212 n\u2211 j=1 cjhj . (2)\nNote that here m is the dimension of the input features and we have replaced p by m to follow the convention of machine learning literature. The conditional independence of visible and hidden variables makes it easy to calculate their conditional probabilities:\nP(Hj = 1|v) = \u03c3 ( m\u2211 i=1 wijvi + cj ) (3)\nand\nP(Vi = 1|h) = \u03c3  n\u2211 j=1 wijhj + bi  , (4) where \u03c3(x) denotes the sigmoid function, i.e., \u03c3(x) := 1/(1 + e\u2212x).\nThe principle of training an RBM is based on the maximum likelihood estimation (MLE), i.e., to learn the parameter set \u03b8 that maximizes the log-likelihood lnL(S;\u03b8) = \u2211 v\u2208S ln p(v;\u03b8) of the training set S. The gradient of the log-likelihood is given by\n\u2202 lnL(S;\u03b8) \u2202\u03b8\n\u221d \u2212Ep(h|v)q(v) [ \u2202E(v,h;\u03b8)\n\u2202\u03b8\n] + Ep(h,v) [ \u2202E(v,h;\u03b8)\n\u2202\u03b8\n] , (5)\nwhere q(v) denotes the empirical distribution. The above gradient is of exponential complexity and thus intractable. One may approximate the expectations by the sampling methods, e.g., Gibbs sampling, but this requires running the Markov chain long enough to achieve its equilibrium, which is also less efficient. Intuitively, during sampling we can run the Markov chain for only a few steps before its convergence, which results in the contrastive divergence (CD) algorithm [16]. CD has been shown to be sufficient for training products of experts and is now a standard training method for RBMs. With k-step CD (usually k = 1), the gradient of the log-likelihood for one training sample v(0) can be approximated by\nCDk(v (0);\u03b8) = \u2212 \u2211 h p(h|v(0))\u2202E(v (0),h;\u03b8) \u2202\u03b8 + \u2211 h p(h|v(k))\u2202E(v (k),h;\u03b8) \u2202\u03b8 , (6)\nwhere v(k) is the sample value after running the Markov chain for k steps and v(0) is the original value of the observed data. Note that Eq. 6 is in its general form, and the update rules (based on the gradient ascent algorithm) for other RBM-liked models can be derived easily."}, {"heading": "3 Elastic restricted Boltzmann machines", "text": "We have mentioned that RBMs are universal approximators of discrete distributions [21]. However, in the case of \u201cp n\u201d, i.e., the dimension of data features is much larger than the sample size, the traditional RBM models and their corresponding training algorithms do fall into a dilemma of model complexity vs. generalization, where the bias-variance trade-off play a more important role in the model designing. Mathematically speaking, limited by the insufficient training samples, the \u201cp N\u201d case always incurs the ill-posed problem [22], whose false solutions discover the false structures in the data that results in the overfitting phenomena. It has been shown that the well-studied regularization theory can solve the ill-posed problem satisfactorily and has been widely used in statistics and optimization fields [7, 22]."}, {"heading": "3.1 From complexity to generalization", "text": "Based on the discussion in [20], the number of bits it takes to identify an input data determines the amount of constraint each training example imposes on the model parameters. Thus, it is reasonable to fit m \u00d7 n parameters to m \u00d7 N training bits if N n. This implies that we should determine the number of hidden units to be small enough limited by the sample size, which yields large bias but small variance. On the other hand, since the input feature dimension is severely large, and adding hidden units improves the modeling power at least for BBRBM [21], we need large enough number of hidden units to characterize the complicated probabilistic distribution of the high-dimensional input data, which yields small bias but large variance. To address this particular bias-variance trade-off issue, here we consider the following regularized optimization problem,\nminimize W \u2212 1 |S| lnL(S), subject to \u2016W\u20161\u2264 t for some t, (7)\nwhere |S| is the sample size, \u2016 \u00b7 \u20161 denotes the l1-norm and \u2016W\u20161:= \u2211\nij |wij |. Note that here we only consider the regularization of weights. It is well-known that the l1 regularization generates\nthe sparsity of the model parameters. For example, in linear regression, LASSO [8] plays the role of variable selection and omits those predictors less responsible for the regression objective. For the RBM model, the l1 regularization term in Problem (7) will shrink most weights to be 0, which results in that each input bit is only responsible for a few hidden units. This enables us to add more hidden units (i.e., increasing the model complexity) for boosting RBM\u2019s modeling power without losing its generalization ability (i.e., preventing overfitting). Therefore, we have tackled the bias-variance trade-off in the \u201cp N\u201d case preliminarily. Indeed, since the weights minimizing Problem (7) are sparse, each hidden unit is actually activated by a few visible units (see Eq. 3). Thus optimizing Problem (7) can be treated as automatically selecting prominent variables/features responsible for hidden units."}, {"heading": "3.2 From generalization to complexity", "text": "It has been shown that in the \u201cp N\u201d problem, there are strong correlations across visible variables, which clusters the variables into groups [6, 9]. Different from linear models (e.g., linear regression and logistic regression), in which variable correlations are embodied by the corresponding coefficients, RBMs can model this grouping effect with hidden variables, which is also the main superiority of the latent variable models. However, the l1 regularization appearing in Problem (7) also restricts the model flexibility, which weakens RBM\u2019s ability to fully discover the variable correlations. In particular, it has been shown theoretically that LASSO tends to randomly select only one variable from the correlated group [23].\nTo make a further compromise between the model generalization and its expression power, the well-known l2 regularization turns to be our natural choice. Intuitively, let us first consider the following simple optimization problem,\nminimize wij \u2211 ij w2ij , subject to \u2211 ij |wij | = c,\nwhere c is a constant. Geometrically speaking, we want to minimize the radius of a ball (expressed as the object function) in a high-dimensional space with the restriction that the ball intersects with a given convex polyhedron (expressed as the constraint). Obviously, the solutions of this problem are given when the ball and the fixed convex polyhedron are tangent, where all coordinates of the solution vectors have equal modulus. Thus, by adding another external l2 regularization term in Problem (7), the weights in the RBM are averaged to some extent, which imposes similar weights between correlated variables explicitely.\nAt last, we obtain the following optimization problem,\nminimize W \u2212 1 |S| lnL(S),\nsubject to \u2016W\u20161\u2264 t1 and \u2016W\u201622\u2264 t2 for some t1, t2, (8)\nwhere \u2016 \u00b7 \u20162 denotes the l2-norm and \u2016W\u201622:= \u2211 ij w 2 ij . In particular, Problem (8) is equivalent to\nminimize W \u2212 1 |S| lnL(S) + \u03bb1 \u2016W\u20161 +\u03bb2 \u2016W\u201622, (9)\nwhere \u03bb1 and \u03bb2 are two fixed coefficients measuring the contributions of the corresponding regularization terms. Let \u03b1 = \u03bb2/(\u03bb1 +\u03bb2), then the above problem is further equivalent to the problem\nminimize W \u2212 1 |S| lnL(S),\nsubject to (1\u2212 \u03b1)\u2016W\u20161+\u03b1\u2016W\u201622\u2264 t for some t, (10)\nwhere we refer to (1\u2212\u03b1)\u2016W\u20161+\u03b1\u2016W\u201622 as the elastic regularization term1, which is a convex combination of the l1- and l2-norms. The corresponding regularization technique is referred to as the elastic regularization, and the resulting RBM with Problem (10) is called the elastic restricted Boltzmann machine (eRBM). Note that when \u03b1 = 0, Problem (10) degenerates to Problem (7), while when \u03b1 = 1, Problem (10) loses its l1 regularization term and is called the weight decay method in neural networks [20]. In this paper, we only consider \u03b1 \u2208 [0, 1). Also note that the elastic regularization term is strictly convex as the l2 regularization is considered, i.e., \u03b1 > 0."}, {"heading": "4 Theoretical analysis", "text": "To characterize the effects of the elastic regularization term of Problem (9) (or equivalently Problem (10)), here we provide several theoretical results on both the extreme situation in which several visible variables are exactly positively correlated, namely, they are identical, as well as the general case. Techniques used here are analogous to those in [9]. Note that in this manuscript, we can only consider the local minima of Problem (9) because of its non-convexity. For brevity, we denote the objective function of Problem (9) as O(W) := L(W)+R(W), where\nL(W) = \u2212 1 |S| lnL(S), R(W) = \u03bb1 \u2016W\u20161 +\u03bb2 \u2016W\u201622 .\nFurthermore, let us denote a local minimizer of Problem (9) in the neighbourhood U as W\u0302, which means that \u2200W \u2208 U , we have O(W) \u2265 O(W\u0302).\nLemma 1. Assume that for the training examples, Vi = Vj, where i, j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}. Then we have the following conclusions: (i) If \u03bb1, \u03bb2 > 0, then w\u0302ik = w\u0302jk, \u2200k \u2208 {1 \u00b7 \u00b7 \u00b7n}. (ii) If \u03bb2 = 0, then w\u0302ikw\u0302jk \u2265 0, \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 , n}. For fixed k, W\u0302\u2217 is another minimizer of Problem (9), where\nw\u0302\u2217gh =  w\u0302gh \u2200h \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, if g 6= i, j, w\u0302gh if g = i or g = j, but h 6= k, s(w\u0302ik + w\u0302jk) if g = i and h = k,\n(1\u2212 s)(w\u0302ik + w\u0302jk) if g = j and h = k,\nfor |s\u2212 w\u0302ik/(w\u0302ik + w\u0302jk)| is small enough.\nProof. (i) First, let us fix \u03bb1, \u03bb2 > 0. If w\u0302ik 6= w\u0302jk for some k, let us consider another weight vector W\u0302\u2217 as follows:\nw\u0302\u2217gh =  w\u0302gh \u2200h \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, if g 6= i, j, w\u0302gh if g = i or g = j, but h 6= k, (1\u2212 \u03b5)w\u0302ik + \u03b5w\u0302jk if g = i and h = k, \u03b5w\u0302ik + (1\u2212 \u03b5)w\u0302jk if g = j and h = k,\nwhere \u03b5 < 1 is small enough such that for some \u03b4 > 0, B(W\u0302, \u03b5 |w\u0302ik \u2212 w\u0302jk| + \u03b4) \u2282 U . Here the open ball B(x0, r) is defined by B(x0, r) := {x : |x \u2212 x0| < r,x \u2208 Rm\u00d7n}. Hence, W\u0302\u2217 is also located in the neighbourhood U . Since Vi = Vj , it is evident that L(W\u0302) = L(W\u0302 \u2217). However, since \u03bb1, \u03bb2 > 0, the elastic regularization term R(W) is strictly convex, which yields R(W\u0302\u2217) < R(W\u0302). Here comes a contradiction.\n1 This terminology comes from [9] in part, and its intuition can also be found there.\n(ii) As \u03bb2 = 0 and the regularization term R(W) degenerates to the l1 penalty, it is not strictly convex now. Suppose that w\u0302ikw\u0302jk < 0 for some k, consider the same W\u0302 \u2217 as that in (i). Without loss of generality, we assume that w\u0302ik > 0 and w\u0302jk < 0, and at the same time, we set \u03b5 to be small enough (without contradicting the constraints in (i)) such that \u03b5 < 1/2, (1\u2212\u03b5)w\u0302ik+ \u03b5w\u0302jk > 0 while \u03b5w\u0302ik+(1\u2212\u03b5)w\u0302jk < 0. Thus, we have |(1\u2212 \u03b5)w\u0302ik + \u03b5w\u0302jk|+ |\u03b5w\u0302ik + (1\u2212 \u03b5)w\u0302jk| = (1\u22122\u03b5)(w\u0302ik\u2212w\u0302jk) < |w\u0302ij |+|w\u0302jk|, which yields R(W\u0302\u2217) < R(W\u0302) contradicting to the assumption. The case of w\u0302ik < 0 and w\u0302jk > 0 can be discussed in the same manner.\nSince we have validated that w\u0302ik and w\u0302jk cannot own the reverse signs, we have R(W\u0302 \u2217) = R(W\u0302), and further O(W\u0302\u2217) = O(W\u0302). After replacing the above \u03b5 by s, we note that s shall satisfy that d(W\u0302,W\u0302\u2217) = |s\u2212 w\u0302ik/(w\u0302ik + w\u0302jk)| is small enough to make W\u0302\u2217 \u2208 U .\nLemma 1 provides us with nice properties of the eRBM. First, it guarantees the same weight solutions for two exactly correlated variables. Though empirical data rarely get this extreme correlations, Lemma 1 presents the potential of the eRBM to model variable correlations explicitly, which satisfies our requirement discussed in Section 3.2. In addition, without the l2 regularization, the problem (9) may have infinite solutions around some local minimum, which makes it less stable. Note that this is a concrete illustration of the regularization technique to solve the ill-posed problem.\nLet us consider the Pearson\u2019s correlation coefficient defined by\n\u03c1X,Y = cov(X,Y )\n\u03c3X\u03c3Y = E ((X \u2212 \u00b5X)(Y \u2212 \u00b5Y )) \u03c3X\u03c3Y , (11)\nwhere cov denotes the covariance, \u03c3 is the standard deviation, and \u00b5 represents the mean. For the reverse direction, we have the following lemma.\nLemma 2. Suppose that in an RBM, for the visible variables Vi and Vj, their weights are equal to each other, i.e., wik = wjk, \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, then their Pearson\u2019s correlation coefficient equals to 1.\nProof. Since the weights of Vi and Vj are equal to each other, we can treat them as the identical variable. Thus based on the definition of Pearson\u2019s correlation coefficient, the conclusion of this lemma follows after simple computations, which are omitted here.\nThe complexity of the RBM model lies in that the same distribution may have various model configurations, which implies that though two visible variables are correlated, their weights can be vastly distinct. However, this ill-posed problem can be solved by our eRBM model perfectly. In fact, we have the following somewhat anti-intuitive theorem.\nTheorem 3. For the solutions of eRBM, we have Vi = Vj, i, j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} if and only if wik = wjk, \u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 , n}.\nProof. This is a direct result of Lemma 1 and Lemma 2.\nNext, we consider the general case, in which the assumption of two identical variables is dropped. We have the following general result that correlates/bounds the difference between the parameter paths of variables to/by their correlations quantitatively.\nTheorem 4. Suppose that the regularization coefficients \u03bb1, \u03bb2 are both positive in the eRBM, and let W\u0302 be a local minimizer of Problem (9). Assume that the empirical distribution q(v) and the model distribution p(v) fit well, i.e., there is a constant C > 0 such that p(v)/q(v) < C, \u2200v \u2208 {0, 1}m. Also assume that w\u0302ikw\u0302jk > 0 for some k \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}. Following [9], we define\nD\u03bb1,\u03bb2(i, j, k) := |w\u0302ik(\u03bb1, \u03bb2)\u2212 w\u0302jk(\u03bb1, \u03bb2)| .\nThen D\u03bb1,\u03bb2(i, j, k) is bounded above by the empirical correlation of variables Vi and Vj. Precisely, we have D\u03bb1,\u03bb2(i, j, k) = O(Pe(vi 6= vj)), (12) where we use P and Pe to denote the model and empirical probabilities, respectively.\nProof. First, under the distribution regularity assumption, we claim that the probabilities P(Vi 6= Vj) and Pe(Vi 6= Vj) also fit well. Indeed, for P(Vi = 0, Vj = 1)/Pe(Vi = 0, Vj = 1), we have\nP(Vi = 0, Vj = 1) Pe(Vi = 0, Vj = 1) = \u2211 \u00b7\u00b7\u00b7 ,vi=0,vj=1,\u00b7\u00b7\u00b7 p(v)\u2211 \u00b7\u00b7\u00b7 ,vi=0,vj=1,\u00b7\u00b7\u00b7 q(v)\n= \u2211 p(v)\u2211\n\u00b7\u00b7\u00b7 ,vi=0,vj=1,\u00b7\u00b7\u00b7 q(v) \u2264 \u2211\n\u00b7\u00b7\u00b7 ,vi=0,vj=1,\u00b7\u00b7\u00b7\np(v)\nq(v)\n\u2264 2m\u22122C.\nAs for P(Vi = 1, Vj = 0)/Pe(Vi = 1, Vj = 0), we can get the same upper bound after similar calculations.\nSince w\u0302ikw\u0302jk > 0 , we have sgn{w\u0302ik} = sgn{w\u0302jk}. According to the assumption, the local minimizer W\u0302 satisfies\n\u2202O(W)\n\u2202wgh \u2223\u2223\u2223\u2223 W=W\u0302 = 0, if w\u0302gh 6= 0.\nThus we have\n\u2202L(W)\n\u2202wik \u2223\u2223\u2223\u2223 W=W\u0302 +\u03bb1sgn{w\u0302ik}+ 2\u03bb2w\u0302ik = 0, (13)\n\u2202L(W)\n\u2202wjk \u2223\u2223\u2223\u2223 W=W\u0302 +\u03bb1sgn{w\u0302jk}+ 2\u03bb2w\u0302jk = 0. (14)\nSubtracting Eq. 13 from Eq. 14 yields\nw\u0302ik \u2212 w\u0302jk = 1\n2\u03bb2\n( \u2202L(W)\n\u2202wjk \u2223\u2223\u2223\u2223 W=W\u0302 \u2212 \u2202L(W) \u2202wik \u2223\u2223\u2223\u2223 W=W\u0302 ) . (15)\nFurthermore, according to Eq. 5, the above equation can be written as\nw\u0302ik \u2212 w\u0302jk= 1\n2\u03bb2|S|\n( ( Ep(h|v)q(v)[VjHk]\u2212 Ep(v,h)[VjHk] ) \u2212 ( Ep(h|v)q(v)[ViHk]\u2212 Ep(v,h)[ViHk] ) ) .\nNext follows several computations,\nD\u03bb1,\u03bb2(i, j, k) = |w\u0302ik \u2212 w\u0302jk|\n= 1 2\u03bb2|S| \u00b7 \u2223\u2223\u2223Ep(h|v)q(v) [(Vi\u2212Vj)Hk]\u2212Ep(v,h) [(Vi\u2212Vj)Hk] \u2223\u2223\u2223 = 1\n2\u03bb2|S| \u00b7 \u2223\u2223\u2223Pijk(1|1, 0)\u2206ij(1, 0)\u2212 Pijk(1|0, 1)\u2206ij(1, 0)\u2223\u2223\u2223\n\u2264 1 2\u03bb2|S| \u00b7 ( \u2223\u2223\u2223\u2223Pe(Vi = 1, Vj = 0)(1\u2212 P(Vi = 1, Vj = 0)Pe(Vi = 1, Vj = 0) )\u2223\u2223\u2223\u2223 +\n\u2223\u2223\u2223\u2223Pe(Vi = 0, Vj = 1)(1\u2212 P(Vi = 0, Vj = 1)Pe(Vi = 0, Vj = 1) )\u2223\u2223\u2223\u2223 )\n\u22641 + 2 m\u22122C\n\u03bb2|S| Pe(Vi 6= Vj),\nwhere Pijk(x3|x1, x2) is short for P(Hk = x3|Vi = x1, Vj = x2) and \u2206ij(x1, x2) represents Pe(Vi = x1, Vj = x2)\u2212 P(Vi = x1, Vj = x2), x1, x2, x3 \u2208 {0, 1}. This completes the proof.\nAlgorithm 1 (k-step elastic contrastive divergence)\nRequire: eRBM with initialized parameters W, b and c, training set S. Ensure: Gradient approximations \u2206wij , \u2206bi and \u2206cj for i = 1, . . . ,m, j = 1, . . . , n.\n1: for v \u2208 S do 2: v(0) \u2190 v 3: for t = 0, . . . , k \u2212 1 do 4: for j = 1, . . . , n do 5: Sample h (t) j \u223c p(hj |v(t)) 6: end for 7: for i = 1, . . . ,m do 8: Sample v (t+1) i \u223c p(vi|h(t))\n9: end for 10: end for 11: for i = 1, . . . ,m, j = 1, . . . , n do 12: \u2206wij \u2190 \u2206wij + p(Hj = 1|v(0))v(0)i \u2212 p(Hj = 1|v(k))v (k) i \u2212 \u03bb1sgn{wij} \u2212 \u03bb2wij 13: \u2206bi \u2190 \u2206bi + v(0)i \u2212 v (k) i 14: \u2206cj \u2190 \u2206cj + p(Hj = 1|v(0))\u2212 p(Hj = 1|v(k)) 15: end for 16: end for\nFrom Theorem 2, we find that if \u03bb1 and \u03bb2 vary, the the quantity D\u03bb1,\u03bb2(i, j, k) describes the difference between the parameter paths of the variables Vi and Vj . Note that Eq. 12 bounds D\u03bb1,\u03bb2 with the empirical probability Pe(Vi 6= Vj) = 1 \u2212 Pe(Vi = Vj), which represents the positive correlation between the binary variables Vi and Vj . If Vi and Vj are highly positively correlated, i.e., Pe(Vi 6= Vj) is nearly 0, their weight difference can be guaranteed to be particularly small. Note that for the negative correlation where Pe(Vi 6= Vj) is almost 1, if we replace Vi by 1\u2212Vi and perform the similar discussions, the same conclusion follows. Theorem 2 presents the very nice property that in the general situation, the correlated variables can obtain similar weights in eRBMs, which increases the model flexibility as well as the generalization."}, {"heading": "5 Model training", "text": "With only a few modifications, our eRBM model can be trained efficiently based on the CD algorithm. In the previous sections, we have noticed that the CD algorithm is used to approximate the gradient of the log-likelihood function, i.e., Eq. 5. Meanwhile, we note that the optimization function of eRBMs (i.e., Problem (9)) contains the elastic regularization term besides the likelihood function. Therefore, to learn parameters of eRBMs, we should integrate the derivatives of the l1- and l2-norms into the CD algorithm.\nIt is trivial to compute the derivative of l2-norm for model parameters, i.e.,\n\u2202 \u2016W \u201622 \u2202wij = 2wij .\nAs for the l1-norm, here we adopt the subgradient method [24], i.e.,\n\u2202 \u2016W \u20161 \u2202wij \u2248 sgn{wij},\nwhich is widely used in convex optimization. The adapted CD algorithm, which is called elastic contrastive divergence (eCD), is shown Algorithm 1. Though intermediate variables have to be sampled (for k times) during the\nCD/eCD algorithm, attributed to the conditional independence of visible and hidden variables (see Eqs. 3 and 4), the Gibbs sampling can be performed quite efficiently. Also, it has been verified empirically that the 1-step CD algorithm can yield satisfiable training results [20]. We note that several training parameters, e.g., learning rate, training batch size and momentum [20], are omitted in Algorithm 1 for brevity."}, {"heading": "6 Related work", "text": "The idea of regularization to solve the \u201cp N\u201d problem and augment model generalization is widely used in statistics [7, 25]. There are many other regularization/penalty terms with various properties equipping statistical models, see [7] for a nice review. In particular, the traditional ridge regression [5], LASSO [8] and elastic net [9] use similar regularization techniques adopted in our study. However, different from our eRBMs, these base models are linear in nature and have no latent variables, which may lack the ability to model complicated nonlinear statistics. Moreover, both supervised and unsupervised learning are practicable in eRBMs, which extends the application scope of our method, like in the semi-supervised scenarios.\nSeveral code packages have implemented the weight decay technique [20] to train an RBM. We have shown that the weight decay is a special case of our elastic regularization given \u03b1 = 1 in Problem (10), see Section 3.2 for details. In particular, our work focuses on addressing the \u201cp N\u201d problem from the beginning (by following the bias-variance trade-off principle), and gradually derives the elastic regularization term combining both l1- and l2-norms.\nWe have noted that the l1-norm on weights leads to sparse solutions. For instance, LASSO searches for a few significant and explainable predictors while ignoring other less important factors. Indeed, sparsity is always a dominant topic in machine learning, signal processing and statistics [5, 13]. The consideration of sparsity first appears in computational neuroscience (the visual system [26]) and is further embodied as sparse coding [27]. Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32]. These sparse models are seeking for sparse latent representations of the input data, in which (in the language of RBMs) the hidden variables are activated (computing Eq. 3 and getting high probability) in only a few fraction while leaving most others silent. The detailed motivation and advantage of introducing sparsity can be found in an excellent survey [13]. However, we want to emphasize that this representation sparsity (sparse hidden variables) is not our goal (sparse weights) in this study. Indeed, these two sparsity are not conceptually orthogonal and we can further integrate the representation sparsity into our method."}, {"heading": "7 Conclusion", "text": "The \u201cp N\u201d problem challenges the existing statistical and computational models for cancer data analysis. In this manuscript, we propose a novel graphical model, called the elastic restricted Boltzmann machines (eRBMs), to address this problem. In the principle of the bias-variance trade-off, we reformalize the optimization objective of the traditional RBMs by combining the l1 and l2 regularization. Comprehensive theoretical analysis demonstrates that our eRBM models gain elegant properties satisfying our primary motivations. In addition, we develop an efficient training algorithm, referred to as the elastic contrastive divergence (eCD), for eRBMs based on the classic CD algorithm."}, {"heading": "Acknowledgements", "text": "The author thanks Dr. T. Chen, Dr. N. Chen and Dr. J. Zeng for their valuable comments and suggestions on this manuscript."}], "references": [{"title": "Expanding the Computational Toolbox for Mining Cancer Genomes", "author": ["L. Ding", "M.C. Wendl", "J.F. McMichael", "B.J. Raphael"], "venue": "Nat Rev Genet, vol. 15, pp. 556\u2013570, Aug. 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Big Data Mining Yields Novel Insights on Cancer", "author": ["P. Jiang", "X.S. Liu"], "venue": "Nat Genet, vol. 47, pp. 103\u2013104, Feb. 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Principles and Methods of Integrative Genomic Analyses in Cancer", "author": ["V.N. Kristensen", "O.C. Lingjaerde", "H.G. Russnes", "H.K.M. Vollan", "A. Frigessi", "A.-L. Borresen-Dale"], "venue": "Nat Rev Cancer, vol. 14, pp. 299\u2013313, May 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "The Cancer Genome Atlas Pan-Cancer Analysis Project", "author": ["T.C.G.A.R. Network", "J.N. Weinstein", "E.A. Collisson", "G.B. Mills", "K.R.M. Shaw", "B.A. Ozenberger", "K. Ellrott", "I. Shmulevich", "C. Sander", "J.M. Stuart"], "venue": "Nat Genet, vol. 45, pp. 1113\u20131120, Oct. 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Predicting the Clinical Status of Human Breast Cancer by Using Gene Expression Profiles", "author": ["M. West", "C. Blanchette", "H. Dressman", "E. Huang", "S. Ishida", "R. Spang", "H. Zuzan", "J.A. Olson", "J.R. Marks", "J.R. Nevins"], "venue": "Proceedings of the National Academy of Sciences, vol. 98, no. 20, pp. 11462\u201311467, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "A Selective Overview of Variable Selection in High Dimensional Feature Space", "author": ["J. Fan", "J. Lv"], "venue": "Statistica Sinica, vol. 20, pp. 101\u2013148, Jan. 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression Shrinkage and Selection Via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B, vol. 58, pp. 267\u2013288, 1994.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Regularization and Variable Selection via the Elastic Net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301\u2013320, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "An Introduction to Restricted Boltzmann Machines", "author": ["A. Fischer", "C. Igel"], "venue": "Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications (L. Alvarez, M. Mejail, L. Gomez, and J. Jacobo, eds.), vol. 7441 of Lecture Notes in Computer Science, pp. 14\u201336, Springer Berlin Heidelberg, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing the Dimensionality of Data with Neural Networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504 \u2013 507, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, pp. 1\u2013127, Jan. 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 448\u2013455, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Representational Power of Restricted Boltzmann Machines and Deep Belief Networks", "author": ["N. Le Roux", "Y. Bengio"], "venue": "Neural Comput., vol. 20, pp. 1631\u20131649, June 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Training Products of Experts by Minimizing Contrastive Divergence", "author": ["G.E. Hinton"], "venue": "Neural Comput., vol. 14, pp. 1771\u20131800, Aug. 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Replicated Softmax: An Undirected Topic Model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, eds.), pp. 1607\u20131614, Curran Associates, Inc., 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, (New York, NY, USA), pp. 791\u2013798, ACM, 2007. 11", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Predicting Drug-Target Interactions Using Restricted Boltzmann Machines", "author": ["Y. Wang", "J. Zeng"], "venue": "Bioinformatics, vol. 29, no. 13, pp. i126\u2013i134, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines", "author": ["G. Hinton"], "venue": "Tech. Rep. UTML-TR-2010- 003, Dept. of Computer Science, Univ.of Toronto, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Representational Power of Restricted Boltzmann Machines and Deep Belief Networks", "author": ["N. Le Roux", "Y. Bengio"], "venue": "Neural Comput., vol. 20, pp. 1631\u20131649, June 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Least Angle Regression", "author": ["B. Efron", "T. Hastie", "L. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 32, pp. 407\u2013499, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association, vol. 96, no. 456, pp. 1348\u20131360, 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Sparse Coding with An Overcomplete Basis Set: A Strategy Employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research, vol. 37, pp. 3311\u20133325, Dec. 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Emergence of Simple-Cell Receptive Field Properties by Learning A Sparse Code for Natural Images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Nature, vol. 381, pp. 607\u2013609, June 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1828}, {"title": "Sparse Feature Learning for Deep Belief Networks", "author": ["M. aurelio Ranzato", "Y. lan Boureau", "Y.L. Cun"], "venue": "Advances in Neural Information Processing Systems 20 (J. Platt, D. Koller, Y. Singer, and S. Roweis, eds.), pp. 1185\u20131192, Curran Associates, Inc., 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient Learning of Sparse Representations with An Energy-Based Model", "author": ["M. aurelio Ranzato", "C. Poultney", "S. Chopra", "Y.L. Cun"], "venue": "Advances in Neural Information Processing Systems 19 (B. Sch\u00f6lkopf, J. Platt, and T. Hoffman, eds.), pp. 1137\u20131144, MIT Press, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "author": ["M. Ranzato", "F. Huang", "Y. Boureau", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, vol. 0, (Los Alamitos, CA, USA), pp. 1\u20138, IEEE, June 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "3D Object Recognition with Deep Belief Nets", "author": ["V. Nair", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, eds.), pp. 1339\u20131347, Curran Associates, Inc., 2009. 12", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The availability of these large amounts of cancer molecular profiles facilitates extensive cancer disease studies and further novel biological discoveries, in which appropriate computational and statistical tools are needed to perform the data analysis task [1\u20133], e.", "startOffset": 258, "endOffset": 263}, {"referenceID": 1, "context": "The availability of these large amounts of cancer molecular profiles facilitates extensive cancer disease studies and further novel biological discoveries, in which appropriate computational and statistical tools are needed to perform the data analysis task [1\u20133], e.", "startOffset": 258, "endOffset": 263}, {"referenceID": 2, "context": "The availability of these large amounts of cancer molecular profiles facilitates extensive cancer disease studies and further novel biological discoveries, in which appropriate computational and statistical tools are needed to perform the data analysis task [1\u20133], e.", "startOffset": 258, "endOffset": 263}, {"referenceID": 3, "context": "For example, a typical cancer genomic dataset downloaded from The Cancer Genome Atlas (TCGA) [4] contains only hundreds (denoted by N) of samples, but each sample contains more than 10k (denoted by p) measurements of genomic profiling, such as gene expression and copy number variation.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "This well-known \u201cp N\u201d problem [5, 6] requires the designed model to be able to address the bias-variance trade-off issue [5] effectively: On one hand, as the feature dimension p is large, the model should be equipped with enough potential/complexity for discovering complicated statistical characteristics across the input features; one the other hand, we need the model to perform well in sensitivity/generalization since the observed sample size N is extremely limited compared to p, which will lead to overfitting easily.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "This well-known \u201cp N\u201d problem [5, 6] requires the designed model to be able to address the bias-variance trade-off issue [5] effectively: On one hand, as the feature dimension p is large, the model should be equipped with enough potential/complexity for discovering complicated statistical characteristics across the input features; one the other hand, we need the model to perform well in sensitivity/generalization since the observed sample size N is extremely limited compared to p, which will lead to overfitting easily.", "startOffset": 30, "endOffset": 36}, {"referenceID": 4, "context": "This well-known \u201cp N\u201d problem [5, 6] requires the designed model to be able to address the bias-variance trade-off issue [5] effectively: On one hand, as the feature dimension p is large, the model should be equipped with enough potential/complexity for discovering complicated statistical characteristics across the input features; one the other hand, we need the model to perform well in sensitivity/generalization since the observed sample size N is extremely limited compared to p, which will lead to overfitting easily.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "In statistics and machine learning fields, one popular principle to tackle the \u201cp N\u201d problem is variable selection [7], also referred to as feature selection, which integrates additional penalty/regularization terms into the original cost function, and performs estimation along with selecting pivotal features.", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "LASSO [8] and its descendant elastic net [9] are two of the most classic methods realizing feature selection.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "LASSO [8] and its descendant elastic net [9] are two of the most classic methods realizing feature selection.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "Restricted Boltzmann machines (RBMs) [10] have been widely studied and used in the machine learning fields.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 82, "endOffset": 89}, {"referenceID": 11, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 82, "endOffset": 89}, {"referenceID": 12, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 82, "endOffset": 89}, {"referenceID": 11, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "In particular, RBMs have been proven to own the universal potential of approximating discrete distributions [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": ", the contrastive divergence (CD) algorithm [16], can be used to train RBMs efficiently.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "In fact, based on the original RBMs, various modifications have been made in different applications, for instance, Gaussian RBMs are proposed to model images [10], replicated softmax model are used to model word distributions and extract latent topics in the documents [17], RBMs can also be made to perform collaborative filtering tasks [18].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "In fact, based on the original RBMs, various modifications have been made in different applications, for instance, Gaussian RBMs are proposed to model images [10], replicated softmax model are used to model word distributions and extract latent topics in the documents [17], RBMs can also be made to perform collaborative filtering tasks [18].", "startOffset": 269, "endOffset": 273}, {"referenceID": 17, "context": "In fact, based on the original RBMs, various modifications have been made in different applications, for instance, Gaussian RBMs are proposed to model images [10], replicated softmax model are used to model word distributions and extract latent topics in the documents [17], RBMs can also be made to perform collaborative filtering tasks [18].", "startOffset": 338, "endOffset": 342}, {"referenceID": 18, "context": "Note that in the bioinformatics field, Wang and Zeng have proposed an RBM-liked model to predict drug\u2013target interactions effectively [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "1) [10] are undirected graphical models (also referred to as Markov random fields) which describe the probability distributions of binary variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Various generalizations [20] of RBMs make them effective to model different types of data, e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": ", count vectors [17] and real-valued data [20].", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": ", count vectors [17] and real-valued data [20].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "Intuitively, during sampling we can run the Markov chain for only a few steps before its convergence, which results in the contrastive divergence (CD) algorithm [16].", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "We have mentioned that RBMs are universal approximators of discrete distributions [21].", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "It has been shown that the well-studied regularization theory can solve the ill-posed problem satisfactorily and has been widely used in statistics and optimization fields [7, 22].", "startOffset": 172, "endOffset": 179}, {"referenceID": 19, "context": "1 From complexity to generalization Based on the discussion in [20], the number of bits it takes to identify an input data determines the amount of constraint each training example imposes on the model parameters.", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "On the other hand, since the input feature dimension is severely large, and adding hidden units improves the modeling power at least for BBRBM [21], we need large enough number of hidden units to characterize the complicated probabilistic distribution of the high-dimensional input data, which yields small bias but large variance.", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "For example, in linear regression, LASSO [8] plays the role of variable selection and omits those predictors less responsible for the regression objective.", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "2 From generalization to complexity It has been shown that in the \u201cp N\u201d problem, there are strong correlations across visible variables, which clusters the variables into groups [6, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 8, "context": "2 From generalization to complexity It has been shown that in the \u201cp N\u201d problem, there are strong correlations across visible variables, which clusters the variables into groups [6, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 21, "context": "In particular, it has been shown theoretically that LASSO tends to randomly select only one variable from the correlated group [23].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "Note that when \u03b1 = 0, Problem (10) degenerates to Problem (7), while when \u03b1 = 1, Problem (10) loses its l1 regularization term and is called the weight decay method in neural networks [20].", "startOffset": 184, "endOffset": 188}, {"referenceID": 8, "context": "Techniques used here are analogous to those in [9].", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "1 This terminology comes from [9] in part, and its intuition can also be found there.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "Following [9], we define D\u03bb1,\u03bb2(i, j, k) := |\u0175ik(\u03bb1, \u03bb2)\u2212 \u0175jk(\u03bb1, \u03bb2)| .", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Also, it has been verified empirically that the 1-step CD algorithm can yield satisfiable training results [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": ", learning rate, training batch size and momentum [20], are omitted in Algorithm 1 for brevity.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "The idea of regularization to solve the \u201cp N\u201d problem and augment model generalization is widely used in statistics [7, 25].", "startOffset": 116, "endOffset": 123}, {"referenceID": 22, "context": "The idea of regularization to solve the \u201cp N\u201d problem and augment model generalization is widely used in statistics [7, 25].", "startOffset": 116, "endOffset": 123}, {"referenceID": 6, "context": "There are many other regularization/penalty terms with various properties equipping statistical models, see [7] for a nice review.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "In particular, the traditional ridge regression [5], LASSO [8] and elastic net [9] use similar regularization techniques adopted in our study.", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "In particular, the traditional ridge regression [5], LASSO [8] and elastic net [9] use similar regularization techniques adopted in our study.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "In particular, the traditional ridge regression [5], LASSO [8] and elastic net [9] use similar regularization techniques adopted in our study.", "startOffset": 79, "endOffset": 82}, {"referenceID": 19, "context": "Several code packages have implemented the weight decay technique [20] to train an RBM.", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "Indeed, sparsity is always a dominant topic in machine learning, signal processing and statistics [5, 13].", "startOffset": 98, "endOffset": 105}, {"referenceID": 12, "context": "Indeed, sparsity is always a dominant topic in machine learning, signal processing and statistics [5, 13].", "startOffset": 98, "endOffset": 105}, {"referenceID": 23, "context": "The consideration of sparsity first appears in computational neuroscience (the visual system [26]) and is further embodied as sparse coding [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "The consideration of sparsity first appears in computational neuroscience (the visual system [26]) and is further embodied as sparse coding [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 25, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 193, "endOffset": 197}, {"referenceID": 26, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 232, "endOffset": 239}, {"referenceID": 27, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 232, "endOffset": 239}, {"referenceID": 28, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 232, "endOffset": 239}, {"referenceID": 19, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 260, "endOffset": 268}, {"referenceID": 29, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 260, "endOffset": 268}, {"referenceID": 12, "context": "The detailed motivation and advantage of introducing sparsity can be found in an excellent survey [13].", "startOffset": 98, "endOffset": 102}], "year": 2017, "abstractText": "Restricted Boltzmann machines (RBMs) are endowed with the universal power of modeling (binary) joint distributions. Meanwhile, as a result of their confining network structure, training RBMs confronts less difficulties (compared with more complicated models, e.g., Boltzmann machines) when dealing with approximation and inference issues. However, in certain computational biology scenarios, such as the cancer data analysis, employing RBMs to model data features may lose its efficacy due to the \u201cp N\u201d problem, in which the number of features/predictors is much larger than the sample size. The \u201cp N\u201d problem puts the bias-variance trade-off in a more crucial place when designing statistical learning methods. In this manuscript, we try to address this problem by proposing a novel RBM model, called elastic restricted Boltzmann machine (eRBM), which incorporates the elastic regularization term into the likelihood/cost function. We provide several theoretical analysis on the superiority of our model. Furthermore, attributed to the classic contrastive divergence (CD) algorithm, eRBMs can be trained efficiently. Our novel model is a promising method for future cancer data analysis. \u2217 Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China. E-mail: zhangsai13@mails.tsinghua.edu.cn. 1 ar X iv :1 51 0. 03 62 3v 1 [ cs .L G ] 1 3 O ct 2 01 5", "creator": "TeX"}}}