{"id": "1510.07641", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2015", "title": "Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks", "abstract": "We present riehl a novel application of rul LSTM frisch recurrent boardings neural networks to zoheir multilabel defacto classification of kesi diagnoses given variable - ripatti length time cristiano series dumping of clinical phoney measurements. Our method mahf outperforms a kino strong baseline jambhala on groombridge a rg3 variety 47.83 of dico metrics.", "histories": [["v1", "Mon, 26 Oct 2015 20:18:56 GMT  (97kb,D)", "http://arxiv.org/abs/1510.07641v1", null], ["v2", "Tue, 21 Mar 2017 21:28:55 GMT  (97kb,D)", "http://arxiv.org/abs/1510.07641v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zachary c lipton", "david c kale", "randall c wetzel"], "accepted": false, "id": "1510.07641"}, "pdf": {"name": "1510.07641.pdf", "metadata": {"source": "CRF", "title": "Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks", "authors": ["Zachary C. Lipton", "David C. Kale"], "emails": ["zlipton@cs.ucsd.edu", "dkale@usc.edu", "rwetzel@chla.usc.edu"], "sections": [{"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10]. LSTM RNNs can capture long range dependencies and nonlinear dynamics. Clinical time series data, as recorded in the pediatric intensive care unit (PICU), exhibit these properties and others, including irregular sampling and non-random missing values [11]. Symptoms of acute respiratory distress syndrome, for example, often do not appear for 24- 48 hours after lung injury [12]. Other approaches like Markov models, conditional random fields, and Kalman filters deal with sequential data, but are ill-equipped to learn long-range dependencies. Some models require domain knowledge or feature engineering, offering less chance for serendipitous discovery. Neural networks learn representations, potentially discovering unforeseen structure.\nThis paper presents a preliminary empirical study of LSTM RNNs applied to supervised phenotyping of multivariate PICU time series. We classify each episode as having one or more diagnoses from among over one hundred possibilities. Prior works have applied RNNs to health data, including electrocardiograms [13, 14, 15] and glucose measurements [16]. RNNs have also been used for prediction problems in genomics [8, 10, 9]. A variety of works have applied feed-forward neural networks to health data for prediction and pattern mining, but none have used RNNs or directly handled variable length sequences [17, 18, 19, 20]. To our knowledge, this work is the first to apply modern LSTMs to a large data set of multivariate clinical time series. Our experiments show that LSTMs can successfully classify clinical time series from raw measurements, naturally handling challenges like variable sequence length and high dimensional output spaces."}, {"heading": "2 Data Description", "text": "Our experiments use a collection of fully anonymized clinical time series extracted from the electronic health records system at Children\u2019s Hospital LA [11, 20] as a part of an IRB-approved study. The data consist of 10, 401 PICU episodes, each a multivariate time series of 13 variables including vital signs, lab results, and subjective assessments. The episodes vary in length from 12 hours to 30 days. Each episode has zero or more diagnostic labels from an in-house taxonomy, similar to ICD-9 codes, used for research and billing. There are 128 distinct labels indicating a variety of conditions, such as acute respiratory distress, congestive heart failure, seizures, renal failure, and sepsis.\nar X\niv :1\n51 0.\n07 64\n1v 1\n[ cs\n.L G\n] 2\n6 O\nct 2\nThe original data are irregularly sampled multivariate time series with missing values and occasionally missing variables. We resample all time series to an hourly rate (similar to [11]), taking the mean measurement within each one hour window and filling gaps by propagating measurements forward or backward. When time series are missing entirely, we impute a clinically normal value.1 We rescale variables to a [0, 1] interval using ranges defined by clinical experts."}, {"heading": "3 Methods and Experiments", "text": "We cast the problem of phenotyping clinical time series as multilabel classification, and our proposed LSTM RNN uses memory cells with forget gates as described in [21] but without peephole connections as described in [22]. As output, we use a fully connected layer atop the highest LSTM layer, with a sigmoid activation function because the problem is multilabel. Binary cross-entropy is the loss at each output node. Among architectures that we tested, the simplest and most effective passes over the data in chronological order, outputting predictions only at the final sequence step.\nWe train the network using stochastic gradient descent with momentum. Absent momentum, the variance of the gradient is large, and single examples occasionally destroy the model. Interestingly, the presence of exploding gradients had no apparent connection to the loss on the particular example that caused it. To combat exploding gradients, we experimented with `22 weight decay, gradient clipping, and truncated backpropagation. We test various settings for the number of layers and nodes, choosing the best using validation performance.\nWe evaluate the LSTM-based models against a variety of baselines. All models are trained on 80% of the data and tested on 10%. The remaining 10% is used for hyper-parameter optimization, e.g., regularization strength and early stopping. We report micro- and macro-averaged area under the ROC curve\n(AUC) and F1 score. We also report precision at 10, which captures the fraction of true diagnoses among the model\u2019s top 10 predictions, with a best possible score of 0.2818 on these data. We provide results for a base rate model that predicts diagnoses in descending order by incidence to provide a minimum performance baseline. Logistic regression with `22 regularization and carefully engineered features encoding domain knowledge formed a strong baseline. Nonetheless, an LSTM with two layers of 128 hidden units achieved the best overall performance on all metrics but precision at 10, while using only raw time series as input.\nOverall classification performance for 128 PICU phenotypes Model Micro AUC Macro AUC Micro F1 Macro F1 Precision at 10 Base rate 0.7170 0.5 0.1366 0.0339 0.0753 Linear, last 12 hours raw data 0.8041 0.7286 0.2263 0.1004 0.0986 Linear, engineered features 0.8277 0.7628 0.2498 0.1254 0.1085 LSTM, 2 layers, 128 nodes each 0.8324 0.7717 0.2577 0.1304 0.1078"}, {"heading": "4 Discussion", "text": "Our results indicate that LSTM RNNs can be successfully applied to the problem of phenotyping critical care patients given clinical time series data. Promising early experiments with gradient normalization suggest that we can improve our results further. Our next steps to advance this research include advanced optimization and regularization strategies, techniques to directly handle missing values and irregular sampling, and extending this work to a larger PICU data set with a richer set of measurements, including treatments and medications. Additionally, there remain many questions about the interpretability of neural networks when applied to complex medical problems. We are developing methods to expose the patterns of health and illness learned by LSTMs to clinical users and to make practical use of the distributed representations learned by LSTMs in applications like patient similarity search.\n1Many variables are recorded at rates proportional to how quickly they change, and when a variable is entirely absent, it is often because clinical staff believed it to be normal and chose not to measure it."}, {"heading": "Acknowledgments", "text": "Zachary C. Lipton was supported by the Division of Biomedical Informatics at the University of California, San Diego, via training grant (T15LM011271) from the NIH/NLM. David Kale was supported by the Alfred E. Mann Innovation in Engineering Doctoral Fellowship. The VPICU was supported by grants from the Laura P. and Leland K. Whittier Foundation.\nWe acknowledge NVIDIA Corporation for Tesla K40 GPU hardware donation and Professors Charles Elkan and Julian McAuley for their support."}], "references": [{"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks", "author": ["Marcus Liwicki", "Alex Graves", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. 9th Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles", "author": ["Gianluca Pollastri", "Darisz Przybylski", "Burkhard Rost", "Pierre Baldi"], "venue": "Proteins: Structure, Function, and Bioinformatics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Neural network model of gene expression", "author": ["Ji\u0159\u0131\u0301 Vohradsk\u00fd"], "venue": "The FASEB Journal,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Inference of genetic regulatory networks with recurrent neural network models using particle swarm optimization", "author": ["Rui Xu", "Donald Wunsch II", "Ronald Frank"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering models", "author": ["Ben M. Marlin", "David C. Kale", "Robinder G. Khemani", "Randall C. Wetzel"], "venue": "IHI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Murray and Nadel\u2019s textbook of respiratory medicine: 2-volume", "author": ["Robert J. Mason", "V. Courtney Broaddus", "Thomas Martin", "Talmadge E. King Jr.", "Dean Schraufnagel", "John F. Murray", "Jay A. Nadel"], "venue": "Health Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Artificial neural networks for automatic ecg analysis", "author": ["Rosaria Silipo", "Carlo Marchesi"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Adaptive blind signal processing-neural network approaches", "author": ["Shun-ichi Amari", "Andrzej Cichocki"], "venue": "Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Combining recurrent neural networks with eigenvector methods for classification of ecg beats", "author": ["Elif Derya \u00dcbeyli"], "venue": "Digital Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A solution for missing data in recurrent neural networks with an application to blood glucose prediction", "author": ["Volker Tresp", "Thomas Briegel"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "A neural network based model for predicting psychological conditions", "author": ["Filip Dabek", "Jesus J. Caban"], "venue": "In Brain Informatics and Health,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Use of an artificial neural network to predict head injury", "author": ["Anand I. Rughani", "Travis M. Dumont", "Zhenyu Lu", "Josh Bongard", "Michael A. Horgan", "Paul L. Penar", "Bruce I Tranmer"], "venue": "outcome: clinical article. Journal of neurosurgery,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data", "author": ["Thomas A. Lasko", "Joshua C. Denny", "Mia A. Levy"], "venue": "PLoS ONE, 8(6):e66341,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Deep computational phenotyping", "author": ["Zhengping Che", "David C. Kale", "Wenzhe Li", "Mohammad Taha Bahadori", "Yan Liu"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Felix A. Gers", "Nicol N. Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Zachary C. Lipton", "John Berkowitz", "Charles Elkan"], "venue": "arXiv preprint arXiv:1506.00019,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 2, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 3, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 4, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 5, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 6, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 7, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 9, "context": "Recurrent neural networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) [1], powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 297, "endOffset": 325}, {"referenceID": 10, "context": "Clinical time series data, as recorded in the pediatric intensive care unit (PICU), exhibit these properties and others, including irregular sampling and non-random missing values [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "Symptoms of acute respiratory distress syndrome, for example, often do not appear for 2448 hours after lung injury [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "Prior works have applied RNNs to health data, including electrocardiograms [13, 14, 15] and glucose measurements [16].", "startOffset": 75, "endOffset": 87}, {"referenceID": 13, "context": "Prior works have applied RNNs to health data, including electrocardiograms [13, 14, 15] and glucose measurements [16].", "startOffset": 75, "endOffset": 87}, {"referenceID": 14, "context": "Prior works have applied RNNs to health data, including electrocardiograms [13, 14, 15] and glucose measurements [16].", "startOffset": 75, "endOffset": 87}, {"referenceID": 15, "context": "Prior works have applied RNNs to health data, including electrocardiograms [13, 14, 15] and glucose measurements [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "RNNs have also been used for prediction problems in genomics [8, 10, 9].", "startOffset": 61, "endOffset": 71}, {"referenceID": 9, "context": "RNNs have also been used for prediction problems in genomics [8, 10, 9].", "startOffset": 61, "endOffset": 71}, {"referenceID": 8, "context": "RNNs have also been used for prediction problems in genomics [8, 10, 9].", "startOffset": 61, "endOffset": 71}, {"referenceID": 16, "context": "A variety of works have applied feed-forward neural networks to health data for prediction and pattern mining, but none have used RNNs or directly handled variable length sequences [17, 18, 19, 20].", "startOffset": 181, "endOffset": 197}, {"referenceID": 17, "context": "A variety of works have applied feed-forward neural networks to health data for prediction and pattern mining, but none have used RNNs or directly handled variable length sequences [17, 18, 19, 20].", "startOffset": 181, "endOffset": 197}, {"referenceID": 18, "context": "A variety of works have applied feed-forward neural networks to health data for prediction and pattern mining, but none have used RNNs or directly handled variable length sequences [17, 18, 19, 20].", "startOffset": 181, "endOffset": 197}, {"referenceID": 19, "context": "A variety of works have applied feed-forward neural networks to health data for prediction and pattern mining, but none have used RNNs or directly handled variable length sequences [17, 18, 19, 20].", "startOffset": 181, "endOffset": 197}, {"referenceID": 10, "context": "Our experiments use a collection of fully anonymized clinical time series extracted from the electronic health records system at Children\u2019s Hospital LA [11, 20] as a part of an IRB-approved study.", "startOffset": 152, "endOffset": 160}, {"referenceID": 19, "context": "Our experiments use a collection of fully anonymized clinical time series extracted from the electronic health records system at Children\u2019s Hospital LA [11, 20] as a part of an IRB-approved study.", "startOffset": 152, "endOffset": 160}, {"referenceID": 10, "context": "We resample all time series to an hourly rate (similar to [11]), taking the mean measurement within each one hour window and filling gaps by propagating measurements forward or backward.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "1 We rescale variables to a [0, 1] interval using ranges defined by clinical experts.", "startOffset": 28, "endOffset": 34}, {"referenceID": 20, "context": "We cast the problem of phenotyping clinical time series as multilabel classification, and our proposed LSTM RNN uses memory cells with forget gates as described in [21] but without peephole connections as described in [22].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "We cast the problem of phenotyping clinical time series as multilabel classification, and our proposed LSTM RNN uses memory cells with forget gates as described in [21] but without peephole connections as described in [22].", "startOffset": 218, "endOffset": 222}, {"referenceID": 22, "context": "Figure 1: LSTM memory cell with forget gate as depicted in [23] We train the network using stochastic gradient descent with momentum.", "startOffset": 59, "endOffset": 63}], "year": 2017, "abstractText": "We present a novel application of LSTM recurrent neural networks to multilabel classification of diagnoses given variable-length time series of clinical measurements. Our method outperforms a strong baseline on a variety of metrics.", "creator": "LaTeX with hyperref package"}}}