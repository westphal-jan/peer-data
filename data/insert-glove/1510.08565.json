{"id": "1510.08565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "Attention with Intention for a Neural Network Conversation Model", "abstract": "In semblog a conversation or a exclamatory dialogue rothemund process, elfish attention and intention play salesmanship intrinsic roles. anti-fungal This paper 9:20 proposes a dykeman neural network aguiluz based svartholm approach that linseed models jltv the palmquist attention seoud and masuku intention processes. It orthorhombic essentially consists of berm\u00fadez three mischel recurrent 1.3645 networks. bhudevi The maroleng encoder bud network is a 54-minute word - level model representing source side 28.50 sentences. filmworks The unloosed intention beig network ghazni is byd a recurrent tearoom network lysander that gotthelf models the dynamics of guldur the intention process. shelley The decoder network gioia is a recurrent azofeifa network produces gebauer responses 6:36 to revokes the input ventotene from frumentius the source visclosky side. It is a nord-ouest language tassin model that is dependent on neo-platonic the intention dennison and verin has an radisich attention mechanism to passow attend marsac to 63.65 particular salamanca source side words, 2003 when taeb predicting 95.11 a trapezes symbol guichet in kinga the minervois response. burdeau The model is dsto trained amass end - to - 706 end suburbanite without labeling data. Experiments kaun show theoretically that forty-niners this cauayan model kinleith generates natural responses krasnopol to geschichte user inputs.", "histories": [["v1", "Thu, 29 Oct 2015 05:31:28 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v1", null], ["v2", "Mon, 2 Nov 2015 02:17:15 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v2", null], ["v3", "Thu, 5 Nov 2015 07:26:01 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.HC cs.LG", "authors": ["kaisheng yao", "geoffrey zweig", "baolin peng"], "accepted": false, "id": "1510.08565"}, "pdf": {"name": "1510.08565.pdf", "metadata": {"source": "CRF", "title": "Attention with Intention for a Neural Network Conversation Model", "authors": ["Kaisheng Yao", "Geoffrey Zweig", "Baolin Peng"], "emails": ["kaisheny@microsoft.com", "gzweig@microsoft.com", "blpeng@se.cuhk.edu.hk"], "sections": [{"heading": "1 Introduction", "text": "A conversation process is a process of communication of thoughts through words. It may be considered as a structural process that stresses the role of purpose and processing in discourse [7]. Essentially, the discourse structure is intimately connected with two nonlinguistic notions: intention and attention. In processing an utterance, attention explicates the processing of utterances, for example, paying attention to particular words in a sentence. On the other hand, intention is higher level than attention and has its primary role of explaining discourse structure and coherence. Clearly, a conversation process is inherently complicated because of the two levels of structures.\nA conversation process may be cast as a sequence-to-sequence mapping task. In this task, the source side of the conversation is from one person and the target side of the conversation is from another person. The sequence-to-sequence mapping task includes machine translation, graphemeto-phoneme conversation, named entity tagging, etc. However, an apparent difference of a dialogue process from these tasks is that a dialogue process involves multiple turns, whereas usually the above tasks involve only one turn of mapping a source sequence to its target sequence.\nNeural network based approaches have been successfully applied in sequence-to-sequence mapping tasks. They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4]. Among those neural network-based approaches, one particular approach, which is called encoder-decoder framework [1, 13], aims at relaxing much requirement on human labeling.\nConversation models have been typically designed to be domain specific with much knowledge such as rules [3,18]. Recent methods [15] relax such requirement to some extent but their whole systems\n\u2217Presented at NIPS Workshop on Machine Learning for Spoken Language Understanding and Interaction 2015.\nar X\niv :1\n51 0.\n08 56\n5v 1\n[ cs\n.N E\n] 2\n9 O\nare still trained with manual labels because of their sub-components that require so. Manual labels are error prone and expensive. Therefore, it is appealing to train a system end-to-end without manual labels. Recent works in [10, 12, 14] are in this approach.\nIn general, however, using knowledge is helpful. For example, the alignment information between the source and target side is critical in grapheme-to-phoneme conversation [17] to outperform a strong baseline using n-gram models [2]. In a neural network based machine translation system [6], the alignment information is used to outperform a strong phrase-based baseline [5].\nIn the context of modeling conversation process, a neural network model may be built with the knowledge of the structural information of conversation processes. In particular, the network may incorporate the notion of intention and attention. To test this, we developed a model that consists of three recurrent neural networks (RNNs). The source side RNN, or encoder network, encodes the source side inputs. The target side RNN, or decoder network, uses an attention mechanism to attend to particular words in the source side, when predicting a symbol in its response to the source side. Importantly, this attention in the target side is conditioned on the output from an intention RNN. This model, which has the structural knowledge of the conversation process, is trained end-to-end without labels. We experimented with this model and observed that it generates natural responses to user inputs."}, {"heading": "2 Background", "text": "In the theory of discourse in [7], discourse structure is composed of three separate but related components. The first is the linguistic structure, which is the structure of the sequence of utterance. The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate. The second structure is the intentional structure, which captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The third is the attentional state that is dynamic, and records the objects, properties, and relations that are salient at each point of the discourse.\nIn many examples we observe, there are usually just one linguistic segment that consists of all the utterances. Therefore, in the following, we consider a discourse with two structures: intention and attention.\nIn the example in Table 1, there is a clear flow of intentions. The user states the problem, with the user\u2019s intention of conveying the problem to the agent. The agent receives the words, processes them, and communicates back to the user. The user responds to the agent afterwards. Therefore, the whole conversation process consists of three intentions processed sequentially. The first is the intention of communication of the problem. The second intention is the process of resolving the issue. The third is the intention of acknowledgment. In processing each of the intentions, the user and the agent pay attention to particular words. For example, when resolving the issue, the agent pays attention to words such as \u201dvirus\u201d."}, {"heading": "3 The model", "text": ""}, {"heading": "3.1 The attention with intention (AWI) model", "text": "We propose a model that attempts to represent the structural process of intentions and the associated attentions. Figure 1 illustrates the model. It shows three layers of processing: encoder network, intention network, and decoder network.\nThe encoder network has inputs from the current source side input. Because the source side in the current turn is also dependent on the previous turn, the source side encoder network is linked with the output from the previous target side. The encoder network creates a representation of the source side in the current turn.\nThe intention network is dependent on its past state, so that it memories the history of intentions. It therefore is a recurrent network, taking a representation of the source side in the current turn and updating its hidden state.\nThe decoder is a recurrent network for language modeling that outputs symbol at each time. This output is dependent on the current intention from the intention network. It also pays attention to particular words in the source side.\nIn more details, a conversation has in totoal U turns. At turn u, a user in the source side, denoted in superscript (s), has an input sequence of ~x(s,u) = (x(s,u)t : t = 1, \u00b7 \u00b7 \u00b7 , T ) with length T . An agent in the target side, denoted in superscript (t), responds to the user with ~y(t,u) = (y(t,u)j : j = 1, \u00b7 \u00b7 \u00b7 J) with length J . The proposed model is a conditional model of the target given the source, p(~y(t,u)|~x(s,u)). If there is no confusion, we may omit the session index u in the following."}, {"heading": "3.2 Encoder network", "text": "The encoder network reads the input sentence ~x(s), and converts them into a fixed-length or a variable length representation of the source side sequence. There are many choices to encode the source side. The approach we use is an RNN such that\nh (s) t = f ( x (s) t , h (s) t-1 ) (1)\nwhere f(\u00b7) is an RNN. h(s)t is the hidden state at time t in the source side. The initial state h (s) t with t = 0 is a learned parameter vector.\nOne form of the output from this encoder is the last hidden state activity c(s)T = h (s) T . This is used as a representation of the source side in the current turn to the intention network. The other form is a variable-length representation, to be used in the attention model described in Sec. 3.4. A general description of the variable length representation is as follows\nc (s) t = q({h (s) t ,\u2200t = 0, \u00b7 \u00b7 \u00b7T}) (2)\nwhere q(\u00b7) might be a linear network or a nonlinear network."}, {"heading": "3.3 Intention network", "text": "The signal from the encoder network is fed into an intention network to model the intention process. Following [7], the intention process is a dynamic process to model the intrinsic dynamics of conversation, in which an intention in one turn is dependent on the intention in the previous turn. This property might be modeled using a Markov model, but we choose an RNN.\nInterestingly, the hidden state of an RNN in a certain turn may be considered as a distributed representation of the intention. Different from the usual process of training distributed representation of words [9], the distribution representation of intentions are trained with previous turns as their context. We use a first order RNN model, in which a hidden state is dependent explicitly on its previous state.\nThe intention model in AWI is therefore an RNN as follows h(i,k) = f ( c (s) T , h (i,k-1) )\n(3)\nwhere c(s)T is the fixed dimension representation of the source side described in Sec. 3.2. k is the index of the current turn."}, {"heading": "3.4 Decoder network", "text": "The last step is to decode the sequence in the target side, which is framed as a language model over each symbol, generated left to right. In this framework, the decoder computes conditional probability as\np(y (t) j |y (t) 1 , \u00b7 \u00b7 \u00b7 , y (t) j-1 , ~x (s)) = g(y (t) j-1 , h (t) j , c (t) j ) (4)\nwhere the hidden state in the decoder is computed using an RNN\nh (t) j = f ( y (t) j-1 , h (t) j-1 , c (t) j ) (5)\nThe c(t)j is a vector to represent the context to generate y (t) j . It is dependent on the source side as\nc (t) j = z ( h (t) j-1 , {c (s) t : \u2200t = {1, \u00b7 \u00b7 \u00b7 , T}} ) (6)\nwhere z(\u00b7) summerizes the variable-length source side representations {c(s)t } using weighted average. The weight is computed using a content-based alignment model [1] that produces high scores if the target side hidden state in previous time h(t)j-1 and c (s) t are similar. More formally, the weight \u03b1jt for the context c (s) t is computed using\n\u03b1jt = exp ejt\u2211\nm exp(ejm) (7)\nwhere ejt = a(h (t) j-1 , c (s) t ) (8)\nThe alignment model enables an attention to particular words, represented as a vector c(s)t in the source side. Since the decoder network generates responses on condition of the attention and also the intention, our model is called attention with intention (AWI) model."}, {"heading": "3.5 Implementation details", "text": "All of the recurrent networks are implemented using a recently proposed depth-gated long-shortterm memory (LSTM) network [16]. The context vector c(s)t is an embedding vector of the source side word at time t.\nThe alignment model in Eq. (8) follows the attention model in [1], in which ejt is calculated as\nejt = ~v > tanh ( W(ah)h (t) j-1 +W (ae)c (s) t ) , (9)\nwhich is a neural network with one hidden layer of size A and a single output, parameterised by W(ae) \u2208 RA\u00d7H , W(ah) \u2208 RA\u00d7H and ~v \u2208 RA. H and A are the hidden layer dimension and alignment dimension."}, {"heading": "4 Evaluation", "text": "We used an in-house dialogue dataset. The dataset consists of dialogues from a helpdesk chat service. In this service, costumers seeks helps on computer related issues from human agents. Training consists of 10000 dialogues with 96913 turns or conversations. Number of tokens is 2215047 in the source side and 2378950 in the target side. The vocabulary size is 9085 including words from both side. Development set data has 1000 dialogues with 9971 turns. Test set data has 500 dialogues with 5232 turns.\nWe use sentence-level SGD without momentum. Learning rate is initialized to 0.1. Development set is used to control the learning rate. The learning rate is halved when perplexity on the development is increased. One epoch of training has one pass of the training data. The order of training dialogues is randomly shuffled in the beginning of each epoch. The order of turns in the a dialogue is however kept."}, {"heading": "4.1 Performances measure in perplexity", "text": "An objective comparison of different models for conversation is still an open question. We report perplexity (PPL), though it may have drawbacks, to compare different models. Table 2 presents results in perplexity with two models with different hiden layer sizes. Results show that a larger model with 200 hidden layer dimension has lower PPL than the model with 50 dimension."}, {"heading": "4.2 Examples of outputs from the trained model", "text": "Table 3 lists an example of the conversation process between a human and the trained model. The model has two layers of LSTMs and other setups are the same as used in Sec 4.1. Similarly as observed in [11], the model produces natural responses to user inputs. The flow of intentions is clearly seen in this example."}, {"heading": "5 Related work", "text": "Our work is related to the recent work in [10, 12, 14], which uses an encoder-decoder framework to model conversation. The work in [10] is a model for single turn conversation. The work in [14] is a simple encoder-decoder method using a fixed-dimension representation of the source side. The work in [12] also uses a fixed-dimension representaiton of the source side but has an additional RNN to model dialogue context. This additional RNN is similar to the intention RNN in AWI model.\nHowever, AWI model differs from [12] in that it incorprates the concept of attention and intention based on the theory in [7]. Therefore, attention mechanism is essential to AWI. The model in [12] doesn\u2019t have an attention model.\nBecause it is not yet clear what objective measure to use to compare different models, it is hard to make claims of superiority of these models. We believe AWI model is an alternative to the models in [12, 14]."}, {"heading": "6 Conclusions and discussions", "text": "We have presented a model that incorporates attention and intention processes in a neural network model. Preliminary experiments show that this model generates natural responses to user inputs. Future works include experiments on common dataset to compare different models and incorporating objective functions such as goals."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Joint-sequence models for grapheme-to-phoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech Communication,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "The ravenclaw dialog management framework: architecture and systems", "author": ["D. Bohus", "A.I. Rudnicky"], "venue": "Computer, Speech and Language,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Listen, attend and spell", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "[cs.CL],", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "In ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Attention, intentions, and the structure of discourse", "author": ["B.J. Grosz", "C.L. Sidner"], "venue": "Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Efcient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "In ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J.G. Simonsen", "J.-Y. Nie"], "venue": "[cd.NE],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversation responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "In NAACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A nerual converstion model", "author": ["O. Vinyals", "Q.V. Le"], "venue": "In ICML Deep Learning Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convoulutional sentence reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "Technical report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Depth-gated LSTM", "author": ["K. Yao", "T. Cohn", "E. Vylomova", "K. Duh", "C. Dyer"], "venue": "[cs.NE],", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["K. Yao", "G. Zweig"], "venue": "In INTERSPEECH,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J.D. Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "It may be considered as a structural process that stresses the role of purpose and processing in discourse [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 61, "endOffset": 69}, {"referenceID": 5, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 61, "endOffset": 69}, {"referenceID": 12, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 61, "endOffset": 69}, {"referenceID": 7, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "Among those neural network-based approaches, one particular approach, which is called encoder-decoder framework [1, 13], aims at relaxing much requirement on human labeling.", "startOffset": 112, "endOffset": 119}, {"referenceID": 12, "context": "Among those neural network-based approaches, one particular approach, which is called encoder-decoder framework [1, 13], aims at relaxing much requirement on human labeling.", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Conversation models have been typically designed to be domain specific with much knowledge such as rules [3,18].", "startOffset": 105, "endOffset": 111}, {"referenceID": 17, "context": "Conversation models have been typically designed to be domain specific with much knowledge such as rules [3,18].", "startOffset": 105, "endOffset": 111}, {"referenceID": 14, "context": "Recent methods [15] relax such requirement to some extent but their whole systems \u2217Presented at NIPS Workshop on Machine Learning for Spoken Language Understanding and Interaction 2015.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "Recent works in [10, 12, 14] are in this approach.", "startOffset": 16, "endOffset": 28}, {"referenceID": 11, "context": "Recent works in [10, 12, 14] are in this approach.", "startOffset": 16, "endOffset": 28}, {"referenceID": 13, "context": "Recent works in [10, 12, 14] are in this approach.", "startOffset": 16, "endOffset": 28}, {"referenceID": 16, "context": "For example, the alignment information between the source and target side is critical in grapheme-to-phoneme conversation [17] to outperform a strong baseline using n-gram models [2].", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "For example, the alignment information between the source and target side is critical in grapheme-to-phoneme conversation [17] to outperform a strong baseline using n-gram models [2].", "startOffset": 179, "endOffset": 182}, {"referenceID": 5, "context": "In a neural network based machine translation system [6], the alignment information is used to outperform a strong phrase-based baseline [5].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "In a neural network based machine translation system [6], the alignment information is used to outperform a strong phrase-based baseline [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "In the theory of discourse in [7], discourse structure is composed of three separate but related components.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Following [7], the intention process is a dynamic process to model the intrinsic dynamics of conversation, in which an intention in one turn is dependent on the intention in the previous turn.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Different from the usual process of training distributed representation of words [9], the distribution representation of intentions are trained with previous turns as their context.", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "The weight is computed using a content-based alignment model [1] that produces high scores if the target side hidden state in previous time h j-1 and c (s) t are similar.", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "All of the recurrent networks are implemented using a recently proposed depth-gated long-shortterm memory (LSTM) network [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "(8) follows the attention model in [1], in which ejt is calculated as ejt = ~v > tanh ( W(ah)h (t) j-1 +W (ae)c (s) t ) , (9)", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "Similarly as observed in [11], the model produces natural responses to user inputs.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "Our work is related to the recent work in [10, 12, 14], which uses an encoder-decoder framework to model conversation.", "startOffset": 42, "endOffset": 54}, {"referenceID": 11, "context": "Our work is related to the recent work in [10, 12, 14], which uses an encoder-decoder framework to model conversation.", "startOffset": 42, "endOffset": 54}, {"referenceID": 13, "context": "Our work is related to the recent work in [10, 12, 14], which uses an encoder-decoder framework to model conversation.", "startOffset": 42, "endOffset": 54}, {"referenceID": 9, "context": "The work in [10] is a model for single turn conversation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "The work in [14] is a simple encoder-decoder method using a fixed-dimension representation of the source side.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "The work in [12] also uses a fixed-dimension representaiton of the source side but has an additional RNN to model dialogue context.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "However, AWI model differs from [12] in that it incorprates the concept of attention and intention based on the theory in [7].", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "However, AWI model differs from [12] in that it incorprates the concept of attention and intention based on the theory in [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 11, "context": "The model in [12] doesn\u2019t have an attention model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "We believe AWI model is an alternative to the models in [12, 14].", "startOffset": 56, "endOffset": 64}, {"referenceID": 13, "context": "We believe AWI model is an alternative to the models in [12, 14].", "startOffset": 56, "endOffset": 64}], "year": 2017, "abstractText": "In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.", "creator": "LaTeX with hyperref package"}}}