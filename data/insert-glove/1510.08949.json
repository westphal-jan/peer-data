{"id": "1510.08949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Testing Visual Attention in Dynamic Environments", "abstract": "roeser We 59.47 investigate instrumentation attention dwyfor as the active voa pursuit kuehnert of krainik useful cornishmen information. sumin This mondrag\u00f3n contrasts with yes attention as shahada a bhairavi mechanism haverford for the anole attenuation of polone irrelevant delightedly information. mongers We l-r also consider shrimper the interventricular role engerman of 11/24 short - term wogaman memory, whose poos use is critical to any ruetten model incapable of teabag simultaneously lujie perceiving all astur information on which cou its samhadana output benedictines depends. schieffelin We wolbers present several simple synthetic 1978-1983 tasks, aleati which become feyzullah considerably signes more boulmerka interesting when we impose celerity strong polynice constraints 81.62 on how a model can interact with its supergroup input, and on how flowerbed long it can take eng. to tamboura produce sedlak its output. n\u00e1rodn\u00ed We commericial develop a bkl model with a maren different sheathed structure from those capsa seen mateu in previous sooty work, botany and we quincea\u00f1era train segolene it 46.41 using stochastic variational bakare inference with a hopkins learned proposal distribution.", "histories": [["v1", "Fri, 30 Oct 2015 01:39:54 GMT  (1320kb,D)", "http://arxiv.org/abs/1510.08949v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["philip bachman", "david krueger", "doina precup"], "accepted": false, "id": "1510.08949"}, "pdf": {"name": "1510.08949.pdf", "metadata": {"source": "CRF", "title": "Testing Visual Attention in Dynamic Environments", "authors": ["Philip Bachman", "David Krueger", "Doina Precup"], "emails": ["phil.bachman@gmail.com", "dkrueger@email.com", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "One can interpret attention, viewed as a behavioural phenomenon, as a necessary adaptation to intrinsic constraints on perception. For example, if an agent exists in an environment which makes 100 bits of information available per clock tick, but the agent is only capable of observing 10 bits per clock tick, then the agent must be careful about how it directs its perceptual capacity around the environment while capturing what bits it can. The attentiveness of the agent arises not from avoiding noise, but from pursuing signal.\nWe present several tasks designed to test the capabilities of models which combine visual attention mechanisms and sequential decision making. In spite of their simple structure, these tasks become challenging when we impose strong constraints on how a model can interact with its input, and on how many steps it can take to produce its output. The inputs and outputs in these tasks are either time-varying sequences or multiple presentations of a fixed value. The model constructs its output over a sequence of steps, and at each step it can only perform a single reading of its current input through a moveable, low-resolution sensor. To succeed at these tasks a model must use short-term memory for aggregating information across multiple sensor readings, to effectively construct its output and guide future use of its sensor. These tasks extend previous work, e.g. [1, 2, 7], by considering time-varying inputs/outputs, and by putting \u201ctime-pressure\u201d on output construction (when working with a fixed input).\nWe develop a model suited to these tasks and train it using stochastic variational inference with a learned proposal distribution.1 We empirically show that, given its limited perceptual capacity, our model can perform surprisingly well. The tasks are sufficiently difficult to leave clear room for improvement, particularly in terms of how many times the model must attempt a task before learning a successful policy.\n1One could also think of this training method as Guided Policy Search [6] \u2013 see [3] for more on this view.\nar X\niv :1\n51 0.\n08 94\n9v 1\n[ cs\n.L G\n] 3\n0 O\nct 2"}, {"heading": "2 Task and Model Descriptions", "text": ""}, {"heading": "2.1 Our Tasks", "text": "We define tasks based on both static and sequential inputs. In the static input task, which we call \u201churried copying\u201d, the model reconstructs an input using a sequence of readings received from its sensor. At each of several steps, the model decides where to apply its sensor, and how to update its reconstruction of the input. The model thus performs a sort of non-linear adaptive compressive sensing. To make the task more challenging, we limit the number, T , and dimension, k, of the readings so that the product kT is significantly smaller than the input dimension. We encourage the model to continuously refine its prediction using a cost which simulates evaluating reconstruction at a termination time determined by a poisson random variable.\nWe also introduce a family of sequential observation and prediction tasks, all of which involve tracking and copying objects from an input video. In these tasks, the model attempts to reconstruct the trajectory of a designated object (or objects) in the presence of noise and/or distractor objects. The model is restricted to observing the inputs through a sequence of low-resolution sensor readings. In our tests we take one reading per video frame. Given a target subset of objects in the input video, the model must reconstruct the input video with all noise and non-target objects removed. Thus, the model must locate and track the target objects while operating under strong constraints on perceptual capacity. Examples are given in Fig. 3.\nTo generate object paths for our videos, we sample a random (norm-bounded) initial velocity and, at each time-step, we resample a new velocity with probability .05. Objects bounce off the image boundary. Object trajectories in our videos are non-deterministic and thus require ongoing prediction and observation for accurate reconstruction. We add uniform noise in [0, 1] to each pixel independently with probability .05. Finally, we clip all pixel values to remain within the [0, 1] interval."}, {"heading": "2.2 Our Model", "text": "To solve tasks that require interleaving a sequence of perception actions (i.e. placing and reading from an attention mechanism) with a sequence of updates to a \u201cbelief state\u201d (which provides the output/prediction at each step), we develop a model built around a pair of LSTMs. We call these LSTMs the \u201ccontroller\u201d and the \u201cobserver\u201d. At step t, the observer receives input rt from an attention module which reads from the current\ninput xt. The read op uses location, scale, precision, etc. specified by latent variables zct sampled from a distribution conditioned on the state sct\u22121 of the controller in the previous step. After the observer updates its own state, its updated state sot determines the distribution of another set of latent variables. A sample z o t of these latent variables provides an input to the controller, which updates its own state to get sct . The current belief state y\u0302t is a function of the previous belief state y\u0302t\u22121 and the current controller state sct . We illustrate this model in Fig. 1.\nFor the read op, we use a moveable 2x2 grid of differentiable Gaussian filters, as in the DRAW model from [4]. We repeat this grid at 1x and 2x scales, for rudimentary foveation2. At each step we specify the location and scale of the grid, as well as separate \u201creading strengths\u201d (non-negative multipliers) for the 1x and 2x scales. We compute the belief state y\u0302t directly as a deterministic function of the controller state sct . All functions and distributions in our model depend on trainable parameters. For complete descriptions of our model, our training method, and our train/test data generation, see Github.\nTo train our model, we add a \u201cguide module\u201d which mimics the role of the observer, but receives an additional input produced by applying the read op at time t to the reconstruction residual yt \u2212 y\u0302t, where yt indicates the target output at time t. The guide module can be interpreted as providing a variational posterior approximation in a directed graphical model, or as the source of guide trajectories in an application of Guided Policy Search. See [3] for more discussion of this view.\n3 Results and Discussion\nWe present results on \u201churried copying\u201d with TFD [8] and MNIST [5], and on our detection/tracking tasks with synthetic video data. On image/video tasks, the model pays no reconstruction cost on the first 2/5 frames, respectively. In a sense, this splits each trial into a \u201cwarm-up\u201d phase and a \u201ctest\u201d phase. Hurried copy results are in Fig. 2, and sequential tracking results are in Fig. 3.\nOur model reliably learned to track a single object despite the presence of background noise, distractor objects, and random velocity resets. It produced high-fidelity reconstructions of the target object, despite its limited bandwidth sensor. On the more challenging two-object tasks, the model often appeared to follow the objects\u2019 mean location while changing the sensor scale and bandwidth to capture both objects in its attention at each step.3 While it was able to reconstruct the objects in the correct locations, the reconstructions were not sharp. In general, the cross seemed to be more difficult for the model to reconstruct than the circle. The model also seemed to have\nmore difficulty producing high fidelity reconstructions when tracking objects with different shapes.\nOn the static image tasks, we observed that the model\u2019s first evaluated reconstruction (on the 3rd timestep) is already quite good, despite having only read 12 floating point values at that time. It continues to refine its\n2we omit the 2x scale on our MNIST/TFD experiments. 3We also observed an interesting mode of behavior in two-object tracking without distractors where the attention mechanism learned to alternate between tracking the two objects in successive time-steps, using distinct foci for each object. The model produced this behavior consistently across different input sequences. Due to time constraints, we have been unable to consistently learn this behavior in two-object tracking with distractors.\nreconstruction noticeably on MNIST, but these refinements are relatively minor touch-ups. On both datasets, it learned an input-independent attention trajectory. We are investigating the causes of this homogeneity."}, {"heading": "3.1 Discussion", "text": "We presented a set of tasks which provide a test-bed for models combining visual attention with sequential prediction. Though simple in form, our detection and tracking tasks demand sophisticated behavior for consistent success. E.g., to track multiple objects in the presence of distractors, attention must be divided among the target objects while using short-term memory and knowledge of environmental dynamics to estimate the location of objects not currently attended to. We presented a suitable model and some preliminary empirical results showing that our tasks are within reach of current methods, but with plenty of room to grow."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Jimmy Ba", "Roger Grosse", "Ruslan Salakhutdinov", "Brendan Frey"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodomyr Mnih", "Koray Kavucuoglu"], "venue": "In ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Data generation as sequential decision making", "author": ["Philip Bachman", "Doina Precup"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Guided policy search", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodomyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavucuoglu"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "The toronto face database", "author": ["Joshua Susskind", "Adam Anderson", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2, 7], by considering time-varying inputs/outputs, and by putting \u201ctime-pressure\u201d on output construction (when working with a fixed input).", "startOffset": 0, "endOffset": 9}, {"referenceID": 1, "context": "[1, 2, 7], by considering time-varying inputs/outputs, and by putting \u201ctime-pressure\u201d on output construction (when working with a fixed input).", "startOffset": 0, "endOffset": 9}, {"referenceID": 6, "context": "[1, 2, 7], by considering time-varying inputs/outputs, and by putting \u201ctime-pressure\u201d on output construction (when working with a fixed input).", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "One could also think of this training method as Guided Policy Search [6] \u2013 see [3] for more on this view.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "One could also think of this training method as Guided Policy Search [6] \u2013 see [3] for more on this view.", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "We add uniform noise in [0, 1] to each pixel independently with probability .", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "Finally, we clip all pixel values to remain within the [0, 1] interval.", "startOffset": 55, "endOffset": 61}, {"referenceID": 3, "context": "For the read op, we use a moveable 2x2 grid of differentiable Gaussian filters, as in the DRAW model from [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "See [3] for more discussion of this view.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "We present results on \u201churried copying\u201d with TFD [8] and MNIST [5], and on our detection/tracking tasks with synthetic video data.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "We present results on \u201churried copying\u201d with TFD [8] and MNIST [5], and on our detection/tracking tasks with synthetic video data.", "startOffset": 63, "endOffset": 66}], "year": 2015, "abstractText": "We investigate attention as the active pursuit of useful information. This contrasts with attention as a mechanism for the attenuation of irrelevant information. We also consider the role of short-term memory, whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends. We present several simple synthetic tasks, which become considerably more interesting when we impose strong constraints on how a model can interact with its input, and on how long it can take to produce its output. We develop a model with a different structure from those seen in previous work, and we train it using stochastic variational inference with a learned proposal distribution.", "creator": "LaTeX with hyperref package"}}}