{"id": "1511.01042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2015", "title": "Detecting Interrogative Utterances with Recurrent Neural Networks", "abstract": "In this nonbeliever paper, kimberlee we therdsak explore different tourcoing neural ralegh network toader architectures birjand that can predict fregosi if nalbandian a speaker rapetti of relevancy a nymphe given utterance shahak is 18q asking a okeafor question or making gbe a statement. We malama com - pare the outcomes of 01:20 regularization inquest methods brumskine that socred are latrice popularly 1,826 used fotheringay to dearness train erazo deep neural watley networks furtado and 29-14 study kapuscinski how different context functions can tuvans affect the looker classification performance. mid-80s We also compare the efficacy colding of gated activation tamis functions parakramabahu that are obadele favorably used 483 in dipped recurrent neural networks and othmer study how to tocom combine ss-4 multimodal inputs. bushed We indeterminable evaluate linguae our bonaire models on two multimodal xfl datasets: rohack MSR - 2,000-kilometer Skype and CALLHOME.", "histories": [["v1", "Tue, 3 Nov 2015 19:26:16 GMT  (91kb,D)", "http://arxiv.org/abs/1511.01042v1", "6 pages, accepted to NIPS 2015 Workshop on Machine Learning for Spoken Language Understanding and Interaction"], ["v2", "Mon, 16 Nov 2015 03:54:19 GMT  (91kb,D)", "http://arxiv.org/abs/1511.01042v2", "6 pages, accepted to NIPS 2015 Workshop on Machine Learning for Spoken Language Understanding and Interaction"]], "COMMENTS": "6 pages, accepted to NIPS 2015 Workshop on Machine Learning for Spoken Language Understanding and Interaction", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["junyoung chung", "jacob devlin", "hany hassan awadalla"], "accepted": false, "id": "1511.01042"}, "pdf": {"name": "1511.01042.pdf", "metadata": {"source": "CRF", "title": "Detecting Interrogative Utterances with Recurrent Neural Networks", "authors": ["Junyoung Chung", "Jacob Devlin", "Hany Hassan Awadalla"], "emails": ["junyoung.chung@umontreal.ca", "hanyh}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Spoken language understanding is a long-term goal of machine learning and potentially has a huge impact in practical applications. However, the difficulty of processing speech signals itself is a bottleneck, for instance, the core part of speech translation has to be processed in the text domain. In other words, a failure of capturing the key features in the speech signals can lead the next applications into unexpected results.\nIdentifying whether a given utterance is a question or not can be one of the key features in applications such as speech translation. Unfortunately, a speech recognition system is likely to fail achieving two goals at a same time: (1) extract text sequences from the input utterances, (2) detect questions. We can think of a question detection system that works independently and unburdens the load of the speech recognition system [15, 10, 4, 14, 3]. Later, an annotation of being a question can form a set with the output of the speech recognition system and handed over to the machine translation system.\nPrevious studies have focused on using hand-designed features and classifiers such as support vector machines (SVMs) [10, 14] or tree-based classifiers [15, 4, 3]. The classifiers used in these systems are shallow and simple, but there are considerable efforts on designing features based on domain knowledge. However, there is no guarantee that these hand-designed features are optimal to solve the problem.\nIn this study, we will let the model to learn the features from the training examples and the objective function. We propose a recurrent neural network (RNN) based system with various model architectures that can detect questions using multimodal inputs. Our question detection system runs as fast as other real-time systems at the test time, receives multimodal inputs and returns a scalar score value y\u0302 \u2208 [0, 1]. We evaluate our models on two multimodal datasets, which consist with pairs of text transcripts and audio signals. Our experiments reveal what types of context functions, regularization methods, state transition functions of RNNs and data domains are helpful in RNN-based question detection systems.\n\u2217Work done while the author was at Microsoft.\nar X\niv :1\n51 1.\n01 04\n2v 1\n[ cs\n.C L\n] 3\nN ov"}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Types of Questions", "text": "Questions can have different canonical forms, and they are usually not standardized. However, we can divide the questions into three groups based upon some criteria. Table 1 shows an example from each group. We note that declarative questions are rather unclear to differentiate from non-question statements by looking into their canonical forms because they usually do not contain any wh-words. However, audio signals might contain the features that can be useful when making predictions on this kind of examples, where a question usually contains a rising pitch at the end of the utterance."}, {"heading": "2.2 Neural Networks", "text": "An RNN can process a sequence x = (x1,x2, . . . ,xT ) by recursively applying a transition function g to each symbol:\nst = g(xt, st\u22121), (1)\nwhere g is usually a deterministic non-linear transition function. g gains extra strength to capture long-term memories when implemented with gated activation functions [6] such as long short-term memory [LSTM, 7] or gated recurrent unit [GRU, 5]. We can add more hidden layers in advance or subsequent to the RNN to increase the capacity of the model such that:\nzt = h(g(f(xt), st\u22121)), (2)\nwhere, a sequence z = (z1, z2, . . . , zT ) is the transformed feature representation of the input sequence x, and f and h are additional hidden layers. Instead of using the whole z, we can apply a context function c to reduce the dimensionality and take only the abstract information out of z. The context function c can be either defined as introduced in [5]:\nc1(z) = zT , (3)\nor as introduced in [1]:\nc2(z) = T\u2211 t=1 \u03b1tst, (4)\nwhere \u03b1t is the weight of each annotation ht. c(z) can be used as the learned features for the logistic regression classifier:\ny\u0302 = \u03c3(c(z)), (5)\nwhere \u03c3 is a notation of a sigmoid function.\nIn this study, we implement f and hwith deep neural networks (DNNs) using fully-connected layers and rectified linear units [11] as non-linearity, g with a recurrent layer using either GRU or LSTM as the state transition function, and the context function c with either Eq. (3) or Eq. (4)."}, {"heading": "3 Proposed Models", "text": "We take a neural network based approach where we can stack multiple feedforward and recurrent layers to learn hierarchical features from the training examples and the objective function via stochastic gradient descent.\nWe consider two types of inputs, which are text transcripts and audio signals of utterances. Depending on what types of inputs are used, we can divide the models into three groups: (1) receive only text inputs, (2) receive only audio inputs and (3) receive both inputs. When a model receives both inputs as (3), we can think of a simple but naive way of combining two different features as shown as \u2018Combinational\u2019 in Fig. 1. For a model in each group, it can choose the context function to become as either Eq. (3) or Eq. (4), so the number of combinations becomes six. However, there is another model, receives both inputs, uses Eq. (4) as the context function, but uses a different way of combining two features that is depicted as \u2018Conditional\u2019 in Fig. 1.\nFor each model in each group, we train it with three different ways: (1) without any regularization methods, (2) use dropout [13] and (3) use batch normalization (BN) [8] (note that we are not the first to apply batch normalization to a neural network architecture that contains an RNN [9]). However, there is another diversity, the state transition function of the RNN hidden state, which can be implemented either as a GRU or an LSTM. Therefore, for each model, there are six different candidates to compare with. Recall that we have seven different models, each model has six different variations, there is a total of 42 candidates to be tested on two datasets that are MSR-Skype and CALLHOME."}, {"heading": "4 Experiment Settings", "text": ""}, {"heading": "4.1 Datasets", "text": "MSR-Skype MSR-Skype dataset contains 18, 006 examples given as text-audio pairs, and the proportion of positive and negative examples are well-balanced. Each example is an utterance, which is segmented manually. We only use examples that contain 3 to 25 words to train the models. We use 80% of the examples as a training set and reserve 20% of the examples to validate and evaluate the models.\nCALLHOME We use a subset of the original CALLHOME, where the text transcripts are created by human annotators. There are 2, 528 examples given as text-audio pairs. Utterances are segmented manually, and the train/validation/test splits are divided as same as the MSR-Skype dataset.\nPreprocessing For the text data, we remove punctuations, commas, question marks, exclamation marks to prevent the model from making decisions based on these special tokens. We do not consider pretraining word representation vectors with external datasets, however, they are learned jointly with the objective function during the training procedure. Therefore, h1 in \u2018Single\u2019 (only when x is text data) and h1T in \u2018Combinational\u2019 and \u2018Conditional\u2019 become continuous vector representations of the words (in this case, we do not apply non-linearity). We built the dictionary from MSR-Skype and CALLHOME, which contains 13,911 vocabularies.\nWe extract MFCC from the raw audio signals with 40ms frame duration, and 15ms overlap. The lengths of the audio sequences (after extracting MFCC) could be significantly longer than the text sequences, therefore, in order to reduce the number of timesteps, we concatenate four frames into one chunk and treat it as a single frame."}, {"heading": "4.2 Results", "text": "Table 2 shows the results of the models trained on MSR-Skype dataset. We can observe a few tendencies in the obtained results depending on what kind of variations are applied to the models (c1 or c2, GRU or LSTM, dropout or batch normalization and types of inputs).\nIn general, using both input sources are helpful, but the advantage is not that impressive when batch normalization is used for training. The lengths of the audio sequences are usually longer than the text sequences, and attention mechanism (c2) [1] is known to be a nice solution to deal with long sequences. Therefore, when the model can only take audio inputs, c2 is a better option than c1.\nDropout will help in most cases, however, when using both input sources, the performance does not improve that much. In fact, the performance gets worse than the models, which do not use dropout. We assume that the optimization problem becomes difficult with dropout when the models receive both input sources, hence, in this case we need more care in using dropout. Batch normalization improves the performance with a huge gap for the models that receive text source as inputs. However, batch normalization does not help the models that can only receive audio inputs. The best performance is achieved by a model that receives both input sources (combinational), uses c2 as context function, uses batch normalization for training and uses LSTM as the state transition function of the RNN.\nTable 3 shows the result of each model trained on CALLHOME. We can observe that c2 helps the models that only take audio inputs, and batch normalization improves the performance of the models that includes text source as their inputs. The best performance is achieved by a model that takes both input sources (combinational), uses c1 as context function, uses batch normalization for training and uses GRU as the state transition function of the RNN.\nIn Table 4, we test our models on sequences with different lengths. We use the same models that were trained on MSR-Skype, without any regularization methods. The sequences are divided into\nthree groups depending on the number of words contained in each sequence. Short sequences have less than 5 words, long sequences have more than 20 words, and intermediate sequences contain 5 to 20 words. We observe that the models achieve the best performance on intermediate sequences, and the models tend to do better jobs on short and long sequences when the inputs contain text source. The performance degradations on short or long sequences compared to intermediate sequences are smaller when we use both input sources (see \u2018combination\u2019 and \u2018condition\u2019, especially models lose less performance against long sequences).\nTable 5 shows some test examples that neither contain wh-words nor have canonical form of questions, which we have already introduced as declarative questions in Sec. 2.1. In this kind of questions, there are usually rising pitches at the end of the audio signals. For the models, which receive the audio source as inputs, can benefit from having audio information as shown in Table 5 (see \u2018audio\u2019, \u2018combination\u2019 and \u2018condition\u2019). For the models, which receive only text source as inputs, do not have relevant information to guess whether the given utterances are questions or not.\nThe predicted scores from the models using both inputs are sometimes less than the scores from the model using only audio inputs. We assume that these models have to make compromise between the text features and audio features when these two these two are in conflict. However, given the training objective, it is difficult to expect that the models will completely ignore one of the features, instead, the models will tend to learn more smooth decision boundaries."}, {"heading": "5 Conclusion", "text": "We explore various types of RNN-based architectures for detecting questions in English utterances. We discover some features that can help the models to achieve better scores in the question detection task. Different types of inputs can complement each other, and the models can benefit from using both text and audio sources as inputs. Attention mechanism (c2) helps the models that receive long audio sequences as inputs. Regularization methods can help the models to generalize better, however, when the models receive multimodal inputs, we need to be more careful on using these regularization methods."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano [2]. We acknowledge the support of the following agencies for research funding and computing support: Microsoft, NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Speaker role recognition using question detection and characterization", "author": ["T. Bazillon", "B. Maza", "M. Rouvier", "F. Bechet", "A. Nasr"], "venue": "In INTERSPEECH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Any questions? automatic question detection in meetings", "author": ["K. Boakye", "B. Favre", "D. Hakkani-T\u00fcr"], "venue": "In IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In NIPS Workshop on Deep Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Batch normalized recurrent neural networks", "author": ["C. Laurent", "G. Pereyra", "P. Brakel", "Y. Zhang", "Y. Bengio"], "venue": "arXiv preprint arXiv:1510.01378,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Analysis of statistical question classification for fact-based questions", "author": ["D. Metzler", "W.B. Croft"], "venue": "Information Retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1929}, {"title": "Exploiting salient patterns for question detection and question retrieval in community-based question answering", "author": ["K. Wang", "T.-S. Chua"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Detection of questions in chinese conversational speech", "author": ["J. Yuan", "D. Jurafsky"], "venue": "In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}], "referenceMentions": [{"referenceID": 14, "context": "We can think of a question detection system that works independently and unburdens the load of the speech recognition system [15, 10, 4, 14, 3].", "startOffset": 125, "endOffset": 143}, {"referenceID": 9, "context": "We can think of a question detection system that works independently and unburdens the load of the speech recognition system [15, 10, 4, 14, 3].", "startOffset": 125, "endOffset": 143}, {"referenceID": 3, "context": "We can think of a question detection system that works independently and unburdens the load of the speech recognition system [15, 10, 4, 14, 3].", "startOffset": 125, "endOffset": 143}, {"referenceID": 13, "context": "We can think of a question detection system that works independently and unburdens the load of the speech recognition system [15, 10, 4, 14, 3].", "startOffset": 125, "endOffset": 143}, {"referenceID": 2, "context": "We can think of a question detection system that works independently and unburdens the load of the speech recognition system [15, 10, 4, 14, 3].", "startOffset": 125, "endOffset": 143}, {"referenceID": 9, "context": "Previous studies have focused on using hand-designed features and classifiers such as support vector machines (SVMs) [10, 14] or tree-based classifiers [15, 4, 3].", "startOffset": 117, "endOffset": 125}, {"referenceID": 13, "context": "Previous studies have focused on using hand-designed features and classifiers such as support vector machines (SVMs) [10, 14] or tree-based classifiers [15, 4, 3].", "startOffset": 117, "endOffset": 125}, {"referenceID": 14, "context": "Previous studies have focused on using hand-designed features and classifiers such as support vector machines (SVMs) [10, 14] or tree-based classifiers [15, 4, 3].", "startOffset": 152, "endOffset": 162}, {"referenceID": 3, "context": "Previous studies have focused on using hand-designed features and classifiers such as support vector machines (SVMs) [10, 14] or tree-based classifiers [15, 4, 3].", "startOffset": 152, "endOffset": 162}, {"referenceID": 2, "context": "Previous studies have focused on using hand-designed features and classifiers such as support vector machines (SVMs) [10, 14] or tree-based classifiers [15, 4, 3].", "startOffset": 152, "endOffset": 162}, {"referenceID": 0, "context": "Our question detection system runs as fast as other real-time systems at the test time, receives multimodal inputs and returns a scalar score value \u0177 \u2208 [0, 1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 5, "context": "g gains extra strength to capture long-term memories when implemented with gated activation functions [6] such as long short-term memory [LSTM, 7] or gated recurrent unit [GRU, 5].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "The context function c can be either defined as introduced in [5]:", "startOffset": 62, "endOffset": 65}, {"referenceID": 0, "context": "or as introduced in [1]:", "startOffset": 20, "endOffset": 23}, {"referenceID": 10, "context": "In this study, we implement f and hwith deep neural networks (DNNs) using fully-connected layers and rectified linear units [11] as non-linearity, g with a recurrent layer using either GRU or LSTM as the state transition function, and the context function c with either Eq.", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "We implement the RNN with its bidirectional variant [12].", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "For each model in each group, we train it with three different ways: (1) without any regularization methods, (2) use dropout [13] and (3) use batch normalization (BN) [8] (note that we are not the first to apply batch normalization to a neural network architecture that contains an RNN [9]).", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "For each model in each group, we train it with three different ways: (1) without any regularization methods, (2) use dropout [13] and (3) use batch normalization (BN) [8] (note that we are not the first to apply batch normalization to a neural network architecture that contains an RNN [9]).", "startOffset": 167, "endOffset": 170}, {"referenceID": 8, "context": "For each model in each group, we train it with three different ways: (1) without any regularization methods, (2) use dropout [13] and (3) use batch normalization (BN) [8] (note that we are not the first to apply batch normalization to a neural network architecture that contains an RNN [9]).", "startOffset": 286, "endOffset": 289}, {"referenceID": 0, "context": "The lengths of the audio sequences are usually longer than the text sequences, and attention mechanism (c2) [1] is known to be a nice solution to deal with long sequences.", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "The authors would like to thank the developers of Theano [2].", "startOffset": 57, "endOffset": 60}], "year": 2017, "abstractText": "In this paper, we explore different neural network architectures that can predict if a speaker of a given utterance is asking a question or making a statement. We compare the outcomes of regularization methods that are popularly used to train deep neural networks and study how different context functions can affect the classification performance. We also compare the efficacy of gated activation functions that are favorably used in recurrent neural networks and study how to combine multimodal inputs. We evaluate our models on two multimodal datasets: MSR-Skype and CALLHOME.", "creator": "LaTeX with hyperref package"}}}