{"id": "1511.02455", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2015", "title": "(Yet) Another Theoretical Model of Thinking", "abstract": "1,321 This paper mmea presents 76101 a puckle theoretical, idealized mallock model mouring of farhan the blondish thinking process with 1946-1952 the following wangdi characteristics: 1) the larchmont model \u03bcm can produce complex jsi thought sequences miac and southwind can lebesgue be 52-point generalized joviane to hatuel new macklin inputs, cdr 2) sperimentale it weekley can receive ius and maintain input ziprasidone information indefinitely tooele for sapwood generation of thoughts whyman and later use, pentel and flaring 3) it exaltation supports learning forane while gager executing. semu The crux chirped of keer the doft model lies within the plassey concept of internal 31f consistency, mosakeng or gobin the generated thoughts br\u00fcder should always importuning be consistent with the inputs from martin-l\u00f6f which they rodna are created. Its merit, mianyang apart from archidamus the daptone capability wener to generate new creative thoughts pavlic from umpteenth internal flush mechanism, manufactuers depends on the potential todt to help superbowl.com training to remedying generalize better. lucani This puraskaram is underlayment consequently enabled ,120 by separating input information zhol into several t\u00edtulo parts to plusquellic be handled by 2,085 different kamina processing formula components with accessing a mwanawasa focus mechanism to fetch serian information for gullible each. nebulous This docter modularized ellement view megadoses with focus kochi binds plumbs the model noelle with 462,500 the computationally capable Turing machines. And worthies as erred a final remark, ostr\u00f3wek this paper constructively occurance shows ragno that jovtchev the 70.8 computational stromae complexity welti of the shanter model guoan is sensacion at least, matlack if alava not surpass, altero that of amounts a revija universal Turing datatypes machine.", "histories": [["v1", "Sun, 8 Nov 2015 08:20:53 GMT  (98kb,D)", "https://arxiv.org/abs/1511.02455v1", null], ["v2", "Sat, 14 Nov 2015 05:11:59 GMT  (98kb,D)", "http://arxiv.org/abs/1511.02455v2", null], ["v3", "Mon, 15 Feb 2016 16:01:18 GMT  (99kb,D)", "http://arxiv.org/abs/1511.02455v3", null], ["v4", "Mon, 17 Apr 2017 15:47:17 GMT  (99kb,D)", "http://arxiv.org/abs/1511.02455v4", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["patrick virie"], "accepted": false, "id": "1511.02455"}, "pdf": {"name": "1511.02455.pdf", "metadata": {"source": "CRF", "title": "(Yet) Another Theoretical Model of Thinking", "authors": ["Patrick Virie"], "emails": ["p.virie@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This paper presents a theoretical, idealized model of thinking. Thinking, as a mental process, is one of the most sophisticate products of intelligence. It allows us to perform procedural simulations in order to predict the future outcomes of given present states. Having the model of thinking would enable the explanation of our mind and would also facilitate the building of replicas of it.\nDue to its deep inherent association with our minds, thinking is one of the fundamental concepts in philosophy [1]. Perhaps most of the modern attempts to model the process of thinking are arguably inspired by Alan Turing\u2019s work [2], which addresses many philosophical and technical questions about the machines that can think. From that point onward, the term \u201dthinking\u201d has been associated with various meanings even within the context of computation. Our interpretation of the word thinking would only limited to the continuous process of generating data from selected inputs.\nAs the reader goes through the content, a question might come up in mind, \u201dthis does not seem to be how the brain works.\u201d The purpose of this paper however, is not to postulate the actual process of thinking that happens in one\u2019s brain; we are only interested a theoretical fraction that captures the ideal essence of thinking such as how new ideas are composed from experience and generalization of thinking sequences. From this motive, the model that we develop in this paper only has to maintain the following characteristics: a) it must permit complex transformation within sequence and generalize to unseen inputs, b) it can receive and maintain information indefinitely and selectively use them to generate future sequences, and c) it should support learning while executing. From which necessities are these characteristics derived? And how do we address them?\nIf we manage to represent the information of any moment of thought using a structure within a mathematically defined space, thinking process can be defined as a sequence of transformations between the structures within that space. To be able to capture real world sequences, the transformations should be sufficiently expressive. This problem can recently be addressed by the re-popularized concept of deep learning [3\u20135]. Deep learning allows stacking of simple transformations, which are individually easy to analyze, to express a complex non-linear transformation.\nar X\niv :1\n51 1.\n02 45\n5v 4\n[ cs\n.A I]\n1 7\nA pr\n2 01\n7\nEven though thoughts are very fluid and alternating from moment to moment, yet everything happens in mind. Every information that we generate or receive at some points in time shall be used at some other times to generate a new data. Therefore to address the second characteristic, the model must be able to memorize information. The success of the recurrent models for time-related tasks shows that we might use them as a prototype of our model [6, 7]. This will be discussed in Section 3. Furthermore to show that the complexity of the model could rival that of our mind, we could try to relate the model with the behavior of a universal Turing machine [8]. Section 4 will contribute to this aim.\nFinally, we aim to develop a model that allows learning process to happen homogeneously along the execution path and simultaneously at the execution time. This requirement is not crucial in the process of thinking, but it is useful for any actual system that implements our model to have some kind of online-learning capability. We will show that learning while executing is possible within our model in Section 3.\nBefore going to the model, we will first discuss the constraint that constitutes the essence of our model, called internal consistency. We will relate this constraint with the notion of generalization within the scope of thinking. What can we guarantee given the inputs can change? How would we define generalization in the context of thought? And how can we implement the constraint in a real machine? This is where we start."}, {"heading": "2 Internal consistency", "text": "For a generative system that tries to model the world, it usually consists of two sides of data representation: one that represents the observed, visible states of the world, and the other that represents the hidden states [9]. The hidden states are alternative representations of the visible states, and as the name implies, each represents another way of how to represent a data. For example, we say 2+3 is an alternative representation of 5, so do 1 + 4 and any other summation of two numbers that equal 5.\nWhy do we have to be bothered with alternative representations? Alternative representation systems usually have some neat inter-basis characteristics. For example, the bases that contain information in the hidden states can be made less correlated, less redundant, or sometimes be completely independent. Independent bases are desirable for generative models, since we can represent the distributions of the world efficiently as the products of distributions of individual bases [10]: P (X0, X1, . . .) = \u220f i P (Xi) where Xis are random variables. It is up to applications to define the best sets of bases for the hidden states. This is in fact conformed to what deep learning tries to achieve.\nTo further illustrate why the alternative representations are necessary as the fundamental of our thinking model. What if whenever we try to imagine the Mona Liza but what comes out from our mind instead turns out to be the Scream. Is it not frustrating? Though these two historical paintings are of artistic merits, recalling a wrong one while we originally intend for the other can hardly be recognized as a trait of intelligence without proper reasons. In control theory, engineers attempt to design systems with closed-loop sensory feedbacks to rectify undesirable outputs. The systems can immediately identify when their previous outputs deviate from the expectation in order to compute controls with proper compensation schemes. Goodfellow et al. suggested a neat mathematicallyproven training technique to regularize this behavior in neural networks by introducing adversarial networks [11]. The adversarial networks will attempt to identify whether data are the results from generative networks or the real distributions. The generative ones must be trained to counter this\ndiscrimination as best as they could. Despite the success of these techniques, would it not be better if we can build systems that can inherently prevent all of these action-intention inconsistencies? We can say that the systems are perfectly adaptable without the need for compensatory countermeasures. In fact, this is an empirical trait that any intelligent systems should have, something that we humans have, at least to some degrees.\nLet us suppose for now that the inputs of a generative system that tries to generate visible states of the world are never-been-seen-before hidden states. How can we guarantee that the system would produce the correct visible states as the outputs? The generalization of alternative representations would be achieved by making sure that the generated visible representations are always conformed to the hidden states that cause them. For example, suppose that we have a system that tries to produce the addition result of any two numbers, say A and B as the hidden state, we say the system is generalized when its generative function is A + B. We call the phenomenon where the hidden states of a system are always alternative representations of its visible states internal consistency. This is the best we can do for generalization.\nWhat does this heuristic mean computationally?"}, {"heading": "2.1 The non-sharing property and preservation of variance", "text": "Let v represents a visible state from the set V , and h represents a hidden state from the set H . A system is said to be internally consistent when the mapping from a visible state to any hidden state and the reconstruction from one of the reachable hidden states always results in the visible state itself: \u2211\nh\nP (v|h)P (h|v) = 1 \u2200v \u2208 V (2.1)\nOr the mapping preserves variance in V . Please note that this paper uses probabilistic short forms; namely, P (h|v) is a short form of P (h\u0302 = h|v\u0302 = v) where h\u0302, v\u0302 are random variables. To better understand the connection between internal consistency and preservation of variance, we need to consider the fact that an alternative representation of any data may not be unique. A visible state can have many alternative hidden states of representation, but those states must only correspond to none other than the visible state itself. Definition 2.1. Let a set of alternative hidden states of a visible state is a set whereas every element can be generatively mapped into the visible state. The non-sharing property is satisfied if and only if the forward mapping from the visible state only results in an element from the set.\nThe non-sharing property implies that, when we transform a hidden state into its corresponding visible state, there is no other visible state that better matches the hidden state available. Let us think about the addition example again. Suppose that we want to produce the visible state of 2 + 3, which is 5, how do we guarantee that the produced 5 is correct? It is by converting back 5 into a hidden state, which may result in 1 + 4. Then we verify that 1 + 4 and 2 + 3 belong to the same set. Lemma 2.1. Preservation of variance implies the non-sharing property.\nProof. \u2211 h P (h|v)P (v|h) is a convex combination of P (v|h) \u2200v, h. If there exists P (v|h) < 1 then\u2211\nh P (h|v)P (v|h) < 1. Therefore, for any h with non-zero P (h|v), P (v|h) = 1. Lemma 2.2. The non-sharing property is equivalent to variance preservation.\nProof. For any h with non-zero P (h|v), \u2211 h P (v|h)P (h|v) = 1 when P (v|h) = 1, \u2200v. Also, from Lemma 2.1. This completes the proof.\nFrom Lemma 2.2, for a system to be internally consistent, it must at least preserve the variance in the visible states. Internal consistency may not be perfectly achieved in practice. We can however approach it by gradually training the system to maximize reconstruction chance:\nmax \u2211 h P (v|h)P (h|v) \u2200v (2.2)\nLearning hidden representation while maximizing the reconstruction chance aligns with the goal of autoencoder training [4, 12]. Preservation of variance is therefore another justification for learning representation with autoencoders.\nOne nice property of the non-sharing property is that it can be stacked to create a deep expressive architecture that permits multi-layer hidden data transformation. This way, it is possible to find a good complex representation for any data domain by having each layer gradually removes correlation in the data and promotes little-by-little independency for the data in the adjacent layer. Training to build a deep internally consistent system is as simple as training to preserve the variance between layers. Lemma 2.3. Stacking of variance preservation satisfies the non-sharing property.\nProof. Let the prime notations of one dimension vectors represent their variants. Given a variance preservation layer, \u2211 h P (v|h)P (h|v) = 1, stacking another layer on top of it preserves the non-\nsharing property: \u2211 h P (v|h) ( \u2211 h\u2032 P (h|h\u2032)P (h\u2032|h))P (h|v) = \u2211 h P (v|h)1P (h|v) = 1. We can apply this action multiple times to build a multi-layer architecture. This completes the induction.\nPreservation of variance suggests a way to generate innovative yet relevant visible states from new hidden states, and therefore allows us to guarantee internal consistency, i.e., alternative representation for unseen inputs. To show this in a system, we require the knowledge or the detail of system implementation. We will see in the next section a way to implement an internally consistent system."}, {"heading": "2.2 Implementation in a linear system", "text": "We discuss a linear neural network as an internally consistent system. Each layer of the network can be mathematically expressed as a matrix multiplication: h = Wv where W represents a forward linear mapping weight matrix, v is a visible state vector, and h is a hidden state vector. To achieve variance preservation, the generative mapping from a hidden state to a visible state W \u2032 must fulfills this equation: v = W \u2032Wv. For the analytical purpose, it is even simpler to consider filling the entire linear span of the visible state set, i.e., to make W \u2032W = I . In this regard, the generative matrix W \u2032 must be the left inverse of the weight matrix W , thus preserving variance of the visible states. The following subsection shows that we can guarantee the non-sharing property for unseen visible states when the inverse exists."}, {"heading": "2.2.1 A linear neural layer that satisfies internal consistency", "text": "Let us define the concept of equilibrium in a linear neural layer. Definition 2.2. An equilibrium is a setting where a visible state and and only one of its hidden states correspond to each other.\nIn other words, for any h \u2208 Hv or the set of alternative hidden representations of v, every P (v|h) = 1, and only an element h\u0302 \u2208 Hv receives all the probabilistic mass, P (h\u0302|v) = 1. For short notation, h\u0302 \u21d0\u21d2 v. This is partly due to the determinism of linear systems that allows no more than one h to correspond to Wv and only one v to reciprocally correspond to W \u2032h. Lemma 2.4. A linear system that allows hidden-visible transformation to reach the equilibrium in one step from any hidden state has the non-sharing property.\nProof. To prove this statement, we show that, when the non-sharing property does not hold, there always exists a path greater than one step. Let v\u2032 \u2192 h \u2192 v be a path. Such a part violates the non-sharing property when v\u2032 6= v. Suppose there exists an h\u2032 such that h\u2032 \u2192 v. h\u2032 can never be h, because h \u2192 v and h \u2192 v\u2032 cannot be true at the same time; this contradicts the determinism of linear systems. From here we can conclude that there is at least more than one step from h\u2032 to reach the closest equilibrium, h\u2032 \u2192 v\u2032 \u2192 h \u21d0\u21d2 v.\nProposition 2.5. A linear system where its forward mapping has the left inverse satisfies the nonsharing property for unseen hidden states.\nProof. Consider a path h\u2032 \u2192 v\u2032 \u2192 h\u2032\u2032 \u2192 v\u2032\u2032 \u2192 . . . in its linear form: h\u2032 \u2192 W \u2032h\u2032 \u2192 WW \u2032h\u2032 \u2192 W \u2032WW \u2032h\u2032 \u2192 . . . Since W \u2032W = I , the path can be truncated: h\u2032 \u2192 W \u2032h\u2032 \u2192 WW \u2032h\u2032 \u2192 IW \u2032h\u2032 = W \u2032h\u2032 \u21d0\u21d2 WW \u2032h\u2032. From any h\u2032, the system reaches the equilibrium W \u2032h\u2032 \u21d0\u21d2 WW \u2032h\u2032 in one step. Thus, it satisfies the non-sharing property according to Lemma 2.4.\nCorollary 2.6. Stacking of linear internally consistent systems satisfies the non-sharing property for unseen hidden states.\nProof. This is true by Proposition 2.3. It can also be seen that when each layer satisfies the nonsharing property, any forward pass in the stack is always an equilibrium path. Since the stack only takes one single generative pass from a hidden state to generate its visible state, it satisfies the nonsharing property according to Lemma 2.4."}, {"heading": "2.2.2 Missing variance", "text": "The discussion of alternative representation has led us to a kind of problems where in some applications we might want to construct visible states purely from given hidden states, i.e., constructing v from h. Many real life problems belong to such category, to state a few, giving an artistic style and an abstract shape, how to create a detailed painting containing the shape with the style [13, 14] or reconstructing visual images from brain reading signals [15]. The difficulty of these problems, apart from finding the generative mapping, lies mostly in the fact that it is nearly impossible to perfectly provide all the variance for the generative construction. The construction requires that all the variance in the hidden state have to be filled; otherwise the result visible state may suffer the lack of sufficient details. This might coincide why even for us it is sometimes hard to imagine the precise details of some concepts. Before discussing how could we fill the missing variance, we need a way to represent it first.\nWe introduce a hidden residue as a part of a hidden state that is not given as an input. Let W represents an assumed-given visible-to-hidden mapping weight, and U represents a weight for visible-toresidue mapping. To satisfy internal consistency, we must have the left inverse W \u2032|U \u2032 of W |U such that\n(W \u2032|U \u2032)(W |U)v = (W \u2032W + U \u2032U)v = v (2.3)\nA|B is a matrix as a concatenation result between the matrix A and B of the same number of columns.\nWe can constructing v from h via this relationship:\nv = W \u2032h+ U \u2032r (2.4)\nwhere r is the have-to-be-inferred hidden residue state. The process of inference must be automatically done by the system. In the next section, we explore the strategies to train a system with such capability following the internal consistency constraint."}, {"heading": "2.3 Training internal consistency in a linear system", "text": "The goal of training is to search for a set of bases that provides good alternative representations and preserves variance in the data. This is unsupervised learning. Given a set of visible states v \u2208 V , we wish to find the forward mapping weight W , its generative weight W \u2032, also perhaps along with the residue mapping U , and its inverse U \u2032, subject to Equation 2.3.\nDespite that the exact solution for finding the weights can be found via any decomposition process, it is more favorable in applications with large amount of data to use an iterative based algorithm. Reconstruction ICA is a good candidate for training [16]. Consider one form of its objective (without the residue weight), minW \u2211 v\u2208training data ( ||W \u1d40Wv \u2212 v||22 + \u03bb||Wv||1 ) , Le et al. shows that\nLemma 2.7 (Le et al., 2011). The reconstruction term in RICA\u2019s objective is equivalent to ||(W \u1d40W \u2212 I)E\u039b 12 ||2F or the orthonormality cost with weights in the space rotated by eigenvectors and scaled by eigenvalues.\n\u039b is a diagonal eigenvalue matrix, and E is a matrix whose columns are eigenvectors of the covariance matrix of the training data. \u03bb in RICA\u2019s objective is a sparsity coefficient that controls how much learning effort contributes to finding independent bases. When the eigenvalues of the training data are real positive, we can see that the solution to RICA involves ones where W \u1d40W approaches the identity, which in turn makes the system that implements RICA satisfies internal consistency.\nAlthough RICA is originally derived with the generative weight as the transpose of the forward weight and without the residue weights, we can extend it to support a non-transpose system by updating the gradients for both W and W \u2032 separately, and expand the weight into the form W |U , which includes the residue weight."}, {"heading": "2.3.1 Linear transpose bases", "text": "In deep learning, it has been a common approach to use the transpose of the forward weight as the generative one. The transpose acts as a good learning regulator originally presented in Oja rule\u2019s [17].\nFor a linear neural layer with transposes, the weights that suffice the condition for internal consistency follow\nv = (W \u1d40W + U\u1d40U) v\nwhich also implies that\nU\u1d40Uv = (I \u2212W \u1d40W )v W \u1d40Wv = (I \u2212 U\u1d40U)v\nWhen v is any vector from the entire linear span of the visible state set, we can see that\nU\u1d40U = (I \u2212W \u1d40W ) W \u1d40W = (I \u2212 U\u1d40U)\nUnless we allow complex values in the neural network, the last two equations suggest that only the weights that make I \u2212W \u1d40W and I \u2212 U\u1d40U have real positive eigenvalues can be decomposed into U\u1d40U andW \u1d40W respectively. If the weights are valid and the square roots of their eigenvalues exist, knowing one weight allows us to immediately extract the other weight via any eigen decomposition process of the form\nU\u1d40U = I \u2212W \u1d40W = E\u039bE\u1d40\nU = \u039b 1 2E\u1d40\n\u039b is a diagonal eigenvalue matrix, and E is a matrix whose columns are eigenvectors of I \u2212W \u1d40W , and vice versa for W .\nSince each eigenvalue represents the variance along each principle axis in the visible state set, this means each of the weights, W and U , cannot extract more information than that presenting in the visible state set."}, {"heading": "2.3.2 Addressing missing variance", "text": "When some parts of the hidden states are not given, we can fill them with the priors from training. This suggests a system with some form of internal memory that can remember the information provided in the hidden states during the training. The memory allows the system to later infer the residue states conditioned on the available part of the given hidden states.\nWe choose to fulfill the role of the memory with a belief network stacked on top of the hidden units due to its simplicity [3] among other generative techniques. A belief network is a generative model that can be trained to generate training data\u2019s distribution P (v) where v \u2208 the training set. It is a stack of restricted Boltzmann machines trained with the contrastive divergence algorithm, a variant of gradient ascent with the following update rule: \u2207W \u221d\u2211 v P (v) (\u2211 h P (h|v)hv\u1d40 \u2212 \u2211 h\u2032,v\u2032 P (h \u2032, v\u2032)h\u2032v\u2032\u1d40 )\n. At convergence, the values of the hidden units h conditioned on the visible units\u2019 v should match the stationary distribution due to bipartite nature of the machines. Inference in a belief network creates a Markov chain with its stationary distribution conforms to P (v).\nIn our case, we use the top layer belief network to model the distribution of the hidden and the residue state, i.e., v of the belief network is simply h|r of our system. Given h, running the inference in the form of repetitive sampling should provide us the missing variance conditioned on it. Then the hidden state and the residue state are fed down to construct the visible state following Equation 2.4.\nWe will later see that this variance filling mechanism can be used as a memory extension for our thinking model. This augmentation allows us to increase to the model\u2019s capacity to quickly access memory and also facilitates the learning while executing procedure."}, {"heading": "2.4 Extension to non-linear", "text": "In some applications, having hidden states that capture some non-linear traits in visible states has never been without practical advantages. We can use the rectified linear activation function as a means to achieve this [18]. The non-linear behavior of rectified linear units can convey non-linear information from visible states to hidden states while satisfying internal consistency given the presence of mirror bases.\nLemma 2.8. A rectified linear layer with mirror bases satisfies internal consistency in the same manner to those with linear bases.\nProof. LetWu be represented by \u03c1(Wu)+\u03c1(\u2212Wu) when \u03c1 is a rectified linear projection of every row of Wu. Let k, u be column vectors of the same length:\n\u03c1(k\u1d40u) = { k\u1d40u when k\u1d40u > 0 0 otherwise\nLetW \u2032 represents the left inverse of W . It can be seen that for any v, W \u2032\u03c1(Wv)+\u2212W \u2032\u03c1(\u2212Wv) = W \u2032Wv = v.\nA pair of mirror bases contains mutually exclusive linear polars tied together to form a linear basis. Although within a layer, each mirror pair behaves like a linear basis and follows internal consistency, we can have non-linear transfer at the adjacent layer by assigning a different weight value to each of the rectified linear base in the pair."}, {"heading": "2.4.1 A more general way to choose non-linear bases", "text": "Here we suggest a method to modify a linear mapping into a non-linear one that, of course, satisfies internal consistency.\nLet us start from the basic linear internal consistency in a linear layer.\nFor any v,\nv = W \u2032Wv\n= W \u2032IWv\n= W \u2032 [I I . . .] [ S0 S1 . . . ] Wv\nThe last one requires that \u2211 i Si = I (2.5)\nNote that each basis set Si is not only a constant matrix but can also be a collection of functions where their resolved values depend on to what values they are being multiplied. Example 2.1. Let \u03c3 represents a step function at 0:\n\u03c3(x).x = { 1.x = x if x \u2265 0 0.x = 0 otherwise\nWe can see that\n\u03c3(x).x = \u03c1(x)\nThe product of the step function with its input behaves like a rectified linear function, and the mirror pair of \u03c3 is in fact 1\u2212 \u03c3. With this, we have an option to choose basis sets as follow:\nS0 = [ \u03c3 0 . . . 0 \u03c3 . . . . . . . . . . . . ]\nS1 =\n[ 1\u2212 \u03c3 0 . . .\n0 1\u2212 \u03c3 . . . . . . . . . . . . ] which satisfies Equation 2.5.\nFor a layer, we can then expressively have\nh = [ S0 S1 . . . ] Wv (2.6)\nv = W \u2032 [I I . . .]h.\nNote that the formed generative and forward mapping are not symmetry. The generative mapping is just a summation before being multiplied with the left inverse while the forward activation is non-linear following the choice of basis sets.\nTo allow non-linear transfer across layers, the adjacent layer can assign a different weight value to each basis. We can use this fact with a finite number of basis functions to represent almost any bounded complex transformation according to the universal approximation theorem [19].\nAs a theoretical remark, we can also see that the infinite number of local basis functions can approximate any transformation in fact:\nf(x) = \u222b \u221e 0+ f(x\u0302) x\u0302 .\u03c0x\u0302(x).xdx\u0302+ \u222b 0\u2212 \u2212\u221e f(x\u0302) x\u0302 .\u03c0x\u0302(x).x dx\u0302\nwhere\n\u03c0x\u0302(x) = { 1 if x = x\u0302 0 otherwise\nIf we treat f(x\u0302)x\u0302 as the adjacent layer weight for the basis that corresponds to x\u0302 value, we can regard \u03c0x\u0302 as the basis itself. The following equation holds:\ny \u222b \u221e \u2212\u221e \u03c0x\u0302(x).xdx\u0302 = y.1.x \u2200y, x\nIn the multiplicative case however, there is an only exception for the critical point at x = 0, and there is no information to be transferred. This can be leveraged with a tweak to both the forward and the generative mapping to detect the critical point and assign a unique basis value to allow correct inversion.\nThe application of this proposal is that we can perform the following steps when training internal consistency in a neural layer:\n1. Train its linear weight matrices following v = W \u2032Wv\n2. Choose non-linear basis sets [ S0 S1 . . . ] such that \u2211 i Si = I\nThis technique can be applied many times in a stack to form alternating layers of linear and nonlinear transfer functions.\nA variation of the technique can be used in convolutional networks [20] as well:\nv = W \u2032\u2217W \u2217 v v = W \u2032\u2217 \u03b4\u2217W \u2217 v\nv = W \u2032\u2217 { S0 S1 . . . } \u2217W \u2217 v (2.7)\n\u03b4 is the Dirac delta function, the identity of the convolution operation. For a short notation we\ndefine the following quantity { S0 S1 . . . } an alternative representation of the identity I that belongs to any tensor operation ? with the distributive property such that\nA ? { S0 S1 . . . } ? B = A ? I ? B\n\u2211 i Si = I\nA and B belong to the domain of the ? operation.\nFrom Equation 2.7 we can have\nhi = Si\u2217W \u2217 v (2.8) v = W \u2032\u2217 \u2211 i hi\nIn practice however, it is not convenient to convolve a tensor with a basis set that depends on the value of its operand as in Equation 2.8. We can leverage this with the convolution theorem and the inner product:\nv = F \u2032(F(W \u2032) \u00b7 F(F \u2032(F(W ) \u00b7 F(v)))) v = F \u2032(F(W \u2032) \u00b7 F(W ) \u00b7 F(v))) v = F \u2032(F(W \u2032) \u00b7 1 \u00b7 F(W ) \u00b7 F(v)))\nv = F \u2032(F(W \u2032) \u00b7 { S0 S1 . . . } \u00b7 F(W ) \u00b7 F(v)))\nF and F \u2032 are the Fourier transform and its inverse respectively. We can now multiply each element individually as in the linear example:\nhi = F \u2032(Si \u00b7 F(W ) \u00b7 F(v)) (2.9)\nAgain, we can first train the convolution kernels W and W \u2032 before choosing the non-linear transfer functions."}, {"heading": "2.5 Temporal internal consistency", "text": "Recurrent neural networks belong to a class of expressive architectures that we can use to build generative temporal systems for modeling temporal sequences [21]. If we treat the hidden states as the theme of a temporal sequence, we can have the visible states that can change through time by treating past states as the condition for the present time step. For each past configuration, the current visible state will satisfy the non-sharing property. But does the non-sharing property apply for unseen conditions as well?\nLet vt represents the visible state at a time t. ~v\\t denotes a collection of visible states over many time steps into some past excluding vt. Consider the expectation of temporal variance preservation conditioned on ~v\\t: \u2211 ~v\\t P (~v\\t|vt) \u2211 h P (vt|h,~v\\t)P (h|vt, ~v\\t) = 1. We can rearrange the term\nto treat the past data as a part of the hidden state; and together with the original hidden state, we can use them to produce the visible state of the present time step:\u2211\n~v\\t\n[\u2211 h P (vt|~v\\t, h)P (h|~v\\t, vt) ] P (~v\\t|vt) = \u2211 ~v\\t \u2211 h P (vt|~v\\t, h)P (h,~v\\t|vt) = 1\nLemma 2.9. Following the proof of Lemma 2.1, satisfying the conditional non-sharing property is equivalent to satisfying the non-sharing property when the condition is accounted as a part of the hidden state.\nTherefore in any conditional alternative representation system, we can consider the conditions as a part of the hidden states. As time progresses, each visible state will always be alternatively represented by the combination of a hidden state and some past visible states, satisfying the non-sharing property for any condition and thus the internal consistency constraint.\nTo build a temporal internally consistent system, the training algorithm has to preserve the variance found in the present visible state conditioned on the given past visible states. Due to the relation from Lemma 2.9, we propose another proposition: Proposition 2.10. If an algorithm that is used to train the conditional non-sharing property shares the same routines with one that is used to train the non-sharing property when the condition is accounted as a part of the hidden state, that algorithm will generalize the non-sharing property to unseen conditions.\nIt remains depending on the system implementation to provide sufficient criteria that can guarantee the generality of the algorithm to unseen conditions. Analogous to the non-conditional case, it is straightforward to see that conditional RICA can be regarded as such an algorithm.\nWe can also extend the expressions to describe multilayer systems:\n\u2211 ~v0\\t \u2211 v1t P (v0t |~v0\\t, v 1 t ) \u2211 ~v1\\t \u2211 v2t P (v1t |~v1\\t, v 2 t ) (. . .)P (v 2 t , ~v 1 \\t|v 1 t ) P (v1t , ~v0\\t|v0t ) = \u2211 ~v0\\t \u2211 v1t P (v0t |~v0\\t, v 1 t ) \u2211 ~v1\\t \u2211 v2t P (v1t |~v1\\t, v 2 t ) (. . .)P (v 2 t , ~v 1 \\t|v 1 t ) P (v1t |~v0\\t, v0t ) P (~v0\\t|v0t )\nThe superscripts denote layers to which the associated states belong.\nThe temporal model allows visible states to change despite having been corresponded to a fixed hidden state. The hidden states can also change following the temporal progression of the visible states. We could stop here and propose these two alternative transformation as the model of thinking unless we wish to integrate the concept of attention into our thinking paradigm."}, {"heading": "3 Yet another model of thinking", "text": "Our model of thinking is a temporal apparatus that continually generates data to form a sequence of thoughts using the information from somewheres and some times in the sequence itself. In a manner similar to other alternative representation models, we represent the generated thoughts with visible\nstates, and we use hidden states to relate information in the thought sequence. The hidden states are divided and grouped forming a finite number of processing modules which we call components. Each component acts as an information cache whose content is fetched depending on a spatiotemporal cue, or simply focus. Components are used to provide the means to choose information from various sources and combine them in a creative yet deducible way to form the sequence of thinking.\nWhile recurrent neural networks are useful, their recurrent connections have a disadvantage compared to focuses. To generate a thought using recurrent connections that requires some information up to some long past in the sequence, the recurrent connections have to cover throughout a lot of past steps in the sequence. But this would come into a shortcoming when we consider our minds\u2019 capability to switch between different thoughts. At one time, a song is just an ear-worm in our mind, but at another time, we would have no problem to switch to another song. If it is the extensive temporal recurrent connections we have in our mind, switching thoughts would not be so simple. The cue that signals the switching would have to compete with many others. This idea suggests that the model should permit only short temporal connections, and should rather rely on another mechanism to fetch information, such as the focus. With the focus, the model can access past information at any time while allowing the ability to abruptly change thoughts.\nThe impression of the model is best illustrated visually in Figure 8."}, {"heading": "3.1 The model and its behavior", "text": "In our model, the degrees of freedom that change between consecutive thought steps derive only from focuses. In particular, a focus controls from where and when information should be fetched into a component. A focus is similar to a pointer in the context of programming, namely, an address that points to a memory content in a program. Though the memory content can be dynamically altered, the part of the program that executes and controls the pointer remained static. It is this concept that we bring into our model with focuses, and this allows us to generalize the model to some degree. The focus also allows the model to choose which portion of information should and shall be processed at a time yielding a biologically inspired capability to recognize structures in noisy environments if any.\nLet us define the focus of a component as a tuple of a selective focus s and a generative focus g, f = (s, g). The selective focus defines from where in the previous visible states the information content in the component is fetched, and the generative focus defines upon which portion of the new visible state the information content will be placed.\nAt the beginning of each time step t, the model starts by activating the focus of each component using only the recent thoughts.\nvt, vt\u22121, ft\u22121, . . . , vt\u2212\u03c4 , ft\u2212\u03c4 \u2192 f = (s, g)\u2192 h (3.1)\nThe activation of the focus is unidirectional from a finite number \u03c4 of latest visible states in the sequence, with optionally the same number of previous focuses, to the current focus of each component. The lack of a backward mapping means the non-sharing property between the visible states and the focus is not defined. Yet when the activation is deterministic, the non-sharing property is implicitly hold because we can always find a backward mapping that can sustain it.\nOnce the focuses of every component are generated, the model then fetches the content h for each component, corresponding to each individual\u2019s selective focus s, and uses it together with the contents from other components to generate a new visible state, a new thought for the thought sequence, following the internal consistency constraint.\n\u2200v \u2211 h P (v|h,g)P (h|v,g\u22121) = 1 (3.2)\nWe call this the combination rule, i.e., the generated thought must preserve all the information provided in the components. h represents a collection of all components\u2019 contents. g is a collection of generative focuses from all components that dictates how should h be combined. In order to complete the equation, we require the inverses of the generative focuses g\u22121 . They act as the selective focuses that choose information from the newly generated thought back into the components.\nSince the generative focuses of the components may sometimes not overlap one another. To make the combination rule always valid, the content mapping parameters of each individual component must satisfy preservation of variance. For each component i,\n\u2200vi \u2211 hi P (vi|hi)P (hi|vi) = 1 (3.3)\nThis equation serves as a constraint that explains the behavior of the mapping parameters for each individual component; it is not mandated to hold for every portion of the generated thought; there can be times when this equation contradicts Equation 3.2, e.g., when the generative focuses place the mutually contradictory contents of two or more components on the same portion of the thought. Nevertheless, it must hold for the mapping parameters of each component.\nAll of these equations constitute the thinking process of our model."}, {"heading": "3.2 Advantages", "text": "For the reader, it is best to pause here and discuss the advantages this model can accomplish.\nFirst, the multi component model allows each component to store information from a different source and be ready to combine with others\u2019 to form a new thought. As we mention in the previous section that when the model has been trained to follow internal consistency, we could have a generalization guarantee for the alternative representation to any new combination. Creativity is therefore governed by the focuses of the components that select information, and from which allows new thoughts to form.\nSir Isaac Newton had postulated in his seminal work [22] that every surrounding change in the environment comes from either motion or transformation. Some objects may alter their intrinsic properties, but the rest only move. Using our model, it is possible to have a representation of any environment state that separates motion from the background, and we let the focuses handle the mechanic of the moving part. This way we aim for a better generalization when training the model.\nThe last reason is in fact a means to reduce hypothesis variance with a limited amount of training data. To see roughly why separating focuses and contents can help reduce hypothesis variance, we can count the amount of training examples required for two neural network implementations, i.e., with and without focuses. Let M be the number of our model\u2019s components, T be the total number of past states we kept for the network model without focuses, N be the number of content bits per each state, K be the number of values per bit, Finally we let each individual bit of focus has two values, i.e., focus or not focus. In the worst case scenario where the networks can only memorize the input-output pairs and do not generalize them, the lower-bound of the required number of examples to memorize input-output mappings is the size of domain times the size of co-domain. For the network model without focus, the required number of examples is the size of total past states we\nkept times the size of the new state, T (KN )\u00d7KN . We will let the reader work out for the required number of examples for our model, which is M ( KN/M \u00d7KN/M ) + (KN \u00d7 2N ). We can see that for caseM = 2, K \u2265 2, andN \u2265 2; the amount of examples for training the model without focuses is greater than the number required for our model. Usually for a fair comparison we let T = M , i.e., the number of states we kept for the network model without focuses is equal to the number of components of our model.\nT (KN )\u00d7KN = TK2N\n= 2K2N > M ( KN/M \u00d7KN/M ) + (KN \u00d7 2N )\n= MK2N/M + (2K)N\n= (2N + 1)KN\nThis crude estimation only provides an intuition of why separating focuses can help reducing hypothesis variance of a neural network however, as we do not take account of the ability to generalize of the compared models nor the dependencies in the training data."}, {"heading": "3.3 Existence", "text": "Here we show that in general, we can always build a multicomponent-multilayer internally consistent system that allows non-linear representation of the visible state sequences following our model\u2019s behavior.\nProposition 3.1. There exist non-linear implementations of the model that satisfy Equation 3.2 and Equation 3.3.\nGiving some examples will take care of the proof.\nExample 3.1. In the first example, we show that a linear, single-layer, internally consistent implementation of the model exists. Then by Lemma 2.3 and Lemma 2.8, we can put any desired number of non-linear layers at the bottom of it to create a stack of non-linear internal consistency layers.\nFrom Equation 3.2, we interpret it into a linear form:\nv = GW \u2032WG\u22121v \u2200v (3.4)\nwhich can be expanded as the example below:\nv = [G0 G1 G2] [ W \u20320 0 0 0 W \u20321 0 0 0 W \u20322 ][ W0 0 0 0 W1 0 0 0 W2 ]G\u221210G\u221211 G\u221212  v Gi and G\u22121i are the generative focus matrix of a component i and its inverse respectively. Wi and W \u2032i are the forward content mapping matrix of a component i and the corresponding generative mapping matrix. To satisfy Equation 3.3, we require that\nx = W \u2032iWix for any x = G \u22121 i v (3.5)\nhence,\nv = GG\u22121v (3.6)\nIf we limit ourselves to allow each generative focus matrix Gi to only contain 0 or 1, and to only be formed by a combinatorial basis shuffling of the identity such that GG\u22121 = I , it can be implied that each individual Gi must not intersect one another. From here, we can see that there are at least as many settings as the factorials of the dimension of v.\nExample 3.2. As an alternative of Example 3.1, if we fix G to I , the number of valid settings depends on the choice of individual Wi that satisfies W \u2032W v = v. The task is left for us to choose the bases of each individualWi such that they are not correlated with those of the other components.\nExample 3.3. In a convolutional neural network with a pooling layer [23], the generative focus is always the residue of the pooling layer that satisfies variance preservation.\nW \u2217 v pool\u2212\u2212\u2192 (h, g) The generative focus here indicates from where the pooling result has taken the input. If W has the inverse W \u2032 such that W \u2032\u2217W = \u03b4, then we can have\nv = W \u2032\u2217(GG\u22121(W \u2217 v)) where for each component i\nhi = Gi(W \u2217 v) The matrix Gi contains the spatial information of g.\nFor a multilayer implementation of the model, the mechanic of components and focuses should stay at the top of the stack as the executive function that controls thought. We let the lower layers to act as the non-linear transfer function between the bottommost visible states and the topmost hidden states in the components."}, {"heading": "3.4 Learning while executing", "text": "Thoughts are creative, and yet no one but ourselves can teach us to think with nothing but executing input sequences as the examples. A good thinking model should allow learning while executing.\nLike other machine learning paradigms, the model works in two phases: executing and training. The difference is in our model these phases both use the same execution path with sample sequences of visible states as the only inputs. The system that implements the model should learn to generate the sequences and also generalize them. We are allowed, however, to devise specials of such sequences especially for the sake of training.\nWe impose that any machine can learn while executing if a) the learning happens along the path of executing, and b) the learning happens in the direction of executing. These are the conditions for learning while executing. This type of training prohibits more than one step of the back propagation through time algorithm [24] that is used to train recurrent neural networks. Though our model could definitely receive benefit from having some kind of a long-term guide especially for training the selective focus.\nDuring the training phase of our model, we advice to train first the content parameters of the components."}, {"heading": "3.4.1 Content training", "text": "The content mapping parameters of each component can be trained unsupervisedly with data supplied by an initial selective focus and a generative focus and specially designed visible state sequences to leverage them.\nLet us consider a linear executing step of the model.\nv = GW \u2032WSv \u2200v (3.7)\nIn the manner similar to Equation 3.4, S is a collection of the selective focuses of all components. v represents a collection of past visible states.\nAlso for each component according to Equation 3.3,\nSiv = W \u2032 iWiSiv \u2200i, v (3.8)\nMerging executing equations yields\nv = GSv (3.9)\nEquation 3.9 suggests how to design the training sequences, i.e., the next step visible state v in the sample sequence should somehow represent the aggregation of all selected information from the past Sv according to G.\nGiven a focus f = (s, g), each component will passively generate a hidden state with the current parameter values and then produce a visible state. The process of learning can utilize this path:\nSv tie\u2212\u2212\u2192WSv = h tie\u2212\u2212\u2192W \u2032h = G\u22121v\na tie\u2212\u2212\u2192 b denotes a supervised learning that learns mapping from a to b, following its direction. We can see that learning of the content parameters do not break the learning while executing conditions."}, {"heading": "3.4.2 Focus training", "text": "This section shows that there is a possibility to train our model\u2019s focuses while satisfying the learning-while-executing conditions.\nThe generative focus is straightforward to train since it can be obtained straightforwardly by content optimization. Equation 3.9 implies that we have to choose the generative focus G of all components such that their combined content best matches v, fulfilling the equation. Or in a convolutional network, Example 3.3 shows that the generative focus g of a component can be extracted along with the content while training the content parameters. Then we can immediately tie the recent thoughts with it.\nvt, vt\u22121, ft\u22121, . . . , vt\u2212\u03c4 , ft\u2212\u03c4 tie\u2212\u2212\u2192 g\nOnce the content parameters and the generative focus have been trained, the selective focus can be trained by activate-and-tie mechanism with optionally reinforcement learning as a guide [25, 26]. If the randomly chosen selective focus can allow the model to predict the next step visible state in the given sample sequence, according to Equation 3.9, the activation of that focus is subsequently enhanced. We will show in the next section that with the presence of memory mechanism this can be made easier."}, {"heading": "3.5 Extension", "text": ""}, {"heading": "3.5.1 Memory component", "text": "Memory is the actual implementation of a component to allow its selective focus to fetch a visible state from the past. The selective focus can either be in the form of a) a temporal cue that contains the location in time relative to the present, or b) a part of a previous hidden state that allows us to fetch the residue information of that state. In the latter case, the memory acts as a most-recent-hash that only allows the latest content associated with a cue the be retrieved. Although, a variance filling system such as the belief network, mentioned in Section 2.3.2, would serve this purpose, this nature of the model grants us the option to use other types of memory implementation such as the exact hash table, which is free from the iterative-based training paradigm. The memory does not have to generalize to unseen hidden states; it only has to maintain the association between the given hidden states and their residue information, and we let the lower distributive representation system to handle the generalization of the focus mechanism and thought generation.\nWhen the selective focus is a part of some hidden state, we can use this fact to help training it. Example 3.4. During a focus training phase, we can design a sample sequence so that the focus of the immediate previous step can be trained one-by-one. Given an initial selective focus s\u2032 that always points to the current visible state, we can derive a hidden state that contains the information of the supposed selective focus for the previous step together with its residue:\nvt \u2212\u2192s\u2032t st\u22121|r\nWe can then tie past states to the produced focus:\nvt\u22121, vt\u22122, ft\u22122, . . . , vt\u2212\u03c4 , ft\u2212\u03c4 tie\u2212\u2212\u2192 st\u22121"}, {"heading": "3.5.2 Context component", "text": "Section 2.5 discusses the possibility of having an alternative representation model to pass on temporal conditions between steps while satisfying internal consistency. This suggests that sometimes\nthe model must be allowed to carry extra degrees of freedom between steps of a thought sequence in the form of contexts. This is in order to represent the dynamic of some applications, to call a few, generating a sequence of music with a fixed theme, modeling a world object that changes its appearance while moving, or addressing the transformation part of the environment according to Newton\u2019s postulation.\nInstead of allowing direct recursive links between steps, we can use components to pass on contexts. We can consider a context component as a special component whose selective focus directly transforms a visible state to the component\u2019s content. For example, the model can cache the visible state of the immediate previous step in a context component for current use.\nDespite the flexibility provided by contexts, there can be a model without context which is, in terms of complexity, equivalent to the context counterpart. When a context and its origin visible state are the alternative representation of each other, we can always replace a context component\u2019s direct transformation with a normal selective focus mechanism that fetches context-equivalent data from predefined locations in a thought sequence that hold them. We shall use this fact to facilitate the elaboration of the proof of our model\u2019s complexity."}, {"heading": "3.5.3 External inputs", "text": "In some applications, inputs of the model are not only those generated in the sequence but also those received externally during the execution. Cabessa et al. presents a theoretical framework of Super-Turing machines [27]. They are interactive Turing machines capable to handle external inputs during program execution. We, on the other hand, do not treat external inputs as separated entities but rather a part of visible states that are generated by external mechanisms. The augmented visible state (v, u) therefore comprises of the part v that is generated by the model and the other part u that is written onto the state by the environment. The augmented part should seamlessly work with the focuses the way the normal visible state does."}, {"heading": "4 The model as a universal simulator", "text": "We use computers to achieve much, ranging from calculating the total of a shopping cart to putting men on the moon. They are also potent to be used as simulators, simulating the trajectories of robots or computing the motion of planets, for example, with limited versatility but the accuracy not less than that of our brains. Because in theory we can regard computers as universal Turing machines [8], perhaps to prove that our model can sustain the thought process it is to show that the model can simulate any Turing machine as such.\nThere have been many efforts to relate neural networks to Turing machines. Take Siegelmann and Sontag\u2019s work [28], for example, as one among the originally cited. And during the time we compose this work, two of such stand prominent among the hype in deep learning. Graves et al. presented neural Turing machines, which were carefully designed using Long short-term memory (LSTM) [6,29], and were tested to complete a few algorithmic tasks. Zaremba and Sutskever further extended the work with various of techniques, notably reinforcement learning, to address the mechanism of Turing machine heads [26].\nTuring machine heads are comparable to our selective focuses in a sense when considered the ability to fetch and utilize information from the past. Our model also bears a resemblance to LSTM in this very aspect. While Graves et al. used LSTM to build Turing machines from neural networks, the work is not a proof that LSTM by itself can simulate any Turing machine, but rather the use of LSTM and other neural networks to implement Turing machine components that could. This paper, starting from the concept of alternative representation, develops the underlying theory that guarantees the generalization to unseen inputs, and integrates the concept of focus to allow the model to manipulate information in a way that resembles a Turing machine\u2019s behavior. If we show that our model can simulate any Turing machine, we could say that this work bridges the gap between LSTM and Turing machines, providing another evidence that perhaps a neural network can also be viewed as a universal Turing machine.\nBefore we go to our proof, we consider another simple lemma that relates Turing machine\u2019s symbols with our state representations.\nLemma 4.1. There always exists a set of alternative symbols for a Turing machine\u2019s transition function,\n(s, th)\u2192 (s\u2032, t\u2032h, h\u2032)\nthat satisfies the non-sharing property from any current state s and the tape content th associated with any current head h to a new state s\u2032, a new head location h\u2032, or a new content t\u2032h to be written over the current head.\nSince the transition function of a Turing machine is one-directional and deterministic, we can readily implement it in a system with the non-sharing property. Proposition 3.1 suggests that we can potentially use a multi-layer implementation to generatively map any current state and the tape content associated with any current head to a new state, a new head location, and a new content to be written over the current head. Here the lemma certifies that we can always find a set of alternative symbols that satisfies Equation 3.2, and now we are ready to show example algorithms for simulating a Turing machine on our model with, of course, their complexity analysis."}, {"heading": "4.1 An algorithm", "text": "When running a program on a Turing machine, the dynamic of the program during the execution time prevents us from diverting the focus to directly point to the Turing machine\u2019s head location. We can resort to search for the current head content, which is generatively produced somewhere in the visible state sequence. At the start of each Turing machine step, using only a fixed number of recent thoughts the algorithm makes the model looks back into the sequence to resolve the current head content.\nConsider a Turing machine\u2019s transition function, (s, th) \u2192 (s\u2032, t\u2032h, h\u2032). Since our original model does not have memory, it is required to write down all the symbols onto the visible state sequences. Let each square bracket represents a visible state of our model\u2019s sequence. Here is a portion of the sequence involved in one Turing machine step: [. . . ,m\u22121] , [ \u03b2, s, h, (th\u22121 , h\u22121), th\u22121 , \u2217, \u2217 ] , [ \u03b1, s, h, \u2217, hf\u22121 , thf\u22121 , f\u22121, 1 ] , . . . , [\u03b1, s, h, \u2217, h, th, f\u2212m,m] ,\nThe sequence comprises of a pivot step, tagged with \u03b2, and several search steps, tagged with \u03b1. At the pivot step, s is the current state, and h is the current head. The parenthesis in the pivot groups the new content of the previous head location with the head itself. Next in the step is the yet-to-beresolved th\u22121 content for the current head. To resolve this value, the search steps will jump from one pivot step to another looking for the head\u2019s content in the parentheses. Each search step, starting with \u03b1, carries s, h to be used to compute the symbols of the next Turing machine step at the end of the search. The search progresses by updating these four parameters: the found head, the content of the found head, the focus location of the found head, and the step counter of the search. The model keeps track of the found head to conditionally decide when to end the search. The focus location allows the model to track the current search location. The step counter allows the model to compute how many steps to jump to find the next pivot step. And it is already obvious why we keep the found head content. These four parameters allow the model to evaluate the selective focus using only the two most recent visible states. \u2217 represents a wild card symbol which we do not interest at the time it depicts. We encourage the reader to become familiar with this sequence before moving forward.\nTo generate a new visible state, the model utilizes components. Each individual component copies information from the sequence following the focus, satisfying internal consistency, and combines with that of the others to generate symbols (Lemma 4.1). For the sake of simplicity, we also use context components on this algorithm\u2019s illustration. It is not hard to see that the information keeps in each context component satisfies the non-sharing property. Therefore, we can entirely replace each context with some initial symbols at the very beginning of the sequence and a focus mechanism to fetch and combine them, without affecting the algorithm\u2019s performance (Section 3.5.2). At each search step (with \u03b1), the model decides whether to stop or continue the search and puts symbols on the components. The following describes the templates of what information will be carry in the\ncomponents:\n[ \u03b1, s, h, th\u22121 , hf\u2212m , thf\u2212m , f\u2212m,m ] \u2192\nstep tag (context) Turing machine\u2019 states (copy)\ncurrent search head (copy) current search content (copy) current search focus (context)\nsearch iteration (context)\n\u2192 new visible state\ni.e., when hf\u2212m \u2208 {h, \u03c6} the templates collect these symbols:\n\u03b2 s, h hf\u2212m thf\u2212m \u2217 \u2217 = \u03b2 s, h h th \u2217 \u2217 \u2192 [\u03b2, s\u2032, h\u2032, (t\u2032h, h), th, \u2217, \u2217]\u2192 . . .\notherwise,\n\u03b1 s, h\nhf\u2212m\u22121 thf\u2212m\u22121\nf\u2212m\u22121 = f\u2212m \u2212m\u2212m \u2212 1 m+ 1\n\u2192 [ \u03b1, s, h, \u2217, hf\u2212m\u22121 , thf\u2212m\u22121 , f\u2212m\u22121,m+ 1 ] \u2192 . . .\n\u03c6 represents an empty or any unrecognized symbol at the start of the sequence. For the first search step after a pivot step,\n[. . . ,m\u22121] , [ \u03b2, s, h, (th\u22121 , h\u22121), th\u22121 , \u2217, \u2217 ] \u2192\n\u03b1 s, h hf\u22121 thf\u22121\nf\u22121 = \u2212m\u22121 \u2212 1 1\n\u2192 [ \u03b1, s, h, \u2217, hf\u22121 , thf\u22121 , f\u22121, 1 ] \u2192 . . .\nf\u22121 is fixed to the previous pivot step. The components we show here are merely the templates of the real components. They do not correspond injectively to the symbols on the visible state, rather each symbol on the visible state derives from a mapping from some of the symbols presenting in them. For example, the new state s\u2032 would require the old state and the found head content to be generated. More importantly, the reader can verify that the identities of the symbols in each component template can be determined within at most two most recent visible states. Since the first visible state in the sequence has to be given as the input, this completes the induction for each transformation step of a Turing machine."}, {"heading": "4.1.1 Complexity analysis", "text": "Let N be the current number of Turing machine steps counting from the beginning of program execution. The worst case time complexity of the algorithm to execute the step, contributed by the search procedure, are bounded by O(N), assuming that each step is required to search to the begin of the sequence.\nWhat is the average time complexity to execute a step?\nFor a vanilla Turing machine, the head only moves left or right. Under the assumption that there is an equal probability for the head to move left or right considering all possible programs, we can plot a random walk graph of the head\u2019s locations up until the head reaches the current location (see Figure 9).\nConsider the head\u2019s location of a Turing machine after N steps of execution, the total number of paths to that point is equal to 2N = |P\u221e|+ |P0|+ |P1|+ . . .+ |PN\u22122| where |Pn| is the number of paths where the previous visits of the current location were at the n-th step, |P\u221e| is the number of paths that never visit the location.\nThe expected total of the search steps is given by\nE [search steps at N ] =\n[ 1\n2N N\u22121\u2211 n=0 (N \u2212 n)|Pn|\n] + [ N\n2N\n( 2N \u2212\nN\u22121\u2211 n=0 |Pn|\n)] (4.1)\nThe first term in Equation 4.1 is the expected number of steps from the last visit of the current location. |Pn| can be computed by counting the number of paths, on the left and the right of the Pascal\u2019s semi-triangle, avoiding the current location (see Figure 9). It is given by the Catalan number:\n|Pn| = 2C(N \u2212 n\u2212 2, 0)2n\nC(2m, 0) = 1\nm+ 1\n( 2m\nm ) C(2m+ 1, 0) = 0\nusing Stirling approximation on the Catalan number, we can compute the asymptote: |Pn| = O ( 2N\n(N \u2212 n)3/2\n)\nThe asymptote of the first term is given by O( \u221a N):\n1\n2N N\u22121\u2211 n=0 (N \u2212 n)|Pn| = N\u22122\u2211 n=0 2(N \u2212 n)C(N \u2212 n\u2212 2, 0) 2 n 2N\n= N\u22122\u2211 n=0 O ( 1 (N \u2212 n)1/2 ) Because 1\n(N\u2212n)1/2 is monotonically increasing, we can find its bounding via the integral test:\u222b N\u22121 0 j (N \u2212 n+ 1)1/2 dn < N\u22122\u2211 n=0 O ( 1 (N \u2212 n)1/2 ) < \u222b N\u22121 0 k (N \u2212 n)1/2 dn \u2203j, k \u2208 R+\n\u22122j(N \u2212 n+ 1)1/2 \u2223\u2223\u2223\u2223N\u22121 0 < N\u22122\u2211 n=0 O ( 1 (N \u2212 n)1/2 ) < \u22122k(N \u2212 n)1/2 \u2223\u2223\u2223\u2223N\u22121 0\nO( \u221a N) < N\u22122\u2211 n=0 O ( 1 (N \u2212 n)1/2 ) < O( \u221a N)\nThe second term in Equation 4.1 accounts for the paths where the current head location has never been visited before. In this case, the model has to search the entire sequence yielding the asymptote of O(N) for the term. Fortunately, we can make a modification to the algorithm to get rid of this term by adding two extra copy components that keeps track of the leftmost and rightmost bounds of\nall the visited head locations. When the head location exceeds one of the bounds, the model updates the bound, skips the search and immediately writes the empty symbol as the head\u2019s content. This augmentation improves the average time complexity of the algorithm to O( \u221a N).\nCan we do better?"}, {"heading": "4.2 A constant time algorithm", "text": "The idea of this algorithm is to let every step maintains the step counts from the current to the previous left of the current head location, to the previous right of it, and to itself. Let us consider now three steps of a visible state sequence from the start of a program. From step n = 0, the head moves left, left, then right, producing the following sequence:\n[ Tag, s, h, th, t\u2032h, dn\u22121\u2192n, ln, cn, rn ]\n[ \u03b2, s0, h0, th0 , \u2217, \u2217, 0, 0, 0 ] , [ \u03b3, s1, h1, \u2217, t\u2032h0 , L, \u2217, \u2217, \u2217 ] , [ \u03b1, \u2217, \u2217, \u2217, \u2217, \u2217, \u2217, \u22121, \u22121 ] , [ \u03b2, s1, h1, th1 , \u2217, \u2217, \u22121, \u22121, \u22121 ] , [ \u03b3, s2, h2, \u2217, t\u2032h1 , L, \u2217, \u2217, \u2217 ] , [ \u03b1, \u2217, \u2217, \u2217, \u2217, \u2217, \u2217, \u22122, \u22121 ] , [ \u03b2, s2, h2, th2 , \u2217, \u2217, \u22122, \u22122, \u22121 ] , [ \u03b3, s3, h3, \u2217, t\u2032h2 , R, \u2217, \u2217, \u2217 ] , [ \u03b1, \u2217, \u2217, \u2217, \u2217, \u2217, \u22121, \u22122, \u2217 ] , [ \u03b2, s3, h3, th3 , \u2217, \u2217, \u22121, \u22122, \u22123 ] , [ . . . . . . . . . . . . . . . . . . . . . . . . . . . ] ,\ndn\u22121\u2192n represents a head direction from step n\u2212 1 to n. The previous step counts for the previous left, right, and the current position are ln rn cn respectively. A Turing machine\u2019s step starts at \u03b2 with its states and the previous step counts. At \u03b3 step, the next Turing machine\u2019s states are produced. At \u03b1 step, the previous step counts to the current head location and one of left or right are produced depending on the head direction. At the next \u03b2, our model finishes the Turing\u2019s step with the fetched head content and the last step count. The generation of the step counts follows these rules,\ncn = { ln \u2212 1 if dn\u22121\u2192n = R rn \u2212 1 otherwise\nln = { cn + l(n+cn) if dn\u22121\u2192n = L \u22121 otherwise\nrn = { cn + r(n+cn) if dn\u22121\u2192n = R \u22121 otherwise\nThe component templates are as follow:\n[Tag, s, h, th, t\u2032h, dn\u22121\u2192n, ln, cn, rn]\u2192\nstep tag (context) Turing machine\u2019 states (copy) previous head content (copy)\nhead direction (context) step counts (context)\n\u2192 new visible state\nAgain, the reader can verify that the identities of the symbols in each component template can be determined within this time the most recent visible state. This completes the algorithm. And because each Turing machine\u2019s step only requires two extra steps to be simulated in our model. We can conclude that the time complexity of this algorithm is O(1).\nThe existence of this algorithm by itself allows us to state the main theorem of this paper. Theorem 4.2. The model with an arbitrary depth and a finite number of components can simulate any vanilla Turing machine and only be slowed by within a constant factor compared to the machine it simulates."}, {"heading": "4.3 With memory components", "text": "We can augment the components with the most-recent-hash memory mechanism like one we introduced in Section 3.5.1. This enhancement allows the tape content associated with any head location, not just ones limited by the left-right-only movement of the vanilla Turing machines, to be read or written in a single step. An execution step of a Turing machine simulated on our model with the most-recent-hash memory is manifested by these expressions:\n[\u03b2, h, \u2217, s]\u2192 \u03b1\nh read\u2212\u2212\u2212\u2192 th s\n\u2192 [\u03b1, h\u2032, t\u2032h, s\u2032]\u2192 \u03b2\nt\u2032h write\u2212\u2212\u2212\u2192 h s\u2032 \u2192 [\u03b2, h\u2032, t\u2032h, s\u2032]\u2192 . . .\nFor each Turing machine\u2019s step, our model only needs one extra step to complete the read-write cycle with one memory component to read and write the tape contents. It infers the head content th at the \u03b2 \u2192 \u03b1 step and memorizes the new head content t\u2032h at the \u03b2 \u2192 \u03b1 step, both while focusing on the head symbol at the \u03b2 step. The efficiency is readily apparent."}, {"heading": "5 Discussion", "text": "To state it one last time, this paper presents a theoretical thinking model. The model consists of components where all combine the information that each fetches from some part of the former thoughts in order to creatively compose a new one. Each component has the ability to invariantly extract information from any when and where with its selective focus, which in turn depends on and is driven only by the most recent thoughts. The combination mechanism is governed by the concept of internal consistency. Internal consistency is especially useful for a system that operates through time, has the capacity to alternatingly and repeatedly recognize and generate data, and requires that the newly generated data are relevant to the cause from which they are originated, such as the thinking process.\nThe explicit use of the focuses brings an advantage to our model. It happens that, in our world, physical interaction of between objects tend to correlate their spatio-temporal locations. Because of this, physical mechanic can be hypothetically simulated using a hierarchy of execution within a system. Our model allows the focus mechanism to handle a higher order of execution, while having each component handles the transformation within its responsible detail. This way we might be able to achieved generalization with limited training data. It is also a means to reduce the hypothesis variance of the model\u2019s implementation while, as much as possible, preserving the degrees of freedom of thinking.\nThe problem this model tries to solve is in fact the inverse of filtering in signal processing. In filtering we try to discover a recognition function, or simply an estimate in the context of signal process, while taking the generative function, or the input control, and process noise into account. In this work however, we attempt to find the generative function that is simply the perfect inverse of the recognition function. Furthermore the approach of filtering has also been widely applied to find\ndistributed weights for combining series of measurements that signify the same information proportionally to each measurement\u2019s confidential quantity. While in our model however, the distributed weights are the mapping from hidden states to visible states, and are formed by training with the only limitation to preservation of variance.\nThroughout the length of this work, we have discussed several models. The first model that we introduce in Section 2 is the hidden-visible bipartite model with the hidden and visible state representations that are always the alternative representations of each other. Applications that involve this basic model include factor analysis and some knowledge representation where preservation of variance is required. Adding the temporal support to the first model gives rise to the temporal version of it (Section 2.5). The possibility to further enhance the temporal model with indefinitely long memory retention has eventually led the discussion to our thinking model in Section 3. We dedicate Section 4 to show that the model can simulate any Turing machine with the computational complexity rivaling that of a universal Turing machine. Table 1 summarizes all the models we have discussed in this work.\nTo make an intelligence machine with the generative capability, one essentially requires a decent way to internally represent the world. Deep learning and techniques such as sparse representation are particularly designed to address this requirement. In this paper, we present another important factor that allows generalization guarantee of newly generated data to be \u201crelevant\u201d, at least to all the data accumulated during the course of the machine\u2019s execution. Internal consistency is not merely a hypothetical trait of intelligent machines when they manipulate data. As in humans, we believe that when we factorize knowledge into parts to identify the similarity and distinction in the information and to be able to combine with other knowledge to form a new idea is when we truly understand something. This conviction serves as the very motive of this work."}], "references": [{"title": "Computing machinery and intelligence", "author": ["Alan M Turing"], "venue": "Mind, pages 433\u2013460,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1950}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Two-tape simulation of multitape turing machines", "author": ["Fred C Hennie", "Richard Edwin Stearns"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1966}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": "MIT press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Independent component analysis: algorithms and applications", "author": ["Aapo Hyv\u00e4rinen", "Erkki Oja"], "venue": "Neural networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Reconstructing visual experiences from brain activity evoked by natural movies", "author": ["Shinji Nishimoto", "An T Vu", "Thomas Naselaris", "Yuval Benjamini", "Bin Yu", "Jack L Gallant"], "venue": "Current Biology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Ica with reconstruction cost for efficient overcomplete feature learning", "author": ["Quoc V Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["Erkki Oja"], "venue": "Journal of mathematical biology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1982}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "An artificial neural network for spatiotemporal bipolar patterns: Application to phoneme classification", "author": ["Les E Atlas", "Toshiteru Homma", "Robert J Marks II"], "venue": "In Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Modeling human motion using binary latent variables", "author": ["Graham W Taylor", "Geoffrey E Hinton", "Sam T Roweis"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "The principia: mathematical principles of natural philosophy", "author": ["Isaac Newton"], "venue": "Univ of California Press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "The utility driven dynamic error propagation network", "author": ["AJ Robinson", "Frank Fallside"], "venue": "University of Cambridge Department of Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1987}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "The computational power of interactive recurrent neural networks", "author": ["J\u00e9r\u00e9mie Cabessa", "Hava T Siegelmann"], "venue": "Neural Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Turing computability with neural nets", "author": ["Hava T Siegelmann", "Eduardo D Sontag"], "venue": "Applied Mathematics Letters,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1991}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Perhaps most of the modern attempts to model the process of thinking are arguably inspired by Alan Turing\u2019s work [2], which addresses many philosophical and technical questions about the machines that can think.", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "This problem can recently be addressed by the re-popularized concept of deep learning [3\u20135].", "startOffset": 86, "endOffset": 91}, {"referenceID": 2, "context": "This problem can recently be addressed by the re-popularized concept of deep learning [3\u20135].", "startOffset": 86, "endOffset": 91}, {"referenceID": 3, "context": "This problem can recently be addressed by the re-popularized concept of deep learning [3\u20135].", "startOffset": 86, "endOffset": 91}, {"referenceID": 4, "context": "The success of the recurrent models for time-related tasks shows that we might use them as a prototype of our model [6, 7].", "startOffset": 116, "endOffset": 122}, {"referenceID": 5, "context": "The success of the recurrent models for time-related tasks shows that we might use them as a prototype of our model [6, 7].", "startOffset": 116, "endOffset": 122}, {"referenceID": 6, "context": "Furthermore to show that the complexity of the model could rival that of our mind, we could try to relate the model with the behavior of a universal Turing machine [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "For a generative system that tries to model the world, it usually consists of two sides of data representation: one that represents the observed, visible states of the world, and the other that represents the hidden states [9].", "startOffset": 223, "endOffset": 226}, {"referenceID": 8, "context": "Independent bases are desirable for generative models, since we can represent the distributions of the world efficiently as the products of distributions of individual bases [10]: P (X0, X1, .", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "suggested a neat mathematicallyproven training technique to regularize this behavior in neural networks by introducing adversarial networks [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 2, "context": "Learning hidden representation while maximizing the reconstruction chance aligns with the goal of autoencoder training [4, 12].", "startOffset": 119, "endOffset": 126}, {"referenceID": 10, "context": "Learning hidden representation while maximizing the reconstruction chance aligns with the goal of autoencoder training [4, 12].", "startOffset": 119, "endOffset": 126}, {"referenceID": 11, "context": "Many real life problems belong to such category, to state a few, giving an artistic style and an abstract shape, how to create a detailed painting containing the shape with the style [13, 14] or reconstructing visual images from brain reading signals [15].", "startOffset": 183, "endOffset": 191}, {"referenceID": 12, "context": "Many real life problems belong to such category, to state a few, giving an artistic style and an abstract shape, how to create a detailed painting containing the shape with the style [13, 14] or reconstructing visual images from brain reading signals [15].", "startOffset": 183, "endOffset": 191}, {"referenceID": 13, "context": "Many real life problems belong to such category, to state a few, giving an artistic style and an abstract shape, how to create a detailed painting containing the shape with the style [13, 14] or reconstructing visual images from brain reading signals [15].", "startOffset": 251, "endOffset": 255}, {"referenceID": 14, "context": "Reconstruction ICA is a good candidate for training [16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "The transpose acts as a good learning regulator originally presented in Oja rule\u2019s [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "We choose to fulfill the role of the memory with a belief network stacked on top of the hidden units due to its simplicity [3] among other generative techniques.", "startOffset": 123, "endOffset": 126}, {"referenceID": 16, "context": "We can use the rectified linear activation function as a means to achieve this [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "We can use this fact with a finite number of basis functions to represent almost any bounded complex transformation according to the universal approximation theorem [19].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "A variation of the technique can be used in convolutional networks [20] as well:", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "Recurrent neural networks belong to a class of expressive architectures that we can use to build generative temporal systems for modeling temporal sequences [21].", "startOffset": 157, "endOffset": 161}, {"referenceID": 20, "context": "Sir Isaac Newton had postulated in his seminal work [22] that every surrounding change in the environment comes from either motion or transformation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "In a convolutional neural network with a pooling layer [23], the generative focus is always the residue of the pooling layer that satisfies variance preservation.", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "This type of training prohibits more than one step of the back propagation through time algorithm [24] that is used to train recurrent neural networks.", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "Once the content parameters and the generative focus have been trained, the selective focus can be trained by activate-and-tie mechanism with optionally reinforcement learning as a guide [25, 26].", "startOffset": 187, "endOffset": 195}, {"referenceID": 24, "context": "Once the content parameters and the generative focus have been trained, the selective focus can be trained by activate-and-tie mechanism with optionally reinforcement learning as a guide [25, 26].", "startOffset": 187, "endOffset": 195}, {"referenceID": 25, "context": "presents a theoretical framework of Super-Turing machines [27].", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "Because in theory we can regard computers as universal Turing machines [8], perhaps to prove that our model can sustain the thought process it is to show that the model can simulate any Turing machine as such.", "startOffset": 71, "endOffset": 74}, {"referenceID": 26, "context": "Take Siegelmann and Sontag\u2019s work [28], for example, as one among the originally cited.", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "presented neural Turing machines, which were carefully designed using Long short-term memory (LSTM) [6,29], and were tested to complete a few algorithmic tasks.", "startOffset": 100, "endOffset": 106}, {"referenceID": 27, "context": "presented neural Turing machines, which were carefully designed using Long short-term memory (LSTM) [6,29], and were tested to complete a few algorithmic tasks.", "startOffset": 100, "endOffset": 106}, {"referenceID": 24, "context": "Zaremba and Sutskever further extended the work with various of techniques, notably reinforcement learning, to address the mechanism of Turing machine heads [26].", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "This paper presents a theoretical, idealized model of the thinking process with the following characteristics: 1) the model can produce complex thought sequences and can be generalized to new inputs, 2) it can receive and maintain input information indefinitely for the generation of thoughts and later use, and 3) it supports learning while executing. The crux of the model lies within the concept of internal consistency, or the generated thoughts should always be consistent with the inputs from which they are created. Its merit, apart from the capability to generate new creative thoughts from an internal mechanism, depends on the potential to help training to generalize better. This is consequently enabled by separating input information into several parts to be handled by different processing components with a focus mechanism to fetch information for each. This modularized view with the focus binds the model with the computationally capable Turing machines. And as a final remark, this paper constructively shows that the computational complexity of the model is at least, if not surpass, that of a universal Turing machine.", "creator": "LaTeX with hyperref package"}}}