{"id": "1511.03908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Learning Human Identity from Motion Patterns", "abstract": "holsbeeck We semi-mythical present big-box a semper large - gourmets scale michelmore study, dixler exploring billionths the baginton capability sm\u00e5 of temporal deep bearskin neural networks burian in interpreting natural human kinematics and inset introduce the 92-76 first method iolani for gopperth active biometric morr authentication shelah with mobile v-1710 inertial sensors. 90-point At rahlves Google, we have mediated created glucosamine a clitheroe first - of - greasy its - kind pitlick dataset left-side of prawer human denars movements, 6809 passively collected kaster by multiyear 1500 sawmilling volunteers cornrows using vaticanus their smartphones daily over splat several low-grade months. sluices We (delissen 1) compare strassman several neural 1453 architectures for vishwavidyalaya efficient learning taza of mid-pacific temporal nepalnews.com multi - modal data realization representations, (homophobe 2) gissler propose an optimized peetam shift - http://www.united.com invariant sconochini dense convolutional hypnotherapy mechanism (chitterlings DCWRNN) 43.01 and (3) incorporate saturns the discriminatively - trained dynamic features in a husbandman probabilistic banister generative framework non-constituency taking into i-vtec account temporal characteristics. sebring Our 11.11 results interparty demonstrate, 5.14 that human ngl kinematics kreesha convey important information about user identity and 95.25 can britsch serve change as babahoyo a potted valuable 4.23 component wholesale of vernita multi - 158.2 modal authentication fioravanti systems.", "histories": [["v1", "Thu, 12 Nov 2015 14:48:53 GMT  (1598kb)", "https://arxiv.org/abs/1511.03908v1", "10 pages, 6 figures, 2 tables"], ["v2", "Tue, 8 Dec 2015 15:23:06 GMT  (1427kb)", "http://arxiv.org/abs/1511.03908v2", "10 pages, 6 figures, 2 tables"], ["v3", "Wed, 9 Dec 2015 01:59:58 GMT  (2044kb)", "http://arxiv.org/abs/1511.03908v3", "10 pages, 6 figures, 2 tables"], ["v4", "Thu, 21 Apr 2016 16:04:00 GMT  (3357kb,D)", "http://arxiv.org/abs/1511.03908v4", "10 pages, 6 figures, 2 tables"]], "COMMENTS": "10 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["natalia neverova", "christian wolf", "griffin lacey", "lex fridman", "deepak chandra", "brandon barbello", "graham taylor"], "accepted": false, "id": "1511.03908"}, "pdf": {"name": "1511.03908.pdf", "metadata": {"source": "CRF", "title": "Learning Human Identity from Motion Patterns", "authors": ["Natalia Neverova", "Christian Wolf", "Griffin Lacey", "Lex Fridman", "Deepak Chandra", "Brandon Barbello", "Graham Taylor"], "emails": ["firstname.surname@liris.cnrs.fr", "laceyg@uoguelph.ca,", "gwtaylor@uoguelph.ca", "fridman@mit.edu", "dchandra@google.com,", "bbarbello@google.com."], "sections": [{"heading": null, "text": "Index Terms\u2014Authentication, Biometrics (access control), Learning, Mobile computing, Recurrent neural networks.\nI. INTRODUCTION\nFor the billions of smartphone users worldwide, remembering dozens of passwords for all services we need to use and spending precious seconds on entering pins or drawing sophisticated swipe patterns on touchscreens becomes a source of frustration. In recent years, researchers in different fields have been working on creating fast and secure authentication alternatives that would make it possible to remove this burden from the user [29], [6].\nHistorically, biometrics research has been hindered by the difficulty of collecting data, both from a practical and legal perspective. Previous studies have been limited to tightly constrained lab-scale data collection, poorly representing real world scenarios: not only due to the limited amount and variety of data, but also due to essential self consciousness of participants performing the tasks. In response, we created an unprecedented dataset of natural prehensile movements (i.e. those in which an object is seized and held, partly or wholly, by the hand [20]) collected by 1,500 volunteers over several months of daily use (Fig. 1).\nApart from data collection, the main challenges in developing a continuous authentication system for smartphones are (1) efficiently learning task-relevant representations of noisy\nManuscript created: April 22, 2016. N. Neverova and C. Wolf are with INSA-Lyon, LIRIS, UMR5205, F-69621, Universit\u00e9 de Lyon, CNRS, France. E-mail: firstname.surname@liris.cnrs.fr G. Lacey and G. Taylor are with the School of Engineering, University of Guelph, Canada. E-mail: laceyg@uoguelph.ca, gwtaylor@uoguelph.ca L. Fridman is with MIT, Boston, USA. Email: fridman@mit.edu D. Chandra and B. Barbello are with Google, Mountain View, USA. Email: dchandra@google.com, bbarbello@google.com. This work was done while N. Neverova, G. Lacey and L. Fridman were at Google, Mountain View, USA.\ninertial data, and (2) incorporating them into a biometrics setting, characterized by limited resources. Limitations include low computational power for model adaptation to a new user and for real-time inference, as well as the absence (or very limited amount) of \u201cnegative\u201d samples.\nIn response to the above challenges, we propose a noncooperative and non-intrusive method for on-device authentication based on two key components: temporal feature extraction by deep neural networks, and classification via a probabilistic generative model. We assess several popular deep architectures including one-dimensional convolutional nets and recurrent neural networks for feature extraction. However, apart from the application itself, the main contribution of this work is in developing a new shift-invariant temporal model which fixes a deficiency of the recently proposed Clockwork recurrent neural networks [18] yet retains their ability to explicitly model multiple temporal scales."}, {"heading": "II. RELATED WORK", "text": "Exploiting wearable or mobile inertial sensors for authentication, action recognition or estimating parameters of a particular activity has been explored in different contexts. Gait analysis has attracted significant attention from the biometrics community as a non-contact, non-obtrusive authentication method resistant to spoofing attacks. A detailed overview and benchmarking of existing state-of-the-art is provided in [23]. Derawi et al. [9], for example, used a smartphone attached to the human body to extract information about walking cycles, achieving 20.1% equal error rate.\nThere exist a number of works which explore the problem of activity and gesture recognition with motion sensors, including methods based on deep learning. In [12] and [7], exhaustive\nar X\niv :1\n51 1.\n03 90\n8v 4\n[ cs\n.L G\n] 2\n1 A\npr 2\n01 6\n2 overviews of preprocessing techniques and manual feature extraction from accelerometer data for activity recognition are given. Perhaps most relevant to this study is [25], the first to report the effectiveness of RBM-based feature learning from accelerometer data, and [5], which proposed a data-adaptive sparse coding framework. Convolutional networks have been explored in the context of gesture and activity recognition [11], [34]. Lefebvre et al. [19] applied a bidirectional LSTM network to a problem of 14-class gesture classification, while Berlemont et al. [4] proposed a fully-connected Siamese network for the same task.\nWe believe that multi-modal frameworks are more likely to provide meaningful security guarantees. A combination of face recognition and speech [17], and of gait and voice [32] have been proposed in this context. Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21]."}, {"heading": "III. A GENERATIVE BIOMETRIC FRAMEWORK", "text": "Our goal is to separate a user from an impostor based on a time series of inertial measurements (Fig. 1). Our method is based on two components: a feature extraction pipeline which associates each user\u2019s motion sequence with a collection of discriminative features, and a biometric model, which accepts those features as inputs and performs verification. While the feature extraction component is the most interesting and novel aspect of our technique, we delay its discussion to Section IV. We begin by discussing the data format and the biometric model."}, {"heading": "A. Movement data", "text": "Each reading (frame) in a synchronized raw input stream of accelerometer and gyroscope data has the form {ax, ay, az, \u03c9x, \u03c9y, \u03c9z} \u2208 R6, where a represents linear acceleration, \u03c9 angular velocity and x, y, z denote projections on corresponding axes, aligned with the phone. There are two important steps we take prior to feature extraction. Obfuscation-based regularization \u2014 it is important to differentiate between the notion of \u201cdevice\u201d and \u201cuser\u201d. In the dataset we collected (Section VI), each device is assigned to a single user, thus all data is considered to be authentic. However, in real-world scenarios such as theft, authentic and imposter data may originate from the same device.\nIn a recent study [8], it was shown that under lab conditions a particular device could be identified by a response of its motion sensors to a given signal. This happens due to imperfection in calibration of a sensor resulting in constant offsets and scaling coefficients (gains) of the output, that can be estimated by calculating integral statistics from the data. Formally, the measured output of both the accelerometer and gyroscope can be expressed as follows [8]:\na = ba + diag(\u03b3a)a\u0303, \u03c9 = b\u03c9 + diag(\u03b3\u03c9)\u03c9\u0303, (1)\nwhere a\u0303 and \u03c9\u0303 are real acceleration and angular velocity vectors, ba and b\u03c9 are offset vectors and \u03b3a and \u03b3\u03c9 represent gain errors along each coordinate axes.\nTo partially obfuscate the inter-device variations and ensure decorrelation of user identity from device signature in the learned data representation, we introduce low-level additive (offset) and multiplicative (gain) noise per training example. Following [8], the noise vector is obtained by drawing a 12- dimensional (3 offset and 3 gain coefficients per sensor) obfuscation vector from a uniform distribution \u00b5 \u223c U12[0.98, 1.02].\nData preprocessing \u2014 In addition, we extract a set of angles \u03b1{x,y,z} and \u03d5{x,y,z} describing the orientation of vectors a and \u03c9 in the phone\u2019s coordinate system (see Fig. 1), compute their magnitudes |a| and |\u03c9| and normalize each of the x, y, z components. Finally, the normalized coordinates, angles and magnitudes are combined in a 14-dimensional vector x(t) with t indexing the frames."}, {"heading": "B. Biometric model", "text": "Relying on cloud computing to authenticate a mobile user is unfeasible due to privacy and latency. Although this technology is well established for many mobile services, our application is essentially different from others such as voice search, as it involves constant background collection of particularly sensitive user data. Streaming this information to the cloud would create an impermissible threat from a privacy perspective for users and from a legal perspective for service providers. Therefore, authentication must be performed on the device and is constrained by available storage, memory and processing power. Furthermore, adapting to a new user should be quick, resulting in a limited amount of training data for the \u201cpositive\u201d class. This data may not be completely representative of typical usage. For these reasons, a purely discriminative setting involving learning a separate model per user, or even fine-tuning a model for each new user would hardly be feasible.\nTherefore, we adapt a generative model, namely a Gaussian Mixture Model (GMM), to estimate a general data distribution in the dynamic motion feature space and create a universal background model (UBM). The UBM is learned offline, i.e. prior to deployment on the phones, using a large amount of pre-collected training data. For each new user we use a very small amount of enrollment samples to perform online (i.e. on-device) adaptation of the UBM to create a client model. The two models are then used for real time inference of trust scores allowing continuous authentication.\nUniversal background model \u2014 let y=f({x(t)})\u2208RN be a vector of features extracted from a raw sequence of prehensile movements by one of the deep neural networks described in Section IV. Probability densities are defined over these feature vectors as a weighted sum of M multi-dimensional Gaussian distributions parameterized by \u0398={\u00b5i,\u03a3i, \u03c0i}, where \u00b5i is a mean vector, \u03a3i a covariance matrix and \u03c0i a mixture coefficient:\np(y|\u0398) = M\u2211 i=1 \u03c0iN (y;\u00b5i,\u03a3i), (2)\nNi(y) = 1\u221a\n(2\u03c0)N |\u03a3i| e\u2212\n(y\u2212\u00b5i) \u2032\u03a3i \u22121(y\u2212\u00b5i)\n2 . (3)\n3 feature maps (1)\nfeature maps (2) feature maps (3) discriminative pretraining fully connected layer distribution of the extracted dynamic features\ntime\nsynchronized stream of accelerometer and gyroscope data long sequences)(\nfully connected layer\nfeature maps (1) feature maps (2) feature maps (3)\ntemporal units discriminative pretraining\ndiscriminative pretraining distribution of the extracted dynamic features time\nsynchronized stream of accelerometer and gyroscope data set of short sequences)(\nThe UBM p(y|\u0398UBM) is learned by maximising the likelihood of feature vectors extracted from the large training set using the expectation-maximisation (EM) algorithm.\nThe client model p(y|\u0398client) is adapted from the UBM. Both models share the same weights and covariance matrices to avoid overfitting from a limited amount of enrollment data. Along the lines of [26], maximum a posteriori (MAP) adaptation of mean vectors for a given user is performed. This has an immediate advantage over creating an independent GMM for each user, ensuring proper alignment between the welltrained background model and the client model by updating only a subset of parameters that are specific to the given user. In particular, given a set of Q enrollment samples {yq} from the new device, we create a client-specific update to the mean of each mixture component i as follows:\nEi({yq})= 1\nni Q\u2211 q=1 Pr(i|yq)yq, (4)\nwhere ni = Q\u2211\nq=1 Pr(i|yq), P r(i|yq) = \u03c0ipi(yq)\u2211M j=1\u03c0jpj(yq) . (5)\nFinally, the means of all Gaussian components are updated according to the following rule:\n\u00b5\u0302i = \u03b1iEi({yq}) + (1\u2212 \u03b1i)\u00b5i, where \u03b1i = ni\nni + r , (6)\nwhere r is a relevance factor balancing the background and client models. In our experiments, r is held fixed.\nScoring \u2014 given a set of samples Y={ys} from a given device, authenticity is estimated by scoring the feature vectors against the UBM and the client model, thresholding the loglikelihood ratio:\n\u039b(Y ) = log p(Y |\u0398client)\u2212 log p(Y |\u0398UBM). (7)\nAs a final step, zt-score normalization [2] is performed to compensate for inter-session and inter-person variations and reduce the overlap between the distribution of scores from authentic users and impostors."}, {"heading": "IV. LEARNING EFFECTIVE AND EFFICIENT REPRESENTATIONS", "text": "Learning effective and efficient data representations is key to our entire framework since its ability to perform in the realworld is defined by such criteria as latency, representational power of extracted features and inference speed of the feature extractor. The first two conditions are known to contradict each other as performance of a standalone feature typically grows with integration time [21].\nTwo paradigms which strike a balance between representational power and speed have dominated the feature learning landscape in recent years. These are multi-scale temporal aggregation via 1-dimensional convolutional networks Fig. 2a, and explicit modeling of temporal dependencies via recurrent neural networks Fig. 2b.\nThe former model, popular in speech recognition [13], involves convolutional learning of integrated temporal statistics from short and long sequences of data (referred to as \u201cshort-term\u201d and \u201clong-term\u201d convnets). Short-term architectures produce outputs at relatively high rate (1 Hz in our implementation) but fail to model context. Long-term networks can learn meaningful representations at different scales, but suffer from a high degree of temporal inertia and do not generalize to sequences of arbitrary length.\nRecurrent models which explicitly model temporal evolutions can generate low-latency feature vectors built in the context of previously observed user behavior. The dynamic nature of their representations allow for modeling richer temporal structure and better discrimination among users acting under different conditions. There have been a sufficiently large number of neural architectures proposed for modeling temporal dependencies in different contexts: the baseline methods compared in this work are summarized in Fig. 3c. The rest of this section provides a brief description of these models. Then, Section V introduces a new shift-invariant model based on modified Clockwork RNNs [18].\nAll feature extractors are first pretrained discriminatively for a multi-device classification task, then, following removal of the output layer the activations of the penultimate hidden layer are provided as input (which we denote, for conciseness1, by y) to the generative model described in Section III. The final outputs of the background and client models are integrated over a 30 sec window. Accordingly, after 30 sec the user is either authenticated or rejected."}, {"heading": "A. Classical RNN and Clockwork RNN", "text": "The vanilla recurrent neural network (RNN) is governed by the update equation\nh(t) = \u03c8(Uh(t\u22121) + Wx(t)), (8)\nwhere x is the input, h(t) denotes the network\u2019s hidden state at time t, W and U are feed-forward and recurrent weight matrices, respectively, and \u03c8 is a nonlinear activation function, typically tanh. The output is produced combining the hidden\n1This decision was made to avoid introducing notation to index hidden layers, as well as simplify and generalize the presentation in the previous section, where the y are taken as generic temporal features.\n4 \u2a09 \u2a09\n\u2a09\n\u2a09\nx(t)\nh(t)\nh(t 1)\nc(t 1)\nc(t)\ni(t) f (t) o(t)g (t)\n\u2a09 x(t)\nh(t 1)\n\u2a09 \u2a09\n\u2a09\n\u2a09\nh(t)\nh\u0303(t) r(t)\nz(t) z(t) 1 z(t)\ninput, x(t) x(t+1)x(t 1)\nW W W\nU U UU\nV V V h(t 1) h(t) h(t+1)\no\u0303(t 1) output, o\u0303(t) o\u0303(t+1)\n(a) (b)\ninput, x(t) x(t+1)x(t 1) x(t+2)x(t 2)\nV V V V V\nk=0\nk=1\nk=2\nh(t 1) h(t) h(t+1) WWW W W\nU UUU UU\nh(t+2)h (t 2)\no\u0303(t 2) o\u0303(t 1) output, o\u0303(t) o\u0303(t+1) o\u0303(t+2)\n(c)\nFig. 3. Temporal models: (a) a basic recurrent unit; (b) an LSTM unit [14]; (c) Clockwork RNN [18] with 3 bands and a base of 2; Increasing k indicates lower operating frequency. Grey color indicates inactivity of a unit..\nstate in a similar way, o(t) = \u03c6(Vh(t)), where V is a weight matrix.\nOne of the main drawbacks of this model is that it operates at a predefined temporal scale. In the context of free motion which involves large variability in speed and changing intervals between typical gestures, this may be a serious limitation. The recently proposed Clockwork RNN (CWRNN) [18] operates at several temporal scales which are incorporated in a single network and trained jointly. It decomposes a recurrent layer into several bands of high frequency (\u201cfast\u201d) and low frequency (\u201cslow\u201d) units (see Fig. 3c). Each band is updated at its own pace. The size of the step from band to band typically increases exponentially (which we call exponential update rule) and is defined as nk, where n is a base and k is the number of the band.\nIn the CWRNN, fast units (shown in red) are connected to all bands, benefitting from the context provided by the slow bands, while the low frequency units ignore noisy high frequency oscillations. Equation (8) from classical RNNs is modified, leading to a new update rule for the k-th band of output h at iteration t as follows:\nh (t) k =\n{ \u03c8 ( U(k)h\n(t\u22121) k +W(k)x\n(t) )\nif (t mod nk)=0,\nh (t\u22121) k otherwise.\nwhere U(k) and W(k) denote rows k from matrices U and W. Matrix U has an upper triangular structure, which corresponds to the connectivity between frequency bands. This equation is intuitively explained in the top part of Fig. 4, inspired from [18]. Each line corresponds to a band. At time step t=6 for instance, the first two bands k = 0 and k = 1 get updated. The triangular structure of the matrix results in each band getting updated from bands of lower (or equal) frequency\n\u00b7 \u00b7 + + \u00b7 \u00b7= = h(t) h(t 1) x(t)\nx(t)\nh(t)\nW\nW\nU\nU H\n1 2 4 8 16\n1 2 4 8 16\n\u00b7 + \u00b7= h(t) h(t 1) x(t) WU\nFig. 4. Updates made by the Clockwork RNN [18] (top) and our proposed Dense CWRNN (bottom). Units and weights colored in blue are the ones updated or read at the example time step t=6.\nonly. In Fig. 4, not active rows are also shown as zero (black) in U and W. In addition to multi-scale dynamics, creating sparse connections (high-to-low frequency connections are missing) reduces the number of free parameters and inference complexity."}, {"heading": "B. Long Short-Term Memory", "text": "Long Short-Term Memory (LSTM) networks [14], another variant of RNNs, and their recent convolutional extensions [10], [27] have proven to be, so far, the best performing models for learning long-term temporal dependencies. They handle information from the past through additional gates, which regulate how a memory cell is affected by the input signal. In particular, an input gate allows to add new memory to the cell\u2019s state, a forget gate resets the memory and an output gate regulates how gates at the next step will be affected by the current cell\u2019s state.\nThe basic unit is composed of input i, output o, forget f , and input modulation g gates, and a memory cell c (see Fig. 3b). Each element is parameterized by corresponding feed-forward (W) and recurrent (U) weights and bias vectors.\nDespite its effectiveness, the high complexity of this architecture may appear computationally wasteful in the mobile setting. Furthermore, the significance of learning long-term dependencies in the context of continuous mobile authentication is compromised by the necessity of early detection of switching between users. Due to absence of annotated ground truth data for these events, efficient training of forgetting mechanisms would be problematic."}, {"heading": "C. Convolutional learning of RNNs", "text": "Given the low correlation of individual frames with user identity, we found it strongly beneficial to make the input layer convolutional regardless of model type, thereby forcing earlier fusion of temporal information. To simplify the presentation, we have not made convolution explicit in the description of the methods above, however, it can be absorbed into the inputto-hidden matrix W.\n5 input, x(t) x(t+1)x(t 1) x(t+2)x(t 2) V V V V V k=0 k=1 h(t 1) h(t) h(t+1) WWW W W UUU UU h(t+2)h (t 2) k=2 U o\u0303(t 2) o\u0303(t 1) output, o\u0303(t) o\u0303(t+1) o\u0303(t+2)\nFig. 5. Proposed dense clockwork RNN with the same parameters as the original clockwork RNN shown in Fig. 3a."}, {"heading": "V. DENSE CONVOLUTIONAL CLOCKWORK RNNS", "text": "Among the existing temporal models we considered, the clockwork mechanisms appear to be the most attractive due to low computational burden associated with them in combination with their high modeling capacity. However, in practice, due to inactivity of \u201cslow\u201d units for long periods of time, they cannot respond to high frequency changes in the input and produce outputs which, in a sense, are stale. Additionally, in our setting, where the goal is to learn dynamic data representations serving as an input to a probabilistic framework, this architecture has one more weakness which stems from the fact that different bands are active at any given time step. The network will respond differently to the same input stimuli applied at different moments in time. This \u201cshift-variance\u201d convolutes the feature space by introducing a shift-associated dimension.\nIn this work, we propose a solution to both issues, namely \u201ctwined\u201d or \u201cdense\u201d clockwork mechanisms (DCWRNN, see Fig. 5), where during inference at each scale k there exist nk parallel threads shifted with respect to each other, such that at each time a unit belonging to one of the threads fires, updating its own state and providing input to the higher frequency units. All weights between the threads belonging to the same band are shared, keeping the overall number of parameters in the network the same as in the original clockwork architecture. Without loss of generality, and to keep the notation uncluttered of unnecessary indices, in the following we will describe a network with a single hidden unit hk per band k. The generalization to multiple units per band is straightforward, and the experiments were of course performed with the more general case.\nThe feedforward pass for the whole dense clockwork layer (i.e. all bands) can be given as follows:\nh(t) = \u03c8 ( Wx(t) + \u2206(UH) ) (9)\nwhere H=[ h(t\u22121) . . .h(t\u2212n k) . . .h(t\u2212n K) ] is a matrix concatenating the history of hidden units and we define \u2206(\u00b7) as an operator on matrices returning its diagonal elements in a column vector. The intuition for this equation is given in Fig. 4, where we compare the update rules of the original CWRNN and the proposed DCWRNN using an example of a network with 5 hidden units each associated with one of K=5 base n=2 bands. To be consistent, we employ the same matrix form as in the original CWRNN paper [18]) and show\ncomponents, which are inactive at time t, in dark gray. As mentioned in Section IV-A, in the original CWRNN, at time instant t=6, for instance, only unit h1 and h2 are updated, i.e. the first two lines in Fig. 4. In the dense network, all hidden units hk are updated at each moment in time.\nIn addition, what was vector of previous hidden states h(t\u22121) is replaced with a lower triangular \u201chistory\u201d matrix H of size K\u00d7K which is obtained by concatenating several columns from the history of activations h. Here, K is the number of bands. Time instances are not sampled consecutively, but strided in an exponential range, i.e. n, n2, . . . nK . Finally, the diagonal elements of the dot product of two triangular matrices form the recurrent contribution to the vector h(t). The feedforward contribution is calculated in the same way as in a standard RNN.\nThe practical implementation of the lower-triangular matrix containing the history of previous hidden activations in the DCWRNN requires usage of an additional memory buffer whose size can be given as m= \u2211K k=1 |hk|(nk\u22121\u22121), whereas here we have stated the general case of |hk|\u22651 hidden units belonging to band k.\nDuring training, updating all bands at a constant rate is important for preventing simultaneous overfitting of highfrequency and underfitting of low-frequency bands. In practice it leads to a speedup of the training process and improved performance. Finally, due to the constant update rate of all bands in the dense network, the learned representations are invariant to local shifts in the input signal, which is crucial in unconstrained settings when the input is unsegmented. This is demonstrated in Section VII."}, {"heading": "VI. DATA COLLECTION", "text": "The dataset introduced in this work is a part of a more general multi-modal data collection effort performed by Google ATAP, known as Project Abacus. To facilitate the research, we worked with a third party vendor\u2019s panel to recruit and obtain consent from volunteers and provide them with LG Nexus 5 research phones which had a specialized read only memory (ROM) for data collection. Volunteers had complete control of their data throughout its collection, as well as the ability to review and delete it before sharing for research. Further, volunteers could opt out after the fact and request that all of their data be deleted. The third party vendor acted as a privacy buffer between Google ATAP and the volunteers.\nThe data corpus consisted of 27.62 TB of smartphone sensor signals, including images from a front-facing camera, touchscreen, GPS, bluetooth, wifi, cell antennae, etc. The motion data was acquired from three sensors: accelerometer, gyroscope and magnetometer. This study included approximately 1,500 volunteers using the research phones as their primary devices on a daily basis. The data collection was completely passive and did not require any action from the volunteers in order to ensure that the data collected was representative of their regular usage.\nMotion data was recorded from the moment after the phone was unlocked until the end of a session (i.e., until it is locked again). For this study, we set the sampling rate for\n6\nthe accelerometer and gyroscope sensors to 200 Hz and for the magnetometer to 5 Hz. However, to prevent the drain of a battery, the accelerometer and gyro data were not recorded when the device was at rest. This was done by defining two separate thresholds for signal magnitude in each channel. Finally, accelerometer and gyroscope streams were synchronized on hardware timestamps.\nEven though the sampling rate of the accelerometer and the gyroscope was set to 200 Hz for the study, we noticed that intervals between readings coming from different devices varied slightly. To eliminate these differences and decrease power consumption, for our research we resampled all data to 50 Hz. For the following experiments, data from 587 devices were used for discriminative feature extraction and training of the universal background models, 150 devices formed the validation set for tuning hyperparameters, and another 150 devices represented \u201cclients\u201d for testing."}, {"heading": "VII. EXPERIMENTAL RESULTS", "text": "In this section, we use an existing but relatively small inertial dataset to demonstrate the ability of the proposed DCWRNN to learn shift-invariant representations. We then describe our study involving a large-scale dataset which was collected \u201cin the wild\u201d.\nA. Visualization: HMOG dataset\nTo explore the nature of inertial sensor signals, we performed a preliminary analysis on the HMOG dataset [33] containing similar data, but collected in constrained settings as a part of a lab study. This data collection was performed with the help of 100 volunteers, each performing 24 sessions of predefined tasks, such as reading, typing and navigation, while sitting or walking.\nUnfortunately, direct application of the whole pipeline to this corpus is not so relevant due to 1) absence of task-totask transitions in a single session and 2) insufficient data to form separate subsets for feature learning, the background model, client-specific subsets for enrollment, and still reserve a separate subset of \u201cimpostors\u201d for testing that haven\u2019t been seen during training.\nHowever, a detailed visual analysis of accelerometer and gyroscope streams has proven that the inertial data can be seen as a combination of periodic and quasi-periodic signals (from walking, typing, natural body rhythms and noise), as well non-periodic movements. This observation additionally motivates the clockwork-like architectures allowing for explicit modelling of periodic components.\nIn this subsection, we describe the use of HMOG data to explore the shift-invariance of temporal models that do not have explicit reset gates (i.e. RNN, CWRNN and DCWRNN). For our experiment, we randomly selected 200 sequences of normalized accelerometer magnitudes and applied three different networks each having 8 hidden units and a single output neuron. All weights of all networks were initialized randomly from a normal distribution with a fixed seed. For both clockwork architectures we used a base 2 exponential setting rule and 8 bands.\nFinally, for each network we performed 128 runs (i.e. 2K\u22121) on a shifted input: for each run x the beginning of the sequence was padded with x\u22121 zeros. The resulting hidden activations were then shifted back to the initial position and superimposed. Fig. 6 visualizes the hidden unit traces for two sample sequences from the HMOG dataset, corresponding to two different activities: reading while walking and writing while sitting. The figure shows that the RNN and the dense version of the clockwork network can be considered shiftinvariant (all curves overlap almost everywhere, except for minor perturbance at the beginning of the sequence and around narrow peaks), while output of the CWRNN is highly shiftdependent.\nFor this reason, in spite of their atractiveness in the context of multi-scale periodic and non-periodic signals, the usage of the CWRNN for the purpose of feature learning from unsegmented data may be suboptimal due to high shiftassociated distortion of learned distributions, which is not the case for DCWRNNs."}, {"heading": "B. Large-scale study: Google Abacus dataset", "text": "We now evaluate our proposed authentication framework on the real-world dataset described in Section VI. Table IV in the Appendix provides architectural hyper-parameters chosen for two 1-d Convnets (abbreviated ST and LT for short and log-term) as well as the recurrent models. To make a fair comparison, we set the number of parameters to be approximately the same for all of the RNNs. The ST Convnet is trained on sequences of 50 samples (corresponding to a 1 s data stream), LT Convnets take as input 500 samples (i.e. 10 s). All RNN architectures are trained on sequences of 20 blocks of 50 samples each with 50% of inter-block overlap to ensure smooth transitions between blocks (therefore, also a 10 s duration). For the dense and sparse clockwork architectures we set the number of bands to 3 with a base of 2. All layers in all architectures use tanh activations. During training, the networks produce a softmax output per block in the sequence, rather than only for the last one. The mean per-block negative log likelihood loss taken over all blocks is minimized.\nThe dimensionality of the feature space produced by each of the networks is PCA-reduced to 100. GMMs with 256 mixture components are trained for 100 iterations after initialization with k-means (100 iterations). MAP adaptation for each device\n7 DCWRNNCWRNNRNNInput\nFig. 6. On spatial invariance. From left to right: original sequence and traces of RNN, CWRNN and DCWRNN units. The first row: reading while walking, the second row: typing while sitting.\nis performed in 5 iterations with a relevance factor of 4 (set empirically). For zt-score normalization, we exploit data from the same training set and create 200 t-models and 200 z-sequences from non-overlapping subsets. Each t-model is trained based on UBM and MAP adaptation. All hyperparameters were optimized on the validation set.\nThe networks are trained using stochastic gradient descent, dropout in fully connected layers, and negative log likelihood loss. In the temporal architectures we add a mean pooling layer before applying the softmax. Each element of the input is normalized to zero mean and unit variance. All deep nets were implemented with Theano [3] and trained on 8 Nvidia Tesla K80 GPUs. UBM-GMMs were trained with the Bob toolbox [1] and did not employ GPUs.\nFeature extraction \u2014 we first performed a quantitative evaluation of the effectiveness of feature extractors alone as a multi-class classification problem, where one class corresponds to one of 587 devices from the training set. This way, one class is meant to correspond to one \u201cuser\u201d, which is equal to \u201cdevice\u201d in the training data (assuming devices do not\nchange hands). To justify this assumption, we manually annotated periods of non-authentic usage based on input from the smartphone camera and excluded those sessions from the test and training sets. Experiments showed that the percentage of such sessions is insignificant and their presence in the training data has almost no effect on the classification performance.\nNote that for this test, the generative model was not considered and the feature extractor was simply evaluated in terms of classification accuracy. To define accuracy, we must consider that human kinematics sensed by a mobile device can be considered as a weak biometric and used to perform a soft clustering of users in behavioral groups. To evaluate the quality of each feature extractor in the classification scenario, for each session we obtained aggregated probabilities over target classes and selected the 5% of classes with highest probability (in the case of 587 classes, the top 5% corresponds to the 29 classes). After that, the user behavior was considered to be interpreted correctly if the ground truth label was among them.\nThe accuracy obtained with each type of deep network with its corresponding number of parameters is reported in Table I. These results show that the feed forward convolutional architectures generally perform poorly, while among the temporal models the proposed dense clockwork mechanism Conv-DCWRNN appeared to be the most effective, while the original clockwork network (Conv-CWRNN) was slightly outperformed by the LSTM.\nAuthentication evaluation \u2014 when moving to the binary authentication problem, an optimal balance of false rejection and false acceptance rates, which is not captured by classification accuracy, becomes particularly important. We use a validation subset to optimize the generative model for the minimal equal error rate (EER). The obtained threshold value \u03b8EER is then used to evaluate performance on the test set using the half total error rate (HTER) as a criterion: HTER = 1/2[FAR(\u03b8EER) + FRR(\u03b8EER)], where FAR and FRR are false acceptance and false rejection rates, respectively. For the validation set, we also provide an average of per-device\n8\nand per-session EERs (obtained by optimizing the threshold for each device/session separately) to indicate the upper bound of performance in the case of perfect score normalization (see italicized rows in Table II).\nAn EER of 20% means that 80% of the time the correct user is using the device, s/he is authenticated, only by the way s/he moves and holds the phone, not necessarily interacting with it. It also means that 80% of the time the system identifies the user, it was the correct one. These results align well with the estimated quality of feature extraction in each case and show that the context-aware features can be efficiently incorporated in a generative setting.\nTo compare the GMM performance with a traditional approach of retraining, or finetuning a separate deep model for each device (even if not applicable in a mobile setting), we randomly drew 10 devices from the validation set and replaced the output layer of the pretrained LSTM feature extractor with a binary logistic regression. The average performance on this small subset was 2% inferior with respect to the GMM, due to overfitting of the enrollment data and poor generalization to unobserved activities. This is efficiently handled by mean-only MAP adaptation of a general distribution in the probabilistic setting.\nAnother natural question is whether the proposed model learns something specific to the user \u201cstyle\u201d of performing tasks rather than a typical sequence of tasks itself. To explore this, we performed additional tests by extracting parts of each session where all users interacted with the same application (a popular mail client, a messenger and a social network application). We observed that the results were almost identical to the ones previously obtained on the whole dataset, indicating low correlation with a particular activity."}, {"heading": "VIII. MODEL ADAPTATION FOR A VISUAL CONTEXT", "text": "Finally, we would like to stress that the proposed DCWRNN framework can also be applied to other sequence modeling tasks, including the visual context. The described model is not specific to the data type and there is no particular reason why it cannot be applied to the general human kinematic problem (such as, for example, action or gesture recognition from motion capture).\nTo support this claim, we have conducted additional tests of the proposed method within a task of visual gesture recognition. Namely, we provide results on the motion capture (mocap) modality of the ChaLearn 2014 Looking at People gesture dataset [28]. This dataset contains about 14000 instances of Italian conversational gestures with the aim to detect, recognize and localize gestures in continuous noisy recordings. Generally, this corpus comprises multimodal data captured with the Kinect and therefore includes RGB video, depth stream and mocap data. However, only the last channel is used in this round of experiments. The model evaluation is performed using the Jaccard index, penalizing for errors in classification as well as imprecise localization.\nDirect application of the GMM to gesture recognition is suboptimal (as the vocabulary is rather small and defined in advance), therefore, in this task we perform end-to-end discriminative training of each model to evaluate the effectiveness of feature extraction with the Dense CWRNN model.\nIn the spirit of [21] (the method ranked firstst in the ECCV 2014 ChaLearn competition), we use the same skeleton descriptor as input. However, as in the described authentication framework, the input is fed into a convolutional temporal architecture instead of directly concatenating frames in a spatio-temporal volume. The final aggregation and localization step correspond to [21]. Table III reports both the Jaccard index and per-sequence classification accuracy and shows that in this application, the proposed DCWRNN also outperforms the alternative solutions."}, {"heading": "IX. CONCLUSION", "text": "From a modeling perspective, this work has demonstrated that temporal architectures are particularly efficient for learning of dynamic features from a large corpus of noisy temporal signals, and that the learned representations can be further incorporated in a generative setting. With respect to the particular application, we have confirmed that natural human kinematics convey necessary information about person identity and therefore can be useful for user authentication on mobile devices. The obtained results look particularly promising, given the fact that the system is completely non-intrusive and non-cooperative, i.e. does not require any effort from the user\u2019s side.\nNon-standard weak biometrics are particularly interesting for providing the context in, for example, face recognition or speaker verification scenarios. Further augmentation with data extracted from keystroke and touch patterns, user location, connectivity and application statistics (ongoing work) may be a key to creating the first secure non-obtrusive mobile authentication framework.\nFinally, in the additional round of experiments, we have demonstrated that the proposed Dense Clockwork RNN can be successfully applied to other tasks based on analysis of sequential data, such as gesture recognition from visual input.\nAPPENDIX\nIn this section, we provide additional detail for reproducability that was not provided in the main text.\n9\nHyper-parameter selection\nTable IV provides the complete set of hyper-parameters that were chosen based on a held-out validation set. For convolutional nets, we distinguish between convolutional layers (Conv, which include pooling) and fully-connected layers (FCL). For recurrent models, we report the total number of units (in the case of CWRNN and DCWRNN, over all bands).\nDetails on zt-normalization\nHere we provide details on the zt-normalization that were not given in Section 3. Recall that we estimate authenticity, given a set of motion features by scoring the features against a universal background model (UBM) and client model. Specifically, we threshold the log-likelihood ratio in Eq. 7. An offline z-step (zero normalization) compensates for intermodel variation by normalizing the scores produced by each client model to have zero mean and unit variance in order to use a single global threshold:\n\u039bz(Y |\u0398client) = \u039b(Y )\u2212 \u00b5(Z|\u0398client)\n\u03c3(Z|\u0398client) , (10)\nwhere Y is a test session and Z is a set of impostor sessions. Parameters are defined for a given user once model enrollment is completed. Then, the T -norm (test normalization) compensates for inter-session differences by scoring a session against a set of background T -models.\n\u039bzt(Y ) = \u039bz(Y |\u0398client)\u2212 \u00b5z(Y |\u0398T )\n\u03c3z(Y |\u0398T ) . (11)\nThe T-models are typically obtained through MAP-adaptation from the universal background model in the same way as all client models, but using different subsets of the training corpus. The Z-sequences are taken from a part of the training data which is not used by the T-models."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank their colleagues Elie Khoury, Laurent El Shafey, S\u00e9bastien Marcel and Anupam Das for valuable discussions."}], "references": [{"title": "Bob: a free signal processing and machine learning toolbox for researchers", "author": ["A. Anjos", "L. El Shafey", "R. Wallace", "M. G\u00fcnther", "C. McCool", "S. Marcel"], "venue": "ACMMM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Score normalization for text-independent speaker verification systems", "author": ["R. Auckenthaler", "M. Carey", "H. Lloyd-Thomas"], "venue": "Digital Signal Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Siamese Neural Network based Similarity Metric for Inertial Gesture Classification and Rejection", "author": ["S. Berlemont", "G. Lefebvre", "S. Duffner", "C. Garcia"], "venue": "In FG,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Using unlabeled data in a sparse-coding framework for human activity recognition", "author": ["S. Bhattacharya", "P. Nurmi", "N. Hammerla", "T. Plotz"], "venue": "In Pervasive and Mobile Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Silentsense: Silent user identification via touch and movement behavioral biometrics", "author": ["C. Bo", "L. Zhang", "X.-Y. Li", "Q. Huang", "Y. Wang"], "venue": "In MobiCom,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A tutorial on human activity recognition using body-worn inertial sensors", "author": ["A. Bulling", "U. Blanke", "B. Schiele"], "venue": "In ACM Computing Surveys,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Exploring ways to mitigate sensorbased smartphone fingerprinting", "author": ["A. Das", "N. Borisov", "M. Caesar"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Unobtrusive User- Authentication on Mobile Phones Using Biometric Gait Recognition", "author": ["M.O. Derawi", "C. Nickel", "P. Bours", "C. Busch"], "venue": "IIH-MSP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "3D gesture classification with convolutional neural networks", "author": ["S. Duffner", "S. Berlemont", "G. Lefebre", "C. Garcia"], "venue": "In ICASSP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Preprocessing techniques for context recognition from accelerometer data", "author": ["D. Figo", "P. Diniz", "D. Ferreira", "J. Cardoso"], "venue": "In Pervasive and Ubiquituous Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Combining modality specific deep neural networks for emotion recognition", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "c. G\u00fcl\u00e7ehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio"], "venue": "ICMI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei- Fei"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Bi-Modal Biometric Authentication on Mobile Phones in Challenging Conditions", "author": ["E. Khoury", "L. El Shafey", "C. McCool", "M. Gunther", "S. Marcel"], "venue": "In Image and Vision Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "BLSTM-RNN Based 3D Gesture Classification", "author": ["G. Lefebvre", "S. Berlemont", "F. Mamalet", "C. Garcia"], "venue": "In ICANN,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "The prehensile movements of the human hand", "author": ["J. Napier"], "venue": "In Journal of Bone and Joint Surgery,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1956}, {"title": "Moddrop: adaptive multi-modal gesture recognition", "author": ["N. Neverova", "C. Wolf", "G.W. Taylor", "F. Nebout"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kin", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Benchmarking the performance of svms and hmms for accelerometer-based biometric gait recognition", "author": ["C. Nickel", "H. Brandt", "C. Busch"], "venue": "ISSPIT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Feature learning for activity  10 recognition in ubiquitous computing", "author": ["T. Plotz", "N.Y. Hammerla", "P. Olivier"], "venue": "In 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital Signal Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "ChaLearn Looking at People Challenge 2014: Dataset and Results", "author": ["S.Escalera", "X.Bar\u00f3", "J.Gonz\u00e0lez", "M.Bautista", "M.Madadi", "M.Reyes", "V. Ponce", "H.Escalante", "J.Shotton", "I.Guyon"], "venue": "ECCVW,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "HMOG: A New Biometric Modality for Continuous Authentication of Smartphone Users", "author": ["Z. Sitova", "J. Sedenka", "Q. Yang", "G. Peng", "G. Zhou", "P. Gasti", "K. Balagani"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Multimodal learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["G. Taylor", "G. Hinton", "S. Roweis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Unobtrusive multimodal biometrics for ensuring privacy and information security with personal devices", "author": ["E. Vildjiounaite"], "venue": "In Pervasive Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "A multimodal data set for evaluating continuous authentication performance in smartphones", "author": ["Q. Yang", "G. Peng", "D.T. Nguyen", "X. Qi", "G. Zhou", "Z. Sitova", "P. Gasti", "K.S. Balagani"], "venue": "In SenSys", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "In recent years, researchers in different fields have been working on creating fast and secure authentication alternatives that would make it possible to remove this burden from the user [29], [6].", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "In recent years, researchers in different fields have been working on creating fast and secure authentication alternatives that would make it possible to remove this burden from the user [29], [6].", "startOffset": 193, "endOffset": 196}, {"referenceID": 19, "context": "those in which an object is seized and held, partly or wholly, by the hand [20]) collected by 1,500 volunteers over several months of daily use (Fig.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "However, apart from the application itself, the main contribution of this work is in developing a new shift-invariant temporal model which fixes a deficiency of the recently proposed Clockwork recurrent neural networks [18] yet retains their ability to explicitly model multiple temporal scales.", "startOffset": 219, "endOffset": 223}, {"referenceID": 22, "context": "A detailed overview and benchmarking of existing state-of-the-art is provided in [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "[9], for example, used a smartphone attached to the human body to extract information about walking cycles, achieving 20.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "In [12] and [7], exhaustive ar X iv :1 51 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "In [12] and [7], exhaustive ar X iv :1 51 1.", "startOffset": 12, "endOffset": 15}, {"referenceID": 23, "context": "Perhaps most relevant to this study is [25], the first to report the effectiveness of RBM-based feature learning from accelerometer data, and [5], which proposed a data-adaptive sparse coding framework.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "Perhaps most relevant to this study is [25], the first to report the effectiveness of RBM-based feature learning from accelerometer data, and [5], which proposed a data-adaptive sparse coding framework.", "startOffset": 142, "endOffset": 145}, {"referenceID": 10, "context": "Convolutional networks have been explored in the context of gesture and activity recognition [11], [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "[19] applied a bidirectional LSTM network to a problem of 14-class gesture classification, while Berlemont et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] proposed a fully-connected Siamese network for the same task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "A combination of face recognition and speech [17], and of gait and voice [32] have been proposed in this context.", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "A combination of face recognition and speech [17], and of gait and voice [32] have been proposed in this context.", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 174, "endOffset": 178}, {"referenceID": 28, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 186, "endOffset": 190}, {"referenceID": 20, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "In a recent study [8], it was shown that under lab conditions a particular device could be identified by a response of its motion sensors to a given signal.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "Formally, the measured output of both the accelerometer and gyroscope can be expressed as follows [8]:", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "Following [8], the noise vector is obtained by drawing a 12dimensional (3 offset and 3 gain coefficients per sensor) obfuscation vector from a uniform distribution \u03bc \u223c U12[0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 24, "context": "Along the lines of [26], maximum a posteriori (MAP) adaptation of mean vectors for a given user is performed.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "As a final step, zt-score normalization [2] is performed to compensate for inter-session and inter-person variations and reduce the overlap between the distribution of scores from authentic users and impostors.", "startOffset": 40, "endOffset": 43}, {"referenceID": 20, "context": "The first two conditions are known to contradict each other as performance of a standalone feature typically grows with integration time [21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "The former model, popular in speech recognition [13], involves convolutional learning of integrated temporal statistics from short and long sequences of data (referred to as \u201cshort-term\u201d and \u201clong-term\u201d convnets).", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "Then, Section V introduces a new shift-invariant model based on modified Clockwork RNNs [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "Temporal models: (a) a basic recurrent unit; (b) an LSTM unit [14]; (c) Clockwork RNN [18] with 3 bands and a base of 2; Increasing k indicates lower operating frequency.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "Temporal models: (a) a basic recurrent unit; (b) an LSTM unit [14]; (c) Clockwork RNN [18] with 3 bands and a base of 2; Increasing k indicates lower operating frequency.", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "The recently proposed Clockwork RNN (CWRNN) [18] operates at several temporal scales which are incorporated in a single network and trained jointly.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "4, inspired from [18].", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Updates made by the Clockwork RNN [18] (top) and our proposed Dense CWRNN (bottom).", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Long Short-Term Memory (LSTM) networks [14], another variant of RNNs, and their recent convolutional extensions [10], [27] have proven to be, so far, the best performing models for learning long-term temporal dependencies.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Long Short-Term Memory (LSTM) networks [14], another variant of RNNs, and their recent convolutional extensions [10], [27] have proven to be, so far, the best performing models for learning long-term temporal dependencies.", "startOffset": 112, "endOffset": 116}, {"referenceID": 25, "context": "Long Short-Term Memory (LSTM) networks [14], another variant of RNNs, and their recent convolutional extensions [10], [27] have proven to be, so far, the best performing models for learning long-term temporal dependencies.", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "To be consistent, we employ the same matrix form as in the original CWRNN paper [18]) and show components, which are inactive at time t, in dark gray.", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "To explore the nature of inertial sensor signals, we performed a preliminary analysis on the HMOG dataset [33] containing similar data, but collected in constrained settings as a part of a lab study.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "All deep nets were implemented with Theano [3] and trained on 8 Nvidia Tesla K80 GPUs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "UBM-GMMs were trained with the Bob toolbox [1] and did not employ GPUs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 20, "context": "Single network [21] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "Ensemble [21] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "Namely, we provide results on the motion capture (mocap) modality of the ChaLearn 2014 Looking at People gesture dataset [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 20, "context": "In the spirit of [21] (the method ranked firstst in the ECCV 2014 ChaLearn competition), we use the same skeleton descriptor as input.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "The final aggregation and localization step correspond to [21].", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "We present a large-scale study exploring the capability of temporal deep neural networks to interpret natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind dataset of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We (1) compare several neural architectures for efficient learning of temporal multi-modal data representations, (2) propose an optimized shift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate the discriminatively-trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems. Finally, we demonstrate that the proposed model can be successfully applied also in a visual context.", "creator": "LaTeX with hyperref package"}}}