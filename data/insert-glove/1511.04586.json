{"id": "1511.04586", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "Character-based Neural Machine Translation", "abstract": "We ashrama introduce capill a ketuanan neural changeability machine pantheon translation model playmate that gleaners views scheffel the caney input 1.091 and c\u00e1mara output mid-wilshire sentences hollanda as sequences of kempler characters stenting rather than words. Since oberkommando word - huasheng level information drie provides itep a 41.05 crucial zaje\u010dar source of bias, 2-0-6-0 our input sjoeberg model composes byong representations mcdermot of character sequences riyals into representations etrog of 119.70 words (as hispanoamericana determined by whitespace boundaries ), calluna and ajb then these cowherds are translated jien using a loped joint attention / translation sharratt model. plexiform In the endgame target language, the kazansky translation ilustres is cotsakos modeled showered as a ambridge sequence of undersize word vectors, montlucon but headmaster each brecknock word is keong generated one character l\u2019art at beermann a seat time, auspex conditional on the nomoto previous character 37,500 generations pescennius in abarbanel each zongmi word. As the representation freudians and madhumati generation scheinman of meiss words ideaglobal.com is performed wineries at superconductivity the iap character level, our kreuziger model is synchronised capable of interpreting http://www.gm.com/ and 90-run generating 30.6 unseen dgi word longitude forms. A hannezo secondary falouji benefit of sacrifice this approach is mainichi that thondup it glcnac alleviates much of the challenges bastidas associated with preprocessing / tokenization chieftaincy of the source and target cyberspace languages. We show that homeopathic our deathbed model can acari achieve bhanwar translation results grantly that kenechi are tanri on 147-member par with conventional word - based models.", "histories": [["v1", "Sat, 14 Nov 2015 17:36:43 GMT  (201kb,D)", "http://arxiv.org/abs/1511.04586v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wang ling", "isabel trancoso", "chris dyer", "alan w black"], "accepted": true, "id": "1511.04586"}, "pdf": {"name": "1511.04586.pdf", "metadata": {"source": "CRF", "title": "CHARACTER-BASED NEURAL MACHINE TRANSLATION", "authors": ["Wang Ling", "Isabel Trancoso"], "emails": ["wlin@inesc-id.pt", "isabel.trancoso@inesc-id.pt", "cdyer@cs.cmu.edu", "awb@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the past, efforts at performing translation at the character-level (Vilar et al., 2007) or subwordlevel (Neubig et al., 2013) have failed to produce competitive results compared to word-based counterparts (Brown et al., 1993; Koehn et al., 2007; Chiang, 2005), with the exception of closely related languages (Nakov & Tiedemann, 2012). However, developing sequence to sequence models that are capable at reading and generating and generating words at the character basis is attractive for multiple reasons. Firstly, it opens the possibility for models reason about unseen source words, such as morphological variants of observed words. Secondly, it allows the production of unseen target words effectively recasting translation as an open vocabulary task. Finally, we benefit from a significant reduction of the source and target vocabulary size as only characters need to be modelled explicitly. As the number of word types increases rapidly with the size of the dataset Heaps (1978), while the number of letter types in the majority languages is fixed, character MT models can potentially solve many scalability issues in MT, both in terms of computational speed and memory requirements. Namely, the computational cost of performing a softmax over the whole vocabulary, and the memory needed to represent each existing word type explicitly.\nIn this work, we present a neural translation model that learns to encode and decode using at the character level. We show that contrarily to previous belief (Vilar et al., 2007; Neubig et al., 2013), models that work at the character level can generate results competitive with word-based models. This is accomplished by indirectly incorporating the knowledge of words into the model using a hierarchical architecture that generates the word representations from characters, maps the word representation into the target language and the continuous space and then proceeds to generate the\nar X\niv :1\n51 1.\n04 58\n6v 1\n[ cs\n.C L\n] 1\n4 N\ntarget word character by character. As the composition of the words is based on characters, the model can learn, for instance, morphological aspects in the source language, allowing it to build effective representations for unseen words. On the other hand, the character-based generation model allows the model to perform translation into word forms that are not present in the training corpus."}, {"heading": "2 CHARACTER-BASED MACHINE TRANSLATION MODEL", "text": "This section describes our character-based machine translation model. As an automatic translation task, it perform the translation of a source sentence s = s0, . . . , sn, where si is the source word at index i into the target sentence t = t0, . . . , tm, where tj is the target word at index j. This can be decomposed into a serie of predictions, where the model predicts the next target word tp, given the source sentence s and the previously generated target words t0, . . . , tp\u22121.\nNotation We shall represent vectors with lowercase bold letters (e.g. a), matrixes with uppercase bold letters (e.g. A), scalar values as regular lowercase letters (e.g. a) and sets as regular uppercased letters (e.g. A). When referring to whole source and target sentences, we shall use the variables s and t, respectively. Individual source and target words, shall be referred as si and tj , where i and j are the indexes of the words within the sentence. Furthermore, we use variables n and m to refer to the lengths of the source and target sentences. Finally, we shall define that s0 and t0 represent a special start of sentence token denoted as SOS and that sn and tm represent a special end of sentence token denoted as EOS. To refer to individual characters, we shall use the notation si,u and tj,v , which denotes the u-th character in the i-th word in the source sentence and to the v-th character in the j-th word in the target sentence, respectively. We use the variables x and y as the lengths of the source and target words. We also define that the first character within a word is always a start of word token denoted as SOW and the last characters si,x and sj,y are always the end of word character EOW."}, {"heading": "2.1 JOINT ALIGNMENT AND TRANSLATION MODEL", "text": "While our model can be applied to any neural translation model , we shall adapt the attention-based translation model presented in (Bahdanau et al., 2015), which is described as follows. An illustration of the model is shown in Figure 1. In this model, the translation of a new target word tp, given the source sentence s0, . . . , sn and the current target context t0, . . . , tp\u22121 is performed in the following steps.\n1-Source Word Projection Source words are mapped into ds,w-dimentional vectors. The most common approach to perform this projection is through a word lookup table, where each word type is attributed a independent set of parameters. Thus, a word lookup table with S words will require ds,w \u00d7 S parameters, where ds,w is the size of the word embeddings.\n2-Context via BLSTMs A context-aware representation of each source word vector s0, . . . , sn is obtained by using two LSTMs (Hochreiter & Schmidhuber, 1997). The forward LSTM generates the state sequence gf0 , . . . ,g f n from the input sequence s0, . . . , sn, and encode the left context. Then, the backward LSTM, reads the input sequence in the reverse order sn, . . . , s0 generating the backward state sequence gbn, . . . ,g b 0, which encodes the right context. Consequently, for each word si, the global context is obtained as a linear combination of the respective forward state gfi and backward state gbi .\n3-Target Word Projection The projection of target words essentially follows the same approach as the projection of the source words. A distinct word lookup table for the target language is used with dt,w \u00d7 T parameters, where dt,w is the dimensionality of the target word vectors and T is the size of the target vocabulary.\n4-Context via Forward LSTM Unlike the source sentence s, the target sentence is not known a priori. Thus, a forward LSTM is built over the translated sentence t0, . . . , tp\u22121, generating states lf0 , . . . , l f p\u22121. As result, state l f p\u22121 encodes the currently translated context.\n5-Alignment via Attention For each target context lfp\u22121, the attention model learns the attention that is attributed to each of the source vectors b0, . . . ,bn. Essentially, the model computes a score zi for each source word by applying the following function:\nzi = s tanh(Wtl f p\u22121 +Wsbi),\nwhere the source vector bi and target vector l f p\u22121 are combined into a ds-dimensional vector using the parameters Ws and Wt, respectively, and a non-linearity is applied (In our case, we apply a hiperbolic tangent function tanh). Then, the the score is obtained using the vertical vector s. This operation is performed for each source index obtaining the scores z0, . . . , zn. Then, a softmax over all scores as is performed as follows:\nai = exp(zi)\u2211\nj\u2208[0,n] exp(zj)\nThis function yields a set of attention coefficients a0, . . . , an with the property that each coefficient is a value between 0 and 1, and the sum of all coefficients is 1. In MT, this can be interpreted as a soft alignment for the target word tp over each of the source words. These attentions are used to obtain representation of the source sentence for predicting word wp, which is simply the weighted average a = \u2211 i\u2208[0,n] aibi.\n6-Target Word Generation In the previous steps, the model builds two vectors that are used for the prediction of the next word in the translation tp. The first is the target context vector l f p\u22121, which encodes the information of all words preceding tp, and a, which contains the information of the most likely source word/words to generate wp. A common approach to word prediction is to apply a softmax function over the target language vocabulary. More formally, given the conditioned\nvariables, the source attention a and the target context lfp\u22121, the probability of a given word type t p being the next translated word tp is given by:\nP (tp|a, lfp\u22121) = exp(eS\ntp a a+S tp l l f p\u22121)\u2211\nj\u2208[0,T ] exp(e Sjaa+S j l l f p\u22121)\n,\nwhere Sa and Sl are the parameters that map the conditioned vectors into a score for each word type in the target language vocabulary T . The parameters for a specific word type j are obtained as Sja and Sjl , respectively. Then, scores are normalized into a probability."}, {"heading": "2.2 CHARACTER-BASED MACHINE TRANSLATION", "text": "We now present our adaptation of the word-based neural network model to operate over character sequences rather than word sequences. However, unlike previous approaches that attempt to discard the notion of words completely (Vilar et al., 2007; Neubig et al., 2013), we propose an hierarhical architecture, which replaces the word lookup tables (steps 1 and 3) and the word softmax (step 6) with character-based alternatives, which compose the notion of words from individual characters. The advantage of this approach is that we benefit from properties of character-based approaches (e.g. compactness and orthographic sensitivity), but can also easily be incorporated into any word-based neural approaches.\nCharacter-based Word Representation The work in (Ling et al., 2015; Ballesteros et al., 2015) proposes a compositional model for learning word vectors from characters. Similar to word lookup tables, a word string sj is mapped into a ds,w-dimensional vector, but rather than allocating parameters for each individual word type, the word vector sj is composed by a series of transformation using its character sequence sj,0, . . . , sj,x.\nThe illustration of the model is shown in 2. Essentially, the model builds a representation of the word using characters, by reading characters from left to right and vice-versa. More formally, given an input word sj = sj,0, . . . , sj,x, the model projects each character into a continuous ds,c-dimensional vectors sj,0, . . . , sj,x using a character lookup table. Then, it builds a forward LSTM state sequence hf0 , . . . ,h f k by reading the character vectors sj,0, . . . , sj,x. Another, backward LSTM reads the character vectors in the reverse order generating the backward states hbk, . . . ,h b 0. Finally, the\nrepresentation of the word sj is obtained by combining the final states as follows:\nsj = Ds,fh f k +Ds,bh b 0 + bs,d,\nwhere Ds,f , Ds,b and bs,d are parameters that determine how the states are combined.\nAs the C2W model maps a word string into a vector, we can simply replace the word lookup tables (steps 1 and 3) with two C2W models in order to obtain a character-based translation model at the input. Next, we shall describe a method to output words at the character level.\nCharacter-based Word Generation A word softmax requires a separate set of parameters for each word type. Thus, a word softmax cannot generate unseen words in the training set, and requires a large amount of parameters due to the fact that each word type must be modelled independently. Furthermore, another well know problem is that the whole target vocabulary T must be traversed for each prediction during both training and testing phases. While at training time approximations such as noise contrastive estimation (Gutmann & Hyvarinen, 2010) can be applied, traversing T is still required at test time.\nWe address these problems by defining a character-based word generation model. An illustration of the V2C (vector to characters) is shown in Figure 3. We define a character vocabulary for the target language Tc and a given word tj as a sequence of characters tj,0, . . . , tj,y , the probability of a given word is redefined as:\nP (wp|a, lfp\u22121) = \u220f\ni\u2208[0,y]\nP (wj,i|wk,0, . . . , wj,j\u22121,a, lfp\u22121)\nThe intuition is that rather than learning to predict single words, our model predicts the character sequence of the output word. Each prediction is dependent on the input of the model (aligned source words a and and target word context lfp\u22121) and also on the previously generated character context tj,0, . . . , tj,q\u22121, where q is the index of the last predicted character.\nThe character context is generated by a LSTM. First, we project each target character tj,0, . . . , tj,q\u22121 with a character lookup table in the target language (we use the same lookup table used in the C2W model for the target language) into a dt,c-dimensional vector tj,0, . . . , tj,q\u22121. Then, each vector is concatenated to the vectors a and lfp\u22121, and passed as the input to an LSTM, generating the sequence of states yf0 , . . . ,y f q\u22121. Then, the prediction of the character tk,q obtained as the softmax function:\nP (tj,q|tj,0, . . . , tj,q\u22121,a, lfp\u22121) = exp(S\nwk,q y y f q\u22121)\u2211\ni\u2208[0,Tc] exp(S i yy f q\u22121)\n,\nwhere Sy are the parameters that convert the state y into a score for each output character, and Siy denotes the parameters respective to the character type i. The word terminates once the end of word token EOW is generated.\nFinally, the model is also required to produce the end of sentence token EOS, similarly to a word softmax. In our model, we simply consider the EOS token as a word whose only character is EOS. In another words, it must generate the sequence EOS,EOS."}, {"heading": "2.3 TRAINING", "text": "During training the whole set of target words t0, . . . , tm are known, and we simply maximize the log likelihood that the sequence of words log p(t0 | a, SOS) + . . . + log p(tm | a, lfm\u22121). More formally, we wish to maximize the log likelihood of the training data defined as:\u2211\n(s,t)\u2208D \u2211 p\u2208[0,m] log p(tq | a, lfm\u22121)\nwhere D is the set of parallel sentences used as training data.\nIn the word-based model, optimizing this function can be performed by maximizing the word softmax objective. In the character-based model, this each word prediction is factored into a set of character predictions. More concretely, we we maximize the following log-likelihood:\n\u2211 (s,t)\u2208D \u2211 p\u2208[0,m] \u2211 q\u2208[0,length(tp)] log p(tp,q | tk,0, . . . , tk,q\u22121,a, lfm\u22121)"}, {"heading": "2.4 DECODING", "text": "In previous work (Bahdanau et al., 2015), decoding is performed using beam search. In the wordbased approach, we define a stack of translation hypothesis per timestamp A, where each position is a set of translation hypothesis. With A0 = SOS, at each timestamp j, we condition on each of the previous contexts t = Aj\u22121 and add new hypothesis for all words obtained in the softmax function. For instance, given the partial translations A1 = A,B, and the vocabulary T = A,B,C, then A2 would be composed by AA,AB,AC,BA,BB,BC. We set a beam kw, which defines the number of hypothesis to be expanded prioritizing hypothesis with the highest sentence probability. An hypothesis is final once it generates the end of sentence token EOS.\nWhereas, the word softmax simply returns a list of candidates for the next word by iterating through each of the target word types, the V2C model can generate an infinite number of target words. Thus, in the character-based MT model, a second decoding process is needed to return the list of top scoring words at each timestamp. That is, we define a second beam search decoder with beam kc, and perform a stack-based coding on the character-level for each word prediction. The decoder defines a stack B, where at each timestamp, a new character is generated for each hypothesis. In this case, the beam search is run until kw final hypothesis are found (generation of EOW), as it must return at least kw new hypothesis to ensure that the word level search is complete."}, {"heading": "2.5 LAYER-WISE TRAINING", "text": "Our character-based model defines a three layer hierarchy. Firstly, characters are composed into word vectors using the C2W model. Then, the attention model searches for the next source word to translate. Finally, the generation of the target word is obtained using the V2C model. Each of these layers contain in many cases multiple non-linear projections, training is bound to be significantly more complex than word-based models. In practice, this causes more epochs to be necessary for convergence, and the model can converge to a suboptimal local optimum. Furthermore, while the\nV2C model is generally more efficient than a word softmax, the introduction of the C2W model significantly slows down training, as simple word table lookup is replaced by a compositional model.\nInspired by previous work on training deep multi-layered perceptrons (Bengio et al., 2007), we start by training the attention and V2C models, which are directly linked to the output of the model. The C2W model, which is the bottleneck of the model, is temporarily replaced by word lookup tables, and the model is trained to maximize the translation score in the development set.\nThe C2W model is introduced afterwards by first training the C2W model to produce the same word vectors as the word lookup tables for all training word types. More formally, given a word w, and the embeddings from the word lookup table w\u03021, . . . , \u02c6wdw , and the embeddings produced by the C2W model w1, . . . ,wdw , we wish to optimize the parameters to minimize the square distance (w\u03021 \u2212 w1)2, . . . , (wdw)2. As result, C2W model will generate similar embeddings as the word lookup table, and replacing them will not degenerate the results significantly. Finally, the full set of parameters (C2W, attention model and V2C) are fine-tuned to maximize the translation quality on the development set."}, {"heading": "2.6 WEAK SUPERVISION FOR ATTENTION MODEL", "text": "A problem with attention models is the fact that finding the latent attention coefficients ai, requires a large number of epochs (Hermann et al., 2015). Furthermore, as the attention model defined in (Bahdanau et al., 2015) does not define any domain knowledge regarding word alignments, such as distortion, symmetry and fertility (Brown et al., 1993) this model is likely to overfit for small amounts of data. To address this problem, we use IBM model 4 to produce the word alignments for the training parallel sentences, impose a soft restriction to induce the attention model to produce alignments similar to word alignments produced by the IBM model. More formally, given that the target word is aligned to the source word at index k in IBM model 4, we wish the model to maximize the coefficient ak. As ak is obtained from a softmax, this is essentially means that we wish to maximize the probability that the target word wk is selected. As word alignments tend to be one-to-one, for target words with multiple or no alignments, we simply set no soft restriction."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we present and analyse the results using our proposed character-based model."}, {"heading": "3.1 SETUP", "text": "We test our model in two datasets. First, we 600k sentence pairs for training from Europarl (Koehn, 2005), in the English-Portuguese language pair. Then, we define another 500 sentence pairs for development and 500 sentence pairs for testing. We also use the English-French 20k sentence pairs from BTEC as a small scale experiment. As development and test sets, we the CSTAR03 and IWSLT04 held out sets, respectively.\nBoth languages were tokenized with Penn Tree Bank tokenizer1. As for casing, word-based models trained using the lowercased parallel data. At testing time, we uppercase using the model using the script at (Koehn et al., 2007). This is a common practice for word-based models, as the sparcity induced by the same word in different casings (Play vs play) is problematic for word based models. For the character-based model, the model is trained on the true case on both source and target sides. That is, the model is responsible for interpreting and generating true cased sentences. Finally, evaluation is performed using BLEU (Papineni et al., 2002) and we always use a single sentence as reference.\nAs for the hyper parameters of the model, all LSTM states and cells are set to 150 dimensions. Word projection dimensions for the source and target languages ds,w and dt,w were set to 50. Similarly, the character projection dimensions were also set to ds,c and dt,c. For the alignment model, dz was set to 100. Finally, the beam search at the work level kw was set to 5, and the beam size for the character level search kc was set to 5. Training is performed until there is no BLEU (Papineni et al., 2002) improvement for 5 epochs on the development set. Systems are trained using mini-batch\n1https://www.cis.upenn.edu/ treebank/tokenization.html\ngradient descent with mini-batches of 40 sentence pairs. For word-based models on Europarl, rather than normalizing over the whole word inventory, we use noise contrastive estimation (Gutmann & Hyvarinen, 2010), which subsamples a set of 100 negative samples at each prediction."}, {"heading": "3.2 RESULTS", "text": "For the BTEC dataset, the word-based neural model achieves a BLEU score of 15.38 in the French to English translation direction, while the character-based model achieves a score of 15.45. On the Europarl, the word-based model obtains a BLEU score of 19.29, while our proposed character-based model obtains a BLEU score of 19.49. While, differences are not significant, this is a strong results if we consider that previous work (Vilar et al., 2007; Neubig et al., 2013) revealed significantly lower translation quality using character-based approaches.\nWord Representation In order to analyse the word representation obtained after training the model, we generate the vector representation of the C2W models trained on the Europarl translation task with 600k sentence pairs for all words in the training set. Table1 provides examples for words in English and Portuguese, and words whose representations are closest to them, measure in terms of cosine similarity. We observe evidence that the fact that word representations are composed from characters as atomic units is not a limitation of their representative capabilities, as the model can learn that orthographically divergent words (answer vs. response) have similar meanings. Compared to the word-based model, we can observe that in many cases, the C2W model, prefers to gather words that are orthographically similar, such as different forms of answer and responder, which does not happen in word lookup tables. This is because, different conjugations in English are not always translated into the same word, and as lookup tables do not regard the orthographic similarity between words, source words that do not share translations are essentially distinct words and placed in different vector spaces. However, the sequential nature of the C2W model, makes it desirable to place these words in the same vector space as less memorization is required. Consequently, not only less parameters are needed to encode such word groups, but also, in the event a unknown verb conjugation is observed at test time, a reasonable vector can still be found for such a word. For instance, if we do not know the translation of answered, we can still translate the word as answer and obtain a reasonable translation of the sentence.\nEvidently, the assumption that similar words have similar meanings is not always true. For instance, the word well-founded is not similar to much-needed. In such cases, our model would behave equivalently to the word-based model.\nWord Generation A strong aspect in the V2C model is that the model can generate unseen words. In Table 2, we provide three examples of unknown words that have been generated in Portuguese. The first is the translation of unknown word subsidisation to subsidade. Unfortunately, this is incorrect as subsidade does not exist in the Portuguese vocabulary. However, it is encouraging to observe that the model is trying to translate the unknown word from its parts. Firstly, the English suffix -ation and the Portuguese suffix -dade are common endings for nouns. Furthermore, it can roughly copy the stem of the word subsi. Unfortunately, the correct translation is subs\u0131\u0301dio. As the model has not seen any of these forms, it is unable to decide, which is the correct form. This hints that the V2C model could be pre-trained on large amounts of parallel data in order to recognize existing word forms, which will be left as future work. On the other hand, the word-based model simply translates this word to autores, which is the translation for authors.\nAn example of an instance that a unseen word correctly is the plural of the noun reconstruc\u0327a\u0303o. This is done by learning the general Portuguese rule for nouns ending in -a\u0303o, which is converted into -o\u0303es, whereas in general nouns are converted into plural by simply adding an -s. The reason for generating the plural form, while the English word is in singular, is the fact that its preceding word as is an determiner for plural words. This hints that cross word dependencies in the generation model are perserved."}, {"heading": "4 RELATED WORK", "text": "Our work is related to the recent advances in neural machine translation models, where a single neural network is trained to maximize the conditional probability of the target sentence t, given the source s. The different models proposed define different architectures to estimate this probability. These include the usage of convolutional archituctures (Kalchbrenner & Blunsom, 2013), LSTM encoder-decoders (Sutskever et al., 2014) and attention-based models (Bahdanau et al., 2015). However, in all these approaches, the representation and generation of words are always performed at the word level, using a word lookup table and softmax.\nIn our work, we focus on the definition of mechanisms to perform the word representation and generation on the character level. Thus, our methods are applicable to neural MT models proposed previously. On the representation level, it has been shown that words can be composed using character-based approaches (Santos & Zadrozny, 2014; Kim et al., 2015; Ling et al., 2015; Ballesteros et al., 2015).\nGenerating output as a sequence of characters is receiving increasing attention in other domains. For example, speech recognition models (Chan et al., 2015; Maas et al., 2015) and language modeling (Sutskever et al., 2011; Mikolov et al., 2012). While this work is the first neural architecture we are aware of that uses hierarchical models, a variety of Bayesian language and translation models have been proposed that use subword models to generate word types which are in turn used to generate text (Chahuneau et al., 2013a; Goldwater et al., 2011)."}, {"heading": "5 CONCLUSION", "text": "In this work, we presented an approach to perform automatic translation using characters as atomic units. Our approach uses compositional models to compose words from individual characters in order to learn orthographically sentient word representations. Then, we define a generation model to produce translated words at the character level. We show that our methods can improve over equivalent word-based neural translation models, as our models can learn to interpret and generate unseen words.\nAs we present an end-to-end translation system that makes the open vocabulary assumption, we leave much room for future work, as our models make very simplistic assumptions about language. Much of the prior information regarding morphology (Chahuneau et al., 2013b), cognates (Beinborn et al., 2013) and rare word translation (Sennrich et al., 2015) among others, should be incorporated for better translation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["REFERENCES Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Ballesteros", "Miguel", "Dyer", "Chris", "Smith", "Noah A"], "venue": "In Proc. EMNLP,", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Cognate production using character-based machine translation", "author": ["Beinborn", "Lisa", "Zesch", "Torsten", "Gurevych", "Iryna"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Beinborn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Beinborn et al\\.", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["Brown", "Peter F", "Pietra", "Vincent J. Della", "Stephen A. Della", "Mercer", "Robert L"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Knowledge-rich morphological priors for Bayesian language models", "author": ["Chahuneau", "Victor", "Dyer", "Chris", "Smith", "Noah A"], "venue": "In Proc. NAACL,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Chahuneau", "Victor", "Schlinger", "Eva", "Smith", "Noah A", "Dyer", "Chris"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Listen, attend, and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["Chiang", "David"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Chiang and David.,? \\Q2005\\E", "shortCiteRegEx": "Chiang and David.", "year": 2005}, {"title": "Producing power-law distributions and damping word frequencies with two-stage language models", "author": ["Goldwater", "Sharon", "Griffiths", "Thomas L", "Johnson", "Mark"], "venue": null, "citeRegEx": "Goldwater et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2011}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyvarinen", "Aapo"], "venue": null, "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Information Retrieval: Computational and Theoretical Aspects", "author": ["Heaps", "Harold S"], "venue": null, "citeRegEx": "Heaps and S.,? \\Q1978\\E", "shortCiteRegEx": "Heaps and S.", "year": 1978}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "Seattle, October", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "CoRR, abs/1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Koehn", "Philipp"], "venue": "In Conference Proceedings: the tenth Machine Translation Summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Lu\u0131\u0301s", "Tiago", "Marujo", "Astudillo", "Ram\u00f3n Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "In Proc. EMNLP,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Maas", "Andrew L", "Xie", "Ziang", "Jurafsky", "Dan", "Ng", "Andrew Y"], "venue": "In Proc. NAACL,", "citeRegEx": "Maas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "Subword language modeling with neural networks. preprint (http://www", "author": ["Mikolov", "Tom\u00e1\u0161", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "Hai-Son", "Kombrink", "Stefan", "J. Cernocky"], "venue": "fit. vutbr. cz/imikolov/rnnlm/char. pdf),", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages. In The 50th Annual Meeting of the Association for Computational Linguistics", "author": ["Nakov", "Preslav", "Tiedemann", "J\u00f6rg"], "venue": "Proceedings of the Conference,", "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Substring-based machine translation", "author": ["Neubig", "Graham", "Watanabe", "Taro", "Mori", "Shinsuke", "Kawahara", "Tatsuya"], "venue": "Machine Translation,", "citeRegEx": "Neubig et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Cicero D", "Zadrozny", "Bianca"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra"], "venue": "CoRR, abs/1508.07909,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey"], "venue": "In Proc. ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Can we translate letters", "author": ["Vilar", "David", "Peter", "Jan-T", "Ney", "Hermann"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 27, "context": "In the past, efforts at performing translation at the character-level (Vilar et al., 2007) or subwordlevel (Neubig et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 21, "context": ", 2007) or subwordlevel (Neubig et al., 2013) have failed to produce competitive results compared to word-based counterparts (Brown et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 4, "context": ", 2013) have failed to produce competitive results compared to word-based counterparts (Brown et al., 1993; Koehn et al., 2007; Chiang, 2005), with the exception of closely related languages (Nakov & Tiedemann, 2012).", "startOffset": 87, "endOffset": 141}, {"referenceID": 27, "context": "We show that contrarily to previous belief (Vilar et al., 2007; Neubig et al., 2013), models that work at the character level can generate results competitive with word-based models.", "startOffset": 43, "endOffset": 84}, {"referenceID": 21, "context": "We show that contrarily to previous belief (Vilar et al., 2007; Neubig et al., 2013), models that work at the character level can generate results competitive with word-based models.", "startOffset": 43, "endOffset": 84}, {"referenceID": 4, "context": ", 2013) have failed to produce competitive results compared to word-based counterparts (Brown et al., 1993; Koehn et al., 2007; Chiang, 2005), with the exception of closely related languages (Nakov & Tiedemann, 2012). However, developing sequence to sequence models that are capable at reading and generating and generating words at the character basis is attractive for multiple reasons. Firstly, it opens the possibility for models reason about unseen source words, such as morphological variants of observed words. Secondly, it allows the production of unseen target words effectively recasting translation as an open vocabulary task. Finally, we benefit from a significant reduction of the source and target vocabulary size as only characters need to be modelled explicitly. As the number of word types increases rapidly with the size of the dataset Heaps (1978), while the number of letter types in the majority languages is fixed, character MT models can potentially solve many scalability issues in MT, both in terms of computational speed and memory requirements.", "startOffset": 88, "endOffset": 867}, {"referenceID": 0, "context": "While our model can be applied to any neural translation model , we shall adapt the attention-based translation model presented in (Bahdanau et al., 2015), which is described as follows.", "startOffset": 131, "endOffset": 154}, {"referenceID": 27, "context": "However, unlike previous approaches that attempt to discard the notion of words completely (Vilar et al., 2007; Neubig et al., 2013), we propose an hierarhical architecture, which replaces the word lookup tables (steps 1 and 3) and the word softmax (step 6) with character-based alternatives, which compose the notion of words from individual characters.", "startOffset": 91, "endOffset": 132}, {"referenceID": 21, "context": "However, unlike previous approaches that attempt to discard the notion of words completely (Vilar et al., 2007; Neubig et al., 2013), we propose an hierarhical architecture, which replaces the word lookup tables (steps 1 and 3) and the word softmax (step 6) with character-based alternatives, which compose the notion of words from individual characters.", "startOffset": 91, "endOffset": 132}, {"referenceID": 17, "context": "Character-based Word Representation The work in (Ling et al., 2015; Ballesteros et al., 2015) proposes a compositional model for learning word vectors from characters.", "startOffset": 48, "endOffset": 93}, {"referenceID": 1, "context": "Character-based Word Representation The work in (Ling et al., 2015; Ballesteros et al., 2015) proposes a compositional model for learning word vectors from characters.", "startOffset": 48, "endOffset": 93}, {"referenceID": 0, "context": "In previous work (Bahdanau et al., 2015), decoding is performed using beam search.", "startOffset": 17, "endOffset": 40}, {"referenceID": 3, "context": "Inspired by previous work on training deep multi-layered perceptrons (Bengio et al., 2007), we start by training the attention and V2C models, which are directly linked to the output of the model.", "startOffset": 69, "endOffset": 90}, {"referenceID": 12, "context": "A problem with attention models is the fact that finding the latent attention coefficients ai, requires a large number of epochs (Hermann et al., 2015).", "startOffset": 129, "endOffset": 151}, {"referenceID": 0, "context": "Furthermore, as the attention model defined in (Bahdanau et al., 2015) does not define any domain knowledge regarding word alignments, such as distortion, symmetry and fertility (Brown et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 4, "context": ", 2015) does not define any domain knowledge regarding word alignments, such as distortion, symmetry and fertility (Brown et al., 1993) this model is likely to overfit for small amounts of data.", "startOffset": 115, "endOffset": 135}, {"referenceID": 22, "context": "Finally, evaluation is performed using BLEU (Papineni et al., 2002) and we always use a single sentence as reference.", "startOffset": 44, "endOffset": 67}, {"referenceID": 22, "context": "Training is performed until there is no BLEU (Papineni et al., 2002) improvement for 5 epochs on the development set.", "startOffset": 45, "endOffset": 68}, {"referenceID": 27, "context": "While, differences are not significant, this is a strong results if we consider that previous work (Vilar et al., 2007; Neubig et al., 2013) revealed significantly lower translation quality using character-based approaches.", "startOffset": 99, "endOffset": 140}, {"referenceID": 21, "context": "While, differences are not significant, this is a strong results if we consider that previous work (Vilar et al., 2007; Neubig et al., 2013) revealed significantly lower translation quality using character-based approaches.", "startOffset": 99, "endOffset": 140}, {"referenceID": 26, "context": "These include the usage of convolutional archituctures (Kalchbrenner & Blunsom, 2013), LSTM encoder-decoders (Sutskever et al., 2014) and attention-based models (Bahdanau et al.", "startOffset": 109, "endOffset": 133}, {"referenceID": 0, "context": ", 2014) and attention-based models (Bahdanau et al., 2015).", "startOffset": 35, "endOffset": 58}, {"referenceID": 15, "context": "On the representation level, it has been shown that words can be composed using character-based approaches (Santos & Zadrozny, 2014; Kim et al., 2015; Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 107, "endOffset": 195}, {"referenceID": 17, "context": "On the representation level, it has been shown that words can be composed using character-based approaches (Santos & Zadrozny, 2014; Kim et al., 2015; Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 107, "endOffset": 195}, {"referenceID": 1, "context": "On the representation level, it has been shown that words can be composed using character-based approaches (Santos & Zadrozny, 2014; Kim et al., 2015; Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 107, "endOffset": 195}, {"referenceID": 7, "context": "For example, speech recognition models (Chan et al., 2015; Maas et al., 2015) and language modeling (Sutskever et al.", "startOffset": 39, "endOffset": 77}, {"referenceID": 18, "context": "For example, speech recognition models (Chan et al., 2015; Maas et al., 2015) and language modeling (Sutskever et al.", "startOffset": 39, "endOffset": 77}, {"referenceID": 25, "context": ", 2015) and language modeling (Sutskever et al., 2011; Mikolov et al., 2012).", "startOffset": 30, "endOffset": 76}, {"referenceID": 19, "context": ", 2015) and language modeling (Sutskever et al., 2011; Mikolov et al., 2012).", "startOffset": 30, "endOffset": 76}, {"referenceID": 9, "context": "While this work is the first neural architecture we are aware of that uses hierarchical models, a variety of Bayesian language and translation models have been proposed that use subword models to generate word types which are in turn used to generate text (Chahuneau et al., 2013a; Goldwater et al., 2011).", "startOffset": 256, "endOffset": 305}, {"referenceID": 2, "context": ", 2013b), cognates (Beinborn et al., 2013) and rare word translation (Sennrich et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 24, "context": ", 2013) and rare word translation (Sennrich et al., 2015) among others, should be incorporated for better translation.", "startOffset": 34, "endOffset": 57}], "year": 2015, "abstractText": "We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.", "creator": "LaTeX with hyperref package"}}}