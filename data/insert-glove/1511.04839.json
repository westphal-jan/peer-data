{"id": "1511.04839", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Nonparametric Canonical Correlation Analysis", "abstract": "Canonical adjuvant correlation analysis (CCA) apprise is a 1.6450 fundamental sonde technique in multi - (573) view falaj data polyhedron analysis warrant98 and representation learning. Several small-size nonlinear restraints extensions of ragbag the classical linear latveria CCA 171.8 method retargeted have greengage been warburton proposed, zagel including mellors kernel and deep jvs neural mestizaje network 46.93 methods. These approaches 104.97 restrict neulander attention antipasti to akihito certain 27-billion families www.netarrant.net of sail nonlinear projections, which pathfinding the user must vadis specify (aiton by corkhill choosing a koty kernel or florek a statuary neural network xiangming architecture ), and are chatillon computationally trancoso demanding. Interestingly, the theory 9.97 of sub-labels nonlinear nocera CCA hollandiae without 48-7 any functional restrictions, has nimmons been studied sardeh in the population morrisburg setting by Lancaster already tolentino in the ellerbe 50 ' s. However, puiset these results, portray have gemellus not hotmail inspired yadollah practical sproule algorithms. In svensk this paper, transphobic we rethymnon revisit Lancaster ' 35-a s grimsbury theory, baskins and shreds use seminaries it puits to devise lippstadt a practical guineans algorithm saint-malo for forty nonparametric CCA (NCCA ). whammy Specifically, we knoppix show that arcimboldo the colophons most duelo correlated shimonoseki nonlinear projections of 1,243 two addicting random vectors can eadberht be expressed in 54-year-old terms turnback of coiffed the madrassas singular shahabuddin value euro900 decomposition of a certain operator conservancy associated with their 3,429 joint density. Thus, by mu\u00f1eca estimating traminer the black-body population kalaw density 83-run from tweakui data, sassanians NCCA soulfully reduces to kyawswa solving four-barrel an chivalric eigenvalue sankei system, 54.08 superficially hampton like viruses kernel lechlade CCA but, nrw importantly, nannestad without having to compute 3,529 the inverse kayembe of kaons any 84.16 kernel terrify matrix. microprocessing We also derive a partially inequities linear maricar CCA (PLCCA) 131.75 variant in isom which one of metafictional the views anterior undergoes a seminary linear shope projection while brauerei the other i-278 is nebo nonparametric. bryd PLCCA turns out to adipati have 0-21 a similar form emoscosocoxnews.com to marti\u0107 the togias classical linear CCA, but with 1928-1929 a nonparametric tarikh regression term jarry replacing the u\u011fur linear regression in c02 CCA. husamettin Using karasuma a kishida kernel warcraft density herlong estimate 22.11 based on funimation a emmi small number of cycnus nearest clifftops neighbors, piailug our NCCA intra-abdominal and kolb PLCCA algorithms are memory - efficient, asjha often run much goldlion faster, soloviev and achieve ship-based better mujo performance odense than rosenblith kernel difc CCA and lamarre comparable performance borowsky to deep CCA.", "histories": [["v1", "Mon, 16 Nov 2015 06:25:59 GMT  (753kb)", "https://arxiv.org/abs/1511.04839v1", "Submission to ICLR 2016"], ["v2", "Tue, 17 Nov 2015 16:26:17 GMT  (753kb)", "http://arxiv.org/abs/1511.04839v2", "Submission to ICLR 2016"], ["v3", "Sun, 10 Jan 2016 15:16:00 GMT  (754kb)", "http://arxiv.org/abs/1511.04839v3", "Submission to ICLR 2016"], ["v4", "Sun, 7 Feb 2016 16:11:45 GMT  (1436kb)", "http://arxiv.org/abs/1511.04839v4", null]], "COMMENTS": "Submission to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tomer michaeli", "weiran wang", "karen livescu"], "accepted": true, "id": "1511.04839"}, "pdf": {"name": "1511.04839.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Canonical Correlation Analysis", "authors": ["Tomer Michaeli", "Weiran Wang", "Karen Livescu"], "emails": ["tomer.m@technion.ac.il", "weiranwang@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n04 83\n9v 4\n[ cs\n.L G\n] 7\nF eb"}, {"heading": "1 Introduction", "text": "A common task in data analysis is to reveal the common variability in multiple views of the same phenomenon, while suppressing view-specific noise factors. Canonical correlation analysis (CCA) [Hotelling, 1936] is a classical statistical technique that targets this goal. In CCA, linear projections of two random vectors are sought, such that the resulting low-dimensional vectors are maximally correlated. This tool has found widespread use in various fields, including recent application to natural language processing [Dhillon et al., 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al., 2014].\nOne of the shortcomings of CCA is its restriction to linear mappings, since many real-world multi-view datasets exhibit highly nonlinear relationships. To overcome this limitation, several extensions of CCA have been proposed for finding maximally correlated nonlinear projections. In kernel CCA (KCCA) [Akaho, 2001, Melzer et al., 2001, Bach and Jordan, 2002, Hardoon et al., 2004], these nonlinear mappings are chosen from two reproducing kernel Hilbert spaces (RKHS). In deep CCA (DCCA) [Andrew et al., 2013], the projections are obtained from two deep neural networks that are trained to output maximally correlated vectors. Nonparametric CCA-type methods, which are not limited to specific function classes, include the alternating conditional expectations (ACE) algorithm and its extensions [Breiman and Friedman, 1985, Balakrishnan et al., 2012, Makur et al., 2015]. Nonlinear CCA methods are advantageous over linear CCA in a range of applications [Hardoon et al., 2004, Melzer et al., 2001, Wang et al., 2015b]. However, existing nonlinear CCA approaches are very computationally demanding, and are often impractical to apply on large data.\nInterestingly, the problem of finding the most correlated nonlinear projections of two random variables has been studied by Lancaster [1958] and Hannan [1961], long before the derivation of KCCA, DCCA and ACE. They characterized the optimal projections in the population setting, without restricting the solution to an RKHS or to have any particular parametric form. However, these theoretical results have not inspired practical algorithms.\nIn this paper, we revisit Lancaster\u2019s theory, and use it to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution to the nonlinear CCA problem can be expressed in terms of the singular value decomposition (SVD) of a certain operator, which is defined via the population density. Therefore, to obtain a practical method, we estimate the density from training data and use the estimate in the solution. The resulting algorithm reduces to solving an eigenvalue system with a particular kernel that depends on the joint distribution between the views. While superficially similar to other eigenvalue methods, it is fundamentally different from them and in particular has crucial advantages over KCCA. For example, unlike KCCA, NCCA does not involve computing the inverse of any matrix, making it computationally feasible on large data where KCCA (even using approximation techniques) is impractical. We elucidate this and other contrasts in Sec. 3 below. We show that NCCA achieves state-of-the art performance, while being much more computationally efficient than KCCA and DCCA.\nIn certain situations, nonlinearity is needed for one view but not for the other. In such cases, it may be advantageous to constrain the projection of the second view to be linear. An additional contribution of this paper is the derivation of a closed-form solution to this partially linear CCA (PLCCA) problem in the population setting. We show that PLCCA has essentially the same form as linear CCA, but with the optimal linear predictor term in CCA replaced by an optimal nonlinear predictor in PLCCA. Thus, moving from the population setting to sample data entails simply using nonlinear regression to estimate this predictor. The resulting algorithm is efficient and, as we demonstrate on realistic data, sometimes matches DCCA and significantly outperforms CCA and KCCA."}, {"heading": "2 Background", "text": "We start by reviewing the original CCA algorithm [Hotelling, 1936]. Let X \u2208 RDx and Y \u2208 RDy be two random vectors (views). The goal in CCA is to find a pair of L-dimensional projections W\u22a41 X , W \u22a4 2 Y that are maximally correlated, but where different dimensions within each view are constrained to be uncorrelated. Assuming for notational simplicity that X and Y have zero mean, the CCA problem can be written as1\nmax W1,W2 E\n[\n( W\u22a41 X )\u22a4( W\u22a42 Y ) ]\n(1)\ns.t. E [ ( W\u22a41 X )( W\u22a41 X )\u22a4] = E [ ( W\u22a42 Y )( W\u22a42 Y )\u22a4] = I,\nwhere the maximization is over W1 \u2208 RDx\u00d7L,W2 \u2208 RDy\u00d7L. This objective has been extensively studied and is known to be optimal in several senses: It maximizes the mutual information for certain distributions p(x,y) [Borga, 2001], maximizes the likelihood for certain latent variable models [Bach and Jordan, 2005], and is equivalent to the information bottleneck method when p(x,y) is Gaussian [Chechik et al., 2005].\nThe CCA solution can be expressed as (W1,W2) = (\u03a3 \u22121/2 xx U,\u03a3 \u22121/2 yy V), where \u03a3xx = E[XX \u22a4], \u03a3yy = E[Y Y \u22a4], \u03a3xy = E[XY \u22a4], and U \u2208 RDx\u00d7L and V \u2208 RDy\u00d7L are the top L left and right singular vectors of the matrix T = \u03a3\u22121/2xx \u03a3xy\u03a3 \u22121/2 yy (see [Mardia et al., 1979]). In practice, the joint distribution p(x,y) is rarely known, and only paired multi-view samples {(xi,yi)}Ni=1 are available, so the population covariances are replaced by their empirical estimates.2\nTo facilitate the analogy with partially linear CCA (Sec. 3.2), we note that the CCA solution can also be expressed in terms of the optimal predictor (in the mean squared error sense) of X from Y , given by X\u0302 = \u03a3xy\u03a3 \u22121 yy Y , and its covariance \u03a3x\u0302x\u0302 = \u03a3xy\u03a3 \u22121 yy \u03a3yx. Specifically, U corresponds to the eigenvectors of K = TT \u22a4 = \u03a3\u22121/2xx \u03a3x\u0302x\u0302\u03a3 \u22121/2 xx , and, by algebraic manipulation, the optimal projections can be written as\nW\u22a41 X = U \u22a4\u03a3\n\u2212 1 2\nxx X, W \u22a4 2 Y = D \u2212 1 2U\u22a4\u03a3\n\u2212 1 2 xx X\u0302, (2)\nwhere D is a diagonal matrix with the top L eigenvalues of K on its diagonal. Since the representation power of linear mappings is limited, several nonlinear extensions of problem (1) have been proposed. These methods find two maximally correlated nonlinear projections f : RDx \u2192 RL and g : RDy \u2192 RL by 1Here and throughout, expectations are with respect to the joint distribution of all random variables (capital letters) appearing within the square brackets of the expectation operator E. 2\u03a3xy \u2248 1\nN\n\u2211N i=1 xiy \u22a4 i and similarly for \u03a3xx and \u03a3yy .\nsolving\nmax f\u2208A,g\u2208B\nE [ f(X)\u22a4g(Y ) ]\n(3)\ns.t. E [ f(X)f(X)\u22a4 ] = E [ g(Y )g(Y )\u22a4 ] = I,\nwhere A and B are two families of (possibly nonlinear) measurable functions. Observe that if (f(x),g(y)) is a solution to (3), then (Rf(x),Rg(y)) is also a solution, for any orthogonal matrix R. This ambiguity can be removed by adding the additional constraints E[fi(X)gj(Y )] = 0, \u2200i 6= j (see, e.g., Hardoon et al. [2004]). Here we do not pursue this route, and simply focus on one solution among this family of solutions.\nAlternating conditional expectations (ACE): The ACE method [Breiman and Friedman, 1985] treats the case of a single projection (L = 1), where B is the class of all zero-mean scalar-valued functions g(Y ), and A is the class of additive models f(X) =\n\u2211Dx \u2113=1 \u03b3\u2113\u03c6\u2113(X\u2113) with zero-mean scalar-valued functions \u03c6\u2113(X\u2113). The ACE algorithm\nminimizes the objective (3) by iteratively computing the conditional expectation of each view given the other. Recently, Makur et al. [2015] extended ACE to multiple dimensions by whitening the vector-valued f(X) and g(Y ) during each iteration. In practice, the conditional expectations are estimated from training data using nonparametric regression. Since this computationally demanding step has to be repeatedly applied until convergence, ACE and its extensions are impractical to apply on large data.\nKernel CCA (KCCA): In KCCA [Lai and Fyfe, 2000, Akaho, 2001, Melzer et al., 2001, Bach and Jordan, 2002, Hardoon et al., 2004], A and B are two reproducing kernel Hilbert spaces (RKHSs) associated with user-specified kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7). By the representer theorem, the projections can be written in terms of the training samples as f\u2113(x) = \u2211N i=1 \u03b1i,\u2113kx(x,xi) and g\u2113(y) = \u2211N i=1 \u03b2i,\u2113kx(y,yi) with some coefficients {\u03b1i,\u2113} and {\u03b2i,\u2113}. Letting Kx = [kx(xi,xj)] and Ky = [ky(yi,yj)] denote the N\u00d7N kernel matrices, the optimal coefficients can be computed from the top L eigenvectors of the matrix (Kx+ rxI)\u22121Ky(Ky+ ryI)\u22121Kx, where rx and ry are positive regularization parameters. Computation of the exact solution is intractable for large datasets due to the memory cost of storing the kernel matrices and the time complexity of solving dense eigenvalue systems. Several approximate techniques have been proposed, largely based on low-rank kernel matrix approximations [Bach and Jordan, 2002, Hardoon et al., 2004, Arora and Livescu, 2012, Lopez-Paz et al., 2014].\nDeep CCA (DCCA): In the more recently proposed DCCA approach [Andrew et al., 2013], A and B are the families of functions that can be implemented using two deep neural networks of predefined architecture. As a parametric method, DCCA scales better than approximate KCCA for large datasets [Wang et al., 2015b].\nPopulation solutions: Lancaster [1958] studied a variant of problem (3), where A and B are the families of all measurable functions. This setting may seem too unrestrictive. However, it turns out that in the population setting, the optimal projections are well-defined even without imposing smoothness in any way. Lancaster characterized the optimal (possibly nonlinear) mappings fi and gi for one-dimensional X and Y (Dx = Dy = 1). In particular, he showed that if X,Y are jointly Gaussian, then the optimal projections are Hermite polynomials. Eagleson [1964] extended this analysis to the Gamma, Poisson, binomial, negative binomial, and hypergeometric distributions. Hannan [1961] gave Lancaster\u2019s characterization a functional analysis interpretation, which confirmed its validity also for multi-dimensional views.\nOur approach: Lancaster\u2019s population solution has never been used for devising a practical CCA algorithm that works with sample data. Here, we revisit Lancaster\u2019s result, extend it to a semi-parametric setting, and devise practical algorithms that work with sample data. Clearly, in the finite-sample setting, it is necessary to impose smoothness. Our approach to imposing smoothness is different from KCCA, which formulates the problem as one of finding the optimal smooth solution (in an RKHS) and then approximates it from samples. Here, we first derive the optimal solution among all (not necessarily smooth) measurable functions, and then approximate it by using smoothed versions of the true densities, which we estimate from data. As we show, the resulting algorithm has significant advantages over KCCA."}, {"heading": "3 Nonparametric and partially linear CCA", "text": "We treat the following two variants of the nonlinear CCA problem (3): (i) Nonparametric CCA in which both A and B are the sets of all (nonparametric) measurable functions; (ii) Partially linear CCA (PLCCA), in which A is the set of all linear functions f(x) = WTx, and B is the set of all (nonparametric) measurable functions g(y). We start by deriving closed-form solutions in the population setting, and then plug in an empirical estimate of p(x,y)."}, {"heading": "3.1 Nonparametric CCA (NCCA)", "text": "Let A and B be the sets of all (nonparametric) measurable functions of X and Y , respectively. Note that the coordinates of f(x) and g(y) are constrained to satisfy E[f2i (X)] = E[g 2 i (Y )] = 1, so that we may write (3) as an optimization problem over the Hilbert spaces\nHx = { q : RDx \u2192 R \u2223 \u2223 E[q2(X)] < \u221e } , Hy = { u : RDy \u2192 R \u2223 \u2223 E[u2(Y )] < \u221e } ,\nwhich are endowed with the inner products \u3008q, r\u3009Hx = E[q(X)r(X)] and \u3008u, v\u3009Hy = E[u(Y )v(Y )]. To do so, we express the correlation between fi(X) and gi(Y ) as\nE[fi(X)gi(Y )] =\n\u222b\nfi(x)\n( \u222b\ngi(y)s(x,y)p(y)dy\n)\np(x)dx = \u3008fi,Sgi\u3009Hx , (4)\nwhere3\ns(x,y) = p(x,y)\np(x)p(y) (5)\nand S : Hy \u2192 Hx is the operator defined by4 (Su)(x) = \u222b u(y)s(x,y)p(y)dy. Thus, problem (3) can be written as\nmax \u3008fi,fj\u3009Hx=\u03b4ij \u3008gi,gj\u3009Hy=\u03b4ij\nL \u2211\ni=1\n\u3008Sgi, fi\u3009Hx , (6)\nwhere \u03b4ij is Kronecker\u2019s delta function. When S is a compact operator, the solution to problem (6) can be expressed in terms of its SVD (see e.g., [Bolla, 2013, Proposition A.2.8]). Specifically, in this case S possesses a discrete set of singular values \u03c31 \u2265 \u03c32 \u2265 . . . and corresponding left and right singular functions \u03c8i \u2208 Hx, \u03c6i \u2208 Hy , and the maximal value of the objective in (6) is precisely \u03c31 + . . .+ \u03c3L and is attained with\nfi(x) = \u03c8i(x), gi(y) = \u03c6i(y). (7)\nThat is, the optimal projections are the singular functions of S and the canonical correlations are its singular values: E[fi(X)gi(Y )] = \u03c3i.\nThe NCCA solution (7), has several interesting interpretations. First, note that log s(x,y) is the pointwise mutual information (PMI) between X and Y , which is a common measure of statistical dependence. Since the optimal projections are the top singular functions of s(x,y), the NCCA solution may be interpreted as an embedding which preserves as much of the (exponentiation of the) PMI between X and Y as possible. Second, note that the operator S corresponds to the optimal predictor (in mean square error sense) of one view based on the other, as (Sgi)(x) = E[gi(Y )|X = x] and (S\u2217fi)(y) = E[fi(X)|Y = y]. Therefore, the NCCA projections can also be thought of as approximating the best predictors of each view based on the other. Finally, note that rather than using SVD, the NCCA solution can be also expressed in terms of the eigen-decomposition of a certain operator. Specifically, the optimal view\n3Formally, s(x,y) is the Radon-Nikodym derivative of the joint probability measure w.r.t. the product of marginal measures, assuming the former is absolutely continuous w.r.t. the latter.\n4To see that Su \u2208 Hx for every u \u2208 Hy , note that (Su)(x) = E[u(Y )|X = x] and thus \u2016Su\u20162Hx = E[(E[u(Y )|X]) 2] \u2264 E[u2(Y )] =\n\u2016u\u20162 Hy < \u221e.\n1 projections are the eigenfunctions of K = SS\u2217 (and the view 2 projections are eigenfunctions of S\u2217S), which is the operator defined by (Kq)(x) = \u222b q(x)k(x,x\u2032)p(x)dx, with the kernel\nk(x,x\u2032) =\n\u222b\ns(x,y)s(x\u2032,y)p(y)dy. (8)\nThis shows that NCCA resembles other spectral dimensionality reduction algorithms, in that the projections are the eigenfunctions of some kernel. However, in NCCA, the kernel is not specified by the user. From (8), we see that k(x,x\u2032) corresponds to the inner product between s(x, \u00b7) and s(x\u2032, \u00b7) (equivalently p(y|x)/p(y) and p(y|x\u2032)/p(y)). Therefore, as visualized in Fig. 1, in NCCA x is considered similar to x\u2032 if the conditional distribution of Y given X = x is similar to that of Y given X = x\u2032.\nA sufficient condition for S to be compact is that it be a Hilbert-Schmidt operator, i.e., that \u222b\u222b\n|s(x, y)|2p(x)dx p(y)dy < \u221e.\nSubstituting (5), this condition can be equivalently written as E[s(X,Y )] < \u221e. This can be thought of as a requirement that the statistical dependence between X and Y should not be too strong. In this case, the singular values \u03c3i tend to zero as i tends to \u221e. Furthermore, the largest singular value of S is always \u03c31 = 1 and is associated with the constant functions\u03c81(x) = \u03c61(y) = 1. To see this, note that for any pair of unit-norm functions\u03c8 \u2208 Hx, \u03c6 \u2208 Hy , we have that \u3008\u03c8,S\u03c6\u3009Hx =E[\u03c8(X)\u03c6(Y )]\u2264 \u221a\nE[\u03c82(X)]E[\u03c62(Y )] = 1 and this bound is clearly attained with \u03c8(x) = \u03c6(y) = 1. Thus, we see that the first nonlinear CCA projections are always constant functions f1(x) = g1(y) = 1. These projections are perfectly correlated, but carry no useful information on the common variability in X and Y . Therefore, in practice, we discard them. The rest of the projections are orthogonal to the first and therefore have zero mean: E[f\u2113(X)] = E[g\u2113(Y )] = 0 for \u2113 \u2265 2."}, {"heading": "3.2 Partially linear CCA (PLCCA)", "text": "The above derivation of NCCA can be easily adapted to cases in which A and B are different families of functions. As an example, we next derive PLCCA, in which A is the set of all linear functions of X while B is still the set of all (nonparametric) measurable functions of Y .\nLet f(x) = W\u22a4x, where W \u2208 RDx\u00d7L. In this case, the constraint that E[f(X)f(X)\u22a4] = I corresponds to the restriction that W\u22a4\u03a3xxW = I. By changing variables to W\u0303 = \u03a3 1/2 xx W and denoting the ith column of W\u0303 by w\u0303i, the constraint simplifies to w\u0303\u22a4i w\u0303j = \u03b4ij . Furthermore, we can write the objective (3) as\nL \u2211\ni=1\nE\n[\nw\u0303\u22a4i \u03a3 \u2212 1 2 xx Xgi(Y )\n]\n=\nL \u2211\ni=1\nw\u0303\u22a4i E [ \u03a3 \u2212 1 2 xx E[X |Y ] gi(Y ) ] =\nL \u2211\ni=1\nw\u0303\u22a4i SPLgi, (9)\nwhere SPL : Hy \u2192 RDx is the operator defined by SPLu = \u03a3\u22121/2xx \u222b E[X |Y = y]u(y)p(y)dy. Therefore, Problem\n(3) now takes the form\nmax w\u0303\u22a4i w\u0303j=\u03b4ij\n\u3008gi,gj\u3009Hy=\u03b4ij\nL \u2211\ni=1\nw\u0303\u22a4i SPLgi, (10)\nwhich is very similar to (6). Note that here the domain of the operator SPL is infinite dimensional (the space Hy), but its range is finite-dimensional (the Euclidian space RDx). Therefore, SPL is guaranteed to be compact without any restrictions on the joint probability p(x,y). The optimal w\u0303i\u2019s are thus the top L singular vectors of SPL and the optimal gi\u2019s are the top L right singular functions of SPL.\nThe PLCCA solution can be expressed in more convenient form by noting that the optimal w\u0303i\u2019s are also the top L eigenvectors of the matrix KPL = SPLS\u2217PL, given by\nKPL = E\n[\n(\n\u03a3 \u2212 1 2 xx E[X |Y ]\n)(\n\u03a3 \u2212 1 2 xx E[X |Y ]\n)\u22a4]\n= \u03a3 \u2212 1\n2 xx \u03a3x\u0302x\u0302\u03a3\n\u2212 1 2 xx . (11)\nHere, \u03a3x\u0302x\u0302 = E[E[X |Y ]E[X |Y ]\u22a4] denotes the covariance of X\u0302 = E[X |Y ], the optimal predictor of X from Y . Denoting the top L eigenvectors of KPL by U, and reverting the change of variables, we get that W = \u03a3 \u22121/2 xx U.\nHaving determined the optimal f(x) = W\u22a4x, we can compute the optimal g(y) using the following lemma5.\nLemma 3.1. Assume that E[E[f(X)|Y ]E[f(X)|Y ]\u22a4] is a non-singular matrix. Then the function g optimizing (3) for a fixed f is given by\ng(Y ) = ( E [ E[f(X)|Y ]E[f(X)|Y ]\u22a4 ])\u2212 1 2 E[f(X)|Y ]. (12)\nSubstituting f(x) = W\u22a4x = U\u22a4\u03a3\u22121/2xx x into (12), we obtain that the partially linear CCA projections are\nW\u22a4X = U\u22a4\u03a3 \u2212 1 2 xx X, g(Y ) = D \u2212 1 2U\u22a4\u03a3 \u2212 1 2 xx X\u0302, (13)\nwhere D is the diagonal L\u00d7 L matrix that has the top L eigenvalues of KPL on its diagonal. Comparing (13) with (2), we see that PLCCA has the exact same form as CCA. The only difference is that here X\u0302 is the optimal nonlinear predictor of X from Y (a nonlinear function of Y ), whereas in CCA, X\u0302 corresponded to the best linear predictor of X from Y (a linear function of Y )."}, {"heading": "3.3 Practical implementations", "text": "The NCCA and PLCCA solutions require knowing the joint probability density p(x,y) of the views. Given a set of training data {(xi,yi)}Ni=1 drawn independently from p(x,y), we can estimate p(x,y) and plug it into our formulas. There are many ways of estimating this density. We next present the algorithms resulting from using one particular choice, namely the kernel density estimates (KDEs)\np\u0302(x) = 1N\n\u2211N\ni=1 w ( \u2016x\u2212 xi\u20162/\u03c32x ) , (14)\np\u0302(y) = 1N\n\u2211N\ni=1 w ( \u2016y\u2212 yi\u20162/\u03c32y ) ,\np\u0302(x,y) = 1N\n\u2211N\ni=1 w ( \u2016x\u2212 xi\u20162/\u03c32x + \u2016y \u2212 yi\u20162/\u03c32y ) ,\nwhere w(t) \u221d e\u2212t/2 is the Gaussian kernel, and \u03c3x and \u03c3y are the kernel widths of the two views. We note that, theoretically, KDEs suffer from the curse of dimensionality, and use of other density estimation methods is certainly possible. However, we make two important observations. First, real-world data sets often have low-dimensional manifold structure, and the KDE accuracy is affected only by the intrinsic dimensionality. As shown\n5A simpler version of this lemma, in which f(x) = y and g is linear, appeared in Eldar and Oppenheim [2003]. The proof of Lemma 3.1 is provided in the Supplemntary Material and follows closely that of [Eldar and Oppenheim, 2003, Theorem 1].\nin [Ozakin and Gray, 2009], if the data lies on an r-dimensional manifold, then the KDE converges to the true density at a rate of6 O(n\u2212 4r+4 ). Indeed, KDEs have been shown to work well in practice in relatively high dimensions [Georgescu et al., 2003], as is also confirmed in our experiments. Second, the NCCA algorithm resulting from working with KDEs involves the same Gaussian affinity matrices used in (Gaussian kernel) KCCA. Thus, intuitively, the amount of smoothness required for obtaining accurate results in high dimensions is similar for NCCA and KCCA. Nevertheless, NCCA has a clear advantage over KCCA in terms of both performance and computation.\nPLCCA Using the above KDEs, the conditional expectation x\u0302(y) = E[X |Y = y] needed for the PLCCA solution (13) reduces to the Nadaraya-Watson nonparametric regression [Nadaraya, 1964, Watson, 1964]\nx\u0302(y) =\n\u2211N i=1 w ( \u2016y \u2212 yi\u20162/\u03c32y )\nxi \u2211N\ni=1 w ( \u2016y \u2212 yi\u20162/\u03c32y )\n. (15)\nThe population moments \u03a3x\u0302x\u0302 = E[X\u0302X\u0302\u22a4] and \u03a3xx = E[XX\u22a4] can then be replaced by the empirical moments of {x\u0302(yi)} and {xi}.\nNCCA The quadratic form \u3008Sgi, fi\u3009Hx is given byE[(Sgi)(X)fi(X)] and is approximated by 1N \u2211N \u2113=1(Sgi)(x\u2113)f(x\u2113). Furthermore, (Sgi)(x\u2113) is equal to E[s(x\u2113, Y )gi(Y )] and thus can be approximated by 1N \u2211N m=1 s(x\u2113,ym)g(ym), where s(x\u2113,ym) = p(x\u2113,ym)\np(x\u2113)p(ym) . Therefore, defining the N \u00d7N matrix S = [s(x\u2113,ym)], and stacking the projections\nof the data points into the N \u00d7 1 vectors fi = 1\u221aN (fi(x1), . . . , fi(xN )) \u22a4 and gi = 1\u221aN (gi(y1), . . . , gi(yN )) \u22a4, the NCCA objective can be approximated by 1N \u2211L i=1 f \u22a4 i Sgi. Similarly, the NCCA constraints become f \u22a4 i fj = g \u22a4 i gj = \u03b4ij . This implies that the optimal fi and gi are the top L singular vectors of S. Recall that in the continuous formulation, the first pair of singular functions are constant functions. Therefore, in practice, we compute the top L + 1 singular vectors of S and discard the first one. To construct the matrix S we use the kernel density estimates (14) for joint and marginal probability distributions over (x,y).\nThe NCCA implementation, with the specific choice of Gaussian KDEs, is given in Algorithm 1. If the input dimensionality is too high, we first perform PCA on the inputs for more robust density estimates. To make our algorithm computationally efficient, we truncate the Gaussian affinities Wxij to zero if xi is not within the k-nearest neighbors of xj (similarly for view 2). This leads to a sparse matrix S, whose SVD can be computed efficiently.\nTo obtain out-of-sample mapping for a new view 1 test sample x, we use the Nystro\u0308m method [Williams and Seeger, 2001], which avoids recomputing SVD. Specifically, recall that the view 1 projections are the eigenfunctions of the positive definite kernel k(x,x\u2032) of (8). Computing this kernel function between x and the training samples leads to (notice the corresponding view 2 input of x is not needed)\nk(x,xi) =\nN \u2211\nm=1\ns(x,ym)s(xi,ym). (16)\nThus, applying the Nystro\u0308m method, the projections of x can be approximated as\nfi(x) = 1\n\u03c32i\nN \u2211\nn=1\nk(x,xn)fi(xn) = 1\n\u03c3i\nN \u2211\nn=1\ns(x,yn)gi(yn)\nfor i = 1, . . . , L + 1, where \u03c3i is the ith singular value of S. The second equality follows from substituting (16) and using the fact that fi and gi are singular vectors of S. Note again that since the affinity matrices are sparse, the mappings are computed via fast sparse matrix multiplication.\nRelationship with KCCA Notice that NCCA is not equivalent to KCCA with any kernel. KCCA requires two kernels, each of which only sees one view; the NCCA kernel (8) depends on both views through their joint distribution. In terms of practical implementation, our KDE-based NCCA solves a different eigenproblem and does not involve any full matrix inverses. Indeed, both methods compute the SVD of the matrix Q\u22121x W xWyQ\u22121y . However, in NCCA,\n6This requires normalizing the KDE differently, but the scaling cancels out in s(x,y) = p(x,y)/p(x)p(y).\nAlgorithm 1 Nonparametric CCA with Gaussian KDE\nInput: Training data {(xi,yi)}Ni=1, test sample x. 1: Construct affinity matrices for each view\nWxij \u2190 exp { \u2212 \u2016xi\u2212xj\u2016 2\n2\u03c32x\n} , Wyij \u2190 exp { \u2212 \u2016yi\u2212yj\u2016 2\n2\u03c32y\n}\n.\n2: Normalize Wx to be right stochastic and Wy to be left stochastic, i.e.,\nWxij \u2190 Wxij/ \u2211N l=1 W x il, W y ij \u2190 W y ij/ \u2211N l=1 W y lj .\n3: Form the matrix S \u2190 WxWy . 4: Compute U \u2208 RN\u00d7(L+1),V \u2208 RN\u00d7(L+1), the first L+1 left and right singular vectors of S, with corresponding\nsingular values \u03c31, . . . , \u03c3L+1. Output: At train time, compute the projections i = 1, . . . , L+ 1 of the training samples as\nfi(xn) \u2190 \u221a NUn,i, gi(yn) \u2190 \u221a NVn,i.\nAt test time, calculate a new row of Wx for x as\nWxN+1,j \u2190 exp { \u2212 \u2016x\u2212xj\u2016 2\n2\u03c32x\n}\n,\nWxN+1,j \u2190 WxN+1,j/ \u2211N l=1 W x N+1,l\nand a new row of S as SN+1 \u2190 WxN+1Wy , and compute the projections of x as\nfi(x) \u2190 1\n\u03c3i\nN \u2211\nn=1\nSN+1,n gi(yn), i = 1, . . . , L+ 1.\nQx,Qy are diagonal matrices containing the sums of rows/columns of Wx/Wy , whereas in KCCA, Qx = Wx+rxI, Qy = W\ny + ryI, for some positive regularization parameters rx, ry . Moreover, in NCCA this factorization gives the projections, whereas in KCCA it gives the coefficients in the RKHS.\nAn additional key distinction is that NCCA does not require regularization in order to be well defined. In contrast, KCCA must use regularization, as otherwise the matrix it factorizes collapses to the identity matrix, and the resulting projections are meaningless. This is due to the fact that KCCA attempts to estimate covariances in the infinitedimensional feature space, whereas NCCA is based on estimating probability densities in the primal space.\nThe resulting computational differences are striking. The number of training samples N is often such that the N \u00d7 N matrices in either NCCA or KCCA cannot even be stored in memory. However, these matrices are sparse, with only kN entries if we retain k neighbors. Therefore, in NCCA the storage problem is alleviated and matrix multiplication and eigendecomposition are O(kN2) operations instead of O(N3). In KCCA, one cannot take advantage of truncated kernel affinities, because of the need to compute the inverses of kernel matrices, which are in general not sparse, so direct computation is often infeasible in terms of both memory and time. Low-rank KCCA approximations (as used in our experiments below) with rank M have a time complexity O(M3 +M2N), which is still challenging with typical ranks in the thousands or tens of thousands."}, {"heading": "4 Related work", "text": "Several recent multi-view learning algorithms use products or sums of single-view affinity matrices, diffusion matrices, or Markov transition matrices. The combined kernels constructed in these methods resemble our matrix S = WxWy . Such an approach has been used, for example, for multi-view spectral clustering [de Sa, 2005, Zhou and Burges, 2007, Kumar et al., 2011], metric fusion [Wang et al., 2012], common manifold learning [Lederman and Talmon, 2014], and\nmulti-view nonlinear system identification [Boots and Gordon, 2012]. Note, however, that in NCCA the matrix S corresponds to the product WxWy only when using a separable Gaussian kernel for estimating the joint density p(x,y). If a non-separable density estimate is used, then the matrix S no longer resembles the previously proposed multiview kernels. Furthermore, although algorithmically similar, NCCA arises from a completely different motivation: It maximizes the correlation between the views, whereas these other methods do not."}, {"heading": "5 Experiments", "text": "In the following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features (FKCCA, Lopez-Paz et al. [2014]) and Nystro\u0308m approximation (NKCCA, Williams and Seeger [2001]) as described in Wang et al. [2015b], and deep CCA (DCCA, Andrew et al. [2013]).\nIllustrative example We begin with the 2D synthetic dataset (1000 training samples) in Fig. 2(a,b), where samples of the two input manifolds are colored according to their common degree of freedom. Clearly, a linear mapping in view 1 cannot unfold the manifold to align the two views, and linear CCA indeed fails (results not shown). We extract a one-dimensional projection for each view using different nonlinear CCAs, and plot the projection g(y) vs. f(x) of test data (a different set of 1000 random samples from the same distribution) in Fig. 2(c-f). Since the second view is essentially a linear manifold (plus noise), for NKCCA we use a linear kernel in view 2 and a Gaussian kernel in view 1, and for DCCA we use a linear network for view 2 and two hidden layers of 512 ReLU units for view 1. Overall, NCCA achieves better alignment of the views while compressing the noise (variations not described by the common degree of freedom). While DCCA also succeeds in unfolding the view 1 manifold, it fails to compress the noise.\nX-Ray Microbeam Speech Data The University of Wisconsin X-Ray Micro-Beam (XRMB) corpus [Westbury, 1994] consists of simultaneously recorded speech and articulatory measurements. Following Andrew et al. [2013] and Lopez-Paz et al. [2014], the acoustic view inputs are 39D Mel-frequencey cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2019JW11\u2019. As in [Lopez-Paz et al., 2014], we randomly shuffle the frames and generate splits of 30K/10K/11K frames for training/tuning/testing, and we refer to the result as the \u2019JW11-s\u2019 setup (random splits better satisfy the i.i.d. assumption of train/tune/test data than splits by utterances as in [Andrew et al., 2013]). We extract 112D projections with each algorithm and measure the total correlation between the two views of the test set, after an additional 112D linear CCA. As in prior work, for both FKCCA and NKCCA we use rank-6000 approximations for the kernel matrices; for DCCA we use two ReLU [Nair and Hinton, 2010] hidden layers of width 1800/1200 for view 1/2 respectively and run stochastic optimization with minibatch size 750\nas in [Wang et al., 2015a] for 100 epochs. Kernel widths for FKCCA/NKCCA, learning rate and momentum for DCCA, kernel widths and neighborhood sizes for NCCA/PLCCA are selected by grid search based on total tuning set correlation. Sensitivity to their values is mild over a large range; e.g., setting the kernel widths to 30-60% of the sample L2 norm gives similarly good results. For NCCA/PLCCA, input dimensionalities are first reduced by PCA to 20% of the original ones (except that PLCCA does not apply PCA for view 2 in order to extract a 112D projection). The total correlation achieved by each algorithm is given in Table 1. We also report the running time (in seconds) of the algorithms (measured with a single thread on a workstation with a 3.2GHz CPU and 56G main memory), each using its optimal hyperparameters, and including the time for exact 15-nearest neighbor search for NCCA/PLCCA. Overall, NCCA achieves the best canonical correlation while being much faster than the other nonlinear methods.\nNoisy MNIST handwritten digits dataset We now demonstrate the algorithms on a noisy MNIST dataset, generated identically to that of Wang et al. [2015b] but with a larger training set. View 1 inputs are randomly rotated images (28 \u00d7 28, gray scale) from the original MNIST dataset [LeCun et al., 1998], and the corresponding view 2 inputs are randomly chosen images with the same identity plus additive uniform pixel noise. We generate 450K/10K/10K pairs of images for training/tuning/testing (Wang et al. [2015b] uses a 50K-pair training set). This dataset satisfies the multi-view assumption that given the label, the views are uncorrelated, so that the most correlated subspaces should retain class information and exclude the noise. Following Wang et al. [2015b], we extract a low-dimensional projection of the view 1 images with each algorithm, run spectral clustering to partition the splits into 10 classes (with clustering parameters tuned as in [Wang et al., 2015b]), and compare the clustering with ground-truth labels and report the clustering accuracy. We also train a one-vs.-one linear SVM [Chang and Lin, 2011] on the projections with highest cluster accuracy for each algorithm (we reveal labels of 10% of the training set for fast SVM training) and report the classification error rates. The tuning procedure is as for XRMB except that we now select the projection dimensionality from {10, 20, 30}. For NCCA/PLCCA we first reduce dimensionality to 100 by PCA for density estimation and exact nearest neighbor search, and use a randomized algorithm [Halko et al., 2011] to compute the SVD of the 450K \u00d7 450K matrix S; for RKCCA/NKCCA we use an approximation rank of 5000; for DCCA we use 3 ReLU hidden layers of 1500 units in each view and train with stochastic optimization of minibatch size 4500. Clustering and classification results on the original 784D view 1 inputs are recorded as the baseline. Table 2 shows the clustering accuracy and classification error rates on the test set, as well as training run times, and Figure 3 shows t-SNE embeddings [van der Maaten and Hinton, 2008] of several algorithms with their optimal hyper-parameters. NCCA and DCCA achieve near perfect class separation.\nDiscussion Several points are worth noting regarding the experiments. First, the computation for NCCA and PLCCA is dominated by the exact kNN search; approximate search [Arya et al., 1998, Andoni and Indyk, 2006] should make NCCA/PLCCA much more efficient. Second, we have not explored the space of choices for density estimates; alternative choices, such as adaptive KDE [Terrell and Scott, 1992], could also further improve performance. Our current choice of KDE would seem to require large training sets for high-dimensional problems. Indeed, with less training data we do observe a drop in performance, but NCCA still outperforms KCCA; for example, using a 50K subset of the MNIST training set\u2014an order of magnitude less data\u2014the classification error rates when using FKCCA/NKCCA/DCCA/NCCA are 5.9/5.2/2.9/4.7%."}, {"heading": "6 Conclusion", "text": "We have presented closed-form solutions to the nonparametric CCA (NCCA) and partially linear CCA (PLCCA) problems. As opposed to kernel CCA, which restricts the nonparametric projections to lie in a predefined RKHS, we have addressed the unconstrained setting. We have shown that the optimal nonparametric projections can be obtained from the SVD of a kernel defined via the pointwise mutual information between the views. This leads to a simple algorithm that outperforms KCCA and matches deep CCA on multiple datasets, while being more computationally efficient than either for moderate-sized data sets. Future work includes leveraging approximate nearest neighbor search and alternative density estimates."}, {"heading": "A Proof of Lemma 3.1", "text": "Let the eigen-decomposition of the second-order moment of E[f(X)|Y ] be E[E[f(X)|Y ]E[f(X)|Y ]\u22a4] = ADA\u22a4 and define U = A\u22a4E[f(X)|Y ] and g\u0303(Y ) = A\u22a4g(Y ). Then the objective in (3) can be written as E[f(X)\u22a4g(Y )] = E[E[f(X)|Y ]\u22a4g(Y )] = E[(A\u22a4E[f(X)|Y ])\u22a4(A\u22a4g(Y ))] = E[U\u22a4g\u0303(Y )]. Similarly, the constraint I = E[g(Y )g(Y )\u22a4] can be expressed as I = A\u22a4A = E[(A\u22a4g(Y ))(A\u22a4g(Y ))\u22a4] = E[g\u0303(Y )g\u0303(Y )\u22a4]. Therefore, the optimization problem (3) can be written in terms of g\u0303 as\nmax g\u0303\nE [ U\u22a4g\u0303(Y ) ] s.t. E [ g\u0303(Y )g\u0303(Y )\u22a4 ] = I. (17)\nOur objective is the sum of correlations in all L dimensions. Let us consider the correlation in the jth dimension. From the Cauchy-Schwartz inequality, we have\nE[Uj g\u0303j(Y )] \u2264 \u221a E [ U2j ] E[g\u0303j(Y )2] = \u221a E [ U2j ]\nwith equality if and only if g\u0303j(Y ) = cjUj for some scalar cj with probability 1. Note that choosing each g\u0303j(Y ) to be proportional to Uj is valid, since the dimensions ofU are uncorrelated (asE[UU\u22a4] = A\u22a4E [ E[f(X)|Y ]E[f(X)|Y ]T ] A = D). In order for each g\u0303j(Y ) to have unit second order moment, we must have cj = 1/ \u221a E[U2j ] = 1/ \u221a Djj . Therefore, g\u0303(Y ) = D\u22121/2U so that g(Y ) = AD\u2212 1 2A\u22a4U = (E[E[f(X)|Y ]E[f(X)|Y ]\u22a4])\u22121/2E[f(X)|Y ], proving the lemma."}, {"heading": "Acknowledgement", "text": "Thanks to Nathan Srebro, Ryota Tomioka, and Yochai Blau for fruitful discussions. This research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency."}], "references": [{"title": "A kernel method for canonical correlation analysis", "author": ["S. Akaho"], "venue": "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001). Springer-Verlag,", "citeRegEx": "Akaho.,? \\Q2001\\E", "shortCiteRegEx": "Akaho.", "year": 2001}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Andoni and Indyk.,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2006}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements", "author": ["R. Arora", "K. Livescu"], "venue": "In Symposium on Machine Learning in Speech and Language Processing (MLSLP),", "citeRegEx": "Arora and Livescu.,? \\Q2012\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2012}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["R. Arora", "K. Livescu"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913),", "citeRegEx": "Arora and Livescu.,? \\Q2013\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2013}, {"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["S. Arya", "D.M. Mount", "N.S. Netanyahu", "R. Silverman", "A.Y. Wu"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Arya et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Arya et al\\.", "year": 1998}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Technical Report 688,", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Sparse additive functional and kernel CCA", "author": ["S. Balakrishnan", "K. Puniyani", "J. Lafferty"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Balakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2012}, {"title": "Spectral Clustering and Biclustering: Learning Large Graphs and Contingency Tables", "author": ["M. Bolla"], "venue": null, "citeRegEx": "Bolla.,? \\Q2013\\E", "shortCiteRegEx": "Bolla.", "year": 2013}, {"title": "Two manifold problems with applications to nonlinear system identification", "author": ["B. Boots", "G. Gordon"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Boots and Gordon.,? \\Q2012\\E", "shortCiteRegEx": "Boots and Gordon.", "year": 2012}, {"title": "Canonical correlation: A tutorial", "author": ["M. Borga"], "venue": null, "citeRegEx": "Borga.,? \\Q2001\\E", "shortCiteRegEx": "Borga.", "year": 2001}, {"title": "Estimating optimal transformations for multiple regression and correlation", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Breiman and Friedman.,? \\Q1985\\E", "shortCiteRegEx": "Breiman and Friedman.", "year": 1985}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Information bottleneck for Gaussian variables", "author": ["G. Chechik", "A. Globerson", "N. Tishby", "Y. Weiss"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chechik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2005}, {"title": "Spectral clustering with two views", "author": ["V. de Sa"], "venue": "In Workshop on Learning with Multiple Views", "citeRegEx": "Sa.,? \\Q2005\\E", "shortCiteRegEx": "Sa.", "year": 2005}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["P. Dhillon", "D. Foster", "L. Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Polynomial expansions of bivariate distributions", "author": ["G. Eagleson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Eagleson.,? \\Q1964\\E", "shortCiteRegEx": "Eagleson.", "year": 1964}, {"title": "MMSE whitening and subspace whitening", "author": ["Y.C. Eldar", "A.V. Oppenheim"], "venue": "IEEE Trans. Info. Theory,", "citeRegEx": "Eldar and Oppenheim.,? \\Q2003\\E", "shortCiteRegEx": "Eldar and Oppenheim.", "year": 2003}, {"title": "Mean shift based clustering in high dimensions: A texture classification example", "author": ["B. Georgescu", "I. Shimshoni", "P. Meer"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Georgescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Georgescu et al\\.", "year": 2003}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Y. Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "An algorithm for the principal component analysis of large data sets", "author": ["N. Halko", "P.-G. Martinsson", "Y. Shkolnisky", "M. Tygert"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "The general theory of canonical correlation and its relation to functional analysis", "author": ["E.J. Hannan"], "venue": "Journal of the Australian Mathematical Society,", "citeRegEx": "Hannan.,? \\Q1961\\E", "shortCiteRegEx": "Hannan.", "year": 1961}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "Co-regularized multi-view spectral clustering", "author": ["A. Kumar", "P. Rai", "H. Daum\u00e9 III"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2011}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "Int. J. Neural Syst.,", "citeRegEx": "Lai and Fyfe.,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe.", "year": 2000}, {"title": "The structure of bivariate distributions", "author": ["H. Lancaster"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Lancaster.,? \\Q1958\\E", "shortCiteRegEx": "Lancaster.", "year": 1958}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Common manifold learning using alternating-diffusion", "author": ["R.R. Lederman", "R. Talmon"], "venue": "Technical Report YALEU/DCS/TR-1497,", "citeRegEx": "Lederman and Talmon.,? \\Q2014\\E", "shortCiteRegEx": "Lederman and Talmon.", "year": 2014}, {"title": "Randomized nonlinear component analysis", "author": ["D. Lopez-Paz", "S. Sra", "A. Smola", "Z. Ghahramani", "B. Schoelkopf"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2014}, {"title": "An efficient algorithm for information decomposition and extraction", "author": ["A. Makur", "F. Kozynski", "S.-L. Huang", "L. Zheng"], "venue": "In 53rd Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "Makur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makur et al\\.", "year": 2015}, {"title": "Multivariate Analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": null, "citeRegEx": "Mardia et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Mardia et al\\.", "year": 1979}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["T. Melzer", "M. Reiter", "H. Bischof"], "venue": "In Proc. of the 11th Int. Conf. Artificial Neural Networks", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "Nadaraya.,? \\Q1964\\E", "shortCiteRegEx": "Nadaraya.", "year": 1964}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Submanifold density estimation", "author": ["A. Ozakin", "A. Gray"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ozakin and Gray.,? \\Q2009\\E", "shortCiteRegEx": "Ozakin and Gray.", "year": 2009}, {"title": "Variable kernel density estimation", "author": ["G.R. Terrell", "D.W. Scott"], "venue": "The Annals of Statistics,", "citeRegEx": "Terrell and Scott.,? \\Q1992\\E", "shortCiteRegEx": "Terrell and Scott.", "year": 1992}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Unsupervised metric fusion by cross diffusion", "author": ["B. Wang", "J. Jiang", "W. Wang", "Z.-H. Zhou", "Z. Tu"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML 2015),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Smooth regression analysis", "author": ["G.S. Watson"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "Watson.,? \\Q1964\\E", "shortCiteRegEx": "Watson.", "year": 1964}, {"title": "X-Ray Microbeam Speech Production", "author": ["J.R. Westbury"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "Westbury.,? \\Q1994\\E", "shortCiteRegEx": "Westbury.", "year": 1994}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Extensions of sparse canonical correlation analysis with applications to genomic data", "author": ["D.M. Witten", "R.J. Tibshirani"], "venue": "Statistical applications in genetics and molecular biology,", "citeRegEx": "Witten and Tibshirani.,? \\Q2009\\E", "shortCiteRegEx": "Witten and Tibshirani.", "year": 2009}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J.C. Burges"], "venue": "In Proc. of the 24th Int. Conf. Machine Learning", "citeRegEx": "Zhou and Burges.,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Burges.", "year": 2007}], "referenceMentions": [{"referenceID": 24, "context": "Canonical correlation analysis (CCA) [Hotelling, 1936] is a classical statistical technique that targets this goal.", "startOffset": 37, "endOffset": 54}, {"referenceID": 16, "context": "This tool has found widespread use in various fields, including recent application to natural language processing [Dhillon et al., 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 4, "context": ", 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 45, "context": ", 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al.", "startOffset": 64, "endOffset": 93}, {"referenceID": 20, "context": ", 2011], speech recognition [Arora and Livescu, 2013], genomics [Witten and Tibshirani, 2009], and cross-modal retrieval [Gong et al., 2014].", "startOffset": 121, "endOffset": 140}, {"referenceID": 2, "context": "In deep CCA (DCCA) [Andrew et al., 2013], the projections are obtained from two deep neural networks that are trained to output maximally correlated vectors.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": "In kernel CCA (KCCA) [Akaho, 2001, Melzer et al., 2001, Bach and Jordan, 2002, Hardoon et al., 2004], these nonlinear mappings are chosen from two reproducing kernel Hilbert spaces (RKHS). In deep CCA (DCCA) [Andrew et al., 2013], the projections are obtained from two deep neural networks that are trained to output maximally correlated vectors. Nonparametric CCA-type methods, which are not limited to specific function classes, include the alternating conditional expectations (ACE) algorithm and its extensions [Breiman and Friedman, 1985, Balakrishnan et al., 2012, Makur et al., 2015]. Nonlinear CCA methods are advantageous over linear CCA in a range of applications [Hardoon et al., 2004, Melzer et al., 2001, Wang et al., 2015b]. However, existing nonlinear CCA approaches are very computationally demanding, and are often impractical to apply on large data. Interestingly, the problem of finding the most correlated nonlinear projections of two random variables has been studied by Lancaster [1958] and Hannan [1961], long before the derivation of KCCA, DCCA and ACE.", "startOffset": 22, "endOffset": 1009}, {"referenceID": 0, "context": "In kernel CCA (KCCA) [Akaho, 2001, Melzer et al., 2001, Bach and Jordan, 2002, Hardoon et al., 2004], these nonlinear mappings are chosen from two reproducing kernel Hilbert spaces (RKHS). In deep CCA (DCCA) [Andrew et al., 2013], the projections are obtained from two deep neural networks that are trained to output maximally correlated vectors. Nonparametric CCA-type methods, which are not limited to specific function classes, include the alternating conditional expectations (ACE) algorithm and its extensions [Breiman and Friedman, 1985, Balakrishnan et al., 2012, Makur et al., 2015]. Nonlinear CCA methods are advantageous over linear CCA in a range of applications [Hardoon et al., 2004, Melzer et al., 2001, Wang et al., 2015b]. However, existing nonlinear CCA approaches are very computationally demanding, and are often impractical to apply on large data. Interestingly, the problem of finding the most correlated nonlinear projections of two random variables has been studied by Lancaster [1958] and Hannan [1961], long before the derivation of KCCA, DCCA and ACE.", "startOffset": 22, "endOffset": 1027}, {"referenceID": 24, "context": "2 Background We start by reviewing the original CCA algorithm [Hotelling, 1936].", "startOffset": 62, "endOffset": 79}, {"referenceID": 11, "context": "This objective has been extensively studied and is known to be optimal in several senses: It maximizes the mutual information for certain distributions p(x,y) [Borga, 2001], maximizes the likelihood for certain latent variable models [Bach and Jordan, 2005], and is equivalent to the information bottleneck method when p(x,y) is Gaussian [Chechik et al.", "startOffset": 159, "endOffset": 172}, {"referenceID": 7, "context": "This objective has been extensively studied and is known to be optimal in several senses: It maximizes the mutual information for certain distributions p(x,y) [Borga, 2001], maximizes the likelihood for certain latent variable models [Bach and Jordan, 2005], and is equivalent to the information bottleneck method when p(x,y) is Gaussian [Chechik et al.", "startOffset": 234, "endOffset": 257}, {"referenceID": 14, "context": "This objective has been extensively studied and is known to be optimal in several senses: It maximizes the mutual information for certain distributions p(x,y) [Borga, 2001], maximizes the likelihood for certain latent variable models [Bach and Jordan, 2005], and is equivalent to the information bottleneck method when p(x,y) is Gaussian [Chechik et al., 2005].", "startOffset": 338, "endOffset": 360}, {"referenceID": 32, "context": "The CCA solution can be expressed as (W1,W2) = (\u03a3 \u22121/2 xx U,\u03a3 \u22121/2 yy V), where \u03a3xx = E[XX \u22a4], \u03a3yy = E[Y Y \u22a4], \u03a3xy = E[XY \u22a4], and U \u2208 RDx\u00d7L and V \u2208 RDy\u00d7L are the top L left and right singular vectors of the matrix T = \u03a3\u22121/2 xx \u03a3xy\u03a3 \u22121/2 yy (see [Mardia et al., 1979]).", "startOffset": 245, "endOffset": 266}, {"referenceID": 23, "context": ", Hardoon et al. [2004]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 12, "context": "Alternating conditional expectations (ACE): The ACE method [Breiman and Friedman, 1985] treats the case of a single projection (L = 1), where B is the class of all zero-mean scalar-valued functions g(Y ), and A is the class of additive models f(X) = \u2211Dx l=1 \u03b3l\u03c6l(Xl) with zero-mean scalar-valued functions \u03c6l(Xl).", "startOffset": 59, "endOffset": 87}, {"referenceID": 12, "context": "Alternating conditional expectations (ACE): The ACE method [Breiman and Friedman, 1985] treats the case of a single projection (L = 1), where B is the class of all zero-mean scalar-valued functions g(Y ), and A is the class of additive models f(X) = \u2211Dx l=1 \u03b3l\u03c6l(Xl) with zero-mean scalar-valued functions \u03c6l(Xl). The ACE algorithm minimizes the objective (3) by iteratively computing the conditional expectation of each view given the other. Recently, Makur et al. [2015] extended ACE to multiple dimensions by whitening the vector-valued f(X) and g(Y ) during each iteration.", "startOffset": 60, "endOffset": 473}, {"referenceID": 2, "context": "Deep CCA (DCCA): In the more recently proposed DCCA approach [Andrew et al., 2013], A and B are the families of functions that can be implemented using two deep neural networks of predefined architecture.", "startOffset": 61, "endOffset": 82}, {"referenceID": 25, "context": "Population solutions: Lancaster [1958] studied a variant of problem (3), where A and B are the families of all measurable functions.", "startOffset": 22, "endOffset": 39}, {"referenceID": 17, "context": "Eagleson [1964] extended this analysis to the Gamma, Poisson, binomial, negative binomial, and hypergeometric distributions.", "startOffset": 0, "endOffset": 16}, {"referenceID": 17, "context": "Eagleson [1964] extended this analysis to the Gamma, Poisson, binomial, negative binomial, and hypergeometric distributions. Hannan [1961] gave Lancaster\u2019s characterization a functional analysis interpretation, which confirmed its validity also for multi-dimensional views.", "startOffset": 0, "endOffset": 139}, {"referenceID": 18, "context": "As shown 5A simpler version of this lemma, in which f(x) = y and g is linear, appeared in Eldar and Oppenheim [2003]. The proof of Lemma 3.", "startOffset": 90, "endOffset": 117}, {"referenceID": 36, "context": "in [Ozakin and Gray, 2009], if the data lies on an r-dimensional manifold, then the KDE converges to the true density at a rate of6 O(n\u2212 4 r+4 ).", "startOffset": 3, "endOffset": 26}, {"referenceID": 19, "context": "Indeed, KDEs have been shown to work well in practice in relatively high dimensions [Georgescu et al., 2003], as is also confirmed in our experiments.", "startOffset": 84, "endOffset": 108}, {"referenceID": 44, "context": "To obtain out-of-sample mapping for a new view 1 test sample x, we use the Nystr\u00f6m method [Williams and Seeger, 2001], which avoids recomputing SVD.", "startOffset": 90, "endOffset": 117}, {"referenceID": 39, "context": ", 2011], metric fusion [Wang et al., 2012], common manifold learning [Lederman and Talmon, 2014], and", "startOffset": 23, "endOffset": 42}, {"referenceID": 29, "context": ", 2012], common manifold learning [Lederman and Talmon, 2014], and", "startOffset": 34, "endOffset": 61}, {"referenceID": 10, "context": "multi-view nonlinear system identification [Boots and Gordon, 2012].", "startOffset": 43, "endOffset": 67}, {"referenceID": 29, "context": "5 Experiments In the following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features (FKCCA, Lopez-Paz et al. [2014]) and Nystr\u00f6m approximation (NKCCA, Williams and Seeger [2001]) as described in Wang et al.", "startOffset": 151, "endOffset": 175}, {"referenceID": 29, "context": "5 Experiments In the following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features (FKCCA, Lopez-Paz et al. [2014]) and Nystr\u00f6m approximation (NKCCA, Williams and Seeger [2001]) as described in Wang et al.", "startOffset": 151, "endOffset": 237}, {"referenceID": 29, "context": "5 Experiments In the following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features (FKCCA, Lopez-Paz et al. [2014]) and Nystr\u00f6m approximation (NKCCA, Williams and Seeger [2001]) as described in Wang et al. [2015b], and deep CCA (DCCA, Andrew et al.", "startOffset": 151, "endOffset": 274}, {"referenceID": 2, "context": "[2015b], and deep CCA (DCCA, Andrew et al. [2013]).", "startOffset": 29, "endOffset": 50}, {"referenceID": 43, "context": "X-Ray Microbeam Speech Data The University of Wisconsin X-Ray Micro-Beam (XRMB) corpus [Westbury, 1994] consists of simultaneously recorded speech and articulatory measurements.", "startOffset": 87, "endOffset": 103}, {"referenceID": 30, "context": "As in [Lopez-Paz et al., 2014], we randomly shuffle the frames and generate splits of 30K/10K/11K frames for training/tuning/testing, and we refer to the result as the \u2019JW11-s\u2019 setup (random splits better satisfy the i.", "startOffset": 6, "endOffset": 30}, {"referenceID": 2, "context": "assumption of train/tune/test data than splits by utterances as in [Andrew et al., 2013]).", "startOffset": 67, "endOffset": 88}, {"referenceID": 35, "context": "As in prior work, for both FKCCA and NKCCA we use rank-6000 approximations for the kernel matrices; for DCCA we use two ReLU [Nair and Hinton, 2010] hidden layers of width 1800/1200 for view 1/2 respectively and run stochastic optimization with minibatch size 750", "startOffset": 125, "endOffset": 148}, {"referenceID": 2, "context": "Following Andrew et al. [2013] and Lopez-Paz et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "Following Andrew et al. [2013] and Lopez-Paz et al. [2014], the acoustic view inputs are 39D Mel-frequencey cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2019JW11\u2019.", "startOffset": 10, "endOffset": 59}, {"referenceID": 28, "context": "View 1 inputs are randomly rotated images (28 \u00d7 28, gray scale) from the original MNIST dataset [LeCun et al., 1998], and the corresponding view 2 inputs are randomly chosen images with the same identity plus additive uniform pixel noise.", "startOffset": 96, "endOffset": 116}, {"referenceID": 13, "context": "-one linear SVM [Chang and Lin, 2011] on the projections with highest cluster accuracy for each algorithm (we reveal labels of 10% of the training set for fast SVM training) and report the classification error rates.", "startOffset": 16, "endOffset": 37}, {"referenceID": 21, "context": "For NCCA/PLCCA we first reduce dimensionality to 100 by PCA for density estimation and exact nearest neighbor search, and use a randomized algorithm [Halko et al., 2011] to compute the SVD of the 450K \u00d7 450K matrix S; for RKCCA/NKCCA we use an approximation rank of 5000; for DCCA we use 3 ReLU hidden layers of 1500 units in each view and train with stochastic optimization of minibatch size 4500.", "startOffset": 149, "endOffset": 169}, {"referenceID": 35, "context": "Noisy MNIST handwritten digits dataset We now demonstrate the algorithms on a noisy MNIST dataset, generated identically to that of Wang et al. [2015b] but with a larger training set.", "startOffset": 132, "endOffset": 152}, {"referenceID": 26, "context": "View 1 inputs are randomly rotated images (28 \u00d7 28, gray scale) from the original MNIST dataset [LeCun et al., 1998], and the corresponding view 2 inputs are randomly chosen images with the same identity plus additive uniform pixel noise. We generate 450K/10K/10K pairs of images for training/tuning/testing (Wang et al. [2015b] uses a 50K-pair training set).", "startOffset": 97, "endOffset": 329}, {"referenceID": 26, "context": "View 1 inputs are randomly rotated images (28 \u00d7 28, gray scale) from the original MNIST dataset [LeCun et al., 1998], and the corresponding view 2 inputs are randomly chosen images with the same identity plus additive uniform pixel noise. We generate 450K/10K/10K pairs of images for training/tuning/testing (Wang et al. [2015b] uses a 50K-pair training set). This dataset satisfies the multi-view assumption that given the label, the views are uncorrelated, so that the most correlated subspaces should retain class information and exclude the noise. Following Wang et al. [2015b], we extract a low-dimensional projection of the view 1 images with each algorithm, run spectral clustering to partition the splits into 10 classes (with clustering parameters tuned as in [Wang et al.", "startOffset": 97, "endOffset": 582}, {"referenceID": 37, "context": "Second, we have not explored the space of choices for density estimates; alternative choices, such as adaptive KDE [Terrell and Scott, 1992], could also further improve performance.", "startOffset": 115, "endOffset": 140}], "year": 2016, "abstractText": "Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster\u2019s theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.", "creator": "LaTeX with hyperref package"}}}