{"id": "1511.05926", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Combining Neural Networks and Log-linear Models to Improve Relation Extraction", "abstract": "The last decade has cinema witnessed menhirs the success of osbourne the existance traditional rehydrating feature - portugal based method on hyflex exploiting greenaway the discrete structures such as words or lexical re-grouped patterns listserv to extract relations saraju from boltz text. undercooked Recently, convolutional and recurrent neural dogberry networks has 51.1 provided riedmatten very redburn effective mechanisms synnott to altbach capture blomkvist the hidden neddick structures lagard\u00e8re within cicala sentences hoshangabad via abwr continuous increased representations, sturluson thereby madalina significantly advancing toungoo the performance edonkey of tusiad relation extraction. kilocharacter The advantage norten of gribbon convolutional neural networks mtel is lavaka their capacity machination to generalize lsps the consecutive k - peps grams in \u00e1lamos the sentences neto while recurrent hambantota neural networks prabowo are effective to encode olivarez long sebha ranges of henshilwood sentence 40-1 context. 2-ball This paper proposes footnoted to 12.98 combine the ah-1 traditional feature - based method, 3/4cup the convolutional nanticoke and gwyn recurrent oblitas neural gardos networks klong to documentarians simultaneously benefit from bankest their outhustling advantages. Our systematic kurzman evaluation of states-general different cedrus network architectures and combination methods mmmmm demonstrates 6,500-home the effectiveness pelado of this approach ambulance and anx results bartosik in the state - of - merkulov the - art dependencies performance converged on bulba the supercenters ACE mowat 2005 luncheonette and 1,290 SemEval ziviyeh dataset.", "histories": [["v1", "Wed, 18 Nov 2015 20:17:39 GMT  (34kb)", "http://arxiv.org/abs/1511.05926v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["thien huu nguyen", "ralph grishman"], "accepted": false, "id": "1511.05926"}, "pdf": {"name": "1511.05926.pdf", "metadata": {"source": "CRF", "title": "Combining Neural Networks and Log-linear Models to Improve Relation Extraction", "authors": ["Thien Huu Nguyen"], "emails": ["thien@cs.nyu.edu", "grishman@cs.nyu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n05 92\n6v 1\n[ cs\n.C L\n] 1\n8 N\nThe last decade has witnessed the success of the traditional feature-based method on exploiting the discrete structures such as words or lexical patterns to extract relations from text. Recently, convolutional and recurrent neural networks has provided very effective mechanisms to capture the hidden structures within sentences via continuous representations, thereby significantly advancing the performance of relation extraction. The advantage of convolutional neural networks is their capacity to generalize the consecutive kgrams in the sentences while recurrent neural networks are effective to encode long ranges of sentence context. This paper proposes to combine the traditional feature-based method, the convolutional and recurrent neural networks to simultaneously benefit from their advantages. Our systematic evaluation of different network architectures and combination methods demonstrates the effectiveness of this approach and results in the state-of-the-art performance on the ACE 2005 and SemEval dataset."}, {"heading": "1 Introduction", "text": "We studies the relation extraction (RE) problem, one of the important problem of information extraction and natural language processing (NLP). Given two entity mentions in a sentence (relation mentions), we need to identify the semantic relationship (if any) between the two entity mentions. One example is the recognition of the Located relation between \u201cHe\u201d and \u201cTexas\u201d in the sentence \u201cHe lives in Texas\u201d.\nThe two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013). These research extensively studies the leverage of linguistic analysis and knowledge resources to construct the feature representations, involving the combination of discrete properties such as lexicon, syntax, gazetteers. Although these approaches are able to exploit the symbolic (discrete) structures within relation mentions, they also suffer from the difficulty to generalize over the unseen words (Gormley et al., 2015), motivating some very recent work on employing the continuous representations of words (word embeddings) to do RE. The most popular method involves neural networks (NNs) that effectively learn hidden structures of relation mentions from such word embeddings, thus achieving the top performance for RE (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015).\nThe NN research for relation extraction and classification has centered around two main network architectures: convolutional neural networks (CNNs) (dos Santos et al., 2015; Zeng et al., 2015) and recursive/recurrent neural networks (Socher et al., 2012; Xu et al., 2015). The distinction between convolutional neural networks\nand recurrent neural networks (RNNs) for RE is that the former aim to generalize the local and consecutive context (i.e, the k-grams) of the relation mentions (Nguyen and Grishman, 2015a) while the latter adaptively accumulate the context information in the whole sentence via the memory units, thereby encoding the global and possibly unconsecutive patterns for RE (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). Consequently, the traditional feature-based method (i.e, the log-linear or MaxEnt model with hand-crafted features), the CNNs and the RNNs tend to focus on different angles for RE. Guided from this intuition, in this work, we propose to combine the three models to further improve the performance of RE.\nWhile the architecture design of CNNs for RE is quite established due to the extensive studies in the last couple of years, the application of RNNs to RE is only very recent and the optimal designs of RNNs for RE are still an ongoing research. In this work, we first perform a systematic exploration of various network architectures to seek the best RNN model for RE. In the next step, we extensively study different methods to assemble the log-linear model, CNNs and RNNs for RE, leading to the combined models that yield the state-of-the-art performance on the ACE 2005 and SemEval dataset. To the best of our knowledge, this is the first work to systematically examine the RNN architectures as well as combine them with CNNs and the traditional feature-based approach for RE."}, {"heading": "2 Models", "text": "Relation mentions consist of sentences marked with two entity mentions of interest. In this paper, we examine two different representations for the sentences in RE: (i) the standard representation, called SEQ that takes all the words in the sentences into account and (ii) the dependency representation, called DEP that only considers the words along the dependency paths between the two entity mention heads of the sentences. In the following, unless indicated specifically, all the statements about the sentences hold for both representations SEQ and DEP.\nThroughout this paper, for convenience, we assume that the input sentences of the relation mentions have the same fixed length n. This can be\nachieved by setting n to the length of the longest input sentences and padding the shorter sentences with a special token. Let W = w1w2 . . . wn be the input sentence of some relation mention, where wi is the i-th word in the sentence. Also, let wi1 and wi2 be the two heads of the two entity mentions of interest. In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).\n- The real-valued word embedding vector ei of wi, obtained by looking up the word embedding table E.\n- The real-valued distance embedding vectors di1 , di2 to encode the relative distances i \u2212 i1 and i \u2212 i2 of wi to the two entity heads of interest wi1 and wi2 : di1 = D[i \u2212 i1], di2 = D[i \u2212 i2] where D is the distance embedding table (initialized randomly). The objective is to inform the networks the positions of the two entity mentions for relation prediction.\n- The real-valued embedding vectors for entity types ti and chunks qi to embed the entity type and chunking information for wi. These vectors are generated by looking up the entity type and chunk embedding tables (also initialized randomly) (i.e, T and Q respectively) for the entity type enti and chunking label chunki of wi: ti = T [enti], qi = Q[chunki].\n- The binary vector pi with one dimension to indicate whether the word wi is on the dependency path between wi1 and wi2 or not.\n- The binary vector gi whose dimensions correspond to the possible relations between words in the dependency trees. The value at a dimension of gi is only set to 1 if there exists one edge of the corresponding relation connected to wi in the dependency tree.\nThe transformation from the word wi to the vector xi = [ei, di1 , di2 , ti, qi, pi, gi] essentially converts the relation mention with the input sentence W into a real-valued matrix X = [x1, x2, . . . , xn], to be used by the neural networks presented below."}, {"heading": "2.1 The Separate Models", "text": "We describe two typical NN architectures for RE underlying the combined models in this work."}, {"heading": "2.1.1 The Convolutional Neural Networks", "text": "In CNNs (Kalchbrenner et al., 2014; Kim, 2014), given a window size of k, we have a set of ck feature maps (filters). Each feature map f is a weight matrix f = [f1, f2, . . . , fk] where fi is a vector to be learnt during training as the model parameters. The core of CNNs is the application of the convolutional operator on the input matrix X and the filter matrix f to produce a score sequence sf = [sf1, s f 2, . . . , s f\nn\u2212k+1], interpreted as a more abstract representation of the input matrix X:\nsfi = g(\nk\u22121\u2211\nj=0\nfj+1xj+i + b)\nwhere b is a bias term and g is the tanh function. In the next step, we further abstract the scores in sf by aggregating it via the max function to obtain the max-pooling score sfmax. We then repeat this process for all the ck feature maps with different window sizes k to generate a vector of the maxpooling scores. In the final step, we pass this vector into some standard multilayer neural network, followed by a softmax layer to produce the probabilistic distribution pC(y|X) over the possible relation classes y in the prediction task."}, {"heading": "2.1.2 The Recurrent Neural Networks", "text": "In RNNs, we consider the input matrix X = [x1, x2, . . . , xn] as a sequence of column vectors indexed from 1 to n. At each step i, we compute the hidden vector hi from the current input vector xi and the previous hidden vector hi\u22121 using the non-linear transformation function \u03a6: hi = \u03a6(xi, hi\u22121).\nThis recurrent computation can be done via three different directional mechanisms: (i) the forward mechanism that recurs from 1 to n and generate the forward hidden vector sequence: R(x1, x2, . . . , xn) = h1, h2, . . . , hn, (ii) the backward mechanism that runs RNNs from n to 1 and results in the backward hidden vector sequence R(xn, xn\u22121, . . . , x1) = h \u2032 n, h \u2032 n\u22121, . . . , h \u2032 1 1, and (iii) the bidirectional mechanism that performs RNNs in both directions to produce the forward and backward hidden vector sequences, and then concatenate them at each position to generate the new hidden vector sequence hb1, h b 2, . . . , h b n: h b i = [hi, h \u2032\ni]. 1The initial hidden vectors are set to the zero vector.\nGiven the hidden vector sequence h1, h2, . . . , hn obtained from one of the three mechanisms above, we study two following strategies to generate the representation vector vR for the initial relation mention. Note that this representation vector can be again fed into some standard multilayer neural network with a softmax layer in the end, resulting in the distribution pR(y|X) for the RNN models:\n- The HEAD strategy: In this strategy, vR is the concatenation of the hidden vectors at the positions of the two entity mention heads of interest: vR = [hi1 , hi2 ]. This is motivated by the importance of the two mention heads in RE (Sun et al., 2011; Nguyen and Grishman, 2014).\n- The MAX strategy: This strategy is similar to our max-pooling mechanism in CNNs. In particular, vR is obtained by taking the maximum along each dimension of the hidden vectors h1, h2, . . . , hn. The idea is to further abstract the hidden vectors by retaining only the most important feature in each dimension.\nRegarding the non-linear function, the simplest form of \u03a6 in the literature considers it as a one-layer feed-forward neural network, called FF : hi = FF (xi, hi\u22121) = \u03c6(Uxi + V hi\u22121) where \u03c6 is the sigmoid function. Unfortunately, the application of FF causes the socalled \u201cvanishing/exploding gradient\u201d problems (Bengio et al., 1994), making it challenging to train RNNs properly (Pascanu et al., 2012). These problems are overcome by the long-short term memory units (LSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2009). In this work, we apply a variant of the memory units: the Gated Recurrent Units from Cho et al. (2014), called GRU . GRU is shown to be much simpler than LSTM in terms of computation but still achieves the comparable performance (Cho et al., 2014)."}, {"heading": "2.2 The Combined Models", "text": "We first present three different methods to assemble CNNs and RNNs: ensembling, stacking and voting, to be investigated in this work. The combination of the neural networks with the log-linear model would be discussed in the next section."}, {"heading": "2.2.1 Ensembling", "text": "In this method, we first run some CNN and RNN in Section 2.1 over the input matrix X to gather the corresponding distributions pC(y|X) and pR(y|X). We then combine the CNN and RNN by multiplying their distributions (element-wise): pensemble(y|X) = 1 Z pC(y|X)pR(y|X) (Z is a normalization constant)."}, {"heading": "2.2.2 Stacking", "text": "The overall architecture of the stacking method is to use one of the two network architectures (i.e, CNNs and RNNs) to generalize the hidden vectors of the other architecture. The expectation is that we can learn more effective features for RE via such a deeper architecture by alternating between the local and global representations provided by CNNs and RNNs.\nWe examine two variants for this method. The first variant, called RNN-CNN, applies the CNN model in Section 2.1.1 on the hidden vector sequence generated by some RNN in Section 2.1.2 to perform RE. The second variant, called CNN-RNN, on the other hand, utilize the CNN model to acquire the hidden vector sequence, that is, in turn, fed as the input into some RNN for RE. For the second variant, as the length of the hidden vector s f = [sf1, s f 2, . . . , s f\nn\u2212k+1] in the CNN model depends on the specified window size k for the feature map f , we need to pad the input matrix X with \u230ak\n2 \u230b zero column vectors on both sides to en-\nsure the same fixed length n for all the hidden vectors: sf = [sf1, s f 2, . . . , s f n]. Besides, we need to rearrange the scores in the hidden vectors from different feature maps of the CNN so they are grouped according to the positions in the sentence, thus being compatible with the input requirement of RNNs."}, {"heading": "2.2.3 Voting", "text": "Instead of integrating CNNs and RNNs at the model level as the two previous methods, the voting method makes decision for a relation mention X by voting the individual decisions of the different models. While there are several voting schemes in the literature, for this work, we employ the simplest scheme of majority voting. If there are more than one relation classes receiving the highest number of votes, the relation class returned by a model and having the highest probability would be chosen."}, {"heading": "2.3 The Hybrid Models", "text": "In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015). Specifically, in such integration models (called the hybrid models), the relation class distribution is obtained from the element-wise multiplication between the distributions of the neural network models and the log-linear model. Let us take the ensembling model in Section 2.2.1 as an example. The corresponding hybrid model in this case would be: phybrid(y|X) = 1\nZ pC(y|X)pR(y|X)plogin(y|X), as-\nsuming plogin(y|X) be the distribution of the loglinear model and Z be the normalization constant. The parameters of the log-linear model are learnt jointly with the parameters of the neural networks.\nHypothesis: Let S be the set of relation mentions correctly predicted by some neural network model in some dataset (the coverage set). The introduction of the log-linear model into this neural network model essentially changes the coverage set of the network, resulting in the new coverage set S\u2032 that might or might not subsume the original set S. In this work, we hypothesize that although S and S\u2032 overlap, there are still some relation mentions that only belong to either set. Consequently, we propose to implement a majority voting system (called the hybrid-voting system) on the outputs of the network and its corresponding hybrid model to enhance both models.\nNote that the voting models in Section 2.2.3 involve the voting on two models (i.e, CNN and RNN). In order to integrate the log-linear model into such voting models, we first augment the separate CNN and RNN models with the log-linear model before we perform the voting procedure on the resulting models. Finally, the corresponding hybridvoting systems would involve the voting on four models (CNN, hybrid CNN, RNN and hybrid RNN)."}, {"heading": "2.4 Training", "text": "We train the models by minimizing the negative log-likelihood function using the stochastic gradient descent algorithm with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014).\nThe gradients are computed via back-propagation while regularization is executed by a dropout on the hidden vectors before the the multilayer neural networks (Hinton et al., 2012). During training, besides the weight matrices, we also optimized the embedding tables E,D, T,Q to achieve the optimal state. Finally, we rescale the weights whose l2-norms exceed a hyperparameter (Kim, 2014; Nguyen and Grishman, 2015a)."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Resources and Parameters", "text": "For all the experiments below, we utilize the pretrained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) to initialize the word embedding table E. The parameters for CNNs and traning the networks are inherited from the previous studies, i.e, the window size set for feature maps = {2, 3, 4, 5}, 150 feature maps for each window size, 50 dimensions for all the embedding tables (except the word embedding table E), the dropout rate = 0.5, the mini-batch size = 50, the hyperparameter for the l2 norms = 3 (Kim, 2014; Nguyen and Grishman, 2015a). Regarding RNNs, we employ 300 units in the hidden layers."}, {"heading": "3.2 Dataset", "text": "We evaluate our models on two datasets: the ACE 2005 dataset for relation extraction and the SemEval-2010 Task 8 dataset (Hendrickx et al., 2010) for relation classification.\nThe ACE 2005 corpus comes with 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2015c; Gormley et al., 2015), we use news (the union of bn and nw) as the training data, a half of bc as the development set and the remainder (cts, wl and the other half of bc) as the test data. Note that we are using the data prepared by Gormley et. al (2015), thus utilizing the same data split on bc as well as the same data processing and NLP toolkits. The to-\ntal number of relations in the training set is 43,4972. We employ the BIO annotation scheme to capture the chunking information for words in the sentences and only mark the entity types of the two entity mention heads (obtained from human annotation) for this dataset.\nThe SemEval dataset concerns the relation classification task that aims to determine the relation type (or no relation) between two entities in sentences. In order to make it compatible with the previous research (Socher et al., 2012; Gormley et al., 2015), for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from Socher et al. (2012) and encoded by the real-valued vectors for each word). The other settings are also adopted from the past studies (Socher et al., 2012; Xu et al., 2015)."}, {"heading": "3.3 RNN Architectures", "text": "This section evaluates the performance of various RNN architectures for RE on the development set. In particular, we compare different design combinations of the four following factors: (i) sentence representations (i.e, SEQ or DEP), (ii) transformation functions \u03a6 (i.e, FF or GRU), (iii) the strategies to employ the hidden vector sequence for RE (i.e, HEAD or MAX), and (iv) the directions to run RNNs (i.e, forward (\u2192), backward (\u2190) or bidirectional (\u21c0\u21bd)). Table 1 presents the results.\nThe main conclusions include:\n2It was an error in Gormley et al. (2015) that reported 43,518 total relations in the training set. The authors acknowledged this error.\n(i) Assuming the same choices for the other three corresponding factors, GRU is more effective than FF, SEQ is better than DEP most of the time and HEAD outperforms MAX (except the case where SEQ and GRU are applied) for RE with RNNs.\n(ii) Regarding the direction mechanisms, the bidirectional mechanism achieves the best performance for the HEAD strategy while the forward direction is the best mechanism for the MAX strategy. This can be partly explained by the lack of past or future context information in the HEAD strategy when we follow the backward or forward direction respectively.\nThe best performance corresponds to the application of the SEQ representation, the GRU function and the MAX strategy that would be used in all the RNN models below. We call such RNN models with the forward, backward and bidirectional mechanism FORWARD, BACKWARD and BIDIRECT respectively. We also apply the SEQ representation for the CNN model (called CNN) in the following experiments for consistency."}, {"heading": "3.4 Evaluating the Combined Models", "text": "We evaluate the combination methods for CNNs and RNNs presented in Section 2.2. In particular, for each method, we examine three models that are combined from one of the three RNN models FORWARD, BACKWARD, BIDIRECT and the CNN model. For instance, in the stacking method, the three combined models corresponding to the RNN-\nCNN variant are FORWARD-CNN, BACKWARDCNN, BIDIRECT-CNN while the three combined models corresponding to the CNN-RNN variant are CNN-FORWARD, CNN-BACKWARD, CNNBIDIRECT. The notations for the other methods are self-explained. The model performance on the development set is given in Table 3.4 that also includes the performance of the separate models (i.e, CNN, FORWARD, BACKWARD, BIDIRECT) for convenient comparison.\nThe first observation is that the ensembling method is not an effective way to combine CNNs and RNNs as its performance is worse than the separate models. Second, regarding the stacking method, the best way to combine CNNs and RNNs in this framework is to assemble the CNN model and the FORWARD model. In fact, the combination of the CNN and FORWARD models helps to improve the performance of the separate models in both variants of this method (referring to the models CNN-FORWARD and FORWARD-CNN). Finally, the voting method is also helpful as it outperforms the separate models with the CNN-BIDIRECT and CNN-BACKWARD combinations.\nFor the following experiments, we would only focus on the three best combined models in this section, i.e, the CNN-FORWARD model in the stacking method (called STACK-FORWARD) and the CNNBIDIRECT, CNN-BACKWARD models in the voting methods (called VOTE-BIDIRECT and VOTEBACKWARD respectively)."}, {"heading": "3.5 Evaluating the Hybrid Models", "text": "This section investigates the hybrid and hybridvoting models (Section 2.3) to see if they can further improve the performance of the neural network models. In particular, we evaluate the separate models: CNN, BIDIRECT, FORWARD, BACKWARD, and the combined models: STACK-FORWARD, VOTE-BIDIRECT and VOTE-BACKWARD when they are augmented with the traditional log-linear model (the hybrid models). Besides, in order to verify the hypothesis in Section 2.3, we also test the corresponding hybrid-voting models. The experimental results are shown in Table 3. There are three main conclusions:\n(i) For all the models in columns \u201cNeural Networks\u201d, \u201cHybrid Models\u201d and \u201cHybrid-Voting Mod-\nels\u201d, we see that the combined models outperform their corresponding separate models (only except the hybrid model of VOTE-BACKWARD), thereby further confirming the benefits of the combined models.\n(ii) Comparing columns \u201cNeural Networks\u201d and \u201cHybrid Models\u201d, we find that the traditional loglinear model significantly helps the CNN model. The effects on the other models are not clear.\n(iii) More interestingly, for all the neural networks being examined (either separate or combined), the corresponding hybrid-voting systems substantially improve both the neural network models as well as the corresponding hybrid models, testifying to the hypothesis about the hybrid-voting approach in Section 2.3. Note that the simpler voting systems on three models: the log-linear model, the CNN model and some RNN model (i.e, either BIDIRECT, FORWARD or BACKWARD) produce the worse performance than the hybrid-voting methods (the respective performance is 66.13%, 65.27%, and 65.96%)."}, {"heading": "3.6 Comparing to the State-of-the-art", "text": "The state-of-the-art system on the ACE 2005 for the unseen domains has been the feature-rich composi-\ntional embedding model (FCM) and the hybrid FCM model from Gormley et al. (2015). In this section, we compare the proposed hybrid-voting systems with these state-of-the-art systems on the test domains bc, cts, wl. Table 4 reports the results. For completeness, we also include the performance of the log-linear model and the separate models CNN, BIDIRECT, FORWARD, BACKWARD, serving as the other baselines for this work.\nFrom the table, we see that although the separate neural networks outperform the FCM model across domains, they are still worse than the hybrid FCM model due to the introduction of the log-linear model into FCM. However, when the networks are combined and integrated with the log-linear model, they (the hybrid-voting systems) become significantly better than the FCM models across all domains (up to 2% improvement on the average absolute F score), yielding the state-of-the-art performance for the unseen domains in this dataset."}, {"heading": "3.7 Relation Classification Experiments", "text": "We further evaluate the proposed systems for the relation classification task on the SemEval dataset. Ta-\nble 5 presents the performance of the seprate models, the proposed systems as well as the other representative systems on this task. The most important observation is that the hybrid-voting systems VOTEBIDIRECT and VOTE-BACKWARD achieve the state-of-the-art performance for this dataset, further highlighting their benefit for relation classification. The hybrid-voting STACK-FORWARD system performs less effectively in this case, possibly due to the small size of the SemEval dataset that is not sufficient to training such a deep model."}, {"heading": "3.8 Analysis", "text": "In order to better understand the reason helping the combination of CNNs and RNNs outperform the individual networks, we evaluate the performance breakdown per relation for the CNN and BIDIRECT models. The results on the development set of the ACE 2005 dataset are provided in Tabel 6.\nOne of the main insights is although CNN and BIDIRECT have the comparable overall perfor-\nmance, their recalls on individual relations are very diverged. In particular, the BIDIRECT has much better recall for the PHYS relation while the recalls of CNN are significantly better for the ART, ORG-AFF and GEN-AFF relations. A closer investigation reveals two facts: (i) the PHYS relation mentions that are only correctly predicted by BIDIRECT involve the long distances between two entity mentions, such as the PHYS relation between \u201cSome\u201d (a person entity) and \u201cdesert\u201d (a location entity) in the following sentence: \u201cSome of the 40,000 British troops are kicking up a lot of dust in the Iraqi desert making sure that nothing is left behind them that could hurt them.\u201d, and (ii) the ART, ORGAFF, GEN-AFF relation mentions only correctly predicted by CNN contains the patterns between the two entity mentions that are short but meaningful enough to decide the relation classes, such as \u201cThe Iraqi unit in possession of those guns\u201d (the ART relation between \u201cunit\u201d and \u201cguns\u201d), or \u201cthe al Qaeda chief operations officer\u201d (the ORG-AFF relation between \u201cal Qaeda\u201d and \u201cofficer\u201d). The failure of CNN on the PHYS relation mentions with long distances originates from its mechanism to model short and consecutive k-grams (up to length 5 in our case), causing the difficulty to capture the long and/or unconsecutive patterns. BIDIRECT, on the other hand, fails to predict the short (but expressive enough) patterns for ART, ORG-AFF, GEN-AFF because it involves the hidden vectors that only model the context words outside the short patterns, potentially introducing unnecessary and noisy information into the max-pooling scores for prediction. Eventually, the combination of RNNs and CNNs helps to compensate the drawbacks of each model."}, {"heading": "4 Related Work", "text": "Starting from the invention of the distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al., 2011), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), paraphrase identification (Yin and Schu\u0308tze, 2015),\nevent extraction (Nguyen and Grishman, 2015b; Chen et al., 2015) for CNNs and machine translation (Cho et al., 2014; Bahdanau et al., 2015) for RNNs, to name a few.\nFor relation extraction/classification, most work on neural networks has focused on the relation classification task. In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al. (2015) and Zhang and Wang (2015) investigate recurrent NNs. Regarding CNNs, Zeng et al. (2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al. (2015) work on the feature-rich compositional embedding models. Finally, the only work that combines NN architectures is due to Liu et al. (2015) but it only focuses on the stacking of the recursive NNs and CNNs for relation classification."}, {"heading": "5 Conclusion", "text": "We investigate different methods to combine CNNs, RNNs as well as the hybrid models to integrate the log-linear model into the NNs. The experimental results demonstrate that the simple majority voting between CNNs, RNNs and their corresponding hybrid models is the best combination method. We achieve the state-of-the-art performance for both relation extraction and relation classification. In the future, we plan to further evaluate the proposed methods on the other tasks such as event extraction and slot filling in the KBP evaluation."}, {"heading": "Acknowledgment", "text": "We would like to thank Matthew Gormley and Mo Yu for providing the dataset. Thank you to Kyunghyun Cho and Yifan He for valuable suggestions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Automatic information extraction", "author": ["Ralph Weischedel", "Alex Zamanian"], "venue": "In Proceedings of the International Conference on Intelligence Analysis", "citeRegEx": "Boschee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boschee et al\\.", "year": 2005}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005a] Razvan Bunescu", "Raymond Mooney"], "venue": "HLT-EMNLP", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Subsequence kernels for relation extraction", "author": ["Bunescu", "Mooney2005b] Razvan Bunescu", "Raymond J. Mooney"], "venue": null, "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Exploiting background knowledge for relation extraction", "author": ["Chan", "Roth2010] Yee S. Chan", "Dan Roth"], "venue": "In COLING", "citeRegEx": "Chan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2010}, {"title": "Event extraction via dynamic multi-pooling convolutional neural networks. In ACL-IJCNLP", "author": ["Chen et al.2015] Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "Lon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "In CoRR", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Dependency tree kernels for relation extraction", "author": ["Culotta", "Sorensen2004] Aron Culotta", "Jeffrey Sorensen"], "venue": null, "citeRegEx": "Culotta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2004}, {"title": "Classifying relations by ranking with convolutional neural networks. In ACLIJCNLP", "author": ["Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Chain based rnn for relation classification", "author": ["Ebrahimi", "Dou2015] Javid Ebrahimi", "Dejing Dou"], "venue": null, "citeRegEx": "Ebrahimi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ebrahimi et al\\.", "year": 2015}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Gormley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al.2009] A. Graves", "Marcus EichenbergerLiwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelli-", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "In CoRR,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A systematic exploration of the feature space for relation extraction", "author": ["Jiang", "Zhai2007] Jing Jiang", "ChengXiang Zhai"], "venue": null, "citeRegEx": "Jiang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2007}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction", "author": ["Nanda Kambhatla"], "venue": null, "citeRegEx": "Kambhatla.,? \\Q2004\\E", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG"], "venue": "ACL-IJCNLP", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2008] Andriy Mnih", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "Employing word representations and regularization for domain adaptation of relation extraction", "author": ["Nguyen", "Grishman2014] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "2015a. Relation extraction: Perspective from convolutional neural networks", "author": ["Nguyen", "Grishman2015a] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "2015b. Event detection and domain adaptation with convolutional neural networks. In ACL-IJCNLP", "author": ["Nguyen", "Grishman2015b] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Convolution kernels on constituent, dependency and sequential structures for relation extraction", "author": ["Alessandro Moschitti", "Giuseppe Riccardi"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2009}, {"title": "Semantic representations for domain adaptation: A case study on the tree kernel-based method for relation extraction", "author": ["Barbara Plank", "Ralph Grishman"], "venue": "ACLIJCNLP", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In arXiv preprint arXiv:1211.5063", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Plank", "Moschitti2013] Barbara Plank", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2013}, {"title": "Exploiting constituent dependencies for tree kernel-based semantic relation extraction", "author": ["Qian et al.2008] Longhua Qian", "Guodong Zhou", "Fang Kong", "Qiaoming Zhu", "Peide Qian"], "venue": "In COLING", "citeRegEx": "Qian et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2008}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["Sun et al.2011] Ang Sun", "Ralph Grishman", "Satoshi Sekine"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Combining word embeddings and feature embeddings for fine-grained relation extraction", "author": ["Yu et al.2015] Mo Yu", "Matthew R. Gormley", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "In CoRR,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Exploring various knowledge in relation extraction", "author": ["Chinatsu Aone", "Anthony Richardella"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Relation classification via convolutional deep neural network. In COLING", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Relation classification via recurrent neural network", "author": ["Zhang", "Wang2015] Dongxu Zhang", "Dong Wang"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A composite kernel to extract relations between entities with both flat and structured features", "author": ["Zhang et al.2006] Min Zhang", "Jie Zhang", "Jian Su", "GuoDong Zhou"], "venue": "COLING-ACL", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Tree kernel-based relation extraction with context-sensitive structured parse tree information", "author": ["Zhou et al.2007] GuoDong Zhou", "Min Zhang", "DongHong Ji", "QiaoMing Zhu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 21, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 3, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 46, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 35, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 41, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 45, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 47, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 33, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 29, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 14, "context": "Although these approaches are able to exploit the symbolic (discrete) structures within relation mentions, they also suffer from the difficulty to generalize over the unseen words (Gormley et al., 2015), motivating some very recent work on employing the continuous representations of words (word embeddings) to do RE.", "startOffset": 180, "endOffset": 202}, {"referenceID": 43, "context": "The NN research for relation extraction and classification has centered around two main network architectures: convolutional neural networks (CNNs) (dos Santos et al., 2015; Zeng et al., 2015) and recursive/recurrent neural", "startOffset": 148, "endOffset": 192}, {"referenceID": 34, "context": "networks (Socher et al., 2012; Xu et al., 2015).", "startOffset": 9, "endOffset": 47}, {"referenceID": 37, "context": "networks (Socher et al., 2012; Xu et al., 2015).", "startOffset": 9, "endOffset": 47}, {"referenceID": 8, "context": "e, the k-grams) of the relation mentions (Nguyen and Grishman, 2015a) while the latter adaptively accumulate the context information in the whole sentence via the memory units, thereby encoding the global and possibly unconsecutive patterns for RE (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 248, "endOffset": 300}, {"referenceID": 46, "context": "In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 261, "endOffset": 320}, {"referenceID": 35, "context": "In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 261, "endOffset": 320}, {"referenceID": 14, "context": "In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 261, "endOffset": 320}, {"referenceID": 20, "context": "In CNNs (Kalchbrenner et al., 2014; Kim, 2014), given a window size of k, we have a set of ck feature maps (filters).", "startOffset": 8, "endOffset": 46}, {"referenceID": 22, "context": "In CNNs (Kalchbrenner et al., 2014; Kim, 2014), given a window size of k, we have a set of ck feature maps (filters).", "startOffset": 8, "endOffset": 46}, {"referenceID": 35, "context": "This is motivated by the importance of the two mention heads in RE (Sun et al., 2011; Nguyen and Grishman, 2014).", "startOffset": 67, "endOffset": 112}, {"referenceID": 1, "context": "Unfortunately, the application of FF causes the socalled \u201cvanishing/exploding gradient\u201d problems (Bengio et al., 1994), making it challenging to train RNNs properly (Pascanu et al.", "startOffset": 97, "endOffset": 118}, {"referenceID": 31, "context": ", 1994), making it challenging to train RNNs properly (Pascanu et al., 2012).", "startOffset": 54, "endOffset": 76}, {"referenceID": 8, "context": "GRU is shown to be much simpler than LSTM in terms of computation but still achieves the comparable performance (Cho et al., 2014).", "startOffset": 112, "endOffset": 130}, {"referenceID": 8, "context": "In this work, we apply a variant of the memory units: the Gated Recurrent Units from Cho et al. (2014), called GRU .", "startOffset": 85, "endOffset": 103}, {"referenceID": 46, "context": "In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 235, "endOffset": 294}, {"referenceID": 35, "context": "In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 235, "endOffset": 294}, {"referenceID": 14, "context": "In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 235, "endOffset": 294}, {"referenceID": 40, "context": "We train the models by minimizing the negative log-likelihood function using the stochastic gradient descent algorithm with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014).", "startOffset": 175, "endOffset": 200}, {"referenceID": 22, "context": "We train the models by minimizing the negative log-likelihood function using the stochastic gradient descent algorithm with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014).", "startOffset": 175, "endOffset": 200}, {"referenceID": 17, "context": "The gradients are computed via back-propagation while regularization is executed by a dropout on the hidden vectors before the the multilayer neural networks (Hinton et al., 2012).", "startOffset": 158, "endOffset": 179}, {"referenceID": 22, "context": "Finally, we rescale the weights whose l2-norms exceed a hyperparameter (Kim, 2014; Nguyen and Grishman, 2015a).", "startOffset": 71, "endOffset": 110}, {"referenceID": 22, "context": "5, the mini-batch size = 50, the hyperparameter for the l2 norms = 3 (Kim, 2014; Nguyen and Grishman, 2015a).", "startOffset": 69, "endOffset": 108}, {"referenceID": 23, "context": "For all the experiments below, we utilize the pretrained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) to initialize the word embedding table E.", "startOffset": 107, "endOffset": 129}, {"referenceID": 16, "context": "We evaluate our models on two datasets: the ACE 2005 dataset for relation extraction and the SemEval-2010 Task 8 dataset (Hendrickx et al., 2010) for relation classifica-", "startOffset": 121, "endOffset": 145}, {"referenceID": 14, "context": ", 2015c; Gormley et al., 2015), we use news (the union of bn and nw) as the training data, a half of bc as the development set and the remainder (cts, wl and the other half of bc) as the test data. Note that we are using the data prepared by Gormley et. al (2015), thus utilizing the same data split on bc as well as", "startOffset": 9, "endOffset": 264}, {"referenceID": 34, "context": "In order to make it compatible with the previous research (Socher et al., 2012; Gormley et al., 2015), for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from Socher et al.", "startOffset": 58, "endOffset": 101}, {"referenceID": 14, "context": "In order to make it compatible with the previous research (Socher et al., 2012; Gormley et al., 2015), for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from Socher et al.", "startOffset": 58, "endOffset": 101}, {"referenceID": 34, "context": "The other settings are also adopted from the past studies (Socher et al., 2012; Xu et al., 2015).", "startOffset": 58, "endOffset": 96}, {"referenceID": 37, "context": "The other settings are also adopted from the past studies (Socher et al., 2012; Xu et al., 2015).", "startOffset": 58, "endOffset": 96}, {"referenceID": 14, "context": ", 2012; Gormley et al., 2015), for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from Socher et al. (2012) and encoded by the real-valued vectors for each word).", "startOffset": 8, "endOffset": 214}, {"referenceID": 14, "context": "It was an error in Gormley et al. (2015) that reported 43,518 total relations in the training set.", "startOffset": 19, "endOffset": 41}, {"referenceID": 14, "context": "The state-of-the-art system on the ACE 2005 for the unseen domains has been the feature-rich compositional embedding model (FCM) and the hybrid FCM model from Gormley et al. (2015). In this section, we compare the proposed hybrid-voting systems with these state-of-the-art systems on the test domains bc, cts, wl.", "startOffset": 159, "endOffset": 181}, {"referenceID": 16, "context": "SVM (Hendrickx et al., 2010) 82.", "startOffset": 4, "endOffset": 28}, {"referenceID": 34, "context": "2 RNN (Socher et al., 2012) 77.", "startOffset": 6, "endOffset": 27}, {"referenceID": 34, "context": "6 MVRNN (Socher et al., 2012) 82.", "startOffset": 8, "endOffset": 29}, {"referenceID": 42, "context": "4 CNN (Zeng et al., 2014) 82.", "startOffset": 6, "endOffset": 25}, {"referenceID": 14, "context": "1\u2020 FCM (Gormley et al., 2015) 83.", "startOffset": 7, "endOffset": 29}, {"referenceID": 14, "context": "0 Hybrid FCM (Gormley et al., 2015) 83.", "startOffset": 13, "endOffset": 35}, {"referenceID": 23, "context": "4 DepNN (Liu et al., 2015) 83.", "startOffset": 8, "endOffset": 26}, {"referenceID": 37, "context": "6 SDP-LSTM (Xu et al., 2015) 83.", "startOffset": 11, "endOffset": 28}, {"referenceID": 2, "context": "Starting from the invention of the distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al.", "startOffset": 73, "endOffset": 188}, {"referenceID": 36, "context": "Starting from the invention of the distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al.", "startOffset": 73, "endOffset": 188}, {"referenceID": 24, "context": "Starting from the invention of the distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al.", "startOffset": 73, "endOffset": 188}, {"referenceID": 10, "context": ", 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al., 2011), sentence modeling and clas-", "startOffset": 109, "endOffset": 133}, {"referenceID": 20, "context": "sification (Kalchbrenner et al., 2014; Kim, 2014), paraphrase identification (Yin and Sch\u00fctze, 2015),", "startOffset": 11, "endOffset": 49}, {"referenceID": 22, "context": "sification (Kalchbrenner et al., 2014; Kim, 2014), paraphrase identification (Yin and Sch\u00fctze, 2015),", "startOffset": 11, "endOffset": 49}, {"referenceID": 7, "context": "event extraction (Nguyen and Grishman, 2015b; Chen et al., 2015) for CNNs and machine translation (Cho et al.", "startOffset": 17, "endOffset": 64}, {"referenceID": 8, "context": ", 2015) for CNNs and machine translation (Cho et al., 2014; Bahdanau et al., 2015) for RNNs, to name a few.", "startOffset": 41, "endOffset": 82}, {"referenceID": 0, "context": ", 2015) for CNNs and machine translation (Cho et al., 2014; Bahdanau et al., 2015) for RNNs, to name a few.", "startOffset": 41, "endOffset": 82}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al.", "startOffset": 15, "endOffset": 64}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al. (2015) and Zhang and Wang (2015) investigate recurrent NNs.", "startOffset": 15, "endOffset": 147}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al. (2015) and Zhang and Wang (2015) investigate recurrent NNs.", "startOffset": 15, "endOffset": 173}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al. (2015) and Zhang and Wang (2015) investigate recurrent NNs. Regarding CNNs, Zeng et al. (2014) examine CNNs via the sequential representation of sentences, dos Santos et al.", "startOffset": 15, "endOffset": 235}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning.", "startOffset": 72, "endOffset": 169}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al.", "startOffset": 72, "endOffset": 247}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al. (2015) work on the feature-rich compositional embedding models.", "startOffset": 72, "endOffset": 273}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al. (2015) work on the feature-rich compositional embedding models. Finally, the only work that combines NN architectures is due to Liu et al. (2015) but it only focuses on the stacking of the recursive NNs and CNNs for relation classification.", "startOffset": 72, "endOffset": 412}], "year": 2015, "abstractText": "The last decade has witnessed the success of the traditional feature-based method on exploiting the discrete structures such as words or lexical patterns to extract relations from text. Recently, convolutional and recurrent neural networks has provided very effective mechanisms to capture the hidden structures within sentences via continuous representations, thereby significantly advancing the performance of relation extraction. The advantage of convolutional neural networks is their capacity to generalize the consecutive kgrams in the sentences while recurrent neural networks are effective to encode long ranges of sentence context. This paper proposes to combine the traditional feature-based method, the convolutional and recurrent neural networks to simultaneously benefit from their advantages. Our systematic evaluation of different network architectures and combination methods demonstrates the effectiveness of this approach and results in the state-of-the-art performance on the ACE 2005 and SemEval dataset.", "creator": "LaTeX with hyperref package"}}}