{"id": "1511.06341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Communicating Semantics: Reference by Description", "abstract": "Messages buftea often refer ytl to entities such brimley as eighteens people, slinger places hirokazu and events. jianghuai Correct gwydion identification of the gomart intended outfalls reference canci\u00f3n is an funder essential part mussing of communication. Lack spellissy of shared unique amemiya names olympiad often complicates entity borgens reference. yupa Shared easingwold knowledge can tropos be gurindji used tmu to wvt construct gangemi uniquely midwinter identifying huggers descriptive references harith for skee entities volva with mismatch ambiguous names. We introduce a tmartinez mathematical model for \" turnabouts Reference by torrejon Description \" and provide waytha results naturae on wentian the conditions under djoliba which, with cloeren high probability, programs can addingham construct trophys unambiguous cides references to rivi\u00e8re most entities in ch-46 the hacktivist domain of suseno discourse.", "histories": [["v1", "Thu, 19 Nov 2015 20:14:43 GMT  (153kb,D)", "https://arxiv.org/abs/1511.06341v1", null], ["v2", "Fri, 20 Nov 2015 19:33:36 GMT  (153kb,D)", "http://arxiv.org/abs/1511.06341v2", null], ["v3", "Thu, 21 Jan 2016 00:42:06 GMT  (440kb,D)", "http://arxiv.org/abs/1511.06341v3", null], ["v4", "Mon, 7 Mar 2016 16:41:38 GMT  (442kb,D)", "http://arxiv.org/abs/1511.06341v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ramanathan v guha", "vineet gupta"], "accepted": false, "id": "1511.06341"}, "pdf": {"name": "1511.06341.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Vineet Gupta"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Messages often need to refer to real world entities. Communicating a reference to an entity is trivial when the sender and receiver share an unambiguous name for it. However, nearly all symbols in use are ambiguous and could refer to multiple entities. The word \u2018Lincoln\u2019 could for example, refer to the city, the president, or the film. In such cases, ambiguity can be resolved by augmenting the symbol with a unique description \u2014 \u2018Lincoln, the President\u2019. We call this Reference by Description. This leverages a combination of language (the possible references of \u2018Lincoln\u2019) and knowledge/context (that there was only one President named Lincoln) that the sender and receiver share to unambiguously communicate a reference.\nThis method of disambiguation is common in human communications. For example in the New York Times headline [17] \u2018John McCarthy, Pioneer in Artificial Intelligence ...\u2019 the term \u2018John McCarthy\u2019 alone is ambiguous. It could refer to a computer scientist, a politician or even a novel or film. In order to disambiguate the reference, the headline includes the description \u201cPioneer in Artificial Intelligence\u201d. An analysis, in the appendix, of 50 articles of different genres from different newspaper/magazine articles shows how \u2018Reference by Description\u2019 is ubiquitous in human communication.\nThe significance of correctly constructing and resolving entity references goes beyond human communication. The problem of correctly constructing and resolving entity references across different systems is at the core of data and application interoperability. The following example illustrates this.\nConsider an application which helps a user select and watch a movie or TV Show. The application has a database of movies and shows, which the user can browse through, look at reviews from movie review sites such as RogerEbert.com, IMDb, Rotten Tomatoes and the other (language specific) sites. Then, it identifies a service (such as Amazon, Hulu, Netflix, ...) from which the movie may be purchased or rented. Given that there are about 500,000 movies and 3 million TV episodes, one of the most difficult parts of building such an application is communicating references to these entities\nar X\niv :1\n51 1.\n06 34\n1v 4\n[ cs\n.C L\n] 7\nM ar\n2 01\n6\n(movies and TV shows) with these different sites. Expecting a large number of different sites to use the same unique identifiers for these millions of entities is unrealistic.\nHumans do not and cannot have a unique name for everything in the world. Yet, we communicate in our daily lives about things that do not have a unique name (like John McCarthy) or lack a name (like his first car). Our long term goal is to enable programs to achieve communication just as effectively. We believe that like humans, applications such as these too have to use \u2018Reference by Description\u2019.\nThe problem of entity reference is also closely related to that of privacy preserving information sharing. When the entity about whom information is shared is a person, and it is done without the person\u2019s explicit consent (as with sharing of user profiles for ad targeting), it is critical this information not uniquely identify the person.\nWe are interested in a computational model of Reference by Description, which can answer questions such as: What is the minimum that needs to be shared for two communicating parties to understand each other? How big does a description need to be? When can we bootstrap from little to no shared language? What is the computational cost of using descriptions instead of unique names?\nIn this paper, we present a simple yet general model for \u2018Reference by Description\u2019. We devise measures for shared knowledge and shared language and derive the relation between these and the size of descriptions. From this, we answer the above questions. We validate these answers with experiments on a set of random graphs."}, {"heading": "2 Background", "text": "Formal study of descriptions started with Frege [10] and Russell\u2019s [22] descriptivist theory of names, in which names/identity are equivalent to descriptions. Kripke [16] argued against this position using examples where differences in domain knowledge could yield vastly different descriptions of the same entity. We focus not on the philosophical underpinnings of names/identity, but rather on enabling unambiguous communication between software programs.\nIn [23], the founding paper of information theory, Shannon referred to this problem, saying \u2018Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities\u2019, but he passed over it, saying \u2018These semantic aspects of communication are irrelevant to the engineering problem.\u2019\nComputational treatments of descriptions started with linking duplicates in census records [25]. In computer science, problems in database integration, data warehousing and data feed processing motivated the development of specialized algorithms for detecting duplicate items, typically about people, brands and products. This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc. Reference resolution has also been studied in computational linguistics, which has developed specialized algorithms for resolving pronouns, anaphora, etc.\nSometimes, we can pick a representation for the domain that facilitates reference by description. Keys in relational databases [6] are the best example of this.\nThe goal of privacy preserving information sharing [7] is the complement of unambiguous communication of references, ensuring that the information shared does not reveal the identity of the entities referred to in the message. [1], [18] discuss the difficulty of doing this while [15] shows how this can be done for search logs.\nWe have two main contributions in this paper. Most computational treatments to date have focused on specific heuristic algorithms for specific kinds of data. We present a general information theoretic model for answering questions like how much knowledge must be shared to be able to construct unambiguous references. Second, in contrast to previous treatments which use simple propositional / feature vector representations, we allow for richer, relational representations."}, {"heading": "3 Communication model", "text": "We extend the classical information theoretic model of communication from symbol sequences to sets of relations between entities. In our model of communication (Fig. 1):\n1. Messages are about an underlying domain. A number of fields from databases and artificial intelligence to number theory have modeled their domains as a set of entities and a set of N-tuples on these entities. We use this model to represent the domain. Since arbitrary N-tuples can be constructed out of 3-tuples [21], we restrict ourselves to 3-tuples, i.e., directed labeled graphs. We will refer to the domain as the graph, and the entities as nodes, which may be people, places, etc. or literal values such as strings and numbers. The graph has N nodes. 2. The sender and receiver each have a copy of a portion of this graph. The arcs that the copies have could be different, both between them and from the underlying graph. The nodes are assumed to be the same. 3. The sender and receiver each associate a name (or other identifying string) with each arc label and zero or more names with each of the nodes in the graph. Multiple nodes may share the same name. Some subset of these names are shared. 4. Each message encodes a subset of the graph. We assume that the sender and receiver share the grammar for encoding the message. 5. The communication is said to be successful if the receiver correctly identifies the nodes that the message refers to.\nWhen a node\u2019s name is ambiguous, the sender may augment the message with a description that uniquely identifies it. Given a node X in a graph, every subgraph that includes X is a description of X . If a particular subgraph cannot be mistaken for any other subgraph, it is a unique description forX . The nodes, other thanX , in the description are called \u2018descriptor nodes\u2019. In this paper, we assume that the number arc labels is much smaller than the number of nodes and arcs and that arc labels are unambiguous and shared.\nFigures 2-4 illustrate examples of this model that are situated in the following context: Sally and Dave, two researchers, are sharing information about students in their field. They each have some information about students and faculty: who they work for and what universities they attend(ed). Fig. 1 illustrates the communication model in this context.\nAs the examples in Figures 2-4 illustrate, the structure of the graph and amount of language and knowledge shared together determine the size of unique descriptions. In this paper, we are interested in the relationship between stochastic characterizations of shared knowledge, shared language and description size. As discussed in the appendix, we can also approach this from a combinatorial, logical or algorithmic perspective.\nIn an earlier iterations of this paper [13] we presented this model and the solution for simpler version of this problem.\nCommunication model"}, {"heading": "4 Quantifying Sharing", "text": "When the sender describes a nodeX by specifying an arc betweenX and a node named N1, she expects the receiver to know both which node N1 refers to (shared language) and which nodes have arcs going to this node (shared domain knowledge). We distinguish between these two."}, {"heading": "4.1 Shared Language / Linguistic Ambiguity", "text": "Let pij be the probability of name Ni referring to the jth object. The Ambiguity of Ni is\nAi = N\u2211 j=0 \u2212pij log(pij)\nAi is the conditional entropy \u2014 the entropy of the probability distribution of over the set of entities given that the name was Ni. When there is no ambiguity in Ni, Ai =\nStudies at Works for Co-authors with Alumnus of Student VG RG\nK S M B\n1 2 3 4\nSally and Dave\u2019s shared view of the domain:\nSally wants to share information about student 3 (the numbers corresponding to the students are not shared. We have added them here for our use only).\n? RG\nIf she sends the following message: Dave could interpret it as:\nRG\n3\nRG\n4\nSo Sally adds a description to the message: Dave interprets this correctly as:\n? RG\nS\nRG\nS\n3\nor\nFlat message descriptions\n0. Conversely, if Ni could refer to any node in the graph with equal probability, the ambiguity is Ai = log(N). Given a set of names in a message, if we assume that the intended object references are independent, the ambiguity rate associated with a sequence of names is the average of the ambiguity of the names. We use the Asymptotic Equipartition Property [5] to estimate the expected number of candidate referents of a set of names from from their ambiguity. The expected number of interpretations, of \u3008N1N2N3 . . . ND\u3009 is 2DAd .\nLinguistic ambiguity may arise not just from a single name corresponding to multiple entities (like the earlier example of \u2018Lincoln\u2019) but also from variations of a name (such as \u2019Google Inc.\u2019 vs \u2019Google\u2019 or even \u2019Goggle\u2019). Both these can be modeled with the definition of Ambiguity discussed here.\nTypically, entities with more ambiguous names are described using entities less ambiguous names. We distinguish the ambiguity rate of the nodes being described (Ax) from that of the nodes used in the description (Ad).\nHere, Sally and Dave only have the initials of both advisors (G).\nStudies at Works for Co-authors with Alumnus of Student G G\nK S M B\n1 2 3 4\n? G\nS\nThe previous message is now ambiguous and could be interpreted as (2) or (3).\nS\n2\nG G\nS\n3\nSally has to augment it with a richer description to disambiguate it for Dave, which he interprets correctly.\n? G\nS M\nG\nM\n3\nS\nor\nDeep message descriptions"}, {"heading": "4.2 Shared Domain Knowledge", "text": "Shared knowledge of the graph enables encoding/decoding of descriptions. Amount of sharing is not just a function of how much of the graph is shared, but the amount of information/knowledge in the graph. We want to characterize the information/knowledge content of the graph from the perspective of its ability to support distinguishing descriptions. We introduce Salience, a measure of how much differentiation is possible between nodes in the graph.\nThe classical measure for information (rate) is entropy. Let the adjancency matrix of the graph be generated by a process with entropy Hg . Then the information content of a sufficiently large randomly chosen set of L arcs will be LHg . However, given that the goal of our descriptions is to distinguish a given node (X) from other nodes, we do not want to randomly choose arcs involving the node. We would like to pick the arcs that are most likely to distinguish X . We now introduce a quantity, which we call \u2018Salience\u2019, to quantify the ability of the graph to generate distinguishing descriptions.\nG G\nK S M B\n1 2 3 4\nG G\nK S M B\n1 2 3 4\nDave is now confused about the alma mater\u2019s of both advisors:\nSally\u2019s view of the domain:\nSally sends a message with richer descriptions (Y is a co-author of the student):\nWhich Dave interprets as:\nDave\u2019s view of the domain:\n? G\nS Y B\nM\n1 G\nS Y B\nM\nx x x\n2 G\nS Y B\nM\nx x\n3 G\nS Y B\nMx 4 G\nS Y B\nMx\nx x\n\u2713\nSally has to augment her description with annotations of the co-author of the student (Y). Dave can use a process similar to decoding Hamming codes to deduce that she is most likely describing student (3).\nGiven a graph G, there is an arc from a X (the node being described) to each of the other nodes in the graph.1 Each of these \u3008arc\u2212 label, target\u3009 pairs is a \u2018statement\u2019 about X . Each description of X is a subset of the statements about X .\nGiven a particular set ofD nodes, \u3008D1, D2, D3, ...DD\u3009, let the set of arcs fromX to these D nodes be \u3008LXD1 , LXD2 , LXD3 ...LXDD \u3009. Let the probability of this sequence (or more generally, \u2019shape\u2019) occurring between these D nodes and any randomly chosen node in the graph be pi. In Information Theoretic terms, the information content of this set of arcs, or this description, is \u2212 log pi. As pi decreases, the information content of the description and the likelihood of the description uniquely identifying X increases. The Salience of a description its information content.\nThe information content of a description grows with its length. The \u2018Salience Rate\u2019 of a set of descriptions (or a single description) is the average salience of the descriptions in the ensemble divided by the average length (number of arcs) of the descriptions in the ensemble.\n1 We label the absence of an arc between two nodes itself as a kind of arc label.\nConsider an ensemble of size S of descriptions, of average sizeD, that hold true for a given node X , containing \u03b3 distinct shapes, with prior probabilities of holding true for a randomly chosen other node in the graph of p1, p2, ...p\u03b3 . Let the fraction of the S descriptions that have these shapes be q1, q2, ...q\u03b3 respectively. Assuming independence between the pi, the Salience Rate (F ) of the descriptions in this ensemble is:\nF = 1\nD \u2211 i \u2212qi log pi (1)\nNote that pi is not a distribution. The sum of the pi\u2019s in an ensemble may be less than 1. So, though the definition of salience bears superficial similarity to Cross entropy and Kullback-Liebler Divergence, they are different.\nWhen the description (of X) is constructed by randomly selecting arcs that include X , the salience of the description is the entropy rate (Hg) of the adjacency matrix of the graph.\nThe salience of a description of an entity is a measure of how well it captures the most distinctive aspects of that entity. The salience of an ensemble is meaningful only in the context of the larger graph it is derived from. Since the pi depend on the rest of the graph, the same ensemble, relative to a different graph might have a different salience. For example, consider the following description: \u2018X is a current US Senator, who studied at Harvard, ...\u2019. If the rest of the graph was about US senators, since many of the other nodes in the graph also satisfy this description, this description has a low salience. On the other hand, if the rest of the graph is about actors, this description has very high salience and uniquely identifies X.\nA description with a higher salience is more likely to be unique. The sender may search through multiple candidate descriptions (of a given salience) to find a unique description. We are interested in an ensemble of S candidate descriptions, at least one of which will be unique, with a probability of 1 \u2212 , where can be made arbitrarity small. We will show later (equation 15) that for a given , as S increases, the required information content of the description decreases, up to a certain minimum. Beyond that, increasing S does not reduce the required information content of descriptions. In other words, having more descriptions does not completely compensate for a lack of more informative descriptions. If we want at most a small constant number (independent of N ) nodes to not have unambiguous descriptions, S \u221d logN .\nGiven our interest in constructing unique descriptions, we restrict our attention to ensembles that have the highest salience rate. We will use the symbol F for the salience rate of an adquately large (log(N)) ensemble with the highest salience rate. Given a description of length L and D nodes and a salience rate of F , the probability of a randomly chosen node satisfying this description is 2\u2212LF .\nGiven a set of D nodes, we have only considered the arcs from X to these D nodes. We can extend our treatment to include some number, say bD (b < D/2) arcs between the D nodes in addition to the D arc between X and the D nodes. The salience in such descriptions is the combination of the salience from X to the D arcs plus the salience from from the bD arcs.\nThe salience of a graph is a measure of the underlying graph\u2019s ability to provide unique descriptions. Differences in the view of the graph between the sender and re-\nceiver affect how much of this ability can be used. E.g., if \u2018color\u2019 is one attribute of the nodes but the receiver is blind, then the number of distinguishable descriptions is reduced. Some differences in the structure of the graph may be correlated. E.g., if the receiver is color blind, some colors (such as black and white) may be recognized correctly while certain other colors are indistinguishable. We use the mutual information between the sender\u2019s and receiver\u2019s versions of an ensemble as the measure of their shared knowledge. As before, consider an ensemble of size S of descriptions, containing \u03b3 distinct shapes. Let P (xi) be the probability of the ith shape occurring between a randomly chosen node and a randomly chosen set ofD other nodes in the graph as seen by the sender. Given an ensemble, let Q(xi) be the probability of the ith shape occurring in the ensemble. Let P (yi) be the probability of the ith shape occurring between a randomly chosen node and a randomly chosen set of D other nodes in the graph as seen by the sender. Given an ensemble, let Q(yi) be the probability of the ith shape occurring in the ensemble. P (yi|xi) is the conditional probability of the receiver\u2019s view having the ith relation between a randomly chosen node and D descriptor nodes, given that it has this relation occurs in the sender\u2019s view.\nThe sender chooses a description from the ensemble and sends it to the receiver (through a noiseless channel). The information content of the description to the receiver is not the same as it is to the sender. The information content of the description to the receiver is a function of the mutual information between the sender\u2019s and receiver\u2019s view of the underlying graph. Shared salience is defined as:\nShared Salience = \u2211 i \u2212Q(xi)Q(yi|xi) logP (xi)P (yi|xi) (2)\nShared salience is to salience as mutual information is to entropy. If the descriptions in an ensemble are constructed by randomly choosing statements or if the descriptions become very large, shared salience tends to the mutual information."}, {"heading": "4.3 Salience of Random Graphs", "text": "Given our interest in probabilistic estimates on the sharing required, we focus on graphs generated by stochastic processes, i.e., Random Graphs. The most commonly studied Random Graph model is that proposed by Erdos and Renyi [9]. The Erdos-Renyi random graph G(N, p) has N nodes where the probability of a randomly chosen pair of nodes being connected is p. The Salience Rate of a sufficiently large G(N, p) graph is \u2212 log(p) (or \u2212 log(1\u2212 p), whichever is larger).\nMore recent work on stochastic graph models has tried to capture some of the phenomenon found in real world graphs. Watts, Newman, et. al. [24], [19] study small world graphs, where there are a large number of localized clusters and yet, most nodes can be reach from every other node in a small number of hops. This phenomenon is often observed in social network graphs. \u2018Knowledge graphs\u2019, such as DBPedia [2] and Freebase [3], that represent relations between people, places, events, etc., tend to exhibit complex dependencies between the different arcs in/out of a node. Learning probabilistic models [11] of such dependencies is an active area of research.\nG(N, p) graphs assume independence between arcs, i.e., the probability of an arc Li occurring between two nodes is independent of all other arcs in the graph. Clustering\nand the kind of phenomenon found in knowledge graphs can be modeled by discharging this independence assumption and replacing it with appropriate conditional probabilities. For example, the clustering phenomenon occurs when the probability of two nodes A and C being connected (by some arc) is higher if there is a third node B that is connected to both of them.\nWhen the graph is generated by a first order Markov process, i.e., the probability of an arc appearing between two nodes is independent of other arcs in the graph, as in G(N, p), calculating F is simple. For more complex graphs where there are conditional dependencies between the arcs in/out from a node, we need to model the graph generating process as a second or even higher order Markov processes to compute F ."}, {"heading": "4.4 Description Shapes", "text": "The number of nodes (D) in a description of length (L) depends on its shape. The decoding complexity of a description is a function of both its size and its shape. Flat descriptions, which are the easiest to decode only use arcs between the node being described and the nodes in the description. E.g., Jim, who lives in Palo Alto, CA, age 56 yrs, works for Stanford, studied at UC Berkeley, married to Jane. The description consists of the arcs from node being described, Jim, to the descriptor nodes, Palo Alto CA, 56 yrs, Stanford, UC Berkeley and Jane. The length (L) flat descriptions, i.e., the number of arcs included, is the same as the number of nodes (D), i.e., L = D. They have O(aD) decoding complexity, where a is the average degree of each node.\nL3L2L1\nFlat Deep Intermediate\nDescription Shapes\nFlat descriptions are most effective when the descriptors have low ambiguity and do not require disambiguation themselves. In the previous description of Jim, the terms \u2018Palo Alto, CA\u2019, \u2018Stanford\u2019 and \u2018UC Berkeley\u2019 are relatively unambiguous. Most descriptions in daily communication are relatively flat. When low ambiguity descriptor terms are not available, the descriptor terms might need to be disambiguated themselves. In such cases adding \u2018depth\u2019 to the description is helpful.\nIn their most general form, deep descriptions do not impose any constraints on the shape. E.g., Jim, who is married to Jane who went to school with Jim\u2019s sister and whose sister\u2019s child goes to the same school that Jim\u2019s child goes to. This description includes a number of links between the descriptor nodes (Jane, Jane\u2019s school, Jim\u2019s\nsister, etc.). The goal is capture some unique set of relationships that serve to unambiguously identify the node being described. Decoding deep descriptions involves solving a subgraph isomorphism problem and is NP-complete. These descriptions have length up to L = D2/2 and have O(ND) decoding complexity.\nA trade off between decoding complexity and expressiveness can be achieved with by constraining the arcs (between descriptor nodes) that are included in the description. More specifically, the D nodes in the description can be arranged into square blocks where only the arcs within each block are included in the description. We assume that there are Db links between the descriptor nodes giving us L = D(b + 1). This is illustrated in Fig. 1, with the label \u2019intermediate\u2019. When b = D/2, these reduce to the general form of deep descriptions."}, {"heading": "5 Description size for unambiguous reference", "text": "Problem Statement: The sender is trying to communicate a message that mentions a large number of randomly chosen entities, whose average ambiguity is Ax. The overall graph has N nodes. Each entity in the message has a description, involving on average, D descriptor nodes, whose ambiguity rate is Ad. The description includes bD (b \u2264 D/2) arcs between the descriptor nodes, which may be used to reduce the ambiguity in the descriptor nodes themselves, if any. We are interested in the average number of arcs and nodes required in the description.\nIn the most general terms, the ambiguity resolved by a description is less than or equal to its information content. More specifically, if F is the salience rate of the graph, under the assumption of uniform salience rate, we have:\nD = Ax\nF \u2212max(0, Ad \u2212 bF ) (3)\nEquation 33 covers a range of communication scenarios, some of which we discuss now. Proofs and empirical validation of equation 33 and its application to these scenarios are in the appendix We first examine the impact of the structure of the description, which affects the computational cost of constructing and decoding it."}, {"heading": "5.1 Flat Descriptions", "text": "Flat descriptions (Fig. 2), which are the easiest to decode, only use arcs between the node being described and the descriptor nodes. They can be decoded in O(aD), where a is the average degree of a node. For flat descriptions, b = 0, giving,\nD = Ax\nF \u2212Ad Flat descriptions (4)\nFlat descriptions are very easy to decode, but require relatively unambiguous descriptor nodes, i.e., F Ad. Most descriptions in human communication fall into this category."}, {"heading": "5.2 Deep Descriptions", "text": "If the descriptor nodes themselves are very ambiguous (F \u2212 Ad is small), the ambiguity of the descriptor nodes can be reduced by adding bD arcs between them. If the descriptor nodes are considerably less ambiguous than the node being described (Ad < Ax/2), all ambiguity in the descriptor nodes can be eliminated by including Ad/F links between them, giving us:"}, {"heading": "5.3 Purely Structural Descriptions", "text": "When the sender and receiver don\u2019t share any linguistic knowledge, all nodes are maximally ambiguious (Ax = Ad = logN ). We have to rely purely on the structure of the graph. We have:\nD = 2 log(N)/F Purely structural descriptions (6)\nBy using detailed descriptions that include multiple attributes of each of the descriptor nodes, we can bootstrap communication even when there is almost no shared language."}, {"heading": "5.4 Limiting Sender Computation", "text": "The sender may not be able to search through multiple candidate descriptions, checking for uniqueness. We are interested in D such that every candidate description of size D and salience rate F is very likely unique. Assuming unambiguous descriptor nodes, we have:\nD = log(N) +Ax\nF Flat Landmark descriptions (7)\nWhen the descriptions are constructed by randomly choosing facts about the entity, the salience rate is equal to the entropy of the adjacency matrix of the graph, Hg . In this case, the the nodes can use the same set of descriptor nodes, whence the name \u2018landmark descriptions\u2019."}, {"heading": "5.5 Language vs Knowledge + Computation trade off", "text": "Consider a node with no name (Ax = log(N)). Given a set of candidate descriptions with salience rate F , we consider two kinds of descriptions which are at opposite ends of the spectrum in the use of language vs knowledge. We could use purely structural descriptions (eq 6), which use no shared language. We could also, use a flat landmark description (eq. 34) which makes much greater use shared language and ignores most of shared graph structure/knowledge. Though the number of nodes D = 2 log(N)/F is the same in both, flat descriptions are of length O(D), require no computation to generate and can be interpreted in time O(aD). The former, in contrast are of length O(D2). Further, since generating and interpreting them involves solving a subgraph isomorphism problem, they may require O(N2 log(N)/F ) time to interpret. This contrast illustrates tradeoff between shared language, shared domain knowledge and computational cost. We can overcome the lack of shared language by using shared knowledge, but only at the cost of exponential computation."}, {"heading": "5.6 Minimum Sharing Required", "text": "When there are relatively unambiguous descriptor nodes available,Ax/F , the minimum size of the description forX , is a measure of the difficulty of communicating a reference toX . It can be high either becauseX is very ambiguous (Ax \u2192 log(N)) and/or because very little unique is known about it (F \u2192 0). When Ax/F \u2265 N it is not possible to communicate a reference to X . As the ambiguity of the descriptor nodes increases, domain knowledge has to play a greater role in disambiguation. In the limit, when there are no names, we have to use purely structural descriptions. In this case, 2 log(N)/F has to be less than N ."}, {"heading": "5.7 Non-identifying descriptions", "text": "We are interested in comparing the number of statements that can be made about an entity, while still keeping it indistinguishable fromK other entities [7], with the number of statements required to uniquely identify it. For this comparison to be meaningful, in both cases, we use statements with the same salience rate (F ). Since the purpose is to hide the identity of the entity, its name is not included in the description, i.e., Ax = log(N). For flat descriptions we have:\nD \u2264 log(N)\u2212 log(K) F \u2212Ad\n(8)\nComparing this to equation 4 (with Ax = log(N)), we see that there is only a small size difference (log(K)/(F\u2212Ad)) between K-Anonymous descriptions and the shortest unique description. This is because of the phase change (discussed in the appendix), wherein at aroundD = Ax/(F\u2212Ad), the probability of finding a unique description of size D abruptly goes from \u2248 0 to \u2248 1. Though most descriptions of size Ax/(F \u2212Ad) are not unique, for every node, there is at least one such description that is unique.\nGiven the statistical nature of equation 8, it is a neccessary, but not sufficient condition for privacy. Given a large set of entity descriptions, if the average size of description\nis close to or larger than this limit, then, with high probability, at least some of the entities have been uniquely identified."}, {"heading": "6 Conclusion", "text": "As Shannon [23] alluded to, communication is not just correctly transmitting a symbol sequence, but also understanding what these symbols denote.\nEven when the symbols are ambiguous, using descriptions, the sender can unambiguously communicate which entities the symbols refer to. We introduced a model for \u2018Reference by Description\u2019 and show how the size of the description goes up as the amount of shared knowledge, both linguistic and domain, goes down. We showed how unambiguous references can be constructed from purely ambiguous terms, at the cost of added computation. The framework in this paper opens many directions for next steps:\n\u2013 Our model makes a number of simplifying assumptions. It assumes that the sender has knowledge of what the receiver knows. An example of this assumption breaking down is when two strangers speaking different languages have to communicate. It often involves a protocol of pointing to something and uttering its name in order to both establish some shared names and to understand what the other knows. A related problem appears in the case of broadcast communication, where different receivers may have different levels of knowledge, some of which is unknown to the sender. A richer model, that incorporates a probabilistic characterization of not just the domain, but also of the receiver\u2019s knowledge, would be a big step towards capturing these phenomena. \u2013 We have assumed large graphs and long messages. In practice, context is used to circumscribe the graph. Understanding the relation between context and descriptions would be very interesting. \u2013 Though our communication model makes no assumptions about the graph, the simple form of the results presented here arise out of assumptions about ergodicity and uniformity of salience rate (which are analogous to those made in [23]). Versions of these results that don\u2019t make these assumptions would be useful. \u2013 Though we have touched briefly on practical applications of our model, much work remains to be done. The first task is the development of algorithms for constructing unique descriptions."}, {"heading": "Acknowledgments", "text": "The first author thanks Phokion Kolaitis and Andrew Tompkins for providing a home in IBM research to start this work and Bill Coughran at Google to complete it. Carolyn Au did the figures. Carolyn Au, Vint Cerf, Madeleine Clark, Evgeniy Gabrilovich, Neel Guha, Sreya Guha, Asha Guha, Joe Halpern, Maggie Johnson, Brendan Juba, Arun Majumdar, Peter Norvig, Mukund Sundarajan and Alfred Spector provided feedback on drafts of this paper."}, {"heading": "Appendices", "text": "We have the following appendices:\nAppendix A: Derivation of results and various special cases Appendix B: Empirical validation of results on random graphs Appendix C: Studies on usage of descriptive references on a set of newspaper/magazine\narticles Appendix D: Alternate problem formulations"}, {"heading": "A Derivation of Results", "text": ""}, {"heading": "A.1 Problem Statement", "text": "We are given a large graph G with N nodes and a message, which is a subgraph of G. The message contains a number of randomly chosen nodes. We construct descriptions for each of the nodes (X) in this subgraph so that it is uniquely identified. We are interested in a stochastic characterization of the relationship between the amount of shared domain knowledge, shared language, the number of nodes in the description (D) and the number of arcs (L) in the description.\nFor expository reasons, we go through the derivation for the simpler case, where there is no ambiguity in the descriptor nodes. We then extend this proof to the case where the descriptor nodes themselves may be ambiguous.\nWe model the graph corresponding to the domain of discourse, and the message being transmitted, as being generated by a stochastic processes. We carry over the assumption from Shannon [23] and information theory [5] that sequences of symbols (in our case, entries in the adjacency matrix) are generated by an ergodic process."}, {"heading": "A.2 Unambiguous Descriptor Nodes", "text": "Consider a node X in the message, which has an ambiguity of Ax2. The description for X involvesD unambiguous descriptor nodes. There are a number of possible configurations ofD arcs fromX to the descriptor nodes. Let us call these \u3008CXD1, CXD2, CXD3, ...\u3009. Let the probability of the ith of these occuring between a randomly chosen node a set of D descriptor nodes be pi.\nThe sender looks through an ensemble of S descriptions, i.e., S possible combinations of D descriptor nodes in search of a unique description for X . Let the probability a randomly chosen element of this ensemble of these descriptions having the configuration CXDi be qi.\nThere are 2Ax \u2212 1 nodes that might be mistaken for X . Consider one particular element in the ensemble that has the configuration CXDi with X . It provides a disambiguating description forX if none of the 2Ax\u22121 nodes that we are trying to distinguish\n2 More precisely, we let the average ambiguity rate of the nodes in the message be Ax. Henceforth, even though we are dealing with average value of properties of entities in the message, for the sake of clarity in the presentation, we will treat the corresponding properties of the node X and its descriptor nodes as proxies for the average.\nX from is also in the configuration CXDi with this set of descriptor nodes. The probability of this is\n(1\u2212 pi)2 Ax\u22121 (9)\nThere are S such sets of descriptors. We want the probability of none of these S descriptions being unique to be less than . Assuming independence between the descriptions in the ensemble, the probability of this is:\u220f\nDi\u2208S (1\u2212 (1\u2212 pi)2\nAx\u22121) \u2264 (10)\nThis product is over the candidate descriptions in the ensemble. Since we are interested in descriptions that are unlikely to be satisfied by other nodes, we can restrict our attention to the case where pi is very small, or more specifically, (2Ax\u22121)pi 1. This allows us to use the binomial approximation, using which we get:\u220f\nDi\u2208S (1\u2212 (1\u2212 pi(2Ax \u2212 1))) \u2264 (11)\nAssuming 2Ax 1 \u220f Di\u2208S pi2 Ax \u2264 (12)\nTaking logs, \u2211 Di\u2208S log pi + SAx \u2264 log( ) (13)\nFor sufficiently large S, the different description configurations will occur in proportion to their likelihood, i.e. the number of times shape CXDi occurs in the set S is approximately Sqi. So, \u2211 Di\u2208S log pi can be rewritten as, \u2211 j Sqj log pj where j ranges over all the possible description configurations.\u2211 Di\u2208S log pi = \u2211 j Sqj log pj = \u2212SFD (14)\nwhere FD is the average salience of the descriptions in the ensemble.\nFD \u2265 Ax \u2212 log( )\nS (15)\nFrom this, we see that the impact of the size of S on the average FD required is a function of . Increasing S beyond the stage where log( )S is sufficiently small does not provide additional benefit.\nIf we want at most a small constant (say one) node to not have a unique description, = 1/N , in which case we get,\nFD = Ax \u2212 log(N)\nS (16)\nIf S is sufficiently large so that log(N)S is negligible, we get,\nFD = Ax (17)\nIf the salience rate of the ensemble is F , i.e., FD = FD, where D is the average length of the description in the ensemble, we have\nD = Ax/F (18)\nThis gives us an upper bound on D. We now show that this is also a lower bound (under the assumption, Ax 0). To simplify, we demonstrate the proof for the average case, where FD = DF . We are interested in large graphs where the number of ambiguous nodes does not grow with the size of the graph. So we let = 1/N . We can rewrite equation 28 as:\n2(Ax\u2212DF )S = 2(Ax+ log(N) S \u2212DF )S (19)\nThis is the number of ambiguous nodes. Let\nD = (1 + \u03b4)(Ax +\nlog(N) S )\nF (20)\nwhere \u03b4 can be positive or negative. We get\n2\u03b4(Ax+log(N)/S)S = Number of ambiguous nodes (21)\nClearly, for large N , the only way the number of ambiguous nodes does not grow with N is if \u03b4 \u2264 0, showing that equation 33 is a lower bound as well.\nA variant of this derivation shows the sensitivity of the whether a set of descriptions are unambiguous to the description size. Using the earlier approximations, we get the probability pu of a node having a uniquely identifying description of size D as:\npu = (1\u2212 (1\u2212 2\u2212FD)2 Ax )S (22)\npu = 2 (Ax\u2212FD)S (23)\nLet D = AxF + \u03b4, where \u03b4 can be positive or negative. This gives us\npu = 2 \u03b4S (24)\nGiven the size of S (O(logN)), in the exponent, for large N , it is easy to see how as \u03b4 goes from negative to positive, at around \u03b4 = 0, there is a \u2018phase change\u2019 and pu abruptly goes from being pu \u2248 0 to pu \u2248 1. So, for a certain D which is just less than given by equation 33 almost none of the nodes have unique descriptions. When the description size reaches that given by equation 33 there is an abrupt change and almost all nodes have unique descriptions.\nIf the shared salience between the sender and receiver is M , then, by using arguments identical to those in [23], we have\nD = Ax/M (25)\nNote again that this is the upper bound on the average number of descriptor nodes for the entities in a sufficiently long message. When Ax is low, description sizes will be small and individual nodes in the message may have shorter or longer descriptions. However, this bound still applies to the average description length for a large set of nodes."}, {"heading": "A.3 Ambiguous Descriptor Nodes", "text": "We now consider the case where the descriptor nodes themselves are ambiguous. Let the ambiguity rate of the descriptor nodes be Ad. The description also includes bD (0 \u2264 b \u2264 D/2) arcs between the descriptor nodes. The role of these bD arcs is to reduce the ambiguity of the descriptor nodes.\nThere are a number of possible configurations of bD arcs between a randomly chosen set ofD candidate descriptor nodes. Let us call these \u3008CbD1, CbD2, CbD3...\u3009 Let the probability of the ith of these occurring amongst a randomly chosen set ofD descriptor nodes be qi3.\nThe sender looks through S possible combinations of D descriptor nodes in search of a unique description for X . Consider one particular set i of D descriptor nodes for X . Let the configuration of the arcs between X and the D nodes be CXDi and the configuration of the bD arcs between the D nodes be CbDi.\nThere are 2DAd sets of D nodes which have the same names as these descriptor nodes. Only one of these is the intended set of descriptor nodes. One of the other 2DAd\u2212 1 sets of descriptor nodes can be mistaken for the intended descriptor nodes only if the bD arcs between them also has the configuration CbDi, the probability of which is qi. Similarly, there are 2Ax \u22121 nodes that might be mistaken for X . The probability of one of these having the configuration CXDi with a set of D descriptor nodes is pi. This set of descriptor nodes provides a disambiguating description for X if,\n1. None of the 2Ax \u2212 1 nodes that we are trying to distinguish X from is in the configuration CXDi with this set of descriptor nodes AND 2. None of the 2Ax \u2212 1 nodes that we are trying to distinguish X from is in the configuration CXDi with one of the set of nodes that the descriptor nodes could be mistaken for.\nThe probability of this set of descriptor nodes providing a unique description for X is:\n(1\u2212 pi)2 Ax\u22121(1\u2212 piqi)(2 Ax\u22121)(2DAd\u22121) (26)\nAs before, assume that 2Ax 1. To simplify the analysis, we only consider the case where the ambiguity in the descriptor nodes is not insignificant, i.e., 2DAd 1. With these assumptions, we get:\n3 Ambiguity amongst the descriptor nodes themselves might lead to some of these configuration of arcs might being automorphic to others. In this paper, we ignore this.\n(1\u2212 pi)2 Ax (1\u2212 piqi)2 Ax+DAd (27)\nThere are S such sets of descriptors. The probability of none of these S descriptions being unique should be less than .\u220f\nDi\u2208S (1\u2212 (1\u2212 pi)2 Ax (1\u2212 piqi)2 Ax+DAd ) < (28)\nUsing the binomial approximation (as before, we assume that the number of descriptions is large and hence pi and piqi are very small),\u220f\nDi\u2208S (1\u2212 (1\u2212 pi2Ax)(1\u2212 piqi2Ax+DAd)) < (29)\nMultiplying out, and ignoring terms with higher powers of pi and qi,\u220f Di\u2208S pi2 Ax ( 1 + qi2 DAd ) < (30)\nTaking logs, \u2211 Di\u2208S log pi + SAx + \u2211 Di\u2208S log(1 + qi2 DAd) < (31)\nqi is the probability of the ith shape in the ensemble occurring between a randomly chosen set of D nodes and is equal to 2\u2212bDF , where F is the salience rate for the ensemble with respect to the graph. So,\u2211\nDi\u2208S log pi + SAx + \u2211 Di\u2208S log(1 + 2DAd\u2212bDF ) < (32)\nAssume log(1 + 2DAd\u2212bDF ) \u2248 D(Ad \u2212 bF ), bF \u2264 Ad. If bF > Ad, log(1 + 2DAd\u2212bDF ) \u2248 0. So, log(1+ 2DAd\u2212bDF ) \u2248 Dmax(0, Ad \u2212 bF ). Letting pi = 2\u2212DF as before, we get:\nD \u2248 log(N)/S +Ax F \u2212max(0, Ad \u2212 bF )\n(33)\nA.4 Searching through candidate descriptions\nDescription size (and hence decoding cost) is influenced by S, the number of potential descriptions the sender can search through. S = 1 corresponds to the case where D is sufficiently large, so that any selection ofD nodes will likely form a unique shape. This minimizes the sender\u2019s computation at the expense of description length and receivers compute cost.\nTheD nodes can be selected such that the sameD nodes are used to describe all the remaining N \u2212D nodes or each node can use a different set of D nodes to describe it. We call these \u2018landmarks\u2019 nodes and the associated descriptions are called \u2018landmark descriptions\u2019. From eq. (33), we have:\nD = log(N) +Ax\nF \u2212max(0, Ad \u2212 bF ) (34)\nIntuitively, the size of D given by equation 34 answers the following question: how many randomly chosen facts, at the salience rate F , about an node does one have to specify to uniquely identify that node, with high probability. Note that in this case, the sender is not looking at the other nodes in the domain to see if the description is unique. If the description is longer that the size given by equation 34, it is very likely unique.\nRemember that if the statements are chosen at random from the graph, the salience rate is Hg . So, restricting ourselves to flat descriptions composed of randomly chosen statements about the object, we have:\nD = log(N) +Ax Hg \u2212Ad\n(35)\nOne interesting case of this is where the \u2018randomly\u2019 chosen nodes (in terms of which X is described) is the same for allX . Such a set of descriptor nodes serve as \u2019landmarks\u2019 in terms of which all other nodes are described. If the landmark nodes are unambiguous and the X have no name, we have the special case where:\nD = 2 log(N)\nHg (36)\nAt the other extreme, the sender could search through enough descriptions to find the smallest set of descriptor nodes for uniquely identifying X . For each description, the sender checks to see if the description is indeed unique. It is enough for the sender to search through S randomly chosen descriptions such that logNS \u2248 C, whereC is a small constant (which can be ignored). In practice, by going through candidate descriptions in something better than random order, far fewer than O(log(N)) descriptions need to be considered. In this case:\nD = Ax\nF \u2212max(0, Ad \u2212 bF ) (37)"}, {"heading": "A.5 Flat Descriptions", "text": "The shape of the description influences decoding complexity. Flat descriptions, which are the easiest to decode, only use arcs between the node being described and the nodes in the description \u2014 no arcs between other nodes in the description are considered. Thus for these b = 0.\nD = Ax\nF \u2212Ad Flat descriptions (38)\nIn the case where we do not have a name for the node being described, we get:\nD = logN\nF \u2212Ad Flat descriptions (39)\nAs expected, flat descriptions are longer when the sender can not search through multiple candidates.\nD = log(N) +Ax F \u2212Ad Flat landmark descriptions (40)\nIf F < Ad, we cannot use flat descriptions."}, {"heading": "A.6 Deep Descriptions", "text": "When Ad > 0, the number of nodes in the description may be reduced by using deep descriptions. In deep descriptions, in addition to the arcs between the descriptor nodes and X , arcs between the D nodes may also be included in the description. We include bD arcs between the descriptor nodes in the description.\nWe can restrict the search for descriptions (i.e., S = 1), giving us deep landmark descriptions. If b \u2264 AdF , we have:\nD = log(N) +Ax (b+ 1)F \u2212Ad Deep Landmark Descriptions (41)\nNote that if (b + 1)F < Ad, then communication will not be possible. When the descriptor nodes are unambiguous, adding depth does not provide any utility. For sufficiently large S,\nD = Ax\n(b+ 1)F \u2212Ad Deep Descriptions (42)\nWhenAd < 2Ax andAd/F < D/2 and we can eliminate ambiguity in the descriptor nodes without increasing D, giving us:\nD = Ax F Deep descriptions (43)\nGiven a particular \u3008F,Ad, Ax\u3009, deep descriptions have the fewest nodes. AsF decreases or Ax (or Ad) increases, we need bigger descriptions."}, {"heading": "A.7 Purely Structural Descriptions", "text": "For purely structural descriptions (i.e. no names), Ax = Ad = logN . Allowing b = D/2 in equation 42:\nD = log(N) (D/2 + 1)F \u2212 log(N) \u2248 log(N) DF/2\u2212 log(N)\n(44)\nD2F = 2D(1 + log(N)) \u2248 2D log(N) (45) From which we get:\nD = 2 log(N)\nF (46)"}, {"heading": "A.8 Message Composition / Self Describing Messages", "text": "We have allowed for the entities in a message to be randomly chosen. The entities in the message may or may not have relations between them. For example, if the message is a set of census records or entries from a phone book, the different entities in the message will likely not be part of each other\u2019s short descriptions. In contrast, in a message like a news article, the entities that appear do so because of the relations between them and can be expected to appear in each other\u2019s descriptions.\nWe call messages, where all the descriptors for the entities in the message are other entities in the message, \u2018self describing\u2019 messages. We are interested in the size of such messages.\nLet the message include the relationship of each node to all the other nodes. Setting Ax = Ad in equation 41 and assuming Ad/F < D/2 and Ad log(N), we get:\nD = log(N)\nF (47)\nAll messages with at least as many nodes as given by equation 47, that include all the relations between the nodes, are self describing.\nComparing eq. 34 to eq. 43 we see that while most sets of Ax/F nodes do not have a unique set of relations, about O(1/ log(N)) of them do. Setting b = D/2 and Ax = Ad = A in equation 42, and approximating, we get the minimum size of a self describing message:\nD = 2A\nF (48)"}, {"heading": "A.9 Communication Overhead", "text": "Descriptions can be used to overcome linguistic ambiguity, but at the cost of added computational complexity in encoding and decoding descriptions. We now look at the impact of descriptions on the channel capacity required to send the message. We only consider the case where the sender and receiver share the same view of the graph.\nConsider a message that includes some number of arcs connecting W nodes. If we assume that the number of distinct arc labels is much less than N , most of the communication cost is in referring to the W nodes in the message.\nNow, consider the case where each node in the graph is assigned a unique name. Each name will require log(N) bits to encode. If the message is a random selection of arcs from graph, we need W log(N) bits to encode references to the nodes.\nNext consider encoding references to these W nodes using flat descriptions with unambiguous landmarks where the descriptions are constructed by randomly picking statements about each node, i.e., F = Hg . This is the case covered by equation 36. We will need descriptions of length 2 log(N)/Hg . These descriptions are strings from the adjacency matrix and have an entropy rate of Hg . So, the string of length 2 log(N)/Hg can be communicated using 2 log(N) bits and references to W nodes will require 2M log(N) bits. Comparing this to using unique names for each node, we see that there is a overhead factor of 2.\nNow consider encoding references to theseW nodes using purely structural descriptions. These descriptions require 2 log(N)/Hg nodes and are of length 2(log(N)/Hg)2 and we will require 2 log(N)2/Hg bits to represent each description, giving us an overhead of 2 log(N)/Hg . We see that purely structural descriptions are not only very hard to decode, but they also incur a significant channel capacity overhead. This is assuming that the nodes in the message are randomly chosen. However, if the message is self describing, each of the nodes in the message serves as descriptors for the other nodes in the message, amortizing the description cost. Since there are 2 log(N)/Hg nodes in the message, there is no overhead."}, {"heading": "A.10 Non-identifying descriptions", "text": "We are interested in the question of how much can be revealed about an entity without uniquely identifying it. This is of use in applications involving sharing data for research purposes, for delivering personalized content, personalized ads, etc.\nWe are interested in comparing the number of statements that can be made about an entity, while still keeping it indistinguishable fromK other entities [7], with the number of statements required to uniquely identify it. For this comparison to be meaningful, the descriptions need to be drawn from the same ensemble of descriptions. For a given salience rate F , we are interested in how big D can be, such that at least K other nodes satisfy this description. Since the purpose is to hide the identity of the entity, the name of the entity is not included in the description, i.e., Ax = log(N).\nGiven N nodes and R descriptions, assume that each node is randomly assigned a description. The probability that a given description corresponds to r nodes is approximately:\npr \u2248 e\u2212\u03bb\u03bbr\nr! (49)\nwhere \u03bb = NR , since this is a Poisson process with parameter \u03bb. However, since there are 2DAd interpretations for D, let us find the number of other nodes that also map to these 2DAd descriptions. Since the sum of Poisson processes is a Poisson process, we find that the probability of r additional nodes mapping to any of these 2DAd descriptions is\npr \u2248 e\u2212\u03bb2 DAd (\u03bb2DAd)r\nr! (50)\nThus the approximate number of nodes with fewer that K such conflicts is\nK\u22121\u2211 r=0 Npr = Ne \u2212\u03bb2DAd K\u22121\u2211 r=0 (\u03bb2DAd)r r! (51)\nWe want this number to be a small constant U , thus\nK\u22121\u2211 r=0 (\u03bb2DAd)r r! = Ue\u03bb2 DAd N (52)\nThe left hand side consists of the firstK terms of the Taylor series of ex. Using Taylor\u2019s theorem we have\nex \u2212 K\u22121\u2211 r=0 xr r! \u2264 e xxK K! (53)\nSubstituting the summation, we get:\ne\u03bb2 DAd \u2212 Ue\n\u03bb2DAd N \u2264 e \u03bb2DAd (\u03bb2DAd)K K! (54)\nThus\nK! ( 1\u2212 U\nN\n) \u2264 (\u03bb2DAd)K (55)\nUsing Stirling\u2019s approximation K! \u2248 (K/e)K and taking K-th roots of both sides,\nK\ne\n( 1\u2212 U\nN\n) 1 K\n\u2264 \u03bb2DAd (56)\nSince U/N is small, we can use the binomial approximation to get\nK\ne\n( 1\u2212 U\nKN ) = KN \u2212 U eN \u2264 N2 DAd F (57)\nThus R \u2264 eN 22DAd\nKN\u2212U . Since U KN , we can ignore U . For sufficiently large descriptions, on average, the probability of a description size D holding for an object is 2\u2212DF where F is the salience rate of the description. So, we have:\nR = 2DF \u2264 eN2 DAd\nK (58)\nTaking logs we get:\nD \u2264 logN \u2212 log Ke F \u2212Ad \u2248 logN \u2212 logK F \u2212Ad (59)\nDiscussion on non-identifying descriptions Comparing equation 59 to equation 39, we see that D for \u2018K-anonymity\u2019 is very close to the D for the shortest description of that node. For any node, there are a lot \u2013 ( N D ) \u2013 of descriptions of size logN\u2212logKF\u2212Ad . All of these are satisfied by at least K other nodes. In fact, we could use a slightly larger value of D, as given in equation 59 and most descriptions of this size would be satisfied by at least one other node. Most descriptions of size logN/(F \u2212 Ad) do not reveal identity. But for every node, there is at least one description of this size that uniquely identifies it. Once the description size exceeds 2 logN/(F \u2212 Ad), then, with high probability, every description of that size uniquely identifies the node.\nThis behavior of descriptions \u2014 until a certain size D < logN/(F \u2212 Ad) they are, with high probability ambiguous, but once they cross that threshold, there is a \u2018phase transition\u2019 and their ambiguity cannot be guaranteed \u2014 follows from from the\nanalysis in the earlier section on the phase transition in the likelihood of finding unique description of size D.\nWe also note that the limits we have derived are for the average length of descriptions for the entities in a sufficiently long message. Therefore, limits derived above are a neccessary, but not sufficient condition for anonymity. If the average size of the descriptions in a sufficiently large set of descriptions is higher than that given by equation 59, then, with high probability, anonymity has not been preserved."}, {"heading": "B Empirical Validation", "text": "We validate our results for description length for on a set of random graphs with different structural, connection and naming properties. We look at three kinds of random graphs:\n1. Erdos-Renyi random graph withN nodes, where the probability of two nodes being connected is p. Nodes are divided into two categories, the first are assigned names with ambiguity Ax and the second are assigned names with ambiguity Ad. 2. A random bipartite graph where the probability of a node from one size (same number of nodes on both sides) being connected to a node from the other side is p. Nodes on one side are assigned names with ambiguity Ax and nodes on the other side are assigned names with ambiguity Ad. 3. A random graph with local clustering, constructed as follows: We start with a number of Erdos-Renyi graphs that are not connected to each other. With then pick a number of pairs of nodes from different clusters and add a link between them, with probability p. Within each node, we divide nodes into two categories and assign them names with ambiguity Ax and Ad.\nWe look at three categories of descriptions:\n1. Flat descriptions with unambiguous descriptors (equation 18). 2. Flat descriptions with ambiguous descriptors (equation 38). 3. Deep descriptions (equation 42).\nWe vary the following parameters:\n1. The salience rate for the graph. Note that the salience. rate for each of these graphs is \u2212log(p). 2. Ax, the ambiguity in the nodes being described. 3. Ax, the ambiguity in the descriptor nodes\nFor each value of p, Ax and Ad, we generated 10 random instances of the graphs and (with N = 1000). For each instance, computed the shortest description for 100 of the nodes. We compared the lengths of these descriptions with those predicted by the corresponding equations. Figures 2-9 compare the actual description lengths with those predicted by our theory.\nOverall the predicted lengths correspond fairly closely to the observed lengths. The differences between observed and predicted lengths are because of the approximations made in deriving equations 18, 38 and 42.\nDescription length as a function of salience, for flat descriptions with no ambiguity in descriptor nodes. Ambiguity in nodes being described is held constant at log2 100 (i.e., 100 nodes corresponding to each name).\nDescription length as a function of salience, for flat descriptions. Ambiguity in nodes being described is held constant at log2 100 (i.e., 100 nodes corresponding to each name). Ambiguity in descriptor nodes is log2 1.4 (i.e., 1.4 nodes corresponding to each name, on average).\nDescription length as a function of salience, for deep descriptions. Ambiguity in nodes being described is held constant at log2 100 (i.e., 100 nodes corresponding to each name). Ambiguity in descriptor nodes is log2 8 (i.e., 8 nodes corresponding to each name, on average). Number of arcs between descriptor nodes (bD) comes from the actual graph.\nDescription length as a function of ambiguity in nodes being described, for flat descriptions. Salience rate is held constant at \u2212 log2 0.01 and there is no ambiguity in the descriptor nodes.\nDescription length as a function of ambiguity in nodes being described, for flat descriptions. Salience rate is held constant at \u2212 log2 0.01. Ambiguity in descriptor nodes is log2 1.4 (i.e., 1.4 nodes corresponding to each name, on average).\nDescription length as a function of ambiguity in the nodes being described, for deep descriptions. Salience rate is held constant at \u2212 log2 0.01. Ambiguity in descriptor nodes is log2 10 (i.e., 10 nodes corresponding to each name, on average). Number of arcs between descriptor nodes (bD) comes from the actual graph, which accounts for the jagged nature of the predicted length.\nDescription length as a function of ambiguity in the descriptor nodes, for flat descriptions. Salience rate is held constant at \u2212 log2 0.01. Ambiguity in the nodes being described is held constant at log2 100 (i.e., 100 nodes corresponding to each name, on average).\nDescription length as a function of ambiguity in the descriptor nodes, for deep descriptions. Salience rate is held constant at \u2212 log2 0.01. Ambiguity in the nodes being described is held constant at log2 100 (i.e., 100 nodes corresponding to each name, on average). Number of arcs between descriptor nodes (bD) comes from the actual graph."}, {"heading": "C Ubiquity of Reference by Description", "text": "Reference by description is ubiquitous in everyday human communication. Below are the results of an analysis of 50 articles from 7 different news sources, covering 3 different kinds of articles \u2014 analysis/opinion pieces, breaking news and wedding announcements/obituaries. We extracted the references to people, places and organizations from these articles. In each article entity pair, we examined the first reference in the article to that entity and analyzed it. The number of entities referenced and the size of descriptions, as measured by the number of descriptor entities in the description, are given in tables 1 and 2. We found the following:\n1. Almost all references to people have descriptions. The only exceptions are very well known figures (e.g., Obama, Ronald Reagan). 2. References to many places, especially countries and large cities, do not have associated descriptions. References to smaller places, such as smaller cities and neighbourhoods follow a stylized convention, which gives the city, state and if neccessary, the country. 3. News articles tend to contain more references to public figures, who have shorter descriptions, reflecting the assumption of their being known to readers. 4. Wedding/obituary announcements, in contrast, tend to feature descriptions of greater detail. 5. The names of many organizations are, in effect, descriptions (e.g., Palo Alto Unified School District).\nArticle Type \u2192 Analysis/Opinion Breaking Obituaries Total Source \u2193 Piece News Weddings NY Times 23 6 24 53 BBC 14 24 4 42 Atlantic 121 0 0 121 CNN 10 11 6 27 Telegraph 12 20 10 42 LA Times 16 21 9 46 Washing. Post 28 23 8 59 Total 224 104 61 389\nNumber of entity reference in each source, broken down by article type\nGiven that these articles are stories, the entities that are mentioned in them are closely related. Consequently, there exists no clear distinction between the parts of the description that serve to identify an entity from the message itself."}, {"heading": "D Alternate Problem Formulations", "text": "Descriptions based on random graph models are only one way of looking at the problem of Reference by Description. In this section, we briefly look at three alternate formula-\nArticle Type \u2192 Analysis/Opinion Breaking Obituaries Average Source \u2193 Piece News Weddings NY Times 2.9 3 3.5 3.18 BBC 3 2.54 3.5 2.78 Atlantic 3.7 0 0 3.7 CNN 2.5 2.63 4 2.88 Telegraph 2.6 2.95 3.2 2.9 LA Times 2.43 2.52 3.55 2.69 Washing. Post 3.14 3.65 4 3.45 Average 3.29 2.92 3.57 3.23\nAverage description size in each source, broken down by article type\ntions. In all of these formulations, we continue to use the model described in section 3. We modify our analysis of descriptions and/or the richness of descriptions."}, {"heading": "D.1 Combinatorial analysis", "text": "Here we look at descriptions from a combinatorial perspective. Variations exist for the following combinatorial decision problem: Given a graph with N nodes, B names and description size D, where each node is assigned one or zero of these names, does each node have a unique description, with fewer than D nodes? In the worst case, B = 0, in which case verification of a possible solution includes solving a subgraph isomorphism problem. Since subgraph isomorphism is known to be NP complete, this problem is NP complete. Since there are ( N D ) sets of D nodes, we might need to solve as many subgraph isomorphism problems."}, {"heading": "D.2 Descriptions and Logical Formulae", "text": "Here, we continue to model the domain of discourse as a directed labelled graph, but instead of restricting descriptions to subgraphs, we allow for a richer description language. More specifically, we use formulae in first order logic as descriptions.\nConsider the class of descriptions where the node being described has no name (Ax = log(N)), where some of the nodes in the description similarly have no name and others have no ambiguity. This class of descriptions can be represented as a first order formula with a single free variable (corresponding to the node being described) and some constant symbols (nodes with shared names). The formula is a unique descriptor for a node when the node is the only binding for the free variable that satisfies the formula. The simplest class of descriptions, \u2018flat descriptions\u2019, corresponds to the logical formula:\nLx1(X,S1) \u2227 Lx2(X,S2) \u2227 ... \u2227 LxS (X,SS) (60)\nwhere Lxi is the label of the arc between X and the i th descriptor node.\nDeep descriptions can be seen as introducing existentially quantified variables (corresponding to the descriptor nodes with no names) into the description. For the sake\nof simplicity, we consider graphs with a single label, L (and Lnull indicating no arc). Consider a description fragment such as Li(X,Sj), where Li is either L or Lnull. Let us allow a single nameless descriptor node. This corresponds to\n(\u2203y Li(X, y) \u2227 Li(y, Sj))\nIntroducing two nameless descriptor nodes corresponds to\n(\u2203 (y1, y2) Li(X, y1) \u2227 Li(X, y2) \u2227 Li(y1, y2) \u2227 Li(y2, y1) \u2227 Li(y1, Sj) \u2227 Li(y2, Sj))\nWe can define different categories of descriptions by introducing constraints on the scope of the existential quantifiers. For example the nameless descriptor nodes can be segregated so that there are separate subgraphs relating the node being described to each of the shared nodes. The logical form of these descriptions (for a single nameless descriptor node) look like:\n(\u2203yLi(X, y) \u2227 Li(y, S1)) \u2227 (\u2203yLi(X, y) \u2227 Li(y, S2)) \u2227 (\u2203yLi(X, y) \u2227 Li(y, S3)) \u2227 . . .\nMore complex descriptions can be created by allowing universally quantified variables, disjunctions, negations, etc. The axiomatic formulation also allows some of the shared domain knowledge to be expressed as axioms. The down side of such a flexible framework is that very few guarantees can be made about the computational complexity of decoding descriptions."}, {"heading": "D.3 Algorithmic descriptions", "text": "We can allow descriptions to be arbitrary programs that take an entity (using some appropriate identifier that cannot be used in the communication) from the sender and output an entity (using a different identifier, understood by the receiver) to the receiver.\nThis approach allows us to handle graphs with structures/regularities that cannot be modeled using stochastic methods. An interesting question is the size of the smallest program required for a given domain, sender and receiver. If the shared domain knowledge is very low, the program will simply have to store a mapping from sender identifier to receiver identifier. As the shared domain knowledge increases, the program can construct and interpret descriptions."}], "references": [{"title": "On k-anonymity and the curse of dimensionality", "author": ["C.C. Aggarwal"], "venue": "In Proceedings of the 31st international conference on Very Large Data Bases, pages 901\u2013909,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Dbpedia-a crystallization point for the web of data", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": "Web Semantics: science, services and agents on the world wide web, 7(3):154\u2013165,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "In 2008 ACM SIGMOD, pages 1247\u20131250. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Hardening soft information sources", "author": ["W.W. Cohen", "H. Kautz", "D. McAllester"], "venue": "In Proceedings of the sixth ACM international conference on Knowledge Discovery and Data mining, pages 255\u2013259,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Elements of Information Theory", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley-Interscience,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "An introduction to database systems", "author": ["C. Data"], "venue": "Addison-Wesley publ.,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1975}, {"title": "Secure databases: Protection against user influence", "author": ["D. Dobkin", "A.K. Jones", "R.J. Lipton"], "venue": "ACM Transactions on Database systems (TODS), 4(1):97\u2013106,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1979}, {"title": "Duplicate record detection: A survey", "author": ["A.K. Elmagarmid", "P.G. Ipeirotis", "V.S. Verykios"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 19:1\u201316,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "On random graphs", "author": ["P. Erd\u0151s", "A. R\u00e9nyi"], "venue": "Publicationes Mathematicae Debrecen, 6:290\u2013 297,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1959}, {"title": "Sense and reference", "author": ["G. Frege"], "venue": "The philosophical review, pages 209\u2013230,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1948}, {"title": "Learning probabilistic relational models", "author": ["L. Getoor", "N. Friedman", "D. Koller", "A. Pfeffer"], "venue": "In Relational data mining, pages 307\u2013335. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Entity resolution: theory, practice & open challenges", "author": ["L. Getoor", "A. Machanavajjhala"], "venue": "Proceedings of the VLDB Endowment, 5(12):2018\u20132019,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Communicating and resolving entity references", "author": ["R.V. Guha"], "venue": "CoRR, abs/1406.6973,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Communicating semantics: Reference by description", "author": ["R.V. Guha", "V. Gupta"], "venue": "CoRR, abs/1511.06341,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Releasing search queries and clicks privately", "author": ["A. Korolova", "K. Kenthapadi", "N. Mishra", "A. Ntoulas"], "venue": "In Proceedings of the 18th international conference on World wide web, pages 171\u2013180. ACM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Naming and necessity", "author": ["S.A. Kripke"], "venue": "Springer,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1972}, {"title": "John mccarthy, pioneer in artificial intelligence, dies at 84", "author": ["J. Markoff"], "venue": "New York Times,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Myths and fallacies of personally identifiable information", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "Communications of the ACM, 53(6):24\u201326,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaling and percolation in the small-world network model", "author": ["M.E. Newman", "D.J. Watts"], "venue": "Physical Review E, 60(6):7332,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Identity uncertainty and citation matching", "author": ["H. Pasula", "B. Marthi", "B. Milch", "S. Russell", "I. Shpitser"], "venue": "In NIPS. MIT Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Mathematical logic", "author": ["W. Quine"], "venue": "Harvard University Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1940}, {"title": "On denoting", "author": ["B. Russell"], "venue": "Mind, pages 479\u2013493,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1905}, {"title": "The mathematical theory of communication", "author": ["C. Shannon"], "venue": "Bell System Technical Journal, 27:379\u2013423,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1948}, {"title": "Collective dynamics of small-worldnetworks", "author": ["D.J. Watts", "S.H. Strogatz"], "venue": "nature, 393(6684):440\u2013442,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "The state of record linkage and current research problems", "author": ["W.E. Winkler"], "venue": "In Statistical Research Division, US Census Bureau. Citeseer,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 16, "context": "For example in the New York Times headline [17] \u2018John McCarthy, Pioneer in Artificial Intelligence .", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "Formal study of descriptions started with Frege [10] and Russell\u2019s [22] descriptivist theory of names, in which names/identity are equivalent to descriptions.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "Formal study of descriptions started with Frege [10] and Russell\u2019s [22] descriptivist theory of names, in which names/identity are equivalent to descriptions.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Kripke [16] argued against this position using examples where differences in domain knowledge could yield vastly different descriptions of the same entity.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "In [23], the founding paper of information theory, Shannon referred to this problem, saying \u2018Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities\u2019, but he passed over it, saying \u2018These semantic aspects of communication are irrelevant to the engineering problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "\u2019 Computational treatments of descriptions started with linking duplicates in census records [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "Keys in relational databases [6] are the best example of this.", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "The goal of privacy preserving information sharing [7] is the complement of unambiguous communication of references, ensuring that the information shared does not reveal the identity of the entities referred to in the message.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "[1], [18] discuss the difficulty of doing this while [15] shows how this can be done for search logs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[1], [18] discuss the difficulty of doing this while [15] shows how this can be done for search logs.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "[1], [18] discuss the difficulty of doing this while [15] shows how this can be done for search logs.", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "Since arbitrary N-tuples can be constructed out of 3-tuples [21], we restrict ourselves to 3-tuples, i.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "In an earlier iterations of this paper [13] we presented this model and the solution for simpler version of this problem.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "We use the Asymptotic Equipartition Property [5] to estimate the expected number of candidate referents of a set of names from from their ambiguity.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "The most commonly studied Random Graph model is that proposed by Erdos and Renyi [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 23, "context": "[24], [19] study small world graphs, where there are a large number of localized clusters and yet, most nodes can be reach from every other node in a small number of hops.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[24], [19] study small world graphs, where there are a large number of localized clusters and yet, most nodes can be reach from every other node in a small number of hops.", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "\u2018Knowledge graphs\u2019, such as DBPedia [2] and Freebase [3], that represent relations between people, places, events, etc.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "\u2018Knowledge graphs\u2019, such as DBPedia [2] and Freebase [3], that represent relations between people, places, events, etc.", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "Learning probabilistic models [11] of such dependencies is an active area of research.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "We are interested in comparing the number of statements that can be made about an entity, while still keeping it indistinguishable fromK other entities [7], with the number of statements required to uniquely identify it.", "startOffset": 152, "endOffset": 155}, {"referenceID": 22, "context": "As Shannon [23] alluded to, communication is not just correctly transmitting a symbol sequence, but also understanding what these symbols denote.", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "\u2013 Though our communication model makes no assumptions about the graph, the simple form of the results presented here arise out of assumptions about ergodicity and uniformity of salience rate (which are analogous to those made in [23]).", "startOffset": 229, "endOffset": 233}], "year": 2016, "abstractText": "Messages often refer to entities such as people, places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for \u2018Reference by Description\u2019, derive results on the conditions under which, with high probability, programs can construct unambiguous references to most entities in the domain of discourse and provide empirical validation of these results.", "creator": "LaTeX with hyperref package"}}}