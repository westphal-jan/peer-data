{"id": "1511.06428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "A Controller-Recognizer Framework: How necessary is recognition for control?", "abstract": "carington Recently l\u00f6tschberg there popov36 has kirsanov been peterka growing interest jeetan in ioe building active sacer visual object recognizers, 0230gmt as opposed druon to the usual passive 98.73 recognizers voith which 122.22 classifies ccx a given 1.4950 static image into postle a predefined set rogation of object 501c3 categories. lynchpin In provosts this opportunities paper verreaux we rebirthing propose to 11,950 generalize these wenfu recently proposed end - hau to - donghae end c-27j active visual recognizers into a controller - cdta recognizer framework. A mahru model 22:45 in the controller - recognizer ntawukuriryayo framework hkd consists bunma of assurer a wiap controller, which interfaces 1060s with barilius an spineflower external manipulator, monon and a recognizer which paratypes classifies close-knit the visual input oppositional adjusted turtletaub by gherardini the manipulator. We jorgic describe two hanchongryun most bestfares.com recently boreyko proposed controller - recognizer gyre models - - recurrent attention sheikhpura model and sumac spatial conservatively transformer network - - ceeac as corp.-led representative koreshkov examples of controller - recognizer magin models. 20b Based biaxial on chiaravalle this akra description we yoshitada observe hitless that ansgar most nekton existing end - uchino to - m47 end controller - +60 recognizers deas tightly, or petta completely, couple a peterman controller midatlantic and recognizer. lagarto We ask magnetics a question whether this tight yunas coupling is necessary, and try to 107.90 answer spying this ara\u00f1a empirically fantasmas by slow-wave building schwarzenbauer a taraz controller - recognizer model goodykoontz with gnu/linux a dartboards decoupled ablution controller zella and recognizer. measurement Our giovinco experiments revealed that it 13-race is homelink not 33.10 always necessary to lilburne tightly couple ibolya them and that by costco decoupling a s\u00f8r-varanger controller biome and statist recognizer, there is cal\u00f3 a out-group possibility of kung-fu building brahmic a econoline generic joannis controller that anastrozole is e-190 pretrained limerick and siddeeq works together mustain with any ballfields subsequent arrows recognizer.", "histories": [["v1", "Thu, 19 Nov 2015 22:38:53 GMT  (311kb,D)", "https://arxiv.org/abs/1511.06428v1", "ICLR 2016 submission"], ["v2", "Fri, 27 Nov 2015 03:51:33 GMT  (545kb,D)", "http://arxiv.org/abs/1511.06428v2", "ICLR 2016 submission"], ["v3", "Mon, 7 Dec 2015 18:47:15 GMT  (311kb,D)", "http://arxiv.org/abs/1511.06428v3", "ICLR 2016 submission"], ["v4", "Tue, 9 Feb 2016 20:58:21 GMT  (463kb,D)", "http://arxiv.org/abs/1511.06428v4", null]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["marcin moczulski", "kelvin xu", "aaron courville", "kyunghyun cho"], "accepted": false, "id": "1511.06428"}, "pdf": {"name": "1511.06428.pdf", "metadata": {"source": "META", "title": "A Controller-Recognizer Framework:  How necessary is recognition for control?", "authors": ["Marcin Moczulski", "Kelvin Xu", "Aaron Courville", "Kyunghyun Cho"], "emails": ["MARCIN.MOCZULSKI@STCATZ.OX.AC.UK", "KELVIN.XU@UMONTREAL.CA", "AARON.COURVILLE@UMONTREAL.CA", "KYUNGHYUN.CHO@NYU.EDU"], "sections": [{"heading": "1. Introduction", "text": "The success of deep learning, in particular convolutional networks, in computer vision has largely been due to breakthroughs in passive object recognition from a static image (Krizhevsky et al., 2012; LeCun et al., 1998). Most of the successful convolutional networks for object recognition (Szegedy et al., 2014; Simonyan & Zisserman, 2014) are passive in the sense that they recognize an object without having any ability to act on it to improve recognition. Also, they work with static images in the sense that these models lack the ability or mechanism to manipulate an input image themselves.\nIt has been only very recently that these passive neural network recognizers have become more active. This is often done by letting the model actively attend to a sequence of smaller regions of an input image (Mnih et al., 2014; Ba et al., 2014; Denil et al., 2012) or by allowing the model to distort the input image (Jaderberg et al., 2015). In general, these recognizers have become active by allowing them access to a controller which acts upon an external manipulator that either adjusts the recognizer\u2019s view or controls an external mechanism that directly manipulates the environment (which is viewed by the recognizer as an image.)\nIn this paper, we generalize this shift in the paradigm of object recognition with neural networks by defining a controller-recognizer framework. In this framework, a neural network based object recognition system consists of a controller and a recognizer. The recognizer can be any object recognizer that perceives the surrounding environment as a 2-D image. The controller has access to an external mechanism (often a black-box) which can either adjust the surrounding environment directly or a view of the recog-\nar X\niv :1\n51 1.\n06 42\n8v 4\n[ cs\n.L G\n] 9\nnizer. A full controller-recognizer model is defined by the exact specifications of how these recognizer and controller components are coupled with each other.\nWe show that many recently proposed neural network based active object recognizers fall into this controllerrecognizer family. More specifically, we explain in detail the specifications of the recurrent attention model (RAM, Mnih et al., 2014) and the spatial transformer network (STN, Jaderberg et al., 2015) under this controllerrecognizer framework. From this we notice that these existing controller-recognizers tightly, or sometimes completely, couple the controller and recognizer such that the recognizer has deep access to the inner workings of the controller or that the controller relies heavily on the recognizer.\nBased on this observation, in this paper we ask ourselves whether this is the only option in designing an end-toend trainable controller-recognizer model. This question is natural considering the number of potentially undesirable properties of a tightly-coupled pair of controller and recognizer such as a lack of a clear way to use existing well-performing recognizer architectures (i.e. convolutional classifiers) or limited compatibility with external black-box manipulators.\nAs a first stab at answering this question, we design a controller-recognizer model with a decoupled controller/recognizer. In this decoupled model, the controller first manipulates an input image by issuing a sequence of image manipulation commands to an external, nondifferentiable image manipulator. After a fixed number of commands were issued, the resulting image is sent to the recognizer for it to detect an object. In this setting, the internal representations of the controller and recognizer are completely separate.\nWith this decoupled controller-recognizer model, we test a wide variety of training strategies to empirically confirm (1) the possibility of training a decoupled model jointly and (2) the possibility of having a general, pretrained controller for a subsequent recognition task with potential mismatch between the data used to train the controller and a recognizer. Furthermore, we aim to show that the existing benchmark task of recognizing a randomly placed handwritten digit in a large canvas (potentially with clutter) can in fact be solved by this decoupled model at a level comparable to a tightly coupled model.\nOur experimental results show that a decoupled controllerrecognizer model achieves a level of performance comparable with a tightly coupled model and performs well in transfer settings. This opens the potential of having a model with a single, generic controller manipulating the environment to maximize the performance of multiple recognizers."}, {"heading": "2. Controller-Recognizer Framework", "text": "In this paper, we are interested in models that exploit the ability of control in order to recognize an object based on vision. These models can be described as consisting of a controller and a recognizer. In general, a controller of this type of models manipulates either the external environment or the model itself to adjust the model\u2019s view. This adjusted view of the external environment is used by a recognizer, and therefore the controller\u2019s objective is to adjust the view so as to maximize the recognizer\u2019s performance. See Fig. 1 for a graphical illustration.\nThis is in contrast to existing supervised object recognition models such as the widely used convolutional neural network (see, e.g., LeCun et al., 1998; Krizhevsky et al., 2012). This approach to vision-based object recognition using neural networks is static approach in the sense that the model does have any means of influencing the environment. This means that the model has to work with whatever input, but has no control over how it can be manipulated in order to maximize recognition rate."}, {"heading": "2.1. Criteria for Categorizing Controller-Recognizer Models", "text": "Our controller-recognizer framework includes a broad family of models. Among these we are interested in fullytrainable end-to-end models, often implemented as a deep neural network. Although we focus on a subset of models, comprised of end-to-end trainable deep neural networks, there are many possible variants, either already proposed or not, and in this section, we try to describe how they can be categorized.\nFirst, we can classify each of these controller-recognizer models based on the type of manipulator used by the controller. A controller may manipulate the model itself in or-\nder to move its gaze over a static input.1 In recent literature, this is often referred to as an attention mechanism (see, e.g., Denil et al., 2012; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014). On the other hand, a controller may also have access to an external, often black-box, mechanism, and this external black-box manipulates the input actively based upon the commands issued by the controller.\nSecond, a controller-recognizer model can be classified based upon the training objective(s) of the controller. The ultimate goal of the controller is eventually to maximize the recognition rate by the recognizer, but this does not necessarily imply that this is the only training objective available. For instance, a controller may be trained jointly to focus on an object of interest as well as to explore the input scene (i.e., maximize the model\u2019s coverage over the input scene) in order to detect the existence of an object which will ultimately be recognized. In this case, the training objectives for the controller are (1) to maximize the recognition rate and (2) to maximize the exploration.\nAnother closely-related criterion is the level of generality of the controller. By the generality of the controller, we mean specifically whether a given controller can be used for multiple recognition tasks. As evident in animals, a single controller can be utilized for multiple downstream recognition tasks (visual recognition, speech recognition, haptic perception etc.) One can use the controller to bring an object in interest to the center of view to better recognize, or at the same time use it to remove distractions in the scene (i.e., denoising.)\nYet another criterion is specific to neural network based controller-recognizer models. Regardless of its end goal, a deep neural network automatically extracts a continuous vector representation of the input. A neural controller will have a continuous vector representation of the original input, adjusted input (by itself) and potentially a sequence of control commands. This representation may be used by a recognizer, rather than having the recognizer work directly on the adjusted input returned as a result of the controller.\nThis criterion reflects the strength of coupling between the controller and the recognizer, and is closely related to the generality of the controller. Stronger coupling implies that the controller\u2019s behaviour as well as its internal representation are highly customized for a subsequent recognizer, leading to less generality of the controller. On the other hand, when the controller and recognizer are weakly coupled, the generality of the controller increases and may be more suitable to be used with multiple training objectives and recognition models. Therefore, we consider the generality of the controller as a sub-criterion of the coupling\n1 By static input we mean a situation when a model does not actively, directly manipulate the observed environment.\nstrength.\nThe coupling strength also has consequences on the training strategy. If the controller and recognizer are strongly coupled, it is quite likely that they will have to be trained simultaneously. This is not necessary true if the coupling strength is weak. In this case, one can think of bootstrapping, or pretraining, the controller with another training objective, which is useful for a wide set of potential downstream recognition tasks, before coupling this pretrained controller with other recognizers.\nWe summarize this criteria here as a list, and next describe representative examples under this framework:\n1. Manipulator: attention mechanism, external blackbox, internal white-box, etc.\n2. Training objectives: final recognition rate, exploration rate, etc.\n3. Strength of coupling: how tightly a controller and recognizer are coupled\n(a) Generality of controller: single recognition task vs. multiple recognition tasks\n(b) Training strategy: sequential vs. simultaneous"}, {"heading": "2.2. Example 1: Recurrent Attention Model", "text": "A recurrent attention model (RAM) is a representative example of controller-recognizer models, recently proposed by Mnih et al. (2014). RAM was designed to work on a large image efficiently by controlling the model\u2019s gaze of a small view area over the input image.\nAt each time step, RAM receives as input a subset of the whole input image from the gaze\u2019s location determined at the previous step. This subset is used to update the hidden state (continuous vector representation of the input.) Based on this updated state RAM computes the next location of the gaze, which is equivalent to adjusting itself to move its gaze to the next location. RAM also predicts when to stop and finally what is the object type.\nWe analyze this model according to the criteria we have defined earlier. See Fig. 2 as a reference.\nThe manipulator used by the RAM is an attention mechanism. Mnih et al. (2014) however also showed that it is indeed possible to use the RAM for playing a game, meaning that the RAM is able to interact with the external black-box to maximise the final objective.\nBoth the controller and the recognizer in RAM are tuned to maximize the final recognition rate (or the reward from the game) which is the only training objective.\nThe controller introduced as a part of the RAM is tightly coupled with the recognizer by being a part of one recur-\nrent neural network. The controller and recognizer share the same set of parameters and the internal hidden state, meaning that it is not possible to use the controller on another task once it is trained together with the existing recognizer. This makes it difficult to reuse the pretrained controller for another downstream recognition task, unless all of them are trained simultaneously (i.e., multitask learning, Caruana, 1997; Collobert et al., 2011)."}, {"heading": "2.3. Example 2: Spatial Transformer Networks", "text": "More recently, Jaderberg et al. (2015) proposed to modify a convolutional neural network, which is a recognition only model, to include a controller. The overall network is called a spatial transformer network (STN).\nThe STN employs a differentiable spatial transformer as a manipulator. The spatial transformer is able to warp an input image based on transformation parameters computed by a localisation network embedded inside a convolutional neural network. The biggest advantage of having this differentiable spatial transformer is that one can take the derivative of the final recognition rate with respect to the transformation performed based on the transformation parameters, which enables the use of backpropagation to compute a low-variance gradient estimate.\nSimilarly to what we have done with the RAM, let us analyse the STN according to the criteria for controllerrecognizer models. See Fig. 3 as a reference.\nFirst, the target of the STN\u2019s controller is the internal, transparent manipulator\u2013the spatial transformer (ST). The ST works on either the raw input image or the intermediate feature maps from the convolutional neural network. This manipulator, which works directly on the input image, is more flexible in transforming the input than the attention\nmechanism of the RAM.\nSecond, one important characteristics of the STN is that both the controller\u2013localisation network\u2013 and its target manipulator\u2013spatial transformer\u2013 are all tuned to maximise the final recognition rate.\nAs it is quite clear from its description, the controller and recognizer in the STN are completely coupled. The controller works directly on the internal continuous vector representation of the recognizer, and the recognizer\u2019s hidden states are used as an input to the controller. The controller, recognizer as well as the manipulator must be trained simultaneously."}, {"heading": "3. Is It Necessary to Tightly Couple Controller and Recognizer?", "text": "We noticed that both the recurrent attention model (RAM) and spatial transformer network (STN) tightly, or completely, couple the controller and recognizer. This is also observed in most of the recently proposed controllerrecognizer models such as the Fixation NADE by Zheng et al. (2014).\nThere are a number of implications from this tight coupling of the controller and recognizer.\nThis tight coupling, especially the complete coupling such as in the spatial transformer network, implies that they need to be trained simultaneously. This simultaneous training naturally and obviously makes the controller specialized for the recognition tasks used during training. Consequently, it is unclear whether the trained controller will be any useful for other subsequent recognition tasks that\narise after the original controller-recognizer is trained. In other words, if there is another recognition task that may benefit from having a controller, the whole new controllerrecognizer will have to be trained from scratch.\nA further consequence of this reliance on a single objective of recognition rate is that the controller of a controllerrecognizer model can only be trained in a supervised manner. This is unsatisfactory as the role of the controller is often to bring an object to the center of the recognizer\u2019s view, which is substantially a weaker, or easier, objective than the full recognition.\nThis reliance on the simultaneous training of the controller and recognizer is quite contrary to what is observed in infant development. Infants are known to exhibit visual attention already in the first few weeks after their birth (Chapter 3 of Ruff & Rothbart, 2001). This happens without any strong external reward, which is in the case of controllerrecognizer framework a recognition rate, implying that the controller, in this case an attention mechanism similar to the one from the RAM, is being trained/tuned on its own. This serves as an existence proof of the possibility of training a controller separately from a recognizer also in this controller-recognizer framework.\nEarlier in 1991, Schmidhuber & Huber (1991) proposed a very specific approach to training a controller on its own without a subsequent recognizer, as in Fig. 4 (a). Similarly to the RAM discussed earlier in Sec. 2.2, the controller in their case is an attention mechanism implemented as a recurrent neural network, but without any recognizer. Their goal was to show that it is possible to train a controller\u2013 attention mechanism without explicit supervision on the types of objects the controller is following. They achieved this by training the controller with a reward given only when the controller managed to move its attention to a part of the input image that contains an object.\nThe significance of the work by Schmidhuber & Huber\n(1991) is that the controller pretrained in their method can be used later with a separate recognizer that takes as input only a small subset of the input image selected by this controller. There are two advantages in this procedure. First, the recognizer can be made substantially simpler as it does not need to be invariant to translation or rotation, as this is handled by the pretrained controller. Second, as the recognizer takes as input only a small subset of the input image, computational efficiency of the recognizer greatly increases.\nThese observations on infant development and the earlier work by Schmidhuber & Huber (1991) raise a question on the necessity of strongly coupling a controller and a recognizer in the controller-recognizer framework. The success of those recent controller-recognizer models, such as the recurrent attention model and spatial transformer network, does not answer this question. This question naturally leads us to ask what other competitive variants within general controller-recognizer framework, according to the criteria outlines in Sec. 2.1, are possible."}, {"heading": "4. Decoupled Controller and Recognizer", "text": "In this paper, we aim at answering the questions posed in the previous section. Among them, the main question is the possibility and extent of building a controller-recognizer model with decoupled controller and recognizer. First, let us describe what we mean by \u201cdecoupled\u201d.\nA recognizer decoupled from a controller takes as input an image manipulated by the controller only. In other words, the recognizer does not have access to the internal state of the controller. One obvious consequence of this is that a recognizer can be trained on its own regardless of the state of the controller, although the ability of the controller in manipulating an input image will significantly influence the final recognition quality. Similarly, a controller decoupled from a recognizer works independently from the recognizer. See from Fig. 4 (b) that there is no direct path between the controller and recognizer."}, {"heading": "4.1. Model Description", "text": "Input Canvas The world is a large w \u00d7 h canvas X . On the canvas, a number of objects, including the target object, are placed. The controller and recognizer have their own window of view into the world. The recognizer always observes the canvas through the center window of size wr \u00d7 hr, while the controller has two possible views. Similarly to the recognizer, the controller may observe the canvas through the center window of size wc\u00d7hc (cropped view), or the controller may view the whole canvas but in a lower resolution of wc \u00d7 hc (subsampled view.) Additionally, we test the case where the controller is allowed the full\nview of the canvas (full view.)\nManipulator In this paper, we use an external black-box manipulator M which permits a set of possible actions{ a1, a2, . . . , aNa } and a gain knob p which decides on the degree to which a selected action is performed. The manipulator we used is implemented using an imaging library with the following actions; (1) shift up, (2) shift down, (3) shift right, and (4) shift left. For these actions, the gain knob p \u2208 [0, 1] corresponds to the percentage of a prespecified maximum shift rounded to the nearest integer. Additionally, the manipulator may decide not to act on the input canvas by issuing a no-action.\nController The controller is implemented as a recurrent neural network. At each time step, the controller looks at the current configuration of the input canvas and updates its internal hidden state:\nht = \u03c6 (ht\u22121,xt) ,\nwhere \u03c6 is a recurrent activation function such as long short-term memory units (LSTM, Hochreiter & Schmidhuber, 1997) and gated recurrent units (GRU, Cho et al., 2014). In our experiments, we use 40 GRU\u2019s to implement \u03c6.\nThe internal hidden state is initialized as a function of the initial input canvas x0:\nh0 = finit(x0),\nwhich is a small multilayer perceptron.\nGiven a new hidden state ht, the controller computes the action distribution by\np(at = a j |x<t) =\nexp ( w>ajht )\u2211Na j\u2032=1 exp ( w>ajht\n) , where waj is a parameter vector for the j-th action. The controller then either samples, or selects the most likely, action a\u0303t from this distribution.\nSimilarly, the controller computes the gain distribution pt|x<t \u223c N ( m(ht), s(ht) 2 ) ,\nwhere N (m, s2) is a normal distribution with mean m and variance s2, and\nm(ht) = \u03c3(w > mht), s(ht) = \u03c3(w > s ht)\nWe choose the gain p\u03030t to be either a sample or the mean of this distribution. The selected gain is further processed to lie between 0 and 1:\np\u0303t = \u03c3(5.5 \u00b7 (p\u03030t \u2212 0.5)).\nThe selected action and associated gain are fed into the manipulator. The manipulator adjusts the input canvas X accordingly to result in the next time step\u2019s view xt. We let the controller manipulate the input canvas for at most T steps, and we write by\nxT = fcont(x0, a\u03031:T , p\u03031:T ) (1)\nto represent this whole process. Note that the manipulator M is included in fcont, and that a\u03031:T and p\u03031:T are the sampled action and gain variables.\nRecognizer As the controller is completely decoupled from a recognizer, we can use any existing off-the-shelf image recognizer. More specifically, we use a generic convolutional neural network with the configuration presented in Table 1. The recognizer returns a class-conditional probability distribution over the labels p(y = k|xT ), where xT is the final canvas configuration from Eq. (1)."}, {"heading": "4.2. Training Strategies", "text": ""}, {"heading": "4.2.1. JOINT STRATEGY", "text": "Although the controller and recognizer are decoupled, we can jointly train them to maximize\nCjoint = 1\nN N\u2211 n=1 cjoint(x n 0 , y n),\nwhere\ncjoint(x0, y) = \u2212 log \u2211\na1:T ,p1:T\np(y|fcont(x0, a1:T , p1:T )).\n(xn0 , y n) is the n-th example from a training set Djoint.\nWe use stochastic gradient descent (SGD) algorithm to update the parameters of both the controller and recognizer,\nwhere the gradient\u2207l is approximated by\n\u2207cjoint \u2248\u2212 1\nM M\u2211 m=1 \u2207 log p(y|fcont(x0, am1:T , pm1:T ))\n+ log p(y|fcont(x0, am1:T , pm1:T ) ( T\u2211\nt=1\n\u2207 log p(at = amt |x<t)\n+\u2207 log p(pt = pmt |x<t) ) ,\nwhere am1:T and p m 1:T are the action and gain variables sampled at the m-th trial. In our experiment, we set M = 1, meaning that we run the controller only once for each training example. This approximation is necessary, as the manipulatorM is non-differentiable.\nThis approximation to the gradient is known as REINFORCE (Williams, 1992). We use REINFORCE to jointly train the decoupled controller-recognizer model, augmented by the variance reduction techniques proposed by Mnih & Gregor (2014).\nWe call this training approach the joint strategy."}, {"heading": "4.2.2. DECOUPLED STRATEGY", "text": "Because the controller is separate from the recognizer, we can pretrain it in advance of training the recognizer. We use the objective proposed by Schmidhuber & Huber (1991), where the controller\u2019s goal is to bring an object in interest (which is known to the trainer) to the center of visual perception which is in our case the center of the canvas. We define the cost of each trial (i.e., running the controller for a single example) as\ncpre(x0,m) = 1\u2212 cosine(fcont(x0, a\u03031:T , p\u03031:T ),m),\nwhere m is a mask vector corresponding to an input canvas with itsw\u2032\u00d7h\u2032 center window set to 1 and 0 otherwise, and\ncosine(a,b) = a>b\n\u2016a\u2016\u2016b\u2016 .\nSimilarly to joint training, we can minimize this pretraining cost function by REINFORCE. Note that this pretraining does not require any labelled example and can be done purely in a unsupervised manner. We denote by the training set used to pretrain the controller as Dpre .\nTraining Recognizer Once the controller is pretrained, we freeze it and train the recognizer. This is done, for each training example, by running the controller (and manipulator) on the input canvas, feeding in the center window to the recognizer, computing the gradient of the recognition cost, to which we refer as cpost, w.r.t. the recognizer\u2019s parameters and updating them accordingly:\n\u2207cpost = \u2207 log p(y|fcont(x0, a\u03031:T , p\u03031:T )).\nWe use Dpost to refer to the training set used for tuning the recognizer. Because the recognizer is trained separately from the controller, we may either use the same training set, i.e., Dpost = Dpre, or a different set, i.e., Dpost 6= Dpre.\nWe call this training approach a decoupled strategy. See Fig. 5 (a)\u2013(c) for graphical illustrations of these training strategies.\nIn addition, we also test a strategy where the decoupled strategy is followed by finetuning the controller toward minimizing Cpost, which we call the decoupled+finetune strategy."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Dataset", "text": "We evaluate the proposed decoupled controller-recognizer model on the classification task using the cluttered and translated MNIST (CT-MNIST), closely following (Mnih et al., 2014) where the recurrent attention model (RAM, see Sec. 2.2) was proposed.\nEach example in the CT-MNIST consists of a 60\u00d7 60 canvas on which a target handwritten digit together with multiple partial digits are randomly scattered. The controller sees and manipulates the canvas by issuing commands to the image library based manipulator. When the controller is done, the recognizer looks at the 28\u00d7 28 center window of the final canvas and outputs the label distribution.\nWe build the following subsets of the CT-MNIST for the decoupled training strategies:\n1. CT-MNIST-Full: CT-MNIST as it is 2. CT-MNIST-Thin: Digits with labels {0, 1, 2, 3, 9}\n3. CT-MNIST-Thick: Digits with labels {4, 5, 6, 7, 8} 4. CT-MNIST-Natural-X: CT-MNIST with natural im-\nage background2 of opacity set to X (see Fig. 6)\nThese datasets are used with the decoupled training strategy. By having Dpre 6= Dpost, we test the generality of the pretrained controller."}, {"heading": "5.2. Results and Analysis", "text": "In Table 2, we report the recognition error rate on the test set for each combination. We use the alphabet index to refer to a specific row in the table.\n2 We use the Berkeley Segmentation Dataset (Martin et al., 2001).\nHow well does the decoupled model do? From (a) and (o), we see that the proposed decoupled controllerrecognizer model, when jointly trained, works as well as the more tightly coupled controller-recognizer model (RAM). However, we notice that when the controller and recognizer are separately trained (row (b)), the performance slightly degrades, but this is overcome by finetuning the controller subsequently (row (c)). See Fig. 7 for an example of the controller moving a digit to the center window. This confirms that it is indeed possible to decouple the controller and recognizer in the controller-recognizer framework.\nOne potential criticism of the settings (a\u2013c) is that the controller has the full view of the canvas unlike the RAM of which controller has only a partial view of the canvas, via attention mechanism, at a time. In the rows (e\u2013g) and (i\u2013k), we present the results using the controller that has a more restricted view of the canvas. In both cases, we see that the decoupled model works as well as, or sometimes better than, the RAM, further supporting the decoupled model as a viable model in the controller-recognizer framework.\nHow transferable is the pretrained controller? First, let us consider rows (d), (h) and (l). In these cases, the controller was trained on CT-MNIST-Thin but was used for CT-MNIST-Thick. From the low error rates, it is clear that the controller is able to easily manipulate the digits that were not seen before for recognition. We further observed qualitatively that this is indeed the case. This weakly supports that a full-extent, fine-grained recognition is not necessary nor useful for control.\nFinally, from the rows (m\u2013n) we see that the controller can\nmanipulate the canvas even when its background is covered with natural images which the controller has never been exposed to before. As expected, the controller\u2019s ability to manipulate degrades as the natural image background becomes brighter, i.e., higher opacity, but despite the visible differences, recognition performance degrades gracefully."}, {"heading": "6. Conclusion", "text": "The main contribution of this paper is the introduction of a controller-recognizer framework under which many recently proposed active recognizers, such as recurrent attention model (RAM) and spatial transformer network can be studied and analyzed. This framework allows us to view the active recognizer as a composite of two separate modules, controller and recognizer, and by doing so, gives us a systematic way to build a novel controller-recognizer model and evaluate it.\nAs an example, we proposed a decoupled controllerrecognizer model, which separates the controller and recognizer. This decoupling allows us to devise a diverse set of learning and inference scenarios, such as pretraining a controller on one data set and using it together with a recognizer on another data set (transfer setting.) Our empirical evaluation confirms that the proposed decoupled model indeed works well for most of these scenarios. These experiments opens a door to a possibility of having a single, generic controller is weakly coupled with a variety of subsequent recognizers."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank the following for research funding and computing support: NSERC, FRQNT, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung. Kyunghyun Cho and Kelvin Xu thank Facebook for their generous support. The authors would also like to thank C\u0327ag\u0306lar Gu\u0308lc\u0327ehre and Jamie Kiros for interesting discussions during the course of this work."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Multitask learning", "author": ["Caruana", "Rich"], "venue": "Machine learning,", "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural Computation,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1506.02025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In Proc. 8th Int\u2019l Conf. Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Attention in early development: Themes and variations", "author": ["Ruff", "Holly Alliger", "Rothbart", "Mary Klevjord"], "venue": null, "citeRegEx": "Ruff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ruff et al\\.", "year": 2001}, {"title": "Learning to generate artificial fovea trajectories for target detection", "author": ["Schmidhuber", "Juergen", "Huber", "Rudolf"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Schmidhuber et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 1991}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Zheng", "Yin", "Zemel", "Richard S", "Zhang", "Yu-Jin", "Larochelle", "Hugo"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "We describe two recently proposed controllerrecognizer models\u2013 the recurrent attention model (Mnih et al., 2014) and spatial transformer network (Jaderberg et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 6, "context": ", 2014) and spatial transformer network (Jaderberg et al., 2015)\u2013 as representative examples of controller-recognizer models.", "startOffset": 40, "endOffset": 64}, {"referenceID": 7, "context": "The success of deep learning, in particular convolutional networks, in computer vision has largely been due to breakthroughs in passive object recognition from a static image (Krizhevsky et al., 2012; LeCun et al., 1998).", "startOffset": 175, "endOffset": 220}, {"referenceID": 8, "context": "The success of deep learning, in particular convolutional networks, in computer vision has largely been due to breakthroughs in passive object recognition from a static image (Krizhevsky et al., 2012; LeCun et al., 1998).", "startOffset": 175, "endOffset": 220}, {"referenceID": 10, "context": "This is often done by letting the model actively attend to a sequence of smaller regions of an input image (Mnih et al., 2014; Ba et al., 2014; Denil et al., 2012) or by allowing the model to distort the input image (Jaderberg et al.", "startOffset": 107, "endOffset": 163}, {"referenceID": 0, "context": "This is often done by letting the model actively attend to a sequence of smaller regions of an input image (Mnih et al., 2014; Ba et al., 2014; Denil et al., 2012) or by allowing the model to distort the input image (Jaderberg et al.", "startOffset": 107, "endOffset": 163}, {"referenceID": 4, "context": "This is often done by letting the model actively attend to a sequence of smaller regions of an input image (Mnih et al., 2014; Ba et al., 2014; Denil et al., 2012) or by allowing the model to distort the input image (Jaderberg et al.", "startOffset": 107, "endOffset": 163}, {"referenceID": 6, "context": ", 2012) or by allowing the model to distort the input image (Jaderberg et al., 2015).", "startOffset": 60, "endOffset": 84}, {"referenceID": 7, "context": "This is in contrast to existing supervised object recognition models such as the widely used convolutional neural network (see, e.g., LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 122, "endOffset": 178}, {"referenceID": 16, "context": "1 In recent literature, this is often referred to as an attention mechanism (see, e.g., Denil et al., 2012; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014).", "startOffset": 76, "endOffset": 163}, {"referenceID": 10, "context": "1 In recent literature, this is often referred to as an attention mechanism (see, e.g., Denil et al., 2012; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014).", "startOffset": 76, "endOffset": 163}, {"referenceID": 0, "context": "1 In recent literature, this is often referred to as an attention mechanism (see, e.g., Denil et al., 2012; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014).", "startOffset": 76, "endOffset": 163}, {"referenceID": 10, "context": "A recurrent attention model (RAM) is a representative example of controller-recognizer models, recently proposed by Mnih et al. (2014). RAM was designed to work on a large image efficiently by controlling the model\u2019s gaze of a small view area over the input image.", "startOffset": 116, "endOffset": 135}, {"referenceID": 10, "context": "Mnih et al. (2014) however also showed that it is indeed possible to use the RAM for playing a game, meaning that the RAM is able to interact with the external black-box to maximise the final objective.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "Graphical illustration of a recurrent attention model by (Mnih et al., 2014).", "startOffset": 57, "endOffset": 76}, {"referenceID": 3, "context": "This makes it difficult to reuse the pretrained controller for another downstream recognition task, unless all of them are trained simultaneously (i.e., multitask learning, Caruana, 1997; Collobert et al., 2011).", "startOffset": 146, "endOffset": 211}, {"referenceID": 6, "context": "More recently, Jaderberg et al. (2015) proposed to modify a convolutional neural network, which is a recognition only model, to include a controller.", "startOffset": 15, "endOffset": 39}, {"referenceID": 6, "context": "Graphical illustration of a spatial transformer network by (Jaderberg et al., 2015).", "startOffset": 59, "endOffset": 83}, {"referenceID": 16, "context": "This is also observed in most of the recently proposed controllerrecognizer models such as the Fixation NADE by Zheng et al. (2014).", "startOffset": 112, "endOffset": 132}, {"referenceID": 10, "context": "We evaluate the proposed decoupled controller-recognizer model on the classification task using the cluttered and translated MNIST (CT-MNIST), closely following (Mnih et al., 2014) where the recurrent attention model (RAM, see Sec.", "startOffset": 161, "endOffset": 180}, {"referenceID": 10, "context": "( ) The best performance reported by Mnih et al. (2014). \u2020 Should be used for comparison primarily for CT-MNIST-Full experiments.", "startOffset": 37, "endOffset": 56}, {"referenceID": 9, "context": "2 We use the Berkeley Segmentation Dataset (Martin et al., 2001).", "startOffset": 43, "endOffset": 64}], "year": 2016, "abstractText": "Recently there has been growing interest in building \u201cactive\u201d visual object recognizers, as opposed to \u201cpassive\u201d recognizers which classifies a given static image into a predefined set of object categories. In this paper we propose to generalize recent end-to-end active visual recognizers into a controller-recognizer framework. In this framework, the interfaces with an external manipulator, while the recognizer classifies the visual input adjusted by the manipulator. We describe two recently proposed controllerrecognizer models\u2013 the recurrent attention model (Mnih et al., 2014) and spatial transformer network (Jaderberg et al., 2015)\u2013 as representative examples of controller-recognizer models. Based on this description we observe that most existing end-to-end controller-recognizers tightly couple the controller and recognizer. We consider whether this tight coupling is necessary, and try to answer this empirically by investigating a decoupled controller and recognizer. Our experiments revealed that it is not always necessary to tightly couple them, and that by decoupling the controller and recognizer, there is a possibility to build a generic controller that is pretrained and works together with any subsequent recognizer.", "creator": "LaTeX with hyperref package"}}}