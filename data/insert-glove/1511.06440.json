{"id": "1511.06440", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Towards Principled Unsupervised Learning", "abstract": "benjarvus General trifoliata unsupervised recognition learning dyadic is nicelli a u-14 long - standing conceptual arnould problem half-timbered in machine learning. fantoni Supervised d'\u00eatre learning is burgage successful imac because thrum it buckovski can be solved throat by khuang the minimization of the mondex training error cost conceptualist function. Unsupervised learning is not as successful, because peapod the markville unsupervised rausch objective 6.5-foot may be fazzani unrelated snz to the supervised task of interest. For an most-viewed example, l'carriere density modelling darkon and spcs reconstruction rivier have gaikwad often been 16-yards used for falstein unsupervised gribbin learning, logans but they shatterproof did redistributes not kth produced banyoles the sophronius sought - hinemoa after performance franny gains, 315,000 because they have no knowledge few of echl the wachs sought - after sabar supervised stapley tasks.", "histories": [["v1", "Thu, 19 Nov 2015 23:04:23 GMT  (487kb,D)", "http://arxiv.org/abs/1511.06440v1", null], ["v2", "Thu, 3 Dec 2015 17:24:22 GMT  (489kb,D)", "http://arxiv.org/abs/1511.06440v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilya sutskever", "rafal jozefowicz", "karol gregor", "danilo rezende", "tim lillicrap", "oriol vinyals"], "accepted": false, "id": "1511.06440"}, "pdf": {"name": "1511.06440.pdf", "metadata": {"source": "CRF", "title": "TOWARDS PRINCIPLED UNSUPERVISED LEARNING", "authors": ["Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor", "Danilo Rezende", "Tim Lillicrap", "Oriol Vinyals"], "emails": ["ilyasu@google.com", "rafalj@google.com", "karolg@google.com", "danilor@google.com", "countzero@google.com", "vinyals@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Supervised learning is successful for two reasons: it is equivalent to the minimization of the training error, and stochastic gradient descent (SGD) is highly effective at minimizing training error. As a result, supervised learning is robust, reliable, and highly successful in practical applications.\nUnsupervised learning is not as successful, mainly because it is not clear what the unsupervised cost function should be. The goal of unsupervised learning is often to improve the performance of a supervised learning task for which we do not have a lot of data. Due to the lack of labelled examples, unsupervised cost functions do not know which of the many possible supervised tasks we care about. As a result, it is difficult for the unsupervised cost function to improve the performance of the supervised cost function. The disappointing empirical performance of unsupervised learning supports this view.\nIn this paper, we present a cost function that generalizes the ideas of Casey (1986). We illustrate the idea in the setting of speech recognition. It is possible to evaluate the quality of a speech recognition system by measuring the linguistic plausibility of its typical outputs, without knowing whether these outputs are correct for their inputs. Thus, we can measure the performance of our system without the use of any input-output examples.\nWe formalize and generalize this idea as follows: in conventional supervised learning, we are trying to find an unknown function F from X to Y . Each training case (xi, yi) imposes a soft constraint on F : F (xi) = yi (1) We solve these equations by local optimization with the backpropagation algorithm (Rumelhart et al., 1986).\nar X\niv :1\n51 1.\n06 44\n0v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nWe now present the unsupervised cost function. Let D be the true data distribution over the inputoutput pairs (x, y) \u223c D. While we do not have access to many labelled samples (x, y) \u223c D, we often have access to large quantities of unlabelled samples from x \u223c D and y \u223c D. This assumption is valid whenever unlabelled inputs and unlabelled outputs are abundant.\nWe can use uncorrelated samples from x \u223c D and y \u223c D to impose a valid constraint on F :\nDistr [F (x)] = Distr [y] (2)\nwhere Distr [z] denotes the distribution of the random variable z. This constraint is valid in the following sense: if we have a function F such that F (xi) = yi is true for every possible training case, then Distr [F (x)] = Distr [y] is satisfied as well. Thus, the unsupervised constraint can be seen as an additional labelled \u201ctraining case\u201d that conveys a lot of information whenever the output space is very large.\nThis constraint can be turned into the following cost function\nKL[Distr [y] \u2016Distr [F (x)]] (3)\nWe call it the output distribution matching (ODM) cost function, the cost literally matches the distributions of the outputs.\nFor the constraint Distr [F (x)] = Distr [y] to be highly informative of the optimal parameters, it is necessary for the output space Y to be large, since otherwise the ODM cost will be trivial to optimize. If the output space is large, then the ODM cost is, in principle, substantially more useful than conventional unsupervised cost functions which are unrelated to the ultimate supervised cost function. In contrast, the ODM cost is nearly guaranteed to improve the final supervised performance whenever there exists a very high performing function, because a supervised function that perfectly maps every input to its desired output is also a global minimum of the ODM cost. See Figure 1. It also means that a practitioner has a high chance of improving their supervised performance if they are able to optimize the ODM cost.\nWe also show how the ODM cost can be used for one-shot domain adaptation. Given a test datapoint from a distribution that is very different from the training one, our model can, in certain settings, identify a nearly unique corresponding datapoint that happens to obey the training data distribution, which can then be classified correctly. We implement this idea using a generative model as a mechanism for aligning two distributions without supervision.\nThis allows each new test case to be from significantly different distribution, so it is no longer necessary to assume that the test set follows a particular distribution. This capability allows our models, in principle, to work robustly with unexpected data. However, the ability to perform oneshot domain adaptation is not universal, and it can be achieved only in certain restricted settings."}, {"heading": "2 RELATED WORK", "text": "There has been a number of publications that considered matching statistics as an unsupervised learning signal. The early work of Casey (1986) casts the problem of unsupervised OCR as a problem of decoding a cipher, where there is an unknown mapping from images to characters identities, which must be inferred from the known statistics of language. This idea has been further explored in the context of machine translation (typically in the form of matching bigrams) (Knight et al., 2006; Huang et al., 2006; Snyder et al., 2010; Ravi & Knight, 2008), and in unsupervised lexicon induction (Fung & McKeown, 1997; Koehn & Knight, 2002; Haghighi et al., 2008; Mikolov et al., 2013b). Notably, Knight et al. (2006) discusses the connection between unsupervised learning and language decipherment, and formulates a generative model similar to the one presented in this work.\nSimilar ideas have recently been proposed for domain adaptation where a model learns to map the new distribution back onto the training distribution (Tzeng et al., 2014; Gani et al., 2015). These approaches are closely related to the ODM cost, since they are concerned with transforming the new distribution back to the training distribution.\nThere has been a lot of other work on unsupervised learning with neural networks, which is largely concerned with the concept of \u201cpre-training\u201d. In pre-training, we first train the model with an unsupervised cost function, and finish training the model with the supervised cost. This concept was introduced by Hinton et al. (2006) and Hinton & Salakhutdinov (2006) and later by Bengio et al. (2007). Other work used the k-means objective for pre-training (Coates et al., 2011), and this list of references is far from exhaustive.\nMore recent examples of unsupervised pre-training are the Skip-gram model (Mikolov et al., 2013a) and its generalization to sentences, the Skip-thought vectors model of Kiros et al. (2015). These models use well-motivated unsupervised objective functions that appear to be genuinely useful for a wide variety of language-processing tasks."}, {"heading": "3 METHODS", "text": "In this section, we present several approaches for optimizing the ODM cost."}, {"heading": "3.1 ODM COSTS AS GENERATIVE MODELS", "text": "The ODM cost can be formulated as the following generative model. Let P (x) be a model fitted to the marginal x \u223c D, and let P\u03b8(y) be the distribution:\nP\u03b8(y) = \u2211 x P\u03b8(y|x)P (x) (4)\nThe objective is to find a conditional P\u03b8(y|x) (which corresponds to F\u03b8(x) where \u03b8 are the parameters) so that P\u03b8(y) matches the marginal distribution y \u223c D. If P (x) is an excellent model of x \u223c D, then the cost Ey\u223cD[\u2212 logP\u03b8(y)] is precisely equivalent to the ODM cost of Eq. 3, modulo an additive constant. A similar generative model was presented by Knight et al. (2006).\nIt is desirable to train generative models using the variational autoencoder (VAE) of Kingma & Welling (2013). However, VAE training forces y to be continuous, which is undesirable since many domains of interest are discrete. We address this by proposing the following generative model, which we term the xyh-model:\nPx(x) = \u222b h Px(x|h)P (h)dh (5)\nPx(y) = \u222b h Py(y|h)P (h)dh (6)\nwhose cost function is\nL = Ex\u223cD[\u2212 logPx(x)] + Ey\u223cD[\u2212 logPy(y)] (7)\nHere h is continuous while x and y are either continuous or discrete. It is clear that this model can also be trained on labelled (x, y), whenever such data is available.\nAlthough the negative log probability of the xyh-model is not identical to the ODM cost, the two are closely related. Specifically, whenever the capacity of Px and Py is limited, the xyh-model will be forced to represent the structure that is common to Distr [x] and Distr [y] in P (h), while placing the domain specific structures into Px(x|y) and Py(x|y). For example, in case of speech recognition, P (h) could contain the language model, since it is not economical to store the language model in Px(x|h) and Py(y|h)."}, {"heading": "3.2 THE DUAL AUTOENCODER", "text": "We implemented the xyh-generative model but had difficulty getting sensible results in our early experiments. We were able to get better results by designing an novel autoencoder model that is inspired by the xyh-model, which we call the dual autoencoder, which is shown in Figure 2. It consists of two autoencoders whose \u201cinnermost weights\u201d are shared with each other. More formally, the dual autoencoder has the form\nx\u2032 = f(A0f(WNf(WN\u22121 . . . f(W1f(B0x)) . . .))) (8) y\u2032 = f(A1f(WNf(WN\u22121 . . . f(W1f(B1y)) . . .))) (9)\nwhere f is the nonlinearity, and the cost is\nL = Ex\u223cD [L1(x, x \u2032)] + Ey\u223cD [L2(y, y \u2032)] (10)\nwhere L1 and L2 are appropriate loss functions.\nEqs. 8 and 9 describe two autoencoders that map x to x\u2032 and y to y\u2032, respectively. By sharing the weightsW1 . . . ,WN between the autoencoders, the matrices (A0, B0) and (A1, B1) are encouraged to use compatible representations for the two modalities that align x with y in the absence of a direct supervised signal. While not principled, we found this approach to work surprisingly well on simple problems."}, {"heading": "3.3 GENERATIVE ADVERSARIAL NETWORKS TRAINING", "text": "The Generative Adversarial Network (Goodfellow et al., 2014) is a procedure for training a \u201cgenerator\u201d to produce samples that are statistically indistinguishable from a desired distribution. The generator is a neural network G that transforms a source of noise z into samples from some distribution:\nz \u2192 G(z) (11)\nThe GAN training algorithm maintains an adversary D(z)\u2192 [0, 1] whose goal is to distinguish between samples x from the data distribution and samples from the generator G(z), and the generator G learns to fool the discriminator. Eventually, if GAN training is successful,G converges to a model such that the distribution of G(z) is indistinguishable from the target distribution.\nThe generative adversarial network offers a direct way of training generative models, and it had enjoyed considerable success in learning models of natural images (Denton et al., 2015). We use the GAN training method to train our unsupervised objective Distr [F (x)] = Distr [y] by requiring that F produces samples that are indistinguishable from the target distribution."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 DUAL AUTOENCODERS ON MNIST PERMUTATION TASK", "text": "We begin exploring the ODM cost by selecting a simple artificial task where the distributions overX and Y have rich internal structure but whose relationship is simple. We chose Y to be the distribution of MNIST digits, and X to be the distribution of MNIST digits whose pixels are permuted (with the same permutation on all digits). See Figure 2. We call it the MNIST permutation task. The goal of the task is to learn the unknown permutation using no supervised data.\nWe used a dual autoencoder whose architecture is 784-100-100-100-784, where the weights in the 100-100-100 subnetwork were shared between the two autoencoders. While the results were insensitive to the learning rate, it was important to use a random initialization that is substantially smaller than is typically prescribed for training neural networks models (namely, a unit Gaussian scaled by 0.003/ \u221a #rows +#cols for each matrix).\nWe found that the dual autoencoder was easily able to recover the permutation without using any input-output examples, as shown in Figure 3.\nCuriously, we found that the sigmoid nonlinearity achieved by far the best results on this task, and that the Relu and the Tanh nonlinearity were much less successful for a wide range of hyperparameters. Paradoxically, we suspect that the slower learning of the sigmoid nonlinearity was beneficial for the dual autoencoder, because it caused the \u201ccapacity\u201d of the 100-100-100 network to grow slowly; it is plausible that the most critical learning stage took place when the 100-100-100 network had the lowest capacity (namely, when its output consisted of a very low-dimensional subspace)."}, {"heading": "4.1.1 DUAL AUTOENCODER FOR CIPHERS", "text": "We also tested the dual autoencoder on a character and a word cipher, tasks that were also considered by Knight et al. (2006). In this task, we are given two text corpora that follow the same distribution, but where the characters (or the words) of one of the corpora is scrambled. In detail, we convert a text file into a list of integers by randomly assigning characters (or words) to integers and by consistently using this assignment throughout the file. By doing this twice, we get two data sources that have identical underlying statistics but different symbols. The goal is to find the hidden correspondence between the symbols in both streams. It is not a difficult task since it can be accomplished by inspecting the frequency. Despite its simplicity, this task provides us with another way to evaluate our model.\nWe used the dual autoencoder architecture as before, where the input (as well as the the desired output) is represented with a bag of 10 consecutive characters from a random sentence from the text corpus. The loss function is the softmax cross entropy.\nFor the character-level experiments, we used the architecture 100-25-25-100, and for the wordlevel experiments we used 1000-100-100-1000 \u2014 where we used a vocabulary of the 1000 most frequent symbols in both data streams. Both architectures achieved perfect identification of the symbol mapping in less than 10,000 parameter updates when the sigmoid nonlinearity was used.\nWhile the dual autoencoder was successful on these tasks, its success critically relied on the underlying statistics of the two datasets being very similar. When we trained each autoencoder on data from a different languages (namely on English and Spanish books from Project Gutenberg), it failed\nto learn the correct correspondence between words with similar meanings. This indicates that the dual autoencoder is sensitive to systematic differences in the two distributions."}, {"heading": "4.2 GENERATIVE ADVERSARIAL NETWORKS FOR MNIST CLASSIFICATION", "text": "Next, we evaluated the GAN-based model on an artificial OCR task that was constructed from the MNIST dataset. In this task, we used the text from The Origin of Species downloaded from Project Gutenberg, and arbitrarily replaced each letter with a digit between 0 and 9 in a consistent manner. We call this sequence of numbers the MNIST label file. We then replaced each label with a random image of an MNIST digit of the same class, thus obtaining a very long sequence of images of MNIST digits which we call the MNIST image sequence.\nThe goal of this problem was to train an MLP that mapped MNIST digits to 10-dimensional vectors without using any input-output examples.\nMore formally, we trained an MLP F to map each MNIST digit into a 10-dimensional vector representing their classification. We also used a convolutional adversary to train F to make sure that the distribution of 20 consecutive predictions (F (x1), . . . , F (x20)) is statistically indistinguishable from the distribution of 20 consecutive labels from the MNIST label file (y1, . . . , y20), where the inputs (x1, . . . , x20) are randomly drawn from the MNIST image sequence.\nA typical architecture of our classifier network was 784-300-300-10. The adversary consist of a 1D convolutional neural network with the following architecture: the first three layers are convolutional with a kernel of width 7, whit the following numbers of filter maps: 10-200-200. It is followed by global max pooling over time, and the remainder of the architecture consist of the fully connected layers 200-200-1. The nonlinearity was always ReLU, except for the output layer of both networks, which was the identity.\nIn these experiments, we used the squared loss and not the cross-entropy loss for the supervised objective. If we used the softmax loss, our model\u2019s output layer would have to use the softmax layer, which we found to not work well with GAN training. The squared loss was necessary for using a linear layer for the outputs.\nIt is notable that GAN training was also fragile, and the sensitive hyperparameters are given here:\n\u2022 We initialized each matrix of the generator with a unit Gaussian scaled by 1.4/ \u221a #rows +#cols; for the discriminator we used 1.0/ \u221a #rows +#cols\n\u2022 The batchsize is 200 \u2022 Total number of parameter updates: 6000 \u2022 Learning rate of generator: 0.1/batchsize for 2000 updates; 0.03/batchsize for another\n2000 updates; and then 0.01/batchsize for 2000 updates \u2022 Learning rate of discriminator: 0.005/batchsize\n\u2022 Learning rate of the supervised squared loss: 0.005/batchsize\nWhile learning was extremely rapid, it was highly sensitive to the choice of learning rates.\nWhile GAN training of the ODM cost alone was insufficient to find an accurate classifier the problem, we were able to achieve 4.7% test error on MNIST classification using 4 labelled MNIST examples. Here, each labelled example consists of a single labelled digit, and not a single sequence of 20 labelled digits. The result is insensitive to the specific set of 4 labelled examples. When using 2 labelled MNIST digits, the classification test error increased to 72%."}, {"heading": "5 ONE-SHOT LEARNING AND DOMAIN ADAPTATION", "text": "If the function that we wish to learn, F , has a small number of parameters, then it should be possible to infer F from a very small number of examples \u2014 and in some cases, from a single example.\nThe possibility of inferring F from a single sample has positive implications for domain adaptation: assuming that the training distribution comes from domain D and that the test distribution comes from domain D\u2032. If we are able to instantly map a sample from x \u223c D\u2032 to an \u201cappropriate\u201d sample fromD, we will be able to classify samples fromD\u2032 without any extra training: first map the sample to D, then use a classifier that is able to correctly classify samples from D. Such domain adaptation is attractive, since it allows us to apply our function F to inputs whose statistics differ significantly from the data distribution D, and its one shot nature means that the classifier can correctly react to inputs that follow unexpected statistics.\nWe propose to solve these domain adaptation tasks using the generative model of Eq. 4. Let P (x) be a model of the data distribution and let P\u03b8(y|x) be an unknown likelihood function. Then, given a test case y form a distribution that differs from the data distribution, we can try to infer the unknown x by solving the following optimization problem:\nx\u2217 = argmaxx,\u03b8P\u03b8(y|x)P (x) (12)\nWe emphasize that this optimization is run from scratch for each new test case y. This approach can succeed only when the likelihood P\u03b8(y|x) has few parameters and can be uniquely identified from the above equation. The conditions under which this is likely to hold are discussed in sec. 6.1.\nTo demonstrate these ideas, we use the MNIST dataset for training and the 1-MNIST dataset for testing. The 1-MNIST dataset is obtained by replacing the intensity u of each pixel with 1 \u2212 u. Thus the 1-MNIST distribution differs very significantly from the MNIST distribution, and as a result, neural networks trained on MNIST will be incapable of correctly classifying instances of 1-MNIST.\nOur concrete model choices are the following: P (x) is implemented with a next-row-prediction LSTM with three hidden layers that has been trained to fit the MNIST distribution with the binary cross entropy loss, and P (y|x) is a small convolutional neural network (CNN) with one hidden layer: its first convolution has 5 filters of size 5\u00d75 and its second convolution has one filter of size 5\u00d75.\nGiven a test image y from the 1-MNIST dataset, we optimize logP\u03b8(y|x)P (x) over x and \u03b8. We used significant L2 regularization, and optimized this cost with 104 steps of Adagrad with 5 random restarts. The results are illustrated in Fig. 5.\nWhile the results obtained in this section are preliminary, they show that in addition to synthesis and denoising, generative models can be used for aligning distributions and for expanding the set of distributions to which the model is applied."}, {"heading": "6 DISCUSSION", "text": ""}, {"heading": "6.1 WHEN IS SUPERVISION NEEDED?", "text": "When does the ODM fully determine the best F ? If the function class is too capable, then F can map any distribution over the inputs to any distribution over the outputs, so the ODM cost cannot possibly recover F . However, since the ODM cost is consistent, it is likely to improve generalization by eliminating unsuitable functions from consideration. Further, if the output space Y is small, then the ODM cost will not convey enough information to be of significant help to the supervised cost.\nHowever, there is a setting where the ODM cost function could, in principle, completely determine the best supervised function F . It should succeed whenever the input distribution and the output distribution contains long-range dependencies, while the function class of F is chosen from such a class that it is incapable of modifying the distribution\u2019s long range dependencies by being local through time: a localized change in the input causes a localized change in the output. This setting is illustrated in Fig. 6."}, {"heading": "6.2 LIMITATIONS", "text": "ODM-based training has a number of limitations. The output space must be large, and the \u201cshared hidden structure\u201d of the two space has to be sufficiently similar. The simple models that were used in this paper are unlikely to successfully learn a correspondence between two spaces whose hidden shared structures are not very similar. It is conceivable that as we manage to train more expressive generative models, they will become able to notice the natural correspondences between hidden structures that are not identical.\nThe greater limitation of model is caused by the lack of an extremely good method for fitting generative models. As these algorithms get better with time, it will become possible to optimize the ODM cost in a wider variety of settings, and in a more robust way."}, {"heading": "7 CONCLUSIONS", "text": "In this paper, we showed that the ODM cost provides a generic approach for unsupervised learning that is entirely consistent with the supervised cost function. Although we were not able to develop a reliable method that can train, e.g., an attention model Bahdanau et al. (2014) with the ODM objective, we presented evidence that the ODM objective provides a sensible way of training functions without the use of input-output examples. We expect better techniques for optimizing the ODM cost to make it universally applicable and useful."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Text OCR by solving a cryptogram", "author": ["Casey", "Richard G"], "venue": "International Business Machines Incorporated, Thomas J. Watson Research Center,", "citeRegEx": "Casey and G.,? \\Q1986\\E", "shortCiteRegEx": "Casey and G.", "year": 1986}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily", "Chintala", "Soumith", "Szlam", "Arthur", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1506.05751,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "A technical word-and term-translation aid using noisy parallel corpora across language groups", "author": ["Fung", "Pascale", "McKeown", "Kathleen"], "venue": "Machine translation,", "citeRegEx": "Fung et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Fung et al\\.", "year": 1997}, {"title": "Domain-adversarial training of neural networks", "author": ["Gani", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "Fran\u00e7ois", "Marchand", "Mario", "Lempitsky", "Victor"], "venue": "arXiv preprint arXiv:1505.07818,", "citeRegEx": "Gani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gani et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Haghighi", "Aria", "Liang", "Percy", "Berg-Kirkpatrick", "Taylor", "Klein", "Dan"], "venue": "In ACL,", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Cryptogram decoding for optical character recognition", "author": ["Huang", "Gary", "Learned-Miller", "Erik G", "McCallum", "Andrew"], "venue": "University of Massachusetts-Amherst Technical Report,", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Unsupervised analysis for decipherment problems", "author": ["Knight", "Kevin", "Nair", "Anish", "Rathod", "Nishit", "Yamada", "Kenji"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions,", "citeRegEx": "Knight et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Knight et al\\.", "year": 2006}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["Koehn", "Philipp", "Knight", "Kevin"], "venue": "In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2002}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Attacking decipherment problems optimally with low-order n-gram models", "author": ["Ravi", "Sujith", "Knight", "Kevin"], "venue": "In proceedings of the conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ravi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2008}, {"title": "Learning representations by backpropagating", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A statistical model for lost language decipherment", "author": ["Snyder", "Benjamin", "Barzilay", "Regina", "Knight", "Kevin"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Snyder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2010}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Tzeng", "Eric", "Hoffman", "Judy", "Zhang", "Ning", "Saenko", "Kate", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "Each training case (xi, yi) imposes a soft constraint on F : F (xi) = yi (1) We solve these equations by local optimization with the backpropagation algorithm (Rumelhart et al., 1986).", "startOffset": 159, "endOffset": 183}, {"referenceID": 13, "context": "This idea has been further explored in the context of machine translation (typically in the form of matching bigrams) (Knight et al., 2006; Huang et al., 2006; Snyder et al., 2010; Ravi & Knight, 2008), and in unsupervised lexicon induction (Fung & McKeown, 1997; Koehn & Knight, 2002; Haghighi et al.", "startOffset": 118, "endOffset": 201}, {"referenceID": 11, "context": "This idea has been further explored in the context of machine translation (typically in the form of matching bigrams) (Knight et al., 2006; Huang et al., 2006; Snyder et al., 2010; Ravi & Knight, 2008), and in unsupervised lexicon induction (Fung & McKeown, 1997; Koehn & Knight, 2002; Haghighi et al.", "startOffset": 118, "endOffset": 201}, {"referenceID": 19, "context": "This idea has been further explored in the context of machine translation (typically in the form of matching bigrams) (Knight et al., 2006; Huang et al., 2006; Snyder et al., 2010; Ravi & Knight, 2008), and in unsupervised lexicon induction (Fung & McKeown, 1997; Koehn & Knight, 2002; Haghighi et al.", "startOffset": 118, "endOffset": 201}, {"referenceID": 8, "context": ", 2010; Ravi & Knight, 2008), and in unsupervised lexicon induction (Fung & McKeown, 1997; Koehn & Knight, 2002; Haghighi et al., 2008; Mikolov et al., 2013b).", "startOffset": 68, "endOffset": 158}, {"referenceID": 20, "context": "Similar ideas have recently been proposed for domain adaptation where a model learns to map the new distribution back onto the training distribution (Tzeng et al., 2014; Gani et al., 2015).", "startOffset": 149, "endOffset": 188}, {"referenceID": 6, "context": "Similar ideas have recently been proposed for domain adaptation where a model learns to map the new distribution back onto the training distribution (Tzeng et al., 2014; Gani et al., 2015).", "startOffset": 149, "endOffset": 188}, {"referenceID": 3, "context": "Other work used the k-means objective for pre-training (Coates et al., 2011), and this list of references is far from exhaustive.", "startOffset": 55, "endOffset": 76}, {"referenceID": 5, "context": ", 2010; Ravi & Knight, 2008), and in unsupervised lexicon induction (Fung & McKeown, 1997; Koehn & Knight, 2002; Haghighi et al., 2008; Mikolov et al., 2013b). Notably, Knight et al. (2006) discusses the connection between unsupervised learning and language decipherment, and formulates a generative model similar to the one presented in this work.", "startOffset": 113, "endOffset": 190}, {"referenceID": 4, "context": ", 2014; Gani et al., 2015). These approaches are closely related to the ODM cost, since they are concerned with transforming the new distribution back to the training distribution. There has been a lot of other work on unsupervised learning with neural networks, which is largely concerned with the concept of \u201cpre-training\u201d. In pre-training, we first train the model with an unsupervised cost function, and finish training the model with the supervised cost. This concept was introduced by Hinton et al. (2006) and Hinton & Salakhutdinov (2006) and later by Bengio et al.", "startOffset": 8, "endOffset": 512}, {"referenceID": 4, "context": ", 2014; Gani et al., 2015). These approaches are closely related to the ODM cost, since they are concerned with transforming the new distribution back to the training distribution. There has been a lot of other work on unsupervised learning with neural networks, which is largely concerned with the concept of \u201cpre-training\u201d. In pre-training, we first train the model with an unsupervised cost function, and finish training the model with the supervised cost. This concept was introduced by Hinton et al. (2006) and Hinton & Salakhutdinov (2006) and later by Bengio et al.", "startOffset": 8, "endOffset": 546}, {"referenceID": 1, "context": "(2006) and Hinton & Salakhutdinov (2006) and later by Bengio et al. (2007). Other work used the k-means objective for pre-training (Coates et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 1, "context": "(2006) and Hinton & Salakhutdinov (2006) and later by Bengio et al. (2007). Other work used the k-means objective for pre-training (Coates et al., 2011), and this list of references is far from exhaustive. More recent examples of unsupervised pre-training are the Skip-gram model (Mikolov et al., 2013a) and its generalization to sentences, the Skip-thought vectors model of Kiros et al. (2015). These models use well-motivated unsupervised objective functions that appear to be genuinely useful for a wide variety of language-processing tasks.", "startOffset": 54, "endOffset": 395}, {"referenceID": 13, "context": "A similar generative model was presented by Knight et al. (2006). It is desirable to train generative models using the variational autoencoder (VAE) of Kingma & Welling (2013).", "startOffset": 44, "endOffset": 65}, {"referenceID": 13, "context": "A similar generative model was presented by Knight et al. (2006). It is desirable to train generative models using the variational autoencoder (VAE) of Kingma & Welling (2013). However, VAE training forces y to be continuous, which is undesirable since many domains of interest are discrete.", "startOffset": 44, "endOffset": 176}, {"referenceID": 7, "context": "The Generative Adversarial Network (Goodfellow et al., 2014) is a procedure for training a \u201cgenerator\u201d to produce samples that are statistically indistinguishable from a desired distribution.", "startOffset": 35, "endOffset": 60}, {"referenceID": 4, "context": "The generative adversarial network offers a direct way of training generative models, and it had enjoyed considerable success in learning models of natural images (Denton et al., 2015).", "startOffset": 163, "endOffset": 184}, {"referenceID": 13, "context": "1 DUAL AUTOENCODER FOR CIPHERS We also tested the dual autoencoder on a character and a word cipher, tasks that were also considered by Knight et al. (2006). In this task, we are given two text corpora that follow the same distribution, but where the characters (or the words) of one of the corpora is scrambled.", "startOffset": 136, "endOffset": 157}], "year": 2015, "abstractText": "General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the sought-after supervised tasks. In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large. We demonstrate that the ODM cost works well on number of small and semiartificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain.", "creator": "LaTeX with hyperref package"}}}