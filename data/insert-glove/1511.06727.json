{"id": "1511.06727", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters", "abstract": "Hyperparameter chetumal selection fire-resistance generally kolar relies on elayne running lanphier multiple dubailand full nymphe training trials, chakrabongse with hyperparameter kohala selection levassor based np on validation set 15th performance. We propose a ozoli\u0146\u0161 gradient - brio based kopli approach for locally hyksos adjusting hyperparameters lacour on vongerichten the thessaloniki fly pooh-bah in which we horseriding adjust the hyperparameters stewardship so d'ansembourg as novatek to run-scorer make the model parameter urbizo gradients, lysosomal and pelezi hence tunas updates, jackness more buah advantageous wsbk-tv for the validation cost. hiko We explore 50-page the approach for tuning yellowbird regularization hyperparameters mid-2000s and yankee find that in experiments krey on MNIST peverel the resulting regularization intercepts levels bailian are witten within voio the subtype optimal regions. The dardanelles method is wapato less computationally sekiyama demanding rattanakiri compared czersk to friendster similar gradient - based manilal approaches to euro98 hyperparameter marsha selection, shanked only vandeweghe requires 21-2 a serioux few lamacq trials, and 649,000 consistently finds solid hyperparameter values 2.20 which makes it a envies useful tool for coping training neural network incisive models.", "histories": [["v1", "Fri, 20 Nov 2015 19:10:16 GMT  (157kb,D)", "https://arxiv.org/abs/1511.06727v1", "8 pages, 5 figures"], ["v2", "Wed, 16 Dec 2015 06:28:07 GMT  (153kb,D)", "http://arxiv.org/abs/1511.06727v2", "8 pages, 5 figures. added references, fixed typos"], ["v3", "Fri, 17 Jun 2016 19:25:32 GMT  (872kb,D)", "http://arxiv.org/abs/1511.06727v3", "9 pages, 7 figures. Accepted at ICML 2016"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jelena luketina", "tapani raiko", "mathias berglund", "klaus greff"], "accepted": true, "id": "1511.06727"}, "pdf": {"name": "1511.06727.pdf", "metadata": {"source": "META", "title": "Scalable Gradient-Based Tuning of  Continuous Regularization Hyperparameters", "authors": ["Jelena Luketina", "Mathias Berglund", "Klaus Greff", "Tapani Raiko"], "emails": ["JELENA.LUKETINA@AALTO.FI", "MATHIAS.BERGLUND@AALTO.FI", "KLAUS@IDSIA.CH", "TAPANI.RAIKO@AALTO.FI"], "sections": [{"heading": "1. Introduction", "text": "Specifying and training artificial neural networks requires several design choices that are often not trivial to make. Many of these design choices boil down to the selection of hyperparameters. The process of hyperparameter selection is in practice often based on trial-and-error and grid or random search (Bergstra and Bengio, 2012). There are also a\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nnumber of automated methods (Bergstra et al., 2011; Snoek et al., 2012), all of which rely on multiple complete training runs with varied fixed hyperparameters, with the hyperparameter selection based on the validation set performance.\nAlthough effective, these methods are expensive as the user needs to run multiple full training runs. In the worst case, the number of needed runs also increases exponentially with the number of hyperparameters to tune, if an extensive exploration is desired. In many practical applications such an approach is too tedious and time-consuming, and it would be useful if a method existed that could automatically find acceptable hyperparameter values in one training run even if the user did not have a strong intuition regarding good values to try for the hyperparameters.\nIn contrast to these methods, we treat hyperparameters similar to elementary1 parameters during training, in that we simultaneously update both sets of parameters using stochastic gradient descent. The gradient of elementary parameters is computed as in usual training from the cost of the regularized model on the training set, while the gradient of hyperparameters (hypergradient) comes from the cost of the unregularized model on the validation set. For simplicity, we will refer to the training set as T1 and to the validation set (or any other data set used exclusively for training the hyperparameters) as T2. The method itself will be called T1 \u2212 T2, referring to the two simultaneous optimization processes.\nSimilar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients\n1Borrowing the expression from Maclaurin et al. (2015), we refer to the model parameters customarily trained with backpropagation as elementary parameters, and to all other parameters as hyperparameters.\nar X\niv :1\n51 1.\n06 72\n7v 3\n[ cs\n.L G\n] 1\n7 Ju\nthrough the entire history of parameter updates Maclaurin et al. (2015). Moreover, these methods make changes to the hyperparameter only once the elementary parameter training has ended. These drawbacks make them too expensive for use in modern neural networks, which often require millions of parameters and large data sets.\nElements distinguishing our approach are: 1. By making some very rough approximations, our\nmethod for modifying hyperparameters avoids using computationally expensive terms, including the computation of the Hessian or its inverse. This is because with the T1\u2212T2 method, hyperparameter updates are based on stochastic gradient descent, instead of Newton\u2019s method. Furthermore, any dependency of elementary parameters on hyperparameters beyond the last update is disregarded. As a result, additional computational and memory overhead therefore becomes comparable to back-propagation.\n2. Hyperparameters are trained simultaneously with elementary parameters. Feedback and feedforward passes can be computed simultaneously for the training and validation set, further reducing the computational cost.\n3. We add batch normalization (Ioffe and Szegedy, 2015) and adaptive learning rates (Kingma and Ba, 2015) to the process of hyperparameter training, which diminishes some of the problems of gradient-based hyperparameter optimization. Through batch normalization, we can counter internal covariate shifts. This eliminates the need for different learning rates at each layer, as well as speeding up adjustment of the elementary parameters to the changes in hyperparameters. This is particularly relevant when parametrizing each of the layers with a separate hyperparameter.\nA common assumption is that the choice of hyperparameters affects the whole training trajectory, i.e. changing a hyperparameter on the fly during training has a significant effect on the training trajectory. This \u201chysteresis effect\u201d implies that in order to measure how a hyperparameter combination influences the validation set performance, the hyperparameters need to be kept fixed during the whole training procedure. However, to our knowledge this has not been systematically studied. If the hysteresis effect is weak enough and the largest changes to the hyperparameter happen early on, it becomes possible to train the model while tuning the hyperparameters on the fly during training, and then use the final hyperparameter values to retrain the model if a fixed set of hyperparameters is desired. We also explore this approach.\nAn important design choice when training neural network models is which regularization strategy to use in order to ensure that the model generalizes to data not included in the training set. Common regularization strategies involve\nadding explicit terms to the model or the cost function during training, such as penalty terms on the model weights or injecting noise to inputs or neuron activations. Injecting noise is particularly relevant for denoising autoencoders and related models (Vincent et al., 2010; Rasmus et al., 2015), where performance strongly depends on the level of noise.\nAlthough the proposed method could work in principle for any continuous hyperparameter, we have specifically focused on studying tuning of regularization hyperparameters. We have chosen to use Gaussian noise added to the inputs and hidden layer activations, in addition to L2 weight penalty. A third, often used, regularization method that involves a hyperparameter choice is dropout (Srivastava et al., 2014). However, we have omitted studying dropout as it is not trivial to compute a gradient on the dropout rate. Moreover, dropout can be seen as a form of multiplicative Gaussian noise (Wang and Manning, 2013). We also omit study adapting the learning rate, since we suspect that the local gradient information is not sufficient to determine optimal learning rates.\nIn Section 2 we present details on the proposed method. The method is tested with multiple MLP and CNN network structures and regularization schemes, detailed in Section 3. The results of the experiments are presented in Section 3.1."}, {"heading": "2. Proposed Method", "text": "We propose a method, T1\u2212T2, for tuning continuous hyperparameters of a model using the gradient of the performance of the model on a separate validation set T2. In essence, we train a neural network model on a training set T1 as usual. However, for each update of the network weights and biases, i.e. the elementary parameters of the network, we tune the hyperparameters so as to make the direction of the weight update as beneficial as possible for the validation cost on a separate dataset T2.\nFormally, when training a neural network model, we try to minimize an objective function that depends on the training set, model weights and hyperparameters that determine the strength of possible regularization terms. When using gradient descent, we denote the optimization objective function C\u03031(\u00b7) and the corresponding weight update as:\nC\u03031(\u03b8|\u03bb, T1) = C1(\u03b8|\u03bb, T1) + \u2126(\u03b8,\u03bb), (1) \u03b8t+1 = \u03b8t + \u03b71\u2207\u03b8C\u03031(\u03b8t|\u03bbt, T1), (2)\nwhere C\u03031(\u00b7) and \u2126(\u00b7) are cost and regularization penalty terms, T1 = {(xi, yi)} is the training data set, \u03b8 = {Wl, bl} a set of elementary parameters including weights and biases of each layer, \u03bb denotes various hyperparameters that determine the strength of regularization, while \u03b71 is a learning rate. Subscript t refers to the iteration number.\nAssuming T2 = {(xi, yi)} is a separate validation data set, the generalization performance of the model is measured with a validation cost C2(\u03b8t+1, T2), which is usually a function of the unregularized model. Hence the value of the cost function of the actual performance of the model does not depend on the regularizer directly, but only on the elementary parameter updates. The gradient of the validation cost with respect to \u03bb is:\n\u2207\u03bbC2 = (\u2207\u03b8C2)(\u2207\u03bb\u03b8t+1)\nWe only consider the influence of the regularization hyperparameter on the current elementary parameter update, \u2207\u03bb\u03b8t+1 = \u03b71\u2207\u03bb\u2207\u03b8C\u03031 based on Eq. (2). The hyperparameter update is therefore:\n\u03bbt+1 = \u03bbt + \u03b72(\u2207\u03b8C2)(\u2207\u03bb\u2207\u03b8C\u03031) (3)\nwhere \u03b72 is a learning rate.\nThe method is greedy in the sense that it only depends on one parameter update, and hence rests on the assumption that a good hyperparameter choice can be evaluated based on the local information within only one elementary parameter update."}, {"heading": "2.1. Motivation and analysis", "text": "The most similar previously proposed model is the incremental gradient version of the hyperparameter update from (Chen and Hagan, 1999). However their derivation of the\nhypergradient assumes a Gauss-Newton update of the elementary parameters, making computation of the gradient and the hypergradient significantly more expensive.\nA well justified closed form for the term \u2207\u03bb\u03b8 is available once the elementary gradient has converged (Foo et al., 2008), with the update of the form (4). Comparing this expression with the T1 \u2212 T2 update, (3) can be considered as approximating (4) in the case when gradient is near convergence and the Hessian can be well approximated by identity \u22072\u03b8C\u03031 = I:\n\u03bbt+1 = \u03bbt + (\u2207\u03b8C2)(\u22072\u03b8C\u03031)\u22121(\u2207\u03bb\u2207\u03b8C\u03031). (4)\nAnother approach to hypergradient computation is given in Maclaurin et al. (2015). There, the term \u2207\u03bb\u03b8T (T denoting the final iteration number) considers effect of the hyperparameter on the entire history of updates:\n\u03b8T = \u03b80 + \u2211\n0<k<T\n4\u03b8k,k+1(\u03b8k(\u03bbt), \u03bbt, \u03b7k), (5)\n\u03bbt+1 = \u03bbt + (\u2207\u03b8C2)(\u2207\u03bb\u03b8T ). (6)\nIn the simplest case where the update is formed only with the current gradient4\u03b8k,k+1 = \u2212\u03b71,k\u2207\u03b8C\u03031, i.e. not including the momentum term or adaptive learning rates, the update of a hyperparameter is formed by collecting the elements from the entire training procedure:\n\u03bbt+1 = \u03bbt + \u03b72\u2207\u03b8C2 \u2211\n0<k<T\n\u03b71\u2207\u03bb\u2207\u03b8C\u03031,k. (7)\nEq. (3) can therefore be considered as an approximation of (7), where we consider only the last update instead of backpropagating through the whole weight update history and updating the hyperparameters without resetting the weights.\nIn theory, approximating the Hessian with identity might cause difficulties. From Equation (3), it follows that the method converges when (\u2207\u03b8C2)(\u2207\u03bb\u2207\u03b8C\u03031) = 0, or in other words, for all components i of the hyperparameter vector \u03bb, \u2207\u03b8C2 is orthogonal to \u2202\u2207\u03b8C\u03031\u2202\u03bbi . This is in contrast to the standard optimization processes that converge when the gradient is zero. In fact, we cannot guarantee convergence at all. Furthermore, if we replace the global (scalar) learning rate \u03b71 in Equation (2) with individual learning rates \u03b71,j for each elementary-parameter \u03b8j,t, the point of convergence could change.\nIt is clear that the identity Hessian assumption is an approximation that will not hold strictly. However, arguably, batch normalization (Ioffe and Szegedy, 2015) is eliminating part of the problem, by making the Hessian closer to identity (Vatanen et al., 2013; Raiko et al., 2012), making the approximation more justified. Another step towards making even closer approximation are transformations that further whiten the hidden representations (Desjardins et al., 2015)."}, {"heading": "2.2. Computational cost", "text": "The most computationally expensive term of the proposed method is (\u2207\u03b8C2)(\u2207\u03bb\u2207\u03b8C\u03031), where the exact complex-\nity depends on the details of the implementation and the hyperparameter. When training L2 penalty regularizers \u2126(\u03b8) = \u2211 j \u03bbj 2 \u03b8 2 j , the additional cost is negligible, as\n\u22022C\u03031 \u2202\u03bbj\u2202\u03b8k = \u03b8j\u03b4j,k, where \u03b4 is the indicator function.\nThe gradient of the cost with respect to a noise hyperparameter \u03c3l at layer l, can be computed as \u2202C\u03031\u2202\u03c3l =\n(\u2207hlC\u03031) ( \u2202hl \u2202\u03c3l )> , where hl is hidden layer l activation. In case of additive Gaussian noise, where noise is added as hl \u2192 hl + \u03c3Le, where e is a random vector sampled from the standard normal distribution with the same dimensionality as hl, the derivative becomes \u2202C\u03031\u2202\u03c3l = \u2202C\u03031 \u2202hl\ne>. It can be shown that the cost of computing this term scales comparably to backpropagation, due to the properties of R and L-operators (Pearlmutter, 1994; Schraudolph, 2002).\nFor our implementation, the cost of computing the hypergradients of a model with additive noise in each layer, was around 3 times that of backpropagation. We reduced this cost further by making one hyperparameter update per each 10 elementary parameter updates. While it did not change performance of the method, it reduced the additional cost to about only 30% that of backpropagation. The cost could be possibly reduced even further by making hyperparameter updates even less frequently, though we have not explored this further."}, {"heading": "3. Experiments", "text": "The goal of the experimental section is to address the following questions:\n\u2022 Will the method find new hyperparameters which improve the performance of the model, compared to the initial set of hyperparameters?\n\u2022 Can we observe hysteresis effects, i.e. will the model obtained, while simultaneously modifying parameters and hyperparameters, perform the same as a model trained with a hyperparameter fixed to the final value?\n\u2022 Can we observe overfitting on the validation set T2? When hyperparameters are tuned for validation performance, is the performance on the validation set still indicative of the performance on the test set?\nWe test the method on various configurations of multilayer perceptrons (MLPs) with ReLU activation functions (Dahl et al., 2013) trained on the MNIST (LeCun et al., 1998) and SVHN (Netzer et al., 2011) data set. We also test the method on two convolutional architectures (CNNs) using CIFAR10 (Krizhevsky, 2009). The CNN architectures used were modified versions of model All-CNN-C from (Springenberg et al., 2014) and a baseline model from (Rasmus et al., 2015), using ReLU and leakyReLU activations (Xu et al., 2015). The models were implemented with the Theano package (Team, 2016). All the code, as well as the exact configurations used in the experiments can be found in the project\u2019s Github repository2.\nFor MNIST we tried various network sizes: shallow 1000\u00d7 1000 \u00d7 1000 to deep 4000 \u00d7 2000 \u00d7 1000 \u00d7 500 \u00d7 250. Training set T1 had 55 000 samples, and validation T2 had 5 000 samples. The split between T1 and T2 was made using a different random seed in each of the experiments to avoid overfitting to a particular subset of the training set. Data preprocessing consisted of only centering each feature.\n2https://github.com/jelennal/t1t2\nIn experiments with SVHN we tried 2000\u00d7 2000\u00d7 2000 and 4000\u00d7 2000\u00d7 1000\u00d7 500\u00d7 250 architectures. Global contrast normalization was used as the only preprocessing step. Out of 73257 training samples, we picked a random 65 000 samples for T1 and the remaining 8 257 samples for T2. None of the SVHN experiments used tied hyperparameters, i.e. each layer was parametrized with a separate hyperparameter, which was tuned independently.\nTo test on CIFAR-10 with convolutional networks, we used 45 000 samples for T1 and 5 000 samples for T2. The data was preprocessed using global contrast normalization and whitening.\nWe regularized the models with additive Gaussian noise to the input with standard deviation n0 and each hidden layer with standard deviation n1; or a combination of additive noise to the input layer and L2 penalty with strength multiplier l2 for weights in each of the layers. Because L2 penalty matters less in models using batch normalization, in experiments using L2 penalty we did not use batch normalization. We tried both tied regularization levels (one hyperparameter for all hidden layers) and having separate regularization parameters for each layer. As a cost function, we use cross-entropy for both T1 and T2.\nEach of the experiments were run with 200-300 epochs, using batch size 100 for both elementary and hyperparameter training. To speed up elementary parameter training, we use an annealed ADAM learning rate schedule (Kingma and Ba, 2015) with a step size of 10\u22123 (MLPs) or 2 \u00b7 10\u22123 (CNNs). For tuning noise hyperparameters, we use vanilla gradient descent with a step size 10\u22121; while for L2 hyperparameters, step sizes were significantly smaller, 10\u22124. In experiments on larger networks we also use ADAM for tuning hyperparameters, with the step size 10\u22123 for noise and\n10\u22126 for L2. We found that while the learning rate did not significantly influence the general area of convergence for a hyperparameter, too high learning rates did cause too noisy and sudden hyperparameter changes, while too low learning rates resulted in no significant changes of hyperparameters. A rule of thumb is to use a learning rate corresponding to the expected order of magnitude of the hyperparameter. Moreover, if the hyperparameter updates are utilized less frequently, the learning rate should be higher.\nIn most experiments, we first measure the performance of the model trained using some fixed, random hyperparameters sampled from a reasonable interval. Next, we train the model with T1 \u2212 T2 from that random hyperparameter initialization, measuring the final performance. Finally, we rerun the training procedure with the fixed hyperparameter set to the final hyperparameter values found by T1 \u2212 T2. Note that in all the scatter plots, points with the same color indicate the same model configuration: same number of neurons and layers, learning rates, use of batch normalization, and the same types of hyperparameters tuned just with different initializations."}, {"heading": "3.1. Results", "text": "Figure 1 illustrates resulting hyperparameters changes during T1\u2212T2 training. To see how the T1\u2212T2 method behaves, we visualized trajectories of hyperparameter values during training in the hyperparameter cost space. For each point in the two-dimensional hyperparameter space, we compute the corresponding test cost without T1\u2212 T2. In other words, the background of the figures corresponds to grid search on the two-dimensional hyperparameter interval. The initial regularization hyperparameter value is denoted with a star, while the final value is marked with a square.\nAs can be seen from the figure, all runs converge to a reasonable set of hyperparameters irrespective of the starting value, gradually moving to a point of lower log-likelihood. Note that because the optimal values of learning rates for each hyperparameter direction are unknown, hyperparameters will change the most along directions corresponding to either the local gradient or the higher relative learning rate.\nOne way to use the proposed method is to tune the hyperparameters, and then rerun the training from the beginning using fixed values for the hyperparameters set to the final values acquired at the end of T1 \u2212 T2 training. Figure 4 illustrates how much T1 \u2212 T2 can improve initial hyperparameters. Each point in the grid corresponds to the test performance of a model fully trained with two different fixed hyperparameters: one is the initial hyperparameter before being tuned with T1 \u2212 T2 (x-axis), the other is final hyperparameter found after tuning the initial hyperparameter with T1 \u2212 T2 (y-axis). As can be seen from the plot, none of the models trained with hyperparameters found by T1 \u2212 T2 performed poorly, regardless of how poor the performance was with the initial hyperparameters.\nNext we explore the strength of the hysteresis effect, i.e. how the performance of a model with a different hyperparameter history compares to the performance of a model with a fixed hyperparameter. In Figure 5 we plot the error after a run using T1 \u2212 T2, compared to the test error if the model is rerun with the hyperparameters fixed to the values at the end of T1 \u2212 T2 training. The results indicate that there is a strong correlation, with in most cases, reruns performing somewhat better. The method can be used for training models with fixed hyperparameters, or as a baseline for further hyperparameter finetuning. The hysteresis effect was stronger on CIFAR-10, where retraining produced significant improvements.\nWe explore the possibility of overfitting on the validation set. Figure 6 (right) shows the validation error compared to the final test error of a model trained with T1 \u2212 T2. We do not observe overfitting, with validation performance being strongly indicative of the test set performance. For MNIST all the results cluster tightly in the region of low error, hence there is no apparent structure. It should be noted though, that in these experiments we had at most 20 hyperparameters, making overfitting to validation set unlikely."}, {"heading": "4. Discussion and Conclusion", "text": "We have proposed a method called T1 \u2212 T2 for gradientbased automatic tuning of continuous hyperparameters during training, based on the performance of the model on a separate validation set. We experimented on tuning regularization hyperparameters when training different model structures on the MNIST and SVHN datasets.The T1 \u2212 T2 model consistently managed to improve on the initial levels of additive noise and L2 weight penalty. The improvement was most pronounced when the initial guess of the regularization hyperparameter values was far from the optimal value.\nAlthough T1 \u2212 T2 is unlikely to find the best set of hyperparameters compared to an exhaustive search where the model is trained repeatedly with a large number of hyperparameter proposals, the property that it seems to find values fairly close to the optimum is useful e.g. in situations where the user does not have a prior knowledge on good intervals for regularization selection; or the time to explore the full hyperparameter space. The method could also be used in combination with random search, redirecting the random hyperparameter initializations to better regions.\nWhile the T1 \u2212 T2 method is helpful for minimizing the objective function on the validation set, as illustrated in Figure 7, a set of hyperparameters minimizing a continuous objective like cross-entropy, might not be optimal for the classification error. It may be worthwhile to try objective functions which approximate the classification error better, as well as trying the method on unsupervised objectives.\nAs a separate validation set is used for tuning of hyperparameters, it could be possible to overfit to the validation set. However, our experiments indicated that this effect is not practically significant in the settings tested in this paper, which had at most 10-20 hyperparameters.\nThe method could be used to tune a much larger number of hyperparameters than what was computationally feasible before. It could also be used to tune hyperparameters other than continuous regularization hyperparameters, using continuous versions of those hyperparameters. For example, consider the following implementation of a continuously parametrized number of layers: the final softmax layer takes input from all the hidden layers, however the contribution of each layer is weighted with a continuous function a(i), such that one layer and its neighboring layers contribute the most, e.g. output = softmax[ \u2211 i=1..L a(i)W\u0303ihi], where L is the number of layers and a(i) = e(i\u2212m) 2/v. Which layer contributes the most to the output layer, is determined with a differentiable function, and parameters of this function, m and v in this example, could in principle be trained using the T1\u2212 T2 method."}, {"heading": "5. Acknowledgements", "text": "We are very thankful to many colleagues for the helpful conversations and feedback, particularly Dario Amodei and Harri Valpola. Special thanks to Antti Rasmus for the technical assistance. Also thanks to Olof Mogren and Mikael Kageback, who provided detailed comments on the paper draft. Jelena Luketina and Tapani Raiko were funded by the Academy of Finland. Mathias Berglund was funded by the HICT doctoral education network."}], "references": [{"title": "Gradient-based optimization of hyperparameters", "author": ["Y. Bengio"], "venue": "Neural computation, 12(8), 1889\u20131900.", "citeRegEx": "Bengio,? 2000", "shortCiteRegEx": "Bengio", "year": 2000}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "J. Mach. Learn. Res., 13, 281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["J.S. Bergstra", "R. Bardenet", "Y. Bengio", "B. K\u00e9gl"], "venue": "Ad-", "citeRegEx": "Bergstra et al\\.,? 2011", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Optimal use of regularization and cross-validation in neural network modeling", "author": ["D. Chen", "M.T. Hagan"], "venue": "International Joint Conference on Neural Networks, pages 1275\u20131289.", "citeRegEx": "Chen and Hagan,? 1999", "shortCiteRegEx": "Chen and Hagan", "year": 1999}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "ICASSP, pages 8609\u20138613.", "citeRegEx": "Dahl et al\\.,? 2013", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Natural neural networks", "author": ["G. Desjardins", "K. Simonyan", "R. Pascanu", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Desjardins et al\\.,? 2015", "shortCiteRegEx": "Desjardins et al\\.", "year": 2015}, {"title": "Efficient multiple hyperparameter learning for log-linear models", "author": ["Foo", "C.-s.", "C.B. Do", "A. Ng"], "venue": "Advances in neural information processing systems (NIPS), pages 377\u2013384.", "citeRegEx": "Foo et al\\.,? 2008", "shortCiteRegEx": "Foo et al\\.", "year": 2008}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "the International Conference on Learning Representations (ICLR), San Diego. arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "Adaptive regularization in neural network modeling", "author": ["J. Larsen", "C. Svarer", "L.N. Andersen", "L.K. Hansen"], "venue": "Neural Networks: Tricks of the Trade, pages 113\u2013132. Springer.", "citeRegEx": "Larsen et al\\.,? 1998", "shortCiteRegEx": "Larsen et al\\.", "year": 1998}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "International Conference on Machine Learning.", "citeRegEx": "Maclaurin et al\\.,? 2015", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 4. Granada, Spain.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Fast Exact Multiplication by the Hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation, pages 147\u2013160.", "citeRegEx": "Pearlmutter,? 1994", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["T. Raiko", "H. Valpola", "Y. LeCun"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 924\u2013932.", "citeRegEx": "Raiko et al\\.,? 2012", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "Semi-supervised learning with ladder network", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "Neural Information Processing Systems.", "citeRegEx": "Rasmus et al\\.,? 2015", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["N.N. Schraudolph"], "venue": "Neural Computation, 14(7), 1723\u20131738.", "citeRegEx": "Schraudolph,? 2002", "shortCiteRegEx": "Schraudolph", "year": 2002}, {"title": "Practical Bayesian Optimization of Machine Learning Algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "ArXiv e-prints.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": null, "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1), 1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["T.T.D. Team"], "venue": null, "citeRegEx": "Team,? \\Q2016\\E", "shortCiteRegEx": "Team", "year": 2016}, {"title": "Pushing stochastic gradient towards second-order methods\u2013backpropagation learning with transformations in nonlinearities", "author": ["T. Vatanen", "T. Raiko", "H. Valpola", "Y. LeCun"], "venue": "Neural Information Processing, pages 442\u2013449. Springer.", "citeRegEx": "Vatanen et al\\.,? 2013", "shortCiteRegEx": "Vatanen et al\\.", "year": 2013}, {"title": "Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. antoine Manzagol"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Fast dropout training", "author": ["S.I. Wang", "C.D. Manning"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML).", "citeRegEx": "Wang and Manning,? 2013", "shortCiteRegEx": "Wang and Manning", "year": 2013}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["B. Xu", "N. Wang", "M. Li"], "venue": "CoRR.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "The process of hyperparameter selection is in practice often based on trial-and-error and grid or random search (Bergstra and Bengio, 2012).", "startOffset": 112, "endOffset": 139}, {"referenceID": 2, "context": "number of automated methods (Bergstra et al., 2011; Snoek et al., 2012), all of which rely on multiple complete training runs with varied fixed hyperparameters, with the hyperparameter selection based on the validation set performance.", "startOffset": 28, "endOffset": 71}, {"referenceID": 18, "context": "number of automated methods (Bergstra et al., 2011; Snoek et al., 2012), all of which rely on multiple complete training runs with varied fixed hyperparameters, with the hyperparameter selection based on the validation set performance.", "startOffset": 28, "endOffset": 71}, {"referenceID": 10, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 0, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 3, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 6, "context": "Similar approaches have been proposed since the late 1990s; however, these methods either require computation of the inverse Hessian (Larsen et al., 1998; Bengio, 2000; Chen and Hagan, 1999; Foo et al., 2008), or propagating gradients", "startOffset": 133, "endOffset": 208}, {"referenceID": 12, "context": "Borrowing the expression from Maclaurin et al. (2015), we refer to the model parameters customarily trained with backpropagation as elementary parameters, and to all other parameters as hyperparameters.", "startOffset": 30, "endOffset": 54}, {"referenceID": 12, "context": "through the entire history of parameter updates Maclaurin et al. (2015). Moreover, these methods make changes to the hyperparameter only once the elementary parameter training has ended.", "startOffset": 48, "endOffset": 72}, {"referenceID": 7, "context": "We add batch normalization (Ioffe and Szegedy, 2015) and adaptive learning rates (Kingma and Ba, 2015) to the process of hyperparameter training, which diminishes some of the problems of gradient-based hyperparameter optimization.", "startOffset": 27, "endOffset": 52}, {"referenceID": 8, "context": "We add batch normalization (Ioffe and Szegedy, 2015) and adaptive learning rates (Kingma and Ba, 2015) to the process of hyperparameter training, which diminishes some of the problems of gradient-based hyperparameter optimization.", "startOffset": 81, "endOffset": 102}, {"referenceID": 23, "context": "Injecting noise is particularly relevant for denoising autoencoders and related models (Vincent et al., 2010; Rasmus et al., 2015), where performance strongly depends on the level of noise.", "startOffset": 87, "endOffset": 130}, {"referenceID": 16, "context": "Injecting noise is particularly relevant for denoising autoencoders and related models (Vincent et al., 2010; Rasmus et al., 2015), where performance strongly depends on the level of noise.", "startOffset": 87, "endOffset": 130}, {"referenceID": 20, "context": "A third, often used, regularization method that involves a hyperparameter choice is dropout (Srivastava et al., 2014).", "startOffset": 92, "endOffset": 117}, {"referenceID": 24, "context": "Moreover, dropout can be seen as a form of multiplicative Gaussian noise (Wang and Manning, 2013).", "startOffset": 73, "endOffset": 97}, {"referenceID": 3, "context": "The most similar previously proposed model is the incremental gradient version of the hyperparameter update from (Chen and Hagan, 1999).", "startOffset": 113, "endOffset": 135}, {"referenceID": 6, "context": "A well justified closed form for the term \u2207\u03bb\u03b8 is available once the elementary gradient has converged (Foo et al., 2008), with the update of the form (4).", "startOffset": 102, "endOffset": 120}, {"referenceID": 12, "context": "Another approach to hypergradient computation is given in Maclaurin et al. (2015). There, the term \u2207\u03bb\u03b8T (T denoting the final iteration number) considers effect of the hyperparameter on the entire history of updates:", "startOffset": 58, "endOffset": 82}, {"referenceID": 7, "context": "However, arguably, batch normalization (Ioffe and Szegedy, 2015) is eliminating part of the problem, by making the Hessian closer to identity (Vatanen et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 22, "context": "However, arguably, batch normalization (Ioffe and Szegedy, 2015) is eliminating part of the problem, by making the Hessian closer to identity (Vatanen et al., 2013; Raiko et al., 2012), making the approximation more justified.", "startOffset": 142, "endOffset": 184}, {"referenceID": 15, "context": "However, arguably, batch normalization (Ioffe and Szegedy, 2015) is eliminating part of the problem, by making the Hessian closer to identity (Vatanen et al., 2013; Raiko et al., 2012), making the approximation more justified.", "startOffset": 142, "endOffset": 184}, {"referenceID": 5, "context": "Another step towards making even closer approximation are transformations that further whiten the hidden representations (Desjardins et al., 2015).", "startOffset": 121, "endOffset": 146}, {"referenceID": 14, "context": "It can be shown that the cost of computing this term scales comparably to backpropagation, due to the properties of R and L-operators (Pearlmutter, 1994; Schraudolph, 2002).", "startOffset": 134, "endOffset": 172}, {"referenceID": 17, "context": "It can be shown that the cost of computing this term scales comparably to backpropagation, due to the properties of R and L-operators (Pearlmutter, 1994; Schraudolph, 2002).", "startOffset": 134, "endOffset": 172}, {"referenceID": 4, "context": "We test the method on various configurations of multilayer perceptrons (MLPs) with ReLU activation functions (Dahl et al., 2013) trained on the MNIST (LeCun et al.", "startOffset": 109, "endOffset": 128}, {"referenceID": 11, "context": ", 2013) trained on the MNIST (LeCun et al., 1998) and SVHN (Netzer et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 13, "context": ", 1998) and SVHN (Netzer et al., 2011) data set.", "startOffset": 17, "endOffset": 38}, {"referenceID": 9, "context": "We also test the method on two convolutional architectures (CNNs) using CIFAR10 (Krizhevsky, 2009).", "startOffset": 80, "endOffset": 98}, {"referenceID": 19, "context": "The CNN architectures used were modified versions of model All-CNN-C from (Springenberg et al., 2014) and a baseline model from (Rasmus et al.", "startOffset": 74, "endOffset": 101}, {"referenceID": 16, "context": ", 2014) and a baseline model from (Rasmus et al., 2015), using ReLU and leakyReLU activations (Xu et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 25, "context": ", 2015), using ReLU and leakyReLU activations (Xu et al., 2015).", "startOffset": 46, "endOffset": 63}, {"referenceID": 21, "context": "The models were implemented with the Theano package (Team, 2016).", "startOffset": 52, "endOffset": 64}, {"referenceID": 8, "context": "To speed up elementary parameter training, we use an annealed ADAM learning rate schedule (Kingma and Ba, 2015) with a step size of 10\u22123 (MLPs) or 2 \u00b7 10\u22123 (CNNs).", "startOffset": 90, "endOffset": 111}], "year": 2016, "abstractText": "Hyperparameter selection generally relies on running multiple full training trials, with selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters during training of the model. Hyperparameters are adjusted so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST, SVHN and CIFAR-10, the resulting regularization levels are within the optimal regions. The additional computational cost depends on how frequently the hyperparameters are trained, but the tested scheme adds only 30% computational overhead regardless of the model size. Since the method is significantly less computationally demanding compared to similar gradientbased approaches to hyperparameter optimization, and consistently finds good hyperparameter values, it can be a useful tool for training neural network models.", "creator": "LaTeX with hyperref package"}}}