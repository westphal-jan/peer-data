{"id": "1511.06841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification", "abstract": "Connectionist temporal po\u00e7o classification (9mm CTC) noirish based morpholino supervised ailech sequence evangelisti training of neuenkirchen recurrent adrenocorticotropic neural bazel networks (RNNs) dirtbags has shown great chodo success in botto many nobel-prize machine friedrichshain learning areas 11.90 including casandra end - patronizing to - roadtrip end bengtsson speech ciriaco and handwritten double-crossed character recognition. agache For rynck the masacre CTC taqa training, however, 1.60 it nacorda is sliba required to poitevin unroll streetly the RNN by the length gasela of downregulate an input sequence. ikavian This unrolling wicker requires those a catalog lot spender of patino memory and hinders freewheel a small footprint l\u00e9onard implementation ashlyn of padmanabha online 125.80 learning or anthropic adaptation. 98.43 Furthermore, the louima length of pandeiro training sequences 336,000 is usually himalia not uniform, 29.71 which makes content-based parallel training oversupplied with longhurst multiple sequences inefficient on tarvis shared 0.020 memory newmont models superstition such as galga graphics domitia processing nohlgren units (GPUs ). ludford In this work, wigs we tajiri introduce culverhouse an forough expectation - mutherglobe.com maximization (EM) based careerism online jolivet CTC algorithm that enables mtoner unidirectional almond-shaped RNNs sonner to learn sequences that are west-northwest longer rbgh than the amount of skips unrolling. talbert The RNNs can friedeburg also be chigir trained to crispbread process corded an pechacek infinitely commotes long input sequence bookmark without stubbed pre - mcconnon segmentation maksutov or external escamilla reset. rodovia Moreover, the proposed approach louderback allows 97.53 efficient parallel training lateritic on GPUs. cheeta For evaluation, concord end - to - non-abelian end speech recognition uhrin examples are balcon presented wermiel on bevmark the subgrade Wall Street electro-diesel Journal (coffelt WSJ) corpus.", "histories": [["v1", "Sat, 21 Nov 2015 05:22:37 GMT  (510kb)", "https://arxiv.org/abs/1511.06841v1", null], ["v2", "Mon, 30 Nov 2015 19:10:36 GMT  (394kb)", "http://arxiv.org/abs/1511.06841v2", "Submitted to ICLR 2016"], ["v3", "Tue, 1 Dec 2015 12:09:14 GMT  (394kb)", "http://arxiv.org/abs/1511.06841v3", "Submitted to ICLR 2016"], ["v4", "Thu, 7 Jan 2016 20:52:42 GMT  (395kb)", "http://arxiv.org/abs/1511.06841v4", "Submitted to ICLR 2016"], ["v5", "Thu, 2 Feb 2017 13:42:49 GMT  (395kb)", "http://arxiv.org/abs/1511.06841v5", "Final version: Kyuyeon Hwang and Wonyong Sung, \"Sequence to Sequence Training of CTC-RNNs with Partial Windowing,\" Proceedings of The 33rd International Conference on Machine Learning, pp. 2178-2187, 2016. URL:this http URL"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1511.06841"}, "pdf": {"name": "1511.06841.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kyuyeon.hwang@gmail.com", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 84\n1v 5\n[ cs\n.L G\n] 2\nF eb"}, {"heading": "1 INTRODUCTION", "text": "Supervised sequence learning is a regression task where the objective is to learn a mapping function from the input sequence x to the corresponding output sequence z for all (x, z) \u2208 S with the given training set S, where x and z can have different lengths. When combined with recurrent neural networks (RNNs), supervised sequence learning has shown great success in many applications including machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al., 2008; Frinken et al., 2012). Although several attention-based models have been introduced recently, connectionist temporal classification (CTC) (Graves et al., 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al., 2014; Graves et al., 2008; Frinken et al., 2012).\nThe CTC based sequence training is usually applied to bidirectional RNNs (Graves & Schmidhuber, 2005), where both the past and the future information is considered for generating the output at each frame. However, the output of the bidirectional RNNs is available after all of the frames in the input sequence are fed into the RNNs because the future information is backward propagated from the end of the sequence. Therefore, the bidirectional RNNs are not suitable for realtime applications such as incremental speech recognition (Fink et al., 1998), that require low-latency output from the RNN. On the other hand, unidirectional RNNs only make use of the past information with\nsome performance sacrifice and are suitable for the low-latency applications. Moreover, the CTCtrained unidirectional RNNs do not need to be unrolled (or unfolded) at the test time. It is shown by Graves et al. (2012) that CTC can also be employed for sequence training of unidirectional RNNs on a phoneme recognition task. In this case, the unidirectional RNN also learns the suitable amount of the output delay that is required to accurately process the input sequence.\nFor the CTC training of both unidirectional and bidirectional RNNs, it is required to unroll the RNNs by the length of the input sequence. By unrolling an RNN N times, every activations of the neurons inside the network are replicated N times, which consumes a huge amount of memory especially when the sequence is very long. This hinders a small footprint implementation of online learning or adaptation. Also, this \u201cfull unrolling\u201d makes a parallel training with multiple sequences inefficient on shared memory models such as graphics processing units (GPUs), since the length of training sequences is usually not uniform, and thus a load imbalance problem occurs. For unidirectional RNNs, this problem can be addressed by concatenating sequences to make a very long stream of sequences, and training the RNNs with synchronized fixed-length unroll-windows over multiple training streams (Chen et al., 2014; Hwang & Sung, 2015). However, it is not straightforward to apply this approach to the CTC training, since the standard CTC algorithm requires full unrolling for the backward variable propagation, which starts from the end of the sequence.\nIn this paper, we propose an expectation-maximization (EM) based online CTC algorithm for sequence training of unidirectional RNNs. The algorithm allows training sequences to be longer than the amount of the network unroll. Moreover, it can be applied to infinitely long input streams with roughly segmented target sequences (e.g. only with the utterance boundaries and the corresponding transcriptions for training an end-to-end speech recognition RNN). Then, the resulting RNN can run continuously without pre-segmentation or external reset. Due to the fixed unroll amount, the proposed algorithm is suitable for online learning or adaptation systems with constrained hardware resource. Furthermore, the approach can directly be combined with the GPU based parallel RNN training algorithm described in Hwang & Sung (2015). For evaluation, we present examples of end-to-end speech recognition on the Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) with continuously running RNNs.1 Experimental results show that the proposed online CTC algorithm performs comparable to the almost fully unrolled CTC training even with the small unroll amount that is shorter than the average length of the sequences in the training set. Also, the reduced amount of unroll allows more parallel sequences to be trained concurrently with the same memory use, which results in greatly improved training speed on a GPU.\nThe paper is organized as follows. In Section 2, the standard CTC algorithm is explained. Section 3 contains the definition of the online sequence training problem and proposes the online CTC algorithm. In Section 4, the algorithm is extended for the continuously running RNNs, which is followed by its parallelization in Section 5. In Section 6, the proposed algorithm is evaluated with speech recognition examples. Concluding remarks follow in Section 7."}, {"heading": "2 CONNECTIONIST TEMPORAL CLASSIFICATION (CTC)", "text": "1Further experiments are performed on TIMIT (Garofolo et al., 1993) in Appendix B.\nThe CTC algorithm (Graves et al., 2006; 2012) considers the order of the output labels of RNNs with ignoring the alignments or timings by introducing an additional blank label, b. For the set of target labels, L, and its extended set with the additional CTC blank label, L\u2032 = L \u222a {b}, the path, \u03c0, is defined as a sequence over L\u2032, that is, \u03c0 \u2208 L\u2032T , where T is the length of the input sequence, x. Then, the output sequence, z \u2208 L\u2264T , is represented by z = F(\u03c0) with the sequence to sequence mapping functionF . F maps any path \u03c0 with the length T into the shorter sequence of the label, z, by first merging the consecutive same labels into one and then removing the blank labels. Therefore, any sequence of the raw RNN outputs with the length T can be decoded into the shorter labeling sequence, z, with ignoring timings. This enables the RNNs to learn the sequence mapping, z = G(x), where x is the input sequence and z is the corresponding target labeling for all (x, z) in the training set, S. More specifically, the gradient of the loss function L(x, z) = \u2212 ln p(z|x) is computed and fed to the RNN through the softmax layer (Bridle, 1990), of which the size is |L\u2032|.\nAs depicted in Figure 1, the CTC algorithm employs the forward-backward algorithm for computing the gradient of the loss function, L(x, z). Let z\u2032 be the sequence over L\u2032 with the length of 2|z|+1, where z\u2032u = b for odd u and z \u2032 u = zu/2 for even u. Then, the forward variable, \u03b1, and the backward variable, \u03b2, are initialized by\n\u03b1(1, u) =\n\n\n y1b if u = 1 y1z1 if u = 2 0 otherwise , \u03b2(T, u) = { 1 if u = |z\u2032|, |z\u2032| \u2212 1 0 otherwise , (1)\nwhere ytk is the softmax output of the label k \u2208 L \u2032 at time t. The variables are forward and backward propagated as\n\u03b1(t, u) = ytz\u2032 u\nu \u2211\ni=f(u)\n\u03b1(t\u2212 1, i) , \u03b2(t, u) =\ng(u) \u2211\ni=u\n\u03b2(t+ 1, i)yt+iz\u2032 i , (2)\nwhere\nf(u) =\n{\nu\u2212 1 if z\u2032u = b or z \u2032 u\u22122 = z \u2032 u u\u2212 2 otherwise , g(u) =\n{\nu+ 1 if z\u2032u = b or z \u2032 u+2 = z \u2032 u u+ 2 otherwise (3)\nwith the boundary conditions:\n\u03b1(t, 0) = 0, \u2200t , \u03b2(t, |z\u2032|+ 1) = 0, \u2200t. (4)\nThen, the error gradient with respect to the input of the softmax layer at time t, atk, is computed as\n\u2202L(x, z)\n\u2202atk = ytk \u2212\n1\np(z|x)\n\u2211\nu\u2208B(z,k)\n\u03b1(t, u)\u03b2(t, u), (5)\nwhere B(z, k) = {u : z\u2032u = k} and p(z|x) = \u03b1(T, |z \u2032|) + \u03b1(T, |z\u2032| \u2212 1)."}, {"heading": "3 ONLINE SEQUENCE TRAINING", "text": ""}, {"heading": "3.1 PROBLEM DEFINITION", "text": "Throughout the paper, the online sequence training problem is defined as follows.\n\u2022 The training set S consists of pairs of the input sequence x and the corresponding target sequence z, that is, (x, z) \u2208 S.\n\u2022 The estimation model M learns the general mapping z = G(x), where the training sequences (x, z) \u2208 S are sequentially given.\n\u2022 For each (x, z) \u2208 S and at time t, only the fraction of the input sequence up to time t, x1:t, and the entire target sequence, z, are given, where 1 \u2264 t \u2264 |x|. The length of the input sequence, |x|, is unknown except when t = |x|.\n\u2022 The parameters of the estimation modelM is updated in the manner of online learning, that is, they can be frequently updated even before seeing the entire input sequence x.\nThis online learning problem usually occurs in real world when a human learns a language from texts and the corresponding audio. For example, when watching movies with subtitles, we are given the entire target sequence (subtitle for the current utterance) and the input sequence (the corresponding audio) up to the current time, t. We cannot access the future audio and even do not know exactly when the utterance will end (at t = |x|).\nWhen RNNs are trained with the standard CTC algorithm, it is difficult to determine how much amount of unrolling is needed before the entire sequence x is given, since the length of x is unknown at time t < |x|. Also, it is not easy to learn the sequences that are longer than the unroll amount, which is often constrained by the hardware resources."}, {"heading": "3.2 OVERVIEW OF THE PROPOSED APPROACH", "text": "We propose an online CTC algorithm where the RNN can learn the sequences longer than the unroll amount, h. The algorithm is based on the truncated backpropagation through time (BPTT) algorithm (Werbos, 1990) with the forward step size of h\u2032 and the unroll amount of h, which is called BPTT(h; h\u2032), as proposed in Williams & Peng (1990). Algorithm 1 describes the BPTT(h; h\u2032) algorithm combined with the CTC loss, where T is the length of the training sequence, x\nAlgorithm 1 Online CTC training with BPTT(h; h\u2032) for a single sequence 1: procedure BPTT(h; h\u2032) 2: \u03c40 \u2190 0 3: n\u2190 1 4: while \u03c4n\u22121 < T do 5: \u03c4 \u2032n \u2190 max{1, nh\n\u2032 \u2212 h+ 1} \u22b2 Start index of unrolling 6: \u03c4n \u2190 min{nh\n\u2032, T } \u22b2 End index of unrolling 7: RNN forward activation from t = \u03c4n\u22121 + 1 to \u03c4n 8: CTC(h; h\u2032) error computation on the softmax output layer \u22b2 Algorithm 2 9: RNN backward error propagation from t = \u03c4n to \u03c4 \u2032n\n10: RNN gradient computation and weight update 11: n\u2190 n+ 1 12: end while 13: end procedure\nHowever, although BPTT(h; h\u2032) is designed for online training of RNNs, employing the standard CTC loss function requires full unrolling of the networks. Therefore, we propose the CTC(h; h\u2032) algorithm for computing the CTC loss in the online manner as in BPTT(h; h\u2032) as in Algorithm 2. The algorithm is also depicted in Figure 2 with the example in which the length of the sequence, T = |x|, is 2.5 times as long as the unroll amount.\nAlgorithm 2 CTC(h; h\u2032) error computation at the iteration n 1: procedure CTC(h; h\u2032) 2: \u03c4n\u22121 \u2190 (n\u2212 1)h \u2032\n3: \u03c4 \u2032n \u2190 max{1, nh \u2032 \u2212 h+ 1} 4: \u03c4 \u2032n+1 \u2190 max{1, (n+ 1)h \u2032 \u2212 h+ 1} 5: \u03c4n \u2190 min{nh \u2032, T } 6: if n = 1 then 7: Initialize the CTC forward variable, \u03b1, at t = 1 \u22b2 (1) 8: end if 9: CTC forward propagation of \u03b1 from t = \u03c4n\u22121 + 1 to \u03c4n \u22b2 (2) 10: if \u03c4n = T then \u22b2 CTC-TR in Section 3.3 11: Initialize the CTC-TR backward variable, \u03b2, at t = T \u22b2 (1) 12: CTC-TR backward propagation of \u03b2 from t = T to \u03c4 \u2032n \u22b2 (2) 13: CTC-TR error computation with \u03b1 and \u03b2 on t \u2208 [\u03c4 \u2032n, T ] \u22b2 (5) 14: else \u22b2 CTC-EM in Section 3.4 15: Initialize the CTC-EM backward variable, \u03b2\u03c4n , at t = \u03c4n \u22b2 (11) 16: CTC-EM backward propagation of \u03b2\u03c4n from t = \u03c4n to \u03c4 \u2032 n \u22b2 (2) 17: CTC-EM error computation with \u03b1 and \u03b2\u03c4n on t \u2208 [\u03c4 \u2032 n, \u03c4 \u2032 n+1 \u2212 1] \u22b2 (5) 18: Set error to zero on t \u2208 [\u03c4 \u2032n+1, \u03c4n] 19: end if 20: end procedure\nCTC(h; h\u2032) consists of two CTC algorithms. The first one is the truncated CTC (CTC-TR), which is basically the standard CTC algorithm applied at the last iteration with truncation. In the other iterations, the generalized EM based CTC algorithm (CTC-EM) is employed from t = max{1, nh\u2032\u2212 h+1} to max{0, (n+1)h\u2032\u2212h} with the modified backward variable, \u03b2\u03c4 . The CTC-TR and CTCEM algorithms are explained in Section 3.3 and Section 3.4, respectively. Note that simply setting h = 2h\u2032 works well in practice. In this setting, we denote the algorithm as CTC(2h\u2032; h\u2032)."}, {"heading": "3.3 CTC-TR: STANDARD CTC WITH TRUNCATION", "text": "With the standard CTC algorithm, it is not possible to compute the backward variables when \u03c4n < T , as the future information beyond \u03c4n cannot be accessed. Therefore, we only compute the CTC errors at the last iteration, where \u03c4n = T as in Algorithm 2. In this case, however, the gradients are only available in the unroll range. Since the backward propagation is truncated at the beginning of the unroll range, we call the CTC algorithm in this range as truncated CTC, or CTC-TR. Also, we call the range that is covered by the CTC-TR algorithm as the CTC-TR coverage.\nThe RNN can be trained only with CTC-TR if there are sufficient labels that occur within the CTCTR coverage. However, the CTC-TR coverage decreases by making the unroll amount smaller. Then, the percentage of the effective training frames, which actually generate the output errors, goes down, and the efficiency of training decreases. Also, the effective size of the training set gets smaller, which results in the loss of the generalization performance of the RNN. Therefore, for maintaining the training performance while reducing the unroll amount, it is critical to make use of the full training frames by employing the CTC-EM algorithm, which is described in Section 3.4."}, {"heading": "3.4 CTC-EM: ONLINE CTC WITH EXPECTATION-MAXIMIZATION (EM)", "text": "Assume that only the fraction of the input sequence, x1:\u03c4 , is available. Then, as shown in the Figure 3, there are |z|+1 possible partial labelings.2 Let z1:m be the subsequence of z with the first m labels. Also we define Z as the set that consists of these labeling sequences:\nZ = {z1:m : 0 \u2264 m \u2264 |z|}. (6)\nOne of the most simple approach for training the network under this condition is to choose the most likely partial alignment from Z and compute the standard CTC error by regarding the partial alignment as the ground truth labeling. For example, we can select z1:m\u2032 where m\u2032 = argmaxm \u03b1(\u03c4,m)\n2Although z1:m is not possible by the standard CTC formulation when m > \u03c4 , we can still say that z1:m is a possible labeling with a probability of zero without loss of generality.\nsince \u03b1(\u03c4,m) is a posterior probability p(z1:m|x1:\u03c4 ,w(n)) with the current network parameterw(n). This is a well-known hard-EM approach. This simple idea can easily be extended to the more sophisticated soft-EM approach as follows. First, select one of the partial labelings in Z with the probability p(z1:m|Z,x1:\u03c4 ,w(n)) estimated by the RNN with current parameters (E-step). Then, maximize the probability of that labeling by adjusting the parameters (M-step).\nThis optimization problem is readily reduced into the generalized EM algorithm. Specifically, the expectation step is represented as\nQ\u03c4 (w|x, z,w (n)) = E\nz1:m|Z,x1:\u03c4 ,w(n) [ln p(z1:m|x1:\u03c4 ,w)] (7)\n=\n|z| \u2211\nm=0\np(z1:m|Z,x1:\u03c4 ,w (n)) ln p(z1:m|x1:\u03c4 ,w), (8)\nwhere w(n) is the set of the network parameters at the current iteration, n. In the maximization step of the generalized EM approach, we try to maximize Q\u03c4 by finding new parameters w(n+1) that satisfies Q\u03c4 (w(n+1)|x, z,w(n)) \u2265 Q\u03c4 (w(n)|x, z,w(n)). As proved in Appendix A, this is equivalent to the optimization problem where the objective is to minimize the loss function defined as L\u03c4 (x, z) = \u2212 ln p(Z|x1:\u03c4 ). Then, the gradient of the loss function with respect to the input of the softmax layer is\n\u2202L\u03c4 (x, z)\n\u2202atk = ytk \u2212\n1\np(Z|x1:\u03c4 )\n\u2211\nu\u2208B(z,k)\n\u03b1(t, u)\u03b2\u03c4 (t, u), (9)\nwhere p(Z|x1:\u03c4 ) can be computed by\np(Z|x1:\u03c4 ) =\n|z\u2032| \u2211\nu=1\n\u03b1(\u03c4, u) (10)\nand the backward variable, \u03b2\u03c4 (t, u), is initialized as\n\u03b2\u03c4 (\u03c4, u) = 1, \u2200u. (11)\nThe new backward variable is backward propagated using the same recursion in (2), and the error gradients are computed with (5) as in the standard CTC algorithm. See Appendix A for the derivation of the above equations."}, {"heading": "4 TRAINING CONTINUOUSLY RUNNING RNNS", "text": "In this section, the proposed online CTC algorithm in Section 3 is extended for training infinitely long streams. The training stream can be naturally very long with the target sequence boundaries, or can be generated by concatenating training sequences in a certain order. When trained on this training stream without external reset of the RNN at the sequence boundaries, the resulting RNN can also continuously process infinitely long input streams without pre-segmentation or external\nreset. This property is useful for realtime speech recognition or keyword spotting (spoken term detection) systems since we can remove the frontend voice activity detector (Sohn et al., 1999) for detecting and pre-segmenting utterances.\nThe CTC(h; h\u2032) algorithm can directly be applied to the infinitely long training streams as shown in Figure 4. When the sequence boundaries are reached during the forward activation, we perform CTC-TR, initialize the forward variable, and process the next sequence with some frame offset. Also, care should be taken on the transition of CTC labels at the boundary. Assume that the last label of the sequence n and the first label of the sequence n + 1 are the same. Then, a CTC blank label should be inserted between two sequences since the same labels that occur consecutively in the decoding path are folded into one label. In practice, this folding can easily be prevented by forcing the blank label at the first frame of each sequence by modifying the initialization of the forward variable as follows:\n\u03b1c(1, u) =\n{\ny1b if u = 1 0 otherwise , (12)\nwhere the subscript c indicates the continuous CTC training."}, {"heading": "5 PARALLEL TRAINING", "text": "In a massively parallel shared memory model such as a GPU, efficient parallel training is achieved by making use of the memory hierarchy. For example, computing multiple frames together reduces the number of read operations of the network parameters from the slow off-chip memory by temporarily storing them on the on-chip cache memory and reuse them multiple times. For training RNNs on a GPU, this parallelism can be explored by employing multiple training sequences concurrently (Hwang & Sung, 2015).\nThe continuous CTC(h; h\u2032) algorithm in Section 4 can be directly extended for parallel training with multiple streams. Since the forward step size and the unroll amount is fixed, the RNN forward, backward, gradient computation, and weight update steps can be synchronized over multiple training streams. Thus, the GPU based parallelization approach in Hwang & Sung (2015) can be employed for the RNN training. Although the computations in the CTC(h; h\u2032) algorithm are relatively fewer than those of the RNN, further speed-up can be achieved by parallelizing the CTC algorithm similarly."}, {"heading": "6 EXPERIMENTS", "text": ""}, {"heading": "6.1 END-TO-END SPEECH RECOGNITION WITH RNNS", "text": "For the evaluation of the proposed approach, we present examples of character-level speech recognition with end-to-end trained RNNs without external language models. The speech recognition\nRNN is similar to the one in Graves & Jaitly (2014) except that our model employs unidirectional long short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997) trained with the online CTC algorithm on the continuous stream of speech, instead of the bidirectional LSTM network with the sentence-wise CTC training.\nSpecifically, the experiments are performed with the deep unidirectional LSTM network with 3 LSTM layers, where each layer has 768 LSTM cells. The output layer is a 31-dimensional softmax layer. Each unit of the softmax layer represents one of the posterior probabilities of 26 alphabet characters, two special characters (. and \u2019), a whitespace character, the end of sentence (EOS) symbol, and the CTC blank label. The input of the network is a 123-dimensional vector that consists of a 40-dimensional log Mel-frequency filterbank feature vector plus energy, and their delta and delta-delta values. The feature vectors are extracted from the speech waveform in every 10 ms with 25 ms Hamming window using HTK (Young et al., 1997). Before being fed into the RNN, feature vectors are element-wisely normalized to the zero mean and the unit standard deviation, where the statistics are extracted from the training set."}, {"heading": "6.2 WALL STREET JOURNAL (WSJ) CORPUS", "text": "The experiments are performed on the Wall Street Journal (WSJ) (Paul & Baker, 1992) corpus. For the training, the subset of the WSJ SI-284 set is used, where only the utterances with non-verbalized punctuations (NVPs) are included, resulting in about 71 hours of utterances. The histogram of the length of the sequences in the training set is shown in Figure 5. Note that the average length of the sequences is 772.5 frames. If we unroll the network over N frames, the sequences longer than N frames will not be fully covered by CTC-TR.\nIn Figure 6, the CTC-TR coverage is further analyzed with respect to the length of the sequence and the unroll amount. When the stream of sequences are trained with the continuos CTC algorithm, the\nCTC-TR coverage varies depending on the frame offsets of CTC(h; h\u2032). The average coverage is calculated assuming that the offset is uniformly distributed. If the probability that a certain frame is included in the coverage is greater than zero, then the frame is included in the maximum coverage. For the experiments, we only consider CTC(2h\u2032; h\u2032), that is, the unroll amount is twice as much as the forward step size. Then, unrolling the network 1,024 times results in the CTC-TR coverage of 79.48 % on average and 95.69 % at maximum. On the other hand, when the unroll amount is 512, CTC-TR only covers 48.16 % on average and 63.27 % at maximum. Note that the full coverage is achieved when CTC-TR is combined with CTC-EM.\nThe WSJ Nov\u201993 20K development set and the WSJ Nov\u201992 20K evaluation set are used as the development (validation) set and the test (evaluation) set, respectively. For the final evaluation of the network after training, a single test stream is used that is generated by concatenating all of the 333 utterances in the test set."}, {"heading": "6.3 TRAINING PROCEDURE", "text": "The networks are trained on a GPU as in Section 5 with the memory usage constraint. To maintain the memory usage same while changing the unroll amount, we fixed the total amount of unrolling over multiple training streams to 16,384. For example, the number of parallel streams become 8 with the unroll amount of 2,048 and 32 with 512 times of unrolling. The total amount of GPU memory usage is about 9.5 GiB in our implementation based on Hwang & Sung (2015).\nThe performance evaluation of the network is performed at every 10,485,760 training frames (i.e. N continuous training streams with the length of 10, 485, 760/N each) in terms of word error rate (WER) on the 128 parallel development streams of which length is 16,384 each. For this intermediate evaluation, best path decoding (Graves et al., 2006) is employed for fast computation.\nFor the online update of the RNN parameters, the stochastic gradient descent (SGD) method is employed and accelerated by the Nesterov momentum of 0.9 (Nesterov, 1983; Bengio et al., 2013). Also, the network is annealed by combining the early stopping technique as follows. If the network performance based on the intermediate evaluation is not improved for 11 consecutive times (10 times of retry), the learning rate is reduced by the factor of 10 and the training is resumed from the second best network. The training starts from the learning rate of 10\u22125 and finishes when the learning rate becomes less than 10\u22127.\nThe pre-trained network is used for CTC-TR and CTC-EM combined training because the expectation step of CTC-EM requires the RNN to align the target labels in a certain level. The pre-trained networks are obtained by early stopping the CTC-TR training of the networks when the performance is not improved during 6 consecutive intermediate evaluations using the learning rate of 10\u22125. For the CTC-TR and CTC-EM combined training with the unroll amount of 512, 1,024, and 2,048, the training starts from the pre-trained network that is trained with the same amount of unrolling. Otherwise, for the combined training with the unrolling less than 512 times, we use the pre-trained network with the unroll amount of 512."}, {"heading": "6.4 EVALUATION", "text": "Figure 7 shows the convergence curves in terms of WER on the development set with various unroll amounts and training algorithms, where the unroll amount is twice the forward step size. The convergence speed of the CTC-TR only training decreases when the unroll amount becomes smaller. This is because the percentage of the effective training frames become smaller due to the reduced CTC-TR coverage. Also, it can be observed that the performance of the CTC-TR only trained network with 512 times of unrolling converges to the worse WER than those of the other networks due to the reduced size of the effective training set. On the other hand, the convergence curves of the CTC-TR and CTC-EM combined training with the unroll amounts of 256 and 512 are similar to that of the CTC-TR only training with 2,048 times of unrolling. Considering that the average sequence length of the training set is 772.5 frames, the results are quite encouraging.\nThe evidence of the similar convergence curves with the different unroll amounts implies that the training can be accelerated under the memory usage constraint by employing more parallel training streams with less unrolling. To examine how much speed-up can be achieved on a GPU, further experiments are performed as in Table 1. The training speed is measured on the system equipped\nwith NVIDIA GeForce Titan X GPU and Intel Xeon E5-2620 CPU. For the final character error rate (CER) and WER report on the test set, the output of the RNN is decoded by the CTC beam search (Graves & Jaitly, 2014) without language models. As shown in the table, we can achieve a great amount of speedup without sacrificing much WERs. Also, it is possible to train a network with only 64 times of unrolling, which corresponds to 640 ms window, at the cost of 4.5% relative WER. See Appendix B for further experiments on TIMIT (Garofolo et al., 1993)."}, {"heading": "7 CONCLUDING REMARKS", "text": "Throughout the paper, the online CTC(h; h\u2032) algorithm is proposed for online sequence training of unidirectional RNNs. The algorithm consists of CTC-TR and CTC-EM. CTC-TR is the standard CTC algorithm with truncation and CTC-EM is the generalized EM based algorithm that covers the training frames that CTC-TR cannot be applied. The proposed algorithm requires the unroll amount less than the length of the training sequence and is suitable for small footprint online learning systems or massively parallel implementation on a shared memory model such as a GPU. Also, the online CTC algorithm is extended for training continuously running RNNs without external reset, and evaluated in the TIMIT experiments with a continuous input speech. On the WSJ corpus, the experimental results indicate that when the memory capacity is constrained, the proposed approach achieves significant speed-up on a GPU without sacrificing the performance of the resulting RNN much. We expect that further acceleration of training will be possible with lower performance loss when different unroll amounts are used in the pre-training, main training, and annealing stages."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2015R1A2A1A10056051)."}, {"heading": "A DERIVATION OF THE CTC-EM EQUATIONS", "text": "In the maximization step, the objective is to obtain the derivative of Q\u03c4 (w|x, z,w(n)) with respect to the input of the softmax layer, atk, at time t. We first differentiate Q\u03c4 with respect to y t k at w = w(n):\n\u2202Q\u03c4 (w|x, z,w (n))\n\u2202ytk\n\u2223 \u2223 \u2223 \u2223\nw=w(n) =\n|z| \u2211\nm=0\np(z1:m|Z,x1:\u03c4 ,w (n))\n\u2202 ln p(z1:m|x1:\u03c4 ,w (n))\n\u2202ytk . (13)\nWith Bayes\u2019 rule, we obtain\np(z1:m|Z,x1:\u03c4 ,w (n)) =\np(z1:m, Z|x1:\u03c4 ,w (n))\np(Z|x1:\u03c4 ,w(n)) =\np(z1:m|x1:\u03c4 ,w (n))\np(Z|x1:\u03c4 ,w(n)) , (14)\nand with simple calculus,\n\u2202 ln p(z1:m|x1:\u03c4 ,w (n))\n\u2202ytk =\n1\np(z1:m|x1:\u03c4 ,w(n))\n\u2202p(z1:m|x1:\u03c4 ,w (n))\n\u2202ytk . (15)\nThen, (13) becomes\n\u2202Q\u03c4 (w|x, z,w (n))\n\u2202ytk\n\u2223 \u2223 \u2223 \u2223\nw=w(n) =\n1\np(Z|x1:\u03c4 ,w(n))\n|z| \u2211\nm=0\n\u2202p(z1:m|x1:\u03c4 ,w (n))\n\u2202ytk (16)\n= 1\np(Z|x1:\u03c4 ,w(n))\n\u2202p(Z|x1:\u03c4 ,w (n))\n\u2202ytk . (17)\nIf we define the loss function to be minimized as\nL\u03c4 (x, z) = \u2212 ln p(Z|x1:\u03c4 ), (18)\nthen its derivative equals to (17) with the opposite sign:\n\u2202L\u03c4 (x, z)\n\u2202ytk = \u2212\n\u2202Q\u03c4(w|x, z,w (n))\n\u2202ytk\n\u2223 \u2223 \u2223 \u2223\nw=w(n) . (19)\nFrom now on, we drop w(n) without loss of generality. Let\n\u03b2\u03c4,m(\u03c4, u) =\n{\n1 if u = 2m, 2m+ 1 0 otherwise . (20)\nFollowing the standard CTC forward-backward equations in Graves et al. (2012),\np(z1:m|x1:\u03c4 ) =\n|z\u2032| \u2211\nu=1\n\u03b1(t, u)\u03b2\u03c4,m(t, u) (21)\n\u2202p(z1:m|x1:\u03c4 )\n\u2202ytk =\n1\nytk\n\u2211\nu\u2208B(z,k)\n\u03b1(t, u)\u03b2\u03c4,m(t, u). (22)\nFrom (21) and (22), p(Z|x1:\u03c4 ) and its derivative become\np(Z|x1:\u03c4 ) =\n|z| \u2211\nm=0\np(z1:m|x1:\u03c4 ) =\n|z\u2032| \u2211\nu=1\n\u03b1(t, u)\u03b2\u03c4 (t, u) (23)\n\u2202p(Z|x1:\u03c4 )\n\u2202ytk =\n|z| \u2211\nm=0\n\u2202p(z1:m|x1:\u03c4 )\n\u2202ytk =\n1\nytk\n\u2211\nu\u2208B(z,k)\n\u03b1(t, u)\u03b2\u03c4 (t, u), (24)\nwhere the new backward variable for p(Z|x1:\u03c4 ) is\n\u03b2\u03c4 (t, u) =\n|z| \u2211\nm=0\n\u03b2\u03c4,m(t, u), (25)\nwhich results in the simple initialization as\n\u03b2\u03c4 (\u03c4, u) = 1, \u2200u. (26)\nThen, the error gradients become\n\u2202L\u03c4 (x, z)\n\u2202ytk = \u2212\n1\np(Z|x1:\u03c4 )\n1\nytk\n\u2211\nu\u2208B(z,k)\n\u03b1(t, u)\u03b2\u03c4 (t, u) (27)\n\u2202L\u03c4 (x, z)\n\u2202atk = ytk \u2212\n1\np(Z|x1:\u03c4 )\n\u2211\nu\u2208B(z,k)\n\u03b1(t, u)\u03b2\u03c4 (t, u), (28)\nwhere\np(Z|x1:\u03c4 ) =\n|z\u2032| \u2211\nu=1\n\u03b1(\u03c4, u). (29)"}, {"heading": "B PHONEME RECOGNITION ON TIMIT", "text": "B.1 TIMIT CORPUS\nThe TIMIT corpus (Garofolo et al., 1993) contains American English recordings of 630 speakers from 8 major dialect regions in the United States. The training set contains about 3.1 hours of 3,696 utterances from 462 speakers after removing the SA recordings, in which only two sentences are spoken by multiple speakers. Figure 8 shows the histogram of the length of the training sequences, where the feature frames are extracted with the 10 ms period. The average length of the training sequences is 304 frames. We use the core test set with 192 utterances as the test set. The development set contains the remaining 1,152 utterances that are obtained by excluding the core test set from the complete test set. The corpus also includes the full phonetic transcriptions.\nB.2 NETWORK STRUCTURE\nThe network structure is a deep unidirectional LSTM RNN with 3 LSTM hidden layers, where each LSTM layer has 512 cells. The input is the same log Mel-frequency filterbank feature as in the WSJ experiments. The training procedure is also similar. The original TIMIT transcriptions are based on 61 phonetic labels. Accordingly, the RNN output is a 62-dimensional vector that consists of the probabilities of the original 61 phonemes and the extra CTC label. However, after decoding, they are mapped to 39 phonemes for evaluation as in Lee & Hon (1989).\nB.3 TRAINING PROCEDURE\nFor the experiments, the continuous CTC(2h\u2032; h\u2032) algorithm is employed so that the resulting RNN can run continuously on a infinitely long stream of the input speech. The networks are pre-trained with ADADELTA (Zeiler, 2012), where the local learning rates are adaptively adjusted using the statistics of the recent gradient values. Before the online CTC training with the unroll amount greater than of equal to 512, the pre-training is performed for the 8 M (8\u00d7 220) training frames with the unroll amount of 2,048, the learning rate of 10\u22125, the Nesterov momentum of 0.9, and the RMS decay rate of 0.99 for ADADELTA. On the other hand, we pre-trained the network with 12 M frames for the subsequent CTC training with less than 512 unroll steps. Unlike in the WSJ experiments, it is observed that applying the standard SGD method at the beginning often fails to initiate the training. We consider this is because the gradient computed by the SGD method is initially not noisy enough to help the parameters escape from the initial saddle point.\nAfter the pre-training, the standard SGD is applied with the Nesterov momentum of 0.9. The training starts with the learning rate of 10\u22124. The intermediate evaluations are performed at every 2 M (2\u00d7 220) training frames on the development set with the best path decoding. If the phoneme error rate (PER) fails to improve during 6 consecutive evaluations, the learning rate decreases by the factor of 2 and the parameters are restored to those of the second best network. The training finishes when the learning rate becomes less than 10\u22126.\nThe network is regularized with dropout (Hinton et al., 2012) in both the pre-training and the main training stages following the approach in Zaremba et al. (2014), that is, dropout is only applied on the non-recurrent connections. The dropout rate is fixed to 0.5 throughout the experiments.\nB.4 EVALUATION\nThe networks are evaluated on the very long test stream that is obtained by concatenating the entire test sequences. For the evaluation, the network output is decoded by the CTC beam search. The experiments are repeated 4 times and the mean and standard deviation estimates of PERs are reported based on the reduced 39-phoneme set.\nThe RNNs are unrolled 64, 128, 256, 512, 1,024, and 2,048 times. As shown in Table 2, the various unroll amounts make little difference to the final PERs on the test set. When the RNN is unrolled only 128 times, which is less than the average length of training sequences, the best PER of 20.73\u00b10.40% is obtained. On the other hand, the training with the unroll amount of 2,048 results in slightly degraded performance since it becomes harder for RNNs to catch the dependencies between the input and output sequences due to the noisy input frames from the consecutive sequences.\nThe performance of the proposed online CTC algorithm is compared with the other models in Table 3. The other models employ early stopping to prevent overfitting and add weight noise while training for regularization. The bidirectional attention-based model in Chorowski et al. (2015) shows 17.6% PER with utterance-wise decoding. However, the PER increases to about 20% with the long test sequences that are generated by concatenating 11 utterances. On the other hand, our CTC(128; 64)-trained unidirectional RNNs show 20.73\u00b10.40% PER with a very long test stream that is made by concatenating the entire 192 test utterances. Note that, unlike the CTC-trained unidirectional RNNs, the bidirectional models require unrolling in test time and have to listen the entire speech before generating outputs. Therefore, the proposed unidirectional RNN models are more suitable for realtime low-latency speech recognition systems without sacrificing much performance."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio", "Yoshua", "Boulanger-Lewandowski", "Nicolas", "Pascanu", "Razvan"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["Bridle", "John S"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle and S.,? \\Q1990\\E", "shortCiteRegEx": "Bridle and S.", "year": 1990}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch", "author": ["Chen", "Xie", "Wang", "Yongqiang", "Liu", "Xunying", "Gales", "Mark JF", "Woodland", "Philip C"], "venue": "In Proc. Interspeech,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1506.07503,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Incremental speech recognition for multimodal interfaces", "author": ["Fink", "Gernot", "Schillo", "Christoph", "Kummert", "Franz", "Sagerer", "Gerhard"], "venue": "In Industrial Electronics Society,", "citeRegEx": "Fink et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Fink et al\\.", "year": 1998}, {"title": "A novel word spotting method based on recurrent neural networks. Pattern Analysis and Machine Intelligence", "author": ["Frinken", "Volkmar", "Fischer", "Andreas", "R Manmatha", "Bunke", "Horst"], "venue": "IEEE Transactions on,", "citeRegEx": "Frinken et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Frinken et al\\.", "year": 2012}, {"title": "DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1", "author": ["Garofolo", "John S", "Lamel", "Lori F", "Fisher", "William M", "Fiscus", "Jonathon G", "Pallett", "David S"], "venue": "NASA STI/Recon Technical Report N,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen", "Fern\u00e1ndez", "Santiago"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex", "year": 2012}, {"title": "Deepspeech: Scaling up endto-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hwang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2015}, {"title": "Speaker-independent phone recognition using hidden Markov models", "author": ["Lee", "Kai-Fu", "Hon", "Hsiao-Wuen"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "Lee et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1989}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)", "author": ["Nesterov", "Yurii"], "venue": "In Doklady AN SSSR,", "citeRegEx": "Nesterov and Yurii.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["Paul", "Douglas B", "Baker", "Janet M"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Paul et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Paul et al\\.", "year": 1992}, {"title": "A statistical model-based voice activity detection", "author": ["Sohn", "Jongseo", "Kim", "Nam Soo", "Sung", "Wonyong"], "venue": "Signal Processing Letters, IEEE,", "citeRegEx": "Sohn et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 1999}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Werbos", "Paul J"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos and J.,? \\Q1990\\E", "shortCiteRegEx": "Werbos and J.", "year": 1990}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Williams", "Ronald J", "Peng", "Jing"], "venue": "Neural Computation,", "citeRegEx": "Williams et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1990}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "When combined with recurrent neural networks (RNNs), supervised sequence learning has shown great success in many applications including machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014), speech recognition (Graves et al.", "startOffset": 157, "endOffset": 222}, {"referenceID": 25, "context": "When combined with recurrent neural networks (RNNs), supervised sequence learning has shown great success in many applications including machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014), speech recognition (Graves et al.", "startOffset": 157, "endOffset": 222}, {"referenceID": 6, "context": "When combined with recurrent neural networks (RNNs), supervised sequence learning has shown great success in many applications including machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014), speech recognition (Graves et al.", "startOffset": 157, "endOffset": 222}, {"referenceID": 15, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 17, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 1, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 7, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 4, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 14, "context": ", 2015), and handwritten character recognition (Graves et al., 2008; Frinken et al., 2012).", "startOffset": 47, "endOffset": 90}, {"referenceID": 9, "context": ", 2015), and handwritten character recognition (Graves et al., 2008; Frinken et al., 2012).", "startOffset": 47, "endOffset": 90}, {"referenceID": 13, "context": "Although several attention-based models have been introduced recently, connectionist temporal classification (CTC) (Graves et al., 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 17, "context": ", 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al., 2014; Graves et al., 2008; Frinken et al., 2012).", "startOffset": 133, "endOffset": 220}, {"referenceID": 14, "context": ", 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al., 2014; Graves et al., 2008; Frinken et al., 2012).", "startOffset": 133, "endOffset": 220}, {"referenceID": 9, "context": ", 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al., 2014; Graves et al., 2008; Frinken et al., 2012).", "startOffset": 133, "endOffset": 220}, {"referenceID": 8, "context": "Therefore, the bidirectional RNNs are not suitable for realtime applications such as incremental speech recognition (Fink et al., 1998), that require low-latency output from the RNN.", "startOffset": 116, "endOffset": 135}, {"referenceID": 5, "context": "For unidirectional RNNs, this problem can be addressed by concatenating sequences to make a very long stream of sequences, and training the RNNs with synchronized fixed-length unroll-windows over multiple training streams (Chen et al., 2014; Hwang & Sung, 2015).", "startOffset": 222, "endOffset": 261}, {"referenceID": 10, "context": "It is shown by Graves et al. (2012) that CTC can also be employed for sequence training of unidirectional RNNs on a phoneme recognition task.", "startOffset": 15, "endOffset": 36}, {"referenceID": 5, "context": "For unidirectional RNNs, this problem can be addressed by concatenating sequences to make a very long stream of sequences, and training the RNNs with synchronized fixed-length unroll-windows over multiple training streams (Chen et al., 2014; Hwang & Sung, 2015). However, it is not straightforward to apply this approach to the CTC training, since the standard CTC algorithm requires full unrolling for the backward variable propagation, which starts from the end of the sequence. In this paper, we propose an expectation-maximization (EM) based online CTC algorithm for sequence training of unidirectional RNNs. The algorithm allows training sequences to be longer than the amount of the network unroll. Moreover, it can be applied to infinitely long input streams with roughly segmented target sequences (e.g. only with the utterance boundaries and the corresponding transcriptions for training an end-to-end speech recognition RNN). Then, the resulting RNN can run continuously without pre-segmentation or external reset. Due to the fixed unroll amount, the proposed algorithm is suitable for online learning or adaptation systems with constrained hardware resource. Furthermore, the approach can directly be combined with the GPU based parallel RNN training algorithm described in Hwang & Sung (2015). For evaluation, we present examples of end-to-end speech recognition on the Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) with continuously running RNNs.", "startOffset": 223, "endOffset": 1305}, {"referenceID": 10, "context": "Further experiments are performed on TIMIT (Garofolo et al., 1993) in Appendix B.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": "The CTC algorithm (Graves et al., 2006; 2012) considers the order of the output labels of RNNs with ignoring the alignments or timings by introducing an additional blank label, b.", "startOffset": 18, "endOffset": 45}, {"referenceID": 24, "context": "This property is useful for realtime speech recognition or keyword spotting (spoken term detection) systems since we can remove the frontend voice activity detector (Sohn et al., 1999) for detecting and pre-segmenting utterances.", "startOffset": 165, "endOffset": 184}, {"referenceID": 13, "context": "For this intermediate evaluation, best path decoding (Graves et al., 2006) is employed for fast computation.", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": "9 (Nesterov, 1983; Bengio et al., 2013).", "startOffset": 2, "endOffset": 39}, {"referenceID": 10, "context": "See Appendix B for further experiments on TIMIT (Garofolo et al., 1993).", "startOffset": 48, "endOffset": 71}], "year": 2017, "abstractText": "Connectionist temporal classification (CTC) based supervised sequence training of recurrent neural networks (RNNs) has shown great success in many machine learning areas including end-to-end speech and handwritten character recognition. For the CTC training, however, it is required to unroll (or unfold) the RNN by the length of an input sequence. This unrolling requires a lot of memory and hinders a small footprint implementation of online learning or adaptation. Furthermore, the length of training sequences is usually not uniform, which makes parallel training with multiple sequences inefficient on shared memory models such as graphics processing units (GPUs). In this work, we introduce an expectation-maximization (EM) based online CTC algorithm that enables unidirectional RNNs to learn sequences that are longer than the amount of unrolling. The RNNs can also be trained to process an infinitely long input sequence without pre-segmentation or external reset. Moreover, the proposed approach allows efficient parallel training on GPUs. For evaluation, phoneme recognition and end-to-end speech recognition examples are presented on the TIMIT and Wall Street Journal (WSJ) corpora, respectively. Our online model achieves 20.7% phoneme error rate (PER) on the very long input sequence that is generated by concatenating all 192 utterances in the TIMIT core test set. On WSJ, a network can be trained with only 64 times of unrolling while sacrificing 4.5% relative word error rate (WER).", "creator": "LaTeX with hyperref package"}}}