{"id": "1511.08952", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2015", "title": "Bootstrapping Ternary Relation Extractors", "abstract": "changle Binary relation schiavone extraction methods have creswell been vbac widely studied plainsmen in recent years. 115.86 However, few beru methods have erigeron been mayapur developed farinas for higher n - ary relation classmates.com extraction. vsm One limiting factor diuresis is the mun effort 386-member required dialtone to generate mutually training data. dagsavisen For defla binary lif relations, 25-centimeter one zhelyu only has 20.08 to provide a nosan few dozen pairs peromyscus of entities kahsr per relation, as gunns training nvc data. cookridge For ternary relations (intercalation n = bandala 3 ), each elisavet training outboxed instance is ellenoff a triplet of 100.28 entities, d'annunzio placing a greater cognitive punti load breiman on thailand.in people. erichthonius For dubber example, many vtel people know that deniz Google aiaw acquired Youtube appalachian but a370 not the dollar amount or navair the gabbai date 168th of the acquisition and lodgepole many people incapacitation know that 3,012 Hillary Clinton is almagro married to jaish-e-mohammed Bill lepton Clinton peregrini by adamkus not the location or date of their northmore wedding. This timofeyevich makes chaku higher vkhutemas n - nary training data 2002-2010 generation sikhism a 1,000-yard time elise consuming 300-ton exercise maraveh in abasto searching piffer the vient Web. ashvin We foibles present monroes a resource for training vermaak ternary sapphires relation latinoamericanos extractors. This tosoh was bologoye generated screwvala using kse a recreationists minimally villarejo supervised matre yet bases effective phuoc approach. We wschodnia present statistics on the geri size and coccoid the kuijken quality stingingly of changquan the dataset.", "histories": [["v1", "Sun, 29 Nov 2015 00:49:13 GMT  (54kb,D)", "http://arxiv.org/abs/1511.08952v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ndapandula nakashole"], "accepted": false, "id": "1511.08952"}, "pdf": {"name": "1511.08952.pdf", "metadata": {"source": "CRF", "title": "Bootstrapping Ternary Relation Extractors", "authors": ["Ndapandula Nakashole"], "emails": ["ndapa@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Developing techniques for higher-nary relation extraction is a natural next step after the wellstudied case of binary relations (Auer et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Mitchell et al., 2015). In the literature, prominent binary relation extraction methods are mostly semi-supervised (Suchanek et al., 2009; Carlson et al., 2010) or unsupervised (Mausam et al., 2012; Fader et al., 2011a). Semisupervised methods tend to have higher precision than unsupervised methods and therefore are commonly used to populate knowledge bases of facts\n(Nakashole et al., 2011; Mitchell et al., 2015). In such settings, relations of interest are predefined, i.e, company acquisitions or protein-protein interactions. However, in semi-supervised approaches, one needs to provide seed examples for each relation to bootstrap the extractor. This can be expensive, especially if there are many relations of interest. For ternary relations, hand specifying training instances per relation requires even more time since each training instance is a triplet of three entities as opposed to a pair of entities for binary relations. Most people know that Google acquired Youtube but not the dollar amount or the date of the acquisition, and most people know that Hillary Clinton is married to Bill Clinton by not the location or date of their wedding. This makes ternary training data generation a time consuming exercise in searching the Web.\nIn this paper we present a resource for training ternary extractors. The resource was generated using a minimally supervised yet high precision method. Our method leverages a very common language construction: prepositional phrases (PPs). PPs such as \u201cin X\u201d, \u201cat Y\u201d, and \u201cfor Z\u201d express details about the where, when, and why of binary relation instances. This makes PPs wellsuited to extending binary relations by one more argument, extending them to ternary relations.\nConsider the following occurrences of 5-item sequences of the form: N1, V, N2, P, N3; where the Ns are noun phrases, V is a verb and P is a preposition.\n(1) Mercedes-Benz bought Chrysler for $40 billion (2) CBS bought WCCO from General Mills (3) Joe Lieberman endorsed McCain for president (4) The New Yorker endorsed Obama over Romney\nIn a large Web extracted corpus, one sees many 5-item sequences similar to each of the 6 types shown above. The verbs and prepo-\nar X\niv :1\n51 1.\n08 95\n2v 1\n[ cs\n.C L\n] 2\n9 N\nov 2\n01 5\nsitions do not change but the arguments N1-3 change. From a large volume of such occurrences, we can learn templates for populating ternary relations. For example, from many occurrences of tuples of type (1), we can generate the template: <organization>bought <organization>for <dollar amount>. We go further by having labels for all three argument placeholders. For this particular template the labels are: AcquisationEventAcquirer, AcquisationEventAcquired, AcquisationEventAmount for the N0, N1, N3, argument placeholders respectively. Similarly, from tuples of type (3), we can generate the template: <person>endorsed <politician>for <political office>. And the corresponding argument labels are: EndorsementEventEndorser, EndorsementEventEndorsed, EndorsementEventOffice, for the N0, N1, N3 argument placeholders respectively. A triplet of argument labels is considered to be a ternary relation. And matching triplets of entities are considered to be instances of ternary relations.\nIn summary, the contributions of this paper are twofold:\n1) Resource: A resource for boostrapping ternary relation extraction. It contains ternary relations and their instances. It also contains templates used to populate ternary relations. We make the data available for future research. It is also attached as supplementary data to this submission.\n2) Data Generation Method: We describe the approach used to generate this resource, which others can replicate to generate similar resources in their domains of interest."}, {"heading": "2 Related Work", "text": "To supervise binary relation extraction methods, there is an abundance of resources. Knowledge bases (KBs) such as DBPedia (Auer et al., 2007), YAGO (Suchanek et al., 2007), Freebase(Bollacker et al., 2008), and NELL (Mitchell et al., 2015), as well as Open IE tools and resources such as Reverb (Fader et al., 2011b), and Ollie (Mausam et al., 2012) contains many millions of binary relation instances that can be used to distantly train binary relation extraction. However, these knowledge bases are highly impoverished when it comes to ternary relations. In contrast, we provide a resource that can be used directly to train and encourage more research on the\nhigher-nary machine reading and relation extraction. A number of works have studied temporal scoping of facts by adding a time dimension to facts (Wang et al., 2010; Wijaya et al., 2014). While temporal scope generates ternary relations, for example by using reification, this only deals with one type of ternary relation. In contrast, our resource contains 50 ternary relations, spanning 18 high level topics or event types."}, {"heading": "3 Data Generation", "text": "In this section we present our method for generating the ternary relations and their instances."}, {"heading": "3.1 Input", "text": "As input, our method takes a natural language text corpus and high level topics or even types of interest. Our method automatically learns many different ternary relations relevant for each event type. There are two advantages to specifying event types of interest instead of directly thinking in terms of ternary relations. First, a broad event type can capture many relevant ternary relations that naturally appear in the data. Second, it requires much less human effort to specify one event type than manually specifying a list of all conceivable ternary relations, some of which might not be present in the data.\nTable 1 shows the event types covered in our resource. For each event type, we specified at most three trigger verbs that indicate a potential mention of the event type. We will later describe how to automatically extend event trigger verbs in an iterative manner. This is done in a similar way to extending seed instances or patterns of binary relations (Suchanek et al., 2009; Nakashole et al.,\n2011; Mitchell et al., 2015)."}, {"heading": "3.2 Candidate Template Generation", "text": "Once we have event types defined with their trigger verbs, we can generate ternary relations for each event type. The first step is to generate ternary relation templates. An example template is: <person>endorsed <politician>for <political office>. We generate these templates directly from the data. We do this by first parsing the raw corpus, and extracting 5-item sequences of the form: N1, V, N2, P, N3; where the Ns are noun phrases, V is a verb such that V is a trigger verb of one of the events, and P is a preposition. We generate templates from 5-item sequences as follows: we replace every noun phrase N1-N3 by its semantic type. In particular, we do lookups of entity types in two types of semantic hierarchies, WordNet and the NELL type hierarchy (Carlson et al., 2010). We found the two type systems to be complementary: WordNet contains more common nouns whereas NELL contains more proper nouns. Our generated templates can therefore contain a mixture of WordNet types and NELL types. For example, for the MurderEvent, the following is a valid template that our approach generated: <NEL person>killed <NEL person>with <WDN weapon>. The semantic types in our templates are prefixed by three letters, NEL for NELL types, and WDN for WordNet types.\nWe retain, as candidate templates, all the templates of the form: <N1 type>V <N2 type>P <N3 type>, whose support size is >= 3. That is, the template was generated from three or more 5-item sequences, N1, V, N2, P, N3, with distinct noun arguments (N1-N3)."}, {"heading": "3.3 Template Filtering", "text": "From the candidate templates, a final set of templates is generated. To do this, we manually filter out all templates that do not express useful ternary relations for the topic at hand. Once we have filtered out the templates, for each template, we manually label each template with descriptive labels for the its corresponding noun phrase placeholders. For example, <NEL person>killed <NEL person>with <WDN weapon> is labeled with MurderEventMurderer, MurderEventMurdered, MurderEventInstrument. Each triple of labels is considered to be a single ternary rela-\ntion. Therefore we have the ternary relation Murderer Murdered Instrument. Such a ternary relation would has instances such as Bob,Alice,knife, which indicates that Bob murdered Alice with a knife.\nWe obtain instances of templates, and hence ternary relations, by retaining the supporting 5- item sequences of each of the accepted templates. Notice that each instance has labeled noun phrases because the instances inherit argument labels from their templates.\nIt is worth noting that template filtering is the part requiring the most manual supervision. All the other parts are automated. While, the specification of event types and their trigger verbs is also manual, it is quite fast, requiring only up to three trigger verbs per event type."}, {"heading": "3.4 Iterative Template Generation", "text": "In order to extend the size of the resource, we increase the number of trigger verbs per event type. We do this automatically. First, from the raw data, we extract 5-item sequences of the form N1, V,N2, AP,N3. There are two main differences between these sequences and the 5-item sequences we have worked with up to now. First, here V is any verb, not limited to the trigger verbs that were manually specified. Second, we no longer limit the phrase between N2 and N3 to prepositional phrases, P is now AP = any phrase. We limit the length of AP to a maximum of three words. We then find 5-item sequences where all three arguments N1 \u2212 3 match the arguments of an instance of a template from Section 3.3. Thus, we are using the instances generated so far as distant supervision to discover new templates.\nAll new (V,AP ) pairs forming candidate templates that occur with more than 10 distinct instances of an existing template qualify a new promoted templates. We increase the minimum support required from 3 to 10 to avoid introducing noisy templates. A new template has the same argument role labels as the original template whose instances overlap with the instances of the new template. This process extends the trigger verbs, and is not limited to prepositions for the extraction of the third argument. This also allows the generation of more instances from the newly discovered templates. Notice that the number of\nternary relations remain constant from the initial template generation step where we manually label templates with argument roles."}, {"heading": "4 Evaluation", "text": "We applied our data generation process to Wikipedia (WKP) and the ClueWeb09 (CWB) corpus. We first generated an initial set of candidate templates from Wikipedia, using the method described in Section 3.2. We did not apply this step to the ClueWeb corpus as it can be noisy. We then manually filtered the generated candidate templates as described in Section 3.3. This is iteration 0. We had a total of 186 templates at iteration 0, and 50 ternary relations across 18 event types. The number of templates increase across iterations. Therefore, in subsequent iterations, we obtain more templates that can be used to populate our 50 ternary relations.\nFrom the initial templates, we generate template instances both from Wikipedia and the ClueWeb corpus. These are triples of entities whose types match the argument types of the template, and they occur with the lexical items that appear in the template. We then use the instances as distant supervision to generate more templates as described in Section 3.4. Again, we only discover new templates from Wikipedia, using only ClueWeb to find instances for the discovered templates. At iteration 1, we discovered an additional 174 templates.\nOur method picked up new templates until iteration 3, making a total of 502 templates. Figure 1 shows the cumulative number of templates across iterations. Also shown in Figure 1 is precision of templates. We manually assessed precision at every iteration, we did this by randomly selecting 100 templates discovered at every iteration, or all templates if less than 100 templates were discovered in a given iteration. Precision is 100% at all iterations, except for iteration 2 where it dropped to 95%. This was due to a few cases of semantic drift not being cutoff by thresholds of our methods. This led our method do discover templates with verbs such as \u201cresigned as\u201d in templates associated with hiring events, we marked these as wrong.\nFigure 3.2 shows the cumulative number of instances picked up at every iteration. We started with 31, 161 from WKP and CWB corpora at iteration 0 and ended up with 61, 380 instances by\nthe third and final iteration. This number could be increased by 1) allowing discovery of templates from the ClueWeb corpus or other large corpora 2) lowering the high thresholds on the minimum support size of learned templates . Since instances are generated from templates, their precision can be inferred from the precision of the templates. To a small extent, instances can also contains errors stemming from: noun phrase chunking and semantic types; these errors can be fixed by using accurate better chunkers and semantic typing systems."}, {"heading": "5 Conclusion", "text": "In this paper our goal is to address the bottleneck that has throttled research in higher n-ary relation extraction. To this end, we generated a training data resource for ternary relation extractors. We described a method for learning and populating ternary relations initially only using prepositional phrase based templates. Additionally, our method also learns templates that are not based on prepositions, in an iterative manner. We hope this resource encourages research on ternary relation extraction."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary G. Ives"], "venue": "In The Semantic Web,", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Open information extraction for the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni"], "venue": "In IJCAI,", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD International", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Coupled semi-supervised learning for information extraction", "author": ["Justin Betteridge", "Richard C. Wang", "Estevam R. Hruschka", "Jr.", "Tom M. Mitchell"], "venue": "In Proceedings of the Third ACM International Conference on Web", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Clausie: Clause-based open information extraction", "author": ["Del Corro", "Gemulla2013] Luciano Del Corro", "Rainer Gemulla"], "venue": "In Proceedings of the 22Nd International Conference on World Wide Web,", "citeRegEx": "Corro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Corro et al\\.", "year": 2013}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011a] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011b] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "A largescale classification of english verbs", "author": ["Kipper et al.2008] Karin Kipper", "Anna Korhonen", "Neville Ryant", "Martha Palmer"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Kipper et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kipper et al\\.", "year": 2008}, {"title": "Open language learning for information extraction", "author": ["Mausam et al.2012] Mausam", "Michael Schmitz", "Robert Bart", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Mausam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mausam et al\\.", "year": 2012}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Language-aware truth assessment of fact candidates", "author": ["Nakashole", "Tom M. Mitchell"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Nakashole et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2014}, {"title": "A knowledge-intensive model for prepositional phrase attachment", "author": ["Nakashole", "Tom M. Mitchell"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Nakashole et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2015}, {"title": "Real-time population of knowledge bases: opportunities and challenges", "author": ["Nakashole", "Weikum2012] Ndapandula Nakashole", "Gerhard Weikum"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Scalable knowledge harvesting with high precision and high recall", "author": ["Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Nakashole et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2011}, {"title": "Query-time reasoning in uncertain RDF knowledge bases with soft and hard rules", "author": ["Mauro Sozio", "Fabian M. Suchanek", "Martin Theobald"], "venue": "In Proceedings of the Second International Workshop", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Patty: A taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Fine-grained semantic typing of emerging entities", "author": ["Tomasz Tylenda", "Gerhard Weikum"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Nakashole et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Sofie: A selforganizing framework for information extraction", "author": ["Mauro Sozio", "Gerhard Weikum"], "venue": "In Proceedings of the 18th International Conference on World Wide Web,", "citeRegEx": "Suchanek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2009}, {"title": "Timely yago: harvesting, querying, and visualizing temporal knowledge from wikipedia", "author": ["Wang et al.2010] Yafang Wang", "Mingjie Zhu", "Lizhen Qu", "Marc Spaniol", "Gerhard Weikum"], "venue": "In Proceedings of the 13th International Conference on Extend-", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Ctps: Contextual temporal profiles for time scoping facts via entity state change detection", "author": ["Wijaya et al.2014] Derry Wijaya", "Ndapandula Nakashole", "Tom Mitchell"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Wijaya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wijaya et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Developing techniques for higher-nary relation extraction is a natural next step after the wellstudied case of binary relations (Auer et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Mitchell et al., 2015).", "startOffset": 128, "endOffset": 239}, {"referenceID": 17, "context": "Developing techniques for higher-nary relation extraction is a natural next step after the wellstudied case of binary relations (Auer et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Mitchell et al., 2015).", "startOffset": 128, "endOffset": 239}, {"referenceID": 2, "context": "Developing techniques for higher-nary relation extraction is a natural next step after the wellstudied case of binary relations (Auer et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Mitchell et al., 2015).", "startOffset": 128, "endOffset": 239}, {"referenceID": 3, "context": "Developing techniques for higher-nary relation extraction is a natural next step after the wellstudied case of binary relations (Auer et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Mitchell et al., 2015).", "startOffset": 128, "endOffset": 239}, {"referenceID": 18, "context": "In the literature, prominent binary relation extraction methods are mostly semi-supervised (Suchanek et al., 2009; Carlson et al., 2010) or unsupervised (Mausam et al.", "startOffset": 91, "endOffset": 136}, {"referenceID": 3, "context": "In the literature, prominent binary relation extraction methods are mostly semi-supervised (Suchanek et al., 2009; Carlson et al., 2010) or unsupervised (Mausam et al.", "startOffset": 91, "endOffset": 136}, {"referenceID": 8, "context": ", 2010) or unsupervised (Mausam et al., 2012; Fader et al., 2011a).", "startOffset": 24, "endOffset": 66}, {"referenceID": 13, "context": "Semisupervised methods tend to have higher precision than unsupervised methods and therefore are commonly used to populate knowledge bases of facts (Nakashole et al., 2011; Mitchell et al., 2015).", "startOffset": 148, "endOffset": 195}, {"referenceID": 0, "context": "Knowledge bases (KBs) such as DBPedia (Auer et al., 2007), YAGO (Suchanek et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 17, "context": ", 2007), YAGO (Suchanek et al., 2007), Freebase(Bollacker et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 2, "context": ", 2007), Freebase(Bollacker et al., 2008), and NELL (Mitchell et al.", "startOffset": 17, "endOffset": 41}, {"referenceID": 8, "context": ", 2011b), and Ollie (Mausam et al., 2012) contains many millions of binary relation instances that can be used to distantly train binary relation extraction.", "startOffset": 20, "endOffset": 41}, {"referenceID": 19, "context": "A number of works have studied temporal scoping of facts by adding a time dimension to facts (Wang et al., 2010; Wijaya et al., 2014).", "startOffset": 93, "endOffset": 133}, {"referenceID": 20, "context": "A number of works have studied temporal scoping of facts by adding a time dimension to facts (Wang et al., 2010; Wijaya et al., 2014).", "startOffset": 93, "endOffset": 133}, {"referenceID": 3, "context": "In particular, we do lookups of entity types in two types of semantic hierarchies, WordNet and the NELL type hierarchy (Carlson et al., 2010).", "startOffset": 119, "endOffset": 141}], "year": 2015, "abstractText": "Binary relation extraction methods have been widely studied in recent years. However, few methods have been developed for higher n-ary relation extraction. One limiting factor is the effort required to generate training data. For binary relations, one only has to provide a few dozen pairs of entities per relation, as training data. For ternary relations (n=3), each training instance is a triplet of entities, placing a greater cognitive load on people. For example, many people know that Google acquired Youtube but not the dollar amount or the date of the acquisition and many people know that Hillary Clinton is married to Bill Clinton by not the location or date of their wedding. This makes higher n-nary training data generation a time consuming exercise in searching the Web. We present a resource for training ternary relation extractors. This was generated using a minimally supervised yet effective approach. We present statistics on the size and the quality of the dataset.", "creator": "LaTeX with hyperref package"}}}