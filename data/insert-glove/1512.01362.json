{"id": "1512.01362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Proposition of a Theoretical Model for Missing Data Imputation using Deep Learning and Evolutionary Algorithms", "abstract": "In the scourby last couple of decades, dulce there has seige been mongkol major advancements in the domain of missing data imputation. prodemocracy The wintz techniques precipices in the double-decked domain include tedrow amongst 41.58 others: agret Expectation Maximization, Neural Networks mumia with gasc Evolutionary heteroatoms Algorithms uttering or c-raf optimization melony techniques and K - parfums Nearest franchisors Neighbor manrico approaches diffractive to suceso solve the problem. The al-saffar presence slowdown of izmailovo missing data entries in nickolas databases render the spares tasks of w\u01d2 decision - making ervan and data darom analysis ev'ry nontrivial. www.caiso.com As a mchenry result isight this blotting area gwahng has attracted a lot of research cafepress.com interest rutka with the aim velits being exd5 to yield lyhoog accurate hyperspeed and herington time efficient and sensitive fiances missing data imputation noveski techniques especially hendri when sang-e time sensitive applications are pimpin concerned like medalia power plants lifeline and benkiser winding rivas processes. tamworth In 91.3 this haishi article, considering arbitrary and iphig\u00e9nie monotone missing f-4j data function patterns, speleers we 78-rpm hypothesize insolvencies that the use of deep s\u00e9n\u00e9galais neural antimissile networks ipim built 89-82 using compendiums autoencoders augustin and ashenburg denoising beitzel autoencoders in conjunction runyan with pauleta genetic 77.49 algorithms, 39.09 swarm lembalemba intelligence 263.4 and maximum likelihood estimator 26.12 methods 4,739 as baliani novel data imputation shareh techniques krauskopf will lead appropriations to better gualeguaychu imputed values barotse than existing techniques. Also considered voltages are 34-yard the podell missing at anneliese random, 500-man missing completely kuchak at random and missing downfall not stippling at random missing stahn data owens-illinois mechanisms. We also sword intend to rabiu use tijeras fuzzy 615,000 logic narmashir in tandem waqas with quattrochi deep neural helal networks to perform illuminatus the missing data divesting imputation tasks, mapai as well sanei as serialisation different building topps blocks for .329 the 357 deep mering neural networks like Stacked Restricted Boltzmann Machines and spleens Deep szczytniki Belief brier Networks to klieg test our hypothesis. ladurie The motivation qxd4 behind this hormonally article lekhwiya is the need federal-provincial for missing data imputation 2.20 techniques tashiro that 6,996 lead mecs to jazz-oriented better schacher imputed values ghd than ekpo existing bwf methods with trueblood higher disd accuracies and lao lower subservient errors.", "histories": [["v1", "Fri, 4 Dec 2015 10:39:59 GMT  (358kb,D)", "http://arxiv.org/abs/1512.01362v1", "14 Pages, 4 figures, journal, experiments will be added testing the hypotheses"]], "COMMENTS": "14 Pages, 4 figures, journal, experiments will be added testing the hypotheses", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["collins leke", "tshilidzi marwala", "satyakama paul"], "accepted": false, "id": "1512.01362"}, "pdf": {"name": "1512.01362.pdf", "metadata": {"source": "CRF", "title": "Proposition of a Theoretical Model for Missing Data Imputation using Deep Learning and Evolutionary Algorithms", "authors": ["Collins Leke", "Tshilidzi Marwala"], "emails": ["collinsl@uj.ac.za", "tmarwala@uj.ac.za", "psatyakama@student.uj.ac.za"], "sections": [{"heading": null, "text": "Keywords: Deep Neural Network, Stacked Autoencoder, Stacked Denoising Autoencoder, Stacked Restricted Boltzmann Machine, Swarm Intelligence, Genetic Algorithms, Maximum Likelihood Estimator, Fuzzy Logic, Missing Data"}, {"heading": "1. Introduction", "text": "Decision-making and data analysis tasks are made nontrivial by the presence of missing data in a database. The decisions made by decision makers are likely to be more accurate and reliable with complete datasets than with incomplete datasets containing missing data\nar X\niv :1\n51 2.\n01 36\n2v 1\n[ cs\n.N E\nentries. Also, data analysis and data mining tasks yield more representative results and statistics when all the required data is available. As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al. (2015), Koko et al. (2015), Mistry et al. (2009), Nelwamondo et al. (2007), Leke et al. (2014), Mohamed et al. (2007), Abdella and Marwala (2005), Zhang et al. (2011) and Zhang (2011)]. Applications such as in medicine, manufacturing or energy that use sensors in instruments to report vital information that makes time sensitive decisions, may fail when there are missing data in the database. In such cases, it is very important to have a system capable of imputing the missing data from the failed sensors with high accuracy as quickly as possible. The imputation procedure in such cases requires the approximation of the missing value taking into account the interrelationships that exist between the values of other sensors in the system. There are several reasons that could lead to data being missing in a dataset. These could be as a result of data entry errors or respondents not answering certain questions in a survey during the data collection phase. Furthermore, failure in instruments and sensors could be a reason for missing data entries. The table below depicts a database consisting of seven feature variables with the values of some of the variables missing. The variables are X1, X2, X3, X4, X5, X6 and X7.\nConsider that the database in question has several records of the seven variables with some of the data entries for some variables not available. The question of interest is, can we say with some degree of certainty what the missing data entries are? Furthermore, can we introduce techniques for approximation of the missing data when correlation and interrelationships between the variables in the database are considered? We aim to use deep learning techniques, Genetic Algorithms (GAs), Maximum Likelihood Estimator (MLE) and Swarm Intelligence (SI) techniques to approximate the missing data in databases with the different models created catering to the different missing data mechanisms and patterns. Therefore, with knowledge of the presence of interrelationships or lack thereof between feature variables in the respective datasets, one will know exactly what model is relevant to the imputation task at hand. Also we plan to use fuzzy logic with deep learning techniques to perform the imputation tasks."}, {"heading": "2. Background", "text": "In this section, we present details on the problem of missing data and the Deep Learning techniques we aim to use to solve the problem."}, {"heading": "2.1 Missing Data", "text": "Missing data is a scenario in which some of the components of the dataset are not available for all feature variables, or may not even be defined within the problem domain in the sense that the values do not match the problem definition by either being outliers or inaccurate (Rubin, 1978). This produces a variety of problems in several application domains that rely on the access to complete and quality data. As a result, techniques aimed at handling the problem have been an area of research for a while in several disciplines [Allison (1999), Little and Rubin (2014) and Rubin (1978)]. Missing data may occur in several ways in a dataset. For example, it may occur due to several participants non-response to questions in the data collection process or data entry process . There are also other situations in which missing data may occur due to failures of sensors or instruments in the data recording process for sectors that use these. The following subsubsections present the different missing data mechanisms."}, {"heading": "2.1.1 Missing Data Mechanisms", "text": "The way to handle missing data in a reasonable manner depends on how the data points go missing. According to Little and Rubin (2014), there exist three missing data mechanisms. They are: Missing Completely at Random (MCAR), Missing at Random (MAR), and a Missing not at Random or Non-Ignorable case(MNAR or NI).\nMissing Completely at Random\nMCAR scenario arises when the chances of there being a missing data entry for a feature variable is not dependent on the feature variable itself or on any of the other feature variables in the dataset (Leke et al., 2014). This implies that the missing value is independent of the feature variable being considered or the other feature variables within the dataset (Rubin, 1978). In Table 1, the nature of the missing value in X2 for row 5 is said to be MCAR if the nature of this missing value does not depend on X1, X3, X4, X5, X6 and X7 and the variable X2 itself.\nMissing at Random\nMAR occurs if the chances of there being a missing value in a specific feature variable depends on all the other feature variables within the dataset, but not on the feature variable of interest (Leke et al., 2014). MAR means the value for the feature variable is missing, but conditional on some other feature variable observed in the dataset, although not on the feature variable of interest (Scheffer, 2002). In Table 1, the nature of the missing value in X2 is said to be MAR if the missing nature of the value depends on X1, X3, X4, X5, X6 and X7 but not on X2 itself.\nMissing Not at Random or Non-Ignorable Case\nThe third type of missing data mechanism is the non-ignorable case. The non-ignorable case occurs when the chances of there being a missing entry in variable, X2 for example, is influenced by the value of the variable X2 regardless of whether or not the other variables in the dataset are altered and modified [Leke et al. (2014), Allison (1999)]. In this case, the pattern of missing data is not random and it is impossible to predict this missing data using the rest of the variables in the dataset. Non-ignorable missing data is the most difficult to approximate and model than the other two missing data mechanisms (Rubin, 1978). In Table 1 the nature of the missing value in X2 is said to be non-ignorable if the missing value in X2 depends on the variable itself and not on the other variables."}, {"heading": "2.2 Missing Data Patterns", "text": "There are two main missing data patterns defined by Little and Rubin (2014). These patterns are the arbitrary and monotone missing data patterns. In the arbitrary missing data pattern, missing observations may occur anywhere and the ordering of the variables is of no importance as in rows 1 to 5. In monotone missing patterns, the ordering of the variables is of importance and occurrence is not random. In this case, if we have a dataset with variables as in Table 1, it is said to be a monotone missing pattern if a variable Xj is observed for a particular scenario, and this implies that all the previous variables Xk, where k < j, are also observed for that scenario (Little and Rubin, 2014). Table 1 shows an arbitrary missing data pattern from rows 1 to 5 and a monotone missing data pattern from rows 6 to 9. In Table 1 the missing values are random and can happen at any point in the dataset from rows 1 to 5 while it can be seen that missing values have some common order in rows 6 to 9. This means that if the values for a variable Xj are missing, so are the values for other variables Xi, where i > j."}, {"heading": "2.3 Deep Learning", "text": "Deep Learning comprises of several algorithms in machine learning that make use of a cataract of nonlinear processing units organized into a number of layers that extract and transform features from the input data [Deng et al. (2013), Deng and Yu (2014)]. Each of the layers use the output from the previous layer as input and a supervised or unsupervised algorithm could be used in the training or building phase. With these come applications in supervised and unsupervised problems like classification and pattern analysis respectively. It is also based on the unsupervised learning of multiple levels of features or representations of the input data whereby higher-level features are obtained from lower level features to yield a hierarchical representation of the data (Deng and Yu, 2014). By learning multiple levels of representations that depict different levels of abstraction of the data, we obtain a hierarchy of concepts.\nThere are different types of Deep Learning architectures such as Convolutional Neural Networks (CNN), Convolutional Deep Belief Networks (CDBN), Deep Neural Networks (DNN), Deep Belief Networks (DBN), Stacked (Denoising) Auto-Encoders (SAE/SDAE) and Deep/Stacked Restricted Boltzmann Machines (DBM). We intend to make use of DNNs and SAEs predominantly, and the others with the exception of CNNs and CDBNs. DNNs\nare commonly understood in terms of the Universal Approximation Theorem, Probabilistic Inference or Discrete Signal Processing. An artificial neural network (ANN) with numerous hidden layers of nodes between the input layer and the output layer is known as a DNN. They are typically designed as feed forward networks and can be trained discriminatively utilizing standard back propagation with updates of the weights being done by use of stochastic gradient descent. Typical choices for the activation and cost functions are the softmax and cross entropy functions for classification tasks, with sigmoid and standard error functions used for regression or prediction tasks with normalized inputs. In Figures 1-3, the architectures of four deep learning techniques are depicted. Figure 1 shows a DNN with eight input nodes in the input layer, three hidden layers each with nine nodes and an output layer with four nodes. The nodes from each layer are connected with those from the subsequent and preceding layers. Figure 2 shows a DBN and a DBM whereby the first layer of nodes (bottom-up) is the input layer (v) with visible units representing the database feature variables and the subsequent layers of nodes are binary hidden nodes (h). The arrows in the DBN indicate that the training is a top-down approach while the lack of arrows in the DBM is a result of the training being both top-down and bottom-up. In Figure 3, we see individual RBMs being stacked together to form the encoder part of an autoencoder, which is transposed to yield the decoder part. The autoencoder is then fine-tuned using back propagation to modify the interconnecting weights with the aim being to minimize the network error."}, {"heading": "2.4 Related Work", "text": "In this section, we present some of the work that has been done by researchers to address the problem of missing data. In Zhang et al. (2011), it is suggested that information within incomplete cases, that is, instances with missing values be used when estimating missing\nvalues. A nonparametric iterative imputation algorithm (NIIA) is proposed that leads to a root mean squared error value of at least 0.5 on the imputation of continuous values and a classification accuracy of at most 87.3% on the imputation of discrete values with varying ratios of missingness. Lobato el al. (2015) present a multi-objective genetic algorithm approach for missing data imputation. It is observed that the results obtained outperform some of the well known missing data methods with accuracies in the 90 percentile. In Zhang (2011), the shell-neighbor method is applied in missing data imputation by means of the Shell-Neighbor Imputation (SNI) algorithm which is observed to perform better than the kNearest Neighbor imputation method in terms of imputation and classification accuracy as it takes into account the left and right nearest neighbors of the missing data as well as varying number of nearest neighbors contrary to k-NN that considers just fixed k nearest neighbors. Rana et al. (2015) use robust regression imputation for missing data in the presence of outliers and investigate its effectiveness. Abdella and Marwala (2005) implement a hybrid genetic algorithm-neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while Aydilek and Arslan (2012) create a hybrid k-Nearest Neighbor-Neural Network system for the same purpose. In some cases, neural networks were used with Principal Component Analysis (PCA) and genetic algorithm as in Mistry et al. (2009), Mohamed et al. (2007) and Nelwamondo et al. (2007). Leke et al. (2014) use a hybrid of Auto-Associative neural networks or autoencoders with\ngenetic algorithm, simulated annealing and particle swarm optimization to impute missing data with high levels of accuracy in cases where just one feature variable has missing input entries. Novel algorithms for missing data imputation and comparisons between existing techniques can be found in papers such as Schafer and Graham (2002), Liew et al. (2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al. (2010) and Kalaycioglu et al. (2015)."}, {"heading": "3. Theoretical Model", "text": "In this section, we outline the methodology used to address the problem of missing data. The approach used to design the novel imputation techniques with SAE/SDAE involves the following six steps which are depicted in figure 4:\n1. Train the Deep Neural Network with a complete set of records to recall the inputs as the outputs. Inputs are the dataset feature variables, for example X1 to X7 in Table 1, and the outputs are these same feature variables as the aim is to reproduce these inputs at the output layer. For the network to be able to do this, it needs to extract information from the input data, which is captured in the updated network weights and biases. The extraction of information is done during the training phase whereby lower level features are extracted from the input data after which low-level features are extracted till high-level features are obtained yielding a hierarchical representation of the input data. The overall idea is that features are extracted from features to get as good a representation of the data as possible. In the encoder phase mentioned in the previous section, a deterministic mapping function, f\u03b8, creates a hidden representation, y, of the input data x. It is typically represented by an affine mapping and subsequently a nonlinearity, f\u03b8 (x) = s (Wx+ b) (Isaacs, 2014). The \u03b8 parameter comprises of the matrix of weights W and the vector of offsets/biases b. In the decoder phase, y being the hidden representation is remapped to z which is a vector reconstruction in the input space with z = g\u03b8\u2032 (y) (Isaacs, 2014). The function g\u03b8\u2032 is the decoder function which is an affine mapping deliberately ensued by a non-linearity with squashing traits that either follows the form g\u03b8\u2032 (y) = W \u2032y + b\u2032\nor g\u03b8\u2032 (y) = s (W \u2032y + b\u2032) with the parameter set \u03b8 \u2032 comprising of the transpose of the weights and biases from the encoder (Isaacs, 2014).\n2. Obtain the objective function from step 1 as depicted in Figure 4 as input to the optimization techniques. The updated weights and biases mentioned in step 1 are gotten by back propagating the error at the output layer obtained by comparing the actual output to the network output through the network. The function or equation used to compare the actual output to the network output is used as the objective function. z from step 1 is not explained as a rigorous regeneration of x but rather as the parameters of a distribution p (X|Z = z) in probabilistic terms, that may yield x with high probability (Isaacs, 2014). This thus leads to p (X|Y = y) = p ( X|Z = g\u03b8\u2032 (y) ) .\nFrom this, we obtain an associated reconstruction error which is to be optimized by the optimization techniques and is of the form L (x, z) \u221d \u2212logp (x|z). This equation could also be written as \u03b4AE (\u03b8) = \u03a3tL ( x(t), g\u03b8 ( f\u03b8 ( x(t) ))) (Bengio et al., 2013). For a denoising autoencoder, the reconstruction error to be optimized is expressed\nas \u03b4DAE = \u03a3tEq(x\u0303|x(t)) [ L ( x(t), g\u03b8 (f\u03b8 (x\u0303)) )] where Eq(x\u0303|x(t)) [.] averages over the cor-\nrupted examples x\u0303 drawn from a corruption process q ( x\u0303|x(t) ) (Bengio et al., 2013).\n3. Approximate the missing data entries using the approximation techniques. MCAR, MAR and MNAR missing data mechanisms will be considered as well as arbitrary and monotone missing data patterns. Different models will be created to experiment with these and test the hypothesis. In testing the hypothesis, we use the test set of data which consist of known feature variable values Xk and unknown or missing feature variable values Xu as input to the trained deep learning technique. The Xk values are passed as input to the network while the Xu values are first estimated by the approximation techniques before being passed into the network as input. The optimal Xu value is obtained when the objective function from step 2 is minimized.\n4. Use the now completed database with the approximated missing values in the trained Deep Learning method from step 1 to observe whether or not the objective has been minimized. In this case, that will be checking if the error is minimized as we attempt to reconstruct the input.\n5. If so, the complete dataset is presented as output.\n6. If not, do step 3."}, {"heading": "4. Possible Benefits", "text": "In this research, we are introducing novel data imputation techniques which we expect will be of benefit to the research community interested in missing data imputation. Some of these are:\n\u2022 With the techniques introduced, we expect to yield improved missing data imputation accuracies compared against existing methods by looking at the relative prediction accuracy, correlation coefficient, standard square error, mean and root mean squared errors and other relevant representative metrics in comparison to existing techniques. This expectation stems from the manner in which deep learning methods extract information and features from the input data.\n\u2022 With literature stating that deep neural networks are capable of representing and approximating more complex functions and relations than simple neural networks, we hope these techniques will be applicable in a variety of sectors regardless of the complexity of the problem with high accuracy. This will be tested against existing techniques and the aforementioned.\n\u2022 Possible parallelization of the imputation tasks using the methods to be introduced could lead to faster imputed missing values which benefits time sensitive applications."}, {"heading": "5. Possible Limitations", "text": "Although there are possible benefits to using the novel techniques to be introduced, there could possibly be limitations observed, for example:\n\u2022 Using Deep Neural Networks could possibly lead to a lot of time being required to do the imputations and obtaining a complete dataset due to the number of parameters that need to be optimized during training and also the number of computations done during testing. The full effect of long computation times could be felt in time sensitive applications such as in medicine, finance or manufacturing. The slow computation time could be addressed by parallelizing the processes on a multicore system. Each core could handle the imputation of the missing data value(s) in different rows depending on the number of cores. Also, dynamic programming could be used to speed up the computation time.\n\u2022 Besides time being a factor, there could also be a problem of space required to do the computations. To address these two drawbacks, a complexity analysis will be done to verify the time and space complexities of the proposed methods. Anything less than O ( n2 ) will be preferable with O (n log n) being regarded as the ideal complexity for\nboth."}, {"heading": "6. Conclusion", "text": "In this article, we propose a new hypothesis that the use of deep learning techniques in conjunction with swarm intelligence, genetic algorithms and maximum likelihood estimator methods will lead to better imputations due to the fact that a hierarchical representation of the input data is obtained as higher level features are further extracted from lower level features in deep learning methods. This hypothesis is investigated by taking into account a comparison between the techniques to be introduced and the existing methods like Neural Networks with Genetic Algorithm, Auto-Associative Neural Network with Genetic Algorithm, K-Nearest Neighbor with Neural Networks, Neural Networks with Principal Component Analysis and Genetic algorithm and so on. The main motivation behind this hypothesis is the need to provide datasets with highly representative and accurate feature values from which trustworthy decisions and data analytics and statistics will emerge."}], "references": [{"title": "The use of genetic algorithms and neural networks to approximate missing data in database", "author": ["M. Abdella", "T. Marwala"], "venue": "Computational Cybernetics,", "citeRegEx": "Abdella and Marwala,? \\Q2005\\E", "shortCiteRegEx": "Abdella and Marwala", "year": 2005}, {"title": "Multiple imputation for missing data: A cautionary tale", "author": ["Allison", "Paul D"], "venue": null, "citeRegEx": "Allison and D.,? \\Q1999\\E", "shortCiteRegEx": "Allison and D.", "year": 1999}, {"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["I. Arel", "D.C. Rose", "T.P. Karnowski"], "venue": "Computational Intelligence Magazine,", "citeRegEx": "Arel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Arel et al\\.", "year": 2010}, {"title": "A novel hybrid approach to estimating missing values in databases using k-nearest neighbors and neural networks", "author": ["I.B. Aydilek", "A. Arslan"], "venue": "International Journal of Innovative Computing, Information and Control", "citeRegEx": "Aydilek and Arslan,? \\Q2012\\E", "shortCiteRegEx": "Aydilek and Arslan", "year": 2012}, {"title": "An introduction to modern missing data analyses", "author": ["A.N. Baraldi", "C.K. Enders"], "venue": "Journal of School Psychology. Elsevier", "citeRegEx": "Baraldi and Enders,? \\Q2010\\E", "shortCiteRegEx": "Baraldi and Enders", "year": 2010}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on. IEEE", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Recent advances in deep learning for speech research at Microsoft", "author": ["L. Deng", "J. Li", "Huang", "J.-T", "K. Yao", "D. Yu", "F. Seide", "M. Seltzer", "G. Zweig", "X. He", "J. Williams", "others"], "venue": "Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Deep learning: methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Foundations and Trends in Signal Processing. Now Publishers Inc", "citeRegEx": "Deng and Yu,? \\Q2014\\E", "shortCiteRegEx": "Deng and Yu", "year": 2014}, {"title": "Review: a gentle introduction to imputation of missing values", "author": ["A.R.T. Donders", "G.J. van der Heijden", "T. Stijnen", "K.G. Moons"], "venue": "Journal of clinical epidemiology. Elsevier", "citeRegEx": "Donders et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Donders et al\\.", "year": 2006}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "Manzagol", "P.-A", "P. Vincent", "S. Bengio"], "venue": "The Journal of Machine Learning Research. JMLR. org", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "A Different Approach to the Problem of Missing Data", "author": ["X. Gu", "N. Matloff"], "venue": "arXiv preprint arXiv:1509.04992", "citeRegEx": "Gu and Matloff,? \\Q2015\\E", "shortCiteRegEx": "Gu and Matloff", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": "Science. American Association for the Advancement of Science", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Representational learning for sonar ATR", "author": ["J.C. Isaacs"], "venue": "Proc. SPIE. doi: 10.1117/12.2053057", "citeRegEx": "Isaacs,? \\Q2014\\E", "shortCiteRegEx": "Isaacs", "year": 2014}, {"title": "Missing data imputation using statistical and machine learning methods in a real breast cancer problem", "author": ["J.M. Jerez", "I. Molina", "P.J. Gar\u0107\u0131a-Laencina", "E. Alba", "N. Ribelles", "M. Mart\u0301\u0131n", "L. Franco"], "venue": "Artificial intelligence in medicine. Elsevier", "citeRegEx": "Jerez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jerez et al\\.", "year": 2010}, {"title": "A comparison of multiple-imputation methods for handling missing data in repeated measurements observational studies", "author": ["O. Kalaycioglu", "A. Copas", "M. King", "R.Z. Omar"], "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),", "citeRegEx": "Kalaycioglu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalaycioglu et al\\.", "year": 2015}, {"title": "Missing data treatment method on cluster analysis", "author": ["E.E.M. Koko", "A.I.A. Mohamed"], "venue": "International Journal of Advanced Statistics and Probability,", "citeRegEx": "Koko and Mohamed,? \\Q2015\\E", "shortCiteRegEx": "Koko and Mohamed", "year": 2015}, {"title": "Multiple imputation for missing data: fully conditional specification versus multivariate normal imputation. American journal of epidemiology", "author": ["K.J. Lee", "J.B. Carlin"], "venue": null, "citeRegEx": "Lee and Carlin,? \\Q2010\\E", "shortCiteRegEx": "Lee and Carlin", "year": 2010}, {"title": "Modeling of missing data prediction: Computational intelligence and optimization algorithms", "author": ["C. Leke", "B. Twala", "T. Marwala"], "venue": "Systems, Man and Cybernetics (SMC),", "citeRegEx": "Leke et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leke et al\\.", "year": 2014}, {"title": "Missing value imputation for gene expression data: computational techniques to recover missing data from available information. Briefings in bioinformatics", "author": ["Liew", "A.W.-C", "Law", "N.-F", "H. Yan"], "venue": null, "citeRegEx": "Liew et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liew et al\\.", "year": 2011}, {"title": "MultiObjective Genetic Algorithm For Missing Data Imputation", "author": ["F. Lobato", "C. Sales", "I. Araujo", "V. Tadaiesky", "L. Dias", "L. Ramos", "A. Santana"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lobato et al\\.", "year": 2015}, {"title": "Statistical analysis with missing data", "author": ["R.J. Little", "D.B. Rubin"], "venue": null, "citeRegEx": "Little and Rubin,? \\Q2014\\E", "shortCiteRegEx": "Little and Rubin", "year": 2014}, {"title": "Missing value imputation method for disaster decision-making using K nearest neighbor", "author": ["X. Ma", "Q. Zhong"], "venue": "Journal of Applied Statistics. Taylor & Francis", "citeRegEx": "Ma and Zhong,? \\Q2015\\E", "shortCiteRegEx": "Ma and Zhong", "year": 2015}, {"title": "Missing Data Estimation Using Principle Component Analysis and Autoassociative Neural Networks", "author": ["F.J. Mistry", "F.V. Nelwamondo", "T. Marwala"], "venue": "Journal of Systemics, Cybernatics and Informatics", "citeRegEx": "Mistry et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mistry et al\\.", "year": 2009}, {"title": "Estimating missing data using neural network techniques, principal component analysis and genetic algorithms", "author": ["A.K. Mohamed", "F.V. Nelwamondo", "T. Marwala"], "venue": "Proceedings of the Eighteenth Annual Symposium of the Pattern Recognition Association of South Africa", "citeRegEx": "Mohamed et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2007}, {"title": "Goodbye, listwise deletion: Presenting hot deck imputation as an easy and effective tool for handling missing data", "author": ["T.A. Myers"], "venue": "Communication Methods and Measures. Taylor & Francis", "citeRegEx": "Myers,? \\Q2011\\E", "shortCiteRegEx": "Myers", "year": 2011}, {"title": "Missing data: A comparison of neural network and expectation maximisation techniques", "author": ["F.V. Nelwamondo", "S. Mohamed", "T. Marwala"], "venue": "arXiv preprint arXiv:0704.3474", "citeRegEx": "Nelwamondo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nelwamondo et al\\.", "year": 2007}, {"title": "Robust Regression Imputation For Missing Data in the Presence of Outliers", "author": ["S. Rana", "A.H. John", "H. Midi", "A. Imon"], "venue": "Far East Journal of Mathematical Sciences. Pushpa Publishing House", "citeRegEx": "Rana et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rana et al\\.", "year": 2015}, {"title": "Multiple imputations in sample surveys-a phenomenological Bayesian approach to nonresponse. Proceedings of the survey research methods section of the American Statistical Association", "author": ["Rubin", "Donald B"], "venue": "American Statistical Association", "citeRegEx": "Rubin and B.,? \\Q1978\\E", "shortCiteRegEx": "Rubin and B.", "year": 1978}, {"title": "Deep learning is difficult", "author": ["J. Sandberg", "Y. Barnard"], "venue": "Instructional Science. Springer", "citeRegEx": "Sandberg and Barnard,? \\Q1997\\E", "shortCiteRegEx": "Sandberg and Barnard", "year": 1997}, {"title": "Missing data: our view of the state of the art", "author": ["J.L. Schafer", "J.W. Graham"], "venue": "Psychological methods. American Psychological Association", "citeRegEx": "Schafer and Graham,? \\Q2002\\E", "shortCiteRegEx": "Schafer and Graham", "year": 2002}, {"title": "Dealing with missing data", "author": ["J. Scheffer"], "venue": "Massey University", "citeRegEx": "Scheffer,? \\Q2002\\E", "shortCiteRegEx": "Scheffer", "year": 2002}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks. Elsevier", "citeRegEx": "Schmidhuber,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Flexible imputation of missing data", "author": ["S. Van Buuren"], "venue": "CRC press", "citeRegEx": "Buuren,? \\Q2012\\E", "shortCiteRegEx": "Buuren", "year": 2012}, {"title": "Missing data imputation by utilizing information within incomplete instances", "author": ["S. Zhang", "Z. Jin", "X. Zhu"], "venue": "Journal of Systems and Software. Elsevier", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Shell-neighbor method and its application in missing data imputation", "author": ["S. Zhang"], "venue": "Applied Intelligence. Springer", "citeRegEx": "Zhang,? \\Q2011\\E", "shortCiteRegEx": "Zhang", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al.", "startOffset": 273, "endOffset": 299}, {"referenceID": 2, "context": "As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al. (2015), Koko et al.", "startOffset": 273, "endOffset": 319}, {"referenceID": 2, "context": "As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al. (2015), Koko et al. (2015), Mistry et al.", "startOffset": 273, "endOffset": 339}, {"referenceID": 2, "context": "As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al. (2015), Koko et al. (2015), Mistry et al. (2009), Nelwamondo et al.", "startOffset": 273, "endOffset": 361}, {"referenceID": 2, "context": "As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al. (2015), Koko et al. (2015), Mistry et al. (2009), Nelwamondo et al. (2007), Leke et al.", "startOffset": 273, "endOffset": 387}, {"referenceID": 2, "context": "As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al. (2015), Koko et al. (2015), Mistry et al. (2009), Nelwamondo et al. (2007), Leke et al. (2014), Mohamed et al.", "startOffset": 273, "endOffset": 407}, {"referenceID": 2, "context": "As a result, there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [Aydilek and Arslan (2012), Rana et al. (2015), Koko et al. (2015), Mistry et al. (2009), Nelwamondo et al. (2007), Leke et al. (2014), Mohamed et al. (2007), Abdella and Marwala (2005), Zhang et al.", "startOffset": 273, "endOffset": 430}, {"referenceID": 0, "context": "(2007), Abdella and Marwala (2005), Zhang et al.", "startOffset": 8, "endOffset": 35}, {"referenceID": 0, "context": "(2007), Abdella and Marwala (2005), Zhang et al. (2011) and Zhang (2011)].", "startOffset": 8, "endOffset": 56}, {"referenceID": 0, "context": "(2007), Abdella and Marwala (2005), Zhang et al. (2011) and Zhang (2011)].", "startOffset": 8, "endOffset": 73}, {"referenceID": 21, "context": "As a result, techniques aimed at handling the problem have been an area of research for a while in several disciplines [Allison (1999), Little and Rubin (2014) and Rubin (1978)].", "startOffset": 136, "endOffset": 160}, {"referenceID": 21, "context": "As a result, techniques aimed at handling the problem have been an area of research for a while in several disciplines [Allison (1999), Little and Rubin (2014) and Rubin (1978)].", "startOffset": 136, "endOffset": 177}, {"referenceID": 21, "context": "According to Little and Rubin (2014), there exist three missing data mechanisms.", "startOffset": 13, "endOffset": 37}, {"referenceID": 18, "context": "MCAR scenario arises when the chances of there being a missing data entry for a feature variable is not dependent on the feature variable itself or on any of the other feature variables in the dataset (Leke et al., 2014).", "startOffset": 201, "endOffset": 220}, {"referenceID": 18, "context": "MAR occurs if the chances of there being a missing value in a specific feature variable depends on all the other feature variables within the dataset, but not on the feature variable of interest (Leke et al., 2014).", "startOffset": 195, "endOffset": 214}, {"referenceID": 31, "context": "MAR means the value for the feature variable is missing, but conditional on some other feature variable observed in the dataset, although not on the feature variable of interest (Scheffer, 2002).", "startOffset": 178, "endOffset": 194}, {"referenceID": 18, "context": "The non-ignorable case occurs when the chances of there being a missing entry in variable, X2 for example, is influenced by the value of the variable X2 regardless of whether or not the other variables in the dataset are altered and modified [Leke et al. (2014), Allison (1999)].", "startOffset": 243, "endOffset": 262}, {"referenceID": 18, "context": "The non-ignorable case occurs when the chances of there being a missing entry in variable, X2 for example, is influenced by the value of the variable X2 regardless of whether or not the other variables in the dataset are altered and modified [Leke et al. (2014), Allison (1999)].", "startOffset": 243, "endOffset": 278}, {"referenceID": 21, "context": "In this case, if we have a dataset with variables as in Table 1, it is said to be a monotone missing pattern if a variable Xj is observed for a particular scenario, and this implies that all the previous variables Xk, where k < j, are also observed for that scenario (Little and Rubin, 2014).", "startOffset": 267, "endOffset": 291}, {"referenceID": 21, "context": "2 Missing Data Patterns There are two main missing data patterns defined by Little and Rubin (2014). These patterns are the arbitrary and monotone missing data patterns.", "startOffset": 76, "endOffset": 100}, {"referenceID": 7, "context": "It is also based on the unsupervised learning of multiple levels of features or representations of the input data whereby higher-level features are obtained from lower level features to yield a hierarchical representation of the data (Deng and Yu, 2014).", "startOffset": 234, "endOffset": 253}, {"referenceID": 6, "context": "Deep Learning comprises of several algorithms in machine learning that make use of a cataract of nonlinear processing units organized into a number of layers that extract and transform features from the input data [Deng et al. (2013), Deng and Yu (2014)].", "startOffset": 215, "endOffset": 234}, {"referenceID": 6, "context": "Deep Learning comprises of several algorithms in machine learning that make use of a cataract of nonlinear processing units organized into a number of layers that extract and transform features from the input data [Deng et al. (2013), Deng and Yu (2014)].", "startOffset": 215, "endOffset": 254}, {"referenceID": 34, "context": "In Zhang et al. (2011), it is suggested that information within incomplete cases, that is, instances with missing values be used when estimating missing", "startOffset": 3, "endOffset": 23}, {"referenceID": 28, "context": "In Zhang (2011), the shell-neighbor method is applied in missing data imputation by means of the Shell-Neighbor Imputation (SNI) algorithm which is observed to perform better than the kNearest Neighbor imputation method in terms of imputation and classification accuracy as it takes into account the left and right nearest neighbors of the missing data as well as varying number of nearest neighbors contrary to k-NN that considers just fixed k nearest neighbors.", "startOffset": 3, "endOffset": 16}, {"referenceID": 21, "context": "Rana et al. (2015) use robust regression imputation for missing data in the presence of outliers and investigate its effectiveness.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Abdella and Marwala (2005) implement a hybrid genetic algorithm-neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while Aydilek and Arslan (2012) create a hybrid k-Nearest Neighbor-Neural Network system for the same purpose.", "startOffset": 0, "endOffset": 27}, {"referenceID": 0, "context": "Abdella and Marwala (2005) implement a hybrid genetic algorithm-neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while Aydilek and Arslan (2012) create a hybrid k-Nearest Neighbor-Neural Network system for the same purpose.", "startOffset": 0, "endOffset": 222}, {"referenceID": 0, "context": "Abdella and Marwala (2005) implement a hybrid genetic algorithm-neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while Aydilek and Arslan (2012) create a hybrid k-Nearest Neighbor-Neural Network system for the same purpose. In some cases, neural networks were used with Principal Component Analysis (PCA) and genetic algorithm as in Mistry et al. (2009), Mohamed et al.", "startOffset": 0, "endOffset": 431}, {"referenceID": 0, "context": "Abdella and Marwala (2005) implement a hybrid genetic algorithm-neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while Aydilek and Arslan (2012) create a hybrid k-Nearest Neighbor-Neural Network system for the same purpose. In some cases, neural networks were used with Principal Component Analysis (PCA) and genetic algorithm as in Mistry et al. (2009), Mohamed et al. (2007) and Nelwamondo et al.", "startOffset": 0, "endOffset": 454}, {"referenceID": 0, "context": "Abdella and Marwala (2005) implement a hybrid genetic algorithm-neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while Aydilek and Arslan (2012) create a hybrid k-Nearest Neighbor-Neural Network system for the same purpose. In some cases, neural networks were used with Principal Component Analysis (PCA) and genetic algorithm as in Mistry et al. (2009), Mohamed et al. (2007) and Nelwamondo et al. (2007). Leke et al.", "startOffset": 0, "endOffset": 483}, {"referenceID": 0, "context": "Abdella and Marwala (2005) implement a hybrid genetic algorithm-neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while Aydilek and Arslan (2012) create a hybrid k-Nearest Neighbor-Neural Network system for the same purpose. In some cases, neural networks were used with Principal Component Analysis (PCA) and genetic algorithm as in Mistry et al. (2009), Mohamed et al. (2007) and Nelwamondo et al. (2007). Leke et al. (2014) use a hybrid of Auto-Associative neural networks or autoencoders with", "startOffset": 0, "endOffset": 503}, {"referenceID": 24, "context": "Novel algorithms for missing data imputation and comparisons between existing techniques can be found in papers such as Schafer and Graham (2002), Liew et al.", "startOffset": 120, "endOffset": 146}, {"referenceID": 15, "context": "Novel algorithms for missing data imputation and comparisons between existing techniques can be found in papers such as Schafer and Graham (2002), Liew et al. (2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al.", "startOffset": 147, "endOffset": 166}, {"referenceID": 15, "context": "Novel algorithms for missing data imputation and comparisons between existing techniques can be found in papers such as Schafer and Graham (2002), Liew et al. (2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al.", "startOffset": 147, "endOffset": 180}, {"referenceID": 14, "context": "(2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al.", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": "(2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al.", "startOffset": 45, "endOffset": 71}, {"referenceID": 4, "context": "(2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al.", "startOffset": 45, "endOffset": 90}, {"referenceID": 4, "context": "(2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al. (2010) and Kalaycioglu et al.", "startOffset": 45, "endOffset": 111}, {"referenceID": 4, "context": "(2011), Myers (2011), Lee and Carlin (2010), Baraldi and Enders (2010), Van Buuren (2012), Jerez et al. (2010) and Kalaycioglu et al. (2015).", "startOffset": 45, "endOffset": 141}, {"referenceID": 13, "context": "It is typically represented by an affine mapping and subsequently a nonlinearity, f\u03b8 (x) = s (Wx+ b) (Isaacs, 2014).", "startOffset": 101, "endOffset": 115}, {"referenceID": 13, "context": "In the decoder phase, y being the hidden representation is remapped to z which is a vector reconstruction in the input space with z = g\u03b8\u2032 (y) (Isaacs, 2014).", "startOffset": 142, "endOffset": 156}, {"referenceID": 13, "context": "The function g\u03b8\u2032 is the decoder function which is an affine mapping deliberately ensued by a non-linearity with squashing traits that either follows the form g\u03b8\u2032 (y) = W \u2032y + b\u2032 or g\u03b8\u2032 (y) = s (W \u2032y + b\u2032) with the parameter set \u03b8 \u2032 comprising of the transpose of the weights and biases from the encoder (Isaacs, 2014).", "startOffset": 303, "endOffset": 317}, {"referenceID": 13, "context": "z from step 1 is not explained as a rigorous regeneration of x but rather as the parameters of a distribution p (X|Z = z) in probabilistic terms, that may yield x with high probability (Isaacs, 2014).", "startOffset": 185, "endOffset": 199}, {"referenceID": 5, "context": "This equation could also be written as \u03b4AE (\u03b8) = \u03a3tL ( x(t), g\u03b8 ( f\u03b8 ( x(t) ))) (Bengio et al., 2013).", "startOffset": 80, "endOffset": 101}, {"referenceID": 5, "context": "] averages over the corrupted examples x\u0303 drawn from a corruption process q ( x\u0303|x(t) ) (Bengio et al., 2013).", "startOffset": 88, "endOffset": 109}], "year": 2015, "abstractText": "In the last couple of decades, there has been major advancements in the domain of missing data imputation. The techniques in the domain include amongst others: Expectation Maximization, Neural Networks with Evolutionary Algorithms or optimization techniques and K-Nearest Neighbor approaches to solve the problem. The presence of missing data entries in databases render the tasks of decision-making and data analysis nontrivial. As a result this area has attracted a lot of research interest with the aim being to yield accurate and time efficient and sensitive missing data imputation techniques especially when time sensitive applications are concerned like power plants and winding processes. In this article, considering arbitrary and monotone missing data patterns, we hypothesize that the use of deep neural networks built using autoencoders and denoising autoencoders in conjunction with genetic algorithms, swarm intelligence and maximum likelihood estimator methods as novel data imputation techniques will lead to better imputed values than existing techniques. Also considered are the missing at random, missing completely at random and missing not at random missing data mechanisms. We also intend to use fuzzy logic in tandem with deep neural networks to perform the missing data imputation tasks, as well as different building blocks for the deep neural networks like Stacked Restricted Boltzmann Machines and Deep Belief Networks to test our hypothesis. The motivation behind this article is the need for missing data imputation techniques that lead to better imputed values than existing methods with higher accuracies and lower errors.", "creator": "LaTeX with hyperref package"}}}