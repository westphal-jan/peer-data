{"id": "1512.01715", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding", "abstract": "listener-supported This sadyrkulov paper frankton presents siocon a restricted 4.7 visual Turing squabbling test (VTT) gbl for story - kashruth line based jasieniec deep understanding in long - term 84-page and fimian multi - doughs camera captured videos. loht Given antiterrorism a ancon set of bikeways videos of solio a descente scene (gr\u00fcnerl\u00f8kka such walvis as 4,162 a traditional multi - room office, yapped a trishula garden, two-part and lochnagar a parking lot.) and a sta.lucia sequence of story - line 0.059 based queries, 153rd the task is dapsone to 1.325 provide answers either simply komiyama in berezutsky binary ccac form \" 7-for-8 true / false \" (to battleground a polar spdc query) abundance or schom in weinhard an accurate probolinggo natural wex language hyperboreans description (phagocytosis to a fragmenting non - luxon polar saada query ). Queries, polar ginastera or non - polar, foe consist of mopan view - based queries which lukaszewicz can high-tide be answered mwawa from a particular albon camera piraro view zellman and offended scene - centered rewey queries which k.e. involves joint barek inference dangxiang across drl different cameras. sumitani The trigger story vaulting lines are umfraville collected erebuni to smartha cover debatably spatial, skibby temporal troupers and skating causal mash-up understanding 33 of geonosis input videos. postech The ntpc data and ascetical queries meathead distinguish galtier our VTT from recently minamata proposed visual belkin question answering in ntagweek.htm images and reverend video viamonte captioning. A 78.68 vision jorgen system bernays is proposed to perform komuro joint mescheloff video citytrust and query parsing which integrates ecofin different dkr vision modules, marculescu a 41.14 knowledge 5.38 base deok and hassen a floodwalls query engine. The \u00e2m system provides egill unified chac interfaces non-prescription for 3/1st different vacuum-tube modules so laneway that cbs-affiliated individual modules can nieder be reconfigured polyphase to test soybeans a lonn new method. gorillas We provide macdowell a hboc benchmark 1968/1969 dataset armah and a toolkit for ontology guided 170.50 story - upe line laul query generation which consists kwc of damurphy about urell 93. 5 damiani hours videos captured bynes in four different fagus locations neuropsychologist and 3, cegielnia 426 humanly queries sfsu split clapham into guillaume 127 aerial story lines. We beatitude also provide a 238.6 baseline fray implementation stawiski and result analyses.", "histories": [["v1", "Sun, 6 Dec 2015 00:40:02 GMT  (1012kb,D)", "https://arxiv.org/abs/1512.01715v1", null], ["v2", "Wed, 16 Dec 2015 19:19:25 GMT  (1851kb,D)", "http://arxiv.org/abs/1512.01715v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hang qi", "tianfu wu", "mun-wai lee", "song-chun zhu"], "accepted": false, "id": "1512.01715"}, "pdf": {"name": "1512.01715.pdf", "metadata": {"source": "CRF", "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding", "authors": ["Hang Qi", "Tianfu Wu", "Mun-Wai Lee", "Song-Chun Zhu"], "emails": ["hangqi@cs.ucla.edu,", "szchu}@stat.ucla.edu", "mlee@i-a-i.com"], "sections": [{"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Motivation and Objective", "text": "During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]\n\u2217Contribute equally to this work.\nwere proposed. Those tasks are evaluated based on either classification or detection accuracy, focusing on a coarse level understanding of data. In the area of natural language and text processing, there have been well-studied text-based question answering (QA). For example, a chatterbot named Eugene Goostman1 was reported as the first computer program which has passed the famed Turing test [36] in an event organized at the University of Reading. The success of text-based QA and the recent achievements of individual vision modules have inspired visual Turing tests (VTT) [9, 25] where image-based questions (so-called visual question answering, VQA) or story-line queries are used to test a computer vision system. VTT has been suggested as a\n1https://en.wikipedia.org/wiki/Eugene_Goostman\nar X\niv :1\n51 2.\n01 71\n5v 2\n[ cs\n.C V\n] 1\n6 D\nec 2\nmore suitable evaluation framework going beyond measuring the accuracy of labels and bounding boxes. Most existing work on VTT focus on images and emphasize free-form and open-ended Q/A\u2019s [2, 1].\nIn this paper, we are interested in a restricted visual Turing test (VTT) \u2013 story-line based visual query answering in long-term and multi-camera captured videos. Our VTT emphasizes a joint spatial, temporal, and causal understanding of scenes and events, which are largely unexplored in computer vision. By \u201crestricted\u201d, we mean the queries are designed based on a selected ontology. Figure 1 shows two examples in our VTT dataset. Consider the question how we shall test whether a computer vision system understands, for example, a conference room. In VQA [1], the input is an image and a \u201cbag-of-questions\u201d (e.g., is this a conference room?) and the task is to provide a natural language answer (either in a multiple-choice manner or with free-form responses). In our VTT, to understand a conference room, the input consists of multi-camera captured videos and storyline queries covering basic questions (e.g., Q1, for a coarse level understanding) and difficult ones (e.g., Qk) involving spatial, temporal, and causal inference for a deeper understanding. More specifically, to answer Qk correctly, a computer vision system would need to build a scene-centered representation for the conference room (i.e., put chairs and tables in 3D), to detect, track, re-identify, and parse people coming into the room across cameras, and to understand the concept of sitting in a chair (i.e., the pose of a person and scene-centered spatial relation between a person and a chair), etc. If a computer vision system can further unfold the intermediate representation to explicitly show how it derives the answer, it enhances the \u201ctrust\u201d that we have on the system that it has gain a correct understanding of the scene.\nWeb-scale images vs. long-term and multi-camera captured videos. Web-scale images emphasize the breadth that a computer vision system can learn and handle in different applications. Those images are often of album photo styles collected from different image search engines such as Flickr, Google, Bing, and Facebook. This paper focuses on long-term and multi-camera captured videos usually produced by video surveillance, which are also important data sources in the visual big data epic and have important security or or law enforcement applications. Furthermore, as the example in Figure 1 shows, mutli-camera videos can facilitate a much deeper understanding of scenes and events. The two types of datasets are complementary, but the latter has not been explored in a QA setting.\nFree-form and open-ended Q/A\u2019s vs. restricted storyline based queries. Free-form and open-ended Q/A\u2019s are usually collected through crowd-sourcing platforms like Amazon Mechanical Turk (MTurk) to achieve diversities. However, it is hard to obtain well-posed pairs from a massive amount of untrained workers on the Internet. This\nis challenging even for simple tasks like image labeling as investigated in the ImageNet dataset [4] and the LabelMe dataset [16]. For the video datasets in this paper, it is impractical to use MTurk to collect story-line based queries covering long-term temporal ranges and across multi-cameras. Instead, we adopt a selected yet sufficiently expressive ontology (shown in Figure 3) in generating queries. Following the statistical principles stated in Geman et al.\u2019s Turing test framework [9], we design a easyto-use toolkit by which several people with certain expertise can create a large number of story lines covering different interesting and important spatial, temporal and, causal aspects in videos with the quality of queries and answers controlled.\nQuest for an integrated vision system. Almost all the recent methods proposed for image captioning and VQA are based on the combination of convolutional neural network [21, 17] and recurrent neural network like long shortterm memory [14]. On the one hand, it is exciting to see much progress have been made in terms of performance. On the other hand, it shows the restricted setting of the tasks in image captioning and VQA. The proposed VTT entails an integrated vision system which cannot be handled by training convolutional and recurrent neural networks directly, to the best of our knowledge. We present a prototype vision system as our baseline implementation which integrates different vision modules (where the state-of-the-art CNN based components can be applied), a knowledge base, and a query engine."}, {"heading": "1.2. Overview", "text": "Figure 2 illustrates a systematic overview of the proposed VTT which consists of four components: i) Multi-camera video dataset collection: Existing datasets are either focusing on single individual images or short video sequences with clear action or event boundaries. Our multiple-camera video dataset includes a rich set of activities in both indoor and outdoor scenes. Videos are collected by multiple cameras with overlapping field-of-views during the same time window. A variety types of sensors are used: stationary HD video cameras located on the ground and rooftop, moving cameras mounted on bicycles and automobiles, and infrared cameras. The camera parameters are provided as meta data. The videos capture daily activities of a group of people and different events in a scene which include routine ones (e.g., an ordinary group launch, playing four square soccer game) and abnormal ones (e.g., evacuating from a building during a fire alarm) with large appearance and structural variations exhibited.\nii) Ontology guided story-line based query/answer collection: We are interested in a selected ontology as listed in Figure 3. The ontology is sufficiently expressive to represent different aspects of spatial, temporal, and causal understanding in videos from basic level (e.g., identifying objects\nand parts) to fine-grained level (e.g., does person A have a clear-line-of-sight to person B?). Based on the ontology, we build a toolkit for story-line query generation following the statistical principles stated in [9]. Queries organized in multiple story lines are designed to evaluate a computer vision system from basic object detection queries to more complex relationship queries, and further probe the system\u2019s ability in reasoning from the physical and social perspectives, which entails human-like commonsense reasoning. Crosscamera referencing queries requires the ability to integrate visual signals from multiple overlapping sensors.\niii) Integrated vision system: We build a computer vision system that can be used to study the organization of modules designed for different tasks and interactions between them to improve the overall performance. It is designed with two principles in mind: first, well-established computer vision tasks shall be incorporated so that we can built upon the existing achievements; second, the modules shall\nbe loosely coupled so that it allows user to replace one or more modules with alternatives to study the performance in an integrated environment. We define a set of APIs for each individual task and connect all modules into a pipeline. After the system has processed the input videos and saved the results in its knowledge-base, it fetches queries from the evaluation server one after another at the testing time.\niv) Q/A evaluation server: We provide a web service API through which a computer vision system can interact with the evaluation server over HTTP connections. The evaluation server iterates through a stream of queries grouped by scenes. In each scene, queries are further grouped into story lines. A query is not available to the system until the previous story lines and all previous queries in the same story line have finished. The correct answer is provided to the system after each query. This information can be used by the system to be adaptive with the ability to learn from the provided answers. The answer can be used to\nupdate the previous understanding such that any conflict has to be resolved and wrong interpretations can be discarded.\nFigure 4 shows an example of a full workflow of our system. We have spent more than 30 person-year in total to collect the data and build the whole system. Our prototype system has passed a detailed third-party evaluation involving more than 1,000 queries. We plan to release the whole system to the computer vision community and organize competition and regular workshop in the near future."}, {"heading": "2. Related Work and Our Contributions", "text": "Question answering is the natural way of effective communication between human beings. Integrating computer vision and natural language processing, as well as other modal knowledge, has been a hot topic in the recent development of deeper image and scene understanding.\nVisual Turing Test. Inspired by the generic Turing test principle in AI [36], Geman et al. proposed a visual Turing test [9] for object detection tasks in images which organizes queries into story lines, within which queries are connected and the complexities are increased gradually \u2013 similar to conversations between human beings. In a similar\nspirit, Malinowski and Fritz [24, 25] proposed a multi-word method to address factual queries of scene images. In the dataset and evaluation framework proposed in this paper, we adopt similar evaluation structure to [9], but focus on a more complex scenario which features videos and overlapping cameras to facilitate a broader scope of vision tasks.\nImage Description and Visual Question Answering. To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently. The state-of-the-art methods have shown, however, a coarse level understanding of an image (i.e., labels and bounding boxes of appeared objects) together with natural language n-gram statistics suffices to generate reasonable captions. Microsoft COCO [22] provides descriptions or captions for images. Question answering focuses on specific contents on the image and evaluate the system\u2019s abilities using human generated question. Unlike the image description task where a generated sentence is consider correct as long as it describes the dominant objects and activities in the image, human generated questions can ask all details and even hidden knowledge that require deduction. In such scenario, a pre-trained end-to-end sys-\ntem may not necessarily perform well as the question space is too large to be covered by training data. IQA [30] converts image descriptions into Q/A pairs. VQA [1] evaluates in a free-formed and open-ended questions about images, where the question-answer pairs are given by human annotators. Although it encourages participants to pursuit a deep and specific understanding about the image, it only focuses on the content of the image and does not address many other fundamental aspects of computer vision like 3D scene parsing, camera registration, etc. Moreover, actions are not static concepts, temporal information are largely missing in images. Our Contributions: This paper makes two main contribution to deep scene and event understanding:\ni) It presents a new visual Turing test benchmark consisting of a long-term and multi-camera captured video dataset and a large number of ontology-guided storyline based queries.\nii) It presents a prototype integrated vision system consisting of a well-designed architecture, various vision modules, a knowledge base, and a query engine."}, {"heading": "3. Dataset", "text": "In this section, we introduce the video dataset we collected for the VTT. In our dataset, we organize data by multiple independent scenes. Each scene consists of video footage from eight to twelve cameras with overlapping fields of view during the same time period. By now, we have a total number of 14 collections captured at 4 different locations: two indoor (an office and an auditorium) and two outdoor (a parking lot and a garden). Table 1 gives a summary of the data collections.\nOur dataset reflects real-world video surveillance data and poses unique challenges to modern computer vision algorithms:\nVaried number of entities. In our dataset, activities in the scene could involve individuals as well as multiple interacting entities.\nRich events and activities. The activities captured in the dataset involves different degrees of complexities: from the simplest single-person actions to the group sport activities which involve as many as dozens of people.\nUnknown action boundary. Unlike existing action or activity dataset where each action data point is well segmented and each segment only contains one single action, our dataset consists of multiple video streams. Actions and activities are not pre-segmented and multiple actions may happen at the same time. Such characteristic preserves more information about the spatial context of one action and correlation between multiple actions.\nMultiple overlapping cameras. This requires the system to perform multi-object tracking across multiple cameras with re-identification and 3D geometry reasoning.\nVaried scales and view points. Most of our data are collected in 1920x1080 resolution, however, because of the difference in cameras\u2019 mounting points, a person who only occupies a couple of hundred pixels in bird\u2019s-eye views may occlude the entire view frame when he or she stands very close to a ground camera.\nIllumination variation. Areas covered by different cameras have different illumination conditions: some areas are covered by dark shadows whereas some other areas have heavy reflection.\nInfrared cameras and moving cameras. Apart from regular RGB signals, our dataset provides infrared videos as a supplementary. Moving cameras (i.e., cameras mounted on moving objects) also provide additional challenges to the\ndataset and reveal more spatial structure of the scene. The complexity of our VTT dataset. To demonstrate the difficulties of our dataset, we conduct a set of experiments on a typical subset of data using the state-of-the-art object detection models [31] and multiple-object tracking methods [29]. A summary of the data and results are shown in Table 2."}, {"heading": "4. Queries", "text": "A query is a first-order logic sentence (with modification) composed using variables, predicates (as shown in Figure 3), logical operators (\u2227,\u2228,\u00ac), arithmetic operators, and quantifiers (\u2203 and \u2200). The answer to a query is either true or false meaning whether the fact stated by the sentence holds given the data and the system\u2019s state of belief. The formal language representation eliminates the need of natural language processing and allows us to focus computer vision problems on a constrained set of predicates.\nWe evaluate computer vision systems by asking a sequence of queries organized into multiple story lines. Each story line explores a natural event across a period of time in a way similar to conversations between humans. At the beginning of a story line, major objects of interest are defined first. The vision system under evaluation shall indicate whether it detects these objects. A correct detection establishes a mutual conversation context for consecutive queries, which ensures the vision system and queries are referring to the same objects in later interactions. When the system fails to detect an object, however, the evaluation server will skip the queries regarding that object. Because neither answering these queries correctly nor wrongly reveals the system\u2019s performance in interpreting the designated data.\nObject definition queries. To define an object, specifications of object type, time, and location are three components. Object type is specified by object predicates in\nthe ontology. A time t is either a view-centric frame number in a particular video or a scene-centric wall clock time. A location is either a point (x, y) or a bounding box (x1, y1, x2, y2) represented by its two diagonal points, where a point can be specified either in view-centric coordinates (i.e. pixels) or in scene-centric coordinates (i.e. latitude-longitude, or coordinates in a customized reference coordinate system, if defined). For example, an object definition query regarding a person in the form of first-order logic sentence would look like:\n\u2203p person(p; time = t; location = (x1, y1, x2, y2)) when the designated location is a bounding box. Note that the statements made by object definition queries are always true, as they aim to establish the conversation context.\nNon-definition queries. Non-definition queries in a story line explores a system\u2019s spatial, temporal and causal understanding of events in a scene regarding the detected objects. The query space consists of all possible combinations of predicates in the ontology with the detected objects (and/or objects interacting with the detected ones) being the arguments. When expressing complex activities or relationships, multiple predicates are typically conjuncted by \u2227 to form a query. For example, suppose M1 and F1 are two detected people confirmed by object detection queries, the following query states \u201cM1 is a male, F1 is a female, and there is a clear line of sight between them at time t1\u201d:\nmale(M1)\u2227female(F1)\u2227clear-line-of-sight(M1, F1; time = t1).\nNote that the location is not specified, because once M1 and F1 is identified and detected, we assume the vision system can track them over time.\nMoreover, story lines unfold fine-grained knowledge about the event in the scene as it goes. In particular, given the detected objects and established context, querying about objects interacting with the detected ones becomes unambiguous. As in the example shown in Figure 4, even the ball is not specified by any object definition queries (and actually it is hard to detect the ball even the position is given), once the two people interacting with the ball are identified, it becomes legitimate to ask if \u201cthe female catches a ball at time t2\u201d:\n\u2203b ball(b) \u2227 catching(F1, b; time = t2), and if \u201cthe male and female are playing a ball game together over the period of t1 to t2\u201d:\ngame(M1, F1; time = (t1, t2)).\nTimes and locations are specified the same way as in object definition queries with an extension that a time period (t1, t2) can be specified by a starting time and a ending time.\nCorrectly answering such queries is non-trivial as it requires joint cognitive reasoning based on spatial, temporal, and casual information across multiple cameras over a time period.\nIn non-polar cases, we support three types of questions: \u201cwhat\u201d, \u201cwhen\u201d, and \u201cwhere\u201d, to which the answers are object labels, time intervals, and location polygons, respectively.\nCurrently, we have created 3,426 queries in the dataset. Figure 5 shows the distribution of predicates in selected categories. Though we try to be unbiased in general, we do consider some predicates are more common in and important than others and thus make the distribution non-uniform. For example, among all occurrence of object predicates, \u201cperson\u201d takes 55.9%, which is reasonable because human activities are our major point of interest. Meanwhile, we are also building a query generation toolkit on the top of Vatic [37] for rapid query creation with respect to the statistical properties discussed by Geman et al. in [9]. In the implementation, queries are presented in the form of XML documents as shown in Figure 6 for easy parsing."}, {"heading": "5. System", "text": "We designed and implemented a computer vision system to perform the test as shown in Figure 2. It consists of three major parts: an offline parsing pipeline which decompose the visual perception into multiple sub-tasks, a knowledge base which stores parsing results (including entities, properties, and relations between them), and a query engine which answers queries by searching the knowledge base. The system also features a flexible architecture and a visualization toolkit."}, {"heading": "5.1. Offline parsing pipeline", "text": "Offline parsing pipeline processes the multiple-view videos. Each view is first processed by a single-view parsing pipeline where video sequences from multiple cameras are handled independently. Then multiple-view fusion matches tracks from multiple views, reconciles results from single-view parsing, and generates scene-based results for answering questions.\nTo take advantage of achievements in various sub-areas in computer vision, we organize a pipeline of modules, each of which focuses on one particular group of predicates by generating corresponding labels for the input data. Every\nmodule gets access to the original video sequence and products from previous modules in the pipeline. The implemented modules are described as follows. Most components are derived from the state-of-the-art methods at the time we developed the system last year.\nScene parsing generates a homography matrix for each sensor by camera calibration and also produces estimated depth map and segmentation label map for each camera view. The implementation is derived from [23].\nObject detection [34, 31] processes the video frames and generates bounding boxes for major objects of interest.\nMultiple object tracking [29] generates tracks for all detected objects.\nHuman attributes [28] classifies appearance attributes of detected human including gender, color of clothes, type of clothes, and accessories (e.g. hat, backpack, glasses).\nAction detection detects human actions and poses in the scene. The implementation is derived form [42, 43, 40].\nBehavior detection parses human-human, humanscene, and human-object interactions.\nVehicle parsing [41, 15, 13] produces bounding boxes and fluent labels for specific parts of detected cars (e.g. fender, hood, trunk, windows, lights).\nMultiple-view fusion merges the tracks and bounding boxes from multiple views based on appearance and geometry cues.\nThe middle-left part of Figure 4 shows the dependencies between these modules in the system."}, {"heading": "5.2. Knowledge base and query answering", "text": "We employ a generic graph-based data model to store knowledge. The detected objects, actions, attribute labels are all modeled as nodes, the connections between them are modeled as edges. In our implementation, the parsing results are stored into Resource Description Framework (RDF) graphs [38], in the from of triple expressions, which can be queried by a standard query language SPARQL [39]. Given that the questions are formal language, our query engine first parses the query and transforms the query into a sequence of SPARQL statements. Apache Jena [27] is used to execute these statements and to return answers derived from the knowledge base. Figure 8 shows the architecture of query engine.\nIn practice, it is infeasible to pre-calculate all possible predicates and save each individual knowledge segment into the knowledge base. For example, pre-calculating all \u201cclear-line-of-sight(x, y)\u201d relationships would involve pairwise combination across all detected humans. This strategy is obviously inefficient in that the portion of data being queried with this predicate is actually sparse. Alternatively, we designed a online computation module which evaluates binary and trinary relationships only at the testing time when such predicates appear in a query.\nEvaluation protocols. The computer vision system talks to the evaluation server over HTTP connections. At the beginning of the evaluation, the system first acquires an session id from the evaluation server. Then the system repeatedly request the next available scene, storyline, query in the session from the evaluation server. In this protocol, the evaluation server maintains the states of evaluation sessions internally and ensures the vision system cannot overwrite the submitted answer to any query."}, {"heading": "5.3. Design Decisions", "text": "The system is architected with two goals bearing in mind: first, we want to incorporate existing tasks in computer vision; second, the architecture shall be flexible enough for replacing a module with alternatives to pursuit incremental improvements later. To this end, we defined a set of APIs for each vision task and connect all the modules using remote procedure calls (RPC). This enables the system to only focus on the logical connection between mod-\nules and provides the implementation flexibility for individual components. In practice, we deploy all modules onto different dedicated machines. Under the RPC interfaces, computation-intensive algorithms usually utilize GPU and MPI internally to pursuit faster calculation and data parallelism. This design allows us to use this system as an experiment platform by switching between alternative models and implementations for studying their effects and contributions to query answering.\nTo make the system easy to use, we also developed a dashboard with visualization tools for rapid development and experiment. Figure 7 shows a screenshot of the visualization."}, {"heading": "6. Evaluation", "text": "Our prototype system has been evaluated by an independent third-party company which collected the datasets and created 1,160 polar queries in a subset of data (see the upper parts in Table 3). The company was invited to administrate the independent test under the same grant on which we worked. During the test, the testing data was available to our system two weeks before the story-line query evaluation. We performed the offline parsing within the two weeks by deploying our system on a small cluster consisting of 10 workstations. During the evaluation, our system did not utilize the ground-truth answers received after each response for consecutive queries.\nAmong the 1,160 queries, 243 queries are object def-\ninitions, 197 (81%) of which are successfully detected, For non-definition queries, we either provided binary \u201ctrue/false\u201d answers or claimed \u201cunable to respond\u201d (when our implementation cannot handle or recognize some of the predicates involved in a query). Table 3 shows the accuracy as the ratio of correctly answered queries to number of the responded non-definition queries. Note that during the evaluation, for simplicity, the object definition queries are not included in the accuracy calculation, because they aim to establish mutual knowledge for consecutive queries in the story line, which ensures the evaluation server and the system are discussing the same objects. Therefore, the ground-truth answers to these queries are actually always \u201ctrue\u201d. One can obtain an 100% accuracy in object definition queries by a trivial method (answering \u201ctrue\u201d at all times) with the risk of not discussing the same objects in consecutive queries. Now, we are extending this by generating more object definition queries to which the answers can be \u201cfalse\u201d for evaluating detection performance. These queries does not serve to establish conversation context, therefore for the story lines starting with an object definition query whose ground-truth answer is false, we randomly sample the predicates and relations to generate the remaining queries.\nFigure 9 further breakdowns the accuracy by the number of unique predicates and the category of predicates in a query, respectively.\nBreakdown by number of predicates. Most queries have either one, two, or three predicates. This is a natural result of the choice to avoid overcomplicating the queries. As the number of predicates increases, the accuracy of our prototype system decreases, since a wrong prediction in any of the predicates may cause answering the query incorrectly. The queries with one, two, or three predicates can mostly be explained as follows:\ni) One predicate: These are queries that deal only with the predicates for the various types of objects (people, car, etc.). Most of these queries (243) are object definition queries; the others (46) deal with counting objects (e.g., \u201chow many people are in the scene?\u201d).\nii) Two predicates: These queries are mostly queries involving unary predicates operating on an object. One predicate is used to define the object (usually person or automobile), and the unary predicate is the second predicate involved.\niii) Three predicates: These queries are mostly queries involving binary predicates operating on two objects. Two predicates are used to define the operands, and the binary\npredicate is the third predicate involved. Breakdown by category. When looking at the accuracy by categories, our prototype system perform well in classic computer vision tasks (detection, part-of relations, actions, behaviors). However, queries involving spatial reasoning and interactions between human and objects or scene are still challenging and open to further research."}, {"heading": "7. Discussion and Conclusion", "text": "This paper presented a restricted visual Turing test (VTT) for deeper scene and event understanding in longterm and multi-camera videos. Our VTT emphasizes a joint spatial, temporal and causal understanding by utilizing scene-centered representation and story-line based queries. The dataset and queries distinguish the proposed VTT from the recent proposed visual question answering (VQA). We also presented a prototype integrated vision system which obtained reasonable results in our VTT.\nIn our on-going work, we are generating more story-line based queries and setting up a website for holding a VTT competition. In the proposed competition, we will release the whole system as a playground. Our system architecture allows a user to substitute one or more modules with their own methods and then run through the VTT to see the improvements. One of our next steps is to create a publicly available \u201cvision module market\u201d where researchers can evaluate different individual components from the VTT perspective besides the traditional metrics.\nAcknowledgement. This work is supported by DARPA MSEE FA 8650-11-1-7149 and DARPA SIMPLEX N66001-15-C-4035. We would like to thank Josh Walters and his colleagues at BAE Systems, the third-party collaborator in the project who administrated the test, Alexander Grushin and his colleagues at I-A-I for the effort in testing the system. We also thank members in the VCLA Lab at UCLA who contributed perception algorithms in their published work into the baseline test."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizwiz: nearly real-time answers to visual questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "User Interface Software and Technology, pages 333\u2013342,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical semantic indexing for large scale image retrieval", "author": ["J. Deng", "A.C. Berg", "F. Li"], "venue": "CVPR, pages 785\u2013792,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.A. Eslami", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, 111(1):98\u2013136,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "S.M.M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D.A. Forsyth"], "venue": "ECCV, pages 15\u201329,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "TPAMI, 32(9):1627\u20131645, Sept.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "The pyramid match kernel: Efficient learning with sets of features", "author": ["K. Grauman", "T. Darrell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing 3d objects in cluttered images", "author": ["M. Hejrati", "D. Ramanan"], "venue": "NIPS, pages 602\u2013610,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning 3d object templates by quantizing geometry and appearance spaces", "author": ["W. Hu", "S. Zhu"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Undoing the damage of dataset bias", "author": ["A. Khosla", "T. Zhou", "T. Malisiewicz", "A.A. Efros", "A. Torralba"], "venue": "ECCV, pages 158\u2013171,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1106\u20131114,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR, pages 1601\u20131608,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "MOTChallenge 2015: Towards a benchmark for multitarget tracking", "author": ["L. Leal-Taix\u00e9", "A. Milan", "I. Reid", "S. Roth", "K. Schindler"], "venue": "arXiv:1504.01942 [cs], Apr.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B.E. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W.E. Hubbard", "L.D. Jackel"], "venue": "Neural Computation, 1(4):541\u2013551,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Microsoft coco: Com- 10  mon objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Single-view 3d scene parsing by attributed grammar", "author": ["X. Liu", "Y. Zhao", "S.-C. Zhu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 684\u2013 691. IEEE,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS, pages 1682\u20131690,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "CoRR, abs/1410.8027,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Jena: A semantic web toolkit", "author": ["B. McBride"], "venue": "IEEE Internet computing, 6(6):55\u201359,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Attributed grammars for joint estimation of human attributes, part and pose", "author": ["S. Park", "S.-C. Zhu"], "venue": "Proc. of International Conference on Computer vision (ICCV),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Globallyoptimal greedy algorithms for tracking a variable number of objects", "author": ["H. Pirsiavash", "D. Ramanan", "C.C. Fowlkes"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1201\u20131208. IEEE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "arXiv preprint arXiv:1505.02074,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "ICCV, pages 433\u2013440,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), pages 1\u201342, April", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminatively trained and-or tree models for object detection", "author": ["X. Song", "T. Wu", "Y. Jia", "S.-C. Zhu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3278\u20133285. IEEE,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminatively trained and-or tree models for object detection", "author": ["X. Song", "T.-F. Wu", "Y. Jia", "S.-C. Zhu"], "venue": "CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, 59(236):433\u2013460,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1950}, {"title": "Action recognition by dense trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3169\u20133176. IEEE,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and-or models to represent context and occlusion for car detection and viewpoint estimation", "author": ["T. Wu", "B. Li", "S.-C. Zhu"], "venue": "IEEE Trans on Pattern Analysis and Machine Intelligence,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint action recognition and pose estimation from video", "author": ["B. Xiaohan Nie", "C. Xiong", "S.-C. Zhu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1293\u20131301,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Animated pose templates for modeling and detecting human actions", "author": ["B.Z. Yao", "B.X. Nie", "Z. Liu", "S.-C. Zhu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 36(3):436\u2013452,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "A reconfigurable tangram model for scene representation and categorization", "author": ["J. Zhu", "T. Wu", "S.-C. Zhu", "X. Yang", "W. Zhang"], "venue": "TIP, 2015 (Accepted)", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "A stochastic grammar of images", "author": ["S.-C. Zhu", "D. Mumford"], "venue": "Found. Trends. Comput. Graph. Vis., 2(4):259\u2013362, Jan.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 10, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 18, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 40, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 7, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 34, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 41, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 9, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 30, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 4, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 245, "endOffset": 248}, {"referenceID": 32, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 269, "endOffset": 273}, {"referenceID": 20, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 312, "endOffset": 324}, {"referenceID": 16, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 312, "endOffset": 324}, {"referenceID": 11, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 312, "endOffset": 324}, {"referenceID": 3, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 357, "endOffset": 360}, {"referenceID": 35, "context": "For example, a chatterbot named Eugene Goostman1 was reported as the first computer program which has passed the famed Turing test [36] in an event organized at the University of Reading.", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "The success of text-based QA and the recent achievements of individual vision modules have inspired visual Turing tests (VTT) [9, 25] where image-based questions (so-called visual question answering, VQA) or story-line queries are used to test a computer vision system.", "startOffset": 126, "endOffset": 133}, {"referenceID": 24, "context": "The success of text-based QA and the recent achievements of individual vision modules have inspired visual Turing tests (VTT) [9, 25] where image-based questions (so-called visual question answering, VQA) or story-line queries are used to test a computer vision system.", "startOffset": 126, "endOffset": 133}, {"referenceID": 1, "context": "Most existing work on VTT focus on images and emphasize free-form and open-ended Q/A\u2019s [2, 1].", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "Most existing work on VTT focus on images and emphasize free-form and open-ended Q/A\u2019s [2, 1].", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "In VQA [1], the input is an image and a \u201cbag-of-questions\u201d (e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "This is challenging even for simple tasks like image labeling as investigated in the ImageNet dataset [4] and the LabelMe dataset [16].", "startOffset": 102, "endOffset": 105}, {"referenceID": 15, "context": "This is challenging even for simple tasks like image labeling as investigated in the ImageNet dataset [4] and the LabelMe dataset [16].", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "\u2019s Turing test framework [9], we design a easyto-use toolkit by which several people with certain expertise can create a large number of story lines covering different interesting and important spatial, temporal and, causal aspects in videos with the quality of queries and answers controlled.", "startOffset": 25, "endOffset": 28}, {"referenceID": 20, "context": "Almost all the recent methods proposed for image captioning and VQA are based on the combination of convolutional neural network [21, 17] and recurrent neural network like long shortterm memory [14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 16, "context": "Almost all the recent methods proposed for image captioning and VQA are based on the combination of convolutional neural network [21, 17] and recurrent neural network like long shortterm memory [14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 13, "context": "Almost all the recent methods proposed for image captioning and VQA are based on the combination of convolutional neural network [21, 17] and recurrent neural network like long shortterm memory [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "Based on the ontology, we build a toolkit for story-line query generation following the statistical principles stated in [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 35, "context": "Inspired by the generic Turing test principle in AI [36], Geman et al.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "proposed a visual Turing test [9] for object detection tasks in images which organizes queries into story lines, within which queries are connected and the complexities are increased gradually \u2013 similar to conversations between human beings.", "startOffset": 30, "endOffset": 33}, {"referenceID": 23, "context": "In a similar spirit, Malinowski and Fritz [24, 25] proposed a multi-word method to address factual queries of scene images.", "startOffset": 42, "endOffset": 50}, {"referenceID": 24, "context": "In a similar spirit, Malinowski and Fritz [24, 25] proposed a multi-word method to address factual queries of scene images.", "startOffset": 42, "endOffset": 50}, {"referenceID": 8, "context": "In the dataset and evaluation framework proposed in this paper, we adopt similar evaluation structure to [9], but focus on a more complex scenario which features videos and overlapping cameras to facilitate a broader scope of vision tasks.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 76, "endOffset": 87}, {"referenceID": 17, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 76, "endOffset": 87}, {"referenceID": 25, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 76, "endOffset": 87}, {"referenceID": 31, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Microsoft COCO [22] provides descriptions or captions for images.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "IQA [30] converts image descriptions into Q/A pairs.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "VQA [1] evaluates in a free-formed and open-ended questions about images, where the question-answer pairs are given by human annotators.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "For Detection: AP is calculated as in PASCAL VOC 2012 [5] based on results by Faster-RCNN [31].", "startOffset": 54, "endOffset": 57}, {"referenceID": 30, "context": "For Detection: AP is calculated as in PASCAL VOC 2012 [5] based on results by Faster-RCNN [31].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "For Tracking: MOTA and MOTP are calculated as in Multiple Object Tracking Benchmark [20] based on results by [29].", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "For Tracking: MOTA and MOTP are calculated as in Multiple Object Tracking Benchmark [20] based on results by [29].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "To demonstrate the difficulties of our dataset, we conduct a set of experiments on a typical subset of data using the state-of-the-art object detection models [31] and multiple-object tracking methods [29].", "startOffset": 159, "endOffset": 163}, {"referenceID": 28, "context": "To demonstrate the difficulties of our dataset, we conduct a set of experiments on a typical subset of data using the state-of-the-art object detection models [31] and multiple-object tracking methods [29].", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "in [9].", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "The implementation is derived from [23].", "startOffset": 35, "endOffset": 39}, {"referenceID": 33, "context": "Object detection [34, 31] processes the video frames and generates bounding boxes for major objects of interest.", "startOffset": 17, "endOffset": 25}, {"referenceID": 30, "context": "Object detection [34, 31] processes the video frames and generates bounding boxes for major objects of interest.", "startOffset": 17, "endOffset": 25}, {"referenceID": 28, "context": "Multiple object tracking [29] generates tracks for all detected objects.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "Human attributes [28] classifies appearance attributes of detected human including gender, color of clothes, type of clothes, and accessories (e.", "startOffset": 17, "endOffset": 21}, {"referenceID": 38, "context": "The implementation is derived form [42, 43, 40].", "startOffset": 35, "endOffset": 47}, {"referenceID": 39, "context": "The implementation is derived form [42, 43, 40].", "startOffset": 35, "endOffset": 47}, {"referenceID": 36, "context": "The implementation is derived form [42, 43, 40].", "startOffset": 35, "endOffset": 47}, {"referenceID": 37, "context": "Vehicle parsing [41, 15, 13] produces bounding boxes and fluent labels for specific parts of detected cars (e.", "startOffset": 16, "endOffset": 28}, {"referenceID": 14, "context": "Vehicle parsing [41, 15, 13] produces bounding boxes and fluent labels for specific parts of detected cars (e.", "startOffset": 16, "endOffset": 28}, {"referenceID": 12, "context": "Vehicle parsing [41, 15, 13] produces bounding boxes and fluent labels for specific parts of detected cars (e.", "startOffset": 16, "endOffset": 28}, {"referenceID": 26, "context": "Apache Jena [27] is used to execute these statements and to return answers derived from the knowledge base.", "startOffset": 12, "endOffset": 16}], "year": 2015, "abstractText": "This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form \u201ctrue/false\u201d (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or nonpolar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines . We also provide a baseline implementation and result analyses.", "creator": "LaTeX with hyperref package"}}}