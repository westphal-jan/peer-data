{"id": "1512.03549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2015", "title": "Words are not Equal: Graded Weighting Model for building Composite Document Vectors", "abstract": "abaco Despite demoted the success 79-75 of demoralizes distributional ghq semantics, yoneda composing phrases from physiography word vectors lassies remains pawlawski an important challenge. anglica Several 81/100 methods have 37-38 been 1951 tried bortolini for cerrejon benchmark gawthorpe tasks tent such m-flo as fewster sentiment classification, reibey including word degregory vector averaging, ceremonious matrix - tyranny vector approaches based on parsing, and piskunov on - the - fly nautiloids learning hulwan of mai paragraph sujoy vectors. ignatiy Most a\u0161\u0161ur models kharian usually coeval omit stop words from the 1920s composition. Instead archfoes of counter-terrorism such an yes - no decision, we ballboy consider frost-free several graded schemes where tie-breaks words pentre are saiky\u014d weighted according to joans their discriminatory erkang relevance noncooperative with ngp respect 1/7th to its gezi use peetha in jianhua the superquinn document (batticaloa e. mecom g. , idf ). hamid Some of depailler these ovc methods (particularly rounders tf - idf) antagonist are 2,900 seen venturebeat to champs-\u00e9lys\u00e9es result in rickshaw a 286.6 significant nkufo improvement in performance rebraca over pechora prior hongli state of coryton the art. 31.10 Further, combining raso such 90mm approaches malin into wky an ensemble b\u00fcy\u00fck\u015fehir based on leafield alternate classifiers worths such mahallas as vahanam the RNN sequentially model, urasenke results bokeh in flickinger an 1. 6% moskau performance sundstroem improvement on aeria the bee-hive standard IMDB so\u010di movie mtetwa review dataset, 2 and a rompin 7. 01% kalsi improvement s.e.s. on Amazon aflutter product reviews. zmp Since 2,202 these are physican language free sipan models cercel and can be andalite obtained multi-spectral in an droemer unsupervised seductively manner, gaveled they netopia are 8:59 of 2,159 interest eagling also pcie for under - resourced hoda languages mizzen such as yowza Hindi almondvale as well gartel and forcefield many more putland languages. We demonstrate 115.25 the language free aspects by showing refreezing a gain of saltpetre 12% for cagan two jahl review raelians datasets over earlier hexion results, and lukic also nyhan release a new larger dataset for alian future testing (impulso Singh, lennart 2015 ).", "histories": [["v1", "Fri, 11 Dec 2015 08:44:45 GMT  (57kb)", "http://arxiv.org/abs/1512.03549v1", "10 Pages, 2 Figures, 11 Tables"]], "COMMENTS": "10 Pages, 2 Figures, 11 Tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["pranjal singh", "amitabha mukerjee"], "accepted": false, "id": "1512.03549"}, "pdf": {"name": "1512.03549.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["pranjals16@gmail.com", "amit@cse.iitk.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n03 54\n9v 1\n[ cs\n.C L\n] 1\n1 D\nec 2\nDespite the success of distributional semantics, composing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. Most models usually omit stop words from the composition. Instead of such an yes-no decision, we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document (e.g., idf). Some of these methods (particularly tf-idf) are seen to result in a significant improvement in performance over prior state of the art. Further, combining such approaches into an ensemble based on alternate classifiers such as the RNN model, results in an 1.6% performance improvement on the standard IMDB movie review dataset, and a 7.01% improvement on Amazon product reviews. Since these are language free models and can be obtained in an unsupervised manner, they are of interest also for underresourced languages such as Hindi as well and many more languages. We demonstrate the language free aspects by showing a gain of 12% for two review datasets over earlier results, and also release a new larger dataset for future testing (Singh, 2015)."}, {"heading": "1 Introduction", "text": "Language representation is a very crucial aspect to perform various NLP tasks and has been looked into great detail in recent times. Language representation models have fallen into broadly two categories: ones which require hand-trained language databases such as treebanks (e.g., (Socher et al., 2013)) and ones which are language agnostic and work on raw corpora (e.g., LDA, BOW, SkipGram, NLM, etc.). Liu (2015) compare various language agnostic models for topic modeling.\nLanguage independent models such as LDA and BOW have been quite effective since long time. Variants of BOW such as tf-idf had changed the perception of researchers towards these models when they were proved effective in various NLP tasks. LDA was able to model inter and intra documental statistical and relational structure quite well overcoming the drawbacks of BOW. But the semantic and syntactical dependencies were still ignored. After the introduction of neural language vector models, NLP saw a huge diversion in representation of words and documents. For indi-\nvidual words, vectors are obtained via distributional learning, the mechanisms for which varies from document-term matrix factorization (Landauer and Dumais, 1997), various forms of deep learning (Collobert et al., 2011; Turian et al., 2010; Socher et al., 2013), optimizing models to explain co-occurrence constraints (Mikolov et al., 2013b; Pennington et al., 2014), etc. Once the word vectors have been assigned, similarity between words can be captured via cosine distances. The same models have been extended ((Le and Mikolov, 2014)) with new variables to build vector models for sentences and documents. These models include the essence of individual words as well as their relative order in terms of sentence vector which was earlier absent in word vectors. The advantage of these approaches is that they can capture both the syntactic and the semantic similarity between words/documents in terms of their projections onto a high-dimensional vector space; further, it seems that one can tune the privileging of syntax over semantics by using local as opposed to large contexts (Huang et al., 2012).\nSome grammarians have been trying to find whether sentence meaning accrues by combining word meanings, or whether words gain their meanings based on the context they appear in (Matilal, 2001). (Turney et al., 2010) give a detailed overview of various vector space models and their composition. A surprising event in Information Theory has higher information content than an expected event (Shanon, 1948). The same happens when we give weights to word vectors. We give more weight to events which evoke surprise and less weight to events which are expected. The most popular weighting concept in this domain is the idea of tf-idf which we have utilized in this work (Refer Table 1).\nIn this work, we focus on a graded approach to assessing the importance of each word in a compositional models. Graded models such as tf-idf have long been used in NLP, but they do not seem to have been used in word vector composition tasks yet. The intuition is that in discrete cutoff functions, while simple, raise questions regarding threshold (what constitutes a stop word), and do not degrade performance gradually (Fig. 1).\nWe claim that the document vectors in hand is a much better representation of each document than doing it separately. This can be justified by the fact that we now incorporate contribution of\neach word as per its importance as well as well as that of document without ignoring tf-idf representation which performs considerably well in tasks such as retrieval. Hence, we call this as composite document vector representation. We then go a step ahead to build an ensemble of our model and recurrent neural network, which essentially has the properties of a generative model, to achieve state-of-the-art result on IMDB movie review dataset (94.19%) and on Amazon electronics reviews dataset with a significant improvement over previous best. The world class results in English clearly indicate the efficacy of our approach and improvements in Hindi depict the deficiency in other models which were used earlier."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Neural Language Model", "text": "Language modeling problem involved using frequency counts of n-grams for so many years but it ignores a large number of n-grams which are not seen while training leading to data sparsity and over-fitting of training data. Also these ngram models along with BOW suffers from the curse of dimensionality. Neural Networks tend to overcome the drawbacks of n-gram models because they can model continuous variables or distributed representation, which is a necessity if we would like to find better generalizations over the highly discrete word sequences (Bengio et al., 2003). Neural language models were introduced by Bengio et al., 2001 (revised in 2003(Bengio et al., 2003)). They build a mapping C from each word i of the vocabulary V to a feature vector C(i) \u2208 Rm, m is the number of features; a probability function g over words expressed with\nC; and finally learn the word vector and parameters of probability function. Morin(2005) proposed a hierarchical model to speed up the training cost by clustering similar words before computing their probability in order to only have to do one computation per word cluster at the output layer of the NN. Le(2011) combined neural networks with n-gram language models in a unified approach. They cluster words to structure the output vocabulary. Mikolov(2010) achieved the best reduction in perplexity by using recurrent neural network which uses the the current input as well as the output of the previous iteration. Mikolov(2011) present several modifications of the original recurrent neural network language model (RNN LM). The present approaches that lead to more than 15 times speedup for both training and testing phases. Collobert(2011) show the use of semi-supervised learning using deep neural networks to perform at the state-of-the-art of various NLP tasks. Wang(2014) propose a word vector neural-network model, which takes both sentiment and semantic information into account. This word vector expression model learns word semantics and sentiment at the same time as well as fuses unsupervised contextual information and sentence level supervised labels. Neelakantan(2014) took word vector models to next level where they proposed multiple embeddings per word. The problem that still remains in hand is that either these models are computationally expensive or they have failed to generalize properly. We, therefore, adopt skipgram model (Mikolov et al., 2013b), details about which have been discussed in next section, because deep network model of Collobert et al.(2008) takes too much time for training (skipgram reduces computational complexity from O(V) to O(log V) (Morin and Bengio, 2005))."}, {"heading": "2.2 Sentiment Analysis", "text": "Majority of the existing work in this field is in English (Pang and Lee, 2008). Medagoda(2013) surveys sentiment analysis in non-English languages while (Sharma et al., 2014) give a summary of work done in Hindi in the field of opinion mining. There have been heuristic based and machine learning based models used in this domain. Heuristic based methods, in general, classify text sentiments on the basis of total number of derived positive or negative sentiment oriented fea-\ntures. But these models rely heavily on human engineered features which, in general, is a domain and language dependent task. Several groups have attempted to improve the situation by modeling the composition of words into larger contexts (Le and Mikolov, 2014; Socher et al., 2013; Johnson and Zhang, 2014; Baroni et al., 2014).\nPang(2004) achieved an accuracy of 87.2% (Pang et al. 2004) on a dataset that discarded objective sentences and used text categorization techniques on the subjective sentences. Le(2014) use paragraph vector model and obtain 92.6% accuracy on IMDB movie review dataset. More difficult challenges involve short texts with nonstandard vocabularies,as in twitter. Here, some authors focus on building extensive feature sets (e.g. Mohammad et al.(2013); F-score 89.14).\nHowever, most of the work on sentiment analysis in Hindi has not attempted to form richer compositional analyses. For the type of corpora used here, the best results, obtained by combining a sentiment lexicon with hand-crafted rules (e.g. modeling negation and \u201dbut\u201d phrases), reach an accuracy of 80% (Mittal et al., 2013). Joshi(2010) compared three approaches: In-language sentiment analysis, Machine Translation and Resource Based Sentiment Analysis. By using WordNet linking, words in English SentiWordNet were replaced by equivalent Hindi words to get H-SWN. The final accuracy achieved by them is 78.1%. Bakliwal(2012) traversed the WordNet ontology to antonyms and synonyms to identify polarity shifts in the word space. Further improvements were achieved by using a partial stemmer (there is no good stemmer / morphological analyzer for Hindi), and focusing on adjective/adverbs (seed words given to the system); their final accuracy was 79.0% for the product review dataset. Mukherjee et al. (2012) presented the inclusion of discourse markers in a bag-of-words model and how it improved the sentiment classification accuracy by 2-4%.\nMany approaches seek to improve their performance by combining POS-tags and even parse tree structures into the models for higher accuracies in specific tasks (Socher et al., 2013). One problem in this approach is that of combining the word vectors to build document vectors because of issues in merging parse trees. Also these models are language dependent and computationally very expensive."}, {"heading": "3 Method", "text": "The algorithms and data structures used in this thesis have been introduced and discussed below."}, {"heading": "3.1 Distributed Representation", "text": "Mikolov et al. (2013b) proposed two neural network models for building word vectors from large unlabeled corpora; Continuous Bag of Words(CBOW) and Skip-Gram. In the CBOW model, the context is the input, and one tries to learn a vector for the central word; in Skip grams, the input is the target word and one tries to guess the set of contexts. We have adopted skipgram model to build vector representations for words as it performs better with larger vocabulary.\nEach current word acts as an input to a loglinear classifier with continuous projection layer, and predict words within a certain range before and after the current word. The objective is to maximize the probability of the context given a word within a language model:\np(c|w; \u03b8) = exp vc.vw\n\u2211 c\u2032\u2208C expvc.vw\nwhere vc and vw \u2208 Rd are vector representations for context c and word w respectively. C is the set of all available contexts. The parameters \u03b8 are vci , vwi for w \u2208 V , c \u2208 C , i \u2208 1, ...., d (a total of |C| \u00d7 |V | \u00d7 d parameters).\nThis distributed representation of sentences and documents (Le and Mikolov, 2014) modifies word2vec (Skip-Gram) algorithm to unsupervised learning of continuous representations for larger blocks of text, such as sentences, paragraphs or entire documents. The algorithm represents each document by a dense vector which is later trained and tuned to predict words in the document. In this framework, every paragraph is mapped to a unique vector and id, represented by a matrix D, which is a column matrix. Every word is mapped to a unique vector and word vectors are concatenated or averaged to predict the context, i.e., the next word. The paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs. The word vector matrix W, however, is shared across paragraphs. i.e., the vector for \u201dgood\u201d is the same for all paragraphs. The paragraph vector represents the missing information from the current context and can act as a memory of the topic of the paragraph. The advantage of using paragraph vectors is that they inherit the\nproperty of word vectors, i.e., the semantics of the words. In addition, they also take into consideration a small context around each word which is in close resemblance to the n-gram model with a large n. This property is crucial because the ngram model preserves a lot of information of the sentence/paragraph, which includes the word order also. This model also performs better than the Bag-of-Words model which would create a very high-dimensional representation that has very poor generalization.\nOur model incorporates property of document vector as well as property of word vectors to build an enhanced representation of documents without ignoring the properties of tf-idf representation."}, {"heading": "3.2 Semantic Composition", "text": "The Principle of Compositionality is that meaning of a complex expression is determined by the meaning of its parts or constituents and the rules which guide this combination. It is also known as Frege\u2019s Principle. In our case, the constituents are word vectors and the expression in hand is the sentence/document vector. For example,\nThe movie is funny and the screenplay is good\nAnalyzing the results from Table 2, we observed that when we deal with large number of features, there is a presence of large number of zeros and presence of a single zero in a feature will make that features contribution zero in the final vector, which happens in our case and thus multiplicative composition fails. We, therefore, adopt both simple and idf weighted average methods in our work. The advantage with addition is that, it doesnot increase the dimension of the vector and captures high level semantics with ease. In fact, (Zou et al., 2013) have used simple average to construct phrase vectors which they have later used to find phrase level similarity using cosine distance. (Mikolov et al., 2013c) showed that relations between words are reflected to a large extent in the\noffsets between their vector embeddings. They also use additive composition to reflect semantic dependencies.\nqueen - king \u2248 woman - man\n(Blacoe and Lapata, 2012) clearly show that vectors of Neural Language Model and Distributed Model when used with additive composition outperform those with multiplicative composition in Paraphrase Classification task. DM vectors outperform by nearly giving accuracy difference of 6%. They also perform very well on Phrase similarity tasks. We, therefore, propose graded weighting schema for better composition of vectors which is described below."}, {"heading": "3.2.1 Graded Weighting", "text": "We describe two approaches to incorporate graded weighting into word vectors for building document vectors. Let vwi be the vector representation of the ith word. Then document vector vdi for ith document is:\nvdi =\n{\n0 wk \u2208 stopwords \u2211\nwk\u2208di\nvwk wk /\u2208 stopwords\nThe above equation is 0-1 step-function which ignores contribution of all stop words. Now we propose another schema which weighs the contribution of each word while building document vector with a graded approach. We define idf(t, d) = log( |D|\ndf(t) ) where t is the term, d is the document and other notations are same as in previous subsection. The new document vector representation considering this graded schema is:\nvdi =\n{\n0 idf(wk, di) \u2264 \u03b4 \u2211\nwk\u2208di\nidf(wk, di).vwk otherwise\nwhere \u03b4 is a pre-defined threshold below which the word has no importance and above which the idf terms gives importance to that particular word. Till date, everyone has ignored how to effectively use vector composition techniques and as a result, this area has seen very less attention. But we have successfully used idf values to give weights to word vectors and hence obtain much better sentence/document vectors. The advantage of this model is that once we obtain idf values from training corpus, we can directly use it with test corpus without any additional computation. The results\n(see 4.3) obtained by using this technique clearly demonstrate how effective it is for tasks such as sentiment analysis."}, {"heading": "3.3 Composite Representation", "text": "This experiment redefined document representation in NLP used for sentiment classification. It has the property of including both syntactic and semantic properties of a piece of text. The limitations of skip-gram word vectors have been fulfilled by document vectors and hence we achieve stateof-the-art results on IMDB movie review dataset as well as amazon electronics review dataset.\nWe first generated n-dimensional word vectors by training skip-gram model on the datasets. We then assigned weights to word vectors for each document to create document vectors. This now acts as a feature set for that particular document. We then created tf-idf vectors for each document. This can be seen as a vector representation of that particular document. We then concatenated these document vectors with document vectors obtained after training the desired dataset separately with the model proposed in (Le and Mikolov, 2014). Discrimination weighted vectors give a great boost to classification accuracies on various datasets and hence justifies our claim."}, {"heading": "3.4 Dimensionality Reduction", "text": "Dimensionality Reduction is the process of reducing the number of random variables in such a way that the remaining variables effectively reproduce most of the variability of the dataset. The reason for using such techniques is because of the curse of dimensionality which is a phenomena that occurs in high-dimension but doesn\u2019t occur in lowdimension. Table 3 summarizes how feature selection has improved classification accuracy on the 700 Movie review dataset. With ANOVA-F, we selected around 4k features but with PCA, this number was just 50. So, the low accuracy with PCA can be attributed to the fact that we may have lost some important features in low dimension. Also, PCA cannot work with size of dimension d >size of learning set. This sharp decrease in accuracy in both cases happens because ANOVA-F selects features with larger variance across group and thus reduces noise to a larger extent whereas PCA reduces angular variance which is not effective in this case due to the distribution of data points in high-dimensional space.\nThis newly released dataset is much larger than previous standard dataset and very less focused towards sentiment of the review."}, {"heading": "4 Experiment", "text": "In this section, we describe the experiments and analyze the results."}, {"heading": "4.1 Datasets", "text": "We have used 3 datasets for experiments in Hindi and 5 for English. All the datasets including our self created Hindi dataset are described below. We experimented on two Hindi review datasets. One is the Product Review dataset (LTG, IIIT Hyderabad) containing 350 Positive reviews and 350 Negative reviews. The other is a Movie Review dataset (CFILT, IIT Bombay) containing 127 Positive reviews and 125 Negative reviews. Each review is around 1-2 sentences long and the sentences are mainly focused on sentiment, either positive or negative. Our 700-Movie Review Corpus in Hindi contains movie reviews from websites such as Dainik Jagran and Navbharat Times. The movie reviews are longer than the previous corpus and contains subjects other than sentiment. There are in total 697 movie reviews from both the websites. The statistics compiled is described below.\nFor experiments in English, we trained on IMDB movie review dataset (Maas et al.(2013)) which consists of 25,000 positive and 25,000 negative reviews. It also contains an additional 50,000 unlabeled documents for unsupervised learning.\nThe Trip Advisor Review dataset contains around 240K reviews (206MB) from hotel domain. Reviews with overall rating >=3 were annotated as positive and those with overall rating <3 were annotated as negative. The dataset was split into 80-20 ratio for training and testing purpose.\nWe also took amazon reviews for our experiments. Reviews with overall rating >=3 were annotated as positive and those with overall rating <3 were annotated as negative. The dataset was split into 80-20 ratio for training and testing purpose. There were 3 review datasets: Electronics dataset consists of 1,241,778 reviews, Watches Dataset consists of 68,356 reviews and MP3 Dataset consists of 31,000 reviews."}, {"heading": "4.2 SkipGram or CBOW", "text": "We present an interesting experiment to demonstrate that skipgram indeed performs better than CBOW. SkipGram model tends to predict a context given a word whereas CBOW model predicts a word given a context. It seems intuitive and also from observation (Mikolov et al., 2013a) that SkipGram will perform better on semantic tasks and CBOW on syntactic tasks. We now try to evaluate how they differ on classification accuracies on the two datasets: Watches and MP3. Figure 2 show that skipgram outperforms CBOW on sentiment classification task. It can be justified by the fact that sentiment inclination of a document is more oriented towards semantics of that document rather than just syntax and our results clearly demonstrate this fact."}, {"heading": "4.3 Results", "text": "Table 5 summarizes the results obtained by others and by us on the IMDB movie review dataset. We have gone above the previous best (Le and Mikolov, 2014) by a margin of 1.33% using discrimination weighting. The main contributor for improvement in results is our new document vector which overcomes the weaknesses of BOW and document vectors taken separately.\nTables 6 demonstrate the effectiveness of our proposed graded weighting technique. Without tfidf features, our proposed method performs better in the graded idf weighting case and when we include tf-idf features, 0-1 weighting perform better than idf graded technique and both perform better than the previous state-of-the-art. We see that with larger weights there is a decrease in accuracy and that is because we are now filtering out more words which are important while building document vector. Table 7 is a further improvement in results once we incorporate predictions of RNNLM and Composite document vector model together(voting ensemble). Here, we first trained a RNNLM and then obtained predictions on test reviews in terms of probability. We trained Linear SVM classifier using new Document Vectors and then obtained predictions on test reviews. We then merged these two predictions using a voting based approach to obtain final classification.\nTable 8 presents result of experiment conducted on famous Amazon electronics review dataset (Leskovec and Krevl, 2014). Our vector averaging method alone has beaten previous best by 3.3%.\nTable 9 represents the results using five different techniques for feature set construction. We see that there is a slight improvement in accuracy on both datasets once we remove stop-words but the major breakthrough occurs once we used weighted averaging technique for construction of document vectors from word vectors.\nTable 10 and 11 compares our best method with other methods which have performed well using techniques such as tf-idf, subjective lexicon, etc."}, {"heading": "5 Conclusion", "text": "In this work we present an early experiment on the possibilities of distributional semantic models (word vectors) for low-resource, highly inflected languages such as Hindi. What is interesting is that our word vector averaging method along with tf-idf results in improvements of accuracy compared to existing state-of-the art methods for sentiment analysis in Hindi (from 80.2% to 90.3% on IITB Movie Review Dataset). Also from Table 1, we can see that paragraph vector proposed by (Le and Mikolov, 2014) doesn\u2019t perform well owing to the fact that the Hindi dataset just contains single sentences highlighting the weakness of this model. The size of the corpus is also small to learn paragraph vectors. Thus, our model overcomes these weaknesses with a better document representation. We observe that pruning high-frequency stop words improves the accuracy by around 0.45%. This is most likely because such words tend to occur in most of the documents and don\u2019t contribute to sentiment. For example, the word EPSm(Film) occurs in 139/252 documents in Movie Reviews(55.16%) and has little effect on sentiment. Similarly words such as Es AT (Siddharth) occur in 2/252 documents in Movie Reviews(0.79%). These words don\u2019t provide much information.\nWe also see that when number of features accumulate to a large number than there are few redundant features creating noise in the representation of the text. We tried to reduce this noise by using feature variance techniques. The large increase in accuracy(around 11%) justifies our claim.\nBefore concluding, we return to the unexpectedly high improvement in accuracy achieved. One possibility we considered is that when the skipgrams are learned from the entire review corpus, it\nincorporates some knowledge of the test data. But this seems unlikely since the difference in including this vs not including it, is not too significant. The best explanation may be that the earlier methods, which were all in some sense based on a sentiWordnet, and at that one that was initially translated from English, were essentially very weak. This is also clear in an analysis from (Bakliwal et al., 2012), which shows intern-annotator agreement on sentiment words are very poor (70%) - i.e. about 30% of these words have poor human agreement. Compared to this, the word vector model provides considerable power underlining the claim that distributional semantics is a topic worth exploring for Indian languages.\nOur experiments on new dataset and existing datasets show that our method is competitive with existing methods including state-of-the-art. This new concept of document vectors can overcome the weaknesses of existing models which were either deficient in capturing syntactic or semantic properties of text. These models failed to incorporate contribution of each word while we have tapped this area and hence achieved state-of-the art results. The ensemble of RNNLM and Composite Document Vector has beaten state-of-the-art by a significant margin and has opened this area for future research. These models have the advantage that they don\u2019t require parsing at any step neither do they require a lot of heavy pre-processing. These tasks require a lot of extra effort and they slow the progress a lot."}, {"heading": "6 Future Work", "text": "Distributional semantics approaches remain relatively under-explored for Indian languages, and our results suggest that there may be substantial benefits to exploring these approaches for Indian languages. While this work has focused on sentiment classification, it may also improve a range of tasks from verbal analogy tests to ontology learning, as has been reported for other languages. For future work, we can explore various compositional models - a) weighted average - where weights are determined based on cosine distances in vector space; b) weighted multiplicative models. Identifying morphological variants would be another direction to explore for better accuracy. With regard to sentiment analysis, the idea of aspect-based models (or part-based sentiment analysis), which looks into constituents in a document and classify\ntheir sentiment polarity separately, remains to be explored in Hindi.\nIn English, our Composite document vectors has led open a new area to look at where there can be many possible ensembles which may improve our work. Also, we could incorporate multiple word vectors here as well to distinguish between polysemous words. Another interesting and open area is to look at Region of Importance in NLP where we filter out sentiment oriented sentences and phrases from a unfocused corpus which contains text from various domains. The code and parameters are available at github for future research."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Despite the success of distributional semantics, composing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. Most models usually omit stop words from the composition. Instead of such an yes-no decision, we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document (e.g., idf). Some of these methods (particularly tf-idf) are seen to result in a significant improvement in performance over prior state of the art. Further, combining such approaches into an ensemble based on alternate classifiers such as the RNN model, results in an 1.6% performance improvement on the standard IMDB movie review dataset, and a 7.01% improvement on Amazon product reviews. Since these are language free models and can be obtained in an unsupervised manner, they are of interest also for underresourced languages such as Hindi as well and many more languages. We demonstrate the language free aspects by showing a gain of 12% for two review datasets over earlier results, and also release a new larger dataset for future testing (Singh, 2015).", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}