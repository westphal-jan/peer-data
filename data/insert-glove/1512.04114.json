{"id": "1512.04114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "Building and Measuring Privacy-Preserving Predictive Blacklists", "abstract": "Collaborative non-interactive approaches to network aggadah defense lazing are madalina being increasingly advocated, ribordy aiming ethology to steelton proactively predict straight-six and djoko speed up detection 10-10 of attacks. In edda particular, four-yard a romansh lot stockings of giesler attention has tyto recently been given reassume to the problem of screwed predictive blacklisting, i. nonminority e. , hangover forecasting 50.90 attack secemin sources evaluated based on dentsu Intrusion thirty-three Detection negara Systems (8,080 IDS) alerts tallis contributed by different ma'an organizations. While 1.5440 collaboration mid-major allows 522nd the mugrage discovery of groups 4-79 of floride correlated ru486 attacks targeting similar michaels victims, it bogo also lcsd raises rw96 important privacy and security aegerter challenges, hopmans thus gizzi motivating privacy - preserving d'amato approaches mubarakpur to the 5,084 problem. Although varah recent work drydocked provides encouraging ka\u0142uszyn results on saxby the opportunities feasibility of proto-indo-iranian collaborative lb7 predictive blacklisting bramble via analysts limited 52.5 data chronobiology sharing, 84.38 a number bangarra of badreddin open refaced problems defectives remain msowoya unaddressed, 01:15 which this paratype paper sets to uca address. We introduce kneale a privacy - gharyan friendly rijke system hoseyn for ayana predictive hartstein blacklisting 1905-1907 featuring trava a semi - trusted authority lipka that rygg clusters organizations based euphoric on cityplace the hetaera similarity of their logs, homeports without access mmo to marciano these caggiano logs. gilkes Entities in the alfi same cluster gathorne-hardy then spyros securely share relevant logs with xhevat each other, government-held and build predictive blacklists.", "histories": [["v1", "Sun, 13 Dec 2015 20:05:53 GMT  (234kb,D)", "https://arxiv.org/abs/1512.04114v1", null], ["v2", "Fri, 19 Feb 2016 08:45:45 GMT  (204kb,D)", "http://arxiv.org/abs/1512.04114v2", null], ["v3", "Thu, 9 Jun 2016 08:59:38 GMT  (228kb,D)", "http://arxiv.org/abs/1512.04114v3", null], ["v4", "Wed, 1 Mar 2017 16:08:02 GMT  (151kb,D)", "http://arxiv.org/abs/1512.04114v4", null]], "reviews": [], "SUBJECTS": "cs.CR cs.AI", "authors": ["luca melis", "apostolos pyrgelis", "emiliano de cristofaro"], "accepted": false, "id": "1512.04114"}, "pdf": {"name": "1512.04114.pdf", "metadata": {"source": "CRF", "title": "Building and Measuring Privacy-Preserving Predictive Blacklists", "authors": ["Luca Melis", "Apostolos Pyrgelis", "Emiliano De Cristofaro"], "emails": [], "sections": [{"heading": null, "text": "vocated to improve timeliness and effectiveness of threat mitigation. Among these, collaborative predictive blacklisting (CPB) aims to forecast attack sources based on alerts contributed by multiple organizations that might be targeted in similar ways. Alas, CPB proposals thus far have only focused on improving hit counts, but overlooked the impact of collaboration on false positives and false negatives. Moreover, sharing threat intelligence often prompts important privacy, confidentiality, and liability issues. In this paper, we first provide a comprehensive measurement analysis of two state-of-the-art CPB systems: one that uses a trusted central party to collect alerts [Soldo et al., Infocom\u201910] and a peer-to-peer one relying on controlled data sharing [Freudiger et al., DIMVA\u201915], studying the impact of collaboration on both correct and incorrect predictions. Then, we present a novel privacy-friendly approach that significantly improves over previous work, achieving a better balance of true and false positive rates, while minimizing information disclosure. Finally, we present an extension that allows our system to scale to very large numbers of organizations."}, {"heading": "1 Introduction", "text": "Filtering connections from/to hosts regarded as malicious is a common practice to reduce the number and the impact of attacks. As it is unfeasible to perform expensive classification tasks on each connection, filtering is typically performed using periodically updated lists of suspicious hosts \u2013 or blacklists. These can be created locally or obtained from alert repositories, such as DShield.org.\nZhang et al. [36] are the first to introduce the concept of collaborative predictive blacklisting (CPB), whereby different entities send their logs to a trusted entity, which in turn provides customized blacklists based on relevance ranking. The intuition is that attacks are often correlated, mounted by the same sources against different networks. In fact, Katti et al. [22] show that attack correlation persists over time, suggesting that collaboration between victims can significantly improve malicious IP detection time. Soldo et al. [33] then improve on [36] relying on clustering and implicit recommendation. Overall, collaborative approaches to threat mitigation, besides CPB, are increasingly advocated, with more and more efforts promoting information sharing, e.g., by\nCERT [6], Facebook [2], and the White House [34]. In this paper, we focus on two main challenges with respect to the impact of collaboration on (i) false positives/negatives and (ii) privacy. Prior CPB proposals [33, 36] only focus on improving \u201chit counts\u201d, i.e., the number of successfully predicted attacks, but fail to account for incorrect predictions (i.e., false positives/negatives). Furthermore, real-world deployment of collaborative blacklisting is hindered by confidentiality, liability, trust, and competitiveness concerns, as sharing alerts could harm an organization\u2019s reputation and lead to disclosure of sensitive information about customers and business practices [1, 3]. Freudiger et al. [16] are the first to investigate privacy-friendly approaches to CPB: their intuition is to let organizations interact in a pairwise manner to privately estimate the benefits of collaboration, and then have them share data with entities that are likely to yield the most benefits. However, as discussed later in Section 4, their pairwise protocol by Freudiger et al. only scales to a few collaborators."}, {"heading": "1.1 Roadmap", "text": "First, we provide an experimental evaluation of existing collaborative predictive blacklisting (CPB) proposals (Section 4). We use alerts contributed to DShield.org by 70 organizations reporting an average of 4,000 daily events over a 15-day time window. We re-implement and compare the centralized technique by Soldo et al. [33] vs. the peer-topeer one based on controlled data sharing by Freudiger et al. [16]. We find that the former achieves high hit counts (almost doubling correct predictions compared to no collaboration) and relatively high recall (58%), but its accuracy is ultimately quite poor (F1=14%) due to a significant increase in false positives (8% precision). Whereas, the latter only slightly improves hit counts compared to no collaboration, but also yields fewer incorrect predictions, thus resulting in better accuracy overall (up to 29% F1).\nThen, we propose a novel approach aiming to capture the best of the two worlds (Section 5). We use a hybrid model relying on a semi-trusted authority (STA), which acts as a coordinating entity to facilitate clustering, without having access to the raw data. The STA clusters contributors based on the similarity of their logs (without seeing these logs), and helps organizations in the same cluster to share relevant data. We experiment with a few clustering algorithms using the number of common attacks as a measure of similarity, which is computed in a privacy-preserving way, and experi-\nar X\niv :1\n51 2.\n04 11\n4v 4\n[ cs\n.C R\n] 1\nM ar\n2 01\n7\nment with privacy-friendly within-clusters sharing strategies, i.e., only disclosing the details of common attacks and/or privately discovering correlated attacks. Our experimental results show that our hybrid model balances out the increase in hit counts and in false positives brought about by information sharing (74% recall, 19% precision, 30% F1 score), achieving up to 4x increase in hit counts compared to Freudiger et al. [16] and doubling overall accuracy compared to Soldo et al. [33]. Finally, we present a scalability analysis of our scheme (Section 6), introducing a simple variant that allows it to efficiently scale to very large numbers of organizations."}, {"heading": "1.2 Contributions", "text": "In summary, our paper makes two main contributions. First, we present a measurement study of existing CPB approaches aiming to capture the overall effects of collaboration, highlighting important open problems. Second, we introduce a novel, privacy-friendly and scalable approach to CPB that achieves a better balance between hit counts and incorrect predictions. Our system minimizes the amount of information disclosed in the process and achieves scalability in the presence of large numbers of collaborating entities."}, {"heading": "2 Related Work", "text": "Collaborative Intrusion Detection. Katti et al. [22] are among the first to measure correlated attacks, i.e., attacks mounted by the same sources against different networks, establishing that they are very common yet highly targeted. They show that attack correlation persists over time and suggest that collaboration between victims could significantly improve malicious IP detection time. In [36], Zhang et al. introduce highly predictive blacklisting, having different organizations contribute alerts to a central repository, such as DShield.org, which in turn provides them with daily personalized (predictive) blacklists. The prediction uses a relevance ranking scheme similar to PageRank, measuring the correlation of an attacker to a contributor based on their history as well as the attacker\u2019s recent log production patterns. Then, Soldo et al. [33] improve on [36] using an implicit recommendation system to discover similar victims as well as clusters of correlated victims and attackers. In their model, the presence of attacks performed by the same source around the same time leads to stronger similarity among victims, and a neighborhood model (k-NN) is applied to cluster similar victims. Cross Association (CA) co-clustering [7] is then used to discover groups of correlated attackers and victims, and prediction within the cluster is done via a time-series algorithm \u2013 Exponentially Weighted Moving Average (EWMA) \u2013 capturing attacks\u2019 temporal trends.\nBeyond blacklisting, other work focuses on other collaborative security problems. Felegyhazi et al. [14] perform proactive prediction of malicious domain use: starting from a seed of confirmed bad domains, they predict clusters of related domains based on name server features (zone files containing sub-domains and authoritative name servers), and in-\nfer new bad domains. Liu et al. [26], based on externally observable properties of an organization\u2019s network, aim to predict breaches without the organization\u2019s cooperation. Woods et al. [35] apply data mining to identify subsets of shared information that are semantically related, while Garrido et al. [17] introduce game-theoretic models to analyze the effects of cyber-security information sharing among organizations. Sirivianos et al. [32] propose a collaborative system that enables hosts with no email classification functionality to check whether a host is a spammer or not. Each host then assesses the trustworthiness of spam reporters by auditing their reports and leveraging the social network of the reporters\u2019 administrators. Privacy In Collaborative Intrusion Detection. Porras and Shmatikov [30] discuss privacy risks prompted by sharing security-related data and propose anonymization and sanitization techniques to address them. However, follow-up work [9, 25] demonstrates that these techniques make data less useful and anyway prone to de-anonymization.\nBurkhart et al. [5] introduce a few privacy-preserving protocols based on secure multiparty computation (MPC) for aggregation of network statistics. This is also explored in [4], where entities send encrypted data to a central repository that aggregates contributions. However, statistics only identify the most prolific attack sources and yield global models, which, as discussed in [36], miss a significant number of attacks and yield poor prediction performance. Nagaraja et al. [28] introduce an inference algorithm, BotGrep, to privately discover botnet hosts and links in network traffics, relying on Private Set Intersection [12]. Davidson et al. [10] propose a game-theoretic model for software vulnerability sharing between two competing parties. Their protocol relies on a private set operation (PSO) technique to limit the amount of information disclosed. However, it does not scale for more than two entities. Finally, Freudiger et al. [16] focus on collaborative predictive blacklisting based on a pairwise controlled data sharing approach. They focus on identifying which metrics (e.g., number of common attacks) can be used to privately estimate the benefits of collaboration between two organizations, rather than proposing a deployable system. In fact, as discussed later, their pairwise approach does not scale to many organizations."}, {"heading": "3 Preliminaries", "text": ""}, {"heading": "3.1 Background", "text": "Time Series Prediction. We use Exponentially Weighted Moving Average (EWMA) to perform prediction. Given a signal over time r(t), we indicate with r\u0303(t+1) the predicted value of r(t+1), given past observations r(t\u2032) at time t\u2032 \u2264 t. The predicted signal is computed as:\nr\u0303(t+ 1) = t\u2211 t\u2032=1 \u03b1 \u00b7 (1\u2212 \u03b1)t\u2212t \u2032 \u00b7 r(t\u2032) (1)\nwhere \u03b1 \u2208 (0, 1) is a smoothing coefficient, t\u2032 \u2208 {1, . . . , t} denotes the training window, and t + 1 is the time slot to be\npredicted. For small values of \u03b1, EWMA aggregates past information uniformly across the training window, while, with a large \u03b1, the prediction algorithm focuses more on events taking place in the recent past.\nMetrics. To evaluate the performance of the predictions, we use true and false positives and negatives, denoted, respectively, as TP, FP, TN, FN. We derive precision (PPV), recall (TPR), and F1-Score, respectively, as TP/(TP+FP), TP/(TP+FN), and the harmonic mean of PPV and TPR. We also measure the average improvement/increase in TP, FP, and FN when compared to when no collaboration occurs between organizations, using, resp., TPimpr = (TPc\u2212TP)/TP, FPincr = (FPc \u2212 FP)/FP, and FNincr = (FNc \u2212 FN)/FN, where the c notation denotes values after collaboration.\nCryptographic protocols. In the rest of the paper, we use a number of cryptographic protocols for privacy-preserving computations. To ease presentation, we defer their description to Appendix A."}, {"heading": "3.2 Dataset", "text": "Aiming to design a meaningful empirical analysis of CPB, we gather a dataset of blacklisted IP addresses from DShield.org, a collaborative firewall log correlation system to which various organizations volunteer daily alerts. Each entry in the logs consists of a pseudonymized Contributor ID (the target), the source IP address (the attacker), source and target port number, and the timestamp. With DShield\u2019s permission, we have collected logs using a JavaScript web crawler, gathering, on average, 10 million logs every day. We exclude entries for invalid or non-routable IP addresses, and discard port numbers, then, for each IP address, we extract its /24 subnet and use /24 addresses for all experiments, following experimental choices made in prior work [16, 33, 36].\nTraining and Testing Sets. We use the DShield dataset both as a training set and a testing set (i.e., ground truth), considering a sliding window of 5 days for training and 1 day for testing, as done in [16, 33]. We select a 15-day period (i.e., 10 sliding windows) and restrict our evaluations to a reasonablysized sample of regularly contributing organizations. We select the top-100 contributors, based on the number of unique IPs reported, provided that they report logs every day. Most contributors (around 60%) submit less than 100K logs, while 20% submit between 100K and 500K, and only a handful of organizations contribute very large amounts of logs (above 1M). We then pick 70 organizations, for each time window, leaving out the top-10 and the bottom-20 contributors. We do so, like in previous work [16, 33], to minimize bias. Our final sample dataset includes 30 million attacks, contributed by 118 unique organizations (as the 70 contributors selected in each time window vary) over 15 days, each reporting a daily average of 600 suspicious (unique) IPs and 4,000 attack events.\nNote that we have also repeated our experiments on a larger number of organizations (150) and on two more sets of DShield logs using 15-day periods from other time inter-\nvals, but have not found any significant difference in the results. Moreover, we remark that the way we count FP is an upper bound since we do not have ground truth as to whether the absence of an IP from the testing set occurs when the IP is not suspicious or if it simply does not generate requests. Nonetheless, it does not really matter for our evaluations since our main goal is to compare different approaches with each other.\nNotation. We use O = {Oi}ni=1 to denote a group of n organizations, where each Oi holds a dataset Di of alerts, i.e., suspicious IP addresses along with the related timestamps. We aim to predict IP addresses generating attacks to each Oi in the next day, using, as the training set, both its local dataset Di, as well the set D\u2032i, with suspicious IP addresses obtained by collaborating with other organizations. As discussed above, for each time window, we consider n = 70 organizations using alerts collected from DShield."}, {"heading": "4 Existing Collaborative Predictive Blacklisting Approaches", "text": ""}, {"heading": "4.1 Soldo et al. [33]\u2019s Implicit Recommendation (No Privacy)", "text": "We first evaluate Soldo et al [33]\u2019s CPB approach based on implicit recommendation. We do so with a twofold goal: (1) to evaluate false positives and false negatives, which were not taken into consideration in [33], and (2) to compare against privacy-friendly approaches, presented later. Essentially, Soldo et al.\u2019s work builds on [36], which bases on a relevance ranking scheme similar to PageRank, measuring the correlation of an attacker to a contributor relying on their history as well as the attacker\u2019s recent log production patterns. Soldo et al. improve [36] by using an implicit recommendation system to discover similar victims as well as groups of correlated victims and attackers. The presence of attacks performed by the same source around the same time leads to stronger victim similarity, and a neighborhood model (kNN) is applied to cluster similar victims. Cross Association (CA) co-clustering [7] is then used to discover groups of correlated attackers and victims, and prediction within the cluster is done via EWMA to capture attacks\u2019 temporal trends. We have re-implemented their system in Python and used Chakrabarti\u2019s implementation of Cross Association (CA) as per [7], and run the experiments on the dataset introduced in Section 3.2.\nWe start by measuring the basic predictor which only relies on a local EWMA time series algorithm (TS), using \u03b1 = 0.9 as it yields the best results, then, apply the co-clustering techniques (TS-CA), and, finally, implement their full scheme by combining k-NN to cluster victims based on their similarity with CA and TS (TS-CA-k-NN). Fig. 1 illustrates the improvement/increase in TP, FP, FN (compared to the TS baseline) as well as TPR, PPV, and F1, with various k values (ranging from 1 to 35) used by the k-NN algorithm to dis-\ncover similar organizations. Obviously, the k-NN parameter k does not affect TS-CA and TS.\nFig. 1(a) shows that, with TS-CA-k-NN, TPimpr increases significantly with k, almost doubling the \u201chit counts\u201d compared to the TS baseline, whereas, TS-CA improves less (0.67). On the other hand, however, there is FPincr too, 5- to 50-fold, as clusters become bigger (Fig. 1(b)), and naturally, this stark increase in FP leads to low precision, as shown in Fig. 1(e). FNs also always increase compared to TS (Fig. 1(c)), specifically, they double with TS-CA and increase between 0.55 and 0.95 (less for larger k values) compared to TS. Moreover, FNincr affects TPR (Fig. 1(d)), with an increase between 0.58 and 0.66. TPimpr does not correspond to a comparable increase in TPR, due to the poor FN performance, as shown by the fact that TS-CA-k-NN reaches 0.95 in TPimpr but only at most 0.66 TPR compared to 0.59 with the baseline TS. Overall, Soldo et al.\u2019s techniques do not perform well in practice, as they yield lower F1 measures (0.16 with TS-CA, and at most 0.14 with TS-CA-k-NN) than a simple local time-series prediction (0.26) \u2013 see Fig. 1(f)."}, {"heading": "4.2 Freudiger et al. [16]\u2019s Peer-to-Peer Controlled Data Sharing", "text": "Next, we evaluate the system by Freudiger et al. [16], whereby organizations interact pairwise, aiming to privately estimate the benefits of collaboration, and then share data with entities that are likely to yield the most benefits. The\nauthors also use DShield data and perform prediction using EWMA. They find that: (1) the number of common attacks is the best predictor of benefits, which can be estimated privately, using Private Set Intersection Cardinality (PSICA) [11]; and (2) sharing only the intersection of attacks \u2013 which can be done privately using Private Set Intersection (PSI) [12] \u2013 is almost as beneficial as sharing everything.1 Their goal is really to assess benefit estimation/sharing strategies, rather than to focus on deployment. They assume a network of 100 organizations, select the \u201ctop 50\u201d among all possible 4950 pairs (in terms of estimated benefits), and only experiment on those. Naturally, without a coordinating entity, it is impossible to rank the pairs, so they suggest that collaboration should take place: either between organizations whose estimated benefits are above a threshold, although it is not stated how to set this threshold; or by each organization selecting the top x ones with the biggest estimated benefits, but do not experiment with or discuss how x impacts overhead or true/false positives. Thus, we replicate both approaches, specifically, by experimenting: (A) with the top 1% to 5% of global pairs, and (B) having each organization pick 1 to 35 most similar organizations.\nFig. 2 shows the improvement/increase in TP, FP, FN (compared to a baseline with no sharing) as well as TPR, PPV and F1 with increasing percentage of global pairs (A). Similarly, Fig. 3 presents the results for approach (B).\n1See Appendix A for background on these cryptographic protocols.\nLooking at TPimpr, (A) yields 13% increase when 3% of global pairs are selected whereas for (B), i.e. picking local pairs, TPimpr increases along with the number of local pairs selected. When an organization collaborates with its 5 most similar ones, we observe a 27% TPimpr while when it collaborates with 35, TPimpr reaches 45% (see Fig. 3(a)). Figs. 2(b) and 3(b), show that (A) has a rather small FPincr (13% increase when the 75 top pairs are selected) compared to (B) which is affected by the number of pairs that each organizations picks for collaboration. When an organization collaborates with 5 others a 25% FPincr is observed on average while when it collaborates with 30 others FPincr reaches 80%. Moreover, Figs. 2(c) and 3(c) illustrate that both approaches achieve a decrease in false negatives with the second approach achieving bigger decreases as the number of collaborators increases. Finally, in (A), when 4% of all pairs (i.e., when \u223c 100 pairs are selected), we get an F1 measure of 0.29. As per (B), we observe that F1 measures are slightly affected by the number of pairs that each organization chooses for collaboration. The best F1 score (0.27) is obtained when an organization picks 5 others for collaboration. Overall, both approaches improve precision and recall of the system, resulting in an enhanced F1 score, compared to a local approach. Although the increase in TP is not as high as with the non-private approach of [33], a more balanced increase of false positives and a decrease of the false negatives seems possible. However, the system is limited in scalability, due to its peer-to-peer design. Moreover, when organizations share only the intersection of their datasets it is not possible for them to obtain events about new attackers that they have never witnessed before in their own datasets."}, {"heading": "5 A Novel Approach to PrivacyFriendly CPB", "text": "In this section, we introduce a novel privacy-friendly system which relies on a semi-trusted authority, or STA, acting as a coordinating entity to facilitate clustering without having access to the raw data. In other words, the STA clusters contributors based on the similarity of their logs (without accessing these logs), and helps organizations in the same cluster to share relevant logs."}, {"heading": "5.1 Overview", "text": "Our system involves four steps: (1) First, organizations interact in a pairwise manner to privately compute a similarity measure of their logs, based on the number of common attacks. Then, (2) the STA collects the similarity measures from each organization to a matrix depicted as O2O and performs clustering on it. Next, (3) the STA reports to each organization the identifiers of other organizations in the same cluster (if any), so that they collaboratively, yet privately, share logs to boost the accuracy of their predictions, by either sharing common attacks (intersection), corre-\nlated attacks (IP2IP), or both.2 Finally, (4) each organization performs EWMA prediction based on their logs, augmented with those shared from entities in the same cluster.\nThis approach is hybrid in that interaction is privacyfriendly, using a central party (STA) which is not trusted with data in-the-clear, but only with similarity measures. Moreover, we follow a data minimization approach as organizations only share information about common and correlated attacks: specifically, in (1), the number of common attacks is computed privately using PSI-CA [11], while, in (3), sharing of common and correlated attacks is also privacy-preserving, as we rely on PSI [12] and private aggregation [27]. Settings. We use datasets and settings from Section 3.2. We cluster organizations (Step (2)) utilizing various algorithms, i.e., Agglomerative Clustering, k-means, and k-NN. Moreover, for the IP2IP method, we only consider the top-1000 attackers (i.e., the top-1000 heavy hitters) in each cluster, for each 5-day training-set window \u2013 rather than looking for correlations over all the /24 IP space \u2013 and for each IP we extract 50 correlated ones. Finally, as in previous experiments, we set \u03b1 = 0.9 for EWMA."}, {"heading": "5.2 Results", "text": "We now present the results of our extensive experimental evaluation. As we use different clustering algorithms, we refer the reader to Appendix B for a brief overview of them. 2For comparison, we also consider baseline approaches, i.e., sharing nothing (local) or sharing everything (global).\nOur experiments are written in Python, using the scikit-learn machine learning suite, and will be made available with the final version of the paper.\nAgglomerative Clustering. We consider different numbers of desired clusters (k), ranging from 1 to 35, setting affinity and linkage parameters to cosine and average, respectively, to indicate what distance measures to use between sets of observations. In Fig. 4(a)\u20134(c), we plot average TPimpr, FPincr, and FNincr with increasing number of clusters. Unsurprisingly, IP2IP achieves smaller TPimpr results (at most 0.20) than global (up to 1.10), which however incurs higher FNincr (0.3) and above all FPincr (1000). Fig. 4(d) shows that recall (TPR) always improves when sharing, with intersection reaching 0.76. When combining intersection and IP2IP, TPR slowly degrades with smaller clusters (peaks at 0.68 for k = 5), while, with IP2IP, it increases (0.67 for k = 35). Because of the increase in FN, global performs worse in terms of recall (0.66) although obtaining the best TPimpr. From Fig. 4(e), we observe that local yields the best precision (0.16), followed by intersection (0.15, slightly growing for larger k), while IP2IP and IP2IP+intersection slowly increase up to 0.13. Global performs poorly overall (up to 0.04) due to high FP. Finally, Fig. 4(f) plots the F1 measure: intersection achieves slightly better scores than local (0.26 vs 0.25), while its combination with IP2IP, or just IP2IP are slightly worse (0.22).\nk-means. Next, we use k-means for clustering and obtain re-\nsults similar to agglomerative clustering. Thus, we decide to restrict to stronger correlations, by only taking into account organizations closer to the cluster\u2019s centroid, and excluding the rest of them as outliers. We set a distance threshold and experiment with it empirically, finding that the optimal setting is the 40th percentile, i.e., the cluster distance value be-\nlow which 40% of the organizations can be found. Fig. 5(a)\u2013 5(c) plot the average improvement in TP and increase in FP and FN, respectively. TPimpr is almost constant with IP2IP (0.2) independent of the cluster sizes, while with the other methods it decreases faster due to the distance thresholds, ranging from 1.1 with global for k = 1 to 0.1 of intersection for k = 35. IP2IP shows steady FNincr values compared to other methods (\u22120.2, i.e., a 20% decrease) which leads to a better performance in TPR, as shown in Fig. 5(d), for k \u2265 10 (up to 0.71). Furthermore, intersection yields the best performance in FNincr (\u22120.52), with k = 1. Fig. 5(f) shows the best F1 measure (0.30) is reached with k = 5, due to a peak both in PPV and TPR. IP2IP performs slightly worse (0.23) than local (0.28) while poor F1 values for global, with k = 35, (0.18) are due to its bad PPV (0.10) \u2013 see Fig. 5(e).\nk-NN. Recall that k indicates the number of nearest neighbors that each entity considers as its most similar ones. Thus, organizations can end up in more than one neighborhood. Since the algorithm builds a neighborhood for each organization, not all clusters have the same strength, so we only consider strong clusters in terms of their members similarity and as done with k-means, after tuning the parameters, we set a distance threshold as the 40th percentile to leave possible outliers out of the clusters. From Fig. 6(a), we observe that IP2IP+intersection yields the second best performance in TPimpr (0.38, with k = 35), while global peaks at 0.60. In terms of FPincr, IP2IP doubles it (for k = 35), while intersection achieves the lowest value with 0.51 (again,\nfor k = 35). As with previous clustering algorithms, we notice that intersection yields the best decrease in FN, i.e., \u22120.5 with k = 35. Intersection also achieves the highest TPR (up to 0.77) with larger cluster sizes (i.e., for k \u2265 10), while its combination with the IP2IP reduces it (0.71) \u2013 see Fig. 6(d). Fig. 6(e) shows that intersection has the best PPV (0.19 for k = 15), similar to local (0.18), while IP2IP performs worse (0.16) due to higher FPincr (almost doubling the FP for k = 35). Finally, from Fig. 6(f), note that intersection yields the highest F1 (0.30 for k = 15)."}, {"heading": "5.3 Discussion", "text": "We summarize the best results for each clustering algorithm, in terms of highest F1, recall, precision, and TPimpr, in Tables 1\u20134. We note that intersection is that sharing mechanism that maximizes all metrics, except for TPimpr, which is instead maximized with IP2IP+intersection. Both k-means and k-NN peak at 0.30 in F1 including, respectively, 280 and 240 collaborators over all time windows. Agglomerative clustering involves all 700 contributors and achieves F1 = 0.27. k-NN, k = 35 yields the best results for TPR (0.77), while both k-NN, k = 35 and k-means, k = 5 achieve 0.19 in PPV. In terms of TPimpr, k-means reaches a maximum of 0.61 with k = 1 and clusters of size 28 on average, selecting 270 collaborators overall. Slightly lower improvements are achieved with other clustering algorithms, but with more collaborators benefiting from sharing, as well as fewer FP.\nMain take-aways. Data sharing always helps organizations forecast attacks, compared to performing predictions locally. Predicting based on all data from collaborators yields the highest improvement in TPimpr \u2013 especially for bigger clusters \u2013 but with a dramatic increase in FPincr. When organizations share correlated attacks (IP2IP), we observe a steady TPimpr, while sharing common attacks (intersection) outperforms the former when bigger clusters are formed. However, intersection introduces lower FPincr, ultimately leading to better precision and F1 measures. IP2IP+intersection always outperforms the two separate methods in terms of TPimpr, thus, it is the recommended strategy if one only wants to maximize the number of predicted attacks.\nImpact of cluster size. With agglomerative clustering, each organization is assigned to exactly one cluster and thus participates in/benefits from collaboration. We observe higher TPR for bigger clusters and, generally, a stable improvement in TP is achieved on average. Similar results are obtained with k-means when all organizations are assigned to clusters. However, when we set a distance threshold, creating more consistent clusters, we observe fluctuations in TPR: as clusters get smaller much faster (in relation to k value), IP2IP starts outperforming intersection. This indicates that correlated attacks can improve knowledge of organizations and enhance their local predictions, especially in smaller clusters. With k-NN, a different behavior is observed: for smaller clusters, IP2IP achieves higher TPR (up to 0.7 for k = 5) but, as clusters get bigger, intersection yields the best results (up to 0.77 for k = 35). Overall, collaborating in big clusters leads to high TPimpr but at the same time it introduces significant FPincr.\nIncrease/Improvement in TP/FP/FN. We also find that for all clustering algorithms, maximizing TPimpr always leads to higher FPincr, from 1.51 of k-NN up to 5.33 of Agglomerative. The settings that maximize the F1 measure, TPR, and PPV, (when sharing intersection) also minimize FNincr, e.g. agglomerative with k = 1 achieves \u22120.53 FNincr. In general, we observe that (privacy-friendly) collaboration does yield a remarkable increase in TP but also in FP, which results in a limited improvement in F1 score compared to predicting using local logs only. However, as discussed earlier, note that we count FP in a conservative way and that our main goal is really to measure the effect of different collaboration strategies on the prediction (as well as comparing to state of the art CPB techniques [16, 33]), seeking to improve TP while keeping the increase in FP as low as possible."}, {"heading": "5.4 Comparison to Soldo et al. and Freudiger et al.", "text": "We observe that [33] achieves higher maximum TPimpr (0.95) than our hybrid approach (0.61 with k-means, k = 1). However, our privacy-preserving techniques outperform [33] both in terms of recall (TPR) and precision (PPV). For instance, with k-NN, k = 35 our system yields a TPR of 0.77 (vs. 0.66 for [33]). Similarly, with k-means, k = 5 our system\u2019s PPV reaches 0.19 whereas [33]\u2019s best precision is 0.08. As a result, our hybrid model yields larger (up to 2x) F1 scores (e.g., 0.3 with k-NN, k = 15) than [33] (0.14).\nMoreover, our novel hybrid approach yields better results than [16]-(A), in terms of TPimpr and TPR. For example, in [16]-(A) TPimpr reaches only up to 0.13 (top 3% of global pairs) while in our system it reaches 0.61 with k-means, k = 1 (i.e., up to 4x improved hit counts). Likewise, we\nachieve a TPR of 0.77 (e.g., k-NN, k = 35) while the best TPR of [16]-(A) is 0.66 (top 1% of global pairs). Although the F1 scores achieved in both cases are similar \u2013 0.30 for our hybrid system vs. 0.29 for [16]-(A) \u2013 with our model more organizations benefit from collaboration. Finally, we observe that our hybrid approach yields fairly similar results to [16]- (B) in terms of TPR, PPV and F1. Nevertheless, our system achieves better TPimpr than [16]-(B) (0.61 for k-means, k = 1 vs. 0.45 with 35 local pairs) since not only common but also correlated attacks are shared within the clusters."}, {"heading": "6 Implementing At Scale", "text": "As discussed above, our system involves four steps: (1) secure computation of pairwise similarity, (2) clustering, (3) secure data sharing within the clusters, and (4) time-series prediction. To assess its scalability, we need to evaluate computation and communication complexities incurred by each step. Naturally, (1) and (3) dominate complexities as they require running a number of cryptographic protocols that depends on the number of organizations involved. In fact, clustering incurs a negligible overhead: on commodity hardware, to perform clustering with 1,000 organizations, it takes 6.1ms for k-means, 81ms for agglomerative, and 5.2ms for k-NN (k = 2). Also, time-series EWMA prediction requires 4.6\u00b5s per IP, so it takes 4.6ms for 1,000 IPs.\nAs we compute pairwise similarity based on the amount of common attacks between two organizations, and support its secure computation via PSI-CA [11], step (1) requires a number of protocol runs quadratic in the number of organizations. In our experiments (see details below), it takes 1.98s and 2.12MB bandwidth for one protocol execution, using 2048- bit moduli, with sets of size 4,000 (the average number of attacks observed by each organization). As for (3), i.e., secure within-cluster sharing of events related to common attacks (intersection), we rely on PSI-DT [12], and it takes 1.24s and 2.18MB for a single execution with the same settings. Thus, complexities may quickly become prohibitive when more organizations are involved or more alerts are used.\nServer-aided Secure Computation. Aiming to improve scalability, we introduce a variant supporting secure computation of pairwise similarity as well as secure log sharing without a quadratic number of public-key operations/quadratic communication overhead. Recall that we rely on a semi-trusted authority, STA, for clustering and coordination, which is assumed to follow protocol specifications and not to collude with other organizations, thus, we can actually use it to also help with secure computations. Inspired by Kamara et al.\u2019s server-aided PSI [21], we extend our framework by replacing public-key cryptography operations with pseudo-random permutations (PRP), which we instantiate using AES. Specifically, we minimize interactions among pairs of organizations so that the complexity incurred by each of them is constant, while only imposing a minimal, linear communication overhead on STA.\nAlgorithm 1 ENCRYPTION [All Organizations] for each Oi \u2208 O do\nSi \u2190 \u2205, Ei \u2190 \u2205, Ki \u2190 \u2205 for each (dj , timej) \u2208 Di do\nfor cnt := 1 to COUNT (dj) do Si \u2190 Si \u222a PRPk(dj ||cnt) kj \u2190 H(dj ||cnt) Ei \u2190 Ei \u222a Enckj (dj , timej) Ki \u2190 Ki \u222a kj\nSend Si, Ei to STA and store Ki\nAlgorithm 2 O2O COMPUTATION [STA] for each Oi \u2208 O do\nfor each Oj 6= Oi do O2O[i, j]\u2190 |Si \u2229 Sj | Buff[i, j]\u2190 {(`, Ej`),\u2200` \u2208 INDEX(Si \u2229 Sj)}\nPerform Clustering on O2O[\u00b7, \u00b7] Send relevant Buff[\u00b7, \u00b7] entries to organizations in the same cluster\nAlgorithm 3 LOG SHARING [Organizations in C\u2217] for each Oi \u2208 C\u2217 do\nS\u2032i \u2190 \u2205 for each Oj 6= Oi \u2208 C\u2217 do\nfor each (`, Ej`) \u2208 Buff[i, j] do S\u2032i = S \u2032 i \u222a Deck`(E`)\nOur extension involves four phases: (1) setup, where, as in [21], one organization generates a random key \u03ba and sends it to the other organizations, (2) encryption (Algorithm 1), where each organization Oi evaluates the PRP on each entry dj in their sets and encrypts the associated timestamp timej , (3) O2O computation (Algorithm 2), where STA computes the magnitude of common attacks between each pair of organizations in order to perform clustering, and (4) log sharing (Algorithm 3), where organizations in the same cluster C\u2217 receive information about common attacks (S\u2032i-s). Note that building the O2O matrix is actually optimized using hash tables (i.e., dense hash set and dense hash map from Sparehash [19]). Also, since sets in our system are multi-sets, we concatenate counters to the IP address, so that the STA cannot tell which and how many IPs appear more than once.\nExperimental Evaluation. To fully grasp the scalability of the server-aided extension, and compare it to using \u201ctraditional\u201d PSI-CA and PSI-DT, we report execution times for increasing number of participating organizations. We benchmark the performance of PSI-CA [11] and PSI-DT [12] using 2048-bit moduli, modifying the OpenSSL/GMP-based C implementation of [13], as well as the PRP-based scheme presented above and inspired by Kamara et al.\u2019s work [21]. Experiments are run using two 2.3GHz Intel Core i5 CPUs with 8GB of RAM connected via a 100Mbps Ethernet link.\nFigures 7(a) and 7(b) plot computation and communica-\ntion complexities incurred by an individual organization visa\u0300-vis the total number of organizations involved in the system, while Fig. 7(c) reports the communication overhead introduced on the STA-side for the PRP scheme. Observe that complexities for PSI-CA/PSI-DT protocols on each organization grow linearly in the number of organizations (hence, quadratic overall). For instance, if 1,000 organizations are involved, it would take about 16 minutes per organization, each transmitting 1GB. Whereas, the PRP-based scheme incurs constant complexities on each organization (57.6ms and 120KB) and an appreciably low communication overhead on the STA (about 100MB) for 1,000 organizations.\nIP2IP. We also evaluate the IP2IP method whereby organizations interact with STA in order to discover cluster-wide correlated attacks. Assuming clusters of 100 organizations and an IP2IP matrix of (224 \u00b7 224)/2 (recall that we consider the whole /24 IP space), we measure a 2.7s running time per organization with 41KB of bandwidth as well as a 0.07s overhead on the STA with 4.1MB bandwidth. Recall that we use the private Count-Min sketch based implementation by Melis et al. [27], which results in the private aggregation of 10,336 elements. Note that, even if clusters are bigger than 100, as detailed in [27], one can still perform private aggregation on multiple subgroups (e.g., of size 100) without endangering organizations\u2019 privacy.\nSecurity. Our system does not leak any information about the logs of each organization to the STA, with or without using the server-aided variant. Clustering is performed over similarity measures computed obliviously to STA, and so does within-cluster data sharing. Privacy-preserving computation occurs by using existing secure protocols such as PSICA/PSI-DT by De Cristofaro et al. [12, 11]), server-aided PSI by Kamara et al. [21], as well as private recommendation via succinct sketches by Melis et al. [27]. Therefore, we do not provide any additional proofs in the paper as the security of our techniques straightforwardly relies on that of these protocols."}, {"heading": "7 Conclusion", "text": "In this paper, we first presented the results of a comprehensive measurement study of collaborative predictive blacklisting (CPB) techniques, specifically, one relying on trusted central party by Soldo et al. [33] and another using privacypreserving data sharing by Freudiger et al. [16]. Then, we introduced and evaluated a novel, hybrid approach that improves upon the first two. Our experiments evaluated correct and incorrect predictions, as well as the real-world impact of collaboration (e.g., the improvement on true positives and the increase of false positives/negatives), using a dataset of alerts obtained from DShield.org. We found that, overall, having access to more (attack) logs does not necessarily result in better predictions \u2013 in fact, the approach proposed by Soldo et al. [33], although considered the state of the art in CPB, achieves high hit counts (almost doubling the number of correct predictions) but also suffers from very poor precision due to very high FP. On the other hand, the privacy-friendly decentralized system proposed by Freudiger et al. [16] achieves better accuracy than [33], but with a much smaller improvement in TP. Moreover, their system does not scale due its peer-to-peer nature.\nThen, our analysis shows that our novel hybrid approach manages to outperform [33] in terms of accuracy (up to 2x) and [16] in terms of hit counts (up to 4x), while maintaining an acceptable level of privacy and achieving high scalability. As part of future work, we plan to conduct a longitudinal measurement to fully grasp the effectiveness of privacyenhanced CPB in the wild, as well as study other collaborative security problems, e.g., in the context of spam, malware samples, and DNS poisoning."}, {"heading": "A Cryptography Background", "text": "Adversarial Model. We use standard security models for secure two-party computation and consider semi-honest adversaries. In the rest of this paper, the term adversary refers to insiders, i.e., protocol participants. Outside adversaries are not considered, since their actions can be mitigated via standard network security techniques. Following definitions in [18], protocols secure in the presence of semi-honest adversaries assume that parties faithfully follow all protocol specifications and do not misrepresent any information related to their inputs, e.g., size and content. However, during or after protocol execution, any party might (passively) attempt to infer additional information about the other party\u2019s input. This model is formalized by considering an ideal implementation where a trusted third party (TTP) receives the inputs of both parties and outputs the result of the defined\nfunction. Security in the presence of semi-honest adversaries requires that, in the real implementation of the protocol (without a TTP), each party does not learn more information than in the ideal implementation. Finally, we assume that parties do not collude with each other to recover other participants inputs. Private Set Intersection (PSI): a cryptographic protocol between two parties, server and client, on input, respectively, S = {s1, . . . , sw} and C = {c1, . . . , cv}. At the end, the client learns S \u2229 C. There are several PSI instantiations, with different complexities and cryptographic assumptions, ranging from those based on Oblivious Polynomial Evaluation (OPE) [15], to linear-complexity protocols based Oblivious PseudoRandom Functions (OPRFs) [12], as well as optimized garbled circuits [29] leveraging Oblivious Transfer Extension [20]. Naturally, the PSI definition above implies that only one party (client) learns the set intersection, however, in the semi-honest model, PSI can trivially be turned to a \u201cmutual PSI\u201d [23] (i.e., both parties learn the intersection) by executing PSI twice with inverted roles. Private Set Intersection Cardinality (PSI-CA): a cryptographic protocol between two parties, server and client, on input, respectively, S = {s1, . . . , sw} and C = {c1, . . . , cv}. At the end, the client learns |S \u2229 C|. That is, PSI-CA is a more stringent version of PSI as the client only learns how many items are in intersection. While it is possible to modify garbled circuits based PSI constructions to support PSICA [29], to the best of our knowledge, there is no available description of the corresponding circuit or ready-to-use implementation, therefore, we use the special purpose PSI-CA protocol from [11]. This protocol is secure in the Random Oracle Model under the One-More Diffie-Hellman assumption in the presence of semi-honest adversaries. It incurs communication and computational complexities linear in the size of the sets: parties need to exchange O(v + w) group items, and compute O(v + w) modular exponentiations with short exponents. Similar to PSI, in the semi-honest model, two executions of PSI-CA with inverted roles yield a mutual PSI-CA where both parties learn the cardinality of the set intersection. PSI with Data Transfer (PSI-DT): a cryptographic protocol between server and client on input, respectively, S\u2032 = {(s1, data1) . . . , (sw, dataw)} and C = {c1, . . . , cv}. At the end, the client obtains {(si, datai | \u2203 cj s.t. si = cj}. In other words, the client not only learns which items are in the intersection, but also gets related data records. Special purpose protocols for PSI-DT have been proposed [15, 12], but we do not know of any available garbled circuits based instantiation. Hence, we use the PSI-DT protocol described in [12], secure in the Random Oracle Model under the OneMore RSA assumption in the presence of semi-honest adversaries. It incurs communication and computational complexities linear in the size of the sets: parties need to exchange O(v + w) group items, and compute O(w) RSA-CRT exponentiations and O(v) modular multiplications if one picks a small RSA public exponent (e.g., 3 or 17). Once again, in the semi-honest model, two executions of PSI-DT with inverted roles trivially yield a mutual PSI-DT where both parties learn the intersection. Server-aided PSI [21]. In [21], Kamara et al. propose a server-aided PSI relying on a semi-honest server: during a setup phase, parties {P1, . . . , Pn} jointly generate a secret key \u03ba for a pseudorandom permutation (PRP). Each party Pi then randomly permutes their set Si, by computing PRP\u03ba(Si), and sends the result to the server. This then computes the intersection of the labels PRP\u03ba(Si), i = 1, . . . , n and returns it to all the parties. Finally, each Pi outputs the inverse of PRP() over the intersection of the labels. The protocol is secure in the presence of a semi-honest server and honest parties, or a honest server and any collusion of malicious parties, if the PRP is secure. Efficient Private Recommendation via Succinct Sketches [27]. A privacy-friendly recommender system based on Item-KNN [31] has been introduced by Melis et al. [27]. Their construction involves a \u201ctally\u201d server (the BBC in their application example) and a set of users (visitors of BBC\u2019s broadcasting site iPlayer). The main goal of their system is to train the recommender system using only aggregate statistics. Specifically, they build a global matrix of co-views (i.e., pairs of programs watched by the same user) in a privacy-preserving way, by relying on (i) private data aggregation based on secret sharing (inspired by [24]), and (ii) Count-Min sketches [8] to reduce the computation/communication overhead from linear to logarithmic in the size of the matrix, trading off an upper-bounded error with increased efficiency. If M denotes the number of items, the compact representation of the IP2IP through the Count-Min Sketch has size O(log(M \u2217M/2)). More precisely, given parameters ( , \u03b4), the Count-Min Sketch is a matrix of size L = d \u00d7 w where d = dln (M \u2217M)/(2 \u2217 \u03b4)e and w = de/ e. Melis et al. [27] set = \u03b4 = 0.01, yielding, e.g., L = 4, 896 for M = 1, 000, and L = 10, 336 for M = 224. The parameters ( , \u03b4) give an upper bounded error for the estimated counters c\u0302i amounting to c\u0302i \u2264 ci + \u2211 j |cj | with probability 1 \u2212 \u03b4, where ci is the true element. As demonstrated empirically by Melis et al. [27], the error ultimately introduces a negligible impact on the accuracy of the aggregation as well as the recommendation. Finally, the computational overhead introduced by the cryptographic operations for private aggregation, as demonstrated experimentally in [27], are in the order of seconds even with thousands of items."}, {"heading": "B Clustering Algorithms", "text": "Agglomerative Clustering. Hierarchical Clustering algorithms build nested clusters by merging or splitting them successively. The hierarchy is represented as a tree, with the root being the unique cluster that gathers all the samples, and\nthe leaves the clusters with only one sample. Agglomerative clustering performs hierarchical clustering using a bottomup approach: each observation starts in its own cluster, and clusters are successively merged together. Different linkage criteria determine the actual metric used to merge, e.g., average linkage minimizes the average of the distances between all observations of pairs of clusters, while complete linkage minimizes the maximum distance between the observations of pairs of clusters. k-means. k-means clustering separates samples in groups of equal variance, minimizing inertia or within-cluster sum of squares. The k-means algorithm requires the number of clusters to be specified as it divides a set of N samples X into k disjoint clusters C, each described by the mean \u00b5j of the samples in the cluster. The means are commonly called the cluster \u201ccentroids\u201d and the algorithm chooses centroids that\nminimize \u2211n i=0 min\u00b5j\u2208C(||xj \u2212 \u00b5i||2). The algorithm includes three steps: (1) choosing the initial centroids, often by choosing k samples from X; (2) assigning each sample to its nearest centroid; and (3) creating new centroids by taking the mean value of all samples assigned to each previous centroid. The algorithm loops between (2) and (3) until the difference between the old and the new centroids is below a threshold.\nk-Nearest Neighbors (k-NN). k-NN is a simple machine learning algorithm that finds a predefined number of training samples closest in distance to a new sample. The number of samples can be a user-defined constant and the distance can be any metric measure: standard Euclidean distance is the most common choice. In Section 5, we employ unsupervised k-NN to identify, for each organization, its most similar ones."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Collaborative security initiatives are increasingly often ad-<lb>vocated to improve timeliness and effectiveness of threat mit-<lb>igation. Among these, collaborative predictive blacklisting<lb>(CPB) aims to forecast attack sources based on alerts con-<lb>tributed by multiple organizations that might be targeted in<lb>similar ways. Alas, CPB proposals thus far have only fo-<lb>cused on improving hit counts, but overlooked the impact of<lb>collaboration on false positives and false negatives. More-<lb>over, sharing threat intelligence often prompts important pri-<lb>vacy, confidentiality, and liability issues. In this paper, we<lb>first provide a comprehensive measurement analysis of two<lb>state-of-the-art CPB systems: one that uses a trusted cen-<lb>tral party to collect alerts [Soldo et al., Infocom\u201910] and a<lb>peer-to-peer one relying on controlled data sharing [Freudi-<lb>ger et al., DIMVA\u201915], studying the impact of collaboration<lb>on both correct and incorrect predictions. Then, we present<lb>a novel privacy-friendly approach that significantly improves<lb>over previous work, achieving a better balance of true and<lb>false positive rates, while minimizing information disclosure.<lb>Finally, we present an extension that allows our system to<lb>scale to very large numbers of organizations.", "creator": "LaTeX with hyperref package"}}}