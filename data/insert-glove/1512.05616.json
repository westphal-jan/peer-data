{"id": "1512.05616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Deep-Spying: Spying using Smartwatch and Deep Learning", "abstract": "Wearable cryptozoology technologies tailwheel are teschen today on outcrossing the braunwald rise, becoming mount more gertrude common e-toiba and 1,564 broadly available pagewood to mainstream users. In kaiman fact, 109.16 wristband elite and armband devices barai such sociobiology as smartwatches estime and 78-day fitness trackers ghonda already -11:00 took an important sabretooth place in argentinean the consumer electronics pup market and anneke are sungrebe becoming corundum ubiquitous. moine By drank their very nature 11:07 of wolpert being gelovani wearable, idr these devices, however, provide a o'flanagan new pervasive attack yevgen surface threatening marudai users privacy, premonstratensians among kyogen others.", "histories": [["v1", "Thu, 17 Dec 2015 14:58:26 GMT  (5999kb,D)", "http://arxiv.org/abs/1512.05616v1", "Security, Side-Channel Attack, Keystroke Inference, Motion Sensors, Deep Learning, Recurrent Neural Network, Wearable Computing"]], "COMMENTS": "Security, Side-Channel Attack, Keystroke Inference, Motion Sensors, Deep Learning, Recurrent Neural Network, Wearable Computing", "reviews": [], "SUBJECTS": "cs.CR cs.CY cs.LG", "authors": ["tony beltramelli", "sebastian risi"], "accepted": false, "id": "1512.05616"}, "pdf": {"name": "1512.05616.pdf", "metadata": {"source": "META", "title": "Deep-Spying: Spying using Smartwatch and Deep Learning", "authors": ["Tony Beltramelli"], "emails": [], "sections": [{"heading": null, "text": "Wearable technologies are today on the rise, becoming more common and broadly available to mainstream users. In fact, wristband and armband devices such as smartwatches and fitness trackers already took an important place in the consumer electronics market and are becoming ubiquitous. By their very nature of being wearable, these devices, however, provide a new pervasive attack surface threatening users privacy, among others.\nIn the meantime, advances in machine learning are providing unprecedented possibilities to process complex data efficiently. Allowing patterns to emerge from high dimensional unavoidably noisy data.\nThe goal of this work is to raise awareness about the potential risks related to motion sensors built-in wearable devices and to demonstrate abuse opportunities leveraged by advanced neural network architectures.\nThe LSTM-based implementation presented in this research can perform touchlogging and keylogging on 12-keys keypads with above-average accuracy even when confronted with raw unprocessed data. Thus demonstrating that deep neural networks are capable of making keystroke inference attacks based on motion sensors easier to achieve by removing the need for non-trivial preprocessing pipelines and carefully engineered feature extraction strategies. Our results suggest that the complete technological ecosystem of a user can be compromised when a wearable wristband device is worn.\nKeywords: Security, Side-Channel Attack, Keystroke Inference, Motion Sensors, Deep Learning, Recurrent Neural Network, Wearable Computing\ni\nAcknowledgments\nI would first like to deeply thank Sebastian Risi for his insightful advice during the entire duration of this thesis and his immediate interest in the project idea. Special thanks also go to the seven voluntary participants who took some of their time to help me collect valuable data.\nFurthermore, I would like to thank my beloved parents Jean-Claude and Marie, my sister Leslie and all my family for their continuous support and encouragement throughout my master studies. I would also like to thank Cecilie for inspiring me and for her constant support.\nFinally, I would like to thank the REAL lab, the PITlab, and the IT department at the IT University of Copenhagen for providing me with hardware and computational resources allowing me to conduct the experiments detailed in this work.\nii\nContents\nAbstract i\nAcknowledgments ii\nContents iii\nList of figures vii\nList of tables x"}, {"heading": "1 Introduction 1", "text": "1.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . 3"}, {"heading": "2 Related Work 5", "text": "2.1 Key Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.1.1 Computer Security . . . . . . . . . . . . . . . . . . . . 6 2.1.2 Wearable Computing . . . . . . . . . . . . . . . . . . . 6 2.1.3 Machine Learning . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Background: Artificial Neural Network . . . . . . . . . . . . . 8\n2.2.1 Recurrent Neural Network . . . . . . . . . . . . . . . . 11 2.2.2 Long Short-Term Memory . . . . . . . . . . . . . . . . 12 2.2.3 Backpropagation . . . . . . . . . . . . . . . . . . . . . 15\n2.3 Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.3.1 Motion-Based Keystroke Inference Attack . . . . . . . 17\n2.3.1.1 Keylogging . . . . . . . . . . . . . . . . . . . 18 2.3.1.2 Touchlogging . . . . . . . . . . . . . . . . . . 19\n2.3.2 Classification of Motion Sensors Signal . . . . . . . . . 20\n2.3.2.1 Classifier Model . . . . . . . . . . . . . . . . . 20\niii\nContents\n2.3.2.2 Data Analysis and Feature Extraction . . . . 21\n2.3.3 Additional Mentions . . . . . . . . . . . . . . . . . . . 23"}, {"heading": "3 Attack Description 25", "text": "3.1 Attacker Goals . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2 Threat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.3 Attack Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . 27"}, {"heading": "4 System 29", "text": "4.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . 29 4.2 Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.2.1 Wearable Application . . . . . . . . . . . . . . . . . . . 30 4.2.2 Mobile Application . . . . . . . . . . . . . . . . . . . . 31 4.2.3 Training Application . . . . . . . . . . . . . . . . . . . 33\n4.3 Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34"}, {"heading": "5 Data Analytics 36", "text": "5.1 Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.2 Pre-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n5.2.1 Calibration . . . . . . . . . . . . . . . . . . . . . . . . 38 5.2.2 Median Filter . . . . . . . . . . . . . . . . . . . . . . . 40 5.2.3 Butterworth Filter . . . . . . . . . . . . . . . . . . . . 41 5.2.4 Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . 43\n5.3 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.3.1 Sensor Fusion . . . . . . . . . . . . . . . . . . . . . . . 46 5.3.2 Segmentation . . . . . . . . . . . . . . . . . . . . . . . 49\n5.3.2.1 Segmentation from Labels . . . . . . . . . . . 49 5.3.2.2 Heuristic Segmentation . . . . . . . . . . . . . 51\n5.3.3 Statistical Features . . . . . . . . . . . . . . . . . . . . 53\n5.4 Classifier Model . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n5.4.1 Performance Evaluation . . . . . . . . . . . . . . . . . 54\niv\nContents\n5.4.2 Sensor Fusion Benchmark . . . . . . . . . . . . . . . . 56 5.4.3 Model Benchmark . . . . . . . . . . . . . . . . . . . . 58"}, {"heading": "6 Evaluation 61", "text": "6.1 Empirical Data Collection . . . . . . . . . . . . . . . . . . . . 61 6.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n6.2.1 Experiment 1: Touchlogging Attack . . . . . . . . . . . 64 6.2.2 Experiment 2: Keylogging Attack . . . . . . . . . . . . 66 6.2.3 Experiment 3: from Touchlogging to Keylogging . . . . 69\n6.3 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70"}, {"heading": "7 Conclusion 72", "text": "7.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\nBibliography 75\nAppendices 85"}, {"heading": "A Backpropagation 86", "text": ""}, {"heading": "B Signal Pre-processing 88", "text": "B.1 Gyroscope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 B.2 Accelerometer . . . . . . . . . . . . . . . . . . . . . . . . . . . 91"}, {"heading": "C Confusion Matrices from Model Benchmark 94", "text": "C.1 Model Training with Statistical Features . . . . . . . . . . . . 94 C.2 Model Training with Data Segment as Features . . . . . . . . 95"}, {"heading": "D Experiment Results 98", "text": "D.1 Results for Experiment 1: Touchlogging Attack . . . . . . . . 99\nD.1.1 FNN-Sigmoid . . . . . . . . . . . . . . . . . . . . . . . 99 D.1.2 FNN-Tanh . . . . . . . . . . . . . . . . . . . . . . . . . 101\nv\nContents\nD.1.3 RNN-LSTM . . . . . . . . . . . . . . . . . . . . . . . . 103\nD.2 Results for Experiment 2: Keylogging Attack . . . . . . . . . . 106\nD.2.1 FNN-Sigmoid . . . . . . . . . . . . . . . . . . . . . . . 106 D.2.2 FNN-Tanh . . . . . . . . . . . . . . . . . . . . . . . . . 108 D.2.3 RNN-LSTM . . . . . . . . . . . . . . . . . . . . . . . . 110\nD.3 Results for Experiment 3: from Touchlogging to Keylogging . 112\nvi\nList of Figures\n2.1 Related research fields. . . . . . . . . . . . . . . . . . . . . . . 5 2.2 A feedforward neural network with 3 input neurons, 2 hidden\nlayers h and h\u2032 containing 4 neurons each, and 2 output neurons. 9\n2.3 A standard feedforward cell i with three other neurons a, b,\nand c connected to its input. . . . . . . . . . . . . . . . . . . . 10\n2.4 A standard RNN cell (see text for details). . . . . . . . . . . . 11 2.5 A RNN can be seen as an unfolded deep FNN. The depth\ncorresponds to the length n of the input sequence. . . . . . . . 12\n2.6 LSTM memory cell (see text for details). . . . . . . . . . . . . 13\n3.1 Smartphone attack overview. . . . . . . . . . . . . . . . . . . . 27 3.2 ATM attack overview. . . . . . . . . . . . . . . . . . . . . . . 28\n4.1 System overview using the Client-Server architectural model. . 30 4.2 Components communication during data acquisition. . . . . . 32 4.3 Training devices. . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.4 Data analytics server main components. . . . . . . . . . . . . 35\n5.1 Data processing pipeline. . . . . . . . . . . . . . . . . . . . . . 36 5.2 Noisy raw signals recorded from motion sensors. . . . . . . . . 37 5.3 Signals after calibration. . . . . . . . . . . . . . . . . . . . . . 39 5.4 Signals after median filtering. . . . . . . . . . . . . . . . . . . 41 5.5 Signals after Butterworth filtering. . . . . . . . . . . . . . . . 43 5.6 Signals ready for feature extraction after the last step of the\npre-processing pipeline. . . . . . . . . . . . . . . . . . . . . . . 44\n5.7 Effect of the pre-processing pipeline on the recorded signal. . . 45 5.8 Gyroscope and accelerometer mean signal aligned with time\nfitting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nvii\nList of Figures\n5.9 Segmentation of a gyroscope signal consisting of two sequences\nof the same four keys. . . . . . . . . . . . . . . . . . . . . . . . 50\n5.10 Example of peaks detected (shown as green circles) from the\ngyroscope Peak-to-Average Power Ratios with actual keystroke positions (shown as vertical dashed lines). . . . . . . . . . . . 52\n5.11 Neural network used for sensor fusion benchmark. . . . . . . . 56 5.12 Neural network template. . . . . . . . . . . . . . . . . . . . . . 58 5.13 Hidden layer benchmark (see Table 5.2 for references). . . . . 60\n6.1 Neural network architectures selected for experiments. . . . . . 62 6.2 F1 Score for the different ANN architectures during the touchlog-\nging experiment. . . . . . . . . . . . . . . . . . . . . . . . . . 64\n6.3 F1 Score for the different ANN architectures on the keylogging\nexperiment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\nB.1 Gyroscope raw signal. . . . . . . . . . . . . . . . . . . . . . . . 88 B.2 Gyroscope signal after calibration. . . . . . . . . . . . . . . . . 89 B.3 Gyroscope signal after median filtering. . . . . . . . . . . . . . 89 B.4 Gyroscope signal after low-pass Butterworth filtering. . . . . . 90 B.5 Gyroscope signal after Kalman filtering. . . . . . . . . . . . . 90 B.6 Pre-processed gyroscope signal with labels. . . . . . . . . . . . 90 B.7 Accelerometer raw signal. . . . . . . . . . . . . . . . . . . . . 91 B.8 Accelerometer signal after calibration. . . . . . . . . . . . . . . 91 B.9 Accelerometer signal after median filtering. . . . . . . . . . . . 92 B.10 Accelerometer signal after high-pass Butterworth filtering. . . 92 B.11 Accelerometer signal after Kalman filtering. . . . . . . . . . . 92 B.12 Pre-processed accelerometer signal with labels. . . . . . . . . . 93\nC.1 Feedforward Sigmoid. . . . . . . . . . . . . . . . . . . . . . . . 94 C.2 Feedforward Tanh. . . . . . . . . . . . . . . . . . . . . . . . . 95 C.3 Feedforward Sigmoid. . . . . . . . . . . . . . . . . . . . . . . . 95 C.4 Feedforward Tanh. . . . . . . . . . . . . . . . . . . . . . . . . 96\nviii\nList of Figures\nC.5 Recurrent LSTM with forget gate. . . . . . . . . . . . . . . . . 96 C.6 Recurrent LSTM with peephole connections. . . . . . . . . . . 97\nD.1 Touchlogging with FNN-Sigmoid P-T . . . . . . . . . . . . . . 99 D.2 Touchlogging with FNN-Sigmoid P-H . . . . . . . . . . . . . . 100 D.3 Touchlogging with FNN-Sigmoid R-T . . . . . . . . . . . . . . 100 D.4 Touchlogging with FNN-Sigmoid R-H . . . . . . . . . . . . . . 101 D.5 Touchlogging with FNN-Tanh P-T . . . . . . . . . . . . . . . 101 D.6 Touchlogging with FNN-Tanh P-H . . . . . . . . . . . . . . . 102 D.7 Touchlogging with FNN-Tanh R-T . . . . . . . . . . . . . . . 102 D.8 Touchlogging with FNN-Tanh R-H . . . . . . . . . . . . . . . 103 D.9 Touchlogging with RNN-LSTM P-T . . . . . . . . . . . . . . . 103 D.10 Touchlogging with RNN-LSTM P-H . . . . . . . . . . . . . . . 104 D.11 Touchlogging with RNN-LSTM R-T . . . . . . . . . . . . . . . 104 D.12 Touchlogging with RNN-LSTM R-H . . . . . . . . . . . . . . 105 D.13 Keylogging with FNN-Sigmoid P-T . . . . . . . . . . . . . . . 106 D.14 Keylogging with FNN-Sigmoid P-H . . . . . . . . . . . . . . . 107 D.15 Keylogging with FNN-Sigmoid R-T . . . . . . . . . . . . . . . 107 D.16 Keylogging with FNN-Sigmoid R-H . . . . . . . . . . . . . . . 108 D.17 Keylogging with FNN-Tanh P-T . . . . . . . . . . . . . . . . . 108 D.18 Keylogging with FNN-Tanh P-H . . . . . . . . . . . . . . . . . 109 D.19 Keylogging with FNN-Tanh R-T . . . . . . . . . . . . . . . . . 109 D.20 Keylogging with FNN-Tanh R-H . . . . . . . . . . . . . . . . 110 D.21 Keylogging with RNN-LSTM P-T . . . . . . . . . . . . . . . . 110 D.22 Keylogging with RNN-LSTM P-H . . . . . . . . . . . . . . . . 111 D.23 Keylogging with RNN-LSTM R-T . . . . . . . . . . . . . . . . 111 D.24 Keylogging with RNN-LSTM R-H . . . . . . . . . . . . . . . . 112 D.25 Touchlogging to Keylogging with RNN-LSTM P-T . . . . . . 112 D.26 Touchlogging to Keylogging with RNN-LSTM P-H . . . . . . 113 D.27 Touchlogging to Keylogging with RNN-LSTM R-T . . . . . . 113 D.28 Touchlogging to Keylogging with RNN-LSTM R-H . . . . . . 114\nix\nList of Tables\n2.1 Typical motion sensors on mobile and wearable devices. . . . . 7\n2.2 Popular activation functions used in ANNs. . . . . . . . . . . 10\n5.1 Fusion strategy benchmark results (average values for 100 train-\ning Epochs). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n5.2 Hidden layer benchmark (see Figure 5.13 for graphical repre-\nsentation). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n6.1 Touchlogging using FNN-Sigmoid with statistical features. . . 65\n6.2 Touchlogging using FNN-Tanh with data segment as features. 65\n6.3 Touchlogging using RNN-LSTM with data segment as features. 66\n6.4 Keylogging using FNN-Sigmoid with statistical features. . . . 67\n6.5 Keylogging using FNN-Tanh with data segment as features. . 68\n6.6 Keylogging using RNN-LSTM with data segment as features. . 69\n6.7 Results from RNN-LSTM trained for touchlogging and evalu-\nated for keylogging with data segments used as features. . . . 70\nx\n1 Introduction\nThis chapter will first introduce the reader to the problem being addressed in this research and the related implications. Finally, the methodology employed to provide a practical proof-of-concept system will be shortly described."}, {"heading": "1.1 Problem Statement", "text": "The keyboard is one of the oldest human-computer interface and still one of the most common devices to input information into various types of machines. Some of this information can be sensitive and highly valuable, such as passwords, PINs, social security numbers, and credit card numbers. Related works (detailed in Chapter 2) have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen [16, 84, 66]. Other research has proved that the motion sensors from a smartphone standing on a flat surface can be used to infer the keystrokes typed on a nearby physical computer keyboard [61]. Moreover, recently published works have demonstrated that smartwatches motion sensors could be exploited to infer keystrokes on both virtual and physical keyboards [81, 59, 56].\n1\nChapter 1. Introduction\nThe security of Wearable Wristband and Armband Devices (WADs) such as smartwatches and fitness trackers is of great concern not directly because of the device itself being exploitable1 but because of the very nature of wearable devices being wearable. A smartwatch is indeed potentially worn for an extended period such as the whole day, offering a pervasive attack surface to cyber-criminals.\nThe implications are therefore significant: exploiting motion sensors for keystrokes inference can happen continuously as long as a WAD is worn. More dramatically, the whole technological ecosystem of the user is compromised each time a WAD is worn. One can indeed extrapolate that keystrokes inference attack is possible on any keypad used by a person while she is wearing a WAD. For example, the virtual keyboard of a tablet computer, a smartphone touchscreen, the physical keyboard of a laptop computer, the keypad of an electronic building access system, the keypad of a hotel room safe or even the keyboard of a bank ATM. Eavesdropping on WAD sensors can thus have implications reaching far beyond a simple privacy leakage and have the potential to cause important damages.\nMoreover, recent advances in Artificial Intelligence and notably Deep Learn-\ning have allowed algorithms to solve problems with impressive performance sometimes even surpassing human experts. Deep neural networks are able to process robustly noisy real-world data and can automatically learn features from raw data. These powerful models have successfully been applied to complex tasks in the fields of Computer Vision, Natural Language Processing and Speech Recognition [54, 73]. Deep Learning has however comparatively been used poorly to process time series data such as motion sensors [52]. These state-of-the-art scientific tools used to require advanced\n1A smartwatch is not dramatically different from a smartphone from a technological point of view, the sensors are the same and the features are largely similar. It is even reasonable to assume that a smartwatch is much more limited because of power and performance limitations.\n2\nChapter 1. Introduction\nknowledge to be implemented and applied successfully. However, their remarkable qualities lead to the development of various Open-Source projects [79, 21, 20, 9, 7, 28, 47, 36, 78] making them available and free to use by anyone. Even though this offers great possibilities to everyone from businesses to hobbyists, it potentially puts Deep Learning in the toolbox of cyber-criminals.\nTo the best of our knowledge, keystroke inference in related works has only been performed using shallow models requiring manual feature extraction and carefully engineered signal processing pipelines. The use of a deep neural network approach can drastically cut down the number of technical steps towards successful keystroke inference. Thus making the attack a more plausible threat against users of WAD.2\nTwo research goals can thus be formulated:\n\u2022 Assess the practicality of motion-based keystroke inference attack using wearable wristband/armband technology.\n\u2022 Assess the practicality of motion-based keystroke inference attack using deep neural network models."}, {"heading": "1.2 Methodology", "text": "The research questions enunciated in Section 1.1 are answered in a practical manner. First, a system is implemented to collect, process, analyze mo-\n2A bug in Android was discovered while working on this research project and was thought to lead to a vulnerability. In fact, the bug leads applications targeting Android Wear to grant some permissions without them being explicitly defined in the manifest file [8]. A responsible disclosure process was thus initiated with Google to fix the issue. After further investigations, the problem turned out to be a bug in the Android SDK with no serious security implications towards users.\n3\nChapter 1. Introduction\ntion sensors data and perform experimental indirect passive attacks such as keylogging and touchlogging. Second, experiments are conducted to collect data in a deployment environment. Finally, the results are interpreted and discussed.\nChallenges that will be addressed in particular are:\n1. Data Acquisition: The system should allow sensor recording at specific\nkeystroke intervals in the continuous data stream.\n2. Data Pre-processing: For comparison purpose in different contexts,\ndata obtained from sensor outputs need to be reduced to a meaningful synthesis to limit the effect of noise.\n3. Feature Extraction: For comparison purpose with traditional methods,\nsuccessful classification traditionally relies on carefully chosen discriminant features.\n4. Classification: The artificial neural network should be trained as opti-\nmally as possible in order to avoid over-fitting and improve generalization.\n5. Evaluation: The quality of the classifier must be quantified to assess\nits performance in practice with exotic data as input.\n4\n2 Related Work\nThis chapter\u2019s goal is twofold. First to define the key concepts of the theoretical and technical background on which this work is based. This research project is highly multidisciplinary and established at the intersection of various research fields (as illustrated in Figure 2.1). The second goal is thus to review and reflect on the previous relevant studies and the current stateof-the-art in the related fields. The core focus is the security of wearable technologies and relies primarily on machine learning methods for data analysis.\n5\nChapter 2. Related Work"}, {"heading": "2.1 Key Concepts", "text": ""}, {"heading": "2.1.1 Computer Security", "text": "Passive Attack: A passive attack is characterized by an attacker eavesdropping on a communication channel. In such an attack, the attacker does not attempt to break the system or alter the transmitted data (i.e. active attack). Instead, the attacker is monitoring the exchanged packets to gain information about the target (e.g. the users, the system, the communicating entities) [15].\nSide-channel Attack: A side-channel attack is defined by an attacker using side-channel information to obtain insights about a system. Side-channel information can be gained from data leaked at the physical layer (e.g. timing information, power consumption, electromagnetic emanations, sound) [19].\nKeylogging and Touchlogging: Keylogging is the action of recording the keys entered on a keyboard by a user. Similarly, touchlogging is the action of recording the buttons pressed on a touch screen, or the coordinates of the touch events allowing the inference of the keys virtually touched by the user [23]. Keystroke inference refers to techniques used to perform keylogging or touchlogging from side-channels."}, {"heading": "2.1.2 Wearable Computing", "text": "Wearable Technology: Wearable technologies are envisioned to be small and portable computers integrated into clothes or worn continuously (e.g. glasses, armband, wristband). These devices are designed to be extensively mobile and operate in environments that may have limited computing infrastructure support [50].\n6\nChapter 2. Related Work\nMotion Sensors: Modern mobile and wearable devices usually come with built-in motion sensors measuring the movements of the device. Analyzing the output data of such sensors allow the estimation of specific types of motion that the device undergoes such as translation, tilt, shake, rotation, or swing. The typical motion sensors available in standard devices are listed in Table 2.1. Software-based sensors usually derive their data from hardwarebased sensors, namely the accelerometers (one for each axis x, y, and z), and the gyroscope [42, 40, 43, 1].\n7\nChapter 2. Related Work"}, {"heading": "2.1.3 Machine Learning", "text": "Classification: In the field of machine learning, classification is the process of assigning categories to data. The classifier is a function assigning labels to data by building a statistical model (i.e. data structure) based on a training set containing example data with known associated classes. This process is known as supervised learning since the statistical model needs to be trained with expert-annotated data to classify subsequently unseen samples [12, 31].\nFeature Extraction: Building a relevant statistical model is only possible if the information used is describing the problem in a meaningful way. The term Feature Extraction is used to refer to methods allowing the selection of such valuable information in a raw dataset. This dimensionality reduction process consists of building feature vectors of discrete length allowing to reduce the volume of data to process and improving its quality. Feature Engineering is the process of manual feature selection from experts. Unsupervised Feature Learning is a process where the model selects features automatically through training [12, 73]."}, {"heading": "2.2 Background: Artificial Neural Network", "text": "Artificial Neural Network (ANN) is a class of biologically-inspired statistical model consisting of a set of connected nodes (i.e. neurons) where each connection (i.e. synapses) has a weight associated with it [12, 33]. A typical ANN consists of an input layer with a number of input neurons equal to the length of the feature vector. Its output layer can be built with a variable number of neurons depending on the task at hand (e.g. equal to the number of classes for classification, two output neurons for binary classification, one output neuron for regression). ANNs are usually additionally composed of\n8\nChapter 2. Related Work\nhidden layers each containing a variable number of neurons as depicted in Figure 2.2.\nThe network is activated by feeding its input layer with a feature vector that will be mapped to an output vector thanks to the network internal structure. The neurons map inputs to outputs by using a predefined activation function (examples listed in Table 2.2). The output value of a given neuron i can be computed as follows:\nyi = \u03c6(xi) = \u03c6 ( n\u2211 j=1 Wijyj ) (2.1)\nWith \u03c6 the activation function of the neuron, n the number of neurons connected to neuron i, W the weight associated with the connection between two neurons, and y the output value. The input x to a neuron is thus the weighted sum of outputs from connected neurons as illustrated in Figure 2.3.\n9\nAn ANN is trained by adjusting its weights until the correct output vector is generated from a given input so as to minimize the global error. The terms Feedforward Neural Network (FNN) and Vanilla Neural Network are used to refer to the most basic ANN architecture where the neurons are connected forward in an acyclic way. That is, the activation flow is unidirectional from the input layer to the output layer.\n10\nChapter 2. Related Work"}, {"heading": "2.2.1 Recurrent Neural Network", "text": "Recurrent Neural Network (RNN) is a class of ANN able to model relationships between sequential data-points. This type of model have gain great interest for application in context where data with time relation have to be processed. The network can indeed associate data in series by using a feedback loop allowing information to persist over time. That is, when an input vector is fed into the network, it will produce an output vector from the new input vector but also according to the vector previously seen as illustrated in Figure 2.4. This memory state makes RNNs particularly suitable for processing sequential data such as time series, sound, video or text.\nMathematically, the output of an RNN cell can be expressed as follows:\nyt = \u03c6 \u2032(Wxixt +Wyiyt\u22121) (2.2)\nwith xt the new input vector at time t, yt\u22121 the previously produced output vector, and the activation function \u03c6\u2032 the Hyperbolic Tangent (Tanh). As shown in Figure 2.5, RNNs can be seen as unfolded deep FNNs where each layer is connected to its past instance. It is thus possible to use RNN to map one input to one output, one input to many outputs, many inputs to one output, or many inputs to many outputs.\n11\nDespite the interesting properties of RNN, Bengio et al. [10] have shown that standard RNNs are in practice unable to learn long-term dependencies in contexts where information need to be connected over long time intervals. In fact, training RNN with gradient descent methods such as Backpropagation (details in Section 2.2.3) lead to gradually vanishing gradient because of nested activation functions. In the case of RNN where the depth can be both layer-related and time-related, this leads the network to be unable to associate information separated over long periods because the error cannot be preserved over such intervals."}, {"heading": "2.2.2 Long Short-Term Memory", "text": "Hochreiter and Schmidhuber [34] overcame the limitations of standard RNN by introducing a new architecture termed Long Short-Term Memory (LSTM) which allows the association of input with memories remote in time by preserving the backpropagated error through time and layers. While many LSTM implementation variants have been proposed [30, 45], the following detailed LSTM cell use a forget gate [27] with no bias for simplicity reasons.\n12\nAs depicted in Figure 2.6, an LSTM memory cell consists of many different gates that can learn to store, read, write and erase information. Weights are associated with the connection between the different gates and are updated during training. The cell state c learns to memorize information by connecting one of its output to its inputs as traditional RNN cells. The input gate i is used to control the error flow on the inputs of cell state c to avoid input weight conflicts that occur in traditional RNN because the same weight has to be used for both storing certain inputs and ignoring others. The output gate o controls the error flow from the outputs of the cell state c to prevent output weight conflicts that happen in standard RNN because the same weight has to be used for both retrieving information and not retrieving others. The LSTM memory block can thus use i to decide when to write information in\n13\nChapter 2. Related Work\nc and use o to decide when to read information from c [34]. Additionally, a forget gate f is used to reset memory and, as a result, help the network process continuous sequences or sequences that are not segmented with precise starting and ending time [27]. The different gates outputs can be computed as follows:\nit = \u03c6(Wxixt +Wyiyt\u22121) (2.3)\nft = \u03c6(Wxfxt +Wyfyt\u22121) (2.4)\not = \u03c6(Wxoxt +Wyoyt\u22121) (2.5) zt = \u03c6 \u2032(Wxzxt +Wyzyt\u22121) (2.6)\nct = ftct\u22121 + itzt (2.7)\nyt = ot\u03c6 \u2032(ct) (2.8)\nWith W the weight, xt the new input vector, yt\u22121 the previously produced output vector, ct\u22121 the previously produced cell state\u2019s output, \u03c6 the Logistic Function (Sigmoid), and \u03c6\u2032 the Hyperbolic Tangent (Tanh).\nOne popular LSTM architecture alternative was proposed by Gers et al. [26] to allow recurrent networks to distinguish between sequences of variable length. This implementation adds peephole connections to pass the previous cell state to the input gate and the forget gate and the current cell state to the output gate. The gates outputs can thus be computed as follows:\nit = \u03c6(Wxixt +Wyiyt\u22121 +Wcict\u22121) (2.9)\nft = \u03c6(Wxfxt +Wyfyt\u22121 +Wcfct\u22121) (2.10)\not = \u03c6(Wxoxt +Wyoyt\u22121 +Wcoct) (2.11)\n14\nChapter 2. Related Work"}, {"heading": "2.2.3 Backpropagation", "text": "Backpropagation is a popular algorithm designed to train ANNs using a gradient descent method to minimize the network prediction error [70]. The network error is computed from a loss function (e.g. Mean Squared Error, Root Mean Squared Error, Quadratic Loss, Minkowski-R Error) by comparing the predicted vector with the expected vector. The error is propagated backward from the output layer to the input layer to adjust the weights along the way. Since each weight is responsible for a portion of the error, they are updated using the chain rule (the reader is invited to refer to Appendix A for additional theory details) to compute the partial derivative with respect to each weight such that:\n\u2202E\n\u2202Wij = eiyj (2.12)\nWith E the output error to minimize, W the weight to update, y the output value, and e the local error. The local error e is computed differently depending on the neuron type. If the neuron belongs to the output layer, the error is proportional to the difference between the predicted value and the expected output. Otherwise, the error of hidden neurons is proportional to the weighted sum of errors from connected neurons [12, 31, 33]. That is, the error of a neuron i is computed as follows:\nei =  \u2202\u03c6(xi) \u2202xi (Ti \u2212 yi) if i \u2208 output layer, \u2202\u03c6(xi)\n\u2202xi ( n\u2211 j=1 Wijej ) otherwise;\n(2.13)\nWith \u2202\u03c6(xi) \u2202xi the derivative of the activation function (see Table 2.2), x the input to the neuron (computed from Equation 2.1), T the target expected\n15\nChapter 2. Related Work\noutput1, and y the predicted output. Since the error is being backpropagated, j refer to neurons connected to i in the next higher layer. The gradient thus represent the change to all the weights with regard to the change in the global output error. The weights can finally be updated such that:\nWij = Wij \u2212 \u03b7 eiyj (2.14)\nWith \u03b7 the learning rate, a constant value usually chosen in the range (0.0, 1.0) used to tune the training algorithm by determining how much the weights are updated at each training iteration. A high learning rate can quicken the training process by doing big training steps but can prevent the global minima from being reached if too big. A low learning rate allows precise steps towards the solution but can lead to convergence in local minima if too small.\nMany methods have been developed to further improve and optimize ANN training (e.g. Adaptive Learning Rate, Bias, Weight Decay). Moreover, different variants to the Backpropagation algorithm have been implemented to increase the performance of the algorithm or adapt the technique to different ANN architectures. A significant alternative is Backpropagation Through Time [82] used to train RNNs."}, {"heading": "2.3 Literature Review", "text": "The goal of this section is to investigate and understand the methods and techniques used in previous studies from relevant similar research fields. The\n1For regression tasks, the expected output vector usually consist of one or more continuous values. For classification tasks, the targeted output vector usually consists of binary values. For example for three classes, class a: \u30081, 0, 0\u3009, class b: \u30080, 1, 0\u3009, and class c: \u30080, 0, 1\u3009.\n16\nChapter 2. Related Work\nobjective being to compare alternative approaches, analyze their respective advantages and disadvantages, and inform and justify decisions made during the whole research process."}, {"heading": "2.3.1 Motion-Based Keystroke Inference Attack", "text": "Side-channel keystroke inference have been extensively explored in various studies. Traditionally by exploiting characteristics of physical keyboards such as electromagnetic waves [80], sound [2, 85, 11], and timing [75, 25]. However, such side-channels are ineffective to exploit virtual keyboard, albeit sound have been successfully exploited on smartphones [71].\nStudies have shown the great potential of recovering sound, music, voice conversations, and even typing by simply observing slight vibrations in the environment produced by physical events [24, 6]. Such investigations have shown that motion invisible to the human eye can convey a surprisingly significant amount of information, establishing motion as a pertinent source of valuable data and thus a reliable side-channel. Although the camera can be used to detect motion, we are here interested in motion sensors because they are currently available on the majority of WAD, which is not the case for cameras.\nUsing motion as a side-channel imply that only movement dynamics will be used to attempt to infer information about the system. In our case, inferring keystrokes entered on a physical or virtual keyboard by a user. This attack works based on the observation that device motion during a keystroke is correlated to and consistent with keys typed by the user. In the case of such attack on a smartphone device, it is reasonable to assume that the motion of the device, while the user is typing, is affected by multiple factors. For example, the device dimension, the screen orientation, the sensor chips\n17\nChapter 2. Related Work\nspecifications, the keyboard layout, the user habits, and the relative user position and motion. This assumption leads many security researchers to question the practicality of such an attack. However, studies [17, 3] have shown that motion-based keystroke inference attack remains effective and practical despite the obvious assumptions that the previously enunciated factors might alter the robustness and the accuracy of the inference. Motion is thus established as a significant side-channel allowing the leakage of sensitive information."}, {"heading": "2.3.1.1 Keylogging", "text": "Marquardt et al. [61] have shown that the motion sensors output from a smartphone standing on a flat surface can be used to infer keystrokes typed on a nearby physical computer keyboard standing on the same surface. Their attack scenario is based on two observations. First, that access to the accelerometer data was not protected by any mobile operating system, thus allowing any installed application to monitor the accelerometer events. Secondly, that many users place their smartphones on the desk next to their computer keyboard when they are working. In their experiment setup, an iPhone device is collecting the accelerometer data to send them to a remote server where data processing and classification is performed. They demonstrated the ability to recover up to 80% of typed content by matching abstracted words with candidate dictionaries after classification. This research, therefore, shows the great potential of successfully inferring keystrokes from subtle motions such as small vibrations.\n18\nChapter 2. Related Work"}, {"heading": "2.3.1.2 Touchlogging", "text": "Related works [16, 84, 66, 63] have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen. Cai et al. [16] demonstrated that a malicious Android application can infer as much as 70% of the keystrokes entered on a number-only virtual keyboard on an Android device. For their attack to work, the user, however, need to grant the application access to the motion sensors at install time. Cai et al. believed this assumption is not unrealistic considering that most users will not treat motion data as sensitive as camera or microphone for example.\nIn their paper, Owusu et al. [66] proposed a system that reads accelerometer data to extract 6-character passwords on an Android device. Their experiment consists of a QWERTY virtual keyboard used to perform the keystroke reconstruction attack. Additionally, a data collection screen is used to collect ground truth from acceleration measurements matching key presses at specific screen regions. They managed to break 59 of 99 passwords using only the accelerometer data.\nXu et al. [84] introduced a keystroke inference attack by using a Trojan application running on the Android platform. First, the host application is used to train the system when the user is interacting with it. Finally, keystroke inference is performed by the Trojan when the user enters sensitive information into the device (e.g. password of screen lock, numbers entered during a phone call). They were able to infer the majority of keys entered by the users in various experiment configurations.\nMiluzzo et al. [63] have demonstrated that the motion sensors built-in smartphones and tablets could be used to infer keystrokes entered on a complete 26-letters keyboard with an accuracy reaching as much as 90%. In their\n19\nChapter 2. Related Work\nwork, they have also shown that combining both the accelerometer and the gyroscope can leverage the accuracy of the classification. Their approach combines the results of multiple shallow classifiers to improve the prediction quality."}, {"heading": "2.3.2 Classification of Motion Sensors Signal", "text": "Inferring keystrokes require the attacker to be able to associate measured raw sensor data with specific labels corresponding to the keys entered (i.e. the attacker needs to classify the motion signals). Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction. In both fields, the use of accelerometer sensors is historically studied more deeply although some studies explored sensors fusion to increase robustness [74, 58]. While approaches used in these fields can be borrowed, an important difference exists. In fact, the motion duration and the motion amplitude of a keystroke are respectively much shorter and lower than the motion caused by a gesture (e.g. finger swipe, hand waving) or an activity (e.g. running, sitting). Since classification is an approach to recognizing patterns in the signal, one can argue that patterns can emerge more significantly in long signals with high amplitude. Therefore making keystroke motions hard to classify."}, {"heading": "2.3.2.1 Classifier Model", "text": "According to Tanwani [77], the classification accuracy of a given algorithm is largely dependent on the nature of the dataset rather than the classification algorithm itself. In fact, the choice of a classifier model varies greatly in the related studies with no major advantage of one model over one another;\n20\nChapter 2. Related Work\nconfirming that classification quality relies mostly on the feature extraction strategy and the quality of the training set (i.e. ground truth). Successful classification is traditionally only possible after suitable pre-processing to clean the signal and judicious feature extraction to select meaningful information from the data to represent the motion event in a relevant way. That is, good feature vectors contain features distinctive between motion signals from different keystrokes and consistent among motion signals caused by the same keystroke.\nAs mentioned by Cai et al. the smaller the required training set, the easier the attack. In their implementation [17], they showed that the inference accuracy level stabilizes when the training set reaches a certain size (i.e. 12 for alphabet-only keyboard and 8 for number-only keyboard). It is also important to note that the choice of the motion sensor can affect the quality of the classification. In fact, studies have shown that the gyroscope is a better side-channel than the accelerometer for keystroke inference [1, 17, 63]."}, {"heading": "2.3.2.2 Data Analysis and Feature Extraction", "text": "Processing data from motion sensors such as accelerometers and gyroscopes bring additional challenges. In fact, hardware specification such as the motion sensor sampling rate is not fixed and is much lower than the sampling rates of acoustic and electromagnetic sensors used in related eavesdropping attacks [61, 17, 1]. In consequences, deciphering individual keystroke becomes difficult because the sensors return new values as sensor events only when these values are different from those reported in the previous event.\nCai et al. [17] proposed a pre-processing solution allowing them to use signal analysis methods. The approach, termed de-jittering, consists of normalizing the sensors sampling rate to obtain a constant interval sampling.\n21\nChapter 2. Related Work\nIn their work, they build feature vectors from time domain data such as the duration of the motion data segment, the peaks time difference, the number of spikes, the peaks interval, the attenuation rate, and the vertex angle between peaks. In another work, Cai et al. [16] experimented with the patterns produced by the motion during keystrokes. They first identified the starting and ending time of keystrokes by calculating the Peak-to-Average Power Ratio of the pitch angle and the roll angle. Then they observed that when these angles values are plotted, distinctive lobes appear on the pattern with some interesting properties. In fact, they noticed that lobes directions are similar for same keys while the angles of the lobes vary for different keys. Based on this 2D representation, they built three pairs of features consisting of geometric metrics such as the angle between the direction axis of the upper lobe and the lower lobe with the x-axis, the angles of the two dominating edges, and finally the average width of both the upper and lower lobe.\nOwusu et al. [66] solved the sampling rate problem by using an approach involving linear interpolation to create consistent sampling intervals throughout the recorded accelerometer data. They extracted the individual motion signals from each keypress using Root Mean Square anomalies for spike detection. In their work, they used a set of 46 features consisting of 44 acceleration stream information (i.e. min, max, Root Mean Square, number of local peaks, number of local crests, etc.) and two meta-features (i.e. total time and window size). A wrapper [49] was then used for feature subset selection to maximize the accuracy of the prediction.\nIn their work, Marquardt et al. [61] used a 100ms long time window as Asonov et al. [2] to extract features from the signal. They overcame the sampling rate problem by using a combination of domains to build the feature vectors. That is, time domain features (i.e. Root Mean Square, skewness, variance, kurtosis), spectral domain features (i.e. Fast Fourier Transform), and cepstrum features (i.e. Mel-frequency Cepstral Coefficients). For their\n22\nChapter 2. Related Work\nattack to work, Marquardt et al. introduced an approach creating word profiles. First, each word in a training dictionary is broken down into its constituent characters and character-pairs. Secondly, they defined keypress events as pairs of successive events and their relation to each other. That is, the horizontal location (i.e. left or right) of each keypress and the distance between consecutive keypresses (i.e. near or far). Finally, words can be represented by profiles corresponding to the successive events leading to this word being entered. The feature vectors are then used to train two different classifiers, one for classifying left or right and one for classifying near or far. These labels are then used to specify the word profiles. During the logging phase, a word matching module is in charge of determining the actual text entered on the keyboard by scoring the predicted profiles against each word in a dictionary.\nXu et al. [84] selected features from the signal in a time window bounded by the touch-down and the touch-up events triggered when the user interact with the touchscreen. They used three features to determine if the touch event occurred on the left side or on the right side of the screen (i.e. roll angle variations) and three features to determine if the touch event occurred on the top or the bottom of the screen (i.e. pitch angle variations)."}, {"heading": "2.3.3 Additional Mentions", "text": "While Deep-Spying was implemented, related studies worth mentioning were published. Wang et al. [81] developed a system to perform keylogging on a laptop keyboard using the motion sensors of a smartwatch. Their approach requires statistical features extraction to train a classifier, and non-trivial techniques such as point cloud fitting and Bayesian Inference.\nUsing the linear accelerometer built-in a smartwatch, Maiti et al. [59]\n23\nChapter 2. Related Work\ncreated a system to infer keystrokes entered by a user on a number-only keyboard displayed on a smartphone touchscreen. They managed to reach an accuracy of 90% thanks to a solution relying on shallow classifiers trained using supervised learning with engineered feature vectors containing 54 values.\nIn their paper, Liu et al. [56] focused on performing keylogging with a smartwatch on both a number-only physical keypad and a standard QWERTY computer keyboard. They used the outputs of an accelerometer to train classification algorithms and acoustic emanations recorded from the microphone built-in the smartwatch to help identify keystrokes and segment the signal appropriately. The described approach depends on time domain features (i.e. displacement vectors) and spectral domain features (i.e. Fast Fourier Transform).\n24\n3 Attack Description\nThis chapter will provide an overview of three envisioned attack scenarios where an attacker (Eve) uses a smartphone wirelessly paired with a WAD worn by the victim (Alice) to perform a motion-based keystroke inference attack. All scenarios are similar regarding attacker goal, threat model, and methods employed. They only differ on the type of keypad being eavesdropped."}, {"heading": "3.1 Attacker Goals", "text": "The attacker goal is to eavesdrop on the keys entered by the victim on a virtual or a physical keyboard. Eve could monitor Alice\u2019s keystrokes for various reasons.1 Generally speaking, Eve might want to:\n\u2022 Steal passwords and other credentials.\n\u2022 Steal sensitive information such as PINs, social security numbers, and credit card numbers.\n\u2022 Direct spying by eavesdropping on messages typed. 1Gaining access to Alice\u2019s private information could be used to impersonate Alice, to\nsteal money or to get access to password-protected content for example.\n25\nChapter 3. Attack Description"}, {"heading": "3.2 Threat Model", "text": "Pairing a WAD with a smartphone is a risky task and can be a vector of choice for an attacker to gain access to the WAD. In fact, device pairing has been extensively studied in the field of Computer Security [76, 62, 4, 18, 29] and has proven to be challenging. The establishment of a key between two devices in the presence of an active adversary remains a hard problem and existing solutions usually require trade-offs. With many users unaware of the risks, it is not unrealistic to consider the possibility of a WAD being paired with an attacker\u2019s smartphone. Bluetooth technology is currently the preferred medium used to pair smartwatches and fitness trackers with smartphones [41, 39, 44]. This communication channel being wireless further increases the risk of device pairing.2\nOnce the WAD is paired with the attacker\u2019s device, applications can be installed wirelessly on the WAD. Because the security risks of motion sensors are not well understood and often underestimated, current smartwatch operating systems (i.e. Android Wear 5.1 Lollipop, Apple Watch watchOS 2) does not require any user permissions for an application to use the motion sensors. Additionally, applications can run as Services in the background without displaying any GUI. Alice would, therefore, be unaware that an unknown application installed on her WAD by Eve is monitoring her motions.\n2Wireless Networks offer by definition more opportunities to eavesdroppers than traditional wired networks because of their very nature of wireless transmission. The data is transmitted using radio waves through air, allowing anyone with a suitable receiver to collect and decode signals exchanged between two parties. The range of Bluetooth technology is application specific: Core Specification [13] mandates a minimum range of 10 meters while the signal can still be transmitted up to 100 meters. External antennas can also potentially be used by an attacker to receive the signal from a further away location.\n26\nChapter 3. Attack Description"}, {"heading": "3.3 Attack Scenarios", "text": "It is important to note that it is assumed that the victim is wearing the WAD on the wrist of the preferred hand used to interact with keyboards. In fact, in our attack scenario it would be harder, if not impossible, for the attacker to infer keystrokes if the victim is right-handed but is wearing the WAD on the left hand for example. The use of an exploit or the deployment of malware to compromise the WAD is not in the scope of this research work. Instead, it is assumed that the attacker has already compromised the WAD and can eavesdrop on the sensors output. The following attack scenarios will be studied to assess their feasibility.\nTouchlogging Attack on Smartphone Touchscreen: Alice is typing her PIN code on the touchscreen virtual keyboard of her smartphone and Eve is trying to infer the keystrokes from the motion sensors signal of the compromised WAD as illustrated in Figure 3.1.\nKeylogging Attack on ATM-like Keypad: Alice is entering her credit card password on an ATM-style physical keypad and Eve is trying to infer the keystrokes from the motion sensors signal of the compromised WAD as shown in Figure 3.2.\n27\nFrom Touchlogging Training to Keylogging Attack: This attack scenario is based on two assumptions. First, that it would be difficult, for example, for an attacker to gain access to an ATM to train his algorithm with labeled data. However, getting access to, or installing malware on the victim\u2019s smartphone is more realistic. The second assumption is that the motion of the user\u2019s hand while typing on a touchscreen is extrapolated to be similar to the motion when she is typing on a physical keypad. Conceding that the typing surfaces are oriented with equivalent angles and that Alice is typing using the same technique on both user interfaces. That is, if the thumb is preferred to enter keystrokes on touchscreens, the thumb should also be used to enter keys on physical keypads. This for the resulting motions to potentially display similar patterns. This attack scenario will thus explore the practicality of a machine learning algorithm trained for touchlogging to be used as such to perform keylogging attacks targeting unfamiliar devices.\n28\n4 System\nThis chapter\u2019s purpose is twofold. First, to introduce the reader to the system architecture, its different components, and their relationships. Second, to describe each component respective purpose and the methods employed for their implementation."}, {"heading": "4.1 System Architecture", "text": "The system should take WAD sensor data as input and infer keystrokes as output. The main architectural model adopted is Client-Server because of its flexibility. In fact, this distributed system paradigm allows client machines with limited computational resources (e.g. mobile devices, wearable computers) to delegate heavy computations to more powerful machines such as a networked server. A server host provides services to the different clients connected to it while the clients initiate communication sessions with the server that await incoming requests.\nAs shown in Figure 4.1, the system consists of two clients connected to a processing server on the same local network. A smartphone client is acting as a proxy by receiving motion sensor events from the WAD and by relaying\n29\nthe data to the processing server. In the training phase, a second client is in charge of sending the labels to the server. One can see here an important advantage of the Client-Server model. This architecture indeed allows flexibility to experiment with different types of training devices without having to change the rest of the system. That is, the very same services provided by the networked server are directly available to any client connected to the same local network."}, {"heading": "4.2 Client", "text": ""}, {"heading": "4.2.1 Wearable Application", "text": "A wearable application is implemented to read the sensor values from the WAD and was tested on a Sony SmartWatch 3. The developed software is\n30\nChapter 4. System\nwritten in Java and designed to be deployed on devices running Android Wear API level 21. The application primary goal is to listen to accelerometer and gyroscope sensor events and to send them for further processing. Because such WAD has limited networking capabilities, the smartwatch is not able to send motion sensors data directly to the server. Instead, the smartwatch is paired using Bluetooth with a compatible Android device to establish a communication channel allowing the use of the Android Wearable Data Layer API to encapsulate the sensor data. The paired mobile phone is then relaying the data to the server."}, {"heading": "4.2.2 Mobile Application", "text": "This mobile application is needed for the relay device and was tested on an LG Nexus 4 smartphone and implemented in Java to target devices running Android API level 19. As shown in Figure 4.2, a recording session begins when a user requests the mobile client to start recording. The smartphone then sends one message to the server to initiate a new session and one message to the WAD to start listening to motion sensor events. When the user is typing on the training client, labels with timestamps1 are sent to the server. In the meantime, the user\u2019s hand motions are recorded by the WAD and reported to the relay device as an Android Wearable Data Layer message containing a timestamp value, a three-dimensional array (i.e. one dimension per axis), and the type of sensor that spread the event (i.e. gyroscope or accelerometer). The relay device stores the data locally in a buffer for each sensor type. Once a buffer is full (i.e. reach a defined size limit), the data are serialized to the JSON data-interchange format [65] and sent to the server through a TCP socket connection. JSON makes it easy for humans to read the data and for machines to parse, which enable fast to implement and less\n1The timestamps are measured in ms and require the devices (i.e. the WAD and the training device) to have synchronized clocks.\n31\nChapter 4. System\nerror-prone cross-device communication protocols.\nA recording session stops when the user requests the mobile client to stop recording. The relay device then sends a message to the WAD to stop listening to motion sensors and wait until it receives the last message that was waiting to be transmitted.2 Once the WAD receives the last message, the smartphone flushes the buffer by serializing and transmitting all of its remaining data. Finally, the relay smartphone sends a message to the server to close the session.\n2As a result of Bluetooth limited throughput and the important amount of motion events to be sent, the relay device sometimes need to wait few minutes for all the packets to be received successfully from the WAD.\n32\nChapter 4. System"}, {"heading": "4.2.3 Training Application", "text": "Two different training softwares are implemented to experiment with both touchlogging and keylogging. Despite the fact that the technologies used are different for the two training platforms, both applications are communicating with the server using the same communication protocol. During training, labels corresponding to entered keys and their respective timestamps are serialized to JSON and sent to the server using the HTTP protocol [22].\nTouchlogging: To enable any touchscreen device (e.g. smartphone, tablet) to be used as a training application, the touchlogging training application is implemented using web technologies HTML and JavaScript. The UI can thus be scaled and adapted to any screen size as shown in Figure 4.3 (a) where the interface is displayed on an iPhone 4S screen. The virtual keyboard is displayed on a surface of size 65mm\u00d7 50mm.\nKeylogging: The keylogging training application is implemented in C++ for the Open-Source Arduino microcontroller [57]. This training device consists of an Arduino UNO board with an Ethernet shield for networking capabilities and a physical keypad for label input. Because of hardware limitations, the application need to request the current timestamp in ms to the server at the beginning of the training session.3 The built physical prototype is depicted in Figure 4.3 (b). The keypad effective surface is 55mm\u00d7 45mm in size.\n3The microcontroller stores long variables on 32bits and lack a Unix epoch clock. The rest of the system is using Unix timestamps in ms consisting of 13 digits hence require at least 41bits to be stored assuming that the first digit will not change until 2033 from now. Therefore making the hardware unable to handle such numbers. To solve this problem, it is first assumed that recording sessions are under one hour in duration. Then the reference timestamp received from the server is split: the first 5 digits are stored in a char array, and the remaining 8 digits are stored in a long which require now only 27 bits. When a key is entered by the user, the long value is incremented by the number of ms that have passed since the time at which the training session started. This value is concatenated with the first digits stored in the char array and ready to be sent to the server.\n33\nChapter 4. System"}, {"heading": "4.3 Server", "text": "The server is needed to fulfil two main tasks. First to receive data from the different clients, organize them and save them persistently. Secondly and most importantly to perform data analytics by using the previously saved data; namely keystroke inference from motion sensor measurements.\nThe TCP socket and HTTP modules used to manage the data acquisition process are implemented in Java, and the data analytics process is implemented in Python to benefit from flexible data structures and scientific computation tools.4 When the server receives the end-of-session message from the relay device, the data are first sorted by timestamps because they are not guaranteed to be ordered when received. The server then saved them\n4The ANNs are implemented using modules available in the PyBrain Open-Source library [7] with a C++ wrapper additionally used to speed up the computations. Some experiments were also performed using Torch [21] and the programming language Lua.\n34\nin one CSV file per sensor. Figure 4.4 presents the main components of the data analytics pipeline and their connections. The raw data can initially be pre-processed to mitigate the effect of noise and measurement inaccuracies. Features are then extracted from the data in time windows corresponding to the keystrokes duration. During the training phase, the classification model is trained with the extracted features by iteratively evaluating the prediction outputs until a satisfying accuracy is reached or a maximum number of iterations have occurred. At the end of this phase, the trained model is serialized in XML and saved persistently. During the logging phase, the classification model is deserialized from the file system, and its inputs are activated with newly recorded data to attempt to predict the labels.\n35\n5 Data Analytics\nThe first objective of this chapter is to detail how the data are recorded and what methodologies have been used to clean the signal. As shown in Figure 5.2, the raw signal is subject to noise and, therefore, can be pre-processed before to be used for data analysis purpose. On one hand, the important amount of noise can potentially obfuscate patterns and largely alter the classification accuracy. On the other hand, a deep neural network architecture would, in theory, be able to handle such noisy data. Pre-processing can thus be applied optionally depending on the experiments.\nTo remove noise and smooth the signal, four pre-processing operations are applied to both sensors data as illustrated in Figure 5.1. The reader is invited to refer to Appendix B for visual examples depicting the improvement\n36\nChapter 5. Data Analytics\nof the signal after each pre-processing operation on a 12-keystrokes signal. Secondly, this chapter details the signal segmentation approaches and the feature extraction strategy employed to reduce the data to a meaningful selection. Finally, the statistical models chosen to perform classification are described.\n37\nChapter 5. Data Analytics"}, {"heading": "5.1 Data Acquisition", "text": "Sensor Recording: The data are acquired from the gyroscope and accelerometer sensors built-in the smartwatch. Sensor events data are stored in tuples (ti, xi, yi, zi), i = 1...n, where ti is the time in ms at which the event occurred, xi, yi, zi are the values along the three axes x, y, and z, respectively, and n is the total number of motion sensor events in an entire recording session. We observed that while the sampling rate was not constant, the delay between sensor events varies slightly enough for us to initially ignore the sampling rate problem during pre-processing.\nLabel Recording: The training device reports labels in the form of tuples (tj, lj), j = 1...m, where tj is the time in ms at which the keystroke appended, lj is the label (i.e. the value of the entered key), and m is the total number of keystrokes in the entire recording session."}, {"heading": "5.2 Pre-processing", "text": ""}, {"heading": "5.2.1 Calibration", "text": "Both motion sensors need to be calibrated to align all three axes. In fact, the accelerometer axes contain values in different absolute ranges because of the effect of gravity (as illustrated in Figure 5.2 (b)). Although the gyroscope axes should average to zero, a small non-zero difference was observed. Calibration is performed by subtracting each sensor value with the mean of its axes, such that:\n38\nChapter 5. Data Analytics\nf(vi) v\u2208{X,Y,Z}\n= vi \u2212 v (5.1)\nWhere v is the amplitude value on the given axis. The result of this\noperation can be seen in Figure 5.3.\n39\nChapter 5. Data Analytics"}, {"heading": "5.2.2 Median Filter", "text": "The moving median filter is a first pre-processing step used to mitigate the effect of noise in the data. The moving mean filter is a possible alternative but has the disadvantage to attenuate the trends in the data. The moving median removes the noise while preserving the signal pattern and is applied with a sliding window to compute the median value in a fixed range [46], that is:\nf(vi) v\u2208{X,Y,Z} = Median(vi\u2212 1 2 (w\u22121), ..., vi, ..., vi+ 1 2 (w\u22121)) (5.2)\nWhere v is the amplitude value on the given axis and w is an odd number representing the size of the sliding window. Since the sensors sampling frequencies are different, the number of data-points at the end of a recording session is different. Hence, the sliding window size has to be different for each sensor to remove noise optimally while preserving the signal as much as possible. Experiments show that w = 9 and w = 5 provide satisfying filtering results for the gyroscope and the accelerometer, respectively. As shown in Figure 5.4, the operation helps to remove noise but the signals need to be further processed to be smoother.\n40"}, {"heading": "5.2.3 Butterworth Filter", "text": "The Android API makes it possible to define a delay at which the events are received. The documentation however clearly stipulates that the specified delay is just a hint to the system and that events can be received faster or slower than the specified rate. In our context, knowing the maximum delay between sensor events in the worth case scenario allow us to optimize the pre-processing algorithm to clean the signal appropriately. The maximum\n41\nChapter 5. Data Analytics\ndelay for the gyroscope and the accelerometer was found to be 10 000\u00b5s and 62 500\u00b5s, respectively. Elementary physics tells us that the sampling rate frequency can be computed from the sampling delay such that:\nf = 1\nd \u00b7 10\u22126 (5.3)\nWhere d is the sampling delay in \u00b5s and f is the frequency in Hz. Using the maximum delay measured in the data acquisition step, we can estimate that the sensors will report new data with a frequency of 100Hz and 16Hz for the gyroscope and the accelerometer, respectively.\nFor the gyroscope, we only want to keep signals with a frequency lower than a certain cutoff frequency and attenuates signals with frequencies higher, hence the use of a low-pass filter. At contrary for the accelerometer, we want to attenuate signals with frequencies lower than the cutoff frequency, thus the need to use a high-pass filter [14]. Using the frequencies previously calculated, the filters can be applied with Nyquist frequencies set to 8Hz and 50Hz, for the gyroscope and the accelerometer, respectively. The resulting signals can be seen in Figure 5.5.\n42"}, {"heading": "5.2.4 Kalman Filter", "text": "The Kalman Filter algorithm produces estimates that minimize Mean Squared Error by observing a series of measurements. Even with data containing statistical noise, the filter can produce estimates allowing patterns to emerge more significantly from the signal [32]. This advanced filtering technique proved to be useful to our application context by smoothing the signal evenly and attenuating irregular peaks and pits (as shown in Figure 5.6).\n43\nThe signals are finally normalized before to be returned. The output of the pre-processing pipeline is a smoother signal where the effect of noise is reduced while the signal patterns are preserved as much as possible. Figure 5.7 compares both motion sensor signals before and after being pre-processed.\n44\n45\nChapter 5. Data Analytics"}, {"heading": "5.3 Feature Extraction", "text": ""}, {"heading": "5.3.1 Sensor Fusion", "text": "As mentioned in Section 2.3.2, studies have shown that sensors fusion can increase the robustness of motion sensor outputs classification. In fact, sensors can be subject to inaccurate measurements while recording, merging their outputs with other sensors can minimize uncertainty and provide more accurate measurements. However, the accelerometer is recorded with a sampling frequency significantly lower than the gyroscope1, thus making sensor fusion hard using the recorded data as such. Moreover, data-points need to be evenly distributed to allow trends to emerge from the data. The sampling rate can be made constant by distributing the data-points evenly over the complete recording session duration. The implemented algorithm is described as follows:\n1. First, create a new set T \u2032 of k elements with:\nk = 1\n\u03b1 (tn \u2212 t 0) + 1 (5.4)\nWhere t is the timestamp at which a motion sensor event have been recorded, n is the total number of events in an entire recording session, and \u03b1 is a constant integer referring to the target sampling rate that was defined to be 2ms. Then populate T \u2032 such that:\nt\u2032i t\u2032\u2208 T \u2032 = t 0 + \u03b1 i (5.5)\n1By a factor of 6.25 according to the sensors maximum delay measured in the data acquisition step. This factor was confirmed experimentally by dividing the total number of data-point recorded for the gyroscope with the total number of data-point recorded for the accelerometer during a recording session.\n46\nChapter 5. Data Analytics\nWhere i is the index position of the time value in T \u2032.\n2. Secondly, perform a union operation T \u2032\u2032 = T \u2032 \u222a T where T \u2032 is the previously generated target set with constant time intervals, and T is a\nset generated from the measurements with variable time intervals. The operation ensures that no timestamp duplicates exist and return a new set T \u2032\u2032 of size k\u2032.\n3. Third, create a new list of tuples (t\u2032i, x \u2032 i, y \u2032 i, z \u2032 i), i = 1...k \u2032 where t\u2032 is\nset to the values in T \u2032\u2032, while x\u2032i, y \u2032 i, and z \u2032 i are respectively set to xi, yi, and zi when the values are known from the recorded measurements (i.e. when t\u2032i \u2208 T ).\n4. Fourth, linear interpolation is used to compute the unknown values in\nthe tuples list, that is:\nf(vi) v\u2208{X\u2032,Y \u2032,Z\u2032}\n= vi\u22121 + 1\nj \u2212 (i\u2212 1) (vj \u2212 vi\u22121) (5.6)\nWhere vi is an unknown value on the given axis at the position index i, and j is the position index of the next known value in the time series. Note that the algorithm is computing missing values from left to right so vi\u22121 is always known when vi is computed.\n5. Finally, since the known values have now been used to compute the\nmissing data-points, the last step consists of keeping only the tuples where t\u2032i \u2208 T \u2032, the previously generated target set with constant time intervals.\nLinear interpolation allowed both sensor tuple sets to contain data-points evenly distributed over the recording session duration. However, it is not yet possible to fuse the sensors because the accelerometer timestamps values are different from the gyroscope\u2019s since the sensors have been reporting new\n47\nChapter 5. Data Analytics\nevents asynchronously independent of each others. The sensors have thus recorded data in slightly shifted time windows. The accelerometer measurements are therefore processed using the same previously described approach to normalize the sampling rate. That is, linear interpolation (Equation 5.6) is used to compute unknown accelerometer values at the same timestamps as the gyroscope\u2019s. Figure 5.8 shows the two sensors mean signals after linear interpolation of the accelerometer to fit the gyroscope time values.\nThe sensor fusion algorithm returns a vector for each time frame ti and allow multiple combinations of axes to perform different classification experiments. For example, \u3008gxi, gyi, gzi, ai\u3009 is a possible four-dimensions vector returned by the fusion algorithm. With x, y, and z values along the different axes of the accelerometer a and the gyroscope g. Mean values gi and ai are simply the average of the three axes values (i.e. gi = 1 3 (gxi + gyi + gzi)).\n48\nChapter 5. Data Analytics"}, {"heading": "5.3.2 Segmentation", "text": "The server is receiving a continuous stream of motion sensor events. Therefore, it is important to segment the data stream into sections corresponding to each keystroke before to perform classification."}, {"heading": "5.3.2.1 Segmentation from Labels", "text": "As mentioned in Section 5.1, tuples containing labels and timestamps in ms are sent to the server during training. These time values are used as reference points to segment the signal into m pieces, where m is the total number of keystrokes in the entire recording session (Figure 5.9 shows an example of a signal after segmentation). We defined a fixed-size sampling window to select subsets of data-points occurring during keystrokes as follows:\n[ti \u2212 \u03b1, ti + \u03b1[= {vi \u2208 {X, Y,X} vi\u2212\u03b1 \u2264 vi < vi+\u03b1} (5.7)\nWhere \u03b1 is half the size of the sampling window. Considering that Asonov et al. [2] determined the duration of a key-press to be approximately 100ms and knowing that our target sampling rate was defined to be 2ms (see Section 5.3.1), we defined a sampling window of 50 data-points. Thus \u03b1 = 25 in our implementation.\n49\n50\nChapter 5. Data Analytics"}, {"heading": "5.3.2.2 Heuristic Segmentation", "text": "As explained in Chapter 3, it is assumed that the attacker has not compromised the target device on which the user is entering keys. Therefore, both the training phase and the logging phase cannot realistically rely on keystroke timestamps to segment the signal. Observations have showed that keystrokes tend to cause high peaks in the signal. Therefore, the Peak-to-Average Power Ratio can be used to detect such peaks as follows:\n1. First of all, we observed that the gyroscope\u2019s signal peaks were better\naligned with the actual keystroke timestamps than the accelerometer\u2019s. Considering that the sensors data are three-dimensional, the signals on the three axes are merged by simply calculating the gyroscope\u2019s mean signal g such that:\ngi = 1\n3 (gxi + gyi + gzi) (5.8)\n2. Secondly, the mean signal can now be used to compute the Peak-to-\nAverage Power Ratio defined as the square Crest Factor as follows:\nf(vi) = ( vi r(g) )2 (5.9)\nWhere vi is the amplitude of the mean signal g at the index position i and r(g) return the Root Mean Square of the signal such that:\nr(g) = \u221a\u221a\u221a\u221a 1 n n\u2211 i=1 gi 2 (5.10)\nWhere n is the total number of data-points in the gyroscope\u2019s average signal g.\n51\nChapter 5. Data Analytics\n3. Finally, the peaks can be detected by applying the following First-Order\nLogic rule:\npeak(ri)\u2192 (ri > ri\u22121) \u2227 (ri > ri+1) \u2227 (ri > \u03b1) (5.11)\nWhere ri is the Peak-to-Average Power Ratio at the index position i, and \u03b1 is a constant value \u03b1 = 0.4 used to discard peaks too small to be the result of a keystroke. If the rule is evaluated to true, then there is a peak in the signal at position i.\nAs shown in Figure 5.10, this heuristic allows the detection of potential keystroke positions. Once the peak positions are known, it is possible to segment the signal by following the same approach described in Section 5.3.2.1.\n52\nChapter 5. Data Analytics"}, {"heading": "5.3.3 Statistical Features", "text": "Statistical models such as RNNs are designed to process sequential data by performing Unsupervised Feature Learning. However, traditional FNNs usually need engineered features to model the data efficiently. The following statistical vector is therefore calculated:\n\u3008min,max, RMS, \u03c1, \u03bb, \u03c3, \u03ba, \u03b3\u3009 (5.12)\nWith RMS the Root Mean Square, \u03c1 the number of peaks in the signal detected using the approach described in Section 5.3.2.2, \u03bb the Crest Factor computed from Equation 5.9, \u03c3 the skewness, \u03ba the kurtosis, and \u03b3 the variance. This statistical vector is computed for all three axes for both sensors. Thus returning a statistical feature vector of length 48."}, {"heading": "5.4 Classifier Model", "text": "The classifier takes accelerometer and gyroscope data as input and output classes corresponding to keystrokes. A typical input vector consists of values normalized in the range [\u22121, 1] and the output is a binary vector of the same length as the number of labels. That is, each label is assigned a binary representation stored in a look-up table. Different multi-class classification models were implemented to compare their respective efficiency to process the data. Each of the models is trained with supervised learning thanks to a dataset containing labeled data-points. An online approach is used to update the weights each time a training example is shown to the network. The weights are updated using an improved variant of the Backpropagation iterative gradient descent termed Rprop- [69, 37]. For result reproducibility,\n53\nChapter 5. Data Analytics\nthe weights are initialized with a pseudo-random number generator seeded with a hard-coded constant integer. All models rely on the same loss function to compute the network error. Namely, the Mean Squared Error defined as follows:\nE = 1\nn n\u2211 i=1 (Ti \u2212 yi)2 (5.13)\nWith n the number of neurons in the output layer, T the target expected\noutput, and y the predicted output."}, {"heading": "5.4.1 Performance Evaluation", "text": "Measuring how well a classifier performs allows the selection of an optimal solution for the problem at hand. Some metrics thus need to be defined to assess the performance and the effectiveness of a classification model.\nPrecision, Recall, and F1 Score: The precision P is important to measure the overall quality of the classification results while the recall R determines the classifier capacity to analyse efficiently the majority of the data it is exposed to, such that:\nP = TP\nTP + FP ,R =\nTP\nTP + FN (5.14)\nWhere TP is the number of true positives (i.e. the number of correctly classified items), FP is the number of false positives (i.e. the number of incorrectly classified items, with TP + FP equal to the collection size), and FN the number of false negatives (i.e. the number of items incorrectly misclassified). A more accurate performance metrics termed F-score combines\n54\nChapter 5. Data Analytics\nboth precision and recall to provide an overall performance score such that:\nF = (1 + \u03b2)2 P \u00b7R\n\u03b22 \u00b7 P +R (5.15)\nWith \u03b22 = 1 to compute the harmonic mean of precision and recall (i.e.\nF1 Score) [60].\nReliability: The F1 Score is an efficient method to measure the quality of the classification results but does not provide any information about how reliable the results are. Hu\u0308sken and Stagge [35] proposed a method to assess the reliability of a classification algorithm based on the value distribution of the output neurons. That is, if the predicted values of all the output neurons are numerically close, the classifier is not totally convinced by its classification decision. On the contrary, if the value of one output neuron is high while the rest of the neurons produce lower values, the classifier is very confident with its classification decision. The reliability R of a classification result can be computed from the Entropy S such that:\nR = 1\u2212 1 log n S (5.16)\nS = \u2212 n\u2211 i=1 yi log yi (5.17)\nWhere n is the total number of output neurons (i.e. classes), and yi is the output value of the output neuron i. If all the output neurons generate similar values, R will tend to 0 and by opposition tends to 1 if one of the output neuron generates a value numerically far from the others.\nK-Fold Cross-Validation: Testing a classifier traditionally involve splitting the dataset into two parts: one used for training and one used for evalua-\n55\nChapter 5. Data Analytics\ntion. However, the main downside of this method, termed holdout, is that the data used for evaluation are never used to train the classifier and vice versa. Thus leading to a less general performance assessment. Different techniques have been developed to address this issue. A popular solution termed K-Fold Cross-Validation is used in this project to assess the quality of the different classification models. This method first consists of shuffling the dataset and splitting it into k partitions (i.e. folds) approximately equal in size. The classifier is then trained with k\u2212 1 datasets and evaluated on the remaining partition. This process is repeated k times by selecting a different training set and evaluation set such that every partition is used at most one time for evaluation and k \u2212 1 times for training. The evaluation results are finally averaged to represent the global performance of the classifier. All data are thus used for both training and evaluation to provide a more general and accurate performance assessment [48]."}, {"heading": "5.4.2 Sensor Fusion Benchmark", "text": "Experiments were performed to select the sensor fusion strategy holding the best classification results. Observations showed that signals resulting from motions occurring while typing on keys in opposite corners (i.e. 1, 3, \u2217, #) were differentiable enough to be recognized with the naked eye by simply\n56\nChapter 5. Data Analytics\nlooking at the resulting signal patterns (example in Figure 5.9). Thus, motions from keystrokes on these specific keys were recorded to construct a toy dataset based on the assumption that a good classifier model should be able to do as well as the human eye on these simple patterns. The toy dataset contains a total of 120 keystrokes targeting 4 labels, resulting in 30 instances per key.\nTo assess the quality of the different fusion strategies, the vectors were used to train an RNN with one hidden LSTM layer of 9 units (as illustrated in Figure 5.11) for 100 Epochs using the toy dataset. That is, 100 passes through the entire training dataset. The number of units in the linear input layer depends on the length of the feature vector returned by the fusion algorithm. The output layer is a standard linear layer and the evaluation is performed with the same dataset used for training. Although this would be a bad approach for evaluating the performance of the classifier itself, the goal here is only to assess the quality of the features returned from the\n57\nChapter 5. Data Analytics\nfusion algorithm. In fact, a good fusion strategy allows the generation of feature vectors that the classifier should be able to memorize and remember. Thus, a fusion strategy is satisfying if the classifier can learn to generate the appropriate output if the same input vector is seen again. As shown in Table 5.1, the best sensor fusion strategy is a six-dimensions vector consisting of the three axes of both the gyroscope and the accelerometer."}, {"heading": "5.4.3 Model Benchmark", "text": "Now that a fusion strategy has been chosen to leverage the quality of the predictions, it is possible to compare different types of models. Each ANN is built following the architecture template illustrated in Figure 5.12 and trained for 100 Epochs on the same toy dataset used in Section 5.4.2. A linear layer is first used to forward the input vector to the internal structure of the network. Each layer is then fully connected to the next layer in the network. Since classification is performed, a Softmax layer is finally used as output (see Table 2.2 for mathematical definition of Softmax). The compared models only differ from the type of hidden layer employed and the type of features they are trained on. The hidden layers each contains 128 hidden units. The following results are measured using K-Fold Cross-Validation with k = 5. The results are therefore averaged mean and averaged standard deviation. Confusion matrices generated during this benchmark can be seen in Appendix C.\n58\nChapter 5. Data Analytics\nMultilayer FNN: Standard FNN is one of the most simple ANN architecture and is interesting for comparing their performance with more advanced ANNs such as networks with recurrent architecture. Layers with different activation functions are compared to select an appropriate FNN architecture for the problem at hand. Standard Rprop- is used for training. As presented in Table 5.2 and Figure 5.13, FNN with Sigmoid hidden layers can process statistical features significantly more efficiently than Tanh layers. However, when the network is trained with data segments directly, Tanh layers are able to make more reliable predictions with a smaller standard deviation.\nMultilayer LSTM: Since LSTMs are designed to process sequential data (as explained in Section 2.2), they are especially suitable to process motion sensors from WADs. The training algorithm is however slightly different from the one used to train multilayer FNNs. To allow time-related patterns to emerge, Backpropagation Through Time is used to pass along the LSTM internal state to the next recurrent LSTM instance (i.e. the previous cell state ct\u22121, the previous output vector yt\u22121, and the new datapoint xt). The LSTM initial cell state is a vector consisting of null values that is passed\n59\nChapter 5. Data Analytics\ntogether with the first data-point of the sequence. During evaluation, the predictions from an FNN are returned as generated by the neural network. However, because LSTM units first need to be activated to initialize their memory cell internal structure, the outputs generated for the whole sequence are contributing to the final prediction result. The output vectors are simply added together and normalized before to be returned. Two different LSTM implementations were studied: standard recurrent LSTM consisting of a forget gate, and LSTM with peephole connections. Table 5.2 and Figure 5.13 show that while LSTM with peepholes is able to make remarkably good predictions in some cases, the standard deviation remains higher than when a standard recurrent LSTM layer is used. Thus making the peephole implementation less robust in this specific application context.\n60\n6 Evaluation\nThis chapter first goal is to describe the different experiments setup to collect empirical data from various people. Secondly, to describe the different results returned by the system. Finally, to interpret the results and their relations with the research questions enunciated in Section 1.1."}, {"heading": "6.1 Empirical Data Collection", "text": "Seven persons external to this research and aged between 23 and 30 have participated in the following experiments. Each person was asked to enter multiple series of keystrokes on a touchscreen and a keypad while wearing a WAD on their wrist. To prevent the influence of external motions, the participants were required to sit in a comfortable position allowing them to stay still for the entire duration of the recording session. Each dataset contains 240 keystrokes with 20 instances of each of the 12 labels (i.e. 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, \u2217, #). If a classifier makes its predictions at random, the probability of correctly classifying a keystroke K is P (K) = 1\n12 .\n61\nChapter 6. Evaluation"}, {"heading": "6.2 Experiments", "text": "Classification Model: As detailed in Section 5.4, different ANN architectures were compared to select appropriate models for experimenting with real data. The number of output units is increased to reflect the number of labels in the collected measurements. The terms FNN-Sigmoid and FNN-Tanh are used to refer to the feedforward implementations depicted in Figure 6.1 (a) and (b), respectively. FNN-Sigmoid is preferred to process statistical features while FNN-Tanh is the feedforward architecture chosen to process data segments as features. The chosen recurrent implementation to process data sequences is shown in Figure 6.1 (c) and referred to as RNN-LSTM.\n62\nChapter 6. Evaluation\nData Preparation Schemes: The experiments require many different data preparation schemes to compare the performance of the classifiers in various contexts. Notably, we want to observe how well deep models can process both pre-processed data and noisy raw data. We consider the data raw when non-trivial pre-processing operations are completely ignored (only calibration as described in Section 5.2.1 is allowed, which is used mostly to ignore the effect of gravity on the accelerometer measurements). The feature extraction steps detailed in Section 5.3 are subsequently applied indifferently to the data being pre-processed or raw (including normalizing the sampling rate during sensor fusion as described in Section 5.3.1). The following data preparation schemes are used to train and evaluate the three classifiers:\n\u2022 P-T : Pre-processed data with timestamp-based segmentation.\n\u2022 P-H : Pre-processed data with heuristic segmentation.\n\u2022 R-T : Raw data with timestamp-based segmentation.\n\u2022 R-H : Raw data with heuristic segmentation.\nThe results are measured using K-Fold Cross-Validation with k = 5 by training every classifier for 100 Epochs. The results are averaged over all folds and the seven participants. Each one of the three classifiers is thus trained for five folds on four different data schemes. Leading to a total of 60 training sessions for each participant in every respective experiment. These computationally intensive tasks were performed on a dedicated machine and took several days to complete. The reader is invited to refer to Chapter 3 to read about the motivations behind the following experiments. Appendix D provides confusion matrices and loss graphs for further details.\n63\nChapter 6. Evaluation"}, {"heading": "6.2.1 Experiment 1: Touchlogging Attack", "text": "The goal of this experiment is to assess the likelihood of a touchlogging attack on a smartphone touchscreen using motion sensor outputs from a WAD. The participants were invited to enter keystrokes on our training application running on an iPhone 4S (interface shown in Figure 4.3 (a)). Figure 6.2 provides a graphical representation comparing the performances of the three classifiers. The results are detailed in the subsequent Tables.\nThe results presented in Table 6.4 shows that FNN-Sigmoid is better at analysing pre-processed data than raw data. Despite performing worse on unprocessed data, it is a surprise that the model can still classify such features with reliable predictions and low standard deviation. The lower F1 Score on raw data can be explained by the fact that the model relies on statistical features. As a consequence, raw data displays unstable statistical properties, thus demonstrating the benefit of pre-processing the measurements beforehand to train such classifier.\n64\nUnexpectedly, the FNN-Tanh implementation is able to learn from data segment in a relatively acceptable way. Thus, allowing the model to make predictions similar in quality to the FNN-Sigmoid model. Even though being very basic, the two-layered model seems to be able to perform Unsupervised Feature Learning to some extent. Table 6.2 additionally shows that FNNTanh can make slightly better predictions than FNN-Sigmoid on raw data, probably because the former can learn features from the data it is exposed to without relying on engineered statistical features sensible to noise. However, the evolution of the loss during training (as illustrated in Figures D.5 and D.6 in Appendix D) suggests that FNN-Tanh is harder to train on raw data when patterns are difficult to learn by the network naive internal structure.\n65\nChapter 6. Evaluation\nAs depicted in Figure D.9 and as expected, RNN-LSTM can be trained efficiently on time-series data. In fact, this architecture outperforms the other ANNs in most cases as illustrated in Figure 6.2. Although the model displays no difficulty processing raw data, it nevertheless seems to have trouble analysing heuristically-segmented raw data. Thanks to its capacity to process sequence data, RNN-LSTM is the champion of the touchlogging task."}, {"heading": "6.2.2 Experiment 2: Keylogging Attack", "text": "This experiment\u2019s purpose is to determine the feasibility of a keylogging attack by analysing WAD motions while typing on an ATM-like physical keypad. The keystrokes were entered by the participants on our training device (depicted in Figure 4.3 (b)) by using the same method to type on both the touchscreen and the keypad. That is, if the index finger were preferred to enter keystrokes on the smartphone, the index finger should also be used to enter keys on the physical keypad to use the same data for Experiment 3 (see Chapter 3 for motivation details).\n66\nDespite the fact that the FNN-Sigmoid is convinced by its predictions, the quality of its decisions is far from satisfying. In fact, Table 6.4 shows that the average reliability score is high with a small standard deviation even thought the standard deviation of the F1 Score is very important. The predictions average close to and below average.\nSimilarly to the FNN-Sigmoid model, FNN-Tanh is not performing optimally during the keylogging task as conveyed by Table 6.5. Its predictions\n67\nChapter 6. Evaluation\nfor raw data classification are indeed below average. FNN-Tanh reliability is fluctuating with a great standard deviation, showing that the model has difficulties to generate strong predictions. This ANN architecture clearly struggles to achieve Unsupervised Feature Learning on the keypad dataset.\nThe graphical comparison represented in Figure 6.3 distinctly shows the performance of the RNN-LSTM implementation surpassing the other models in a significant way. In fact, the predictions are above average with a small standard deviation. RNN-LSTM performs equally on the four different data preparation schemes and is even able to make its best predictions when exposed to the most chaotic data scheme, namely, raw data with heuristic segmentation (i.e. R-H). This ANN is undeniably the best performing ANN architecture for the keylogging task.\n68"}, {"heading": "6.2.3 Experiment 3: from Touchlogging to Keylogging", "text": "For this experiment, the classifier is trained for 200 Epochs with the complete dataset recorded during Experiment 1 when users are entering keys on a smartphone touchscreen. The logging phase is later performed using the full measurement collection recorded for Experiment 2 with users typing on an ATM-like keypad. It is worth noting that the classifier is trained and later evaluated with features generated by the same data scheme. That is, when the model is trained with pre-processed data with timestamp-based segmentation (i.e. P-T), the same data scheme is applied to the evaluation dataset. This experimentation is completed using the RNN-LSTM model because it is the model yielding the best results in both previous experiments.\nThe results presented in Table 6.7 are calculated at once when the evaluation dataset is shown to the classifier. Although the returned predictions are far from excellent, RNN-LSTM is still able to recognize patterns from unknown signals recorded when users are typing on a different keyboard than the one used for training. In this application context, it is worth noting that the classifier can perform better when it is both trained and evaluated with pre-processed data. Similarly to the results presented in Figure 6.2 and Table\n69\nChapter 6. Evaluation\n6.3, RNN-LSTM have trouble learning from raw data with timestamp-based segmentation (i.e. R-T).1"}, {"heading": "6.3 Discussions", "text": "The LSTM implementation is undeniably the best classification model used in the experiments. In fact, this architecture can achieve touchlogging and keylogging with a maximum accuracy of 73% and 59%, respectively. The LSTM model can also successfully classify signals with an accuracy of 19% when the dataset used for training and logging are originated from two different keypads.\nAcross all the different results, the heuristically-based segmentation usually leverages better classification decisions than timestamp-based segmentation. This might be caused by two factors. First, the accuracy of the timestamp sent to the server by the training devices is questionable. As detailed in Section 5.3.2.1, keystrokes are thought to happen in a 100ms time window on average and the timestamps used by the system are measured in\n1The F1 Score is 0.082 and corresponds to a random guess over 12 possible labels. Thus showing that RNN-LSTM is completely unable to make educated predictions from R-T.\n70\nChapter 6. Evaluation\nms. Even though these time values are expected to match actual keystrokes when aligned with the signal (as shown on Figure 5.9), it is likely that small time measuring inaccuracies can lead to worse classification results. Second, the heuristic segmentation works by measuring physical properties of the signal (as explained in Section 5.3.2.2). With this in mind and given the experiments results, it is reasonable to assume that these physical properties are resilient across keystrokes and consequently a robust method for signal segmentation.\nAs a reminder, both the touchlogging and the keylogging datasets contains the same number of measurements with each keystroke being equally represented. The important difference between the FNNs performances during the two first experiments is thus thought to be caused by the data themselves. In fact, the size of the keypad built for Experiment 2 is smaller than the size of the touchscreen used for Experiment 1. As a consequence, this slight difference in size produces tinier motions with less extreme value variation. Thus leading naive models to struggle finding subtle patterns in the signal, even when manually selected statistical features are used.\nUsing a recurrent model such as LSTM have proved to yield in general far better results than simple feedforward models. Thanks to its internal structure designed to process sequential data, the RNN-LSTM model behaves robustly regardless of the data collection used and can even learn to distinguish patterns in noisy measurements. An impressive characteristic of the recurrent model is indeed its ability to process raw data as well as pre-processed data as demonstrated in the experiments.\n71\n7 Conclusion\nThe purpose of this chapter is mainly to summarize and reflect on our findings. Conceivable future works are also considered to extend this project."}, {"heading": "7.1 Summary", "text": "The system developed in this work can perform touchlogging and keylogging with an accuracy of 73% and 59%, respectively. Despite the fact that these results are smaller than the ones claimed in related works, our classifier can perform equally successfully when confronted with raw unprocessed data. Thus demonstrating that deep neural networks are capable of making keystroke inference attacks based on motion sensors easier to achieve by removing the need for non-trivial pre-processing pipelines and carefully engineered feature extraction strategies. All related works rely heavily on such techniques as presented in Chapter 2.\nMoreover, the system is still able to infer keystrokes with an accuracy of 19% when trained and evaluated with datasets recorded from different keypads. This result suggests that an attacker could log keys from a wide range of devices even if its classifier is trained with measurements from a different compromised device.\n72\nChapter 7. Conclusion\nDramatically, these observations imply that a cyber-criminal would be able, in theory, to eavesdropped on any device operated by the user while wearing a WAD. Thus granting access to sensitive and highly valuable information and possibly causing important damages.\nTo minimize the risk of such attacks, users should always wear their WAD on their less preferred hand for device interaction. For example, a righthanded person should wear the WAD on its left arm. Because of the demonstrated risks, the different operations systems powering wearable technologies should require user permissions before any application is allowed to use the accelerometer and the gyroscope. Furthermore, a permission system should restrict or allow access to the motion sensors in specific contexts or for trusted applications only."}, {"heading": "7.2 Future Work", "text": "Convolutional Neural Network (CNN) is a class of powerful deep models designed to process multi-dimensional arrays such as images and video frames [53]. A possible addition to this project could involve experiments with such ANN architecture to classify motion sensors measurement sequences. It would indeed be interesting to compare the performance of CNN with LSTM in our application context.\nA significant extension of this work could be the implementation of a more dynamic system allowing automatic signal segmentation and classification in real-time. For example, LSTM layers could be trained to identify keystrokes in the measurement flow, segment the signal automatically, and pass the resulting data forward to other layers responsible for further processing and classification.\n73\nChapter 7. Conclusion\nAn analogous attack targeting WAD users could be implemented to reconstruct hand-written messages by recording and analysing the hand\u2019s motion. For example, gesture-based password (e.g. Android lock screen) could potentially be cracked using such an attack.\nIn order to assess the security threat in specific contexts, it would be interesting to perform further experiments using standard hardware such as actual ATM keypads, electronic building access system keypads, or hotel room safe keypads.\nUser\u2019s moves can create important motion interferences that can potentially obfuscate keystroke signal patterns. Experiments in such conditions could be conducted to compare the results and the likelihood of such an attack in these contexts (e.g. controlled environment when the user is sitting, uncontrolled environment when she is walking).\nBenchmarking different smartwatch and fitness tracker models could potentially show that some products allow more accurate keystroke inference than others. Thus possibly proving that users are at greater risk when using specific WAD models.\nThe motion of WAD could also potentially be used for the identification\nand tracking of users as studied in similar research [64, 38].\nSome WAD models come built-in with a wide range of sensors including Galvanic Skin Response (GSR), heart rate sensor, Electromyography (EMG), or ambient light sensor. Fusing motion sensors with one or many of these additional sensors might further improve the accuracy and the robustness of the keystroke predictions.\n74\nBibliography\n[1] Ahmed Al-Haiqi, Mahamod Ismail, and Rosdiadee Nordin. On the best\nsensor for keystrokes inference attack on android. Procedia Technology, 11:989\u2013995, 2013.\n[2] Dmitri Asonov and Rakesh Agrawal. Keyboard acoustic emanations. In\nProceedings of the IEEE Symposium on Security and Privacy, page 3. IEEE, 2004.\n[3] Adam J Aviv, Benjamin Sapp, Matt Blaze, and Jonathan M Smith.\nPracticality of accelerometer side channels on smartphones. In Proceedings of the 28th Annual Computer Security Applications Conference, pages 41\u201350. ACM, 2012.\n[4] Dirk Balfanz, Diana K Smetters, Paul Stewart, and H Chi Wong. Talk-\ning to strangers: Authentication in ad-hoc wireless networks. In NDSS, 2002.\n[5] Ling Bao and Stephen S Intille. Activity recognition from user-annotated\nacceleration data. In Pervasive computing, pages 1\u201317. Springer, 2004.\n[6] Andrea Barisani and Daniele Bianco. Sniffing keystrokes with lasers/-\nvoltmeters. Proceedings of Black Hat USA, 2009.\n[7] Justin Bayer, Tom Schaul, and Thomas Ru\u0308ckstie\u00df. Pybrain: Python-\nbased reinforcement learning, artificial intelligence and neural network library. https://github.com/pybrain/pybrain, Online, accessed 04- 11-2015.\n75\nBibliography\n[8] Tony Beltramelli. Android wear permissions bug. https://github.\ncom/tonybeltramelli/Android-Wear-Permissions-Bug, Online, accessed 29-11-2015.\n[9] Sander D Benanne, Jan Schlu\u0308ter, and Colin Raffel. Lasagne:\nLightweight library to build and train neural networks in theano. https: //github.com/Lasagne/Lasagne, Online, accessed 04-11-2015.\n[10] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term\ndependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157\u2013166, 1994.\n[11] Yigael Berger, Avishai Wool, and Arie Yeredor. Dictionary attacks using\nkeyboard acoustic emanations. In Proceedings of the 13th ACM conference on Computer and communications security, pages 245\u2013254. ACM, 2006.\n[12] Christopher M Bishop. Pattern recognition and machine learning.\nspringer, 2006.\n[13] Inc. Bluetooth SIG. Bluetooth specifications. https://www.bluetooth.\norg/en-us/specification/adopted-specifications, Online, accessed 05-08-2015.\n[14] Stephen Butterworth. On the theory of filter amplifiers. Wireless Engi-\nneer, 7(6):536\u2013541, 1930.\n[15] Levente Buttyan and Jean-Pierre Hubaux. Security and cooperation in\nwireless networks: thwarting malicious and selfish behavior in the age of ubiquitous computing. Cambridge University Press, 2007.\n[16] Liang Cai and Hao Chen. Touchlogger: Inferring keystrokes on touch\nscreen from smartphone motion. In HotSec, 2011.\n76\nBibliography\n[17] Liang Cai and Hao Chen. On the practicality of motion based keystroke\ninference attack. Springer, 2012.\n[18] Claude Castelluccia and Pars Mutaf. Shake them up!: a movement-\nbased pairing protocol for cpu-constrained devices. In Proceedings of the 3rd international conference on Mobile systems, applications, and services, pages 51\u201364. ACM, 2005.\n[19] Shuo Chen, Rui Wang, XiaoFeng Wang, and Kehuan Zhang. Side-\nchannel leaks in web applications: A reality today, a challenge tomorrow. In Security and Privacy (SP), 2010 IEEE Symposium on, pages 191\u2013206. IEEE, 2010.\n[20] Franc\u0327ois Chollet. Keras: Theano-based deep learning library. https:\n//github.com/fchollet/keras, Online, accessed 04-11-2015.\n[21] Ronan Collobert, Clement Farabet, Koray Kavukcuoglu, and Soumith\nChintala. Torch: Scientific computing for luajit. https://github.com/ torch/torch7, Online, accessed 04-11-2015.\n[22] W3C (World Wide Web Consortium). Http - hypertext transfer proto-\ncol. http://www.w3.org/Protocols/, Online, accessed 30-09-2015.\n[23] Dimitrios Damopoulos, Georgios Kambourakis, and Stefanos Gritzalis.\nFrom keyloggers to touchloggers: Take the rough with the smooth. Computers & Security, 32:102\u2013114, 2013.\n[24] Abe Davis, Michael Rubinstein, Neal Wadhwa, Gautham J Mysore,\nFredo Durand, and William T Freeman. The visual microphone: Passive recovery of sound from video. ACM Trans. Graph, 33(4):79, 2014.\n[25] Denis Foo Kune and Yongdae Kim. Timing attacks on pin input de-\nvices. In Proceedings of the 17th ACM conference on Computer and communications security, pages 678\u2013680. ACM, 2010.\n77\nBibliography\n[26] Felix Gers, Ju\u0308rgen Schmidhuber, et al. Recurrent nets that time and\ncount. In Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEEINNS-ENNS International Joint Conference on, volume 3, pages 189\u2013 194. IEEE, 2000.\n[27] Felix A Gers, Ju\u0308rgen Schmidhuber, and Fred Cummins. Learning to for-\nget: Continual prediction with lstm. Neural computation, 12(10):2451\u2013 2471, 2000.\n[28] Adam Gibson. Dl4j: Deep learning for java. https://github.com/\ndeeplearning4j/deeplearning4j, Online, accessed 04-11-2015.\n[29] Michael T Goodrich, Michael Sirivianos, John Solis, Gene Tsudik, and\nErsin Uzun. Loud and clear: Human-verifiable authentication based on audio. In Distributed Computing Systems, 2006. ICDCS 2006. 26th IEEE International Conference on, pages 10\u201310. IEEE, 2006.\n[30] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u0301\u0131k, Bas R Steune-\nbrink, and Ju\u0308rgen Schmidhuber. LSTM: A search space odyssey. arXiv preprint arXiv:1503.04069, 2015.\n[31] Jiawei Han, Micheline Kamber, and Jian Pei. Data mining: concepts\nand techniques: concepts and techniques. Elsevier, 2011.\n[32] Simon Haykin. Kalman filtering and neural networks, volume 47. John\nWiley & Sons, 2004.\n[33] Simon Haykin and Neural Network. A comprehensive foundation. Neural\nNetworks, 2(2004), 2004.\n[34] Sepp Hochreiter and Ju\u0308rgen Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735\u20131780, 1997.\n[35] Michael Hu\u0308sken and Peter Stagge. Recurrent neural networks for time\nseries classification. Neurocomputing, 50:223\u2013235, 2003.\n78\nBibliography\n[36] IDSIA. Brainstorm: Fast, flexible and fun neural networks. https:\n//github.com/IDSIA/brainstorm, Online, accessed 04-11-2015.\n[37] Christian Igel and Michael Hu\u0308sken. Empirical evaluation of the improved\nrprop learning algorithms. Neurocomputing, 50:105\u2013123, 2003.\n[38] Jarmo Ilonen. Keystroke dynamics. Advanced Topics in Information\nProcessing\u2013Lecture, pages 03\u201304, 2003.\n[39] Apple Inc. Apple watch developer. https://developer.apple.com/\nwatchkit/, Online, accessed 05-08-2015.\n[40] Apple Inc. Motion events. https://developer.apple.com/\nlibrary/ios/documentation/EventHandling/Conceptual/ EventHandlingiPhoneOS/motion_event_basics/motion_event_ basics.html, Online, accessed 23-06-2015.\n[41] Google Inc. Android wear developer. https://developer.android.\ncom/wear/, Online, accessed 05-08-2015.\n[42] Google Inc. Motion sensors. http://developer.android.com/guide/\ntopics/sensors/sensors_motion.html, Online, accessed 23-06-2015.\n[43] Google Inc. Sensor types. https://source.android.com/devices/\nsensors/sensor-types.html, Online, accessed 28-07-2015.\n[44] Jawbone Inc. Jawbone fitness tracker developer. https://jawbone.\ncom/up/developer/, Online, accessed 28-11-2015.\n[45] Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical\nexploration of recurrent network architectures. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2342\u20132350, 2015.\n[46] BI Justusson. Median filtering: Statistical properties. Springer, 1981.\n79\nBibliography\n[47] Andrej Karpathy. Convnetjs: Deep learning in javascript. https://\ngithub.com/karpathy/convnetjs, Online, accessed 04-11-2015.\n[48] Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy\nestimation and model selection. In Ijcai, volume 14, pages 1137\u20131145, 1995.\n[49] Ron Kohavi and George H John. Wrappers for feature subset selection.\nArtificial intelligence, 97(1):273\u2013324, 1997.\n[50] John Krumm. Ubiquitous computing fundamentals. CRC Press, 2009.\n[51] Jennifer R Kwapisz, Gary M Weiss, and Samuel A Moore. Activity\nrecognition using cell phone accelerometers. ACM SigKDD Explorations Newsletter, 12(2):74\u201382, 2011.\n[52] Martin La\u0308ngkvist, Lars Karlsson, and Amy Loutfi. A review of un-\nsupervised feature learning and deep learning for time-series modeling. Pattern Recognition Letters, 42:11\u201324, 2014.\n[53] Yann LeCun and Yoshua Bengio. Convolutional networks for images,\nspeech, and time series. The handbook of brain theory and neural networks, 3361(10), 1995.\n[54] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Na-\nture, 521(7553):436\u2013444, 2015.\n[55] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan.\nuwave: Accelerometer-based personalized gesture recognition and its applications. Pervasive and Mobile Computing, 5(6):657\u2013675, 2009.\n[56] Xiangyu Liu, Zhe Zhou, Wenrui Diao, Zhou Li, and Kehuan Zhang.\nWhen good becomes evil: Keystroke inference with smartwatch. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1273\u20131285. ACM, 2015.\n80\nBibliography\n[57] Arduino LLC. Arduino uno. https://www.arduino.cc/en/Main/\narduinoBoardUno, Online, accessed 30-09-2015.\n[58] Paul Lukowicz, Holger Junker, Mathias Sta\u0308ger, Thomas von Bueren,\nand Gerhard Tro\u0308ster. Wearnet: A distributed multi-sensor system for context aware wearables. In UbiComp 2002: Ubiquitous Computing, pages 361\u2013370. Springer, 2002.\n[59] Anindya Maiti, Murtuza Jadliwala, Jibo He, and Igor Bilogrevic.\n(smart) watch your taps: side-channel keystroke inference attacks using smartwatches. In Proceedings of the 2015 ACM International Symposium on Wearable Computers, pages 27\u201330. ACM, 2015.\n[60] Christopher D Manning, Prabhakar Raghavan, Hinrich Schu\u0308tze, et al.\nIntroduction to information retrieval, volume 1. Cambridge university press Cambridge, 2008.\n[61] Philip Marquardt, Arunabh Verma, Henry Carter, and Patrick Traynor.\n(sp) iphone: decoding vibrations from nearby keyboards using mobile phone accelerometers. In Proceedings of the 18th ACM conference on Computer and communications security, pages 551\u2013562. ACM, 2011.\n[62] Jonathan M McCune, Adrian Perrig, and Michael K Reiter. Seeing-is-\nbelieving: Using camera phones for human-verifiable authentication. In Security and privacy, 2005 IEEE symposium on, pages 110\u2013124. IEEE, 2005.\n[63] Emiliano Miluzzo, Alexander Varshavsky, Suhrid Balakrishnan, and\nRomit Roy Choudhury. Tapprints: your finger taps have fingerprints. In Proceedings of the 10th international conference on Mobile systems, applications, and services, pages 323\u2013336. ACM, 2012.\n81\nBibliography\n[64] Fabian Monrose and Aviel D Rubin. Keystroke dynamics as a biometric\nfor authentication. Future Generation computer systems, 16(4):351\u2013359, 2000.\n[65] JSON org. Ecma-404 the json data interchange standard. http://www.\njson.org/, Online, accessed 30-09-2015.\n[66] Emmanuel Owusu, Jun Han, Sauvik Das, Adrian Perrig, and Joy Zhang.\nAccessory: password inference using accelerometers on smartphones. In Proceedings of the Twelfth Workshop on Mobile Computing Systems & Applications, page 9. ACM, 2012.\n[67] Timo Pylva\u0308na\u0308inen. Accelerometer based gesture recognition using con-\ntinuous hmms. In Pattern Recognition and Image Analysis, pages 639\u2013 646. Springer, 2005.\n[68] Nishkam Ravi, Nikhil Dandekar, Preetham Mysore, and Michael L\nLittman. Activity recognition from accelerometer data. In AAAI, volume 5, pages 1541\u20131546, 2005.\n[69] Martin Riedmiller and Heinrich Braun. A direct adaptive method for\nfaster backpropagation learning: The rprop algorithm. In Neural Networks, 1993., IEEE International Conference on, pages 586\u2013591. IEEE, 1993.\n[70] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learn-\ning internal representations by error propagation. Technical report, DTIC Document, 1985.\n[71] Roman Schlegel, Kehuan Zhang, Xiao-yong Zhou, Mehool Intwala, Apu\nKapadia, and XiaoFeng Wang. Soundcomber: A stealthy and contextaware sound trojan for smartphones. In NDSS, volume 11, pages 17\u201333, 2011.\n82\nBibliography\n[72] Thomas Schlo\u0308mer, Benjamin Poppinga, Niels Henze, and Susanne Boll.\nGesture recognition with a wii controller. In Proceedings of the 2nd international conference on Tangible and embedded interaction, pages 11\u201314. ACM, 2008.\n[73] Ju\u0308rgen Schmidhuber. Deep learning in neural networks: An overview.\nNeural Networks, 61:85\u2013117, 2015.\n[74] Muhammad Shoaib, Stephan Bosch, Ozlem Durmaz Incel, Hans\nScholten, and Paul JM Havinga. Fusion of smartphone motion sensors for physical activity recognition. Sensors, 14(6):10146\u201310176, 2014.\n[75] Dawn Xiaodong Song, David Wagner, and Xuqing Tian. Timing analysis\nof keystrokes and timing attacks on ssh. In USENIX Security Symposium, volume 2001, 2001.\n[76] Frank Stajano. The resurrecting duckling. In Security Protocols, pages\n183\u2013194. Springer, 2000.\n[77] Ajay Kumar Tanwani, Jamal Afridi, M Zubair Shafiq, and Muddassar\nFarooq. Guidelines to select machine learning scheme for classification of biomedical datasets. In Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics, pages 128\u2013139. Springer, 2009.\n[78] Google Brain Team. Tensorflow. https://github.com/tensorflow/\ntensorflow, Online, accessed 17-11-2015.\n[79] Berkeley Vision and Learning Center. Caffe: a fast open framework\nfor deep learning. https://github.com/BVLC/caffe, Online, accessed 04-11-2015.\n[80] Martin Vuagnoux and Sylvain Pasini. Compromising electromagnetic\nemanations of wired and wireless keyboards. In USENIX security symposium, pages 1\u201316, 2009.\n83\nBibliography\n[81] He Wang, Ted Tsung-Te Lai, and Romit Roy Choudhury. Mole: Motion\nleaks through smartwatch sensors. In Proceedings of the 21st Annual International Conference on Mobile Computing and Networking, pages 155\u2013166. ACM, 2015.\n[82] Paul J Werbos. Backpropagation through time: what it does and how\nto do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990.\n[83] Jiahui Wu, Gang Pan, Daqing Zhang, Guande Qi, and Shijian Li. Ges-\nture recognition with a 3-d accelerometer. In Ubiquitous intelligence and computing, pages 25\u201338. Springer, 2009.\n[84] Zhi Xu, Kun Bai, and Sencun Zhu. Taplogger: Inferring user inputs on\nsmartphone touchscreens using on-board motion sensors. In Proceedings of the fifth ACM conference on Security and Privacy in Wireless and Mobile Networks, pages 113\u2013124. ACM, 2012.\n[85] Li Zhuang, Feng Zhou, and J Doug Tygar. Keyboard acoustic emana-\ntions revisited. ACM Transactions on Information and System Security (TISSEC), 13(1):3, 2009.\n84\nAppendices\n85\nA Backpropagation\nThis appendix provides additional details about the Backpropagation algorithm [31, 12, 70, 33] to support Section 2.2. The total network error E is computed from a loss function, such as the Mean Squared Error formalized in Equation 5.13. According to the chain rule, the gradient can be expressed as:\n\u2202E \u2202Wij = \u2202E \u2202yi \u2202yi \u2202xi \u2202xi \u2202Wij\n(A.1)\nFirst, the partial derivative with respect to Wij can be computed from\nEquation 2.1 as follows:\n\u2202xi \u2202Wij = yj (A.2)\nSecondly, the partial derivative with respect to xi is:\n\u2202yi \u2202xi = \u2202\u03c6(xi) \u2202xi (A.3)\nWhich is the derivative of the activation function of neuron i. Thirdly, the\npartial derivative with respect to yi can be computed as follows:\n86\nAppendix A. Backpropagation\n\u2202E \u2202yi =  \u2202 \u2202yi (Ti \u2212 yi) if i \u2208 output layer, \u2202\n\u2202yi ( n\u2211 j=1 Wij \u2202E \u2202yj ) otherwise;\n(A.4)\nFourthly, combining the partial derivatives allow the computation of the\nerror at a given neuron i such that:\n\u2202E \u2202yi \u2202yi \u2202xi = ei =  \u2202\u03c6(xi) \u2202xi (Ti \u2212 yi) if i \u2208 output layer, \u2202\u03c6(xi)\n\u2202xi ( n\u2211 j=1 Wijej ) otherwise;\n(A.5)\nFinally, the weight can be updated from the gradient as follows:\n\u2202E \u2202Wij = \u2202E \u2202yi \u2202yi \u2202xi \u2202xi \u2202Wij = eiyj (A.6)\nWij = Wij \u2212 \u03b7 eiyj (A.7)\nWith \u03b7 the learning rate.\n87\nB Signal Pre-processing\nThis appendix illustrates the different pre-processing operations applied to the sensor signals. On the following figures, both sensors data have been recorded during the same typing session and the values along the three axis (i.e. x, y, and z) are processed.\nB.1 Gyroscope\n88\n89\n90\nAppendix B. Signal Pre-processing\nB.2 Accelerometer\n91\n92\n93\nC Confusion Matrices from Model\nBenchmark\nThis appendix shows the confusion matrices generated during the benchmark detailed in Section 5.4.3 and performed to compare different neural network architectures on different types of features.\nC.1 Model Training with Statistical Features\n94\nC.2 Model Training with Data Segment as\nFeatures\n95\n96\n97\nD Experiment Results\nThis appendix provides result details (i.e. classifier loss during training, confusion matrices from evaluation) to support Chapter 6.\n98\nAppendix D. Experiment Results\nD.1 Results for Experiment 1: Touchlogging\nAttack\nD.1.1 FNN-Sigmoid\n99\n100\nD.1.2 FNN-Tanh\n101\n102\nD.1.3 RNN-LSTM\n103\n104\n105\nAppendix D. Experiment Results\nD.2 Results for Experiment 2: Keylogging\nAttack\nD.2.1 FNN-Sigmoid\n106\n107\nD.2.2 FNN-Tanh\n108\n109\nD.2.3 RNN-LSTM\n110\n111\nD.3 Results for Experiment 3: from Touchlog-\nging to Keylogging\n112\n113\n114"}], "references": [{"title": "On the best sensor for keystrokes inference attack on android", "author": ["Ahmed Al-Haiqi", "Mahamod Ismail", "Rosdiadee Nordin"], "venue": "Procedia Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Keyboard acoustic emanations", "author": ["Dmitri Asonov", "Rakesh Agrawal"], "venue": "In Proceedings of the IEEE Symposium on Security and Privacy,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Practicality of accelerometer side channels on smartphones", "author": ["Adam J Aviv", "Benjamin Sapp", "Matt Blaze", "Jonathan M Smith"], "venue": "In Proceedings of the 28th Annual Computer Security Applications Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Talking to strangers: Authentication in ad-hoc wireless networks", "author": ["Dirk Balfanz", "Diana K Smetters", "Paul Stewart", "H Chi Wong"], "venue": "In NDSS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Activity recognition from user-annotated acceleration data", "author": ["Ling Bao", "Stephen S Intille"], "venue": "In Pervasive computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Sniffing keystrokes with lasers/voltmeters", "author": ["Andrea Barisani", "Daniele Bianco"], "venue": "Proceedings of Black Hat USA,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Pybrain: Pythonbased reinforcement learning, artificial intelligence and neural network library. https://github.com/pybrain/pybrain, Online, accessed", "author": ["Justin Bayer", "Tom Schaul", "Thomas R\u00fcckstie\u00df"], "venue": "Bibliography", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Android wear permissions bug", "author": ["Tony Beltramelli"], "venue": "https://github. com/tonybeltramelli/Android-Wear-Permissions-Bug, Online,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Lasagne: Lightweight library to build and train neural networks in theano", "author": ["Sander D Benanne", "Jan Schl\u00fcter", "Colin Raffel"], "venue": "https: //github.com/Lasagne/Lasagne, Online,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Dictionary attacks using keyboard acoustic emanations", "author": ["Yigael Berger", "Avishai Wool", "Arie Yeredor"], "venue": "In Proceedings of the 13th ACM conference on Computer and communications security,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": "springer,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "On the theory of filter amplifiers", "author": ["Stephen Butterworth"], "venue": "Wireless Engineer,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1930}, {"title": "Security and cooperation in wireless networks: thwarting malicious and selfish behavior in the age of ubiquitous computing", "author": ["Levente Buttyan", "Jean-Pierre Hubaux"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Touchlogger: Inferring keystrokes on touch screen from smartphone motion", "author": ["Liang Cai", "Hao Chen"], "venue": "HotSec,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "On the practicality of motion based keystroke inference", "author": ["Liang Cai", "Hao Chen"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Shake them up!: a movementbased pairing protocol for cpu-constrained devices", "author": ["Claude Castelluccia", "Pars Mutaf"], "venue": "In Proceedings of the 3rd international conference on Mobile systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Sidechannel leaks in web applications: A reality today, a challenge tomorrow", "author": ["Shuo Chen", "Rui Wang", "XiaoFeng Wang", "Kehuan Zhang"], "venue": "In Security and Privacy (SP),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Keras: Theano-based deep learning library. https: //github.com/fchollet/keras, Online, accessed 04-11-2015", "author": ["Fran\u00e7ois Chollet"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Torch: Scientific computing for luajit. https://github.com/ torch/torch7, Online, accessed 04-11-2015", "author": ["Ronan Collobert", "Clement Farabet", "Koray Kavukcuoglu", "Soumith Chintala"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "From keyloggers to touchloggers: Take the rough with the smooth", "author": ["Dimitrios Damopoulos", "Georgios Kambourakis", "Stefanos Gritzalis"], "venue": "Computers & Security,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "The visual microphone: Passive recovery of sound from video", "author": ["Abe Davis", "Michael Rubinstein", "Neal Wadhwa", "Gautham J Mysore", "Fredo Durand", "William T Freeman"], "venue": "ACM Trans. Graph,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Timing attacks on pin input devices", "author": ["Denis Foo Kune", "Yongdae Kim"], "venue": "In Proceedings of the 17th ACM conference on Computer and communications security,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Recurrent nets that time and count", "author": ["Felix Gers", "J\u00fcrgen Schmidhuber"], "venue": "In Neural Networks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Dl4j: Deep learning for java. https://github.com/ deeplearning4j/deeplearning4j, Online, accessed 04-11-2015", "author": ["Adam Gibson"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Loud and clear: Human-verifiable authentication based on audio", "author": ["Michael T Goodrich", "Michael Sirivianos", "John Solis", "Gene Tsudik", "Ersin Uzun"], "venue": "In Distributed Computing Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Kout\u0144\u0131k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Data mining: concepts and techniques: concepts and techniques", "author": ["Jiawei Han", "Micheline Kamber", "Jian Pei"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Kalman filtering and neural networks, volume 47", "author": ["Simon Haykin"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Recurrent neural networks for time series classification", "author": ["Michael H\u00fcsken", "Peter Stagge"], "venue": "Neurocomputing, 50:223\u2013235,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Empirical evaluation of the improved rprop learning", "author": ["Christian Igel", "Michael H\u00fcsken"], "venue": "algorithms. Neurocomputing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Keystroke dynamics. Advanced Topics in Information Processing\u2013Lecture", "author": ["Jarmo Ilonen"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Motion events. https://developer.apple.com/ library/ios/documentation/EventHandling/Conceptual/ EventHandlingiPhoneOS/motion_event_basics/motion_event_ basics.html, Online, accessed 23-06-2015", "author": ["Apple Inc"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Median filtering: Statistical properties", "author": ["BI Justusson"], "venue": "Springer", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1981}, {"title": "Convnetjs: Deep learning in javascript", "author": ["Andrej Karpathy"], "venue": "https:// github.com/karpathy/convnetjs, Online,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["Ron Kohavi"], "venue": "In Ijcai,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1995}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John"], "venue": "Artificial intelligence,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1997}, {"title": "Ubiquitous computing fundamentals", "author": ["John Krumm"], "venue": "CRC Press,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "Activity recognition using cell phone accelerometers", "author": ["Jennifer R Kwapisz", "Gary M Weiss", "Samuel A Moore"], "venue": "ACM SigKDD Explorations Newsletter,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "A review of unsupervised feature learning and deep learning for time-series modeling", "author": ["Martin L\u00e4ngkvist", "Lars Karlsson", "Amy Loutfi"], "venue": "Pattern Recognition Letters,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1995}, {"title": "uwave: Accelerometer-based personalized gesture recognition and its applications", "author": ["Jiayang Liu", "Lin Zhong", "Jehan Wickramasuriya", "Venu Vasudevan"], "venue": "Pervasive and Mobile Computing,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "When good becomes evil: Keystroke inference with smartwatch", "author": ["Xiangyu Liu", "Zhe Zhou", "Wenrui Diao", "Zhou Li", "Kehuan Zhang"], "venue": "In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2015}, {"title": "Wearnet: A distributed multi-sensor system for context aware wearables", "author": ["Paul Lukowicz", "Holger Junker", "Mathias St\u00e4ger", "Thomas von Bueren", "Gerhard Tr\u00f6ster"], "venue": "In UbiComp 2002: Ubiquitous Computing,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2002}, {"title": "smart) watch your taps: side-channel keystroke inference attacks using smartwatches", "author": ["Anindya Maiti", "Murtuza Jadliwala", "Jibo He", "Igor Bilogrevic"], "venue": "In Proceedings of the 2015 ACM International Symposium on Wearable Computers,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}, {"title": "Introduction to information retrieval, volume 1", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2008}, {"title": "sp) iphone: decoding vibrations from nearby keyboards using mobile phone accelerometers", "author": ["Philip Marquardt", "Arunabh Verma", "Henry Carter", "Patrick Traynor"], "venue": "In Proceedings of the 18th ACM conference on Computer and communications security,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2011}, {"title": "Seeing-isbelieving: Using camera phones for human-verifiable authentication", "author": ["Jonathan M McCune", "Adrian Perrig", "Michael K Reiter"], "venue": "In Security and privacy,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2005}, {"title": "Tapprints: your finger taps have fingerprints", "author": ["Emiliano Miluzzo", "Alexander Varshavsky", "Suhrid Balakrishnan", "Romit Roy Choudhury"], "venue": "In Proceedings of the 10th international conference on Mobile systems, applications, and services,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2012}, {"title": "Keystroke dynamics as a biometric for authentication", "author": ["Fabian Monrose", "Aviel D Rubin"], "venue": "Future Generation computer systems,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2000}, {"title": "Accessory: password inference using accelerometers on smartphones", "author": ["Emmanuel Owusu", "Jun Han", "Sauvik Das", "Adrian Perrig", "Joy Zhang"], "venue": "In Proceedings of the Twelfth Workshop on Mobile Computing Systems & Applications,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "Accelerometer based gesture recognition using continuous hmms", "author": ["Timo Pylv\u00e4n\u00e4inen"], "venue": "In Pattern Recognition and Image Analysis,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Activity recognition from accelerometer data", "author": ["Nishkam Ravi", "Nikhil Dandekar", "Preetham Mysore", "Michael L Littman"], "venue": "In AAAI,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2005}, {"title": "A direct adaptive method for faster backpropagation learning: The rprop algorithm", "author": ["Martin Riedmiller", "Heinrich Braun"], "venue": "In Neural Networks,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 1993}, {"title": "Learning internal representations by error propagation", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1985}, {"title": "Soundcomber: A stealthy and contextaware sound trojan for smartphones", "author": ["Roman Schlegel", "Kehuan Zhang", "Xiao-yong Zhou", "Mehool Intwala", "Apu Kapadia", "XiaoFeng Wang"], "venue": "In NDSS,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}, {"title": "Gesture recognition with a wii controller", "author": ["Thomas Schl\u00f6mer", "Benjamin Poppinga", "Niels Henze", "Susanne Boll"], "venue": "In Proceedings of the 2nd international conference on Tangible and embedded interaction,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2008}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "Fusion of smartphone motion sensors for physical activity", "author": ["Muhammad Shoaib", "Stephan Bosch", "Ozlem Durmaz Incel", "Hans Scholten", "Paul JM Havinga"], "venue": "recognition. Sensors,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2014}, {"title": "Timing analysis of keystrokes and timing attacks on ssh", "author": ["Dawn Xiaodong Song", "David Wagner", "Xuqing Tian"], "venue": "In USENIX Security Symposium,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2001}, {"title": "The resurrecting duckling", "author": ["Frank Stajano"], "venue": "In Security Protocols,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2000}, {"title": "Guidelines to select machine learning scheme for classification of biomedical datasets. In Evolutionary Computation, Machine Learning and Data", "author": ["Ajay Kumar Tanwani", "Jamal Afridi", "M Zubair Shafiq", "Muddassar Farooq"], "venue": "Mining in Bioinformatics,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2009}, {"title": "Compromising electromagnetic emanations of wired and wireless keyboards", "author": ["Martin Vuagnoux", "Sylvain Pasini"], "venue": "In USENIX security symposium,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2009}, {"title": "Mole: Motion leaks through smartwatch sensors", "author": ["He Wang", "Ted Tsung-Te Lai", "Romit Roy Choudhury"], "venue": "In Proceedings of the 21st Annual International Conference on Mobile Computing and Networking,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 1990}, {"title": "Gesture recognition with a 3-d accelerometer", "author": ["Jiahui Wu", "Gang Pan", "Daqing Zhang", "Guande Qi", "Shijian Li"], "venue": "In Ubiquitous intelligence and computing,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2009}, {"title": "Taplogger: Inferring user inputs on smartphone touchscreens using on-board motion sensors", "author": ["Zhi Xu", "Kun Bai", "Sencun Zhu"], "venue": "In Proceedings of the fifth ACM conference on Security and Privacy in Wireless and Mobile Networks,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "Related works (detailed in Chapter 2) have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen [16, 84, 66].", "startOffset": 162, "endOffset": 174}, {"referenceID": 69, "context": "Related works (detailed in Chapter 2) have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen [16, 84, 66].", "startOffset": 162, "endOffset": 174}, {"referenceID": 53, "context": "Related works (detailed in Chapter 2) have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen [16, 84, 66].", "startOffset": 162, "endOffset": 174}, {"referenceID": 49, "context": "Other research has proved that the motion sensors from a smartphone standing on a flat surface can be used to infer the keystrokes typed on a nearby physical computer keyboard [61].", "startOffset": 176, "endOffset": 180}, {"referenceID": 66, "context": "Moreover, recently published works have demonstrated that smartwatches motion sensors could be exploited to infer keystrokes on both virtual and physical keyboards [81, 59, 56].", "startOffset": 164, "endOffset": 176}, {"referenceID": 47, "context": "Moreover, recently published works have demonstrated that smartwatches motion sensors could be exploited to infer keystrokes on both virtual and physical keyboards [81, 59, 56].", "startOffset": 164, "endOffset": 176}, {"referenceID": 45, "context": "Moreover, recently published works have demonstrated that smartwatches motion sensors could be exploited to infer keystrokes on both virtual and physical keyboards [81, 59, 56].", "startOffset": 164, "endOffset": 176}, {"referenceID": 60, "context": "These powerful models have successfully been applied to complex tasks in the fields of Computer Vision, Natural Language Processing and Speech Recognition [54, 73].", "startOffset": 155, "endOffset": 163}, {"referenceID": 42, "context": "Deep Learning has however comparatively been used poorly to process time series data such as motion sensors [52].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "However, their remarkable qualities lead to the development of various Open-Source projects [79, 21, 20, 9, 7, 28, 47, 36, 78] making them available and free to use by anyone.", "startOffset": 92, "endOffset": 126}, {"referenceID": 18, "context": "However, their remarkable qualities lead to the development of various Open-Source projects [79, 21, 20, 9, 7, 28, 47, 36, 78] making them available and free to use by anyone.", "startOffset": 92, "endOffset": 126}, {"referenceID": 8, "context": "However, their remarkable qualities lead to the development of various Open-Source projects [79, 21, 20, 9, 7, 28, 47, 36, 78] making them available and free to use by anyone.", "startOffset": 92, "endOffset": 126}, {"referenceID": 6, "context": "However, their remarkable qualities lead to the development of various Open-Source projects [79, 21, 20, 9, 7, 28, 47, 36, 78] making them available and free to use by anyone.", "startOffset": 92, "endOffset": 126}, {"referenceID": 25, "context": "However, their remarkable qualities lead to the development of various Open-Source projects [79, 21, 20, 9, 7, 28, 47, 36, 78] making them available and free to use by anyone.", "startOffset": 92, "endOffset": 126}, {"referenceID": 37, "context": "However, their remarkable qualities lead to the development of various Open-Source projects [79, 21, 20, 9, 7, 28, 47, 36, 78] making them available and free to use by anyone.", "startOffset": 92, "endOffset": 126}, {"referenceID": 7, "context": "In fact, the bug leads applications targeting Android Wear to grant some permissions without them being explicitly defined in the manifest file [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "the users, the system, the communicating entities) [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "timing information, power consumption, electromagnetic emanations, sound) [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Similarly, touchlogging is the action of recording the buttons pressed on a touch screen, or the coordinates of the touch events allowing the inference of the keys virtually touched by the user [23].", "startOffset": 194, "endOffset": 198}, {"referenceID": 40, "context": "These devices are designed to be extensively mobile and operate in environments that may have limited computing infrastructure support [50].", "startOffset": 135, "endOffset": 139}, {"referenceID": 34, "context": "Software-based sensors usually derive their data from hardwarebased sensors, namely the accelerometers (one for each axis x, y, and z), and the gyroscope [42, 40, 43, 1].", "startOffset": 154, "endOffset": 169}, {"referenceID": 0, "context": "Software-based sensors usually derive their data from hardwarebased sensors, namely the accelerometers (one for each axis x, y, and z), and the gyroscope [42, 40, 43, 1].", "startOffset": 154, "endOffset": 169}, {"referenceID": 11, "context": "This process is known as supervised learning since the statistical model needs to be trained with expert-annotated data to classify subsequently unseen samples [12, 31].", "startOffset": 160, "endOffset": 168}, {"referenceID": 28, "context": "This process is known as supervised learning since the statistical model needs to be trained with expert-annotated data to classify subsequently unseen samples [12, 31].", "startOffset": 160, "endOffset": 168}, {"referenceID": 11, "context": "Unsupervised Feature Learning is a process where the model selects features automatically through training [12, 73].", "startOffset": 107, "endOffset": 115}, {"referenceID": 60, "context": "Unsupervised Feature Learning is a process where the model selects features automatically through training [12, 73].", "startOffset": 107, "endOffset": 115}, {"referenceID": 11, "context": "synapses) has a weight associated with it [12, 33].", "startOffset": 42, "endOffset": 50}, {"referenceID": 9, "context": "[10] have shown that standard RNNs are in practice unable to learn long-term dependencies in contexts where information need to be connected over long time intervals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Hochreiter and Schmidhuber [34] overcame the limitations of standard RNN by introducing a new architecture termed Long Short-Term Memory (LSTM) which allows the association of input with memories remote in time by preserving the backpropagated error through time and layers.", "startOffset": 27, "endOffset": 31}, {"referenceID": 27, "context": "While many LSTM implementation variants have been proposed [30, 45], the following detailed LSTM cell use a forget gate [27] with no bias for simplicity reasons.", "startOffset": 59, "endOffset": 67}, {"referenceID": 35, "context": "While many LSTM implementation variants have been proposed [30, 45], the following detailed LSTM cell use a forget gate [27] with no bias for simplicity reasons.", "startOffset": 59, "endOffset": 67}, {"referenceID": 24, "context": "While many LSTM implementation variants have been proposed [30, 45], the following detailed LSTM cell use a forget gate [27] with no bias for simplicity reasons.", "startOffset": 120, "endOffset": 124}, {"referenceID": 30, "context": "c and use o to decide when to read information from c [34].", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "Additionally, a forget gate f is used to reset memory and, as a result, help the network process continuous sequences or sequences that are not segmented with precise starting and ending time [27].", "startOffset": 192, "endOffset": 196}, {"referenceID": 23, "context": "[26] to allow recurrent networks to distinguish between sequences of variable length.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "Backpropagation is a popular algorithm designed to train ANNs using a gradient descent method to minimize the network prediction error [70].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "Otherwise, the error of hidden neurons is proportional to the weighted sum of errors from connected neurons [12, 31, 33].", "startOffset": 108, "endOffset": 120}, {"referenceID": 28, "context": "Otherwise, the error of hidden neurons is proportional to the weighted sum of errors from connected neurons [12, 31, 33].", "startOffset": 108, "endOffset": 120}, {"referenceID": 67, "context": "A significant alternative is Backpropagation Through Time [82] used to train RNNs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 65, "context": "Traditionally by exploiting characteristics of physical keyboards such as electromagnetic waves [80], sound [2, 85, 11], and timing [75, 25].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "Traditionally by exploiting characteristics of physical keyboards such as electromagnetic waves [80], sound [2, 85, 11], and timing [75, 25].", "startOffset": 108, "endOffset": 119}, {"referenceID": 10, "context": "Traditionally by exploiting characteristics of physical keyboards such as electromagnetic waves [80], sound [2, 85, 11], and timing [75, 25].", "startOffset": 108, "endOffset": 119}, {"referenceID": 62, "context": "Traditionally by exploiting characteristics of physical keyboards such as electromagnetic waves [80], sound [2, 85, 11], and timing [75, 25].", "startOffset": 132, "endOffset": 140}, {"referenceID": 22, "context": "Traditionally by exploiting characteristics of physical keyboards such as electromagnetic waves [80], sound [2, 85, 11], and timing [75, 25].", "startOffset": 132, "endOffset": 140}, {"referenceID": 58, "context": "However, such side-channels are ineffective to exploit virtual keyboard, albeit sound have been successfully exploited on smartphones [71].", "startOffset": 134, "endOffset": 138}, {"referenceID": 21, "context": "Studies have shown the great potential of recovering sound, music, voice conversations, and even typing by simply observing slight vibrations in the environment produced by physical events [24, 6].", "startOffset": 189, "endOffset": 196}, {"referenceID": 5, "context": "Studies have shown the great potential of recovering sound, music, voice conversations, and even typing by simply observing slight vibrations in the environment produced by physical events [24, 6].", "startOffset": 189, "endOffset": 196}, {"referenceID": 15, "context": "However, studies [17, 3] have shown that motion-based keystroke inference attack remains effective and practical despite the obvious assumptions that the previously enunciated factors might alter the robustness and the accuracy of the inference.", "startOffset": 17, "endOffset": 24}, {"referenceID": 2, "context": "However, studies [17, 3] have shown that motion-based keystroke inference attack remains effective and practical despite the obvious assumptions that the previously enunciated factors might alter the robustness and the accuracy of the inference.", "startOffset": 17, "endOffset": 24}, {"referenceID": 49, "context": "[61] have shown that the motion sensors output from a smartphone standing on a flat surface can be used to infer keystrokes typed on a nearby physical computer keyboard standing on the same surface.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Related works [16, 84, 66, 63] have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen.", "startOffset": 14, "endOffset": 30}, {"referenceID": 69, "context": "Related works [16, 84, 66, 63] have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen.", "startOffset": 14, "endOffset": 30}, {"referenceID": 53, "context": "Related works [16, 84, 66, 63] have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen.", "startOffset": 14, "endOffset": 30}, {"referenceID": 51, "context": "Related works [16, 84, 66, 63] have shown that the data from the motion sensors of a smartphone can be used to infer keystrokes entered on its touchscreen.", "startOffset": 14, "endOffset": 30}, {"referenceID": 14, "context": "[16] demonstrated that a malicious Android application can infer as much as 70% of the keystrokes entered on a number-only virtual keyboard on an Android device.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[66] proposed a system that reads accelerometer data to extract 6-character passwords on an Android device.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[84] introduced a keystroke inference attack by using a Trojan application running on the Android platform.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[63] have demonstrated that the motion sensors built-in smartphones and tablets could be used to infer keystrokes entered on a complete 26-letters keyboard with an accuracy reaching as much as 90%.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction.", "startOffset": 110, "endOffset": 121}, {"referenceID": 55, "context": "Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction.", "startOffset": 110, "endOffset": 121}, {"referenceID": 41, "context": "Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction.", "startOffset": 110, "endOffset": 121}, {"referenceID": 54, "context": "Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction.", "startOffset": 182, "endOffset": 198}, {"referenceID": 59, "context": "Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction.", "startOffset": 182, "endOffset": 198}, {"referenceID": 44, "context": "Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction.", "startOffset": 182, "endOffset": 198}, {"referenceID": 68, "context": "Motion sensors signal classification have been explored extensively in studies involving activity recognition [5, 68, 51] in the field of Pervasive Computing and gesture recognition [67, 72, 55, 83] in the area of Human-Computer Interaction.", "startOffset": 182, "endOffset": 198}, {"referenceID": 61, "context": "In both fields, the use of accelerometer sensors is historically studied more deeply although some studies explored sensors fusion to increase robustness [74, 58].", "startOffset": 154, "endOffset": 162}, {"referenceID": 46, "context": "In both fields, the use of accelerometer sensors is historically studied more deeply although some studies explored sensors fusion to increase robustness [74, 58].", "startOffset": 154, "endOffset": 162}, {"referenceID": 64, "context": "According to Tanwani [77], the classification accuracy of a given algorithm is largely dependent on the nature of the dataset rather than the classification algorithm itself.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "In their implementation [17], they showed that the inference accuracy level stabilizes when the training set reaches a certain size (i.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "In fact, studies have shown that the gyroscope is a better side-channel than the accelerometer for keystroke inference [1, 17, 63].", "startOffset": 119, "endOffset": 130}, {"referenceID": 15, "context": "In fact, studies have shown that the gyroscope is a better side-channel than the accelerometer for keystroke inference [1, 17, 63].", "startOffset": 119, "endOffset": 130}, {"referenceID": 51, "context": "In fact, studies have shown that the gyroscope is a better side-channel than the accelerometer for keystroke inference [1, 17, 63].", "startOffset": 119, "endOffset": 130}, {"referenceID": 49, "context": "In fact, hardware specification such as the motion sensor sampling rate is not fixed and is much lower than the sampling rates of acoustic and electromagnetic sensors used in related eavesdropping attacks [61, 17, 1].", "startOffset": 205, "endOffset": 216}, {"referenceID": 15, "context": "In fact, hardware specification such as the motion sensor sampling rate is not fixed and is much lower than the sampling rates of acoustic and electromagnetic sensors used in related eavesdropping attacks [61, 17, 1].", "startOffset": 205, "endOffset": 216}, {"referenceID": 0, "context": "In fact, hardware specification such as the motion sensor sampling rate is not fixed and is much lower than the sampling rates of acoustic and electromagnetic sensors used in related eavesdropping attacks [61, 17, 1].", "startOffset": 205, "endOffset": 216}, {"referenceID": 15, "context": "[17] proposed a pre-processing solution allowing them to use signal analysis methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] experimented with the patterns produced by the motion during keystrokes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[66] solved the sampling rate problem by using an approach involving linear interpolation to create consistent sampling intervals throughout the recorded accelerometer data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "A wrapper [49] was then used for feature subset selection to maximize the accuracy of the prediction.", "startOffset": 10, "endOffset": 14}, {"referenceID": 49, "context": "[61] used a 100ms long time window as Asonov et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] to extract features from the signal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 69, "context": "[84] selected features from the signal in a time window bounded by the touch-down and the touch-up events triggered when the user interact with the touchscreen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 66, "context": "[81] developed a system to perform keylogging on a laptop keyboard using the motion sensors of a smartwatch.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[59]", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[56] focused on performing keylogging with a smartwatch on both a number-only physical keypad and a standard QWERTY computer keyboard.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "In fact, device pairing has been extensively studied in the field of Computer Security [76, 62, 4, 18, 29] and has proven to be challenging.", "startOffset": 87, "endOffset": 106}, {"referenceID": 50, "context": "In fact, device pairing has been extensively studied in the field of Computer Security [76, 62, 4, 18, 29] and has proven to be challenging.", "startOffset": 87, "endOffset": 106}, {"referenceID": 3, "context": "In fact, device pairing has been extensively studied in the field of Computer Security [76, 62, 4, 18, 29] and has proven to be challenging.", "startOffset": 87, "endOffset": 106}, {"referenceID": 16, "context": "In fact, device pairing has been extensively studied in the field of Computer Security [76, 62, 4, 18, 29] and has proven to be challenging.", "startOffset": 87, "endOffset": 106}, {"referenceID": 26, "context": "In fact, device pairing has been extensively studied in the field of Computer Security [76, 62, 4, 18, 29] and has proven to be challenging.", "startOffset": 87, "endOffset": 106}, {"referenceID": 6, "context": "The server then saved them The ANNs are implemented using modules available in the PyBrain Open-Source library [7] with a C++ wrapper additionally used to speed up the computations.", "startOffset": 111, "endOffset": 114}, {"referenceID": 19, "context": "Some experiments were also performed using Torch [21] and the programming language Lua.", "startOffset": 49, "endOffset": 53}, {"referenceID": 36, "context": "The moving median removes the noise while preserving the signal pattern and is applied with a sliding window to compute the median value in a fixed range [46], that is:", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "At contrary for the accelerometer, we want to attenuate signals with frequencies lower than the cutoff frequency, thus the need to use a high-pass filter [14].", "startOffset": 154, "endOffset": 158}, {"referenceID": 29, "context": "Even with data containing statistical noise, the filter can produce estimates allowing patterns to emerge more significantly from the signal [32].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "[2] determined the duration of a key-press to be approximately 100ms and knowing that our target sampling rate was defined to be 2ms (see Section 5.", "startOffset": 0, "endOffset": 3}, {"referenceID": 56, "context": "The weights are updated using an improved variant of the Backpropagation iterative gradient descent termed Rprop- [69, 37].", "startOffset": 114, "endOffset": 122}, {"referenceID": 32, "context": "The weights are updated using an improved variant of the Backpropagation iterative gradient descent termed Rprop- [69, 37].", "startOffset": 114, "endOffset": 122}, {"referenceID": 48, "context": "F1 Score) [60].", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "H\u00fcsken and Stagge [35] proposed a method to assess the reliability of a classification algorithm based on the value distribution of the output neurons.", "startOffset": 18, "endOffset": 22}, {"referenceID": 38, "context": "All data are thus used for both training and evaluation to provide a more general and accurate performance assessment [48].", "startOffset": 118, "endOffset": 122}, {"referenceID": 43, "context": "Convolutional Neural Network (CNN) is a class of powerful deep models designed to process multi-dimensional arrays such as images and video frames [53].", "startOffset": 147, "endOffset": 151}, {"referenceID": 52, "context": "The motion of WAD could also potentially be used for the identification and tracking of users as studied in similar research [64, 38].", "startOffset": 125, "endOffset": 133}, {"referenceID": 33, "context": "The motion of WAD could also potentially be used for the identification and tracking of users as studied in similar research [64, 38].", "startOffset": 125, "endOffset": 133}], "year": 2015, "abstractText": "Wearable technologies are today on the rise, becoming more common and broadly available to mainstream users. In fact, wristband and armband devices such as smartwatches and fitness trackers already took an important place in the consumer electronics market and are becoming ubiquitous. By their very nature of being wearable, these devices, however, provide a new pervasive attack surface threatening users privacy, among others. In the meantime, advances in machine learning are providing unprecedented possibilities to process complex data efficiently. Allowing patterns to emerge from high dimensional unavoidably noisy data. The goal of this work is to raise awareness about the potential risks related to motion sensors built-in wearable devices and to demonstrate abuse opportunities leveraged by advanced neural network architectures. The LSTM-based implementation presented in this research can perform touchlogging and keylogging on 12-keys keypads with above-average accuracy even when confronted with raw unprocessed data. Thus demonstrating that deep neural networks are capable of making keystroke inference attacks based on motion sensors easier to achieve by removing the need for non-trivial preprocessing pipelines and carefully engineered feature extraction strategies. Our results suggest that the complete technological ecosystem of a user can be compromised when a wearable wristband device is worn.", "creator": "LaTeX with hyperref package"}}}