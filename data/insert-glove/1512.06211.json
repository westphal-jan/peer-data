{"id": "1512.06211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2015", "title": "Test-Driven Development of ontologies (extended version)", "abstract": "konkin Emerging ontology camerman authoring mounded methods to luminal add qasir knowledge to an oestreich ontology ksat focus djimon on ameliorating gaudreau the maca validation panzerkampfwagen bottleneck. ngwe The verification of vonnie the newly aguardiente added cavalry axiom is still relly one goer of trying 62.73 and axxess seeing nupeng what cioculescu the reasoner perini says, saracino because baumgart a belhar systematic li\u00e9geois testbed cacapa for ontology authoring gruzdev is missing. We sought george@pbpost.com to rebt address 4.4-kilometer this monarchia by introducing idel the approach of preu\u00dfische test - 84.81 driven development for hayate ontology 2147 authoring. gilmerton We specify leflore 36 templin generic tests, as sylviane TBox queries allege and rappeneau TBox axioms dominium tested through bruises individuals, tait\u014d and clenney structure edifice their welteke inner tuggeranong workings gunigundo in an ` open box ' - way, manicurist which cover the 831,000 OWL pagenet 2 x-43 DL 4motion language features. This is implemented as a Protege plugin so that siti one roorkee can perform a escapist TDD test bridgeman as a truncate black lenau box test. We evaluated mauvaise the makhloufi two test approaches kingsland on lewandowska their performance. The cuffley TBox valadon queries \u0111uri\u0161i\u0107 were faster, and hobby that irreproachable effect taysir is abdullaev more pronounced phillip the \u0cac larger pitman the brocaded ontology temarii is. We provide a radeon general glimpsed sequence leusden of a TDD process amaker for ontology engineering as salars a foundation www.expedia.com for a fleurant TDD ljubljanska methodology.", "histories": [["v1", "Sat, 19 Dec 2015 09:15:24 GMT  (308kb,D)", "http://arxiv.org/abs/1512.06211v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["c maria keet", "agnieszka lawrynowicz"], "accepted": false, "id": "1512.06211"}, "pdf": {"name": "1512.06211.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["C. Maria Keet", "Agnieszka Lawrynowicz"], "emails": ["mkeet@cs.uct.ac.za", "agnieszka.lawrynowicz@cs.put.poznan.pl"], "sections": [{"heading": "1 Introduction", "text": "The process of ontology development has progressed much over the past 20 years, aided by development information systems-oriented methodologies, such as methontology for a single-person/group monolithic ontology to ontology networks with NeOn [31], and both stand-alone and collaborative tools, such as Prote\u0301ge\u0301 [10] and MoKI [11]. They are examples of generic, high-level information systems-oriented methodologies, but support for effective ontology authoring\u2014 adding the right axioms and adding the axioms right\u2014has received some attention only more recently. Processes at this \u2018micro\u2019 level of the development process, rather than the \u2018macro\u2019 level, may use the reasoner to propose axioms with FORZA [16], use Ontology Design Patterns (ODPs) [7], and start gleaning ideas from software engineering practices, notably exploring the notion of unit tests [35], eXtreme Design with ODPs [3], and Competency Question (CQ)-based authoring using SPARQL [28]. However, testing whether a CQ can be answered does not say how to add/change the knowledge represented in the ontology, FORZA considers simple object properties only, and eXtreme Design limits one to ODPs that have to come from some place. Put differently, there is no systematic testbed for ontology engineering, to implement the CQ in the authoring ar X iv :1\n51 2.\n06 21\n1v 1\n[ cs\n.A I]\n1 9\nD ec\nprocess in a piecemeal fashion, other than manual efforts by a knowledge engineer to add or change something and running the reasoner to check its effects. This still puts a high dependency on expert knowledge engineering, which ideally should not be in the realm of an art, but rather at least a systematic process for good practices.\nWe aim to address this problem by borrowing another idea from software engineering: test-driven development (TDD). TDD ensures that what is added to the program core (here: ontology) does indeed have the intended effect specified upfront. Moreover, TDD in principle is cognitively a step up from the \u2018just add stuff and lets see what happens\u2019-attitude, therewith deepening the understanding of the ontology authoring process and the logical consequences of an axiom. In addition, it would make roll-backs and conflicting CQs easier to manage. At an informal, high level, one can specify the following three scenarios of usage.\nI. CQ-driven TDD Developers (domain experts, knowledge engineers etc) specify CQs. A CQ is translated automatically into one or more axioms. This (these) axiom(s) are the input of the relevant TDD test(s) to be carried out. The developers who specify the CQs could be oblivious to the inner workings of the two-step process of translating the CQ and testing the axiom(s). II-a. Ontology authoring-driven TDD - the knowledge engineer The knowledge engineer knows which axiom s/he wants to add, types it, which is then fed directly into the TDD system. II-b. Ontology authoring-driven TDD - the domain expert As there is practically only a limited amount of \u2018types\u2019 of axioms to add, one could create templates, alike the notion of the \u201clogical macro\u201d ODP [27]. For instance, a domain expert could choose the all-some template from a list, which then in the TDD system amounts to an axiom of the form C v \u2203R.D. The domain expert instantiates it with relevant domain entities (e.g., Professor v \u2203teaches.Course), and the related TDD test is then run automatically. The domain expert need not necessarily know the logic, but behind the usability interface, what gets sent to the TDD system is that axiom.\nWhile in each case the actual testing can be hidden from the user\u2019s view, it is necessary to specify what actually happens during such testing and how it is tested (in a similar way that it needed to be clear how the OWL reasoner works). Here, we assume that either the first step of the CQ process is completed, or the knowledge engineer adds the axiom, or that the template is populated, respectively; i.e., that we are at the stage where the axioms are fed into the TDD test system. To realise the testing, a number of questions have to be answered:\n1. Given the TDD procedure in software engineering\u2014check that desired feature is absent, code it, test again\u2014then what does that mean for ontology testing when transferred to ontology development? 2. TDD requires so-called mock objects for \u2018incomplete\u2019 parts of the code, and mainly for methods; is there a parallel to it in ontology development, or can that aspect of TDD be ignored?\n3. In what way, and where, (if at all) can this be integrated as a methodological step in existing ontology engineering methodologies that are typically based on waterfall, iterative, or lifecycle principles rather than agile methodologies? To work this out for ontologies, we take some inspiration from TDD for conceptual modelling. Tort et al. [34] essentially specify \u2018unit tests\u2019 for each feature/possible addition to a conceptual model, and test such an addition against sample individuals. Translating this to OWL ontologies, such testing is possible by means of ABox individuals, and then instead if using an ad hoc algorithm, one can avail of the automated reasoner. In addition, for ontologies, one can avail of a query language for the TBox, namely, SPARQL-OWL [18], and most of the test can be specified in that language as well. We define TBox and ABoxdriven TDD tests for the basic axioms one can add to an OWL 2 DL ontology. Mock objects are required especially for TDD tests for the \u2018RBox\u2019, i.e., for the object property-specific axioms. To examine practical feasibility for the ontology engineer and determine which TDD strategy is the best option for ontology engineering, we first implemented this by means of a Prote\u0301ge\u0301 plugin as basic interface, and, second, evaluated the plugin on performance by comparing TBox and ABox TDD tests. TBox queries generally outperform the ABox ones, and this difference is more pronounced with larger ontologies. Finally, we outline a TDD ontology development methodology, which, while having overlap with some extant methodologies, cannot not neatly be squeezed into them. Overall, we thus add a new mechanism and tool to the ontology engineer\u2019s \u2018toolbox\u2019 to enable systematic development of ontologies in an agile way.\nThe remainder of the paper is structured as follows. Section 2 describes related works on TDD in software and ontology development. Section 3 summarises the TDD tests and Section 4 evaluates them on performance with the Prote\u0301ge\u0301 plugin. We discuss in Section 5, where we also answer the above-mentioned research questions, and conclude in Section 6."}, {"heading": "2 Related work", "text": "Before assessing related works in ontology engineering, we first describe some pertinent aspects of TDD from software engineering.\nTDD in software development The principal introduction of TDD is described in [2]: it is essentially a methodology of software development, where one writes new code only if an automated test has failed, and eliminating duplication in doing so. The test come from some place: a feature requirement should have a test specification to go with it, or, more concretely,\u201cIf you can\u2019t write test for what you are about to code, then you shouldn\u2019t even be thinking about coding.\u201d [20]. That is, TDD permeates the whole of what was traditionally the waterfall or iterative methodology. Shrivastava and Jain [29] summed up the sequence as follows: 1) Write a test for a piece of functionality (that was based on a requirement), 2) Run all tests to check that the new test fails, 3) Write relevant code that passes the test, 4) Run the specific test to verify it passes, 5)\nRefactor the code, and 6) Run all tests to verify that the changes to the code did not change the external behaviour of the software (regression testing). The important difference with unit tests, is that TDD is a test-first approach rather than the more commonly known \u2018test-last\u2019 approach (design, code, test), and therewith moves into the realm of a methodology of its own permeating all steps in the development rather than only the testing phase with its unit tests for individual classes or components.\nTDD is said to result in being more focussed, improved communication, improved understanding of required software behaviour, and reduced design complexity [20]. Quantitatively, TDD produced code passes more externally defined tests\u2014i.e, better software quality\u2014and involves less time spent on debugging, and experiments with students showed that the test-first group wrote more tests and were significantly more productive than the test-last group [14].\nWhile all this focuses on the actual programming, the underlying ideas have been applied to conceptual data modelling [33,34], resulting in a TDD authoring process, compared to, among others, Halpin\u2019s conceptual schema design procedure [13]. Tort and Olive\u0301 reworked the test specification into a test-driven way of modelling, where each UML Class Diagram language feature has its own test specification in OCL that involves creating the objects that should, or ought not to, instantiate the UML classes and associations [34]. Also here, the test is supposed to fail first, then the model is updated, and then the test ought to pass. The tool that implements it was evaluated with modellers, which made clear, among others, that more time was spent on developing and revising the conceptual model, in the sense of fixing errors, than on writing the test cases [34].\nTests in ontology engineering An early explorative work on borrowing the notion of testing from software engineering to apply it to ontology engineering is described in [35], which explores several adaptation options: testing with the axiom and its negation, formalising CQs, checks by means on integrity constraints, autoepistemic operators, and domain and range assertions. Working with CQs has shown to be, relatively, the most popular approach in the years since. A substantial step in the direction of test-driven ontology engineering was proposed by Ren et al [28], who analyse CQs for use with SPARQL queries that then would be tested against the ontology. It focuses on finding patterns in the natural language-based CQs, the sample SPARQL queries are querying for individuals only, and the formalisation stops at what has to be tested, not how that can, or should, be done. Earlier work on CQs and queries include the OntologyTest tool, which allows the user to specify tests to check the functional requirements of the ontology based on the CQs, using ABox instances and SPARQL queries [8]. Unlike extensive CQ and query patters, it specifies different types of tests focussed on the ABox rather than knowledge represented in the TBox, such as \u201cinstantiation tests\u201d (instance checking) and \u201crecovering tests\u201d (query for a class\u2019 individuals) and using mock individuals where applicable [8]; other instance-oriented test approaches have been proposed as well, although\nfocussing on RDF/Linked Data rather than the OWL ABox [19]. A NeON plugin with similar functionality and methodical steps within the eXtreme Design approach also has been proposed [3], but not the types of tests. A more basic variant is incorporated in the EFO Validator3 for the Experimental Factor Ontology (EFO), which has tests for presence of a specific class or relation. Neither are based on the principle of TDD where, according to its specification, a test first has to fail, then the code is modified, and then the TDD test should pass. Warrender and Lord\u2019s approach [36] does take that in consideration. They focus on unit tests for TBox testing where each query to the ontology requires a new test declaration. The tests have to be specified in Clojure with its own unfamiliar Tawny-Owl notation, describes only subsumption tests although the Karyotype ontology it is applied to is in ALCHI, and the tests themselves are very tailored to the actual ontology rather than having reusable \u2018templates\u2019 for the tests covering all OWL language features. On the positive side, it can avail of some existing infrastructure for software testing rather than reinventing that technological wheel.\nThe ontology unit test notion of axiom and its negation of [35] has been used, in a limited sense, in advocatus diaboli, where in the absence of a disjointness axiom between classes, it shows the consequences to the modeller to approve or disapprove, and if the latter is selected, the tool adds the disjointness axiom [6]. Some of the OOPS! pitfalls that the software checks has some test-flavour to it [25], such as suggesting possible symmetry. FORZA shows the permissible relations one can add that will not lead to an inconsistency, which is based on domain and range constraints of properties and the selected entities one wants to relate [16]; that is, also not hypothesising a failure/absence of required knowledge in the TBox as such, though it can be seen as a variant of the domain & range assertions unit tests in [35].\nConcerning overarching methodologies, none of the 9 methodologies reviewed by [9] are TDD-based, nor is the MeltingPoint Garc\u0301\u0131a et al. propose themselves. A recent Agile-inspired tool and methodology is OntoMaven. Aspect OntoMaven [24] is an extension to OntoMaven that is based on reusing ideas from Apache Maven, advocated as being both a tool and supporting agile ontology engineering, such as COLM4. Regarding tests, besides the OntoMvnTest with \u2018test cases\u2019 for the usual syntax checking, consistency, and entailment, the documentation states it should be possible to reuse Maven plug-ins for further test types [24], but this has not been followed through yet. A different Agile-inspired method is, eXtreme Design with content ODPs [26], although this is also more a design method for rapid turnaround times rather than test-driven. Likewise, the earlier proposed RapidOWL is based on \u201citerative refinement, annotation and structuring of a knowledge base\u201d [1] rather permeating the test-driven approach throughout the methodology. RapidOWL does mention the notion of \u201cshort releases\u201d, which is very compatible with TDD cf. NeON\u2019s waterfall-inspired \u2018write\n3 http://www.ebi.ac.uk/fgpt/sw/efovalidator/index.html 4 http://www.corporate-semantic-web.de/colm.html\nmany CQs first\u2019 [31], but not how this is to be achieved other than publishing new versions quickly.\nThus, full TDD ontology engineering as such has not been proposed yet, to the best of our knowledge. While the idea of unit tests\u2014which potentially could become part of TDD test\u2014has been proposed, there is a dearth of actual specifications as to what exactly is, or should be, going on in such as test. It is also unclear whether even when one were to specify basic tests for each language feature, whether they can be put together in a modular fashion for the more complex axioms that can be declared with OWL 2. Further, there is no regression testing to check that perhaps an earlier modelled CQ\u2014and thus a passed test\u2014conflicts with a later one, and identifying which ones are conflicting."}, {"heading": "3 TDD specification for ontologies", "text": "Before introducing the TBox and RBox TDD tests, first the general procedure in introduced and some clarifications are given on notation, the notion of true/false of a TDD test, and mock entities."}, {"heading": "3.1 Preliminaries", "text": "Taking the TDD approach of devising a test that demonstrates absence of the feature (i.e., test failure), add feature, test again whether the test passes, the generalised TDD principle for ontologies then becomes: 1. input: CQ and transform this into an axiom (optionally) 2. given: axiom x of type X to be added to the ontology. 3. check the vocabulary elements of x are in ontology O (itself a TDD test) 4. run TDD test twice:\n(a) the first execution should fail (check O 2 x or not present), (b) update the ontology (add x), and (c) run the test again which then should pass (check that O |= x) and such\nthat there is no new inconsistency or undesirable deduction 5. Run all previous successful tests, which still have to pass (i.e., regression\ntesting) In the following two subsections, we define such TDD test for TBox and RBox axioms with respect to OWL 2 DL features. For the TDD tests, there are principally two options: a TDD test at the TBox-level (where possible), or always using individuals explicitly asserted in the ABox. We specify tests for both approaches. For the test specifications, we use the OWL 2 standard notation for the ontology\u2019s vocabulary: C,D,E, ... \u2208 VC , R,S, ... \u2208 VOP , and a, b, ... \u2208 VI .\nFor the TBox tests, we use SPARQL-OWL [18] where possible/applicable5. Its notation is principally reusing OWL functional syntax-style notation merged with SPARQL\u2019s queried objects (i.e., ?x) for the formulation of the query, and\n5 While SPARQL-DL [30] might be more well-known than SPARQL-OWL, that version does not permit TBox queries with object properties, whereas SPARQL-OWL does, and we need those features\nadding a variable for the query answer; e.g., \u03b1\u2190 SubClassOf (?x D) will return all subclasses of class D. The query rewriting of SPARQL-OWL has been described in [18] and a tool implementing the algorithms, OWL-BGP, is freely available6. While for some tests, one can use Prote\u0301ge\u0301\u2019s intuitive DL query tab to perform the tests, this is possible only for a fraction of the tests, and therefore omitted.\nSome TBox and all ABox tests will require additional classes or individuals that only serve the purpose of testing and have to be removed after the test terminates successfully. This resembles the notion of mock objects in software engineering [21,17], which we shall transport into the ontology setting. While TDD tests for ontologies do not need stubs for class methods, in some cases a test does need additional elements for it to be testable. Therefore, we do use mock class for a temporary OWL class, mock individual for an ABox individual created for the TDD test, and mock axiom for any auxiliary axiom that has to be added solely for testing purposes; they are to be removed from the ontology after completion of the test. (Possibly this can be taken further, using mock ontology modules, alike the mock databases [32], but this is not needed at this stage.)\nSteps 3 and 4a in the sequence listed above may give an impression of epistemic queries. It has to be emphasised that there is a fine distinction between 1) checking when an element is in the vocabulary of the TBox of the ontology (in VC or VOP ) versus autoepistemic queries, and 2) whether something is logically true or false versus a test evaluating to true or false. Proposals for epistemic extensions for OWL exist, which concern instance retrieval and integrity constraints checking [22] that reduces the K-operator to plain OWL queries, or closed world reasoning [12]. The TDD test focus in step 3, above, is of a distinctly different flavour. We need to know whether an element is present in the ontology, but we do not want to \u2018know\u2019 or assert things about individuals (say, that when there is an instance of country, that there must also be an object of capital associated with it, as in, KCountry v A\u2203hasCapital.ACapital). As such, an epistemic query language is not needed for the TBox-level axioms. In the TDD context, the epistemic-sounding \u2018not asserted in or inferred from the ontology\u2019 is to be understood in the context of a TDD test, like whether an ontology has some class C in its vocabulary, not whether it is \u2018known to exist\u2019 in one\u2019s open or closed world based on the knowledge represented in the ontology."}, {"heading": "3.2 Test patterns for TBox axioms", "text": "The tests are introduced in sequence, where the primed test names are the ones with explicit individuals.\nClass subsumption, Tcs or T \u2032 cs. When the axiom to add is of type C v D, then O |= \u00ac(C v D) should be true if it were not present. Logically, this means O \u222a \u00ac\u00ac(C v D) should be inconsistent, i.e., O \u222a (\u00acC t D). Given the current Semantic Web technologies, it is easier to just query the ontology for the\n6 https://github.com/iliannakollia/owl-bgp\nsuperclasses of C and to ascertain that D is not in query answer \u03b1 rather than create and/or execute tailor-made tableau algorithms. Thus, a test T can be true or false. In SPARQL-OWL notation and quasi algorithm for comprehensiveness, Tcs is: Require: Test T (C v D) . i.e., Tcs 1: \u03b1\u2190 SubClassOf(?x D) 2: if C /\u2208 \u03b1 then . C v D is neither asserted nor entailed in the ontology 3: return T (C v D) is false 4: else 5: return T (C v D) is true 6: end if\nAfter adding C v D to the ontology, the same test is run, which should evaluate to D \u2208 \u03b1 and therewith T (C v D) is true.\nThe TTD test with individuals concerns a check whether an instance of C is also an instance of D. That is, for T \u2032cs we have: Require: Test T (C v D) . i.e., T \u2032cs 1: Create a mock object a 2: Assert C(a) 3: \u03b1\u2190 Type(?x D) 4: if a /\u2208 \u03b1 then . C v D is neither asserted nor entailed in the ontology 5: return T (C v D) is false 6: else 7: return T (C v D) is true 8: end if\nClass disjointness, Tcd or T \u2032 cd. One can assert the complement, C v \u00acD, or disjointness, C u D v \u22a5. Let us consider the former first, for a test Tcdc . For the test, then \u00ac(C v \u00acD) should be true, or T (C v \u00acD) false (in the sense of \u2018not be in the ontology\u2019), Testing for the latter only does not suffice, however, as there are more cases where O 2 C v D holds, but disjointness is not really applicable\u2014being classes in distinct sub-trees in the TBox\u2014or holds when disjointness is asserted already, which is when C and D are sibling classes. For this complement with the inclusion axiom in OWL, we simply can query for the complement in the ontology: Require: Test T (C v \u00acD) . i.e., test Tcdc 1: \u03b1\u2190 ObjectComplementOf(C ?x) 2: if D /\u2208 \u03b1 then . thus O 2 C v \u00acD 3: return T (C v \u00acD) is false 4: else 5: return T (C v \u00acD) is true 6: end if\nConcerning the stronger version of disjointness, C uD v \u22a5, in SPARQL-OWL notation: Require: Test T (C uD v \u22a5) . i.e., test Tcdd"}, {"heading": "1: \u03b1\u2190 DisjointClasses(?x D)", "text": "2: if C /\u2208 \u03b1 then . thus, O 2 C uD v \u22a5 3: return T (C uD v \u22a5) is false 4: else 5: return T (C uD v \u22a5) is true 6: end if The second option for the test, T \u2032cd, is to involve the ABox and use a query or run the reasoner. The sequence of steps is as follows, availing of the reasoner only and no additional queries are necessary: Require: Test T (C v \u00acD) or T (C uD v \u22a5) . i.e., test T \u2032cd 1: Create individual a . that is, a is a mock object 2: Assert C(a) and D(a); 3: ostate\u2190 Run the reasoner 4: if ostate == consistent then . the test fails, i.e., then either O 2 C v \u00acD or\nO 2 C uD v \u22a5 directly or through one or both of their superclasses 5: return T (C v \u00acD) or T (C uD v \u22a5) is false 6: else . the ontology is inconsistent, the test passed; thus either C v \u00acD\nor C uD v \u22a5 is already asserted among both their superclasses or among C or D and a superclass of D or C, respectively.\n7: return T (C v \u00acD) or T (C uD v \u22a5) is true 8: end if\nFurther, from a modelling viewpoint, it would make sense to also require C and D to be siblings. The sibling requirement can be added as an extra check in the interface to alert the modeller to it, but not be enforced from a logic viewpoint.\nClass equivalence, Tce and T \u2032 ce. When the axiom to add is of the form C \u2261 D, then O |= \u00ac(C \u2261 D) should be true before the edit, or O 2 C \u2261 D false. The latter is easier to test, for we can simply run aforementioned Tcs test twice, once for C v D and once for D v C: if both are true, then O |= C \u2261 D, if one of them or neither is true, then O 2 C \u2261 D. More succinctly though, one can use the following SPARQL-OWL query: Require: Test T (C \u2261 D) . i.e., test Tce 1: \u03b1\u2190 EquivalentClasses(?x D) 2: if C /\u2208 \u03b1 then . thus, O 2 C \u2261 D 3: return T (C \u2261 D) is false 4: else 5: return T (C \u2261 D) is true 6: end if\nSubsequently, it has to be added to the ontology, queried again, and evaluate to D \u2208 \u03b1. However, this works only when D is an atomic class, not when D is a complex one.\nFor T \u2032ce with individuals, we specify an extended version of T \u2032 cs as follows:\nRequire: Test T (C \u2261 D) . i.e., test T \u2032ce 1: Create a mock object a\n2: Assert C(a) 3: \u03b1\u2190 Type(?x D) 4: if a /\u2208 \u03b1 then . thus, O 2 C \u2261 D 5: Delete C(a) and a 6: return T (C \u2261 D) is false 7: else 8: Delete C(a) 9: Assert D(a) 10: \u03b1\u2190 Type(?x C) 11: if a /\u2208 \u03b1 then . thus, O 2 C \u2261 D 12: return T (C \u2261 D) is false 13: else 14: return T (C \u2261 D) is true 15: end if 16: Delete D(a) and a 17: end if\nSimple existential quantification, Teq or T \u2032 eq. Let the axiom type X be of the form C v \u2203R.D, then O 2 \u00ac(C v \u2203R.D) should be true, or O |= C v \u2203R.D false (or: not asserted) before the ontology edit. One could do a first check that D is not a descendant of R but if it is, then it may be the case that C \u2032 v \u2203R.D, with C a different class from C \u2032. This still requires one to confirm that C is not a subclass of \u2203R.D. In SPARQL-OWL, we can combine this into one query/TDD test: Require: Test T (C v \u2203R.D) . i.e., test Teq 1: \u03b1\u2190 SubClassOf(?x ObjectSomeValuesFrom(R D)) 2: if C /\u2208 \u03b1 then . thus, O 2 C v \u2203R.D 3: return T (C v \u2203R.D) is false 4: else 5: return T (C v \u2203R.D) is true 6: end if If the test passes, i.e., C /\u2208 \u03b1, then the axiom is to be added to the ontology, the query run again, and if C \u2208 \u03b1, then the test cycle is completed.\nFrom a cognitive, or modelling, viewpoint, desiring to add a CQ that amounts to C v \u2203R.\u00acD (\u2018each C has an outgoing arc R to anything that is not a D\u2019) may look different, but \u00acD \u2261 D\u2032, so it amounts to testing C v \u2203R.D\u2032, i.e., essentially the same pattern. This also can be formulated directly into a SPARQL-OWL query, encapsulated in a TDD test: Require: Test T (C v \u2203R.\u00acD) . i.e., test Teqnd 1: \u03b1\u2190 SubClassOf(?x ObjectSomeValuesFrom(R ObjectComplementOf(D))) 2: if C /\u2208 \u03b1 then . thus, O 2 C v \u2203R.\u00acD 3: return T (C v \u2203R.\u00acD) is false 4: else 5: return T (C v \u2203R.\u00acD) is true 6: end if\nIt is slightly different for C v \u00ac\u2203R.D (\u2018all Cs do not have an outgoing arc R to D\u2019). The query with TDD test is as follows: Require: Test T (C v \u00ac\u2203R.D) . i.e., test Teqnr 1: \u03b1\u2190 SubClassOf(?x ObjectComplementOf(ObjectSomeValuesFrom(R D))) 2: if C /\u2208 \u03b1 then . thus, O 2 C v \u00ac\u2203R.D 3: return T (C v \u00ac\u2203R.D) is false 4: else 5: return T (C v \u00ac\u2203R.D) is true 6: end if\nhence, then the test fails, so it can be added, the test runs gain, and then passes. The TDD test T \u2032eq with individuals can be carried out as follows. Require: Test T (C v \u2203R.D) . i.e., test T \u2032eq 1: Create two mock objects, a and b 2: Assert C(a), D(b), and R(a, b) 3: \u03b1\u2190 Type(?x, ObjectSomeValuesFrom(R D)) 4: if a /\u2208 \u03b1 then . thus, O 2 C v \u2203R.D 5: return T(C v \u2203R.D) is false 6: else 7: return T(C v \u2203R.D) is true 8: end if 9: Delete C(a), D(b), R(a, b), a, and b\nThe two negated cases are as follows. Require: Test T (C v \u2203R.\u00acD) . i.e., test T \u2032eqnd 1: Create two mock objects, a and b 2: Assert C(a), \u00acD(b), and R(a, b) 3: \u03b1\u2190 Type(?x, ObjectSomeValuesFrom(R ObjectComplementOf(D))) 4: if a /\u2208 \u03b1 then . thus, O 2 C v \u2203R.\u00acD 5: return T(C v \u2203R.\u00acD) is false 6: else 7: return T(C v \u2203R.\u00acD) is true 8: end if 9: Delete C(a), D(b), R(a, b), a, and b Require: Test T (C v \u00ac\u2203R.D) . i.e., test T \u2032eqnr 1: Create two mock objects, a and b 2: Assert C(a), D(b), and R(a, b) 3: ostate\u2190 Run the reasoner 4: if ostate == consistent then . thus, O 2 C v \u00ac\u2203R.D 5: return T(C v \u00ac\u2203R.D) is false 6: else 7: return T(C v \u00ac\u2203R.D) is true 8: end if 9: Delete C(a), D(b), R(a, b), a, and b\nSimple universal quantification, Tuq or T \u2032 uq. Let X be C v \u2200R.D, then O 2 \u00ac(C v \u2200R.D) should hold, or O |= C v \u2200R.D false (not be present in the ontology), before the ontology edit. This has a similar pattern to the one for existential quantification, Require: Test T (C v \u2200R.D) . i.e., test Tuq 1: \u03b1\u2190 SubClassOf(?x ObjectAllValuesFrom(R D)) 2: if C /\u2208 \u03b1 then . thus, O 2 C v \u2200R.D 3: return T (C v \u2200R.D) is false 4: else 5: return T (C v \u2200R.D) is true 6: end if which then can be added and the test ran again. The TDD test for T \u2032uq with individuals is much alike T \u2032eq: Require: Test T (C v \u2200R.D) . i.e., test T \u2032uq 1: Create two mock objects, a and b 2: Assert C(a), D(b), and R(a, b) 3: \u03b1\u2190 Type(?x, ObjectAllValuesFrom(R D)) 4: if a /\u2208 \u03b1 then . thus, O 2 C v \u2200R.D 5: return T(C v \u2200R.D) is false 6: else 7: return T(C v \u2200R.D) is true 8: end if 9: Delete C(a), D(b), R(a, b), a, and b"}, {"heading": "3.3 Test patterns for object propery axioms", "text": "From here onward, the tests deal with object properties more specifically\u2014 sometimes called the RBox\u2014that more often than not do not lend themselves well for easy querying in the DL query tab, for the DL query tab returns results on classes and individuals only, because DL query is essentially a class expression language, i.e., it can only express (complex) classes. However, SPARQL-OWL and the automated reasoner can be used fairly straightforwardly, so it is essentially an interface limitation that can be solved with adding a new plugin for TDD.\nDomain axiom, Tda or T \u2032 da. Let X be \u2203R v C that is not yet in O. To verify that it is not, O |= \u00ac(\u2203R v C) should be true, or O |= \u2203R v C false. With SPARQL-OWL, there are two options. First, one can query for the domain: Require: Test T (\u2203R v C) . i.e., test Tda 1: \u03b1\u2190 ObjectPropertyDomain(R ?x) 2: if C /\u2208 \u03b1 then . thus, O 2 \u2203R v C 3: return T (\u2203R v C) is false 4: else 5: return T (\u2203R v C) is true 6: end if\nAlternatively, one can query for the superclasses of \u2203R (noting that it is shorthand for \u2203R.>), where the above-listed query in the TDD is replaced with:\n\u03b1\u2190 SubClassOf(SomeValuesFrom(R Thing) ?x) Note that C \u2208 \u03b1 only will be returned if C is the only domain class of R or when C u C \u2032 (but not if it is C t C \u2032, which is a superclass of C). Alternatively, with the individuals: Require: Test T (\u2203R v C) . i.e., test T \u2032da 1: Check R \u2208 VOP and C \u2208 VC 2: Add individuals a and topObj 3: Add R(a, topObj) as object property assertion 4: Run the reasoner 5: if a /\u2208 C then . O 2 \u2203R v C (also in the strict sense as is or with a\nconjunction), hence then the test fails as intended.\n6: return T (\u2203R v C) is false 7: else 8: return T (\u2203R v C) is true 9: end if\n10: Delete individuals a and topObj\nIf the answer is empty, then R does not have any domain specified yet, and if C /\u2208 \u03b1, then O 2 \u2203R v C, so that it can be added and the test run again to complete the cycle.\nRange axiom, Tra or T \u2032 ra. When X is a range axiom, \u2203R\u2212 v D should not be in the ontology before the TDD test. This is similar to the domain axiom test: Require: Test T (\u2203R\u2212 v D) . i.e., test Tra 1: \u03b1\u2190 ObjectPropertyRange(R ?x) 2: if D /\u2208 \u03b1 then . thus, O 2 \u2203R\u2212 v D 3: return T (\u2203R\u2212 v D) is false 4: else 5: return T (\u2203R\u2212 v D) is true 6: end if\nor, in the second option, to replace the TDD query with \u03b1\u2190 SubClassOf(SomeValuesFrom(ObjectInverseOf(R) Thing) ?x) The answer will be D \u2208 \u03b1 if O |= \u2203R\u2212 v D or O |= \u2203R\u2212 v D uD\u2032, and only owl:Thing \u2208 \u03b1 if no range was declared for R. The test with individuals is as follows: Require: Test T (\u2203R\u2212 v D) . i.e., test T \u2032ra 1: Check R \u2208 VOP and D \u2208 VC 2: Add individual a and topObj to the ABox 3: Add R(topObj, a) as object property assertion 4: if a /\u2208 D then . O 2 \u2203R\u2212 v D, as intended 5: return T (\u2203R\u2212 v D) is false 6: else\n7: return T (\u2203R\u2212 v D) is true 8: end if 9: Delete R(topObj, a) and individuals a and topObj\nObject property subsumption and equivalence, Tps and Tpe, and T \u2032 ps and T \u2032pe. When axiom type X is a property subsumption, R v S, then we have to test that O |= \u00ac(R v S), or that R v S fails. The SPARQL-OWL query in the Tps(R v S) is as follows: Require: Test T (R v S) . i.e., test Tps 1: \u03b1\u2190 SubObjectPropertyOf(?x S) 2: if R /\u2208 \u03b1 then . thus, O 2 R v S 3: return T (R v S) is false 4: else 5: return T (R v S) is true 6: end if\nOne cannot query this as such in Prote\u0301ge\u0301\u2019s DL query tab, other than by using individuals, i.e., a version of T \u2032ps. With the OWL semantics, for R v S to hold, it means that, given some individuals a and b, that if R(a, b) then S(a, b). The desired result is computed by the reasoner anyhow. The steps for the TDD test: Require: Test T (R v S) . i.e., test T \u2032ps 1: Check R,S \u2208 VOP 2: Add individuals a, b to the ABox, add R(a, b) 3: Run the reasoner 4: if S(a, b) /\u2208 \u03b1 then . (practically: not shown in the \u201cproperty assertions\u201d in the\nindividuals tab for a, with \u201cShow inferences\u201d checked), thus O 2 R v S 5: return T (R v S) is false 6: else 7: return T (R v S) is true 8: end if 9: Delete R(a, b), and individuals a and b\nThen the modeller would add R v S, run the test again, and it should then infer S(a, b), as O |= R v S. This, however, does not guarantee R v S was added, and not inadvertently R \u2261 S. Their difference can be easily observed with the following set-up: Require: Test T (R \u2261 S) . i.e., test T \u2032pe 1: Check R,S \u2208 VOP 2: Add mock individuals a, b, c, d to the ABox 3: Add R(a, b) and S(c, d) as object property assertion 4: Run the reasoner 5: if S(a, b) \u2208 \u03b1 and R(c, d) /\u2208 \u03b1 then . O |= R v S, hence the ontology edit\nexecuted correctly\n6: return T (R \u2261 S) is false 7: else . i.e. {S(a, b), R(c, d)} \u2208 \u03b1, so O |= R \u2261 S 8: T (R \u2261 S) is true"}, {"heading": "9: end if", "text": "10: Delete R(a, b) and S(c, d), and a, b, c, d\nFor object property equivalence at the Tbox level, i.e., R \u2261 S, one could use Tps twice, or simply use the EquivalentObjectProperties with SPARQL-OWL: Require: Test T (R \u2261 S) . i.e., test Tpe 1: \u03b1\u2190 EquivalentObjectProperties(?x S) 2: if R /\u2208 \u03b1 then . thus, O 2 R \u2261 S 3: return T (R \u2261 S) is false 4: else 5: return T (R \u2261 S) is true 6: end if\nObject property inverses, Tpi and T \u2032 pi. There are two options here since OWL 2, and it is hard to choose which one is \u2018better\u2019: using explicit inverses tends to be chosen for understanding (e.g., teaches with inverse declared explicitly as taught by), whereas using an \u2018implicit\u2019 inverse (e.g., teaches and teaches\u2212) improved reasoner performance in at least one instance [15]. Practically, for the failure-test of TDD, we can test only the former case, as the latter is only used in axioms. Also here one can choose between a TBox or an ABox approach. The TBox approach with a SPARQL-OWL query for Tpi(R v S\u2212): Require: Test T (R v S\u2212) . i.e., test Tpi 1: \u03b1\u2190 InverseObjectProperties(?x S) 2: if R /\u2208 \u03b1 then . thus, O 2 R v S\u2212 3: return T (R v S\u2212) is false 4: else 5: return T (R v S\u2212) is true 6: end if\nUsing the Abox, we again have to work with mock objects: Require: test T (R v S\u2212) . i.e., test T \u2032pi 1: Check R,S \u2208 VOP 2: . Assume S is intended to be the inverse of R (with R and S having different\nnames), and we check for its absence:\n3: Add mock individuals a, b to the ABox 4: Add R(a, b) as object property assertion 5: Run the reasoner 6: if O 2 S(b, a) then . O 2 R v S\u2212, hence the test fails, as intended 7: return T (R v S\u2212) is false 8: else 9: return T (R v S\u2212) is true\n10: end if 11: Delete mock individuals a and b\nThen add R v S\u2212, run the test again, which then should evaluate to true.\nObject property chain, Tpc or T \u2032 pc. When X is one of the permissible chains (except for transitivity; see below), such asR\u25e6S v S, S\u25e6R v S,R\u25e6S1\u25e6...\u25e6Sn v S (with n > 1). This is increasingly more cumbersome to test, for the simple fact that many more entities are involved, hence, more opportunity to have incomplete knowledge represented in the ontology and, hence, more hassle to find all the permutations that lead to not having the desired effect. Perhaps the easiest way to check whether the ontology has the property chain, is to search the owl file for owl:propertyChainAxiom, with the relevant properties included in order, or, for that matter, the SPARQL-OWL query in the TDD test: Require: Test T (R \u25e6 S v S) . i.e., test Tpc 1: \u03b1\u2190 SubObjectPropertyOf(ObjectPropertyChain(R S) ?x) 2: if S /\u2208 \u03b1 then . thus, O 2 R \u25e6 S v S 3: return T (R \u25e6 S v S) is false 4: else 5: return T (R \u25e6 S v S) is true 6: end if\nand similarly with the other permutations. However, simply checking the owl file or executing the query misses three aspects of chains: i) there is no point in having a property chain if the properties involved are never used in the intended way anyway, 2) this cannot ascertain that it does only what was intended, and 3) whether the chain is allowed in OWL 2, i.e., not violating \u2018interfering\u2019 constraints (the properties have to be \u2018simple\u2019). For O |= R \u25e6S v S to be interesting for the ontology, also at least one O |= C v \u2203R.D and one O |= D v \u2203S.E should be present. If they all were, then a SPARQL-OWL query\n\u03b1\u2190 SubClassOf(?x ObjectSomeValuesFrom(S E)) will have C \u2208 \u03b1. If either of the three axioms are not present, then C /\u2208 \u03b1. From the perspective of the ABox testing, we need the following sequence of steps: Require: Test T (R \u25e6 S v S) . i.e., test T \u2032pc 1: Check R,S \u2208 VOP and C,D,E \u2208 VC 2: if C,D,E /\u2208 VC then 3: Add the missing class(es) (C, D, and/or E) as mock classes 4: end if 5: Run the test Teq or T \u2032 eq, for both C v \u2203R.D and for D v \u2203S.E\n6: if Teq is false then 7: Add C v \u2203R.D, D v \u2203S.E, or both, as mock axiom 8: end if 9: if O |= C v \u2203S.D then . then test is meaningless, for it would not test the\nproperty chain\n10: Add mock class C \u2032, mock axiom C \u2032 v \u2203R.D 11: Verify with Teq or T \u2032 eq 12: \u03b1\u2190 SubClassOf(?x ObjectSomeValuesFrom(S E)) 13: if C \u2032 /\u2208 \u03b1 then . thus, O 2 R \u25e6 S v S 14: return T (R \u25e6 S v S) is false 15: else 16: return T (R \u25e6 S v S) is true\n17: end if 18: else . so, O 2 C v \u2203S.D 19: \u03b1\u2190 SubClassOf(?x ObjectSomeValuesFrom(S E)) 20: if C /\u2208 \u03b1 then . thus, O 2 R \u25e6 S v S 21: return T (R \u25e6 S v S) is false 22: else 23: return T (R \u25e6 S v S) is true 24: end if 25: end if 26: Delete all mock objects, classes, and axioms Assuming that the test fails, i.e., C /\u2208 \u03b1 (resp. C \u2032 /\u2208 \u03b1) and thus O 2 R \u25e6 S v S, then add the property chain and run the test again, which then should pass (i.e., C \u2208 \u03b1). When it does, any mock classes and axioms should be removed.\nThe procedure holds similarly for the other permissible combinations of object properties in a property chain/complex role inclusion.\nObject property characteristics, Tpx . Testing absence/presence of the object property characteristics is surely feasible with mock individuals in the ABox, but is doable only for transitivity and local reflexivity in the TBox.\nR is functional, T \u2032pf , i.e., some object has at most one individual R-successor. The TDD test procedure is as follows. Require: Test T (Func(R)) . i.e., test T \u2032pf 1: Check R \u2208 VOP and a, b, c \u2208 VI ; if not present, add. 2: Assert mock axioms R(a, b), R(a, c), and b 6= c, if not present already. 3: Run reasoner 4: if O is consistent then . thus, O 2 Func(R) 5: return T (Func(R)) is false 6: else . O is inconsistent 7: return T (Func(R)) is true 8: end if 9: Remove mock axioms and individuals, as applicable\nR is inverse functional, T \u2032pif . This is as above, but then in the other direction, i.e., R(b, a), R(c, a) with b, c declared distinct, will result in a consistent ontology when O 2 InvFun(R). Thus: Require: Test T (InvFun(R)) . i.e., test T \u2032pif 1: Check R \u2208 VOP and a, b, c \u2208 VI ; if not present, add. 2: Assert mock axioms R(b, a), R(c, a), and b 6= c, if not present already. 3: Run reasoner 4: if O is consistent then . thus, O 2 InvFun(R) 5: return T (InvFun(R)) is false 6: else . O is inconsistent 7: return T (InvFun(R)) is true 8: end if 9: Remove mock axioms and individuals, as applicable\nR is transitive, Tpt or T \u2032 pt , so that with R(a, b) and R(b, c), it will infer R(a, c). As with the object property chain test (Tpc), this object property characteristic is only \u2018interesting\u2019 for the ontology engineer if there are at least two related axioms so that one obtains a non-empty deduction thanks to the transitive object property. Further, it is the only object property characteristic that has a real effect in the TBox. If the relevant axioms are not asserted, they have to be added.\nRequire: Test T (Trans(R)) . i.e., test Tpt 1: Check R \u2208 VOP and C,D,E,\u2208 VC 2: if C,D,E, /\u2208 VC then 3: Add the missing class(es) (C, D, and/or E as mock classes) 4: end if 5: if C v \u2203R.D and D v \u2203R.E are not asserted then 6: add C v \u2203R.D and D v \u2203R.E to O 7: end if 8: \u03b1\u2190 SubClassOf(?x ObjectSomeValuesFrom(R E)) 9: if C /\u2208 \u03b1 then . thus, O 2 Trans(R) 10: return T (Trans(R)) is false 11: else 12: return T (Trans(R)) is true 13: end if 14: Remove mock classes and axioms, as applicable\nThe ABox-based test is as follows. Require: Test T (Trans(R)) . i.e., test T \u2032pt 1: Check R \u2208 VOP , a, b, c \u2208 VI . If not, introduce a, b, c as mock objects. 2: Assert mock axioms R(a, b) and R(b, c), if not present already. 3: Run reasoner 4: if R(a, c) /\u2208 \u03b1 then . thus, O 2 Trans(R) 5: return T (Trans(R)) is false 6: else 7: return T (Trans(R)) is true 8: end if 9: Remove mock individuals\nR is symmetric, T \u2032ps , so that with R(a, b), it will infer R(b, a). The test-tofail\u2014assuming R \u2208 VOP \u2014is as follows. Require: Test T (Sym(R)) . i.e., test T \u2032ps 1: Check R \u2208 VOP . Introduce a, b as mock objects (a, b \u2208 VI). 2: Assert mock axiom R(a, b). 3: \u03b1\u2190 ObjectPropertyAssertion(R x? a) 4: if b /\u2208 \u03b1 then . thus, O 2 Sym(R) 5: return T (Sym(R)) is false 6: else 7: return T (Sym(R)) is true 8: end if 9: Remove mock assertions and individuals\nAlternative to the query in line 3, one can check in the ODE whether R(b, a) is inferred (yellow in the Prote\u0301ge\u0301 Individuals tab).\nR is asymmetric, T \u2032pa . This is easier to test with the negative, i.e., assert objects symmetrically and distinct, and if the ontology is not inconsistent, then O 2 Asym(R). More precisely: Require: Test T (Asym(R)) . i.e., test T \u2032pa 1: Check R \u2208 VOP . Introduce a, b as mock objects (a, b \u2208 VI). 2: Assert mock axioms R(a, b) and R(b, a). 3: Run reasoner 4: if O not inconsistent then . thus, O 2 Asym(R) 5: return T (Asym(R)) is false 6: else 7: return T (Asym(R)) is true 8: end if 9: Remove mock axioms and individuals\nR is reflexive, T \u2032prg or T \u2032 prg . The object property can be either globally reflexive (Ref(R)), or locally (C v \u2203R.Self). Global reflexivity is typically not what one wants, but if in the exceptional case the modeller does, then the following test should be executed. Require: Test T (Ref(R)) . i.e., test T \u2032prg 1: Check R \u2208 VOP . 2: Introduce a as mock objects (a \u2208 VI). 3: Run the reasoner 4: if R(a, a) /\u2208 O then . thus, O 2 Ref(R) 5: return T (Ref(R)) is false 6: else 7: return T (Ref(R)) is true 8: end if 9: Remove mock object a\nAnd adding Ref(R) will have the test evaluate to true. Local reflexivity uses Self in an axiom; that is, we need to check whether O |= C v \u2203R.Self. This is essentially the same as Teq but then with Self instead of the explicit class, i.e.: Require: Test T (C v \u2203R.Self) . i.e., test Tprl 1: \u03b1\u2190 SubClassOf(?x ObjectSomeValuesFrom(R Self)) 2: if C /\u2208 \u03b1 then . thus, O 2 C v \u2203R.Self 3: return T (C v \u2203R.Self) is false 4: else 5: return T (C v \u2203R.Self) is true 6: end if\nor, in ABox variant: Require: Test T (C v \u2203R.Self) . i.e., test T \u2032prl 1: Check R \u2208 VOP . Introduce a as mock objects (a \u2208 VI). 2: Assert mock axiom C(a) 3: \u03b1\u2190 Type(?x C), PropertyValue(a R ?x)\n4: if a /\u2208 \u03b1 then . thus, O 2 C v \u2203R.Self 5: return T (C v \u2203R.Self) is false 6: else 7: return T (C v \u2203R.Self) is true 8: end if 9: Remove C(a) and mock object a\nR is irreflexive, T \u2032pir . As with asymmetry, this is easier to test with the converse: assert R(a, a), run the reasoner, then the ontology is consistent (i.e., then O 2 Irr(R)). Add Irr(R), run the reasoner, then O |= \u22a5, and finally remove mock individual and assertion. Require: Test T (Irr(R)) . i.e., test T \u2032pi 1: Check R \u2208 VOP , and add a \u2208 VI 2: Assert mock axiom R(a, a) 3: Run reasoner 4: if O is consistent then . thus, O 2 Irr(R) 5: return T (Irr(R)) is false 6: else . O is inconsistent 7: return T (Irr(R)) is true 8: end if 9: Remove mock axiom and individual, as applicable\nThis concludes the basic tests. While the logic permits that some class C on the left-hand side of the inclusion axiom may be a complex and defined concept, we do not consider such cases here, as due to the tool design of the most widely used ODE, Prote\u0301ge\u0301, the left-hand side of the inclusion axiom has only a single class C."}, {"heading": "4 Experimental evaluation with a Prote\u0301ge\u0301 plugin for TDD", "text": "We describe the design of the plugin and evaluation of the performance tests in this section."}, {"heading": "4.1 Design", "text": "In order to support ontology engineers in performing TDD, we have implemented the Prote\u0301ge\u0301 plugin named TDDOnto. The plugin provides a view where the user may specify the set of tests to be run. After their execution, the status of the tests is displayed. It is also possible to add a selected axiom to the ontology (and re-run the test). Fig. 1 presents the screenshot of the TDDOnto plugin."}, {"heading": "4.2 Evaluation of the TBox and ABox based TDD tests", "text": "The aim of the evaluation is to assess which option is the more feasible one. This can be done from a plugin usability viewpoint and from the algorithmic\nviewpoint, in the sense of which type of test is faster (though slow response times slow down the ontology authoring process and is also a dimension of usability). We take the latter option here. The question, then, is Which TDD approach\u2014 queries or mock objects\u2014is better? We describe the set-up of the experiment first, and then proceed to the results and discussion.\nSet-up of the experiment. To assess this quantitatively, we pose the following two general hypotheses: H1: Query-based TDD is faster overall. H2: Classification time of the ontology contributes the most to overall perfor-\nmance (time) of a TDD test. The reason why we think that H1 will hold is because once classified, one can query multiple times without having to classify the ontology again, and for some mock-object TDD tests the ontology should be inconsistent, which is a more cumbersome step to deal with than checking membership of a class or individual in a query answer. The reason for expecting H2 to hold is that the other operations\u2014adding and removing entities, testing for membership\u2014can be executed in linear time, whereas there are not many ontologies in a language that is linear or less in data complexity. These general hypotheses can be refined to suit statistical tests for each hypotheses: H10: There is no difference between query-based and mock-object based TDD\ntests. H1a: There is a difference, with query-based having lower values for time taken\nto execute the tests. H20: TDD overall execution times are arbitrarily subdivided into ontology clas-\nsification time and the TDD test part. H2a: There is a difference, with ontology classification time taking much more\n(>> 50%) of the TDD overall execution times than the TDD test part. The performance is expected to depend on the ontology\u2019s content that is being revised, as reasoning time does. It is unclear whether the overall TDD test execution time and what is attributed to plain ontology classification\u2014also depends on the characteristics of the ontology. If it does, it is due to something\ninternal to the reasoner, to which we do not have access. Notwithstanding, we would like to obtain an indication whether there might be interference regarding this aspect. Therefore we categorise test ontologies into groups depending on the number of their axioms.\nTesting for/adding the axioms to the selected ontologies can be done in two ways: adding new elements, or reusing elements of the ontology. The former is certainly easier to carry out, the latter is truer to the intended use of a TDD test. In the experiments we followed the latter option and randomly selected existing ontology classes and properties for the tests.\nMaterials and methods. The results presented in this section are computed based on the tests perfomed on 67 OWL ontologies from the TONES repository 7, downloaded from the mirror available at OntoHub 8. We selected those ontologies from all available TONES ontologies, while omitting the other ones that were either not in OWL (but in OBO format) or were having datatypes incompatible with OWL 2, causing exceptions of the reasoner or out of memory exceptions. The results were computed on Mac Book Air with 1.3 GHz Intel Core i5 CPU and 4 GB RAM. As an OWL reasoner, we used the same reasoner that is built-in into OWL-BGP, namely HermiT 1.3.8.\nThe tests were generated randomly, and each test kind was repeated 3 times to obtain more reliable results. We divided our ontologies into 4 groups, depending on the overall number of their axioms: up to 100 (20 ontologies), 100\u20131000 axioms (35 ontologies), 1000\u201310,000 axioms (10 ontologies), and over 10,000 (2 ontologies). All the experimental results are available at https://semantic.cs. put.poznan.pl/wiki/aristoteles/doku.php.\nResults and discussion. During our experiments we also found out that not all the features of OWL 2 are covered by OWL-BGP. In particular, RBox axioms (e.g., subPropertyOf) and property characteristics were not handled. Therefore, we only present the comparative results of the tests that could be run in both settings: ABox tests and TBox SPARQL-OWL tests.\nThe statistics are presented in the Fig 2, where X axis presents the groups of the ontologies (the ranges of the minimum and maximum number of the axioms each ontology in the group has). Note that the Y axis is scaled logarithmically. On the figure, there is a box plot presenting for every group of ontologies: the median m (horizontal line within the box); the first and third quartile (bottom and top line of the box); the lowest value above m\u2212 1.5 \u00b7 IQR (short horizontal line below the box), the highest value below m+ 1.5 \u00b7 IQR (short horizontal line above the box), where IQR (interquartile range) is represented with the height of the box; and outliers (points above and below of the short lines).\nFrom the results as displayed in Fig. 2, it follows that TBox (SPARQL-OWL) tests are generally faster than the ABox ones, and these differences are larger\n7 http://rpc295.cs.man.ac.uk:8080/repository/browser 8 https://ontohub.org/repositories\nin the sets of larger ontologies. It is also apparent that the ontology classification times are large\u2014in fact, higher on average\u2014in comparison to the times of running the test.\nIn the Fig. 3, we present the running times per test type and the kind of the tested axiom. Again, the TBox based method is generally faster, with an exception of testing disjointness.\nWe have also tested two alternative TBox querying approaches (based on SPARQL-OWL and based on using OWL API and the reasoner). The results of this comparison between SPARQL-OWL and OWL API are presented in Figures 4 and 5 and showing even better performance of the TBox TDD tests."}, {"heading": "5 Discussion", "text": "One might have wondered: why not simply browse the ontology to see if the axiom is already there? There are several reasons that the browsing-way-out is not ideal. First, then one does not know the implications (unless first classified). Second, browsing large ontologies is cumbersome, and more easily results in cognitive overload that hampers speedy ontology development. Third, the browsing is purely manual checking, making it easy to overlook something. The process of finding and adding an axiom is amenable to automation that solves these three issues. Instead of manual checks and sample axioms that have to be all manually added and possibly removed during the regular \u2018add something and lets see what happens\u2019, this can be managed in one fell swoop. In addition, the TDD tests give also the convenience and benefit of systematic regression testing. There are some hurdles to it realisation, however, which we discuss in the next subsection. Afterward, we outline how a TDD methodology for ontology engineering may look like in general terms."}, {"heading": "5.1 Reflections on specifying and implementing a TDD tool", "text": "Aside from the consideration whether one would have to go the way of epistemic queries, the first main aspect was how to specify the TBox tests. For instance, some tests can be done easily with the DL query tab in Prote\u0301ge\u0301; e.g. Tcs\u2019s SPARQL-OWL query amounts to either:\nC select Super classes\nor, in the other direction: D select Sub classes without even having to face the consideration of having blank nodes/unnamed classes (complex class expressions) on the right-hand-side of the inclusion axiom that was not supported in Kollia et al\u2019s BGP [18]. The DL Query tab interface does not have some of the required functionality, notably regarding tests for object properties, it would have to be redesigned to function as TDD test interface anyway, and it is not ideal for TDD test performance testing due to unclear times taken up by the Prote\u0301ge\u0301 interface processing. Therefore, these DL Query tab specifications have been omitted from this paper.\nThe more principled aspect underlying the TDD test realisation, however, is the technique to obtain the answer of a TDD test: SPARQL SELECT-queries, SPARQL-OWL\u2019s BGP that uses SPARQL answering engine and HermiT v1.3.8, SPARQL-DL with ASK queries also using the OWL API and one of the OWL 2 DL reasoners. While the difference in performance between the ABox test and TBox tests are generally explainable\u2014the former always modifies the ontology, so requires an extra classification step\u2014it is not for the outlier (disjointness) or why in some cases the difference is larger (subsumption, equivalence) than\nin others (queries with quantifiers). Further, it may be the case that overall performance may be different when a different reasoner is used, as reasoners do differ [23]. Likewise, we observed a trend towards bigger differences ABox vs SPARQL testing with larger ontologies. We do not aim to address this issue here, but note it for further investigation into the matter.\nA related issue is the maturity of the tools used for the performance evaluation. Of the ontologies selected for testing, several returned errors, which were due to incompatible data types of an OWL DL ontology with OWL 2 DLtailored tools. Further, especially querying in the context of the TDD tests for object properties faced limitations, as most of the required features were not implemented9. This forced us to redesign the experiment into one of \u2018test what can be done\u2019 now and infer tendencies from that so as to have a solid, experimentally motivated, basis for deciding which technique likely will have the best chance\n9 it is, however, possible to carry out the sequence of each of the ABox test \u2018manually\u2019 by adding the individuals, relations, run the reasoner and check the instance classification results.\nof success, hence, the best candidate for extending the corresponding tool. This appeared to be indeed preferring TBox tests over ABox tests, although most of the RBox test will have to be carried out as ABox tests.\nFinally, once all test are implemented and a multi-modal interface developed to cater for the three principal use case scenarios and any other requirements emanating from the TDD methodology (see next section), user evaluations are to be conducted to evaluate whether also for ontology engineering the TDD benefits can be reaped, as observed for conceptual modelling and software development."}, {"heading": "5.2 A step toward a TDD ontology engineering methodology", "text": "A methodology is a structured collection of, principally, methods and techniques, processes, people having roles possibly in teams, and quality measures and standards across the process (see, e.g., [4]), and has been shown to improve the overall results compared to doing something without a methodology. This is not to say that when a full-fledged TDD ontology engineering methodology has been developed, it should be the ontology engineering methodology. Like for software engineering\u2019s \u2018methodology selection grid\u2019 [4], it will exceedingly suit some ontology development projects but perhaps not others. Here, with the previously described test specifications and their experimental evaluation only, we do not purport to have a full TDD methodology, but a foundational step in that direction that indicates where and how it differs in design compared to the typical waterfall, iterative, or lifecycle-based methodologies. We rework the software development TDD procedure (recall Section 2) into a sequence of steps applicable to ontology engineering, as follows: 1. Choose the usage scenario as outlined in Section 1: CQ-driven TDD (for-\nmalised CQ specification translated into an axiom); Authoring-driven knowledge engineer (the axiom one wants to add), or Authoring-driven domain expert (a selected template populated); 2. (Have) Select(ed) a test for the axiom; 3. Run test to check that it fails; 4. Write relevant knowledge in the ontology that should pass the test, i.e.,\nadd classes, object properties, or axioms, as applicable (this may simply be clicking a button to add the formalised CQ, the provided axiom, or the filled-in template); 5. Classify the ontology to check that nothing contradictory was added; 6. Run the same test as before, to verify it passes; 7. Optionally refactor the formally represented knowledge (including classifica-\ntion and checking there are no undesirable deductions, and possibly asserting the implicit knowledge); 8. Run all tests to verify that the changes to the ontology did not change the intended deductions from the ontology (regression testing); resolve any conflicting axioms or CQs (possibly by another TDD cycle). A sketch of the process is depicted in Fig. 6. It is possible to refine these steps further, such as a way to manage the deductions following from having added new knowledge and how to handle an inconsistency or undesirable deduction\ndue to contradictory CQs (alike resolving conflicting requirements in software engineering) that may surface in steps 5 and 8. These detailed aspects are left for future work."}, {"heading": "5.3 Answering the research questions", "text": "The answers to the questions posed in Section 1 can be summarised as follows. The first question was formulated rather broadly: Given the TDD procedure in software engineering, then what does that mean for ontology testing when transferred to ontology development? The main answer to this is the specification of tests for each type of axiom one can add to the ontology, which can be realised in different ways, namely, queries over the TBox and through individuals in the ABox. While the general idea is thus the same\u2014requirement, test specification, test fails, change something, run test again to check it passes\u2014the means of conducting a test for ontologies is thus different. One does not check code \u2018functionality\u2019 but whether the some piece of knowledge is present and represented in the way as intended.\nRegarding the second question TDD requires so-called mock objects for \u2018incomplete\u2019 parts of the code, and mainly for methods; is there a parallel to it in ontology development, or can that aspect of TDD be ignored? can be answered in\nthe affirmative. In particular for the general ABox tests and the so-called RBox, this \u2018mock\u2019 thing had to be refined into mock individuals, mock classes, and mock axioms. The experimental evaluation showed this approach is not as fast as TBox tests, but there is no avoiding some ABox test especially when testing most of the object property characteristics.\nLast, In what way, and where, (if at all) can this be integrated as a methodological step in existing ontology engineering methodologies that are typically based on waterfall, iterative, or lifecycle principles rather than agile methodologies? TDD ontology engineering, like TDD for software development, has its own procedure. While some aspects overlap, such as CQs (requirements), and the \u2018formalisation\u2019 step in methontology [5] (writing the axioms, by the knowledge engineer), both can be avoided as well: the former by the engineering, the latter by the domain expert. With some stretching of the notion of \u2018lifecycle\u2019, the lifecycle is one of a TDD cycle only, which can be part of a larger lifecycle of multiple TDD cycles. There is no single neat \u2018plug-in\u2019 point for TDD into the waterfall and iterative methodologies, however."}, {"heading": "6 Conclusions", "text": "This paper introduced 36 tests for Test-Driven Development of ontologies, specifying what has to be tested, and how. Tests were specified both at the TBox-level with queries and for ABox individuals, using mock objects. The implementation of the main tests demonstrated that the TBox test approach performs better.A high-level 8-step process for TDD ontology engineering was proposed. Future work pertains to extending tools to also implement the remaining tests, elaborate on the methodology, and conduct use-case evaluations.\nAcknowledgments This research has been supported by the National Science Centre, Poland, within the grant number 2014/13/D/ST6/02076."}], "references": [{"title": "The RapidOWL methodology\u2013towards Agile knowledge engineering", "author": ["S. Auer"], "venue": "Enabling Technologies: Infrastructure for Collaborative Enterprises, 2006. WETICE \u201906. 15th IEEE International Workshops on. pp. 352\u2013357", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Test-Driven Development: by example", "author": ["K. Beck"], "venue": "Addison-Wesley, Boston, MA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Ontology testing \u2013 methodology and tool", "author": ["E. Blomqvist", "A. Sepour", "V. Presutti"], "venue": "18th International Conference on Knowledge Engineering and Knowledge Management (EKAW\u201912). LNAI, vol. 7603, pp. 216\u2013226. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting a project\u2019s methodology", "author": ["A. Cockburn"], "venue": "IEEE Software 17(4), 64\u201371", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Building a chemical ontology using METHONTOLOGY and the ontology design environment", "author": ["M. Fern\u00e1ndez", "A. G\u00f3mez-P\u00e9rez", "A. Pazos", "J. Pazos"], "venue": "IEEE Expert: Special Issue on Uses of Ontologies January/February, 37\u201346", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Advocatus diaboli exploratory enrichment of ontologies with negative constraints", "author": ["S. Ferr\u00e9", "S. Rudolph"], "venue": "18th International Conference on Knowledge Engineering and Knowledge Management (EKAW\u201912). LNAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Ontology design patterns", "author": ["A. Gangemi", "V. Presutti"], "venue": "Staab, S., Studer, R. (eds.) Handbook on Ontologies, pp. 221\u2013243. Springer Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "OntologyTest: A tool to evaluate ontologies through tests defined by the user", "author": ["S. Garca-Ramos", "A. Otero", "M. Fern\u00e1ndez-L\u00f3pez"], "venue": "10th International Work-Conference on Artificial Neural Networks, IWANN 2009 Workshops, Proceedings, Part II. LNCS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Developing ontologies within decentralized settings", "author": ["A. Garcia", "K. O\u2019Neill", "L.J. Garcia", "P. Lord", "R. Stevens", "O. Corcho", "F. Gibson"], "venue": "Chen, H., et al. (eds.) Semantic e-Science. Annals of Information Systems 11, pp. 99\u2013139. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "The evolution of Prot\u00e9g\u00e9: an environment for knowledge-based systems development", "author": ["J.H. Gennari", "M.A. Musen", "R.W. Fergerson", "W.E. Grosso", "M. Crub\u00e9zy", "H. Eriksson", "N.F. Noy", "S.W. Tu"], "venue": "International Journal of Human-Computer Studies 58(1), 89\u2013123", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Moki: The enterprise modelling wiki", "author": ["C. Ghidini", "B. Kump", "S. Lindstaedt", "N. Mabhub", "V. Pammer", "M. Rospocher", "L. Serafini"], "venue": "Proceedings of the 6th Annual European Semantic Web Conference", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Closed world reasoning in the semantic web through epistemic operators", "author": ["S. Grimm", "B. Motik"], "venue": "CEUR-WS, vol", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Information Modeling and Relational Databases", "author": ["T. Halpin"], "venue": "San Francisco: Morgan Kaufmann Publishers", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Software architecture improvement through test-driven development", "author": ["D.S. Janzen"], "venue": "Companion to 20th ACM SIGPLAN Conference 2005. pp. 240\u2013241. ACM Proceedings", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploring reasoning with the DMOP ontology", "author": ["C.M. Keet", "C. d\u2019Amato", "Z. Khan", "A. Lawrynowicz"], "venue": "(eds.) 3rd Workshop on Ontology Reasoner Evaluation (ORE\u201914). CEUR-WS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Ontology authoring with FORZA", "author": ["C.M. Keet", "M.T. Khan", "C. Ghidini"], "venue": "Proceedings of the 22nd ACM international conference on Conference on Information & Knowledge Management", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Mock object models for test driven development", "author": ["T. Kim", "C. Park", "C. Wu"], "venue": "Proceedings of the Fourth International Conference on Software Engineering Research, Management and Applications (SERA06). IEEE Computer Society", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "SPARQL Query Answering over OWL Ontologies", "author": ["I. Kollia", "B. Glimm", "I. Horrocks"], "venue": "Proceedings of the 8th Extended Semantic Web Conference (ESWC\u201911). LNCS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Test-driven evaluation of linked data quality", "author": ["D. Kontokostas", "P. Westphal", "S. Auer", "S. Hellmann", "J. Lehmann", "R. Cornelissen", "A. Zaveri"], "venue": "Proceedings of the 23rd international conference on World Wide Web (WWW\u201914). pp. 747\u2013758. ACM proceedings", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparative study of test driven development with traditional techniques", "author": ["S. Kumar", "S. Bansal"], "venue": "International Journal of Soft Computing and Engineering 3(1), 352\u2013360", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Extreme Programming Examined, chap", "author": ["T. Mackinnon", "S. Freeman", "P. Craig"], "venue": "Endo-testing: unit testing with mock objects, pp. 287\u2013301. Addison-Wesley, Boston, MA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Epistemic querying of OWL knowledge bases", "author": ["A. Mehdi", "S. Rudolph", "S. Grimm"], "venue": "Antoniou, G., Grobelnik, M., Simperl, E.P.B., Parsia, B., Plexousakis, D., Leenheer, P.D., Pan, J.Z. (eds.) The Semantic Web: Research and Applications - 8th Extended Semantic Web Conference, ESWC 2011, Heraklion, Crete, Greece, May 29-June 2, 2011, Proceedings, Part I. Lecture Notes in Computer Science, vol. 6643, pp. 397\u2013409. Springer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "The OWL Reasoner Evaluation (ORE) 2015 competition report", "author": ["B. Parsia", "N. Matentzoglu", "R. Goncalves", "B. Glimm", "A. Steigmiller"], "venue": "Proceedings of the 11th International Workshop on Scalable Semantic Web Knowledge Base Systems (SSWS\u201915). CEUR-WS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Aspect OntoMaven - aspect-oriented ontology development and configuration with OntoMaven", "author": ["A. Paschke", "R. Schaefermeier"], "venue": "Tech. Rep. 1507.00212v1,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Validating ontologies with OOPS", "author": ["M. Poveda-Villal\u00f3n", "M.C. Su\u00e1rez-Figueroa", "A. G\u00f3mez-P\u00e9rez"], "venue": "International Conference on Knowledge Engineering and Knowledge Management (EKAW\u201912). LNAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "extreme design with content ontology design patterns", "author": ["V. Presutti", "E Daga"], "venue": "Proc. of WS on OP\u201909. CEUR-WS, vol. 516, pp. 83\u201397", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "A library of ontology design patterns: reusable solutions for collaborative design of networked ontologies", "author": ["V. Presutti", "A. Gangemi", "S. David", "G.A. de Cea", "M.C. Surez-Figueroa", "E. MontielPonsoda", "M. Poveda"], "venue": "NeOn deliverable D2.5.1, NeOn Project, Institute of Cognitive Sciences and Technologies (CNR)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards competency question-driven ontology authoring", "author": ["Y. Ren", "A. Parvizi", "C. Mellish", "J.Z. Pan", "K. van Deemter", "R. Stevens"], "venue": "Extended Semantic Web Conference (ESWC\u201914). LNCS, Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Metrics for test case design in test driven development", "author": ["D.P. Shrivastava", "R. Jain"], "venue": "International Journal of Computer Theory and Engineering, 2(6), 952\u2013956", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "SPARQL-DL: SPARQL Query for OWL-DL", "author": ["E. Sirin", "B. Parsia"], "venue": "Proceedings of the Third International Workshop OWL: Experiences and Directions (OWLED\u201907). CEUR-WS", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "NeOn methodology for building contextualized ontology networks", "author": ["M.C. Su\u00e1rez-Figueroa", "G.A. de Cea", "C. Buil", "K. Dellschaft", "M. Fern\u00e1ndez-Lopez", "A. Garcia", "A. G\u00f3mez-P\u00e9rez", "G. Herrero", "E. Montiel-Ponsoda", "M. Sabou", "B. Villazon-Terrazas", "Z. Yufei"], "venue": "NeOn Deliverable D5.4.1, NeOn Project", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Moda: Automated test generation for database applications via mock objects", "author": ["K. Taneja", "Y. Zhang", "T. Xie"], "venue": "Proceedings of the IEEE/ACM international conference on Automated software engineering (ASE\u201910)). pp. 289\u2013292. ACM proceedings", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "An approach to testing conceptual schemas", "author": ["A. Tort", "A. Oliv\u00e9"], "venue": "Data & Knowledge Engineering 69, 598\u2013618", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "An approach to test-driven development of conceptual schemas", "author": ["A. Tort", "A. Oliv\u00e9", "M.R. Sancho"], "venue": "Data & Knowledge Engineering 70, 1088\u20131111", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Unit tests for ontologies", "author": ["D. Vrande\u010di\u0107", "A. Gangemi"], "venue": "OTM workshops 2006. LNCS, vol. 4278, pp. 1012\u20131020. Springer", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 30, "context": "The process of ontology development has progressed much over the past 20 years, aided by development information systems-oriented methodologies, such as methontology for a single-person/group monolithic ontology to ontology networks with NeOn [31], and both stand-alone and collaborative tools, such as Prot\u00e9g\u00e9 [10] and MoKI [11].", "startOffset": 243, "endOffset": 247}, {"referenceID": 9, "context": "The process of ontology development has progressed much over the past 20 years, aided by development information systems-oriented methodologies, such as methontology for a single-person/group monolithic ontology to ontology networks with NeOn [31], and both stand-alone and collaborative tools, such as Prot\u00e9g\u00e9 [10] and MoKI [11].", "startOffset": 311, "endOffset": 315}, {"referenceID": 10, "context": "The process of ontology development has progressed much over the past 20 years, aided by development information systems-oriented methodologies, such as methontology for a single-person/group monolithic ontology to ontology networks with NeOn [31], and both stand-alone and collaborative tools, such as Prot\u00e9g\u00e9 [10] and MoKI [11].", "startOffset": 325, "endOffset": 329}, {"referenceID": 15, "context": "Processes at this \u2018micro\u2019 level of the development process, rather than the \u2018macro\u2019 level, may use the reasoner to propose axioms with FORZA [16], use Ontology Design Patterns (ODPs) [7], and start gleaning ideas from software engineering practices, notably exploring the notion of unit tests [35], eXtreme Design with ODPs [3], and Competency Question (CQ)-based authoring using SPARQL [28].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "Processes at this \u2018micro\u2019 level of the development process, rather than the \u2018macro\u2019 level, may use the reasoner to propose axioms with FORZA [16], use Ontology Design Patterns (ODPs) [7], and start gleaning ideas from software engineering practices, notably exploring the notion of unit tests [35], eXtreme Design with ODPs [3], and Competency Question (CQ)-based authoring using SPARQL [28].", "startOffset": 183, "endOffset": 186}, {"referenceID": 34, "context": "Processes at this \u2018micro\u2019 level of the development process, rather than the \u2018macro\u2019 level, may use the reasoner to propose axioms with FORZA [16], use Ontology Design Patterns (ODPs) [7], and start gleaning ideas from software engineering practices, notably exploring the notion of unit tests [35], eXtreme Design with ODPs [3], and Competency Question (CQ)-based authoring using SPARQL [28].", "startOffset": 293, "endOffset": 297}, {"referenceID": 2, "context": "Processes at this \u2018micro\u2019 level of the development process, rather than the \u2018macro\u2019 level, may use the reasoner to propose axioms with FORZA [16], use Ontology Design Patterns (ODPs) [7], and start gleaning ideas from software engineering practices, notably exploring the notion of unit tests [35], eXtreme Design with ODPs [3], and Competency Question (CQ)-based authoring using SPARQL [28].", "startOffset": 324, "endOffset": 327}, {"referenceID": 27, "context": "Processes at this \u2018micro\u2019 level of the development process, rather than the \u2018macro\u2019 level, may use the reasoner to propose axioms with FORZA [16], use Ontology Design Patterns (ODPs) [7], and start gleaning ideas from software engineering practices, notably exploring the notion of unit tests [35], eXtreme Design with ODPs [3], and Competency Question (CQ)-based authoring using SPARQL [28].", "startOffset": 387, "endOffset": 391}, {"referenceID": 26, "context": "Ontology authoring-driven TDD - the domain expert As there is practically only a limited amount of \u2018types\u2019 of axioms to add, one could create templates, alike the notion of the \u201clogical macro\u201d ODP [27].", "startOffset": 197, "endOffset": 201}, {"referenceID": 33, "context": "[34] essentially specify \u2018unit tests\u2019 for each feature/possible addition to a conceptual model, and test such an addition against sample individuals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In addition, for ontologies, one can avail of a query language for the TBox, namely, SPARQL-OWL [18], and most of the test can be specified in that language as well.", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "TDD in software development The principal introduction of TDD is described in [2]: it is essentially a methodology of software development, where one writes new code only if an automated test has failed, and eliminating duplication in doing so.", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "\u201d [20].", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": "Shrivastava and Jain [29] summed up the sequence as follows: 1) Write a test for a piece of functionality (that was based on a requirement), 2) Run all tests to check that the new test fails, 3) Write relevant code that passes the test, 4) Run the specific test to verify it passes, 5)", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "TDD is said to result in being more focussed, improved communication, improved understanding of required software behaviour, and reduced design complexity [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "e, better software quality\u2014and involves less time spent on debugging, and experiments with students showed that the test-first group wrote more tests and were significantly more productive than the test-last group [14].", "startOffset": 214, "endOffset": 218}, {"referenceID": 32, "context": "While all this focuses on the actual programming, the underlying ideas have been applied to conceptual data modelling [33,34], resulting in a TDD authoring process, compared to, among others, Halpin\u2019s conceptual schema design procedure [13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 33, "context": "While all this focuses on the actual programming, the underlying ideas have been applied to conceptual data modelling [33,34], resulting in a TDD authoring process, compared to, among others, Halpin\u2019s conceptual schema design procedure [13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 12, "context": "While all this focuses on the actual programming, the underlying ideas have been applied to conceptual data modelling [33,34], resulting in a TDD authoring process, compared to, among others, Halpin\u2019s conceptual schema design procedure [13].", "startOffset": 236, "endOffset": 240}, {"referenceID": 33, "context": "Tort and Oliv\u00e9 reworked the test specification into a test-driven way of modelling, where each UML Class Diagram language feature has its own test specification in OCL that involves creating the objects that should, or ought not to, instantiate the UML classes and associations [34].", "startOffset": 278, "endOffset": 282}, {"referenceID": 33, "context": "The tool that implements it was evaluated with modellers, which made clear, among others, that more time was spent on developing and revising the conceptual model, in the sense of fixing errors, than on writing the test cases [34].", "startOffset": 226, "endOffset": 230}, {"referenceID": 34, "context": "Tests in ontology engineering An early explorative work on borrowing the notion of testing from software engineering to apply it to ontology engineering is described in [35], which explores several adaptation options: testing with the axiom and its negation, formalising CQs, checks by means on integrity constraints, autoepistemic operators, and domain and range assertions.", "startOffset": 169, "endOffset": 173}, {"referenceID": 27, "context": "A substantial step in the direction of test-driven ontology engineering was proposed by Ren et al [28], who analyse CQs for use with SPARQL queries that then would be tested against the ontology.", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "Earlier work on CQs and queries include the OntologyTest tool, which allows the user to specify tests to check the functional requirements of the ontology based on the CQs, using ABox instances and SPARQL queries [8].", "startOffset": 213, "endOffset": 216}, {"referenceID": 7, "context": "Unlike extensive CQ and query patters, it specifies different types of tests focussed on the ABox rather than knowledge represented in the TBox, such as \u201cinstantiation tests\u201d (instance checking) and \u201crecovering tests\u201d (query for a class\u2019 individuals) and using mock individuals where applicable [8]; other instance-oriented test approaches have been proposed as well, although", "startOffset": 295, "endOffset": 298}, {"referenceID": 18, "context": "focussing on RDF/Linked Data rather than the OWL ABox [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "A NeON plugin with similar functionality and methodical steps within the eXtreme Design approach also has been proposed [3], but not the types of tests.", "startOffset": 120, "endOffset": 123}, {"referenceID": 34, "context": "The ontology unit test notion of axiom and its negation of [35] has been used, in a limited sense, in advocatus diaboli, where in the absence of a disjointness axiom between classes, it shows the consequences to the modeller to approve or disapprove, and if the latter is selected, the tool adds the disjointness axiom [6].", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "The ontology unit test notion of axiom and its negation of [35] has been used, in a limited sense, in advocatus diaboli, where in the absence of a disjointness axiom between classes, it shows the consequences to the modeller to approve or disapprove, and if the latter is selected, the tool adds the disjointness axiom [6].", "startOffset": 319, "endOffset": 322}, {"referenceID": 24, "context": "Some of the OOPS! pitfalls that the software checks has some test-flavour to it [25], such as suggesting possible symmetry.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "FORZA shows the permissible relations one can add that will not lead to an inconsistency, which is based on domain and range constraints of properties and the selected entities one wants to relate [16]; that is, also not hypothesising a failure/absence of required knowledge in the TBox as such, though it can be seen as a variant of the domain & range assertions unit tests in [35].", "startOffset": 197, "endOffset": 201}, {"referenceID": 34, "context": "FORZA shows the permissible relations one can add that will not lead to an inconsistency, which is based on domain and range constraints of properties and the selected entities one wants to relate [16]; that is, also not hypothesising a failure/absence of required knowledge in the TBox as such, though it can be seen as a variant of the domain & range assertions unit tests in [35].", "startOffset": 378, "endOffset": 382}, {"referenceID": 8, "context": "Concerning overarching methodologies, none of the 9 methodologies reviewed by [9] are TDD-based, nor is the MeltingPoint Gar\u0107\u0131a et al.", "startOffset": 78, "endOffset": 81}, {"referenceID": 23, "context": "Aspect OntoMaven [24] is an extension to OntoMaven that is based on reusing ideas from Apache Maven, advocated as being both a tool and supporting agile ontology engineering, such as COLM.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "Regarding tests, besides the OntoMvnTest with \u2018test cases\u2019 for the usual syntax checking, consistency, and entailment, the documentation states it should be possible to reuse Maven plug-ins for further test types [24], but this has not been followed through yet.", "startOffset": 213, "endOffset": 217}, {"referenceID": 25, "context": "A different Agile-inspired method is, eXtreme Design with content ODPs [26], although this is also more a design method for rapid turnaround times rather than test-driven.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Likewise, the earlier proposed RapidOWL is based on \u201citerative refinement, annotation and structuring of a knowledge base\u201d [1] rather permeating the test-driven approach throughout the methodology.", "startOffset": 123, "endOffset": 126}, {"referenceID": 30, "context": "many CQs first\u2019 [31], but not how this is to be achieved other than publishing new versions quickly.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "For the TBox tests, we use SPARQL-OWL [18] where possible/applicable.", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": "5 While SPARQL-DL [30] might be more well-known than SPARQL-OWL, that ver-", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "The query rewriting of SPARQL-OWL has been described in [18] and a tool implementing the algorithms, OWL-BGP, is freely available.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "This resembles the notion of mock objects in software engineering [21,17], which we shall transport into the ontology setting.", "startOffset": 66, "endOffset": 73}, {"referenceID": 16, "context": "This resembles the notion of mock objects in software engineering [21,17], which we shall transport into the ontology setting.", "startOffset": 66, "endOffset": 73}, {"referenceID": 31, "context": "(Possibly this can be taken further, using mock ontology modules, alike the mock databases [32], but this is not needed at this stage.", "startOffset": 91, "endOffset": 95}, {"referenceID": 21, "context": "Proposals for epistemic extensions for OWL exist, which concern instance retrieval and integrity constraints checking [22] that reduces the K-operator to plain OWL queries, or closed world reasoning [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Proposals for epistemic extensions for OWL exist, which concern instance retrieval and integrity constraints checking [22] that reduces the K-operator to plain OWL queries, or closed world reasoning [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": ", teaches and teaches\u2212) improved reasoner performance in at least one instance [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "D select Sub classes without even having to face the consideration of having blank nodes/unnamed classes (complex class expressions) on the right-hand-side of the inclusion axiom that was not supported in Kollia et al\u2019s BGP [18].", "startOffset": 224, "endOffset": 228}, {"referenceID": 22, "context": "Further, it may be the case that overall performance may be different when a different reasoner is used, as reasoners do differ [23].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": ", [4]), and has been shown to improve the overall results compared to doing something without a methodology.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "Like for software engineering\u2019s \u2018methodology selection grid\u2019 [4], it will exceedingly suit some ontology development projects but perhaps not others.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "While some aspects overlap, such as CQs (requirements), and the \u2018formalisation\u2019 step in methontology [5] (writing the axioms, by the knowledge engineer), both can be avoided as well: the former by the engineering, the latter by the domain expert.", "startOffset": 101, "endOffset": 104}], "year": 2015, "abstractText": "Emerging ontology authoring methods to add knowledge to an ontology focus on ameliorating the validation bottleneck. The verification of the newly added axiom is still one of trying and seeing what the reasoner says, because a systematic testbed for ontology authoring is missing. We sought to address this by introducing the approach of test-driven development for ontology authoring. We specify 36 generic tests, as TBox queries and TBox axioms tested through individuals, and structure their inner workings in an \u2018open box\u2019-way, which cover the OWL 2 DL language features. This is implemented as a Protege plugin so that one can perform a TDD test as a black box test. We evaluated the two test approaches on their performance. The TBox queries were faster, and that effect is more pronounced the larger the ontology is. We provide a general sequence of a TDD process for ontology engineering as a foundation for a TDD methodology.", "creator": "LaTeX with hyperref package"}}}