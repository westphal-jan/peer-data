{"id": "1512.06658", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "Deep Learning for Surface Material Classification Using Haptic And Visual Information", "abstract": "When a orlic user takuo scratches gimli a hand - unproblematic held tweede rigid weight-shift tool euro77 across an acclimatised object heye surface, an 13.6-billion acceleration signal can brighid be captured, walruses which branxton carries tranquillitatis relevant shakya information about contractors the surface. saltbush More iniquities importantly, such a 8:00 haptic signal is complementary pooping to lisogor the visual banzuke appearance 13.37 of the aesir surface, which suggests the combination magam of both linn modalities for nanshan the landsting recognition \u0111\u00e1 of the ucb surface material. In this 50.55 paper, gonal we salpetriere present suqian a trauger novel deep tcap learning method dealing with the surface material bicheno classification preservative problem based engraving on cyaneus a Fully --------------- Convolutional Network (massimiliano FCN ), which american-owned takes as input the reproach aforementioned acceleration signal and natta a corresponding kastoria image of the pachyderms surface shehhi texture. sjogren Compared to antitakeover previous minnery surface material classification trescott solutions, which rely slappers on kautsky a careful design of hand - 102,000 crafted space-time domain - 943,000 specific features, esmerian our damas method automatically angora extracts proffer discriminative features bate utilizing vigoreaux the cannistraro advanced prahlad deep platanus learning ritualised methodologies. black/white Experiments rawska performed on .183 the TUM surface undershirts material zords database kabine demonstrate that gantu our 222.3 method 65.65 achieves feikens state - 10-seeded of - lauded the - art tetrick classification c-38 accuracy conatus robustly five-volume and boultbee efficiently.", "histories": [["v1", "Mon, 21 Dec 2015 15:22:16 GMT  (8593kb,D)", "http://arxiv.org/abs/1512.06658v1", "8 pages, under review as a paper at Transactions on Multimedia"], ["v2", "Sun, 1 May 2016 07:00:56 GMT  (14540kb,D)", "http://arxiv.org/abs/1512.06658v2", "8 pages, under review as a paper at Transactions on Multimedia"]], "COMMENTS": "8 pages, under review as a paper at Transactions on Multimedia", "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["haitian zheng", "lu fang", "mengqi ji", "matti strese", "yigitcan ozer", "eckehard steinbach"], "accepted": false, "id": "1512.06658"}, "pdf": {"name": "1512.06658.pdf", "metadata": {"source": "CRF", "title": "DEEP LEARNING FOR SURFACE MATERIAL CLASSIFICATION USING HAPTIC AND VISUAL INFORMATION", "authors": ["Haitian Zheng", "Lu Fang", "Mengqi Ji", "Matti Strese", "Yigitcan \u00d6zer", "Eckehard Steinbach"], "emails": ["zhenght@mail.ustc.edu.cn", "mji}@ust.hk", "eckehard.steinbach}@tum.de"], "sections": [{"heading": null, "text": "object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.\nIndex Terms\u2014Surface material classification, convolutional neural network, haptic signal, hybrid inputs\nI. INTRODUCTION When a rigid tool slides on a surface, the resulting vibrations of the tool can be recorded using acceleration sensors. The captured haptic signal contains information about characteristic properties of the surface [5], such as hardness and roughness of the material. There has been increasing interest to recognize surface materials using robots [5][9][11][12] and to recreate the haptic feel of real surfaces. However, surface material classification from tool/surface interaction data becomes particularly challenging if freehand movements are considered. This is what we study in this paper using the special tool shown in Fig. 1.\nBefore surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface. These approaches mainly rely on handcrafted image features including LBP features [2], filter\nbank features [1][4], co-occurrence matrix based features [3] etc, combined with appropriate machine learning tools to distinguish the different texture types.\nRecently, Convolutional Neural Networks (CNN) have become a popular tool for patten recognition. In the context of surface classification, [20][21] aim to classify texture image patches by training CNNs, while our previous work [9] proposes an auto-encoder pre-trained CNN for classifying surface materials using acceleration segments1. However,\n\u2022 compared to the haptic signal, which can be of arbitrary length (up to several seconds, typically), the CNN has a relative small receptive field. In order to reconcile the disagreement of the two mentioned sizes, an inefficient sliding window-based approach needs to be adapted; \u2022 regardless of the significant progress in haptic (acceleration)-only or image-only surface material classification, few approaches deal with hybrid data input. For instance, two types of surface material with similar image appearance can lead to completely dif-\n1[9], a preliminary version of this work, has appeared at MLSP 2015. It focuses on unprocessed one-dimensional acceleration signals for surface material classification. As an extension, this work dives into a more efficient preprocessing-free solution with hybrid-inputs (acceleration signals as well as images).\nar X\niv :1\n51 2.\n06 65\n8v 1\n[ cs\n.R O\n] 2\n1 D\nec 2\nferent acceleration data. Additionally, the majority of the previously proposed haptic or image-based surface classification methods heavily rely on hand-engineered features and domain specific knowledge.\nTo overcome the inefficiency of CNN based approaches, a Fully-Convolutional Neural Networks (FCN) approach is adapted in our work. FCN [16] is a special type of convolutional neural network which replaces fully connected layers with convolutional layers with 1 \u00d7 1 convolution kernels. Without fully connected layers, the FCN is able to take input of arbitrary size, and outputs label predictions at every receptive field. More importantly, compared to the approach of \u2018CNN + sliding window\u2019, the FCN can be trained and tested more efficiently.\nDifferent from previous approaches in adapting FCN, we propose an efficient and systematic FCN scheme for recognizing hybrid data \u2013 haptic and image: the FCN for the haptic data is trained using concepts developed for speech recognition, as the used acceleration signals share similar characteristics with speech data [5] and [10]; the FCN for the image-based surface material recognition is trained by fine-tuning the network weights from [13], inspired by transfer learning [30]. Afterwards, an additional hybrid network further integrates the hybrid/visual predictions for better classification. As the concurrent work [35] also applies deep learning for haptic/visual hybrid input classification, we address that with our proposed FCN network and additional max-voting scheme, more efficient and accurate classification results can be achieved, leading such hybrid classification being piratical for real applications. In other words, from machine learning perspective, we propose a max-voting scheme that boosts the accuracy of multiple predictions, while the naive sliding windows approach is replaced by fully convolution network to further boost the efficiency. Experiments conducted on a publicly available texture dataset [5] demonstrate the superior performance of our scheme in both efficiency and accuracy for surface material classification.\nThe remaining paper is structured as follows. In Section II, we discuss the related work and present CNN and FCN. In Section III, the details of our method are elaborated. Section IV describes the surface classification experiments on the TUM dataset and discusses the results. Finally, Section V concludes the paper."}, {"heading": "II. RELATED WORK", "text": "Convolutional neural networks (CNN), a type of trainable multistage feed-forward artificial neural network that extracts a hierarchical feature representation, are a powerful tool for both image and speech recognition. A typical CNN consists of convolution layers, pooling layers, and fully connected layers. The main ingredients of a CNN can briefly be summarized as follows:\n\u2022 Convolution layers extract feature maps from the input by applying consecutive convolution operations between the input and trainable kernels, followed by a non-linear activation function. \u2022 Pooling layers usually follow convolution layers, aiming to reduce both the dimensionality and translation sensitivity of the input feature maps. For image recognition, pooling layers significantly boost spatial translation invariance, while for spectrogram recognition pooling layers lead to temporal-frequency invariance. \u2022 Fully-connected layers are usually used at the ending stage of a CNN, providing more flexible feature mapping. In a fully-connected layer, the input feature vector is linearly converted into a new feature vector before being fed into a non-linear activation function. \u2022 Softmax layer is usually used for handling the multiple label regression problem. At the end of the neural network, the softmax layer outputs the normalized exponential of the input vector, which indicates the probability of each label.\nThe activation function of the CNN can be chosen among sigmoid function, tanh function, and rectified linear (ReLu) function [17], among which, ReLu becomes more and more popular due to its efficiency for training and effectiveness for improving the classification performance. Additionally, dropout regularization [18] is commonly used in the fullyconnected layer, which significantly reduces co-adaptation between features, and hence prevents over-fitting and boosts the classification performance significantly.\nIn the field of image recognition, CNNs have experienced great success. In particular, with the help of large datasets (i.e., ImageNet), CNN methods [13] [14] [15] have taken over the lead in large scale visual recognition challenges (ILSVRC) since 2012. For a small dataset, learning the millions of parameters of a CNN is usually impractical and may lead to over-fitting. CNNs, however, still successfully show their power \u2013 studies on transfer learning [30] [31] [32] show that the trained CNN model for one specific vision task usually learns a good representation of natural images, which works for other visual recognition tasks as well. Inspired by the success of transfer learning, we investigate in this paper how to handle the surface texture recognition task using the relatively small TUM image dataset [5].\nCNN has also been shown to be a powerful tool for speech recognition. Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN). The superior performance is attributed to the property of temporal-frequency translation invariance inherent with CNN [24]. In addition to speech recognition, CNNs are applied to acoustic recognition tasks such as music genre classification [27] and music onset detection [28]. Motivated by the previous work on 1-D speech signal recognition, and evidence in [5][10] which show that the acceleration signals captured during the interaction with\nan object surface share certain characteristics with speech signals, we investigate a CNN method to deal with our haptic acceleration signal based recognition task.\nThough CNNs provide powerful recognition abilities, additional designing is required to handle the special properties of the acceleration signals which describe the surface material. Specifically, the input can be of arbitrary size and contains frequently repeated local patterns. Thus it is more desirable to take local signal segments as the input, rather than the entire signal. Intuitively, the most simple texture prediction pipeline could be: 1) convert the acceleration input into segments using sliding windows; 2) predict every segment with a trained CNN; 3) perform max-voting among multiple CNN predictions. Our experiments show that with a carefully trained CNN, the aforementioned workflow achieves high prediction accuracy. However, a sliding window based approach is inappropriate for real-world applications, as dense prediction of the CNN is computationally expensive.\nIn our work, to alleviate this inefficiency, the naive sliding windows approach (step 1 and 2) is replaced by the FCN. FCN is a special type of convolutional neural network which replaces fully connected layers which output n filters with convolutional layers with n 1 \u00d7 1 convolution kernels. The key observation of FCN is that fully-connected layers in a CNN is special convolutional layers: convolutional layer which takes a 1\u00d71 feature map and outputs another 1\u00d71 feature map. In contrast to a fully-connected layer which takes fixed-length input merely, the corresponding convolutional layer is generalized to take input of arbitrary size, which is extremely beneficial for extracting dense features at every spatial location.\nTaking this key observation into account, the FCN can replace the fully-connected layers of a CNN by convolutional layer with the same weight. Given input data of arbitrary size, FCN sequentially performs convolution or max-pooling operation, and provides label predictions at every receptive field. Compared to the sliding windows approach which heavily re-computes feature maps within overlapping \u201cwindows\u201d, FCN is computationally less expensive. As further noted in [16], both training and inference of FCN can be performed by standard neural network approaches, leading to an efficient and systematic scheme."}, {"heading": "III. PROPOSED CNN SCHEME WITH HYBRID INPUT", "text": "In this section, the proposed CNN scheme with hybrid input will be elaborated by starting with the explanation of hybrid data recording (Section III-A), followed by the proposed surface texture classification schemes (Section IIIB) called HapticNet, VisualNet and FusionNet.\nIII-A. Hybrid Data Recording\nAcceleration Data When a human strokes a rigid tool over an object surface, the exerted normal force, the tangential scan velocity, and the angle between the tool and the surface might vary during the surface exploration and between subsequent exploration sessions. These scan-time parameters strongly influence the nature of the recorded acceleration signals [6]. Fig. 2 shows an example of an acceleration data trace, where the scan velocity is linearly increasing and reveals, how this change influences the data trace with regard to its signal power and variance.\nThe variability of the acceleration signals thus complicates the texture classification process. However, a user study in [7] reveals that the exerted normal force and tangential velocity are not equally relevant. This has been validated during the recreation of the texture vibrations with voice coil actuators. In fact, the perceived user realism during the experiment depends more strongly on different velocity conditions, whereas changes in normal forces during the scan interaction have a weaker influence on the perception of the displayed tactile signals. We infer from this observation that the recording of acceleration signals depends stronger on different velocity conditions.\nIn our work, we use the haptic stylus from [10], which is a free-to-wield object with a stainless steel tooltip, shown in Fig. 1. In [10], a three-axis LIS344ALH accelerometer (ST Electronics) with a range of \u00b16 g was applied to collect the raw acceleration data traces. All three axis were combined to one using DFT321 (see [8]). This approach, which still contains the spectral characteristics of the three axes, was adapted in order to have less computational effort in terms of feature calculation.\nImage Data Different from the acceleration modality, the images of the surface objects are taken in a non-dynamic way of recording. However, differences in distance, rotation, light condition as well as focus also complicate the collection of uniform images for the surface classification task.\nFig. 3 shows all the used surfaces and their abbreviated names, where the images of the surfaces are taken by the rear camera of a common smartphone (Samsung S4 Mini) and have a resolution of 8 Mega-Pixels. The illumination and view conditions differ within the same class of surfaces. For each surface, there are 5 images under daylight and 5 images under ambient light conditions. Viewing direction and camera distance are chosen arbitrarily for each picture, resulting in variations within each individual class. As an example, we choose three textured surfaces and plot the raw acceleration and image data in Fig. 4.\nIII-B. Surface Material Classification As shown in Fig. 2, the acceleration signal usually starts with a short impulse signal, captured when the rigid device initially touches the material surface. The following data is a much longer movement signal when the device moves over the surface. As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains. Therefore, the 1-D raw acceleration data of the movement signal is transferred to its spectrogram Xh by computing the squared magnitude of the short-time Discrete Fourier Transformation (DFT).\nUnlike previous work, which applies CNN for recognition, we use a trained FCN to achieve dense prediction for the spectrogram input Xh. FCN replaces the CNN\u2019s fullyconnected layers by 1 \u00d7 1 convolutional layers. Without fully-connected layers, FCN produces dense prediction when given input of arbitrary length. Usually adjacent predictions share overlapping receptive fields and intermediate features. Since FCN does not recompute the overlapping intermediate features, compared to the approach of \u2018CNN + sliding windows\u2019, FCNs are much more efficient.\nHapticNet Our proposed FCN network for haptic (acceleration) data, denoted as HapticNet, takes input of arbitrary length, and the output is a series of softmaxed-vectors {y1, \u00b7 \u00b7 \u00b7 ,yn} representing the categorical probability distributions at different temporal locations (illustrated in Fig. 5(a)). The detailed structure of HapticNet is shown in Table I. The HapticNet consists of three normal convolution maxpooling layers, and two 1 \u00d7 1 convolutional layers which replace the CNN\u2019s fully connected layers. Followed by the final convolutional layer is the softmax layer, which gives categorical probability distributions at every temporal location. The sizes of the convolutional kernel and maxpooling, as well as the output size of each layer are shown in Table I.\nWith the predicted output distribution vectors yi at each temporal location i by the FCN, the corresponding class labels ci are obtained by finding the label with maximum probability, i.e.,\nci = argmax c (yi[c]), (1)\nwhere yi[c] represents the c-th element of vector yi, and c enumerates all surface material categories. In order to obtain the class label of the entire haptic signal C from multiple predictions {c1, \u00b7 \u00b7 \u00b7 , cn}, a max-voting procedure that selects the class label with the maximum vote is adapted,\nC = argmax c \u2211 i (1(ci = c)). (2)\nNote that the above fully-connected layer is very small, thus it is trainable using a small dataset without over-fitting.\nVisualNet The classification pipeline for image data is similar to the one used for haptic data, i.e., an input image of arbitrary size is fed into a FCN network (denoted as VisualNet) for outputting categorical probability distributions {y1,1, \u00b7 \u00b7 \u00b7 ,ym,n} at every spatial position. As illustrated in Fig. 5(b), the class labels are obtained by applying Eqn. (1) for every {yi,j}. Then, the class label of the entire image is obtained by applying Eqn. (2) accordingly.\nThe structure of our VisualNet is motivated by the widely used CNN structure AlexNet proposed in [13]. In order to provide dense prediction, the last three fully connected layers are replaced by convolution layers. In addition, the kernel sizes of these three convolution layers are changed from {4096, 4096, 1000} to {300, 300, 69} for predicting 69 texture classes and preventing overfitting. We use the learned convolutional layers from AlexNet to initialize our corresponding top convolution layers, and the random weights are used to initialize the remaining convolution layers. Training is conducted by backpropagating the entire network using the\nTUM image dataset. The overall pipeline for image classification (VisualNet) is shown in Fig. 5(b), which predicts a distribution vector at every spatial location given the image input.\nFusionNet For the scenario, where both haptic and image data are available, the FusionNet classification network shown in Fig. 5(c) is proposed. To better understand the FusionNet, suppose we have two textures A and B. It is likely that the prediction of either HapticNet or VisualNet (A or B) falls into the same bin of the 69-dimensional category vector. However, it is highly unlikely that both A and B fall into the same bin in a 692-dimensional space.\nTherefore, given both haptic and image prediction vectors, an additional fully-connected layer is trained, which exports the material classes again. To deal with the multiple predictions made by HapticNet and VisualNet, we randomly concatenate samples from HapticNet and VisualNet, then feed the concatenated vectors into the new 1 \u00d7 1 convolu-\ntional layer to achieve categorical probability distributions. Given multiple predictions, max-voting is applied to obtain the final output class label. The overall pipeline of FusionNet is illustrated in Fig. 5(c). First, HapticNet and VisualNet are used to predict the given haptic input and image input respectively. Afterwards, random concatenation and prediction are performed. With multiple predictions (the distribution vectors on the left of Fig. 5(c)), max-voting is further applied for final prediction."}, {"heading": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "text": "Our implementation is based on the Caffe deep learning framework [33], using a computer with 8GB RAM and Nvidia GPU GTX-860M. The proposed HapticNet, VisualNet and FusionNet are evaluated using the TUM Haptic Texture dataset [5], which contains 69 surface material classes and each class consists of 10 acceleration signals + 10 images, respectively. The acceleration signals have been recorded during free-hand movements and hence strongly vary in contact force and tool velocity. The images contain various samples recorded under different viewing and lighting conditions. During our training stage, the images are further augmented by random rotation.\nThe dataset is separated into a training set which contains 9 haptic data traces and 9 images for each surface material class, and the testing set with the remaining haptic data traces and images in each class. The training set is used to train the proposed network, and the testing set is used to validate the performance of the network and the following max-voting scheme. The corresponding hyper-parameters for training HapticNet, VisualNet, and FusionNet are depicted in Table II.\nHapticNet The spectrograms of the haptic signal in log domain with frame length being 500 and frame increment being 250 are extracted. During training, we extract 62100 haptic spectrogram samples of size 50\u00d7 300. For the testing stage, we used 69 full-length spectrograms of the haptic signal. The HapticNet is trained using the adaptive subgradient methods [34] with the spectrograms of the training set. After training, we verify the haptic baseline system by feeding HapticNet with testing spectrograms, computing the class label at every location, then max-voting among multiple predictions, as described in Section III-A.\nThe classification accuracy is shown in the Haptic Classification part of Table III. Specifically, we compare our results with several existing methods [5][10][9]. [5] proposes to combine MFCC features with Gaussian Mixture Model (GMM) for movement phase recognition, which is denoted as \u2018MFCC + GMM\u2019 here. The following work [10] carefully discusses variant features for representation the movement signal, and variant discriminative models are proposed for predictions. Considering that [5] uses \u201caveraged features\u201d to test the prediction accuracy, which performs an underlying model averaging, the obtained results are listed in the maxvoting column. [9] is the first work which uses the raw acceleration data to train the CNN and achieves competitive classification results.\nVisualNet Texture images are firstly resized to be of halfsize, where the texture structure can be preserved by the 224\u00d7224 receptive field of AlexNet [13]. During the training phase, we feed VisualNet with a total of 20700 training\nimages of size 512\u00d7 512 and train VisualNet by stochastic gradient descent. During the testing phase, we feed a testing image to VisualNet to compute the class label at every location. Finally, max voting is performed for predicting the class label of the entire texture image. The testing accuracy of the concerned scheme is shown in the Visual Classification part of Table III. As expected, the max-voting accuracy is significantly boosted up to 98.6%, when compared to the individual fragment accuracy of 88.4%\nFusionNet During the training stage of FusionNet, we generate 10 samples for each haptic-image pair, and totally 50000 hybrid pairs. During the testing stage, we generate 100 samples for the haptic-image hybrid pair. When using the proposed FusionNet, the accuracy of surface material classification using the hybrid haptic-image data is shown in the Fusion Classification part of Table III. As expected, the accuracy of haptic and image predictions is significantly boosted with the fusion framework: we observed that all testing samples (69 samples, 1 sample per class) are correctly classified with the fusion framework.\nTime profiling To demonstrate the efficiency of our algorithm, we time-profile HapticNet/VisualNet and compare the results with the two corresponding \u2018CNN + sliding windows\u2019 approaches. For the sliding windows approaches, we transfer the weight of HapticNet/VisualNet to two Convolutional Neural Networks which share the same layer settings with HapticNet/VisualNet, while the 1\u00d7 1 convolutional layer is replaced by a fully-connected layer. With exactly the same weights, the sliding windows approaches achieve the same fragment/max-voting accuracy as HapticNet and VisualNet. However, because FCN is able to avoid the redundant feature computation that exists in the \u2018CNN + sliding windows\u2019 approaches, it is expected to be more efficient. To further validate the efficiency of the FCN based approaches (HapticNet/VisualNet), we profile the average running time of HapticNet/VisualNet in comparison to the \u2018CNN + the sliding windows approaches on a Nvidia GTX 860M graphics card. As shown in Table IV, for haptic classification, the sliding windows approach takes 195.4ms to run, while HapticNet merely takes 20.5ms to run. Moreover, for visual classification, the sliding windows approach takes 7903.4ms to run, while VisualNet takes 154.3ms to run."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "We introduce a surface material classification method which uses Fully Convolutional Neural Networks. For predicting individual haptic input or image input, we apply FCN with max-voting framework. We then design a fusion network dealing with both haptic and image input. Experiments on the TUM Haptic Texture Database demonstrate that our proposed system can achieve competitive classification accuracy compared to existing schemes. In our future work, we are aiming at further extending the current \u2018FCN + max-voting\u2019 and hybrid classification schemes to more input types (image input, haptic acceleration signal, haptic speed signal, sound signal) for further improving the flexibility and robustness of our system."}, {"heading": "VI. REFERENCES", "text": "[1] L. Liu, and P. Fieguth, Texture classification from random features, in Pattern Analysis and Machine Intelligence, IEEE Transactions on 34, no. 3 (2012): 574-586. [2] T. Ojala, P. Matti, and M. Topi, Multiresolution grayscale and rotation invariant texture classification with local binary patterns, in Pattern Analysis and Machine Intelligence, IEEE Transactions on 24, no. 7 (2002): 971- 987. [3] Arvis, Vincent, Christophe Debain, Michel Berducat, and Albert Benassi, Generalization of the cooccurrence matrix for colour images: application to colour texture classification, in Image Analysis and Stereology 23, no. 1 (2011): 63-72. [4] M. Varma and A. Zisserman, Texture Classification: Are Filter Banks Necessary, in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 691-698, 2003. [5] M. Strese, J.-Y. Lee, C. Schuwerk, Q. Han, H.-G. Kim and E.Steinbach, A haptic texture database for toolmediated texture recognition and classification, in Proc. of IEEE HAVE, Dallas, Texas, USA, October 2014. [6] Romano, Joseph M. and Kuchenbecker, Katherine J., Creating realistic virtual textures from contact acceleration data in IEEE Transactions on Haptics, pp. 109\u2013119, April 2012. [7] Romano, Joseph M. and Kuchenbecker, Katherine J., Should Haptic Texture Vibrations Respond to User Force and Speed in IEEE World Haptics Conference, June 2015. [8] N. Landin, J. M. Romano, W. McMahan, and K. J. Kuchenbecker, Dimensional reduction of high-frequency accelerations for haptic rendering, in Haptics: Generating and Perceiving Tangible Sensations, Springer, pp. 79- 86, 2010. [9] M. Ji, L. Fang, H. Zheng, M. Strese and E.Steinbach, Preprocessing-free Surface Material Classification Using Convolutional Neural Networks Pretrained by Sparse Autoencoder (ACNN), in Proc. of Machine Learning\nfor Signal Processing (MLSP), Boston, USA, September 2015. [10] M. Strese, C. Schuwerk and E. Steinbach, Surface Classification Using Acceleration Signals Recorded During Human Free hand Movement, in Proc. of IEEE World Haptics Conference, Chicago, USA, June 2015. [11] J. M. Romano and K. J. Kuchenbecker, Methods for Robotic Tool-Mediated Haptic Surface Recognition, in IEEE Haptics Symposium (HAPTICS), Houston, Texas, USA, February 2014. [12] J. A. Fishel and G. E. Loeb, Bayesian exploration for intelligent identification of textures, in Frontiers in neurorobotics, vol. 6, no. 4, pp. 1-20, June 2012. [13] A. Krizhevsky, I. Sutskever and G. E. Hinton, Imagenet classification with deep convolutional neural networks, in Advances in neural information processing systems (NIPS), pp. 1097-1105. 2012. [14] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in arXiv preprint 1409.1556 (2014). [15] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke and A. Rabinovich, Going deeper with convolutions, In arXiv preprint 1409.4842 (2014). [16] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic segmentation, in Computer Vision and Pattern Recognition (CVPR), 2015. [17] V. Nair, and G. E. Hinton, Rectified linear units improve restricted boltzmann machines, in Proceedings of the 27th International Conference on Machine Learning (ICML), 2010. [18] S. Wager, S. Wang, and P. Liang, Dropout training as adaptive regularization, in Advances in Neural Information Processing Systems, pp. 351-359. 2013. [19] Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998. [20] F. H. C. Tivive, and A. Bouzerdoum, Texture classification using convolutional neural network, in TENCON, 2006 IEEE Region 10 Conference, pp. 1-4. IEEE, 2006. [21] L. G. Hafemann, An Analysis of Deep Neural Networks for Texture Classification, M.Sc. Dissertation, Retrieved from http://dspace.c3sl.ufpr.br:8080/dspace/handle/1884/36967, 2014. [22] Abdel-Hamid, Ossama, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu, Convolutional neural networks for speech recognition, in Audio, Speech, and Language Processing, IEEE/ACM Transactions on 22, no. 10 (2014): 1533-1545. [23] O. Abdel-Hamid, A. Mohamed, J. Hui, and G. Penn, Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition, in Acous-\ntics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pp. 4277-4280. IEEE, 2012. [24] T. N. Sainath, A. Mohamed, B. Kingsbury and B. Ramabhadran, Deep Convolutional Neural Networks for LVCSR, In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8614 - 8618. IEEE, 2013. [25] P. Swietojanski, A. Ghoshal and S. Renals, Convolutional Neural Networks for Distant Speech Recognition, in Hands-free Speech Communication and Microphone Arrays (HSCMA), 2014 4th Joint Workshop on, pp. 172- 176. IEEE, 2014. [26] O. Abdel-Hamid, L. Deng and D. Yu, Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition, in INTERSPEECH, 2013, ISCA. [27] Q. Kong, X. Feng and Y. Li, Music Genre Classification Using Convolutional Neural Network, Retrieved from http://www.terasoft.com.tw/conf/ismir2014/ LBD%5CLBD17.pdf [28] J. Schluter and S.Bock, Improved musical onset detection with Convolutional Neural Networks, in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pp. 6979-6983. IEEE, 2014 [29] J. Schluter, and B. Sebastian, Improved musical onset detection with convolutional neural networks, in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pp. 6979-6983. IEEE, 2014. [30] M. Oquab, B. Leon, L. Ivan, and S. Sivic, Learning and transferring mid-level image representations using convolutional neural networks, in Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 1717-1724. IEEE, 2014. [31] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng and T. Darrell, DeCAF: A deep convolutional activation feature for generic visual recognition, in arXiv preprint 1310.1531 (2013). [32] M. D. Zeiler and R. Fergus, Visualizing and understanding convolutional networks, in European Conference on Computer Vision (ECCV), 2014, Springer International Publishing, pp. 818-833. [33] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick and et. al., Caffe: Convolutional architecture for fast feature embedding, in Proceedings of the ACM International Conference on Multimedia, ACM, pp. 675- 678, 2014. [34] J. Duchi, E. Hazan, and Y. Singer, Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, in Journal of Machine Learning Research, 2011. [35] Y. Gao, L. A. Hendricks, K. J. Kuchenbecker, T. Darrell, Deep Learning for Tactile Understanding From Visual and Haptic Data, arXiv preprint arXiv:1511.06065."}], "references": [{"title": "Texture classification from random features", "author": ["L. Liu", "P. Fieguth"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 34, no. 3 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiresolution grayscale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "P. Matti", "M. Topi"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 24, no. 7 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Generalization of the cooccurrence matrix for colour images: application to colour texture classification, in Image Analysis and Stereology", "author": ["Arvis", "Vincent", "Christophe Debain", "Michel Berducat", "Albert Benassi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Texture Classification: Are Filter Banks Necessary", "author": ["M. Varma", "A. Zisserman"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 691-698", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Creating realistic virtual textures from contact acceleration data", "author": ["Romano", "Joseph M", "Kuchenbecker", "Katherine J"], "venue": "IEEE Transactions on Haptics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Should Haptic Texture Vibrations Respond to User Force and Speed in IEEE World", "author": ["Romano", "Joseph M", "Kuchenbecker", "Katherine J"], "venue": "Haptics Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Dimensional reduction of high-frequency accelerations for haptic rendering", "author": ["N. Landin", "J.M. Romano", "W. McMahan", "K.J. Kuchenbecker"], "venue": "Haptics: Generating and Perceiving Tangible Sensations, Springer, pp. 79- 86", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Strese and E.Steinbach, Preprocessing-free Surface Material Classification Using Convolutional Neural Networks Pretrained by Sparse Autoencoder (ACNN), in Proc. of Machine Learning  for Signal Processing", "author": ["M. Ji", "L. Fang", "M.H. Zheng"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Surface Classification Using Acceleration Signals Recorded During Human Free hand Movement", "author": ["M. Strese", "C. Schuwerk", "E. Steinbach"], "venue": "in Proc. of IEEE World Haptics Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Methods for Robotic Tool-Mediated Haptic Surface Recognition, in IEEE Haptics Symposium (HAPTICS)", "author": ["J.M. Romano", "K.J. Kuchenbecker"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Bayesian exploration for intelligent identification of textures, in Frontiers in neurorobotics", "author": ["J.A. Fishel", "G.E. Loeb"], "venue": "vol. 6,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks, in Advances in neural information processing systems (NIPS)", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint 1409.1556 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint 1409.4842 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout training as adaptive regularization, in Advances in Neural Information", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Texture classification using convolutional neural network", "author": ["F.H.C. Tivive", "A. Bouzerdoum"], "venue": "TENCON, 2006 IEEE Region 10 Conference, pp. 1-4. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "An Analysis of Deep Neural Networks for Texture Classification", "author": ["L.G. Hafemann"], "venue": "M.Sc. Dissertation, Retrieved from http://dspace.c3sl.ufpr.br:8080/dspace/handle/1884/36967", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for speech recognition, in Audio, Speech, and Language Processing", "author": ["Abdel-Hamid", "Ossama", "Abdel-rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on 22, no", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "J. Hui", "G. Penn"], "venue": "Acous-  tics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pp. 4277-4280. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep Convolutional Neural Networks for LVCSR", "author": ["T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8614 - 8618. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional Neural Networks for Distant Speech Recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Hands-free Speech Communication and Microphone Arrays (HSCMA), 2014 4th Joint Workshop on, pp. 172- 176. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "INTERSPEECH", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Music Genre Classification Using Convolutional Neural Network, Retrieved from http://www.terasoft.com.tw/conf/ismir2014/ LBD%5CLBD17.pdf", "author": ["Q. Kong", "X. Feng", "Y. Li"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Improved musical onset detection with Convolutional Neural Networks, in Acoustics", "author": ["J. Schluter", "S.Bock"], "venue": "Speech and Signal Processing (ICASSP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["J. Schluter", "B. Sebastian"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pp. 6979-6983. IEEE", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "B. Leon", "L. Ivan", "S. Sivic"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 1717-1724. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint 1310.1531 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long"], "venue": "Girshick and et. al., Caffe: Convolutional architecture for fast feature embedding, in Proceedings of the ACM International Conference on Multimedia, ACM, pp. 675- 678", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "There has been increasing interest to recognize surface materials using robots [5][9][11][12] and to recreate the haptic feel of real surfaces.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "There has been increasing interest to recognize surface materials using robots [5][9][11][12] and to recreate the haptic feel of real surfaces.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "There has been increasing interest to recognize surface materials using robots [5][9][11][12] and to recreate the haptic feel of real surfaces.", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 114, "endOffset": 118}, {"referenceID": 9, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "These approaches mainly rely on handcrafted image features including LBP features [2], filter Fig.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "bank features [1][4], co-occurrence matrix based features [3] etc, combined with appropriate machine learning tools to distinguish the different texture types.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "bank features [1][4], co-occurrence matrix based features [3] etc, combined with appropriate machine learning tools to distinguish the different texture types.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "bank features [1][4], co-occurrence matrix based features [3] etc, combined with appropriate machine learning tools to distinguish the different texture types.", "startOffset": 58, "endOffset": 61}, {"referenceID": 17, "context": "In the context of surface classification, [20][21] aim to classify texture image patches by training CNNs, while our previous work [9] proposes an auto-encoder pre-trained CNN for classifying surface materials using acceleration segments1.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "In the context of surface classification, [20][21] aim to classify texture image patches by training CNNs, while our previous work [9] proposes an auto-encoder pre-trained CNN for classifying surface materials using acceleration segments1.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "In the context of surface classification, [20][21] aim to classify texture image patches by training CNNs, while our previous work [9] proposes an auto-encoder pre-trained CNN for classifying surface materials using acceleration segments1.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "1[9], a preliminary version of this work, has appeared at MLSP 2015.", "startOffset": 1, "endOffset": 4}, {"referenceID": 14, "context": "FCN [16] is a special type of convolutional neural network which replaces fully connected layers with convolutional layers with 1 \u00d7 1 convolution kernels.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "Different from previous approaches in adapting FCN, we propose an efficient and systematic FCN scheme for recognizing hybrid data \u2013 haptic and image: the FCN for the haptic data is trained using concepts developed for speech recognition, as the used acceleration signals share similar characteristics with speech data [5] and [10]; the FCN for the image-based surface material recognition is trained by fine-tuning the network weights from [13], inspired by transfer learning [30].", "startOffset": 326, "endOffset": 330}, {"referenceID": 11, "context": "Different from previous approaches in adapting FCN, we propose an efficient and systematic FCN scheme for recognizing hybrid data \u2013 haptic and image: the FCN for the haptic data is trained using concepts developed for speech recognition, as the used acceleration signals share similar characteristics with speech data [5] and [10]; the FCN for the image-based surface material recognition is trained by fine-tuning the network weights from [13], inspired by transfer learning [30].", "startOffset": 440, "endOffset": 444}, {"referenceID": 27, "context": "Different from previous approaches in adapting FCN, we propose an efficient and systematic FCN scheme for recognizing hybrid data \u2013 haptic and image: the FCN for the haptic data is trained using concepts developed for speech recognition, as the used acceleration signals share similar characteristics with speech data [5] and [10]; the FCN for the image-based surface material recognition is trained by fine-tuning the network weights from [13], inspired by transfer learning [30].", "startOffset": 476, "endOffset": 480}, {"referenceID": 15, "context": "The activation function of the CNN can be chosen among sigmoid function, tanh function, and rectified linear (ReLu) function [17], among which, ReLu becomes more and more popular due to its efficiency for training and effectiveness for improving the classification performance.", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Additionally, dropout regularization [18] is commonly used in the fullyconnected layer, which significantly reduces co-adaptation between features, and hence prevents over-fitting and boosts the classification performance significantly.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": ", ImageNet), CNN methods [13] [14] [15] have taken over the lead in large scale visual recognition challenges (ILSVRC) since 2012.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": ", ImageNet), CNN methods [13] [14] [15] have taken over the lead in large scale visual recognition challenges (ILSVRC) since 2012.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": ", ImageNet), CNN methods [13] [14] [15] have taken over the lead in large scale visual recognition challenges (ILSVRC) since 2012.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "CNNs, however, still successfully show their power \u2013 studies on transfer learning [30] [31] [32] show that the trained CNN model for one specific vision task usually learns a good representation of natural images, which works for other visual recognition tasks as well.", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "CNNs, however, still successfully show their power \u2013 studies on transfer learning [30] [31] [32] show that the trained CNN model for one specific vision task usually learns a good representation of natural images, which works for other visual recognition tasks as well.", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "CNNs, however, still successfully show their power \u2013 studies on transfer learning [30] [31] [32] show that the trained CNN model for one specific vision task usually learns a good representation of natural images, which works for other visual recognition tasks as well.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "The superior performance is attributed to the property of temporal-frequency translation invariance inherent with CNN [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "In addition to speech recognition, CNNs are applied to acoustic recognition tasks such as music genre classification [27] and music onset detection [28].", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "In addition to speech recognition, CNNs are applied to acoustic recognition tasks such as music genre classification [27] and music onset detection [28].", "startOffset": 148, "endOffset": 152}, {"referenceID": 8, "context": "Motivated by the previous work on 1-D speech signal recognition, and evidence in [5][10] which show that the acceleration signals captured during the interaction with", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "noted in [16], both training and inference of FCN can be performed by standard neural network approaches, leading to an efficient and systematic scheme.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "Acceleration data trace, reproduced from [10].", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "These scan-time parameters strongly influence the nature of the recorded acceleration signals [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "However, a user study in [7] reveals that the exerted normal force and tangential velocity are not equally relevant.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "In our work, we use the haptic stylus from [10], which is a free-to-wield object with a stainless steel tooltip, shown in Fig.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "In [10], a three-axis LIS344ALH accelerometer (ST Electronics) with a range of \u00b16 g was applied to collect the raw acceleration data traces.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "All three axis were combined to one using DFT321 (see [8]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 21, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "The structure of our VisualNet is motivated by the widely used CNN structure AlexNet proposed in [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "Our implementation is based on the Caffe deep learning framework [33], using a computer with 8GB RAM and Nvidia GPU GTX-860M.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "23% Modified MFCC Decreasing + Naive Bayes[10] \u2013 89%", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "ACNN [9] 81.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "7% Micro- / Macro-Roughness, Hardness, 5 MFCC + Naive Bayes[10] \u2013 91%", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "Micro- / Macro-Roughness, Hardness, 5 MFCC, Friction + Naive Bayes[10] \u2013 95%", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "The HapticNet is trained using the adaptive subgradient methods [34] with the spectrograms of the training set.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Specifically, we compare our results with several existing methods [5][10][9].", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "Specifically, we compare our results with several existing methods [5][10][9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "The following work [10] carefully discusses variant features for representation the movement signal, and variant discriminative models are proposed for predictions.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "[9] is the first work which uses the raw acceleration data to train the CNN and achieves competitive classification results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "VisualNet Texture images are firstly resized to be of halfsize, where the texture structure can be preserved by the 224\u00d7224 receptive field of AlexNet [13].", "startOffset": 151, "endOffset": 155}], "year": 2017, "abstractText": "When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.", "creator": "LaTeX with hyperref package"}}}