{"id": "1512.06927", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "A C++ library for Multimodal Deep Learning", "abstract": "tasted This 115.77 is 190-foot the meltzglobe.com document of al-fihri Multimodal Deep 235u Learning gehlot Library, .589 MDL, tena which intensity is palmes written roxburghe in discounters C + +. It explains principles and chmela implementations with details of bogacheva Restricted neuman Boltzmann Machine, Deep Neural dorky Network, Deep staked Belief 1984-1985 Network, uscgc Denoising Autoencoder, sysonby Deep playfully Boltzmann Machine, Deep franzini Canonical Correlation pollarded Analysis, mohanty and perotistas modal o\u2019shea prediction diarra model. leading-edge MDL uses cange OpenCV xangsane 3. megret 0. vonappen 0, a123 which is 5,360 the only stuttle dependency of guttman this library. plies Most extremism of its bentonville implementation semenzato has 3-on-2 been brerewood tested ambros in enoggera Mac OS. mig-35 It blitzstein also synanon provides haramain interface sherzad for hipper reading nother various data set 150.92 such pcg as MNIST, CIFAR, riina XRMB, salonika and tapan AVLetters. To 123.5 read nummer mat amerindian file, Matlab must be installed because it uses 59-year Matlab / 1-inch c + + interface provided 6.22 by asceticism Matlab. There are una multiple model options jantsch provided. dorrance Different rampurhat gradient descent grain methods, eurowings loss function, barrel-shaped annealing yelang methods, and chitou activation functions are 11:27 given. These sanfrecce options unsaturated are easy agd to extend given bouchard the ciannachta structure of jarak MDL. 31-22 So pe\u00f1arol MDL could tantramar be used as isight a troiani frame radiosurgery for leyen testings in hollered deep jacquie learning.", "histories": [["v1", "Tue, 22 Dec 2015 01:27:23 GMT  (1305kb,D)", "http://arxiv.org/abs/1512.06927v1", "30 pages"], ["v2", "Mon, 28 Dec 2015 20:00:20 GMT  (1304kb,D)", "http://arxiv.org/abs/1512.06927v2", "29 pages"], ["v3", "Tue, 29 Dec 2015 13:39:52 GMT  (1304kb,D)", "http://arxiv.org/abs/1512.06927v3", "29 pages"], ["v4", "Tue, 12 Apr 2016 17:34:29 GMT  (1258kb,D)", "http://arxiv.org/abs/1512.06927v4", "27 pages"]], "COMMENTS": "30 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jian jin"], "accepted": false, "id": "1512.06927"}, "pdf": {"name": "1512.06927.pdf", "metadata": {"source": "CRF", "title": "Multimodal Deep Learning Library", "authors": ["Jian Jin"], "emails": [], "sections": [{"heading": null, "text": "Multimodal Deep Learning Library\nJian Jin\nDepartment of Computer Science Johns Hopkins University"}, {"heading": "1 Multimodal Deep Learning Library", "text": ""}, {"heading": "1.1 Introduction", "text": "This is the document of Multimodal Deep Learning Library, MDL, which is written in C++. It explains principles and implementations with details of Restricted Boltzmann Machine, Deep Neural Network, Deep Belief Network, Denoising Autoencoder, Deep Boltzmann Machine, Deep Canonical Correlation Analysis, and modal prediction model.\nMDL uses OpenCV 3.0.0, which is the only dependency of this library. Most of its implementation has been tested in Mac OS. It also provides interface for reading various data set such as MNIST, CIFAR, XRMB, and AVLetters. To read mat file, Matlab must be installed because it uses Matlab/c++ interface provided by Matlab.\nThere are multiple model options provided. Different gradient descent methods, loss function, annealing methods, and activation functions are given. These options are easy to extend given the structure of MDL. So MDL could be used as a frame for testings in deep learning."}, {"heading": "1.2 Content Description", "text": "Section 2 goes through common networks. Section 3 to section 7 give descriptions of Restricted Boltzmann Machine, Deep Neural Network, Deep Belief Network, Denoising Autoencoder, Deep Boltzmann Machine, and Deep Canonical Correlation Analysis respectively. Section 8 introduces multimodal learning models. Section 9 gives explanation of the library structure. Section 10 presents performance.\nIn sections 3 to 7, each section explains principles and implementations of each model. Section 8 introduces two models in Multimodal learning."}, {"heading": "2 Network Survey", "text": "This section includes a brief description of network models that are mentioned in this document and a summary of deep learning models of MDL."}, {"heading": "2.1 Neural Network", "text": "The Neural Network is a directed graph consists of multiple layers of neurons, which is also referred to as units. In general there is no connection between units of the same layer and there are only connections between adjacent layers. The first layer is the input and is referred to as visible layer v. Above the visible layer there are multiple hidden layers {h1, h2, ..., hn}. And the output of the last hidden layer forms the output layer o.\nIn hidden layers, neurons in layer hi receives input from the previous layer, hi\u22121 or v, and the output\nar X\niv :1\n51 2.\n06 92\n7v 1\n[ cs\n.L G\n] 2\n2 D\nec 2\n01 5\nof hi is the input to the next layer, hi+1 or o. The value transmitted between layers is called activation or emission. Activation is computed as:\na (k) i = f(( nk\u22121\u2211 j=1 a (k\u22121) j w k ij) + b (k) i ) = f(z (k) i ) (1)\nwhere g is the activation function that enables nonlinear representation of the Neural Network. Without activation function the network could only represent linear combination of the input and its power would be much weaker. A common activation function is sigmoid function. z(k)i is the total input of the unit i in layer k. It is computed as the weighted sum of the activations of the previous layer. The weight wkij and the bias bki is learned in bakcpropagation.\nA process called forward propagation computes activations layer by layer. Learning of neural network uses backpropagation, which is a supervised learning algorithm. It computes error based on the network output and the training label, then uses this error to compute gradient of error with respect to the weights and biases of each layer. The model parameters are updated by gradient descent."}, {"heading": "2.2 Markov Random Field", "text": "A Markov Random Field, also called the Markov Network, is an undirected graphical model in which each node is independent of the other nodes given all the nodes connected to it. It describes the distribution of variables in the graph.\nThe Markov Random Field uses energy to decribe the distribution over the graph\nP (u) = 1\nZ e\u2212E(u), (2)\nwhere Z is a partition function defined by\nZ = \u2211 u e\u2212E(u), (3)\nE is the energy specified by the model, and u is the set of states of all the nodes such that\nu = {v1, v2, ..., vn} (4) where vi is the state of node i."}, {"heading": "2.3 Belief Network", "text": "The Belief Network, which is also referred to as the Bayesian Network, is a directed acyclic graph for probabilistic reasoning. It exhibits the conditional dependencies of the model by associating each node X with a conditional probability P (X|Pa(X)) where Pa(X) denotes the parents of X . Here are two of its conditional independence property:\n1. Each node is conditionally independent of its non-descendants given its parents. 2. Each node is conditionally independent of all other nodes given its Markov blanket, which consists of its parents, children, and children\u2019s parents.\nThe inference of Belief Network is to compute the posterior probability distribution\nP (H|V ) = P (H,V )\u2211 H P (H,V )\n(5)\nwhere H is the set of query variables that forms hidden units, and V is the set of evidence variables that forms visible units. Approximate inference involves sampling to compute posterior.\nThe Sigmoid Belief Network [29] is a type of the Belief Network such that P (Xi = 1|Pa(Xi)) = \u03c3( \u2211\nXj\u2208Pa(Xi)\nWjiXj + bi) (6)\nwhere Wji is the weight assigned to the edge from Xj to Xi."}, {"heading": "2.4 Deep Learning Models", "text": "There are many models in deep learning [23]. Below are the ones implemented in MDL.\nThe Restricted Boltzmann Machine is a type of Markov Random Field and is trained in an unsupervised manner. It is the building block for other models and could be used for classification by adding a classifier on top of it.\nThe Deep Neural Network is a neural network with multiple layers. Each layer is initialized by pretraing a Restricted Boltzmann Machine. Then fine tuning would refine the parameters of the model.\nThe Deep Belief Network is a hybrid of the Restricted Boltzmann Machine and the Sigmoid Belief Network. It is a generative model, and is not a feedfoward neural network or multilayer perceptron even though its training is similar to the Deep Neural Network.\nThe Denoising Autoencoder is a type of neural network that has symmetric structure. It could reconstruct the input data and if properly trained could reconstruct corrupted images. However, unlike neural networks, it could be trained in an unsupervised manner.\nThe Deep Boltzmann Machine is another type of Markov Random Field. It is pretrained by stacking Restricted Boltzmann Machines with adjusted weights and biases as an approximation to undirected graphs. Its fine tuninig uses a method called mean field inference.\nThe Deep Canonical Correlation Analysis is used for learning multiview or multimodal data by adding a Kernel Canonical Correlation Analysis layer on top of two networks. It finds the projections of the outputs that maximize their correlations."}, {"heading": "3 Restricted Boltzmann Machine", "text": ""}, {"heading": "3.1 Logic of Restricted Boltzmann Machine", "text": "A Restricted Boltzmann Machine (RBM) [20, 21, 28] is a Markov Random Field consisting of one hidden layer and one visible layer. It is an undirected bipartite graph in which connections are between the hidden layer and the visible layer. Each unit x is a stochastic binary unit such that\nstate(x) =\n{ 1, p\n0, 1\u2212 p (7)\nwhere probability p is defined by the model. Figure 3.1 shows a RBM with four hidden units and six visible units.\nAs a Markov Random Field, a Restricted Boltzmann Machine defines the distribution over the visible layer v and the hidden layer h as\nP (v, h) = 1\nZ e\u2212E(v,h), (8)\nwhere Z is a partition function defined by Z = \u2211 v,h e\u2212E(v,h). (9)\nIts energy E(v, h) is defined by E(v, h) = \u2212 \u2211 i bvi vi \u2212 \u2211 j bhj hj \u2212 \u2211 i \u2211 j viwi,jhj , (10)\nwhere bvi is the bias of the ith visible unit and b h j is the bias of the jth hidden unit.\nGiven the conditional independence property of the Markov Random Field, in RBM probability of one layer given the other layer could be factorized as\nP (h|v) = \u220f j P (hj |v) (11)\nP (v|h) = \u220f i P (vi|h). (12)\nPlug equation (10) in equation (8):\nP (h|v) = P (h, v)\u2211 h\u2032 P (h \u2032, v) =\nexp( \u2211\ni b v i vi + \u2211 j b h j hj + \u2211 i \u2211 j viwi,jhj)\u2211\nh\u2032 exp( \u2211 i b v i vi + \u2211 j b h\u2032 j h \u2032 j + \u2211 i \u2211 j viwi,jh \u2032 j) . (13)\nAnd through derivation one could get exp( \u2211 i b v i vi + \u2211 j b h j hj + \u2211 i \u2211 j viwi,jhj)\u2211\nh\u2032 exp( \u2211 i b v i vi + \u2211 j b h\u2032 j h \u2032 j + \u2211 i \u2211 j viwi,jh \u2032 j) = \u220f j\nexp(bhj + \u2211m i=1Wi,jvi)\n1 + exp(bhj + \u2211m i=1 wi,jvi) . (14)\nCombine equation (11), (13), and (14) one could get\nP (hj = 1|v) = \u03c3 ( bhj + Nv\u2211 i=1 Wi,jvi ) , (15)\nwhere \u03c3 is a sigmoid function\n\u03c3(t) = 1\n1 + exp(\u2212t) . (16)\nSimilarly,\nP (vi = 1|h) = \u03c3 bvi + Nh\u2211 j=1 Wi,jhj  . (17) A Restricted Boltzmann Machine maximizes the likelihood P (x) of the input data x, which is\nP (x) = \u2211 h P (x, h) = \u2211 h 1 Z e\u2212E(h,x) = 1 Z e\u2212F (x), (18)\nwhere F (x) is called Free Energy such that F (x) = \u2212 nv\u2211 i=1 bvi xi \u2212 nh\u2211 j=1 log(1 + exp(bhj + nv\u2211 k=1 Wkjxk)). (19)\nIn training, maximizing the likelihood of the training data is achieved by minimizing the negative log likelihood of the training data. Because the direct solution is intractable, gradient descent is used, in which weights {Wij}, biases of hidden units {bhj }, and biases of visible units {bvi } are updated. The gradient is approximated by an algorithm called Contrastive Divergence. More details are in the training section.\nExpress the hidden states {hj} in a row vector h, hidden biases {bhj } in a row vector bh, and weights {Wij} in a matrix W , which indicates the weight from the visible layer to the hidden layer. The activation of hidden layer is computed as ah = \u03c3(v \u2217W + bh). (20) where \u03c3 is element-wise performed. Restricted Boltzmann Machine acquired tied weights such that\nav = \u03c3(h \u2217WT + bv). (21) In training, the state of the visible layer is initialized as training data."}, {"heading": "3.2 Training of Restricted Boltzmann Machine", "text": "In training of Restricted Boltzmann Machine, the weights and the biases of the hidden and the visible layers are updated by gradient descent. Instead of stochastic gradient descent, in which each update is based on each data sample, batch learning is used in RBM training. In batch learning, each update is based on a batch of training data. There are several epochs in training. Each epoch goes through the training data once.\nFor instance if the input data has 10,000 samples and the number of batches is 200, then there will be 200 updates in each epoch. For each update, gradients will be based on 50 samples. If the number of epochs is 10, commonly there should be a total of 2000 updates in the training process. If the gradients computed are trivial, this process may stop earlier.\nThe gradients of weights are given by Contrastive Divergence as:\n\u2207Wij = \u3008vi \u2217 hj\u3009recon \u2212 \u3008vi \u2217 hj\u3009data (22) where the angle brackets are expectations under the distribution specified by the subscript. The expectations here are approximated by data sample mean. So it would be\n\u2207Wij = m\u2211\nk=1\n((vi \u2217 hj)reconk \u2212 (vi \u2217 hj)datak)/m (23)\nwhere m is the size of each data batch.\nStates of the visible layer and hidden layer form a sample in Gibbs sampling, in which the first sample gives states with the subscript \u201ddata\u201d in equation (22) and the second sample gives states with the subscript \u201drecon\u201d in equation (22). Contrastive Divergence states that one step of Gibbs sampling, which computes the first and the second sample, approximates the descent with high accuracy. In RBM, Gibbs sampling works in the following manner:\nIn Gibbs sampling, each sample X = (x1, . . . , xn) is constructed from a joint distribution p(x1, . . . , xn) by sampling each component variable from its posterior. Specifically, in the (i + 1)th sample X(i+1) = (x\n(i+1) 1 , . . . , x (i+1) n ), x (i+1) j is sampled from\np(Xj |x(i+1)1 , . . . , x (i+1) j\u22121 , x (i) j+1, . . . , x (i) n ), (24)\nin which the latest sampled component variables are used to compute posterior. Sampling each component variable xj once forms a sample.\nEach unit of RBM is a stochastic binary unit and its state is either 0 or 1. To sample hj from\nP (hj = 1|v) = \u03c3 ( bj +\nm\u2211 i=1 Wi,jvi\n) , (25)\nsimply compute aj = \u03c3 (bj + \u2211m\ni=1Wi,jvi). If aj is larger than a random sample from uniform distribution, state of hj is 1, otherwise 0. This method works because the probability that a random sample u from uniform distribution is smaller than aj is aj :\nP (u < aj) = Funiform(aj) = aj . (26)\nSo we have P (hj = 1|v) = P (u < aj). (27)\nThus we could sample by testing if u < aj , since it has probability aj . Each unit has two states. If u < aj fails, the state is 0.\nIn training, first use training data to compute hidden layer posterior with\nP (hj = 1|v) = \u03c3 ( bj +\nm\u2211 i=1 Wi,jvi\n) . (28)\nThe hidden layer state together with the data form the first sample. Then use Gibbs sampling to compute the second sample.\nThe gradient of the visible bias is \u2207bvi = \u3008vi\u3009recon \u2212 \u3008vi\u3009data, (29)\nand the gradient of the hidden bias is\n\u2207bhj = \u3008hj\u3009recon \u2212 \u3008hj\u3009data (30)"}, {"heading": "3.3 Tricks in Restricted Boltzmann Machine Training", "text": "Dropout Dropout [11] is a method to prevent neural networks from overfitting by randomly blocking emissions from certain neurons. It is similar to adding noise. In RBM training, a mask is generated and put on the hidden layer.\nFor instance, suppose the hidden states are\nh = {h1, h2, ..., hn} (31)\nand the dropout rate is r. Then a mask m is generated by\nmi =\n{ 1, ui > r\n0, ui \u2264 r (32)\nwhere ui is a sample from uniform distribution, and i \u2208 {1, 2, ..., n}. The emission of hidden layer would be\nh\u0303 = h. \u2217m (33) where .\u2217 denotes element-wise multiplication of two vectors. h\u0303, instead of h, is used to calculate visible states.\nLearning Rate Annealing There are multiple methods to adapt the learning rate in gradient descent. If the learning rate is trivial, updates may tend to stuck in local minima and waste computation. If it is too large, the updates may bound around minima and could not go deeper. In annealing, learning rate decay helps ensure the learning rate is not too large.\nSuppose \u03b1 is the learning rate. In exponential decay, the annealed rate is\n\u03b1a = \u03b1 \u2217 e\u2212kt, (34)\nwhere t is the index of the current epoch, k is a customized coefficient. In divide decay, the annealed rate is\n\u03b1a = \u03b1/(1 + kt). (35)\nA more common method is step decay: \u03b1a = \u03b1 \u2217 0.5bt/5c. (36)\nwhere learning rate is reduced by half every five epochs. The coefficients in the decay method should be tuned in testings.\nMomentum With momentum \u03c1, the update step is\n\u2206t+1W = \u03c1 \u2217\u2206tW \u2212 r\u2207W. (37)\nThe update value is a portion of previous update value minus the gradient. The intuition behind it is that if the gradient has the same direction as previous update, the update will become larger. If the the gradient is in the different direction from the previous update, the current update will not have the same direction as the gradient and its variance is reduced. In this way, time to converge is reduced.\nMomentum is often applied with annealing so that steps of updates will not be too large. A feasible scheme is\n\u03c1 =\n{ 0.5, t < 5\n0.9, t \u2265 5 (38)\nwhere t is the index of the current epoch.\nWeight Decay In weight decay, a penalty term is added to the gradient as a regularizer. L1 penalty p1 is\np1 = k \u00d7 \u2211 i,j |Wij |. (39)\nL1 penalty causes many weights to become zero and a few weights to become large. L2 penalty p2 is p2 = k \u00d7 \u2211 i,j W 2ij . (40)\nL2 penalty causes weights to become more even and smaller. The coefficient k is customized, sometimes 0.5 would work. Penalty must be, as well as the gradient, multiplied by the learning rate so that annealing will not change the model trained."}, {"heading": "3.4 Classifier of Restricted Boltzmann Machine", "text": "A classifier based on RBM could be constructed by training a classifier layer with softmax activation function on top of the hidden layer. Softmax activation function takes a vector a = {a1, a2, ..., aq} as input, and outputs a vector c = {c1, c2, ..., cq} with the same dimension, specifically\nci = eai\u2211q\nk=1 e ak . (41)\nHere one-of-K scheme is used to present the class distribution. If there are K classes in the training data, then the label of a sample in class i is expressed as a vector of length K with only the ith element as 1, the others as 0. For instance, if the training data has 5 classes, a sample in the forth class has the label\n{0, 0, 0, 1, 0}.\nFor a training set with K classes, there should be K neurons in the classifier layer. For each data sample, the sofxmax activation emits a vector of length K. The index of the maximum element in this emission is the label. The elements of the softmax activation sum to 1 and the activation is the prediction distribution:\nci = P{The sample is in class i}. (42)\nSoftmax activation takes input of dimension K whereas the hidden layer may have dimension of more than 500. So the projection from the hidden layer to the classifier should be learned. If hidden layer has dimension nh and there are K classes, the weight of this projection should be a matrix of dimension nh \u00d7K.\nBackpropagation is used to compute this weight. Among several loss functions, cross entropy loss is a good choice for classification using softmax, which is justified by papers comparing various combinations of activation function and loss function. If the label is presented in one-of-K scheme as a vector t, and prediction distribution is c, cross entropy loss is\nL = \u2212 K\u2211 i=0 tilog(ci). (43)\nUse chain rule: \u2202L\n\u2202Wij = K\u2211 p=1 \u2202L \u2202cp \u2202cp \u2202Wij\n(44)\nand \u2202cp \u2202Wij = K\u2211 q=1 \u2202cp \u2202zq \u2202zq \u2202Wij\n(45)\nwhere c is the output of softmax activation and z is its input. That is to say,\nci = exp(zi)\u2211K\np=1 exp(zp) . (46)\nFurthermore \u2202L\n\u2202cp = \u2212 tp cp (47)\n\u2202cp \u2202zq = { cp(1\u2212 cp) if p = q \u2212cp \u00d7 cq if p 6= q\n(48)\n\u2202zq \u2202Wij = { ahi if q = j 0 if q 6= j (49)\nwhere ahi is the activation of hidden layer. Based on above equations, for combination of cross entropy loss and softmax activation\n\u2202L\n\u2202Wij = ahi (cj \u2212 tj) (50)\nExpressed in row vectors it is \u2207W = (ah)T \u00d7 (c\u2212 t) (51)\nwhere (ah)T is the transpose of row vector ah. This gradient is used in batch learning."}, {"heading": "3.5 Implementation of RBM", "text": "My implementation of RBM is in the header file rbm.hpp. The RBM class stores information of one hidden layer and one activation layer. Here is a selected list of methods of class rbm:\nI. void dropout(double i) -Set dropout rate as input i.\nII. void doubleTrain(dataInBatch &trainingSet, int numEpoch, int inLayer, ActivationType at = sigmoid t, LossType lt = MSE, GDType gd = SGD, int numGibbs = 1) -Train an RBM layer in Deep Boltzmann Machine, which is an undirected graph. This part will be explained in DBM section.\nIII. void singleTrainBinary(dataInBatch &trainingSet, int numEpoch, ActivationType at = sigmoid t, LossType lt = MSE, GDType gd = SGD, int numGibbs = 1); -Train an RBM layer with binary units in Deep Belief Networks and RBM.\nIV. void singleTrainLinear(dataInBatch &trainingSet, int numEpoch, ActivationType at= sigmoid t, LossType lt = MSE, GDType gd = SGD, int numGibbs = 1); -Train an RBM layer with linear units.\nV. void singleClassifier(dataInBatch &modelOut, dataInBatch &labelSet, int numEpoch, GDType gd = SGD); -Build a classifier layer for RBM.\nThe model could be tested by running runRBM.cpp. First train a RBM with one hidden layer:\nRBM rbm(784, 500, 0); rbm.dropout(0.2); rbm.singleTrainBinary(trainingData, 6); dataInBatch modelOut = rbm.g activation(trainingData);\nThe hidden layer has 500 units, and the index of this RBM is 0, which is used for multi-layer model. Then set the dropout rate as 0.2 and train it with 6 epochs. After training, stack another RBM layer with softmax activation function:\nRBM classifier(500, 10, 0); classifier.singleClassifier(modelOut, trainingLabel, 6); classificationError e = classifyRBM(rbm, classifier, testingData, testingLabel, sigmoid t);\nMNIST dataset has 10 classes, so the classifier layer is of dimension 10."}, {"heading": "3.6 Summary", "text": "RBM is the foundation for several multi-layer models. It is crucial that this component is correctly implemented and fully understood. The classifier may be trained with the hidden layer at the same time. Separating these two facilitates checking problems in the implementation."}, {"heading": "4 Deep Neural Network", "text": ""}, {"heading": "4.1 Construction of Deep Neural Network", "text": "A Deep Neural Network (DNN) [8] is a neural network with multiple hidden layers. In neural networks, initialization of weights could greatly effect the training results. Pretraining, in which multiple Restricted Boltzmann Machines are trained to initialize parameters of each DNN layer, provides weight initialization that saves training time. Backpropagation is a time-consuming process. With pretraining, the time consumed by bakckpropagation could be significantly reduced. Figure 4.1 shows a DNN with two hidden layers.\nBelow shows construction of a Deep Neural Network for classification:\nI. Set the architecture of the model, specifically the size of the visible layer and the hidden layers, n0, n1, n2, ..., nN . n0 is the input dimension and input forms a visible layer. nN equals to the number of classes in training data.\nII. Pretrain hidden layers: for i = 1 to N : 1. Train an RBM with the following settings:\nn (i) h = ni\nn(i)v = ni\u22121\ndi = ai\u22121\nwhere\nn (i) h = Dimension of hidden layer of RBM trained for layer i,\nn(i)v = Dimension of visible layer of RBM trained for layer i, di = Input of RBM trained for layer i,\nai\u22121 = Activations of the (i\u2212 1)th DNN layer. 2. Set\nWi =WRBM ,\nbi = b h RBM ,\nai = aRBM .\nRBM is used to initialize weights and biases of each DNN layer. end for.\nIII. Fine Tuning: Use backpropagation to refine weights and biases of each layer. In backpropagation, one epoch goes through training data once. A dozen of epochs may suffice.\nClassification with Deep Neural Network is similar to RBM. The last layer, which uses softmax activation, gives the prediction distribution."}, {"heading": "4.2 Fine Tuning of Deep Neural Network", "text": "As mentioned in the above section, fine tuning uses backpropagation, which is a common learning algorithm used in neural networks. Unlike one-step backpropagation used in training classifier of RBM, this one is a thorough one going through each layer. Backprogation algorithm is:\nI. Perform a pass through all layers of the network, computing total input of each layer {z(1), ..., z(N)} and activations {a(1), ..., a(N)} of each layer. a(i) is the row vector that represents the activation of layer i.\nII. For the last layer, compute \u03b4(N)i as\n\u03b4 (N) i =\n\u2202L\n\u2202z (N) i\n(52)\nwhere L is the classification error. Acquire a row vector \u03b4(N).\nIII. For l = N \u2212 1, ..., 1, compute\n\u03b4(l) = ( \u03b4 (W (l))T ) \u2022 g\u2032(z(l)) (53)\nwhere g is the activation function.\nIV. Compute the gradients in each layer. For l = N, ..., 1, compute\n\u2207W (l)L = (a(l\u22121))T \u03b4(l), (54) \u2207b(l)L = \u03b4(l). (55)\nwhere a(0) is the training data.\nV. Update the weights and biases of each layer using gradient descent.\nIn fine tuning, the training data should be used repeatedly to refine the model parameters."}, {"heading": "4.3 Implementation of Deep Neural Network", "text": "The implementation of Deep Neural Network is in the header file dnn.hpp. It uses class RBMlayer to store architecture information. Here is a selected list of methods of class dnn:\nI. void addLayer(RBMlayer &l) Add a layer to the current model. This object of class RBMlayer should store information of layer size, weight, bias, etc. It could also be modified after added to the model.\nII. void setLayer(std::vector<size t> rbmSize) Object of class dnn could automatically initialize random weights and biases of each layer by inputting a vector of layer sizes.\nIII. void train(dataInBatch &trainingData, size t rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t) This method trains all the layers without classifier. The structure of dnn should be initialized before calling this method.\nIV. void classifier(dataInBatch &trainingData, dataInBatch &trainingLabel, size t rbmEpoch, int preTrainOpt, LossType l = MSE, ActivationType a = sigmoid t) Build a Deep Neural Network with a classifier layer. This function contains pretraining option preTrainOpt. If preTrainOpt=1, pretrain each layer by training RBMs, else randomly initialize layer parameters without pretraining.\nV. void fineTuning(dataInBatch &label, dataInBatch &inputSet, LossType l) Fine tuning step that uses backpropagation.\nVI. classificationError classify(dataInBatch &testingSet, dataInBatch &testinglabel); Perform Classification. The result is stored in the format classficationError."}, {"heading": "4.4 Summary", "text": "Construction of the Deep Neural Network is stacking multiple RBMs in the pretraining process and then performing fine tuning. Because the Deep Neural Network is a directed graph and each layer receives input from the previous adjacent layer, there is no extra inference in training this model."}, {"heading": "5 Deep Belief Network", "text": ""}, {"heading": "5.1 Logic of Deep Belief Network", "text": "A Deep Belief Network (DBN) is a hybrid of a Restricted Boltzmann Machine and a Sigmoid Belief Network. A Deep Belief Network maximizes the likelihood P (x) of the input x. Figure 5.1 shows a DBN.\nFor a Deep Belief Network with N hidden layers, the distribution over the visible layer (input data) and hidden layers is\nP (v, h1, ..., hN ) = P (v|h1)\u00d7 ( N\u22122\u220f k=1 P (hk|hk+1)) ) \u00d7 P (hN\u22121, hN ). (56)\nTo prove this, express the distribution with chain rule:\nP (v, h1, ..., hN ) = P (v|h1, ..., hN )\u00d7 ( N\u22122\u220f k=1 P (hk|hk+1, ..., hN )) ) \u00d7 P (hN\u22121, hN ). (57)\nAnd in the belief network, each node is independent of its ancestors given its parent. So these hold:\nP (v|h1, ..., hN ) = P (v|h1), (58) P (hk|hk+1, ..., hN ) = P (hk|hk+1). (59)\nSo we have\nP (v, h1, ..., hN ) = P (v|h1)\u00d7 ( N\u22122\u220f k=1 P (hk|hk+1)) ) \u00d7 P (hN\u22121, hN ), (60)\nwhere P (v|h1) \u00d7 (\u220fN\u22122 k=1 P (hk|hk+1)) )\nis the distribution over the Sigmoid Belief Network and P (hN\u22121, hN ) is the distribution over Restricted Boltzmann Machine.\nFor classification there should be a layer y on top of the last hidden layer. With layer y that represents prediction distribution, the distribution over the Deep Belief Network is\nP (v, h1, ..., hN , y) = P (v|h1)\u00d7 ( N\u22122\u220f k=1 P (hk|hk+1)) ) \u00d7 P (hN\u22121, hN , y). (61)\nwhere P (hN\u22121, hN , y) could be regarded as the distribution over a RBM which has labels y and the state hN\u22121 as the input.\nIn pretraining of the Deep Belief Network, pretrained RBMs are stacked like in pretraining the Deep Neural Network. However, since this is not a feedforward neural network. A different fine tuning method called Up-Down algorithm is used."}, {"heading": "5.2 Training of Deep Belief Network", "text": "In training, the Deep Belief Network should maximize the likelihood of the training data. With the concavity of the logarithm function, the lower bound of the log likelihood of the training data x could be found:\nlogP (x) = log (\u2211 h Q(h|x)P (x, h) Q(h|x) ) \u2265 \u2211 h Q(h|x) log P (x, h) Q(h|x) , (62)\nand we have \u2211 h Q(h|x) log P (x, h) Q(h|x) = \u2211 h Q(h|x) logP (x, h)\u2212 \u2211 h Q(h|x) logQ(h|x), (63)\nwhere Q(h|x) is an approximation to the true probability P (h|x) of the model.\nIf Q(h|x) = P (h|x), plug it in the right-hand side of (63) we have\u2211 h P (h|x)(logP (h|x) + logP (x))\u2212 \u2211 h P (h|x) logP (h|x)\n= \u2211 h P (h|x) logP (x) = logP (x) \u2211 h P (h|x) = logP (x). (64)\nCombine equation (64), (63) with (62), we could find that when Q(h|x) = P (h|x), the lower bound is tight.\nMoreover, the more different Q(h|x) is from P (h|x), the less tight the bound is. The lower bound of (62) could be expressed as\nlogP (x)\u2212KL(Q(h|x)||P (h|x)). (65)\nLess difference between the approximation Q(h|x) and the true posterior P (h|x) gives lower value of their KL divergence, thus higher bound. Unlike true posterior, the approximations could be factorized\nQ(h|x) = nh\u220f i=1 Q(hi|x). (66)\nConsequently, in training the goal is to find approximation Q(h|x) with high accuracy and at the same time maximizes the bound. This could be done by stacking pretrained RBMs. The lower bound of (62) could be factorized as\u2211\nh\nQ(h|x) log P (x, h) Q(h|x) = \u2211 h Q(h|x)(logP (x|h) + logP (h))\u2212 \u2211 h Q(h|x) logQ(h|x) (67)\nIn the right-hand side of equation (67), Q(h|x) and P (x|h) are given by the first pretrained RBM. So to maximize the lower bound is to maximize \u2211\nh\nQ(h|x) logP (h). (68)\nA RBM maximizes the likelihood of the input data. So staking another pretrained RBM on top of the first hidden layer would maximize the lower bound. Moreover,\nP (h) = \u2211 h(2) P (h, h(2)), (69)\nwhere h(2) is computed by the second pretrained RBM. The second RBM takes sample constructed from Q(h|x) as input. But it could be trained independently since its parameters do not depend on the parameters of the first pretrained RBM. This is why greedy layer-wise pretraining works.\nAfter pretraining is done, use Up-Down algorithm [13] as fine tuning, which is a combination of the training of RBM, and an algorithm called Wake-Sleep algorithm [12] which is for learning of the Sigmoid Belief Network."}, {"heading": "5.3 Classification of Deep Belief Network", "text": "In training the Restricted Boltzmann Machine, dropout is used to alleviate overfitting. This method reminds us that RBM has the ability to predict missing values.\nIn a trained deep belief network, each approximation Q(hk+1|hk) could be computed based on states hk and model parameters. In a deep belief network with classifier, the top RBM takes labels and hidden layer hN\u22121 to compute states of the last hidden layer hN , which is illustrated in Figure 5.2. For better prediction performance, when training the top RBM, dropout is used.\nSuppose l is the set of units that represent prediction distribution. In classification, fill l with zeros and compute approximation Q(hN |l, hN\u22121). Then use the states hN sampled from Q(hN |l, hN\u22121) to compute prediction distribution l by P (l, hN\u22121|hN )."}, {"heading": "5.4 Implementation of Deep Belief Network", "text": "The implementation of Deep Belief Network is in the header file dbn.hpp. It uses class RBMlayer to store architecture information. Here is a selected list of methods of class dnn:\nI. void addLayer(RBMlayer &l) Add a layer to the current model. This object of class RBMlayer should store information of layer size, weight, bias, etc. It could also be modified after added to the model.\nII. void setLayer(std::vector<size t> rbmSize) Object of class dnn could automatically initialize random weights and biases of each layer by inputting a vector of layer sizes.\nIII. void train(dataInBatch &trainingData, size t rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t) This method trains a dbn without classifier. The architecture of dbn should be initialized before calling this method.\nIV. void classifier(dataInBatch &trainingData, dataInBatch &trainingLabel, size t rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t) This method trains a dbn with classifier. The architecture of dbn should be initialized before calling this method.\nV. void fineTuning(dataInBatch &dataSet, dataInBatch &labelSet, int epoch) The fine tuning uses Up-Down algorithm.\nVI. classificationError classify(dataInBatch &testingSet, dataInBatch &testinglabel); Perform Classification."}, {"heading": "5.5 Summary", "text": "It is easy to confuse the Deep Belief Network with the Deep Neural Network. Both of them stack pretrained RBMs in the training process. However, because these two models have distinct structures, their classification processes are different. In the Deep Neural Network, forward propagation gives the prediction distribution whereas in the Deep Belief Network the prediction distribution is computed by one more projection from the last hidden layer. They also use different fine tuning methods."}, {"heading": "6 Denoising Autoencoder", "text": ""}, {"heading": "6.1 Construction of Autoencoder", "text": "Autoencoder(AE) [14] is a type of neural network forming a directed graph. Its symmetricity states that for an autoencoder with (N + 1) layers (including visible layer and output layer), the dimension of each layer is constrained by\nni = nN\u2212i for 0 \u2264 i \u2264 N (70) where ni is the dimension of the ith layer, and n0 is the input dimension. Since the output layer and the visible layer are in the same dimension, it is expected that autoencoders could reconstruct the input data. Thus the training of autoencoders is unsupervised learning because input data is used as labels in fine tuning, and reconstruction errors could be used to access the model. Autoencoders could also be used to construct classifiers by adding a classifier layer on top of it. Figure 6.1 shows an Autoencoder.\nBelow is how to construct an autoencoder: I. Set the architecture of the model, specifically the size of each layer, n0, n1, n2, ..., nN .\nII. Pretraining:\nfor i = 1 to N/2: 1. Train an RBM with the following settings:\nn (i) h = ni\nn(i)v = ni\u22121\ndi = ai\u22121\nwhere\nn (i) h = Dimension of hidden layer of RBM trained for layer i,\nn(i)v = Dimension of visible layer of RBM trained for layer i, di = Input of RBM trained for layer i,\nai\u22121 = Activations of the (i\u2212 1)th layer of autoencoder,\n2. Initialize parameters of the current layer\nWi =WRBM ,\nbi = b h RBM ,\nai = aRBM .\nThe parameters of trained RBM are used to initialize the parameters of the layer. end for.\nfor i = N/2 + 1 to N : Initialize paramers\nWi =W T N\u2212i,\nbi = bN\u2212i.\nend for.\nIII. Fine Tuning: Backpropagation with Mean Square Error. Error is computed based on the reconstruction and the training data."}, {"heading": "6.2 Fine tuning of Autoencoder", "text": "Fine tuning of Autoencoder uses backpropagation, which is:\nI. Perform a forward propagation through all layers that computes layer inputs {z(1), ..., z(N)} and activations {a(1), ..., a(N)}. a(i) is a row vector representing the activation of layer i.\nII. For the last layer, compute \u03b4(N)i as\n\u03b4 (N) i =\n\u2202L\n\u2202z (N) i\n(71)\nwhere L is the reconstruction error. This step acquires a row vector \u03b4(N).\nIII. For l = N \u2212 1, ..., 1, compute \u03b4(l) = ( \u03b4(l+1) (W (l))T ) \u2022 g\u2032(z(l)) (72)\nwhere g is the activation function and here g\u2032 is element-wise performed.\nIV. Compute the gradients in each layer. For l = N, ..., 1, compute\n\u2207W (l)L = (a(l\u22121))T \u03b4(l), (73) \u2207b(l)L = \u03b4(l). (74)\nwhere a(0) is the input data of the autoencoder.\nV. Update the weights and biases of each layer with gradient descent.\nThe reconstruction error of Autoencodr is\nL = 1\n2 nN\u2211 i=1 (a (N) i \u2212 a (0) i ) 2 (75)\nwhere\nnN = Dimension of the output/reconstruction,\na (N) i = Activation of the ith unit in the output layer,\na (0) i = Activation of the ith unit in the visible layer.\nBackpropagation involves computing {\u03b4(i)}, which is based on the derivatives of activation function and error function. Here is how to compute these two values:\nFor sigmoid activation:\ng\u2032(t) = \u2202(1 + e\u2212t)\u22121\n\u2202t =\n1 1 + e\u2212t e\u2212t 1 + e\u2212t . (76)\nThat is to say\ng\u2032(zi) = 1 1 + e\u2212zi (1\u2212 1 1 + e\u2212zi ) = ai(1\u2212 ai). (77)\nFor \u03b4(N)i , use chain rule\n\u03b4 (N) i =\n\u2202L\n\u2202z (N) i\n= nN\u2211 p=1 \u2202L \u2202a (N) p \u2202a (N) p \u2202z (N) i = \u2202L \u2202a (N) i \u2202a (N) i \u2202z (N) i = (a (N) i \u2212 a (0) i )\u00d7 a (N) i (1\u2212 a (N) i ). (78)"}, {"heading": "6.3 Denoising Autoencoder", "text": "The Denoising Autoencoder reconstructs the input from its corrupted version. So it could predict missing values and it is quite straightforward to observe its performance when the input is image data. By putting a denoise mask on the input, the Autoencoders could be transformed to the Denoising Autoencoders. Here is how to make this transformation:\nFirstly a denoise rate r is chosen, and the mask is constructed as follows:\nmi = { 1, if Ui > r 0, otherwise\n1 \u2264 i \u2264 nN (79)\nwhere Ui is the ith sample from uniform distribution, nN is the size of the last layer, which is also the dimension of input data.\nSecondly compute the corrupted input data\na(c) = a(0) \u00b7m = nN\u2211 i=1 a (0) i mi (80)\nFinally use a(c) as the training data to compute the reconstruction and still use the uncorrupted data a(0) as labels in fine tuning.\nFine tuning in the Denoising Autoencoder makes more improvement in reconstruction than in the Autoencoder, and is crucial in the Denoising Autoencoder."}, {"heading": "6.4 Implementation of Denoising Autoencoder", "text": "The implementation of DAE is in the header file autoencoder.hpp.\nI. void addLayer(RBMlayer &l) Add a layer to current DAE. This object of class RBMlayer should store information of layer size, weight, bias, etc. It could also be modified after added to DAE.\nII. void setLayer(std::vector<size t> rbmSize) Object of class AutoEncoder could automatically initialize random weights and biases of each layer by inputting a vector of layer sizes.\nIII. void train(dataInBatch &trainingData, size t rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t) This method trains all the layers without classifier. The structure of this AutoEncoder object should be initialized before calling this method.\nIV. void reconstruct(dataInBatch &testingData) Give the reconstruction of the testingData and stores the result in the model. This method should be called only after the model has been trained.\nV. dataInBatch g reconstruction() Get the reconstruction.\nVI. void fineTuning(dataInBatch &originSet, dataInBatch &inputSet, LossType l) Use backpropagation. Unlike DBN, argument LossType should be MSE instead of CrossEntropy.\nVI. void denoise(double dr) Set the denoise rate as dr. When model is in training, it will detect if denoise rate is set. So if this method is called before training, the Denoising Autoencoder will be trained automatically. Otherwise the Autoencoder will be trained."}, {"heading": "6.5 Summary", "text": "Construction of the Denoising Autoencoder requires pretraining half of its layers, the other half is set by its symmetric structure. Fine tuninig is crucial for Denoising Autoencoder because it uses uncorrupted data to modify the model trained with corrupted data. The performance of Denoising Autoencoder is straightforward to assess because one could observe the reconstructed images."}, {"heading": "7 Deep Boltzmann Machine", "text": ""}, {"heading": "7.1 Logic of Deep Boltzmann Machine", "text": "A Deep Boltzmann Machine(DBM) [26] is a Markov Random Field consisting of multiple layers. Connections exist only between adjacent layers. Intuitively, it could incorporate top-down feedback when computing bottom-up approximations. Figure 7.1 shows a DBM.\nThe energy function of a Deep Boltzmann Machine with N hidden layers is\nE(v, h(1), ..h(N)) = \u2212vTW (1)h(1) \u2212 (h(1))TW (2)h(2) \u2212 ...\u2212 (h(N\u22121))TW (N)h(N) (81)\nwhere W (i) is the weight from the previous layer to the ith hidden layer.\nA Deep Boltzmann Machine maximizes the likelihood of the input data. The gradient of its log likelihood is\n\u2202 logP (v)\n\u2202W (i) = \u3008h(i\u22121)(h(i))T \u3009data \u2212 \u3008h(i\u22121)(h(i))T \u3009model. (82)"}, {"heading": "7.2 Pretraining of Deep Boltzmann Machine", "text": "Because the Deep Boltzmann Machine is an undirected model, the last hidden layer receives input from the previous adjacent layer, and the other hidden layers receive inputs from both directions. So when training Restricted Boltzmann Machines, the weights and biases need to be adjusted for better approximations. The pretraining process is as below:\nI. Set the architecture of the model, specifically the size of each layer, n0, n1, n2, ..., nN . n0 is the dimension of the training data.\nII. Pretrain the first hidden layer: Train an RBM, in which the weight from the visible layer v to the hidden layer h1 is 2W1 and the weight from h1 to v is WT1 . W1 is the weight of the first DBM hidden layer.\nIII. Pretrain intermediate hidden layers: for i = 2 to N \u2212 1: 1. Train an RBM with the following settings:\nn (i) h = ni\nn(i)v = ni\u22121\ndi = ai\u22121\nwhere\nn (i) h = Dimension of hidden layer of RBM trained for layer i,\nn(i)v = Dimension of visible layer of RBM trained for layer i, di = Input of RBM trained for layer i,\nai\u22121 = Activations of the (i\u2212 1)th layer.\n2. Set\nWi =WRBM/2,\nbi = b h RBM/2,\nai = aRBM .\nWeights and biases are adjusted here for better approximations.\nIV. Pretrain the last hidden layer: Train an RBM, in which the weight from the hidden layer hN\u22121 to the hidden layer hN is WN and the weight from hN to hN\u22121 is 2WTN . WN is the weight of the last hidden layer of DBM."}, {"heading": "7.3 Mean Field Inference", "text": "The mean field inference [27] of the Deep Boltzmann Machine involves iterative updates of the approximations Q. It is performed after pretraining. The algorithm is as below:\nAlgorithm Mean Field Inference Initialize M samples {v\u03030,1, h\u03030,1},...,{v\u03030,M , h\u03030,M}with the pretrained model. Each sample consists of states of the visible layer and all the hidden layers. for t = 0 to T (number of iterations) do 1. Variational Inference: for each data sample vn, n = 1 to D do Perform a bottom-up pass with\n\u03bd1j = \u03c3 ( n0\u2211\ni=1\n2W 1ijvi\n) ,\n\u03bd2k = \u03c3 ( n1\u2211\nj=1\n2W 2jk\u03bd 1 j\n) ,\n\u00b7 \u00b7 \u00b7\n\u03bdN\u22121p = \u03c3 ( nN\u22122\u2211\nl=1\n2WN\u22121lp \u03bd N\u22122 l\n) ,\n\u03bdNq = \u03c3 ( nN\u22121\u2211\np=1\nWNpq\u03bd N\u22121 p\n) ,\nwhere {Wij} is the set of pretrained weights. Set \u00b5 = \u03bd and run the mean-field updates with:\n\u00b51j \u2190 \u03c3 ( n0\u2211\ni=1\nW 1ijvi + n2\u2211 k=1 W 2jk\u00b5 2 k ) ,\n\u00b7 \u00b7 \u00b7\n\u00b5N\u22121j \u2190 \u03c3 ( nN\u22122\u2211\ni=1\nWN\u22121ij \u00b5 N\u22122 i + nN\u2211 k=1 WNjk\u00b5 N k ) ,\n\u00b5Nj \u2190 \u03c3 ( nN\u22121\u2211\ni=1\nWNij \u00b5 N\u22121 i\n) .\nSet \u00b5n = \u00b5.\nend for"}, {"heading": "2. Stochastic Approximation:", "text": "for each sample m = 1 to M do\nRunning one step Gibbs sampling. Get (v\u0303t+1,m, h\u0303t+1,m) from (v\u0303t,m, h\u0303t,m) end for\n3. Parameter Update: W 1t+1 =W 1 t + \u03b1t ( 1 D \u2211D n=1 vn(\u00b5 1 n) T \u2212 1M \u2211M m=1 v\u0303t+1,m(h\u0303 1 t+1,m) T )\nW 2t+1 =W 2 t + \u03b1t ( 1 D \u2211D n=1 \u00b5 1 n(\u00b5 2 n) T \u2212 1M \u2211M m=1 h\u0303 1 t+1,m(h\u0303 2 t+1,m) T )\n\u00b7 \u00b7 \u00b7 WnNt+1 =W nN t + \u03b1t ( 1 D \u2211D n=1 \u00b5 nN\u22121 n (\u00b5nNn ) T \u2212 1M \u2211M m=1 h\u0303 nN\u22121 t+1,m(h\u0303 nN t+1,m) T ) Decrease \u03b1t. end for\nTo facilitate computation, M may be chosen as the number of batches."}, {"heading": "7.4 Implementation of Deep Boltzmann Machine", "text": "The implementation of DBM is in the header file dbm.hpp.\nI. void addLayer(RBMlayer &l) Add a layer to current DBM. This object of class RBMlayer should store information of layer size, weight, bias, etc. It could also be modified after added to DBM.\nII. void setLayer(std::vector<size t> rbmSize) Object of class DBM could automatically initialize random weights and biases of each layer by inputting a vector of layer sizes.\nIII. void train(dataInBatch &Data, dataInBatch &label, size t rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t) This method trains DBM as classifier.\nIV. void fineTuning(dataInBatch &data, dataInBatch &label) Fine tuning uses mean field inference.\nV. void classify(dataInBatch &data, dataInBatch &label) Classify the data with DBM. Tesing label is used to compute classification error rate."}, {"heading": "7.5 Summary", "text": "In DBM, each layer receives input from all its adjacent layers. Its training is more complicated than other models."}, {"heading": "8 Multimodal Learning Model", "text": ""}, {"heading": "8.1 Deep Canonical Correlation Analysis", "text": ""}, {"heading": "8.1.1 Canonical Correlation Analysis", "text": "Canonical Correlation Analysis [10] is a method to find the relationship between two sets of variables. Specifically, suppose for two sets of variables X and Y , in which each column consists of samples of a variable, there are two projections\nU = aTX, (83)\nV = bTY. (84)\nThe correlation between U and V is\ncorr(U, V ) = Cov(U, V )\u221a var(U)var(V ) . (85)\nThe goal is to find the projections a and b that maximize this correlation. To ensure a unique solution, two conditions are added to the goal\nV ar(U) = 1, (86) V ar(V ) = 1. (87)\nCentering the data will not change the result but could facilitate computation:\nx = X \u2212 IXITXX/nX , (88) y = Y \u2212 IY ITY Y/nY . (89)\nwhere IX is an nX \u00d7 1 vector of ones, nX is the number of rows of X , and nY is the number of rows of Y . And the covariance matrix of centered data x is\nCov(x, x) = V ar(x) = xTx/(nx \u2212 1). (90)\nThe solution [15] to this problem suggests that a and b are respectively the eigenvectors of the following matrices\nCov(x, x)\u22121Cov(x, y)Cov(y, y)\u22121Cov(y, x), (91)\nCov(y, y)\u22121Cov(y, x)Cov(x, x)\u22121Cov(x, y). (92)\nThe ith eigenvectors of each matrix form the projections weights giving the ith largest correlation, and its corresponding eigenvalue is the square of its correlation value.\nIn computing, one could add each column of y to x and form a matrix m = [x y]. Then\nV ar(m) = [ Cov(x, x) Cov(x, y) Cov(y, x) Cov(y, y) ] . (93)\nCCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34]."}, {"heading": "8.1.2 Kernel Canonical Correlation Analysis", "text": "Kernel Canonical Correlation Analysis [16, 19] projects the data into a higher-dimensional feature space before linear projections using the mapping\n\u03c6 : x\u2192 \u03c6(x). (94)\nIn computation, kernel K(x, y) is used, which is defined as\nK(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009 (95)\nwhere \u3008a, b\u3009 indicates inner product of a and b. The solution suggests that projection a is the top eigenvectos of the matrix\n(K(X,X) + rXIXI T X) \u22121K(Y, Y )(K(Y, Y ) + rY IY I T Y ) \u22121K(X,X), (96)\nwhere rX and rY are regularized terms. And b is given by\nb = 1\n\u03bb (K(Y, Y ) + rY IY I\nT Y ) \u22121K(X,X)a (97)\nwhere \u03bb is the corresponding eigenvalue of a."}, {"heading": "8.1.3 Deep Correlation Analysis", "text": "In Deep Correlation Analysis (DCCA) [2,18,33], A layer of KCCA is added on top of two separately trained Deep Neural Networks, of which each learns the data of one modal. Figure 8.1 shows a example of DCCA. With KCCA on the top, the model could learn the correlation between data of two modals. Deep Canonically Correlated Autoencoders (DCCAE) [32] is an extension of DCCA. Compare to DCCA, it could find solutions with higher correlation sum."}, {"heading": "8.2 Modal Prediction and Transformation", "text": "It is possible to make lower-dimension representation of two modals by building a Bimodal Deep Belief Network [25] as in Figure 8.2(a), in which blue nodes represent data from one modal and green nodes represent data from the other modal. In this process the recognition weights which are used in bottom-up computation and the generative weights which are used in top-down computation should be learned. If the model is trained with tied weights, half of the memory space could be saved since transpose of weight matrix\nwould transform recognition weights to generative weights. The weights of this model could be used to reconstruct data of two modals as in Figure 8.2(b).\nAnother option is to build a Markov Random Field multimodal learning model [30] by combining two Deep Boltzmann Machines. Figure 8.3 shows such a model. This model is constructed by first build two DBMs, each is trained on data of one modal. Then Train an RBM on top of these two DBMs.\nPrediction of data from one modal given data from the other modal could be done by first training a Bimodal Autoencoder in Figure 8.4(a) and then use the modal prediction network in Figure(b) to predict data from two modals. It would be necessary to add noise to corrupt data when training the Bimodal Autoencoder so that it has more power in reconstruction."}, {"heading": "9 Library Structure", "text": ""}, {"heading": "9.1 Data Reading", "text": ""}, {"heading": "9.1.1 MNIST", "text": "MNIST [22] is a selected set of samples from NIST data set. It has one training data set, one training label set, one testing data set, and one testing label set. The training set has 60,000 samples and the testing set has 10,000 samples. Each sample data is a 28\u00d728 grey image which is a handwritten integer between 0 and 9. It has 10 classes, so the label is between 0 (inclusive) and 9 (inclusive).\nThe data is stored in big-endian format. The content of data should be read as unsigned characters. Header file readMNIST.hpp provides functions to read this data set.\nI. cv::Mat imread mnist image(const char* path) Read data of MNIST. Each row is a sample.\nII. cv::Mat imread mnist label(const char* path) Read labels of MNIST. Each row is a number indicating class of that sample."}, {"heading": "9.1.2 CIFAR", "text": "The CIFAR-10 data set [17] consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 5 batches of training images and 1 batch of test images, each consists of 10000 images.\nIn CIFAR-10 data set, each sample consists of a number indicating its class and the values of the image pixels. Put five training data batches and one testing data batch of CIFAR-10 in the same folder named \u201ddata\u201d. The following function in the header file readCIFAR.hpp reads them to four OpenCV matrices:\nvoid imread cifar(Mat &trainingData, Mat &trainingLabel, Mat &testingData, Mat &testingLabel)\nEach row of the read OpenCV matrices consists of the label and the data of a sample."}, {"heading": "9.1.3 XRMB", "text": "In Wisconsin X-ray Microbeam Database (XRMB) [31, 35], the dimension of the MFCC data is 273 and the dimension of the XRMB data is 112. There are label files of fold 0 that provides the phone labels. The data is stored in double-precision format.\nThe following function in the header file readXRMB.hpp reads them:\ncv::Mat imread XRMB data(const char* path, int inputDim)\nindexLabel imread XRMB label(const char* path)\nWhen reading MFCC files, inputDim = 273. When reading XRMB files, inputDim = 112. The label is between 0 (inclusive) and 39 (inclusive). The data is stored sample by sample."}, {"heading": "9.1.4 AvLetters", "text": "AvLetters [24] is the data set recording audio data and video data of different people uttering letters. The dimension of the audio data is 26 and the dimension of the video data is 60\u00d780. The data is stored in single-precision big-endian format. Each file is the data of a person uttering a certain letter. For instance, the file A1 Anya.mfcc contains the audio data of the person named Anya uttering letter \u201dA\u201d.\nThe following function in the header file readAvLetters.hpp reads audio data:\ncv::Mat imread avletters mfcc(const char* path)\nThe output is an OpenCV matrix, of which each row contains data of a sample. The original video data is in MATLAB file format. The header file readMat.hpp contains the function\ncv::Mat matRead(const char* fileName, const char* variableName, const char* saveName).\nIt reads the mat file and at the same time saves it as binary file named as the argument \u201dsaveName\u201d. This header file uses the MATLAB/c++ interface provided by MATLAB and requires an environment setting, which is contained as comments in the header file. There are some problems running the libraries in this interface together with OpenCV. So use this function to transform all MATLAB files to binary files before training models and then read the transformed binary files. The header file readDat.hpp provide the function to read the transformed binary files:\ncv::Mat readBinary(const char* file, int rowSize, int colSize)\nThe output is an OpenCV matrix, of which each row contains data of a sample."}, {"heading": "9.1.5 Data Processing", "text": "Header file processData.hpp stores functions processing data.\ndata oneOfK(indexLabel l, int labelDim) Transfer index label to one-of-k expression.\ndataInBatch corruptImage(dataInBatch input, double denoiseRate)\nGive corrupted data in batches.\nstd::vector<dataInBatch> dataProcess(dataCollection& reading, int numBatch) Build data batches.\ndataCollection shuffleByRow(dataCollection& m) Shuffle the data\ncv::Mat denoiseMask(size t rowSize, size t colSize, double rate) Generate a mask to corrupt data"}, {"heading": "9.2 Computation and Utilities", "text": "activation.hpp includes multiple activation functions, such as sigmoid, tanh, relu, leaky relu, softmax. Each activation function is paired with a function that computes its derivatives to facilitate computation in backpropagation.\ncca.hpp includes functions computing CCA and KCCA.\ngd.hpp includes functions for adaptive gradient descent and stochastic gradient descent, and a function to anneal the learning rate in which three types of annealing methods are provided.\ninference.hpp includes the mean field inference implementation used by DBM.\nkernel.hpp includes multiple kernel functions used by KCCA.\nloss.hpp includes computation of loss functions. MSE, absolute loss, cross entropy, and binary loss are provided together with the functions to compute their derivatives.\nmatrix.hpp includes some OpenCV matrix manipulation functions.\nloadData.hpp contains functions to test data loading by visualization.\nvisualization.hpp contains functions of visualization."}, {"heading": "9.3 Models and Running", "text": "Table 1 shows the header files and files that contains main functions to test each model. Besides these, modalPrediction.cpp contains implementation of a modal prediction model."}, {"heading": "10 Performance", "text": ""}, {"heading": "10.1 Restricted Boltzmann Machine", "text": "The main function to run Restricted Boltzmann Machine is in runRBM.cpp. It uses RBM for classification of MNIST data set. On MNIST, use 60,000 samples for training and 10,000 samples for testing. With hidden layer of size 500. The classification error rate is 0.0707 (Accuracy 92.93%). Multiple deep learning libraries give similar results."}, {"heading": "10.2 Deep Neural Network", "text": "The main function to run Deep Neural Network is in runDNN.cpp. Usually it takes a large number of epochs in backpropagation in training. However this number is greatly reduced by pretraining.\nFor testing, a DNN with hidden layers of size 500, 300, 200 respectively is constructed. On MNIST data set, pretraining alone would gives out the error rate of 0.093 (Accuracy 90.7%). In backpropagation, each epoch goes through once the whole training data for update. One epoch in backpropagation gives out classification error rate of 0.0858 (Accuracy 91.42%). Ten epochs in backpropagation gives out classification error rate of 0.0288 (Accuracy 97.12%). Learning rate is 0.01 in fine tuning. The result is similar to the performance of running DNN on MNIST with MEDAL."}, {"heading": "10.3 Denoising Autoencoder", "text": "The main function to run Denoisinig Autoencoder is in runDAE.cpp. For testing, a Denoising Autoencoder with hidden layers of size 500, 300, 500 is constructed. Figure 10.1 shows the Denoising Autoencoder reconstruction of the corrupted testing set using model trained by training set, in which upper six lines are reconstructions and the lower six lines are uncorrupted input. As a comparison, Figure 10.2 shows the Autoencoder reconstruction of the uncorrupted testing set. Fine tuninig in the Denoising Autoencoder gives more improvement of performance than in the Autoencoder.\nEach epoch goes through once the whole training data for update. On MNIST, run 10 epochs in fine tuning, the reconstruction error computed by MSE in the Denoising Autoencoder are: Average error without fine tuning 6686.69. Average error after fine tuning 3256.34. The reconstruction error computed by MSE in the Autoencoder are: Average error without fine tuning 4463.24. Average error after fine tuning 3182.69.\nSo after sufficient fine tuning, the reconstruction of Denoising Autoencoder is similar to the reconstruction from the uncorrupted images. The reconstruction visualization is comparable to published result.\nAnother test is made with AvLetters data set, which is in runDAE letter.cpp. Figure 10.2 shows denosing avletter audio data. It uses the audio data of a person pronouncing \u201dA\u201d, \u201dB\u201d, \u201dC\u201d, \u201dD\u201d, \u201dE\u201d, \u201dF\u201d, \u201dG\u201d to train the model and reconstructs the data of person pronouncing \u201dH\u201d and \u201dI\u201d."}, {"heading": "10.4 Deep Belief Network", "text": "The main function to run the Deep Belief Network is in runDBN.cpp. For testing, a Deep Belief Network with hidden layers of sizes 500, 300 is constructed. The classification error rate on MNIST without fine tuning is 0.0883 (Accuracy 91.17%). With the Up-down fine tuning method the classification error rate could be reduced to 0.0695 (Accuracy 93.05%). [6] indicates the best performance of DBN on MNIST could achieve the error rate of 1.25%. There are multiple parameters in the training process could affect the result, such as learning rate and tricks in gradient descent. This possibly affect the result."}, {"heading": "10.5 Deep Boltzmann Machine", "text": "The main function to run the Deep Boltzmann Machine is in runDBM.cpp. For testing, a Deep Boltzmann Machine with hidden layers of sizes 500, 500 is constructed. The error rate on MNIST without fine tuning is 0.0937 (Accuracy 90.63%). Mean Field inference improves the accuracy to 93.47%. Again multiple parameters could affect the result. [5] indicates that the accuracy on MNIST could achieve test error of 0.95%. Its source code uses Conjugate Gradient optimization, which is not implemented in this library. This possibly causes the difference."}, {"heading": "10.6 Deep Canonical Correlation Analysis", "text": "The main function to run DCCA is in runDCCA.cpp. The testing data set is CIFAR-10. Each image is divided as the left and the right halves as data from two views. Then two networks trained on two views are built. Each view of data is of dimension 512. Each network gives the output of dimension 20. The DCCA could quickly find the solution in which the sum of the top 20 correlations is 10.8756. Without network below CCA layer, the solution could not be computed in a reasonable time.\nA test on AvLetters was also done. Unfortunately the correlation given is close to 0. This suggests that there is very week linear dependence between data of two modals. So it should be another form of dependence existing."}, {"heading": "10.7 Modal Prediction", "text": "Modal prediction implementation is in the modal prediction.cpp. It trains the modal using audio and video data of uttering letter from \u201dA\u201d to \u201dG\u201d and then uses the audio data of letter \u201dH\u201d and \u201dI\u201d to predict the video data of \u201dH\u201d and \u201dI\u201d. The reconstruction error is 10.46%."}], "references": [{"title": "Using articulatory measurements to learn better acoustic features", "author": ["Galen Andrew", "Raman Arora", "Sujeeth Bharadwaj", "Jeff Bilmes", "Mark Hasegawa-Johnson", "Karen Livescu"], "venue": "In Speech Production in Automatic Speech Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Stochastic optimization for pca and pls", "author": ["Raman Arora", "Andrew Cotter", "Karen Livescu", "Nathan Srebro"], "venue": "In Allerton Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Stochastic optimization of pca with capped msg", "author": ["Raman Arora", "Andy Cotter", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Kernel cca for multi-view learning of acoustic features using articulatory measurements", "author": ["Raman Arora", "Karen Livescu"], "venue": "In MLSLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Multi-view cca-based acoustic features for phonetic recognition across speakers and domains", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Multi-view learning with supervision for transformed bottleneck features", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Self-organizing neural network that discovers surfaces in random-dot stereograms", "author": ["Suzanna Becker", "Geoffrey E Hinton"], "venue": "Nature, 355(6356):161\u2013163,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Multiview acoustic feature learning using articulatory measurements", "author": ["Sujeeth Bharadwaj", "Raman Arora", "Karen Livescu", "Mark Hasegawa-Johnson"], "venue": "In Intl. Workshop on Stat. Machine Learning for Speech Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The \u201dwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Geoffrey E Hinton", "Peter Dayan", "Brendan J Frey", "Radford M Neal"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, pages 321\u2013377,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1936}, {"title": "Nonlinear canonical correlation analysis by neural networks", "author": ["William W Hsieh"], "venue": "Neural Networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "A neural implementation of canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "International Journal of Neural Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Hugo Larochelle", "Yoshua Bengio"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Representational power of restricted boltzmann machines and deep belief networks", "author": ["Nicolas Le Roux", "Yoshua Bengio"], "venue": "Neural computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Extraction of visual features for lipreading", "author": ["Iain Matthews", "Timothy F Cootes", "J Andrew Bangham", "Stephen Cox", "Richard Harvey"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Hugo Larochelle"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Lawrence K Saul", "Tommi Jaakkola", "Michael I Jordan"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1996}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Reconstruction of articulatory measurements with smoothed low-rank matrix completion", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "On deep multi-view representation learning", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff A Bilmes"], "venue": "In Proceedings of ICASSP,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Nathan Srebro"], "venue": "Proceedings of the 53rd Annual Allerton Conference on Communication, Control and Computing (ALLERTON),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "X-ray microbeam speech production database", "author": ["John Westbury", "Paul Milenkovic", "Gary Weismer", "Raymond Kent"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1990}], "referenceMentions": [{"referenceID": 27, "context": "The Sigmoid Belief Network [29] is a type of the Belief Network such that P (Xi = 1|Pa(Xi)) = \u03c3( \u2211", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "A Restricted Boltzmann Machine (RBM) [20, 21, 28] is a Markov Random Field consisting of one hidden layer and one visible layer.", "startOffset": 37, "endOffset": 49}, {"referenceID": 20, "context": "A Restricted Boltzmann Machine (RBM) [20, 21, 28] is a Markov Random Field consisting of one hidden layer and one visible layer.", "startOffset": 37, "endOffset": 49}, {"referenceID": 26, "context": "A Restricted Boltzmann Machine (RBM) [20, 21, 28] is a Markov Random Field consisting of one hidden layer and one visible layer.", "startOffset": 37, "endOffset": 49}, {"referenceID": 10, "context": "3 Tricks in Restricted Boltzmann Machine Training Dropout Dropout [11] is a method to prevent neural networks from overfitting by randomly blocking emissions from certain neurons.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "A Deep Neural Network (DNN) [8] is a neural network with multiple hidden layers.", "startOffset": 28, "endOffset": 31}, {"referenceID": 12, "context": "After pretraining is done, use Up-Down algorithm [13] as fine tuning, which is a combination of the training of RBM, and an algorithm called Wake-Sleep algorithm [12] which is for learning of the Sigmoid Belief Network.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "After pretraining is done, use Up-Down algorithm [13] as fine tuning, which is a combination of the training of RBM, and an algorithm called Wake-Sleep algorithm [12] which is for learning of the Sigmoid Belief Network.", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "Autoencoder(AE) [14] is a type of neural network forming a directed graph.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "1 Logic of Deep Boltzmann Machine A Deep Boltzmann Machine(DBM) [26] is a Markov Random Field consisting of multiple layers.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "3 Mean Field Inference The mean field inference [27] of the Deep Boltzmann Machine involves iterative updates of the approximations Q.", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "1 Canonical Correlation Analysis Canonical Correlation Analysis [10] is a method to find the relationship between two sets of variables.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "(90) The solution [15] to this problem suggests that a and b are respectively the eigenvectors of the following matrices Cov(x, x)\u22121Cov(x, y)Cov(y, y)\u22121Cov(y, x), (91) Cov(y, y)\u22121Cov(y, x)Cov(x, x)\u22121Cov(x, y).", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 4, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 5, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 6, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 8, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 2, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 185, "endOffset": 195}, {"referenceID": 3, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 185, "endOffset": 195}, {"referenceID": 32, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 185, "endOffset": 195}, {"referenceID": 15, "context": "2 Kernel Canonical Correlation Analysis Kernel Canonical Correlation Analysis [16, 19] projects the data into a higher-dimensional feature space before linear projections using the mapping \u03c6 : x\u2192 \u03c6(x).", "startOffset": 78, "endOffset": 86}, {"referenceID": 18, "context": "2 Kernel Canonical Correlation Analysis Kernel Canonical Correlation Analysis [16, 19] projects the data into a higher-dimensional feature space before linear projections using the mapping \u03c6 : x\u2192 \u03c6(x).", "startOffset": 78, "endOffset": 86}, {"referenceID": 1, "context": "3 Deep Correlation Analysis In Deep Correlation Analysis (DCCA) [2,18,33], A layer of KCCA is added on top of two separately trained Deep Neural Networks, of which each learns the data of one modal.", "startOffset": 64, "endOffset": 73}, {"referenceID": 17, "context": "3 Deep Correlation Analysis In Deep Correlation Analysis (DCCA) [2,18,33], A layer of KCCA is added on top of two separately trained Deep Neural Networks, of which each learns the data of one modal.", "startOffset": 64, "endOffset": 73}, {"referenceID": 31, "context": "3 Deep Correlation Analysis In Deep Correlation Analysis (DCCA) [2,18,33], A layer of KCCA is added on top of two separately trained Deep Neural Networks, of which each learns the data of one modal.", "startOffset": 64, "endOffset": 73}, {"referenceID": 30, "context": "Deep Canonically Correlated Autoencoders (DCCAE) [32] is an extension of DCCA.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "2 Modal Prediction and Transformation It is possible to make lower-dimension representation of two modals by building a Bimodal Deep Belief Network [25] as in Figure 8.", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "Another option is to build a Markov Random Field multimodal learning model [30] by combining two Deep Boltzmann Machines.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "1 MNIST MNIST [22] is a selected set of samples from NIST data set.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "2 CIFAR The CIFAR-10 data set [17] consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "3 XRMB In Wisconsin X-ray Microbeam Database (XRMB) [31, 35], the dimension of the MFCC data is 273 and the dimension of the XRMB data is 112.", "startOffset": 52, "endOffset": 60}, {"referenceID": 33, "context": "3 XRMB In Wisconsin X-ray Microbeam Database (XRMB) [31, 35], the dimension of the MFCC data is 273 and the dimension of the XRMB data is 112.", "startOffset": 52, "endOffset": 60}, {"referenceID": 22, "context": "4 AvLetters AvLetters [24] is the data set recording audio data and video data of different people uttering letters.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "[6] indicates the best performance of DBN on MNIST could achieve the error rate of 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] indicates that the accuracy on MNIST could achieve test error of 0.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "The Neural Network is a directed graph consists of multiple layers of neurons, which is also referred to as units. In general there is no connection between units of the same layer and there are only connections between adjacent layers. The first layer is the input and is referred to as visible layer v. Above the visible layer there are multiple hidden layers {h1, h2, ..., hn}. And the output of the last hidden layer forms the output layer o.", "creator": "LaTeX with hyperref package"}}}