{"id": "1512.08422", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Dec-2015", "title": "Natural Language Inference by Tree-Based Convolution and Heuristic Matching", "abstract": "Recognizing wheelabrator entailment urmia and contradiction sinology between two hdg sentences has abani wide 50,000,000 applications lesage in NLP. dysplastic Traditional methods geoscience include berries feature - castanheira rich classifiers laskowski or denne formal reasoning. gutmann However, they uncolored are hungaroring usually limited in terms of beschastnykh accuracy welayta and katsuobushi scope. Recently, the u.s.-iranian renewed cosworth prosperity neural hodan networks 85.46 has made many grandchild improvements savill in 42.1 a variety of overstated NLP hazelnut tasks. conceive In racing particular, the guimet tree - based convolutional giancola neural network (TBCNN) ramil proposed in cis our penalty previous 334 work buli has zoltar achieved mramor high 25.54 performance in reaffirm several sentence - level classification pemble tasks. But whether double-deck TBCNN is krenzler applicable to ringwraiths recognize 2,000-strong entailment akhnaten and metella contradiction cenelec between two sentences remains unknown. In this paper, we 132-member propose TBCNN - moogfest pair hazeltine model gane for vitosha entailment / tell-all contradiction recognition. twice-weekly Experimental furd results incarcerated in bodegas a large urdaneta dataset verifies saadiyat the burtless rationale of fundanga using TBCNN bevill as curassows the brezje sentence - level 32-yard model; -1.7 leveraging additional pilus heuristics fujima like element - pallmeyer wise product / difference zhili further improves ginga the 589,000 result, wi\u015bniewski and renly outperforms published blandi results shilov by a gonorrhoeae large biomolecules margin.", "histories": [["v1", "Mon, 28 Dec 2015 14:28:21 GMT  (225kb,D)", "https://arxiv.org/abs/1512.08422v1", null], ["v2", "Fri, 22 Apr 2016 17:49:46 GMT  (238kb,D)", "http://arxiv.org/abs/1512.08422v2", "Accepted by ACL'16 as a short paper"], ["v3", "Fri, 13 May 2016 16:24:56 GMT  (238kb,D)", "http://arxiv.org/abs/1512.08422v3", "Accepted by ACL'16 as a short paper"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lili mou", "rui men", "ge li", "yan xu", "lu zhang 0023", "rui yan", "zhi jin"], "accepted": true, "id": "1512.08422"}, "pdf": {"name": "1512.08422.pdf", "metadata": {"source": "CRF", "title": "Natural Language Inference by Tree-Based Convolution and Heuristic Matching", "authors": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com", "menruimr@gmail.com", "lige@sei.pku.edu.cn", "xuyan14@sei.pku.edu.cn", "zhanglu@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn", "yanrui02@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "Recognizing entailment and contradiction between two sentences (called a premise and a hypothesis) is known as natural language inference (NLI) in MacCartney (2009). Provided with a premise sentence, the task is to judge whether the hypothesis can be inferred (entailment), or the hypothesis cannot be true (contradiction). Several examples are illustrated in Table 1.\nNLI is in the core of natural language understanding and has wide applications in NLP, e.g., question answering (Harabagiu and Hickl, 2006) and automatic summarization (Lacatusu et al., 2006; Yan et al., 2011a; Yan et al., 2011b). Moreover, NLI is also related to other tasks of sentence pair modeling, including paraphrase detection (Hu et al., 2014), relation recognition of discourse units (Liu et al., 2016), etc.\nTraditional approaches to NLI mainly fall into two groups: feature-rich models and formal reasoning methods. Feature-based approaches typically leverage machine learning models, but require intensive human engineering to represent lexical and syntactic information in two sentences\n\u2217Equal contribution. \u2020Corresponding authors.\n(MacCartney et al., 2006; Harabagiu et al., 2006). Formal reasoning, on the other hand, converts a sentence into a formal logical representation and uses interpreters to search for a proof. However, such approaches are limited in terms of scope and accuracy (Bos and Markert, 2005).\nThe renewed prosperity of neural networks has made significant achievements in various NLP applications, including individual sentence modeling (Kalchbrenner et al., 2014; Mou et al., 2015) as well as sentence matching (Hu et al., 2014; Yin and Schu\u0308tze, 2015). A typical neural architecture to model sentence pairs is the \u201cSiamese\u201d structure (Bromley et al., 1993), which involves an underlying sentence model and a matching layer to determine the relationship between two sentences. Prevailing sentence models include convolutional networks (Kalchbrenner et al., 2014) and recurrent/recursive networks (Socher et al., 2011b). Although they have achieved high performance, they may either fail to fully make use of the syntactical information in sentences or be difficult to train due to the long propagation path. Recently, we propose a novel tree-based convolutional neural network (TBCNN) to alleviate the aforementioned problems and have achieved higher performance in two sentence classification tasks (Mou et al., 2015). However, it is less clear whether TBCNN can be harnessed to model sentence pairs for implicit logical inference, as is in the NLI task.\nIn this paper, we propose the TBCNN-pair neural model to recognize entailment and contradiction between two sentences. We lever-\nAccpeted by ACL\u201916 as a short paper\nar X\niv :1\n51 2.\n08 42\n2v 3\n[ cs\n.C L\n] 1\n3 M\nay 2\n01 6\nage our newly proposed TBCNN model to capture structural information in sentences, which is important to NLI. For example, the phrase \u201criding bicycles on the streets\u201d in Table 1 can be well recognized by TBCNN via the dependency relations dobj(riding,bicycles) and prep on(riding,street). As we can see, TBCNN is more robust than sequential convolution in terms of word order distortion, which may be introduced by determinators, modifiers, etc. A pooling layer then aggregates information along the tree, serving as a way of semantic compositonality. Finally, two sentences\u2019 information is combined by several heuristic matching layers, including concatenation, element-wise product and difference; they are effective in capturing relationships between two sentences, but remain low complexity.\nTo sum up, the main contributions of this paper are two-fold: (1) We are the first to introduce tree-based convolution to sentence pair modeling tasks like NLI; (2) Leveraging additional heuristics further improves the accuracy while remaining low complexity, outperforming existing sentence encoding-based approaches to a large extent, including feature-rich methods and long short term memory (LSTM)-based recurrent networks.1"}, {"heading": "2 Related Work", "text": "Entailment recognition can be viewed as a task of sentence pair modeling. Most neural networks in this field involve a sentence-level model, followed by one or a few matching layers. They are sometimes called \u201cSiamese\u201d architectures (Bromley et al., 1993).\nHu et al. (2014) and Yin and Schu\u0308tze (2015) apply convolutional neural networks (CNNs) as the individual sentence model, where a set of feature detectors over successive words are designed to extract local features. Wan et al. (2015) build sentence pair models upon recurrent neural networks (RNNs) to iteratively integrate information along a sentence. Socher et al. (2011a) dynamically construct tree structures (analogous to parse trees) by recursive autoencoders to detect paraphrase between two sentences. As shown, inherent structural information in sentences is oftentimes important to natural language understanding.\nThe simplest approach to match two sentences, 1 Code is released on:\nhttps://sites.google.com/site/tbcnninference/\nperhaps, is to concatenate their vector representations (Zhang et al., 2015; Hu et al., 2014, Arc-I). Concatenation is also applied in our previous work of matching the subject and object in relation classification (Xu et al., 2015; Xu et al., 2016). He et al. (2015) apply additional heuristics, namely Euclidean distance, cosine measure, and elementwise absolute difference. The above methods operate on a fixed-size vector representation of a sentence, categorized as sentence encoding-based approaches. Thus the matching complexity is O(1), i.e., independent of the sentence length. Word-byword similarity matrices are introduced to enhance interaction. To obtain the similarity matrix, Hu et al. (2014) (Arc-II) concatenate two words\u2019 vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et al. (2015) apply tensor product. In this way, the complexity is of O(n2), where n is the length of a sentence; hence similarity matrices are difficult to scale and less efficient for large datasets.\nRecently, Rockta\u0308schel et al. (2016) introduce several context-aware methods for sentence matching. They report that RNNs over a single chain of two sentences are more informative than separate RNNs; a static attention over the first sentence is also useful when modeling the second one. Such context-awareness interweaves the sentence modeling and matching steps. In some scenarios like sentence pair re-ranking (Yan et al., 2016), it is not feasible to pre-calculate the vector representations of sentences, so the matching complexity is ofO(n). Rockta\u0308schel et al. (2016) further develop a word-by-word attention mechanism and obtain a higher accuracy with a complexity order ofO(n2)."}, {"heading": "3 Our Approach", "text": "We follow the \u201cSiamese\u201d architecture (like most work in Section 2) and adopt a two-step strategy to classify the relation between two sentences. Concretely, our model comprises two parts: \u2022 A tree-based convolutional neural network\nmodels each individual sentence (Figure 1a). Notice that, the two sentences, premise and hypothesis, share a same TBCNN model (with same parameters), because this part aims to capture general semantics of sentences. \u2022 A matching layer combines two sentences\u2019 in-\nformation by heuristics (Figure 1b). After individual sentence models, we design a sentence matching layer to aggregate information. We use simple heuristics, including concate-\nnation, element-wise product and difference, which are effective and efficient.\nFinally, we add a softmax layer for output. The training objective is cross-entropy loss, and we adopt mini-batch stochastic gradient descent, computed by back-propagation."}, {"heading": "3.1 Tree-Based Convolution", "text": "The tree-based convolutoinal neural network (TBCNN) is first proposed in our previous work (Mou et al., 2016)2 to classify program source code; later, we further propose TBCNN variants to model sentences (Mou et al., 2015). This subsection details the tree-based convolution process.\nThe basic idea of TBCNN is to design a set of subtree feature detectors sliding over the parse tree of a sentence; either a constituency tree or a dependency tree applies. In this paper, we prefer the dependency tree-based convolution for its efficiency and compact expressiveness.\nConcretely, a sentence is first converted to a dependency parse tree.3 Each node in the dependency tree corresponds to a word in the sentence; an edge a\u2192b indicates a is governed by b. Edges are labeled with grammatical relations (e.g., nsubj) between the parent node and its children (de Marneffe et al., 2006). Words are represented by pretrained vector representations, also known as word embeddings (Mikolov et al., 2013a).\n2Preprinted on arXiv on September 2014 (http://arxiv.org/abs/1409.5718v1) 3Parsed by the Stanford parser (http://nlp.stanford.edu/software/lex-parser.shtml)\nNow, we consider a set of two-layer subtree feature detectors sliding over the dependency tree. At a position where the parent node is p with child nodes c1, \u00b7 \u00b7 \u00b7 , cn, the output of the feature detector, y, is\ny = f ( Wpp+\nn\u2211 i=1 Wr[ci]ci + b ) Let us assume word embeddings (p and ci) are of ne dimensions; that the convolutional layer y is nc-dimensional. W \u2208 Rnc\u00d7ne is the weight matrix; b \u2208 Rnc is the bias vector. r[ci] denotes the dependency relation between p and ci. f is the non-linear activation function, and we apply ReLU in our experiments.\nAfter tree-based convolution, we obtain a set of feature maps, which are one-one corresponding to original words in the sentence. Therefore, they may vary in size and length. A dynamic pooling layer is applied to aggregate information along different parts of the tree, serving as a way of semantic compositionality (Hu et al., 2014). We use the max pooling operation, which takes the maximum value in each dimension.\nThen we add a fully-connected hidden layer to further mix the information in a sentence. The obtained vector representation of a sentence is denoted as h (also called a sentence embedding). Notice that the same tree-based convolution applies to both the premise and hypothesis.\nTree-based convolution along with pooling enables structural features to reach the output layer with short propagation paths, as opposed to the recursive network (Socher et al., 2011b), which is also structure-sensitive but may suffer from the problem of long propagation path. By contrast, TBCNN is effective and efficient in learning such structural information (Mou et al., 2015)."}, {"heading": "3.2 Matching Heuristics", "text": "In this part, we introduce how vector representations of individual sentences are combined to capture the relation between the premise and hypothesis. As the dataset is large, we prefer O(1) matching operations because of efficiency concerns. Concretely, we have three matching heuristics: \u2022 Concatenation of the two sentence vectors, \u2022 Element-wise product, and \u2022 Element-wise difference. The first heuristic follows the most standard procedure of the \u201cSiamese\u201d architectures, while the latter two are certain measures of \u201csimilarity\u201d or\n\u201ccloseness.\u201d These matching layers are further concatenated (Figure 1b), given by\nm = [h1;h2;h1 \u2212 h2;h1 \u25e6 h2]\nwhere h1 \u2208 Rnc and h2 \u2208 Rnc are the sentence vectors of the premise and hypothesis, respectively; \u201c\u25e6\u201d denotes element-wise product; semicolons refer to column vector concatenation. m \u2208 R4nc is the output of the matching layer.\nWe would like to point out that, with subsequent linear transformation, element-wise difference is a special case of concatenation. If we assume the subsequent transformation takes the form of W [h1 h2]>, where W = [W1 W2] is the weights for concatenated sentence representations, then element-wise difference can be viewed as such that W0(h1\u2212h2) = [W0 \u2212W0][h1 h2]>. (W0 is the weights corresponding to element-wise difference.) Thus, our third heuristic can be absorbed into the first one in terms of model capacity. However, as will be shown in the experiment, explicitly specifying this heuristic significantly improves the performance, indicating that optimization differs, despite the same model capacity. Moreover, word embedding studies show that linear offset of vectors can capture relationships between two words (Mikolov et al., 2013b), but it has not been exploited in sentence-pair relation recognition. Although element-wise distance is used to detect paraphrase in He et al. (2015), it mainly reflects \u201csimilarity\u201d information. Our study verifies that vector offset is useful in capturing generic sentence relationships, akin to the word analogy task."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Dataset", "text": "To evaluate our TBCNN-pair model, we used the newly published Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).4 The dataset is constructed by crowdsourced efforts, each sentence written by humans. Moreover, the SNLI dataset is magnitudes of larger than previous resources, and hence is particularly suitable for comparing neural models. The target labels comprise three classes: Entailment, Contradiction, and Neutral (two irrelevant sentences). We applied the standard train/validation/test split, contraining 550k, 10k, and 10k samples, respectively. Figure 2 presents\n4http://nlp.stanford.edu/projects/snli/\nadditional dataset statistics, especially those relevant to dependency parse trees.5"}, {"heading": "4.2 Hyperparameter Settings", "text": "All our neural layers, including embeddings, were set to 300 dimensions. The model is mostly robust when the dimension is large, e.g., several hundred (Collobert and Weston, 2008). Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as a part of model parameters. We applied `2 penalty of 3\u00d710\u22124; dropout was chosen by validation with a granularity of 0.1 (Figure 2). We see that a large dropout rate (\u2265 0.3) hurts the performance (and also makes training slow) for such a large dataset as opposed to small datasets in other tasks (Peng et al., 2015). Initial learning rate was set to 1, and a power decay was applied. We used stochastic gradient descent with a batch size of 50."}, {"heading": "4.3 Performance", "text": "Table 3 compares our model with previous results. As seen, the TBCNN sentence pair model, followed by simple concatenation alone, outperforms existing sentence encoding-based approaches (without pretraining), including a feature-rich method using 6 groups of humanengineered features, long short term memory\n5We applied collapsed dependency trees, where prepositions and conjunctions are annotated on the dependency relations, but these auxiliary words themselves are removed.\n(LSTM)-based RNNs, and traditional CNNs. This verifies the rationale for using tree-based convolution as the sentence-level neural model for NLI.\nTable 4 compares different heuristics of matching. We first analyze each heuristic separately: using element-wise product alone is significantly worse than concatenation or element-wise difference; the latter two are comparable to each other.\nCombining different matching heuristics improves the result: the TBCNN-pair model with concatenation, element-wise product and difference yields the highest performance of 82.1%. As analyzed in Section 3.2, the element-wise difference matching layer does not add to model complexity and can be absorbed as a special case into simple concatenation. However, explicitly using such heuristic yields an accuracy boost of 1\u20132%. Further applying element-wise product improves the accuracy by another 0.5%.\nThe full TBCNN-pair model outperforms all existing sentence encoding-based approaches, in-\ncluding a 1024d gated recurrent unit (GRU)-based RNN with \u201cskip-thought\u201d pretraining (Vendrov et al., 2015). The results obtained by our model are also comparable to several attention-based LSTMs, which are more computationally intensive than ours in terms of complexity order."}, {"heading": "4.4 Complexity Concerns", "text": "For most sentence models including TBCNN, the overall complexity is at least O(n). However, an efficient matching approach is still important, especially to retrieval-and-reranking systems (Yan et al., 2016; Li et al., 2016). For example, in a retrieval-based question-answering or conversation system, we can largely reduce response time by performing sentence matching based on precomputed candidates\u2019 embeddings. By contrast, context-aware matching approaches as described in Section 2 involve processing each candidate given a new user-issued query, which is timeconsuming in terms of most industrial products.\nIn our experiments, the matching part (Figure 1b) counts 1.71% of the total time during prediction (single-CPU, C++ implementation), showing the potential applications of our approach in efficient retrieval of semantically related sentences."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed the TBCNN-pair model for natural language inference. Our model relies on the tree-based convolutional neural network (TBCNN) to capture sentence-level semantics; then two sentences\u2019 information is combined by several heuristics including concatenation, element-wise product and difference. Experimental results on a large dataset show a high performance of our TBCNN-pair model while remaining a low complexity order."}, {"heading": "Acknowledgments", "text": "We thank all anonymous reviewers for their constructive comments, especially those on complexity issues. We also thank Sam Bowman, Edward Grefenstette, and Tim Rockta\u0308schel for their discussion. This research was supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. 61232015, 61421091, and 61502014."}], "references": [{"title": "Combining shallow and deep NLP methods for recognizing textual entailment", "author": ["Bos", "Markert2005] Johan Bos", "Katja Markert"], "venue": "In Proceedings of the First PASCAL Challenges Workshop on Recognising Textual Entailment,", "citeRegEx": "Bos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2005}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Signature verification using a \u201cSiamese", "author": ["Bromley et al.1993] Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": null, "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of the Language Resource and Evaluation Conference,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Methods for using textual entailment in open-domain question answering", "author": ["Harabagiu", "Hickl2006] Sanda Harabagiu", "Andrew Hickl"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual", "citeRegEx": "Harabagiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2006}, {"title": "Negation, contrast and contradiction in text processing", "author": ["Andrew Hickl", "Finley Lacatusu"], "venue": "In Proceedings of AAAI Conference on Artificial Intelligence,", "citeRegEx": "Harabagiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2006}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He et al.2015] Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "LCCs GISTexter at DUC 2006: Multi-strategy multidocument summarization", "author": ["Andrew Hickl", "Kirk Roberts", "Ying Shi", "Jeremy Bensley", "Bryan Rink", "Patrick Wang", "Lara Taylor"], "venue": "Proceedings of DUC", "citeRegEx": "Lacatusu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lacatusu et al\\.", "year": 2006}, {"title": "StalemateBreaker: A proactive content-introducing approach to automatic humancomputer conversation", "author": ["Li et al.2016] Xiang Li", "Lili Mou", "Rui Yan", "Ming Zhang"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelli-", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Implicit discourse relation classification via multi-task neural networks", "author": ["Liu et al.2016] Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Learning to recognize features of valid textual entailments", "author": ["Trond Grenager", "Marie-Catherine de Marneffe", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of the Human Language Technology", "citeRegEx": "MacCartney et al\\.,? \\Q2006\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2006}, {"title": "Natural Language Inference", "author": ["Bill MacCartney"], "venue": "Ph.D. thesis,", "citeRegEx": "MacCartney.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In NAACL-HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou et al.2016] Lili Mou", "Ge Li", "Lu Zhang", "Tao Wang", "Zhi Jin"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "A comparative study on regularization strategies for embedding-based neural networks", "author": ["Peng et al.2015] Hao Peng", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In Proceedings of the International Conference on Learning", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empiri-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A deep architecture for semantic matching with multiple positional sentence representations. arXiv preprint arXiv:1511.08277", "author": ["Wan et al.2015] Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng"], "venue": null, "citeRegEx": "Wan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Improved relation classification by deep recurrent neural networks with data augmentation", "author": ["Xu et al.2016] Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "venue": "arXiv preprint arXiv:1601.03651", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Timeline generation through evolutionary trans-temporal summarization", "author": ["Yan et al.2011a] Rui Yan", "Liang Kong", "Congrui Huang", "Xiaojun Wan", "Xiaoming Li", "Yan Zhang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural", "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Evolutionary timeline summarization: A balanced optimization framework via iterative substitution", "author": ["Yan et al.2011b] Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Learning to respond with deep neural networks for retrieval based human-computer conversation system", "author": ["Yan et al.2016] Rui Yan", "Yiping Song", "Hua Wu"], "venue": "In Proceedings of the 39th International ACM SIGIR Conference on Research and De-", "citeRegEx": "Yan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Shallow convolutional neural network for implicit discourse relation recognition", "author": ["Zhang et al.2015] Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "tween two sentences (called a premise and a hypothesis) is known as natural language inference (NLI) in MacCartney (2009). Provided with a premise sentence, the task is to judge whether the hypothesis can be inferred (entailment), or the", "startOffset": 104, "endOffset": 122}, {"referenceID": 10, "context": ", question answering (Harabagiu and Hickl, 2006) and automatic summarization (Lacatusu et al., 2006; Yan et al., 2011a; Yan et al., 2011b).", "startOffset": 77, "endOffset": 138}, {"referenceID": 8, "context": "Moreover, NLI is also related to other tasks of sentence pair modeling, including paraphrase detection (Hu et al., 2014), relation recognition of discourse units (Liu et al.", "startOffset": 103, "endOffset": 120}, {"referenceID": 12, "context": ", 2014), relation recognition of discourse units (Liu et al., 2016), etc.", "startOffset": 49, "endOffset": 67}, {"referenceID": 13, "context": "(MacCartney et al., 2006; Harabagiu et al., 2006).", "startOffset": 0, "endOffset": 49}, {"referenceID": 5, "context": "(MacCartney et al., 2006; Harabagiu et al., 2006).", "startOffset": 0, "endOffset": 49}, {"referenceID": 9, "context": "The renewed prosperity of neural networks has made significant achievements in various NLP applications, including individual sentence modeling (Kalchbrenner et al., 2014; Mou et al., 2015) as well as sentence matching (Hu et al.", "startOffset": 144, "endOffset": 189}, {"referenceID": 17, "context": "The renewed prosperity of neural networks has made significant achievements in various NLP applications, including individual sentence modeling (Kalchbrenner et al., 2014; Mou et al., 2015) as well as sentence matching (Hu et al.", "startOffset": 144, "endOffset": 189}, {"referenceID": 2, "context": "A typical neural architecture to model sentence pairs is the \u201cSiamese\u201d structure (Bromley et al., 1993), which involves an underlying sentence model and a matching layer to determine the relationship between two sentences.", "startOffset": 81, "endOffset": 103}, {"referenceID": 9, "context": "Prevailing sentence models include convolutional networks (Kalchbrenner et al., 2014) and recurrent/recursive networks (Socher et al.", "startOffset": 58, "endOffset": 85}, {"referenceID": 17, "context": "in two sentence classification tasks (Mou et al., 2015).", "startOffset": 37, "endOffset": 55}, {"referenceID": 2, "context": "They are sometimes called \u201cSiamese\u201d architectures (Bromley et al., 1993).", "startOffset": 50, "endOffset": 72}, {"referenceID": 21, "context": "Wan et al. (2015) build sentence pair models upon recurrent neural networks (RNNs) to iteratively integrate information along a sentence.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "Socher et al. (2011a) dynamically construct tree structures (analogous to parse trees) by recursive autoencoders to detect paraphrase between two sentences.", "startOffset": 0, "endOffset": 22}, {"referenceID": 24, "context": "sification (Xu et al., 2015; Xu et al., 2016).", "startOffset": 11, "endOffset": 45}, {"referenceID": 25, "context": "sification (Xu et al., 2015; Xu et al., 2016).", "startOffset": 11, "endOffset": 45}, {"referenceID": 7, "context": "He et al. (2015) apply additional heuristics, namely Euclidean distance, cosine measure, and elementwise absolute difference.", "startOffset": 0, "endOffset": 17}, {"referenceID": 21, "context": "(2014) (Arc-II) concatenate two words\u2019 vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 21, "context": "(2014) (Arc-II) concatenate two words\u2019 vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et al. (2015) apply tensor product.", "startOffset": 68, "endOffset": 140}, {"referenceID": 20, "context": "Recently, Rockt\u00e4schel et al. (2016) introduce several context-aware methods for sentence matching.", "startOffset": 10, "endOffset": 36}, {"referenceID": 28, "context": "In some scenarios like sentence pair re-ranking (Yan et al., 2016), it", "startOffset": 48, "endOffset": 66}, {"referenceID": 20, "context": "Rockt\u00e4schel et al. (2016) further develop a word-by-word attention mechanism and obtain a higher accuracy with a complexity order ofO(n2).", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "The tree-based convolutoinal neural network (TBCNN) is first proposed in our previous work (Mou et al., 2016)2 to classify program source code; later, we further propose TBCNN variants", "startOffset": 91, "endOffset": 109}, {"referenceID": 17, "context": "to model sentences (Mou et al., 2015).", "startOffset": 19, "endOffset": 37}, {"referenceID": 8, "context": "A dynamic pooling layer is applied to aggregate information along different parts of the tree, serving as a way of semantic compositionality (Hu et al., 2014).", "startOffset": 141, "endOffset": 158}, {"referenceID": 17, "context": "TBCNN is effective and efficient in learning such structural information (Mou et al., 2015).", "startOffset": 73, "endOffset": 91}, {"referenceID": 7, "context": "Although element-wise distance is used to detect paraphrase in He et al. (2015), it mainly reflects \u201csimilarity\u201d information.", "startOffset": 63, "endOffset": 80}, {"referenceID": 1, "context": "To evaluate our TBCNN-pair model, we used the newly published Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).", "startOffset": 113, "endOffset": 134}, {"referenceID": 19, "context": "3) hurts the performance (and also makes training slow) for such a large dataset as opposed to small datasets in other tasks (Peng et al., 2015).", "startOffset": 125, "endOffset": 144}, {"referenceID": 28, "context": "However, an efficient matching approach is still important, especially to retrieval-and-reranking systems (Yan et al., 2016; Li et al., 2016).", "startOffset": 106, "endOffset": 141}, {"referenceID": 11, "context": "However, an efficient matching approach is still important, especially to retrieval-and-reranking systems (Yan et al., 2016; Li et al., 2016).", "startOffset": 106, "endOffset": 141}], "year": 2016, "abstractText": "In this paper, we propose the TBCNNpair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network (TBCNN) captures sentencelevel semantics; then heuristic matching layers like concatenation, element-wise product/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin.", "creator": "LaTeX with hyperref package"}}}