{"id": "1601.00925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Complex Decomposition of the Negative Distance kernel", "abstract": "meli\u00e1 A Support lamarcus Vector qru Machine (edgware SVM) has baramuli become soavi a very popular al-otaibi machine learning airstrike method wener for baltazar text bogus classification. One tented reason obligate for viscounty this relates 93.03 to the farrokh range nhs of 4-61 existing kernels widener which 0622 allow 9-seol for 2pm classifying ormsby-gore data that ning is archbishops not linearly sovereign separable. The nikulin linear, illyrian polynomial u20 and RBF (posession Gaussian bienveillantes Radial Basis binged Function) 17.39 kernel are deuel commonly used and serve as ousa a basis diliani of h/aca comparison 7:2 in divorcing our study. We show huilongguan how to derive taxotere the primal kcna form of bodziak the quadratic sleepiness Power emmrich Kernel (PK) - - then-ongoing also 16:29 called kinnell the diametrically Negative Euclidean 34-21 Distance alysheba Kernel (NDK) - - by ohio means neoguri of toumany complex 75-kilometer numbers. We vesterdorf exemplify the pinwheel NDK in the \u03c7 framework of text categorization using the Dewey Document Classification (dames DDC) imoneynet as billiton the riviresa target scheme. heckuva Our pelophylax evaluation milsom shows that bilma the power ajay kernel circumscribed produces streatham F - bikfaya scores that are comparable chokila to great-grandchild the reference 2.1-billion kernels, kolu but quatsino is - - jiggled except for oteiza the ltjg linear kernel - - 31.27 faster klag to reznik compute. biagio Finally, muchall we nayyouf show how typewriter to mail/express extend figo the NDK - approach haisla by gupt including the Mahalanobis distance.", "histories": [["v1", "Tue, 5 Jan 2016 18:16:07 GMT  (71kb)", "http://arxiv.org/abs/1601.00925v1", "Proceedings of the IEEE International Conference on Machine Learning an Applications, Miami, Florida, 2015"]], "COMMENTS": "Proceedings of the IEEE International Conference on Machine Learning an Applications, Miami, Florida, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tim vor der br\\\"uck", "steffen eger", "alexander mehler"], "accepted": false, "id": "1601.00925"}, "pdf": {"name": "1601.00925.pdf", "metadata": {"source": "CRF", "title": "Complex Decomposition of the Negative Distance Kernel", "authors": ["Tim vor der Br\u00fcck", "Steffen Eger", "Alexander Mehler"], "emails": ["tim.vorderbrueck@hslu.ch", "steeger@em.uni-frankfurt.de", "amehler@em.uni-frankfurt.de"], "sections": [{"heading": null, "text": "Keywords\u2014SVM, kernel function, text categorization"}, {"heading": "INTRODUCTION", "text": "An SVM has become a very popular machine learning method for text classification [5]. One reason for its popularity relates to the availability of a wide range of kernels including the linear, polynomial and RBF (Gaussian radial basis function) kernel. This paper derives a decomposition of the quadratic Power Kernel (PK) using complex numbers and applies it in the area of text classification. Our evaluation shows that the NDK produces F-scores which are comparable to those produced by the latter reference kernels while being faster to compute \u2013 except for the linear kernel. This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.\nAn SVM is a method for supervised classification introduced by [17]. It determines a hyperplane that allows for the best possible separation of the input data. (Training) data on the same side of the hyperplane is required to be mapped to the same class label. Furthermore, the margin of the hyperplane that is defined by the vectors located closest to the hyperplane is maximized. The vectors on the margin are called Support Vectors (SV). The decision function dc : Rn \u2192 {1, 0,\u22121}, which maps a vector to its predicted class, is given by: dc(x) := sgn(\u3008w,x\u3009 + b) where sgn : R \u2192 {1, 0,\u22121} is the Signum function; the vector w and the constant b are determined by means of SV optimization. If dc(x) = 0 then the vector is located directly on the hyperplane and no decision regarding both classes is possible.\nThe decision function is given in the primal form. It can\nbe converted into the corresponding dual form: dc(x) = sgn( \u2211m\nj=1 yj\u03b1j\u3008x,xj\u3009 + b) where \u03b1j and b are constants determined by the SV optimization and m is the number of SVs xj (the input vector has to be compared only with these SVs). The vectors that are located on the wrong side of the hyperplane, that is, the vectors which prevent a perfect fit of SV optimization, are also considered as SVs. Thus, the number of SVs can be quite large. The scalar product, which is used to estimate the similarity of feature vectors, can be generalized to a kernel function. A kernel function K is a similarity function of two vectors such that the matrix of kernel values is symmetric and positive semidefinite. The kernel function only appears in the dual form. The decision function is given as follows: dc(x) := sgn( \u2211m\nj=1 yj\u03b1jK(x,xj) + b). Let \u03a6 : R n \u2192 Rl be\na function that transforms a vector into another vector mostly of higher dimensionality with l \u2208 N \u222a {\u221e}. It is chosen in such a way that the kernel function can be represented by: K(x1,x2) = \u3008\u03a6(x1),\u03a6(x2)\u3009. Thus, the decision function in the primal form is given by:\ndc(x) := sgn(\u3008w,\u03a6(x)\u3009 + b) (1)\nNote that \u03a6 is not necessarily uniquely defined. Furthermore, \u03a6 might convert the data into a very high dimensional space. In such cases, the dual form should be used for the optimization process as well as for the classification of previously unseen data. One may think that the primal form is not needed. However, it has one advantage: if the normal vector of the hyperplane is known, a previously unseen vector can be classified just by applying the Signum function, \u03a6 and a scalar multiplication. This is often much faster than computing the kernel function for previously unseen vectors and each SV as required when using the dual form.\nThe most popular kernel is the scalar product, also called the linear kernel. In this case, the transformation function \u03a6 is the identity function: Klin(x1,x2) := \u3008x1,x2\u3009. Another popular kernel function is the RBF (Gaussian Radial Basis Function), given by: Kr(x1,x2) := e\u2212\u03b3||x1\u2212x2|| 2\nwhere \u03b3 \u2208 R, \u03b3 > 0 is a constant that has to be manually specified. This kernel function can be represented by a function \u03a6r that transforms the vector into infinite dimensional space [19]. For reasons of simplicity, assume that x is a vector with only one component. Then the transformation function \u03a6r is given by:\n\u03a6r(x) :=e \u2212\u03b3x2[1,\n\u221a\n2\u03b3 1! x,\n\u221a\n(2\u03b3)2\n2! x2,\n\u221a\n(2\u03b3)3\n3! x3, . . .]\u22a4 (2)\nAnother often used kernel is the polynomial kernel:\nKp(x1,x2) := (a\u3008x1,x2\u3009+ c)d (3) For d := 2, a = 1, c = 1, and two vector components, the function \u03a6p is given by [9]: \u03a6p : R2 \u2192 R6 with\n\u03a6p(x) = (1, x 2 1, \u221a 2x1x2, x 2 2, \u221a 2x1, \u221a 2x2) (4)\nIn general, a vector of dimension n is transformed by \u03a6p into a vector of dimension (\nn+d d\n)\n. Thus, the polynomial kernel leads to a large number of dimensions in the target space. However, the number of dimensions is not infinite as in the case of the RBF kernel."}, {"heading": "THE POWER KERNEL", "text": "The PK is a conditionally positive definite kernel given by\nKs(x1,x2) := \u2212||x1 \u2212 x2||p (5) for some p \u2208 R [16], [2], [12]. A kernel is called conditionally positive-definite if it is symmetric and satisfies the conditions [14]\nn \u2211\nj,k=1\ncicjK(xj ,xk) \u2265 0 \u2200ci \u2208 K with m \u2211\ni=1\nci = 0 (6)\nwhere cj is the complex-conjugate of cj . We consider here a generalized form of the PK for p := 2 (also called NDK):\nKpow(x1,x2) := \u2212a||x1 \u2212 x2||2 + c (7) with a, c \u2208 R and a > 0. The expression \u2212a||x1 \u2212 x2||2 + c can also be written as:\n\u2212 a\u3008(x1 \u2212 x2), (x1 \u2212 x2)\u3009+ c = \u2212 a(\u3008x1,x1\u3009 \u2212 2\u3008x1,x2\u3009+ \u3008x2,x2\u3009) + c\n(8)\nFor deciding which class a previously unseen vector belongs to we can use the decision function in the dual form:\ndc(x) : = sgn( m \u2211\nj=1\nyj\u03b1jKpow (x,xj) + b)\n= sgn(\nm \u2211\nj=1\nyj\u03b1j(\u2212a||x\u2212 xj ||2 + c) + b) (9)\nThe decision function shown in formula (9) has the drawback that the previously unseen vector has to be compared with each SV, which can be quite time consuming. This can be avoided, if we reformulate formula (9) to:\ndc(x) = sgn( m \u2211\nj=1\nyj\u03b1j(\u2212a\u3008x,x\u3009+ 2a\u3008x,xj\u3009\u2212\na\u3008xj ,xj\u3009+ c) + b)\n= sgn(\u2212a m \u2211\nj=1\nyj\u03b1j\u3008x,x\u3009 + 2a m \u2211\nj=1\nyj\u03b1j\u3008x,xj\u3009+\nm \u2211\nj=1\nyj\u03b1j(\u2212a\u3008xj ,xj\u3009+ c) + b)\n= sgn(\u2212a\u3008x,x\u3009 m \u2211\nj=1\nyj\u03b1j + 2\u3008x, a m \u2211\nj=1\nyj\u03b1jxj\u3009\u2212\na\nm \u2211\nj=1\nyj\u03b1j\u3008xj ,xj\u3009+ c m \u2211\ni=1\nyj\u03b1j + b)\n(10)\nWith z := a \u2211m j=1 yj\u03b1jxj , u := a \u2211m j=1 yj\u03b1j\u3008xj ,xj\u3009 and c\u2032 = c \u2211m\ni=1 yj\u03b1j , formula (10) can be rewritten as:\ndc(x) = sgn(\u2212a\u3008x,x\u3009 m \u2211\ni=1\nyj\u03b1j +2\u3008x, z\u3009 \u2212u+ c\u2032 + b) (11)\nThe expressions u, z, ( \u2211m i=1 yj\u03b1j), and c \u2032 are identical for every vector x and can be precomputed. Note that there exists no primal form for the NDK based on real number vectors which is stated by the following proposition. Proposition 1 Let a,c \u2208 R with a > 0 and n, l \u2208 N. Then there is no function \u03a6re : Rn \u2192 Rl (re indicates that \u03a6re operates on real numbers) with \u2200x1,x2 \u2208 Rn : \u3008\u03a6re(x1),\u03a6re(x2)\u3009 = \u2212a||x1 \u2212 x2||2 + c .\nProof: If such a function existed, then, for all x \u2208 Rn, ||\u03a6re(x)||2 = \u3008\u03a6re(x),\u03a6re(x)\u3009 = \u2212a \u00b7 0 + c = c\nwhich requires that c \u2265 0 since the square of a real number cannot be negative. Now, consider x,y \u2208 Rn with ||x\u2212y||2 > 2c\na \u2265 0. On the one hand, we have, by the Cauchy-\nSchwarz inequality,\n|\u3008\u03a6re(x),\u03a6re(y)\u3009| \u2264 ||\u03a6re(x)|| \u00b7 ||\u03a6re(y)|| = \u221a c \u00b7 \u221a c = c.\nOn the other hand, it holds that\n| \u2212 a||x\u2212 y||2 + c| = |a||x\u2212 y||2 \u2212 c| > 2c\u2212 c = c, a contradiction.\nAlthough no primal form and therefore no function \u03a6re exists for real number vectors, such a function can be given if complex number vectors are used instead. In this case, the function \u03a6c is defined with a real domain and a complex codomain: \u03a6c : Rn \u2192 C4n+1 and\n\u03a6c(x) :=( \u221a a(x21 \u2212 1), \u221a ai, \u221a 2ax1, \u221a aix21, . . . ,\u221a\na(x2n \u2212 1), \u221a ai, \u221a 2axn, \u221a aix2n, \u221a c)\u22a4\n(12)\nNote that no scalar product can be defined for complex numbers that fulfills the usual conditions of bilinearity and positive-definiteness simultaneously1. Thus, the bilinearity condition is dropped for the official definition and only sesquilinearity is required. The standard scalar product is defined as the sum of the products of the vector components with the associated complex conjugated vector components of the other vector. Let x1,x2 \u2208 Cn, then the scalar product is given by [1]: \u3008x1,x2\u3009 := \u2211n\nk=1 x1kx2k. In contrast, we use a modified scalar product (marked by a \u201c\u2217\u201d) where, analogously to the real vector definition, the products of the vector components are summated: \u3008\u2217x1,x2\u3009 := \u2211n\nk=1 x1kx2k. This product (not a scalar product in strict mathematical sense) is a bilinear form but no longer positive definite. For real\n1This can be verified by a simple calculation: Consider some vector x 6= 0 and x \u2208 Cn. Since \u3008.\u3009 is positive definite: \u3008x,x\u3009 > 0, by bilinearity: \u3008 \u221a \u2212ix, \u221a \u2212ix\u3009 = \u2212i\u3008x,x\u3009 6> 0).\nnumber vectors this modified scalar product is identical to the usual definition. With this modified scalar product we get\n\u3008\u2217\u03a6c(x1),\u03a6c(x2)\u3009 =\u2212 ax211 \u2212 ax221 + 2ax11x21 \u2212 \u00b7 \u00b7 \u00b7 \u2212 ax21n\u2212 ax22n + 2ax1nx2n + c = \u2212a||x1 \u2212 x2||2 + c\n(13)\nwhich is just the result of the NDK. The optimization can be done with the dual form. Thus, no complex number optimization is necessary. For the decision on the class to which a data vector should be assigned we switch to the primal form. The vector w \u2208 C4n+1 is calculated by: w := \u2211m\nj=1 \u03b1jyj\u03a6c(xj) for all SVs xj \u2208 Rn. The decision function is then given by: dc(x) := sgn(\u3008\u2217w,\u03a6c(x)\u3009 + b). Note that the modified scalar product \u3008\u2217w,\u03a6c(x)\u3009 must be a real number. This is stated in the following proposition. Proposition 2 Let w = \u2211m\nj=1 \u03b1jyj\u03a6c(xj) with xj \u2208 Rn, \u03b1j \u2208 R, yj \u2208 {\u22121, 1}, j = 1, . . . ,m,\u03a6c as defined in formula (12) and x \u2208 Rn. Then \u3008\u2217w,\u03a6c(x)\u3009 is a real number.\nProof: \u3008\u2217w,\u03a6c(x)\u3009 is given by:\n\u3008\u2217w,\u03a6c(x)\u3009 = \u3008\u2217 m \u2211\nj=1\n\u03b1jyj\u03a6c(xj),\u03a6c(x)\u3009\n=\nm \u2211\nj=1\n\u3008\u2217\u03b1jyj\u03a6c(xj),\u03a6c(x)\u3009 (\u3008\u2217.\u3009 is bilinear)\n=\nm \u2211\nj=1\n\u03b1jyj\u3008\u2217\u03a6c(xj),\u03a6c(x)\u3009\n=\nm \u2211\nj=1\n\u03b1jyj(\u2212a||xj \u2212 x||2 + c) (see form. (13))\n(14)\nwhich is clearly a real number.\nThe NDK is related to the polynomial kernel since it can also be represented by a polynomial. However, it has the advantage over the polynomial kernel that it is faster to compute, since the number of dimensions in the target space grows only linearly and not exponentially with the number of dimensions in the original space [16], [2]. It remains to show that the decision functions following the primal and dual form are also equivalent for the modified form of the scalar product. This is stated in the follow proposition: Proposition 3 Let x,x1, . . . ,xm \u2208 Rn, \u03b1 \u2208 Rm, y \u2208 {\u22121, 1}m, w := \u2211m\nj=1 \u03b1jyj\u03a6c(xj) and \u3008\u2217\u03a6c(z1),\u03a6c(z2)\u3009 = K(z1, z2) \u2200z1, z2 \u2208 Rn. Then sgn(\u3008\u2217w,\u03a6c(x)\u3009 + b) = sgn( \u2211m j=1 \u03b1jyjK(x,xj) + b).\nProof:\nsgn(\u3008\u2217w,\u03a6c(x)\u3009 + b)\n= sgn(\u3008\u2217 m \u2211\nj=1\n\u03b1jyj\u03a6c(xj),\u03a6c(x)\u3009 + b)\n= sgn( m \u2211\nj=1\n\u3008\u2217\u03b1jyj\u03a6c(xj),\u03a6c(x)\u3009 + b) (\u3008\u2217.\u3009 is bilinear)\n= sgn(\nm \u2211\nj=1\n\u03b1jyj\u3008\u2217\u03a6c(xj),\u03a6c(x)\u3009 + b)\n(15)\n=sgn(\nm \u2211\nj=1\n\u03b1jyjK(xj ,x) + b)\n= sgn(\nm \u2211\nj=1\n\u03b1jyjK(x,xj) + b) (K is symmetric)\n(16)\nNormally, feature vectors for document classification represent the weighted occurrences of lemma or word forms [15]. Thus, such a vector contains a large number of zeros and is therefore usually considered sparse. In this case, the computational complexity of the scalar product can be reduced from O(n) (where n is the number of vector components) to some constant runtime, which is the average number of nonzero vector components. Let I1 be the set of indices of nonzero entries of vector x1 (I1 = {k \u2208 {1, . . . , n} : x1k 6= 0}) and I2 be analogously defined for vector x2. The scalar product of both vectors can then be computed by \u2211\nk\u2208(I1\u2229I2) x1k \u00b7x2k.\nLet us now consider the case that both vectors are transformed to complex numbers before the scalar multiplication. In this case, the modified scalar product\n\u3008\u2217\u03a6c(x1),\u03a6c(x2)\u3009 = \u3008(\u03c6(x11), . . . , \u03c6(x1n)), (\u03c6(x21), . . . , \u03c6(x2n))\u3009+ c\n(17)\nis considered where \u03c6(xk) denotes the transformation of a single real vector component to a complex number vector and is defined as:\n\u03c6 :R \u2192 C4, \u03c6(xk) := ( \u221a a(x2k \u2212 1), \u221a ai, \u221a 2axk, \u221a aix2k)\n(18)\nNote that the partial modified scalar product \u3008\u2217\u03c6(x1k), \u03c6(x2k)\u3009 can be non-zero, if at least one of the two vector components x1k and x2k is non-zero, which is easy to see:\n\u3008\u2217\u03c6(xk), \u03c6(0)\u3009 = \u221a a(x2k \u2212 1) \u00b7 \u221a a(\u22121) + (\u22121)a\n+ \u221a 2axk \u00b7 0 + \u221a aix2k \u00b7 0 = a\u2212 ax2k \u2212 a = \u2212ax2k\n(19)\nOnly if both vector components are zero, one can be sure that the result is also zero:\n\u3008\u2217\u03c6(0), \u03c6(0)\u3009 = (\u2212 \u221a a)(\u2212 \u221a a)+ai\u00b7i+0+0 = a\u2212a = 0 (20)\nThus, the sparse data handling of two transformed complex vectors has to be modified in such a way that vector components associated with the union and not the intersection of non-zero indices are considered for multiplication:\n\u3008\u2217x1,x2\u3009 = \u2211\nk\u2208(I1\u222aI2)\n\u3008\u2217\u03c6(x1k), \u03c6(x2k)\u3009+ c (21)\nA further advantage of the NDK is that the Mahalanobis distance [8] can easily be integrated, which is shown in the remainder of this section. Each symmetric matrix A can be represented by the product V\u22121DV where D is a diagonal matrix with the eigenvalues of A on its diagonals. The square root of a matrix A is then defined as\u221a A := V\u22121D0.5V where D0.5 is the matrix with the square root of the eigenvalues of A on its diagonal. It is obvious\nthat \u221a A \u00b7 \u221a A = A. Proposition 4: Let x, z \u2208 Rn, a := 1, c := 0, then \u3008\u2217\u03a6c( \u221a Cov\u22121x),\u03a6c( \u221a Cov\u22121z)\u3009 = \u2212MH (x, z)2 where MH (x, z) denotes the Mahalanobis distance \u221a\n(x\u2212 z)TCov\u22121(x\u2212 z) and Cov denotes the covariance matrix between the feature values of the entire training data set.\nProof: We have that (with C := \u221a Cov\u22121):\n\u3008\u2217\u03a6c(Cx),\u03a6c(Cz)\u3009 =\u2212 ||Cx\u2212Cz||2 (see formula (13)) =\u2212 (Cx\u2212Cz)\u22a4(Cx\u2212Cz) =\u2212 (C(x \u2212 z))\u22a4(C(x\u2212 z))\n=\u2212 (x\u2212 z)\u22a4 \u221a Cov\u22121 \u22a4\u221a\nCov\u22121(x\u2212 z) =\u2212 (x\u2212 z)\u22a4Cov\u22121(x\u2212 z)\n(22)\n(since the covariance matrix is symmetric and the inverse and square root of a symmetric matrix is also symmetric)\nThis proposition shows that the NDK is just the negative square of the Mahalanobis distance for some vectors x\u0302 and z\u0302 if x\u0302 is set to \u221a Cov\u22121x (analogously for z\u0302), a is set to 1 and c is set to 0. Furthermore, this proposition shows that we can easily extend our primal form to integrate the Mahalanobis distance. For that, we define a function \u03c4 : Rn \u2192 Rn as follows: \u03c4(x) = \u221a Cov\u22121x. With \u03a6m:=\u03a6c \u25e6 \u03c4 we have:\n\u3008\u2217\u03a6m(x),\u03a6m(z)\u3009 = (MH (x, z))2 (23) which shows that we have indeed derived a primal form.\nWe use the NDK for text classification where a text is automatically labeled regarding its main topics (i.e., categories), employing the DDC as one of the leading classification schemes in digital libraries [11]. Documents are classified regarding the 10 top-level categories of the DDC. To this end, we use training sets of documents for which the DDC categories have already been assigned. Lexical features are extracted from this training set and made input to an SVM library (i.e., libsvm [3]) in order to assign one or more DDC categories to previously unseen texts. Features of instance documents are computed by means of the geometric mean of the tfidf values and the GSS coefficients of their lexical constituents [4], [13]."}, {"heading": "EVALUATION", "text": "The evaluation and training was done using 4 000 German documents, containing in total 114 887 606 words and 9 643 022 sentences requiring a storage space of 823.51 MB. There are 400 documents of each top-level DDC category in the corpus. 2 000 of the texts were used for training, 2 000 for evaluation. The corpus consists of texts annotated according to the OAI (Open Archive Initiative) and is presented in [7]. We tested the correctness of category assignment for the 10 toplevel DDC categories. Each document is mapped at least to one DDC category. Multiple assignments are possible. Precision, recall and F-scores were computed for each DDC category using the NDK, the square (polynomial kernel of degree 2),\nthe cubic (polynomial kernel of degree 3), the RBF kernel and the linear kernel (see Tables I and II). The free parameters of the square, the cubic and the RBF kernel are determined by means of a grid search on an independent data set. On the same held-out dataset, we adjusted the SVM-threshold parameter b to optimize the F-scores. The time (on an Intel Core i7) required to obtain the classifications was determined (see Table III for the real / complex NDK, the RBF, square, and linear kernel). The time needed for building the model was not measured because of being required only once and therefore being irrelevant for online text classification.\nThe F-score for the NDK is higher than the F-scores of all other kernels and faster to compute except for the linear kernel \u2013 obviously, the classification using the primal form of the linear kernel is faster than the one using the NDK. Furthermore, the complex decomposition of the NDK leads to a considerable acceleration of the classification process compared to its dual form and should therefore be preferred.\nWe conducted a second experiment where we compared the linear kernel with the NDK now using text snippets (i.e., abstracts instead of full texts) taken once more from our OAI corpus. This application scenario is more realistic for digital libraries that mainly harvest documents by means of their meta data and, therefore, should also be addressed by text classifiers. The kernels were trained on a set of 19 000 abstracts2. This time, the categories are not treated independently of each other. Instead, the classifier selected all categories with signed distance values from the SVM hyperplane that are greater or equal to zero. If all distance values are negative, the category with the largest (smallest absolute) distance was chosen. In this experiment, the linear kernel performed better than the NDK. Using three samples of 500 texts, the F-scores of the linear kernel are (macro-averaged over all main 10 DDC categories): 0.753, 0.735, 0.743, and of the NDK: 0.730, 0.731, 0.737. This result indicates that the heuristic of preferring the categories of highest signed distance to the hyperplane is not optimal for the NDK. We compared our classifier with two other systems, the DDC classifier of [18] and of [6]. [18] reaches an F-score of 0.502, 0.457 and 0.450 on these three samples. The Fscores of the DDC-classifier of [6] are 0.615, 0.604, and 0.574. Obviously, our classifier outperforms these competitors.\nFinally, we evaluated the NDK on the Reuters 21578 corpus3 (Lewis split). This test set is very challenging since 35 of all 93 categories occurring in this split have 10 or less training examples. Furthermore, several texts are intentionally assigned to none of the Reuters categories. We created an artificial category for all texts that are not assigned in this sense. The histogram of category assignments is displayed in Figure 1. The F-scores for the Reuters corpus are given in Table IV. We modified the training data by filling up instances for every category by randomly selecting positive instances such that the ratio of positive examples to all instances are the same for all categories. Hence, in most category samples some training examples were used more than once. This approach prevents from preferring categories due to their multitude of training examples. The F-score of the NDK is lower than the\n2Note that in this evaluation we employed the tfidf score for weighting only, since the use of the GSS coefficient didn\u2019t lead to any improvement in F-score.\n3URL: http://www.daviddlewis.com/resources/testcollections/reuters21578\nhighest macro-averaging F-score obtained by the linear kernel, but outperforms the square and the cubic kernel. Again, the parameters of the NDK, the square, the cubic and the RBF kernels were determined by means of a grid search."}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "We derived a primal form of the NDK by means of complex numbers. In this way, we obtained a much simpler representation compared to the modified dual form. We showed that the primal form (and in principle also the modified dual form) can speed up text classification considerably. The reason is that it does not require to compare input vectors with all support vectors. Our evaluation showed that the F-scores of the NDK are competitive regarding all other kernels tested here while the NDK consumes less time than the polynomial and the RBF kernel. We have also shown that the NDK performs better than the linear kernel when using full texts rather than text snippets. Whether this is due to problems of feature selection/expansion or a general characteristic of this kernel (in the sense of being negatively affected by ultra-sparse features spaces), will be examined in future work. Additionally, we plan to examine the PK with exponents larger than two, to investigate under which prerequisites the PK performs well and to evaluate the extension of the NDK that includes the Mahalanobis distance."}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank Vincent Esche and Tom Kollmar for valuable comments and suggestions for improving our paper."}], "references": [{"title": "Exploring efficient kernel functions for support vector machine based feasability models for analog circuits", "author": ["Shri D. Boolchandani", "Vineet Sahula"], "venue": "International Journal of Design Analysis and Tools for Circuits and Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "The Maximum Margin Approach to Learning Text Classifiers: Methods, Theory, and Algorithms", "author": ["Thorsten Joachims"], "venue": "PhD thesis, Universita\u0308t Dortmund, Informatik, LS VIII,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Learning to classify text using support vector machines", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Talk: Automatische Sacherschlie\u00dfung elektronischer Dokumente nach DDC", "author": ["Mathias L\u00f6sch"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Building a DDC-annotated corpus from OAI metadata", "author": ["Mathias L\u00f6sch", "Ulli Waltinger", "Wolfram Horstmann", "Alexander Mehler"], "venue": "Journal of Digital Information,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "On the generalised distance in statistics", "author": ["Prasanta Chandra Mahalanobis"], "venue": "In Proceedings of the National Institute of Sciences of India,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1936}, {"title": "Introduction to Information Retrieval", "author": ["Christopher Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Enhancing document modeling by means of open topic models: Crossing the frontier of classification schemes in digital libraries by example of the DDC", "author": ["Alexander Mehler", "Ulli Waltinger"], "venue": "Library Hi Tech,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Scale-invariance of support vector machines based on the triangular kernel", "author": ["Hichem Sahbi", "Francois Fleuret"], "venue": "Technical Report 4601,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": "Information Processing & Management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Learning with Kernels - Support Vector Machines, Regularization, Optimization and Beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani"], "venue": "ACM Comput. Surv.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Kernel functions for machine learning", "author": ["C\u00e9sar Souza"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Statistical Learning Theory", "author": ["Vladimir Vapnik"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Hierarchical classification of oai metadata using the DDC taxonomy. In Advanced Language Technologies for Digital Libraries, Lecture Notes in Computer Science, pages 29\u201340", "author": ["Ulli Waltinger", "Alexander Mehler", "Mathias L\u00f6sch", "Wolfram Horstmann"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Support vector machine tutorial", "author": ["Shih-Hung Wu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "An SVM has become a very popular machine learning method for text classification [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "An SVM is a method for supervised classification introduced by [17].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "This kernel function can be represented by a function \u03a6r that transforms the vector into infinite dimensional space [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "Kp(x1,x2) := (a\u3008x1,x2\u3009+ c) (3) For d := 2, a = 1, c = 1, and two vector components, the function \u03a6p is given by [9]: \u03a6p : R \u2192 R with \u03a6p(x) = (1, x 2 1, \u221a 2x1x2, x 2 2, \u221a 2x1, \u221a 2x2) (4)", "startOffset": 112, "endOffset": 115}, {"referenceID": 13, "context": "THE POWER KERNEL The PK is a conditionally positive definite kernel given by Ks(x1,x2) := \u2212||x1 \u2212 x2|| (5) for some p \u2208 R [16], [2], [12].", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "THE POWER KERNEL The PK is a conditionally positive definite kernel given by Ks(x1,x2) := \u2212||x1 \u2212 x2|| (5) for some p \u2208 R [16], [2], [12].", "startOffset": 128, "endOffset": 131}, {"referenceID": 9, "context": "THE POWER KERNEL The PK is a conditionally positive definite kernel given by Ks(x1,x2) := \u2212||x1 \u2212 x2|| (5) for some p \u2208 R [16], [2], [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "A kernel is called conditionally positive-definite if it is symmetric and satisfies the conditions [14]", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "However, it has the advantage over the polynomial kernel that it is faster to compute, since the number of dimensions in the target space grows only linearly and not exponentially with the number of dimensions in the original space [16], [2].", "startOffset": 232, "endOffset": 236}, {"referenceID": 0, "context": "However, it has the advantage over the polynomial kernel that it is faster to compute, since the number of dimensions in the target space grows only linearly and not exponentially with the number of dimensions in the original space [16], [2].", "startOffset": 238, "endOffset": 241}, {"referenceID": 12, "context": "Normally, feature vectors for document classification represent the weighted occurrences of lemma or word forms [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "A further advantage of the NDK is that the Mahalanobis distance [8] can easily be integrated, which is shown in the remainder of this section.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": ", libsvm [3]) in order to assign one or more DDC categories to previously unseen texts.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "Features of instance documents are computed by means of the geometric mean of the tfidf values and the GSS coefficients of their lexical constituents [4], [13].", "startOffset": 150, "endOffset": 153}, {"referenceID": 10, "context": "Features of instance documents are computed by means of the geometric mean of the tfidf values and the GSS coefficients of their lexical constituents [4], [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "The corpus consists of texts annotated according to the OAI (Open Archive Initiative) and is presented in [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 15, "context": "We compared our classifier with two other systems, the DDC classifier of [18] and of [6].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "We compared our classifier with two other systems, the DDC classifier of [18] and of [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 15, "context": "[18] reaches an F-score of 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The Fscores of the DDC-classifier of [6] are 0.", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "F-SCORES OF DIFFERENT KERNELS EVALUATED BY MEANS OF THE OAI CORPUS OF [7].", "startOffset": 70, "endOffset": 73}], "year": 2016, "abstractText": "A Support Vector Machine (SVM) has become a very popular machine learning method for text classification. One reason for this relates to the range of existing kernels which allow for classifying data that is not linearly separable. The linear, polynomial and RBF (Gaussian Radial Basis Function) kernel are commonly used and serve as a basis of comparison in our study. We show how to derive the primal form of the quadratic Power Kernel (PK) \u2013 also called the Negative Euclidean Distance Kernel (NDK) \u2013 by means of complex numbers. We exemplify the NDK in the framework of text categorization using the Dewey Document Classification (DDC) as the target scheme. Our evaluation shows that the power kernel produces F-scores that are comparable to the reference kernels, but is \u2013 except for the linear kernel \u2013 faster to compute. Finally, we show how to extend the NDK-approach by including the Mahalanobis distance. Keywords\u2014SVM, kernel function, text categorization", "creator": "LaTeX with hyperref package"}}}