{"id": "1601.01195", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON 2015", "abstract": "undiscussed This rookmangud paper discusses the experiments carried salzmann out runing by wytw\u00f3rnia us pattern at robbing Jadavpur xuebing University temperton as part of the ocb participation in ICON 2015 khutba task: sternness POS Tagging nosediving for mthembu Code - mixed Indian hashlosha Social Media Text. entryways The tool 2,024 that we jalu have developed for hel101 the williamtown task collected is based bkeefecoxnews.com on Trigram Hidden fleur-de-lys Markov perplexa Model riff-raff that utilizes information phenomenal from dictionary hensby as impactful well as 15.84 some abdennour other floriane word roco level features sprinters to enhance rossner the observation whitened probabilities gameiro of the awoniyi known callery tokens as well garcia as neen unknown tokens. spiritualistic We submitted dejar runs for deterioration Bengali - English, edelbacher Hindi - doraville English and operatic Tamil - English anyama Language thoen pairs. Our 101-year system has tburr@globe.com been qal trained and tested 45.45 on the etro datasets gutawa released for halakha ICON 2015 2-1/2 shared n\u00e4r task: dunsford POS swimmin Tagging For hohenberg Code - heneghan mixed Indian icef Social Media Text. In bridgmohan constrained mode, atps our l'herbier system warsop obtains bytitle average brougham overall minidisc accuracy (semaphores averaged physicist over all panton three language jaegers pairs) of 75. biskupi 60% subpoenaing which is represents very close 1967-1974 to brigman other sohni participating kanki two systems (pepfar 76. elizur 79% for pallet IIITH presbyteries and 75. 79% mancroft for AMRITA_CEN) glutinosa ranked higher than satisfactions our superintendant system. In malisevo unconstrained mode, spanish-based our system blewbury obtains variance average brawd overall hooj accuracy raska of 1947/48 70. 65% 44.41 which is steevens also close faid to the english-only system (72. 85% for shipbuilder AMRITA_CEN) hoesch which patroons obtains the lighter highest tyrol average scaruffi overall bormida accuracy.", "histories": [["v1", "Wed, 6 Jan 2016 14:40:38 GMT  (232kb)", "http://arxiv.org/abs/1601.01195v1", "NLP Tool Contest on \"POS Tagging For Code-mixed Indian Social Media Text\" held in conjunction with International Conference on Natural Language Processing(ICON 2015). arXiv admin note: text overlap witharXiv:1512.03950,arXiv:1405.7397"]], "COMMENTS": "NLP Tool Contest on \"POS Tagging For Code-mixed Indian Social Media Text\" held in conjunction with International Conference on Natural Language Processing(ICON 2015). arXiv admin note: text overlap witharXiv:1512.03950,arXiv:1405.7397", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kamal sarkar"], "accepted": false, "id": "1601.01195"}, "pdf": {"name": "1601.01195.pdf", "metadata": {"source": "CRF", "title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON 2015", "authors": ["Kamal Sarkar"], "emails": ["jukamal2001@yahoo.com"], "sections": [{"heading": null, "text": "Keywords Part-of-Speech Tagging, Code Mixed, Social Media, HMM."}, {"heading": "1. INTRODUCTION", "text": "Part-of-Speech (POS) tagging is the task of assigning grammatical categories (noun, verb, adjective etc.) to words in a natural language sentence [1]. POS tagging can be used in various NLP (Natural Language Processing) applications. The interest in applying NLP methods for analyzing nonstandardized texts, such as social media texts, rapidly is growing [2], because the automatic analysis of social media texts is one of essential requirements for the task of sentiment analysis [3]. Since social media texts contain blog comments or chat messages, it differs from standardized texts in the word usage but also in their grammatical structure. This creates the need for adapting NLP methods to analyzing social media text and in particular, for the adaption of POS tagging methods to such text types. Most state-of-the art taggers have been developed for standardized texts.\nThis paper presents a description of HMM (Hidden Markov Model) based system for POS tagging from Social Media Text in Indian Languages.\nThe ICON 2015 shared task: POS Tagging For Codemixed Indian Social Media Text is defined in this year to build the POS tagger systems for code mixed Indian social media text - Bengali-English, Hindi-English and TamilEnglish language pairs for which training data and test data were provided. Data set for a language pair contains the social media text written in the languages of the concerned pair. For example, for Bengali-English language pair, data set contains the social media text written in English and Hindi. We have participated for all three language pairs.\nPOS Tagger can be developed using both linguistic models and stochastic models. The earliest works on POS tagging [4][5][6] use supervised learning methods.\nSome research work has already done for developing POS tagger for standard texts in Indian languages [7].\nDandapat et. al [8].presents HMM and Maximum Entropy (ME) based approaches for Bengali POS tagging.\nEkbal et. al. [9] presented a POS tagger for Bengali language using Conditional Random Fields (CRF). They also discussed another machine learning based POS tagger using SVM algorithm in [10].\nAn unsupervised Parts-of-Speech Tagger for the Bangla language was proposed by Ali et.al. in [11]. Chakrabarti et.al.[12] has proposed a Layered Parts of Speech Tagging for Bangla.\nA detailed survey on POS tagging for other Indian languages has been presented in [13][14].\nA few attempts have also been made for developing POS tagger for code mixed Indian social media text. A POS Tagging System of English-Hindi Code-Mixed Social Media Content has been presented in [15]. A POS tagging system for Indian Social Media Text on Twitter has been presented in [16]."}, {"heading": "2. PREPARATION OF TRAINING DATA", "text": "The training data released for the ICON 2015 shared task contains three files: one file for Bengali-English Language pair, one file for Hindi-English language pair and one file for Tamil-English language pair. Each line in a file contains tokens in the languages of concerned pair, Language tag and Part-of-Speech tag. The participants are instructed to produce the output in the same format after testing the system on the test data where the test data contains per line a tab separated token and the corresponding language tag. Our system uses a training file for a language pair and converts each sentence into a sequence of pairs of token and tag where each token in this new format is formed by combining the source token and some other information such as language tag. The detailed of this format is discussed in the later sections."}, {"heading": "3. HMM MODEL FOR POS TAGGING", "text": "A POS tagger based on Hidden Markov Model (HMM) finds the best sequence of POS tags 1\nnt that is optimal for a given observation sequence 1\nno . The tagging problem becomes equivalent to searching for\n1\n1 1 1arg max ( | ) ( ) n\nn n n\nt P o t P t (by the application of Bayes\u2019\nlaw), that is, we need to compute:\n1\n1 1 1 1 \u02c6 arg max ( | ) ( )\nn\nn n n n\nt t P o t P t= (1).\nWhere 1 nt is a tag sequence and 1 no is an observation sequence, 1( )\nnP t is the prior probability of the tag sequence and 1 1( | )\nn nP o t is the likelihood of the word sequence.\nIn general, HMM based POS tagging use words in a sentence as an observation sequence [1] [7]. But, we use some additional information such as language tag for disambiguating each token in text. We also use some other information such as whether the token contains any hash tag or not. We use this information in a form of meta tag (details are presented in the subsequent sections). We use a small dictionary of words which contains words with its broad POS categories. If any token is found in the dictionary, we use the broad POS tag as some additional information which we combines with the observation token (details are presented in the subsequent sections).\nUnlike the traditional HMM based POS tagging system, to use this additional information for POS tagging task, we consider a triplet as an observation symbol: <word, metatag, Language tag >. This is a pseudo token used as an observed symbol, that is, for a sentence of n words, the corresponding observation sequence will be as follows:\n(<word1, meta-tag1, L-tag1, >, <word2, meta-tag2, Ltag2>, <word3, meta-tag3, L-tag3 >, .........., <wordn, metatagn, L-tagn,>) . Here an observation symbol oi corresponds to <wordi, meta-tagi, L-tagi, > and L-tag is the language tag and meta-tag is decided based on the additional information (e.g. Hash tag).\nSince Equation (1) is too hard to compute directly, HMM taggers follows Markov assumption according to which the probability of a tag is dependent only on short memory (a small, fixed number of previous tags). For example, a bigram tagger considers that the probability of a tag depends only on the previous tag\nFor our proposed trigram model, the probability of a tag depends on two previous tags and thus 1( )\nnP t is computed as:\n1 1 21 ( ) ( | , )\nn n\ni i ii P t P t t t\u2212 \u2212=\u2248 \u03a0 (2) Depending on the assumption that the probability of a word appearing is dependent only on its own tag,\n1 1( | ) n nP o t can be simplified to:\n1 1 1\n( | ) ( | ) n\nn n i i i P o t P o t = \u2248 \u220f\n(3) Plugging the above mentioned two equations (2) and (3) into (1) results in the following equation by which a bigram tagger estimates the most probable tag sequence:\n1 1\n1 1 1 1 1 1 \u02c6 argmax ( | ) ( ) argmax ( | ) ( | ) n n\nn n n n n\ni i i i it t t P t o P t P o t P t t \u2212 =\n= \u2248 \u220f (4)\nWhere: the tag transition probabilities, 1( | )i iP t t \u2212 , represent the probability of a tag given the previous tag.\n( | )i iP o t represents the probability of an observed symbol given a tag.\nConsidering a special tag tn+1 to indicate the end sentence boundary and two special tags t-1 and t0 at the starting boundary of the sentence and adding these three special tags to the tag set [4], gives the following equation for POS tagging:\n1\n1\n1 1 1 1\n1 2 1 1\n\u02c6 argmax ( | ) ( )\nargmax[ ( | ) ( | , )] ( | )\nn\nn\nn n n n\nt\nn\ni i i i i n n it\nt P t o P t\nP o t P t t t P t t\u2212 \u2212 + =\n= \u2248\n\u220f (5)\nThe equation (5) is still computationally expensive because we need to consider all possible tag sequence of length n. So, dynamic programming approach is used to compute the equation (5).\nAt the training phase of HMM based POS tagging, observation probability matrix and tag transition probability matrix are created. A general Architecture of our developed POS tagger is shown in Figure 1.\nAs we can see from the equation (4), to find the most likely tag sequence for an observation sequence, we need\nto compute two kinds of probabilities: tag transition probabilities and word likelihoods or observation probabilities.\nOur developed trigram HMM tagger requires to compute tag trigram probability, 1 2( | , )i i iP t t t\u2212 \u2212 , which is computed by the maximum likelihood estimate from tag trigram counts. To overcome the data sparseness problem, tag trigram probability is smoothed using deleted interpolation technique [17][4] which uses the maximum likelihood estimates from counts for tag trigram, tag bigram and tag unigram. The observation probability of a observed triplet <word, meta-tag, L-tag >, which is the observed symbol in our case, is computed using the following equation [1][17].\n( , ) ( )( | ) C o t C oP o t = (7)"}, {"heading": "3.1 Viterbi Decoding", "text": "We have used Viterbi algorithm to find the best hidden state sequence given an input HMM and a sequence of observation symbols.\nThe Viterbi algorithm is a standard application of the classic dynamic programming algorithm [18].\nGiven a tag transition probability matrix and the observation probability matrix, Viterbi decoding (used at the testing phase) accepts a sentence from code mixed social media text and finds the most likely tag sequence for the test sentence which is also L-tagged and Meta tagged. Here a sentence is submitted to the viterbi as the observation sequence of triplets:\n(<word1, meta-tag1, L-tag1>, <word2, meta-tag2, L-tag2>, <word3, meta-tag3, L-tag3 >, .........., <wordn, meta-tagn, Ltagn >) . Here an observation symbol oi corresponds to <wordi, meta-tagi, L-tagi,> and L-tag is a language tag and Meta tag is determined based on the dictionary information and Hash tag feature.\nAfter assigning the tag sequence to the observation sequence as mentioned above, L-tag and meta-tag information are removed from the output and thus the output for an input sentence is converted to a POS-tagged sentence.\nOne of the important problems to apply Viterbi decoding algorithm is how to handle unknown triplets in the input. The unknown triplets are triplets which are not present in the training set and hence their observation probabilities are not known. To handle this problem, we estimate the observation probability of an unknown one by analyzing L-tag, meta-tag and the suffix of the word associated with the corresponding the triplet. We estimate the observation probability of an unknown observed triplet in the following ways:\nThe observation probabilities of unknown triplet < word, meta-tag, L-tag> corresponding to a word in the input sentence are decided according to the suffix of a pseudo word formed by adding L-tag and meta-tag to the end of the word. We find the observation probabilities of such unknown pseudo words using suffix analysis [17][4]. of all rare pseudo words (frequency <=2) in the training corpus for the concerned language pairs."}, {"heading": "4. SPECIAL TAGS", "text": ""}, {"heading": "4.1 Meta Tag", "text": "Each token has some properties by which one token differs from another. For example, a token may contain \u201cHash tag\u201d which is frequent in the social media text.\nMeta-tag=\u201dYYYY\u201d(default) if the first character of the token is a Hash symbol (#) then metatag = \"HB\u201d\nelse if the hash tag is present in any other position of a token metatag = \"HE\" End If"}, {"heading": "4.2 Dictionary", "text": "In earlier sections, we have mentioned that we have used some dictionary information as the meta-tag also. A metatag is set to the value of broad POS tag for a token after matching it with the dictionary words and retrieving the corresponding broad POS tag found in the dictionary. The description of dictionary is shown in Table 1.\nWe follow the following rules for assigning to the token this type of broad POS tag extracted from the dictionary: If raw token is found in the dictionary and the broad POS tag of the concerned token is XXXX then meta-tag =\"XXXX\" end if\nSince we have used only verb, pronoun and conjunctions in the dictionaries, XXXX can take one three values: VERB, PNON and CONJ."}, {"heading": "5. EVALUATION AND RESULTS", "text": "We train separately our developed POS tagger based on the training data and tune the parameters of our system on the training data for the respective language pair. After learning the tuning parameters, we test our system on the test data for the concerned language pair. The description of the data for three language pairs is shown in the Table2\nOur developed POS system has been evaluated using the traditional accuracy measure. For training, tuning and testing our system, we have used the datasets for three different language pairs: Bengali-English, Hindi-English and Tamil-English, released by the organizers of ICON 2015 shared task: POS Tagging For Code-mixed Indian Social Media Text.\nTable2. The description of the data for various language pairs\nLanguage Total of sentences Training data Test data\nBengali-English 2837 1459\nHindi-English 729 377\nTamil-English 639 279\nThe organizers of the shared task released the data in two phases: in the first phase, training data is released where training data was language tagged and POS tagged. In the second phase, the test data is released where test data was only language tagged. The contestants are instructed to assign POS tags to the sentences in the test file using their developed systems. The tagged test files for test data sets were finally sent to the organizers for evaluation. The organizers evaluate the different runs submitted by the various teams and send the official results to the participating teams. A total of 10 teams submitted their runs for this contest. For each language pair the contests were done in two different modes: Constrained mode and unconstrained mode. In contrained mode, the participant team is only allowed to use the training corpus. No external resource is allowed. In unconstrained mode, the participant team is allowed to use any external resources (POS tagger, NER, Parser, and additional data) to train their system.\nIn constrained mode, we have not used any dictionary and only Hash tag has been used as the meta-tag. In unconstrained mode, we have used a small dictionary as mentioned in Table 1 and Hash tag has been used as the meta-tag.\nThe results obtained by our system (team code: KS_JU) have been shown in the tables 3 to 8. The results obtained by other participating systems have also been shown in the tables. The second row of the each table shows the overall accuracy obtained by the various systems participated in the contest.\nWe have also evaluated the system based on its consistency across the languages in constrained and unconstrained mode. Average overall accuracy is computed by taking the average of overall accuracy of the system obtained for all three language pairs in a particular mode.\nIn constrained mode, our system obtains average overall accuracy (averaged over all three language pairs) of 75.60% which is very close to other participating two systems (76.79% for IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In unconstrained mode, our system obtains average overall accuracy of 70.65% which is also close to the system (72.85% for AMRITA_CEN) which obtains the highest average overall accuracy."}, {"heading": "U 0.00% 12.50% 62.50% 0.00% 62.50% 93.75% 93.75% 0.00%", "text": ""}, {"heading": "6. CONCLUSION", "text": "This paper describes a POS tagging system for code mixed social media text in Indian Languages. The features such as dictionary based information and some other word level features have been introduced into the HMM model. The experimental results show that performance of our system is comparable with the best performing systems participated in ICON 2015 task: POS Tagging for Code-mixed Indian Social Media Text. The POS tagging system has been developed using Visual Basic platform so that a suitable user interface can be designed for the novice users. The system has been designed in such a way that only changing the training corpus in a file can make the system portable to other Indian languages."}], "references": [{"title": "November. A practical partof-speech tagger for Bengali", "author": ["K. Sarkar", "V. Gayen"], "venue": "In Emerging Applications of Information Technology (EAIT),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Part-of-speech tagging for social media texts. In Language Processing and Knowledge in the Web (pp. 139-150)", "author": ["M. Neunerdt", "B. Trevisan", "M. Reyer", "R. Mathar"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A multi-level annotation model for fine-grained opinion detection in German blog comments", "author": ["B. Trevisan", "M. Neunerdt", "E.M. Jakobs"], "venue": "In Proceedings of KONVENS (Vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "TnT \u2013 A statistical part-of-speech tagger", "author": ["T. Brants"], "venue": "In Proc. Of the 6 Applied NLP Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Automatic partof-speech tagging for bengali: an approach for morphologically rich languages in a poor scenario", "author": ["S. Dandapat", "S. Sarkar", "A. Basu", "2007"], "venue": "Proceedings of the Association for Computational Linguistic, pp. 221-224", "citeRegEx": "5", "shortCiteRegEx": null, "year": 0}, {"title": "Bengali part of speech tagging using conditional random field", "author": ["Ekbal", "et"], "venue": "Proceedings of the 7 International Symposium of Natural Language Processing(", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A Trigram HMM-Based POS Tagger for Indian Languages", "author": ["K. Sarkar", "V. Gayen"], "venue": "In Proceedings of the International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) (pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Automatic partof-speech tagging for bengali: an approach for morphologically rich languages in a poor scenario", "author": ["S. Dandapat", "S. Sarkar", "A. Basu", "2007"], "venue": "Proceedings of the Association for Computational Linguistic, pp. 221-224.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 0}, {"title": "Bengali part of speech tagging using conditional random field", "author": ["A. Ekbal", "et. al"], "venue": "Proceedings of the 7 International Symposium of Natural Language Processing( SNLP-2007), Pattaya, Thailand,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Part of speech tagging in bengali using support vector machine\u201d, ICIT-08", "author": ["A. Ekbal", "S. Bandyopadhyay"], "venue": "IEEE International Conference on Information Technology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "An unsupervised parts-of-speech tagger for the bangla language\u201d, Department of Computer Science, University of British Columbia", "author": ["H. Ali"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Layered parts of speech tagging for Bangla\u201d, Language in Indian www.languageinindia.com, Special Volume: Problems of Parsing in Indian Languages", "author": ["D. Chakrabarti"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Parts of speech tagging for Indian languages: a literature survey", "author": ["P.J. Antony", "K.P. Soman"], "venue": "International Journal of Computer Applications", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Part of speech taggers for morphologically rich indian languages: a survey", "author": ["D. Kumar", "Singh Josan G.", "2010"], "venue": "International Journal of Computer Applications(0975-8887) Volume 6-No.5.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 0}, {"title": "October. Pos tagging of english-hindi code-mixed social media content", "author": ["Y. Vyas", "S. Gella", "J. Sharma", "K. Bali", "M. Choudhury"], "venue": "In Proceedings of the First Workshop on Codeswitching,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Part-of-Speech Tagging for Code-Mixed English-Hindi Twitter and Facebook Chat Messages", "author": ["A. Jamatia", "B. Gamb\u00e4ck", "Das A"], "venue": "In the Proceeding of 10th Recent Advances of Natural Language Processing (RANLP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "An HMM based named entity recognition system for Indian languages: the JU system at ICON 2013.", "author": ["V. Gayen", "K. Sarkar"], "venue": "arXiv preprint arXiv:1405", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition, Preason Education Series", "author": ["D. Jurafsky", "J.H. Martin"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": ") to words in a natural language sentence [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "The interest in applying NLP methods for analyzing nonstandardized texts, such as social media texts, rapidly is growing [2], because the automatic analysis of social media texts is one of essential requirements for the task of sentiment analysis [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "The interest in applying NLP methods for analyzing nonstandardized texts, such as social media texts, rapidly is growing [2], because the automatic analysis of social media texts is one of essential requirements for the task of sentiment analysis [3].", "startOffset": 247, "endOffset": 250}, {"referenceID": 3, "context": "The earliest works on POS tagging [4][5][6] use supervised learning methods.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "The earliest works on POS tagging [4][5][6] use supervised learning methods.", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "The earliest works on POS tagging [4][5][6] use supervised learning methods.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "Some research work has already done for developing POS tagger for standard texts in Indian languages [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "al [8].", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "[9] presented a POS tagger for Bengali language using Conditional Random Fields (CRF).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "They also discussed another machine learning based POS tagger using SVM algorithm in [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "in [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "[12] has proposed a Layered Parts of Speech Tagging for Bangla.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "A detailed survey on POS tagging for other Indian languages has been presented in [13][14].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "A detailed survey on POS tagging for other Indian languages has been presented in [13][14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "A POS Tagging System of English-Hindi Code-Mixed Social Media Content has been presented in [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "A POS tagging system for Indian Social Media Text on Twitter has been presented in [16].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "In general, HMM based POS tagging use words in a sentence as an observation sequence [1] [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "In general, HMM based POS tagging use words in a sentence as an observation sequence [1] [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "Considering a special tag tn+1 to indicate the end sentence boundary and two special tags t-1 and t0 at the starting boundary of the sentence and adding these three special tags to the tag set [4], gives the following equation for POS tagging:", "startOffset": 193, "endOffset": 196}, {"referenceID": 16, "context": "To overcome the data sparseness problem, tag trigram probability is smoothed using deleted interpolation technique [17][4] which uses the maximum likelihood estimates from counts for tag trigram, tag bigram and tag unigram.", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "To overcome the data sparseness problem, tag trigram probability is smoothed using deleted interpolation technique [17][4] which uses the maximum likelihood estimates from counts for tag trigram, tag bigram and tag unigram.", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "The observation probability of a observed triplet <word, meta-tag, L-tag >, which is the observed symbol in our case, is computed using the following equation [1][17].", "startOffset": 159, "endOffset": 162}, {"referenceID": 16, "context": "The observation probability of a observed triplet <word, meta-tag, L-tag >, which is the observed symbol in our case, is computed using the following equation [1][17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "The Viterbi algorithm is a standard application of the classic dynamic programming algorithm [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "We find the observation probabilities of such unknown pseudo words using suffix analysis [17][4].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "We find the observation probabilities of such unknown pseudo words using suffix analysis [17][4].", "startOffset": 93, "endOffset": 96}], "year": 2015, "abstractText": "This paper discusses the experiments carried out by us at Jadavpur University as part of the participation in ICON 2015 task: POS Tagging for Code-mixed Indian Social Media Text. The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information from dictionary as well as some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for Bengali-English, Hindi-English and Tamil-English Language pairs. Our system has been trained and tested on the datasets released for ICON 2015 shared task: POS Tagging For Code-mixed Indian Social Media Text. In constrained mode, our system obtains average overall accuracy (averaged over all three language pairs) of 75.60% which is very close to other participating two systems (76.79% for IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In unconstrained mode, our system obtains average overall accuracy of 70.65% which is also close to the system (72.85% for AMRITA_CEN) which obtains the highest average overall accuracy.", "creator": "PScript5.dll Version 5.2.2"}}}