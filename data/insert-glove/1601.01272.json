{"id": "1601.01272", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Recurrent Memory Networks for Language Modeling", "abstract": "Recurrent Neural sauce Networks (pargeter RNN) landberg have ophichthidae obtained 2076 excellent rizvi result plotline in trenti many natural language funciona processing (NLP) tasks. antia However, redwine understanding and interpreting the tigrina source of mcdougall this kenly success remains a faryd challenge. abysses In this paper, we tannu propose matambanadzo Recurrent t\u00e9l\u00e9toon Memory barq Network (RMN ), 1907-08 a novel RNN macroalgae architecture, that 2,334 not only dogmatic amplifies the 51.7 power 111.24 of peladon RNN but hbc also 95-1 facilitates canticles our understanding judio of 1,188 its euro227 internal .105 functioning subraces and allows lacalle us to homewrecker discover 1263 underlying patterns in scooters data. We catliff demonstrate the power archaeopteryx of RMN on kruja language gauze modeling hypermedia and sentence jankelowitz completion tasks. On language ranucci modeling, RMN outperforms rtl Long 57.45 Short - interamerican Term Memory (40.72 LSTM) duggal network tirelli on tint three large limbaugh German, Italian, and dahk English .92 dataset. irradiance Additionally palaeontology we sadad perform fabbrica in - loganair depth stoics analysis of burillo various 34,000 linguistic parkroyal dimensions that RMN lieben captures. nikou On trevorrow Sentence Completion mce Challenge, oblanceolate for berlage which it is essential cosmopulos to 386th capture khutba sentence ver\u00f3nica coherence, our madcon RMN 19d obtains wagram 69. 2% accuracy, herbstreit surpassing romar the previous gagetown state - tokarev of - pterodactyls the - bungled art by praja a paintball large margin.", "histories": [["v1", "Wed, 6 Jan 2016 18:44:07 GMT  (662kb,D)", "http://arxiv.org/abs/1601.01272v1", "8 pages, 6 figures"], ["v2", "Fri, 22 Apr 2016 11:13:11 GMT  (1036kb,D)", "http://arxiv.org/abs/1601.01272v2", "8 pages, 6 figures. Accepted at NAACL 2016"]], "COMMENTS": "8 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ke m tran", "arianna bisazza", "christof monz"], "accepted": true, "id": "1601.01272"}, "pdf": {"name": "1601.01272.pdf", "metadata": {"source": "CRF", "title": "Recurrent Memory Network for Language Modeling", "authors": ["Ke Tran", "Arianna Bisazza", "Christof Monz"], "emails": ["m.k.tran@uva.nl", "a.bisazza@uva.nl", "c.monz@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "Recurrent Neural Networks (RNNs) (Elman, 1990; Mikolov et al., 2010) are remarkably powerful models for sequential data. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), a specific architecture of RNNs, has a track record of success in many natural language processing tasks such as language modeling (Jo\u0301zefowicz et al., 2015), dependency parsing (Dyer et al., 2015), sentence compression (Filippova et al., 2015), and machine translation (Sutskever et al., 2014).\nWithin the context of natural language processing, a common assumption is that LSTM is able to capture certain linguistic phenomena. Evidence supporting this assumption mainly comes from evaluating LSTMs in downstream applications: Bowman et al. (2015) carefully design two artificial datasets where sentences have explicit recursive structures. They show empirically that while processing the input linearly, LSTMs can implicitly exploit recursive structures of languages. Filippova et al. (2015) find that using explicit syntactic features within LSTMs in their sentence compression model hurts the performance of overall system. They then hypothesize that a basic LSTM is powerful enough to capture syntactic aspects which are useful for compression.\nTo understand and explain which linguistic dimensions are captured by an LSTM is non-trivial. This is due to the fact that the sequences of input histories are compressed into several dense vectors by LSTM\u2019s components whose purposes with respect to representing linguistic information is not evident. To our knowledge, the only attempt to better understand the reasons of an LSTM\u2019s performance and limitations is the work of Karpathy et al. (2015) by means of visualization experiments and cell activation statistics in the context of character-level language modeling.\nOur work is motivated by the difficulty in understanding and interpreting existing RNN architectures from a linguistic point of view. We propose Recurrent Memory Network (RMN), a novel RNN architecture that combines the strengths of both LSTM and Memory Network (Sukhbaatar et al., 2015). In RMN, the Memory Block component\u2014a variant of\nar X\niv :1\n60 1.\n01 27\n2v 1\n[ cs\n.C L\n] 6\nJ an\n2 01\nMemory Network\u2014accesses the most recent input words and selectively attends to words that are relevant for predicting the next word given the current LSTM state. By looking at the attention distribution over history words, our RMN allows us not only to interpret the results but also to discover underlying dependencies present in the data.\nIn this paper, we make the following contributions:\n1. We propose a novel RNN architecture that complements LSTMs in language modeling. We demonstrate that our RMN outperforms competitive LSTM baselines in terms of perplexity on three large German, Italian, and English datasets.\n2. We perform an analysis along various linguistic dimensions that our model captures. This is possible only because the Memory Block allows us to look into its internal states and its explicit use of additional inputs at each time step.\n3. We show that, with a simple modification, our RMN can be successfully applied to NLP tasks other than language modeling. On the Sentence Completion Challenge (Zweig and Burges, 2012), our model achieves an impressive 69.2% accuracy, surpassing the previous state-of-the-art 58.9% by a large margin."}, {"heading": "2 Recurrent Neural Networks", "text": "Recurrent Neural Networks (RNNs) have shown impressive performances on many sequential modeling tasks due to their ability to encode unbounded input histories. However, training simple RNNs is difficult because of the vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013). A simple and effective solution for exploding gradients is gradient clipping proposed by Pascanu et al. (2013). To deal with the more challenging problem of vanishing gradients, several variants of RNNs have been proposed. Among them, Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (Cho et al., 2014) are widely regarded as the most successful variants. In this work, we focus on LSTMs because they have been shown to outperform GRUs on language modeling tasks (Jo\u0301zefowicz et al., 2015). In the follow-\ning, we will detail the LSTM architecture used in this work. Long Short-Term Memory Notation: Throughout this paper, we denote matrices, vectors, and scalars using bold uppercase (e. g., W), bold lowercase (e. g., b) and lowercase (e. g., \u03b1) letters, respectively.\nThe LSTM used in this work is specified as follows:\nit = sigm(Wxixt +Whiht\u22121 + bi)\njt = sigm(Wxjxt +Whjht\u22121 + bj) f t = sigm(Wxfxt +Whfht\u22121 + bf )\not = tanh(Wxoxt +Whoht\u22121 + bo)\nct = ct\u22121 f t + it jt ht = tanh(ct) ot\nwhere xt is the input vector at time step t, ht\u22121 is the LSTM hidden state at the previous time step, W\u2217 and b\u2217 are weights and biases. The symbol denotes Hadamard product or element-wise multiplication.\nDespite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; Jo\u0301zefowicz et al., 2015; Karpathy et al., 2015). There have been few recent attempts to understand the components of an LSTM from an empirical point of view: Greff et al. (2015) carry out a large-scale experiment of eight LSTM variants. The results from their 5,400 experimental runs suggest that forget gates and output gates are the most critical components of LSTMs. Jo\u0301zefowicz et al. (2015) conduct and evaluate over ten thousand RNN architectures and find that the initialization of the forget gate bias is crucial to the LSTM\u2019s performance. While these findings are important to help choosing appropriate LSTM architectures, they do not shed light on what information the hidden states of an LSTM capture.\nBowman et al. (2015) show that a vanilla LSTM, such as described above, performs reasonably well compared to a recursive neural network (Socher et al., 2011) that explicitly exploits tree structures on two artificial datasets. They find that LSTMs can effectively exploit recursive structure in the artificial\ndatasets. In contrast to these simple datasets containing a few logical operations in their experiments, languages exhibit highly complex patterns. The extent to which linguistic assumptions about syntactic structures and compositional semantics are reflected in LSTMs is rather poorly understood. Thus it is desirable to have a more principled mechanism allowing us to inspect recurrent architectures from a linguistic perspective. In the following section, we propose such a mechanism."}, {"heading": "3 Recurrent Memory Network", "text": "It has been demonstrated that RNNs can retain input information over a long period. However, existing RNN architectures make it difficult to analyze what information is exactly retained at their hidden states at each time step, especially when the data has complex underlying structures, which is common in natural language. Motivated by this difficulty, we propose a novel RNN architecture called Recurrent Memory Network (RMN). On linguistic data, the RMN allows us not only to qualify which linguistic information is preserved over time and why this is the case but also to discover dependencies within the data (\u00a75). Our RMN consists of two components: an LSTM and a Memory Block (MB) (\u00a73.1). The MB takes the hidden state of the LSTM and compares it to the most recent inputs using an attention mechanism (Gregor et al., 2015; Bahdanau et al., 2014; Graves et al., 2014). Thus, analyzing the attention weights of a trained model can give us valuable insight into the information that is retained over time in the LSTM.\nIn the following, we describe in detail the MB architecture and the combination of the MB and the LSTM to form an RMN."}, {"heading": "3.1 Memory Block", "text": "The Memory Block (Figure 1) is a variant of Memory Network (Sukhbaatar et al., 2015) with one hop (or a single-layer Memory Network). At time step t, the MB receives two inputs: the hidden state ht of the LSTM and a set {xi} of n most recent words including the current word xt. We refer to n as the memory size. Internally, the MB consists of two lookup tables M and C of size |V | \u00d7 d, where |V | is the size of the vocabulary. With a slight\nabuse of notation we denote Mi = M({xi}) and Ci = C({xi}) as n \u00d7 d matrices where each row corresponds to an input memory embedding mi and an output memory embedding ci of each element of the set {xi}. We use the matrix Mi to compute an attention distribution over the set {xi}:\npt = softmax(Miht) (1)\nWhen dealing with data that exhibits a strong temporal relationship, such as natural language, an additional temporal matrix Ti \u2208 Rn\u00d7d can be used to bias attention with respect to the position of the data points. In this case, equation 1 becomes\npt = softmax ( (Mi +Ti)ht ) (2)\nWe then use the attention distribution pt to compute a context vector representation of {xi}\nst = C T i pt (3)\nFinally, we combine the context vector st and the hidden state ht by a function g(\u00b7) to obtain the output hmt of the MB. Instead of using a simple addition function g(st,ht) = st + ht as in Sukhbaatar et al. (2015), we propose to use a gating unit that decides how much it should trust the hidden state ht and context st at time step t. Our gating unit is a form of Gated Recurrent Unit (Cho et al., 2014; Chung et al., 2014)\nzt = sigm(Wszst +Uhzht) (4)\nrt = sigm(Wsrst +Uhrht) (5)\nh\u0303t = tanh(Wst +U(rt ht)) (6) hmt = (1\u2212 zt) ht + zt h\u0303t (7)\nwhere zt is an update gate, rt is a reset gate.\nThe choice of the composition function g(\u00b7) is crucial for the MB especially when one of its input comes from the LSTM. The simple addition function might overwrite the information within the LSTM\u2019s hidden state and therefore prevent the MB from keeping track of information in the distant past. The gating function, on the other hand, can control the degree of information that flows from the LSTM to the MB\u2019s output."}, {"heading": "3.2 RMN Architectures", "text": "As explained above, our proposed MB receives the hidden state of the LSTM as one of its input. This leads to an intuitive combination of the two units by stacking the MB on top of the LSTM. We call this architecture Recurrent-Memory (RM). The RM architecture, however, does not allow interaction between Memory Blocks at different time steps. To enable this interaction we can stack one more LSTM layer on top of the RM. We call this architecture Recurrent-Memory-Recurrent (RMR)."}, {"heading": "4 Language Model Experiments", "text": "Language models play a crucial role in many NLP applications such as machine translation and speech recognition. Language modeling also serves as a standard test bed for newly proposed models (Sukhbaatar et al., 2015; Kalchbrenner et al., 2015). We conjecture that, by explicitly accessing history words, RMN will offer better predictive power than the existing recurrent architectures. We therefore\nevaluate our RMN architectures against state-of-theart LSTM in term of perplexity."}, {"heading": "4.1 Data", "text": "We evaluate our models on three languages: English, German, and Italian. We are especially interested in German and Italian because of their larger vocabularies and complex agreement patterns. Table 1 summarizes the data used in our experiments.\nThe training data correspond to approximately 1M sentences in each language. For English, we use all the News Commentary data (8M tokens) and 18M tokens from News Crawl 2014 for training. Development and test data are randomly drawn from the concatenation of the WMT 2009-2014 test sets (Bojar et al., 2015). For German, we use the first 6M tokens from the News Commentary data and 16M tokens from News Crawl 2014 for training. For development and test data we use the remaining part of the News Commentary data concatenated with the WMT 2009-2014 test sets (Bojar et al., 2015). Finally, for Italian, we use a selection of 29M tokens from the PAISA\u0300 corpus (Lyding et al., 2014), mainly including Wikipedia pages and, to a minor extent, Wikibooks and Wikinews documents. For development and test we randomly draw documents from the same corpus."}, {"heading": "4.2 Setup", "text": "Our baselines are a 5-gram language model with Kneser-Ney smoothing, a Memory Network (MemN), a vanilla single-layer LSTM, and two stacked LSTMs with two and three layers respectively. N-gram models have been used intensively in many applications for their excellent performance and fast training. Chen et al. (2015) show that n-gram model outperforms a popular feed-forward language model (Bengio et al., 2003) on a one billion word benchmark (Chelba et al., 2013). While\ntaking longer time to train, RNN has been proven superior to n-gram model.\nWe compare these baselines with our two model architectures: RMR and RM. For each of our models, we experiment with two settings: with or without temporal matrix (+tM or \u2013tM), and linear vs. gating composition function. In total, we experiment with eight RMN variants.\nFor all neural network models, we set the dimension of word embeddings, the LSTM hidden states, its gates, the memory input, and output embeddings to 128. The memory size is set to 15. The bias of the LSTM\u2019s forget gate is initialized to 1 (Jo\u0301zefowicz et al., 2015) while all other parameters are initialized uniformly in (\u22120.05, 0.05). The initial learning rate is set to 1 and is halved at each epoch after the forth. All models are trained for 15 epochs with standard stochastic gradient descent (SGD). During training, we rescale the gradients whenever their norm is greater than 5 (Pascanu et al., 2013).\nSentences with the same length are grouped into buckets. Then, mini-batches of 20 sentences are drawn from each bucket. We do not use truncated back-propagation through time, instead gradients are fully back-propagated from the end of each sentence to its beginning. When feeding in a new mini-batch, the hidden states of LSTMs are reset to zeros, which ensures that the data is properly modeled at the sentence level. For our RMN models, instead of using padding, at time step t < n, we use a slice Ti = T[1 : t] \u2208 Rt\u00d7d of the temporal matrix T \u2208 Rn\u00d7d. When t \u2265 n, we set Ti = T."}, {"heading": "4.3 Results", "text": "Perplexities on the test data are given in Table 2. The best results overall are obtained by RM with gating composition. In general, all our RMN models outperform all the competitive baselines. The results also reflect that gating composition is essential to our models.\nOur results agree with the hypothesis of mitigating prediction error by explicitly using the last n words in RNNs (Karpathy et al., 2015). We further observe that using temporal matrix only benefits RM architecture. This can be explained by seeing RM as a principled way to interpolate an LSTM and neural a n-gram model. By contrast, RMR works better without temporal matrix but its overall performance\nis not as good as RM. This suggests that we need a better mechanism to address the interaction between MBs, which we leave to future work."}, {"heading": "5 Attention Analysis", "text": "The goal of our RMN design is twofold: (1) to obtain better predictive power and (2) to facilitate understanding of the model and discovering patterns in data. In section 4, we have validated the predictive power of the RMN, now we investigate the source of this performance based on linguistic assumptions of word co-occurrences and dependency structures."}, {"heading": "5.1 Positional and lexical analysis", "text": "As a first step towards understanding RMN, we look at the average attention weights of each history word position in the MB (Figure 3). One can see that the attention mass tends to concentrate at the rightmost position (the current word) and decreases when moving further to the left (less recent words). This is not surprising since the success of n-gram language model has demonstrated that the most recent words provide important information for predicting the next word. Between the two variants, the RMR average attention mass is more concentrated to the right. This can be explained by the absence of an\nLSTM layer on top, meaning that the MB in the RM architecture has to pay more attention to the more distant words in the past.\nBeyond average attention weights, we are interested in those cases where attention focuses on distant positions. To this end, we randomly sample 100 words from test data and visualize attention distributions over the last 15 words. Figure 4 shows the attention distributions for random samples of German and Italian. Again, in many cases attention weights concentrate around the last word (bottom row), however we observe that many long distance words also receive noticeable attention mass. Interestingly, for many predicted words, attention is distributed evenly over memory positions, possibly indicating cases where the LSTM state contained enough information to predict the next word.\nTo explain the long-distance dependencies, we first hypothesize that our RMN captures some sort of co-occurrence frequency. We run our RM(+tMg) model on German development and test sentences and select those pairs of (predicted word, most attended word) where the MB\u2019s attention concentrates on a word more than six positions to the left. Then we compute the mean of co-occurrence frequencies seen in training data per memory location (Table 3).\nThis shows that our RMN learns co-occurrence implicitly. Moreover, we find that there is no correlation between frequency and memory location. Previous work (Hermans and Schrauwen, 2013; Karpathy et al., 2015) studied this property of LSTM by analyzing simple cases of closing parenthesis and brace. By contrast RMN allows us to discover more interesting dependencies in the data.\nWe manually inspect those high-frequency pairs to find out whether they display certain linguistic phenomena. We find that our RMN discovers, for example, separable verbs and fixed expressions in German. Separable verbs are a frequent phenomenon in German. They typically consist of preposition+verb constructions, such ab+ha\u0308ngen (\u2018to depend\u2019) or aus+schlie\u00dfen (\u2018to exclude\u2019), and can be spelled together (abha\u0308ngen) or apart as in \u2018ha\u0308ngen von der Situation ab\u2019 (\u2018depend on the situation\u2019), depending on the grammatical construction. Figure 5a shows a long-dependency example for the separable verb abha\u0308ngen (to depend). When predicting the verb\u2019s particle ab, the model correctly attends to the verb\u2019s core ha\u0308ngt occurring seven words to the left. Figure 5b and 5c show fixed expression examples from German and Italian, respectively: schlu\u0308sselrolle ... spielen (play a key role) and insignito ... titolo (awarded title). Here too the model correctly attends to the key word in the history despite the long distance from the predicted word. Other interesting examples found by RM in the test data include:\nGerman: findet statt (takes place), kehrte zuru\u0308ck (came back), fragen antworten (questions answers), ka\u0308mpfen gegen (fight against), bleibt erhalten (remains intact), verantwortung u\u0308bernimmt (takes responsibility);\nItalian: sinistra destra (left right), latitudine longitudine (latitude longitude), collegata tramite (connected through), sposo\u0300 figli (got-married children), insignito titolo (awarded title)."}, {"heading": "5.2 Syntactic analysis", "text": "It has been conjectured that RNN, and LSTM in particular, models text so well because it captures syntactic structure implicitly. Unfortunately this has been hard to prove, but with our RMN model we can get closer to answering this important question.\nWe produce dependency parses for our test sets using (Sennrich et al., 2013) for German and (Attardi et al., 2009) for Italian. Then we look at how much attention mass is concentrated on different dependency types. Figure 6 shows, for each language, a selection of ten dependency types that are often long-distance.1 Order is marked with an arrow: e.g. \u2192mod means that the predicted word is a modifier of the attended word, while mod\u2190 means that the attended word is a modifier of the predicted word.2\nWhile in most of the cases closest positions are the most attended, we can see that some dependency types also receive noticeable, if not uniform, attention on the long-distance positions. In German, this is mostly visible for the head of separable verb particles (\u2192avz), which nicely supports our observations in the lexical analysis (\u00a75.1). Other attended dependencies include: auxiliary verbs (\u2192aux) when predicting the second element of a complex tense (hat . . . gesagt / has said); subordinating conjunctions (konj\u2190) when predicting the clause-final inflected verb (dass sie sagen sollten / that they should say); control verbs (\u2192obji) when predicting the infinitive verb (versucht ihr zu helfen / tries to help her). Out of the Italian dependency types selected for their frequent long-distance occurrences (bot-\n1The full plots are available at http://anonymized. The German and Italian dependency tag sets are explained in (Simi et al., 2014) and (Foth, 2006) respectively.\n2Some dependency directions, like obj\u2190 in Italian, are almost never observed due to order constraints of the language.\ntom of Figure 6), the most attended are arguments (arg), complements (comp), objects (obj) and subjects (subj). This suggests that RMN is mainly capturing predicate argument structure in Italian. Notice that syntactic annotation is never used to train the model, but only to analyze its predictions.\nWe can also use RMN to discover which complex dependency paths are important for word prediction. To mention just a few examples, high attention on the German path [subj\u2190,\u2192kon,\u2192cj] indicates that the model captures morphological agreement between coordinate clauses in non-trivial constructions of the kind: spielen die Kinder im Garten und singen / the children play in the garden and sing. In Italian, high attention on the path [\u2192obj,\u2192comp,\u2192prep]\ndenotes cases where the semantic relatedness between a verb and its object does not stop at the object\u2019s head, but percolates down to a prepositional phrase attached to it (passo\u0300 buona parte della sua vita / spent a large part of his life). Interestingly, both local n-gram context and local dependency context would have missed these relations.\nWhile much remains to be explored, our analysis shows that RMN discovers patterns far more complex than pairs of opening and closing parentheses, and suggests that the network\u2019s hidden state captures to a large extent the underlying structure of text."}, {"heading": "6 Sentence Completion Challenge", "text": "Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2012) has recently become a test bed for advancing statistical language modeling. We choose this task to demonstrate the effectiveness of our RMN in capturing sentence coherence. The test set consists of 1,040 sentences selected from five Sherlock Holmes novels by Conan Doyle. For each sentence, a content word is removed and the task is to identify the correct missing word among five given candidates. The task is carefully designed to be non-solvable for local language models such as n-gram. The best reported result is 58.9% accuracy (Mikolov et al., 2013)3 which is far from of the human performance of 91% (Zweig and Burges, 2012).\nOur baseline is a stacked three-layer LSTM. Our models are two variants of RM(+tM-g), each consisting of three LSTM layers followed by a MB. The first variant (unidirectional-RM) uses n words preceding the predicted word, the second (bidirectionalRM) uses the n words preceding and the n words following the predicted word as MB input. We include bidirectional-RM in our experiments to show the flexibility of utilizing future context in RMN.\nWe train all models on the standard training data of the challenge, which consists of 522 novels from Project Gutenberg, preprocessed similarly to Mnih and Kavukcuoglu (2013). After sentence splitting, tokenization and lowercasing, we randomly select 19,000 sentences for validation. Training and validation sets include 47M and 190K tokens respectively. The vocabulary size is about 64,000.\n3The authors use a weighted combination of skip-ngram and RNN without giving any technical details.\nWe initialize and train all the networks as described in \u00a7 4.2. Moreover, for regularization, we place dropout (Srivastava et al., 2014) after each LSTM layer as suggested in (Pham et al., 2014). The dropout rate is set to 0.3 in all the experiments.\nTable 4 summarizes the results. It is worth to mention that our LSTM baseline performs better than the second best published result (55.5%) of an inverse log-bilinear model (Mnih and Kavukcuoglu, 2013). The unidirectional-RM sets a new state of the art for the Sentence Completion Challenge with 69.2% accuracy. Under the same setting of d we observe that using bidirectional context does not bring additional advantage to the model. Mnih and Kavukcuoglu (2013) also report a similar observation. We believe that RMN can achieve further improvements with hyperparameter optimization."}, {"heading": "7 Conclusion", "text": "We have proposed Recurrent Memory Network (RMN), a novel recurrent architecture for language modeling. RMN outperforms LSTM in terms of perplexity on three large dataset and allows us to analyze its behavior from a linguistic perspective. We find that RMN learns important co-occurrences regardless of their distance. Even more interestingly, RMN implicitly captures certain dependency types that are important for word prediction, despite being trained without any syntactic information. Finally RMN obtains excellent performance at modeling sentence coherence, setting a new state-of-theart on a challenging sentence completion task."}], "references": [{"title": "Accurate dependency parsing with a stacked multilayer perceptron. In Proceedings of Evalita\u201909, Evaluation of NLP and Speech Tools for Italian, Reggio Emilia, Italy", "author": ["Felice Dell\u2019Orletta", "Maria Simi", "Joseph Turian"], "venue": null, "citeRegEx": "Attardi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2009}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Transaction on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Treestructured composition in neural networks without tree-structured architectures", "author": ["Christopher D. Manning", "Christopher Potts"], "venue": "In Proceedings of Proceedings of the NIPS 2015 Workshop on Cognitive", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Strategies for Training Large Vocabulary Neural Language Models", "author": ["Chen et al.2015] Welin Chen", "David Grangier", "Michael Auli"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013 decoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proceedings of SSST-8,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Eine umfassende Constraint-Dependenz-Grammatik des Deutschen. Fachbereich Informatik", "author": ["Kilian A. Foth"], "venue": null, "citeRegEx": "Foth.,? \\Q2006\\E", "shortCiteRegEx": "Foth.", "year": 2006}, {"title": "LSTM: A search space odyssey", "author": ["Greff et al.2015] Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Schrauwen2013] Michiel Hermans", "Benjamin Schrauwen"], "venue": null, "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks. CoRR, abs/1506.02078", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "The pais\u00c0 corpus of italian web texts", "author": ["Lyding et al.2014] Verena Lyding", "Egon Stemle", "Claudia Borghetti", "Marco Brunello", "Sara Castagnoli", "Felice Dell\u2019Orletta", "Henrik Dittmann", "Alessandro Lenci", "Vito Pirrelli"], "venue": "In Proceedings of the 9th Web as Corpus", "citeRegEx": "Lyding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lyding et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "JMLR Proceedings,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] Vu Pham", "Christopher Bluche", "Th\u00e9odore Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In International Conference on Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Exploiting synergies between open resources for german dependency parsing, pos-tagging, and morphological analysis", "author": ["Martin Volk", "Gerold Schneider"], "venue": "In Recent Advances in Natural Language Processing (RANLP", "citeRegEx": "Sennrich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2013}, {"title": "Less is more? towards a reduced inventory of categories for training a parser for the italian stanford dependencies", "author": ["Simi et al.2014] Maria Simi", "Cristina Bosco", "Simonetta Montemagni"], "venue": null, "citeRegEx": "Simi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simi et al\\.", "year": 2014}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the Conference on Empirical Meth-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-toend memory networks", "author": ["Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "Advances in Neural Information Processing", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A challenge set for advancing language modeling", "author": ["Zweig", "Burges2012] Geoffrey Zweig", "Chris J.C. Burges"], "venue": "In Proceedings of the NAACL-HLT", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "Recurrent Neural Networks (RNNs) (Elman, 1990; Mikolov et al., 2010) are remarkably powerful models for sequential data.", "startOffset": 33, "endOffset": 68}, {"referenceID": 19, "context": "Recurrent Neural Networks (RNNs) (Elman, 1990; Mikolov et al., 2010) are remarkably powerful models for sequential data.", "startOffset": 33, "endOffset": 68}, {"referenceID": 16, "context": "Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), a specific architecture of RNNs, has a track record of success in many natural language processing tasks such as language modeling (J\u00f3zefowicz et al., 2015), dependency parsing (Dyer et al.", "startOffset": 197, "endOffset": 222}, {"referenceID": 8, "context": ", 2015), dependency parsing (Dyer et al., 2015), sentence compression (Filippova et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": ", 2015), sentence compression (Filippova et al., 2015), and machine translation (Sutskever et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 29, "context": ", 2015), and machine translation (Sutskever et al., 2014).", "startOffset": 33, "endOffset": 57}, {"referenceID": 4, "context": "Evidence supporting this assumption mainly comes from evaluating LSTMs in downstream applications: Bowman et al. (2015) carefully design two artificial datasets where sentences have explicit recursive structures.", "startOffset": 99, "endOffset": 120}, {"referenceID": 4, "context": "Evidence supporting this assumption mainly comes from evaluating LSTMs in downstream applications: Bowman et al. (2015) carefully design two artificial datasets where sentences have explicit recursive structures. They show empirically that while processing the input linearly, LSTMs can implicitly exploit recursive structures of languages. Filippova et al. (2015) find that using explicit syntactic features within LSTMs in their sentence compression model hurts the performance of overall system.", "startOffset": 99, "endOffset": 365}, {"referenceID": 17, "context": "To our knowledge, the only attempt to better understand the reasons of an LSTM\u2019s performance and limitations is the work of Karpathy et al. (2015) by means of visualization experiments and cell activation statistics in the context of character-level language modeling.", "startOffset": 124, "endOffset": 147}, {"referenceID": 28, "context": "We propose Recurrent Memory Network (RMN), a novel RNN architecture that combines the strengths of both LSTM and Memory Network (Sukhbaatar et al., 2015).", "startOffset": 128, "endOffset": 153}, {"referenceID": 2, "context": "However, training simple RNNs is difficult because of the vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 100, "endOffset": 143}, {"referenceID": 22, "context": "However, training simple RNNs is difficult because of the vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 100, "endOffset": 143}, {"referenceID": 7, "context": "Among them, Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (Cho et al., 2014) are widely regarded as the most successful variants.", "startOffset": 95, "endOffset": 113}, {"referenceID": 16, "context": "In this work, we focus on LSTMs because they have been shown to outperform GRUs on language modeling tasks (J\u00f3zefowicz et al., 2015).", "startOffset": 107, "endOffset": 132}, {"referenceID": 2, "context": "However, training simple RNNs is difficult because of the vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013). A simple and effective solution for exploding gradients is gradient clipping proposed by Pascanu et al. (2013). To deal with the more challenging problem of vanishing gradients, several variants of RNNs have been proposed.", "startOffset": 101, "endOffset": 256}, {"referenceID": 12, "context": "Despite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 16, "context": "Despite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 17, "context": "Despite the popularity of LSTM in sequential modeling, its design is not straightforward to justify and understanding why it works remains a challenge (Hermans and Schrauwen, 2013; Chung et al., 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 12, "context": ", 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015). There have been few recent attempts to understand the components of an LSTM from an empirical point of view: Greff et al. (2015) carry out a large-scale experiment of eight LSTM variants.", "startOffset": 8, "endOffset": 206}, {"referenceID": 12, "context": ", 2014; Greff et al., 2015; J\u00f3zefowicz et al., 2015; Karpathy et al., 2015). There have been few recent attempts to understand the components of an LSTM from an empirical point of view: Greff et al. (2015) carry out a large-scale experiment of eight LSTM variants. The results from their 5,400 experimental runs suggest that forget gates and output gates are the most critical components of LSTMs. J\u00f3zefowicz et al. (2015) conduct and evaluate over ten thousand RNN architectures and find that the initialization of the forget gate bias is crucial to the LSTM\u2019s performance.", "startOffset": 8, "endOffset": 423}, {"referenceID": 26, "context": "(2015) show that a vanilla LSTM, such as described above, performs reasonably well compared to a recursive neural network (Socher et al., 2011) that explicitly exploits tree structures on two artificial datasets.", "startOffset": 122, "endOffset": 143}, {"referenceID": 13, "context": "The MB takes the hidden state of the LSTM and compares it to the most recent inputs using an attention mechanism (Gregor et al., 2015; Bahdanau et al., 2014; Graves et al., 2014).", "startOffset": 113, "endOffset": 178}, {"referenceID": 1, "context": "The MB takes the hidden state of the LSTM and compares it to the most recent inputs using an attention mechanism (Gregor et al., 2015; Bahdanau et al., 2014; Graves et al., 2014).", "startOffset": 113, "endOffset": 178}, {"referenceID": 28, "context": "The Memory Block (Figure 1) is a variant of Memory Network (Sukhbaatar et al., 2015) with one hop (or a single-layer Memory Network).", "startOffset": 59, "endOffset": 84}, {"referenceID": 7, "context": "Our gating unit is a form of Gated Recurrent Unit (Cho et al., 2014; Chung et al., 2014)", "startOffset": 50, "endOffset": 88}, {"referenceID": 27, "context": "Instead of using a simple addition function g(st,ht) = st + ht as in Sukhbaatar et al. (2015), we propose to use a gating unit that decides how much it should trust the hidden state ht and context st at time step t.", "startOffset": 69, "endOffset": 94}, {"referenceID": 28, "context": "Language modeling also serves as a standard test bed for newly proposed models (Sukhbaatar et al., 2015; Kalchbrenner et al., 2015).", "startOffset": 79, "endOffset": 131}, {"referenceID": 18, "context": "Finally, for Italian, we use a selection of 29M tokens from the PAIS\u00c0 corpus (Lyding et al., 2014), mainly including Wikipedia pages and, to a minor extent, Wikibooks and Wikinews documents.", "startOffset": 77, "endOffset": 98}, {"referenceID": 3, "context": "(2015) show that n-gram model outperforms a popular feed-forward language model (Bengio et al., 2003) on a one billion word benchmark (Chelba et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 5, "context": ", 2003) on a one billion word benchmark (Chelba et al., 2013).", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "Chen et al. (2015) show that n-gram model outperforms a popular feed-forward language model (Bengio et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "The bias of the LSTM\u2019s forget gate is initialized to 1 (J\u00f3zefowicz et al., 2015) while all other parameters are initialized uniformly in (\u22120.", "startOffset": 55, "endOffset": 80}, {"referenceID": 22, "context": "During training, we rescale the gradients whenever their norm is greater than 5 (Pascanu et al., 2013).", "startOffset": 80, "endOffset": 102}, {"referenceID": 17, "context": "Our results agree with the hypothesis of mitigating prediction error by explicitly using the last n words in RNNs (Karpathy et al., 2015).", "startOffset": 114, "endOffset": 137}, {"referenceID": 17, "context": "Previous work (Hermans and Schrauwen, 2013; Karpathy et al., 2015) studied this property of LSTM by analyzing simple cases of closing parenthesis and brace.", "startOffset": 14, "endOffset": 66}, {"referenceID": 24, "context": "We produce dependency parses for our test sets using (Sennrich et al., 2013) for German and (Attardi et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": ", 2013) for German and (Attardi et al., 2009) for Italian.", "startOffset": 23, "endOffset": 45}, {"referenceID": 25, "context": "The German and Italian dependency tag sets are explained in (Simi et al., 2014) and (Foth, 2006) respectively.", "startOffset": 60, "endOffset": 79}, {"referenceID": 11, "context": ", 2014) and (Foth, 2006) respectively.", "startOffset": 12, "endOffset": 24}, {"referenceID": 20, "context": "9% accuracy (Mikolov et al., 2013)3 which is far from of the human performance of 91% (Zweig and Burges, 2012).", "startOffset": 12, "endOffset": 34}, {"referenceID": 27, "context": "Moreover, for regularization, we place dropout (Srivastava et al., 2014) after each LSTM layer as suggested in (Pham et al.", "startOffset": 47, "endOffset": 72}, {"referenceID": 23, "context": ", 2014) after each LSTM layer as suggested in (Pham et al., 2014).", "startOffset": 46, "endOffset": 65}], "year": 2016, "abstractText": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform indepth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous stateof-the-art by a large margin.", "creator": "LaTeX with hyperref package"}}}