{"id": "1601.02745", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Basic Reasoning with Tensor Product Representations", "abstract": "mike In degetau this paper eastick we pienaar present trix the initial nemchinov development of a general near-sighted theory seljuqs for gr\u00e4tz mapping levien inference picture in paroli predicate logic i.b.m. to computation swedes over pallida Tensor fissures Product schneidem\u00fchl Representations (TPRs; Smolensky (magical 1990 ), Smolensky & amp; Legendre (2006) ). nextdoor After variglog an cadamosto initial erupting brief lectionary synopsis atha of TPRs (Section fastow 0 ), we begin morgan with pythian particular nikole examples of post-breakup inference church-owned with TPRs euro960 in ruddock the ' belkhadem bAbI ' question - begusarai answering task shirtdresses of Weston marduk et al. (2015) (jarai Section kotsu 1 ). half-circle We kitbuqa then present 4-86 a wortmann simplification 1,543 of the general analysis thanyarat that meger suffices monuments for the bmnh bAbI task (sorlie Section battlelines 2 ). mahendravarman Finally, two-second we 4,310 lay hwarang out souffles the 6.01 general tns treatment of topcider inference pegnitz over TPRs (kochar Section m-29 3 ). 52.07 We also toowong show 34.65 the simplification unalaska in 105.63 Section 2 maccabean derives astrocaryum the revenaugh inference szeliga methods ieyoub described cadwgan in gambit Lee candeloro et al. (2016 ); \u2019re this abdulkassim shows how the cigaritis simple methods yafhi of 65.66 Lee hubbell et al. (schmetzer 2016) can be formally honasan extended selph to manawi more general reasoning hemisphere tasks.", "histories": [["v1", "Tue, 12 Jan 2016 06:44:54 GMT  (654kb)", "http://arxiv.org/abs/1601.02745v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["paul smolensky", "moontae lee", "xiaodong he", "wen-tau yih", "jianfeng gao", "li deng"], "accepted": false, "id": "1601.02745"}, "pdf": {"name": "1601.02745.pdf", "metadata": {"source": "CRF", "title": "Basic Reasoning with Tensor Product Representations", "authors": ["Paul Smolensky"], "emails": ["smolensky@jhu.edu", "moontae@cs.cornell.edu", "deng}@microsoft.com"], "sections": [{"heading": null, "text": "Basic Reasoning with Tensor Product Representations\nPaul Smolensky* Department of Cognitive Science Johns Hopkins University Baltimore, MD 21218, USA smolensky@jhu.edu Moontae Lee Department of Computer Science Cornell University Ithaca, NY 14850, USA moontae@cs.cornell.edu Xiaodong He, Wen-tau Yih, Jianfeng Gao & Li Deng Microsoft Research Redmond, WA 98052, USA {xiaohe, scottyih, jfgao, deng}@microsoft.com\nIn this paper we present the initial development of a general theory for mapping inference in predicate logic to computation over Tensor Product Representations (TPRs; Smolensky (1990), Smolensky & Legendre (2006)). After an initial brief synopsis of TPRs (Section 0), we begin with particular examples of inference with TPRs in the \u2018bAbI\u2019 question-answering task of Weston et al. (2015) (Section 1). We then present a simplification of the general analysis that suffices for the bAbI task (Section 2). Finally, we lay out the general treatment of inference over TPRs (Section 3). We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks.\n0 BRIEF SYNOPSIS OF TPR For present purposes, a tensor T(n) of order n over Rd can be taken to be an n-dimensional array of real numbers, each written T\u03b31\u2026 \u03b3n, \u03b3k \u2208 {1, 2, \u2026, d} \u2261 1:d for all k \u2208 1:n. The two types of tensor operations we use are given in (1): the outer or tensor product (1a) is order-increasing, while contraction (1b) is orderdecreasing. Combining the two gives the inner product (1c). If we interpret an order-2 tensor M(2) as a matrix M, and order-1 tensors U(1), V(1) as vectors/column-matrices u, v, then the outer product u vT of matrix algebra corresponds to the tensor product u \u2297 v (1d) while the dot product u \u014a v = uTv and the matrix-vector product Mu correspond to tensor inner products (1e).\n* This work was conducted while the first author was a Visiting Researcher, and the second author held a summer internship, at Microsoft Research, Redmond, WA.\n(1) Tensor operations\na. outer/tensor product U(n) \u2297 V(m) = T(n+m) T\u03b31\u2026 \u03b3n\u03b3 \u20321\u2026 \u03b3 \u2032m \u2261 U\u03b31\u2026 \u03b3nV\u03b3 \u20321\u2026 \u03b3 \u2032m b. contraction Cjk U(n) = T(n\u22122) T\u03b31\u2026 \u03b3j\u22121\u03b3j+1\u2026 \u03b3k\u22121\u03b3k+1\u2026 \u03b3n \u2261 \u03a3\u03b2 T\u03b31\u2026 \u03b3j\u22121\u03b2\u03b3j+1\u2026 \u03b3k\u22121\u03b2\u03b3k+1\u2026 \u03b3n c. inner product [j < n < k] U(n) \u2022jk V(m) = T(n+m\u22122) T \u2261 Cjk U \u2297 V\n\u2234 T\u03b31\u2026 \u03b3j\u22121\u03b3j+1\u2026 \u03b3k\u22121\u03b3k+1\u2026 \u03b3n+m = \u03a3\u03b2 U\u03b31\u2026 \u03b3j\u22121\u03b2\u03b3j+1\u2026 \u03b3nV\u03b3n+1\u2026 \u03b3k\u22121\u03b2\u03b3k+1\u2026 \u03b3n+m d. [U(1) \u2297 V(1)]\u03b3\u03b3 \u2032 = U\u03b3 V\u03b3 \u2032 \u2245 u\u03b3v\u03b3 \u2032 = [uvT]\u03b3\u03b3 \u2032\ne. U(1) \u22c5 V(1) \u2261 U(1) \u202212 V(1) = U\u03b2 V\u03b2 \u2245 u\u03b2v\u03b2 = u \u22c5 v; [M(2) \u202223 U(1)]\u03b3 = M\u03b3\u03b2U\u03b2 \u2245 M\u03b3\u03b2u\u03b2 = [Mu]\u03b3\nFollowing the customary practice, throughout the paper, except where explicitly stated otherwise, we assume an implicit summation over repeated indices in a single factor \u2014 the Einstein Summation Convention. Thus the explicit summation over \u03b2 in (1b\u2212c) would be omitted and left implicit, as in (1e).\nA particular TPR maps a space S of symbolic structures to a vector space RN. The type of a structure s \u2208 S is determined by a set R = {rk} of structural roles that determines a filler/role decomposition b of S: each token structure s is uniquely characterized as a set of filler/role bindings b(s) = {fk/rk}, in which each role rk is bound to a particular filler fk \u2208 F. As an illustration of one type of filler/role decomposition, positional roles, let S be the set of strings over the alphabet of symbols A \u2261 {a, b, c} and let rk be the role of the kth symbol (from the left). The F = A and R = {r1, r2, \u2026}. For the particular string acb, we have bpos(acb) = {a/r1, b/r3, c/r2}; note that the bindings constitute a set. To illustrate the other type of filler/role decomposition, contextual roles, for the same type of structure S, strings, let rx_y denote the role \u2018preceded by symbol x and followed by symbol y \u2019. Then in this new decomposition bcon, acb has only one binding: bcon(acb) = {c/ra\u2212b}. In this decomposition, a string is characterized by its trigrams.\nGiven a filler/role decomposition b for S, a TPR is defined by encoding each filler fk \u2208 F by a filler tensor fk \u2208 VF, and each role rk \u2208 R by a role tensor rk \u2208 VR. Role tensors are a principal innovation of TPR.\nThen the TPR of a structure s with bindings b(s) = {fk/rk} is the tensor s \u2261 \u03a3k fk \u2297 rk \u2208 VS \u2261 VF \u2297 VR \u2261 {f \u2297 r | f \u2208 VF, r \u2208 VR}. Thus in TPR, binding is done via the tensor product. For the positional-role decomposition bpos, the TPR of s = acb is spos = a \u2297 r1 + b \u2297 r3 + c \u2297 r2. We use this type of positional-role TPR below in (18).\nNow consider the contextual-role decomposition bcon of S. Because bcon(acb) = {c/ra\u2212b}, the TPR of s = acb is scon = c \u2297 ra_b. We need a tensor to encode each role rx_y. Such a role is itself a structure, which can be given a filler/role decomposition such that rx_y is the binding x/r\u2014_y, giving rise to the encoding tensor rx_y \u2261 x \u2297 r\u2014_y. For the role tensor r\u2014_y we can choose the filler vector y, so rx_y = x \u2297 y. Then the TPR for s = acb is scon = c \u2297 [a \u2297 b]. An isomorphic encoding is the more mnemonic scon = a \u2297 c \u2297 b. Thus here the role tensors in VR are of order 2, and the vector encoding a string is a tensor of order 3.\nOur primary interest is in vectorial encodings of propositions such as P(a, b, c). We will adopt a contextual TPR such that the encoding of this proposition is P \u2297 a \u2297 b \u2297 c. The corresponding TPR encoding of a set of propositions B = {Pk(ak, bk, ck)} is B = \u03a3k Pk \u2297 ak \u2297 bk \u2297 ck. The space of such order-4 tensors B is a vector space of dimension 4d, assuming each symbol is encoded by an order-1 tensor over Rd, i.e., for each symbol a the components of its tensor encoding are [a]\u03b3, \u03b3 \u2208 1:d.\nWe will assume that the order-1 tensors fk chosen to encode the symbols fk form an orthonormal set: fj \u22c5 fk = \u03b4jk \u2261 [1 IF j = k ELSE 0]. This assures that TPRs can be unbound with perfect accuracy, via the inner product, which \u2018undoes\u2019 the outer product binding. As an example, consider a set of propositions B = {Pk(ak, bk, ck)}, only one of which has the form P2(a2, b2, x). We can find the unique value of x (namely c2) such that P2(a2, b2, x) \u2208 B from B\u2019s TPR encoding B = \u03a3k Pk \u2297 ak \u2297 bk \u2297 ck, by computing x = B \u202215,26,37 (P2 \u2297 a2 \u2297 b2), i.e.,\nx\u03b3 = B\u03c0\u03b1\u03b2\u03b3 [P2]\u03c0[a2]\u03b1[b2]\u03b2 = (\u03a3k [Pk]\u03c0[ak]\u03b1[bk]\u03b2[ck]\u03b3)[P2]\u03c0[a2]\u03b1[b2]\u03b2 = \u03a3k [Pk \u22c5 P2][ak \u22c5 a2][bk \u22c5 b2][ck]\u03b3 = [c2]\u03b3\nbecause for every value of k except k = 2, either Pk \u2260 P2 or ak \u2260 a2 or bk \u2260 b2, so [Pk \u22c5 P2][ak \u22c5 a2][bk \u22c5 b2] = 0.\nComment. While we assume for convenience throughout the paper that the tensors encoding symbols are orthonormal, there is no assumption that they are 1-hot vectors; we presume they are distributed vectors in which many elements are non-zero. Further, all results here would continue to hold if the tensors encoding symbols were merely linearly independent; unbinding would then be done with unbinding filler tensors fk+ replacing the filler tensors fk themselves. The unbinding example of the previous paragraph would become x = B \u202215,26,37 (P2+ \u2297 a2+ \u2297 b2+), where the unbinding tensors fk+ are defined so that fj \u22c5 fk+ = \u03b4jk. Such vectors must exist if the filler tensors {fk} are linearly independent (essentially, fk+ is the kth row of the inverse of the matrix F which has fk as its kth column; F is invertible if the {fk} are linearly independent)."}, {"heading": "1 BABI EXAMPLE", "text": "Consider the example in (2). \u201c@(a, b, t)\u201d denotes \u201ca is at b at time t\u201d (or \u201ca is co-located with b at time t\u201d). (In Lee et al. (2016), the gloss is \u201ca belongs to b\u201d or \u201ca is contained in b\u201d.) \u213a is the information-question operator; assuming that the denotation of a question is the set of answers, \u213ax.P(x) denotes \u201cthe x\u2019s for which it the case that P(x)\u201d = {x | P(x)}. The English question \u201cwhere was the apple before the kitchen?\u201d is assigned a Logical Form (LF) that can be glossed as \u201cthe location x for which it is the case that a [the apple] was at x at some time t and the apple was at k [the kitchen] at the time t\u2032 immediately following t\u201d.\n(2) An example of a type-3 question from the bAbI task:\na. John picked up an apple @(a, j, t1)\nb. John went to the office @(j, f, t2)\nc. John went to the kitchen @(j, k, t3)\nd. John dropped the apple \u00ac @(a, j, t4)\ne. Where was the apple before the kitchen? \u2192 the office\n\u213ax.\u2203t,t\u2032. @(a, x, t) & @(a, k, t\u2032)\n& \u227a(t, t\u2032)\nHere we will assume given a [surface string \u2192 LF] semantic parser that generates the right column of (2) given the left column. We strive to separate issues of commonsense inference per se from issues of NLP narrowly construed, such as identifying: the semantic predicates corresponding to English words, the referents of referring expressions, the antecedents of anaphoric expressions, and the content of elided material. We thus assume given NLP procedures for performing such computations and focus exclusively on the problem of commonsense reasoning with distributed, vectorial representations.\nAll symbols in the predicate logic analysis are encoded in TPR as vectors, or order-1 tensors, in Rd. Thus @ is a vector encoding the symbol \u2018@\u2019, and @\u03c0 \u2208 R, \u03c0 = 1:d, are its d real components. In general, the symbol-encoding vectors such as @ are distributed, in the sense that many components are non-zero (they are not in general 1-hot vectors). For convenience, here we assume these vectors are an orthonormal set (what is actually required is only that they be linearly independent).\nAt each time ti, the reasoning process is in a state B(ti) which we take to be the set of propositions constituting the knowledge base of facts concerning the problem situation; this grows monotonically with ti, as more information arrives: the LF form of the ith sentence, L(ti). \u201c\u227a(t, t\u2032)\u201d denotes the proposition \u201cthe time t immediately precedes the time t\u2032 \u201d. As illustrated in (2), we will often notate times as \u201cti\u201d, i \u2208 N, where \u2200i. \u227a(ti, ti+1). The times {ti} have TPRs {ti} \u2282 Rd that are linearly independent, so there is a linear operator T on Rd satisfying (3).\n(3) Time-increment operator T\nT ti = ti+1\nThe TPR of the time-ti knowledge base B(ti) is the tensor B(ti). This fourth-order tensor is the sum of the TPRs of propositions of the form @(x, y, t) or \u227a(t, t\u2032, \u00f8); \u00f8 is a dummy symbol used for convenience to make \u227a, like @, a predicate that takes 3 arguments. The propositions are given a contextual TPR:\n(4) TPR of propositions\na. @(x, y, t) @ \u2297 x \u2297 y \u2297 t\nb. \u227a(t, t\u2032, \u00f8) \u227a \u2297 t \u2297 t\u02b9 \u2297 \u00f8\nThe four indices in B\u03c0\u03b1\u03b2\u03c4 can be thought of as the proposition-, first-argument-, second-argument, and third-argument-indices. For a proposition with predicate @ such as @(j, k, t) (2c) or @(a, j, t\u2032) (2a), \u03b1 is the index of an actor- or object-vector, \u03b2 is the index of a location- or actor-vector, and \u03c4 is the index of a timevector: the TPR of such proposition is b\u03c0\u03b1\u03b2\u03c4 = @\u03c0 j\u03b1 k\u03b2 t\u03c4 or @\u03c0 a\u03b1 j\u03b2 t\u2032\u03c4.\nThe reasoning in example (2) requires two rules of inference:\n(5) Transitivity Axiom for @\n\u2200x,y,z,t. @(x, y, t) & @(y, z, t) \u21d2 @(x, z, t)\n(6) Persistence Axiom for @\n\u2200x,y,t,t\u2032. @(x, y, t) & \u227a(t,t\u2032) \u21d2 @(x, y, t\u2032)\nIn the vectorial reasoning system we develop, the persistence axiom can be applied at every time t, deriving positions at the immediately following time t\u2032. Expressions like (2d) \u201cJohn dropped the apple\u201d are interpreted as \u00ac@(a, j, t) \u201cnot: the apple is at John [at time t]\u201d, and the tensor b encoding this proposition will be the negation of the vector encoding \u201cthe apple is at John\u201d; b will simply cancel the vector for \u201cthe apple is at John\u201d that was generated by the persistence axiom. Then at subsequent times there is no longer an encoded proposition @(a, j, t) that the persistence axiom can propagate forward.\nThe reasoning needed for (2) can be expressed as\nt1: the apple is at John\nt2: \u21d2persistence the apple is at John, John is at the office \u21d2transitivity the apple is at the office\nt3: \u21d2persistence the apple is at John, John is at the kitchen \u21d2transitivity the apple is at the kitchen\nt4: \u21d2persistence the apple is at the kitchen, the apple is at John\nThe TP encodings of the inference rules (5)\u2212(6) are given in (7)\u2212(8); these encodings are derived by a general procedure that will be illustrated in Section 3.\n(7) TP encoding of the Transitivity Axiom for @\na. \u2200x,y,z,t. @(x, z, t) \u21d0 @(x, y, t) & @(y\u2032, z, t) & [y = y\u2032] b. [V(B(t))]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033 = @\u03c0\u2033 t\u03c4\u2033 [B(t)]\u03c0\u03b1\u03b2\u03c4 @\u03c0t\u03c4 [B(t)]\u03c0\u2032\u03b1\u2032\u03b2\u2032\u03c4\u2032 @\u03c0\u2032t\u03c4\u2032 \u03b4\u03b2\u03b1\u2032 + [B(t)]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033 c. V(B(t)) = V[B(t), B(t); t] d. V = a multilinear tensor operation encoding inference from the Transitivity Axiom:\ni. \u2200x,y,z,t;\u2200p \u2208T . p(x, z, t) \u21d0 p(x, y, t) & p(y, z, t)\nii. V[X, Y; t]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033 = \u03a3p \u2208T p\u03c0\u2033 (p\u03c0[X]\u03c0\u03b1\u03b2\u03c4t\u03c4) (p\u03c0\u2032[Y]\u03c0\u2032\u03b2\u03b2\u2032\u03c4 \u2032 t\u03c4 \u2032) t\u03c4\u2033 iii. (Modified) Penrose diagram (for one p \u2208 T ):\nV[X, Y; t] = X Y | | | | | | | | | | | | p p t p t t | | p( x, z, t) \u21d0 p( x, y, t) & p( y, z, t)\n(8) TP encoding of the Persistence Axiom for @ [note that \u2200i. T ti\u22121 = ti, so \u227a(t,t\u2032) iff t\u2019 = T t iff t = T\u22121 t\u2032]\na. \u2200x,y,t,t\u2032. @(x, y, t\u2032) \u21d0 @(x, y, t) & \u227a(t,t\u2032)\nb. B(ti) = (1 + \u03a3x,y [@ \u2297 x \u2297 y \u2297 ti][@ \u2297 x \u2297 y \u2297 T\u22121 ti]\u22a4) B(ti\u22121) c. B(ti) = (1 + P(ti)) B(ti\u22121) d. P(t) = matrix operating on B-tensors that encodes inference from the Persistence Axiom for @\ni. [P(ti) B(ti\u22121)]\u03c0\u03be\u03b7\u03c4 = P(t)\u03c0\u03be\u03b7\u03c4, \u03c0\u2032\u03be\u2032\u03b7\u2032\u03c4\u2032 [B(ti\u22121)]\u03c0\u2032\u03be\u2032\u03b7\u2032\u03c4\u2032 ii. P(t)\u03c0\u03be\u03b7\u03c4, \u03c0\u2032\u03be\u2032\u03b7\u2032\u03c4\u2032 = @\u03c0 \u03b4\u03be\u03be\u2032 \u03b4\u03b7\u03b7\u2032 t\u03c4 @\u03c0\u2032 [T\u22121]\u03c4\u2032\u03c4\u2033 t\u03c4\u2033\niii. Penrose diagram: P(t) = @ \u03b4 \u03b4 t @ T\u22121 t\nHere and throughout the paper, except where explicitly stated otherwise, we deploy the Einstein Summation Convention, according to which repeated indices are implicitly summed over all their values; e.g., in (7b) there is an implicit sum over \u03c0, \u03b2, \u03c4, \u03c0\u2032, \u03b1\u2032, and \u03c4\u2032. In (7b) and (8d.ii), \u03b4 is the Kronecker \u03b4: \u03b4ij \u2261 [1 IF i=j ELSE 0].\nIn the general case (7d.ii), the encoding of inference using the Transitivity Axiom involves a sum over the set T of all transitive predicates; we will only be using the single transitive predicate @ here, and the other expressions in (7)\u2212(8) deal only with that predicate, which is persistent as well as transitive.\nIn (modified) Penrose Tensor Diagrams such as (7d.iii) and (8d.iii), each box denotes a tensor and the nth line from the left that emanates from the box for tensor A denotes the nth index of A; there are m such lines if A is an mth-order tensor. When two lines are joined, the values of those two indices are set equal and there is sum over all values for the index, as explicitly shown for \u03b2 in (7d.ii), the algebraic expression denoted by the Penrose Diagram (7d.iii). Penrose Diagrams enable complex tensor equations to be written precisely without any indices, thereby making the structure of the equations more transparent.\nThe correspondence between a predicate logic expression and a Penrose Tensor Diagram will be made explicit in Section 3, but the juxtaposition in (7d.iii) of the Penrose diagram and the corresponding predicate logic expression beneath it already suggests the nature of the correspondence visually, with colors of predicate logic symbols matching the colors of their corresponding tensors in the diagram as well as the colors of their corresponding indices in the algebraic expressions (7d.ii), (8d.i).\nIn (8b)\u2212(8c), the identity matrix over tensors, 1, ensures that all the propositions encoded in B at time ti\u22121 (propositions about the problem situation at all times t \u2264 ti\u22121) are carried over and also encoded in B at time ti.\nAn example of the use of the Transitive Inference procedure of (7) is given in (9). (Recall that the TPRs of all symbols form an orthonormal set.)\n(9) Example of Transitivity Inference using (7)\na. Let B(t2) = {@(a, j, t1), @(a, j, t2), @(j, k, t2)}; result of transitive inference: add @(a, k, t2)\nb. Then B(t2) = @ \u2297 a \u2297 j \u2297 t1 + @ \u2297 a \u2297 j \u2297 t2 + @ \u2297 j \u2297 k \u2297 t2\nc. B(t2) \u2297 B(t2) = @ \u2297 a \u2297 j \u2297 t1 \u2297 @ \u2297 j \u2297 k \u2297 t2 + @ \u2297 a \u2297 j \u2297 t2 \u2297 @ \u2297 j \u2297 k \u2297 t2 + \u22ef\nd. [V(B(t2))]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033 = @\u03c0\u2033 t2\u03c4\u2033 [B(t2)]\u03c0\u03b1\u03b2\u03c4 @\u03c0 t2\u03c4 [B(t2)]\u03c0\u2032\u03b1\u2032\u03b2\u2032\u03c4\u2032 @\u03c0\u2032t2\u03c4\u2032\u03b4\u03b2\u03b1\u2032 + [B(t2)]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033\n= @\u03c0\u2033 t2\u03c4\u2033[@ \u2297 a \u2297 j \u2297 t1 \u2297 @ \u2297 j \u2297 k \u2297 t2] \u03c0\u03b1\u03b2\u03c4 \u03c0\u2032\u03b1\u2032\u03b2\u2032\u03c4\u2032 @\u03c0t2\u03c4@\u03c0\u2032t2\u03c4\u2032\u03b4\u03b2\u03b1\u2032\n+ @\u03c0\u2033 t2\u03c4\u2033[@ \u2297 a \u2297 j \u2297 t2 \u2297 @ \u2297 j \u2297 k \u2297 t2] \u03c0\u03b1\u03b2\u03c4 \u03c0\u2032\u03b1\u2032\u03b2\u2032\u03c4\u2032 @\u03c0t2\u03c4@\u03c0\u2032t2\u03c4\u2032\u03b4\u03b2\u03b1\u2032 + \u22ef + [B(t2)]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033\n= @\u03c0\u2033 t2\u03c4\u2033[@\u03c0@\u03c0]a\u03b1[j\u03b2 j\u03b1\u2032\u03b4\u03b2\u03b1\u2032][t1\u03c4t2\u03c4][@\u03c0\u2032@\u03c0\u2032]k\u03b2\u2032[t2\u03c4\u2032t2\u03c4\u2032]\n+ @\u03c0\u2033 t2\u03c4\u2033[@\u03c0@\u03c0]a\u03b1[j\u03b2 j\u03b1\u2032\u03b4\u03b2\u03b1\u2032][t2\u03c4t2\u03c4][@\u03c0\u2032@\u03c0\u2032]k\u03b2\u2032[t2\u03c4\u2032t2\u03c4\u2032] + \u22ef + [B(t2)]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033\n= @\u03c0\u2033 t2\u03c4\u2033a\u03b1 k\u03b2\u2032 + [B(t2)]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033 = [@ \u2297 a \u2297 k \u2297 t2]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033 + [B(t2)]\u03c0\u2033\u03b1\u03b2 \u2032\u03c4\u2033\ne. i.e., V(B(t2)) is the TPR of: @(a, k, t2) \u222a B(t2)\nIn the evaluation of V(B(t2)) in (9d), because the TPRs of all symbols are orthogonal, all terms in B(t2) \u2297 B(t2) (with components [B(t2)]\u03c0\u03b1\u03b2\u03c4[B(t2)]\u03c0\u2032\u03b1\u2032\u03b2\u2032\u03c4\u2032) are annihilated except the single term which is the TPR of the proposition pair \u3008@(a, j, t2), @(j, k, t2)\u3009, because the inner product with @\u03c0t2\u03c4@\u03c0\u2032t2\u03c4\u2032\u03b4\u03b2\u03b1\u2032 gives 0 for any pair \u3008p(x, y, t), p\u2032(z, w, t\u2032)\u3009 unless p = @ = p\u2032, y = z, and t = t2 = t\u2032; e.g., the factor in red brackets [t1\u03c4t2\u03c4] is 0 because t1 and t2 are orthogonal. (The factors [v\u00b5v\u00b5] all equal 1 because the TPRs of all symbols are normalized to length 1.)\nAt each consecutive time ti we also have the following update rule (10) for the (immediate) Temporal Precedence relation \u227a (recall the definition of the time-increment operator T (3)).\n(10) Update rule for the symbolic Temporal Precedence relation T = \u227a and its TPR T\na. T (ti) = \u227a(ti\u22121, ti) \u222a T (ti\u22121)\nb. T(ti) = \u227a \u2297 ti\u22121 \u2297 ti + T(ti\u22121) = \u227a \u2297 ti\u22121 \u2297 T ti\u22121 + T(ti\u22121)\nThe procedure for building the knowledge base B incrementally, as the sentence Si pertaining to each time ti is processed, is given in (11).\n(11) TP Reasoning Algorithm\na. Goal: knowledge base B {facts from story} \u222a {inferred facts} B = @ \u2297 \u2297 J\u2297 t1 + \u227a \u2297 t1\u2297 t2 \u2297 \u00f8 + \u22ef To construct B(ti) / B(ti), loop over sentences i in story: b. B(ti\u22121) given B(ti\u22121) already computed c. inferences from Persistence Axiom\n\u2200k,\u2200p \u2208 P. p(a1, \u2026, am; ti\u22121) \u21d2 p(a1, \u2026, am; tk)\nB(ti) \u2190 B(ti\u22121) + P(ti) B(ti\u22121) P(tk) = Persistence matrix (over tensors)\nd. update \u227a add \u227a(ti\u22121, ti)\nB(ti) \u2190 B(ti) + Ti Ti = \u227a \u2297 ti\u22121 \u2297 T ti\u22121 T = time-update matrix; ti = T ti\u22121\ne. add L(ti) = LF of ith sentence e.g., John picked up an apple\nB(ti) \u2190 B(ti) + L(ti) e.g. @ \u2297 A \u2297 J \u2297 t1 = L(t1)\nf. repeat until no change: g. inferences from Transitivity Axiom:\n\u2200x,y,z,t,p\u2208T. p(x, y, t) & p(y, z, t) \u21d2 p(x, z, t)\nB(ti) \u2190 B(ti) + V[B(ti), B(ti); ti] V = multilinear tensor operation of\nTransitive Inference This algorithm, processing the example (2), will produce (12).\n(12) Algorithm (11) processing example (2)\nSentence i LF: L(i) Inferences T update Explanation\na. John picked up an apple\n@(a, j, t1)\nb. John went to the office\n@(j, f, t2) @(a, j, t2) @(a, f, t2)\n\u227a(t1, t2) \u2200x,y,t,t\u2032. @(x, y, t) & \u227a(t,t\u2032) \u21d2 @(x, y, t\u2032) Persistence\n\u2200x,y,z,t. @(x, y, t) & @(y, z, t) \u21d2 @(x, z, t) Transitivity\nc. John went to the kitchen\n@(j, k, t3) @(a, j, t3) @(a, k, t3)\n\u227a(t2, t3)\nd. John dropped the apple\n\u00ac @(a, j, t4) @(a, k, t4) \u227a(t3, t4) contributes \u2212@ \u2297 a \u2297 k \u2297 t4 which cancels inference from Persistence Axiom\ne. Where was the apple before the kitchen?\n\u2192 the office\n\u213ax.\u2203t,t\u2032. @(a, k, t\u2032) & @(a, x, t) & \u227a(t, t\u2032)\nt\u2032 = t2, t = t3 x = f\nTo answer the query, we construct its TP encoding\u2020:\n(13) The query\nWhere was the apple before the kitchen?\n\u213ax.\u2203t\u2032,t. @(a, k, t\u2032) & @(a, x, t) & \u227a(t, t\u2032, \u00f8)\nx = B B B | | | | | | | | | | | | | @ a k @ a \u227a \u00f8\nPenrose Tensor Diagram\nx \u03b2\n2\n= B \u03c0\n1 \u03b1 1 \u03b2 1 \u03b3 1\nB \u03c0\n2 \u03b1 2 \u03b2 2 \u03b3 2\nB \u03c0\n3 \u03b1 3 \u03b2 3 \u03b3 3\n@ \u03c0\n1\na \u03b1\n1\nk \u03b2\n1\n@ \u03c0\n2\na \u03b1\n2\n\u227a \u03c0\n3\n\u00f8 \u03b3\n3\n\u03b4 \u03b3\n1 \u03b2 3\n\u03b4 \u03b3\n2 \u03b1 3\nComponentwise expression\n\u2020 This tensor equation is the TP encoding of the form of the query expression given in (13); an alternative that replaces one factor of B with a factor T is the TP encoding of an alternative form of the query: \u213ax.\u2203ti. @(a, k, ti+1) & @(a, x, ti). Either form of the query is slightly simplified; the additional requirement \u201cx \u2260 k\u201d is needed if @(a, k, t) persists across two consecutive times. In the TP encoding, this yields an additional factor, or a simple post-processing step, that projects onto the subspace orthogonal to k."}, {"heading": "2 SIMPLIFICATION", "text": ""}, {"heading": "2.1 Deriving the simplification for two-place predicates", "text": "The first simplification is to omit the vector encoding the relation \u201cis at\u201d, which we\u2019ve written \u201c@\u201d. As shown in Lee et al. (2016), for most of the bAbI problem types, this is the only relation needed (they are \u201cuni-relational\u201d), so it is not necessary to encode it explicitly: all items have the same initial tensor factor @. This simplification is shown in the third column of table (14). The matrix-algebra expression xyT and the tensor expression x \u2297 y define exactly the same elements [xyT]jk = xjyk = [x \u2297 y]jk.\n(14) Simplification sufficient for the bAbI task: implicit is at predicate, implicit time stamps\nSymbolic Full TPR Simplification: 1 2 3 4\n{@(x, y, t1), @ \u2297 x \u2297 y \u2297 t1 xy\u22a4 \u2245 x \u2297 y = x \u2297 y \u2297 1 + y \u2297 z \u2297 0 = x \u2297 y \u2297 t1 + y \u2297 z \u2297 t2 @(y, z, t2)} + @ \u2297 y \u2297 z \u2297 t2 yz\u22a4 y \u2297 z 0 1\nThe second simplification is to replace the time stamps with slots in a \u201cmemory\u201d; in table (14), these slots are shown as a vertical queue in the \u201cSimplification: 1\u201d column. Rather than explicitly including a final tensor factor ti that encodes an explicit time stamp, we just locate the item for time ti in the ith position in the queue. If we think of these memory positions as locations in a vector, as shown in the \u201cSimplification: 2\u201d column, however, we can recognize the vector as a sum of two vectors, each the tensor product of the cell entry with a unit column vector, as spelled out in the \u201cSimplification: 3\u201d column. As indicated in the \u201cSimplification: 4\u201d column, this reduces to just the Full TPR representation (with the initial tensor factor @ omitted) \u2014 once we identify t1 = (1, 0), t2 = (0, 1). This analysis obviously trivially extends to any number of time steps.\nWith the two simplifications made above, the tensor implementation of inference from the Transitivity Axiom becomes:\n(15) Simplified Transitive Inference operation (predicate p and time t factors not made explicit)\na. Full form: V[X, Y; t]\u03c0\u2033\u03b1\u03b2 \u2032\u03b3\u2033 = \u03a3p \u2208T p\u03c0\u2033 (p\u03c0[X]\u03c0\u03b1\u03b2\u03b3t\u03b3) (p\u03c0\u2032[Y]\u03c0\u2032\u03b2\u03b2\u2032\u03b3 \u2032 t\u03b3 \u2032) t\u03b3\u2033 b. Simplified form: V[X, Y]\u03b1\u03b2 \u2032 = [X]\u03b1\u03b2[Y]\u03b2\u03b2\u2032 = [XY ]\u03b1\u03b2 \u2032, i.e., simple matrix multiplication\nThis is exactly the form that transitive inference takes in Lee et al. (2016); e.g., in the example type-2 question discussed there, X = fmT and Y = mgT \u2014 for @(football, mary, t) and @(mary, garden, t) \u2014 are combined by matrix multiplication to give XY = f (mT m) gT = fgT \u2014 for @(football, garden, t)."}, {"heading": "2.2 Deriving the simplification for three-place predicates", "text": "The analyses in Lee et al. (2016) of questions in categories 2, 3 and 5 involve the binding of 3 entities rather than 2. For example:\n(16) The representation of \u201cMary travelled to the garden [from the kitchen]\u201d is\nm (g \u2218 k)T, where g \u2218 k \u2261 U [g ; k]; U: R2d \u2192 Rd, U [g ; k] \u2261 R0 g + R1 k; R0, R1: Rd \u2192 Rd\nLet all of the n relevant entities (actors, objects, locations, etc.) {el | l \u2208 1:n} be represented by unit vectors {\u00eal} in Rd, and suppose d = 2m, m \u2208 N, with m \u2265 n. (These assumptions apply to the implementation discussed in Lee et al. (2016).) Assume the generic case in which the {\u00eal} are linearly independent, and let the n-dimensional subspace of Rd that they span be E. Let the restrictions of R0, R1 to E be denoted R0 \u2261 R0|E , R1 \u2261 R1|E . Assume the generic case in which R0, R1 are non-singular, so that {R \u00eal \u2261 \u00eal0} \u2282 Rd, {R \u00eal \u2261 \u00eal1} \u2282 Rd are each linearly independent sets. In order that U be information-preserving, assume that these\ntwo sets are linearly independent of each other, i.e., that the union of these two sets is also a linearly independent set {\u00eal\u03b2 | l \u2208 1:n, \u03b2 \u2208 0:1} \u2282 Rd. This is possible because 2n \u2264 2m = d.\nBecause the R\u03b2 are non-singular, there exist inverses R0\u22121, R1\u22121 respectively defined over span{\u00eal0} \u2261 S0, span{\u00eal1} \u2261 S1; these are linearly independent n-dimensional subspaces of Rd. There is an extension R0+ of R0\u22121 to S \u2261 span(S0 \u222a S1) = range(U|E) such that R0+|S0 = R0\u22121 and R0+|S1 = 0 (namely R0+ (\u03a3l\u03b2 yl\u03b2 \u00eal\u03b2) = \u03a3l yl0 \u00eal ; since R0: \u03a3l xl \u00eal \u21a6 \u03a3l xl \u00eal0, we get R0+R0 = 1|E ). Similarly there exists R1+ such that R1+|S1 = R1\u22121 and R1+|S0 = 0. Thus:\n(17) Inverting \u2218\nR0+(a \u2218 b) = a, R1+(a \u2218 b) = b, for all a, b \u2208 E\nThe binding in (16) can be identified as a Contracted TPR as follows. Recall that the matrix product is a kind of tensor inner product, that is, a contraction of a tensor (outer) product: [Mv]k \u2261 \u03a3j vjMkj \u2261 [C13 v \u2297 M]k where v is the vector v considered as an order-1 tensor and M is the matrix M considered as an order-2 tensor. In particular, [R0 a]k = [C13 a \u2297 R0]k and [R1 a]k = [C13 a \u2297 R1]k . Thus:\n(18) The representation of \u201cm 2 travelled \u0394 3; \u0394 3 = to g 0 from k 1\u201d as a Contracted TPR\n[m (g \u2218 k)T]jk = [m \u2297 \u0394]jk \u201c m 2 travelled \u0394 3\u201d: contextual-binding of fillers m, \u0394 of slots 2, 3\n\u0394 = C13[g \u2297 R0 + k \u2297 R1] \u201c \u0394 3 = to g 0 from k 1\u201d: positional-binding of fillers g, k to roles 0, 1\nR0 and R1 are the tensors representing the roles 0, 1 in \u201cto 0 from 1\u201d.\nAlternatively, eliminating the displacement \u0394, we have, for \u201c m 2 travelled to g 0 from k 1\u201d:\n[m (g \u2218 k)T]jk = C24[m \u2297 (g \u2297 R0 + k \u2297 R1)]jk\nUnbinding the actor a by left-multiplying by aT gives the displacement \u0394 of a that is represented:\n[aT m (g \u2218 k)T]k = \u03a3j [a]j [m (g \u2218 k)T]jk = (\u03a3j [a]j [m]j) (g \u2218 k)Tk = (a \u22c5 m) (g \u2218 k)Tk = (a \u202212 C24[m \u2297 (g \u2297 R0 + k \u2297 R1)])k \u2261 C12,35 [a \u2297 m \u2297 (g \u2297 R0 + k \u2297 R1)]k\n= (a \u202212 m) C13[g \u2297 R0 + k \u2297 R1]k\nThe factor (a \u202212 m) = a \u22c5 m = 1 if a = m (entity vectors are normalized), while a \u22c5 m is considerably less than 1 if the n entity vectors are generically distributed in a considerably larger space Rd, d \u2265 2n; indeed, we have been assuming that these n vectors have been chosen to be orthogonal, in which case we have exactly a \u22c5 m = 0 when a \u2260 m. When a = m the result is C13[g \u2297 R0 + k \u2297 R1], the Contracted TPR of the pair \u3008g, k\u3009 = \u0394: this tells us that the represented displacement of m was: to g from k. The entities g and k can be extracted from \u0394 via the inner products with the duals of the role tensors, as is standard for TPRs: g = \u0394 \u202213 R0+, k = \u0394 \u202213 R1+; these equations are the tensor counterparts of R0+(R0 g + R1k) = g , R1+(R0g + R1k) = k.\nIn Lee et al. (2016), questions in category 5 are treated with an operation * that functions identically to \u2218, with d \u00d7 2d matrix V rather than U. The same analysis just given for \u2218/U applies to show that the */V method amounts to a Contracted TPR."}, {"heading": "2.3 Deriving the simplification for Path Finding (bAbI category 19)", "text": "An example of a bAbI Category 19 problem is given in (19).\n(19) Path-Finding: example problem\nSentence i LF: L(i) Full TPR Model\na. The bedroom is south of the hallway. s(b, h) s \u2297 b \u2297 h b = Sh\nb. The bathroom is east of the office. e(a, o) e \u2297 a \u2297 o a = Eo\nc. The kitchen is west of the garden. w(k, g) w \u2297 k \u2297 g k = Wg\nd. The garden is south of the office. s(g, o) s \u2297 g \u2297 o g = So\ne. The office is south of the bedroom. s(o, b) s \u2297 o \u2297 b o = Sb\nf. How do you go from the garden to the bedroom? \u213aP.P(g, b)\n\u2192 P = p[n, n]\nThe expression \u201cp[s,w]\u201d denotes \u201cthe path consisting of west then south\u201d. p accepts as an argument a list of directions, so that, in general, \u201cp[dn, \u2026, d1]\u201d denotes the path consisting of d1 (\u2208 {n, s, e, w}) followed by d2 followed by \u2026 followed by dn.\nThe rules of inference needed to solve such Path-Finding problems are given in (20).\n(20) Axioms/rules of inference for Path-Finding problems: \u2200x,y,z,d,d1,\u2026,dn,d\u20321,\u2026,d\u2032n\u2032 \u2026\na. n(x, y) \u21d2 s(y, x); s(x, y) \u21d2 n(y, x); e(x, y) \u21d2 w(y, x); w(x, y) \u21d2 e(y, x)\nb. d(x, y) \u21d2 p[d](x, y)\nc. p[dn, \u2026, d1](z, y) & p[d\u2032n\u2032, \u2026, d\u20321]( y, x) \u21d2 p[dn, \u2026, d1, d\u2032n\u2032, \u2026, d\u20321](z, x)\nThe rules in (20a) express the inverse semantics within the pairs n \u2194 s, e \u2194 w. The rule (20b) states, for example, that if x is (one block) north of y (on a Manhattan-like grid of locations) \u2014 n(x, y) \u2014 then the path consisting of (a one-block step in the direction) north \u2014 p[n] \u2014 goes to x from y.\nFinally (20c) asserts that if p[dn, \u2026, d1] \u2014 the path consisting of d1 (\u2208 {n, s, e, w}) followed by d2 followed by \u2026 followed by dn \u2014 leads to z from y, and the path p[d\u2032n\u2032, \u2026, d\u20321] \u2014 consisting of d\u20321 followed by \u2026 followed by d\u2032n\u2032 \u2014 leads to y from x, then p[dn, \u2026, d1, d\u2032n\u2032, \u2026, d\u20321] leads to z from x. This rule resembles the transitivity rule p(z, y) & p( y, x) \u21d2 p(z, x), but whereas transitivity involves only a single relation, the path-finding rule involves the productive combination of multiple relations. In this sense, Path Finding is a \u201cmulti-relational\u201d problem, whereas all the simpler bAbI problem types, reducible to transitivity, are \u201cuni-relational\u201d \u2014 a main point of Lee et al. (2016).\nReasoning with the multi-relational axioms in (20) can be implemented in TPR as in the simpler case of transitive inference (5) above: (7), (11). The third column of table (19) shows the full TPR encoding of the statement of the example problem. The symbolic knowledge base B is a set of stated and inferred propositions {di(xi, yi)} \u222a {p[Pk](wk, zk)}, with Pk = dknk\u22efd k 2dk1. The TPR of B, B, is the direct sum of tensors of the forms di \u2297 xi \u2297 yi and p \u2297 Pk \u2297 wk \u2297 zk, where Pk \u2261 dknk \u2297 \u22ef \u2297 d k 2 \u2297 dk1. To reply to the query \u201cHow do you go from u to v?\u201d we test possible paths P to see whether P leads to v from u; in the bAbI task, only paths up to length 2 need to be considered. To test whether p[P](v, u) \u2208 B, we take the inner product of B with p \u2297 P \u2297 v \u2297 u; the result is 0 or 1, the truth value of p[P](v, u).\nThere is a simplification of the full TP analysis that relies on a vectorial \u2018model\u2019 of the axioms (20), in the sense of model theory in mathematical logic: a set of linear-algebraic objects which are inter-related in ways that satisfy the axioms. The final column of table (19) shows the corresponding representations in\nthis simpler implementation. In the full TP analysis, the vectors encoding locations and directions are arbitrary orthonormal vectors. In the simpler model, there are systematic relations between the encodings of locations and directions. Specifically, the directions north and east are encoded by d \u00d7 d matrices N, E, where locations x are encoded by vectors x \u2208 Rd. Rather than having inference relations such as (20a) implementing the inverse relationships among directions, the matrices encoding south and west are systematically related to those encoding north, east, by: S = N\u22121, W = E\u22121. (This requires that N, E be nonsingular.) And rather than adding to B an arbitrary fact-tensor n \u2297 x \u2297 y, the truth of n(x, y) is encoded in the relation among the encoding vectors and matrices themselves: x = Ny. These conditions ensure that the vectorial encodings of positions and directions provide a model for the axioms (20a): n(x, y), encoded as x = Ny, entails y = N\u22121x = Sx, the encoding of s(y, x).\nIf L is the set of all possible locations for the given problems, then {Ny | y \u2208 L} and {Ey | y \u2208 L} must be independent sets of vectors, i.e., we need the following condition: range(N|L) and range(E|L) are linearly independent, so 2|L| \u2264 d. (This is reminiscent of the conditions on R0, R1 in (16).)\nFor paths, we let the encoding of p[dn, \u2026, d1](z, y) be z = Dn\u22efD1y. These encodings provide a model for the composition axiom, since the encodings of p[dn, \u2026, d1](z, y) and p[d\u2032n\u2032, \u2026, d\u20321]( y, x), z = Dn\u22efD1y and y = D\u2032n\u2032 \u22ef D\u20321\u2032 x, entail that z = Dn\u22efD1D\u2032n\u2032 \u22ef D\u20321\u2032 x, the encoding of p[dn, \u2026, d1, d\u2032n\u2032, \u2026, d\u20321](z, x). Also, the base case expressed by axiom (20b) is satisfied, since d(x, y) and p[d](x, y) have the same encoding, x = Dy.\nIn the simplified approach implemented in Lee et al. (2016), a set of position vectors and direction matrices encoding the statements given in the problem is generated. Then, to test whether p[P](v, u) for a given path P, to determine whether P answers the query \u201chow do you go from u to v?\u201d, the validity of the equation v = Pu is determined, where P = D or D2D1 in accord with P = p[d] or p[d2, d1]."}, {"heading": "2.4 Performance of simplification on bAbI dataset", "text": "The simplification of the full TPR reasoning analysis described above was implemented and the results reported in Lee et al. (2016) are briefly summarized in (21).\n(21) Performance of the simplification on the bAbI tasks\n100% in all question categories except:\nC5: 99.8%\nC16: 99.5%\nBecause the present analysis performs inference by programmed vector procedures rather than learned network computations, this performance cannot be directly compared to that of previous work addressing the bAbI task (including, notably, Peng et. al (2015)\u2019s Neural Reasoner which achieved 66.4%/97.9% and 17.3%/87.0% on tasks 17 and 19, with 1k/10k training examples, respectively; these are the most difficult tasks, on which the previous best performance was 72% and 36%). (Previous best performance on C5/C16 were 99.3%/100%, by the strongly supervised Memory Network of Weston, Chopra & Bordes (2014).)"}, {"heading": "3 GENERAL TREATMENT", "text": "(22) General case of query construction\na.\nb.\nc.\nHow the particular example query (13) follows from the general case (22) is spelled out in (23).\n(23) Derivation of the query (13) from the general case (22)\na. \u213ax.\u2203t\u2032,t. @(a, k, t\u2032) & @(a, x, t) & \u227a(t, t\u2032, \u00f8)\nb. \u213ax22.\u2203e31,e32. p1(c11, c21, e31) p1 = @; c11 = a, c21 = k, e31 = t\u2032 & p2(c12, x 2 2, e 3 2) p2 = @; c 1 2 = a, v 2 2 = x, e 3 2 = t\n& p3(e13, e 2 3, c 3 3) p3 = \u227a; e13 = t, e23 = t\u2032, c33 = \u00f8 & [e31=e 2 3]&[e 3 2=e 1 3]\nc. ans \u03b322 = B\u03c01 \u03b311 \u03b321 \u03b331 B\u03c02 \u03b312 \u03b322 \u03b332 B\u03c03 \u03b313 \u03b323 \u03b333 [c 1 1]\u03b311 [c 2 1]\u03b321 [c 1 2]\u03b312 [c 3 3]\u03b333 \u03b4\u03b331 \u03b323 \u03b4\u03b332 \u03b313\nd. x\u03b22 = B\u03c01 \u03b11 \u03b21 \u03b31 B\u03c02 \u03b12 \u03b22 \u03b32 B\u03c03 \u03b13 \u03b23 \u03b33 a\u03b11 k\u03b21 a\u03b12 \u00f8\u03b33 \u03b4\u03b31\u03b23 \u03b4\u03b32\u03b13\nAnalogous methods allow general TP instantiation of rules of inference, from which the particular forms in (7)\u2212(8) can similarly be derived.\nO\u0301x kx(1) ix(1)!x kx(q) ix(q) \u2203e ke (1) ie (1)!e ke (s) ie (s) k\u22081:n \u039bpk(vk1 ,\u2026,vkm) j\u22081:E \u039b[e \u2032ke ( j) \u2032ie ( j) = e \u2032\u2032ke ( j) \u2032\u2032ie ( j) ] v k i \u2208{c k i ,x k i ,e k i }\nans \u03b3\nkx (1) ix (1)!\u03b3 kx ( q )\nix ( q ) = B\n\u03c0k\u03b3 k 1!\u03b3 k\nm [ck i ]\u03b3 k\ni\ni ,k:vk i =ck\ni \u220f k\u22081:n \u220f \u03b4\n\u03b3 \u2032ke ( j ) \u2032ie ( j ) \u03b3 \u2032\u2032ke ( j ) \u2032\u2032ie ( j )\nj\u22081:E \u220f"}], "references": [{"title": "Reasoning in vector space: An exploratory study of question answering", "author": ["Lee", "Moontae", "He", "Xiaodong", "Yih", "Wen-tau", "Gao", "Jianfeng", "Deng", "Li", "Smolensky", "Paul"], "venue": "Under review for", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["Smolensky", "Paul"], "venue": "Artificial Intelligence,", "citeRegEx": "Smolensky and Paul.,? \\Q1990\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1990}, {"title": "The Harmonic Mind: From Neural Computation to OptimalityTheoretic Grammar. Volume I: Cognitive Architecture", "author": ["Smolensky", "Paul", "Legendre", "G\u00e9raldine"], "venue": null, "citeRegEx": "Smolensky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smolensky et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 0, "context": "We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks.", "startOffset": 88, "endOffset": 162}, {"referenceID": 0, "context": "(In Lee et al. (2016), the gloss is \u201ca belongs to b\u201d or \u201ca is contained in b\u201d.", "startOffset": 4, "endOffset": 22}, {"referenceID": 0, "context": "As shown in Lee et al. (2016), for most of the bAbI problem types, this is the only relation needed (they are \u201cuni-relational\u201d), so it is not necessary to encode it explicitly: all items have the same initial tensor factor @.", "startOffset": 12, "endOffset": 30}, {"referenceID": 0, "context": "This is exactly the form that transitive inference takes in Lee et al. (2016); e.", "startOffset": 60, "endOffset": 78}, {"referenceID": 0, "context": "The analyses in Lee et al. (2016) of questions in categories 2, 3 and 5 involve the binding of 3 entities rather than 2.", "startOffset": 16, "endOffset": 34}, {"referenceID": 0, "context": "(These assumptions apply to the implementation discussed in Lee et al. (2016).) Assume the generic case in which the {\u00eal} are linearly independent, and let the n-dimensional subspace of Rd that they span be E.", "startOffset": 60, "endOffset": 78}, {"referenceID": 0, "context": "In Lee et al. (2016), questions in category 5 are treated with an operation * that functions identically to \u2218, with d \u00d7 2d matrix V rather than U.", "startOffset": 3, "endOffset": 21}, {"referenceID": 0, "context": "In this sense, Path Finding is a \u201cmulti-relational\u201d problem, whereas all the simpler bAbI problem types, reducible to transitivity, are \u201cuni-relational\u201d \u2014 a main point of Lee et al. (2016).", "startOffset": 171, "endOffset": 189}, {"referenceID": 0, "context": "In the simplified approach implemented in Lee et al. (2016), a set of position vectors and direction matrices encoding the statements given in the problem is generated.", "startOffset": 42, "endOffset": 60}, {"referenceID": 0, "context": "The simplification of the full TPR reasoning analysis described above was implemented and the results reported in Lee et al. (2016) are briefly summarized in (21).", "startOffset": 114, "endOffset": 132}], "year": 2016, "abstractText": "In this paper we present the initial development of a general theory for mapping inference in predicate logic to computation over Tensor Product Representations (TPRs; Smolensky (1990), Smolensky & Legendre (2006)). After an initial brief synopsis of TPRs (Section 0), we begin with particular examples of inference with TPRs in the \u2018bAbI\u2019 question-answering task of Weston et al. (2015) (Section 1). We then present a simplification of the general analysis that suffices for the bAbI task (Section 2). Finally, we lay out the general treatment of inference over TPRs (Section 3). We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks.", "creator": "Word"}}}