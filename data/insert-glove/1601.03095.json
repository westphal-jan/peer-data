{"id": "1601.03095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Submodular Optimization under Noise", "abstract": "choto We consider chaudry the westcliff problem 3hr of maximizing deguchi monotone submodular crosswhite functions under noise, vaporization which jeweller to the longview best of our knowledge has not been leeching studied in kungliga the underexplored past. ihab There quitely has millmoor been 2000-2002 a storeyed great te'o deal nicolet of motor-paced work lettings on optimization saint-mathieu of stiffing submodular functions seamers under various constraints, with mellas many 13.000 algorithms that dyckerhoff provide openjdk desirable beimel approximation guarantees. However, 2-96 in mangurian many 2,255 applications joestar we do primaquine not have access blackground to the dlim submodular function meiwa we aim flyfishing to optimize, coverciano but bossey rather yuta to some erroneous or noisy version envelop of it. goudeau This restigouche raises the labounty question of whether rogie provable what-if guarantees strelitzia are n.i. obtainable in '51 presence of error polish-americans and desecrate noise. We sharpless provide initial answers, pebbles by crumlin focusing dadamssptimes.com on the question 74.6 of pets.com maximizing a monotone poorna submodular ni\u00e9pce function hoopeston under cardinality enchiridion constraints 1,208 when hygiea given access to .447 a noisy oracle bfn of the kremers function. We show that:", "histories": [["v1", "Tue, 12 Jan 2016 23:05:24 GMT  (691kb,D)", "https://arxiv.org/abs/1601.03095v1", null], ["v2", "Tue, 12 Apr 2016 22:24:46 GMT  (128kb,D)", "http://arxiv.org/abs/1601.03095v2", null], ["v3", "Fri, 4 Nov 2016 21:33:37 GMT  (126kb,D)", "http://arxiv.org/abs/1601.03095v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.GT", "authors": ["avinatan hassidim", "yaron singer"], "accepted": false, "id": "1601.03095"}, "pdf": {"name": "1601.03095.pdf", "metadata": {"source": "CRF", "title": "Submodular Optimization under Noise", "authors": ["Avinatan Hassidim", "Yaron Singer"], "emails": ["avinatan@cs.biu.ac.il", "yaron@seas.harvard.edu"], "sections": [{"heading": null, "text": "\u2022 For a cardinality constraint k \u2265 2, there is an approximation algorithm whose approximation ratio is arbitrarily close to 1\u2212 1/e;\n\u2022 For k = 1 there is an algorithm whose approximation ratio is arbitrarily close to 1/2. No randomized algorithm can obtain an approximation ratio better than 1/2 + o(1);\n\u2022 If the noise is adversarial, no non-trivial approximation guarantee can be obtained.\n\u2217Supported by ISF 1241/12; \u2020Supported by NSF grant CCF-1301976, CAREER CCF-1452961, Google Faculty Research Award, Facebook Faculty\nAward.\nar X\niv :1\n60 1.\n03 09\n5v 3\n[ cs\n.D S]\n4 N\nov 2\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.4 Paper organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "2 Optimization for Large k 6", "text": "2.1 The Smooth Greedy Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.1.1 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.1.2 Smoothing guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.3 Approximation guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Slick Greedy: Optimal Approximation for Sufficiently Large k . . . . . . . . . . . . . 9\n2.2.1 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.2 Generalizing guarantees of smooth greedy . . . . . . . . . . . . . . . . . . . . 10\n2.2.3 The smooth comparison procedure . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2.4 Approximation guarantee of SLICK GREEDY . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "3 Optimization for Small k 12", "text": "3.1 Combinatorial averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 The Sampled Mean Greedy Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3 Smoothing Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.4 Approximation Guarantee in Expectation . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.5 From Expectation to High Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . 15"}, {"heading": "4 Optimization for Very Small k 16", "text": "4.1 Smoothing Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.2 An Approximation Algorithm for Very Small k . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Information Theoretic Lower Bounds for Constant k . . . . . . . . . . . . . . . . . . . 16\n5 Extensions 17\n5.1 Additive Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.2 Marginal Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.3 Correlated Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.4 Information Degradation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Approximate Submodularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"}, {"heading": "6 Impossibility for Adversarial Noise 23", "text": ""}, {"heading": "7 More related work 26", "text": ""}, {"heading": "8 Acknowledgements 28", "text": "Appendices 36"}, {"heading": "A Combinatorial Smoothing 37", "text": ""}, {"heading": "B Optimization for Large k 45", "text": ""}, {"heading": "C Optimization for Small k 60", "text": ""}, {"heading": "D Optimization for Very Small k 72", "text": ""}, {"heading": "E Noise Distributions 77", "text": "F Additional Examples 79"}, {"heading": "1 Introduction", "text": "In this paper we study the effects of error and noise on submodular optimization. A function f : 2N \u2192 R defined on a ground set N of size n is submodular if for any S, T \u2286 N :\nf(S \u222a T ) \u2264 f(S) + f(T )\u2212 f(S \u2229 T )\nEquivalently, submodularity can be defined in terms of a natural diminishing returns property. For any A,B \u2286 N let fA(B) = f(A \u222aB)\u2212 f(A), then f is submodular if \u2200S \u2286 T \u2286 N, a \u2208 N \\ T :\nfS(a) \u2265 fT (a).\nIn general, submodular functions may require a representation that is exponential in the size of the ground set and the assumption is that we are given access to a value oracle which given a set S returns f(S). It is well known that submodular functions admit desirable approximation guarantees and are heavily used in applications such as market design, data mining, and machine learning (see related work). For the classic problem of maximizing a monotone (i.e. S \u2286 T =\u21d2 f(S) \u2264 f(T )) submodular function under a cardinality constraint, the greedy algorithm which iteratively adds the element with largest marginal contribution into the solution obtains a 1\u2212 1/e approximation [82] which is optimal unless using exponentially-many queries [81] or P=NP [35].\nSince submodular functions can be exponentially representative, it may be reasonable to assume that there are cases where one faces some error in their evaluation. In market design where submodular functions often model agents\u2019 valuations for goods, it seems reasonable to assume that agents do not precisely know their valuations. Even with compact representation, evaluation of a submodular function may be prone to error. In learning and sketching submodular functions, the algorithms produce an approximate version of the function [48, 8, 7, 4, 42, 43, 30, 31, 41, 44, 6].\nCan we retain desirable approximation guarantees in the presence of error?\nFor f : 2N \u2192 R and > 0 we say that f\u0303 : 2N \u2192 R is -erroneous if for every set S \u2286 N , it respects:\n(1\u2212 )f(S) \u2264 f\u0303(S) \u2264 (1 + )f(S)\nFor the canonical problem of maxS:|S|\u2264k f(S), one can trivially approximate the solution within a factor of 1\u2212 1+ using ( n k ) queries with an -erroneous oracle by simply evaluating all possible subsets and returning the best solution (according to the erroneous oracle). Is there a polynomial-time algorithm that can obtain desirable approximation guarantees for maximizing a monotone submodular function under a cardinality constraint given access to -erroneous oracles? In Appendix F we sketch an example showing that the celebrated greedy algorithm fails to obtain an approximation strictly better than O(1/k) for any constant > 0 when given access to an -erroneous oracle f\u0303 instead of f . It turns out that this is not intrinsic to greedy. No algorithm is robust to small errors.\nTheorem (6.1). No randomized algorithm can obtain an approximation strictly better than O(n\u22121/2+\u03b4) to maximizing monotone submodular functions under a cardinality constraint using en\u03b4/n queries to an -erroneous oracle, for any fixed , \u03b4 < 1/2, with high probability.\n1\nSince desirable guarantees are generally impossible with erroneous oracles, we seek natural relaxations of the problem. The first could be to consider stricter classes of functions. It is trivial to show for example, that additive functions (i.e. f(S) = \u2211 a\u2208S f(a)) allow us to obtain a 1\u2212 1+ approximation when given access to -erroneous oracles. Unfortunately, it seems like there are not many interesting classes of submodular functions that enjoy these properties. In fact, our impossibility result applies to very simple affine functions, and even coverage functions like the example in Appendix F. An alternative relaxation is to consider error models that are not necessarily adversarial.\nNoisy oracles. We can equivalently say that f\u0303 : 2N \u2192 R is -erroneous if for every S \u2286 N we have that f\u0303(S) = \u03beSf(S) for some \u03beS \u2208 [1 \u2212 , 1 + ]. The lower bound stated above applies to the case in which the error multipliers \u03beS are adversarially chosen. A natural question is whether some relaxation of the adversarial error model can lead to possibility results.\nDefinition. For a function f : 2N \u2192 R we say that f\u0303 : 2N \u2192 R is a noisy oracle if there exists some distribution D s.t. f\u0303(S) = \u03beSf(S) where \u03beS is independently drawn from D for every S \u2286 N .\nNote that the noisy oracle defined above is consistent: for any S \u2286 N the noisy oracle returns the same answer regardless of how many times it is queried. When the noisy oracle is inconsistent, mild conditions on the noise distribution allow the noise to essentially vanish after logarithmically-many queries, reducing the problem to standard submodular maximization (see e.g. [59, 91]). Consistency implies that the noise is arbitrarily correlated for a given set in different time steps, but i.i.d between different sets. In fact, we will later generalize the model to the case in which \u03beS and \u03beT are i.i.d only when S and T are sufficiently far, and arbitrarily correlated otherwise (see Section 1.3). At this point, we are interested in identifying a natural non worst-case model of corrupted or approximately submodular functions that is amendable to optimization.\nWe will be interested in a class of distributions that avoids trivialities like D \u2286 {0} and is yet general enough to contain natural distributions. In this paper we define a class which we call generalized exponential tail distributions that contains Gaussian, Exponential, and distributions with bounded support which are independent of n (o.w. optimization is impossible, see Appendix E). Note that optimization in this setting always requires that n is sufficiently large. For example, if for every S the noise is s.t. \u03beS = 2100 with probability 1/2100 and 0 otherwise, but n = 50, it is likely that the noisy oracle will always return 0, in which case we cannot do better than selecting an element at random. Throughout the paper we assume that n is sufficiently large.\nDefinition. A noise distribution D has a generalized exponential tail if there exists some x0 such that for x > x0 the probability density function \u03c1(x) = e\u2212g(x), where g(x) = \u2211 i aix\n\u03b1i . We do not assume that all the \u03b1i\u2019s are integers, but only that \u03b10 \u2265 \u03b11 \u2265 . . ., and that \u03b10 \u2265 1. If D has bounded support we only require that either it has an atom at its supremum, or that \u03c1 is continuous and non zero at the supremum.\nFor simplicity, one can always consider the special case where D \u2286 [1 \u2212 , 1 + ], which implies that two sets whose true values are close will remain close in the noisy evaluation. Even when the noise distribution is uniform in [1\u2212 , 1 + ] it is easy to show that the greedy algorithm fails (see Appendix F). The question is whether provable guarantees are achievable in this model.\n2"}, {"heading": "1.1 Main result", "text": "Our main result is that for the problem of optimizing a monotone submodular function under a cardinality constraint, near-optimal approximations are achievable under noise.\nTheorem. For any monotone submodular function there is a polynomial-time algorithm which optimizes the function under a cardinality constraint k > 2 and obtains an approximation ratio that is w.h.p arbitrarily close to 1\u2212 1/e using access to a generalized exponential tail noisy oracle of the function.\nThis proof is a summary of three results, each for a different regime of k. For any > 0 we show:\n\u2022 1 \u2212 1/e \u2212 guarantee for large k: we say that k is large when k \u2208 \u2126(log log n/ 2). For k that is sufficiently larger than log logn/ 2 we give a deterministic algorithm which obtains a (1\u2212 1/e\u2212 ) approximation guarantee w.h.p over the noise distribution;\n\u2022 1 \u2212 1/e \u2212 guarantee for small k: we say that k is small when k \u2208 O(log log n) \u2229 \u2126(1/ ). In this regime the problem is surprisingly harder. We give a different deterministic algorithm which achieves the coveted (1\u2212 1/e\u2212 ) guarantee, w.h.p. over the noise distribution;\n\u2022 Guarantees for very small k: We say that k is very small when it is an arbitrarily small constant. For this case we give a randomized algorithm whose approximation ratio is 1\u22121/k\u2212 w.h.p. over the randomization of the algorithm and the noise distribution. Note that this gives 1\u2212 1/e\u2212 for any k > 2, and 1/2\u2212 for k = 2. We also give a k/(k+ 1) approximation which holds in expectation over the randomization of the algorithm. This achieves 1 \u2212 1/e for k = 2 and 1/2 for k = 1. For k = 1 no randomized algorithm can obtain an approximation ratio better than 1/2 +O(1/ \u221a n) and (2k \u2212 1)/2k +O(1/ \u221a n) for general k.\nAt their core, the algorithms are variants of the classic greedy algorithm. In the presence of noise, greedy fails since it cannot identify the set whose value is maximal in each iteration. To handle noise, we apply a natural approach we call smoothing. In general, by selecting a family of sets H we can define a surrogate function F (S) = \u2211 H\u2032\u2208H f(S \u222a H \u2032) and its noisy analogue\nF\u0303 (S) = \u2211\nH\u2032\u2208H f\u0303(S \u222a H \u2032) which we can evaluate. Intuitively, when H is sufficiently large and chosen appropriately, submodularity and monotonicity can be used to argue that F\u0303 (S) \u2248 F (S). Thus, smoothing essentially makes the noise disappear and instead leaves us to deal with the implications of optimizing with the surrogate F rather than f . In that sense, a large part of the challenge is in using optimization over the surrogate F to approximate the optimum over f , i.e.:\n\u2022 Large k. In this regime, we first define SMOOTH-GREEDY which takes an arbitrary set H of size log log n and runs the greedy algorithm with the surrogate F\u0303 = \u2211 H\u2032\u2286H f\u0303(T \u222a H \u2032) on\nN \\H . In the analysis we show that its output together with H is arbitrarily close to 1\u2212 1/e of the optimal solution evaluated on fH (not f ). The SLICK-GREEDY algorithm runs multiple instantiations of a slightly modified version of SMOOTH-GREEDY with different smoothing sets, and obtains a guarantee arbitrarily close to 1\u2212 1/e of the true optimum;\n\u2022 Small k. In this regime, we use a modified version of greedy which adds a bundle of O(1/ ) elements in each iteration. For each such bundleB we define a surrogate F\u0303 with a smoothing\n3\nneighborhood of elements which are at distance 2 on the {0, 1}n hypercube from B. In each iteration SM-GREEDY identifies the bundleAwhich maximizes F\u0303 , but doesn\u2019t take it. Taking a random bundle A\u0302 from the smoothing neighborhood of A gives the 1\u2212 1/e guarantee but in expectation. To obtain the result w.h.p. SM-GREEDY takes the bundle A\u0302 which maximizes f\u0303(B), over all bundlesB in the smoothing neighborhood ofA. The analysis is then quite technical and strongly leverages the properties of the noise distribution and that k \u2208 O(log log n). It is for this reason it is crucial that SLICK-GREEDY applies to k \u2208 \u2126(log logn);\n\u2022 Very small k. In this case we consider bundles of size k and smoothing with singletons."}, {"heading": "1.2 Extensions", "text": "One of the appealing aspects of the noise model and the algorithms, is that they can easily be extended to a rich variety of related models. In Section 5 we discuss application to additive noise, marginal noise, correlated noise, information degradation, and approximate submodularity, ."}, {"heading": "1.3 Applications", "text": "\u2022 Optimization under noise. When considering optimization under noise, queries can be independent or correlated in time and in space. For f : 2N \u2192 R the noisy oracle is defined as f\u0303(S) = \u03beS(t)f(S) where \u03beS(t) \u223c D, for every step the oracle is queried t \u2208 N and S \u2286 N .\nDefinition. Noise is i.i.d in time if \u03beS(t) and \u03beS(t\u2032) are independent for any t 6= t\u2032 \u2208 N and S \u2286 N . Similarly, we can say that noise is i.i.d in in space if \u03beS(t) and \u03beT (t\u2032) for any S 6= T and t, t\u2032 \u2208 N. The noise distribution is correlated in time (space) if it is not independent in time (space).\nThe case in which the oracle is inconsistent is one where the noise is i.i.d in time and in space. From an algorithmic perspective this problem is largely solved, as discussed above. From Theorem 6.1 we know that there is no poly-time approximation algorithm for the case in which the errors are arbitrarily correlated in time and in space, even when the support of the noise distribution is arbitrarily small. The model we describe assumes the noise is arbitrarily correlated in time, but i.i.d in space. In Section 5 we show how one can relax this assumption. In particular, we show how to generalize the algorithms to obtain approximation ratios arbitrarily close to 1 \u2212 1/e in a noise model where \u03beS(t) and \u03beT (t\u2032) are arbitrarily correlated in time and in space for any t, t\u2032 \u2208 N and S, T for which |S4T | \u2208 O( \u221a k) when k \u2208 \u2126(log logn) and |S4T | \u2208 O(1) when k \u2208 O(log log n). To the best of our knowledge, this is the first step towards studying submodular optimization under any correlation.\n\u2022 Maximizing approximately submodular functions. There are cases where one may wish to optimize an approximately submodular function. Theorem 6.1 implies that being arbitrarily close to a submodular function is not sufficient. In statistics and learning theory, to model the fact that data is generated by a function that is approximately in a class of well behaved functions, the function generating the data f\u0303 is typically assumed to be a noisy version of a function f from a well-behaved class of functions [53, 97, 88]:\nf\u0303(x) = f(x) + \u03bex,\n4\nwhere \u03bex is an i.i.d sample drawn from some distribution D. In regression problems for instance, one assumes that the data is generated by f\u0303(x) = w\u1d40x + \u03bex. This model captures the idea that some phenomena may not exactly behave in a linear manner, but can be approximated by such a model. Making a good prediction then involves optimizing the noisy model. This therefore seems like a natural model to study approximate submodularity, especially in light of Theorem 6.1. Notice that in this case we would be interested in the optimization problem: maxS:|S|\u2264k f\u0303(S). In Section 5 we describe a black-box reduction which allows one to use the algorithms described here to get optimal guarantees.\n\u2022 Active learning. In active learning one assumes a membership oracle that can be queried to obtain labeled data [3]. In noise-robust learning, the task is to get good approximations to the noise-free target f when the examples are corrupted by some noise. In this model the assumption is that noise is consistent and i.i.d, exactly as in our model. That is, we observe f\u0303(x) + \u03bex where x is drawn i.i.d from D and multiple queries return the same answer (see e.g. [49, 55, 89, 56, 13, 40]). Our results apply to additive noise, and thus apply to active learning with noisy membership queries of submodular functions. One example application of active learning where the function is submodular is experimental design [70, 69, 54].\n\u2022 Learning and sketching. In learning and sketching the goal is to generate a surrogate function which approximates the submodular function well (see e.g. [48, 8, 7, 4, 42, 43, 30, 31, 41, 44, 6]). Theorem 6.1 implies that a surrogate which approximates a submodular function arbitrarily well may be inapproximable. Our main result shows that if when sets are sufficiently far the surrogate approximates the function via independent noise, then one can use the surrogate for optimization. This can therefore be used as a stricter benchmark for learning and sketching which allows optimizing a function learned or sketched from data."}, {"heading": "1.4 Paper organization", "text": "The main technical contribution of the paper is the algorithms for the three different regimes of k. The exposition of the algorithms is contained in sections 2, 3, and 4, which can be read independently from each other. For each algorithm, we suppress proofs and additional lemmas to the corresponding section in the appendix. All the algorithms employ smoothing arguments which can be found in Appendix A. The smoothing arguments are used as a black-box in the proofs of each algorithm, and are not required for reading the main exposition. In Section 5 we discuss extensions of the algorithms to related models. In Section 6 we prove the result for adversarial noise. Discussion about additional related work is in Section 7.\n5"}, {"heading": "2 Optimization for Large k", "text": "In this section we describe the SLICK-GREEDY algorithm whose approximation guarantee is arbitrarily close to 1 \u2212 1/e for sufficiently large k. The algorithm is deterministic and for any desired degree of accuracy > 0 can be applied when the cardinality constraint k is in \u2126(log log n/ 2), or more specifically when k \u2265 3168 log log n/ 2. We first describe and analyze the SMOOTH-GREEDY algorithm. This algorithm is then used as a subroutine by the SLICK-GREEDY algorithm."}, {"heading": "2.1 The Smooth Greedy Algorithm", "text": "We begin by describing the smoothing technique used by SMOOTH-GREEDY. We select an arbitrary set H and for a given element a, the smoothing neighborhood is simply H = {H \u2032 \u2286 H : H \u2032 \u222a a}. Throughout the rest of this section we assume that H is an arbitrary set of size `, where ` depends on k. In the case where k \u2265 2400 log nwe will use ` = 25 log n, and when k < 2400 log nwe will use ` = 33 log log n 1. The precise choice for ` will become clear later in this section. Intuitively, ` is on the one hand small enough so that we can afford to sacrifice ` elements for smoothing the noise, and on the other hand ` is large enough so that taking all its subsets gives us a large smoothing neighborhood which enables applying concentration bounds.\nDefinition. For a set S \u2286 N and some fixed set H \u2286 N of size `, we use H(1), . . . ,H(t) to denote all the subsets of H and k\u2032 = k \u2212 `. The smooth value, noisy smooth value and smooth marginal contribution are, respectively:\n(1) F (S \u222a a) := E [ f(S \u222a (H(i) \u222a a) ] =\n1\nt t\u2211 i=1 f ( S \u222a (H(i) \u222a a) ) ;\n(2) F\u0303 (S \u222a a) := E [ f\u0303(S \u222a (H(i) \u222a a) ] =\n1\nt t\u2211 i=1 f\u0303 ( S \u222a (H(i) \u222a a) ) ;\n(3) FS(a) := E [ fS((H (i) \u222a a)) ] = 1\nt t\u2211 i=1 fS ( H(i) \u222a a ) ."}, {"heading": "2.1.1 The algorithm", "text": "The smooth greedy algorithm is a variant of the standard greedy algorithm which replaces the procedure of adding argmaxa\u2208N f(S \u222a a) with its smooth analogue. The algorithm receives a set of elements H of size `, initializes S = \u2205 and at every stage adds to S the element a /\u2208 H for which the smooth noisy value F\u0303 (S \u222a a) is largest. A formal description is added below.\nOverview of the analysis. At a high level, the idea behind the analysis is to compare the performance of the solution returned by the algorithm against an optimal solution which ignores the\n1W.l.o.g. we assume that k < n \u2212 25 logn as for sufficiently large n this then implies that k \u2265 (1 \u2212 )n and by submodularity optimizing with k\u2032 = n\u2212 25 logn suffices to get the 1\u2212 1/e\u2212 guarantee for any fixed > 0.\n6\nAlgorithm 1 SMOOTH-GREEDY Input: budget k, set H\n1: S \u2190 \u2205 2: while |S| < k \u2212 |H| do 3: S \u2190 S \u222a arg maxa/\u2208H F\u0303 (S \u222a a) 4: end while 5: return S\nvalue of H and any of its partial substitutes. More specifically, let OPT denote the value of the optimal solution with k elements evaluated on f and OPTH denote the value of the optimal solution with k\u2032 = k \u2212 ` elements evaluated on fH , where fH(T ) = f(T \u222aH) \u2212 f(H). Essentially, we will show that at every step SMOOTH-GREEDY selects an element whose marginal contribution is larger than that of an element from the optimal solution evaluated on fH (we illustrate this idea in Figure 1). Together with an inductive argument this suffices for a constant factor approximation.\nRelevant iterations. One of the artifacts of noise is that our comparisons are not precise. Specifically, when we select an element that maximizes F\u0303 (S \u222a a), our smoothing guarantee will be that this element respects FS(a) \u2265 (1\u2212 \u03b4) maxb/\u2208H FS(b) for \u03b4 > 0 that depends on and k. This can be guaranteed only for an iteration where two conditions are met: (i) there is at least a single element not yet selected (and not in H) whose marginal contribution is at least /k fraction of OPTH , and (ii) OPTH is sufficiently large in comparison to OPT. We call such iterations -relevant.\nDefinition. For a given iteration of SMOOTH-GREEDY let S be the set of elements selected in previous iterations. The iteration is -relevant if (i) maxb/\u2208H fH\u222aS(b) \u2265 \u00b7OPTHk and (ii) OPTH \u2265 OPT e .\nWe will analyze SMOOTH-GREEDY in the case where the iterations are -relevant as it allows applying the smoothing arguments. In the analysis we will then ignore iterations that are not -relevant at the expense of a negligible loss in the approximation guarantee. The main steps are:\n1. In Lemma 2.1 we show that in each -relevant iteration the (non-noisy) smooth marginal contribution of the element selected in that iteration by the algorithm is w.h.p. an arbitrarily good approximation to maxb/\u2208H FS(b). To do so we need claims B.1, B.2 and B.3;\n2. Next, in Claim 2.3 we show that the element a whose smooth marginal contribution FS(a) is maximal has true marginal contribution fS(a) that is roughly a k\u2032th fraction of the marginal contribution of the optimal solution over fH ;\n3. Finally, in Lemma 2.4 we apply a standard inductive argument to show that the fact that the algorithm selects an element with large smooth value in each step results in an approximation arbitrarily close to 1\u2212 1/e to OPTH (not OPT). In Corollary B.4 we show that the bound against OPTH can already be used to give a constant factor approximation to OPT. To get arbitrarily close to 1\u2212 1/e, SLICK-GREEDY executes multiple instantiations of a generalization of SMOOTH-GREEDY as later described in Section 2.2.\n7"}, {"heading": "2.1.2 Smoothing guarantees", "text": "The first step is to prove Lemma 2.1. This lemma shows that at every step as SMOOTH-GREEDY adds the element that maximizes the noisy value argmaxa/\u2208H F\u0303 (S \u222a a), that element nearly maximizes the (non-noisy) smooth marginal contribution FS , with high probability.\nLemma 2.1. For any fixed > 0, consider an -relevant iteration of SMOOTH-GREEDY where S is the set of elements selected in previous iterations and a \u2208 arg maxb/\u2208H F\u0303 (S \u222a b). Then for \u03b4 = 2/4k and sufficiently large n we have that w.p. \u2265 1\u2212 1/n4:\nFS(a) \u2265 (1\u2212 \u03b4) max b/\u2208H FS(b).\nTo prove the above lemma we use claims B.1, B.2, and B.3. The statements and proofs can be found in Appendix B and are best understood after reading the smoothing section in Appendix A."}, {"heading": "2.1.3 Approximation guarantee", "text": "Lemma 2.1 lets us forget about noise, at least for the remainder of the analysis of SMOOTH-GREEDY. We can now focus on the consequences of selecting an element a which (up to factor 1\u2212 \u03b4) maximizes FS rather than the true marginal contribution fS .\nClaim 2.2. For any > 0, let \u03b4 \u2264 2/4k. Suppose that the iteration is -relevant and let b? \u2208 argmaxb/\u2208H fH\u222aS(b). If FS(a) \u2265 (1\u2212 \u03b4)FS(b?), then:\nfS(a) \u2265 (1\u2212 )fH\u222aS(b?).\nThe principle is similar to Claim B.1. In this version we have a weaker condition since FS(a) is not greater than FS(b?) but rather (1 \u2212 \u03b4)FS(b?), but the claim is less general as it only needs to hold for b?. We therefore use a slightly different approach to prove this claim (see Appendix B).\n8\nClaim 2.3. For any fixed > 0, consider an -relevant iteration of SMOOTH-GREEDY with S as the elements selected in previous iterations. Let a \u2208 argmaxb/\u2208H F\u0303 (S \u222a b). Then, w.p. \u2265 1\u2212 1/n4:\nfS(a) \u2265 ( 1\u2212 )[ 1\nk\u2032\n( OPTH \u2212 f(S) )] .\nThe proof is in Appendix B. We can now state the main lemma of this subsection.\nLemma 2.4. Let S be the set returned by SMOOTH-GREEDY and H its smoothing set. Then, for any fixed > 0 when k \u2265 3`/ with probability of at least 1\u2212 1/n3 we have that:\nf(S \u222aH) \u2265 (1\u2212 1/e\u2212 /3)OPTH .\nTo prove the lemma we show that if OPTH < OPT/e then H alone provides the approximation guarantee. Otherwise we can apply Claim 2.3 using a standard inductive argument to show that S \u222aH provides the approximation. The subtle yet crucial aspect of the proof is that the inductive argument is applied to analyze the quality of the solution against the optimal solution for fH and not against the optimal solution on f . The proof is in Appendix B.\nAs we will soon see, Lemma 2.4 plays a key role in the analysis of the SLICK-GREEDY algorithm. It is worth noting that this lemma can also be used to show that SMOOTH-GREEDY alone provides a constant (\u2248 0.387) albeit suboptimal approximation guarantee (Corollary B.4)."}, {"heading": "2.2 Slick Greedy: Optimal Approximation for Sufficiently Large k", "text": "The reason SMOOTH-GREEDY cannot obtain an approximation arbitrarily close to 1 \u2212 1/e is due to the fact that a substantial portion of the optimal solution\u2019s value may be attributed to H . This would be resolved if we had a way to guarantee that the contribution of H is small. The idea behind SLICK-GREEDY is to obtain this type of guarantee. Intuitively, by running a large albeit constant number of instances of SMOOTH-GREEDY with different smoothing sets, selecting the \u201cbest\u201d solution will ensure the contribution of the smoothing set is relatively minor."}, {"heading": "2.2.1 The algorithm", "text": "We can now describe the SLICK-GREEDY algorithm which is the main result of this section. Given a constant > 0 we set \u03b4 = /6 and generate arbitrary sets H1, . . . ,H1/\u03b4, each of size ` s.t. Hi \u2229 Hj = \u2205 for every i, j \u2208 [1/\u03b4]. We then run a modified version of SMOOTH-GREEDY 1/\u03b4 times: in each iteration j we initialize SMOOTH-GREEDY with Rj = \u222ai 6=jHi 2 and use Hj to generate the smoothing neighborhood. We denote this as SMOOTH-GREEDY(k,Rj , Hj). We then compare the solution Tj = Sj\u222aHj to the best Ti = Si\u222aHi we\u2019ve seen so far using a procedure we call SMOOTHCOMPARE described below. The SMOOTH-COMPARE procedure compares Ti and Tj by using a set Hij s.t. Hij \u2229 (Tj \u222aTi) = \u2205 and |Hij | = `. If Ti wins, the procedure returns Ti and otherwise returns Tj . The SLICK-GREEDY then returns the set Ti that survived the SMOOTH-COMPARE tournament.\n2By initializing the SMOOTH-GREEDY with Rj we mean that the first iteration begins with S = Rj rather than S = \u2205 and following the initialization the algorithm greedily adds k \u2212 |Rj | \u2212 |Hj | elements.\n9\nAlgorithm 2 SLICK-GREEDY Input: budget k\n1: Select `/\u03b4 elements in N and partition them into disjoint sets of equal size H1 . . . , H1/\u03b4 2: Ti \u2190 \u2205 3: for j \u2208 [1/\u03b4] do 4: Rj \u2190 \u222ai 6=jHi 5: Tj \u2190 SMOOTH-GREEDY(k,Rj , Hj) \u222aHj 6: Hij \u2190 arbitrary set of ` elements disjoint from Ti \u222a Tj 7: Ti \u2190 SMOOTH-COMPARE({Ti, Tj}, Hij) 8: end for 9: return Ti\nOverview of the analysis. Consider the smoothing sets H1, . . . ,H1/\u03b4. Let Hl be the smoothing set whose marginal contribution to the others is minimal, i.e. Hl \u2208 argmini\u2208[1/\u03b4] fRi(Hi). Notice that from submodularity we are guaranteed that fRl(Hl) \u2264 \u03b4f(Rl \u222aHl). In this case, the fact that the marginal contribution of Hl to the rest of the smoothing sets Rl is small, together with the fact that the solution is initialized with Rl, enables the tight analysis. The two main steps are:\n1. In Lemma 2.5 we show that w.h.p. Tl provides an approximation arbitrarily close to (1\u22121/e). Intuitively, this happens since the marginal contribution of Hl to the rest of the smoothing sets Rl = \u222aiHi \\ Hl is small, and since the solution to SMOOTH-GREEDY is initialized with Rl, losing the value of Hl is negligible. The proof relies on Claim B.5 and Lemma B.7 that generalize the guarantees of SMOOTH-GREEDY to the case it is initialized (see Appendix);\n2. We then describe and analyze the SMOOTH-COMPARE procedure. In the absence of noise, one can simply select the set whose value is largest. To overcome noise, we run a tournament to extract the solution whose value is approximately largest, or at least arbitrarily close to (1\u2212 1/e)OPT. Specifically, we prove that w.h.p. the set Ti that wins the SMOOTH-COMPARE tournament (i.e. the set Ti returned by SLICK-GREEDY) satisfies f(Ti) \u2265 (1\u2212 /3) min{f(Tl), (1\u2212 1/e\u2212 2 /3)OPT}. Since f(Tl) is arbitrarily close to (1\u2212 1/e)OPT, this concludes the proof."}, {"heading": "2.2.2 Generalizing guarantees of smooth greedy", "text": "Lemma 2.5. Let Sl be the set returned by SMOOTH-GREEDY that is initialized withRl andHl its smoothing set. Then, for any fixed > 0 when k \u2265 36`/ 2 w.p. at least 1\u2212 1/n3 we have that:\nf(Sl \u222aHl) \u2265 (1\u2212 1/e\u2212 2 /3)OPT."}, {"heading": "2.2.3 The smooth comparison procedure", "text": "We can now describe the SMOOTH-COMPARE procedure we use in the algorithm. For a given set Hij \u2286 N of size ` and two sets Ti, Tj \u2286 N \\ Hij , we compare f\u0303(Ti \u222a H \u2032ij) with f\u0303(Tj \u222a H \u2032ij) for all H \u2032ij \u2282 Hij . We select Ti if in the majority of the comparisons with H \u2032ij \u2282 Hij (breaking ties lexicographically) we have that f\u0303(Ti \u222aH \u2032ij) \u2265 f\u0303(Tj \u222aH \u2032ij), and otherwise we select Tj .\n10\nAlgorithm 3 SMOOTH-COMPARE Input: Ti, Tj , Hij \u2286 N \\ (Ti \u222a Tj),\n1: Compare f\u0303(Ti \u222aH \u2032ij) with f\u0303(Tj \u222aH \u2032ij) for all H \u2032ij \u2282 Hij 2: if Ti won the majority of comparisons return Ti otherwise return Tj\nLemma 2.6. Assume k \u2265 96`/ 2. Let Ti be the set that won the SMOOTH-COMPARE tournament. Then, with probability at least 1\u2212 1/n2:\nf(Ti) \u2265 (\n1\u2212 3\n) min {( 1\u2212 1\ne \u2212 2 3\n) OPT, max\nj\u2208[1/\u03b4] f(Tj)\n}\nThe proof of this lemma has two parts.\n1. First we show in Claim B.8 that if a set Ti has moderately larger value than another set Tj (more specifically, if the gap is 1 \u2212 \u03b4/3) then as long as f(Tj) is not arbitrarily close to (1\u22121/e)OPT then f(Ti\u222aH \u2032ij) is larger than f(Tj\u222aH \u2032ij), for anyH \u2032ij \u2286 Hij . At a high level, this is because elements inH \u2032ij are candidates for SMOOTH-GREEDY and the fact that they are not selected indicates that their marginal contribution to Tj = Sj \u222aHj is low. Thus, elements in H \u2032ij cannot add much value, and since |Hij | k adding subsets of Hij does not distort the comparison by much. If f(Tj) is arbitrarily close to (1\u2212 1/e)OPT, we may have that Tj beats Ti, but this would still ultimately result in an approximation arbitrarily close to 1\u2212 1/e;\n2. The next step (Claim B.9) then shows that if for every H \u2032ij we have f(Ti \u222aH \u2032ij) \u2265 f(Tj \u222aH \u2032ij) then with high probability Ti wins the comparison against Tj in SMOOTH-COMPARE.\nUsing these two parts we then conclude since we are running the SMOOTH-COMPARE tournament between 1/\u03b4 sets, the winner is an (1 \u2212 \u03b4/3)1/\u03b4 \u2265 (1 \u2212 /3) approximation to the competing set with the highest value or a set whose approximation is arbitrarily close to 1\u2212 1/e. The claims and proofs can be found in Appendix B."}, {"heading": "2.2.4 Approximation guarantee of SLICK GREEDY", "text": "Finally, putting everything together, we can prove the main result of this section (see Appendix ??).\nTheorem 2.1. Let f : 2N \u2192 R be a monotone submodular function. For any fixed > 0, when k \u2265 3168 log log n/ 2, then given access to a noisy oracle whose noise distribution has a generalized exponential tail, the SLICK-GREEDY algorithm returns a set which is a (1\u22121/e\u2212 ) approximation to maxS:|S|\u2264k f(S), with probability at least 1\u2212 1/n.\n11"}, {"heading": "3 Optimization for Small k", "text": "When k is small we cannot use the smoothing technique from the previous section, since it requires including the smoothing set of size \u0398(log log n) in the solution. In this section we describe the sampled mean method which can be applied to k \u2208 \u2126(1/ ) \u2229O(log log n) and results in a 1\u2212 1/e\u2212 approximation. This result is obtained by applying a greedy algorithm on a surrogate function F : 2N \u2192 R+ which is what we call the sampled mean of f . The use of the surrogate function makes it relatively easy to obtain the 1 \u2212 1/e \u2212 approximation, albeit in expectation. The main technical challenge is the transition from a guarantee that holds in expectation to one that holds with high probability. This difficulty is what limits this method to be applicable only when k ranges between \u2126(1/ ) and O(log log n), and heavily exploits the generalized exponential tail property."}, {"heading": "3.1 Combinatorial averaging", "text": "The sampled-mean method is based on averaging sets to find elements whose marginal contribution is high, which can then be greedily added to the solution. The intuition for this method comes from continuous optimization. Consider optimizing a function f : Rn \u2192 R given access to a noisy value oracle f\u0303 : Rn \u2192 R which for each point x \u2208 Rn returns f\u0303(x) = \u03bexf(x) where \u03bex \u223c D. A natural approach would be to sample t points x1, . . . ,xt from an -ball B around x, for some small > 0, and estimate the value of x using the sampled mean:\nF\u0303 (x) := E [ f\u0303(x) ] = 1\nt \u2211 xi\u223cB f\u0303(xi)\nUnder some smoothness assumptions on f , for sufficiently large t and small , concentration bounds kick in, and one can apply an optimization algorithm on F\u0303 to optimize f . The method in this section translates this idea to a combinatorial domain. To do so effectively, rather than considering singletons a \u2208 N we obtain multidimensionality by considering bundles of size c \u2208 O(1/ ).\nDefinition. Let f : 2N \u2192 R. For a set S \u2286 N and bundle A \u2286 N of fixed size c, we define Aij := (A \\ {ai}) \u222a {aj} for ai \u2208 A and aj /\u2208 S \u222a A, and t = c(n \u2212 c \u2212 |S|). The mean value, noisy mean value, and mean marginal contribution of A given S are, respectively:\n(1) F (S \u222aA) := E [f(S \u222aAij)] = 1\nt \u2211 i\u2208A \u2211 j /\u2208S\u222aA f(S \u222aAij);\n(2) F\u0303 (S \u222aA) := E [ f\u0303(S \u222aAij) ] =\n1\nt \u2211 i\u2208A \u2211 j /\u2208S\u222aA f\u0303(S \u222aAij);\n(3) FS(A) := E [fS(Aij)] = 1\nt \u2211 i\u2208A \u2211 j /\u2208S\u222aA fS(Aij).\nThe above definition mimics the continuous case by considering a bundle of elements A of fixed size c (we will use c \u2248 1/ ) as a point, and the points in the -ball are modeled by all the sets Aij obtained by replacing an element from A with an element from N \\ (S \u222aA). We illustrate this idea in Figure 2. Although the combinatorial analogue is not as well-behaved as the continuous case, the sampled mean approach defined here extracts some of its desirable properties.\n12"}, {"heading": "3.2 The Sampled Mean Greedy Algorithm", "text": "The SM-GREEDY begins with the empty set S and at every iteration considers all bundles of size c \u2208 O(1/ ) to add to S. At every iteration, the algorithm first identifies the bundle A which maximizes the noisy mean value. After identifying A, it then considers all possible bundles Aij and takes the one whose noisy mean value is largest. We describe the algorithm formally below.\nAlgorithm 4 SM-GREEDY Input: budget k, precision > 0, c \u2208 O(1 )\n1: S \u2190 \u2205 2: while |S| < c \u00b7 \u230a k c \u230b do 3: A\u2190 argmaxB:|B|=c F\u0303 (S \u222aB) 4: S \u2190 S \u222a arg maxi\u2208A,j /\u2208S\u222aA f\u0303(S \u222aAij) 5: end while 6: return S\nAt a high level, the major steps in the analysis can be described as follows.\n1. We begin with smoothing guarantees. In Lemma 3.2 we apply Lemma 3.1 as well as other arguments to show that w.h.p. in each iterationA \u2208 argmaxB:|B|=c F\u0303 (S\u222aB) well approximates the bundle with maximal (non-noisy) mean marginal contribution argmaxB:|B|=c FS(B);\n2. Lemma 3.3 argues that if the marginal contribution fS(A\u0302) of the set A\u0302we select at every iteration is close to the mean marginal contribution FS(A) we obtain an approximation arbitrarily close to 1\u2212 1/e. This suffices for an approximation guarantee that holds in expectation;\n3. The last step is Lemma 3.4 which is the technical crux of this section. We show that taking A\u0302 \u2208 argmaxi,j f\u0303(S\u222aAij) in line 4 of the algorithm gives us, with sufficiently high probability that the marginal contribution fS(A\u0302) is arbitrarily close to the mean marginal contribution FS(A). We can therefore invoke Lemma 3.3 and recover the optimal approximation guarantee."}, {"heading": "3.3 Smoothing Guarantees", "text": "We first show that the largest marginal contribution is well approximated by its mean contribution.\nLemma 3.1. For any > 0 and any set S \u2282 N , let A? \u2208 arg maxA:|A|=1/ fS(A). Then:\n(1\u2212 ) fS(A?) \u2264 FS(A?) \u2264 fS(A?).\nThe proof is in Appendix C and exploits a natural property of submodular functions: the removal of a random element from a large set does not significantly affect its value, in expectation.\nSignificant iterations. Similar to the previous section, we define an assumption on the iterations of the algorithm which allows us to employ the smoothing technique in this section.\n13\nDefinition. LetB \u2208 argmaxB:|B|=c fS(B). An iteration of SM-GREEDY is -significant if for the given set S selected before the iteration we have that fS(B) \u2265 \u00b7c\u00b7OPTk .\nThe following lemma implies that at every step we add a bundle whose smooth marginal contribution is comparable with the largest smooth marginal contribution obtainable.\nLemma 3.2. LetA \u2208 argmaxB:|B|=c F\u0303 (S\u222aB) where c \u2265 16 , and assume that the iteration is 4 -significant. Then, with probability at least 1\u2212 e\u2212\u2126(n1/10) we have that:\nFS(A) \u2265 (1\u2212 ) max B:|B|=c FS(B).\nThe proof relies on arguments from the smoothing framework (Appendix A). In this case, the application of smoothing is a bit subtle as we do not apply smoothing on the noisy version of F directly. The proof uses Lemma 3.1 above as well as Claim C.2 which bounds the variation in values of sets A?ij , when A ? \u2208 argmaxB:|B|=c fS(B). Details and proofs are in Appendix C."}, {"heading": "3.4 Approximation Guarantee in Expectation", "text": "Lemma 3.3. Let \u03b4 > 0 and assume k > 16/\u03b42, c = 16/\u03b4. Suppose that in every \u03b4/4-significant iteration of SM-GREEDY when S are the elements selected in previous iterations, A \u2208 argmaxB:|B|=c F\u0303 (S \u222a B), the bundle added A\u0302 respects fS(A\u0302) \u2265 (1\u2212 \u03b4)FS(A). Let S\u0304 be the solution after bk/cc iterations. Then, w.p. \u2265 1\u2212 1/n2:\nf(S\u0304) = (1\u2212 1/e\u2212 5\u03b4)OPT.\nThis lemma implicitly proves an approximation guarantee that holds in expectation. This is simply because we know that if we choose A\u0302 = A \\ {ai} \u222a {aj} uniformly at random over all choices of i \u2208 [c], aj /\u2208 S \u222a A we get E[fS(A\u0302)] = FS(A) > (1 \u2212 \u03b4)FS(A) in every iteration, and thus by Lemma 3.3 we would be arbitrarily close to 1\u2212 1/e, in expectation over all our choices.\n14"}, {"heading": "3.5 From Expectation to High Probability", "text": "From Lemma 3.2 we know that A \u2208 argmaxB:|B|=c F\u0303 (S \u222a B) has mean marginal contribution arbitrarily close to maxB:|B|=c FS(B), but for Lemma 3.3 to hold we need the true marginal contribution fS(A\u0302) to be arbitrarily close to maxB:|B|=c FS(B). Simply adding A can easily lead to an arbitrarily bad approximation (see Appendix F ). In order to prove that SM-GREEDY provides the desired approximation guarantee, we need to show that when A\u0302 \u2208 argmaxi\u2208[c],j /\u2208S\u222aA f\u0303(S \u222a Aij) then with sufficiently high probability fS(A\u0302) is arbitrarily close toFS(A) as required by Lemma 3.3.\nHigh-level overview to show high probability guarantee. Let A? \u2208 argmaxB:|B|=c fS(B) and A \u2208 argmaxB:|B|=c F\u0303 (S \u222a B). We will define two kinds of sets in {Aij}i\u2208[c],j /\u2208S\u222aA, called good and bad. A good set is a set G for which fS(G) \u2265 (1 \u2212 2 )fS(A?) and a bad set is a set B for which fS(B) \u2264 (1\u2212 3 )fS(A?). Our goal is to prove argmax{f\u0303(S \u222aAij) : ai \u2208 A, aj /\u2208 S \u222aA} is w.h.p. not bad. Doing so implies that in every iteration w.h.p. we add a bundle whose true marginal value is at least (1\u2212 3 ) of fS(A?) which is an upper bound on maxB:|B|=c FS(B) (and thus also on FS(A)). Lemma 3.4. For any > 0, suppose we run SM-GREEDY where in each iteration we add a bundle of size c = 16/ . For any /8-significant iteration where the set previously selected is S : |S| \u2208 O(log log n), let A \u2208 argmax F\u0303 (S \u222aA) and A\u0302 = argmax(i,j)\u2208A\u00d7N\\S\u222aA f\u0303(S \u222aAij). Then, w.p. \u2265 1\u2212 3/ log n we have:\nfS(A\u0302) \u2265 (1\u2212 3 )FS(A).\nAt a high level, the proof follows the following steps:\n1. In Claim C.4 we show that for A \u2208 argmaxB:|B|=c F\u0303 (S \u222a B), at least half of the sets in {Aij}i\u2208A,j /\u2208S\u222aA are good, and at most half are bad;\n2. Next, we define two thresholds: \u03b8g and \u03b8b. Intuitively, \u03b8g is a lower bound on the maximum of noise multipliers from the good sets, and \u03b8b is an upper bound on the maximum of noise multipliers from bad sets. We then show in Lemma C.8 that \u03b8g \u2265 (1 \u2212 \u03b3) \u03b8b, for any \u03b3 = \u2126(1/ log logn). This lemma is quite technical, and it is where we fully leverage the property of the generalized exponential tail distribution and the fact that k \u2208 O(log log n);\n3. From \u03b8g \u2265 (1 \u2212 \u03b3) \u03b8b and Claim C.4 we can prove that w.h.p. there is at least one good set whose noisy value is sufficiently larger than the noisy value of a bad set. The fact that a bad set loses to a good set implies that the value of the set we end up selecting must at least be as high as that of a bad set, i.e. fS(A\u0302) \u2265 (1\u2212 3 )fS(A?). Notice that by definition fS(A?) is an upper bound on FS(B) for any bundle B of size c which therefore completes the proof.\nLemma 3.4 above essentially tells us that at every iteration we select the bundle whose marginal contribution is almost maximal. Together with previous arguments from this section, this proves our main theorem for the case in which k \u2208 \u2126(1/ 2) \u2229O(log log n). For k \u2208 \u2126(1 ) \u2229O( 1 2\n) we run a single iteration of SM-GREEDY with c = k (o.w. the approximation is \u2248 1/2, when k = 2c\u2212 1). Theorem 3.5. For any monotone submodular function f : 2N \u2192 R and > 0, when k \u2208 \u2126(1/ ) \u2229O(log log n), there is a (1 \u2212 1/e \u2212 ) approximation for maxS:|S|\u2264k f(S), with probability 1\u2212 4/ log n given access to a noisy oracle whose distribution has a generalized exponential tail.\n15"}, {"heading": "4 Optimization for Very Small k", "text": "The smoothing guarantee from the previous section actually necessitates selecting bundles of size c \u2208 \u0398(1/ ) and does not apply to very small values of k \u2208 O(1/ )3. For small constants we propose a different algorithm that uses a different smoothing technique. The algorithm is simple and applies the same principles as the ones from the previous section. We show that this simple algorithm obtains an approximation ratio arbitrarily close to 1 \u2212 1/e w.h.p. when k > 2 and in expectation when k = 2. For k = 1 we get arbitrarily close to 1/2, which is tight. We show lower bounds for small values of k and in particular when k = 1 show that no algorithm can obtain an expected approximation ratio better than 1/2 + o(1). All proofs and details are in Appendix D."}, {"heading": "4.1 Smoothing Guarantees", "text": "The smoothing here is straightforward. For every set A consider the smoothing neighborhood H(A) = {A \u222a x : x /\u2208 A}, F (A) = EX\u2208H(A)[f(X)] and F\u0303 (A) = EX\u2208H(A)[f\u0303(X)].\nLemma 4.1. Let A \u2208 argmaxB:|B|=k F\u0303 (B). Then, for any fixed > 0 w.p. 1\u2212 e\u2212\u2126( 2(n\u2212k)):\nF (A) \u2265 (1\u2212 ) max B:|B|=k F (B)."}, {"heading": "4.2 An Approximation Algorithm for Very Small k", "text": "Approximation guarantee in expectation. The algorithm will simply select the set A\u0302 to be a random set of k elements from a random set ofH(A) whereA \u2208 argmaxB:|B|=k F\u0303 (B). For any constant k and any fixed > 0 this is a (k/(k + 1)\u2212 ) approximation in expectation (see Theorem D.1).\nHigh probability. To obtain a result that holds w.h.p. we will consider a modest variant of the algorithm above. The algorithm enumerates all possible subsets of size k\u22121, and identifies the set A \u2208 argmaxB:|B|=k\u22121 F\u0303 (B). The algorithm then returns A\u0302 \u2208 argmaxX\u2208H(A) f\u0303(X).\nTheorem 4.2. For any submodular function f : 2N \u2192 R and any fixed > 0 and constant k, there is a (1\u2212 1/k \u2212 )-approximation algorithm for maxS:|S|\u2264k f(S) which only uses a generalized exponential tail noisy oracle, and succeeds with probability at least 1\u2212 6/ log n."}, {"heading": "4.3 Information Theoretic Lower Bounds for Constant k", "text": "Surprisingly, even for k = 1 no algorithm can obtain an approximation better than 1/2, which proves a separation between large and small k. In Claim D.2 we show no randomized algorithm with a noisy oracle can obtain an approximation better than 1/2 +O(1/ \u221a n) for maxa\u2208N f(a), and in Claim D.3 approximation better than (2k \u2212 1)/2k +O(1/ \u221a n) for the optimal set of size k.\n3The dependency on originates in Claim C.2 where we bound on the variation of c\u22121 setsA-i, and thus smoothing depends on c \u2265 4/ .\n16"}, {"heading": "5 Extensions", "text": "In this section we consider extensions of the optimization under noise model. In particular, we show that the algorithms can be applied to several related problems: additive noise, marginal noise, correlated noise, degradation of information, and approximate submodularity."}, {"heading": "5.1 Additive Noise", "text": "Throughout this paper we assumed the noise is multiplicative, i.e. we defined the noisy oracle to return f\u0303(S) = \u03beS \u00b7f(S). An alternative model is one where the noise is additive, i.e. f\u0303(S) = f(S)+\u03beS , where \u03beS \u223c D. The impossibility results for adversarial noise apply to the additive case as well.\nFrom a modeling perspective, the fact that the noise may be independent of the value of the set queried may be an advantage or a disadvantage, depending on the setting. From a technical perspective, the problem remains non-trivial. Fortunately, all the algorithms described above apply to the additive noise model, modulo the smoothing arguments which become straightforward. That is, we still need to apply smoothing on the surrogate functions, but it is easy to show arguments likeA \u2208 argmaxB F\u0303 (S\u222aB) implies w.h.p. FS(A) \u2265 (1\u2212\u03b4) maxb FS(B). In the additive noise model:\nF\u0303 (S \u222aA) = \u2211\nX\u2208H(A)\nf\u0303(S \u222aX) = \u2211\nX\u2208H(A)\n(f(S \u222aX) + \u03beS\u222aX) = \u2211\nX\u2208H(A)\nf(S \u222aX) + \u2211\nX\u2208H(X)\n\u03beS\u222aX\nThus, by applying a concentration bound we can show that a setAwhose smooth value is maximal implies that its non-noisy smooth marginal contribution FS(A) is approximately maximal as well."}, {"heading": "5.2 Marginal Noise", "text": "An alternative noise model is one where the noise acts on the marginals of the distribution. In this model, a query to the oracle is a pair of sets S, T \u2286 N and the oracle returns \u03beS,T \u00b7 fS(T ) in the multiplicative marginal noise model and fS(T ) + \u03beS,T in the additive marginal noise model.\nAdversarial additive marginal noise is generally impossible. If the error is adversarial, and the noise is additive, the lower bound of 6.1 follows for any magnitude of the noise. Letting denote the maximal magnitude of the noise, we consider a function in which no element ever gives a contribution higher than , and then getting marginal information does not help.\nAdversarial multiplicative marginal noise is approximable. If the marginal error is adversarial but multiplicative within factor \u03b1, it is well known one can obtain a 1\u2212 1/e\u03b1 approximation.\nMarginal i.i.d noise is approximable. If one is allowed to query the oracle on any two sets S, T and get \u03beS,T \u00b7fS(T ) (or fS(T )+\u03beS,T ) where \u03beS,T is drawn i.i.d for any pair S, T , then one can simply apply all the algorithms and analysis as is, by always considering f\u2205(S \u222aT ). If one is only allowed\n17\nto query S, T where |T | = 1, the algorithms still work, but we need to be careful with the analysis, since we need to show that we are calling the oracle on different sets. It is easy to show that if the noise is weak and multiplicative (e.g. \u03be \u2208 [1\u2212 , 1+ ]) we can obtain a (1\u22121/e\u2212 ) approximation."}, {"heading": "5.3 Correlated Noise", "text": "As discussed in the Introduction, Theorem 6.1 implies that no algorithm can optimize a monotone submodular function under a cardinality constraint given access to a noisy oracle whose noise multipliers are arbitrarily correlated across sets, even when the support of the distribution is arbitrarily small. In light of this, one may wish to consider special cases of correlated distributions. We first show that even very simple correlations can result in inapproxiability. We then show an interesting class of distributions we call d-correlated, for which optimal guarantees are obtainable.\nImpossibility result for correlated distributions. Having taken the first step showing algorithms for the i.i.d. in space model, a natural question is whether this assumption is necessary.\nTheorem 5.1. Even for unit demand functions there are simple space-correlated distributions for which no algorithm can achieve an approximation strictly better than 1/n.\nProof. Consider a unit demand function f(S) = maxa\u2208S f(a) which operates on a ground set with n elements. There are n \u2212 1 regular elements and one special element a?. The value of f on any regular element is 1, but f(a?) = M for some arbitrarily large M . The noise distribution is such that it returns 1 on sets which do not contain a?, and 1/M on sets that contain a?. The best one can do in this case is to choose a random element without querying the oracle at all.\nGuarantees for d-correlated distributions. Our algorithms can be extended to a model in which querying similar sets may return results that are arbitrarily correlated, as long as querying sets which are sufficiently far from each other gives independent answers.\nDefinition. We say that the noise distribution is d-correlated if for any two sets S and T , such that |S \\ T |+ |T \\ S| > d we have that the noise is applied independently to S and to T .\nNotice that if a distribution is d-correlated, any two points on the hypercube at distance at most d can be arbitrarily correlated. For this model we show that when k \u2208 \u2126(log logn) then we can obtain an approximation arbitrarily close to 1 \u2212 1/e for O( \u221a k)-correlated distributions. Alternatively, in this regime we can get this approximation guarantee for any distribution that is arbitrarily correlated when querying two sets S, T whose symmetric difference is larger than \u221a max{|T |, |S|}. When k \u2208 \u2126(log logn) we can get arbitrarily close to 1\u2212 1/e for O(1)-correlated noise.\nModification of algorithms for large k for \u221a k-correlated noise. For large k, if we have that k d2, then the approximation guarantee we get is still arbitrarily close to 1 \u2212 1/e even when D is d-correlated. To do this, we modify the smoothing neighborhood and the definition of smooth\n18\nvalues as follows. Recall that in SMOOTH-GREEDY, we select an arbitrary set of elements H of size ` for smoothing, and compute the noisy smooth value of S \u222a a by averaging all subsets of H :\nF\u0303 (S \u222a a) = 1 2` \u2211 H\u2032\u2282H f\u0303 ( S \u222a ( a \u222aH \u2032 )) .\nIn the d-correlated case, for each 1 \u2264 i \u2264 d and 1 \u2264 j \u2264 ` we choose a bundle h(i)j of d elements, such that every two bundles are disjoint. Denote H(i) = {h(i)1, . . . h(i)`, and H = di,jh(i)j the set of all elements we used. The noisy smooth value with smoothing set H(i) is now:\nF\u0303 (i)(S \u222a a) = 1 2` \u2211 H\u2032\u2282H(i) f\u0303(S \u222a a \u222aH \u2032)\nwhere we abuse notation and use S \u222a a \u222aH \u2032 instead of S \u222a {a} \u222ah(i)j\u2208H\u2032 h(i)j .\nWe will run SMOOTH-GREEDY with the smoothing sets H(1), . . . ,H(d), where in each iteration i mod d we use H(i) as the smoothing set. Exactly as in the original algorithm, we generate S by iteratively adding k\u2212|H| elements from N \\H that maximize the smooth value in every iteration, and we then return S \u222aH . As before, SLICK- GREEDY employs SMOOTH-GREEDY.\nTo prove correctness of the algorithm we need to show that the evaluations of the surrogate functions are independent. We will first show by induction on |S| that between iterations, the oracle calls are independent.\nClaim 5.2. Any oracle call at iteration i is independent of any previous oracle call at iteration r < i.\nProof. Let S(i) be the set of elements we have already committed to in stage i. Consider an evaluation of f\u0303(S(i) \u222a a \u222a H \u2032) for some non empty H \u2032 \u2282 H(i mod d) at iteration i, and an oracle evaluation f\u0303(S(r)\u222ab\u222aH \u2032\u2032) made at some iteration r < swith some non emptyH \u2032\u2032 \u2282 H(r mod d) and b /\u2208 S(r) \u222a H . If r \u2264 i \u2212 d, then the symmetric difference between S(i) \u222a a and S(r) \u222a b is at least of size d. Since a, b /\u2208 H , and S(i) \u2229 H = \u2205, this means that the symmetric difference of S(i) \u222a a \u222aH \u2032 and S(r) \u222a b \u222aH \u2032\u2032 is at least of size d, for any H \u2032\u2032 \u2282 H(r mod d), and thus the calls are independent. If r > s\u2212 d, then i mod d 6= r mod d, and hence S(i)\u222a a\u222aH \u2032 and S(r)\u222a b\u222aH \u2032\u2032 are independent because of the symmetric difference between H \u2032 and H \u2032\u2032.\nClaim 5.3. When evaluating F\u0303 (i)(S \u222a a), all noise multipliers are independent.\nProof. When evaluating F\u0303 (i)(S \u222a a) we call the noisy oracle on sets of the form S \u222a a \u222a H \u2032. Since each H \u2032 corresponds to a different subset of H(i), and H(i) is a collection of ` bundles of size d, the symmetric difference between every two sets H \u2032, H \u2032\u2032 \u2286 H(i), is at least d.\nAs in the original SMOOTH-GREEDY procedure, we can show that at every iteration, when S is the set of elements we selected in previous iterations, an element a added to S implies that w.h.p. F (S \u222a a) is arbitrarily close to maxb/\u2208H F (S \u222a b) (see Claim 5.3). Let a1, a2, . . . an\u2212|S|\u2212|H| denote the elements which are being considered. For each element ai, we have that if F (S \u222a ai) is non\n19\nnegligible then w.h.p F\u0303 (S \u222a ai) approximates F (S \u222a ai), and if F (S \u222a ai) is negligible then so is F\u0303 (S \u222a ai). While for ai, aj these events may well be correlated, since the probability of failure is inverse polynomially small and there are only n\u2212|S|\u2212|H| events, we can take a union bound and say that with high probability for every i if F (S \u222a ai) is negligible so is F\u0303 (S \u222a ai), and if F (S \u222a ai) is non negligible then it is well approximated by F\u0303 (S \u222a ai).\nThus, we know that at every iteration iwhen S is the set of elements selected in previous iterations, we have selected the element a that is arbitrarily close to maxb/\u2208H F (i)(S \u222a b). From the arguments in the paper we know that this implies that for an arbitrarily small \u03b3 > 0 we have:\nfS(a) \u2265 (1\u2212 \u03b3)fS\u222aH(i)(b) \u2265 (1\u2212 \u03b3)fS\u222aH(b)\nwhere the right inequality is due to submodularity and the fact that H(i) \u2286 H . The guarantees of SMOOTH-GREEDY therefore apply in this case as well. What remains to show is that SLICKGREEDY is unaffected by this modification. This is easy to verify as SLICK-GREEDY takes 1/\u03b4 disjoint sets H1, . . . ,H1/\u03b4, and the arguments discussed apply for every such set. Since we apply SMOOTH-COMPARE 1/\u03b4 times with sets of size ` it is easy to implement as well.\nModification of algorithms for small k for O(1)-correlated noise. A similar idea works also for the small k case, assuming d is constant. In this case, we add c d/ elements at each phase of the algorithm. We modify the definition of F\u0303 in the following way. First we take a an arbitrary partition P1, . . . P(n\u2212|S|)/d on the elements not in S, in which each Pi is of size d, and a partition Q1 . . . Q(|S|+|A|)/d of the elements in S \u222aA. We estimate the value of a set A given S using:\nF\u0303 (S \u222aA) = d 2 (|S|+ |A|)(|N | \u2212 |S| \u2212 |A|) \u2211 Qi \u2208 A \u2211 Pj f\u0303(((S \u222aA) \\Qi) \u222a Pj)\nand modify the rest of the algorithm accordingly.\nCorrectness relies on three steps:\n1. First, when we are in iteration i of the algorithm (after we already added (i \u2212 1)c elements to S), all the sets we apply the oracle on are of size c \u00b7 i, and hence they are independent of any set of size c(i\u2212 1) or less which were used in previous phases;\n2. Second, when we evaluate F\u0303 (S \u222aA) for a specific set A, we only use sets which are independent in the comparison. Here we rely on changing d elements in A each time, and replacing them by another set of d elements;\n3. Finally, we treat each setA separately, and show that if its marginal contribution is negligible then w.h.p its mean smooth value is not too large, and if its marginal contribution is not negligible, then w.h.p. F\u0303 (S \u222aA) approximates F (S \u222aA) well. Taking a union bound over all the bad events we get that the set A chosen has large (non-noisy) smooth mean value."}, {"heading": "5.4 Information Degradation", "text": "We have written the paper as if the algorithm gains no additional information for querying a point twice. The generalization to a case where the algorithm gets more information each time but there\n20\nis a degradation of information is simple: whenever the algorithms we presented here want to query a point just query it multiple times, and feed the expected value of the point given all the information one has to the algorithm. Hence it makes sense to focus on the extreme case where only the first query is helpful, as common in the literature of noisy optimization (e.g. [12])"}, {"heading": "5.5 Approximate Submodularity", "text": "In this paper our goal is to obtain near optimal guarantees as defined on the original function that was distorted through noise. That is, we assume that there is an underlying submodular function which we aim to optimize, and we only get to observe noisy samples of it. An alternative direction would be to consider the problem of optimizing functions that are approximately submodular:\nmax S:|S|\u2264k f\u0303(S)\nThe notion of approximate submodularity has been studied in machine learning [67, 23, 22, 33]. More generally, given the desirable guarantees of submodular functions, it is interesting to understand the limits of efficient optimization with respect to the function classes we aim to optimize.\nImpossibility for -adversarial approximation. If we assume that the function is an adversarial (1 \u00b1 ) approximation of a submodular function, our lower bound from Section 6 for erroneous oracles implies that no polynomial time algorithm can obtain a non-trivial approximation.\nTrivial reduction for noise in [1\u2212 , 1+ ]. WhenD \u2286 [1\u2212 , 1+ ], and the noise is i.i.d across sets, the algorithms in the paper obtain a solution arbitrarily close to ( 1\u2212 1+ ) ( 1\u2212 1e ) of maxS:|S|\u2264k f\u0303(S).\nImpossibility for unbounded noise. If we assume that a noisy process of a distribution with unbounded support altered a submodular function, then there are trivial impossibility results. Suppose that the initial submodular function is the constant function that gives 1 to every set. If we apply (e.g.) Gaussian noise to it, then the optimal algorithm is just to try random sets and hope for the best, and no polynomial time algorithm can achieve a constant factor approximation.\nOptimal approximation via black-box reduction. First, note that there is an algorithm which runs in time nk and finds the optimal subset of size k: query f\u0303 on all subsets of size at most k, and choose the maximal one. Notice that this is in contrast to the setting we study throughout the paper in which there is a lower bound of (2k \u2212 1)/2k + O(1/ \u221a n). The interesting regime is k = \u03c9(1), where there is a black-box reduction from the problem of maximizing a submodular function given an approximately submodular function, to the problem of maximizing an approximately submodular function. Since we can solve the original problem within a factor arbitrarily close to 1 \u2212 1/e we get an optimal approximation guarantee in this case as well. Let maxD(t) = E[max\u03be1,...\u03bet\u223cD{\u03be1, . . . , \u03bet}] be the expected maximum value of t i.i.d samples of D.\n21\nLemma 5.4. An algorithm which uses t \u2264 ( n k ) queries to f\u0303 cannot achieve approximation ratio better than:\nmaxD(t) maxD( ( n k ) ) .\nProof. Suppose that f(S) = 1 for every set S. The best that the algorithm can do is query t sets with at most k elements, and output the maximal one. The approximation ratio of this is exactly\nmaxD(t) maxD( ( n k ) )\nIf the algorithm queries sets with more than k elements, the approximation would deteriorate.\nLemma 5.5. Suppose there exists an algorithm which given k \u2208 \u03c9(1) returns a solution S s.t. f(S) \u2265 \u03b3maxT :|T |\u2264k f(T ) using q queries to a noisy oracle. Then, for any t \u2208 poly(n) there is an algorithm that uses q + t to a noisy oracle and returns a solution S\u2032 s.t.:\nf\u0303(S\u2032) \u2265 ( \u03b3 \u2212 o(1) )( maxD(t) maxD( ( n k ) ) ) max T :|T |\u2264k f\u0303(T ).\nProof. Let r be such that ( n\u2212k r ) \u2265 t. Since t is polynomial in n, we have that r is constant. Run the algorithm to obtain a set G of size k \u2212 r. From submodularity and the fact that r is constant:\nf(G) \u2265 \u03b3 max S:|S|\u2264k\u2212r f(S) \u2265 (1\u2212 r/k)\u03b3 max S:|S|\u2264k f(S) \u2265 (1\u2212 o(1))\u03b3 max S:|S|\u2264k f(S)\nFor every set of r elements {x1, . . . , xr} where xi 6\u2208 G, the algorithm queries f\u0303 on G \u222a {x1, . . . xr}, and chooses the set with maximum value. It is easy to see that the expected value of this set would be at least maxD(t)(1\u2212 r/k)\u03b3maxS:|S|\u2264k f(S), which gives the ratio.\n22"}, {"heading": "6 Impossibility for Adversarial Noise", "text": "In this section we show that there are very simple submodular functions for which no randomized algorithm with access to an -erroneous oracle can obtain a reasonable approximation guarantee with a subexponential number of queries to the oracle. Intuitively, the main idea behind this result is to show that a noisy oracle can make it difficult to distinguish between two functions whose values can be very far from one another. The functions we use are similar to those used to prove information theoretic lower bounds for submodular optimization and learning [79, 84, 36, 8, 95].\nTheorem 6.1. No randomized algorithm can obtain an approximation strictly better than O(n\u22121/2+\u03b4) to maximizing monotone submodular functions under a cardinality constraint using en\u03b4/n queries to an -erroneous oracle, for any fixed , \u03b4 < 1/2.\nProof. We will consider the problem of maxS:|S|\u2264k f(S) where k = n1/2+\u03b4. Let X \u2286 N be a random set constructed by including every element from N with probability n\u22121/2+\u03b4. We will use this set to construct two functions that are close in expectation but whose maxima have a large gap, and show that access to a noisy oracle implies distinguishing between these two functions. The functions are:\n\u2022 f1(S) = min { |S \u2229X| \u00b7 n1/2 + n1/2+\u03b4 , |S| \u00b7 n 1+\u03b4 }\n\u2022 f2(S) = min { |S| \u00b7 n\u03b4 + n1/2+\u03b4 , |S| \u00b7 n 1+\u03b4 }\nNotice that both functions are normalized monotone submodular: when S = \u2205 both functions evaluate to 0, and otherwise are affine. By the Chernoff bound we know that |X| \u2265 n1/2+\u03b4/2 with probability 1 \u2212 e\u2212\u2126(n1/2+\u03b4). Conditioned on this event we have that maxS:|S|\u2264k f1(S) = f1(X) \u2208 O(n1+\u03b4) whereas f2 is symmetric and maxS:|S|\u2264k f2(S) \u2208 O(n1/2+2\u03b4). Thus, an inability to distinguish between these two functions implies there is no approximation algorithm with approximation better than O(n\u22121/2+\u03b4). We define the erroneous oracle as follows. If the function is f2, its oracle returns the exact same value as f2 for any given set. Otherwise, the function is f1 and its erroneous oracle is defined as:\nf\u0303(S) = { f2(S), if (1\u2212 )f1(S) \u2264 f2(S) \u2264 (1 + )f1(S) f1(S) otherwise\nNotice that this oracle is -erroneous, by definition.\nSuppose now that the set X is unknown to the algorithm, and the objective is maxS:|S|\u2264k f1(S). We will first show that no deterministic algorithm that uses a single query to the erroneous oracle f\u0303 can distinguish between f1 and f2, with exponentially high probability (equivalently, we will show that a single query to the algorithm cannot find a set S for which f1(S) < (1 \u2212 )f2(S) or f1(S) > (1 + )f2(S) with exponentially high probability). For a single query algorithm, we can imagine that the set X is chosen after the algorithm chooses which query to invoke, and compute the success probability over the choice of X . In this case, all the elements are symmetric, and the function value is only determined by the size of the set that the single-query algorithm queries.\n23\nIn case the query is a set S of cardinality smaller or equal to n1/2, by the Chernoff bound we have that |S \u2229X| \u2264 (1 + \u03b2)n\u03b4 for any \u03b2 < 1 with probability at least 1\u2212 e\u2212\u2126(\u03b22n\u03b4). Thus:\nn1/2+\u03b4\n\u2264 f1(S) \u2264 ( 1 + \u03b2 + 1 ) n1/2+\u03b4\nn1/2+\u03b4\n\u2264 f2(S) \u2264\n( 1 + 1 ) n1/2+\u03b4\nIt is easy to verify that for \u03b2 < /(1\u2212 ): (1\u2212 )f1(S) \u2264 f2(S) \u2264 (1 + )f1(S). Thus, for any query of size less or equal to n1/2 the likelihood of the oracle returning f1 is 1\u2212 e\u2212\u2126(n \u03b4).\nIn case the oracle queries a set of size greater than n1/2 then again by the Chernoff bound, for any \u03b2 < 1 we have that with probability at least 1\u2212 e\u2212\u2126(\u03b22n1/2):(\n1\u2212 \u03b2 ) |S| n1/2\u2212\u03b4 \u2264 |S \u2229X| \u2264 ( 1 + \u03b2 ) |S| n1/2\u2212\u03b4\nFor \u03b2 \u2264 /(1\u2212 ), this implies that:\n(1\u2212 )f1(S) \u2264 f2(S) \u2264 (1 + )f1(S)\nTherefore, for any fixed \u2208 (0, 1), the algorithm cannot distinguish between f1 and f2 with probability 1 \u2212 e\u2212\u2126(n\u03b4) by querying the erroneous oracle with a set larger than n1/2. To conclude, by a union bound we get that with probability 1\u2212e\u2212\u2126(n\u03b4) no algorithm can distinguish between f1 and f2 using a single query to the erroneous oracle, and the ratio between their maxima is O(n1/2\u2212\u03b4).\nTo complete the proof, suppose we had an algorithm running in time en \u03b4 /n which can approximate the value of a submodular function, given access to an -erroneous oracle with approximation ratio strictly better than O(n\u22121/2+\u03b4) which succeeds with probability 2/3. This would let us solve the following decision problem: Given access to an -erroneous oracle for either f1 or f2, determine which function is being queried. To solve the decision problem, given access to an erroneous oracle of unknown function, we would use the hypothetical approximation algorithm to estimate the value of the maximal set of size n1/2+\u03b4. If this value is strictly more than n1/2+2\u03b4, the function is f1 (since f1(X) = O(n 1+\u03b4)), and otherwise it is f2.\nThe reduction allows us to show that distinguishing between the functions in time en \u03b4 /n and success probability 2/3 is impossible. For purpose of contradiction, suppose that there is a (randomized) algorithm for the decision problem, and let p denote the probability that it outputs f2 if it sees an oracle which is fully consistent with f2. To succeed with probability 2/3, it must be the case that whenever the algorithm gets f1 as an input, it finds a set S for which the noisy oracle returns f1(S) with probability at least 2/3\u2212p/2 \u2265 1/6. Whenever it finds such a set, the algorithm is done, since it can compute f2(S) without calling the oracle, and hence it knows that f1 was chosen in the decision problem.\nIn this case, we know that the algorithm makes up to en \u03b4 /n queries, until it sees a set for which it gets f1(S). But this means that there is an algorithm with success probability at least O(n/6en \u03b4 ) that makes a single query. This algorithm guesses some index i < en \u03b4 /n, and simulates the original algorithm for i\u2212 1 steps (by feeding it with f2 without using the oracle), and then using the oracle\n24\nin step i. If the algorithm guesses i to be the first index in which the exponential time algorithm sees f1(S), then the single query algorithm would succeed. Hence, since we showed that no single query (randomized) algorithm can find a set S such that f1(S) < (1\u2212 )f2(S) or f1(S) > (1+ )f2(S) with just one query this concludes the proof.\nThe following remarks are worth mentioning:\n\u2022 The functions we used in the lower bound are very simple examples of coverage functions;\n\u2022 If one does not require the function to be normalized, then the lower bound holds for affine functions, i.e. f(S) = \u2211 a\u2208S f(a) + C, where C independent of S;\n\u2022 The lower bound is tight: for any -erroneous oracle there is a 1\u2212 1+ \u00b7max{n \u22121/2, 1/k} approxi-\nmation by simply partitioning the ground sets to arbitrary sets of size min{ \u221a n, k}, and select the set whose value according to the erroneous oracle is maximal;\n\u2022 The lower bound applies to additive noise by simply applying an additive version of the Chernoff bound.\nSomewhat surprisingly, the above theorem suggests that a good approximation to a submodular function does not suffice to obtain reasonable approximation guarantees. In particular, guarantees from learning or sketching where the goal is to approximate a submodular function up to constant factors may not necessarily be meaningful for optimization. It is important to note that for some classes of submodular functions such as additive functions (f(S) = \u2211 a\u2208S f(a)), we can obtain algorithms that are robust to adversarial noise. A very interesting open question is to characterize the class of submodular functions that are robust to adversarial noise.\n25"}, {"heading": "7 More related work", "text": "Submodular optimization. Maximizing monotone submodular functions under cardinality and matroid constraints is heavily studied. The seminal works of [80, 46] show that the greedy algorithm gives a factor of 1\u22121/e for maximizing a submodular function under a cardinality constraint and a factor 1/2 approximation for matroid constraints. For max-cover which is a special case of maximizing a submodular function under a cardinality constraint, Feige shows that no poly-time algorithm can obtain an approximation better than 1-1/e unless P=NP [35]. Vondrak presented the continuous greedy algorithm which gives a 1\u2212 1/e ratio for maximizing a monotone submodular function under matroid constraints [94]. This is optimal, also in the value oracle model [79, 61, 81]. It is interesting to note that with a demand oracle the approximation ratio is strictly better than 1 \u2212 1/e [39]. When the function is not monotone, constant factor approximation algorithms are known to be obtainable as well [37, 73, 14, 15]. In general, in the past decade there has been a development in the theory of submodular optimization, through concave relaxations [1, 19], the multilinear relaxation [18, 94, 20], and general rounding technique frameworks [96]. In this paper, the techniques we develop arise from first principles: we only rely on basic properties of submodular functions, concentration bounds, and the algorithms are variants of the standard greedy algorithm.\nSubmodular optimization in game theory. Submodular functions have been studied in game theory almost fifty years ago [90]. In mechanism design submodular functions are used to model agents\u2019 valuations [74] and have been extensively studied in the context of combinatorial auctions (e.g. [27, 28, 26, 79, 16, 25, 83, 32, 29]). Maximizing submodular functions under cardinality constraints have been studied in the context of combinatorial public projects [84, 87, 17, 78] where the focus is on showing the computational hardness associated with not knowing agents valuations and having to resort to incentive compatible algorithms. Our adversarial lower bound implies that if agents err in their valuations, optimization may be hard, regardless of incentive constraints.\nSubmodular optimization in machine learning. In the past decade submodular optimization has become a central tool in machine learning and data mining (see surveys [65, 66, 11]). Problems include identifying influencers in social networks [59, 86] sensor placement [75, 50], learning in data streams [92, 52, 71, 5], information summarization [76, 77], adaptive learning [51], vision [58, 57, 63], and general inference methods [64, 57, 24]. In many cases the submodular function is learned from data, and our work aims to address the case in which there is potential for noise in the model.\nLearning submodular functions. One of the main motivations we had for studying optimization under noise is to understand whether submodular functions that are learned from data can be optimized well. The standard framework in the literature for learning set functions is Probably Mostly Approximately Correct (PMAC) learnability due to Balcan and Harvey [9]. This framework nicely generalizes Valiant\u2019s notion of Probably Approximately Correct (PAC) learnability [93]. Informally, PMAC-learnability guarantees that after observing polynomially-many samples of sets and\n26\ntheir function values, one can construct a surrogate function that is with constant probability over the distributions generating the samples, likely to be an approximation of the submodular function generating the data. Since the seminal paper of Balcan and Harvey there has been a great deal of work on learnability of submodular functions [41, 7, 4, 43, 45, 6]. As discussed in the paper, our lower bounds imply that one cannot optimize the surrogate function PMAC learned from data. If the approximation is via i.i.d noise on sets sufficiently far, this may be possible.\nApproximate submodularity. The concept of approximate submodularity has been studied in machine learning for dictionary selection and feature selection in linear regression [67, 23, 22, 33]. Generally speaking, this line of work considers approximate submodularity by defining a notion of the submodularity ratio of a function, defined in terms of how close it is to have a diminishing returns property. This ratio depends on the instance, which in the worst-case may result in a function that poorly approximates a submodular function. In practice however, these works show that in a broad range of applications the functions of interest are sufficiently close to submodular. Recently, the notion of approximate modularity (i.e. additivity) has been studied in [21] which give an optimal algorithm for approximating an approximately modular function via a modular function. These notions of approximate modularity and approximate submodularity are the model in which we have noise on the marginals. As discussed in Section 5, if the error on the marginals is adversarial, there are regimes in which non-trivial guarantees are impossible. If one assumes the marginal approximations are i.i.d our positive results apply.\nCombinatorial optimization under noise. Combinatorial optimization with noisy inputs can be largely studied through consistent (independent noisy answers when querying the oracle twice) and inconsistent oracles. For inconsistent oracles, it usually suffices to repeat every query O(log n) times, and eliminate the noise. To the best of our knowledge, submodular optimization has been studied under noise only in instances where the oracle is inconsistent or equivalently small enough so that it does not affect the optimization [59, 68]. One line of work studies methods for reducing the number of samples required for optimization (see e.g. [38, 10]), primarily for sorting and finding elements. On the other hand, if two identical queries to the oracle always yield the same result, the noise can not be averaged out so easily, and one needs to settle for approximate solutions, which has been studied in the context of tournaments and rankings [60, 12, 2].\nConvex optimization under noise. Maximizing functions under noise is also an important topic in convex optimization. The analogue of our model here is one where there is a zeroth-order noisy oracle to a convex function. As discussed in the paper, the question of polynomial-time algorithms for noisy convex optimization is straightforward and the work in this area largely aims at improving the convergence rate [34, 47, 62, 72, 85].\n27"}, {"heading": "8 Acknowledgements", "text": "A.H. was supported by ISF 1241/12; Y.S. was supported by NSF grant CCF-1301976, CAREER CCF-1452961, a Google Faculty Research Award, and a Facebook Faculty Gift. We thank Vitaly Feldman who pointed out the application to active learning. We are deeply indebted to Lior Seeman, who has carefully read previous versions of the manuscript and made multiple invaluable suggestions.\n28"}, {"heading": "A Combinatorial Smoothing", "text": "In this section we illustrate a general framework we call combinatorial smoothing that we will use in the subsequent sections. Intuitively, combinatorial smoothing mitigates the effects of noise and enables finding elements whose marginal contribution is large.\nSome intuition. Recall from our earlier discussion that implementing the greedy algorithm requires identifying arg max f(S \u222a a) for a given set S of elements selected by the algorithm in previous iterations. Thus, if for some a, b \u2208 N we can compare S \u222aa and S \u222a b and decide whether f(S\u222aa) > f(S\u222a b) or vice versa, we can implement the greedy algorithm. Put differently, viewing a set as a point on the hypercube, given two points in {0, 1}n we need to be able to tell which one has the larger true value, using a noisy oracle. In a world of continuous optimization, a reasonable approach to estimate the true value of a point in [0, 1]n with access to a noisy oracle is to take a small neighborhood around the point, sample values of points in its neighborhood, and average their values. Taking polynomially-many samples allows concentration bounds to kick in, and using a small enough diameter can often guarantee that the averaged value is a reasonable estimate of the point\u2019s true value. Surprisingly, the spirit of this idea can used in submodular optimization.\nSmoothing neighborhood. For a given subset A \u2286 N a smoothing function is a method which assigns a family of sets H(A) called the smoothing neighborhood. The smoothing function will be used to create a smoothing neighborhood for a small setA. This setAwhose marginal contribution we aim to evaluate, is essentially a candidate for a greedy algorithm. In the application in Section 2 the set A is simply be a single element, whereas in Section 3 the set A is of size O(1/ ).\nDefinition A.1. For a given function f : 2N \u2192 R, A,S \u2286 N , and smoothing neighborhoodH(A):\n\u2022 FS(A) := EX\u2208H(A) [fS(X)] (called the smooth marginal contribution of A),\n\u2022 F (S \u222aA) := EX\u2208H(A) [f(S \u222aX)] (called the smooth value of S \u222aA) \u2022 F\u0303 (S \u222aA) := EX\u2208H(A) [ f\u0303(S \u222aX) ] (called the noisy smooth value of S \u222aA).\nThe idea behind combinatorial smoothing is to select a smoothing neighborhood which includes sets whose value is in some sense close to the value of the set A whose marginal contribution we wish to evaluate. Intuitively, when the sets are indeed close, by averaging the values of the sets in H(A) we can mitigate the effects of noise and produce meaningful statistics (see Figure 3).\nSmoothing arguments\nIn our model, the algorithm may only access F\u0303 (S \u222a A). Ideally, given a set S and a smoothing neighborhoodH(A) we would have liked to apply concentration bounds and show that the noisy smooth value is arbitrarily close to the non-noisy smooth value, i.e. F (S \u222aA) \u2248 F\u0303 (S \u222aA) or:\u2211\ni\u2208H(A)\nf(S \u222aXi) \u2248 \u2211\ni\u2208H(A)\n\u03beif(S \u222aXi)\n37\nIf the values in {f(S \u222a Xi)}|H(A)|i=1 were arbitrarily close, we could simply apply a concentration bound by taking the value of any one of the sets, say S \u222aXj , and for vj = f(S \u222aXj), since all the values are close, we would be guaranteed that:\u2211\ni\u2208H(A)\n\u03beif(S \u222aXi) \u2248 \u2211\ni\u2208H(A)\n\u03beif(S \u222aXj) = vj \u00b7 \u2211\ni\u2208H(A)\n\u03bei\nIn continuous optimization this is usually the case when averaging over an arbitrarily small ball around the point of interest, and concentration bounds apply. In our case, due to the combinatorial nature of the problem, the values of the sets in the smoothing neighborhood may take on very different values. For this reason we cannot simply apply concentration bounds. The purpose of this section is to provide machinery that overcomes this difficulty. The main ideas can be summarized as follows:\n1. In general, there may be cases in which we cannot perform smoothing well and cannot get\n38\nthe noisy smooth values to be similar to the true smooth values. We therefore define a more modest, yet sufficient goal. Since our algorithms essentially try to replace the step of adding the element a \u2208 argmaxb f(S \u222a b) in the greedy algorithm with a\u2032 \u2208 argmaxb F (S \u222a b), it suffices to guarantee that for the set A which maximizes the noisy smooth values, that set also well approximates the (non-noisy) smooth values. More precisely our goal is to show that if for an arbitrarily small \u03b4 > 0 we have that A \u2208 argmaxB F\u0303 (S \u222a B) then F (S \u222aA) \u2265 (1\u2212 \u03b4) maxB F (S \u222aB);\n2. To show thatA \u2208 argmax F\u0303 (S\u222aA) implies F (S\u222aA) \u2265 (1\u2212\u03b4) maxB F (S\u222aB) for an arbitrarily small \u03b4 > 0, we prove two bounds. Lemma A.4 lower bounds the noisy smooth contribution of a set in terms of its (true) smooth contribution. Lemma A.5 upper bounds the smooth noisy contribution of any element against its smooth contribution. The key difference between these lemmas is that Lemma A.4 lower bounds the value in terms the variation of the smoothing neighborhood. The variation of the neighborhood is the ratio between the set with largest value and that with lowest value in the neighborhood. Intuitively, for elements with large values the variation of the neighborhood is bounded, and thus we can show that the noisy smooth value of these elements is nearly as high as their true smooth values.\n3. Together, these lemmas are used in subsequent sections to show that an element with the largest noisy smooth marginal contribution is an arbitrarily good approximation to the element with the largest (non-noisy) smooth marginal contribution. This is achieved by showing that the lower bound on the smooth value of an element with large (non-noisy) smooth marginal contribution beats the upper bound on the smooth (non-noisy) value of an element with slightly smaller smooth contribution.\nThe first lemma gives us tail bounds on the upper and lower bounds of the value of the noise multiplier in any of the calls made by a polynomial-time algorithm. We later use these tail bounds in concentration bounds we use in the smoothing procedures.\nLemma A.2. Let \u03c9max = max{\u03be1, . . . , \u03bem} and \u03c9min = min{\u03be1, . . . , \u03bem}, where \u03bei \u223c D and D is a noise distribution with a generalized exponential tail. For any \u03b4 > 0 and sufficiently large m, we have that:\n\u2022 Pr[\u03c9max < m\u03b4] > 1\u2212 e\u2212\u2126(m \u03b4/ lnm)\n\u2022 Pr[\u03c9min > m\u2212\u03b4] > 1\u2212 e\u2212\u2126(m \u03b4/ lnm)\nProof. As m tends to infinity, this lemma trivial for any noise distribution which is bounded, or has finite support. If the noise distribution is unbounded, we know that its tail is subexponential. Thus, at any given sample the probability of seeing the value m\u03b4 is at most e\u2212O(m\n\u03b4) where the constant in the big O notation depends on the magnitude of the tail. Iterating this a polynomial number of times gives the bound. The proof of the lower bound is equivalent.\nThe definition below of the variation of the neighborhood quantifies the ratio between the largest possible value and the smallest possible value achieved by a set in the neighborhood.\n39\nDefinition A.3. For given sets A,S \u2286 N , the variation of the neighborhood denoted vS(H(A)) is:\nvS(H(A)) = maxT\u2208H(A) fS(T )\nminT\u2208H(A) fS(T ) .\nThe following lemma gives a lower bound on the noisy smooth value in terms of the (non-noisy) smooth value and the variation. Intuitively, when an element has large value its variation is bounded, and the lemma implies that its noisy smooth value is close to its smooth value. Essentially, when the variation is bounded F\u0303 (S) \u2248 (1 \u2212 \u03bb)(1 \u2212 )F (S) for \u03bb and that vanish as n grows large.\nLemma A.4. Let f : 2N \u2192 R, A,S \u2282 N , \u03c9 = maxAi\u2208H(A) \u03beAi , and \u00b5 be the mean of the noise distribution. For = min { 1, 2vS(H) \u00b7 |H(A)|\u22121/4 } for any \u03bb < 1 w.p 1\u2212 e\u2212\u2126( \u03bb2t1/4 \u03c9 ) we have:\nF\u0303 (S \u222aA) > (1\u2212 \u03bb)\u00b5 \u00b7 (f(S) + (1\u2212 ) \u00b7 FS(A)) .\nProof. Let A1, . . . , At be the sets in H(A) and let \u03b11, . . . , \u03b1t denote the corresponding marginal contributions and \u03be1 . . . , \u03bet denote their noise multipliers. In these terms the noisy smooth value is:\nF\u0303 (S \u222aA) = 1 t t\u2211 i=1 \u03bei(f(S) + \u03b1i) = 1 t t\u2211 i=1 \u03beif(S) + 1 t t\u2211 i=1 \u03bei\u03b1i. (1)\nLet \u03c9 be the upper bound on the value of the noise multiplier. Applying the Chernoff bound, we get that for any \u03bb < 1 with probability at least 1\u2212 e\u2212\u2126(\u03bb2t/\u03c9):\n1\nt t\u2211 i=1 \u03beif(S) \u2265 (1\u2212 \u03bb)\u00b5f(S).\nTo complete the proof we need to argue about concentration of the second term in (1). To do so, in our analysis we will consider a fine discretization of {\u03b1i}i\u2208[t] and apply concentration bounds on each discretized value. Define \u03b1max = maxi\u2208[t] \u03b1i and \u03b1min = mini\u2208[t] \u03b1i. We can divide the set of values {\u03b1i}i\u2208[t] to t1/4 bins BIN1, . . . , BINt1/4 , where a value \u03b1i is placed in the bin BINq if\n(q \u2212 1) \u00b7 \u03b1maxt\u22121/4 \u2264 \u03b1i \u2264 q \u00b7 \u03b1maxt\u22121/4\nSay a bin is dense if it contains at least t1/4 values and sparse otherwise. Consider some dense bin BINq and let \u03b1min(q) = mini\u2208BINq \u03b1i and \u03b1max(q) = maxi\u2208BINq \u03b1i. Since every bin is of width \u03b1max \u00b7 t\u22121/4 we know that:\n\u03b1min(q) \u2265 \u03b1max(q) \u2212 \u03b1max \u00b7 t\u22121/4 Applying concentration bounds as above, we get that \u2211\ni\u2208BINq \u03bei \u2265 (1\u2212\u03bb)\u00b5\u00b7|BINq|with probability\n40\nat least 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9) for any \u03bb < 1. Thus, with this probability:\u2211 i\u2208BINq \u03bei\u03b1i \u2265 \u2211 i\u2208BINq \u03bei\u03b1min(q)\n\u2265 (1\u2212 \u03bb)\u00b5 \u00b7 |BINq| \u00b7 \u03b1min(q) \u2265 (1\u2212 \u03bb)\u00b5 \u00b7 |BINq| \u00b7 ( max { 0, \u03b1max(q) \u2212 \u03b1max \u00b7 t\u22121/4 }) > (1\u2212 \u03bb)\u00b5 \u00b7 |BINq| \u00b7 ( max { 0, 1\u2212 \u03b1max\n\u03b1max(q) \u00b7 t\u22121/4\n}) \u03b1max(q)\n\u2265 (1\u2212 \u03bb)\u00b5 \u00b7 |BINq| \u00b7 ( max { 0, 1\u2212 \u03b1max\n\u03b1min \u00b7 t\u22121/4\n}) \u03b1max(q)\n= (1\u2212 \u03bb)\u00b5 \u00b7 |BINq| \u00b7 ( max { 0, 1\u2212 vS (H(A)) \u00b7 t\u22121/4 }) \u03b1max(q)\nTaking a union bound over all (at most t1/4) dense bins, we get that with probability 1 \u2212 e\u2212\u2126(\u03bb\n2t1/4/\u03c9):\u2211 i\u2208dense \u03bei\u03b1i \u2265 (1\u2212 \u03bb)\u00b5 \u00b7 ( 1\u2212max { 0, vS (H(A)) \u00b7 t\u22121/4 }) \u2211 BINq\u2208dense |BINq| \u00b7 \u03b1max(q)\n\u2265 (1\u2212 \u03bb)\u00b5 \u00b7 ( max { 0, 1\u2212 vS (H(A)) \u00b7 t\u22121/4 }) \u2211\ni\u2208dense \u03b1i. (2)\nLet \u03b1 = 1t \u2211t i=1 \u03b1i. Since we have less than t 1/4 elements in a sparse bin, and in total t1/4 bins, the number of elements in sparse bins is at most t1/2. We can use this to effectively lower bound the values in sparse bins in terms of \u03b1:\n\u2211 i\u2208dense \u03b1i = t\u2211 i=1 \u03b1i \u2212 \u2211 i\u2208sparse \u03b1i\n\u2265 max { 0,\nt\u2211 i=1 \u03b1i \u2212 t1/2\u03b1max } \u2265 max { 0, t\u03b1\u2212 t1/2\u03b1max\n} > max { 0, t \u00b7 ( 1\u2212 \u03b1max\n\u03b1min \u00b7 t\u22121/2\n) \u03b1 } = max { 0, t \u00b7 ( 1\u2212 vS(H) \u00b7 t\u22121/2 ) \u03b1 }\n(3)\n41\nPutting (2) and (3) we get that for any \u03bb < 1, with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303S(A) = 1\nt t\u2211 i=1 \u03bei \u00b7 \u03b1i\n\u2265 1 t \u2211 i\u2208dense \u03bei \u00b7 \u03b1i\n\u2265 (1\u2212 \u03bb)\u00b5 \u00b7 (max { 0, 1\u2212 vS (H(A)) \u00b7 t\u22121/4 }\n) \u00b7 1 t \u2211 i\u2208dense \u03b1i\n\u2265 (1\u2212 \u03bb)\u00b5 \u00b7 (max { 0, 1\u2212 vS (H(A)) \u00b7 t\u22121/4 } )(max { 0, 1\u2212 vS (H(A)) \u00b7 t\u22121/2 } )\u03b1\n> (1\u2212 \u03bb)\u00b5 \u00b7 (max { 0, 1\u2212 2vS (H(A)) \u00b7 t\u22121/4 } )\u03b1\n= (1\u2212 \u03bb)\u00b5 \u00b7 (max { 0, 1\u2212 2vS (H(A)) \u00b7 t\u22121/4 } )FS(A)\nTaking a union bound we get that for any positive \u03bb < 1 with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (S \u222aA) = 1 t t\u2211 i=1 \u03beif(S) + 1 t t\u2211 i=1 \u03bei\u03b1i\n> (1\u2212 \u03bb)\u00b5 \u00b7 ( f(S) + (max { 0, 1\u2212 2vS(H(A)) \u00b7 t\u22121/4) \u00b7 FS(A) }) = (1\u2212 \u03bb)\u00b5 \u00b7 ( f(S) + (1\u2212min { 1, 2vS(H(A)) \u00b7 t\u22121/4) \u00b7 FS(A) }) .\nThe next lemma gives us an upper bound on the noisy smooth value. The bound shows that for sufficiently large t (the size of the smoothing neighborhood, which always depends on n), for small \u03bb > 0 we have that F\u0303 (S) \u2248 (1 + \u03bb)F (S) + 3t\u22121/4 \u00b7 \u03b1max. In our applications of smoothing \u03b1max \u2264 OPT, and t is large. Since we use this upper bound to compare against elements whose value is at least some bounded factor of OPT, the dependency of the additive term on \u03b1max will be insignificant.\nLemma A.5. Let f : 2N \u2192 R, A,S \u2286 N , \u03c9 = maxAi\u2208H(A) \u03beAi , \u03b1max = maxAi\u2208H(A) fS(Ai) and \u00b5 be the mean of the noise distribution. For = 3t\u22121/4\u03b1max we have that for any \u03bb < 1 with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (S \u222aA) < (1 + \u03bb)\u00b5 \u00b7 (f(S) + FS(A) + ) .\nProof. As in the proof of Lemma A.4 let A1, . . . , At denote the sets inH(A), and for each set Ai we will again use \u03b1i to denote the marginal value fS(Ai) and \u03bei to denote the noise multiplier \u03beS\u222a{Ai}.\nF\u0303 (S \u222aA) = 1 t t\u2211 i=1 \u03beif(S) + 1 t t\u2211 i=1 \u03bei\u03b1i. (4)\nAs before, we will focus on showing concentration on the second term. Define \u03b1max = maxi \u03b1i and \u03b1min = mini \u03b1i. To apply concentration bounds on the second term, we again partition the values of {\u03b1i}i\u2208[t] to bins of width \u03b1max \u00b7 t\u22121/4 and call a bin dense if it has at least t1/4 values and sparse\n42\notherwise. Using this terminology:\nt\u2211 i=1 \u03bei\u03b1i = \u2211 i\u2208dense \u03bei\u03b1i + \u2211 i\u2208sparse \u03bei\u03b1i.\nLet BIN` be the dense bin whose elements have the largest values. Consider the t1/4/2 largest values in BIN` and call the set of indices associated with these values L. We have:\nt\u2211 i=1 \u03bei\u03b1i = \u2211 i\u2208dense\\L \u03bei\u03b1i + \u2211 i\u2208L\u222asparse \u03bei\u03b1i\nThe set L\u222a sparse is of size at least t1/4/2 and at most t1/4/2 + t1/2. This is because L is of size exactly t1/4/2 and there are at most t1/2 values in bins that are sparse since there are t1/4 bins and a bin that has at least t1/4 is already considered dense. Thus, when \u03c9 is an upper bound on the value of the noise multiplier, from Chernoff, for any \u03bb < 1 with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\u2211\ni\u2208L\u222asparse \u03bei\u03b1i \u2264 \u2211 i\u2208L\u222asparse \u03bei\u03b1max\n< (1 + \u03bb)\u00b5 \u00b7 |L \u222a sparse| \u00b7 \u03b1max\n\u2264 (1 + \u03bb)\u00b5 \u00b7\n( t1/4\n2 + t1/2\n) \u03b1max\n< (1 + \u03bb)\u00b5 \u00b7 2t1/2\u03b1max\nWe will now use the same logic as in the proof of Lemma A.4 to apply concentration bounds on the values in the dense bins. For a dense bin BINq, let \u03b1max(q) and \u03b1min(q) be the maximal and minimal values in the bin, respectively. As in Lemma A.4, for any \u03bb < 1 with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\u2211\ni\u2208BINq \u03bei\u03b1i \u2264 \u2211 i\u2208BINq \u03bei \u00b7 \u03b1max(q)\n\u2264 (1 + \u03bb)\u00b5 \u00b7 \u03b1max(q) \u00b7 |BINq| \u2264 (1 + \u03bb)\u00b5 \u00b7 ( \u03b1min(q) + \u03b1max \u00b7 t\u22121/4 ) \u00b7 |BINq|\n< (1 + \u03bb)\u00b5 \u00b7 ( |BINq| \u00b7 \u03b1min(q) + |BINq|\u03b1max \u00b7 t\u22121/4 ) Applying a union bound we get with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\u2211\ni\u2208dense\\L\n\u03bei\u03b1i < \u2211 q (1 + \u03bb)\u00b5 \u00b7 ( |BINq| \u00b7 \u03b1min(q) + |BINq|\u03b1max \u00b7 t\u22121/4 ) < (1 + \u03bb)\u00b5 \u00b7 t ( \u03b1+ t\u22121/4\u03b1max )\n43\nTogether we have:\n1\nt t\u2211 i=1 \u03bei\u03b1i = 1 t  \u2211 i\u2208dense\\L \u03bei\u03b1i + \u2211 i\u2208L\u222asparse \u03bei\u03b1i  < (1 + \u03bb)\u00b5 \u00b7 ( \u03b1+ t\u22121/4\u03b1max + 2t \u22121/2\u03b1max\n) < (1 + \u03bb)\u00b5 \u00b7 ( \u03b1+ 3t\u22121/4\u03b1max\n) < (1 + \u03bb)\u00b5 \u00b7 ( FS(A) + 3t \u22121/4\u03b1max )\nBy a union bound we get that with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (S \u222aA) = 1 t t\u2211 i=1 \u03beif(S) + 1 t t\u2211 i=1 \u03bei\u03b1i \u2264 (1 + \u03bb)\u00b5 \u00b7 ( f(S) + FS(A) + 3t \u22121/4\u03b1max ) .\n44"}, {"heading": "B Optimization for Large k", "text": "The Smooth Greedy Algorithm\nSmoothing guarantees\nLemma (2.1). For any fixed > 0, consider an -relevant iteration of SMOOTH-GREEDY where S is the set of elements selected in previous iterations and a \u2208 arg maxb/\u2208H F\u0303 (S \u222a b). Then for \u03b4 = 2/4k and sufficiently large n we have that w.p. \u2265 1\u2212 1/n4:\nFS(a) \u2265 (1\u2212 \u03b4) max b/\u2208H FS(b).\nTo prove the above lemma we will need claims B.1, B.2 and B.3. After proving B.3 the proof will follow by verifying that the number of sets in the smoothing set is sufficient to obtain the desired approximation (1\u2212 \u03b4).\nClaim B.1. If FS(a) \u2265 FS(b) then fS(a) \u2265 fS\u222aH(b).\nProof. Assume for purpose of contradiction that fS(a) < fS\u222aH(b). Since f is a submodular function, fS(T ) = f(S \u222a T )\u2212 f(S) is also submodular (hence also subadditive). Therefore \u2200H \u2032 \u2286 H :\nfS(H \u2032 \u222a a) \u2264 fS(H \u2032) + fS(a) subadditivity of fS\n< fS(H \u2032) + fS\u222aH(b) by assumption \u2264 fS(H \u2032) + fS\u222aH\u2032(b) submodularity of fS = fS(H \u2032 \u222a b).\nNotice however, that this contradicts our assumption:\nFS(a) = 1\nt \u2211 H\u2032\u2286H fS(H \u2032 \u222a a) < 1 t \u2211 H\u2032\u2286H fS(H \u2032 \u222a b) = FS(b).\nThe following claim bounds the variation (see Definition A.3) of the smoothing neighborhood of the element we selected. This is a necessary property for later applying the smoothing arguments.\nClaim B.2. Let > 0. For an -relevant iteration of SMOOTH-GREEDY, let S be the set of elements selected in previous iterations. If a? \u2208 arg maxa/\u2208H FS(a) then vS (H(a?)) < 3k/ .\nProof. Let b? \u2208 argmaxb/\u2208H fH\u222aS(b). By the maximality of a? we have that FS(a?) \u2265 FS(b?), and thus by Claim B.1 we get fS(a?) \u2265 fH\u222aS(b?). Since the iteration is -relevant we have that fH\u222aS(b ?) \u2265 \u00b7 OPTH/k, and from monotonicity of f we get:\nmin H\u2032\u2286H\nfS(H \u2032 \u222a a?) \u2265 fS(a?) \u2265 fH\u222aS(b?) \u2265 \u00b7 OPTH k\nand since every set inH(a?) is of size at most k we know that maxH\u2032\u2286H fS(H \u2032\u222aa?) \u2264 OPT. Together with the fact that OPT \u2264 e \u00b7 OPTH we get:\nvS (H(a?)) = maxH\u2032\u2286H fS(H \u2032 \u222a a?) minH\u2032\u2286H fS(H \u2032 \u222a a?) \u2264 OPT OPTH \u00b7 k < 3k .\n45\nWe can now show that in -relevant iterations the value of the element which maximizes the noisy smooth value is comparable to that of the (non-noisy) smooth value, with high probability. Recall that we use t to denote the size of the smoothing neighborhood.\nClaim B.3. Given > 0 assume t \u2265 (\n110k\u00b7logn \u03b4\n)8 . For an -relevant iteration of SMOOTH-GREEDY, let\nS be the elements selected in previous iterations and a \u2208 arg maxb/\u2208H F\u0303 (S \u222a b). Then, w.p. \u2265 1\u2212 1/n4:\nFS(a) \u2265 (1\u2212 \u03b4) max b/\u2208H FS(b).\nProof. Let a? be the element which maximizes smooth marginal contribution:\na? \u2208 argmaxb/\u2208H FS(a)\nWe will show that for any element b whose smooth marginal contribution is a factor of (1 \u2212 \u03b4) smaller than the smooth marginal contribution of a?, then w.h.p. its noisy value of is smaller than that of a?. That is, for any b /\u2208 H for which FS(b) < (1\u2212 \u03b4)FS(a?) we get that F\u0303 (S \u222a b) < F\u0303 (S \u222a a?) with probability at least \u2126(1\u2212 1/n5). The result will then follow by taking a union bound over all comparisons. We will show that a? likely beats b by lower bounding F\u0303 (S\u222aa?) and upper bounding F\u0303 (S \u222a b) using the smoothing arguments from the previous section. We use \u03c9 to denote the value of the largest noise multiplier realized throughout the iterations of the algorithm. We later argue that we can upper bound \u03c9 \u2264 6 log n as the noise distribution has an exponentially decaying tail.\n\u2022 Lower bound on F\u0303 (S \u222a a?): First, from Claim B.2 we know that vS(H(a?)) \u2264 3k/ . Together with Lemma A.4 we get that \u2200\u03bb < 1 with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (S \u222a a?) > (1\u2212 \u03bb)\u00b5 \u00b7 ( f(S) + ( 1\u2212 6k \u00b7 t\u22121/4 ) \u00b7 FS(a?) ) (5)\n\u2022 Upper bound on F\u0303 (S \u222a b): Letting \u03b2max = maxX\u2208H(b) f(X), from Lemma A.5, we get that \u2200\u03bb < 1 with probability 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (S \u222a b) < (1 + \u03bb)\u00b5 \u00b7 ( f(S) + FS(b) + 3t \u22121/4\u03b2max ) (6)\nWe\u2019ll express this inequality in terms of f(S) and FS(a?) as well. First, since all sets in H(b) are of size at most k we also know that \u03b2max \u2264 OPT. Thus:\n3t\u22121/4\u03b2max \u2264 3t\u22121/4 \u00b7 OPT (7)\nWe will now bound OPT in terms of FS(a?). Since every set inH(a?) includes a?, from monotonicity we get that FS(a?) \u2265 fS(a?). Let b? \u2208 argmaxb/\u2208H fH\u222aS(b). Due to the maximality of a? we have that FS(a?) \u2265 FS(b?) and by Claim B.1 we know that fS(a?) \u2265 fS\u222aH(b?). Since the iteration is -relevant we get:\nFS(a ?) \u2265 fS(a?) \u2265 fS\u222aH(b?) \u2265\nfS\u222aH(OH) k \u2265 \u00b7 OPTH k > \u00b7 OPT 3k (8)\n46\nPutting (8) together with (7) we get:\n3t\u22121/4\u03b2max \u2264 k \u00b7 9t\u22121/4 \u00b7 FS(a?)\nPlugging into (6) and using the assumption that FS(b) < (1\u2212 \u03b4)FS(a?) we get: F\u0303 (S \u222a b) < (1 + \u03bb)\u00b5 \u00b7 ( f(S) + FS(b) + ( 9t\u22121/4 \u00b7 k ) FS(a ?) ) (9)\n< (1 + \u03bb)\u00b5 \u00b7 ( f(S) + ( 9t\u22121/4 \u00b7 k + (1\u2212 \u03b4) ) FS(a ?) ) (10)\nPutting (5) together with (10) we get that \u2200\u03bb < 1 with probability at least 1\u2212 2e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (S \u222a a?)\u2212 F\u0303 (S \u222a b) > \u00b5 \u00b7 ( FS(a ?) [ (1\u2212 \u03bb) ( 1\u2212 6k t\u22121/4 ) \u2212 (1 + \u03bb) ( 9k t\u22121/4 + (1\u2212 \u03b4) )] \u2212 2\u03bbf(S) ) \u2265 \u00b5 \u00b7 ( FS(a ?) [ (1\u2212 \u03bb) ( 1\u2212 6k t\u22121/4 ) \u2212 (1 + \u03bb) ( 9k t\u22121/4 + (1\u2212 \u03b4) )] \u2212 2\u03bbOPT\n) > \u00b5 \u00b7 ( FS(a ?) [ (1\u2212 \u03bb) ( 1\u2212 6k t\u22121/4 ) \u2212 (1 + \u03bb) ( 9k t\u22121/4 + (1\u2212 \u03b4) )] \u2212 2\u03bb3k FS(a ?)\n) = \u00b5 \u00b7 ( FS(a ?) [ (1\u2212 \u03bb) ( 1\u2212 6k t\u22121/4 ) \u2212 (1 + \u03bb) ( 9k t\u22121/4 + (1\u2212 \u03b4) ) \u2212 2\u03bb3k\n]) = \u00b5 \u00b7 ( FS(a ?) [ \u03b4 \u2212 15k \u00b7 t\u22121/4 \u2212 \u03bb ( (2\u2212 \u03b4) + 3k \u00b7 t\u22121/4 + 6k\n)]) > \u00b5 \u00b7 ( FS(a ?) [ \u03b4 \u2212 k ( 15t\u22121/4 + 10\u03bb )])\nThe second inequality above is an application of (8) and the fact that f(S) \u2264 OPT since |S| \u2264 k. The third is from (8).\nFor the result to hold we need the above difference to be strictly positive, and hold with probability \u2126(1\u2212 1/n5). Thus, sufficient conditions would be:\n1. k \u00b7 15t \u22121/4 \u2264 \u03b42 , and\n2. 10\u03bb \u2264 \u03b42 , and\n3. 1\u2212 2 exp(\u2212\u03bb2t1/4\u03c9 ) \u2208 \u2126(1\u2212 1/n 5).\nThe first condition holds when t \u2265 (30k/ \u03b4)4; the second condition holds when \u03bb = \u03b4/20k. For \u03c9 = 6 log n and \u03bb = \u03b4/20k, the third condition is satisfied when:\n( \u03b4)2t1/4\n202k2\u03c9 =\n( \u03b4)2t1/4\n202k26 log n \u2265 5 log n\nrearranging:\nt \u2265 120004 ( k log n\n\u03b4 )8 47\nThus, since t in the lemma statement respects:\nt \u2265 ( 110k log n\n\u03b4\n)8 > 120004 ( k log n\n\u03b4 )8 we have that the first, second, and third conditions are met conditioned on \u03c9 \u2264 6 log n. That is, we have that the difference is positive with probability 1\u2212 2 exp(\u2212\u03bb2t1/4\u03c9 ) \u2265 1\u2212 2/n\n5, conditioned on \u03c9 \u2264 6 log n. From lemma A.2 we know that the probability of \u03c9 > 6 log n is smaller than 1/n5 for sufficiently large n. Therefore, by taking a union bound on the probability of the event in which the difference is negative and the probability that \u03c9 > 6 log n, both occurring with probability smaller than 2/n5 we have that the probability of the difference being positive is at least 1\u22124/n5 \u2208 \u2126(1\u2212 1/n5), as required.\nProof of Lemma 2.1. By Claim B.3, when \u03b4 = 2/4k for any fixed > 0 we need to verify that for sufficiently large n:\nt >\n( 110k log n\n\u03b4\n)8 = (440k2 log n)8\n3\nIn the case where k \u2265 log n we use ` = 25 log n and thus t = 2` = n25 and the above inequality holds. When k < log n we use ` = 33 log log n and thus t = log33 n and the above inequality holds in this case as well. We therefore have the result with probability at least 1\u2212 1/n4.4\nApproximation guarantee\nClaim (2.2). For any > 0, let \u03b4 \u2264 2/4k. Suppose that the iteration is -relevant and let b? \u2208 argmaxb/\u2208H fH\u222aS(b). If FS(a) \u2265 (1\u2212 \u03b4)FS(b?), then:\nfS(a) \u2265 (1\u2212 )fH\u222aS(b?).\nProof. First, we upper bound FS(a):\nFS(a) = 1\nt \u2211 H\u2032\u2286H fS(H \u2032 \u222a a) by definition of FS\n= 1\nt \u2211 H\u2032\u2286H ( fS(H \u2032) + fS\u222aH\u2032(a) )\n\u2264 1 t \u2211 H\u2032\u2286H ( fS(H \u2032) + fS(a) )\nby submodularity of f\n= fS(a) + 1\nt \u2211 H\u2032\u2286H fS(H \u2032) t = 2|H|\n4Note that we could have used smaller values of ` to achieve the desired bound. The reason we exaggerate the values of ` is to be consistent with the analysis of SLICK-GREEDY which necessitates these slightly larger values of `.\n48\nNext, we lower bound (1\u2212 \u03b4)FS(b?):\n(1\u2212 \u03b4)FS(b?) = (1\u2212 \u03b4) 1\nt \u2211 H\u2032\u2286H fS(H \u2032 \u222a b?) by definition of FS\n= (1\u2212 \u03b4)1 t \u2211 H\u2032\u2286H ( fS(H \u2032) + fS\u222aH\u2032(b ?) ) \u2265 (1\u2212 \u03b4)1 t \u2211 H\u2032\u2286H ( fS(H \u2032) + fS\u222aH(b ?) )\nby submodularity of f\n= (1\u2212 \u03b4)fH\u222aS(b?)\u2212 \u03b4 1\nt \u2211 H\u2032\u2286H fS(H \u2032) + 1 t \u2211 H\u2032\u2286H fS(H \u2032) t = 2|H|\nSince FS(a) \u2265 (1\u2212 \u03b4)FS(b?) this implies that:\nfS(a) \u2265 (1\u2212 \u03b4)fH\u222aS(b?)\u2212 \u03b4 1\nt \u2211 H\u2032\u2286H fS(H \u2032)\n\u2265 (1\u2212 \u03b4)fH\u222aS(b?)\u2212 \u03b4 1\nt \u2211 H\u2032\u2286H fS(H) monotonicity of f\n\u2265 (1\u2212 \u03b4)fH\u222aS(b?)\u2212 \u03b4fS(H) t = |H \u2032| \u2265 (1\u2212 \u03b4)fH\u222aS(b?)\u2212 \u03b4OPT |H| \u2264 k \u2265 (1\u2212 \u03b4)fH\u222aS(b?)\u2212 e\u03b4OPTH OPTH \u2265 OPT/e\n\u2265 (1\u2212 \u03b4)fH\u222aS(b?)\u2212 e\u03b4 \u00b7 k \u00b7 fH\u222aS(b?) -relevant iteration\n= ( 1\u2212 \u03b4 ( 1 + e \u00b7 k )) fH\u222aS(b ?)\n\u2265 ( 1\u2212 \u03b4 ( 4k )) fH\u222aS(b ?)\n= (1\u2212 )fH\u222aS(b?). \u03b4 \u2264 2/4k\nClaim (2.3). For any fixed > 0, consider an -relevant iteration of SMOOTH-GREEDY with S as the elements selected in previous iterations. Let a \u2208 arg maxb/\u2208S\u222aH F\u0303 (S \u222a b). Then, w.p. \u2265 1\u2212 1/n4:\nfS(a) \u2265 ( 1\u2212 )[ 1\nk\u2032\n( OPTH \u2212 f(S) )] .\nProof. Let O \u2208 argmaxT :|T |\u2264k\u2032fH(T ), o ? \u2208 argmaxo\u2208OfH\u222aS(o) and b ? \u2208 argmaxb/\u2208H fH\u222aS(b). From Lemma 2.1 we know that with probability 1\u2212 1/n4 we have FS(a) \u2265 (1\u2212 \u03b4)FS(b?) for \u03b4 = 2/4k, and together with Claim 2.2 we get:\nfS(a) \u2265 (1\u2212 )fH\u222aS(b?) \u2265 (1\u2212 )fH\u222aS(o?)\nFrom subadditivity fH\u222aS(o?) \u2265 fH\u222aS(O)/k\u2032 and thus:\nfS(a) \u2265 (1\u2212 )fH\u222aS(o?) \u2265 (\n1\u2212 k\u2032\n) fH\u222aS(O) \u2265 ( 1\u2212 k\u2032 )( fH(O)\u2212 f(S) ) .\n49\nLemma (2.4). Let S be the set returned by SMOOTH-GREEDY and H its smoothing set. Then, for any fixed > 0 when k \u2265 3`/ with probability of at least 1\u2212 1/n3 we have that:\nf(S \u222aH) \u2265 (1\u2212 1/e\u2212 /3)OPTH .\nProof. In case OPTH < OPT/e then H alone provides a 1\u2212 1/e\u2212 /3 approximation. To see this, let O \u2208 argmaxT :|T |\u2264k f(T ) and O\u2032 \u2208 argmaxT :|T |\u2264k\u2032 f(T ), and OH \u2208 argmaxT :|T |\u2264k\u2032 fH(T ). We get:\n(1\u2212 /3)f(O) \u2264 f(O\u2032) k\u2032 = k \u2212 ` and k \u2265 3`/ \u2264 f(H \u222aO\u2032) monotonicity = f(H) + fH(O \u2032)\n\u2264 f(H) + fH(OH) optimality of OH < f(H) + f(O)/e eOPTH < OPT\nThus:\nf(H) \u2265 (\n1\u2212 1 e \u2212 3\n) OPT \u2265 ( 1\u2212 1\ne \u2212 3\n) OPTH\nIn case OPTH \u2265 OPT/e we set \u03b3 = min{1/e, /6}. We will use the following notation. At every iteration i \u2208 [k\u2032] of the while loop in the algorithm, we will use ai to denote the element that was added in that step, and Si := {a1, . . . , ai}.\nFirst, notice that if there exists an iteration i that is not \u03b3-relevant, our bound trivially holds:\nfH\u222aSi(OH) \u2264 k\u2032 \u00b7 max o\u2208OH fH\u222aSi(o) \u2264 k\u2032 \u00b7 max b/\u2208Si\u222aH fH\u222aSi(b) \u2264 k\u2032 \u00b7 \u03b3OPTH k < \u03b3OPTH\nSince fH\u222aSi(OH) = f(H \u222a Si \u222a OH) \u2212 f(H \u222a Si), the above inequality implies that f(H \u222a Si) > f(H \u222a Si \u222aOH)\u2212 \u03b3OPTH . But this implies:\nf(S \u222aH) \u2265 f(Si \u222aH) > f(OH \u222a Si \u222aH)\u2212 \u03b3OPTH \u2265 f(OH)\u2212 \u03b3OPTH \u2265 fH(OH)\u2212 \u03b3OPTH = (1\u2212 \u03b3)OPTH \u2265 (1\u2212 1/e)OPTH\nIt remains to prove the approximation guarantee in the case that every iteration is \u03b3-relevant. To do so, we can apply a standard inductive argument on Claim 2.3 to show that S alone provides a 1\u2212 1/e\u2212 /3 approximation. Claim 2.3 states that for \u03b3-relevant iterations, at every stage i \u2208 [k\u2032]:\nf(Si+1)\u2212 f(Si) \u2265 (1\u2212 \u03b3) [ 1\nk\u2032 (fH(OH)\u2212 f(Si))\n] . (11)\nWe will show that at every stage i \u2208 [k\u2032]:\nf(Si) \u2265 (1\u2212 \u03b3) ( 1\u2212 ( 1\u2212 1\nk\u2032\n)i) fH(OH).\n50\nThe proof is by induction on i. For i = 1 we have that Si = {a1} and invoking Claim 2.3 with S = \u2205we get that f(ai) \u2265 (1\u2212 \u03b3) 1k\u2032 fH(OH). Therefore:\nf(S1) = f(a1) \u2265 (1\u2212 \u03b3) 1\nk\u2032 fH(OH) = (1\u2212 \u03b3)\n( 1\u2212 ( 1\u2212 1\nk\u2032\n)) fH(OH).\nWe can now assume the claim holds for i = l < k\u2032 and show that it holds for i = l + 1: f(Sl+1) \u2265 (1\u2212 \u03b3) ( 1\nk\u2032 (fH(OH)\u2212 f(Sl))\n) + f(Sl) By (11)\n> (1\u2212 \u03b3) (( 1\nk\u2032 fH(OH)\n) + ( 1\u2212 1\nk\u2032\n) f(Sl) ) \u03b4 > 0\n\u2265 (1\u2212 \u03b3) ( 1\nk\u2032 fH(OH)\n) + (1\u2212 \u03b3) ( 1\u2212 1\nk\u2032\n)( 1\u2212 ( 1\u2212 1\nk\u2032\n)l) fH(OH) inductive hypothesis\n= (1\u2212 \u03b3) ( 1\u2212 ( 1\u2212 1\nk\u2032\n)l+1) fH(OH)\nNote that for any l > 1 we have that (1\u2212 1/l)l \u2264 1/e, and thus:\nf(S) = f(Sk\u2032)\n\u2265 (1\u2212 1/e\u2212 \u03b3)fH(OH) by the induction > (1\u2212 1/e\u2212 /3)OPTH . \u03b3 = /6\nCorollary B.4. Let S be the set returned by SMOOTH-GREEDY and H be its smoothing set. For any fixed > 0 and k > 3`/ , we have that with probability at least 1\u2212 1/n3:\nf(S \u222aH) > (\ne\u2212 1 2e\u2212 1\u2212\n\u2212 2 ) OPT.\nProof. Let OH \u2208 argmaxT :|T |\u2264k\u2032fH(T ). From Lemma 2.4, with probability at least 1\u2212 1/n 3:\nf(S \u222aH) > (\n1\u2212 1 e \u2212 3\n) f(OH) (12)\nLet O\u2032 \u2208 argmaxT :|T |\u2264k\u2212|H| f(T ). From submodularity and the fact that k \u2265 3`/ > |H|/ we get that (1\u2212 )OPT \u2264 f(O\u2032). Putting everything together:\n(1\u2212 )OPT \u2264 f(O\u2032) submodularity of f \u2264 f(OH \u222aH) monotonicity of f \u2264 f(OH) + f(H) subadditivity of f\n\u2264 (\ne\ne\u2212 1\u2212\n) f(S \u222aH) + f(H) by (12)\n\u2264 (\n2e\u2212 1\u2212 e\u2212 1\u2212\n) f(S \u222aH). monotonicity of f\nTherefore f(S \u222aH) > (\ne\u22121 2e\u22121\u2212 \u2212 2\n) OPT as required.\n51\nSlick Greedy: Optimal Approximation for Sufficiently Large k\nAs described in the main body of the paper, in SLICK-GREEDY we apply a slightly more general version of SMOOTH-GREEDY where in each iteration i \u2208 [1/\u03b4] the algorithm SMOOTH-GREEDY is initialized with the set of elements Ri = \u222aj 6=iHj and uses the smoothing set Hi. SMOOTH-GREEDY from the previous section is a special case in which Ri = \u2205. As one might imagine, the guarantees from the previous section carry over, using the appropriate definitions.\nGeneralizing guarantees of smooth greedy\nTo make the transition to the case in which SMOOTH-GREEDY is being initialized with Ri of size `/\u03b4 \u2212 ` and selects k\u2032\u2032 = k \u2212 |Ri| \u2212 |Hi| = k \u2212 `/\u03b4 elements, we extend our definitions as follows. For a given set Ri used for initialization, it\u2019ll be convenient to consider the function gi(T ) = fRi(T ), and its smooth value Gi(a) = 1 t \u2211t i=1 g (S \u222a (Hi \u222a a)). When the smoothing set is clear from context we will generally use R,H, g,G instead of Ri, Hi, gi, Gi. The value of the optimal solution here is OPT[G] = maxT :|T |\u2264k\u2032\u2032 g(T ) where k\u2032\u2032 = k \u2212 |R| \u2212 |H|. We can then also define OPT[G]H = maxT :|T |\u2264k\u2032\u2032 gH(T ). For a given set S of elements selected by SMOOTH-GREEDY and b? \u2208 argmaxb/\u2208H gS\u222aH(b), an -relevant iteration is one in which gH\u222aS(b?) \u2265 OPT[G]H/k and OPT[G]H \u2265 OPT[G]/e.\nLower bounding the marginal contribution in each iteration. We first show that when SMOOTH-GREEDY is initialized with a set R and run with smoothing set H , then in every \u03b3relevant iteration the element a selected respects gS(a) \u2265 (1\u2212 \u03b3)gH\u222aH(b?). This claim is necessary for proving Lemma B.7 which shows the approximation guarantee of SMOOTH-GREEDY in each iteration of SLICK-GREEDY as well as for proving guarantees of SMOOTH-COMPARE in Lemma 2.6.\nClaim B.5. For a given set R \u2282 N , let g(T ) = fR(T ). For any fixed \u03b3 > 0 consider a \u03b3-relevant iteration of SMOOTH-GREEDY initialized with some set R using smoothing set H s.t. H \u2229R = \u2205, and let S be the set of elements selected before the iteration. If a \u2208 argmaxb/\u2208H F\u0303 (R \u222a S \u222a b) then w.p.\u2265 1\u2212 1/n4:\ngS(a) \u2265 (1\u2212 \u03b3)gH\u222aS(b?)\nProof. Let G denote the smooth value function of g, i.e. G(S \u222a a) = 1t \u2211\nH\u2032\u2282H g(S \u222a H \u2032 \u222a a). The proof in a chaining of four simple arguments. Let \u03bb = \u03b32/4k and \u03b1 = \u03b3\u03bb/3k. We show:\n1. F\u0303 (R \u222a S \u222a a) \u2265 F\u0303 (R \u222a S \u222a b?) =\u21d2 FR\u222aS(a) \u2265 (1\u2212 \u03b1) FR\u222aS(b?) 2. FR\u222aS(a) \u2265 (1\u2212 \u03b1) FR\u222aS(b?) =\u21d2 G(S \u222a a) \u2265 (1\u2212 \u03b1) G(S \u222a b?) 3. G(S \u222a a) \u2265 (1\u2212 \u03b1) G(S \u222a b?) =\u21d2 GS(a) \u2265 (1\u2212 \u03bb) GS(b?) 4. GS(a) \u2265 (1\u2212 \u03bb) GS(b?) =\u21d2 gS(a) \u2265 (1\u2212 \u03b3) gH\u222aS(b?)\nThe above arguments can be justified as follows:\n52\n1. To see F\u0303 (R\u222aT \u222aa) \u2265 F\u0303 (R\u222aT \u222ab?) implies FR\u222aT (a) \u2265 (1\u2212\u03b1)FR\u222aT (b?), we invoke Claim B.3 on S = R \u222a T . To do so, since \u03b1 \u2264 \u03b33/24k2 for sufficiently large n we need to verify:\nt >\n( 110k log n\n\u03b3\u03b1\n)8 = ( 2640k3 log n\n\u03b33 )8 In the case where k \u2265 2400 log n we use ` = 25 log n and thus t = 2` = n25 and the above inequality holds. When k < 2400 log n we use ` = 33 log logn and thus t = log33 n and the above inequality holds in this case as well. We therefore have the result w.p. \u2265 1\u2212 1/n4.\n2. Assuming that FR\u222aS(a) \u2265 (1\u2212 \u03b1)FR\u222aS(b?) we will show that G(S \u222a a) \u2265 (1\u2212 \u03b1)G(S \u222a b?):\nFR\u222aS(a) \u2265(1\u2212 \u03b1)FR\u222aS(b?)\n=\u21d2 1 t \u2211 H\u2032\u2282H fR\u222aS(H \u2032 \u222a a) \u2265(1\u2212 \u03b1)1 t \u2211 H\u2032\u2282H fR\u222aS(H \u2032 \u222a b?) =\u21d2 1 t \u2211 H\u2032\u2282H ( f(R \u222a S \u222aH \u2032 \u222a a)\u2212 f(R \u222a S) ) \u2265(1\u2212 \u03b1)1 t \u2211 H\u2032\u2282H ( f(R \u222a S \u222aH \u2032 \u222a b?)\u2212 f(R \u222a S)\n) =\u21d2 1\nt \u2211 H\u2032\u2282H ( f(R \u222a S \u222aH \u2032 \u222a a)\u2212 f(R) ) \u2265(1\u2212 \u03b1)1 t \u2211 H\u2032\u2282H ( f(R \u222a S \u222aH \u2032 \u222a b?)\u2212 f(R) ) =\u21d2 1\nt \u2211 H\u2032\u2282H fR(S \u222aH \u2032 \u222a a) \u2265(1\u2212 \u03b1) 1 t \u2211 H\u2032\u2282H fR(S \u222aH \u2032 \u222a b?)\n=\u21d2 1 t \u2211 H\u2032\u2282H g(S \u222aH \u2032 \u222a a) \u2265(1\u2212 \u03b1)1 t \u2211 H\u2032\u2282H g(S \u222aH \u2032 \u222a b?) =\u21d2 G(S \u222a a) \u2265(1\u2212 \u03b1)G(S \u222a b?)\n3. G(S \u222a a) \u2265 (1\u2212 \u03b1)G(S \u222a b?) =\u21d2 GS(a) \u2265 (1\u2212 \u03bb)GS(b?): We first argue GS(b?) > \u03b3OPT[G]e\u00b7k\u2032\u2032 :\nGS(b ?) =\n1\nt \u2211 H\u2032\u2282H ( g(S \u222a b? \u222aH \u2032)\u2212 g(S) ) \u2265 1 t \u2211 H\u2032\u2282H ( g(S \u222a b? \u222aH \u2032)\u2212 g(S \u222aH \u2032) ) monotonicity of g \u2265 1 t \u2211 H\u2032\u2282H (g(S \u222a b? \u222aH)\u2212 g(S \u222aH)) submodularity of g = g(S \u222a b? \u222aH)\u2212 g(S \u222aH) = gS\u222aH(b ?) \u2265 \u03b3 k\u2032\u2032 OPT[G]H \u03b3-relevant iteration > \u03b3\ne \u00b7 k\u2032\u2032 OPT[G] OPT[G]H > OPT[G]/e\n53\nNow, in a similar fashion to Claim 2.2:\nGS(a) = G(S \u222a a)\u2212G(S) \u2265 (1\u2212 \u03b1) (G(S \u222a b?)\u2212G(S))\u2212 \u03b1G(S) \u2265 (1\u2212 \u03b1) (G(S \u222a b?)\u2212G(S))\u2212 \u03b1OPT[G]\n\u2265 (1\u2212 \u03b1) (G(S \u222a b?)\u2212G(S))\u2212 \u03b1e \u00b7 k \u2032\u2032\n\u03b3 \u00b7GS(b?) GS(b?) >\n\u03b3OPT[G]\ne \u00b7 k\u2032\u2032\n= (1\u2212 \u03b1) (GS(b?))\u2212 \u03b1 e \u00b7 k\u2032\u2032\n\u03b3 \u00b7GS(b?)\n= ( 1\u2212 \u03b1 ( 1 + e \u00b7 k\u2032\u2032\n\u03b3\n)) GS(b ?)\n= (1\u2212 \u03bb)GS(b?) \u03b1 = \u03bb/3k and k \u2265 k\u2032\u2032 + 1\n4. GS(a) \u2265 (1\u2212 \u03bb)GS(b?) =\u21d2 gS(a) \u2265 (1\u2212 \u03b3)gH\u222aS(b?): by direct application of Claim 2.2\nDefinition B.6. Given two disjoint sets H and R, let OPTH,R = f(H \u222aR \u222aOH,R)\u2212 fR(H) where:\nOH,R \u2208 argmaxT :|T |\u2264k\u2212|H\u222aR| f(H \u222aR \u222a T ).\nNotice that when R = \u2205 we have that OH,R = OH \u2208 argmaxT :|T |\u2264k\u2212|H| fH(T ) as defined in the previous subsection. In that sense, the value of OH,R is that of the optimal solution evaluated on fH when initialized with R. In the same way Lemma 2.4 shows SMOOTH-GREEDY obtains a 1\u2212 1/e\u2212 /3 approximation to OPTH , the following lemma shows that when SMOOTH-GREEDY is initialized with R it obtains the same guarantee against OPTH,R. Details are in Appendix ??.\nLemma B.7. Let S be the set returned by SMOOTH-GREEDY that is initialized with a set R \u2286 N and has H as its smoothing set of size `, which is disjoint from R and S. Then, for any fixed > 0 when k \u2265 3|H \u222aR|/ with probability of at least 1\u2212 1/n3 we have that:\nf(R \u222a S \u222aH) \u2265 (1\u2212 1/e\u2212 /3)OPTH,R.\nProof. Notice that the proof of Lemma 2.4 applies for the application of SMOOTH-GREEDY on any submodular function v where in every \u03b3-relevant iteration vS(a) \u2265 (1 \u2212 \u03b3)vS\u222aH(b?) with probability 1 \u2212 1/n4, for \u03b3 \u2208 min{1/e, /6}, and S being the elements added in the previous iteration. From Claim B.5 we have that for any \u03b3-relevant iteration gS(a) \u2265 (1\u2212 \u03b3)gS\u222aH(b?) w.p. \u2265 1\u2212 1/n4. We can therefore apply the exact same proof on g and get:\ng(S \u222aH) \u2265 (1\u2212 1/e\u2212 /3)OPT[G]H (13)\nLet OH \u2208 argmaxT :|T |\u2264k\u2212|R\u222aH| g(T ) and let OH,R \u2208 argmaxT :|T |\u2264k\u2212|H\u222aR| f(H \u222a R \u222a T ). Observe that by definition of g(X) = fR(X) we have that:\nf(H \u222aR \u222aOH,R) = f(H \u222aR \u222aOH)\n54\nand thus from (13) we get:\nf(R \u222a S \u222aH)\u2212 f(R) = fR(S \u222aH) = g(S \u222aH) \u2265 (1\u2212 1/e\u2212 /3)gH(OH) \u2265 (1\u2212 1/e\u2212 /3) (g(OH \u222aH)\u2212 g(H)) = (1\u2212 1/e\u2212 /3) (fR(OH \u222aH)\u2212 fR(H)) \u2265 (1\u2212 1/e\u2212 /3) (f(R \u222aOH \u222aH)\u2212 f(R)\u2212 fR(H)) \u2265 (1\u2212 1/e\u2212 /3) (f(R \u222aOH,R \u222aH)\u2212 fR(H))\u2212 (1\u2212 1/e\u2212 /3)f(R)\nand we therefore have that f(R \u222a S \u222aH) \u2265 (1\u2212 1/e\u2212 /3) (f(R \u222aOH,R \u222aH)\u2212 fR(H)).\nWe will instantiate the Lemma with R = Rl and H = Hl as discussed above: for any i \u2208 [1/\u03b4] we will define Ri = \u222aj 6=iHj and use the index l to denote the smoothing set in {Hi} 1/\u03b4 i=1 which has the least marginal contribution to the rest, i.e. Hl = argmini\u2208[1/\u03b4] fRi(Hi). We first show that the iteration of SLICK-GREEDY on l finds a solution arbitrarily close to 1\u2212 1/e for sufficiently large k.\nLemma (2.5). Let Sl be the set returned by SMOOTH-GREEDY that is initialized with Rl and Hl its smoothing set. Then, for any fixed > 0 when k \u2265 36`/ 2 with probability of at least 1\u2212 1/n3 we have:\nf(Sl \u222aHl) \u2265 (1\u2212 1/e\u2212 2 /3)OPT\nProof. To ease notation, letR = Rl,H = Hl, andO = Ol whereOl is the solution which maximizes f(H \u222a R \u222a T ) over all subsets T of size at most k \u2212 |H \u222a R|. Let \u03b2 = |H \u222a R|/k. Notice that by submodularity we have that:\nf(H \u222aR \u222aO) \u2265 (\n1\u2212 |H \u222aR| k\n) OPT = (1\u2212 \u03b2)OPT (14)\nNotice also that by the minimality ofH = Hl and submodularity we have that fR(H) \u2264 \u03b4f(H\u222aR). Recall also that \u03b4 = /6 and notice that whenever k \u2265 `/\u03b42 = 36`/ 2 we have that \u03b2 < \u03b4 and hence \u03b2 + \u03b4 < /3. Therefore, by application of Lemma B.7 we get that with probability 1\u2212 1/n3:\nf(S \u222aR \u222aH) \u2265 (\n1\u2212 1 e \u2212 3\n) OPTH,R by Lemma B.7\n= ( 1\u2212 1\ne \u2212 3\n) (f(H \u222aR \u222aO)\u2212 fR(H)) by definition\n\u2265 (\n1\u2212 1 e \u2212 3\n) (f(H \u222aR \u222aO)\u2212 \u03b4 \u00b7 f(H \u222aR)) fR(H) \u2264 \u03b4f(H \u222aR)\n\u2265 (\n1\u2212 1 e \u2212 3\n) ((1\u2212 \u03b4)f(H \u222aR \u222aO)) monotonicity of f\n\u2265 (\n1\u2212 1 e \u2212 3 \u2212 \u03b4 ) (f(H \u222aR \u222aO))\n\u2265 (\n1\u2212 1 e \u2212 3 \u2212 \u03b4 ) (1\u2212 \u03b2)OPT by (14)\n\u2265 (\n1\u2212 1 e \u2212 2 3\n) OPT. \u03b2 + \u03b4 < /3\n55\nThe smooth comparison procedure\nLemma (2.6). Assume k \u2265 96`/ 2. Let Ti be the set that won the SMOOTH-COMPARE tournament. Then, with probability at least 1\u2212 1/n2:\nf(Ti) \u2265 (\n1\u2212 3\n) min {( 1\u2212 1\ne \u2212 2 3\n) OPT, max\nj\u2208[1/\u03b4] f(Tj)\n}\nThe proof of the lemma uses the following two claims.\nClaim B.8. Let Ti = Si \u222aHi and Tj = Sj \u222aHj be two sets that are compared by SMOOTH-COMPARE, and suppose that (i)f(Ti) \u2265 (1 + 2\u03b2)f(Tj) where \u03b2 = |Hij |/k\u2032\u2032 and k\u2032\u2032 = k \u2212 `/\u03b4, and (ii) f(Tj) < (1\u2212 1/e\u2212 2 /3)OPT for any \u2265 3(1\u2212 k\u2032\u2032/k)/2. Then, for any set H \u2032ij \u2286 Hij w.p. \u2265 1\u2212 1/n3:\nf(Ti \u222aH \u2032ij) \u2265 f(Tj \u222aH \u2032ij).\nProof. Recall that Hij \u2229 ( Ti \u222a Tj ) = \u2205. We will argue that assuming f(Tj) < (1\u2212 1/e)OPT, the fact that every element in H \u2032ij was a candidate for selection by SMOOTH-GREEDY and wasn\u2019t selected, implies that w.h.p. either (i) f(Tj) is arbitrarily close to 1 \u2212 1/e (in which case we wouldn\u2019t mind that if it wins the comparison) or (ii) the marginal contribution of H \u2032ij to Tj is bounded from above by 2\u03b2f(Tj) which suffices since then we get:\nf(Tj \u222aH \u2032ij) = f(Tj) + fTj (H \u2032ij) \u2264 (1 + 2\u03b2)f(Tj) < f(Ti) \u2264 f(Ti \u222aH \u2032ij)\nTo prove this, consider the instantiation of SMOOTH-GREEDY initialized with Rj with smoothing set Hj , and let S be the set selected after its k\u2032\u2032 = k\u2212 |Rj | \u2212 |Hj | iterations. Recall that Sj = Rj \u222a S and that Tj = Sj \u222aHj . To ease notation let R = Rj and H = Hj .\nWe will first prove the statement in the case that the iteration is \u03b3-relevant for \u03b3 = 1/4. For every iteration r \u2208 [k\u2032\u2032] let S(r) be the set of elements selected in the previous iterations and a(r) be the element added to the solution at that stage by SMOOTH-GREEDY. From Claim B.5 we know that since a(r) \u2208 argmaxb F\u0303 (R \u222a S(r) \u222a b) and the size of the smoothing neighborhood t is sufficiently large then w.p. \u2265 1\u2212 1/n4:\ngS(r)(a(r)) \u2265 (1\u2212 \u03b3) max b/\u2208H gH\u222aS(r)(b)\n56\nWe therefore have that:\ng(S) = k\u2032\u2032\u2211 r=1 gS(r)(ar)\n\u2265 k\u2032\u2032\u2211 r=1 (1\u2212 \u03b3) max b/\u2208H gS(r)\u222aH(b)\n\u2265 k\u2032\u2032\u2211 r=1 (1\u2212 \u03b3) max b/\u2208H gS\u222aH(b) = k\u2032\u2032(1\u2212 \u03b3) max b/\u2208H gS\u222aH(b) \u2265 k\u2032\u2032(1\u2212 \u03b3) max h\u2208H\u2032ij gS\u222aH(h) \u2265 k \u2032\u2032(1\u2212 \u03b3) |H \u2032ij | gS\u222aH(H \u2032 ij) \u2265 (1\u2212 \u03b3)k \u2032\u2032\n` gS\u222aH(H\n\u2032 ij)\nSince g(T ) = fR(T ) and \u03b3 = 1/4 this implies:\nf(R \u222a S)\u2212 f(R) > k \u2032\u2032\n2`\n( f(R \u222aH \u222aH \u2032ij)\u2212 f(R \u222a S) ) Since Tj = Rj \u222a S \u222aHj = R \u222a S \u222aH we get:\nfTj (H \u2032 ij) <\n2` k\u2032\u2032 f(Tj) = 2\u03b2f(Tj).\nIf the iteration is not \u03b3-relevant, assume first that e \u00b7 OPT[G]H \u2265 OPT[G]. In this case, let OH = argmaxT :|T |\u2264k\u2032\u2032 gH(T ). Notice that the fact that iteration is not relevant in this case says that there is an iteration r for which maxb/\u2208H gH\u222aS(r)(b) < \u03b3OPT[G]H/k and from submodularity of g since S(r) \u2286 S we get maxb/\u2208H gH\u222aS(b) < \u03b3OPT[G]H/k. Thus:\ngH\u222aS(OH) \u2264 k\u2032\u2032 \u00b7 gH\u222aS(b?)\n\u2264 k\u2032\u2032 \u00b7 \u03b3OPT[G]H k\n< \u03b3OPT[G]H\nwhich implies:\ng(H \u222a S) > g(OH \u222aH \u222a S)\u2212 \u03b3OPT[G]H \u2265 gH(OH)\u2212 \u03b3OPT[G]H = (1\u2212 \u03b3)OPT[G]H\n57\nUsing this bound we get:\ngH\u222aS(H \u2032 ij) \u2264 |H \u2032ij | max\nh\u2208H\u2032ij gH\u222aS(h)\n\u2264 |H \u2032ij |max b/\u2208H gH\u222aS(b) \u2264 |H \u2032ij | \u03b3\nk OPT[G]H\n< \u03b3`\nk(1\u2212 \u03b3) g(H \u222a S)\nAgain, as before for \u03b4 = 1/4 we get that in this case:\nfTj (H \u2032 ij) <\n2` k\u2032\u2032 f(Tj) = 2\u03b2f(Tj)\nLastly, it remains to show that if if the iteration is not \u03b3-relevant because e \u00b7 OPT[G]H < OPT[G], we get a contradiction to our assumption that f(Tj) < (1 \u2212 1/e \u2212 2 /3)OPT. To see this, let O \u2208 argmaxT :|T |\u2264k\u2032\u2032 g(T ), and notice that:\ng(H \u222aOH)\u2212 g(H) < g(O)\ne\nhence:\nf(R \u222aH)\u2212 f(R) = g(H)\n> g(H \u222aOH)\u2212 g(O)\ne \u2265 (\n1\u2212 1 e\n) g(O)\n\u2265 (\n1\u2212 1 e\n) (f(R \u222aO))\u2212 f(R)\nWe therefore get that f(Tj) \u2265 f(R \u222a H) > (1 \u2212 1/e)f(O). Notice that since |O| = k\u2032\u2032 and k\u2032\u2032/k \u2265 (1\u2212 2 /3), submodularity implies f(Tj) \u2265 (1\u2212 1/e\u2212 2 /3)OPT, a contradiction.\nClaim B.9. For k \u2265 96`/ 2 suppose that f(Ti) \u2265 (1 + \u03b4/3)f(Tj) and that f(Tj) \u2264 (1\u2212 1/e\u2212 2 /3)OPT. Then, Ti wins in the smooth comparison procedure w.p. \u2265 1\u2212 2/n3.\nProof. Let \u03b2 = |Hij |/k\u2032\u2032 where k\u2032\u2032 = k \u2212 (|Hij | + |Ri|). Since we assume that k \u2265 96` and \u03b4 = /6 this implies that 2\u03b2 < 2/45. We therefore have:\nf(Ti) >\n( 1 + \u03b4\n3\n) f(Tj) = ( 1 + 2\n18\n) f(Tj) > ( 1 + 2\n45\n)2 f(Tj) > (1 + 2\u03b2) 2 f(Tj)\nFrom Claim B.8 this implies that for any H \u2032ij \u2286 Hij we have that with probability at least 1\u2212 1/n3:\nf(Tj \u222aH \u2032ij) \u2264 (1 + 2\u03b2)f(Tj \u222aH \u2032ij)\n58\nWe will condition on this event as well as the event that the maximal value obtained throughout the iterations of the algorithm is \u03bdmax and minimal value is \u03bdmin, and that \u03bdmax/\u03bdmin \u2264 n\u03c4 for some constant \u03c4 > 0.\nPr [ f\u0303(Ti \u222aH \u2032ij) \u2265 f\u0303(Tj \u222aH \u2032ij) \u2223\u2223\u2223f(Ti) \u2265 (1 + \u03b4 3 ) f(Tj) ] = Pr [ \u03beif(Ti \u222aH \u2032ij) \u2265 \u03bejf(Tj \u222aH \u2032ij) \u2223\u2223\u2223f(Ti) \u2265 (1 + \u03b4 3 ) f(Tj)\n] > Pr [ (1 + 2\u03b2) \u00b7 \u03bei\n\u03bej \u2265 1 ] \u2265 1\n2 +\n1\n2 log1+2\u03b2( \u03bdmax \u03bdmin )\nThe last inequality follows from a discretization argument: Consider the m \u2208 O(log n) intervals, where the i\u2019th interval is [\u03bdmin(1 + 2\u03b2)i, \u03bdmin(1 + 2\u03b2)i+1], and i ranges from 0 to log1+2\u03b2( \u03bdmax \u03bdmin\n). Due to symmetry of \u03bei and \u03bej , the likelihood of \u03bei falling in the same or higher interval than \u03bej is:\u2211m\ni=1 i\nm2 =\n1 2 + 1 2m = 1 2 +\n1\n2 log1+2\u03b2( \u03bdmax \u03bdmin\n) =\n1 2 +\n1\n2\u03c4 log1+2\u03b2 n\nApplying a Chernoff bound, for any constants , \u03b4 > 0, s.t. \u03b4/8 > 1 + 2\u03b2, and \u03bdmax/\u03bdmin \u2264 n\u03c4 for some constant \u03c4 > 0, we get that Ti is chosen with probability at least 1\u2212 exp(\u2212\u2126(n/ log(n))), conditioned on \u03bdmax/\u03bdmin < n\u03c4 which by Lemma A.2 occurs with probability 1\u2212 exp(\u2212\u2126(n\u03b1)) for some constant \u03b1 > 0. For sufficiently large n, Ti therefore wins w.p. at least 1\u2212 2/n3.\nProof of Lemma 2.6. Since \u2200i, j \u2208 [1/\u03b4] SMOOTH-COMPARE({Ti, Tj}, Hij) returns Ti as long as f(Ti) \u2265 (1 \u2212 \u03b4/3)f(Tj) and f(Tj) < (1 \u2212 1/e \u2212 2 /3)OPT, and SMOOTH-COMPARE is called 1/\u03b4 times we get:\nf(Ti) \u2265 (\n1\u2212 \u03b4 3\n) 1/\u03b4 \u00d7 min {( 1\u2212 1\ne \u2212 2 3\n) OPT, max\nj\u2208[1/\u03b4] f(Tj) } \u2265 ( 1\u2212\n3\n) \u00d7 min {( 1\u2212 1\ne \u2212 2 3\n) OPT, max\nj\u2208[1/\u03b4] f(Tj)\n} .\n59"}, {"heading": "C Optimization for Small k", "text": "Smoothing Guarantees\nLemma C.1 (3.1). For any > 0 and any set S \u2282 N , let A? \u2208 arg maxA:|A|=1/ fS(A). Then:\n(1\u2212 ) fS(A?) \u2264 FS(A?) \u2264 fS(A?).\nProof. By the maximality of A? we have that f(A?) \u2265 f(A?ij) for any i, j since A?ij is generated by replacing ai \u2208 A? with aj /\u2208 A? \u222aS. Therefore, the average of all Aijs is upper bounded by fS(A?).\nFor the lower bound, let c = 1/ and consider some arbitrary ordering on a1, . . . , ac \u2208 A?. Define A-i = A \\ {ai}. From the diminishing returns property we get that for any i \u2208 [c]:\nfS\u222aA?-i(ai) = f(S \u222aA ? -i \u222a ai) \u2212 f(S \u222aA?-i)\n\u2264 f(S \u222a {a1 . . . , ai}) \u2212 f(S \u222a {a1, . . . , ai\u22121})\nThus: c\u2211 i=1 fS\u222aA?-i(ai) \u2264 c\u2211 i=1 (f(S \u222a {a1 . . . , ai})\u2212 f(S \u222a {a1, . . . , ai\u22121})) = fS(A?) (15) By summing over all A?-i we get the desired bound:\nFS(A ?) =\n1\nc(n\u2212 c\u2212 |S|) n\u2212c\u2212|S|\u2211 j=1 c\u2211 i=1 fS(A ? ij)\n\u2265 1 c c\u2211 i=1 fS(A ? -i) monotonicity, since A ? -i \u2282 A?ij\n= 1\nc c\u2211 i=1 ( fS(A ? -i \u222a ai)\u2212 fS\u222aA?-i(ai) ) = 1\nc c\u2211 i=1 fS(A ?)\u2212 1 c c\u2211 i=1 fS\u222aA?-i(ai)\n\u2265 fS(A?)\u2212 1\nc fS(A\n?) by (15)\n= ( 1\u2212 1\nc\n) fS(A ?)\n= (1\u2212 ) fS(A?).\nThe smoothing lemma. The rest of this subsection is devoted to proving the following important lemma. Intuitively, this lemma implies that at every iteration of SM-GREEDY we identify the bundle which nearly maximizes the mean marginal contribution.\nLemma (3.2). Let A \u2208 argmaxB:|B|=c F\u0303 (S \u222a B) where c \u2265 16 , and assume that the iteration is 4 - significant. Then, with probability at least 1\u2212 e\u2212\u2126(n1/10) we have that:\nFS(A) \u2265 (1\u2212 ) max B:|B|=c FS(B).\n60\nSmoothing neighborhoods. The proof uses the smoothing arguments developed in Section A. Recall that for a given set of elements A \u2286 N a smoothing function is a method which assigns A a family of sets H(A) called the smoothing neighborhood. For a given function f : 2N \u2192 R, A,S \u2286 N , and smoothing neighborhoodH(A) we define:\n(1) FS(A) := EX\u2208H(A) [ fS(X) ]; (2) F(S \u222aA) := EX\u2208H(A) [ f(S \u222aX) ];\n(3) F\u0303(S \u222aA) := EX\u2208H(A) [ f\u0303(S \u222aX) ].\nNote that F(A) 6= F (A). In particular, as discussed above, we do not apply smoothing on the noisy version of F directly, but rather on the noisy version of the function F which is applied on A-i := A \\ {ai}, for all i \u2208 [c]:\nF\u0303(S \u222aA-i) := 1 n\u2212 c\u2212 |S| \u2211\nj /\u2208S\u222aA\nf\u0303(S \u222aA-i \u222a {aj})\nNotice that the smoothing arguments then apply to F since:\nF\u0303 (S \u222aA) = 1 c c\u2211 i=1 F\u0303(S \u222aA-i)\nIn our case, for every A-i, its smoothing neighborhood is:\nH(A-i) = {A-i \u222a {aj} : j /\u2208 S \u222aA}\nThroughout the rest of this section we will use t to denote the number of sets in a smoothing neighborhood ofH(A-i). Note that for every i \u2208 [c] the size of a smoothing neighborhood is:\nt = |H(A-i)| = |N \u222a (S \\A)| = n\u2212 c\u2212 |S| \u2208 O(n).\nSmoothing in the sampled mean method. In order to apply Lemma A.4 in a meaningful way we need to bound the variation of the neighborhoods H(A?-i). To do so, we use the next claim which essentially bounds the variation of the smoothing neighborhoodsH(A?-i), of almost all A?-i.\nClaim C.2. Let A? \u2208 argmaxB:|B|=c fS(B), c \u2265 4/ . Then:\n1\nc c\u2211 i=1 max { 0, 1\u2212 2vS(H(A?-i)) \u00b7 t\u22121/4 } FS(A ? -i) \u2265 (1\u2212 ) fS(A?).\nProof. To bound the average variation of the sets {A?-i}ci=1 we argue that at most one set A?-i will be s.t. fS(A?-i) < fS(A ?)/2. To see this, assume for purpose of contradiction there are A?-i and A ? -j for which fS(A?-i) \u2264 fS(A?-j) < fS(A?)/2, then since A? = A?-i \u222aA?-j we get a contradiction:\nfS(A ?) = fS(A ? -i \u222aA?\u2212j) \u2264 fS(A?-i) + fS(A?\u2212j) < 2 \u00b7\nfS(A ?)\n2 = fS(A\n?).\n61\nWe therefore have at least c\u22121 sets s.t. eachA?-i respects fS(A?-i) \u2265 fS(A?)/2. Call these sets bounded. For any such bounded set A?-i, since A ? -i \u2282 A?ij for any j /\u2208 S \u222aA?, monotonicity implies:\nmin A?ij\u2208H(A?-i)\nfS(A ? ij) \u2265\nfS(A ?)\n2\nFor a given set A?-i note that for every j, every set Aij \u2208 H(A?i ) respects fS(A?ij) \u2264 fS(A?) due to the maximality of A?. Thus for any bounded set A?-i:\nvS(H(A?-i)) = maxA?ij\u2208H(A?i ) fS(A\n? ij)\nminA?ij\u2208H(A?i ) fS(A ? ij) \u2264 fS(A\n?)\nfS(A?)/2 = 2\nLet l be the index of the set A?-i with the lowest value fS(A ? -i). Our discussion above implies that this is the only set whose variation may not be bounded from above by 2. Assume n sufficiently large s.t. t \u2265 212/ 4. We therefore get:\n1\nc c\u2211 i=1 ( max{0, 1\u2212 2vS(H(A?-i))t\u2212 1 4 } ) FS(A ? -i) \u2265 1 c \u2211 i 6=l ( max{0, 1\u2212 2vS(H(A?-i))t\u2212 1 4 } ) FS(A ? -i)\n(16)\n\u2265 1 c \u2211 i 6=l ( 1\u2212 4t\u2212 1 4 ) FS(A ? -i) (17)\n\u2265 1 c \u2211 i 6=l ( 1\u2212 4t\u2212 1 4 ) fS(A ? -i) (18)\n\u2265 ( 1\u2212 4t\u2212 1 4 ) 1 c ( c\u2211 i=1 fS(A ? -i)\u2212 fS(A?\u2212l) ) (19)\n\u2265 ( 1\u2212 4t\u2212 1 4 ) 1 c ( (c\u2212 1)fS(A?)\u2212 fS(A?\u2212l) ) (20)\n\u2265 ( 1\u2212 4t\u2212 1 4 ) 1 c ((c\u2212 1)fS(A?)\u2212 fS(A?)) (21)\n\u2265 ( 1\u2212 4t\u2212 1 4 )(c\u2212 2 c ) fS(A ?) (22)\n\u2265 ( c\u2212 2 c \u2212 4t\u2212 1 4 ) fS(A ?) (23)\n\u2265 (1\u2212 ) fS(A?) (24)\nThe inequality (17) is justified by the bound we established on bounded sets; (18) is due to monotonicity of fS , since FS(A?-i) is an average of the marginal contribution over all possible A ? ij , which is a superset of A?-i; (20) is due to an argument in the proof of Lemma 3.1; (21) is due to the optimality of A?; (24) is due to the assumption on the parameters in the statement of the claim.\nProof of Lemma 3.2. Let A? = arg maxA:|A|=c fS(A) and let B : |B| = c be such that FS(B) < (1\u2212 )FS(A?). We will apply the smoothing arguments and show that with high probability\nF\u0303 (S \u222aA?) > F\u0303 (S \u222aB).\n62\nBy taking a union bound over all possible O(nc) sets B we will then conclude that the set whose smooth noisy contribution is largest must have smooth contribution at least factor of (1\u2212 ) from that of A?, with high probability.\nWe will denote 1 = and 2 = /4. Notice that the conditions of Claim C.2 are met with 2 and that the iteration is 2-significant, which from submodularity implies fS(A?) \u2265 2 \u00b7 f(S)/k.\nFor a set B-i \u2282 B, using Lemma A.5, for t = n\u2212 c\u2212 |S|, when \u03c9 denotes the highest realized value of a noise multiplier, we know that for \u03bb \u2208 [0, 1) with probability 1\u2212 exp ( \u2212\u2126(\u03bb2t1/4/\u03c9) ) :\nF\u0303 (S \u222aB) = 1 c \u2211 i F\u0303(S \u222aB-i)\n< 1\nc \u2211 i (1 + \u03bb)\u00b5 \u00b7 ( f(S) + FS(B-i) + 3t \u22121/4 max Bij\u2208{H(B-i)} fS(Bij) )\n\u2264 (1 + \u03bb)\u00b5 \u00b7 ( f(S) + 3t\u22121/4 max\nBij\u2208{\u222ai\u2208[c]H(B-i)} fS(Bij) +\n1\nc c\u2211 i=1 FS(B-i)\n)\n\u2264 (1 + \u03bb)\u00b5 \u00b7 ( f(S) + 3t\u22121/4fS(A ?) + 1\nc c\u2211 i=1 FS(B-i) ) \u2264 (1 + \u03bb)\u00b5 \u00b7 ( f(S) + 3t\u22121/4fS(A ?) + F (S \u222aB) )\n\u2264 (1 + \u03bb)\u00b5 \u00b7 ( f(S) + 3t\u22121/4fS(A ?) + (1\u2212 1)F (S \u222aA?) )\n\u2264 (1 + \u03bb)\u00b5 \u00b7 ( f(S) + 3t\u22121/4fS(A ?) + (1\u2212 1)fS(A?) )\n= (1 + \u03bb)\u00b5 \u00b7 ( f(S) + fS(A ?) ( 3t\u22121/4 + (1\u2212 1) ))\nWe now need to argue that F\u0303 (S \u222a A?) is sufficiently large to beat F\u0303 (S \u222a B). Assuming n is sufficiently large s.t. t \u2265 220/ 4, from lemmas A.4 and C.2 we know that for \u03bb \u2208 [0, 1) w.p. 1\u2212 e\u2212\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (S \u222aA?) = 1 c c\u2211 i=1 F\u0303(S \u222aA?)\n> (1\u2212 \u03bb)\u00b5 \u00b7 ( f(S) + 1\nc c\u2211 i=1 ( 1\u2212 2v(H(A?i )) \u00b7 t\u22121/4 ) \u00b7 FS(A?) ) > (1\u2212 \u03bb)\u00b5 \u00b7 (f(S) + (1\u2212 2)fS(A?))\n63\nWe therefore get that:\nF\u0303 (S \u222aA?)\u2212 F\u0303 (S \u222aB) \u2265 \u00b5 ( (1\u2212 \u03bb) \u00b7 (f(S) + (1\u2212 2)fS(A?))\u2212 (1 + \u03bb) \u00b7 ( f(S) + fS(A ?) ( 3t\u22121/4 + (1\u2212 1) )))\n\u2265 \u00b5 ( (1\u2212 \u03bb)(1\u2212 2)fS(A?)\u2212 2\u03bbf(S)\u2212 (1 + \u03bb) ( 3t\u22121/4 + (1\u2212 1) ) fS(A ?) )\n\u2265 \u00b5 ( (1\u2212 \u03bb)(1\u2212 2)fS(A?)\u2212 2\u03bbk\n2 fS(A\n?)\u2212 (1 + \u03bb) ( 3t\u22121/4 + (1\u2212 1) ) fS(A ?) ) \u2265 \u00b5 \u00b7 fS(A?) ( (1\u2212 \u03bb)(1\u2212 2)\u2212 2\u03bbk\n2 \u2212 (1 + \u03bb)\n( 3t\u22121/4 + (1\u2212 1) )) \u2265 \u00b5 \u00b7 fS(A?) ( (1\u2212 \u03bb)(1\u2212 2)\u2212 2\u03bbk\n2 \u2212 (1 + \u03bb) ( 4 + (1\u2212 1)) ) \u2265 \u00b5 \u00b7 fS(A?) ( 1\u2212 \u03bb\u2212 2 \u2212 2\u03bbk\n2 \u2212 2 \u2212 \u03bb 2 \u2212 1\u2212 \u03bb+ 1 ) > \u00b5 \u00b7 fS(A?) ( 1 \u2212 3 2 \u2212 \u03bb ( 2k\n2\n))\nFor any \u03bb \u2264 2/2k the difference above is strictly positive. Conditioning on \u03c9 being bounded from above by t1/5 which happens with probability 1\u2212 e\u2212\u2126(t1/5/ log t), since k \u2208 O(log log n) we that the result holds with probability at least 1\u2212 e\u2212\u2126(t1/10).\nApproximation Guarantee in Expectation\nLemma (3.3). Let \u03b4 > 0 and assume k > 16/\u03b42, c = 16/\u03b4. Suppose that in every \u03b4/4-significant iteration of SM-GREEDY when S are the elements selected in previous iterations, A \u2208 argmaxB:|B|=c F\u0303 (S \u222a B), the bundle added A\u0302 respects fS(A\u0302) \u2265 (1\u2212 \u03b4)FS(A). Let S\u0304 be the solution after bk/cc iterations. Then, w.p. \u2265 1\u2212 1/n2:\nf(S\u0304) = (1\u2212 1/e\u2212 5\u03b4)OPT.\nProof. We will analyze the solution only on iterations that are \u03b4/4 relevant since this is when we can apply the smoothing arguments. Since k > 16/\u03b42 and since each iteration is \u03b4/4-significant, by Lemma 3.2 we know that in each iteration A \u2208 argmaxB:|B|=c F\u0303 (S \u222aB) respects with overwhelming probability:\nFS(A) \u2265 (1\u2212 \u03b4) max B:|B|=c FS(B)\nWe will condition on the success of this event in every one of the bk/cc iterations. By a union bound the result will hold w.p. at least 1\u2212 1/n2. We assume that n is sufficiently large s.t. t \u2265 220/\u03b44.\nTo account for the fact that we are only analyzing \u03b4/4-significant iterations, we can compare against (1 \u2212 \u03b4/4) of the optimal value: let k\u0302 be the last \u03b4/4-significant iteration and O\u0302 \u2286 O be the subset of size k\u0302 of the optimal solution whose value is largest. By submodularity:\nf(O\u0302) \u2265 (1\u2212 \u03b4/4)OPT (25)\n64\nSecond, we argue that optimizing over sets of size c rather than singletons is inconsequential when k > c/ . To be convinced, notice that when the algorithm selects c elements in every iteration the total number of elements selected will be k\u2032 > k \u2212 c. Let O\u2032 \u2208 arg maxT :|T |\u2264k\u2032 f(T ). As in previous arguments, from submodularity we have that: (1\u2212 c/k)f(O\u0302) \u2264 f(O\u2032). Since k > c/ we have that:\nf(O\u2032) > (1\u2212 \u03b4)f(O\u0302) > (1\u2212 2\u03b4)OPT (26)\nWe will henceforth analyze the algorithm against O\u2032. In a similar manner to the analysis of the greedy algorithm which selects singletons at every stage i \u2208 [k], we can analyze the greedy algorithm which selects sets of size c at every stage i \u2208 [k\u2032/c]. To ease notation assume bk\u2032/cc = k\u2032/c.\nFor a given stage of the algorithm, assume the set S has been previously selected and that a set A\u0302 is being added into the solution. Let B? = arg maxB\u2286O\u2032:|B|=c fS(B) and A? = arg maxB:|B|=c fS(B).\nfS(A\u0302) \u2265 (1\u2212 \u03b4) max B:|B|=c FS(B) assumption in the statement\n> (1\u2212 2\u03b4)FS(A?) Lemma 3.2 applied with = \u03b4 > (1\u2212 3\u03b4)fS(A?) Lemma 3.1 and c \u2265 1/\u03b4 > (1\u2212 3\u03b4)fS(B?) maximality of A? > (1\u2212 3\u03b4) c k\u2032 \u00b7 fS(O\u2032) subadditivity.\n= (1\u2212 3\u03b4) c k\u2032 \u00b7 ( f(O\u2032 \u222a S)\u2212 f(S) ) \u2265 (1\u2212 3\u03b4) c\nk\u2032 \u00b7 ( f(O\u2032)\u2212 f(S) ) A standard inductive argument stating that at every iteration i \u2208 bk/cc we have that the value of the current solution is at least ( 1\u2212 (1\u2212 1/bk/cc)i ) OPT implies that f(S\u0304) \u2265 (1\u2212 1/e\u2212 3\u03b4) f(O\u2032). Since we lose 2\u03b4 from (26) this concludes our proof.\nFrom Expectation to High Probability\nDefinition C.3. For a given set S, let A? \u2208 argmaxB:|B|=c fS(B), A \u2208 argmaxB:|B|=c F\u0303 (S \u222a B), and A = {Aij}i\u2208A,j /\u2208A. For a fixed > 0:\n\u2022 Aij \u2208 A is -good if fS(Aij) \u2265 (1\u2212 2 )fS(A?); let good(A) denote all -good Aij \u2208 A;\n\u2022 Aij \u2208 A is -bad if fS(Aij) \u2264 (1\u2212 3 )fS(A?); let bad(A) denote all -bad Aij \u2208 A.\nClaim C.4. For a set S \u2286 N let A \u2208 argmaxB:|B|=c F\u0303 (S \u222aB) and assume the iteration is /8-significant and that c \u2265 /2. Then with probability at least 1\u2212 1/n10:\n\u2022 |good(A) | \u2265 c(n\u2212c\u2212|S|)2 ;\n\u2022 |bad(A) | \u2264 c(n\u2212c\u2212|S|)2 .\n65\nProof. Since the sets Aij are distinct both good(A) and bad(A) contain no repetitions and we can argue about their size. To lower bound the size of good(A), let A? \u2208 argmaxA:|A|=c fS(A). When the iteration is /8-significant, from Lemma 3.2 we know that with exponentially high probability:\nFS(A) \u2265 (1\u2212 /2)FS(A?)\nWhen c \u2265 2/ , from Lemma, we know that:\nFS(A ?) \u2265 (1\u2212 /2)fS(A?)\nDenoting m = c(n\u2212 c\u2212 |S|), we get with exponentially high probability:\nFS(A) = 1\nm m\u2211 j=1 c\u2211 i=1 fS(Aij) \u2265 (1\u2212 )fS(A?) (27)\nIn addition, due to the maximality of A? we have that fS(Aij) \u2264 fS(A?) for every i, j. Therefore:\nm\u2211 j=1 c\u2211 i=1 fS(Aij) \u2264 |good(A) | \u00b7 fS(A?) + (m\u2212 |good(A) |) \u00b7 (1\u2212 2 )fS(A?) (28)\nPutting (27) and (28) together we get that for sufficiently large n, with probability at least 1\u22121/n10:\nm(1\u2212 )fS(A?) \u2264 (|good(A) |+ (m\u2212 |good(A) |)(1\u2212 2 )) fS(A?)\nRearranging and using m = c(n \u2212 c \u2212 |S|) we get that |good(A) | \u2265 c(n \u2212 c \u2212 |S|)/2. Since there are a total of c(n\u2212 c\u2212 |S|) it follows that |bad(A) | \u2264 c(n\u2212 c\u2212 |S|) as required.\nDefinition C.5. Let \u03c1(x) denote the probability density function of the noise distribution. For a set S : |S| \u2208 O(log n), c > 0, \u03b3 > 0, we define \u03b8g and \u03b8b as:\n\u2022 \u222b\u221e \u03b8b \u03c1(x)dx = 2c(n\u2212c\u2212|S|) logn ;\n\u2022 \u222b\u221e \u03b8g \u03c1(x)dx = 2 lognc(n\u2212c\u2212|S|) .\nThe following claim immediately follows from the definition, yet it is still useful to specify explicitly. The claim considers c(n \u2212 c \u2212 |S|)/2 samples since this is an upper and lower bound on |good(A) | and |bad(A) |. Therefore the claim gives us the likelihood that the largest noise multiplier of bad(A) does not exceed \u03b8b and that at least one set from good(A) exceeds \u03b8g.\nClaim C.6. For a fixed set S and A \u2208 argmaxB:|B|=c F\u0303 (S \u222aB), let m = c(n\u2212 c\u2212 |S|) and consider m/2 independent samples from the noise distribution. Then:\n\u2022 Pr [ max{\u03be1, . . . \u03bem/2} \u2264 \u03b8b ] > ( 1\u2212 2logn ) ;\n\u2022 Pr [ max{\u03be1 . . . \u03bem/2} \u2265 \u03b8g ] > 1\u2212 2/n.\n66\nProof. For a single sample \u03be from D, we have that:\nPr[\u03be \u2264 \u03b8b] = 1\u2212 2\nm log n\nIf we take m/2 independent samples \u03be1, . . . \u03bem/2, the probability they are all bounded by \u03b8b is:\nPr [ max{\u03be1, . . . \u03be|bad(A) |} \u2264 \u03b8b ] \u2265 (\n1\u2212 2 m log n\n)m 2\n> ( 1\u2212 2\nlog n ) In the case of \u03b8g, the probability that a single sample \u03be taken from D is at most \u03b8g is equal to:\nPr [\u03be \u2264 \u03b8g] = 1\u2212 2 log n\nm\nIf we take independent samples \u03be1, . . . \u03bem/2, the probability they are all bounded by \u03b8g is:\nPr [ max{\u03be1, . . . \u03bec(n\u2212c\u2212|S|)} \u2264 \u03b8g ] = ( 1\u2212 2 log n\nm\n)m 2\n< 2\n2logn =\n2\nn\nAnd accordingly the probability that at least one of these samples is greater than \u03b8g is:\nPr [ max{\u03be1 . . . \u03bem/2} \u2265 \u03b8g ] > 1\u2212 2/n.\nShowing \u03b8g is arbitrarily close to \u03b8b. Lemma C.8 below relates \u03b8g and \u03b8b assuming that D has a generalized exponential tail. This lemma makes the result applicable for Exponential and Gaussian distributions, and it fully leverages the fact that k \u2208 O(log log n). The lemma is quite technical, and we therefore first prove the much simpler case where the distribution is bounded.\nLemma C.7. Assume D has a generalized exponential tail and that D is bounded, then for all \u03b3 \u2208 \u2126(1/ log log n) we have that \u03b8g \u2265 (1\u2212 \u03b3) \u03b8b.\nProof. Let \u03c7 be an upper bound on D. If there is an atom at \u03c7 with some probability \u03b3 > 0, then we are done, as \u03b8g = \u03b8b = \u03c7. Otherwise, since D has a generalized exponential tail we know that \u03c1(\u03c7) = \u03b3 for some \u03b3 > 0, and that \u03c1 is continuous at \u03c7. But then there is some \u03b4 > 0 such that for any \u03c7\u2212 \u03b4 \u2264 x \u2264 \u03c7 we have that \u03c1(x) \u2265 \u03b3/2. Choosing n to be large enough that (1\u2212 )\u03b3 > \u03b3 \u2212 \u03b4, we have that\n\u222b \u03b3 (1\u2212 )\u03b3 \u03c1(x) \u2265 \u03b3/2\nChoosing n large enough such that\n2 log n\nc(n\u2212 c\u2212 |S|) < \u03b3/2\nGives that \u03b8g \u2265 (1\u2212 )\u03c7. As \u03b8b \u2264 \u03c7 we are done.\n67\nLemma C.8. If D has a generalized exponential tail then (1\u2212 \u03b3) \u03b8b \u2264 \u03b8g, \u2200 \u03b3 \u2208 \u2126(1/ log logn).\nProof. The proof follows three stages:\n1. We use properties of D to argue upper and lower bounds for \u03c1(x);\n2. We show an upper bound M on \u03b8b;\n3. We show that integrating a lower bound of \u03c1(X) from (1 \u2212 \u03b3)M to\u221e, yields a probability mass at least logn\u03b3 c(n\u2212c\u2212|S|) . Now suppose for contradiction that \u03b8g < (1 \u2212 \u03b3) \u03b8b, we would get that \u222b\u221e \u03b8g \u03c1(x) is strictly greater than logn\u03b3 c(n\u2212c\u2212|S|) , which contradicts the definition of \u03b8g.\nWe now elaborate each on stage. Recall that by definition of D for x \u2265 x0, we have that \u03c1(x) = e\u2212g(x), where g(x) = \u2211 i ai\u03b1i and that we do not assume that all the \u03b1i\u2019s are integers, but only that \u03b10 \u2265 \u03b11 \u2265 . . ., and that \u03b10 \u2265 1. We do not assume anything on the other \u03b1i values.\nFor the first stage we will show that for every g(x), there exists n0 such that for any n > n0 and x \u2265 (\nlogn 2a0\n)1/\u03b10 we have that for \u03b2 = \u03b3 /100 < 1/100:\n(1 + \u03b2)a0x \u03b10\u22121e\u2212(1+\u03b2)a0x \u03b10 \u2264 \u03c1(x) \u2264 (1\u2212 \u03b2)a0x\u03b10\u22121e\u2212(1\u2212\u03b2)a0x \u03b10\nWe explain both directions of the inequality. To see a0x\u03b10\u22121(1 + \u03b2)e\u2212(1+\u03b2)a0x \u03b10 \u2264 \u03c1(x) we first show: e\u2212(1+\u03b2/2)a0x \u03b10 \u2264 \u03c1(x)\nThis holds since for sufficiently large n, we have that:\nx \u2265 (log n) 1/\u03b10 2a0 \u2265 ( 2 \u2211 i=1 |ai| \u03b2a0 )\u03b10\u2212\u03b11 So the term \u03b22x \u03b10 dominates the rest of the terms. We now show that:\ne\u2212(1+\u03b2/2)a0x \u03b10 \u2265 a0x\u03b10\u22121(1 + \u03b2)e\u2212(1+\u03b2)a0x \u03b10\nThis is equivalent to: e\u03b2a0/2x \u03b10 \u2265 a0x\u03b10\u22121(1 + \u03b2)\nWhich hold for x = log log3 n and large enough n.\nThe other side of the inequality is proved in a similar way. We want to show that:\n\u03c1(x) \u2264 (1\u2212 \u03b2)a0x\u03b10\u22121e\u2212(1\u2212\u03b2)a0x \u03b10\nClearly for x > log log3 n we have that (1\u2212 \u03b2)a0x\u03b10\u22121 > 1. Hence we just need to show that:\n\u03c1(x) \u2264 e\u2212(1\u2212\u03b2)a0x\u03b10\nBut this holds for sufficiently large n s.t.:\nx \u2265 (log n) 1/\u03b10 2a0 \u2265 (\u2211 i=1 |ai| \u03b2a0 )\u03b10\u2212\u03b11 68\nWe now proceed to the second stage, and compute an upper bound on \u03b8b. Note that if\u222b \u221e \u03b8b \u03c1(x) = \u222b \u221e M g(x)\nand for every x \u2265 M we have \u03c1(x) \u2264 g(x) then it must be that M \u2265 \u03b8b. Applying this to our setting, we bound \u03c1(x) \u2264 (1\u2212 \u03b2)a0x\u03b10\u22121e\u2212(1\u2212\u03b2)a0x \u03b10 to get:\n1\nc(n\u2212 c\u2212 |S|) log n = \u222b \u221e M (1\u2212 \u03b2)a0x\u03b10\u22121e\u2212(1\u2212\u03b2)a0x \u03b10\n= \u2212e\u2212(1\u2212\u03b2)a0x\u03b10 |\u221eM = e\u2212(1\u2212\u03b2)a0M \u03b10\nTaking the logarithm of both sides, we get:\n\u2212(1\u2212 \u03b2)a0M\u03b10 = log 1\nc(n\u2212 c\u2212 |S|) log n = \u2212 log(c(n\u2212 c\u2212 |S|) log n)\nMultiplying by \u22121, dividing by (1\u2212 \u03b2)a0 and taking the 1/\u03b10 root we get:\nM =\n( log(c(n\u2212 c\u2212 |S|) log n)\n(1\u2212 \u03b2)a0 )\u03b10 Note that (1\u2212 \u03b3)M > ( logn 2a0 )1/\u03b10 and hence our bounds on \u03c1(x) hold for this regime.\nWe move to the third stage, and bound \u222b\u221e (1\u2212\u03b3)M \u03c1(x) from below. If we show that: \u222b\u221e (1\u2212\u03b3)M \u03c1(x) is greater than logn\u03b3 c(n\u2212c\u2212|S|) , this implies that \u03b8g \u2265 (1 \u2212 \u03b3)M , as \u03b8g is defined as the value such that when we integrate \u03c1(x) from \u03b8g to\u221ewe get exactly logn\u03b3 c(n\u2212c\u2212|S|) . We show:\u222b \u221e\n(1\u2212\u03b3)M \u03c1(x) \u2265 (1 + \u03b2)a0\u03b10x\u03b10\u22121e\u2212(1+\u03b2)a0x \u03b10\n= \u2212e\u2212(1+\u03b2)a0x\u03b10 |\u221e(1\u2212\u03b3)M = e\u2212(1+\u03b2)a0((1\u2212\u03b3)M) \u03b10\n= e\u2212(1+\u03b2)a0M \u03b10 (1\u2212\u03b3)\u03b10\n\u2265 e\u2212(1+\u03b2)a0M\u03b10 (1\u2212\u03b3)\nHowever a0M\u03b10 = ( log(c(n\u2212c\u2212|S|) logn) (1\u2212\u03b2) ) . Since \u03b2 < 0.1 we have that 1+\u03b21\u2212\u03b2 < 1 + 3\u03b2. Substituting both expressions we get:\ne\u2212(1+\u03b2)a0M \u03b10 (1\u2212\u03b3) \u2265 e\u2212(1+3\u03b2)(1\u2212\u03b3) log(c(n\u2212c\u2212|S|) logn)\n=\n( 1\nc(n\u2212 c\u2212 |S|) log n )(1\u2212\u03b3)(1+3\u03b2) \u2265 ( 1\nc(n\u2212 c\u2212 |S|) log n\n)(1\u2212\u03b3 /2)\n69\nWhere we used that \u03b2 = \u03b3 /100 and hence (1 \u2212 \u03b3)(1 + 3\u03b2) < 1 \u2212 \u03b3 /2. We now need to compare this to \u221a logn\n\u03b3 c(n\u2212c\u2212|S|) . To do this, note that:( 1\nc(n\u2212 c\u2212 |S|) log n )(1\u2212\u03b3 /2) \u2265 1 c(n\u2212 c\u2212 |S|)1\u2212\u03b3 /2 log n\n\u2265 2 \u221a logn\nc(n\u2212 c\u2212 |S|) log n\n\u2265 log n \u03b3 c(n\u2212 c\u2212 |S|)\nWhere n is large enough that \u03b32 log(n \u2212 c \u2212 |S|) > \u221a\nlog n. This completes the proof, since \u03b8g \u2265 (1\u2212 \u03b3)M \u2265 (1\u2212 \u03b3) \u03b8b as required.\nLemma (3.4). For any > 0, suppose we run SM-GREEDY where in each iteration we add a bundle of elements of size c = 16/ . For any /8-significant iteration where the set previously selected is S : |S| \u2208 O(log log n), let A \u2208 argmax F\u0303 (S \u222aA) and A\u0302 = argmax(i,j)\u2208A\u00d7N\\S\u222aA f\u0303(S \u222aAij). Then, with probability at least 1\u2212 3/ log n we have that:\nfS(A\u0302) \u2265 (1\u2212 3 )FS(A).\nProof. We will use the above claims to argue that with probability at least 1 \u2212 4/ log n the noisy mean value of any set in bad(A) is smaller than the largest noisy mean value of a set in good(A). Since a bad set is defined as a set B for which fS(B) \u2264 (1 \u2212 3 )fS(A?) this implies that the set returned by the algorithm has value at least (1 \u2212 3 )fS(A?). Since for any set A : |A| = c we have that fS(A?) is an upper bound on FS(A) will complete the proof.\nWe will condition on the event that |good(A) | \u2265 c(n\u2212 c\u2212 |S|)/2 which happens with probability at least 1 \u2212 1/n10 from Claim C.4. Under this assumption, from Claim C.6 we know that with probability at least 1\u22122/n at least one of the noise multipliers of sets in good(A) has value at least \u03b8g, and from Lemma C.8 we know that \u03b8g \u2265 (1\u2212 \u03b3) \u03b8b for any \u03b3 \u2208 \u0398(1/ log logn). Thus:\nmax Aij\u2208good(A) f\u0303(S \u222aAij) = max Aij\u2208good(A) \u03beAij \u00d7 [ f(S) + fS(Aij) ]\n\u2265 \u03b8g \u00d7 [ f(S) + (1\u2212 2 )fS(A?) ] \u2265 (1\u2212 \u03b3) \u03b8b \u00d7 [ f(S) + (1\u2212 2 )fS(A?) ]\nLet B \u2208 argmaxC\u2208bad(A) f\u0303(S \u222aC). From Claim C.6 we know that w.p. at least 1\u2212 2/ log n all noise multipliers of sets in bad(A) are at most \u03b8b. Thus:\nf\u0303(S \u222aB) = max Aij\u2208bad(A) f\u0303(S \u222aAij) = max Aij\u2208bad(A) \u03beAijf(S \u222aAij) \u2264 \u03b8b \u00b7[f(S) + (1\u2212 3 )fS(A?)]\nLet d be some constant such that |S| \u2264 d log log n. Note that the iteration is -significant, and therefore due to the maximality of A? and since f(S) \u2264 OPT and the optimal solution has at most d \u00b7 log logn elements we have that:\nfS(A ?) \u2265\nd log logn f(S).\n70\nSince Lemma C.8 applies to any \u03b3 \u2208 \u0398(1/ log logn), we know that for any constant d there is a large enough value of n such that \u03b3 < 2/3d log log n. Putting it all together and conditioning on all events we have with probability at least 1\u2212 3/ log n:\nf\u0303(S \u222a A\u0302)\u2212 f\u0303(S \u222aB) \u2265 ( (1\u2212 \u03b3) \u03b8b \u00b7[f(S) + (1\u2212 2 )fS(A?)] ) \u2212 ( \u03b8b \u00b7[f(S) + (1\u2212 3 )fS(A?)] ) \u2265 \u03b8b ( fS(A ?) \u2212 \u03b3 \u00d7 [ (1\u2212 2 )fS(A?) + f(S) ] )\n\u2265 \u03b8b ( fS(A ?) \u2212 \u03b3 \u00d7 [ (1\u2212 2 )fS(A?) + d log logn fS(A ?) ] )\n= \u03b8b fS(A ?) ( \u2212 \u03b3 \u00d7 [ (1\u2212 2 ) + d log logn ] ) > \u03b8b fS(A ?) ( \u2212 2\n3d log log n \u00d7\n[ (1\u2212 2 ) + d log log n ] ) > \u03b8b fS(A ?) (\n\u2212 2 3 ) > 0\nSince the difference is strictly positive this implies that with probability at least 1 \u2212 3/ log n a bad set will not be selected by the algorithm which concludes our proof.\nApproximation Guarantee of SM-Greedy\nTheorem C.9. For any monotone submodular function f : 2N \u2192 R and > 0, when k \u2208 \u2126(1/ ) \u2229O(log log n), there is a (1 \u2212 1/e \u2212 ) approximation for maxS:|S|\u2264k f(S), with probability 1\u2212 4/ log n given access to a noisy oracle whose distribution has a generalized exponential tail.\nProof. First, for the case in which k \u2208 \u2126(1/ 2), we can apply SM-GREEDY as described in the main body of the paper. Let \u03b4 = /5 and set c = 16/\u03b4. At any given \u03b4/8-significant iteration of SM-GREEDY from Lemma 3.4 we know that with probability at least 1 \u2212 3/ log n we have that f(A\u0302) \u2265 (1\u2212\u03b4)FS(A), whereA \u2208 argmaxB:|B|=c F\u0303 (B). We can then apply Lemma 3.3 which implies that with probability at least 1\u2212 4logn we have a 1\u2212 1/e\u2212 5\u03b4 = (1\u2212 1/e\u2212 ) approximation.\nIn the case k \u2208 \u2126(1/ ) \u2229 O(1/ 2) note that taking bundles of size c \u2208 O(1/ ) in each iteration may result in a 1/2 approximation. In this case, we therefore enumerate over all possible sets of size c = k and output A\u0302 = argmax f\u0303(Aij) whereA = argmaxB:|B|=k F\u0303 (B). By Lemma 3.4 we know that w.p. 1\u2212 3 log n:\nf(A\u0302) \u2265 (1\u2212 48/c)F (A) = (1\u2212 48/k)F (A) \u2265 (1\u2212 /2)F (A) (29)\nBy the smoothing lemma (Lemma 3.2) we know that for any fixed and sufficiently large n with overwhelming probability F (A) \u2265 (1 \u2212 /2)F (A?) for A? \u2208 argmaxB:|B|=k f(B). By the sampled mean method (Lemma 3.1) we know that F (A?) \u2265 (1\u2212 1/k)f(A?), thus:\nF (A) \u2265 (1\u2212 1/k \u2212 /2)f(A?) (30)\nPutting (29) and (30) together and taking a union bound we get our result.\n71"}, {"heading": "D Optimization for Very Small k", "text": "Smoothing Guarantees\nLemma (4.1). Let A \u2208 argmaxB:|B|=k F\u0303 (B). Then, for any fixed > 0 w.p. 1\u2212 e\u2212\u2126( 2(n\u2212k)):\nF (A) \u2265 (1\u2212 ) max B:|B|=k F (B)\nProof. The proof follows the same reasoning as those from previous sections. Let A? = argmaxB:|B|=k F (B). We will show that w.h.p. no set B for which F (B) < (1 \u2212 )F (A?) beats A. The size of the smoothing set is t = n\u2212 k, and \u03c9 is an upper bound on the noise multiplier.\nNote that the optimality ofA? and submodularity imply that f(A?\u222ax) \u2264 2f(A?), for all x \u2208 N \\A?. Hence from monotonicity the variation is bounded by 2:\nv(A?) = maxx\u2208N\\A f(A ? \u222a x) minx\u2208N\\A f(A? \u222a x) \u2264 2f(A ?) f(A?) = 2\nWe can therefore apply Lemma A.5 and get that with probability at least 1\u2212 e\u2126(\u03bb2t1/4/\u03c9):\nF\u0303 (A?) \u2265 (1\u2212 \u03bb)\u00b5 ( 1\u2212 4t\u22121/4F (A?) )\nTo upper bound F\u0303 (B) for a set B s.t. F (B) < (1\u2212 )F (A?), note that the value of largest set in the smoothing neighborhood is maxx\u2208N\\B f(B \u222a x) \u2264 2f(A?). Hence, from Lemma A.4 we get that with probability at least 1\u2212 e\u2126(\u03bb2t1/4/\u03c9):\nF (B) \u2264 (1 + \u03bb)\u00b5 ( F (B) + 6t\u22121/4F (A?) ) Therefore when n is sufficiently large s.t. t\u22121/4 \u2264 /100 and \u03bb < 1 we get that:\nF (A?)\u2212 F (B) \u2265 (1\u2212 \u03bb)\u00b5(1\u2212 4t\u22121/4)F (A?)\u2212 (1 + \u03bb)\u00b5 ( F (B) + 6t\u22121/4F (A?) ) \u2265 \u00b5 ( (1\u2212 \u03bb)(1\u2212 4\n100 )F (A?)\u2212 (1 + \u03bb)(1\u2212 )F (A?)\u2212 (1 + \u03bb) 6 100 F (A?) ) \u2265 \u00b5 ( (1\u2212 \u03bb)(1\u2212 4\n100 )F (A?)\u2212 (1 + \u03bb)(1\u2212 )F (A?)\u2212 (1 + \u03bb) 6 100 F (A?) ) > \u00b5 \u00b7 F (A?) ( \u2212 2\u03bb\u2212 /5)\nUsing \u03bb < /10 the above inequality is strictly positive. Conditioning on the event of \u03c9 being sufficiently small completes the proof."}, {"heading": "An Approximation Algorithm for Very Small k", "text": "Approximation guarantee in expectation. We first present the algorithm whose approximation guarantee is arbitrarily close to k/(k + 1), in expectation.\n72\nAlgorithm 5 EXP-SMALL-GREEDY Input: budget k\n1: A\u2190 arg maxB :|B|=k F\u0303 (B) 2: x\u2190 select random element from N \\A 3: A\u0302\u2190 random set of size k from A \u222a x 4: return A\u0302\nTheorem D.1. For any submodular function f : 2N \u2192 R, the algorithm EXP-SMALL-GREEDY obtains returns a (k/(k + 1)\u2212 ) approximation for maxS:|S|\u2264k f(S), in expectation, for any fixed > 0.\nProof. From Lemma 3.1 we know that f(A\u0302) \u2265 (k/(k + 1))F (A). Let A? = argmaxB:|B|=k f(B). From monotonicity we know that f(A?) \u2264 F (A?). Applying Lemma 4.1 we get that for the set F (A) \u2265 (1\u2212 )F (A?). Hence:\nf(A\u0302) \u2265 ( k\nk + 1\n) F (A) \u2265 (1\u2212 ) ( k\nk + 1\n) F (A?) \u2265 (1\u2212 ) ( k\nk + 1\n) f(A?) > (( k\nk + 1\n) \u2212 ) OPT.\nHigh probability. To obtain a result w.h.p. we modify the algorithm above. The algorithm enumerates all possible subsets of size k \u2212 1, and then select the set A \u2208 argmaxB:|B|=k\u22121 F\u0303 (B). The algorithm then selects A\u0302 \u2208 argmaxX\u2208H(A) f\u0303(X). A formal description is added below.\nAlgorithm 6 WHP-SMALL-GREEDY Input: budget k\n1: A\u2190 arg maxB :|B|=k\u22121 F\u0303 (B) 2: A\u0302\u2190 argmaxx\u2208N\\A f\u0303(A \u222a x) 3: return A\u0302\nThe analysis of the algorithm is similar to the high probability proof from Section 3.\nTheorem (4.2). For any submodular function f : 2N \u2192 R and any fixed > 0 and constant k, there is a (1\u2212 1/k \u2212 )-approximation algorithm for maxS:|S|\u2264k f(S) which only uses a generalized exponential tail noisy oracle, and succeeds with probability at least 1\u2212 6/ log n.\nProof. Let A \u2208 argmaxB:|B|=k\u22121 F\u0303 (B), and let A? \u2208 argmaxB:|B|=k\u22121 f(B). Since A? is the optimal solution over k\u22121 elements, from submodularity we know that f(A?) \u2265 (1\u22121/k)OPT. What now remains to show is that A\u0302 \u2208 argmaxx\u2208N\\A f\u0303(A \u222a x) is a (1 \u2212 ) approximation to F (A). To do so recall the definitions of good and bad sets from the previous section. Let \u03b4 = /3. Suppose that a set X is in \u03b4-good(A) if f(X) \u2265 (1\u2212 2\u03b4)f(A?) and in \u03b4-bad(A) if f(X) \u2264 (1\u2212 3\u03b4)f(A?). We will show that the set selected has value at least as high as that of a bad set, i.e. (1 \u2212 3\u03b4)f(A?) which will complete the proof.\nWe first show that with probability at least 1 \u2212 6/ log n the noise multiplier of some good set is at least \u03b8g and of a bad set is at most \u03b8b. To do so we will first argue about the size of \u03b4-good(A)\n73\nand \u03b4-bad(A). From Lemma 4.1 and the maximality of A we know that with exponentially high probability F (A) \u2265 (1\u2212 \u03b4)F (A?). Therefore for m = n\u2212 k:\nF (A) = 1\nm \u2211 x/\u2208A f(A \u222a x) \u2265 (1\u2212 \u03b4) 1 m \u2211 x/\u2208A? f(A? \u222a x) \u2265 (1\u2212 \u03b4)f(A?)\nDue to the maximality of A? and submodularity we know that f(A \u222a x) \u2264 2f(A?) for all x /\u2208 A:\u2211 x/\u2208A f(A \u222a x) \u2264 |\u03b4-good(A)|2f(A?) + (m\u2212 |\u03b4-good(A)|)(1\u2212 2\u03b4)f(A?)\nPutting the these bounds on F (A) together and rearranging we get that:\n|\u03b4-good(A)| \u2265 \u03b4 \u00b7m 1 + 2 \u2265 \u03b4m 3\nTherefore, for sufficiently large n the likelihood of at least one set achieving value at least \u03b8g is:\nPr[max{\u03be1, . . . , \u03be\u03b4\u00b7m/3} \u2265 \u03b8g] \u2265 1\u2212 (\n1\u2212 2 log n m\n) \u03b4m 3\n\u2265 1\u2212 2 n\u03b4/3 \u2265 1\u2212 1 log n\nTo bound \u03b4-bad(A) we will simply note that it is trivial that \u03b4-bad(A) < m. Thus, the likelihood that all noise multipliers of bad sets are bounded from above by \u03b8b is:\nPr [max{\u03be1, . . . \u03bem} \u2264 \u03b8b] \u2265 (\n1\u2212 2 m log n\n)m > ( 1\u2212 4\nlog n ) Thus, by a union bound and conditioning on the event in Lemma 4.1 we get that \u03b8b is an upper bound on the value of the noise multiplier of bad sets and \u03b8g is with lower bound on the value of the noise multiplier of a good stem all with probability at least 1 \u2212 6/ log n. From Lemma C.8 we know that for any \u03b3 \u2208 \u0398(1/ log log n) we have that \u03b8g \u2265 (1\u2212 \u03b3) \u03b8b. Thus:\nmax X\u2208\u03b4-good(A) f\u0303(X) = max X\u2208\u03b4-good(A)\n\u03beXf(X) \u2265 \u03b8g \u00b7(1\u2212 2\u03b4)f(A?) \u2265 (1\u2212 \u03b3)Mb \u00b7 (1\u2212 2\u03b4)f(A?)\nLetB \u2208 argmaxC\u2208\u03b4-bad f\u0303(S\u222aC). From Claim C.6 we know that with probability at least 1\u22122/ log n all noise multipliers of sets in bad(A) are at most \u03b8b. Thus:\nf\u0303(S \u222aB) = max X\u2208\u03b4-bad f\u0303(X) = max X\u2208bad(A) \u03beXf(X) \u2264Mb \u00b7 (1\u2212 3\u03b4)f(X)\nPutting it all together we have with probability at least 1\u2212 6/ log n:\nf\u0303(A\u0302)\u2212 f\u0303(B) \u2265Mbf(A?) \u00b7 ((1\u2212 \u03b3)(1\u2212 2\u03b4)\u2212 (1\u2212 3\u03b4)) > \u03b8b f(A?) (\u03b4 \u2212 \u03b3)\nSince Lemma C.8 applies to any \u03b3 \u2208 \u0398(1/ log log n), and \u03b4 is fixed it applies to \u03b3 < \u03b4 and the difference is positive. Since \u03b4 = /6 this completes our proof.\n74\nInformation Theoretic Lower Bounds for Constant k\nSurprisingly, even for k = 1 no algorithm can obtain an approximation better than 1/2, which proves a separation between large and small k.5 The following is a tight bound for k = 1.\nClaim D.2. There exists a submodular function and noise distribution for which w.h.p. no randomized algorithm with a noisy oracle can obtain an approximation better than 1/2 +O(1/ \u221a n) for maxa\u2208N f(a).\nProof. We will construct two functions that are identical except that one function attributes a value of 2 for a special element x? and 1 for all other elements, whereas the other is assigns a value of 1 for each element. In addition, these functions will be bounded from above by 2 so that the only queries that gives any information are those of singletons. More formally, consider the functions f1(S) = min{|S|, 2} and f2(S) = min{g(S), 2}where g : 2N \u2192 R is defined for some x? \u2208 N as:\ng(S) =\n{ 2, if S = x?\n|S|, otherwise\nThe noise distribution will return 2 with probability 1/ \u221a n and 1 otherwise.\nWe claim that no algorithm can distinguish between the two functions with success probability greater than 1/2 + O(1/ \u221a n). For all sets with two or more elements, both functions return 2, and so no information is gained when querying such sets. Hence, the only information the algorithm has to work with is the number of 1, 2, and 4 values observed on singletons. If it sees the value 4 on such a set, it concludes that the underlying function is f2. This happens with probability 1/ \u221a n.\nConditioned on the event that the value 4 is not realized, the only input that the algorithm has is the number of 1s and 2s it sees. The optimal policy is to choose a threshold, such if a number of 2s observed is or above this threshold, the algorithm returns f2 and otherwise it reruns f1. In this case, the optimal threshold is \u221a n+ 1. The probability that f2 has at most \u221a n twos is 1/2\u2212 1/ \u221a n, and so is the probability that f1 has at least \u221a n+ 1 twos, and hence the advantage over a random guess is O(1/ \u221a n) again. An algorithm which approximates the maximal set on f2 with ratio better than 1/2 +\u03c9(1/ \u221a n) can be used to distinguish the two functions with advantage \u03c9(1/ \u221a n). Having ruled this out, the best approximation one can get is 1/2 +O(1/ \u221a n) as required.\nWe generalize the construction to general k. The lower for general k behaves like 2k/(2k \u2212 1), where our upper bound is (k \u2212 1)/k.\nClaim D.3. There exists a submodular function and noise distribution for which w.h.p. no randomized algorithm with a noisy oracle can obtain an approximation better than (2k \u2212 1)/2k + O(1/ \u221a n) for the optimal set of size k.\n5We note that if the algorithm is not allowed to query the oracle on sets of size greater than k, Claim D.2 can be extended to show aO(n) inapproximability, so choosing a random element is almost the best possible course of action.\n75\nProof. Consider the function:\nf1(S) =  2|S|, if |S| < k 2k \u2212 1, if |S| = k 2k, if |S| > k\nand the function f2, which is dependent on the identity of some random set of size k, denoted S? :\nf2(S;S ?) =  2|S|, if |S| < k 2k \u2212 1, if |S| = k, S 6= S? 2k, if S = S?\n2k, if |S| > k\nNote that both functions are submodular.\nThe noise distribution will return 2k/(2k \u2212 1) with probability n\u22121/2 and 1 otherwise. Again we claim that no algorithm can distinguish between the functions with probability greater than 1/2. Indeed, since f1, f2 are identical on sets of size different than k, and their value only depends on the set size, querying these sets doesn\u2019t help the algorithm (the oracle calls on these sets can be simulated). As for sets of size k, the algorithm will see a mix of 2k\u22121, 2k, and at most one value of 4k2/(k\u22121). If the algorithm sees the value 4k2/(k\u22121) then it was given access to f2. However, the algorithm will see this value only with probability 1/ \u221a n. Conditioning on not seeing this value, the best policy the algorithm can adopt is to guess f2 if the number of 2k values is at least 1 + (nk)\u221a n\n, and guess f1 otherwise. The probability of success with this test is 1/2 + O(1/ \u221a n) (regardless of whether the underlying function is f1 or f \u2212 2). Any algorithm which would approximate the best set of size k to an expected ratio better than (2k \u2212 1)/2k + \u03c9(1/ \u221a n) could be used to distinguish between the function with an advantage greater than 1/ \u221a n, and this puts a bound of (2k \u2212 1)/2k +O(1/ \u221a n) on the expected approximation ratio.\n76"}, {"heading": "E Noise Distributions", "text": "As discussed in the Introduction, our goal was to allow noise distribution in the model to potentially be Gaussian, Exponential, uniform and generally bounded. It was important for us that algorithm to be oblivious to the specific noise distribution, and rely on its properties only in the analysis. For achieve this we introduced the class of generalized exponential tail distributions. We recall the definition from the Introduction.\nDefinition. A noise distribution D has a generalized exponential tail if there exists some x0 such that for x > x0 the probability density function \u03c1(x) = e\u2212g(x), where g(x) = \u2211 i aix\n\u03b1i . We do not assume that all the \u03b1i\u2019s are integers, but only that \u03b10 \u2265 \u03b11 \u2265 . . ., and that \u03b10 \u2265 1. If D has bounded support we only require that either it has an atom at its supremum, or that \u03c1 is continuous and non zero at the supremum.\nNote that the definition includes Gaussian and Exponential distributions. For i > 0 it is possible that \u03b1i < 1 which implies that a generalized exponential tail also includes cases where the probability density function denoted \u03c1 respects \u03c1(x) = \u03c1(x0)e\u2212g\n\u2032(x\u2212x0) (we can simply add \u03c1(x0) to g using \u03b1i = 0 for some i, and move from g\u2032(x\u2212 x0) to an equivalent g(x) via a coordinate change).\nThe most important property of the noise distribution is that all of its moments are constant, independent of n. In fact,D describes how the noise affects a single evaluation, and does not depend on the number of elements. This means (for example) that if we could get h(n) independent samples from D, we would be arbitrarily close to the mean, as long as h(n) is monotone in n.\nImpossibility for distributions that depend on n. We note that if the adversary would have been allowed to choose the noise distribution as a function of n, then no approximation would be possible, even if the noise distribution had mean 1. For example, a noise distribution which returns 0 with probability 1 \u2212 1/22n and 22n with probability 1/22n has an expected value of 1, is not always 0, but does not enable any approximation.\nImpossibility for two distributions. One can consider having multiple noise distributions which act on different sets. A noise distribution can be assigned to a set either in adversarial manner, or at random. If sets are assigned to noise distributions in an adversarial manner, it is possible to construct the bad example of the correlated case from Section 5 with just two noise distributions. If sets are assigned to a noise distribution in an i.i.d manner, this reduces to the i.i.d case when there is a single distribution.\nThe relation between n and the distribution As we have explained above, if the distribution depends on n, then approximation is not possible. In particular, this means that if the universe is too small, optimization is not possible. For example, suppose that D returns 0 with probability 1 \u2212 2\u2212100, and otherwise returns 2100. Then D is bounded away from zero, has expectancy 1, but approximation is not possible if n = 50. Hence we need to assume some minimal value n0 that depends on the distribution, and assert an approximation ratio of 1\u2212 1/e\u2212 only for n > n0. We note that n0 is constant, and hence if n \u2264 n0 we can run the \u201coptimal\u201d algorithm of evaluating the noisy oracle over all subsets of n, but the approximation ratio might still be arbitrarily bad.\n77\nWe note that the problem is not \u201cjust\u201d an atom at zero. Suppose that f is additive, and bounded between 1 and 100. if D is uniform over the set 2100i for 1 \u2264 i \u2264 2100 and n = 50 then approximation is not possible; if f\u0303(A) turns out to be larger than f\u0303(B) this says very little about f(A), f(B) - it\u2019s more likely happen due to the noise.\n78"}, {"heading": "F Additional Examples", "text": "In this section we show some examples of how greedy and its variants fail under error and noise.\nGreedy fails with error. In the maximum-coverage problem we are given a family of sets that cover a universe of items, and the goal is to select a fixed number of sets whose union is maximal. This classic problem is an example of maximizing a monotone submodular function under a cardinality constraint. For a concrete example showing how greedy fails with error, consider the instance illustrated in Figure 4. In this instance there is one family of sets A depicted on the left where all sets cover the same two items, and another family of disjoint sets B that each cover a single unique item. Consider an oracle which evaluates sets as follows. For any combination of sets the oracle evaluates the cardinality of the union of the subsets exactly, except for a few special cases: For S = A \u222a b \u2200A \u2286 A, b \u2208 B the oracle returns f\u0303(S) = 2, and for S \u2286 A the oracle returns f\u0303(S) = 2 + \u03b4 for some arbitrarily small \u03b4 > 0. With access to this oracle, the greedy algorithm will only select sets in A which may be as bad as linear in the size of the input. In this example we tricked the greedy algorithm with a 1/3-erroneous oracle, but same consequences apply to an -erroneous oracle for any > 0 by planting (1\u2212 )/ items in A.\nGreedy fails with random noise. In practice, the greedy algorithm is often used although we know the data may be noisy. Hence, a different direction for research could be to analyze the effect of noise on the existing greedy algorithm. Unfortunately, it turns out that the greedy algorithm fails even on very simple examples.\nTheorem F.1. Given a noise distribution that is either uniformly distributed in [1\u2212 , 1 + ] for any > 0, a Gaussian, or an Exponential, the greedy algorithm cannot obtain a constant factor approximation ratio even in the case of maximizing additive functions under a cardinality constraint.\nProof sketch. Consider an additive function, which has two types of elements: k = \u221a n good elements, each worth n1/4, and n \u2212 k bad elements, each worth 1. Suppose that the noise is uniform in [1 \u2212 , 1 + ]. Then after taking k2/3 good elements greedy is much more likely to take bad elements, which leads to an approximation ratio of O(1/n1/6). Similar examples hold for Gaussian and Exponential noise.\nGreedy fails when taking maximal sampled mean bundle. In Section 3 we discuss a greedy algorithm which iteratively takes bundles of O(1/ ) elements that maximize F\u0303 (S \u222a B), where F\u0303 (S \u222aA) = \u2211 i\u2208A,j /\u2208S\u222aA f\u0303(S \u222aAij). To see this can be arbitrarily bad, even when F\u0303 \u2248 F , consider an instance with n \u2212 2 elements N \u2032 s.t. for any S \u2286 N \u2032 the function evaluates to f(S) = M for some arbitrarily large value M > 0, and an additional subset of elements A = {a1, a2} s.t. f(A) = f(a1) = f(a2) = , for some arbitrarily small > 0. Now assume that for any S \u2286 N \u2032 and i \u2208 [2] we have f(S\u222aai) = M+ . The sampled mean ofA is maximal, its value is arbitrarily small.\n79\n80"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We consider the problem of maximizing a monotone submodular function under noise.<lb>There has been a great deal of work on optimization of submodular functions under various<lb>constraints, resulting in algorithms that provide desirable approximation guarantees. In many<lb>applications, however, we do not have access to the submodular function we aim to optimize,<lb>but rather to some erroneous or noisy version of it. This raises the question of whether prov-<lb>able guarantees are obtainable in presence of error and noise. We provide initial answers, by<lb>focusing on the question of maximizing a monotone submodular function under a cardinality<lb>constraint when given access to a noisy oracle of the function. We show that:<lb>\u2022 For a cardinality constraint k \u2265 2, there is an approximation algorithm whose approxi-<lb>mation ratio is arbitrarily close to 1\u2212 1/e;<lb>\u2022 For k = 1 there is an algorithm whose approximation ratio is arbitrarily close to 1/2. No<lb>randomized algorithm can obtain an approximation ratio better than 1/2 + o(1);<lb>\u2022 If the noise is adversarial, no non-trivial approximation guarantee can be obtained. \u2217Supported by ISF 1241/12;<lb>\u2020Supported by NSF grant CCF-1301976, CAREER CCF-1452961, Google Faculty Research Award, Facebook Faculty<lb>Award.<lb>ar<lb>X<lb>iv<lb>:1<lb>60<lb>1.<lb>03<lb>09<lb>5v<lb>3<lb>[<lb>cs<lb>.D<lb>S]<lb>4<lb>N<lb>ov<lb>2<lb>01<lb>6", "creator": "LaTeX with hyperref package"}}}