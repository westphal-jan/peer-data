{"id": "1601.04692", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: a Survey", "abstract": "This hmimssa is 155 a survey umari of 67.90 the method dissimilar of graph d.sc cuts nougat and its applications 8.96 to twort graph funkier clustering depsite of weighted quizzically unsigned and chalongphob signed graphs. I provide a fairly zayre thorough treatment of the method of 30-33 normalized orthopedists graph cuts, suozzi a shahidi deeply original pistis method due mokpo to Shi haiqing and Malik, mccs including jajarkot complete grushina proofs. pogatetz The causwell main thrust nonspecialists of xb this paper rosalyn is chopan the hank method rol\u00f3n of normalized villavicencio cuts. townson I ismoil give intercityexpress a detailed account tessellation for rookeries K = houthis 2 hviid clusters, consiste and draught also unclench for K & gt; 2 clusters, shi'ar based on the work of al-qura Yu and Shi. I also ifield show holby how programm both rayo graph drawing pember and normalized cut venezuelan K - unutilized clustering varicella can sarab-e be easily generalized to download.com handle signed barbodes graphs, which are impostors weighted hadland graphs in lanciano which carabao the racette weight matrix W may proliferating have ravaged negative coefficients. Intuitively, gasps negative kinsky coefficients indicate outmigration distance tryin or sicot dissimilarity. The 1,5 solution swot is uncomfortable to replace the balakirev degree tamam matrix by spaceports the tisi matrix in loyal which rukum absolute values of pollentier the 895,000 weights koalas are kauder used, and hdms to petitt replace municipium the bumgarner Laplacian by the 55.32 Laplacian gridneva with the new imambargah degree baca matrix krinsky of freamunde absolute values. boisduval As far as northmead I know, the generalization of 19-10 K - disassembled way normalized cheddleton clustering to signed graphs vaupel is new. Finally, I zemmour show ginta how the lachrymose method 129.95 of ratio cuts, oreiro in which ex-marine a cut dartboards is normalized claverie by 44.45 the size of smdc the cluster pagano rather belsham than bev its maluf volume, fenosa is lucho just a demartino special lenguaje case butted of normalized barrenness cuts.", "histories": [["v1", "Mon, 18 Jan 2016 20:57:41 GMT  (1079kb,D)", "http://arxiv.org/abs/1601.04692v1", "122 pages. arXiv admin note: substantial text overlap witharXiv:1311.2492"]], "COMMENTS": "122 pages. arXiv admin note: substantial text overlap witharXiv:1311.2492", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["jean gallier"], "accepted": false, "id": "1601.04692"}, "pdf": {"name": "1601.04692.pdf", "metadata": {"source": "CRF", "title": "Spectral Theory of Unsigned and Signed Graphs Applications to Graph Clustering: a Survey", "authors": ["Jean Gallier"], "emails": ["jean@cis.upenn.edu"], "sections": [{"heading": null, "text": "Spectral Theory of Unsigned and Signed Graphs\nApplications to Graph Clustering: a Survey\nJean Gallier Department of Computer and Information Science\nUniversity of Pennsylvania Philadelphia, PA 19104, USA e-mail: jean@cis.upenn.edu\nc\u00a9 Jean Gallier\nJanuary 19, 2016\nar X\niv :1\n60 1.\n04 69\n2v 1\n[ cs\n.L G\n] 1\n8 Ja\nn 20\n16\n2\n3 Abstract: This is a survey of the method of graph cuts and its applications to graph clustering of weighted unsigned and signed graphs. I provide a fairly thorough treatment of the method of normalized graph cuts, a deeply original method due to Shi and Malik, including complete proofs. I also cover briefly the method of ratio cuts, and show how it can be viewed as a special case of normalized cuts. I include the necessary background on graphs and graph Laplacians. I then explain in detail how the eigenvectors of the graph Laplacian can be used to draw a graph. This is an attractive application of graph Laplacians. The main thrust of this paper is the method of normalized cuts. I give a detailed account for K = 2 clusters, and also for K > 2 clusters, based on the work of Yu and Shi. I also show how both graph drawing and normalized cut K-clustering can be easily generalized to handle signed graphs, which are weighted graphs in which the weight matrix W may have negative coefficients. Intuitively, negative coefficients indicate distance or dissimilarity. The solution is to replace the degree matrix D by the matrix D in which absolute values of the weights are used, and to replace the Laplacian L = D \u2212W by the signed Laplacian L = D \u2212W . The signed Laplacian L is always positive semidefinite, and it may be positive definite (for unbalanced graphs, see Chapter 5). As far as I know, the generalization of K-way normalized clustering to signed graphs is new. Finally, I show how the method of ratio cuts, in which a cut is normalized by the size of the cluster rather than its volume, is just a special case of normalized cuts. All that needs to be done is to replace the normalized Laplacian Lsym by the unormalized Laplacian L. This is also true for signed graphs (where we replace Lsym by L).\nThree points that do not appear to have been clearly articulated before are elaborated:\n1. The solutions of the main optimization problem should be viewed as tuples in the K-fold cartesian product of projective space RPN\u22121.\n2. When K > 2, the solutions of the relaxed problem should be viewed as elements of the Grassmannian G(K,N).\n3. Two possible Riemannian distances are available to compare the closeness of solutions: (a) The distance on (RPN\u22121)K . (b) The distance on the Grassmannian.\nI also clarify what should be the necessary and sufficient conditions for a matrix to represent a partition of the vertices of a graph to be clustered.\n4\nContents"}, {"heading": "1 Introduction 7", "text": ""}, {"heading": "2 Graphs and Graph Laplacians; Basic Facts 19", "text": "2.1 Directed Graphs, Undirected Graphs, Weighted Graphs . . . . . . . . . . . . 19 2.2 Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 27"}, {"heading": "3 Spectral Graph Drawing 37", "text": "3.1 Graph Drawing and Energy Minimization . . . . . . . . . . . . . . . . . . . 37 3.2 Examples of Graph Drawings . . . . . . . . . . . . . . . . . . . . . . . . . . 40"}, {"heading": "4 Graph Clustering 45", "text": "4.1 Graph Clustering Using Normalized Cuts . . . . . . . . . . . . . . . . . . . . 45 4.2 Special Case: 2-Way Clustering Using Normalized Cuts . . . . . . . . . . . . 46 4.3 K-Way Clustering Using Normalized Cuts . . . . . . . . . . . . . . . . . . . 56 4.4 K-Way Clustering; Using The Dependencies Among X1, . . . , XK . . . . . . . 70 4.5 Discrete Solution Close to a Continuous Approximation . . . . . . . . . . . . 73"}, {"heading": "5 Signed Graphs 87", "text": "5.1 Signed Graphs and Signed Laplacians . . . . . . . . . . . . . . . . . . . . . . 87 5.2 Signed Normalized Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 5.3 Balanced Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 5.4 K-Way Clustering of Signed Graphs . . . . . . . . . . . . . . . . . . . . . . 97 5.5 Signed Graph Drawing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99"}, {"heading": "6 Graph Clustering Using Ratio Cuts 103", "text": ""}, {"heading": "A Rayleigh Ratios and the Courant-Fischer Theorem 107", "text": ""}, {"heading": "B Riemannian Metrics on Quotient Manifolds 113", "text": "Bibliography 119\n5\n6 CONTENTS\nChapter 1\nIntroduction\nIn the Fall of 2012, my friend Kurt Reillag suggested that I should be ashamed about knowing so little about graph Laplacians and normalized graph cuts. These notes are the result of my efforts to rectify this situation.\nI begin with a review of basic notions of graph theory. Even though the graph Laplacian is fundamentally associated with an undirected graph, I review the definition of both directed and undirected graphs. For both directed and undirected graphs, I define the degree matrix D, the incidence matrix B, and the adjacency matrix A. Then, I define a weighted graph. This is a pair (V,W ), where V is a finite set of nodes and W is a m\u00d7m symmetric matrix with nonnegative entries and zero diagonal entries (where m = |V |). For every node vi \u2208 V , the degree d(vi) (or di) of vi is the sum of the weights of the edges adjacent to vi:\ndi = d(vi) = m\u2211 j=1 wi j.\nThe degree matrix is the diagonal matrix\nD = diag(d1, . . . , dm).\nGiven any subset of nodes A \u2286 V , we define the volume vol(A) of A as the sum of the weights of all edges adjacent to nodes in A:\nvol(A) = \u2211 vi\u2208A m\u2211 j=1 wi j.\nThe notions of degree and volume are illustrated in Figure 1.1. Given any two subset\nA,B \u2286 V (not necessarily distinct), we define links(A,B) by\nlinks(A,B) = \u2211\nvi\u2208A,vj\u2208B\nwi j.\n7"}, {"heading": "8 CHAPTER 1. INTRODUCTION", "text": "The quantity links(A,A) = links(A,A) (where A = V \u2212 A denotes the complement of A in V ) measures how many links escape from A (and A). We define the cut of A as\ncut(A) = links(A,A).\nThe notions of cut is illustrated in Figure 1.2. The above concepts play a crucial role in the theory of normalized cuts. Then, I introduce the (unnormalized) graph Laplacian L of a directed graph G in an \u201cold-fashion,\u201d by showing that for any orientation of a graph G,\nBB> = D \u2212 A = L\nis an invariant. I also define the (unnormalized) graph Laplacian L of a weighted graph G = (V,W ) as L = D \u2212W . I show that the notion of incidence matrix can be generalized\n9 to weighted graphs in a simple way. For any graph G\u03c3 obtained by orienting the underlying graph of a weighted graph G = (V,W ), there is an incidence matrix B\u03c3 such that\nB\u03c3(B\u03c3)> = D \u2212W = L.\nI also prove that\nx>Lx = 1\n2 m\u2211 i,j=1 wi j(xi \u2212 xj)2 for all x \u2208 Rm.\nConsequently, x>Lx does not depend on the diagonal entries in W , and if wi j \u2265 0 for all i, j \u2208 {1, . . . ,m}, then L is positive semidefinite. Then, if W consists of nonnegative entries, the eigenvalues 0 = \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 \u03bbm of L are real and nonnegative, and there is an orthonormal basis of eigenvectors of L. I show that the number of connected components of the graph G = (V,W ) is equal to the dimension of the kernel of L, which is also equal to the dimension of the kernel of the transpose (B\u03c3)> of any incidence matrix B\u03c3 obtained by orienting the underlying graph of G.\nI also define the normalized graph Laplacians Lsym and Lrw, given by\nLsym = D \u22121/2LD\u22121/2 = I \u2212D\u22121/2WD\u22121/2\nLrw = D \u22121L = I \u2212D\u22121W,\nand prove some simple properties relating the eigenvalues and the eigenvectors of L, Lsym and Lrw. These normalized graph Laplacians show up when dealing with normalized cuts.\nNext, I turn to graph drawings (Chapter 3). Graph drawing is a very attractive application of so-called spectral techniques, which is a fancy way of saying that that eigenvalues and eigenvectors of the graph Laplacian are used. Furthermore, it turns out that graph clustering using normalized cuts can be cast as a certain type of graph drawing.\nGiven an undirected graph G = (V,E), with |V | = m, we would like to draw G in Rn for n (much) smaller than m. The idea is to assign a point \u03c1(vi) in Rn to the vertex vi \u2208 V , for every vi \u2208 V , and to draw a line segment between the points \u03c1(vi) and \u03c1(vj). Thus, a graph drawing is a function \u03c1 : V \u2192 Rn.\nWe define the matrix of a graph drawing \u03c1 (in Rn) as a m\u00d7 n matrix R whose ith row consists of the row vector \u03c1(vi) corresponding to the point representing vi in Rn. Typically, we want n < m; in fact n should be much smaller than m.\nSince there are infinitely many graph drawings, it is desirable to have some criterion to decide which graph is better than another. Inspired by a physical model in which the edges are springs, it is natural to consider a representation to be better if it requires the springs to be less extended. We can formalize this by defining the energy of a drawing R by\nE(R) = \u2211\n{vi,vj}\u2208E\n\u2016\u03c1(vi)\u2212 \u03c1(vj)\u20162 ,"}, {"heading": "10 CHAPTER 1. INTRODUCTION", "text": "where \u03c1(vi) is the ith row of R and \u2016\u03c1(vi)\u2212 \u03c1(vj)\u20162 is the square of the Euclidean length of the line segment joining \u03c1(vi) and \u03c1(vj).\nThen, \u201cgood drawings\u201d are drawings that minimize the energy function E . Of course, the trivial representation corresponding to the zero matrix is optimum, so we need to impose extra constraints to rule out the trivial solution.\nWe can consider the more general situation where the springs are not necessarily identical. This can be modeled by a symmetric weight (or stiffness) matrix W = (wij), with wij \u2265 0. In this case, our energy function becomes\nE(R) = \u2211\n{vi,vj}\u2208E\nwij \u2016\u03c1(vi)\u2212 \u03c1(vj)\u20162 .\nFollowing Godsil and Royle [10], we prove that\nE(R) = tr(R>LR),\nwhere L = D \u2212W,\nis the familiar unnormalized Laplacian matrix associated with W , and where D is the degree matrix associated with W .\nIt can be shown that there is no loss in generality in assuming that the columns of R are pairwise orthogonal and that they have unit length. Such a matrix satisfies the equation R>R = I and the corresponding drawing is called an orthogonal drawing . This condition also rules out trivial drawings.\nThen, I prove the main theorem about graph drawings (Theorem 3.2), which essentially says that the matrix R of the desired graph drawing is constituted by the n eigenvectors of L associated with the smallest nonzero n eigenvalues of L. We give a number examples of graph drawings, many of which are borrowed or adapted from Spielman [21].\nThe next chapter (Chapter 4) contains the \u201cmeat\u201d of this document. This chapter is devoted to the method of normalized graph cuts for graph clustering. This beautiful and deeply original method first published in Shi and Malik [20], has now come to be a \u201ctextbook chapter\u201d of computer vision and machine learning. It was invented by Jianbo Shi and Jitendra Malik, and was the main topic of Shi\u2019s dissertation. This method was extended to K \u2265 3 clusters by Stella Yu in her dissertation [23], and is also the subject of Yu and Shi [24].\nGiven a set of data, the goal of clustering is to partition the data into different groups according to their similarities. When the data is given in terms of a similarity graph G, where the weight wi j between two nodes vi and vj is a measure of similarity of vi and vj, the problem can be stated as follows: Find a partition (A1, . . . , AK) of the set of nodes V into different groups such that the edges between different groups have very low weight (which\n11\nindicates that the points in different clusters are dissimilar), and the edges within a group have high weight (which indicates that points within the same cluster are similar).\nThe above graph clustering problem can be formalized as an optimization problem, using the notion of cut mentioned earlier. If we want to partition V into K clusters, we can do so by finding a partition (A1, . . . , AK) that minimizes the quantity\ncut(A1, . . . , AK) = 1\n2 K\u2211 i=1 cut(Ai) = 1 2 K\u2211 i=1 links(Ai, Ai).\nFor K = 2, the mincut problem is a classical problem that can be solved efficiently, but in practice, it does not yield satisfactory partitions. Indeed, in many cases, the mincut solution separates one vertex from the rest of the graph. What we need is to design our cost function in such a way that it keeps the subsets Ai \u201creasonably large\u201d (reasonably balanced).\nAn example of a weighted graph and a partition of its nodes into two clusters is shown in Figure 1.3.\nA way to get around this problem is to normalize the cuts by dividing by some measure of each subset Ai. A solution using the volume vol(Ai) of Ai (for K = 2) was proposed and investigated in a seminal paper of Shi and Malik [20]. Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters. The idea is to minimize the cost function\nNcut(A1, . . . , AK) = K\u2211 i=1 links(Ai, Ai) vol(Ai) = K\u2211 i=1 cut(Ai, Ai) vol(Ai) ."}, {"heading": "12 CHAPTER 1. INTRODUCTION", "text": "The first step is to express our optimization problem in matrix form. In the case of two clusters, a single vector Xx can be used to describe the partition (A1, A2) = (A,A). We need to choose the structure of this vector in such a way that\nNcut(A,A) = X>LX\nX>DX ,\nwhere the term on the right-hand side is a Rayleigh ratio.\nAfter careful study of the original papers, I discovered various facts that were implicit in these works, but I feel are important to be pointed out explicitly.\nFirst, I realized that it is important to pick a vector representation which is invariant under multiplication by a nonzero scalar, because the Rayleigh ratio is scale-invariant, and it is crucial to take advantage of this fact to make the denominator go away. This implies that the solutions X are points in the projective space RPN\u22121. This was my first revelation.\nLet N = |V | be the number of nodes in the graph G. In view of the desire for a scaleinvariant representation, it is natural to assume that the vector X is of the form\nX = (x1, . . . , xN),\nwhere xi \u2208 {a, b} for i = 1, . . . , N , for any two distinct real numbers a, b. This is an indicator vector in the sense that, for i = 1, . . . , N ,\nxi = { a if vi \u2208 A b if vi /\u2208 A.\nThe choice a = +1, b = \u22121 is natural, but premature. The correct interpretation is really to view X as a representative of a point in the real projective space RPN\u22121, namely the point P(X) of homogeneous coordinates (x1 : \u00b7 \u00b7 \u00b7 : xN).\nLet d = 1>D1 and \u03b1 = vol(A), where 1 denotes the vector whose components are all equal to 1. I prove that\nNcut(A,A) = X>LX\nX>DX\nholds iff the following condition holds:\na\u03b1 + b(d\u2212 \u03b1) = 0. (\u2020)\nNote that condition (\u2020) applied to a vector X whose components are a or b is equivalent to the fact that X is orthogonal to D1, since\nX>D1 = \u03b1a+ (d\u2212 \u03b1)b,\nwhere \u03b1 = vol({vi \u2208 V | xi = a}).\n13\nIf we let\nX = { (x1, . . . , xN) | xi \u2208 {a, b}, a, b \u2208 R, a, b 6= 0 } ,\nour solution set is\nK = { X \u2208 X | X>D1 = 0 } .\nActually, to be perfectly rigorous, we are looking for solutions in RPN\u22121, so our solution set is really\nP(K) = { (x1 : \u00b7 \u00b7 \u00b7 : xN) \u2208 RPN\u22121 | (x1, . . . , xN) \u2208 K } .\nConsequently, our minimization problem can be stated as follows:\nProblem PNC1\nminimize X>LX X>DX subject to X>D1 = 0, X \u2208 X .\nIt is understood that the solutions are points P(X) in RPN\u22121.\nSince the Rayleigh ratio and the constraints X>D1 = 0 and X \u2208 X are scale-invariant, we are led to the following formulation of our problem:\nProblem PNC2\nminimize X>LX subject to X>DX = 1, X>D1 = 0, X \u2208 X .\nBecause problem PNC2 requires the constraint X>DX = 1 to be satisfied, it does not have the same set of solutions as problem PNC1 , but PNC2 and PNC1 are equivalent in the sense that they have the same set of minimal solutions as points P(X) \u2208 RPN\u22121 given by their homogeneous coordinates X. More precisely, if X is any minimal solution of PNC1, then X/(X>DX)1/2 is a minimal solution of PNC2 (with the same minimal value for the objective functions), and if X is a minimal solution of PNC2, then \u03bbX is a minimal solution for PNC1 for all \u03bb 6= 0 (with the same minimal value for the objective functions).\nNow, as in the classical papers, we consider the relaxation of the above problem obtained by dropping the condition that X \u2208 X , and proceed as usual. However, having found a solution Z to the relaxed problem, we need to find a discrete solution X such that d(X,Z) is minimum in RPN\u22121. All this is presented in Section 4.2.\nIf the number of clusters K is at least 3, then we need to choose a matrix representation for partitions on the set of vertices. It is important that such a representation be scaleinvariant, and it is also necessary to state necessary and sufficient conditions for such matrices to represent a partition (to the best of our knowledge, these points are not clearly articulated in the literature)."}, {"heading": "14 CHAPTER 1. INTRODUCTION", "text": "We describe a partition (A1, . . . , AK) of the set of nodes V by an N \u00d7 K matrix X = [X1 \u00b7 \u00b7 \u00b7XK ] whose columns X1, . . . , XK are indicator vectors of the partition (A1, . . . , AK). Inspired by what we did when K = 2, we assume that the vector Xj is of the form\nXj = (xj1, . . . , x j N),\nwhere xji \u2208 {aj, bj} for j = 1, . . . , K and i = 1, . . . , N , and where aj, bj are any two distinct real numbers. The vector Xj is an indicator vector for Aj in the sense that, for i = 1, . . . , N ,\nxji = { aj if vi \u2208 Aj bj if vi /\u2208 Aj.\nThe choice {aj, bj} = {0, 1} for j = 1, . . . , K is natural, but premature. I show that if we pick bi = 0, then we have\ncut(Aj, Aj)\nvol(Aj) =\n(Xj)>LXj (Xj)>DXj j = 1, . . . , K,\nwhich implies that\nNcut(A1, . . . , AK) = K\u2211 j=1 cut(Aj, Aj) vol(Aj) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj .\nThen, I give necessary and sufficient conditions for a matrix X to represent a partition.\nIf we let X = {\n[X1 . . . XK ] | Xj = aj(xj1, . . . , x j N), x j i \u2208 {1, 0}, aj \u2208 R, Xj 6= 0 } (note that the condition Xj 6= 0 implies that aj 6= 0), then the set of matrices representing partitions of V into K blocks is\nK = { X = [X1 \u00b7 \u00b7 \u00b7 XK ] | X \u2208 X ,\n(X i)>DXj = 0, 1 \u2264 i, j \u2264 K, i 6= j, X(X>X)\u22121X>1 = 1 } .\nAs in the case K = 2, to be rigorous, the solution are really K-tuples of points in RPN\u22121, so our solution set is really\nP(K) = { (P(X1), . . . ,P(XK)) | [X1 \u00b7 \u00b7 \u00b7 XK ] \u2208 K } .\nRemark: For any X \u2208 X , the condition X(X>X)\u22121X>1 = 1 is redundant. However, when we relax the problem and drop the condition X \u2208 X , the condition X(X>X)\u22121X>1 = 1 captures the fact 1 should be in the range of X.\n15\nIn view of the above, we have our first formulation of K-way clustering of a graph using normalized cuts, called problem PNC1 (the notation PNCX is used in Yu [23], Section 2.1):\nK-way Clustering of a graph using Normalized Cut, Version 1: Problem PNC1\nminimize K\u2211 j=1 (Xj)>LXj (Xj)>DXj subject to (X i)>DXj = 0, 1 \u2264 i, j \u2264 K, i 6= j, X(X>X)\u22121X>1 = 1, X \u2208 X .\nAs in the case K = 2, the solutions that we are seeking are K-tuples (P(X1), . . . ,P(XK)) of points in RPN\u22121 determined by their homogeneous coordinates X1, . . . , XK .\nThen, step by step, we transform problem PNC1 into an equivalent problem PNC2. We eventually relax PNC1 into (\u22171) and PNC2 into (\u22172), by dropping the condition that X \u2208 X .\nOur second revelation is that the relaxation (\u22172) of version 2 of our minimization problem (PNC2), which is equivalent to version 1, reveals that that the solutions of the relaxed problem (\u22172) are members of the Grassmannian G(K,N).\nThis leads us to our third revelation: we have two choices of metrics to compare solutions : (1) a metric on (RPN\u22121)K ; (2) a metric on G(K,N). We discuss the first choice, which is the choice implicitly adopted by Shi and Yu. However, in approximating a discrete solution X by a solution Z of problem (\u22171) we allow more general transformations of the form Q = R\u039b, where R \u2208 O(K), and \u039b is a diagonal invertible matrix. Thus we seek R and \u039b to minimize \u2016X \u2212 ZR\u039b\u2016F . This yields better discrete solutions X.\nIn Chapter 5, I show how both the spectral method for graph drawing and the normalizedcut method for K clusters generalize to signed graphs, which are graphs whose weight matrix W may contain negative entries. The intuition is that negative weights indicate dissimilarity or distance.\nThe first obstacle is that the degree matrix may now contain negative entries. As a consequence, the Laplacian L may no longer be positive semidefinite, and worse, D\u22121/2 may not exist.\nA simple remedy is to use the absolute values of the weights in the degree matrix! We denote this matrix by D, and define the signed Laplacian as L = D \u2212 W . The idea to use positive degrees of nodes in the degree matrix of a signed graph with weights (\u22121, 0, 1) occurs in Hou [14]. The natural step of using absolute values of weights in the degree matrix is taken by Kolluri, Shewchuk and O\u2019Brien [15] and Kunegis et al. [16].\nAs we will see, this trick allows the whole machinery that we have presented to be used to attack the problem of clustering signed graphs using normalized cuts."}, {"heading": "16 CHAPTER 1. INTRODUCTION", "text": "As in the case of unsigned weighted graphs, for any orientation G\u03c3 of the underlying graph of a signed graph G = (V,W ), there is an incidence matrix B\u03c3 such that\nB\u03c3(B\u03c3)> = D \u2212W = L.\nConsequently, B\u03c3(B\u03c3)> is independent of the orientation of the underlying graph of G and L = D \u2212W is symmetric and positive semidefinite. I also show that\nx>Lx = 1\n2 m\u2211 i,j=1 |wij|(xi \u2212 sgn(wij)xj)2 for all x \u2208 Rm.\nAs in Section 4.3, given a partition of V into K clusters (A1, . . . , AK), if we represent the jth block of this partition by a vector Xj such that\nXji = { aj if vi \u2208 Aj 0 if vi /\u2208 Aj,\nfor some aj 6= 0, then the following result holds: For any vector Xj representing the jth block of a partition (A1, . . . , AK) of V , we have\n(Xj)>LXj = a2j(cut(Aj, Aj) + 2links \u2212(Aj, Aj)).\nThe above suggests defining the key notion of signed normalized cut: The signed normalized cut sNcut(A1, . . . , AK) of the partition (A1, . . . , AK) is defined as\nsNcut(A1, . . . , AK) = K\u2211 j=1 cut(Aj, Aj) vol(Aj) + 2 K\u2211 j=1 links\u2212(Aj, Aj) vol(Aj) .\nOur definition of a signed normalized cut appears to be novel.\nBased on previous computations, we have\nsNcut(A1, . . . , AK) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj ,\nwhere X is the N \u00d7K matrix whose jth column is Xj. Observe that minimizing sNcut(A1, . . . , AK) amounts to minimizing the number of positive and negative edges between clusters, and also minimizing the number of negative edges within clusters. This second minimization captures the intuition that nodes connected by a negative edge should not be together (they do not \u201clike\u201d each other; they should be far from each other). It would be preferable if the notion of signed cut only took into account the contribution links+(Aj, Aj) of the positively weighted edges between disjoint clusters, but we have not found a way to achieve this.\n17\nSince\nsNcut(A1, . . . , AK) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj ,\nthe whole machinery of Sections 4.3 and 4.5 can be applied with D replaced by D and L replaced by L. However, there is a new phenomenon, which is that L may be positive definite. As a consequence, 1 is not always an eigenvector of L.\nFollowing Kunegis et al. [16], we show that the signed Laplacian L is positive definite iff G is unbalanced, which means that it contains some cycle with an odd number of negative edges. We also characterize when a graph is balanced in terms of the kernel of the transpose B> of any of its incidence matrices.\nTo generalize the graph drawing method to signed graphs, we explain that if the energy function E(R) of a graph drawing is redefined to be\nE(R) = \u2211\n{vi,vj}\u2208E\n|wij| \u2016\u03c1(vi)\u2212 sgn(wij)\u03c1(vj)\u20162 ,\nthen we obtain orthogonal graph drawings of minimal energy, and we give some examples.\nWe conclude this survey with a short chapter on graph clustering using ratio cuts. The idea of ratio cut is to replace the volume vol(Aj) of each block Aj of the partition by its size, |Aj| (the number of nodes in Aj). Given an unsigned graph (V,W ), the ratio cut Rcut(A1, . . . , AK) of the partition (A1, . . . , AK) is defined as\nRcut(A1, . . . , AK) = K\u2211 i=1 cut(Aj, Aj) |Aj| .\nIf we represent the jth block of this partition by a vector Xj such that\nXji = { aj if vi \u2208 Aj 0 if vi /\u2208 Aj,\nfor some aj 6= 0, then we obtain\nRcut(A1, . . . , AK) = K\u2211 i=1 cut(Aj, Aj) |Aj| = K\u2211 i=1 (Xj)>LXj (Xj)>Xj .\nOn the other hand, the normalized cut is given by\nNcut(A1, . . . , AK) = K\u2211 i=1 cut(Aj, Aj) vol(Aj) = K\u2211 i=1 (Xj)>LXj (Xj)>DXj ."}, {"heading": "18 CHAPTER 1. INTRODUCTION", "text": "Therefore, ratio cut is the special case of normalized cut where D = I! Consequently, all that needs to be done is to replace the normalized Laplacian Lsym by the unormalized Laplacian L (and omit the step of considering Problem (\u2217\u22171)).\nIn the case of signed graphs, we define the signed ratio cut sRcut(A1, . . . , AK) of the partition (A1, . . . , AK) as\nsRcut(A1, . . . , AK) = K\u2211 j=1 cut(Aj, Aj) |Aj| + 2 K\u2211 j=1 links\u2212(Aj, Aj) |Aj| .\nSince we still have\n(Xj)>LXj = a2j(cut(Aj, Aj) + 2links \u2212(Aj, Aj)),\nwe obtain\nsRcut(A1, . . . , AK) = K\u2211 j=1 (Xj)>LXj (Xj)>Xj .\nTherefore, this is similar to the case of unsigned graphs, with L replaced with L. The same algorithm applies, but as in Chapter 5, the signed Laplacian L is positive definite iff G is unbalanced.\nSome of the most technical material on the Rayleigh ratio, which is needed for some proofs in Chapter 3, is the object of Appendix A. Appendix B may seem a bit out of place. Its purpose is to explain how to define a metric on the projective space RPn. For this, we need to review a few notions of differential geometry.\nI hope that these notes will make it easier for people to become familiar with the wonderful theory of normalized graph cuts. As far as I know, except for a short section in one of Gilbert Strang\u2019s book, and von Luxburg [22] excellent survey on spectral clustering, there is no comprehensive writing on the topic of graph cuts.\nChapter 2\nGraphs and Graph Laplacians; Basic Facts"}, {"heading": "2.1 Directed Graphs, Undirected Graphs, Incidence", "text": "Matrices, Adjacency Matrices, Weighted Graphs\nDefinition 2.1. A directed graph is a pair G = (V,E), where V = {v1, . . . , vm} is a set of nodes or vertices , and E \u2286 V \u00d7 V is a set of ordered pairs of distinct nodes (that is, pairs (u, v) \u2208 V \u00d7 V with u 6= v), called edges . Given any edge e = (u, v), we let s(e) = u be the source of e and t(e) = v be the target of e.\nRemark: Since an edge is a pair (u, v) with u 6= v, self-loops are not allowed. Also, there is at most one edge from a node u to a node v. Such graphs are sometimes called simple graphs .\nAn example of a directed graph is shown in Figure 2.1.\n1\n19"}, {"heading": "20 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "For every node v \u2208 V , the degree d(v) of v is the number of edges leaving or entering v:\nd(v) = |{u \u2208 V | (v, u) \u2208 E or (u, v) \u2208 E}|.\nWe abbreviate d(vi) as di. The degree matrix D(G), is the diagonal matrix\nD(G) = diag(d1, . . . , dm).\nFor example, for graph G1, we have\nD(G1) =  2 0 0 0 0 0 4 0 0 0 0 0 3 0 0 0 0 0 3 0 0 0 0 0 2  . Unless confusion arises, we write D instead of D(G).\nDefinition 2.2. Given a directed graph G = (V,E), for any two nodes u, v \u2208 V , a path from u to v is a sequence of nodes (v0, v1, . . . , vk) such that v0 = u, vk = v, and (vi, vi+1) is an edge in E for all i with 0 \u2264 i \u2264 k \u2212 1. The integer k is the length of the path. A path is closed if u = v. The graph G is strongly connected if for any two distinct node u, v \u2208 V , there is a path from u to v and there is a path from v to u.\nRemark: The terminology walk is often used instead of path, the word path being reserved to the case where the nodes vi are all distinct, except that v0 = vk when the path is closed.\nThe binary relation on V \u00d7 V defined so that u and v are related iff there is a path from u to v and there is a path from v to u is an equivalence relation whose equivalence classes are called the strongly connected components of G.\nDefinition 2.3. Given a directed graph G = (V,E), with V = {v1, . . . , vm}, if E = {e1, . . . , en}, then the incidence matrix B(G) of G is the m\u00d7 n matrix whose entries bi j are given by\nbi j =  +1 if s(ej) = vi\n\u22121 if t(ej) = vi 0 otherwise.\nHere is the incidence matrix of the graph G1:\nB =  1 1 0 0 0 0 0 \u22121 0 \u22121 \u22121 1 0 0 0 \u22121 1 0 0 0 1 0 0 0 1 0 \u22121 \u22121 0 0 0 0 \u22121 1 0  .\n2.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 21\n1\nObserve that every column of an incidence matrix contains exactly two nonzero entries, +1 and \u22121. Again, unless confusion arises, we write B instead of B(G).\nWhen a directed graph has m nodes v1, . . . , vm and n edges e1, . . . , en, a vector x \u2208 Rm can be viewed as a function x : V \u2192 R assigning the value xi to the node vi. Under this interpretation, Rm is viewed as RV . Similarly, a vector y \u2208 Rn can be viewed as a function in RE. This point of view is often useful. For example, the incidence matrix B can be interpreted as a linear map from RE to RV , the boundary map, and B> can be interpreted as a linear map from RV to RE, the coboundary map.\nRemark: Some authors adopt the opposite convention of sign in defining the incidence matrix, which means that their incidence matrix is \u2212B.\nUndirected graphs are obtained from directed graphs by forgetting the orientation of the edges.\nDefinition 2.4. A graph (or undirected graph) is a pair G = (V,E), where V = {v1, . . . , vm} is a set of nodes or vertices , and E is a set of two-element subsets of V (that is, subsets {u, v}, with u, v \u2208 V and u 6= v), called edges .\nRemark: Since an edge is a set {u, v}, we have u 6= v, so self-loops are not allowed. Also, for every set of nodes {u, v}, there is at most one edge between u and v. As in the case of directed graphs, such graphs are sometimes called simple graphs .\nAn example of a graph is shown in Figure 2.2.\nFor every node v \u2208 V , the degree d(v) of v is the number of edges incident to v:\nd(v) = |{u \u2208 V | {u, v} \u2208 E}|.\nThe degree matrix D is defined as before."}, {"heading": "22 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Definition 2.5. Given a (undirected) graph G = (V,E), for any two nodes u, v \u2208 V , a path from u to v is a sequence of nodes (v0, v1, . . . , vk) such that v0 = u, vk = v, and {vi, vi+1} is an edge in E for all i with 0 \u2264 i \u2264 k \u2212 1. The integer k is the length of the path. A path is closed if u = v. The graph G is connected if for any two distinct node u, v \u2208 V , there is a path from u to v.\nRemark: The terminology walk or chain is often used instead of path, the word path being reserved to the case where the nodes vi are all distinct, except that v0 = vk when the path is closed.\nThe binary relation on V \u00d7V defined so that u and v are related iff there is a path from u to v is an equivalence relation whose equivalence classes are called the connected components of G.\nThe notion of incidence matrix for an undirected graph is not as useful as in the case of directed graphs\nDefinition 2.6. Given a graph G = (V,E), with V = {v1, . . . , vm}, if E = {e1, . . . , en}, then the incidence matrix B(G) of G is the m\u00d7 n matrix whose entries bi j are given by\nbi j = { +1 if ej = {vi, vk} for some k 0 otherwise.\nUnlike the case of directed graphs, the entries in the incidence matrix of a graph (undirected) are nonnegative. We usually write B instead of B(G).\nThe notion of adjacency matrix is basically the same for directed or undirected graphs.\nDefinition 2.7. Given a directed or undirected graph G = (V,E), with V = {v1, . . . , vm}, the adjacency matrix A(G) of G is the symmetric m\u00d7m matrix (ai j) such that\n(1) If G is directed, then\nai j = { 1 if there is some edge (vi, vj) \u2208 E or some edge (vj, vi) \u2208 E 0 otherwise.\n(2) Else if G is undirected, then\nai j = { 1 if there is some edge {vi, vj} \u2208 E 0 otherwise.\nAs usual, unless confusion arises, we write A instead of A(G). Here is the adjacency matrix of both graphs G1 and G2:"}, {"heading": "2.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 23", "text": "A =  0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0  . If G = (V,E) is a directed or an undirected graph, given a node u \u2208 V , any node v \u2208 V\nsuch that there is an edge (u, v) in the directed case or {u, v} in the undirected case is called adjacent to v, and we often use the notation\nu \u223c v.\nObserve that the binary relation \u223c is symmetric when G is an undirected graph, but in general it is not symmetric when G is a directed graph.\nIf G = (V,E) is an undirected graph, the adjacency matrix A of G can be viewed as a linear map from RV to RV , such that for all x \u2208 Rm, we have\n(Ax)i = \u2211 j\u223ci xj;\nthat is, the value of Ax at vi is the sum of the values of x at the nodes vj adjacent to vi. The adjacency matrix can be viewed as a diffusion operator . This observation yields a geometric interpretation of what it means for a vector x \u2208 Rm to be an eigenvector of A associated with some eigenvalue \u03bb; we must have\n\u03bbxi = \u2211 j\u223ci xj, i = 1, . . . ,m,\nwhich means that the the sum of the values of x assigned to the nodes vj adjacent to vi is equal to \u03bb times the value of x at vi.\nDefinition 2.8. Given any undirected graph G = (V,E), an orientation of G is a function \u03c3 : E \u2192 V \u00d7 V assigning a source and a target to every edge in E, which means that for every edge {u, v} \u2208 E, either \u03c3({u, v}) = (u, v) or \u03c3({u, v}) = (v, u). The oriented graph G\u03c3 obtained from G by applying the orientation \u03c3 is the directed graph G\u03c3 = (V,E\u03c3), with E\u03c3 = \u03c3(E).\nThe following result shows how the number of connected components of an undirected graph is related to the rank of the incidence matrix of any oriented graph obtained from G.\nProposition 2.1. Let G = (V,E) be any undirected graph with m vertices, n edges, and c connected components. For any orientation \u03c3 of G, if B is the incidence matrix of the oriented graph G\u03c3, then c = dim(Ker (B>)), and B has rank m \u2212 c. Furthermore, the nullspace of B> has a basis consisting of indicator vectors of the connected components of G; that is, vectors (z1, . . . , zm) such that zj = 1 iff vj is in the ith component Ki of G, and zj = 0 otherwise."}, {"heading": "24 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Proof. (After Godsil and Royle [10], Section 8.3). We prove that the kernel of B> has dimension c. Since B> is a n\u00d7m matrix, we have\nm = dim(Ker (B>)) + rank(B>),\nso rank(B>) = m\u2212 c. Since B and B> have the same rank, rank(B) = m\u2212 c, as claimed. A vector z \u2208 Rm belongs to the kernel of B> iff B>z = 0 iff z>B = 0. In view of the definition of B, for every edge {vi, vj} of G, the column of B corresponding to the oriented edge \u03c3({vi, vj}) has zero entries except for a +1 and a \u22121 in position i and position j or vice-versa, so we have\nzi = zj.\nAn easy induction on the length of the path shows that if there is a path from vi to vj in G (unoriented), then zi = zj. Therefore, z has a constant value on any connected component of G. It follows that every vector z \u2208 Ker (B>) can be written uniquely as a linear combination\nz = \u03bb1z 1 + \u00b7 \u00b7 \u00b7+ \u03bbczc,\nwhere the vector zi corresponds to the ith connected component Ki of G and is defined such that\nzij = { 1 iff vj \u2208 Ki 0 otherwise.\nThis shows that dim(Ker (B>)) = c, and that Ker (B>) has a basis consisting of indicator vectors.\nFollowing common practice, we denote by 1 the (column) vector whose components are all equal to 1. Since every column of B contains a single +1 and a single \u22121, the rows of B> sum to zero, which can be expressed as\nB>1 = 0.\nAccording to Proposition 2.1, the graph G is connected iff B has rank m\u22121 iff the nullspace of B> is the one-dimensional space spanned by 1.\nIn many applications, the notion of graph needs to be generalized to capture the intuitive idea that two nodes u and v are linked with a degree of certainty (or strength). Thus, we assign a nonnegative weight wi j to an edge {vi, vj}; the smaller wi j is, the weaker is the link (or similarity) between vi and vj, and the greater wi j is, the stronger is the link (or similarity) between vi and vj.\nDefinition 2.9. A weighted graph is a pair G = (V,W ), where V = {v1, . . . , vm} is a set of nodes or vertices , and W is a symmetric matrix called the weight matrix , such that wi j \u2265 0 for all i, j \u2208 {1, . . . ,m}, and wi i = 0 for i = 1, . . . ,m. We say that a set {vi, vj} is an edge iff wi j > 0. The corresponding (undirected) graph (V,E) with E = {{vi, vj} | wi j > 0}, is called the underlying graph of G."}, {"heading": "2.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 25", "text": "Remark: Since wi i = 0, these graphs have no self-loops. We can think of the matrix W as a generalized adjacency matrix. The case where wi j \u2208 {0, 1} is equivalent to the notion of a graph as in Definition 2.4.\nWe can think of the weight wi j of an edge {vi, vj} as a degree of similarity (or affinity) in an image, or a cost in a network. An example of a weighted graph is shown in Figure 2.3. The thickness of an edge corresponds to the magnitude of its weight.\nEncode Pairwise Relationships as a Weighted Graph\nFor every node vi \u2208 V , the degree d(vi) of vi is the sum of the weights of the edges adjacent to vi:\nd(vi) = m\u2211 j=1 wi j.\nNote that in the above sum, only nodes vj such that there is an edge {vi, vj} have a nonzero contribution. Such nodes are said to be adjacent to vi, and we write vi \u223c vj. The degree matrix D is defined as before, namely by D = diag(d(v1), . . . , d(vm)).\nThe weight matrix W can be viewed as a linear map from RV to itself. For all x \u2208 Rm, we have\n(Wx)i = \u2211 j\u223ci wijxj;\nthat is, the value of Wx at vi is the weighted sum of the values of x at the nodes vj adjacent to vi.\nObserve that W1 is the (column) vector (d(v1), . . . , d(vm)) consisting of the degrees of the nodes of the graph."}, {"heading": "26 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Given any subset of nodes A \u2286 V , we define the volume vol(A) of A as the sum of the weights of all edges adjacent to nodes in A:\nvol(A) = \u2211 vi\u2208A d(vi) = \u2211 vi\u2208A m\u2211 j=1 wi j.\nRemark: Yu and Shi [24] use the notation degree(A) instead of vol(A).\nThe notions of degree and volume are illustrated in Figure 2.4.\nObserve that vol(A) = 0 if A consists of isolated vertices, that is, if wi j = 0 for all vi \u2208 A. Thus, it is best to assume that G does not have isolated vertices.\nGiven any two subset A,B \u2286 V (not necessarily distinct), we define links(A,B) by links(A,B) = \u2211\nvi\u2208A,vj\u2208B\nwi j.\nSince the matrix W is symmetric, we have\nlinks(A,B) = links(B,A),\nand observe that vol(A) = links(A, V ).\nThe quantity links(A,A) = links(A,A) (where A = V \u2212 A denotes the complement of A in V ) measures how many links escape from A (and A), and the quantity links(A,A) measures how many links stay within A itself. The quantity\ncut(A) = links(A,A)\nis often called the cut of A, and the quantity\nassoc(A) = links(A,A)"}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 27", "text": "is often called the association of A. Clearly,\ncut(A) + assoc(A) = vol(A).\nThe notions of cut is illustrated in Figure 2.5.\nWe now define the most important concept of these notes: The Laplacian matrix of a graph. Actually, as we will see, it comes in several flavors."}, {"heading": "2.2 Laplacian Matrices of Graphs", "text": "Let us begin with directed graphs, although as we will see, graph Laplacians are fundamentally associated with undirected graph. The key proposition below shows how BB> relates to the adjacency matrix A. We reproduce the proof in Gallier [7] (see also Godsil and Royle [10]).\nProposition 2.2. Given any directed graph G if B is the incidence matrix of G, A is the adjacency matrix of G, and D is the degree matrix such that Di i = d(vi), then\nBB> = D \u2212 A.\nConsequently, BB> is independent of the orientation of G and D \u2212 A is symmetric and positive semidefinite; that is, the eigenvalues of D \u2212 A are real and nonnegative.\nProof. The entry BB>i j is the inner product of the ith row bi, and the jth row bj of B. If i = j, then as\nbi k =  +1 if s(ek) = vi\n\u22121 if t(ek) = vi 0 otherwise"}, {"heading": "28 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "we see that bi \u00b7 bi = d(vi). If i 6= j, then bi \u00b7 bj 6= 0 iff there is some edge ek with s(ek) = vi and t(ek) = vj or vice-versa, in which case, bi \u00b7 bj = \u22121. Therefore,\nBB> = D \u2212 A,\nas claimed.\nFor every x \u2208 Rm, we have x>Lx = x>BB>x = (B>x)>B>x = \u2225\u2225B>x\u2225\u22252\n2 \u2265 0,\nsince the Euclidean norm \u2016 \u20162 is positive (definite). Therefore, L = BB> is positive semidefinite. It is well-known that a real symmetric matrix is positive semidefinite iff its eigenvalues are nonnegtive.\nThe matrix L = BB> = D\u2212A is called the (unnormalized) graph Laplacian of the graph G. For example, the graph Laplacian of graph G1 is\nL =  2 \u22121 \u22121 0 0 \u22121 4 \u22121 \u22121 \u22121 \u22121 \u22121 3 \u22121 0 0 \u22121 \u22121 3 \u22121 0 \u22121 0 \u22121 2  .\nThe (unnormalized) graph Laplacian of an undirected graph G = (V,E) is defined by\nL = D \u2212 A.\nObserve that each row of L sums to zero (because B>1 = 0). Consequently, the vector 1 is in the nullspace of L.\nRemark: With the unoriented version of the incidence matrix (see Definition 2.6), it can be shown that\nBB> = D + A.\nThe natural generalization of the notion of graph Laplacian to weighted graphs is this:\nDefinition 2.10. Given any weighted graph G = (V,W ) with V = {v1, . . . , vm}, the (unnormalized) graph Laplacian L(G) of G is defined by\nL(G) = D(G)\u2212W,\nwhere D(G) = diag(d1, . . . , dm) is the degree matrix of G (a diagonal matrix), with\ndi = m\u2211 j=1 wi j.\nAs usual, unless confusion arises, we write L instead of L(G)."}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 29", "text": "The graph Laplacian can be interpreted as a linear map from RV to itself. For all x \u2208 RV , we have (Lx)i = \u2211 j\u223ci wij(xi \u2212 xj).\nIt is clear that each row of L sums to 0, so the vector 1 is the nullspace of L, but it is less obvious that L is positive semidefinite. One way to prove it is to generalize slightly the notion of incidence matrix.\nDefinition 2.11. Given a weighted graph G = (V,W ), with V = {v1, . . . , vm}, if {e1, . . . , en} are the edges of the underlying graph of G (recall that {vi, vj} is an edge of this graph iff wij > 0), for any oriented graph G\n\u03c3 obtained by giving an orientation to the underlying graph of G, the incidence matrix B\u03c3 of G\u03c3 is the m\u00d7 n matrix whose entries bi j are given by\nbi j =  + \u221a wij if s(ej) = vi \u2212\u221awij if t(ej) = vi 0 otherwise.\nFor example, given the weight matrix\nW =  0 3 6 3 3 0 0 3 6 0 0 3 3 3 3 0  , the incidence matrix B corresponding to the orientation of the underlying graph of W where an edge (i, j) is oriented positively iff i < j is\nB =  1.7321 2.4495 1.7321 0 0 \u22121.7321 0 0 1.7321 0\n0 \u22122.4495 0 0 1.7321 0 0 \u22121.7321 \u22121.7321 \u22121.7321  . The reader should verify that BB> = D \u2212W . This is true in general, see Proposition 2.3.\nIt is easy to see that Proposition 2.1 applies to the underlying graph ofG. For any oriented graph G\u03c3 obtained from the underlying graph of G, the rank of the incidence matrix B\u03c3 is equal to m\u2212 c, where c is the number of connected components of the underlying graph of G, and we have (B\u03c3)>1 = 0. We also have the following version of Proposition 2.2 whose proof is immediately adapted.\nProposition 2.3. Given any weighted graph G = (V,W ) with V = {v1, . . . , vm}, if B\u03c3 is the incidence matrix of any oriented graph G\u03c3 obtained from the underlying graph of G and D is the degree matrix of W , then\nB\u03c3(B\u03c3)> = D \u2212W = L."}, {"heading": "30 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Consequently, B\u03c3(B\u03c3)> is independent of the orientation of the underlying graph of G and"}, {"heading": "L = D \u2212W is symmetric and positive semidefinite; that is, the eigenvalues of L = D \u2212W are real and nonnegative.", "text": "Remark: Given any orientationG\u03c3 of the underlying graph of a weighted graphG = (V,W ), if B is the incidence matrix of G\u03c3, then B> defines a kind of discrete covariant derivative \u2207 : RV \u00d7X (G)\u2192 RV on the set of 0-forms, which is just the set of functions RV . For every vertex vi \u2208 V , we view the set of edges with source or endpoint vi,\nTviG = {(vi, vj) | wij 6= 0} \u222a {(vh, vi) | whi 6= 0},\nas a kind of discrete tangent space at vi. The disjoint union of the tangent spaces TviG is the discrete tangent bundle TG. A discrete vector field is then a function X : V \u2192 TG that assigns to every vertex vi \u2208 V some edge X(vi) = ek \u2208 TviG, and we denote the set of all discrete vectors fields by X (G). For every function f \u2208 RV and for every vector field X \u2208 X (G), we define the function \u2207Xf , a discrete analog of the covariant derivative of the function f with respect to the vector field X, by\n(\u2207Xf)(vi) = B>(f)(X(vi));\nthat is, if X(vi) is the kth edge ek = (vi, vj), then\n(\u2207Xf)(vi) = \u221a wij(fi \u2212 fj),\nelse if X(vi) is the kth edge ek = (vj, vi), then\n(\u2207Xf)(vi) = \u221a wij(fj \u2212 fi).\nThen, the graph Laplacian L is given by\nL = BB>;\nfor every node vi, we have\n(Lx)i = \u2211 j\u223ci wij(xi \u2212 xj).\nThus, L appears to be a discrete analog of the connection Laplacian (also known as Bochner Laplacian), rather than a discrete analog of the Hodge (Laplace\u2013Beltrami) Laplacian; see Petersen [19]. To make the above statement precise, we need to view \u2207f as the function from X (G) to RV given by\n(\u2207f)(X) = \u2207Xf.\nThe set of functions from X (G) to RV is in bijection with the set of functions RX (G)\u00d7V from X (G) \u00d7 V to R, and we can view the discrete connection \u2207 as a linear map \u2207 : RV \u2192 RX (G)\u00d7V . Since both X (G) and V are finite, we can use the inner product on the vector"}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 31", "text": "space RX (G)\u00d7V (and the inner product on RV ) to define the adjoint \u2207\u2217 : RX (G)\u00d7V \u2192 RV of \u2207 : RV \u2192 RX (G)\u00d7V by\n\u3008\u2207\u2217F, f\u3009 = \u3008F,\u2207f\u3009 ,\nfor all f \u2208 RV and all F \u2208 RX (G)\u00d7V . Then, the connection Laplacian \u2207\u2217\u2207 : RV \u2192 RV is indeed equal to L.\nAnother way to prove that L is positive semidefinite is to evaluate the quadratic form x>Lx.\nProposition 2.4. For any m\u00d7m symmetric matrix W = (wij), if we let L = D\u2212W where D is the degree matrix associated with W , then we have\nx>Lx = 1\n2 m\u2211 i,j=1 wi j(xi \u2212 xj)2 for all x \u2208 Rm.\nConsequently, x>Lx does not depend on the diagonal entries in W , and if wi j \u2265 0 for all i, j \u2208 {1, . . . ,m}, then L is positive semidefinite.\nProof. We have\nx>Lx = x>Dx\u2212 x>Wx\n= m\u2211 i=1 dix 2 i \u2212 m\u2211 i,j=1 wi jxixj\n= 1\n2 ( m\u2211 i=1 dix 2 i \u2212 2 m\u2211 i,j=1 wi jxixj + m\u2211 i=1 dix 2 i )\n= 1\n2 m\u2211 i,j=1 wi j(xi \u2212 xj)2.\nObviously, the quantity on the right-hand side does not depend on the diagonal entries in W , and if wi j \u2265 0 for all i, j, then this quantity is nonnegative.\nProposition 2.4 immediately implies the following facts: For any weighted graph G = (V,W ),\n1. The eigenvalues 0 = \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 \u03bbm of L are real and nonnegative, and there is an orthonormal basis of eigenvectors of L.\n2. The smallest eigenvalue \u03bb1 of L is equal to 0, and 1 is a corresponding eigenvector.\nIt turns out that the dimension of the nullspace of L (the eigenspace of 0) is equal to the number of connected components of the underlying graph of G."}, {"heading": "32 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Proposition 2.5. Let G = (V,W ) be a weighted graph. The number c of connected components K1, . . . , Kc of the underlying graph of G is equal to the dimension of the nullspace of L, which is equal to the multiplicity of the eigenvalue 0. Furthermore, the nullspace of L has a basis consisting of indicator vectors of the connected components of G, that is, vectors (f1, . . . , fm) such that fj = 1 iff vj \u2208 Ki and fj = 0 otherwise.\nProof. Since L = BB> for the incidence matrix B associated with any oriented graph obtained from G, and since L and B> have the same nullspace, by Proposition 2.1, the dimension of the nullspace of L is equal to the number c of connected components of G and the indicator vectors of the connected components of G form a basis of Ker (L).\nProposition 2.5 implies that if the underlying graph of G is connected, then the second eigenvalue \u03bb2 of L is strictly positive.\nRemarkably, the eigenvalue \u03bb2 contains a lot of information about the graph G (assuming that G = (V,E) is an undirected graph). This was first discovered by Fiedler in 1973, and for this reason, \u03bb2 is often referred to as the Fiedler number . For more on the properties of the Fiedler number, see Godsil and Royle [10] (Chapter 13) and Chung [4]. More generally, the spectrum (0, \u03bb2, . . . , \u03bbm) of L contains a lot of information about the combinatorial structure of the graph G. Leverage of this information is the object of spectral graph theory .\nIt turns out that normalized variants of the graph Laplacian are needed, especially in applications to graph clustering. These variants make sense only if G has no isolated vertices, which means that every row of W contains some strictly positive entry. In this case, the degree matrix D contains positive entries, so it is invertible and D\u22121/2 makes sense; namely\nD\u22121/2 = diag(d \u22121/2 1 , . . . , d \u22121/2 m ),\nand similarly for any real exponent \u03b1.\nDefinition 2.12. Given any weighted directed graph G = (V,W ) with no isolated vertex and with V = {v1, . . . , vm}, the (normalized) graph Laplacians Lsym and Lrw of G are defined by\nLsym = D \u22121/2LD\u22121/2 = I \u2212D\u22121/2WD\u22121/2\nLrw = D \u22121L = I \u2212D\u22121W.\nObserve that the Laplacian Lsym = D \u22121/2LD\u22121/2 is a symmetric matrix (because L and\nD\u22121/2 are symmetric) and that\nLrw = D \u22121/2LsymD 1/2.\nThe reason for the notation Lrw is that this matrix is closely related to a random walk on the graph G."}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 33", "text": "Since the unnormalized Laplacian L can be written as L = BB>, where B is the incidence matrix of any oriented graph obtained from the underlying graph of G = (V,W ), if we let\nBsym = D \u22121/2B,\nwe get Lsym = BsymB > sym. In particular, for any singular decomposition Bsym = U\u03a3V > of Bsym (with U an m \u00d7 m orthogonal matrix, \u03a3 a \u201cdiagonal\u201d m\u00d7n matrix of singular values, and V an n\u00d7n orthogonal matrix), the eigenvalues of Lsym are the squares of the top m singular values of Bsym, and the vectors in U are orthonormal eigenvectors of Lsym with respect to these eigenvalues (the squares of the top m diagonal entries of \u03a3). Computing the SVD of Bsym generally yields more accurate results than diagonalizing Lsym, especially when Lsym has eigenvalues with high multiplicity.\nThere are simple relationships between the eigenvalues and the eigenvectors of Lsym, and Lrw. There is also a simple relationship with the generalized eigenvalue problem Lx = \u03bbDx.\nProposition 2.6. Let G = (V,W ) be a weighted graph without isolated vertices. The graph Laplacians, L,Lsym, and Lrw satisfy the following properties:\n(1) The matrix Lsym is symmetric and positive semidefinite. In fact,\nx>Lsymx = 1\n2 m\u2211 i,j=1 wi j ( xi\u221a di \u2212 xj\u221a dj )2 for all x \u2208 Rm.\n(2) The normalized graph Laplacians Lsym and Lrw have the same spectrum (0 = \u03bd1 \u2264 \u03bd2 \u2264 . . . \u2264 \u03bdm), and a vector u 6= 0 is an eigenvector of Lrw for \u03bb iff D1/2u is an eigenvector of Lsym for \u03bb.\n(3) The graph Laplacians, L,Lsym, and Lrw are symmetric and positive semidefinite.\n(4) A vector u 6= 0 is a solution of the generalized eigenvalue problem Lu = \u03bbDu iff D1/2u is an eigenvector of Lsym for the eigenvalue \u03bb iff u is an eigenvector of Lrw for the eigenvalue \u03bb.\n(5) The graph Laplacians, L and Lrw have the same nullspace. For any vector u, we have u \u2208 Ker (L) iff D1/2u \u2208 Ker (Lsym).\n(6) The vector 1 is in the nullspace of Lrw, and D 1/21 is in the nullspace of Lsym.\n(7) For every eigenvalue \u03bdi of the normalized graph Laplacian Lsym, we have 0 \u2264 \u03bdi \u2264 2. Furthermore, \u03bdm = 2 iff the underlying graph of G contains a nontrivial connected bipartite component."}, {"heading": "34 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "(8) If m \u2265 2 and if the underlying graph of G is not a complete graph, then \u03bd2 \u2264 1. Furthermore the underlying graph of G is a complete graph iff \u03bd2 = m m\u22121 .\n(9) If m \u2265 2 and if the underlying graph of G is connected then \u03bd2 > 0.\n(10) If m \u2265 2 and if the underlying graph of G has no isolated vertices, then \u03bdm \u2265 mm\u22121 .\nProof. (1) We have Lsym = D \u22121/2LD\u22121/2, and D\u22121/2 is a symmetric invertible matrix (since it is an invertible diagonal matrix). It is a well-known fact of linear algebra that if B is an invertible matrix, then a matrix S is symmetric, positive semidefinite iff BSB> is symmetric, positive semidefinite. Since L is symmetric, positive semidefinite, so is Lsym = D\n\u22121/2LD\u22121/2. The formula\nx>Lsymx = 1\n2 m\u2211 i,j=1 wi j ( xi\u221a di \u2212 xj\u221a dj )2 for all x \u2208 Rm\nfollows immediately from Proposition 2.4 by replacing x by D\u22121/2x, and also shows that Lsym is positive semidefinite.\n(2) Since Lrw = D \u22121/2LsymD 1/2,\nthe matrices Lsym and Lrw are similar, which implies that they have the same spectrum. In fact, since D1/2 is invertible,\nLrwu = D \u22121Lu = \u03bbu\niff D\u22121/2Lu = \u03bbD1/2u\niff D\u22121/2LD\u22121/2D1/2u = LsymD 1/2u = \u03bbD1/2u,\nwhich shows that a vector u 6= 0 is an eigenvector of Lrw for \u03bb iff D1/2u is an eigenvector of Lsym for \u03bb.\n(3) We already know that L and Lsym are positive semidefinite, and (2) shows that Lrw is also positive semidefinite.\n(4) Since D\u22121/2 is invertible, we have\nLu = \u03bbDu\niff D\u22121/2Lu = \u03bbD1/2u\niff D\u22121/2LD\u22121/2D1/2u = LsymD 1/2u = \u03bbD1/2u,\nwhich shows that a vector u 6= 0 is a solution of the generalized eigenvalue problem Lu = \u03bbDu iff D1/2u is an eigenvector of Lsym for the eigenvalue \u03bb. The second part of the statement follows from (2)."}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 35", "text": "(5) Since D\u22121 is invertible, we have Lu = 0 iff D\u22121Lu = Lrwu = 0. Similarly, since D \u22121/2\nis invertible, we have Lu = 0 iff D\u22121/2LD\u22121/2D1/2u = 0 iff D1/2u \u2208 Ker (Lsym).\n(6) Since L1 = 0, we get Lrw1 = D \u22121L1 = 0. That D1/21 is in the nullspace of Lsym\nfollows from (2). Properties (7)\u2013(10) are proved in Chung [4] (Chapter 1).\nA version of Proposition 2.5 also holds for the graph Laplacians Lsym and Lrw. This follows easily from the fact that Proposition 2.1 applies to the underlying graph of a weighted graph. The proof is left as an exercise.\nProposition 2.7. Let G = (V,W ) be a weighted graph. The number c of connected components K1, . . . , Kc of the underlying graph of G is equal to the dimension of the nullspace of both Lsym and Lrw, which is equal to the multiplicity of the eigenvalue 0. Furthermore, the nullspace of Lrw has a basis consisting of indicator vectors of the connected components of G, that is, vectors (f1, . . . , fm) such that fj = 1 iff vj \u2208 Ki and fj = 0 otherwise. For Lsym, a basis of the nullpace is obtained by multiplying the above basis of the nullspace of Lrw by D1/2.\n36 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS\nChapter 3\nSpectral Graph Drawing"}, {"heading": "3.1 Graph Drawing and Energy Minimization", "text": "Let G = (V,E) be some undirected graph. It is often desirable to draw a graph, usually in the plane but possibly in 3D, and it turns out that the graph Laplacian can be used to design surprisingly good methods. Say |V | = m. The idea is to assign a point \u03c1(vi) in Rn to the vertex vi \u2208 V , for every vi \u2208 V , and to draw a line segment between the points \u03c1(vi) and \u03c1(vj) iff there is an edge {vi, vj}. Thus, a graph drawing is a function \u03c1 : V \u2192 Rn.\nWe define the matrix of a graph drawing \u03c1 (in Rn) as a m\u00d7 n matrix R whose ith row consists of the row vector \u03c1(vi) corresponding to the point representing vi in Rn. Typically, we want n < m; in fact n should be much smaller than m. A representation is balanced iff the sum of the entries of every column is zero, that is,\n1>R = 0.\nIf a representation is not balanced, it can be made balanced by a suitable translation. We may also assume that the columns of R are linearly independent, since any basis of the column space also determines the drawing. Thus, from now on, we may assume that n \u2264 m.\nRemark: A graph drawing \u03c1 : V \u2192 Rn is not required to be injective, which may result in degenerate drawings where distinct vertices are drawn as the same point. For this reason, we prefer not to use the terminology graph embedding , which is often used in the literature. This is because in differential geometry, an embedding always refers to an injective map. The term graph immersion would be more appropriate.\nAs explained in Godsil and Royle [10], we can imagine building a physical model of G by connecting adjacent vertices (in Rn) by identical springs. Then, it is natural to consider a representation to be better if it requires the springs to be less extended. We can formalize this by defining the energy of a drawing R by\nE(R) = \u2211\n{vi,vj}\u2208E\n\u2016\u03c1(vi)\u2212 \u03c1(vj)\u20162 ,\n37"}, {"heading": "38 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "where \u03c1(vi) is the ith row of R and \u2016\u03c1(vi)\u2212 \u03c1(vj)\u20162 is the square of the Euclidean length of the line segment joining \u03c1(vi) and \u03c1(vj).\nThen, \u201cgood drawings\u201d are drawings that minimize the energy function E . Of course, the trivial representation corresponding to the zero matrix is optimum, so we need to impose extra constraints to rule out the trivial solution.\nWe can consider the more general situation where the springs are not necessarily identical. This can be modeled by a symmetric weight (or stiffness) matrix W = (wij), with wij \u2265 0. Then our energy function becomes\nE(R) = \u2211\n{vi,vj}\u2208E\nwij \u2016\u03c1(vi)\u2212 \u03c1(vj)\u20162 .\nIt turns out that this function can be expressed in terms of the Laplacian L = D\u2212W . The following proposition is shown in Godsil and Royle [10]. We give a slightly more direct proof.\nProposition 3.1. Let G = (V,W ) be a weighted graph, with |V | = m and W an m \u00d7 m symmetric matrix, and let R be the matrix of a graph drawing \u03c1 of G in Rn (a m\u00d7n matrix). If L = D \u2212W is the unnormalized Laplacian matrix associated with W , then\nE(R) = tr(R>LR).\nProof. Since \u03c1(vi) is the ith row of R (and \u03c1(vj) is the jth row of R), if we denote the kth column of R by Rk, using Proposition 2.4, we have\nE(R) = \u2211\n{vi,vj}\u2208E\nwij \u2016\u03c1(vi)\u2212 \u03c1(vj)\u20162\n= n\u2211 k=1 \u2211 {vi,vj}\u2208E wij(Rik \u2212Rjk)2\n= n\u2211 k=1 1 2 m\u2211 i,j=1 wij(Rik \u2212Rjk)2\n= n\u2211 k=1 (Rk)>LRk = tr(R>LR),\nas claimed.\nNote that L1 = 0,\nas we already observed.\nSince the matrix R>LR is symmetric, it has real eigenvalues. Actually, since L is positive semidefinite, so is R>LR. Then, the trace of R>LR is equal to the sum of its positive eigenvalues, and this is the energy E(R) of the graph drawing."}, {"heading": "3.1. GRAPH DRAWING AND ENERGY MINIMIZATION 39", "text": "If R is the matrix of a graph drawing in Rn, then for any invertible matrix M , the map that assigns \u03c1(vi)M to vi is another graph drawing of G, and these two drawings convey the same amount of information. From this point of view, a graph drawing is determined by the column space of R. Therefore, it is reasonable to assume that the columns of R are pairwise orthogonal and that they have unit length. Such a matrix satisfies the equation R>R = I, and the corresponding drawing is called an orthogonal drawing . This condition also rules out trivial drawings. The following result tells us how to find minimum energy orthogonal balanced graph drawings, provided the graph is connected.\nTheorem 3.2. Let G = (V,W ) be a weigted graph with |V | = m. If L = D \u2212W is the (unnormalized) Laplacian of G, and if the eigenvalues of L are 0 = \u03bb1 < \u03bb2 \u2264 \u03bb3 \u2264 . . . \u2264 \u03bbm, then the minimal energy of any balanced orthogonal graph drawing of G in Rn is equal to \u03bb2+\u00b7 \u00b7 \u00b7+\u03bbn+1 (in particular, this implies that n < m). The m\u00d7n matrix R consisting of any unit eigenvectors u2, . . . , un+1 associated with \u03bb2 \u2264 . . . \u2264 \u03bbn+1 yields a balanced orthogonal graph drawing of minimal energy; it satisfies the condition R>R = I.\nProof. We present the proof given in Godsil and Royle [10] (Section 13.4, Theorem 13.4.1). The key point is that the sum of the n smallest eigenvalues of L is a lower bound for tr(R>LR). This can be shown using an argument using the Rayleigh ratio; see Proposition A.3 (the Poincare\u0301 separation theorem). Then, any n eigenvectors (u1, . . . , un) associated with \u03bb1, . . . , \u03bbn achieve this bound. Because the first eigenvalue of L is \u03bb1 = 0 and because we are assuming that \u03bb2 > 0, we have u1 = 1/ \u221a m. Since the uj are pairwise orthogonal\nfor i = 2, . . . , n and since ui is orthogonal to u1 = 1/ \u221a m, the entries in ui add up to 0. Consequently, for any ` with 2 \u2264 ` \u2264 n, by deleting u1 and using (u2, . . . , u`), we obtain a balanced orthogonal graph drawing in R`\u22121 with the same energy as the orthogonal graph drawing in R` using (u1, u2, . . . , u`). Conversely, from any balanced orthogonal drawing in R`\u22121 using (u2, . . . , u`), we obtain an orthogonal graph drawing in R` using (u1, u2, . . . , u`) with the same energy. Therefore, the minimum energy of a balanced orthogonal graph drawing in Rn is equal to the minimum energy of an orthogonal graph drawing in Rn+1, and this minimum is \u03bb2 + \u00b7 \u00b7 \u00b7+ \u03bbn+1.\nSince 1 spans the nullspace of L, using u1 (which belongs to KerL) as one of the vectors in R would have the effect that all points representing vertices of G would have the same first coordinate. This would mean that the drawing lives in a hyperplane in Rn, which is undesirable, especially when n = 2, where all vertices would be collinear. This is why we omit the first eigenvector u1.\nObserve that for any orthogonal n\u00d7 n matrix Q, since\ntr(R>LR) = tr(Q>R>LRQ),\nthe matrix RQ also yields a minimum orthogonal graph drawing. This amounts to applying the rigid motion Q> to the rows of R.\nIn summary, if \u03bb2 > 0, an automatic method for drawing a graph in R2 is this:"}, {"heading": "40 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "1. Compute the two smallest nonzero eigenvalues \u03bb2 \u2264 \u03bb3 of the graph Laplacian L (it is possible that \u03bb3 = \u03bb2 if \u03bb2 is a multiple eigenvalue);\n2. Compute two unit eigenvectors u2, u3 associated with \u03bb2 and \u03bb3, and let R = [u2 u3] be the m\u00d7 2 matrix having u2 and u3 as columns.\n3. Place vertex vi at the point whose coordinates is the ith row of R, that is, (Ri1, Ri2).\nThis method generally gives pleasing results, but beware that there is no guarantee that distinct nodes are assigned distinct images, because R can have identical rows. This does not seem to happen often in practice."}, {"heading": "3.2 Examples of Graph Drawings", "text": "We now give a number of examples using Matlab. Some of these are borrowed or adapted from Spielman [21].\nExample 1. Consider the graph with four nodes whose adjacency matrix is\nA =  0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0  . We use the following program to compute u2 and u3:"}, {"heading": "A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0];", "text": ""}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019)\nThe graph of Example 1 is shown in Figure 3.1. The function eigs(L) computes the six largest eigenvalues of L in decreasing order, and corresponding eigenvectors. It turns out that \u03bb2 = \u03bb3 = 2 is a double eigenvalue.\nExample 2. Consider the graph G2 shown in Figure 2.2 given by the adjacency matrix\nA =  0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0  . We use the following program to compute u2 and u3:"}, {"heading": "3.2. EXAMPLES OF GRAPH DRAWINGS 41", "text": ""}, {"heading": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0];", "text": ""}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)\nThe function eig(L) (with no s at the end) computes the eigenvalues of L in increasing order. The result of drawing the graph is shown in Figure 3.2. Note that node v2 is assigned to the point (0, 0), so the difference between this drawing and the drawing in Figure 2.2 is that the drawing of Figure 3.2 is not convex."}, {"heading": "42 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "Example 3. Consider the ring graph defined by the adjacency matrix A given in the Matlab program shown below:"}, {"heading": "A = diag(ones(1, 11),1);", "text": "A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1;"}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)\nObserve that we get a very nice ring; see Figure 3.3. Again \u03bb2 = 0.2679 is a double eigenvalue (and so are the next pairs of eigenvalues, except the last, \u03bb12 = 4).\nExample 4. In this example adpated from Spielman, we generate 20 randomly chosen points in the unit square, compute their Delaunay triangulation, then the adjacency matrix of the corresponding graph, and finally draw the graph using the second and third eigenvalues of the Laplacian."}, {"heading": "A = zeros(20,20);", "text": "xy = rand(20, 2); trigs = delaunay(xy(:,1), xy(:,2)); elemtrig = ones(3) - eye(3); for i = 1:length(trigs),\nA(trigs(i,:),trigs(i,:)) = elemtrig;\nend"}, {"heading": "3.2. EXAMPLES OF GRAPH DRAWINGS 43", "text": ""}, {"heading": "A = double(A >0); gplot(A,xy)", "text": ""}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)\nThe Delaunay triangulation of the set of 20 points and the drawing of the corresponding graph are shown in Figure 3.4. The graph drawing on the right looks nicer than the graph on the left but is is no longer planar.\nExample 5. Our last example, also borrowed from Spielman [21], corresponds to the skeleton of the \u201cBuckyball,\u201d a geodesic dome invented by the architect Richard Buckminster Fuller (1895\u20131983). The Montre\u0301al Biosphe\u0300re is an example of a geodesic dome designed by Buckminster Fuller."}, {"heading": "A = full(bucky);", "text": ""}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)\nFigure 3.5 shows a graph drawing of the Buckyball. This picture seems a bit squashed for two reasons. First, it is really a 3-dimensional graph; second, \u03bb2 = 0.2434 is a triple"}, {"heading": "44 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "eigenvalue. (Actually, the Laplacian of L has many multiple eigenvalues.) What we should really do is to plot this graph in R3 using three orthonormal eigenvectors associated with \u03bb2.\nA 3D picture of the graph of the Buckyball is produced by the following Matlab program, and its image is shown in Figure 3.6. It looks better!\n[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)\nChapter 4\nGraph Clustering"}, {"heading": "4.1 Graph Clustering Using Normalized Cuts", "text": "Given a set of data, the goal of clustering is to partition the data into different groups according to their similarities. When the data is given in terms of a similarity graph G, where the weight wi j between two nodes vi and vj is a measure of similarity of vi and vj, the problem can be stated as follows: Find a partition (A1, . . . , AK) of the set of nodes V into different groups such that the edges between different groups have very low weight (which indicates that the points in different clusters are dissimilar), and the edges within a group have high weight (which indicates that points within the same cluster are similar).\nThe above graph clustering problem can be formalized as an optimization problem, using the notion of cut mentioned at the end of Section 2.1.\nGiven a subset A of the set of vertices V , recall that we define cut(A) by cut(A) = links(A,A) = \u2211\nvi\u2208A,vj\u2208A\nwi j,\nand that cut(A) = links(A,A) = links(A,A) = cut(A).\nIf we want to partition V into K clusters, we can do so by finding a partition (A1, . . . , AK) that minimizes the quantity\ncut(A1, . . . , AK) = 1\n2 K\u2211 i=1 cut(Ai).\nThe reason for introducing the factor 1/2 is to avoiding counting each edge twice. In particular,\ncut(A,A) = links(A,A).\nFor K = 2, the mincut problem is a classical problem that can be solved efficiently, but in practice, it does not yield satisfactory partitions. Indeed, in many cases, the mincut solution\n45"}, {"heading": "46 CHAPTER 4. GRAPH CLUSTERING", "text": "separates one vertex from the rest of the graph. What we need is to design our cost function in such a way that it keeps the subsets Ai \u201creasonably large\u201d (reasonably balanced).\nA example of a weighted graph and a partition of its nodes into two clusters is shown in Figure 4.1.\n15\nEncode Pairwise Relationships as a Weighted Graph\n16\nCut the graph into two pieces\nFigure 4.1: A weighted graph and its partition into two clusters.\nA way to get around this problem is to normalize the cuts by dividing by some measure of each subset Ai. One possibility is to use the size (the number of elements) of Ai. Another is to use the volume vol(Ai) of Ai. A solution using the second measure (the volume) (for K = 2) was proposed and investigated in a seminal paper of Shi and Malik [20]. Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters. We will describe this method later. The idea is to minimize the cost function\nNcut(A1, . . . , AK) = K\u2211 i=1 links(Ai, Ai) vol(Ai) = K\u2211 i=1 cut(Ai, Ai) vol(Ai) .\nWe begin with the case K = 2, which is easier to handle.\n4.2 Special Case: 2-Way Clustering Using Normalized\nCuts\nOur goal is to express our optimization problem in matrix form. In the case of two clusters, a single vector X can be used to describe the partition (A1, A2) = (A,A). We need to choose the structure of this vector in such a way that Ncut(A,A) is equal to the Rayleigh ratio\nX>LX X>DX .\n4.2. SPECIAL CASE: 2-WAY CLUSTERING USING NORMALIZED CUTS 47\nIt is also important to pick a vector representation which is invariant under multiplication by a nonzero scalar, because the Rayleigh ratio is scale-invariant, and it is crucial to take advantage of this fact to make the denominator go away.\nLet N = |V | be the number of nodes in the graph G. In view of the desire for a scaleinvariant representation, it is natural to assume that the vector X is of the form\nX = (x1, . . . , xN),\nwhere xi \u2208 {a, b} for i = 1, . . . , N , for any two distinct real numbers a, b. This is an indicator vector in the sense that, for i = 1, . . . , N ,\nxi = { a if vi \u2208 A b if vi /\u2208 A.\nThe correct interpretation is really to view X as a representative of a point in the real projective space RPN\u22121, namely the point P(X) of homogeneous coordinates (x1 : \u00b7 \u00b7 \u00b7 : xN). Therefore, from now on, we view X as a vector of homogeneous coordinates representing the point P(X) \u2208 RPN\u22121.\nLet d = 1>D1 and \u03b1 = vol(A). Then, vol(A) = d\u2212 \u03b1. By Proposition 2.4, we have\nX>LX = (a\u2212 b)2 cut(A,A),\nand we easily check that X>DX = \u03b1a2 + (d\u2212 \u03b1)b2.\nSince cut(A,A) = cut(A,A), we have\nNcut(A,A) = cut(A,A)\nvol(A) +\ncut(A,A)\nvol(A) =\n( 1\nvol(A) +\n1\nvol(A)\n) cut(A,A),\nso we obtain\nNcut(A,A) =\n( 1\n\u03b1 +\n1\nd\u2212 \u03b1\n) cut(A,A) =\nd\n\u03b1(d\u2212 \u03b1) cut(A,A).\nSince X>LX\nX>DX =\n(a\u2212 b)2\n\u03b1a2 + (d\u2212 \u03b1)b2 cut(A,A),\nin order to have\nNcut(A,A) = X>LX\nX>DX ,\nwe need to find a and b so that\n(a\u2212 b)2\n\u03b1a2 + (d\u2212 \u03b1)b2 =\nd\n\u03b1(d\u2212 \u03b1) ."}, {"heading": "48 CHAPTER 4. GRAPH CLUSTERING", "text": "The above is equivalent to\n(a\u2212 b)2\u03b1(d\u2212 \u03b1) = \u03b1da2 + (d\u2212 \u03b1)db2,\nwhich can be rewritten as\na2(\u03b1d\u2212 \u03b1(d\u2212 \u03b1)) + b2(d2 \u2212 \u03b1d\u2212 \u03b1(d\u2212 \u03b1)) + 2\u03b1(d\u2212 \u03b1)ab = 0.\nThe above yields a2\u03b12 + b2(d2 \u2212 2\u03b1d+ \u03b12) + 2\u03b1(d\u2212 \u03b1)ab = 0,\nthat is, a2\u03b12 + b2(d\u2212 \u03b1)2 + 2\u03b1(d\u2212 \u03b1)ab = 0,\nwhich reduces to (a\u03b1 + b(d\u2212 \u03b1))2 = 0.\nTherefore, we get the condition a\u03b1 + b(d\u2212 \u03b1) = 0. (\u2020)\nNote that condition (\u2020) applied to a vector X whose components are a or b is equivalent to the fact that X is orthogonal to D1, since\nX>D1 = \u03b1a+ (d\u2212 \u03b1)b,\nwhere \u03b1 = vol({vi \u2208 V | xi = a}).\nWe claim the following two facts. For any nonzero vector X whose components are a or b, if X>D1 = \u03b1a+ (d\u2212 \u03b1)b = 0, then\n(1) \u03b1 6= 0 and \u03b1 6= d iff a 6= 0 and b 6= 0.\n(2) if a, b 6= 0, then a 6= b.\n(1) First assume that a 6= 0 and b 6= 0. If \u03b1 = 0, then \u03b1a + (d \u2212 \u03b1)b = 0 yields db = 0 with d 6= 0, which implies b = 0, a contradiction. If d \u2212 \u03b1 = 0, then we get da = 0 with d 6= 0, which implies a = 0, a contradiction.\nConversely, assume that \u03b1 6= 0 and \u03b1 6= d. If a = 0, then from \u03b1a+ (d\u2212 \u03b1)b = 0 we get (d \u2212 \u03b1)b = 0, which implies b = 0, contradicting the fact that X 6= 0. Similarly, if b = 0, then we get \u03b1a = 0, which implies a = 0, contradicting the fact that X 6= 0.\n(2) If a, b 6= 0, a = b and \u03b1a+ (d\u2212 \u03b1)b = 0, then \u03b1a+ (d\u2212 \u03b1)a = 0, and since a 6= 0, we deduce that d = 0, a contradiction.\nIf X>D1 = \u03b1a+ (d\u2212 \u03b1)b = 0 and a, b 6= 0, then\nb = \u2212 \u03b1 (d\u2212 \u03b1) a,\n4.2. SPECIAL CASE: 2-WAY CLUSTERING USING NORMALIZED CUTS 49\nso we get\n\u03b1a2 + (d\u2212 \u03b1)b2 = \u03b1(d\u2212 \u03b1) 2\n\u03b12 b2 + (d\u2212 \u03b1)b2 = (d\u2212 \u03b1) ( d\u2212 \u03b1 \u03b1 + 1 ) b2 = (d\u2212 \u03b1)db2 \u03b1 ,\nand\n(a\u2212 b)2 = ( \u2212(d\u2212 \u03b1)\n\u03b1 b\u2212 b )2 = ( d\u2212 \u03b1 \u03b1 + 1 )2 b2 = d2b2 \u03b12 .\nSince\nX>DX = \u03b1a2 + (d\u2212 \u03b1)b2\nX>LX = (a\u2212 b)2 cut(A,A),\nwe obtain\nX>DX = (d\u2212 \u03b1)db2\n\u03b1 =\n\u03b1da2\n(d\u2212 \u03b1)\nX>LX = d2b2\n\u03b12 cut(A,A) =\nd2a2\n(d\u2212 \u03b1)2 cut(A,A).\nIf we wish to make \u03b1 disappear, we pick\na = \u221a d\u2212 \u03b1 \u03b1 , b = \u2212 \u221a \u03b1 d\u2212 \u03b1 ,\nand then\nX>DX = d\nX>LX = d2\n\u03b1(d\u2212 \u03b1) cut(A,A) = dNcut(A,A).\nIn this case, we are considering indicator vectors of the form{ (x1, . . . , xN) | xi \u2208 {\u221a d\u2212 \u03b1 \u03b1 ,\u2212 \u221a \u03b1 d\u2212 \u03b1 } , \u03b1 = vol(A) } ,\nfor any nonempty proper subset A of V . This is the choice adopted in von Luxburg [22]. Shi and Malik [20] use\na = 1, b = \u2212 \u03b1 d\u2212 \u03b1 = \u2212 k 1\u2212 k ,\n50 CHAPTER 4. GRAPH CLUSTERING\nwith k = \u03b1\nd .\nAnother choice found in the literature (for example, in Belkin and Niyogi [2]) is\na = 1 \u03b1 , b = \u2212 1 d\u2212 \u03b1 .\nHowever, there is no need to restrict solutions to be of either of these forms. So, let X = { (x1, . . . , xN) | xi \u2208 {a, b}, a, b \u2208 R, a, b 6= 0 } ,\nso that our solution set is K = { X \u2208 X | X>D1 = 0 } ,\nbecause by previous observations, since vectors X \u2208 X have nonzero components, X>D1 = 0 implies that \u03b1 6= 0, \u03b1 6= d, and a 6= b, where \u03b1 = vol({vi \u2208 V | xi = a}). Actually, to be perfectly rigorous, we are looking for solutions in RPN\u22121, so our solution set is really\nP(K) = { (x1 : \u00b7 \u00b7 \u00b7 : xN) \u2208 RPN\u22121 | (x1, . . . , xN) \u2208 K } .\nConsequently, our minimization problem can be stated as follows:\nProblem PNC1\nminimize X>LX X>DX subject to X>D1 = 0, X \u2208 X .\nIt is understood that the solutions are points P(X) in RPN\u22121.\nSince the Rayleigh ratio and the constraints X>D1 = 0 and X \u2208 X are scale-invariant (for any \u03bb 6= 0, the Rayleigh ratio does not change if X is replaced by \u03bbX, X \u2208 X iff \u03bbX \u2208 X , and (\u03bbX)>D1 = \u03bbX>D1 = 0), we are led to the following formulation of our problem:\nProblem PNC2\nminimize X>LX subject to X>DX = 1, X>D1 = 0, X \u2208 X .\nBecause problem PNC2 requires the constraint X>DX = 1 to be satisfied, it does not have the same set of solutions as problem PNC1. Nevertherless, problem PNC2 is equivalent to problem PNC1, in the sense that if X is any minimal solution of PNC1, then X/(X>DX)1/2 is a minimal solution of PNC2 (with the same minimal value for the objective functions), and if X is a minimal solution of PNC2, then \u03bbX is a minimal solution for PNC1 for all \u03bb 6= 0 (with the same minimal value for the objective functions). Equivalently,\n4.2. SPECIAL CASE: 2-WAY CLUSTERING USING NORMALIZED CUTS 51\nproblems PNC1 and PNC2 have the same set of minimal solutions as points P(X) \u2208 RPN\u22121 given by their homogeneous coordinates X.\nUnfortunately, this is an NP-complete problem, as shown by Shi and Malik [20]. As often with hard combinatorial problems, we can look for a relaxation of our problem, which means looking for an optimum in a larger continuous domain. After doing this, the problem is to find a discrete solution which is close to a continuous optimum of the relaxed problem.\nThe natural relaxation of this problem is to allow X to be any nonzero vector in RN , and we get the problem:\nminimize X>LX subject to X>DX = 1, X>D1 = 0.\nIn order to apply Proposition A.2, we make the change of variable Y = D1/2X, so that X = D\u22121/2Y . Then, the condition X>DX = 1 becomes\nY >Y = 1,\nthe condition X>D1 = 0\nbecomes Y >D1/21 = 0,\nand X>LX = Y >D\u22121/2LD\u22121/2Y.\nWe obtain the problem:\nminimize Y >D\u22121/2LD\u22121/2Y subject to Y >Y = 1, Y >D1/21 = 0.\nBecause L1 = 0, the vector D1/21 belongs to the nullspace of the symmetric Laplacian Lsym = D\n\u22121/2LD\u22121/2. By Proposition A.2, minima are achieved by any unit eigenvector Y of the second eigenvalue \u03bd2 > 0 of Lsym. Since 0 is the smallest eigenvalue of Lsym and since D1/21 belongs to the nullspace of Lsym, as the eigenvectors associated with distinct eigenvalues are orthogonal, the vector Y is orthogonal to D1/21, so the constraint Y >D1/21 = 0 is satisfied. Then, Z = D\u22121/2Y is a solution of our original relaxed problem. Note that because Z is nonzero and orthogonal to D1, a vector with positive entries, it must have negative and positive entries.\nThe next question is to figure how close is Z to an exact solution in X . Actually, because solutions are points in RPN\u22121, the correct statement of the question is: Find an exact solution P(X) \u2208 P(X ) which is the closest (in a suitable sense) to the approximate solution P(Z) \u2208 RPN\u22121. However, because X is closed under the antipodal map, as explained in Appendix B, minimizing the distance d(P(X),P(Z)) on RPN\u22121 is equivalent to minimizing the Euclidean distance \u2016X \u2212 Z\u20162, where X and Z are representatives of P(X) and P(Z) on"}, {"heading": "52 CHAPTER 4. GRAPH CLUSTERING", "text": "the unit sphere (if we use the Riemannian metric on RPN\u22121 induced by the Euclidean metric on RN).\nWe may assume b < 0, in which case a > 0. If all entries in Z are nonzero, due to the projective nature of the solution set, it seems reasonable to say that the partition of V is defined by the signs of the entries in Z. Thus, A will consist of nodes those vi for which xi > 0. Elements corresponding to zero entries can be assigned to either A or A, unless additional information is available. In our implementation, they are assigned to A.\nHere are some examples of normalized cuts found by a fairly naive implementation of the method. The weight matrix of the first example is\nW1 =  0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0  .\nIts underlying graph has 9 nodes and 9 edges and is shown in Figure 4.2 on the left. The normalized cut found by the algorithm is shown in the middle; the edge of the cut is shown in magenta, and the vertices of the blocks of the partition are shown in blue and red. The figure on the right shows the two disjoint subgraphs obtained after deleting the cut edge.\nThe weight matrix of the second example is\nW2 =  0 3 6 3 3 0 0 3 6 0 0 3 3 3 3 0  .\n4.2. SPECIAL CASE: 2-WAY CLUSTERING USING NORMALIZED CUTS 53\nIts underlying graph has 4 nodes and 5 edges and is shown in Figure 4.3 on the left. The normalized cut found by the algorithm is shown in the middle; the edges of the cut are shown in magenta, and the vertices of the blocks of the partition are shown in blue and red. The figure on the right shows the two disjoint subgraphs obtained after deleting the cut edges.\nThe weight matrix W3 of the third example is the adjacency matrix of the complete graph on 12 vertices. All nondiagonal entries are equal to 1, and the diagonal entries are equal to 0. This graph has 66 edges and is shown in Figure 4.4 on the left.\nThe normalized cut found by the algorithm is shown in the middle; the edges of the cut are shown in magenta, and the vertices of the blocks of the partition are shown in blue and red. The figure on the right shows the two disjoint subgraphs obtained after deleting the cut edges. Recall that Lsym = B3B > 3 for any incidence matrix B3 associated with W3, so that for any SVD U3\u03a33V3 of B3, the vectors in U3 are eigenvectors of Lsym for its eigenvalues listed in decreasing order. The normalized Laplacian of this weight matrix has the eigenvalue 1.0909 with multiplicity 11 (and any incidence matrix B3 associated with W3 has the singular value 1.0445 with multiplicity 11). Computing the SVD U3\u03a33V3 of B3 and picking the next to the last eigenvector in U3 yields a partition consisting of 7 and 5 nodes. There are other"}, {"heading": "54 CHAPTER 4. GRAPH CLUSTERING", "text": "eigenvectors that yield partitions with an equal number of elements. Since a complete graph has a lot of symmetries, it is not surprising that there are many different solutions. In fact, examination of the eigenvectors of U3 reveal very unbalanced solutions.\nFor graphs where the number N of edges is very large and the number of edges is O(N2), computing the SVD of the incidence matrix B is not practical. Instead, we compute an SVD for Lsym, which appears to be more stable that diagonalizing Lsym.\nOur naive algorithm treated zero as a positive entry. Now, using the fact that\nb = \u2212 \u03b1a d\u2212 \u03b1 ,\na better solution is to look for a vector X \u2208 RN with Xi \u2208 {a, b} which is closest to a minimum Z of the relaxed problem (in the sense that \u2016X \u2212 Z\u2016 is minimized) and with \u2016X\u2016 = \u2016Z\u2016. We designed the following algorithm.\nA vector X describing a partition (A,A) is of the form\nXi = { a if vi \u2208 A \u2212\u03b2a otherwise,\nwith\n\u03b1 = vol({vi | vi \u2208 A}), \u03b2 = \u03b1\nd\u2212 \u03b1 ,\nand where a > 0 is chosen so that \u2225\u2225X\u2225\u2225 = \u2016Z\u2016. For any solution Z of the relaxed problem, let I+Z = {i | Zi > 0} be the set of indices of positive entries in Z, I \u2212 Z = {i | Zi < 0} the set of indices of negative entries in Z, I0Z = {i | Zi = 0} the set of indices of zero entries in Z. Initially, it makes sense to form a discrete approximation X of Z such that all entries of index in I+Z are assigned the value a > 0 (to be determined later), and all other entries are assigned the value \u2212\u03b2a. In order for X and Z to have the same norm, since\u2225\u2225X\u2225\u22252 = na + \u03b22(N \u2212 na) with\nna = |I+Z |, \u03b1 = vol({vi | i \u2208 I + Z }), \u03b2 =\n\u03b1\nd\u2212 \u03b1 ,\nwe set\na = \u2016Z\u2016\u221a\n(na + \u03b22(N \u2212 na)) .\nThe problems is to determine whether an entry with an index i \u2208 I0Z (which is initially assigned the value \u2212\u03b2a) should be reassigned the value a. To make the decision, we form the new discrete solution X\u0303 obtained from X by adding the index i to I+Z , and updating\n4.2. SPECIAL CASE: 2-WAY CLUSTERING USING NORMALIZED CUTS 55\n\u03b1, \u03b2 and a; this is done in step (2). Then, we compare \u2016X \u2212 Z\u2016 and \u2016X\u0303 \u2212 Z\u2016 and keep the vector that yields the smallest norm. We delete i from I0Z , and repeat step (2). After a finite number of steps, I0Z becomes empty and we obtain a discrete solution X which is typically closer to X than the starting approximate solution.\nWe also need to decide whether to start with Z or \u2212Z (remember that solution are determined up to a nonzero scalar). We proceed as follows. Let Z+ and Z\u2212 be the vectors given by\nZ+i = { Zi if i \u2208 I+Z 0 if i /\u2208 I+Z\nZ\u2212i = { Zi if i \u2208 I\u2212Z 0 if i /\u2208 I\u2212Z .\nAlso let na = |I+Z |, nb = |I \u2212 Z |, let a and b be the average of the positive and negative entries in Z respectively, that is,\na =\n\u2211 i\u2208I+Z Zi\nna b =\n\u2211 i\u2208I\u2212Z Zi\nnb ,\nand let Z+ and Z\u2212 be the vectors given by\n(Z+)i = { a if i \u2208 I+Z 0 if i /\u2208 I+Z\n(Z\u2212)i = { b if i \u2208 I\u2212Z 0 if i /\u2208 I\u2212Z .\nIf \u2225\u2225Z+ \u2212 Z+\u2225\u2225 > \u2225\u2225Z\u2212 \u2212 Z\u2212\u2225\u2225, then replace Z by \u2212Z. Step 1 of the algorithm is to compute an initial approximate discrete solution X.\n(1) Let\nna = |I+Z |, \u03b1 = vol({vi | i \u2208 I + Z }), \u03b2 =\n\u03b1\nd\u2212 \u03b1 ,\nand form the vector X with\nX i = { a if i \u2208 I+Z \u2212\u03b2a otherwise,\nsuch that \u2225\u2225X\u2225\u2225 = \u2016Z\u2016, where the scalar a is determined by\na = \u2016Z\u2016\u221a\n(na + \u03b22(N \u2212 na)) .\nNext, pick some entry with index i \u2208 I0Z and see whether we can impove the solution X by adding i to I+Z ."}, {"heading": "56 CHAPTER 4. GRAPH CLUSTERING", "text": "(2) While I0Z 6= \u2205, pick the smallest index i \u2208 I0Z , compute\nI\u0303+Z = I + Z \u222a {i} n\u0303a = na + 1\n\u03b1\u0303 = \u03b1 + d(vi)\n\u03b2\u0303 = \u03b1\u0303\nd\u2212 \u03b1\u0303 ,\nand then X\u0303 with\nX\u0303j = { a\u0303 if j \u2208 I\u0303+Z \u2212\u03b2\u0303a\u0303 otherwise,\nand\na\u0303 = \u2016Z\u2016\u221a\nn\u0303a + \u03b2\u03032(N \u2212 n\u0303a) .\nSet I0Z = I 0 Z \u2212{i}. If \u2016X\u0303 \u2212Z\u2016 < \u2016X \u2212Z\u2016, then let X = X\u0303, I+Z = I\u0303 + Z , na = n\u0303a, \u03b1 = \u03b1\u0303. Go back to (2).\n(3) The final answer if X.\nI implemented this algorithm, and it seems to do a god job dealing with zero entries in the continuous solution Z.\n4.3 K-Way Clustering Using Normalized Cuts\nWe now consider the general case in which K \u2265 3. Two crucial issues need to be addressed (to the best of our knowledge, these points are not clearly articulated in the literature).\n1. The choice of a matrix representation for partitions on the set of vertices. It is important that such a representation be scale-invariant. It is also necessary to state necessary and sufficient conditions for such matrices to represent a partition.\n2. The choice of a metric to compare solutions. It turns out that the space of discrete solutions can be viewed as a subset of the K-fold product (RPN\u22121)K of the projective space RPN\u22121. Version 1 of the formulation of our minimization problem (PNC1) makes this point clear. However, the relaxation (\u22172) of version 2 of our minimization problem (PNC2), which is equivalent to version 1, reveals that that the solutions of the relaxed problem (\u22172) are members of the Grassmannian G(K,N). Thus, we have two choices of metrics: (1) a metric on (RPN\u22121)K ; (2) a metric on G(K,N). We discuss the first choice, which is the choice implicitly adopted by Shi and Yu. Actually, it appears that it is difficult to deal with the product metric on (RPN\u22121)K induced by a metric on RPN\u22121. Instead, we approximate a metric on (RPN\u22121)K using the Frobenius norm; see Section 4.5 for details.\n4.3. K-WAY CLUSTERING USING NORMALIZED CUTS 57\nWe describe a partition (A1, . . . , AK) of the set of nodes V by an N \u00d7 K matrix X = [X1 \u00b7 \u00b7 \u00b7XK ] whose columns X1, . . . , XK are indicator vectors of the partition (A1, . . . , AK). Inspired by what we did in Section 4.2, we assume that the vector Xj is of the form\nXj = (xj1, . . . , x j N),\nwhere xji \u2208 {aj, bj} for j = 1, . . . , K and i = 1, . . . , N , and where aj, bj are any two distinct real numbers. The vector Xj is an indicator vector for Aj in the sense that, for i = 1, . . . , N ,\nxji = { aj if vi \u2208 Aj bj if vi /\u2208 Aj.\nWhen {aj, bj} = {0, 1} for j = 1, . . . , K, such a matrix is called a partition matrix by Yu and Shi. However, such a choice is premature, since it is better to have a scale-invariant representation to make the denominators of the Rayleigh ratios go away.\nSince the partition (A1, . . . , AK) consists of nonempty pairwise disjoint blocks whose union is V , some conditions on X are required to reflect these properties, but we will worry about this later.\nAs in Section 4.2, we seek conditions on the ajs and the bjs in order to express the normalized cut Ncut(A1, . . . , AK) as a sum of Rayleigh ratios. Then, we reformulate our optimization problem in a more convenient form, by chasing the denominators in the Rayleigh ratios, and by expressing the objective function in terms of the trace of a certain matrix. This will reveal the important fact that the solutions of the relaxed problem are right-invariant under multiplication by a K \u00d7K orthogonal matrix.\nLet d = 1>D1 and \u03b1j = vol(Aj), so that \u03b11 + \u00b7 \u00b7 \u00b7+\u03b1K = d. Then, vol(Aj) = d\u2212\u03b1j, and as in Section 4.2, we have\n(Xj)>LXj = (aj \u2212 bj)2 cut(Aj, Aj), (Xj)>DXj = \u03b1ja 2 j + (d\u2212 \u03b1j)b2j .\nWhen K \u2265 3, unlike the case K = 2, in general we have cut(Aj, Aj) 6= cut(Ak, Ak) if j 6= k, and since\nNcut(A1, . . . , AK) = K\u2211 j=1 cut(Aj, Aj) vol(Aj) ,\nwe would like to choose aj, bj so that\ncut(Aj, Aj)\nvol(Aj) =\n(Xj)>LXj (Xj)>DXj j = 1, . . . , K,\nbecause this implies that\n\u00b5(X) = Ncut(A1, . . . , AK) = K\u2211 j=1 cut(Aj, Aj) vol(Aj) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj ."}, {"heading": "58 CHAPTER 4. GRAPH CLUSTERING", "text": "Since (Xj)>LXj\n(Xj)>DXj = (aj \u2212 bj)2 cut(Aj, Aj) \u03b1ja2j + (d\u2212 \u03b1j)b2j\nand vol(Aj) = \u03b1j, in order to have\ncut(Aj, Aj)\nvol(Aj) =\n(Xj)>LXj (Xj)>DXj j = 1, . . . , K,\nwe need to have (aj \u2212 bj)2\n\u03b1ja2j + (d\u2212 \u03b1j)b2j =\n1\n\u03b1j j = 1, . . . , K.\nThus, we must have\n(a2j \u2212 2ajbj + b2j)\u03b1j = \u03b1ja2j + (d\u2212 \u03b1j)b2j ,\nwhich yields\n2\u03b1jbj(bj \u2212 aj) = db2j .\nThe above equation is trivially satisfied if bj = 0. If bj 6= 0, then\n2\u03b1j(bj \u2212 aj) = dbj,\nwhich yields\naj = 2\u03b1j \u2212 d\n2\u03b1j bj.\nThis choice seems more complicated that the choice bj = 0, so we will opt for the choice bj = 0, j = 1, . . . , K. With this choice, we get\n(Xj)>DXj = \u03b1ja 2 j .\nThus, it makes sense to pick\naj = 1 \u221a \u03b1j\n= 1\u221a\nvol(Aj) , j = 1, . . . , K,\nwhich is the solution presented in von Luxburg [22]. This choice also corresponds to the scaled partition matrix used in Yu [23] and Yu and Shi [24].\nWhen N = 10 and K = 4, an example of a matrix X representing the partition of V = {v1, v2, . . . , v10} into the four blocks\n{A1, A2, A3, A4} = {{v2, v4, v6}, {v1, v5}, {v3, v8, v10}, {v7, v9}},\n4.3. K-WAY CLUSTERING USING NORMALIZED CUTS 59\nis shown below:\nX =  0 a2 0 0 a1 0 0 0 0 0 a3 0 a1 0 0 0 0 a2 0 0 a1 0 0 0 0 0 0 a4 0 0 a3 0 0 0 0 a4 0 0 a3 0  .\nLet us now consider the problem of finding necessary and sufficient conditions for a matrix X to represent a partition of V .\nWhen bj = 0, the pairwise disjointness of the Ai is captured by the orthogonality of the X i: (X i)>Xj = 0, 1 \u2264 i, j \u2264 K, i 6= j. (\u2217) This is because, for any matrix X where the nonzero entries in each column have the same sign, for any i 6= j, the condition\n(X i)>Xj = 0\nsays that for every k = 1, . . . , N , if xik 6= 0 then x j k = 0.\nWhen we formulate our minimization problem in terms of Rayleigh ratios, conditions on the quantities (X i)>DX i show up, and it is more convenient to express the orthogonality conditions using the quantities (X i)>DXj instead of the (X i)>Xj, because these various conditions can be combined into a single condition involving the matrix X>DX. Now, because D is a diagonal matrix with positive entries and because the nonzero entries in each column of X have the same sign, for any i 6= j, the condition\n(X i)>Xj = 0\nis equivalent to (X i)>DXj = 0, (\u2217\u2217)\nsince, as above, it means that for k = 1, . . . , N , if xik 6= 0 then x j k = 0. Observe that the orthogonality conditions (\u2217) (and (\u2217\u2217)) are equivalent to the fact that every row of X has at most one nonzero entry.\nRemark: The disjointness condition\nX1K = 1N\nis used in Yu [23]. However, this condition does guarantee the disjointness of the blocks. For example, it is satisfied by the matrix X whose first column is 1N , with 0 everywhere else."}, {"heading": "60 CHAPTER 4. GRAPH CLUSTERING", "text": "Each Aj is nonempty iff X j 6= 0, and the fact that the union of the Aj is V is captured by the fact that each row of X must have some nonzero entry (every vertex appears in some block). It is not immediately obvious how to state conveniently this condition in matrix form.\nObserve that the diagonal entries of the matrix XX> are the square Euclidean norms of the rows of X. Therefore, we can assert that these entries are all nonzero. Let DIAG be the function which returns the diagonal matrix (containing the diagonal of A),\nDIAG(A) = diag(a1 1, . . . , ann),\nfor any square matrix A = (ai j). Then, the condition for the rows of X to be nonzero can be stated as\ndet(DIAG(XX>)) 6= 0.\nSince every row of any matrix X representing a partition has a single nonzero entry aj, we have X>X = diag ( n1a 2 1, . . . , nKa 2 K ) ,\nwhere nj is the number of elements in Aj, the jth block of the partition. Therefore, an equivalent condition for the columns of X to be nonzero is\ndet(X>X) 6= 0.\nRemark: The matrix\nDIAG(XX>)\u22121/2X\nis the result of normalizing the rows of X so that they have Euclidean norm 1. This normalization step is used by Yu [23] in the search for a discrete solution closest to a solution of a relaxation of our original problem. For our special matrices representing partitions, normalizing the rows will have the effect of rescaling the columns (if row i has aj in column j, then all nonzero entries in column j are equal to aj), but for a more general matrix, this is false. Thus, in general, DIAG(XX>)\u22121/2X is not a solution of the original problem. Still, as we will see in Section 4.5, this matrix is a pretty good approximation to a discrete solution.\nAnother condition which does not involve explicitly a determinant and is scale-invariant stems from the observation that not only\nX>X = diag ( n1a 2 1, . . . , nKa 2 K ) ,\nbut\nX>1N =  n1a1... nKaK  ,\n4.3. K-WAY CLUSTERING USING NORMALIZED CUTS 61\nand these equations imply that\n(X>X)\u22121X>1N =  1 a1 ... 1 aK  , and thus\nX(X>X)\u22121X>1N = 1N . (\u2020)\nWhen aj = 1 for j = 1, . . . , K, we have (X >X)\u22121X>1 = 1K , and condition (\u2020) reduces to\nX1K = 1N .\nNote that because the columns of X are linearly independent, (X>X)\u22121X> is the pseudoinverse X+ of X. Consequently, if X>X is invertible, condition (\u2020) can also be written as\nXX+1N = 1N .\nHowever, it is well known that XX+ is the orthogonal projection of RK onto the range of X (see Gallier [8], Section 14.1), so the condition XX+1N = 1N is equivalent to the fact that 1N belongs to the range of X. In retrospect, this should have been obvious since the columns of a solution X satisfy the equation\na\u221211 X 1 + \u00b7 \u00b7 \u00b7+ a\u22121K X K = 1N .\nWe emphasize that it is important to use conditions that are invariant under multiplication by a nonzero scalar, because the Rayleigh ratio is scale-invariant, and it is crucial to take advantage of this fact to make the denominators go away.\nIf we let X = {\n[X1 . . . XK ] | Xj = aj(xj1, . . . , x j N), x j i \u2208 {1, 0}, aj \u2208 R, Xj 6= 0 } (note that the condition Xj 6= 0 implies that aj 6= 0), then the set of matrices representing partitions of V into K blocks is\nK = { X = [X1 \u00b7 \u00b7 \u00b7 XK ] | X \u2208 X ,\n(X i)>DXj = 0, 1 \u2264 i, j \u2264 K, i 6= j, X(X>X)\u22121X>1 = 1 } .\nSince for matrices in K, the orthogonality conditions (X i)>DXj = 0 are equivalent to the orthogonality conditions (X i)>Xj = 0, and since matrices in X have nonzero columns, X>X is invertible, so the last condition makes sense."}, {"heading": "62 CHAPTER 4. GRAPH CLUSTERING", "text": "As in the case K = 2, to be rigorous, the solution are really K-tuples of points in RPN\u22121, so our solution set is really\nP(K) = { (P(X1), . . . ,P(XK)) | [X1 \u00b7 \u00b7 \u00b7 XK ] \u2208 K } .\nIn view of the above, we have our first formulation of K-way clustering of a graph using normalized cuts, called problem PNC1 (the notation PNCX is used in Yu [23], Section 2.1):\nK-way Clustering of a graph using Normalized Cut, Version 1: Problem PNC1\nminimize K\u2211 j=1 (Xj)>LXj (Xj)>DXj subject to (X i)>DXj = 0, 1 \u2264 i, j \u2264 K, i 6= j, X(X>X)\u22121X>1 = 1, X \u2208 X .\nAs in the case K = 2, the solutions that we are seeking are K-tuples (P(X1), . . . ,P(XK)) of points in RPN\u22121 determined by their homogeneous coordinates X1, . . . , XK .\nRemark: Because\n(Xj)>LXj = (Xj)>DXj \u2212 (Xj)>WXj = vol(Aj)\u2212 (Xj)>WXj,\nInstead of minimizing\n\u00b5(X1, . . . , XK) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj ,\nwe can maximize\n(X1, . . . , XK) = K\u2211 j=1 (Xj)>WXj (Xj)>DXj ,\nsince (X1, . . . , XK) = K \u2212 \u00b5(X1, . . . , XK).\nThis second option is the one chosen by Yu [23] and Yu and Shi [24] (actually, they work with 1 K\n(K \u2212 \u00b5(X1, . . . , XK)), but this doesn\u2019t make any difference). Theoretically, minimizing \u00b5(X1, . . . , XK) is equivalent to maximizing (X1, . . . , XK), but from a practical point of view, it is preferable to maximize (X1, . . . , XK). This is because minimizing solutions of \u00b5 are obtained from (unit) eigenvectors corresponding to the K smallest eigenvalues of Lsym = D\n\u22121/2LD\u22121/2 (by multiplying these eigenvectors by D1/2). However, numerical methods for computing eigenvalues and eigenvectors of a symmetric matrix do much better at computing largest eigenvalues. Since Lsym = I \u2212D\u22121/2WD\u22121/2, the eigenvalues of Lsym\n4.3. K-WAY CLUSTERING USING NORMALIZED CUTS 63\nlisted in increasing order correspond to the eigenvalues of I\u2212Lsym = D\u22121/2WD\u22121/2 listed in decreasing order. Furthermore, v is an eigenvector of Lsym for the ith smallest eigenvalue \u03bdi iff v is an eigenvector of I \u2212Lsym for the (N + 1\u2212 i)th largest eigenvalue \u03bdi. Therefore, it is preferable to find the largest eigenvalues of I\u2212Lsym = D\u22121/2WD\u22121/2 and their eigenvectors. In fact, since the eigenvalues of Lsym are in the range [0, 2], the eigenvalues of 2I\u2212Lsym = I+ D\u22121/2WD\u22121/2 are also in the range [0, 2] (that is, I+D\u22121/2WD\u22121/2 is positive semidefinite).\nLet us now show how our original formulation (PNC1) can be converted to a more convenient form, by chasing the denominators in the Rayleigh ratios, and by expressing the objective function in terms of the trace of a certain matrix.\nFor any N \u00d7N matrix A, because\nX>AX =  (X 1)>\n... (XK)>\nA[X1 \u00b7 \u00b7 \u00b7XK ]\n=  (X1)>AX1 (X1)>AX2 \u00b7 \u00b7 \u00b7 (X1)>AXK (X2)>AX1 (X2)>AX2 \u00b7 \u00b7 \u00b7 (X2)>AXK ... ... . . . ...\n(XK)>AX1 (XK)>AX2 \u00b7 \u00b7 \u00b7 (XK)>AXK  , we have\ntr(X>AX) = K\u2211 j=1 (Xj)>AXj,\nand the conditions (X i)>AXj = 0, 1 \u2264 i, j \u2264 K, i 6= j,\nare equivalent to X>AX = diag((X1)>AX1, . . . , (XK)>AXK).\nAs a consequence, if we assume that\n(X1)>AX1 = \u00b7 \u00b7 \u00b7 = (XK)>AXK = \u03b12,\nthen we have X>AX = \u03b12I,\nand if R is any orthogonal K \u00d7K matrix, then by multiplying on the left by R> and on the right by R, we get\nR>X>AXR = R>\u03b12IR = \u03b12R>R = \u03b12I.\nTherefore, if X>AX = \u03b12I,\nthen (XR)>A(XR) = \u03b12I,"}, {"heading": "64 CHAPTER 4. GRAPH CLUSTERING", "text": "for any orthogonal K\u00d7K matrix R. Furthermore, because tr(AB) = tr(BA) for all matrices A,B, we have\ntr(R>X>AXR) = tr(X>AX).\nSince the Rayleigh ratios (Xj)>LXj\n(Xj)>DXj\nare invariant under rescaling by a nonzero number, by replacing Xj by ((Xj)>DXj)\u22121/2Xj, the denominators become 1, and we have\n\u00b5(X) = \u00b5(X1, . . . , XK) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj\n= \u00b5(((X1)>DX1)\u22121/2X1, . . . , ((XK)>DXK)\u22121/2XK) = K\u2211 j=1 ((Xj)>DXj)\u22121/2(Xj)>L ((Xj)>DXj)\u22121/2Xj = tr(\u039b\u22121/2X>LX\u039b\u22121/2) = tr(\u039b\u22121X>LX),\nwhere \u039b = diag((X1)>DX1, . . . , (XK)>DXK).\nIf (X1)>DX1 = \u00b7 \u00b7 \u00b7 = (XK)>DXK = \u03b12, then \u039b = \u03b12IK , so\n\u00b5(X) = tr(\u039b\u22121X>LX) = 1\n\u03b12 tr(X>LX),\nand for any orthogonal K \u00d7K matrix R,\n\u00b5(RX) = 1\n\u03b12 tr(R>X>LXR) =\n1\n\u03b12 tr(X>LX),\nand thus, \u00b5(X) = \u00b5(XR).\nThe condition X(X>X)\u22121X>1 = 1\nis also invariant if we replace X by XR, where R is any invertible matrix, because\nXR((XR)>(XR))\u22121(XR)>1 = XR(R>X>XR)\u22121R>X>1\n= XRR\u22121(X>X)\u22121(R>)\u22121R>X>1 = X(X>X)\u22121X>1 = 1.\nIn summary we proved the following proposition:\n4.3. K-WAY CLUSTERING USING NORMALIZED CUTS 65\nProposition 4.1. For any orthogonal K \u00d7K matrix R, any symmetric N \u00d7 N matrix A, and any N \u00d7K matrix X = [X1 \u00b7 \u00b7 \u00b7 XK ], the following properties hold:\n(1) \u00b5(X) = tr(\u039b\u22121X>LX), where\n\u039b = diag((X1)>DX1, . . . , (XK)>DXK).\n(2) If (X1)>DX1 = \u00b7 \u00b7 \u00b7 = (XK)>DXK = \u03b12, then\n\u00b5(X) = \u00b5(XR) = 1\n\u03b12 tr(X>LX).\n(3) The condition X>AX = \u03b12I is preserved if X is replaced by XR.\n(4) The condition X(X>X)\u22121X>1 = 1 is preserved if X is replaced by XR.\nNow, by Proposition 4.1(1) and the fact that the conditions in PNC1 are scale-invariant, we are led to the following formulation of our problem:\nminimize tr(X>LX) subject to (X i)>DXj = 0, 1 \u2264 i, j \u2264 K, i 6= j, (Xj)>DXj = 1, 1 \u2264 j \u2264 K, X(X>X)\u22121X>1 = 1, X \u2208 X .\nConditions on lines 2 and 3 can be combined in the equation\nX>DX = I,\nand, we obtain the following formulation of our minimization problem:\nK-way Clustering of a graph using Normalized Cut, Version 2: Problem PNC2\nminimize tr(X>LX) subject to X>DX = I,\nX(X>X)\u22121X>1 = 1, X \u2208 X .\nBecause problem PNC2 requires the constraint X>DX = I to be satisfied, it does not have the same set of solutions as problem PNC1. Nevertherless, problem PNC2 is equivalent to problem PNC1, in the sense that for every minimal solution (X1, . . . , XK) of PNC1, (((X1)>DX1)\u22121/2X1, . . . , ((XK)>DXK)\u22121/2XK) is a minimal solution of PNC2 (with the same minimum for the objective functions), and that for every minimal solution (Z1, . . . , Zk)\n66 CHAPTER 4. GRAPH CLUSTERING\nof PNC2, (\u03bb1Z 1, . . . , \u03bbKZ K) is a minimal solution of PNC1, for all \u03bbi 6= 0, i = 1, . . . , K (with the same minimum for the objective functions). In other words, problems PNC1 and PNC2 have the same set of minimal solutions as K-tuples of points (P(X1), . . . ,P(XK)) in RPN\u22121 determined by their homogeneous coordinates X1, . . . , XK .\nFormulation PNC2 reveals that finding a minimum normalized cut has a geometric interpretation in terms of the graph drawings discussed in Section 3.1. Indeed, PNC2 has the following equivalent formulation: Find a minimal energy graph drawing X in RK of the weighted graph G = (V,W ) such that:\n1. The matrix X is orthogonal with respect to the inner product \u3008\u2212,\u2212\u3009D in RN induced by D, with\n\u3008x, y\u3009D = x >Dy, x, y \u2208 RN .\n2. The rows of X are nonzero; this means that no vertex vi \u2208 V is assigned to the origin of RK (the zero vector 0K).\n3. Every vertex vi is assigned a point of the form (0, . . . , 0, aj, 0, . . . , 0) on some axis (in RK).\n4. Every axis in RK is assigned at least some vertex.\nCondition 1 can be reduced to the standard condition for graph drawings (R>R = I) by making the change of variable Y = D1/2X or equivalently X = D\u22121/2Y . Indeed,\ntr(X>LX) = tr(Y >D\u22121/2LD\u22121/2Y ),\nso we use the normalized Laplacian Lsym = D \u22121/2LD\u22121/2 instead of L,\nX>DX = Y >Y = I,\nand conditions (2), (3), (4) are preserved under the change of variable Y = D1/2X, since D1/2 is invertible. However, conditions (2), (3), (4) are \u201chard\u201d constraints, especially condition (3). In fact, condition (3) implies that the columns of X are orthogonal with respect to both the Euclidean inner product and the inner product \u3008\u2212,\u2212\u3009D, so condition (1) is redundant, except for the fact that it prescribes the norm of the columns, but this is not essential due to the projective nature of the solutions.\nThe main problem in finding a good relaxation of problem PNC2 is that it is very difficult to enforce the condition X \u2208 X . Also, the solutions X are not preserved under arbitrary rotations, but only by very special rotations which leave X invariant (they exchange the axes).\nThe first natural relaxation of problem PNC2 is to drop the condition that X \u2208 X , and we obtain the\nProblem (\u22172)\n4.3. K-WAY CLUSTERING USING NORMALIZED CUTS 67\nminimize tr(X>LX) subject to X>DX = I,\nX(X>X)\u22121X>1 = 1.\nActually, since the discrete solutions X \u2208 X that we are ultimately seeking are solutions of problem PNC1, the preferred relaxation is the one obtained from problem PNC1 by dropping the condition X \u2208 X , and simply requiring that Xj 6= 0, for j = 1, . . . , K: Problem (\u22171)\nminimize K\u2211 j=1 (Xj)>LXj (Xj)>DXj subject to (X i)>DXj = 0, Xj 6= 0 1 \u2264 i, j \u2264 K, i 6= j, X(X>X)\u22121X>1 = 1.\nNow that we dropped the condition X \u2208 X , it is not clear that X>X is invertible in (\u22171) and (\u22172). However, since the columns of X are nonzero and D-orthogonal, they must be linearly independent, so X has rank K and and X>X is invertible.\nAs we explained before, every solution Z = [Z1, . . . , ZK ] of problem (\u22171) yields a solution of problem (\u22172) by normalizing each Zj by ((Zj)>DZj)1/2, and conversely for every solution Z = [Z1, . . . , ZK ] of problem (\u22172), the K-tuple [\u03bb1Z1, . . . , \u03bbKZK ] is a solution of problem (\u22171), where \u03bbj 6= 0 for j = 1, . . . , K. Furthermore, by Proposition 4.1, for every orthogonal matrix R \u2208 O(K) and for every solution X of (\u22172), the matrix XR is also a solution of (\u22172). Since Proposition 4.1(2) requires that all (Xj)>DXj have the same value in order to have \u00b5(X) = \u00b5(XR), in general, if X is a solution of (\u22171), the matrix XR is not necessarily a solution of (\u22171). However, every solution X of (\u22172) is also a solution of (\u22171), for every R \u2208 O(K), XR is a solution of both (\u22172) and (\u22171), and since (\u22171) is scale-invariant, for every diagonal invertible matrix \u039b, the matrix XR\u039b is a solution of (\u22171).\nIn summary, every solution Z of problem (\u22172) yields a family of solutions of problem (\u22171); namely, all matrices of the form ZR\u039b, where R \u2208 O(K) and \u039b is a diagonal invertible matrix. We will take advantage of this fact in looking for a discrete solution X \u201cclose\u201d to a solution Z of the relaxed problem (\u22172).\nObserve that a matrix is of the form R\u039b with R \u2208 O(K) and \u039b a diagonal invertible matrix iff its columns are nonzero and pairwise orthogonal. First, we have\n(R\u039b)>R\u039b = \u039b>R>R\u039b = \u039b2,\nwhich implies that the columns of R\u039b are nonzero and pairwise orthogonal. Conversely, if the columns of A are nonzero and pairwise orthogonal, then\nA>A = \u039b2"}, {"heading": "68 CHAPTER 4. GRAPH CLUSTERING", "text": "for some invertible diagonal matrix \u039b, and then A = R\u039b, where R = A\u039b\u22121 is orthogonal.\nAs a consequence of the invariance of solutions of (\u22172) under multiplication on the right by matrices in O(K), as explained below, we can view the solutions of problem (\u22172) as elements of the Grassmannian G(K,N).\nRecall that the Stiefel manifold St(k, n) consists of the set of orthogonal k-frames in Rn, that is, the k-tuples of orthonormal vectors (u1, . . . , uk) with ui \u2208 Rn. For k = n, the manifold St(n, n) is identical to the orthogonal group O(n). For 1 \u2264 n \u2264 n \u2212 1, the group SO(n) acts transitively on St(k, n), and St(k, n) is isomorphic to the coset manifold SO(n)/SO(n \u2212 k). The Grassmann manifold G(k, n) consists of all (linear) k-dimensional subspaces of Rn. Again, the group SO(n) acts transitively on G(k, n), and G(k, n) is isomorphic to the coset manifold SO(n)/S(O(k) \u00d7O(n \u2212 k)). The group O(k) acts on the right on the Stiefel manifold St(k, n) (by multiplication), and the orbit manifold St(k, n)/O(k) is isomorphic to the Grassmann manifold G(k, n). Furthermore, both St(k, n) and G(k, n) are naturally reductive homogeneous manifolds (for the Stiefel manifold, when n \u2265 3), and G(k, n) is even a symmetric space (see O\u2019Neill [18]). The upshot of all this is that to a large extent, the differential geometry of these manifolds is completely determined by some subspace m of the Lie algebra so(n), such that we have a direct sum\nso(n) = m\u2295 h,\nwhere h = so(n\u2212k) in the case of the Stiefel manifold, and h = so(k)\u00d7 so(n\u2212k) in the case of the Grassmannian manifold (some additional condition on m is required). In particular, the geodesics in both manifolds can be determined quite explicitly, and thus we obtain closed form formulae for distances, etc.\nThe Stiefel manifold St(k, n) can be viewed as the set of all n\u00d7 k matrices X such that\nX>X = Ik.\nIn our situation, we are considering N \u00d7K matrices X such that\nX>DX = I.\nThis is not quite the Stiefel manifold, but if we write Y = D1/2X, then we have\nY >Y = I,\nso the space of matrices X satisfying the condition X>DX = I is the image D(St(K,N)) of the Stiefel manifold St(K,N) under the linear map D given by\nD(X) = D1/2X.\nNow, the right action of O(K) on D(St(K,N)) yields a coset manifold D(St(K,N))/O(K) which is obviously isomorphic to the Grassmann manidold G(K,N).\n4.3. K-WAY CLUSTERING USING NORMALIZED CUTS 69\nTherefore, the solutions of problem (\u22172) can be viewed as elements of the Grassmannian G(K,N). We can take advantage of this fact to find a discrete solution of our original optimization problem PNC2 approximated by a continuous solution of (\u22172).\nRecall that if X>X is invertible (which is the case), condition X(X>X)\u22121X>1 = 1 is equivalent to XX+1 = 1, which is also equivalent to the fact that 1 is in the range of X. If we make the change of variable Y = D1/2X or equivalently X = D\u22121/2Y , the condition that 1 is in the range of X becomes the condition that D1/21 is in the range of Y , which is equivalent to\nY Y +D1/21 = D1/21.\nHowever, since Y >Y = I, we have\nY + = Y >,\nso we get the equivalent problem\nProblem (\u2217\u22172)\nminimize tr(Y >D\u22121/2LD\u22121/2Y ) subject to Y >Y = I,\nY Y >D1/21 = D1/21.\nThis time, the matrices Y satisfying condition Y >Y = I do belong to the Stiefel manifold St(K,N), and again, we view the solutions of problem (\u2217\u22172) as elements of the Grassmannian G(K,N). We pass from a solution Y of problem (\u2217\u22172) in G(K,N) to a solution Z of of problem (\u22172) in G(K,N) by the linear map D\u22121; namely, Z = D\u22121(Y ) = D\u22121/2Y .\nIt is not a priori obvious that the minimum of tr(Y >LsymY ) over all N \u00d7K matrices Y satisfying Y >Y = I is equal to the sum \u03bd1 + \u00b7 \u00b7 \u00b7 + \u03bdK of the first K eigenvalues of Lsym = D\u22121/2LD\u22121/2. Fortunately, the Poincare\u0301 separation theorem (Proposition A.3) guarantees that the sum of the K smallest eigenvalues of Lsym is a lower bound for tr(Y\n>LsymY ). Furthermore, if we temporarily ignore the second constraint, the minimum of problem (\u2217\u22172) is achieved by any K unit eigenvectors (u1, . . . , uK) associated with the smallest eigenvalues\n0 = \u03bd1 \u2264 \u03bd2 \u2264 . . . \u2264 \u03bdK\nof Lsym. 1 We may assume that \u03bd2 > 0, namely that the underlying graph is connected (otherwise, we work with each connected component), in which case Y 1 = D1/21/ \u2225\u2225D1/21\u2225\u2225\n2 , because 1 is in the nullspace of L. Since Y 1 = D1/21/ \u2225\u2225D1/21\u2225\u2225\n2 , the vector D1/21 is in the\nrange of Y , so the condition\nY Y >D1/21 = D1/21\n1Other authors seem to accept this fact as obvious. This is not quite so, and Godsil and Royle [10] provide a rigorous proof using Proposition A.3."}, {"heading": "70 CHAPTER 4. GRAPH CLUSTERING", "text": "is also satisfied. Then, Z = D\u22121/2Y with Y = [u1 . . . uK ] yields a minimum of our relaxed problem (\u22172) (the second constraint is satisfied because 1 is in the range of Z).\nBy Proposition 2.6, the vectors Zj are eigenvectors of Lrw associated with the eigenvalues 0 = \u03bd1 \u2264 \u03bd2 \u2264 . . . \u2264 \u03bdK . Recall that 1 is an eigenvector for the eigenvalue \u03bd1 = 0, and Z1 = 1/ \u2225\u2225D1/21\u2225\u2225 2 . Because, (Y i)>Y j = 0 whenever i 6= j, we have\n(Zi)>DZj = 0, whenever i 6= j.\nThis implies that Z2, . . . , ZK are all orthogonal to D1, and thus, that each Zj has both some positive and some negative coordinate, for j = 2, . . . , K.\nThe conditions (Zi)>DZj = 0 do not necessarily imply that Zi and Zj are orthogonal (w.r.t. the Euclidean inner product), but we can obtain a solution of Problems (\u22172) and (\u22171) achieving the same minimum for which distinct columns Zi and Zj are simultaneously orthogonal and D-orthogonal, by multiplying Z by some K \u00d7 K orthogonal matrix R on the right. Indeed, if Z is a solution of (\u22172) obtained as above, the K \u00d7K symmetric matrix Z>Z can be diagonalized by some orthogonal K \u00d7K matrix R as\nZ>Z = R\u03a3R>,\nwhere \u03a3 is a diagonal matrix, and thus,\nR>Z>ZR = (ZR)>ZR = \u03a3,\nwhich shows that the columns of ZR are orthogonal. By Proposition 4.1, ZR also satisfies the constraints of (\u22172) and (\u22171), and tr((ZR)>L(ZR)) = tr(Z>LZ).\nRemark: Since Y has linearly independent columns (in fact, orthogonal) and since Z = D\u22121/2Y , the matrix Z also has linearly independent columns, so Z>Z is positive definite and the entries in \u03a3 are all positive. Also, instead of computing Z>Z explicitly and diagonalizing it, the matrix R can be found by computing an SVD of Z.\nIn summary, we should look for a solution Z of (\u22172) that corresponds to an element of the Grassmannian G(K,N), and hope that for some suitable orthogonal matrix R and some diagonal invertible matrix \u039b, the vectors in XR\u039b are close to a true solution of the original problem.\n4.4 K-Way Clustering; Using The Dependencies\nAmong X1, . . . , XK\nAt this stage, it is interesting to reconsider the case K = 2 in the light of what we just did when K \u2265 3. When K = 2, X1 and X2 are not independent, and it is convenient to assume that the nonzero entries in X1 and X2 are both equal to some positive real c \u2208 R, so that\nX1 +X2 = c1.\n4.4. K-WAY CLUSTERING; USING THE DEPENDENCIES AMONG X1, . . . , XK 71\nTo avoid subscripts, write (A,A) for the partition of V that we are seeking, and as before let d = 1>D1 and \u03b1 = vol(A). We know from Section 4.2 that\n(X1)>DX1 = \u03b1c2 (X2)>DX2 = (d\u2212 \u03b1)c2,\nso we normalize X1 and X2 so that (X1)>DX1 = (X2)>DX2 = c2, and we consider\nX = [ X1\u221a \u03b1 X2\u221a d\u2212 \u03b1 ] .\nNow, we claim that there is an orthogonal matrix R so that if X as above is a solution to our discrete problem, then XR contains a multiple of 1 as a first column. A similar observation is made in Yu [23] and Yu and Shi [24] (but beware that in these works \u03b1 = vol(A)/ \u221a d). In fact,\nR = 1\u221a d\n( \u221a \u03b1\n\u221a d\u2212 \u03b1\n\u221a d\u2212 \u03b1 \u2212 \u221a \u03b1\n) .\nIndeed, we have\nXR = [ X1\u221a \u03b1 c1\u2212X1\u221a d\u2212 \u03b1 ] R\n= [ X1\u221a \u03b1 c1\u2212X1\u221a d\u2212 \u03b1 ] 1\u221a d ( \u221a \u03b1\n\u221a d\u2212 \u03b1\n\u221a d\u2212 \u03b1 \u2212 \u221a \u03b1\n)\n= 1\u221a d\n[ c1 \u221a d\u2212 \u03b1 \u03b1 X1 \u2212 \u221a \u03b1 d\u2212 \u03b1 (c1\u2212X1) ] .\nIf we let\na = c \u221a d\u2212 \u03b1 \u03b1 , b = \u2212c \u221a \u03b1 d\u2212 \u03b1 ,\nthen we check that \u03b1a+ b(d\u2212 \u03b1) = 0,\nwhich shows that the vector\nZ = \u221a d\u2212 \u03b1 d\u03b1 X1 \u2212 \u221a\n\u03b1\nd(d\u2212 \u03b1) (c1\u2212X1)\nis a potential solution of our discrete problem in the sense of Section 4.2. Furthermore, because L1 = 0,\ntr(X>LX) = tr((XR)>L(XR)) = Z>LZ,\nthe vector Z is indeed a solution of our discrete problem. Thus, we reconfirm the fact that the second eigenvector of Lrw = D\n\u22121L is indeed a continuous approximation to the clustering problem when K = 2. This can be generalized for any K \u2265 2."}, {"heading": "72 CHAPTER 4. GRAPH CLUSTERING", "text": "Again, we may assume that the nonzero entries in X1, . . . , XK are some positive real c \u2208 R, so that\nX1 + \u00b7 \u00b7 \u00b7+XK = c1,\nand if (A1, . . . , AK) is the partition of V that we are seeking, write \u03b1j = vol(Aj). We have \u03b11 + \u00b7 \u00b7 \u00b7+ \u03b1K = d = 1>D1. Since\n(Xj)>DXj = \u03b1jc 2,\nwe normalize the Xj so that (Xj)>DXj = \u00b7 \u00b7 \u00b7 = (XK)>DXK = c2, and we consider\nX = [ X1 \u221a \u03b11 X2 \u221a \u03b12 \u00b7 \u00b7 \u00b7 X K \u221a \u03b1K ] .\nThen, we have the following result. Proposition 4.2. If X = [ X1\u221a \u03b11 X2\u221a \u03b12 \u00b7 \u00b7 \u00b7 XK\u221a \u03b1K ] is a solution of our discrete problem, then there is an orthogonal matrix R such that its first column R1 is\nR1 = 1\u221a d  \u221a \u03b11\u221a \u03b12 ...\u221a \u03b1K  and\nXR = [ c\u221a d 1 Z2 \u00b7 \u00b7 \u00b7 ZK ] .\nFurthermore,\n(XR)>D(XR) = c2I\nand\ntr((XR)>L(XR)) = tr(Z>LZ),\nwith Z = [Z2 \u00b7 \u00b7 \u00b7 ZK ].\nProof. Apply Gram\u2013Schmidt to (R1, e2, . . . , eK) (where (e1, . . . , eK) is the canonical basis of RK) to form an orthonormal basis. The rest follows from Proposition 4.1.\nProposition 4.2 suggests that if Z = [1 Z2 \u00b7 \u00b7 \u00b7 ZK ] is a solution of the relaxed problem (\u22172), then there should be an orthogonal matrix R such that ZR> is an approximation of a solution of the discrete problem PNC1."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 73", "text": ""}, {"heading": "4.5 Finding a Discrete Solution Close to a Continuous", "text": "Approximation\nThe next step is to find an exact solution (P(X1), . . . ,P(XK)) \u2208 P(K) which is the closest (in a suitable sense) to our approximate solution (Z1, . . . , ZK) \u2208 G(K,N). The set K is closed under very special orthogonal transformations in O(K), so we can\u2019t view K as a subset of the Grassmannian G(K,N). However, we can think of K as a subset of G(K,N) by considering the subspace spanned by (X1, . . . , XK) for every [X1 \u00b7 \u00b7 \u00b7XK ] \u2208 K.\nRecall from Section 4.3 that every solution Z of problem (\u22172) yields a family of solutions of problem (\u22171); namely, all matrices of the form ZQ, where Q is a K \u00d7 K matrix with nonzero and pairwise orthogonal columns. Since the solutions ZQ of (\u22171) are all equivalent (they yield the same minimum for the normalized cut), it makes sense to look for a discrete solution X closest to one of these ZQ. Then, we have two choices of distances.\n1. We view K as a subset of (RPN\u22121)K . Because K is closed under the antipodal map, as explained in Appendix B, for every j (1 \u2264 j \u2264 K), minimizing the distance d(P(Xj),P(Zj)) on RPN\u22121 is equivalent to minimizing \u2016Xj \u2212 Zj\u20162, where Xj and Zj are representatives of P(Xj) and P(Zj) on the unit sphere (if we use the Riemannian metric on RPN\u22121 induced by the Euclidean metric on RN). Then, if we use the product distance on (RPN\u22121)K given by\nd ( (P(X1), . . . ,P(XK)), (P(Z1), . . . ,P(ZK)) ) = K\u2211 j=1 d(P(Xj),P(Zj)),\nminimizing the distance d ( (P(X1), . . . ,P(XK)), (P(Z1), . . . ,P(ZK)) ) in (RPN\u22121)K is equivalent to minimizing\nK\u2211 j=1 \u2225\u2225Xj \u2212 Zj\u2225\u2225 2 , subject to \u2225\u2225Xj\u2225\u2225 2 = \u2225\u2225Zj\u2225\u2225 2 (j = 1, . . . , K).\nWe are not aware of any optimization method to solve the above problem, which seems difficult to tackle due to constraints \u2016Xj\u20162 = \u2016Zj\u20162 (j = 1, . . . , K). Therefore, we drop these constraints and attempt to minimize\n\u2016X \u2212 Z\u20162F = K\u2211 j=1 \u2225\u2225Xj \u2212 Zj\u2225\u22252 2 ,\nthe Frobenius norm of X \u2212 Z. This is implicitly the choice made by Yu.\n2. We view K as a subset of the Grassmannian G(K,N). In this case, we need to pick a metric on the Grassmannian, and we minimize the corresponding Riemannian distance d(X,Z). A natural choice is the metric on so(n) given by\n\u3008X, Y \u3009 = tr(X>Y )."}, {"heading": "74 CHAPTER 4. GRAPH CLUSTERING", "text": "This choice remains to be explored.\nInspired by Yu [23] and the previous discussion, given a solution Z of problem (\u22172), we look for pairs (X,Q) with X \u2208 X and where Q is a K\u00d7K matrix with nonzero and pairwise orthogonal columns, with \u2016X\u2016F = \u2016Z\u2016F , that minimize\n\u03d5(X,Q) = \u2016X \u2212 ZQ\u2016F .\nHere, \u2016A\u2016F is the Frobenius norm of A, with \u2016A\u2016 2 F = tr(A >A). Yu [23] and Yu and Shi [24] consider the special case where Q \u2208 O(K). We consider the more general case where Q = R\u039b, with R \u2208 O(K) and \u039b is a diagonal invertible matrix.\nThe key to minimizing \u2016X \u2212 ZQ\u2016F rests on the following computation:\n\u2016X \u2212 ZQ\u20162F = tr((X \u2212 ZQ) >(X \u2212 ZQ))\n= tr((X> \u2212Q>Z>)(X \u2212 ZQ)) = tr(X>X \u2212X>ZQ\u2212Q>Z>X +Q>Z>ZQ) = tr(X>X)\u2212 tr(X>ZQ)\u2212 tr(Q>Z>X) + tr(Q>Z>ZQ) = tr(X>X)\u2212 tr((Q>Z>X)>)\u2212 tr(Q>Z>X) + tr(Z>ZQQ>) = \u2016X\u20162F \u2212 2tr(Q >Z>X) + tr(Z>ZQQ>).\nTherefore, since \u2016X\u2016F = \u2016Z\u2016F is fixed, minimizing \u2016X \u2212 ZQ\u2016 2 F is equivalent to minimizing \u22122tr(Q>Z>X) + tr(Z>ZQQ>).\nThis is a hard problem because it is a nonlinear optimization problem involving two matrix unknowns X and Q. To simplify the problem, we proceed by alternating steps during which we minimize \u03d5(X,Q) = \u2016X \u2212 ZQ\u2016F with respect to X holding Q fixed, and steps during which we minimize \u03d5(X,Q) = \u2016X \u2212 ZQ\u2016F with respect to Q holding X fixed.\nThis second step in which X is held fixed has been studied, but it is still a hard problem for which no closed\u2013form solution is known. Consequently, we further simplify the problem. Since Q is of the form Q = R\u039b where R \u2208 O(K) and \u039b is a diagonal invertible matrix, we minimize \u2016X \u2212 ZR\u039b\u2016F in two stages.\n1. We set \u039b = I and find R \u2208 O(K) that minimizes \u2016X \u2212 ZR\u2016F .\n2. Given X, Z, and R, find a diagonal invertible matrix \u039b that minimizes \u2016X \u2212 ZR\u039b\u2016F .\nThe matrix R\u039b is not a minimizer of \u2016X \u2212 ZR\u039b\u2016F in general, but it is an improvement on R alone, and both stages can be solved quite easily.\nIn stage 1, the matrix Q = R is orthogonal, so QQ> = I, and since Z and X are given, the problem reduces to minimizing \u22122tr(Q>Z>X); that is, maximizing tr(Q>Z>X). To solve this problem, we need the following proposition."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 75", "text": "Proposition 4.3. For any n\u00d7 n matrix A and any orthogonal matrix Q, we have\nmax{tr(QA) | Q \u2208 O(n)} = \u03c31 + \u00b7 \u00b7 \u00b7+ \u03c3n,\nwhere \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3n are the singular values of A. Furthermore, this maximum is achieved by Q = V U>, where A = U\u03a3V > is any SVD for A.\nProof. Let A = U\u03a3V > be any SVD for A. Then we have\ntr(QA) = tr(QU\u03a3V >)\n= tr(V >QU\u03a3).\nThe matrix Z = V >QU is an orthogonal matrix so |zij| \u2264 1 for 1 \u2264 i, j \u2264 n, and \u03a3 is a diagonal matrix, so we have\ntr(Z\u03a3) = z11\u03c31 + \u00b7 \u00b7 \u00b7+ znn\u03c3n \u2264 \u03c31 + \u00b7 \u00b7 \u00b7+ \u03c3n,\nwhich proves the first statement of the proposition. For Q = V U>, we get\ntr(QA) = tr(QU\u03a3V >)\n= tr(V U>U\u03a3V >) = tr(V \u03a3V >) = \u03c31 + \u00b7 \u00b7 \u00b7+ \u03c3n,\nwhich proves the second part of the proposition.\nAs a corollary of Proposition 4.3 (with A = Z>X and Q = R>), we get the following result (see Golub and Van Loan [11], Section 12.4.1):\nProposition 4.4. For any two fixed N \u00d7K matrices X and Z, the minimum of the set\n{\u2016X \u2212 ZR\u2016F | R \u2208 O(K)}\nis achieved by R = UV >, for any SVD decomposition U\u03a3V > = Z>X of Z>X.\nThe following proposition takes care of stage 2.\nProposition 4.5. For any two fixed N\u00d7K matrices X and Z, where Z has no zero column, there is a unique diagonal matrix \u039b = diag(\u03bb1, . . . , \u03bbK) minimizing \u2016X \u2212 Z\u039b\u2016F given by\n\u03bbj = (Z>X)jj\n\u2016Zj\u201622 j = 1, . . . , K."}, {"heading": "76 CHAPTER 4. GRAPH CLUSTERING", "text": "Proof. Since \u039b is a diagonal matrix, we have\n\u2016X \u2212 Z\u039b\u20162 = \u2016X\u201622 \u2212 2tr(\u039b >Z>X) + tr(Z>Z\u039b\u039b>)\n= \u2016X\u201622 \u2212 2tr(Z >X\u039b) + tr(Z>Z\u039b2)\n= \u2016X\u201622 \u2212 2 K\u2211 j=1 (Z>X)jj\u03bbj + K\u2211 j=1 \u2225\u2225Zj\u2225\u22252 2 \u03bb2j .\nThe above functional has a critical point obtained by setting the partial derivatives with respect to the \u03bbj to 0, which gives\n\u22122(Z>X)jj + 2 \u2225\u2225Zj\u2225\u22252\n2 \u03bbj = 0;\nthat is,\n\u03bbj = (Z>X)jj\n\u2016Zj\u201622 .\nSince the functional is a sum of quadratic functions and the coefficients \u2016Zj\u201622 of the \u03bb2j are positive, this critical point is indeed a minimum.\nIt should be noted that Proposition 4.5 does not guarantee that \u039b is invertible. For example, for\nX = 1 00 1 1 0  , Z = 1 11 0 1 \u22121  , we have\nZ>X = ( 1 1 1 1 0 \u22121 )1 00 1 1 0  = (2 1 0 0 ) ,\nso \u03bb2 = 0. When Proposition 4.5 yields a singular matrix, we skip stage 2 (we set \u039b = I).\nWe now deal with step 1, where Q = R\u039b is held fixed. For fixed Z and Q, we would like to find some X \u2208 K with \u2016X\u2016F = \u2016Z\u2016F so that \u2016X \u2212 ZQ\u2016F is minimal. Without loss of generality, we may assume that the entries a1, . . . , aK occurring in the matrix X are positive and all equal to some common value a 6= 0. Recall that a matrix X \u2208 X has the property that every row contains exactly one nonzero entry, and that every column is nonzero.\nTo find X \u2208 K, first we find the shape X\u0302 of X, which is the matrix obtained from X by rescaling the columns of X so that X\u0302 has entries +1, 0. The problem is to decide for each row, which column contains the nonzero entry. After having found X\u0302, we rescale its columns so that \u2016X\u2016F = \u2016Z\u2016F .\nSince\n\u2016X \u2212 ZQ\u20162F = \u2016X\u2016 2 F \u2212 2tr(Q >Z>X) + tr(Z>ZQQ>),"}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 77", "text": "minimizing \u2016X \u2212 ZQ\u2016F is equivalent to maximizing\ntr(Q>Z>X) = tr((ZQ)>X) = tr(X(ZQ)>),\nand since the ith row of X contains a single nonzero entry a in column ji (1 \u2264 ji \u2264 K), if we write Y = ZQ, then\ntr(XY >) = a N\u2211 i=1 yi ji . (\u2217)\nBy (\u2217), since a > 0, the quantity tr(XY >) is maximized iff yiji is maximized for i = 1, . . . , N ; this is achieved if for the ith row of X, we pick a column index ` such that yi` is maximum.\nTo find the shape X\u0302 of X, we first find a matrix X by chosing a single nonzero entry xij = 1 on row i in such a way that yij is maximum according to the following method. If we let\n\u00b5i = max 1\u2264j\u2264K yij Ji = {j \u2208 {1, . . . , K} | yij = \u00b5i},\nfor i = 1, . . . , N , then\nxij = { +1 for some chosen j \u2208 Ji, 0 otherwise.\nOf course, a single column index is chosen for each row. In our implementation, we pick the smallest index in Ji.\nUnfortunately, the matrixX may not be a correct solution, because the above prescription does not guarantee that every column of X is nonzero. When this happens, we reassign certain nonzero entries in columns having \u201cmany\u201d nonzero entries to zero columns, so that we get a matrix in K.\nSuppose column j is zero. Then, we pick the leftmost index k of a column with a maximum number of 1, and if i the smallest index for which X ik = 1, then we set X ik = 0 and X ij = 1. We repeat this reallocation scheme until every column is nonzero.\nWe obtain a new matrix X\u0302 in X , and finally we normalize X\u0302 to obtain X, so that \u2016X\u2016F = \u2016Z\u2016F .\nA practical way to deal with zero columns in X is to simply decrease K. Clearly, further work is needed to justify the soundness of such a method.\nThe above method is essentially the method described in Yu [23] and Yu and Shi [24], except that in these works (in which X,Z and Y are denoted by X\u2217, X\u0303\u2217, and X\u0303, respectively) the entries in X belong to {0, 1}; as described above, for row i, the index ` corresponding to the entry +1 is given by\narg max 1\u2264j\u2264K X\u0303(i, j)."}, {"heading": "78 CHAPTER 4. GRAPH CLUSTERING", "text": "The fact that X may have zero columns is not addressed by Yu. Furthermore, it is important to make sure that X has the same norm as Z, but this normalization step is not performed in the above works. On the other hand, the rows of Z are normalized and the resulting matrix may no longer be a correct solution of the relaxed problem. In practice, it appears to be a good approximation of a discrete solution; see option (3) of the initialization methods for Z described below.\nAny matrix obtained by flipping the signs of some of the columns of a solution ZR of problem (\u22172) is still a solution. Moreover, all entries in X are nonnegative. It follows that a \u201cgood\u201d solution ZQp (that is, close to a discrete solution) should have the property that the average of each of its column is nonnegative. We found that the following heuristic is quite helpful in finding a better discrete solution X. Given a solution ZR of problem (\u22172), we compute ZQp, defined such that if the average of column (ZR)j is negative, then (ZQp)\nj = \u2212(ZR)j, else (ZQp)j = (ZR)j. It follows that the average of every column in ZQp is nonnegative. Then, we apply the above procedure to find discrete solutions X and Xp closest to ZR and ZQp respectively, and we pick the solution corresponding to min{\u2016X \u2212 ZR\u2016F , \u2016Xp \u2212 ZQp\u2016F}. Flipping signs of columns of ZR correspond to a diagonal matrix Rp with entries \u00b11, a very special kind of orthogonal matrix. In summary, the procedure for finding a discrete X close to a continuous ZR also updates R to Qp = RRp. This step appears to be very effective for finding a good initial X.\nThe method due to Yu and Shi (see Yu [23] and Yu and Shi [24]) to find X \u2208 K and Q = R\u039b with R \u2208 O(K) and \u039b diagonal invertible that minimize \u03d5(X,Q) = \u2016X \u2212 ZQ\u2016F is to alternate steps during which either Q is held fixed (step PODX) or X is held fixed (step PODR), except that Yu and Shi consider the special case where \u039b = I.\n(1) In step PODX, the next discrete solution X\u2217 is obtained fom the previous pair (Q\u2217, Z)\nby computing X and then X\u2217 = X\u0302 from Y = ZQ\u2217, as just explained above.\n(2) In step PODR, the next matrix Q\u2217 = R\u039b is obtained from the previous pair (X\u2217, Z) by first computing\nR = UV >,\nfor any SVD decomposition U\u03a3V > of Z>X\u2217, and then computing \u039b from X\u2217 and ZR using Proposition 4.5. If \u039b is singular, then set \u039b = I.\nWe keep track of the progress of the procedure by computing \u03d5(X\u2217, Q\u2217) = \u2016X\u2217 \u2212 ZQ\u2217\u2016F after every step and checking that X\u2217 or \u03d5(X\u2217, Q\u2217) stops changing, whichever comes first. We observed that after a small number of steps, up to machine precision, \u03d5(X\u2217, Q\u2217) stops decreasing, and when this occurs the procedure halts (we also set a maximum number of steps in case \u03d5(X\u2217, Q\u2217) decreases for a very long time). Moreover, looking for Q = R\u039b where R \u2208 O(K) and \u039b is obtained using the method of Proposition 4.5 speeds up the convergence and yields a better discrete solution X.\nThe process of searching for X and Q has an illuminating geometric interpretation in terms of graph drawings. We may assume that the entries in the discrete solution X are 0 or"}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 79", "text": "1. Then the rows of the discrete solutions X correspond to the tips of the unit vectors along the coordinate axes in RK . Every axis contains at least such a point, and the multiplicity of the point along the jth axis is the number of nodes in the jth block of the partition. Similarly, the rows of Z are the nodes of a graph drawing of the weighted graph (V,W ). Multiplying Z on the right by a K \u00d7 K matrix Q (obtaining ZQ) is equivalent to multiplying Z> on the left by Q> (obtaining Q>Z>). This means that the points in RK representing the rows of ZQ are obtained by applying the linear transformation Q> to the columns of Z>. Thus, ZR amounts to applying the rigid motion R> to the graph drawing Z, and Z\u039b (where \u039b is a diagonal invertible matrix) amounts to stretching or shrinking the graph drawing Z in the directions of the axes.\nThen, in step 2 (PODR), we are trying to deform the graph drawing given by Z using a linear map (R\u039b)>, so that the deformed graph drawing ZR\u039b is as close as possible to X (in the sense that \u2016X \u2212 ZR\u039b\u2016F is minimized).\nIn step 1 (PODX), we are trying to approximate the deformed graph drawing ZR\u039b by a discrete graph drawingX (whose nodes are the tips of the unit vectors), so that \u2016X \u2212 ZR\u039b\u2016F is minimized.\nIf we are willing to give up the requirement that the deformed Z is still a solution of problem (\u22171), we have quite a bit of freedom in step 2. For example, we may allow normalizing the rows. This seems reasonable to obtain an initial transformation Q. However, we feel uncomfortable in allowing intermediate deformed Z that are not solutions of (\u22171) during the iteration process. This point should be investigated further.\nIn some sense, we have less freedom in step 1, since the ith row of ZR\u039b is assigned to the jth unit vector iff the index of the leftmost largest coordinate of this row is j. If some axis has not been assigned any row of R, then we reallocate one of the points on an axis with a maximum number of points.\nFigure 4.5 shows a graph (on the left) and the graph drawings X and Z \u2217 R obtained by applying our method for three clusters. The rows of X are represented by the red points along the axes, and the rows of Z \u2217R by the green points (on the right). The original vertices corresponding to the rows of Z are represented in blue. We can see how the two red points correspond to an edge, the three red points correspond to a triangle, and the four red points to a quadrangle. These constitute the clusters.\nIt remains to initialize Q\u2217 to start the process, and then steps (1) and (2) are iterated, starting with step (1). Actually, what we really need is a \u201cgood\u201d initial X\u2217, but to find it, we need an initial R\u2217.\nMethod 1. One method is to use an orthogonal matrix denoted R1, such that distinct columns of ZR1 are simultaneously orthogonal and D-orthogonal. The matrix R1 can be found by diagonalizing Z>Z as Z>Z = R1\u03a3R > 1 , as we explained at the end of Section 4.3. We write Z2 = ZR1.\nMethod 2. The method advocated by Yu [23] is to pick K rows of Z that are as orthogonal to each other as possible and to make a matrix R whose columns consist of these rows"}, {"heading": "80 CHAPTER 4. GRAPH CLUSTERING", "text": "Xc and Rc after step 1\nnormalized to have unit length. The intuition behind this method is that if a continuous solution Z can be sent close to a discrete solution X by a rigid motion, then many rows of Z viewed as vectors in RK should be nearly orthogonal. This way, ZR should contain at least K rows well aligned with the canonical basis vectors, and these rows are good candidates for some of the rows of the discrete solution X.\nThe algorithm given in Yu [23] needs a small correction, because rows are not removed from Z when they are added to R, which may cause the same row to be added several times to R.\nGiven the N \u00d7 K matrix Z (whose columns all have the same norm), we compute a matrix R whose columns are certain rows of Z. We use a vector c \u2208 RN to keep track of the inner products of all rows of Z with the columns R1, . . . , Rk\u22121 that have been constructed so far, and initially when k = 1, we set c = 0.\nThe first column R1 of R is any chosen row of Z.\nNext, for k = 2, . . . , K, we compute all the inner products of Rk\u22121 with all rows in Z, which are recorded in the vector ZRk\u22121, and we update c as follows:\nc = c+ abs(ZRk\u22121).\nWe take the absolute values of the entries in ZRk\u22121 so that the ith entry in c is a score of how orthogonal is the ith row of Z to R1, . . . , Rk\u22121. Then, we choose Rk as any row Zi of Z for which ci is minimal (the customary (and ambiguous) i = arg min c), and we delete this row from Z. The process is repeated (with the updated Z) until k = K."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 81", "text": "At the end of the above process, we normalize the columns of R, to obtain a matrix that we denote R2.\nAfter some experimentation, we found that to obtain a better initial X\u2217, it is may desirable to start from a variant of the continuous solution Z obtained by solving problem (\u22172). We have implemented three methods.\n1. We attempt to rescale the columns of Z by some diagonal invertible matrix \u039b = diag(\u03bb1, . . . , \u03bbK), so that the rows of Z\u039b sum to 1 as much as possible in the leastsquares sense. Since the vector of sums of rows of Z\u039b is Z\u039b1K = Z\u03bb, with \u03bb\n> = (\u03bb1, . . . , \u03bbK), the least-squares problem is to minimize\n\u2016Z\u03bb\u2212 1N\u201622 ,\nand since Z has rank K, the solution is \u03bb = (Z>Z)\u22121Z>1N , and thus,\n\u039b = diag((Z>Z)\u22121Z>1N).\nThe matrix \u039b is singular if some of the columns of Z sum to 0. This happens for regular graphs, where the degree matrix is a multiple of the identity. There are also cases where some of the \u03bbj are very small, so we use a tolerance factor to prevent this, and in case of failure, we set \u039b = I. In case of failure, we may also use ZR1 instead of Z, where R1 is the orthogonal matrix that makes ZR1 both D-orthogonal and orthogonal.\n2. We attempt to rescale the columns of Z by some diagonal invertible matrix \u039b = diag(\u03bb1, . . . , \u03bbK), so that the rows of Z\u039b have unit length as much as possible in the least-squares sense. Since the square-norm of the ith row of Z\u039b is\nK\u2211 j=1 z2ij\u03bb 2 j ,\nif we write Z \u25e6Z for the matrix (z2ij) of square entries of elements in Z (the Hadamard product of Z with itself), the least-squares problem is to mimimize\u2225\u2225Z \u25e6 Z\u03bb2 \u2212 1N\u2225\u222522 , where (\u03bb2)> = (\u03bb21, . . . , \u03bb 2 K). The matrix Z \u25e6 Z may not have rank K, so the leastsquares solution for \u03bb2 is given by the pseudo-inverse of Z \u25e6 Z, as\n\u03bb2 = (Z \u25e6 Z)+1N .\nThere is no guarantee that the vector on the right-hand side has all positive entries, so the method may fail. It may also fail when some of the \u03bbj are very small. We use a tolerance factor to prevent this, and in case of failure, we set \u039b = I."}, {"heading": "82 CHAPTER 4. GRAPH CLUSTERING", "text": "3. We use a method more drastic than (2), which consists in normalizing the rows of Z. Thus, we form the matrix\nNZ = diag((ZZ>) \u22121/2 11 , . . . , (ZZ >) \u22121/2 NN ),\nand we return NZ \u2217 Z. Unlike the methods used in (1) and (2), this method does not guarantee that NZ \u2217 Z is a solution of problem (\u22171). However, since the rows of Z can be interpreted as vectors in RK that should align as much as possible with the canonical basis vectors of RK , this method makes sense as a way to come closer to a discrete solution. In fact, we found that it does well in most cases.\nWe implemented a computer program that prompts the user for various options. To avoid confusion, let us denote the original solution of problem (\u22172) by Z1, and let Z2 = Z1R1, as obtained by initialization method 1. The four options are:\n1. Use the original solution Z1 of problem (\u22172), as well as Z2.\n2. Apply method 1 to Z1 and Z2.\n3. Apply method 2 to Z1 and Z2.\n4. Apply method 3 to Z1 and Z2.\nThen, for each of these options, if we denote by Zinit1 and Zinit2 the solutions returned by the method, our program computes initial solutions X1, X2, X3, X4 as follows:\n1. Use Zinit1 and R = I.\n2. Use Zinit1 and R = R2a, the matrix given by initialization method 2.\n3. Use Zinit2 and R = I.\n4. Use Zinit2 and R = R2b, the matrix given by initialization method 2.\nAfter this, the program picks the discrete solutionX = Xi which corresponds to the minimum of\n\u2016X1\u2212 Zinit1\u2016 , \u2016X2\u2212 Zinit1 \u2217R2a\u2016 , \u2016X3\u2212 Zinit2\u2016 , \u2016X4\u2212 Zinit2 \u2217R2b\u2016 .\nOur experience is that options (3) and (4) tend to give better results. However, it is harder to say whether any of the Xi does a better job than the others, although (2) and (4) seem to do slightly better than (1) and (3). We also give the user the option in step PODR to only compute R and set \u039b = I. It appears that the more general method is hardly more expansive (because finding \u039b is cheap) and always gives better results.\nWe also found that we obtain better results if we rescale Z (and X) so that\u2016Z\u2016F = 100."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 83", "text": "If we apply the method (using method 3 to find the initial R) to the graph associated with the the matrix W1 shown in Figure 4.6 for K = 4 clusters, the algorithm converges in 3 steps and we find the clusters shown in Figure 4.7.\nThe solution Z of the relaxed problem is\nZ =  \u221221.3146 \u22120.0000 19.4684 \u221215.4303 \u22124.1289 0.0000 16.7503 \u221215.4303 \u221221.3146 32.7327 \u221219.4684 \u221215.4303 \u22124.1289 \u22120.0000 16.7503 \u221215.4303 19.7150 0.0000 9.3547 \u221215.4303 \u22124.1289 23.1455 \u221216.7503 \u221215.4303 \u221221.3146 \u221232.7327 \u221219.4684 \u221215.4303 \u22124.1289 \u221223.1455 \u221216.7503 \u221215.4303 19.7150 \u22120.0000 \u22129.3547 \u221215.4303  .\nWe find the following sequence for Q,Z \u2217Q,X:\nQ =  0 0.6109 \u22120.3446 \u22120.7128\n\u22121.0000 0.0000 0.0000 \u22120.0000 0.0000 0.5724 0.8142 0.0969 \u22120.0000 0.5470 \u22120.4672 0.6947\n ,"}, {"heading": "84 CHAPTER 4. GRAPH CLUSTERING", "text": "which is the initial Q obtained by method 1;\nZ \u2217Q =  0.0000 \u221210.3162 30.4065 6.3600 0.0000 \u22121.3742 22.2703 \u22126.1531 \u221232.7327 \u221232.6044 \u22121.2967 2.5884\n0.0000 \u22121.3742 22.2703 \u22126.1531 0.0000 8.9576 8.0309 \u221223.8653 \u221223.1455 \u221220.5505 \u22125.0065 \u22129.3982 32.7327 \u221232.6044 \u22121.2967 2.5884 23.1455 \u221220.5505 \u22125.0065 \u22129.3982 \u22120.0000 \u22121.7520 \u22127.2027 \u221225.6776\n X =  0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0  ;\nQ =  \u22120.0803 0.8633 \u22120.4518 \u22120.2102 \u22120.6485 0.1929 0.1482 0.7213 \u22120.5424 0.0876 0.5546 \u22120.6250 \u22120.5281 \u22120.4581 \u22120.6829 \u22120.2119 \nZ \u2217Q =  \u22120.6994 \u22129.6267 30.9638 \u22124.4169 \u22120.6051 4.9713 21.6922 \u22126.3311 \u22120.8081 \u22126.7218 14.2223 43.5287 \u22120.6051 4.9713 21.6922 \u22126.3311 1.4913 24.9075 6.8186 \u22126.7218 2.5548 6.5028 6.5445 31.3015 41.6456 \u221219.3507 4.5190 \u22123.6915 32.5742 \u22122.4272 \u22120.3168 \u22122.0882 11.6387 23.2692 \u22123.5570 4.9716  X =  0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0  ;\nQ =  \u22120.3201 0.7992 \u22120.3953 \u22120.3201 \u22120.7071 \u22120.0000 0.0000 0.7071 \u22120.4914 \u22120.0385 0.7181 \u22120.4914 \u22120.3951 \u22120.5998 \u22120.5728 \u22120.3951 \nZ \u2217Q =  3.3532 \u22128.5296 31.2440 3.3532 \u22120.8129 5.3103 22.4987 \u22120.8129 \u22120.6599 \u22127.0310 3.2844 45.6311 \u22120.8129 5.3103 22.4987 \u22120.8129 \u22124.8123 24.6517 7.7629 \u22124.8123 \u22120.7181 6.5997 \u22121.5571 32.0146 45.6311 \u22127.0310 3.2844 \u22120.6599 32.0146 6.5997 \u22121.5571 \u22120.7181 4.3810 25.3718 \u22125.6719 4.3810  X =  0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0  .\nDuring the next round, the exact same matrices are obtained and the algorithm stops. Comparison of the matrices Z \u2217 Q and X makes it clear that X is obtained from Z \u2217 Q by"}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 85", "text": "retaining on every row the leftmost largest value and setting the others to 0 (non-maximum supression).\nIn this example, the columns of all X were nonzero, but this may happen, for example when we apply the algorithm to the graph of Figure 4.6 to find K = 5 clusters shown in Figure 4.8.\nWe find that the initial value for Z \u2217Q is\nZ \u2217Q =  \u22125.7716 \u221227.5934 0.0000 \u22129.3618 \u22120.0000 5.5839 \u221220.2099 \u221229.7044 \u22121.2471 \u22120.0000 \u22122.3489 1.1767 \u22120.0000 \u221229.5880 \u221229.7044 5.5839 \u221220.2099 29.7044 \u22121.2471 0.0000 21.6574 \u22127.2879 0.0000 8.1289 0.0000 8.5287 4.5433 \u22120.0000 \u221218.6493 \u221221.0042 \u22122.3489 1.1767 \u22120.0000 \u221229.5880 29.7044 8.5287 4.5433 \u22120.0000 \u221218.6493 21.0042 23.3020 6.5363 \u22120.0000 \u22121.5900 \u22120.0000  .\nThe matrix X1 given by the above method in which we pick the leftmost largest entry on every row has a fourth row equal to 0. The matrix X1 is repaired by migrating a 1 from the second entry of the first column, which contains the largest number of 1\u2019s, yielding the matrix X2; see below.\nX1 =  0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0  X2 =  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 \n86 CHAPTER 4. GRAPH CLUSTERING\nChapter 5\nSigned Graphs"}, {"heading": "5.1 Signed Graphs and Signed Laplacians", "text": "Intuitively, in a weighted graph, an edge with a positive weight denotes similarity or proximity of its endpoints. For many reasons, it is desirable to allow edges labeled with negative weights, the intuition being that a negative weight indicates dissimilarity or distance.\nWeighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs . Such graphs (with weights (\u22121, 0,+1)) were introduced as early as 1953 by Harary [12], to model social relations involving disliking, indifference, and liking. The problem of clustering the nodes of a signed graph arises naturally as a generalization of the clustering problem for weighted graphs. From our perspective, we would like to know whether clustering using normalized cuts can be extended to signed graphs.\nGiven a signed graph G = (V,W ) (where W is a symmetric matrix with zero diagonal entries), the underlying graph of G is the graph with node set V and set of (undirected) edges E = {{vi, vj} | wij 6= 0}.\nThe first obstacle is that the degree matrix may now contain zero or negative entries. As a consequence, the Laplacian L may no longer be positive semidefinite, and worse, D\u22121/2 may not exist.\nA simple remedy is to use the absolute values of the weights in the degree matrix! This idea applied to signed graph with weights (\u22121, 0, 1) occurs in Hou [14]. Kolluri, Shewchuk and O\u2019Brien [15] take the natural step of using absolute values of weights in the degree matrix in their original work on surface reconstruction from noisy point clouds. Given a Delaunay tetrahedralization, they build a graph with positive and negative edges and use the normalized cut method for two clusters to decide which tetrahedra are inside or outside the original object. The triangulated surface (called the eigencrust) consists of the triangles where an inside and an outside tetrahedron meet. The authors state that the Lapacians arising from such graphs are always positive definite, which is not quite correct since this is only true for unbalanced graphs (see Section 5.3). Kunegis et al. [16] appear to be the\n87"}, {"heading": "88 CHAPTER 5. SIGNED GRAPHS", "text": "first to make a systematic study of spectral methods applied to signed graphs. In fact, many results in this section originate from Kunegis et al. [16]. However, it should be noted that only 2-clustering is considered in the above papers.\nAs we will see, the trick of using absolute values of weights in the degree matrix allows the whole machinery that we have presented to be used to attack the problem of clustering signed graphs using normalized cuts. This requires a modification of the notion of normalized cut. This new notion it is quite reasonable, as we will see shortly.\nIf (V,W ) is a signed graph, where W is an m\u00d7m symmetric matrix with zero diagonal entries and with the other entries wij \u2208 R arbitrary, for any node vi \u2208 V , the signed degree of vi is defined as\ndi = d(vi) = m\u2211 j=1 |wij|,\nand the signed degree matrix D as\nD = diag(d(v1), . . . , d(vm)).\nFor any subset A of the set of nodes V , let\nvol(A) = \u2211 vi\u2208A di = \u2211 vi\u2208A m\u2211 j=1 |wij|.\nFor any two subsets A and B of V , define links+(A,B), links\u2212(A,B), and cut(A,A) by\nlinks+(A,B) = \u2211\nvi\u2208A,vj\u2208B wij>0\nwij\nlinks\u2212(A,B) = \u2211\nvi\u2208A,vj\u2208B wij<0\n\u2212wij\ncut(A,A) = \u2211\nvi\u2208A,vj\u2208A wij 6=0\n|wij|.\nNote that links+(A,B) = links+(B,A), links\u2212(A,B) = links\u2212(B,A), and\ncut(A,A) = links+(A,A) + links\u2212(A,A).\nThen, the signed Laplacian L is defined by\nL = D \u2212W,\nand its normalized version Lsym by\nLsym = D \u22121/2 LD \u22121/2 = I \u2212D\u22121/2WD\u22121/2."}, {"heading": "5.1. SIGNED GRAPHS AND SIGNED LAPLACIANS 89", "text": "For a graph without isolated vertices, we have d(vi) > 0 for i = 1, . . . ,m, so D \u22121/2\nis well defined.\nThe signed Laplacian is symmetric positive semidefinite. As for the Laplacian of a weight matrix (with nonnegative entries), this can be shown in two ways. The first method consists in defining a notion of incidence matrix for a signed graph, and appears in Hou [14].\nDefinition 5.1. Given a signed graph G = (V,W ), with V = {v1, . . . , vm}, if {e1, . . . , en} are the edges of the underlying graph of G (recall that {vi, vj} is an edge of this graph iff wij 6= 0), for any oriented graph G\u03c3 obtained by giving an orientation to the underlying graph of G, the incidence matrix B\u03c3 of G\u03c3 is the m\u00d7 n matrix whose entries bi j are given by\nbi j =  + \u221a wij if wij > 0 and s(ej) = vi \u2212\u221awij if wij > 0 and t(ej) = vi\u221a\u2212wij if wij < 0 and (s(ej) = vi or t(ej) = vi) 0 otherwise.\nThen, we have the following proposition whose proof is easily adapted from the proof of Proposition 2.2.\nProposition 5.1. Given any signed graph G = (V,W ) with V = {v1, . . . , vm}, if B\u03c3 is the incidence matrix of any oriented graph G\u03c3 obtained from the underlying graph of G and D is the signed degree matrix of W , then\nB\u03c3(B\u03c3)> = D \u2212W = L.\nConsequently, B\u03c3(B\u03c3)> is independent of the orientation of the underlying graph of G and"}, {"heading": "L = D \u2212W is symmetric and positive semidefinite; that is, the eigenvalues of L = D \u2212W are real and nonnegative.", "text": "Another way to prove that L is positive semidefinite is to evaluate the quadratic form x>Lx. We will need this computation to figure out what is the new notion of normalized cut. For any real \u03bb \u2208 R, define sgn(\u03bb) by\nsgn(\u03bb) =  +1 if \u03bb > 0\n\u22121 if \u03bb < 0 0 if \u03bb = 0.\nProposition 5.2. For any m\u00d7m symmetric matrix W = (wij), if we let L = D\u2212W where D is the signed degree matrix associated with W , then we have\nx>Lx = 1\n2 m\u2211 i,j=1 |wij|(xi \u2212 sgn(wij)xj)2 for all x \u2208 Rm.\nConsequently, L is positive semidefinite."}, {"heading": "90 CHAPTER 5. SIGNED GRAPHS", "text": "Proof. We have\nx>Lx = x>Dx\u2212 x>Wx\n= m\u2211 i=1 dix 2 i \u2212 m\u2211 i,j=1 wijxixj\n= m\u2211\ni,j=1\n(|wij|x2i \u2212 wijxixj)\n= m\u2211\ni,j=1\n(|wij|(x2i \u2212 sgn(wij)xixj)\n= 1\n2\n( m\u2211\ni,j=1\n|wij|(x2i \u2212 2sgn(wij)xixj + x2j)\n)\n= 1\n2 m\u2211 i,j=1 |wij|(xi \u2212 sgn(wij)xj)2,\nand this quantity is nonnegative."}, {"heading": "5.2 Signed Normalized Cuts", "text": "As in Section 4.3, given a partition of V into K clusters (A1, . . . , AK), if we represent the jth block of this partition by a vector Xj such that\nXji = { aj if vi \u2208 Aj 0 if vi /\u2208 Aj,\nfor some aj 6= 0, then we have the following result.\nProposition 5.3. For any vector Xj representing the jth block of a partition (A1, . . . , AK) of V , we have\n(Xj)>LXj = a2j(cut(Aj, Aj) + 2links \u2212(Aj, Aj)).\nProof. Using Proposition 5.2, we have\n(Xj)>LXj = 1\n2 m\u2211 i,k=1 |wik|(Xji \u2212 sgn(wik)X j k) 2.\nThe sum on the righthand side splits into four parts:\n(1) S1 = 1 2 \u2211 i,k\u2208Aj |wik|(X j i \u2212 sgn(wik)X j k) 2. In this case, Xji = X j k = aj, so only negative\nedges have a nonzero contribution, and we have\nS1 = 1\n2 \u2211 i,k\u2208Aj ,wik<0 |wik|(aj + aj)2 = 2a2j links\u2212(Aj, Aj).\n5.2. SIGNED NORMALIZED CUTS 91\n(2) S2 = 1 2 \u2211 i\u2208Aj ,k\u2208Aj |wik|(X j i \u2212 sgn(wik)X j k) 2. In this case, Xji = aj and X j k = 0, so\nS2 = 1\n2 a2j \u2211 i\u2208Aj ,k\u2208Aj |wik| = 1 2 a2jcut(Aj, Aj).\n(3) S3 = 1 2 \u2211 i\u2208Aj ,k\u2208Aj |wik|(X j i \u2212 sgn(wik)X j k) 2. In this case, Xji = 0 and X j k = aj, so\nS3 = 1\n2 a2j \u2211 i\u2208Aj ,k\u2208Aj |wik| = 1 2 a2jcut(Aj, Aj) = 1 2 a2jcut(Aj, Aj).\n(4)\nS4 = 1 2 \u2211 i,k\u2208Aj |wik|(X j i \u2212 sgn(wik)X j k) 2. In this case, Xji = X j k = 0, so\nS4 = 0.\nIn summary,\n(Xj)>LXj = S1 + S2 + S3 + S4 = 2a 2 j links \u2212(Aj, Aj) + a 2 jcut(Aj, Aj),\nas claimed.\nSince with the revised definition of vol(Aj), we also have (Xj)>DXj = a2j \u2211 vi\u2208Aj di = a 2 jvol(Aj),\nwe deduce that (Xj)>LXj\n(Xj)>DXj =\ncut(Aj, Aj) + 2links \u2212(Aj, Aj)\nvol(Aj) .\nThe calculations of the previous paragraph suggest the following definition.\nDefinition 5.2. The signed normalized cut sNcut(A1, . . . , AK) of the partition (A1, . . . , AK) is defined as\nsNcut(A1, . . . , AK) = K\u2211 j=1 cut(Aj, Aj) vol(Aj) + 2 K\u2211 j=1 links\u2212(Aj, Aj) vol(Aj) .\nRemark: Kunegis et al. [16] deal with a different notion of cut, namely ratio cut (in which vol(A) is replaced by the size |A| of A), and only for two clusters. In this case, by a clever choice of indicator vector, they obtain a notion of signed cut that only takes into account the positive edges between A and A, and the negative edges among nodes in A and nodes in A. This trick does not seem to generalize to more than two clusters, and this is why we"}, {"heading": "92 CHAPTER 5. SIGNED GRAPHS", "text": "use our representation for partitions. Our definition of a signed normalized cut appears to be novel.\nBased on previous computations, we have\nsNcut(A1, . . . , AK) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj .\nwhere X is the N \u00d7K matrix whose jth column is Xj. Therefore, this is the same problem as in Chapter 4, with L replaced by L and D replaced by D.\nObserve that minimizing sNcut(A1, . . . , AK) amounts to minimizing the number of positive and negative edges between clusters, and also minimizing the number of negative edges within clusters. This second minimization captures the intuition that nodes connected by a negative edge should not be together (they do not \u201clike\u201d each other; they should be far from each other).\nThe K-clustering problem for signed graphs is related but not equivalent to another problem known as correlation clustering . In correlation clustering, in our terminology and notation, given a graph G = (V,W ) with positively and negatively weighted edges, one seeks a clustering of V that minimizes the sum links\u2212(Aj, Aj) of the absolute values of the negative weights of the edges within each cluster Aj, and minimizes the sum links\n+(Aj, Aj) of the positive weights of the edges between distinct clusters. In contrast to K-clustering, the number K of clusters is not given in advance, and there is no normalization with respect to size of volume. Furthermore, in correlation clustering, only the contribution links+(Aj, Aj) of positively weighted edges is minimized, but our method only allows us to minimize cut(Aj, Aj), which also takes into account negatively weighted edges between distinct clusters. Correlation clustering was first introduced and studied for complete graphs by Bansal, Blum and Chawla [1]. They prove that this problem is NP-complete and give several approximation algorithms, including a PTAS for maximizing agreement. Demaine and Immorlica [5] consider the same problem for arbitrary weighted graphs, and they give an O(log n)-approximation algorithm based on linear programming. Since correlation clustering does not assume that K is given and not not include nomalization by size or volume, it is not clear whether algorithms for correlation clustering can be applied to normalized K-clustering, and conversely."}, {"heading": "5.3 Balanced Graphs", "text": "Since\nsNcut(A1, . . . , AK) = K\u2211 j=1 (Xj)>LXj (Xj)>DXj ,\nthe whole machinery of Sections 4.3 and 4.5 can be applied with D replaced by D and L replaced by L. However, there is a new phenomenon, which is that L may be positive"}, {"heading": "5.3. BALANCED GRAPHS 93", "text": "definite. As a consequence, 1 is not always an eigenvector of L. As observed by Kunegis et al. [16], it is also possible to characterize for which signed graphs the Laplacian L is positive definite. Such graphs are \u201ccousins\u201d of bipartite graphs and were introduced by Harary [12]. Since a graph is the union of its connected components, we restrict ourselves to connected graphs.\nDefinition 5.3. Given a signed graph G = (V,W ) with negative weights whose underlying graph is connected, we say that G is balanced if there is a partition of its set of nodes V into two blocks V1 and V2 such that all positive edges connect nodes within V1 or V2, and negative edges connect nodes between V1 and V2.\nAn example of a balanced graph is shown in Figure 5.1 on the left, in which positive edges are colored green and negative edges are colored red. This graph admits the partition\n({v1, v2, v4, v7, v8}, {v3, v5, v6, v9}).\nOn the other hand, the graph shown on the right contains the cycle (v2, v3, v6, v5, v4, v2) with an odd number of negative edges (3), and thus is not balanced.\nObserve that if we delete all positive edges in a balanced graph, then the resulting graph is bipartite. Then, it is not surprising that connected balanced graphs can be characterized as signed graphs in which every cycle has an even number of negative edges. This is analogous to the characterization of a connected bipartite graph as a graph in which every cycle has even length. The following proposition was first proved by Harary [12]. We give a more direct proof.\nProposition 5.4. If G = (V,W ) is a connected signed graph with negative weights, then G is balanced iff every cycle contains an even number of negative edges.\nProof. If G is balanced, then every cycle must switch from a node in V1 to a node in V2 (or from a node in V2 to a node in V1) an even number of times. Therefore, it contains an even number of negative edges."}, {"heading": "94 CHAPTER 5. SIGNED GRAPHS", "text": "Conversely, assume that G contains no cycle with an odd number of negative edges. Since G is connected, pick some some v0 in V , and let V1 be the set of node reachable from v0 by a path with an odd number of negative edges, and let V2 be the set of node reachable from v0 by a path with an even number of negative edges. Clearly, (V1, V2) is a partition of V . Assume that there is a negative edge {u, v} between two nodes within V1 (or V2). Then, using the paths from v0 to u and v, where the parity of the number of negative edges is the same, we would obtain a cycle with an odd number of negative edges, a contradiction. Therefore, edges between nodes in V1 (or V2) are positive, and negative edges connect nodes in V1 and V2.\nWe can also detect whether a connected signed graph is balanced in terms of the kernel of the transpose of any of its incidence matrices.\nProposition 5.5. If G = (V,W ) is a connected signed graph with negative weights and with m nodes, for any orientation of its underlying graph, let B be the corresponding incidence matrix. The underlying graph of G is balanced iff rank(B) = m \u2212 1. Furthermore, if G is balanced, then there is a vector u with ui \u2208 {\u22121, 1} such that B>u = 0, and the sets of nodes V1 = {vi | ui = \u22121} and V2 = {vi | ui = +1} form a partition of V for which G is balanced.\nProof. Assume that rank(B) = m\u2212 1; this implies that Ker (B>) 6= (0). For any u 6= 0, we have B>u = 0 iff u>B = 0 iff u is orthogonal to every column of B. By definition of B, we have\nui = sgn(wij)uj\niff there is an edge between vi and vj.\nPick node v1 in V and define V1 and V2 as in the proof of Proposition 5.4. The above equation shows that u has the same value on nodes connected by a path with an even number of negative edges, and opposite values on nodes connected by a path with an odd number of negative edges. Since V1 consists of all nodes connected to v1 by a path with an odd number of negative edges and V2 consists of all nodes connected to v1 by a path with an even number of negative edges, it follows that u has the same value c = u1 on all nodes in V1, and the value \u2212c on all nodes in V2. Then, there is no negative edge between any two nodes in V1 (or V2), since otherwise u would take opposite values on theses two nodes, contrary to the fact that u has a constant value on V1 (and V2). This implies that (V1, V2) is a partition of V making G a balanced graph.\nConversely, if G is balanced, then there is a partition (V1, V2) of V such that positive edges connect nodes within V1 or V2, and negative edges connect nodes between V1 and V2. Then, if u is the vector with ui \u2208 {\u22121, 1} defined so that ui = +1 iff vi \u2208 V1 and ui = \u22121 iff vi \u2208 V2, we have\nui = sgn(wij)uj,\nand so B>u = 0, which shows that u \u2208 Ker (B>). Furthermore, the argument in the first part of the proof shows that every vector in Ker (B>) must have the same value c on all"}, {"heading": "5.3. BALANCED GRAPHS 95", "text": "nodes in V1, and the value \u2212c on all nodes in V2, so it must be a multiple of the vector u given by ui = +1 iff vi \u2208 V1 and ui = \u22121 iff vi \u2208 V2. Therefore, dim(Ker (B>)) = 1, and rank = m\u2212 1. The third part of the proposition has already been shown.\nRemark: A simple modification of the proof of Proposition 5.5 shows that if there are c1 components containing only positive edges, c2 components that are balanced graphs, and c3 components that are not balanced (and contain some negative edge), then\nc1 + c2 = m\u2212 rank(B).\nSince by Proposition 5.1 we have L = BB> for any incidence matrix B associated with an orientation of the underlying graph of G, we obtain the following important result (which is proved differently in Kunegis et al. [16]).\nTheorem 5.6. The signed Laplacian L of a connected signed graph G is positive definite iff G is not balanced (possesses some cycle with an odd number of negative edges).\nIf G = (V,W ) is a balanced graph, then there is a partition (V1, V2) of V such that for every edge {vi, vj}, if wij > 0, then vi, vj \u2208 V1 or vi, vj \u2208 V2, and if wij < 0, then vi \u2208 V1 and vj \u2208 V2. It follows that if we define the vector x such that xi = +1 iff vi \u2208 V1 and xi = \u22121 iff vi \u2208 V2, then for every edge {vi, vj} we have\nsgn(wij) = xixj.\nWe call x a bipartition of V .\nThe signed Laplacian of the balanced graph G1 is given by\nL1 =  2 \u22121 0 \u22121 0 0 0 0 0 \u22121 5 1 \u22121 1 0 0 \u22121 0 0 1 3 0 \u22121 \u22121 0 0 0 \u22121 \u22121 0 5 1 0 \u22121 \u22121 0 0 1 \u22121 1 6 \u22121 0 1 \u22121 0 0 \u22121 0 \u22121 4 0 1 \u22121 0 0 0 \u22121 0 0 2 \u22121 0 0 \u22121 0 \u22121 1 1 \u22121 6 1 0 0 0 0 \u22121 \u22121 0 1 3  Using Matlab, we find that its eigenvalues are\n0, 1.4790, 1.7513, 2.7883, 4.3570, 4.8815, 6.2158, 7.2159, 7.3112.\nThe eigenvector corresponding to the eigenvalue 0 is\n(0.3333, 0.3333, \u22120.3333, 0.3333, \u22120.3333, \u22120.3333, 0.3333, 0.3333, \u22120.3333)"}, {"heading": "96 CHAPTER 5. SIGNED GRAPHS", "text": "It gives us the bipartition\n({v1, v2, v4, v7, v8}, {v3, v5, v6, v9}),\nas guaranteed by Proposition 5.5.\nThe signed Laplacian of the unbalanced graph G2 is given by\nL2 =  2 \u22121 0 \u22121 0 0 0 0 0 \u22121 5 1 1 \u22121 0 0 \u22121 0 0 1 3 0 \u22121 \u22121 0 0 0 \u22121 1 0 5 1 0 \u22121 \u22121 0 0 \u22121 \u22121 1 6 \u22121 0 1 \u22121 0 0 \u22121 0 \u22121 4 0 1 \u22121 0 0 0 \u22121 0 0 2 \u22121 0 0 \u22121 0 \u22121 1 1 \u22121 6 1 0 0 0 0 \u22121 \u22121 0 1 3  The eigenvalues of L2 are\n0.5175, 1.5016, 1.7029, 2.7058, 3.7284, 4.9604, 5.6026, 7.0888, 8.1921.\nThe matrix L2 is indeed positive definite (since G2 is unbalanced). Hou [14] gives bounds on the smallest eigenvalue of an unbalanced graph. The lower bound involves a measure of how unbalanced the graph is (see Theorem 3.4 in Hou [14]).\nFollowing Kunegis et al., we can prove the following result showing that the eigenvalues and the eigenvectors of L and its unsigned counterpart L are strongly related. Given a symmetric signed matrix W , we define the unsigned matrix W such that Wij = |wij| (1 \u2264 i, j \u2264 m). We let L be the Laplacian associated with W . Note that\nL = D \u2212W .\nThe following proposition is shown in Kunegis et al. [16]).\nProposition 5.7. Let G = (V,W ) be a signed graph and let W be the unsigned matrix associated with W . If G is balanced, and x is a bipartition of V , then for any diagonalization L = P\u039bP> of L, where P is an orthogonal matrix of eigenvectors of L, if we define the matrix P so that\nPi = xiPi,\nwhere Pi is the ith row of P and Pi is the ith row of P , then P is orthogonal and\nL = P\u039bP>\nis a diagonalization of L. In particular, L and L have the same eigenvalues with the same multiplicities.\n5.4. K-WAY CLUSTERING OF SIGNED GRAPHS 97\nProof. Observe that if we let X = diag(x1, . . . , xm),\nthen P = XP.\nIt follows that P\u039bP> = XP\u039bP>X> = XLX> = XLX,\nsince X is a diagonal matrix. As a consequence, for diagonal entries, we have\nx2iLii = Dii = Lii,\nand for i 6= j, we have\nxixjLij = sgn(wij)Lij = \u2212sgn(wij)wij = \u2212|wij| = \u2212Wij = Lij,\nwhich proves that L = P\u039bP>. It remains to prove that P is orthogonal. Since X is a diagonal matrix whose entries are \u00b11, we have X>X = I, so\nP>P = (XP )>XP = P>X>XP = P>IP = I,\nsince P is orthogonal. Thus, P is indeed orthogonal.\n5.4 K-Way Clustering of Signed Graphs\nUsing the signed Laplacians L and Lsym, we can define the optimization problems as in Section 4.3 and solve them as in Section 4.5, except that we drop the constraint\nX(X>X)\u22121X>1 = 1,\nsince 1 is not necessarily an eigenvector of L. By Proposition A.3, the sum of the K smallest eigenvalues of Lsym is a lower bound for tr(Y\n>LsymY ), and the minimum of problem (\u2217\u22172) is achieved by any K unit eigenvectors (u1, . . . , uk) associated with the smallest eigenvalues\n0 \u2264 \u03bd1 \u2264 \u03bd2 \u2264 . . . \u2264 \u03bdK of Lsym. The difference with unsigned graphs is that \u03bd1 may be strictly positive. Here is the result of applying this method to various examples.\nFirst, we apply our algorithm to find three clusters for the balanced graph G1. The graph G1 as outputted by the algorithm is shown in Figure 5.2 and the three clusters are shown in Figure 5.3. As desired, these clusters do not contain negative edges.\nBy the way, for two clusters, the algorithm finds the bipartition of G1, as desired.\nNext, we apply our algorithm to find three clusters for the unbalanced graph G2. The graph G2 as outputted by the algorithm is shown in Figure 5.2 and the three clusters are shown in Figure 5.3. As desired, these clusters do not contain negative edges.\nThe algorithm finds the same clusters, but this is probably due to the fact that G1 and G2 only differ by the signs of two edges."}, {"heading": "98 CHAPTER 5. SIGNED GRAPHS", "text": ""}, {"heading": "5.5. SIGNED GRAPH DRAWING 99", "text": ""}, {"heading": "5.5 Signed Graph Drawing", "text": "Following Kunegis et al. [16], if our goal is to draw a signed graph G = (V,W ) with m nodes, a natural way to interpret negative weights is to assume that the endpoints vi and vj of an edge with a negative weight should be placed far apart, which can be achieved if instead of assigning the point \u03c1(vj) \u2208 Rn to vj, we assign the point \u2212\u03c1(vj). Then, if R is the m \u00d7 n matrix of a graph drawing of G in Rn, the energy function E(R) is redefined to be\nE(R) = \u2211\n{vi,vj}\u2208E\n|wij| \u2016\u03c1(vi)\u2212 sgn(wij)\u03c1(vj)\u20162 .\nWe obtain the following version of Proposition 3.1.\nProposition 5.8. Let G = (V,W ) be a signed graph, with |V | = m and with W a m \u00d7m symmetric matrix, and let R be the matrix of a graph drawing \u03c1 of G in Rn (a m\u00d7n matrix). Then, we have\nE(R) = tr(R>LR).\nProof. Since \u03c1(vi) is the ith row of R (and \u03c1(vj) is the jth row of R), if we denote the kth column of R by Rk, using Proposition 5.2, we have\nE(R) = \u2211\n{vi,vj}\u2208E\n|wij| \u2016\u03c1(vi)\u2212 sgn(wij)\u03c1(vj)\u20162\n= n\u2211 k=1 \u2211 {vi,vj}\u2208E |wij|(Rik \u2212 sgn(wij)Rjk)2\n= n\u2211 k=1 1 2 m\u2211 i,j=1 |wij|(Rik \u2212 sgn(wij)Rjk)2\n= n\u2211 k=1 (Rk)>LRk = tr(R>LR),\nas claimed.\nThen, as in Chapter 3, we look for a graph drawing R that minimizes E(R) = tr(R>LR) subject to R>R = I. The new ingredient is that L is positive definite iff G is not a balanced graph. Also, in the case of a signed graph, 1 does not belong to the kernel of L, so we do not get a balanced graph drawing.\nIf G is a signed balanced graph, then KerL is nontrivial, and if G is connected, then KerL is spanned by a vector whose components are either +1 or \u22121. Thus, if we use the first n unit eigenvectors (u1, u2, . . . , un) associated with the n smallest eigenvalues 0 = \u03bb1 < \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn of L, we obtain a drawing for which the nodes are partitionned into two sets living in two hyperplanes corresponding to the value of their first coordinate. Let us call such a drawing a bipartite drawing . However, if G is connected, the vector u2 does not\n100 CHAPTER 5. SIGNED GRAPHS\nbelong to KerL, so if m \u2265 3, it must have at least three coordinates with distinct absolute values, and using (u2, . . . , un+1) we obtain a nonbipartite graph. Then, the following version of Theorem 3.2 is easily shown.\nTheorem 5.9. Let G = (V,W ) be a signed graph with |V | = m \u2265 3, assume that G has some negative edge and is connected, and let L = D \u2212W be the signed Laplacian of G.\n(1) If G is not balanced and if the eigenvalues of L are 0 < \u03bb1 \u2264 \u03bb2 \u2264 \u03bb3 \u2264 . . . \u2264 \u03bbm, then the minimal energy of any orthogonal graph drawing of G in Rn is equal to \u03bb1+ \u00b7 \u00b7 \u00b7+\u03bbn The m \u00d7 n matrix R consisting of any unit eigenvectors u1, . . . , un associated with \u03bb1 \u2264 . . . \u2264 \u03bbn yields an orthogonal graph drawing of minimal energy.\n(2) If G is balanced and if the eigenvalues of L are 0 = \u03bb1 < \u03bb2 \u2264 \u03bb3 \u2264 . . . \u2264 \u03bbm, then the minimal energy of any orthogonal nonbipartite graph drawing of G in Rn is equal to \u03bb2 + \u00b7 \u00b7 \u00b7 + \u03bbn+1 (in particular, this implies that n < m). The m \u00d7 n matrix R consisting of any unit eigenvectors u2, . . . , un+1 associated with \u03bb2 \u2264 . . . \u2264 \u03bbn+1 yields an orthogonal nonbipartite graph drawing of minimal energy.\n(3) If G is balanced, for n = 2, a graph drawing of G as a bipartite graph (with positive edges only withing the two blocks of vertices) is obtained from the m\u00d7 2 matrix consisting of any two unit eigenvectors u1 and u2 associated with 0 and \u03bb2.\nIn all cases, the graph drawing R satisfies the condition R>R = I (it is an orthogonal graph drawing).\nOur first example is the signed graph G4 defined by the weight matrix given by the following Matlab program:\nnn = 6; G3 = diag(ones(1,nn),1); G3 = G3 + G3\u2019; G3(1,nn+1) = 1; G3(nn+1,1) = 1; G4 = -G3;\nAll edges of this graph are negative. The graph obtained by using G3 is shown on the left and the graph obtained by using the signed Laplacian of G4 is shown on the right in Figure 5.6.\nThe second example is the signed graph G5 obtained from G3 by making a single edge negative:"}, {"heading": "G5 = G3; G5(1,2) = -1; G5(2,1) = -1;", "text": "The graph obtained by using G3 is shown on the left and the graph obtained by using the signed Laplacian of G5 is shown on the right in Figure 5.7. Positive edges are shown in blue and negative edges are shown in red.\n5.5. SIGNED GRAPH DRAWING 101\nThe third example is the signed graph G6 defined by the weight matrix given by the following Matlab program:\nnn = 24; G6 = diag(ones(1,nn),1); G6 = G6 + G6\u2019; G6(1,nn+1) = 1; G6(nn+1,1) = 1; G6(1,2) = -1; G6(2,1) = -1; G6(6,7) = -1; G6(7,6) = -1; G6(11,12) = -1; G6(12,11) = -1; G6(16,17) = -1; G6(17,16) = -1; G6(21,22) = -1; G6(22,21) = -1;\nThe graph obtained by using absolute values in G6 is shown on the left and the graph obtained by using the signed Laplacian of G6 is shown on the right in Figure 5.8.\nThe fourth example is the signed graph G7 defined by the weight matrix given by the following Matlab program:\nnn = 26; G7 = diag(ones(1,nn),1); G7 = G7 + G7\u2019; G7(1,nn+1) = 1; G7(nn+1,1) = 1; G7(1,2) = -1; G7(2,1) = -1; G7(10,11) = -1; G7(11,10) = -1; G7(19,20) = -1; G7(20,19) = -1;\nThe graph obtained by using absolute values in G7 is shown on the left and the graph obtained by using the signed Laplacian of G7 is shown on the right in Figure 5.9.\n102 CHAPTER 5. SIGNED GRAPHS\nThese graphs are all unbalanced. As predicted, nodes linked by negative edges are far from each other.\nOur last example is the balanced graph G1 from Figure 5.1. The graph obtained by using absolute values in G1 is shown on the left and the bipartite graph obtained by using the signed Laplacian of G1 is shown on the right in Figure 5.10.\nChapter 6\nGraph Clustering Using Ratio Cuts\nIn this short chapter, we consider the alternative to normalized cut, called ratio cut, and show that the methods of Chapters 4 and 5 can be trivially adapted to solve the clustering problem using ratio cuts. All that needs to be done is to replace the normalized Laplacian Lsym by the unormalized Laplacian L, and omit the step of considering Problem (\u2217\u22172). In particular, there is no need to multiply the continuous solution Y by D\u22121/2. The idea of ratio cut is to replace the volume vol(Aj) of each block Aj of the partition by its size, |Aj| (the number of nodes in Aj). First, we deal with unsigned graphs, the case where the entries in the symmetric weight matrix W are nonnegative.\nDefinition 6.1. The ratio cut Rcut(A1, . . . , AK) of the partition (A1, . . . , AK) is defined as\nRcut(A1, . . . , AK) = K\u2211 i=1 cut(Aj, Aj) |Aj| .\nAs in Section 4.3, given a partition of V into K clusters (A1, . . . , AK), if we represent the jth block of this partition by a vector Xj such that\nXji = { aj if vi \u2208 Aj 0 if vi /\u2208 Aj,\nfor some aj 6= 0, then\n(Xj)>LXj = a2j(cut(Aj, Aj)\n(Xj)>Xj = a2j |Aj|.\nConsequently, we have\nRcut(A1, . . . , AK) = K\u2211 i=1 cut(Aj, Aj) |Aj| = K\u2211 i=1 (Xj)>LXj (Xj)>Xj .\n103\n104 CHAPTER 6. GRAPH CLUSTERING USING RATIO CUTS\nOn the other hand, the normalized cut is given by\nNcut(A1, . . . , AK) = K\u2211 i=1 cut(Aj, Aj) vol(Aj) = K\u2211 i=1 (Xj)>LXj (Xj)>DXj .\nTherefore, ratio cut is the special case of normalized cut where D = I. If we let X = {\n[X1 . . . XK ] | Xj = aj(xj1, . . . , x j N), x j i \u2208 {1, 0}, aj \u2208 R, Xj 6= 0 } (note that the condition Xj 6= 0 implies that aj 6= 0), then the set of matrices representing partitions of V into K blocks is\nK = { X = [X1 \u00b7 \u00b7 \u00b7 XK ] | X \u2208 X ,\n(X i)>Xj = 0, 1 \u2264 i, j \u2264 K, i 6= j } .\nHere is our first formulation of K-way clustering of a graph using ratio cuts, called problem PRC1 :\nK-way Clustering of a graph using Ratio Cut, Version 1: Problem PRC1\nminimize K\u2211 j=1 (Xj)>LXj (Xj)>Xj subject to (X i)>Xj = 0, 1 \u2264 i, j \u2264 K, i 6= j, X \u2208 X .\nThe solutions that we are seeking are K-tuples (P(X1), . . . ,P(XK)) of points in RPN\u22121 determined by their homogeneous coordinates X1, . . . , XK . As in Chapter 4, chasing denominators and introducing a trace, we obtain the following formulation of our minimization problem:\nK-way Clustering of a graph using Ratio Cut, Version 2: Problem PRC2\nminimize tr(X>LX) subject to X>X = I,\nX \u2208 X .\nThe natural relaxation of problem PRC2 is to drop the condition that X \u2208 X , and we obtain the\n105\nProblem (R\u22172)\nminimize tr(X>LX) subject to X>X = I.\nThis time, since the normalization condition is X>X = I, we can use the eigenvalues and the eigenvectors of L, and by Proposition A.2, the minimum is achieved by any K unit eigenvectors (u1, . . . , uK) associated with the smallest K eigenvalues\n0 = \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 \u03bbK\nof L. The matrix Z = Y = [u1, . . . , uK ] yields a minimum of our relaxed problem (R\u22172). The rest of the algorithm is as before; we try to find Q = R\u039b with R \u2208 O(K), \u039b diagonal invertible, and X \u2208 X such that \u2016X \u2212 ZQ\u2016 is minimum.\nIn the case of signed graphs, we define the signed ratio cut sRcut(A1, . . . , AK) of the partition (A1, . . . , AK) as\nsRcut(A1, . . . , AK) = K\u2211 j=1 cut(Aj, Aj) |Aj| + 2 K\u2211 j=1 links\u2212(Aj, Aj) |Aj| .\nSince we still have\n(Xj)>LXj = a2j(cut(Aj, Aj) + 2links \u2212(Aj, Aj)),\nwe obtain\nsRcut(A1, . . . , AK) = K\u2211 j=1 (Xj)>LXj (Xj)>Xj .\nTherefore, this is similar to the case of unsigned graphs, with L replaced with L. The same algorithm applies, but as in Chapter 5, the signed Laplacian L is positive definite iff G is unbalanced. Modifying the computer program implementing normalized cuts to deal with ratio cuts is trivial (use L instead of Lsym and don\u2019t multiply Y by D \u22121/2\n). Generally, normalized cut seems to yield \u201cbetter clusters,\u201d but this is not a very satisfactory statement since we haven\u2019t defined precisely in which sense a clustering is better than another. We leave this point as further research.\n106 CHAPTER 6. GRAPH CLUSTERING USING RATIO CUTS"}, {"heading": "Appendix A", "text": "Rayleigh Ratios and the Courant-Fischer Theorem\nThe most important property of symmetric matrices is that they have real eigenvalues and that they can be diagonalized with respect to an orthogonal matrix. Thus, if A is an n\u00d7 n symmetric matrix, then it has n real eigenvalues \u03bb1, . . . , \u03bbn (not necessarily distinct), and there is an orthonormal basis of eigenvectors (u1, . . . , un) (for a proof, see Gallier [8]). Another fact that is used frequently in optimization problem is that the eigenvalues of a symmetric matrix are characterized in terms of what is known as the Rayleigh ratio, defined by\nR(A)(x) = x>Ax\nx>x , x \u2208 Rn, x 6= 0.\nThe following proposition is often used to prove the correctness of various optimization or approximation problems (for example PCA).\nProposition A.1. (Rayleigh\u2013Ritz) If A is a symmetric n\u00d7 n matrix with eigenvalues \u03bb1 \u2264 \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn and if (u1, . . . , un) is any orthonormal basis of eigenvectors of A, where ui is a unit eigenvector associated with \u03bbi, then\nmax x 6=0\nx>Ax\nx>x = \u03bbn\n(with the maximum attained for x = un), and\nmax x 6=0,x\u2208{un\u2212k+1,...,un}\u22a5\nx>Ax\nx>x = \u03bbn\u2212k\n(with the maximum attained for x = un\u2212k), where 1 \u2264 k \u2264 n\u2212 1. Equivalently, if Vk is the subspace spanned by (u1, . . . , uk), then\n\u03bbk = max x 6=0,x\u2208Vk\nx>Ax\nx>x , k = 1, . . . , n.\n107\n108 APPENDIX A. RAYLEIGH RATIOS AND THE COURANT-FISCHER THEOREM\nProof. First, observe that\nmax x 6=0\nx>Ax\nx>x = max x {x>Ax | x>x = 1},\nand similarly,\nmax x 6=0,x\u2208{un\u2212k+1,...,un}\u22a5\nx>Ax\nx>x = max x\n{ x>Ax | (x \u2208 {un\u2212k+1, . . . , un}\u22a5) \u2227 (x>x = 1) } .\nSince A is a symmetric matrix, its eigenvalues are real and it can be diagonalized with respect to an orthonormal basis of eigenvectors, so let (u1, . . . , un) be such a basis. If we write\nx = n\u2211 i=1 xiui,\na simple computation shows that\nx>Ax = n\u2211 i=1 \u03bbix 2 i .\nIf x>x = 1, then \u2211n\ni=1 x 2 i = 1, and since we assumed that \u03bb1 \u2264 \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn, we get\nx>Ax = n\u2211 i=1 \u03bbix 2 i \u2264 \u03bbn ( n\u2211 i=1 x2i ) = \u03bbn.\nThus, max x\n{ x>Ax | x>x = 1 } \u2264 \u03bbn,\nand since this maximum is achieved for en = (0, 0, . . . , 1), we conclude that\nmax x\n{ x>Ax | x>x = 1 } = \u03bbn.\nNext, observe that x \u2208 {un\u2212k+1, . . . , un}\u22a5 and x>x = 1 iff xn\u2212k+1 = \u00b7 \u00b7 \u00b7 = xn = 0 and\u2211n\u2212k i=1 x 2 i = 1. Consequently, for such an x, we have\nx>Ax = n\u2212k\u2211 i=1 \u03bbix 2 i \u2264 \u03bbn\u2212k ( n\u2211 i=k+1 x2i ) = \u03bbn\u2212k.\nThus, max x { x>Ax | (x \u2208 {un\u2212k+1, . . . , un}\u22a5) \u2227 (x>x = 1) } \u2264 \u03bbn\u2212k,\nand since this maximum is achieved for en\u2212k = (0, . . . , 0, 1, 0, . . . , 0) with a 1 in position n\u2212 k, we conclude that\nmax x\n{ x>Ax | (x \u2208 {un\u2212k+1, . . . , un}\u22a5) \u2227 (x>x = 1) } = \u03bbn\u2212k,\nas claimed.\n109\nFor our purposes, we also need the version of Proposition A.1 applying to min instead of max, whose proof is obtained by a trivial modification of the proof of Proposition A.1.\nProposition A.2. (Rayleigh\u2013Ritz) If A is a symmetric n\u00d7 n matrix with eigenvalues \u03bb1 \u2264 \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn and if (u1, . . . , un) is any orthonormal basis of eigenvectors of A, where ui is a unit eigenvector associated with \u03bbi, then\nmin x 6=0\nx>Ax\nx>x = \u03bb1\n(with the minimum attained for x = u1), and\nmin x 6=0,x\u2208{u1,...,ui\u22121}\u22a5\nx>Ax\nx>x = \u03bbi\n(with the minimum attained for x = ui), where 2 \u2264 i \u2264 n. Equivalently, if Wk = V \u22a5k\u22121 denotes the subspace spanned by (uk, . . . , un) (with V0 = (0)), then\n\u03bbk = min x 6=0,x\u2208Wk\nx>Ax\nx>x = min\nx 6=0,x\u2208V \u22a5k\u22121\nx>Ax\nx>x , k = 1, . . . , n.\nPropositions A.1 and A.2 together are known as the Rayleigh\u2013Ritz theorem.\nAs an application of Propositions A.1 and A.2, we give a proof of a proposition which is the key to the proof of Theorem 3.2. First, we need a definition. Given an n\u00d7 n symmetric matrix A and an m\u00d7m symmetric B, with m \u2264 n, if \u03bb1 \u2264 \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn are the eigenvalues of A and \u00b51 \u2264 \u00b52 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u00b5m are the eigenvalues of B, then we say that the eigenvalues of B interlace the eigenvalues of A if\n\u03bbi \u2264 \u00b5i \u2264 \u03bbn\u2212m+i, i = 1, . . . ,m.\nThe following proposition is known as the Poincare\u0301 separation theorem; see Horn and Johnson [13], Section 4.3, Corollary 4.3.16.\nProposition A.3. Let A be an n \u00d7 n symmetric matrix, R be an n \u00d7m matrix such that R>R = I (with m \u2264 n), and let B = R>AR (an m \u00d7m matrix). The following properties hold:\n(a) The eigenvalues of B interlace the eigenvalues of A.\n(b) If \u03bb1 \u2264 \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn are the eigenvalues of A and \u00b51 \u2264 \u00b52 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u00b5m are the eigenvalues of B, and if \u03bbi = \u00b5i, then there is an eigenvector v of B with eigenvalue \u00b5i such that Rv is an eigenvector of A with eigenvalue \u03bbi.\n110 APPENDIX A. RAYLEIGH RATIOS AND THE COURANT-FISCHER THEOREM\nProof. (a) Let (u1, . . . , un) be an orthonormal basis of eigenvectors for A, and let (v1, . . . , vm) be an orthonormal basis of eigenvectors for B. Let Uj be the subspace spanned by (u1, . . . , uj) and let Vj be the subspace spanned by (v1, . . . , vj). For any i, the subpace Vi has dimension i and the subspace R>Ui\u22121 has dimension at most i \u2212 1. Therefore, there is some nonzero vector v \u2208 Vi \u2229 (R>Ui\u22121)\u22a5, and since\nv>R>uj = (Rv) >uj = 0, j = 1, . . . , i\u2212 1,\nwe have Rv \u2208 (Ui\u22121)\u22a5. By Proposition A.2 and using the fact that R>R = I, we have\n\u03bbi \u2264 (Rv)>ARv (Rv)>Rv = v>Bv v>v .\nOn the other hand, by Proposition A.1,\n\u00b5i = max x 6=0,x\u2208{vi+1,...,vn}\u22a5\nx>Bx\nx>x = max x 6=0,x\u2208{v1,...,vi}\nx>Bx\nx>x ,\nso w>Bw\nw>w \u2264 \u00b5i for all w \u2208 Vi,\nand since v \u2208 Vi, we have\n\u03bbi \u2264 v>Bv\nv>v \u2264 \u00b5i, i = 1, . . . ,m.\nWe can apply the same argument to the symmetric matrices \u2212A and \u2212B, to conclude that\n\u2212\u03bbn\u2212m+i \u2264 \u2212\u00b5i,\nthat is, \u00b5i \u2264 \u03bbn\u2212m+i, i = 1, . . . ,m.\nTherefore, \u03bbi \u2264 \u00b5i \u2264 \u03bbn\u2212m+i, i = 1, . . . ,m,\nas desired.\n(b) If \u03bbi = \u00b5i, then\n\u03bbi = (Rv)>ARv (Rv)>Rv = v>Bv v>v = \u00b5i,\nso v must be an eigenvector for B and Rv must be an eigenvector for A, both for the eigenvalue \u03bbi = \u00b5i.\nObserve that Proposition A.3 implies that\n\u03bb1 + \u00b7 \u00b7 \u00b7+ \u03bbm \u2264 tr(R>AR) \u2264 \u03bbn\u2212m+1 + \u00b7 \u00b7 \u00b7+ \u03bbn.\nThe left inequality is used to prove Theorem 3.2.\nFor the sake of completeness, we also prove the Courant\u2013Fischer characterization of the eigenvalues of a symmetric matrix.\n111\nTheorem A.4. (Courant\u2013Fischer) Let A be a symmetric n \u00d7 n matrix with eigenvalues \u03bb1 \u2264 \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbn and let (u1, . . . , un) be any orthonormal basis of eigenvectors of A, where ui is a unit eigenvector associated with \u03bbi. If Vk denotes the set of subspaces of Rn of dimension k, then\n\u03bbk = max W\u2208Vn\u2212k+1 min x\u2208W,x6=0\nx>Ax\nx>x\n\u03bbk = min W\u2208Vk max x\u2208W,x6=0\nx>Ax\nx>x .\nProof. Let us consider the second equality, the proof of the first equality being similar. Observe that the space Vk spanned by (u1, . . . , uk) has dimension k, and by Proposition A.1, we have\n\u03bbk = max x 6=0,x\u2208Vk\nx>Ax\nx>x \u2265 min W\u2208Vk max x\u2208W,x6=0\nx>Ax\nx>x .\nTherefore, we need to prove the reverse inequality; that is, we have to show that\n\u03bbk \u2264 max x 6=0,x\u2208W\nx>Ax\nx>x , for all W \u2208 Vk.\nNow, for any W \u2208 Vk, if we can prove that W\u2229V \u22a5k\u22121 6= (0), then for any nonzero v \u2208 W\u2229V \u22a5k\u22121, by Proposition A.2 , we have\n\u03bbk = min x 6=0,x\u2208V \u22a5k\u22121\nx>Ax\nx>x \u2264 v\n>Av v>v \u2264 max x\u2208W,x6=0 x>Ax x>x .\nIt remains to prove that dim(W \u2229 V \u22a5k\u22121) \u2265 1. However, dim(Vk\u22121) = k \u2212 1, so dim(V \u22a5k\u22121) = n\u2212 k + 1, and by hypothesis dim(W ) = k. By the Grassmann relation,\ndim(W ) + dim(V \u22a5k\u22121) = dim(W \u2229 V \u22a5k\u22121) + dim(W + V \u22a5k\u22121),\nand since dim(W + V \u22a5k\u22121) \u2264 dim(Rn) = n, we get\nk + n\u2212 k + 1 \u2264 dim(W \u2229 V \u22a5k\u22121) + n;\nthat is, 1 \u2264 dim(W \u2229 V \u22a5k\u22121), as claimed.\n112 APPENDIX A. RAYLEIGH RATIOS AND THE COURANT-FISCHER THEOREM"}, {"heading": "Appendix B", "text": "Riemannian Metrics on Quotient Manifolds\nIn order to define a metric on the projective space RPn, we need to review a few notions of differential geometry. First, we need to define the quotient M/G of a manifold by a group acting on M . This section relies heavily on Gallot, Hulin, Lafontaine [9] and Lee [17], which contain thorough expositions and should be consulted for details.\nDefinition B.1. Recall that an action of a group G (with identity element 1) on a set X is a map \u03b3 : G\u00d7X \u2192 X satisfying the following properties:\n(1) \u03b3(1, x) = x, for all x \u2208 X.\n(2) \u03b3(g1, \u03b3(g2, x)) = \u03b3(g1g2, x), for all g1, g2 \u2208 G, and all x \u2208 X.\nWe usually abbreviate \u03b3(g, x) by g \u00b7 x. If X is a topological space and G is a topological group, we say that the action is continuous iff the map \u03b3 is continuous. In this case, for every g \u2208 G, the map x 7\u2192 g \u00b7 x is a homeomorphism. If X is a smooth manifold and G is a Lie group, we say that the action is smooth iff the map \u03b3 is smooth. In this case, for every g \u2208 G, the map x 7\u2192 g \u00b7 x is a diffeomorphism.\nRemark: To be more precise, what we have defined in Definition B.1 is a left action of the group G on the set X. There is also a notion of a right action, but we won\u2019t need it.\nThe quotient of X by G, denoted X/G, is the set of orbits of G; that is, the set of equivalences classes of the equivalence relation ' defined such that, for any x, y \u2208 X,\nx ' y iff (\u2203g \u2208 G)(y = g \u00b7 x).\nThe orbit of x \u2208 X (the equivalence class of x) is the set\nO(x) = {g \u00b7 x | g \u2208 G},\n113\n114 APPENDIX B. RIEMANNIAN METRICS ON QUOTIENT MANIFOLDS\nalso denoted by G \u00b7 x. If X is a topological space, we give X/G the quotient topology. For any subset V of X and for any g \u2208 G, we denote by gV the set\ngV = {g \u00b7 x | x \u2208 V }.\nOne problem is that even if X is Hausdorff, X/G may not be. Thus, we need to find conditions to ensure that X/G is Hausdorff.\nBy a discrete group, we mean a group equipped with the discrete topology (every subset is open). In other words, we don\u2019t care about the topology of G! The following conditions prove to be useful.\nDefinition B.2. Let \u00b7 : G\u00d7X \u2192 X be the action of a group G on a set X. We say that G acts freely (or that the action is free) iff for all g \u2208 G and all x \u2208 X, if g 6= 1 then g \u00b7 x 6= x.\nIf X is a locally compact space and G is a discrete group acting continuously on X, we say that G acts properly (or that the action is proper) iff\n(i) For every x \u2208 X, there is some open subset V with x \u2208 V such that gV \u2229 V 6= \u2205 for only finitely many g \u2208 G.\n(ii) For all x, y \u2208 X, if y /\u2208 G \u00b7 x (y is not in the orbit of x), then there exist some open sets V,W with x \u2208 V and y \u2208 W such that gV \u2229W = 0 for all g \u2208 G.\nThe following proposition gives necessary and sufficient conditions for a discrete group to act freely and properly often found in the literature (for instance, O\u2019Neill [18], Berger and Gostiaux [3], and do Carmo [6], but beware that in this last reference Hausdorff separation is not required!).\nProposition B.1. If X is a locally compact space and G is a discrete group, then a smooth action of G on M is free and proper iff the following conditions hold:\n(i) For every x \u2208 X, there is some open subset V with x \u2208 V such that gV \u2229 V = \u2205 for all g \u2208 G such that g 6= 1.\n(ii) For all x, y \u2208 X, if y /\u2208 G \u00b7 x (y is not in the orbit of x), then there exist some open sets V,W with x \u2208 V and y \u2208 W such that gV \u2229W = 0 for all g \u2208 G.\nProof. Condition (i) of Proposition B.1 implies condition (i) of Definition B.2, and condition (ii) is the same in Proposition B.1 and Definition B.2. If (i) holds, then the action must be free since if g \u00b7 x = x, then gV \u2229 V 6= \u2205, which implies that g = 1.\nConversely, we just have to prove that the conditions of Definition B.2 imply condition (i) of Proposition B.1. By (i) of Definition B.2, there is some open subset U containing x and a finite number of elements of G, say g1, . . . , gm, with gi 6= 1, such that\ngiU \u2229 U 6= \u2205, i = 1, . . . ,m.\n115\nSince our action is free and gi 6= 1, we have gi \u00b7x 6= x, so by Hausdorff separation, there exist some open subsets Wi,W \u2032 i , with x \u2208 Wi and gi \u00b7x \u2208 W \u2032i , such that Wi\u2229W \u2032i = \u2205, i = 1, . . . ,m. Then, if we let\nV = W \u2229 ( m\u22c2 i=1 (Wi \u2229 g\u22121i W \u2032i ) ) ,\nwe see that V \u2229 giV = \u2205, and since V \u2286 W , we also have V \u2229 gV = \u2205 for all other g \u2208 G.\nRemark: The action of a discrete group satisfying the properties of Proposition B.1 is often called \u201cproperly discontinuous.\u201d However, as pointed out by Lee ([17], just before Proposition 9.18), this term is self-contradictory since such actions are smooth, and thus continuous!\nWe also need covering maps.\nDefinition B.3. Let X and Y be two topological spaces. A map \u03c0 : X \u2192 Y is a covering map iff the following conditions hold:\n(1) The map \u03c0 is continuous and surjective.\n(2) For every y \u2208 Y , there is some open subset W \u2286 Y with y \u2208 W , such that \u03c0\u22121(W ) = \u22c3 i\u2208I Ui,\nwhere the Ui \u2286 X are pairwise disjoint open subsets such that the restriction of \u03c0 to Ui is a homeomorphism for every i \u2208 I.\nIf X and Y are smooth manifolds, we assume that \u03c0 is smooth and that the restriction of \u03c0 to each Ui is a diffeomorphism.\nThen, we have the following useful result.\nTheorem B.2. Let M be a smooth manifold and let G be discrete group acting smoothly, freely and properly on M . Then there is a unique structure of smooth manifold on M/G such that the projection map \u03c0 : M \u2192M/G is a covering map.\nFor a proof, see Gallot, Hulin, Lafontaine [9] (Theorem 1.88) or Lee [17] (Theorem 9.19).\nReal projective spaces are illustrations of Theorem B.2. Indeed, if M is the unit nsphere Sn \u2286 Rn+1 and G = {I,\u2212I}, where \u2212I is the antipodal map, then the conditions of Proposition B.1 are easily checked (since Sn is compact), and consequently the quotient\nRPn = Sn/G\nis a smooth manifold and the projection map \u03c0 : Sn \u2192 RPn is a covering map. The fiber \u03c0\u22121([x]) of every point [x] \u2208 RPn consists of two antipodal points: x,\u2212x \u2208 Sn.\nThe next step is see how a Riemannian metric on M induces a Riemannian metric on the quotient manifold M/G.\n116 APPENDIX B. RIEMANNIAN METRICS ON QUOTIENT MANIFOLDS\nDefinition B.4. Given any two Riemmanian manifolds (M, g) and (N, h) a smooth map f : M \u2192 N is a local isometry iff for all p \u2208 M , the tangent map dfp : TpM \u2192 Tf(p)N is an orthogonal transformation of the Euclidean spaces (TpM, gp) and (Tf(p)N, hf(p))). Furthermore, if f is a diffeomorphism, we say that f is an isometry .\nThe Riemannian version of a covering map is the following:\nDefinition B.5. Let (M, g) and (N, h) be two Riemannian manifolds. A map \u03c0 : M \u2192 N is a Riemannian covering map iff the following conditions hold:\n(1) The map \u03c0 is a smooth covering.\n(2) The map \u03c0 is a local isometry.\nThe following theorem is the Riemannian version of Theorem B.2.\nTheorem B.3. Let (M,h) be a Riemannian manifold and let G be discrete group acting smoothly, freely and properly on M , and such that the map x 7\u2192 \u03c3 \u00b7 x is an isometry for all \u03c3 \u2208 G. Then there is a unique structure of Riemannian manifold on N = M/G such that the projection map \u03c0 : M \u2192M/G is a Riemannian covering map.\nProof sketch. For a complete proof see Gallot, Hulin, Lafontaine [9] (Proposition 2.20). To define a Riemannian metric g on N = M/G we need to define an inner product gp on the tangent space TpN for every p \u2208 N . Pick any q1 \u2208 \u03c0\u22121(p) in the fibre of p. Because \u03c0 is a Riemannian covering map, it is a local diffeomorphism, and thus d\u03c0q1 : Tq1M \u2192 TpM is an isometry. Then, given any two tangent vectors u, v \u2208 TpN , we define their inner product gp(u, v) by\ngp(u, v) = hq1(d\u03c0 \u22121 q1 (u), d\u03c0\u22121q1 (v)).\nNow, we need to show that gp does not depend on the choice of q1 \u2208 \u03c0\u22121(p). So, let q2 \u2208 \u03c0\u22121(p) be any other point in the fibre of p. By definition of N = M/G, we have q2 = g \u00b7 q1 for some g \u2208 G, and we know that the map f : q 7\u2192 g \u00b7 q is an isometry of M . Now, since \u03c0 = \u03c0 \u25e6 f we have\nd\u03c0q1 = d\u03c0q2 \u25e6 dfq1 ,\nand since d\u03c0q1 : Tq1M \u2192 TpM and d\u03c0q2 : Tq2M \u2192 TpM are isometries, we get\nd\u03c0\u22121q2 = dfq1 \u25e6 d\u03c0 \u22121 q1 .\nBut dfq1 : Tq1M \u2192 Tq2M is also an isometry, so\nhq2(d\u03c0 \u22121 q2 (u), d\u03c0\u22121q2 (v)) = hq2(dfq1(d\u03c0 \u22121 q1 (u)), dfq1(d\u03c0 \u22121 q2 (v))) = hq1(d\u03c0 \u22121 q1 (u), d\u03c0\u22121q1 (v)).\nTherefore, the inner product gp is well defined on TpN .\n117\nTheorem B.3 implies that every Riemannian metric g on the sphere Sn induces a Riemannian metric g\u0302 on the projective space RPn, in such a way that the projection \u03c0 : Sn \u2192 RPn is a Riemannian covering. In particular, if U is an open hemisphere obtained by removing its boundary Sn\u22121 from a closed hemisphere, then \u03c0 is an isometry between U and its image RPn \u2212 \u03c0(Sn\u22121) \u2248 RPn \u2212 RPn\u22121.\nWe also observe that for any two points p = [x] and q = [y] in RPn, where x, y \u2208 Sn, if x \u00b7 y = cos \u03b8, with 0 \u2264 \u03b8 \u2264 \u03c0, then there are two possibilities:\n1. x \u00b7 y \u2265 0, which means that 0 \u2264 \u03b8 \u2264 \u03c0/2, or\n2. x \u00b7 y < 0, which means that \u03c0/2 < \u03b8 \u2264 \u03c0.\nIn the second case, since [\u2212y] = [y] and x \u00b7 (\u2212y) = \u2212x \u00b7y, we can replace the representative y of q by \u2212y, and we have x \u00b7 (\u2212y) = cos(\u03c0\u2212 \u03b8), with 0 \u2264 \u03c0\u2212 \u03b8 < \u03c0/2. Therefore, in all cases, for any two points p, q \u2208 RPn, we can find an open hemisphere U such that p = [x], q = [y], x, y \u2208 U , and x \u00b7 y \u2265 0; that is, the angle \u03b8 \u2265 0 between x and y is at most \u03c0/2. This fact together with the following simple proposition will allow us to figure out the distance (in the sense of Riemannian geometry) between two points in RPn.\nProposition B.4. Let \u03c0 : M \u2192 N be a Riemannian covering map between two Riemannian manifolds (M, g) and (N, h). Then, the geodesics of (N, h) are the projections of geodesics in (M, g) (i.e., curves \u03c0 \u25e6 \u03b3 in (N, h), where \u03b3 is a geodesic in (M, g)), and the geodesics of (M, g) are the liftings of geodesics in (N, h) (i.e., curves \u03b3 of (M, g), such that \u03c0 \u25e6 \u03b3 is a geodesic in (N, h)).\nThe proof of Proposition B.4 can be found in Gallot, Hulin, Lafontaine [9] (Proposition 2.81).\nNow, if (M, g) is a connected Riemannian manifold, recall that we define the distance d(p, q) between two points p, q \u2208M as\nd(p, q) = inf{L(\u03b3) | \u03b3 : [0, 1]\u2192M},\nwhere \u03b3 is any piecewise C1-curve from p to q, and\nL(\u03b3) = \u222b 1 0 \u221a g(\u03b3\u2032(t), \u03b3\u2032(t)) dt\nis the length of \u03b3. It is well known that d is a metric on M . The Hopf-Rinow Theorem (see Gallot, Hulin, Lafontaine [9], Theorem 2.103) says among other things that (M, g) is geodesically complete (which means that every geodesics \u03b3 of M can be extended to a geodesic \u03b3\u0303 defined on all of R) iff any two points of M can be joined by a minimal geodesic iff (M,d) is a complete metric space. Therefore, in a complete (connected) manifold\nd(p, q) = inf{L(\u03b3) | \u03b3 : [0, 1]\u2192M is a geodesic}.\n118 APPENDIX B. RIEMANNIAN METRICS ON QUOTIENT MANIFOLDS\nIn particular, compact manifolds are complete, so the distance between two points is the infimum of the length of minimal geodesics joining these points.\nApplying this to RPn and the canonical Euclidean metric induced by Rn+1, since geodesics of Sn are great circles, by the discussion above, for any two points p = [x] and q = [y] in RPn, with x, y \u2208 Sn, the distance between them is given by\nd(p, q) = d([x], [y]) = { cos\u22121(x \u00b7 y) if x \u00b7 y \u2265 0 cos\u22121(\u2212x \u00b7 y) if x \u00b7 y < 0.\nHere cos\u22121(z) = arccos(z) is the unique angle \u03b8 \u2208 [0, \u03c0] such that cos(\u03b8) = z. Equivalently,\nd([x], [y]) = cos\u22121(|x \u00b7 y|),\nand d([x], [y]) = min{cos\u22121(x \u00b7 y), \u03c0 \u2212 cos\u22121(x \u00b7 y)}.\nIf the representatives x, y \u2208 Rn+1 of p = [x] and q = [q] are not unit vectors, then d([x], [y]) = cos\u22121 ( |x \u00b7 y| \u2016x\u2016 \u2016y\u2016 ) .\nNote that 0 \u2264 d(p, q) \u2264 \u03c0/2. Now, the Euclidean distance between x and y on Sn is given by\n\u2016x\u2212 y\u201622 = \u2016x\u2016 2 2 + \u2016y\u2016 2 2 \u2212 2x \u00b7 y = 2\u2212 2 cos \u03b8 = 4 sin 2(\u03b8/2).\nThus, \u2016x\u2212 y\u20162 = 2 sin(\u03b8/2), 0 \u2264 \u03b8 \u2264 \u03c0. It follows that for any x \u2208 Sn, and for any subset A \u2286 Sn, a point a \u2208 A minimizes the distance dSn(x, a) = cos\n\u22121(x \u00b7 a) = \u03b8 on Sn iff it minimizes the Euclidean distance \u2016x\u2212 a\u20162 = 2 sin(\u03b8/2) (since 0 \u2264 \u03b8 \u2264 \u03c0). Then, on RP\nn, for any point p = [x] \u2208 RPn and any A \u2286 RPn, a point [a] \u2208 A minimizes the distance d([x], [a]) on RPn iff it minimizes min{\u2016x\u2212 a\u20162 , \u2016x+ a\u20162}. So, we are looking for [b] \u2208 A such that\nmin{\u2016x\u2212 b\u20162 , \u2016x+ b\u20162} = min [a]\u2208A min{\u2016x\u2212 a\u20162 , \u2016x+ a\u20162}\n= min{min [a]\u2208A \u2016x\u2212 a\u20162 , min [a]\u2208A \u2016x+ a\u20162}.\nIf the subset A \u2286 Sn is closed under the antipodal map (which means that if x \u2208 A, then \u2212x \u2208 A), then finding mina\u2208A d([x], [a]) on RPn is equivalent to finding mina\u2208A \u2016x\u2212 a\u20162, the minimum of the Euclidean distance. This is the case for the set X in Section 4.2 and the set K in Section 4.3. Acknowlegments: First, it must be said that the seminal and highly original work of Jianbo Shi and Stella Yu on normalized cuts, was the source of inspiration for this document. I also\n119\nwish to thank Katerina Fragkiadaki for pointing out a number of mistakes in an earlier version of this paper. Roberto Tron also made several suggestions that contributed to improving this report. Katerina, Ryan Kennedy, Andrew Yeager, and Weiyu Zhang made many useful comments and suggestions. Special thanks to Jocelyn Quaintance, Joao Cedoc and Marcelo Siqueira who proofread my manuscript with an eagle\u2019s eye, and made many comments that helped me improve it. Finally, thanks to Dan Spielman for making available his lovely survey on spectral graph theory, and to Kostas for giving me the opportunity to hold hostage a number of people for three Friday afternoons in a row.\n120 APPENDIX B. RIEMANNIAN METRICS ON QUOTIENT MANIFOLDS"}], "references": [{"title": "Correlation clustering", "author": ["Nikhil Bansal", "Avrim Blum", "Shuchi Chawla"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "G\u00e9om\u00e9trie diff\u00e9rentielle: vari\u00e9t\u00e9s, courbes et surfaces. Collection Math\u00e9matiques", "author": ["Marcel Berger", "Bernard Gostiaux"], "venue": "Puf, second edition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Spectral Graph Theory, volume 92 of Regional Conference Series in Mathematics", "author": ["Fan R.K. Chung"], "venue": "AMS, first edition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Correlation clustering with partial information", "author": ["Eric D. Demaine", "Nicole Immorlica"], "venue": "Working Notes of the 6th International Workshop on Approximation Algorithms for Combinatorial Problems, LNCS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Discrete Mathematics. Universitext", "author": ["Jean H. Gallier"], "venue": "Springer Verlag, first edition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Geometric Methods and Applications", "author": ["Jean H. Gallier"], "venue": "For Computer Science and Engineering. TAM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Riemannian Geometry", "author": ["S. Gallot", "D. Hulin", "J. Lafontaine"], "venue": "Universitext. Springer Verlag, second edition", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Algebraic Graph Theory", "author": ["Chris Godsil", "Gordon Royle"], "venue": "GTM No. 207. Springer Verlag, first edition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Gene and F", "author": ["H. Golub"], "venue": "Van Loan, Charles. Matrix Computations. The Johns Hopkins University Press, third edition", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "On the notion of balance of a signed graph", "author": ["Frank Harary"], "venue": "Michigan Math. J.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1953}, {"title": "Bounds for the least laplacian eigenvalue of a signed graph", "author": ["Jao Ping Hou"], "venue": "Acta Mathematica Sinica,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Spectral surface reconstruction from noisy point clouds", "author": ["Ravikrishna Kolluri", "Jonathan R. Shewchuk", "James F. O\u2019Brien"], "venue": "In Symposium on Geometry Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Spectral analysis of signed graphs for clustering, prediction and visualization", "author": ["J\u00e9r\u00f4me Kunegis", "Stephan Schmidt", "Andreas Lommatzsch", "J\u00fcrgen Lerner", "Ernesto William De Luca", "Sahin Albayrak"], "venue": "In SDM\u201910,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Introduction to Smooth Manifolds", "author": ["John M. Lee"], "venue": "GTM No. 218. Springer Verlag, first edition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Semi-Riemannian Geometry With Applications to Relativity", "author": ["Barrett O\u2019Neill"], "venue": "Pure and Applies Math.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1983}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Spectral graph theory", "author": ["Daniel Spielman"], "venue": "In Uwe Naumannn and Olaf Schenk, editors,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A tutorial on spectral clustering", "author": ["von Luxburg Ulrike"], "venue": "Statistics and Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Computational Models of Perceptual Organization", "author": ["Stella X. Yu"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Multiclass spectral clustering", "author": ["Stella X. Yu", "Jianbo Shi"], "venue": "In 9th International Conference on Computer", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}], "referenceMentions": [{"referenceID": 8, "context": "Following Godsil and Royle [10], we prove that E(R) = tr(R>LR),", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We give a number examples of graph drawings, many of which are borrowed or adapted from Spielman [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "This beautiful and deeply original method first published in Shi and Malik [20], has now come to be a \u201ctextbook chapter\u201d of computer vision and machine learning.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "This method was extended to K \u2265 3 clusters by Stella Yu in her dissertation [23], and is also the subject of Yu and Shi [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "This method was extended to K \u2265 3 clusters by Stella Yu in her dissertation [23], and is also the subject of Yu and Shi [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "A solution using the volume vol(Ai) of Ai (for K = 2) was proposed and investigated in a seminal paper of Shi and Malik [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "In view of the above, we have our first formulation of K-way clustering of a graph using normalized cuts, called problem PNC1 (the notation PNCX is used in Yu [23], Section 2.", "startOffset": 159, "endOffset": 163}, {"referenceID": 11, "context": "The idea to use positive degrees of nodes in the degree matrix of a signed graph with weights (\u22121, 0, 1) occurs in Hou [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "The natural step of using absolute values of weights in the degree matrix is taken by Kolluri, Shewchuk and O\u2019Brien [15] and Kunegis et al.", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16], we show that the signed Laplacian L is positive definite iff G is unbalanced, which means that it contains some cycle with an odd number of negative edges.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "As far as I know, except for a short section in one of Gilbert Strang\u2019s book, and von Luxburg [22] excellent survey on spectral clustering, there is no comprehensive writing on the topic of graph cuts.", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "(After Godsil and Royle [10], Section 8.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "Remark: Yu and Shi [24] use the notation degree(A) instead of vol(A).", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "We reproduce the proof in Gallier [7] (see also Godsil and Royle [10]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "We reproduce the proof in Gallier [7] (see also Godsil and Royle [10]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "For more on the properties of the Fiedler number, see Godsil and Royle [10] (Chapter 13) and Chung [4].", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "For more on the properties of the Fiedler number, see Godsil and Royle [10] (Chapter 13) and Chung [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "Properties (7)\u2013(10) are proved in Chung [4] (Chapter 1).", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "As explained in Godsil and Royle [10], we can imagine building a physical model of G by connecting adjacent vertices (in R) by identical springs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "The following proposition is shown in Godsil and Royle [10].", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "We present the proof given in Godsil and Royle [10] (Section 13.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "Some of these are borrowed or adapted from Spielman [21].", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 153, "endOffset": 158}, {"referenceID": 1, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 153, "endOffset": 158}, {"referenceID": 2, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 183, "endOffset": 188}, {"referenceID": 1, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 183, "endOffset": 188}, {"referenceID": 1, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 121, "endOffset": 126}, {"referenceID": 2, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 121, "endOffset": 126}, {"referenceID": 1, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 151, "endOffset": 156}, {"referenceID": 2, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 151, "endOffset": 156}, {"referenceID": 1, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 125, "endOffset": 130}, {"referenceID": 2, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 125, "endOffset": 130}, {"referenceID": 1, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 155, "endOffset": 160}, {"referenceID": 2, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 155, "endOffset": 160}, {"referenceID": 1, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 110, "endOffset": 115}, {"referenceID": 0, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 110, "endOffset": 115}, {"referenceID": 1, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 140, "endOffset": 145}, {"referenceID": 0, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 140, "endOffset": 145}, {"referenceID": 17, "context": "Our last example, also borrowed from Spielman [21], corresponds to the skeleton of the \u201cBuckyball,\u201d a geodesic dome invented by the architect Richard Buckminster Fuller (1895\u20131983).", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 77, "endOffset": 82}, {"referenceID": 2, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 77, "endOffset": 82}, {"referenceID": 1, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 107, "endOffset": 112}, {"referenceID": 2, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 107, "endOffset": 112}, {"referenceID": 1, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 23, "endOffset": 28}, {"referenceID": 2, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 23, "endOffset": 28}, {"referenceID": 1, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 55, "endOffset": 60}, {"referenceID": 3, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 55, "endOffset": 60}, {"referenceID": 16, "context": "A solution using the second measure (the volume) (for K = 2) was proposed and investigated in a seminal paper of Shi and Malik [20].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "This is the choice adopted in von Luxburg [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "Shi and Malik [20] use a = 1, b = \u2212 \u03b1 d\u2212 \u03b1 = \u2212 k 1\u2212 k ,", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "Another choice found in the literature (for example, in Belkin and Niyogi [2]) is", "startOffset": 74, "endOffset": 77}, {"referenceID": 16, "context": "Unfortunately, this is an NP-complete problem, as shown by Shi and Malik [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "which is the solution presented in von Luxburg [22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "This choice also corresponds to the scaled partition matrix used in Yu [23] and Yu and Shi [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "This choice also corresponds to the scaled partition matrix used in Yu [23] and Yu and Shi [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "X1K = 1N is used in Yu [23].", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "This normalization step is used by Yu [23] in the search for a discrete solution closest to a solution of a relaxation of our original problem.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "However, it is well known that XX is the orthogonal projection of R onto the range of X (see Gallier [8], Section 14.", "startOffset": 101, "endOffset": 104}, {"referenceID": 19, "context": "In view of the above, we have our first formulation of K-way clustering of a graph using normalized cuts, called problem PNC1 (the notation PNCX is used in Yu [23], Section 2.", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "This second option is the one chosen by Yu [23] and Yu and Shi [24] (actually, they work with 1 K (K \u2212 \u03bc(X, .", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "This second option is the one chosen by Yu [23] and Yu and Shi [24] (actually, they work with 1 K (K \u2212 \u03bc(X, .", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "In fact, since the eigenvalues of Lsym are in the range [0, 2], the eigenvalues of 2I\u2212Lsym = I+ D\u22121/2WD\u22121/2 are also in the range [0, 2] (that is, I+D\u22121/2WD\u22121/2 is positive semidefinite).", "startOffset": 56, "endOffset": 62}, {"referenceID": 1, "context": "In fact, since the eigenvalues of Lsym are in the range [0, 2], the eigenvalues of 2I\u2212Lsym = I+ D\u22121/2WD\u22121/2 are also in the range [0, 2] (that is, I+D\u22121/2WD\u22121/2 is positive semidefinite).", "startOffset": 130, "endOffset": 136}, {"referenceID": 15, "context": "Furthermore, both St(k, n) and G(k, n) are naturally reductive homogeneous manifolds (for the Stiefel manifold, when n \u2265 3), and G(k, n) is even a symmetric space (see O\u2019Neill [18]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 8, "context": "This is not quite so, and Godsil and Royle [10] provide a rigorous proof using Proposition A.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "A similar observation is made in Yu [23] and Yu and Shi [24] (but beware that in these works \u03b1 = vol(A)/ \u221a d).", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "A similar observation is made in Yu [23] and Yu and Shi [24] (but beware that in these works \u03b1 = vol(A)/ \u221a d).", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Inspired by Yu [23] and the previous discussion, given a solution Z of problem (\u22172), we look for pairs (X,Q) with X \u2208 X and where Q is a K\u00d7K matrix with nonzero and pairwise orthogonal columns, with \u2016X\u2016F = \u2016Z\u2016F , that minimize \u03c6(X,Q) = \u2016X \u2212 ZQ\u2016F .", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Yu [23] and Yu and Shi [24] consider the special case where Q \u2208 O(K).", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Yu [23] and Yu and Shi [24] consider the special case where Q \u2208 O(K).", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "3 (with A = Z>X and Q = R>), we get the following result (see Golub and Van Loan [11], Section 12.", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "The above method is essentially the method described in Yu [23] and Yu and Shi [24], except that in these works (in which X,Z and Y are denoted by X\u2217, X\u0303\u2217, and X\u0303, respectively) the entries in X belong to {0, 1}; as described above, for row i, the index ` corresponding to the entry +1 is given by arg max 1\u2264j\u2264K X\u0303(i, j).", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "The above method is essentially the method described in Yu [23] and Yu and Shi [24], except that in these works (in which X,Z and Y are denoted by X\u2217, X\u0303\u2217, and X\u0303, respectively) the entries in X belong to {0, 1}; as described above, for row i, the index ` corresponding to the entry +1 is given by arg max 1\u2264j\u2264K X\u0303(i, j).", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "The method due to Yu and Shi (see Yu [23] and Yu and Shi [24]) to find X \u2208 K and Q = R\u039b with R \u2208 O(K) and \u039b diagonal invertible that minimize \u03c6(X,Q) = \u2016X \u2212 ZQ\u2016F is to alternate steps during which either Q is held fixed (step PODX) or X is held fixed (step PODR), except that Yu and Shi consider the special case where \u039b = I.", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "The method due to Yu and Shi (see Yu [23] and Yu and Shi [24]) to find X \u2208 K and Q = R\u039b with R \u2208 O(K) and \u039b diagonal invertible that minimize \u03c6(X,Q) = \u2016X \u2212 ZQ\u2016F is to alternate steps during which either Q is held fixed (step PODX) or X is held fixed (step PODR), except that Yu and Shi consider the special case where \u039b = I.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "The method advocated by Yu [23] is to pick K rows of Z that are as orthogonal to each other as possible and to make a matrix R whose columns consist of these rows", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "The algorithm given in Yu [23] needs a small correction, because rows are not removed from Z when they are added to R, which may cause the same row to be added several times to R.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "Such graphs (with weights (\u22121, 0,+1)) were introduced as early as 1953 by Harary [12], to model social relations involving disliking, indifference, and liking.", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "A simple remedy is to use the absolute values of the weights in the degree matrix! This idea applied to signed graph with weights (\u22121, 0, 1) occurs in Hou [14].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "Kolluri, Shewchuk and O\u2019Brien [15] take the natural step of using absolute values of weights in the degree matrix in their original work on surface reconstruction from noisy point clouds.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "[16] appear to be the", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The first method consists in defining a notion of incidence matrix for a signed graph, and appears in Hou [14].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "[16] deal with a different notion of cut, namely ratio cut (in which vol(A) is replaced by the size |A| of A), and only for two clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Correlation clustering was first introduced and studied for complete graphs by Bansal, Blum and Chawla [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "Demaine and Immorlica [5] consider the same problem for arbitrary weighted graphs, and they give an O(log n)-approximation algorithm based on linear programming.", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": "[16], it is also possible to characterize for which signed graphs the Laplacian L is positive definite.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Such graphs are \u201ccousins\u201d of bipartite graphs and were introduced by Harary [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "The following proposition was first proved by Harary [12].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "[16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Hou [14] gives bounds on the smallest eigenvalue of an unbalanced graph.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "4 in Hou [14]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16], if our goal is to draw a signed graph G = (V,W ) with m nodes, a natural way to interpret negative weights is to assume that the endpoints vi and vj of an edge with a negative weight should be placed far apart, which can be achieved if instead of assigning the point \u03c1(vj) \u2208 R to vj, we assign the point \u2212\u03c1(vj).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": ", un) (for a proof, see Gallier [8]).", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "This section relies heavily on Gallot, Hulin, Lafontaine [9] and Lee [17], which contain thorough expositions and should be consulted for details.", "startOffset": 57, "endOffset": 60}, {"referenceID": 14, "context": "This section relies heavily on Gallot, Hulin, Lafontaine [9] and Lee [17], which contain thorough expositions and should be consulted for details.", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "The following proposition gives necessary and sufficient conditions for a discrete group to act freely and properly often found in the literature (for instance, O\u2019Neill [18], Berger and Gostiaux [3], and do Carmo [6], but beware that in this last reference Hausdorff separation is not required!).", "startOffset": 169, "endOffset": 173}, {"referenceID": 2, "context": "The following proposition gives necessary and sufficient conditions for a discrete group to act freely and properly often found in the literature (for instance, O\u2019Neill [18], Berger and Gostiaux [3], and do Carmo [6], but beware that in this last reference Hausdorff separation is not required!).", "startOffset": 195, "endOffset": 198}, {"referenceID": 14, "context": "\u201d However, as pointed out by Lee ([17], just before Proposition 9.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "For a proof, see Gallot, Hulin, Lafontaine [9] (Theorem 1.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "88) or Lee [17] (Theorem 9.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "For a complete proof see Gallot, Hulin, Lafontaine [9] (Proposition 2.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "4 can be found in Gallot, Hulin, Lafontaine [9] (Proposition 2.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "Now, if (M, g) is a connected Riemannian manifold, recall that we define the distance d(p, q) between two points p, q \u2208M as d(p, q) = inf{L(\u03b3) | \u03b3 : [0, 1]\u2192M}, where \u03b3 is any piecewise C-curve from p to q, and", "startOffset": 149, "endOffset": 155}, {"referenceID": 7, "context": "The Hopf-Rinow Theorem (see Gallot, Hulin, Lafontaine [9], Theorem 2.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "d(p, q) = inf{L(\u03b3) | \u03b3 : [0, 1]\u2192M is a geodesic}.", "startOffset": 25, "endOffset": 31}], "year": 2016, "abstractText": "This is a survey of the method of graph cuts and its applications to graph clustering of weighted unsigned and signed graphs. I provide a fairly thorough treatment of the method of normalized graph cuts, a deeply original method due to Shi and Malik, including complete proofs. I also cover briefly the method of ratio cuts, and show how it can be viewed as a special case of normalized cuts. I include the necessary background on graphs and graph Laplacians. I then explain in detail how the eigenvectors of the graph Laplacian can be used to draw a graph. This is an attractive application of graph Laplacians. The main thrust of this paper is the method of normalized cuts. I give a detailed account for K = 2 clusters, and also for K > 2 clusters, based on the work of Yu and Shi. I also show how both graph drawing and normalized cut K-clustering can be easily generalized to handle signed graphs, which are weighted graphs in which the weight matrix W may have negative coefficients. Intuitively, negative coefficients indicate distance or dissimilarity. The solution is to replace the degree matrix D by the matrix D in which absolute values of the weights are used, and to replace the Laplacian L = D \u2212W by the signed Laplacian L = D \u2212W . The signed Laplacian L is always positive semidefinite, and it may be positive definite (for unbalanced graphs, see Chapter 5). As far as I know, the generalization of K-way normalized clustering to signed graphs is new. Finally, I show how the method of ratio cuts, in which a cut is normalized by the size of the cluster rather than its volume, is just a special case of normalized cuts. All that needs to be done is to replace the normalized Laplacian Lsym by the unormalized Laplacian L. This is also true for signed graphs (where we replace Lsym by L). Three points that do not appear to have been clearly articulated before are elaborated: 1. The solutions of the main optimization problem should be viewed as tuples in the K-fold cartesian product of projective space RPN\u22121. 2. When K > 2, the solutions of the relaxed problem should be viewed as elements of the Grassmannian G(K,N). 3. Two possible Riemannian distances are available to compare the closeness of solutions: (a) The distance on (RPN\u22121)K . (b) The distance on the Grassmannian. I also clarify what should be the necessary and sufficient conditions for a matrix to represent a partition of the vertices of a graph to be clustered.", "creator": "LaTeX with hyperref package"}}}