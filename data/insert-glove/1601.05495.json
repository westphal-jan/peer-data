{"id": "1601.05495", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2016", "title": "Data-driven Rank Breaking for Efficient Rank Aggregation", "abstract": "sugita Rank aggregation supertouring systems egyptienne collect espnu ordinal expressivity preferences from tourist individuals tubal to produce a global ranking that bille represents tabc the manbij social preference. casilli Rank - gramado breaking is interviewers a viduidae common oswald practice fructus to post-master reduce balian the t.c. computational complexity petitjean of learning the vellum global borjomi ranking. The t-72 individual preferences are broken chika into yakis pairwise comparisons dusseldorf and applied to efficient miniaturization algorithms crupi tailored for independent timbuktu paired comparisons. However, due to squarepants the lipu ignored dependencies garbin in the data, naive 4,404 rank - gardyne breaking approaches tamc can clock-tower result polari in chatumongol inconsistent ccid estimates. wight The key goldsberry idea aparece to fd produce papillote accurate and mcnuggets unbiased estimates is to treat braingames the presidents pairwise dartford comparisons silcock unequally, quatuor depending ferromagnet on the topology misinterpretation of the 10.66 collected data. In kiwirail this hanya paper, we 0.005 provide the reprinted optimal post-treatment rank - muds breaking estimator, biochemically which not lampo only achieves consistency cherchesov but also javanese achieves spot-on the best kekilli error ewww bound. civility This allows 2,416 us illiano to characterize the fundamental tradeoff chillout between accuracy espn3 and complexity. hetmanate Further, the analysis identifies kolbert how the accuracy depends on advocate-general the horlock spectral wicbc gap hualong of ozio a meyer-landrut corresponding vujacic comparison boordy graph.", "histories": [["v1", "Thu, 21 Jan 2016 02:39:39 GMT  (182kb,D)", "https://arxiv.org/abs/1601.05495v1", "46 pages, 12 figures"], ["v2", "Fri, 7 Oct 2016 14:28:42 GMT  (189kb,D)", "http://arxiv.org/abs/1601.05495v2", "46 pages, 12 figures"]], "COMMENTS": "46 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ashish khetan", "sewoong oh"], "accepted": true, "id": "1601.05495"}, "pdf": {"name": "1601.05495.pdf", "metadata": {"source": "CRF", "title": "Data-driven Rank Breaking for Efficient Rank Aggregation", "authors": ["Ashish Khetan", "Sewoong Oh"], "emails": ["khetan2@illinois.edu", "swoh@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "In several applications such as electing officials, choosing policies, or making recommendations, we are given partial preferences from individuals over a set of alternatives, with the goal of producing a global ranking that represents the collective preference of the population or the society. This process is referred to as rank aggregation. One popular approach is learning to rank. Economists have modeled each individual as a rational being maximizing his/her perceived utility. Parametric probabilistic models, known collectively as Random Utility Models (RUMs), have been proposed to model such individual choices and preferences [40]. This allows one to infer the global ranking by learning the inherent utility from individuals\u2019 revealed preferences, which are noisy manifestations of the underlying true utility of the alternatives.\nTraditionally, learning to rank has been studied under the following data collection scenarios: pairwise comparisons, best-out-of-k comparisons, and k-way comparisons. Pairwise comparisons are commonly studied in the classical context of sports matches as well as more recent applications in crowdsourcing, where each worker is presented with a pair of choices and asked to choose the more favorable one. Best-out-of-k comparisons data sets are commonly available from purchase history of customers. Typically, a set of k alternatives are offered among which one is chosen or purchased by each customer. This has been widely studied in operations research in the context of modeling customer choices for revenue management and assortment optimization. The k-way comparisons are assumed in traditional rank aggregation scenarios, where each person reveals his/her preference as a ranked list over a set of k items. In some real-world elections, voters provide ranked preferences over the whole set of candidates [36]. We refer to these three types of ordinal data collection scenarios as \u2018traditional\u2019 throughout this paper.\nFor such traditional data sets, there are several computationally efficient inference algorithms for finding the Maximum Likelihood (ML) estimates that provably achieve the minimax optimal performance [44, 52, 26]. However, modern data sets can be unstructured. Individual\u2019s revealed ordinal preferences can be implicit, such as movie ratings, time spent on the news articles, and whether the user finished watching the movie or not. In crowdsourcing, it has also been observed that humans are more efficient at performing batch comparisons [24], as opposed to providing the full ranking or choosing the top item. This calls for more flexible approaches for rank aggregation that can take such diverse forms of ordinal data into account. For\nar X\niv :1\n60 1.\n05 49\n5v 2\n[ cs\n.L G\n] 7\nO ct\n2 01\n6\nsuch non-traditional data sets, finding the ML estimate can become significantly more challenging, requiring run-time exponential in the problem parameters.\nTo avoid such a computational bottleneck, a common heuristic is to resort to rank-breaking. The collected ordinal data is first transformed into a bag of pairwise comparisons, ignoring the dependencies that were present in the original data. This is then processed via existing inference algorithms tailored for independent pairwise comparisons, hoping that the dependency present in the input data does not lead to inconsistency in estimation. This idea is one of the main motivations for numerous approaches specializing in learning to rank from pairwise comparisons, e.g., [22, 45, 4]. However, such a heuristic of full rank-breaking defined explicitly in (1), where all pairwise comparisons are weighted and treated equally ignoring their dependencies, has been recently shown to introduce inconsistency [5].\nThe key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. A fundamental question of interest to practitioners is how to choose the weight of each pairwise comparison in order to achieve not only consistency but also the best accuracy, among those consistent estimators using rank-breaking. We study how the accuracy of resulting estimate depends on the topology of the data and the weights on the pairwise comparisons. This provides a guideline for the optimal choice of the weights, driven by the topology of the data, that leads to accurate estimates.\nProblem formulation. The premise in the current race to collect more data on user activities is that, a hidden true preference manifests in the user\u2019s activities and choices. Such data can be explicit, as in ratings, ranked lists, pairwise comparisons, and like/dislike buttons. Others are more implicit, such as purchase history and viewing times. While more data in general allows for a more accurate inference, the heterogeneity of user activities makes it difficult to infer the underlying preferences directly. Further, each user reveals her preference on only a few contents.\nTraditional collaborative filtering fails to capture the diversity of modern data sets. The sparsity and heterogeneity of the data renders typical similarity measures ineffective in the nearest-neighbor methods. Consequently, simple measures of similarity prevail in practice, as in Amazon\u2019s \u201cpeople who bought ... also bought ...\u201d scheme. Score-based methods require translating heterogeneous data into numeric scores, which is a priori a difficult task. Even if explicit ratings are observed, those are often unreliable and the scale of such ratings vary from user to user.\nWe propose aggregating ordinal data based on users\u2019 revealed preferences that are expressed in the form of partial orderings (notice that our use of the term is slightly different from its original use in revealed preference theory). We interpret user activities as manifestation of the hidden preferences according to discrete choice models (in particular the Plackett-Luce model defined in (1)). This provides a more reliable, scale-free, and widely applicable representation of the heterogeneous data as partial orderings, as well as a probabilistic interpretation of how preferences manifest. In full generality, the data collected from each individual can be represented by a partially ordered set (poset). Assuming consistency in a user\u2019s revealed preferences, any ordered relations can be seamlessly translated into a poset, represented as a Hasse diagram by a directed acyclic graph (DAG). The DAG below represents ordered relations a > {b, d}, b > c, {c, d} > e, and e > f . For example, this could have been translated from two sources: a five star rating on a and a three star ratings on b, c, d, a two star rating on e, and a one star rating on f ; and the item b being purchased after reviewing c as well.\nThere are n users or agents, and each agent j provides his/her ordinal evaluation on a subset Sj of d items or alternatives. We refer to Sj \u2282 {1, 2, . . . , d} as offerings provided to j, and use \u03baj = |Sj | to denote the size of the offerings. We assume that the partial ordering over the offerings is a manifestation of her preferences as per a popular choice model known as Plackett-Luce (PL) model. As we explain in detail below, the PL model produces total orderings (rather than partial ones). The data collector queries each user for a partial ranking in the form of a poset over Sj . For example, the data collector can ask for the top item, unordered subset of three next preferred items, the fifth item, and the least preferred item. In this case, an example of such poset could be a < {b, c, d} < e < f , which could have been generated from a total ordering produced by the PL model and taking the corresponding partial ordering from the total ordering. Notice\nthat we fix the topology of the DAG first and ask the user to fill in the node identities corresponding to her total ordering as (randomly) generated by the PL model. Hence, the structure of the poset is considered deterministic, and only the identity of the nodes in the poset is considered random. Alternatively, one could consider a different scenario where the topology of the poset is also random and depends on the outcome of the preference, which is out-side the scope of this paper and provides an interesting future research direction.\nThe PL model is a special case of random utility models, defined as follows [56, 6]. Each item i has a real-valued latent utility \u03b8i. When presented with a set of items, a user\u2019s reveled preference is a partial ordering according to noisy manifestation of the utilities, i.e. i.i.d. noise added to the true utility \u03b8i\u2019s. The PL model is a special case where the noise follows the standard Gumbel distribution, and is one of the most popular model in social choice theory [39, 41]. PL has several important properties, making this model realistic in various domains, including marketing [25], transportation [40, 7], biology [55], and natural language processing [42]. Precisely, each user j, when presented with a set Sj of items, draws a noisy utility of each item i according to\nui = \u03b8i + Zi ,\nwhere Zi\u2019s follow the independent standard Gumbel distribution. Then we observe the ranking resulting from sorting the items as per noisy observed utilities uj \u2019s. Alternatively, the PL model is also equivalent to the following random process. For a set of alternatives Sj , a ranking \u03c3j : [|S|] \u2192 S is generated in two steps: (1) independently assign each item i \u2208 Sj an unobserved value Xi, exponentially distributed with mean e\u2212\u03b8i ; (2) select a ranking \u03c3j so that X\u03c3j(1) \u2264 X\u03c3j(2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 X\u03c3j(|Sj |).\nThe PL model (i) satisfies Luce\u2019s \u2018independence of irrelevant alternatives\u2019 in social choice theory [51], and has a simple characterization as sequential (random) choices as explained below; and (ii) has a maximum likelihood estimator (MLE) which is a convex program in \u03b8 in the traditional scenarios of pairwise, best-outof-k and k-way comparisons. Let P(a > {b, c, d}) denote the probability a was chosen as the best alternative among the set {a, b, c, d}. Then, the probability that a user reveals a linear order (a > b > c > d) is equivalent as making sequential choice from the top to bottom:\nP(a > b > c > d) = P(a > {b, c, d}) P(b > {c, d}) P(c > d)\n= e\u03b8a\n(e\u03b8a + e\u03b8b + e\u03b8c + e\u03b8d)\ne\u03b8b\n(e\u03b8b + e\u03b8c + e\u03b8d)\ne\u03b8c\n(e\u03b8c + e\u03b8d) .\nWe use the notation (a > b) to denote the event that a is preferred over b. In general, for user j presented with offerings Sj , the probability that the revealed preference is a total ordering \u03c3j is P(\u03c3j) =\u220f i\u2208{1,...,\u03baj\u22121}(e \u03b8\u03c3\u22121(i))/( \u2211\u03baj i\u2032=i e \u03b8\u03c3\u22121(i\u2032)). We consider the true utility \u03b8\u2217 \u2208 \u2126b, where we define \u2126b as\n\u2126b \u2261 { \u03b8 \u2208 Rd \u2223\u2223 \u2211 i\u2208[d] \u03b8i = 0 , |\u03b8i| \u2264 b for all i \u2208 [d] } .\nNote that by definition, the PL model is invariant under shifting the utility \u03b8i\u2019s. Hence, the centering ensures uniqueness of the parameters for each PL model. The bound b on the dynamic range is not a restriction, but is written explicitly to capture the dependence of the accuracy in our main results.\nWe have n users each providing a partial ordering of a set of offerings Sj according to the PL model. Let Gj denote both the DAG representing the partial ordering from user j\u2019s preferences. With a slight abuse of notations, we also let Gj denote the set of rankings that are consistent with this DAG. For general partial orderings, the probability of observing Gj is the sum of all total orderings that is consistent with the observation, i.e. P(Gj) = \u2211 \u03c3\u2208Gj P(\u03c3). The goal is to efficiently learn the true utility \u03b8\n\u2217 \u2208 \u2126b, from the n sampled partial orderings. One popular approach is to compute the maximum likelihood estimate (MLE) by solving the following optimization:\nmaximize \u03b8\u2208\u2126b\nn\u2211 j=1 logP(Gj) .\nThis optimization is a simple convex optimization, in particular a logit regression, when the structure of the data {Gj}j\u2208[n] is traditional. This is one of the reasons the PL model is attractive. However, for general posets, this can be computationally challenging. Consider an example of position-p ranking, where each user provides which item is at p-th position in his/her ranking. Each term in the log-likelihood for this data involves summation over O((p\u2212 1)!) rankings, which takes O(n (p\u2212 1)!) operations to evaluate the objective function. Since p can be as large as d, such a computational blow-up renders MLE approach impractical. A common remedy is to resort to rank-breaking, which might result in inconsistent estimates.\nRank-breaking. Rank-breaking refers to the idea of extracting a set of pairwise comparisons from the observed partial orderings and applying estimators tailored for paired comparisons treating each piece of comparisons as independent. Both the choice of which paired comparisons to extract and the choice of parameters in the estimator, which we call weights, turns out to be crucial as we will show. Inappropriate selection of the paired comparisons can lead to inconsistent estimators as proved in [5], and the standard choice of the parameters can lead to a significantly suboptimal performance.\nA naive rank-breaking that is widely used in practice is to apply rank-breaking to all possible pairwise relations that one can read from the partial ordering and weighing them equally. We refer to this practice as full rank-breaking. In the example in Figure 1, full rank-breaking first extracts the bag of comparisons C = {(a > b), (a > c), (a > d), (a > e), (a > f), . . . , (e > f)} with 13 paired comparison outcomes, and apply the maximum likelihood estimator treating each paired outcome as independent. Precisely, the full rank-breaking estimator solves the convex optimization of\n\u03b8\u0302 \u2208 arg max \u03b8\u2208\u2126b \u2211 (i>i\u2032)\u2208C ( \u03b8i \u2212 log ( e\u03b8i + e\u03b8i\u2032 )) . (1)\nThere are several efficient implementation tailored for this problem [22, 28, 44, 37], and under the traditional scenarios, these approaches provably achieve the minimax optimal rate [26, 52]. For general non-traditional data sets, there is a significant gain in computational complexity. In the case of position-p ranking, where each of the n users report his/her p-th ranking item among \u03ba items, the computational complexity reduces from O(n (p\u2212 1)!) for the MLE in (1) to O(n p (\u03ba\u2212 p)) for the full rank-breaking estimator in (1). However, this gain comes at the cost of accuracy. It is known that the full-rank breaking estimator is inconsistent [5]; the error is strictly bounded away from zero even with infinite samples.\nPerhaps surprisingly, Azari Soufiani et al. [5] recently characterized the entire set of consistent rankbreaking estimators. Instead of using the bag of paired comparisons, the sufficient information for consistent rank-breaking is a set of rank-breaking graphs defined as follows.\nRecall that a user j provides his/her preference as a poset represented by a DAG Gj . Consistent rankbreaking first identifies all separators in the DAG. A node in the DAG is a separator if one can partition the rest of the nodes into two parts. A partition Atop which is the set of items that are preferred over the\nseparator item, and a partition Abottom which is the set of items that are less preferred than the separator item. One caveat is that we allow Atop to be empty, but Abottom must have at least one item. In the example in Figure 1, there are two separators: the item a and the item e. Using these separators, one can extract the following partial ordering from the original poset: (a > {b, c, d} > e > f). The items a and e separate the set of offerings into partitions, hence the name separator. We use `j to denote the number of separators in the poset Gj from user j. We let pj,a denote the ranked position of the a-th separator in the poset Gj , and we sort the positions such that pj,1 < pj,2 < . . . < pj,`j . The set of separators is denoted by Pj = {pj,1, pj,2, \u00b7 \u00b7 \u00b7 , pj,`j}. For example, since the separator a is ranked at position 1 and e is at the 5-th position, `j = 2, pj,1 = 1, and pj,2 = 5. Note that f is not a separator (whereas a is) since corresponding Abottom is empty.\nConveniently, we represent this extracted partial ordering using a set of DAGs, which are called rankbreaking graphs. We generate one rank-breaking graph per separator. A rank breaking graph Gj,a = (Sj , Ej,a) for user j and the a-th separator is defined as a directed graph over the set of offerings Sj , where we add an edge from a node that is less preferred than the a-th separator to the separator, i.e. Ej,a = {(i, i\u2032) | i\u2032 is the a-th separator, and \u03c3\u22121j (i) > pj,a}. Note that by the definition of the separator, Ej,a is a non-empty set. An example of rank-breaking graphs are shown in Figure 1.\nThis rank-breaking graphs were introduced in [4], where it was shown that the pairwise ordinal relations that is represented by edges in the rank-breaking graphs are sufficient information for using any estimation based on the idea of rank-breaking. Precisely, on the converse side, it was proved in [5] that any pairwise outcomes that is not present in the rank-breaking graphs Gj,a\u2019s lead to inconsistency for a general \u03b8\n\u2217. On the achievability side, it was proved that all pairwise outcomes that are present in the rank-breaking graphs give a consistent estimator, as long as all the paired comparisons in each Gj,a are weighted equally.\nIt should be noted that rank-breaking graphs are defined slightly differently in [4]. Specifically, [4] introduced a different notion of rank-breaking graph, where the vertices represent positions in total ordering. An edge between two vertices i1 and i2 denotes that the pairwise comparison between items ranked at position i1 and i2 is included in the estimator. Given such observation from the PL model, [4] and [5] prove that a rank-breaking graph is consistent if and only if it satisfies the following property. If a vertex i1 is connected to any vertex i2, where i2 > i1, then i1 must be connected to all the vertices i3 such that i3 > i1. Although the specific definitions of rank-breaking graphs are different from our setting, the mathematical analysis of [4] still holds when interpreted appropriately. Specifically, we consider only those rank-breaking that are consistent under the conditions given in [4]. In our rank-breaking graph Gj,a, a separator node is connected to all the other item nodes that are ranked below it (numerically higher positions).\nIn the algorithm described in (33), we satisfy this sufficient condition for consistency by restricting to a class of convex optimizations that use the same weight \u03bbj,a for all (\u03ba \u2212 pj,a) paired comparisons in the objective function, as opposed to allowing more general weights that defer from a pair to another pair in a rank-breaking graph Gj,a.\nAlgorithm. Consistent rank-breaking first identifies separators in the collected posets {Gj}j\u2208[n] and transform them into rank-breaking graphs {Gj,a}j\u2208[n],a\u2208[`j ] as explained above. These rank-breaking graphs are input to the MLE for paired comparisons, assuming all directed edges in the rank-breaking graphs are independent outcome of pairwise comparisons. Precisely, the consistent rank-breaking estimator solves the convex optimization of maximizing the paired log likelihoods\nLRB(\u03b8) = n\u2211 j=1 `j\u2211 a=1 \u03bbj,a { \u2211 (i,i\u2032)\u2208Ej,a ( \u03b8i\u2032 \u2212 log ( e\u03b8i + e\u03b8i\u2032 ))} , (2)\nwhere Ej,a\u2019s are defined as above via separators and different choices of the non-negative weights \u03bbj,a\u2019s are possible and the performance depends on such choices. Each weight \u03bbj,a determine how much we want to weigh the contribution of a corresponding rank-breaking graph Gj,a. We define the consistent rank-breaking\nestimate \u03b8\u0302 as the optimal solution of the convex program:\n\u03b8\u0302 \u2208 arg max \u03b8\u2208\u2126b LRB(\u03b8) . (3)\nBy changing how we weigh each rank-breaking graph (by choosing the \u03bbj,a\u2019s), the convex program (3) spans the entire set of consistent rank-breaking estimators, as characterized in [5]. However, only asymptotic consistency was known, which holds independent of the choice of the weights \u03bbj,a\u2019s. Naturally, a uniform choice of \u03bbj,a = \u03bb was proposed in [5].\nNote that this can be efficiently solved, since this is a simple convex optimization, in particular a logit regression, with only O( \u2211n j=1 `j \u03baj) terms. For a special case of position-p breaking, the O(n (p \u2212 1)!) complexity of evaluating the objective function for the MLE is now significantly reduced to O(n (\u03ba\u2212 p)) by rank-breaking. Given this potential exponential gain in efficiency, a natural question of interest is \u201cwhat is the price we pay in the accuracy?\u201d. We provide a sharp analysis of the performance of rank-breaking estimators in the finite sample regime, that quantifies the price of rank-breaking. Similarly, for a practitioner, a core problem of interest is how to choose the weights in the optimization in order to achieve the best accuracy. Our analysis provides a data-driven guideline for choosing the optimal weights.\nContributions. In this paper, we provide an upper bound on the error achieved by the rank-breaking estimator of (3) for any choice of the weights in Theorem 8. This explicitly shows how the error depends on the choice of the weights, and provides a guideline for choosing the optimal weights \u03bbj,a\u2019s in a data-driven manner. We provide the explicit formula for the optimal choice of the weights and provide the the error bound in Theorem 2. The analysis shows the explicit dependence of the error in the problem dimension d and the number of users n that matches the numerical experiments.\nIf we are designing surveys and can choose which subset of items to offer to each user and also can decide which type of ordinal data we can collect, then we want to design such surveys in a way to maximize the accuracy for a given number of questions asked. Our analysis provides how the accuracy depends on the topology of the collected data, and provides a guidance when we do have some control over which questions to ask and which data to collect. One should maximize the spectral gap of corresponding comparison graph. Further, for some canonical scenarios, we quantify the price of rank-breaking by comparing the error bound of the proposed data-driven rank-breaking with the lower bound on the MLE, which can have a significantly larger computational cost (Theorem 4).\nNotations. Following is a summary of all the notations defined above. We use d to denote the total number of items and index each item by i \u2208 {1, 2, . . . , d}. \u03b8 \u2208 \u2126b denotes vector of utilities associated with each item. \u03b8\u2217 represents true utility and \u03b8\u0302 denotes the estimated utility. We use n to denote the number of users/agents and index each user by j \u2208 {1, 2, . . . , n}. Sj \u2286 {1, . . . , d} refer to the offerings provided to the j-th user and we use \u03baj = |Sj | to denote the size of the offerings. Gj denote the DAG (Hasse diagram) representing the partial ordering from user j\u2019s preferences. Pj = {pj,1, pj,2, \u00b7 \u00b7 \u00b7 , pj,`j} denotes the set of separators in the DAG Gj , where pj,1, \u00b7 \u00b7 \u00b7 , pj,`j are the positions of the separators, and `j is the number of separators. Gj,a = (Sj , Ej,a) denote the rank-breaking graph for the a-th separator extracted from the partial ordering Gj of user j.\nFor any positive integer N , let [N ] = {1, \u00b7 \u00b7 \u00b7 , N}. For a ranking \u03c3 over S, i.e., \u03c3 is a mapping from [|S|] to S, let \u03c3\u22121 denote the inverse mapping.For a vector x, let \u2016x\u20162 denote the standard l2 norm. Let 1 denote the all-ones vector and 0 denote the all-zeros vector with the appropriate dimension. Let Sd denote the set of d\u00d7 d symmetric matrices with real-valued entries. For X \u2208 Sd, let \u03bb1(X) \u2264 \u03bb2(X) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbd(X) denote its eigenvalues sorted in increasing order. Let Tr(X) = \u2211d i=1 \u03bbi(X) denote its trace and \u2016X\u2016 = max{|\u03bb1(X)|, |\u03bbd(X)|} denote its spectral norm. For two matrices X,Y \u2208 Sd, we write X Y if X \u2212 Y is positive semi-definite, i.e., \u03bb1(X \u2212 Y ) \u2265 0. Let ei denote a unit vector in Rd along the i-th direction."}, {"heading": "2 Comparisons Graph and the Graph Laplacian", "text": "In the analysis of the convex program (3), we show that, with high probability, the objective function is strictly concave with \u03bb2(H(\u03b8)) \u2264 \u2212Cb \u03b3 \u03bb2(L) < 0 (Lemma 11) for all \u03b8 \u2208 \u2126b and the gradient is bounded by \u2016\u2207LRB(\u03b8\u2217)\u20162 \u2264 C \u2032b \u221a log d \u2211 j\u2208[n] `j (Lemma 10). Shortly, we will define \u03b3 and \u03bb2(L), which captures the dependence on the topology of the data, and C \u2032b and Cb are constants that only depend on b. Putting these together, we will show that there exists a \u03b8 \u2208 \u2126b such that\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 2\u2016\u2207LRB(\u03b8\u2217)\u20162 \u2212\u03bb2(H(\u03b8)) \u2264 C \u2032\u2032b\n\u221a log d \u2211 j\u2208[n] `j\n\u03b3 \u03bb2(L) .\nHere \u03bb2(H(\u03b8)) denotes the second largest eigenvalue of a negative semi-definite Hessian matrix H(\u03b8) of the objective function. The reason the second largest eigenvalue shows up is because the top eigenvector is always the all-ones vector which by the definition of \u2126b is infeasible. The accuracy depends on the topology of the collected data via the comparison graph of given data.\nDefinition 1. (Comparison graph H). We define a graph H([d], E) where each alternative corresponds to a node, and we put an edge (i, i\u2032) if there exists an agent j whose offerings is a set Sj such that i, i\n\u2032 \u2208 Sj. Each edge (i, i\u2032) \u2208 E has a weight Aii\u2032 defined as\nAii\u2032 = \u2211\nj\u2208[n]:i,i\u2032\u2208Sj\n`j \u03baj(\u03baj \u2212 1) ,\nwhere \u03baj = |Sj | is the size of each sampled set and `j is the number of separators in Sj defined by rankbreaking in Section 1.\nDefine a diagonal matrix D = diag(A1), and the corresponding graph Laplacian L = D \u2212A, such that\nL = n\u2211 j=1 `j \u03baj(\u03baj \u2212 1) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>. (4)\nLet 0 = \u03bb1(L) \u2264 \u03bb2(L) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbd(L) denote the (sorted) eigenvalues of L. Of special interest is \u03bb2(L), also called the spectral gap, which measured how well-connected the graph is. Intuitively, one can expect better accuracy when the spectral gap is larger, as evidenced in previous learning to rank results in simpler settings [45, 52, 26]. This is made precise in (4), and in the main result of Theorem 2, we appropriately rescale the spectral gap and use \u03b1 \u2208 [0, 1] defined as\n\u03b1 \u2261 \u03bb2(L)(d\u2212 1) Tr(L)\n= \u03bb2(L)(d\u2212 1)\u2211n\nj=1 `j . (5)\nThe accuracy also depends on the topology via the maximum weighted degree defined asDmax \u2261 maxi\u2208[d]Dii = maxi\u2208[d]{ \u2211 j:i\u2208Sj `j/\u03baj}. Note that the average weighted degree is \u2211 iDii/d = Tr(L)/d, and we rescale it by Dmax such that\n\u03b2 \u2261 Tr(L) dDmax =\n\u2211n j=1 `j\ndDmax . (6)\nWe will show that the performance of rank breaking estimator depends on the topology of the graph through these two parameters. The larger the spectral gap \u03b1 the smaller error we get with the same effective sample size. The degree imbalance \u03b2 \u2208 [0, 1] determines how many samples are required for the analysis to hold. We need smaller number of samples if the weighted degrees are balanced, which happens if \u03b2 is large (close to one).\nThe following quantity also determines the convexity of the objective function.\n\u03b3 \u2261 min j\u2208[n]\n{( 1\u2212\npj,`j \u03baj\n)d2e2be\u22122} . (7)\nNote that \u03b3 is between zero and one, and a larger value is desired as the objective function becomes more concave and a better accuracy follows. When we are collecting data where the size of the offerings \u03baj \u2019s are increasing with d but the position of the separators are close to the top, such that \u03baj = \u03c9(d) and pj,`j = O(1), then for b = O(1) the above quantity \u03b3 can be made arbitrarily close to one, for large enough problem size d. On the other hand, when pj,`j is close to \u03baj , the accuracy can degrade significantly as stronger alternatives might have small chance of showing up in the rank breaking. The value of \u03b3 is quite sensitive to b. The reason we have such a inferior dependence on b is because we wanted to give a universal bound on the Hessian that is simple. It is not difficult to get a tighter bound with a larger value of \u03b3, but will inevitably depend on the structure of the data in a complicated fashion.\nTo ensure that the (second) largest eigenvalue of the Hessian is small enough, we need enough samples. This is captured by \u03b7 defined as\n\u03b7 \u2261 max j\u2208[n] {\u03b7j} , where \u03b7j = \u03baj max{`j , \u03baj \u2212 pj,`j} . (8)\nNote that 1 < \u03b7j \u2264 \u03baj/`j . A smaller value of \u03b7 is desired as we require smaller number of samples, as shown in Theorem 2. This happens, for instance, when all separators are at the top, such that pj,`j = `j and \u03b7j = \u03baj/(\u03baj \u2212 `j), which is close to one for large \u03baj . On the other hand, when all separators are at the bottom of the list, then \u03b7 can be as large as \u03baj .\nWe discuss the role of the topology of data captures by these parameters in Section 4."}, {"heading": "3 Main Results", "text": "We present the main theoretical results accompanied by corresponding numerical simulations in this section."}, {"heading": "3.1 Upper Bound on the Achievable Error", "text": "We present the main result that provides an upper bound on the resulting error and explicitly shows the dependence on the topology of the data. As explained in Section 1, we assume that each user provides a partial ranking according to his/her position of the separators. Precisely, we assume the set of offerings Sj , the number of separators `j , and their respective positions Pj = {pj,1, . . . , pj,`j} are predetermined. Each user draws the ranking of items from the PL model, and provides the partial ranking according to the separators of the form of {a > {b, c, d} > e > f} in the example in the Figure 1.\nTheorem 2. Suppose there are n users, d items parametrized by \u03b8\u2217 \u2208 \u2126b, each user j is presented with a set of offerings Sj \u2286 [d], and provides a partial ordering under the PL model. When the effective sample size\u2211n j=1 `j is large enough such that\nn\u2211 j=1 `j \u2265 211e18b\u03b7 log(`max + 2) 2 \u03b12\u03b32\u03b2 d log d , (9)\nwhere b \u2261 maxi |\u03b8\u2217i | is the dynamic range, `max \u2261 maxj\u2208[n] `j, \u03b1 is the (rescaled) spectral gap defined in (5), \u03b2 is the (rescaled) maximum degree defined in (6), \u03b3 and \u03b7 are defined in Eqs. (7) and (8), then the rank-breaking estimator in (3) with the choice of\n\u03bbj,a = 1\n\u03baj \u2212 pj,a , (10)\nfor all a \u2208 [`j ] and j \u2208 [n] achieves\n1\u221a d\n\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225 2 \u2264 4 \u221a 2e4b(1 + e2b)2\n\u03b1\u03b3\n\u221a d log d\u2211n j=1 `j , (11)\nwith probability at least 1\u2212 3e3d\u22123.\nConsider an ideal case where the spectral gap is large such that \u03b1 is a strictly positive constant and the dynamic range b is finite and maxj\u2208[n] pj,`j/\u03baj = C for some constant C < 1 such that \u03b3 is also a constant independent of the problem size d. Then the upper bound in (11) implies that we need the effective sample size to scale as O(d log d), which is only a logarithmic factor larger than the number of parameters to be estimated. Such a logarithmic gap is also unavoidable and due to the fact that we require high probability bounds, where we want the tail probability to decrease at least polynomially in d. We discuss the role of the topology of the data in Section 4.\nThe upper bound follows from an analysis of the convex program similar to those in [44, 26, 52]. However, unlike the traditional data collection scenarios, the main technical challenge is in analyzing the probability that a particular pair of items appear in the rank-breaking. We provide a proof in Section 8.1.\nIn Figure 2 , we verify the scaling of the resulting error via numerical simulations. We fix d = 1024 and \u03baj = \u03ba = 128, and vary the number of separators `j = ` for fixed n = 128000 (left), and vary the number of samples n for fixed `j = ` = 16 (middle). Each point is average over 100 instances. The plot confirms that the mean squared error scales as 1/(` n). Each sample is a partial ranking from a set of \u03ba alternatives chosen uniformly at random, where the partial ranking is from a PL model with weights \u03b8\u2217 chosen i.i.d. uniformly over [\u2212b, b] with b = 2. To investigate the role of the position of the separators, we compare three scenarios. The top-`-separators choose the top ` positions for separators, the random-`-separators among top-half choose ` positions uniformly random from the top half, and the random-`-separators choose the positions uniformly at random. We observe that when the positions of the separators are well spread out among the \u03ba offerings, which happens for random-`-separators, we get better accuracy.\nThe figure on the right provides an insight into this trend for ` = 16 and n = 16000. The absolute error |\u03b8\u2217i \u2212 \u03b8\u0302i| is roughly same for each item i \u2208 [d] when breaking positions are chosen uniformly at random between 1 to \u03ba\u2212 1 whereas it is significantly higher for weak preference score items when breaking positions are restricted between 1 to \u03ba/2 or are top-`. This is due to the fact that the probability of each item being ranked at different positions is different, and in particular probability of the low preference score items being ranked in top-` is very small. The third figure is averaged over 1000 instances. Normalization constant C is n/d2 and 103`/d2 for the first and second figures respectively. For the first figure n is chosen relatively large such that n` is large enough even for ` = 1.\n3.2 The Price of Rank Breaking for the Special Case of Position-p Ranking\nRank-breaking achieves computational efficiency at the cost of estimation accuracy. In this section, we quantify this tradeoff for a canonical example of position-p ranking, where each sample provides the following information: an unordered set of p\u22121 items that are ranked high, one item that is ranked at the p-th position, and the rest of \u03baj \u2212 p items that are ranked on the bottom. An example of a sample with position-4 ranking six items {a, b, c, d, e, f} might be a partial ranking of ({a, b, d} > {e} > {c, f}). Since each sample has only one separator for 2 < p, Theorem 2 simplifies to the following Corollary.\nCorollary 3. Under the hypotheses of Theorem 2, there exist positive constants C and c that only depend on b such that if n \u2265 C(\u03b7d log d)/(\u03b12\u03b32\u03b2) then\n1\u221a d\n\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225 2 \u2264 c\n\u03b1\u03b3\n\u221a d log d\nn . (12)\nNote that the error only depends on the position p through \u03b3 and \u03b7, and is not sensitive. To quantify the price of rank-breaking, we compare this result to a fundamental lower bound on the minimax rate in Theorem 4. We can compute a sharp lower bound on the minimax rate, using the Crame\u0301r-Rao bound, and a proof is provided in Section 8.3.\nTheorem 4. Let U denote the set of all unbiased estimators of \u03b8\u2217 and suppose b > 0, then\ninf \u03b8\u0302\u2208U sup \u03b8\u2217\u2208\u2126b E[\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162] \u2265 1 2p log(\u03bamax)2 d\u2211 i=2 1 \u03bbi(L) \u2265 1 2p log(\u03bamax)2 (d\u2212 1)2 n ,\nwhere \u03bamax = maxj\u2208[n] |Sj | and the second inequality follows from the Jensen\u2019s inequality.\nNote that the second inequality is tight up to a constant factor, when the graph is an expander with a large spectral gap. For expanders, \u03b1 in the bound (12) is also a strictly positive constant. This suggests that rank-breaking gains in computational efficiency by a super-exponential factor of (p \u2212 1)!, at the price of increased error by a factor of p, ignoring poly-logarithmic factors.\n3.3 Tighter Analysis for the Special Case of Top-` Separators Scenario\nThe main result in Theorem 2 is general in the sense that it applies to any partial ranking data that is represented by positions of the separators. However, the bound can be quite loose, especially when \u03b3 is small, i.e. pj,`j is close to \u03baj . For some special cases, we can tighten the analysis to get a sharper bound. One caveat is that we use a slightly sub-optimal choice of parameters \u03bbj,a = 1/\u03baj instead of 1/(\u03baj\u2212a), to simplify the analysis and still get the order optimal error bound we want. Concretely, we consider a special case of top-` separators scenario, where each agent gives a ranked list of her most preferred `j alternatives among \u03baj offered set of items. Precisely, the locations of the separators are (pj,1, pj,2, . . . , pj,`j ) = (1, 2, . . . , `j). Theorem 5. Under the PL model, n partial orderings are sampled over d items parametrized by \u03b8\u2217 \u2208 \u2126b, where the j-th sample is a ranked list of the top-`j items among the \u03baj items offered to the agent. If\nn\u2211 j=1 `j \u2265 212e6b \u03b2\u03b12 d log d , (13)\nwhere b \u2261 maxi,i\u2032 |\u03b8\u2217i \u2212 \u03b8\u2217i\u2032 | and \u03b1, \u03b2 are defined in (5) and (6), then the rank-breaking estimator in (3) with the choice of \u03bbj,a = 1/\u03baj for all a \u2208 [`j ] and j \u2208 [n] achieves\n1\u221a d\n\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225 2 \u2264 16(1 + e 2b)2\n\u03b1\n\u221a d log d\u2211n j=1 `j , (14)\nwith probability at least 1\u2212 3e3d\u22123.\nA proof is provided in Section 8.4. In comparison to the general bound in Theorem 2, this is tighter since there is no dependence in \u03b3 or \u03b7. This gain is significant when, for example, pj,`j is close to \u03baj . As an extreme example, if all agents are offered the entire set of alternatives and are asked to rank all of them, such that \u03baj = d and `j = d \u2212 1 for all j \u2208 [n], then the generic bound in (11) is loose by a factor of (e4b/2 \u221a 2)dd2e\n2be\u22122, compared to the above bound. In the top-` separators scenario, the data set consists of the ranking among top-`j items of the set Sj ,\ni.e., [\u03c3j(1), \u03c3j(2), \u00b7 \u00b7 \u00b7 , \u03c3j(`j)]. The corresponding log-likelihood of the PL model is\nL(\u03b8) = n\u2211 j=1 `j\u2211 m=1 [ \u03b8\u03c3j(m) \u2212 log ( exp(\u03b8\u03c3j(m)) + exp(\u03b8\u03c3j(m+1)) + \u00b7 \u00b7 \u00b7+ exp(\u03b8\u03c3j(\u03baj)) )] , (15)\nwhere \u03c3j(a) is the alternative ranked at the a-th position by agent j. The Maximum Likelihood Estimator (MLE) for this traditional data set is efficient. Hence, there is no computational gain in rank-breaking. Consequently, there is no loss in accuracy either, when we use the optimal weights proposed in the above theorem. Figure 3 illustrates that the MLE and the data-driven rank-breaking estimator achieve performance that is identical, and improve over naive rank-breaking that uses uniform weights. We also compare performance of Generalized Method-of-Moments (GMM) proposed by [4] with our algorithm. In addition, we show that performance of GMM can be improved by optimally weighing pairwise comparisons with \u03bbj,a. MSE of GMM in both the cases, uniform weights and optimal weights, is larger than our rank-breaking estimator. However, GMM is on average about four times faster than our algorithm. We choose \u03bbj,a = 1/(\u03baj \u2212 a) in the simulations, as opposed to the 1/\u03baj assumed in the above theorem. This settles the question raised in [26] on whether it is possible to achieve optimal accuracy using rank-breaking under the top-` separators scenario. Analytically, it was proved in [26] that under the top-` separators scenario, naive rank-breaking with uniform weights achieves the same error bound as the MLE, up to a constant factor. However, we show that this constant factor gap is not a weakness of the analyses, but the choice of the weights. Theorem 5 provides a guideline for choosing the optimal weights, and the numerical simulation results in Figure 3 show that there is in fact no gap in practice, if we use the optimal weights. We use the same settings as that of the first figure of Figure 2 for the figure below.\nTo prove the order-optimality of the rank-breaking approach up to a constant factor, we can compare the upper bound to a Crame\u0301r-Rao lower bound on any unbiased estimators, in the following theorem. A proof is provided in Section 8.5.\nTheorem 6. Consider ranking {\u03c3j(i)}i\u2208[`j ] revealed for the set of items Sj, for j \u2208 [n]. Let U denote the set of all unbiased estimators of \u03b8\u2217 \u2208 \u2126b. If b > 0, then\ninf \u03b8\u0302\u2208U sup \u03b8\u2217\u2208\u2126b\nE[\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162] \u2265 ( 1\u2212 1\n`max `max\u2211 i=1\n1\n\u03bamax \u2212 i+ 1 )\u22121 d\u2211 i=2 1 \u03bbi(L) \u2265 (d\u2212 1) 2\u2211n j=1 `j , (16)\nwhere `max = maxj\u2208[n] `j and \u03bamax = maxj\u2208[n] \u03baj. The second inequality follows from the Jensen\u2019s inequality.\nConsider a case when the comparison graph is an expander such that \u03b1 is a strictly positive constant, and b = O(1) is also finite. Then, the Crame\u0301r-Rao lower bound show that the upper bound in (14) is optimal up to a logarithmic factor."}, {"heading": "3.4 Optimality of the Choice of the Weights", "text": "We propose the optimal choice of the weights \u03bbj,a\u2019s in Theorem 2. In this section, we show numerical simulations results comparing the proposed approach to other naive choices of the weights under various scenarios. We fix d = 1024 items and the underlying preference vector \u03b8\u2217 is uniformly distributed over [\u2212b, b] for b = 2. We generate n rankings over sets Sj of size \u03ba for j \u2208 [n] according to the PL model with parameter \u03b8\u2217. The comparison sets Sj \u2019s are chosen independently and uniformly at random from [d].\nFigure 4 illustrates that a naive choice of rank-breakings can result in inconsistency. We create partial orderings data set by fixing \u03ba = 128 and select ` = 8 random positions in {1, . . . , 127}. Each data set consists of partial orderings with separators at those 8 random positions, over 128 randomly chosen subset of items. We vary the sample size n and plot the resulting mean squared error for the two approaches. The data-driven rank-breaking, which uses the optimal choice of the weights, achieves error scaling as 1/n as predicted by Theorem 2, which implies consistency. For fair comparisons, we feed the same number of pairwise orderings to a naive rank-breaking estimator. This estimator uses randomly chosen pairwise orderings with uniform weights, and is generally inconsistent. However, when sample size is small, inconsistent estimators can achieve smaller variance leading to smaller error. Normalization constant C is 103`/d2, and each point is averaged over 100 trials. We use the minorization-maximization algorithm from [28] for computing the estimates from the rank-breakings.\nEven if we use the consistent rank-breakings first proposed in [5], there is ambiguity in the choice of the weights. We next study how much we gain by using the proposed optimal choice of the weights. The optimal choice, \u03bbj,a = 1/(\u03baj \u2212 pj,a), depends on two parameters: the size of the offerings \u03baj and the position of the separators pj,a. To distinguish the effect of these two parameters, we first experiment with fixed \u03baj = \u03ba and illustrate the gain of the optimal choice of \u03bbj,a\u2019s.\nFigure 5 illustrates that the optimal choice of the weights improves over consistent rank-breaking with uniform weights by a constant factor. We fix \u03ba = 128 and n = 128000. As illustrated by a figure on the right, the position of the separators are chosen such that there is one separator at position one, and the rest of `\u22121 separators are at the bottom. Precisely, (pj,1, pj,2, pj,3, . . . , pj,`) = (1, 128\u2212`+1, 128\u2212`+2, . . . , 127). We consider this scenario to emphasize the gain of optimal weights. Observe that the MSE does not decrease at a rate of 1/` in this case. The parameter \u03b3 which appears in the bound of Theorem 2 is very small when the breaking positions pj,a are of the order \u03baj as is the case here, when ` is small. Normalization constant C is n/d2.\nThe gain of optimal weights is significant when the size of Sj \u2019s are highly heterogeneous. Figure 6 compares performance of the proposed algorithm, for the optimal choice and uniform choice of weights \u03bbj,a\nwhen the comparison sets Sj \u2019s are of different sizes. We consider the case when n1 agents provide their top-`1 choices over the sets of size \u03ba1, and n2 agents provide their top-1 choice over the sets of size \u03ba2. We take n1 = 1024, `1 = 8, and n2 = 10n1`1. Figure 6 shows MSE for the two choice of weights, when we fix \u03ba1 = 128, and vary \u03ba2 from 2 to 128. As predicted from our bounds, when optimal choice of \u03bbj,a is used MSE is not sensitive to sample set sizes \u03ba2. The error decays at the rate proportional to the inverse of the effective sample size, which is n1`1 + n2`2 = 11n1`1. However, with \u03bbj,a = 1 when \u03ba2 = 2, the MSE is roughly 10 times worse. Which reflects that the effective sample size is approximately n1`1, i.e. pairwise comparisons coming from small set size do not contribute without proper normalization. This gap in MSE corroborates bounds of Theorem 8. Normalization constant C is 103/d2."}, {"heading": "4 The Role of the Topology of the Data", "text": "We study the role of topology of the data that provides a guideline for designing the collection of data when we do have some control, as in recommendation systems, designing surveys, and crowdsourcing. The core optimization problem of interest to the designer of such a system is to achieve the best accuracy while minimizing the number of questions."}, {"heading": "4.1 The Role of the Graph Laplacian", "text": "Using the same number of samples, comparison graphs with larger spectral gap achieve better accuracy, compared to those with smaller spectral gaps. To illustrate how graph topology effects the accuracy, we\nreproduce known spectral properties of canonical graphs, and numerically compare the performance of datadriven rank-breaking for several graph topologies. We follow the examples and experimental setup from [52] for a similar result with pairwise comparisons. Spectral properties of graphs have been a topic of wide interest for decades. We consider a scenario where we fix the size of offerings as \u03baj = \u03ba = O(1) and each agent provides partial ranking with ` separators, positions of which are chosen uniformly at random. The resulting spectral gap \u03b1 of different choices of the set Sj \u2019s are provided below. The total number edges in the comparisons graph (counting hyper-edges as multiple edges) is defined as |E| \u2261 ( \u03ba 2 ) n.\n\u2022 Complete graph: when |E| is larger than ( d 2 ) , we can design the comparison graph to be a complete\ngraph over d nodes. The weight Aii\u2032 on each edge is n `/(d(d \u2212 1)), which is the effective number of samples divided by twice the number of edges. Resulting spectral gap is one, which is the maximum possible value. Hence, complete graph is optimal for rank aggregation.\n\u2022 Sparse random graph: when we have limited resources we might not be able to afford a dense graph. When |E| is of order o(d2), we have a sparse graph. Consider a scenario where each set Sj is chosen uniformly at random. To ensure connectivity, we need n = \u2126(log d). Following standard spectral analysis of random graphs, we have \u03b1 = \u0398(1). Hence, sparse random graphs are near-optimal for rank-aggregation.\n\u2022 Chain graph: we consider a chain of sets of size \u03ba overlapping only by one item. For example, S1 = {1, . . . , \u03ba} and S2 = {\u03ba, \u03ba+1, . . . , 2\u03ba\u22121}, etc. We choose n to be a multiple of \u03c4 \u2261 (d\u22121)/(\u03ba\u22121) and offer each set n/\u03c4 times. The resulting graph is a chain of size \u03ba cliques, and standard spectral analysis shows that \u03b1 = \u0398(1/d2). Hence, a chain graph is strictly sub-optimal for rank aggregation.\n\u2022 Star-like graph: We choose one item to be the center, and every offer set consists of this center node and a set of \u03ba\u2212 1 other nodes chosen uniformly at random without replacement. For example, center node = {1}, S1 = {1, 2, . . . , \u03ba} and S2 = {1, \u03ba+1, \u03ba+2, . . . , 2\u03ba\u22121}, etc. n is chosen in the way similar to that of the Chain graph. Standard spectral analysis shows that \u03b1 = \u0398(1) and star-like graphs are near-optimal for rank-aggregation.\n\u2022 Barbell-like graph: We select an offering S = {S\u2032, i, j}, |S\u2032| = \u03ba \u2212 2 uniformly at random and divide rest of the items into two groups V1 and V2. We offer set S n\u03ba/d times. For each offering of set S, we offer d/\u03ba\u2212 1 sets chosen uniformly at random from the two groups {V1, i} and {V2, j}. The resulting graph is a barbell-like graph, and standard spectral analysis shows that \u03b1 = \u0398(1/d2). Hence, a chain graph is strictly sub-optimal for rank aggregation.\nFigure 7 illustrates how graph topology effects the accuracy. When \u03b8\u2217 is chosen uniformly at random, the accuracy does not change with d (left), and the accuracy is better for those graphs with larger spectral gap. However, for a certain worst-case \u03b8\u2217, the error increases with d for the chain graph and the barbell-like graph, as predicted by the above analysis of the spectral gap. We use ` = 4, \u03ba = 17 and vary d from 129 to 2049. \u03ba is kept small to make the resulting graphs more like the above discussed graphs. Figure on left shows accuracy when \u03b8\u2217 is chosen i.i.d. uniformly over [\u2212b, b] with b = 2. Error in this case is roughly same for each of the graph topologies with chain graph being the worst. However, when \u03b8\u2217 is chosen carefully error for chain graph and barbell-like graph increases with d as shown in the figure right. We chose \u03b8\u2217 such that all the items of a set have same weight, either \u03b8i = 0 or \u03b8i = b for chain graph and barbell-like graph. We divide all the sets equally between the two types for chain graph. For barbell-like graph, we keep the two types of sets on the two different sides of the connector set and equally divide items of the connector set into two types. Number of samples n is 100(d \u2212 1)/(\u03ba \u2212 1) and each point is averaged over 100 instances. Normalization constant C is n`/d2."}, {"heading": "4.2 The Role of the Position of the Separators", "text": "As predicted by theorem 2, rank-breaking fails when \u03b3 is small, i.e. the position of the separators are very close to the bottom. An extreme example is the bottom-` separators scenario, where each person is offered \u03ba randomly chosen alternatives, and is asked to give a ranked list of bottom ` alternatives. In other words, the ` separators are placed at (pj,1, . . . , pj,`) = (\u03baj \u2212 `, . . . , \u03ba \u2212 1). In this case, \u03b3 ' 0 and the error bound is large. This is not a weakness of the analysis. In fact we observe large errors under this scenario. The reason is that many alternatives that have large weights \u03b8i\u2019s will rarely be even compared once, making any reasonable estimation infeasible.\nFigure 8 illustrates this scenario. We choose ` = 8, \u03ba = 128, and d = 1024. The other settings are same as that of the first figure of Figure 2. The left figure plots the magnitude of the estimation error for each item. For about 200 strong items among 1024, we do not even get a single comparison, hence we omit any estimation error. It clearly shows the trend: we get good estimates for about 400 items in the bottom, and we get large errors for the rest. Consequently, even if we only take those items that have at least one comparison into account, we still get large errors. This is shown in the figure right. The error barely decays with the sample size. However, if we focus on the error for the bottom 400 items, we get good error rate decaying inversely with the sample size. Normalization constant C in the second figure is 102 x d/` and 102(400)d/` for the first and second lines respectively, where x is the number of items that appeared in rank-breaking at least once. We solve convex program (3) for \u03b8 restricted to the items that appear in rank-breaking at least once. The second figure of Figure 8 is averaged over 1000 instances.\nWe make this observation precise in the following theorem. Applying rank-breaking to only to those weakest d\u0303 items, we prove an upper bound on the achieved error rate that depends on the choice of the d\u0303. Without loss of generality, we suppose the items are sorted such that \u03b8\u22171 \u2264 \u03b8\u22172 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b8\u2217d. For a choice of d\u0303 = `d/(2\u03ba), we denote the weakest d\u0303 items by \u03b8\u0303\u2217 \u2208 Rd\u0303 such that \u03b8\u0303\u2217i = \u03b8\u2217i \u2212 (1/d\u0303) \u2211d\u0303 i\u2032=1 \u03b8 \u2217 i\u2032 , for i \u2208 [d\u0303]. Since \u03b8\u2217 \u2208 \u2126b, \u03b8\u0303\u2217 \u2208 [\u22122b, 2b]d\u0303. The space of all possible preference vectors for [d\u0303] items is given by \u2126\u0303 = {\u03b8\u0303 \u2208 Rd\u0303 : \u2211d\u0303 i=1 \u03b8\u0303i = 0} and \u2126\u03032b = \u2126\u0303 \u2229 [\u22122b, 2b]d\u0303.\nAlthough the analysis can be easily generalized, to simplify notations, we fix \u03baj = \u03ba and `j = ` and assume that the comparison sets Sj , |Sj | = \u03ba, are chosen uniformly at random from the set of d items for all j \u2208 [n]. The rank-breaking log likelihood function LRB(\u03b8\u0303) for the set of items [d\u0303] is given by\nLRB(\u03b8\u0303) = n\u2211 j=1 `j\u2211 a=1 \u03bbj,a { \u2211 (i,i\u2032)\u2208Ej,a I{ i,i\u2032\u2208[d\u0303] }(\u03b8i\u2032 \u2212 log (e\u03b8i + e\u03b8i\u2032))} . (17) We analyze the rank-breaking estimator\n\u0302\u0303 \u03b8 \u2261 max\n\u03b8\u0303\u2208\u2126\u03032b LRB(\u03b8\u0303) . (18)\nWe further simplify notations by fixing \u03bbj,a = 1, since from Equation (24), we know that the error increases by at most a factor of 4 due to this sub-optimal choice of the weights, under the special scenario studied in this theorem.\nTheorem 7. Under the bottom-` separators scenario and the PL model, Sj\u2019s are chosen uniformly at random of size \u03ba and n partial orderings are sampled over d items parametrized by \u03b8\u2217 \u2208 \u2126b. For d\u0303 = `d/(2\u03ba) and any ` \u2265 4, if the effective sample size is large enough such that\nn` \u2265 ( 214e8b\n\u03c72 \u03ba3 `3\n) d log d , (19)\nwhere\n\u03c7 \u2261 1 4\n( 1\u2212 exp ( \u2212 2\n9(\u03ba\u2212 2)\n)) , (20)\nthen the rank-breaking estimator in (18) achieves\n1\u221a d\u0303\n\u2225\u2225\u0302\u0303\u03b8 \u2212 \u03b8\u0303\u2217\u2225\u2225 2 \u2264 128(1 + e 4b)2\n\u03c7\n\u03ba3/2 `3/2\n\u221a d log d\nn` , (21)\nwith probability at least 1\u2212 3e3d\u22123.\nConsider a scenario where \u03ba = O(1) and ` = \u0398(\u03ba). Then, \u03c7 is a strictly positive constant, and also \u03ba/` is s finite constant. It follows that rank-breaking requires the effective sample size n` = O(d log d/\u03b52) in order\nto achieve arbitrarily small error of \u03b5 > 0, on the weakest d\u0303 = ` d/(2\u03ba) items."}, {"heading": "5 Real-World Data Sets", "text": "On real-world data sets on sushi preferences [30], we show that the data-driven rank-breaking improves over Generalized Method-of-Moments (GMM) proposed by [4]. This is a widely used data set for rank aggregation, for instance in [4, 6, 38, 31, 34, 33]. The data set consists of complete rankings over 10 types of\nsushi from n = 5000 individuals. Below, we follow the experimental scenarios of the GMM approach in [4] for fair comparisons.\nTo validate our approach, we first take the estimated PL weights of the 10 types of sushi, using [28] implementation of the ML estimator, over the entire input data of 5000 complete rankings. We take thus created output as the ground truth \u03b8\u2217. To create partial rankings and compare the performance of the data-driven rank-breaking to the state-of-the-art GMM approach in Figure 9, we first fix ` = 6 and vary n to simulate top-`-separators scenario by removing the known ordering among bottom 10\u2212 ` alternatives for each sample in the data set (left). We next fix n = 1000 and vary ` and simulate top-`-separators scenarios (right). Each point is averaged over 1000 instances. The mean squared error is plotted for both algorithms.\nFigure 10 illustrates the Kendall rank correlation of the rankings estimated by the two algorithms and the ground truth. Larger value indicates that the estimate is closer to the ground truth, and the data-driven rank-breaking outperforms the state-of-the-art GMM approach.\nTo validate whether PL model is the right model to explain the sushi data set, we compare the datadriven rank-breaking, MLE for the PL model, GMM for the PL model, Borda count and Spearman\u2019s footrule optimal aggregation. We measure the Kendall rank correlation between the estimates and the samples and show the result in Table 1. In particular, if \u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c3n denote sample rankings and \u03c3\u0302 denote the aggregated ranking then the correlation value is (1/n) \u2211n i=1 ( 1 \u2212 4K(\u03c3\u0302,\u03c3i)\u03ba(\u03ba\u22121) ) , where K(\u03c31, \u03c32) =\u2211\ni<j\u2208[\u03ba] I{(\u03c3\u221211 (i)\u2212\u03c3\u221211 (j))(\u03c3\u221212 (i)\u2212\u03c3\u221212 (j))<0}. The results are reported for different number of samples n and different values of ` under the top-` separators scenarios. When ` = 9, we are using all the complete rankings, and all algorithms are efficient. When ` < 9, we have partial orderings, and Spearman\u2019s footrule optimal\naggregation is NP-hard. We instead use scaled footrule aggregation (SFO) given in [17]. Most approaches achieve similar performance, except for the Spearman\u2019s footrule. The proposed data-driven rank-breaking achieves a slightly worse correlation compared to other approaches. However, note that none of the algorithms are necessarily maximizing the Kendall correlation, and are not expected to be particularly good in this metric.\nWe compare our algorithm with the GMM algorithm on two other real-world data-sets as well. We use jester data set [23] that consists of over 4.1 million continuous ratings between \u221210 to +10 of 100 jokes from 48, 483 users. The average number of jokes rated by an user is 72.6 with minimum and maximum being 36 and 100 respectively. We convert continuous ratings into ordinal rankings. This data-set has been used by [43, 49, 11, 32] for rank aggregation and collaborative filtering.\nSimilar to the settings of sushi data experiments, we take the estimated PL weights of the 100 jokes over all the rankings as ground truth. Figure 11 shows comparative performance of the data-driven rank-breaking and the GMM for the two scenarios. We first fix ` = 10 and vary n to simulate random-10 separators scenario (left). We next take all the rankings n = 73421 and vary ` to simulate random-` separators scenario (rights). Since sets have different sizes, while varying ` we use full breaking if the setsize is smaller than `. Each point is averaged over 100 instances. The mean squared error is plotted for both algorithms.\nWe perform similar experiments on American Psychological Association (APA) data-set [14]. The APA elects a president each year by asking each member to rank order a slate of five candidates. The data-set represents full rankings given by 5738 members of the association in 1980\u2019s election. The mean squared error is plotted for both algorithms under the settings similar to that of jester data-set."}, {"heading": "6 Related Work", "text": "Initially motivated by elections and voting, rank aggregation has been a topic of mathematical interest dating back to Condorcet and Borda [13, 12]. Using probabilistic models to infer preferences has been popularized in\noperations research community for applications such as assortment optimization and revenue management. The PL model studied in this paper is a special case of MultiNomial Logit (MNL) models commonly used in discrete choice modeling, which has a long history in operations research [40]. Efficient inference algorithms has been proposed to either find the MLE efficiently or approximately, such as the iterative approaches in [22, 18], minorization-maximization approach in [28], and Markov chain approaches in [44, 37]. These approaches are shown to achieve minimax optimal error rate in the traditional comparisons scenarios. Under the pairwise comparisons scenario, Negahban et al. [44] provided Rank Centrality that provably achieves minimax optimal error rate for randomly chosen pairs, which was later generalized to arbitrary pairwise comparisons in [45]. The analysis shows the explicit dependence on the topology of data shows that the spectral gap of comparisons graph similar to the one presented in this paper. This analysis was generalized to k-way comparisons in [26] and generalized to best-out-of-k comparisons with sharper bounds in [52]. In an effort to give a guarantee for exact recovery of the top-` items in the ranking, Chen et al. in [10] proposed a new algorithm based on Rank Centrality that provides a tighter error bound for L\u221e norm, as opposed to the existing L2 error bounds. Another interesting direction in learning to rank is non-parametric learning from paired comparisons, initiated in several recent papers such as [16, 50, 53, 54].\nMore recently, a more general problem of learning personal preferences from ordinal data has been studied [58, 33, 15]. The MNL model provides a natural generalization of the PL model to this problem. When users are classified into a small number of groups with same preferences, mixed MNL model can be learned from data as studied in [2, 46, 57]. A more general scenario is when each user has his/her individual preferences, but inherently represented by a lower dimensional feature. This problem was first posed as an inference problem in [35] where convex relaxation of nuclear norm minimization was proposed with provably optimal guarantees. This was later generalized to k-way comparisons in [47]. A similar approach was studied with a different guarantees and assumptions in [48]. Our algorithm and ideas of rank-breaking can be directly applied to this collaborative ranking under MNL, with the same guarantees for consistency in the asymptotic regime where sample size grows to infinity. However, the analysis techniques for MNL rely on stronger assumptions on how the data is collected, and especially on the independence of the samples. It is not immediate how the analysis techniques developed in this paper can be applied to learn MNL.\nIn an orthogonal direction, new discrete choice models with sparse structures has been proposed recently in [19] and optimization algorithms for revenue management has been proposed [20]. In a similar direction, new discrete choice models based on Markov chains has been introduced in [8], and corresponding revenue management algorithms has been studied in [21]. However, typically these models are analyzed in the asymptotic regime with infinite samples, with the exception of [3]. A non-parametric choice models for pairwise comparisons also have been studied in [50, 53]. This provides an interesting opportunities to studying learning to rank for these new choice models.\nWe consider a fixed design setting, where inference is separate from data collection. There is a parallel line of research which focuses on adaptive ranking, mainly based on pairwise comparisons. When performing\nsorting from noisy pairwise comparisons, Braverman et al. in [9] proposed efficient approaches and provided performance guarantees. Following this work, there has been recent advances in adaptive ranking [1, 29, 38]."}, {"heading": "7 Discussion", "text": "We study the problem of learning the PL model from ordinal data. Under the traditional data collection scenarios, several efficient algorithms find the maximum likelihood estimates and at the same time provably achieve minimax optimal performance. However, for some non-traditional scenarios, computational complexity of finding the maximum likelihood estimate can scale super-exponentially in the problem size. We provide the first finite-sample analysis of computationally efficient estimators known as rank-breaking estimators. This provides guidelines for choosing the weights in the estimator to achieve optimal performance, and also explicitly shows how the accuracy depends on the topology of the data.\nThis paper provides the first analytical result in the sample complexity of rank-breaking estimators, and quantifies the price we pay in accuracy for the computational gain. In general, more complex higherorder rank-breaking can also be considered, where instead of breaking a partial ordering into a collection of paired comparisons, we break it into a collection of higher-order comparisons. The resulting higherorder rank-breakings will enable us to traverse the whole spectrum of computational complexity between the pairwise rank-breaking and the MLE. We believe this paper opens an interesting new direction towards understanding the whole spectrum of such approaches. However, analyzing the Hessian of the corresponding objective function is significantly more involved and requires new technical innovations."}, {"heading": "8 Proofs", "text": ""}, {"heading": "8.1 Proof of Theorem 2", "text": "We prove a more general result for an arbitrary choice of the parameter \u03bbj,a > 0 for all j \u2208 [n] and a \u2208 [`j ]. The following theorem proves the (near)-optimality of the choice of \u03bbj,a\u2019s proposed in (10), and implies the corresponding error bound as a corollary.\nTheorem 8. Under the hypotheses of Theorem 2 and any \u03bbj,a\u2019s, the rank-breaking estimator achieves\n1\u221a d\n\u2225\u2225 \u03b8\u0302 \u2212 \u03b8\u2217 \u2225\u2225 2 \u2264 4 \u221a 2e4b(1 + e2b)2 \u221a d log d\n\u03b1 \u03b3\n\u221a\u2211n j=1 \u2211`j a=1 ( \u03bbj,a )2( \u03baj \u2212 pj,a )( \u03baj \u2212 pj,a + 1 ) \u2211n j=1 \u2211`j a=1 \u03bbj,a(\u03baj \u2212 pj,a) , (22)\nwith probability at least 1\u2212 3e3d\u22123, if\nn\u2211 j=1 `j\u2211 a=1 \u03bbj,a(\u03baj \u2212 pj,a) \u2265 26e18b \u03b7\u03b4 \u03b12\u03b2\u03b32\u03c4 d log d , (23)\nwhere \u03b3, \u03b7, \u03c4 , \u03b4, \u03b1, \u03b2, are now functions of \u03bbj,a\u2019s and defined in (7), (8), (25), (27) and (30).\nWe first claim that \u03bbj,a = 1/(\u03baj \u2212 pj,a + 1) is the optimal choice for minimizing the above upper bound on the error. From Cauchy-Schwartz inequality and the fact that all terms are non-negative, we have that\u221a\u2211n\nj=1 \u2211`j a=1 ( \u03bbj,a )2 (\u03baj \u2212 pj,a)(\u03baj \u2212 pj,a + 1)\u2211n\nj=1 \u2211`j a=1 \u03bbj,a(\u03baj \u2212 pj,a)\n\u2265 1\u221a\u2211n j=1 \u2211`j a=1 (\u03baj\u2212pj,a) (\u03baj\u2212pj,a+1) , (24)\nwhere \u03bbj,a = 1/(\u03baj \u2212 pj,a + 1) achieves the universal lower bound on the right-hand side with an equality. Since \u2211n j=1 \u2211`j a=1 (\u03baj\u2212pj,a) (\u03baj\u2212pj,a+1) \u2265 \u2211n j=1 `j , substituting this into (22) gives the desired error bound in (11).\nAlthough we have identified the optimal choice of \u03bbj,a\u2019s, we choose a slightly different value of \u03bb = 1/(\u03baj\u2212pj,a) for the analysis. This achieves the same desired error bound in (11), and significantly simplifies the notations of the sufficient conditions.\nWe first define all the parameters in the above theorem for general \u03bbj,a. With a slight abuse of notations, we use the same notations for H, L, \u03b1 and \u03b2 for both the general \u03bbj,a\u2019s and also the specific choice of \u03bbj,a = 1/(\u03baj \u2212 pj,a). It should be clear from the context what we mean in each case. Define\n\u03c4 \u2261 min j\u2208[n]\n\u03c4j , where \u03c4j \u2261 \u2211`j a=1 \u03bbj,a(\u03baj \u2212 pj,a)\n`j (25)\n\u03b4j,1 \u2261 {\nmax a\u2208[`j ]\n{ \u03bbj,a(\u03baj \u2212 pj,a) } + `j\u2211 a=1 \u03bbj,a } , and \u03b4j,2 \u2261 `j\u2211 a=1 \u03bbj,a (26)\n\u03b4 \u2261 max j\u2208[n]\n{ 4\u03b42j,1 + 2 ( \u03b4j,1\u03b4j,2 + \u03b4 2 j,2 ) \u03baj\n\u03b7j`j\n} . (27)\nNote that \u03b4 \u2265 \u03b42j,1 \u2265 maxa \u03bb2j,a(\u03baj \u2212 pj,a)2 \u2265 \u03c42, and for the choice of \u03bbj,a = 1/(\u03baj \u2212 pj,a) it simplifies as \u03c4 = \u03c4j = 1. We next define a comparison graph H for general \u03bbj,a, which recovers the proposed comparison graph for the optimal choice of \u03bbj,a\u2019s\nDefinition 9. (Comparison graph H). Each item i \u2208 [d] corresponds to a vertex i. For any pair of vertices i, i\u2032, there is a weighted edge between them if there exists a set Sj such that i, i\n\u2032 \u2208 Sj; the weight equals\u2211 j:i,i\u2032\u2208Sj \u03c4j`j \u03baj(\u03baj\u22121) .\nLet A denote the weighted adjacency matrix, and let D = diag(A1). Define,\nDmax \u2261 max i\u2208[d] Dii = max i\u2208[d] { \u2211 j:i\u2208Sj \u03c4j`j \u03baj } \u2265 \u03c4min max i\u2208[d] { \u2211 j:i\u2208Sj `j \u03baj } . (28)\nDefine graph Laplacian L as L = D \u2212A, i.e.,\nL = n\u2211 j=1 \u03c4j`j \u03baj(\u03baj \u2212 1) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>. (29)\nLet 0 = \u03bb1(L) \u2264 \u03bb2(L) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbd(L) denote the sorted eigenvalues of L. Note that Tr(L) = \u2211d i=1 \u2211 j:i\u2208Sj \u03c4j`j/\u03baj =\u2211n\nj=1 \u03c4j`j . Define \u03b1 and \u03b2 such that\n\u03b1 \u2261 \u03bb2(L)(d\u2212 1) Tr(L)\n= \u03bb2(L)(d\u2212 1)\u2211n\nj=1 \u03c4j`j and \u03b2 \u2261 Tr(L) dDmax =\n\u2211n j=1 \u03c4j`j\ndDmax . (30)\nFor the proposed choice of \u03bbj,a = 1/(\u03baj \u2212 pj,a), we have \u03c4j = 1 and the definitions of H, L, \u03b1, and \u03b2 reduce to those defined in Definition 1. We are left to prove an upper bound, \u03b4 \u2264 32(log(`max + 2))2, which implies the sufficient condition in (9) and finishes the proof of Theorem 2. We have,\n\u03b4j,1 = max a\u2208[`j ]\n{ \u03bbj,a(\u03baj \u2212 pj,a) } + `j\u2211 a=1 \u03bbj,a = 1 + `j\u2211 a=1\n1\n\u03baj \u2212 pj,a\n\u2264 1 + `j\u2211 a=1 1 a \u2264 2 log(`j + 2) , (31)\nwhere in the first inequality follows from taking the worst case for the positions, i.e. pj,a = \u03baj \u2212 `j + a\u2212 1 Using the fact that for any integer x, \u2211`\u22121 a=0 1/(x+ a) \u2264 log((x+ `\u2212 1)/(x\u2212 1)), we also have\n\u03b4j,2\u03baj \u03b7j`j \u2264 `j\u2211 a=1\n1 \u03baj \u2212 pj,a max {`j , \u03baj \u2212 pj,`j} `j\n\u2264 min { log(`j + 2) , log (\u03baj \u2212 pj,`j + `j \u2212 1\n\u03baj \u2212 pj,`j \u2212 1 )}max {`j , \u03baj \u2212 pj,`j} `j\n\u2264 log(`j + 2)`j max {`j , \u03baj \u2212 pj,`j \u2212 1} max {`j , \u03baj \u2212 pj,`j} `j \u2264 2 log(`j + 2) , (32)\nwhere the first inequality follows from the definition of \u03b7j , Equation (8). From (31), (32), and the fact that \u03b4j,2 \u2264 log(`j + 2), we have\n\u03b4 = max j\u2208[n]\n{ 4\u03b42j,1 + 2 ( \u03b4j,1\u03b4j,2 + \u03b4 2 j,2 ) \u03baj\n\u03b7j`j\n} \u2264 28(log(`max + 2))2 ."}, {"heading": "8.2 Proof of Theorem 8", "text": "We first introduce two key technical lemmas. In the following lemma we show that E\u03b8\u2217 [\u2207LRB(\u03b8\u2217)] = 0 and provide a bound on the deviation of \u2207LRB(\u03b8\u2217) from its mean. The expectation E\u03b8\u2217 [\u00b7] is with respect to the randomness in the samples drawn according to \u03b8\u2217. The log likelihood Equation (2) can be rewritten as\nLRB(\u03b8) = n\u2211 j=1 `j\u2211 a=1 \u2211 i<i\u2032\u2208Sj I{ (i,i\u2032)\u2208Gj,a }\u03bbj,a(\u03b8iI{ \u03c3\u22121j (i)<\u03c3 \u22121 j (i \u2032) } + \u03b8i\u2032I{ \u03c3\u22121j (i)>\u03c3 \u22121 j (i \u2032) } \u2212 log (e\u03b8i + e\u03b8i\u2032)) .\n(33)\nWe use (i, i\u2032) \u2208 Gj,a to mean either (i, i\u2032) or (i\u2032, i) belong to Ej,a. Taking the first-order partial derivative of LRB(\u03b8), we get\n\u2207iLRB(\u03b8\u2217) = \u2211 j:i\u2208Sj `j\u2211 a=1 \u2211 i\u2032\u2208Sj i\u2032 6=i \u03bbj,a I{ (i,i\u2032)\u2208Gj,a } (I{ \u03c3\u22121j (i)<\u03c3 \u22121 j (i \u2032) } \u2212 exp(\u03b8\u2217i ) exp(\u03b8\u2217i ) + exp(\u03b8 \u2217 i\u2032) ) . (34)\nLemma 10. Under the hypotheses of Theorem 2, with probability at least 1\u2212 2e3d\u22123,\n\u2225\u2225\u2207LRB(\u03b8\u2217)\u2225\u22252 \u2264 \u221a\u221a\u221a\u221a6 log d n\u2211\nj=1 `j\u2211 a=1 ( \u03bbj,a )2( \u03baj \u2212 pj,a )( \u03baj \u2212 pj,a + 1 ) .\nThe Hessian matrix H(\u03b8) \u2208 Sd with Hii\u2032(\u03b8) = \u2202 2LRB(\u03b8) \u2202\u03b8i\u2202\u03b8i\u2032 is given by\nH(\u03b8) = \u2212 n\u2211 j=1 `j\u2211 a=1 \u2211 i<i\u2032\u2208Sj I{ (i,i\u2032)\u2208Gj,a }\u03bbj,a((ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> exp(\u03b8i + \u03b8i\u2032) [exp(\u03b8i) + exp(\u03b8i\u2032)]2 ) . (35)\nIt follows from the definition that \u2212H(\u03b8) is positive semi-definite for any \u03b8 \u2208 Rd. The smallest eigenvalue of \u2212H(\u03b8) is equal to zero and the corresponding eigenvector is all-ones vector. The following lemma lower bounds its second smallest eigenvalue \u03bb2(\u2212H(\u03b8)).\nLemma 11. Under the hypotheses of Theorem 2, if\nn\u2211 j=1 `j\u2211 a=1 \u03bbj,a(\u03baj \u2212 pj,a) \u2265 26e18b \u03b7\u03b4 \u03b12\u03b2\u03b32\u03c4 d log d (36)\nthen with probability at least 1\u2212 d\u22123, the following holds for any \u03b8 \u2208 \u2126b:\n\u03bb2(\u2212H(\u03b8)) \u2265 e\u22124b\n(1 + e2b)2 \u03b1\u03b3 d\u2212 1 n\u2211 j=1 `j\u2211 a=1 \u03bbj,a(\u03baj \u2212 pj,a) . (37)\nDefine \u2206 = \u03b8\u0302 \u2212 \u03b8\u2217. It follows from the definition that \u2206 is orthogonal to the all-ones vector. By the definition of \u03b8\u0302 as the optimal solution of the optimization (3), we know that LRB(\u03b8\u0302) \u2265 LRB(\u03b8\u2217) and thus\nLRB(\u03b8\u0302)\u2212 LRB(\u03b8\u2217)\u2212 \u3008\u2207LRB(\u03b8\u2217),\u2206\u3009 \u2265 \u2212\u3008\u2207LRB(\u03b8\u2217),\u2206\u3009 \u2265 \u2212\u2016\u2207LRB(\u03b8\u2217)\u20162\u2016\u2206\u20162, (38)\nwhere the last inequality follows from the Cauchy-Schwartz inequality. By the mean value theorem, there exists a \u03b8 = a\u03b8\u0302 + (1\u2212 a)\u03b8\u2217 for some a \u2208 [0, 1] such that \u03b8 \u2208 \u2126b and\nLRB(\u03b8\u0302)\u2212 LRB(\u03b8\u2217)\u2212 \u3008\u2207LRB(\u03b8\u2217),\u2206\u3009 = 1 2 \u2206>H(\u03b8)\u2206 \u2264 \u22121 2 \u03bb2(\u2212H(\u03b8))\u2016\u2206\u201622, (39)\nwhere the last inequality holds because the Hessian matrix \u2212H(\u03b8) is positive semi-definite with H(\u03b8)1 = 0 and \u2206>1 = 0. Combining (38) and (39),\n\u2016\u2206\u20162 \u2264 2\u2016\u2207LRB(\u03b8\u2217)\u20162 \u03bb2(\u2212H(\u03b8)) . (40)\nNote that \u03b8 \u2208 \u2126b by definition. Theorem 8 follows by combining Equation (40) with Lemma 10 and Lemma 11."}, {"heading": "8.2.1 Proof of Lemma 10", "text": "The idea of the proof is to view \u2207LRB(\u03b8\u2217) as the final value of a discrete time vector-valued martingale with values in Rd. Define \u2207LGj,a(\u03b8\u2217) as the gradient vector arising out of each rank-breaking graph {Gj,a}j\u2208[n],a\u2208[`j ] that is\n\u2207iLGj,a(\u03b8\u2217) \u2261 \u2211 i\u2032\u2208Sj i\u2032 6=i \u03bbj,a I{ (i,i\u2032)\u2208Gj,a } (I{ \u03c3\u22121j (i)<\u03c3 \u22121 j (i \u2032) } \u2212 exp(\u03b8\u2217i ) exp(\u03b8\u2217i ) + exp(\u03b8 \u2217 i\u2032) ) . (41)\nConsider \u2207LGj,a(\u03b8\u2217) as the incremental random vector in a martingale of \u2211 j=1 `j time steps. Lemma 12 shows that the expectation of each incremental vector is zero. Observe that the conditioning event {i\u2032\u2032 \u2208 S : \u03c3\u22121(i\u2032\u2032) < pj,a} given in (43) is equivalent to conditioning on the history {Gj,a\u2032}a\u2032<a. Therefore, using the assumption that the rankings {\u03c3j}j\u2208[n] are mutually independent, we have that the conditional expectation of \u2207LGj,a(\u03b8\u2217) conditioned on {Gj\u2032,a\u2032\u2032}j\u2032<j,a\u2032\u2032\u2208[`j\u2032 ] is zero. Further, the conditional expectation of \u2207LGj,a(\u03b8\u2217) is zero even when conditioned on the rank breaking due to previous separators {Gj,a\u2032}a\u2032<a that are ranked higher (i.e. a\u2032 < a), which follows from the next lemma.\nLemma 12. For a position-p rank breaking graph Gp, defined over a set of items S, where p \u2208 [|S| \u2212 1], P [ \u03c3\u22121(i) < \u03c3\u22121(i\u2032) \u2223\u2223\u2223 (i, i\u2032) \u2208 Gp] = exp(\u03b8\u2217i ) exp(\u03b8\u2217i ) + exp(\u03b8 \u2217 i\u2032) , (42) for all i, i\u2032 \u2208 S and also\nP [ \u03c3\u22121(i) < \u03c3\u22121(i\u2032) \u2223\u2223\u2223 (i, i\u2032) \u2208 Gp and {i\u2032\u2032 \u2208 S : \u03c3\u22121(i\u2032\u2032) < p}] = exp(\u03b8\u2217i ) exp(\u03b8\u2217i ) + exp(\u03b8 \u2217 i\u2032) . (43)\nThis is one of the key technical lemmas since it implies that the proposed rank-breaking is consistent, i.e. E\u03b8\u2217 [\u2207LRB(\u03b8\u2217)] = 0. Throughout the proof of Theorem 2, this is the only place where the assumption on the proposed (consistent) rank-breaking is used. According to a companion theorem in [5, Theorem 2], it also follows that any rank-breaking that is not union of position-p rank-breakings results in inconsistency, i.e. E\u03b8\u2217 [\u2207LRB(\u03b8\u2217)] 6= 0. We claim that for each rank-breaking graph Gj,a, \u2016\u2207LGj,a(\u03b8\u2217)\u201622 \u2264 (\u03bbj,a)2(\u03baj \u2212 pj,a)(\u03baj \u2212 pj,a + 1). By Lemma 13 which is a generalization of the vector version of the Azuma-Hoeffding inequality found in [27, Theorem 1.8], we have\nP [\u2225\u2225\u2207LRB(\u03b8\u2217)\u2225\u22252 \u2265 \u03b4] \u2264 2e3 exp\n( \u2212\u03b42\n2 \u2211n j=1 \u2211`j a=1 ( \u03bbj,a )2( \u03baj \u2212 pj,a )( \u03baj \u2212 pj,a + 1 )) , which implies the result.\nLemma 13. Let (X1, X2, \u00b7 \u00b7 \u00b7 , Xn) be real-valued martingale taking values in Rd such that X0 = 0 and for every 1 \u2264 i \u2264 n, \u2016Xi \u2212Xi\u22121\u20162 \u2264 ci, for some non-negative constant ci. Then for every \u03b4 > 0,\nP[\u2016Xn\u20162 \u2265 \u03b4] \u2264 2e3e \u2212 \u03b42 2 \u2211n i=1 c2 i . (44)\nIt follows from the upper bound on \u2016\u2207LGj,a(\u03b8\u2217)\u201622 \u2264 c2i with c2i = \u03bb2 ( (kj \u2212 pj,a)2 + (kj \u2212 pj,a) ) . In the expression (41), \u2207LGj,a(\u03b8\u2217) has one entry at pj,a-th position that is compared to (kj \u2212 pj,a) other items and (kj \u2212 pj,a) entries that is compared only once, giving the bound\n\u2016\u2207LGj,a(\u03b8\u2217)\u201622 \u2264 \u03bb2j,a(kj \u2212 pj,a)2 + \u03bb2j,a(kj \u2212 pj,a) ."}, {"heading": "8.2.2 Proof of Lemma 12", "text": "Define event E \u2261 {(i, i\u2032) \u2208 Gp}. Observe that\nE = {( I{(\u03c3\u22121(i)=p} + I{\u03c3\u22121(i\u2032))=p} = 1 ) \u2227 ( \u03c3\u22121(i), \u03c3\u22121(i\u2032) \u2265 p )} .\nConsider any set \u2126 \u2282 S \\ {i, i\u2032} such that |\u2126| = p \u2212 1. Let M denote an event that items of the set \u2126 are ranked in top-(p\u2212 1) positions in a particular order. It is easy to verify the following:\nP [ \u03c3\u22121(i) < \u03c3\u22121(i\u2032) \u2223\u2223\u2223E,M] = P [( \u03c3\u22121(i) < \u03c3\u22121(i\u2032) ) , E,M ] P [ E,M\n] = P [( \u03c3\u22121(i) = p ) ,M ]\nP [( \u03c3\u22121(i) = p ) ,M ] + P [( \u03c3\u22121(i\u2032) = p ) ,M ]\n= exp(\u03b8\u2217i )\nexp(\u03b8\u2217i ) + exp(\u03b8 \u2217 i\u2032)\n= P [ \u03c3\u22121(i) < \u03c3\u22121(i\u2032) ] .\nSince M is any particular ordering of the set \u2126 and \u2126 is any subset of S \\ {i, i\u2032} such that |\u2126| = p \u2212 1, conditioned on event E probabilities of all the possible events M over all the possible choices of set \u2126 sum to 1."}, {"heading": "8.2.3 Proof of Lemma 13", "text": "It follows exactly along the lines of proof of Theorem 1.8 in [27]."}, {"heading": "8.2.4 Proof of Lemma 11", "text": "The Hessian H(\u03b8) is given in (35). For all j \u2208 [n], define M (j) \u2208 Sd as\nM (j) \u2261 `j\u2211 a=1 \u03bbj,a \u2211 i<i\u2032\u2208Sj I{ (i,i\u2032) \u2208 Gj,a }(ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>, (45) and let M \u2261 \u2211n j=1M (j). Observe that M is positive semi-definite and the smallest eigenvalue of M is zero with the corresponding eigenvector given by the all-ones vector. If |\u03b8i| \u2264 b, for all i \u2208 [d], exp(\u03b8i+\u03b8i\u2032 )[exp(\u03b8i)+exp(\u03b8i\u2032 )]2 \u2265 e2b (1+e2b)2 . Recall the definition of H(\u03b8) from Equation (35). It follows that \u2212H(\u03b8) e 2b (1+e2b)2 M for \u03b8 \u2208 \u2126b. Since, \u2212H(\u03b8) and M are symmetric matrices, from Weyl\u2019s inequality we have, \u03bb2(\u2212H(\u03b8)) \u2265 e 2b\n(1+e2b)2 \u03bb2(M).\nAgain from Weyl\u2019s inequality, it follows that\n\u03bb2(M) \u2265 \u03bb2(E[M ])\u2212 \u2016M \u2212 E[M ]\u2016 , (46)\nwhere \u2016 \u00b7 \u2016 denotes the spectral norm. We will show in (51) that \u03bb2(E[M ]) \u2265 2\u03b3e\u22126b(\u03b1/(d\u2212 1)) \u2211n j=1 \u03c4j`j ,\nand in (63) that \u2016M \u2212 E[M ]\u2016 \u2264 8e3b \u221a\n\u03b7\u03b4 log d \u03b2\u03c4d \u2211n j=1 \u03c4j`j .\n\u03bb2(M) \u2265 2e\u22126b\u03b1\u03b3\nd\u2212 1 n\u2211 j=1\n\u03c4j`j \u2212 8e3b \u221a\u221a\u221a\u221a\u03b7\u03b4 log d\n\u03b2\u03c4d n\u2211 j=1 \u03c4j`j \u2265 e\u22126b\u03b1\u03b3 d\u2212 1 n\u2211 j=1 \u03c4j`j , (47)\nwhere the last inequality follows from the assumption that \u2211n j=1 \u03c4j`j \u2265 26e18b \u03b7\u03b4 \u03b12\u03b2\u03b32\u03c4 d log d. This proves the desired claim. To prove the lower bound on \u03bb2(E[M ]), notice that\nE[M ] = n\u2211 j=1 `j\u2211 a=1 \u03bbj,a \u2211 i<i\u2032\u2208Sj P [ (i, i\u2032) \u2208 Gj,a \u2223\u2223\u2223(i, i\u2032 \u2208 Sj)](ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> . (48) The following lemma provides a lower bound on P[(i, i\u2032) \u2208 Gj,a|(i, i\u2032 \u2208 Sj)].\nLemma 14. Consider a ranking \u03c3 over a set S \u2286 [d] such that |S| = \u03ba. For any two items i, i\u2032 \u2208 S, \u03b8 \u2208 \u2126b, and 1 \u2264 ` \u2264 \u03ba\u2212 1,\nP\u03b8 [ \u03c3\u22121(i) = `, \u03c3\u22121(i\u2032) > ` ] \u2265 e\n\u22126b(\u03ba\u2212 `) \u03ba(\u03ba\u2212 1)\n( 1\u2212 `\n\u03ba\n)\u03b1i,i\u2032,`,\u03b8\u22122 , (49)\nwhere the probability P\u03b8 is with respect to the sampled ranking resulting from PL weights \u03b8 \u2208 \u2126b, and \u03b1i,i\u2032,`,\u03b8 is defined as 1 \u2264 \u03b1i,i\u2032,`,\u03b8 = d\u03b1\u0303i,i\u2032,`,\u03b8e, and \u03b1\u0303i,i\u2032,`,\u03b8 is,\n\u03b1\u0303i,i\u2032,`,\u03b8 \u2261 max `\u2032\u2208[`] max \u2126\u2286S\\{i,i\u2032} :|\u2126|=\u03ba\u2212`\u2032\n{ exp(\u03b8i) + exp(\u03b8i\u2032)(\u2211\nj\u2208\u2126 exp(\u03b8j) ) /|\u2126|\n} . (50)\nNote that we do not need max`\u2032\u2208[`] in the above equation as the expression achieves its maxima at ` \u2032 = `, but we keep the definition to avoid any confusion. In the worst case, 2e\u22122b \u2264 \u03b1\u0303i,i\u2032,`,\u03b8 \u2264 2e2b. Therefore,\nusing definition of rank breaking graph Gj,a, and Equations (48) and (49) we have,\nE[M ] \u03b3e\u22126b n\u2211 j=1 `j\u2211 a=1 \u03bbj,a 2(\u03baj \u2212 pj,a) \u03baj(\u03baj \u2212 1) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>\n2\u03b3e\u22126b n\u2211 j=1\n1\n\u03baj(\u03baj \u2212 1) `j\u2211 a=1 \u03bbj,a(\u03baj \u2212 pj,a) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>\n= 2\u03b3e\u22126bL, (51)\nwhere we used \u03b3 \u2264 (1\u2212 pj,`j/\u03baj)\u03b11\u22122 which follows for the definition in (7). (51) follows from the definition of Laplacian L, defined for the comparison graph H in Definition 9. Using \u03bb2(L) = (\u03b1/(d \u2212 1)) \u2211n j=1 \u03c4j`j\nfrom (30), we get the desired bound \u03bb2(E[M ]) \u2265 2\u03b3e\u22126b(\u03b1/(d\u2212 1)) \u2211n j=1 \u03c4j`j .\nNext we need to upper bound \u2016 \u2211n j=1 E[(M j)2]\u2016 to bound the deviation of M from its expectation. To\nthis end, we prove an upper bound on P[\u03c3\u22121j (i) = pj,a | i \u2208 Sj ] in the following lemma.\nLemma 15. Under the hypotheses of Lemma 14,\nP\u03b8 [ \u03c3\u22121(i) = ` ] \u2264 e 6b\n\u03ba\n( 1\u2212 `\n\u03ba+ \u03b1i,`,\u03b8\n)\u03b1i,`,\u03b8\u22121 \u2264 e 6b\n\u03ba\u2212 ` , (52)\nwhere 0 \u2264 \u03b1i,`,\u03b8 = b\u03b1\u0303i,`,\u03b8c, and \u03b1\u0303i,`,\u03b8 is,\n\u03b1\u0303i,`,\u03b8 \u2261 min `\u2032\u2208[`] min \u2126\u2208S\\{i}\n:|\u2126|=\u03ba\u2212`\u2032+1\n{ exp(\u03b8i)(\u2211\nj\u2208\u2126 exp(\u03b8j) ) /|\u2126|\n} . (53)\nIn the worst case, e\u22122b \u2264 \u03b1\u0303i,`,\u03b8 \u2264 e2b. Note that \u03b1i,`,\u03b8 = 0 gives the worst upper bound.\nTherefore using Equation (52), for all i \u2208 [d], we have,\nP [ \u03c3\u22121j (i) \u2208 Pj ] \u2264 min { 1,\ne6b`j \u03baj \u2212 pj,`j\n} \u2264 e\n6b`j max{`j , \u03baj \u2212 pj,`j} \u2264 e 6b\u03b7`j \u03baj , (54)\nwhere we used \u03b7 defined in Equation (8). Define a diagonal matrix D(j) \u2208 Sd and a matrix A(j) \u2208 Sd,\nA (j) ii\u2032 \u2261 I{i,i\u2032\u2208Sj} `j\u2211 a=1 \u03bbj,aI{ (i,i\u2032)\u2208Gj,a } , for all i, i\u2032 \u2208 [d] , (55) and D\n(j) ii = \u2211 i\u2032 6=iA (j) ii\u2032 . Observe that M (j) = D(j) \u2212A(j). For all i \u2208 [d], we have,\nD (j) ii = I{i\u2208Sj} \u03baj\u2211 i\u2032=1 I{ \u03c3\u22121j (i)=i \u2032 } `j\u2211 a=1 \u03bbj,adegGj,a(\u03c3 \u22121 j (i \u2032))\n\u2264 I{ i\u2208Sj }{I{ \u03c3\u22121j (i)\u2208Pj }( max a\u2208[`j ] { \u03bbj,a(\u03baj \u2212 pj,a) } + `j\u2211 a=1 \u03bbj,a ) + I{ \u03c3\u22121j (i)/\u2208Pj }( `j\u2211 a=1 \u03bbj,a )} = I{ i\u2208Sj }{I{ \u03c3\u22121j (i)\u2208Pj }\u03b4j,1 + I{ \u03c3\u22121j (i)/\u2208Pj\n}\u03b4j,2}, (56) where the last equality follows from the definition of \u03b4j,1 and \u03b4j,2 in Equation (26). Note that maxi\u2208[d]{Dii} = \u03b4j,1. Using (54) and (56), we have,\nE [ D\n(j) ii\n] \u2264 I{ i\u2208Sj }{e6b\u03b7`j\n\u03baj\n( \u03b4j,1 +\n\u03b4j,2\u03baj \u03b7`j\n)} . (57)\nSimilarly we have,\nE [( D\n(j) ii )2] \u2264 I{ i\u2208Sj }{e6b\u03b7`j \u03baj ( \u03b42j,1 + \u03b42j,2\u03baj \u03b7`j )} (58)\nFor all i \u2208 [d], we have,\nE\n[ d\u2211\ni\u2032=1\n(( A(j) )2) ii\u2032 ] \u2264 E [( d\u2211 i\u2032=1 A (j) ii\u2032 ) max i\u2208[d] { d\u2211 i\u2032=1 A (j) ii\u2032 }]\n\u2264 E [ D\n(j) ii \u03b4j,1 ] \u2264 I{ i\u2208Sj }{e6b\u03b7`j\n\u03baj\n( \u03b42j,1 +\n\u03b4j,1\u03b4j,2\u03baj \u03b7`j\n)} . (59)\nUsing (58) and (59), we have, for all i \u2208 [d],\nd\u2211 i\u2032=1 \u2223\u2223\u2223E[((M (j))2) ii\u2032 ]\u2223\u2223\u2223 =\nd\u2211 i\u2032=1 \u2223\u2223\u2223\u2223\u2223E[((D(j))2)ii\u2032]\u2212 E[(D(j)A(j))ii\u2032]\u2212 E[(A(j)D(j))ii\u2032]+ E[((A(j))2)ii\u2032] \u2223\u2223\u2223\u2223\u2223\n\u2264 2E [( D\n(j) ii\n)2] + d\u2211 i\u2032=1 ( E [ \u03b4j,1 ( A(j) ) ii\u2032 ] + E [(( A(j) )2) ii\u2032 ])\n\u2264 I{ i\u2208Sj }{e6b\u03b7`j \u03baj ( 4\u03b42j,1 + 2 ( \u03b4j,1\u03b4j,2 + \u03b4 2 j,2 ) \u03baj \u03b7`j )} = I{ i\u2208Sj }{e6b\u03b4\u03b7`j \u03baj } , (60)\nwhere the last equality follows from the definition of \u03b4, Equation (27). To bound \u2016 \u2211n j=1 E[(M (j))2]\u2016, we use the fact that for J \u2208 Rd\u00d7d, \u2016J\u2016 \u2264 maxi\u2208[d] \u2211d i\u2032=1 |Jii\u2032 |. Therefore, we have \u2225\u2225\u2225\u2225\u2225 n\u2211 j=1 E [ (M (j))2 ]\u2225\u2225\u2225\u2225\u2225 \u2264 e6b\u03b4\u03b7maxi\u2208[d] { \u2211 j:i\u2208Sj `j \u03baj }\n= e6b\u03b7\u03b4\n\u03c4 Dmax (61)\n= e6b\u03b7\u03b4\n\u03b2\u03c4d n\u2211 j=1 \u03c4j`j , (62)\nwhere (61) follows from the definition of Dmax in Equation(28) and (62) follows from the definition of \u03b2 in (30). Observe that from Equation (56), \u2016M (j)\u2016 \u2264 2\u03b4j,1 \u2264 2 \u221a \u03b4. Applying matrix Bernstein inequality, we have,\nP [\u2225\u2225M \u2212 E[M ]\u2225\u2225 \u2265 t] \u2264 d exp( \u2212t2/2\ne6b\u03b7\u03b4 \u03b2\u03c4d \u2211n j=1 \u03c4j`j + 4 \u221a \u03b4t/3\n) .\nTherefore, with probability at least 1\u2212 d\u22123, we have,\n\u2225\u2225M \u2212 E[M ]\u2225\u2225 \u2264 4e3b \u221a\u221a\u221a\u221a\u03b7\u03b4 log d\n\u03b2\u03c4d n\u2211 j=1 \u03c4j`j + 64 \u221a \u03b4 log d 3 \u2264 8e3b \u221a\u221a\u221a\u221a\u03b7\u03b4 log d \u03b2\u03c4d n\u2211 j=1 \u03c4j`j , (63)\nwhere the second inequality uses \u2211n j=1 \u03c4j`j \u2265 26(\u03b2\u03c4/\u03b7)d log d which follows from the assumption that\u2211n\nj=1 \u03c4j`j \u2265 26e18b \u03b7\u03b4 \u03c4\u03b32\u03b12\u03b2d log d and the fact that \u03b1, \u03b2 \u2264 1, \u03b3 \u2264 1, \u03b7 \u2265 1, and \u03b4 > \u03c4 2."}, {"heading": "8.2.5 Proof of Lemma 14", "text": "Since providing a lower bound on P\u03b8 [ \u03c3\u22121(i) = `, \u03c3\u22121(i\u2032) > ` ] for arbitrary \u03b8 is challenging, we construct a new set of parameters {\u03b8\u0303j}j\u2208[d] from the original \u03b8. These new parameters are constructed such that it is both easy to compute the probability and also provides a lower bound on the original distribution. We denote the sum of the weights by W \u2261 \u2211 j\u2208S exp(\u03b8j). We define a new set of parameters {\u03b8\u0303j}j\u2208S :\n\u03b8\u0303j =\n{ log(\u03b1\u0303i,i\u2032,`,\u03b8/2) for j = i or i\n\u2032 , 0 otherwise .\n(64)\nSimilarly define W\u0303 \u2261 \u2211 j\u2208S exp(\u03b8\u0303j) = \u03ba\u2212 2 + \u03b1\u0303i,i\u2032,`,\u03b8. We have,\nP\u03b8 [ \u03c3\u22121(i) = `, \u03c3\u22121(i\u2032) > ` ] =\n\u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8j1)\nW\n\u2211 j2\u2208S\nj2 6=i,i\u2032,j1\n( exp(\u03b8j2)\nW \u2212 exp(\u03b8j1) \u00b7 \u00b7 \u00b7\n( \u2211 j`\u22121\u2208S j`\u22121 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22122\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22122 k=j1 exp(\u03b8k)\nexp(\u03b8i) W \u2212 \u2211j`\u22121 k=j1 exp(\u03b8k)\n) \u00b7 \u00b7 \u00b7 ))\n= exp(\u03b8i)\nW\n\u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8j1)\nW \u2212 exp(\u03b8j1) \u2211 j2\u2208S\nj2 6=i,i\u2032,j1\n( exp(\u03b8j2)\nW \u2212 exp(\u03b8j1)\u2212 exp(\u03b8j2) \u00b7 \u00b7 \u00b7\n\u2211 j`\u22121\u2208S j`\u22121 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22122\n( exp(\u03b8j`\u22121)\nW \u2212 \u2211j`\u22121 k=j1 exp(\u03b8k)\n) \u00b7 \u00b7 \u00b7 ))\n(65)\nConsider the last summation term in the above equation and let \u2126` = S \\ {i, i\u2032, j1, . . . , j`\u22122}. Observe that, |\u2126`| = \u03ba\u2212 ` and from equation (50), exp(\u03b8i)+exp(\u03b8i\u2032 )\u2211\nj\u2208\u2126` exp(\u03b8j)\n\u2264 \u03b1\u0303i,i\u2032,`,\u03b8\u03ba\u2212` . We have,\n\u2211 j`\u22121\u2208\u2126`\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22121 k=j1 exp(\u03b8k)\n= \u2211\nj`\u22121\u2208\u2126`\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22122 k=j1 exp(\u03b8k)\u2212 exp(\u03b8j`\u22121)\n\u2265 \u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\nW \u2212 \u2211j`\u22122 k=j1 exp(\u03b8k)\u2212 (\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121) ) /|\u2126`|\n(66)\n=\n\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\nexp(\u03b8i) + exp(\u03b8i\u2032) + \u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\u2212 (\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121) ) /|\u2126`|\n= ( exp(\u03b8i) + exp(\u03b8i\u2032)\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121) + 1\u2212 1 \u03ba\u2212 ` )\u22121\n\u2265 ( \u03b1\u03031 \u03ba\u2212 ` + 1\u2212 1 \u03ba\u2212 ` )\u22121 (67)\n= \u03ba\u2212 `\n\u03b1\u03031 + \u03ba\u2212 `\u2212 1 = \u2211\nj`\u22121\u2208\u2126`\nexp(\u03b8\u0303j`\u22121) W\u0303 \u2212 \u2211j`\u22122 k=j1 exp(\u03b8\u0303k)\u2212 exp(\u03b8\u0303j`\u22121) , (68)\nwhere (66) follows from the Jensen\u2019s inequality and the fact that for any c > 0, 0 < x < c, xc\u2212x is convex in x. Equation (67) follows from the definition of \u03b1\u0303i,i\u2032,`,\u03b8, (50), and the fact that |\u2126`| = \u03ba \u2212 `. Equation (68) uses the definition of {\u03b8\u0303j}j\u2208S .\nConsider {\u2126\u02dc\u0300}2\u2264\u02dc\u0300\u2264`\u22121, |\u2126\u02dc\u0300| = \u03ba\u2212 \u02dc\u0300, corresponding to the subsequent summation terms in (65). Observe that exp(\u03b8i)+exp(\u03b8i\u2032 )\u2211\nj\u2208\u2126 \u02dc\u0300 exp(\u03b8j) \u2264 \u03b1\u0303i,i\u2032,`,\u03b8/|\u2126\u02dc\u0300|. Therefore, each summation term in equation (65) can be lower bounded\nby the corresponding term where {\u03b8j}j\u2208S is replaced by {\u03b8\u0303j}j\u2208S . Hence, we have P\u03b8 [ \u03c3\u22121(i) = `, \u03c3\u22121(i\u2032) > ` ] \u2265 exp(\u03b8i)\nW \u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8\u0303j1)\nW\u0303 \u2212 exp(\u03b8\u0303j1) \u2211 j2\u2208S\nj2 6=i,i\u2032,j1\n( exp(\u03b8\u0303j2)\nW\u0303 \u2212 exp(\u03b8\u0303j1)\u2212 exp(\u03b8\u0303j2) \u00b7 \u00b7 \u00b7\n\u2211 j`\u22121\u2208S j`\u22121 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22122\n( exp(\u03b8\u0303j`\u22121)\nW\u0303 \u2212 \u2211j`\u22121 k=j1 exp(\u03b8\u0303k)\n)))\n\u2265 e \u22124b exp(\u03b8\u0303i)\nW\u0303\n\u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8\u0303j1)\nW\u0303 \u2212 exp(\u03b8\u0303j1) \u2211 j2\u2208S\nj2 6=i,i\u2032,j1\n( exp(\u03b8\u0303j2)\nW\u0303 \u2212 exp(\u03b8\u0303j1)\u2212 exp(\u03b8\u0303j2) \u00b7 \u00b7 \u00b7\n\u2211 j`\u22121\u2208S j`\u22121 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22122\n( exp(\u03b8\u0303j`\u22121)\nW\u0303 \u2212 \u2211j`\u22121 k=j1 exp(\u03b8\u0303k)\n)))\n= ( e\u22124b ) P\u03b8\u0303 [ \u03c3\u22121(i) = `, \u03c3\u22121(i\u2032) > ` ] . (69)\nThe second inequality uses exp(\u03b8i)W \u2265 e \u22122b/\u03ba and exp(\u03b8\u0303i) W\u0303 \u2264 e2b/\u03ba. Observe that exp(\u03b8\u0303j) = 1 for all j 6= i, i\u2032 and exp(\u03b8\u0303i) + exp(\u03b8\u0303i\u2032) = \u03b1\u0303i,i\u2032,`,\u03b8 \u2264 d\u03b1\u0303i,i\u2032,`,\u03b8e = \u03b1i,i\u2032,`,\u03b8 \u2265 1. Therefore, we have\nP\u03b8\u0303 [ \u03c3\u22121(i) = `, \u03c3\u22121(i\u2032) > ` ] = ( \u03ba\u2212 2 `\u2212 1 ) (\u03b1\u0303i,i\u2032,`,\u03b8/2)(`\u2212 1)! (\u03ba\u2212 2 + \u03b1\u0303i,i\u2032,`,\u03b8)(\u03ba\u2212 2 + \u03b1\u0303i,i\u2032,`,\u03b8 \u2212 1) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 2 + \u03b1\u0303i,i\u2032,`,\u03b8 \u2212 (`\u2212 1)) \u2265 (\u03ba\u2212 2)! (\u03ba\u2212 `\u2212 1)! e\u22122b (\u03ba+ \u03b1i,i\u2032,`,\u03b8 \u2212 2)(\u03ba+ \u03b1i,i\u2032,`,\u03b8 \u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba+ \u03b1i,i\u2032,`,\u03b8 \u2212 (`+ 1)) (70) = e\u22122b(\u03ba\u2212 `+ \u03b1i,i\u2032,`,\u03b8 \u2212 2)(\u03ba\u2212 `+ \u03b1i,i\u2032,`,\u03b8 \u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `)\n(\u03ba+ \u03b1i,i\u2032,`,\u03b8 \u2212 2)(\u03ba+ \u03b1i,i\u2032,`,\u03b8 \u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 1)\n= e\u22122b (\u03ba\u2212 1) (\u03ba\u2212 `+ \u03b1i,i\u2032,`,\u03b8 \u2212 2)(\u03ba\u2212 `+ \u03b1i,i\u2032,`,\u03b8 \u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `) (\u03ba+ \u03b1i,i\u2032,`,\u03b8 \u2212 2)(\u03ba+ \u03b1i,i\u2032,`,\u03b8 \u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba) \u2265 e \u22122b\n(\u03ba\u2212 1)\n( 1\u2212 `\n\u03ba )\u03b1i,i\u2032,`,\u03b8\u22121 =\ne\u22122b(\u03ba\u2212 `) \u03ba(\u03ba\u2212 1)\n( 1\u2212 `\n\u03ba\n)\u03b1i,i\u2032,`,\u03b8\u22122 , (71)\nwhere (70) follows from the fact that \u03b1\u0303i,i\u2032,`,\u03b8 \u2265 2e\u22122b. Claim (49) follows by combining Equations (69) and (71)."}, {"heading": "8.2.6 Proof of Lemma 15", "text": "Analogous to the proof of Lemma 14, we construct a new set of parameters {\u03b8\u0303j}j\u2208[d] from the original \u03b8. We denote the sum of the weights by W \u2261 \u2211 j\u2208S exp(\u03b8j). We define a new set of parameters {\u03b8\u0303j}j\u2208S :\n\u03b8\u0303j =\n{ log(\u03b1\u0303i,`,\u03b8) for j = i ,\n0 otherwise . (72)\nSimilarly define W\u0303 \u2261 \u2211 j\u2208S exp(\u03b8\u0303j) = \u03ba\u2212 1 + \u03b1\u0303i,`,\u03b8. We have,\nP\u03b8 [ \u03c3\u22121(i) = ` ] = \u2211 j1\u2208S j1 6=i ( exp(\u03b8j1) W \u2211 j2\u2208S j2 6=i,j1 ( exp(\u03b8j2) W \u2212 exp(\u03b8j1) \u00b7 \u00b7 \u00b7 ( \u2211 j`\u22121\u2208S j`\u22121 6=i, j1,\u00b7\u00b7\u00b7 ,j`\u22122\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22122 k=j1 exp(\u03b8k)\nexp(\u03b8i) W \u2212 \u2211j`\u22121 k=j1 exp(\u03b8k)\n)))\n\u2264 \u2211 j1\u2208S j1 6=i\n( exp(\u03b8j1)\nW\n\u2211 j2\u2208S j2 6=i,j1\n( exp(\u03b8j2)\nW \u2212 exp(\u03b8j1) \u00b7 \u00b7 \u00b7 ( \u2211 j`\u22121\u2208S j`\u22121 6=i, j1,\u00b7\u00b7\u00b7 ,j`\u22122\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22122 k=j1 exp(\u03b8k)\n))) e2b\n\u03ba\u2212 `+ 1 (73)\nConsider the last summation term in the equation (73), and let \u2126` = S \\ {i, j1, . . . , j`\u22122}, such that |\u2126`| = \u03ba\u2212 `+ 1. Observe that from equation (53), exp(\u03b8i)\u2211\nj\u2208\u2126` exp(\u03b8j)\n\u2265 \u03b1\u0303i,`,\u03b8\u03ba\u2212`+1 . We have,\n\u2211 j`\u22121\u2208\u2126`\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22122 k=j1 exp(\u03b8k) =\n\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\nexp(\u03b8i) + \u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\n\u2264 (\n\u03b1\u0303i,`,\u03b8 \u03ba\u2212 `+ 1 + 1 )\u22121 =\n\u03ba\u2212 `+ 1 \u03b1\u0303i,`,\u03b8 + \u03ba\u2212 `+ 1\n= \u2211\nj`\u22121\u2208\u2126`\nexp(\u03b8\u0303j`\u22121) W\u0303 \u2212 \u2211j`\u22122 k=j1 exp(\u03b8\u0303k) , (74)\nwhere (74) follows from the definition of {\u03b8\u0303}j\u2208S . Consider {\u2126\u02dc\u0300}2\u2264\u02dc\u0300\u2264`\u22121, |\u2126\u02dc\u0300| = \u03ba \u2212 \u02dc\u0300+ 1, corresponding to the subsequent summation terms in (73). Observe that exp(\u03b8i)\u2211 j\u2208\u2126 \u02dc\u0300 exp(\u03b8j) \u2265 \u03b1\u0303i,`,\u03b8/|\u2126\u02dc\u0300|. Therefore, each summation term in equation (65) can be lower bounded by the corresponding term where {\u03b8j}j\u2208S is replaced by {\u03b8\u0303j}j\u2208S . Hence, we have\nP\u03b8 [ \u03c3\u22121(i) = ` ] \u2264\n\u2211 j1\u2208S j1 6=i\n( exp(\u03b8\u0303j1)\nW\u0303\n\u2211 j2\u2208S j2 6=i,j1\n( exp(\u03b8\u0303j2)\nW\u0303 \u2212 exp(\u03b8\u0303j1) \u00b7 \u00b7 \u00b7 ( \u2211 j`\u22121\u2208S j`\u22121 6=i, j1,\u00b7\u00b7\u00b7 ,j`\u22122\nexp(\u03b8\u0303j`\u22121) W\u0303 \u2212 \u2211j`\u22122 k=j1 exp(\u03b8\u0303k)\n))) e2b\n\u03ba\u2212 `+ 1\n\u2264 e4b \u2211 j1\u2208S j1 6=i\n( exp(\u03b8\u0303j1)\nW\u0303\n\u2211 j2\u2208S j2 6=i,j1\n( exp(\u03b8\u0303j2)\nW\u0303 \u2212 exp(\u03b8\u0303j1) \u00b7 \u00b7 \u00b7\n( \u2211 j`\u22121\u2208S j`\u22121 6=i, j1,\u00b7\u00b7\u00b7 ,j`\u22122\nexp(\u03b8\u0303j`\u22121) W\u0303 \u2212 \u2211j`\u22122 k=j1 exp(\u03b8\u0303k)\nexp(\u03b8\u0303i) W\u0303 \u2212 \u2211j`\u22121 k=j1 exp(\u03b8\u0303k)\n)))\n\u2264 e4bP\u03b8\u0303 [ \u03c3\u22121(i) = ` ] (75)\nThe second inequality uses \u03b1\u03032/(\u03ba\u2212 `+ \u03b1\u0303i,`,\u03b8) \u2265 e\u22122b/(\u03ba\u2212 `+ 1). Observe that exp(\u03b8\u0303j) = 1 for all j 6= i and\nexp(\u03b8\u0303i) = \u03b1\u0303i,`,\u03b8 \u2265 b\u03b1\u0303i,`,\u03b8c = \u03b1i,`,\u03b8 \u2265 0. Therefore, we have\nP\u03b8\u0303 [ \u03c3\u22121(i) = ` ] = ( \u03ba\u2212 1 `\u2212 1 ) \u03b1\u0303i,`,\u03b8(`\u2212 1)! (\u03ba\u2212 1 + \u03b1\u0303i,`,\u03b8)(\u03ba\u2212 2 + \u03b1\u0303i,`,\u03b8) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `+ \u03b1\u0303i,`,\u03b8)\n\u2264 (\u03ba\u2212 1)! (\u03ba\u2212 `)!\ne2b\n(\u03ba\u2212 1 + \u03b1i,`,\u03b8)(\u03ba\u2212 2 + \u03b1i,`,\u03b8) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `+ \u03b1i,`,\u03b8)\n\u2264 e 2b\n\u03ba\n( 1\u2212 `\n\u03ba+ \u03b1i,`,\u03b8\n)\u03b1i,`,\u03b8\u22121 , (76)\nNote that equation (76) holds for all values of \u03b1i,`,\u03b8 \u2265 0. Claim 52 follows by combining Equations (75) and (76)."}, {"heading": "8.3 Proof of Theorem 4", "text": "Let H(\u03b8) \u2208 Sd be Hessian matrix such that Hii\u2032(\u03b8) = \u2202 2L(\u03b8)\n\u2202\u03b8i\u2202\u03b8i\u2032 . The Fisher information matrix is defined as\nI(\u03b8) = \u2212E\u03b8[H(\u03b8)]. Fix any unbiased estimator \u03b8\u0302 of \u03b8 \u2208 \u2126b. Since, \u03b8\u0302 \u2208 U , \u03b8\u0302 \u2212 \u03b8 is orthogonal to 1. The Crame\u0301r-Rao lower bound then implies that E[\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162] \u2265 \u2211d i=2 1 \u03bbi(I(\u03b8)) . Taking the supremum over both sides gives\nsup \u03b8 E[\u2016\u03b8\u0302 \u2212 \u03b8\u20162] \u2265 sup \u03b8 d\u2211 i=2\n1\n\u03bbi(I(\u03b8)) \u2265 d\u2211 i=2\n1\n\u03bbi(I(0)) .\nThe following lemma provides a lower bound on E\u03b8[H(0)], where 0 indicates the all-zeros vector.\nLemma 16. Under the hypotheses of Theorem 4,\nE\u03b8[H(0)] \u2212 n\u2211 j=1 2p log(\u03baj) 2 \u03baj(\u03baj \u2212 1) \u2211 i\u2032<i\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> . (77)\nObserve that I(0) is positive semi-definite. Moreover, \u03bb1(I(0)) is zero and the corresponding eigenvector is the all-ones vector. It follows that\nI(0) n\u2211 j=1 2p log(\u03baj) 2 \u03baj(\u03baj \u2212 1) \u2211 i\u2032<i\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>\n2p log(\u03bamax)2 n\u2211 j=1\n1 \u03baj(\u03baj \u2212 1) \u2211\ni\u2032<i\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>\ufe38 \ufe37\ufe37 \ufe38 =L ,\nwhere L is the Laplacian defined for the comparison graph H, Definition 1, as `j = 1 for all j \u2208 [n] in this setting. By Jensen\u2019s inequality, we have\nd\u2211 i=2 1 \u03bbi(L) \u2265 (d\u2212 1) 2\u2211d i=2 \u03bbi(L) = (d\u2212 1)2 Tr(L) = (d\u2212 1)2 n ."}, {"heading": "8.3.1 Proof of Lemma 16", "text": "Define Lj(\u03b8) for j \u2208 [n] such that L(\u03b8) = \u2211n j=1 Lj(\u03b8). Let H(j)(\u03b8) \u2208 Sd be the Hessian matrix such that H (j) ii\u2032 (\u03b8) = \u22022Lj(\u03b8) \u2202\u03b8i\u2202\u03b8i\u2032 for i, i\u2032 \u2208 Sj . We prove that for all j \u2208 [n],\nE\u03b8[H(j)(0)] \u2212 2p log(\u03baj)\n2 \u03baj(\u03baj \u2212 1) \u2211\ni\u2032<i\u2208Sj\n(ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> . (78)\nIn the following, we omit superscript/subscript j for brevity. With a slight abuse of notation, we use I{\u2126\u22121(i)=a} = 1 if item i is ranked at the a-th position in all the orderings \u03c3 \u2208 \u2126. Let P[\u03b8] be the likelihood of observing \u2126\u22121(p) = i(p) and the set \u039b (the set of the items that are ranked before the p-th position). We have,\nP(\u03b8) = \u2211 \u03c3\u2208\u2126\n( exp (\u2211p m=1 \u03b8\u03c3(m) )\u220fp a=1 (\u2211\u03ba m\u2032=a exp ( \u03b8\u03c3(m\u2032) )) ) . (79)\nFor i, i\u2032 \u2208 Sj , we have\nHii\u2032(\u03b8) = 1 P(\u03b8) \u22022P(\u03b8) \u2202\u03b8i\u2202\u03b8i\u2032 \u2212 \u2207iP(\u03b8)\u2207i \u2032P(\u03b8)( P(\u03b8) )2 (80)\nWe claim that at \u03b8 = 0,\n\u2212Hii\u2032(0) = \nC1 if i = i \u2032, { \u2126\u22121(i) \u2265 p }\nC2 +A 2 3 \u2212 C3 if i = i\u2032,\n{ \u2126\u22121(i) < p } \u2212B1 if i 6= i\u2032, { \u2126\u22121(i) \u2265 p, \u2126\u22121(i\u2032) \u2265 p\n} \u2212B2 if i 6= i\u2032, { \u2126\u22121(i) \u2265 p, \u2126\u22121(i\u2032) < p\n} \u2212B2 if i 6= i\u2032, { \u2126\u22121(i) < p, \u2126\u22121(i\u2032) \u2265 p\n} \u2212(B3 +B4 \u2212A23) if i 6= i\u2032, { \u2126\u22121(i) < p, \u2126\u22121(i\u2032) < p } .\n(81)\nwhere constants A3, B1, B2, B3, B4, C1, C2 and C3 are defined in Equations (88), (90), (91), (92), (93), (95), (96) and (97) respectively. From this computation of the Hessian, note that we have\nH(0) = \u2211\ni\u2032<i\u2208S (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>\n( Hii\u2032(0) ) . (82)\nwhich follows directly from the fact that the diagonal entries are summations of the off-diagonals, i.e. C1 = B1(\u03ba \u2212 p) + B2(p \u2212 1) and C2 + A23 \u2212 C3 = B2(\u03ba \u2212 p + 1) + (B3 + B4 \u2212 A23)(p \u2212 2). The second equality follows from the fact that C2 = B2(\u03ba\u2212 p+ 1) +B3(p\u2212 2) and A23(p\u2212 1) = B4(p\u2212 2) +C3. Note that since \u03b8 = 0, all items are exchangeable. Hence, E[Hii\u2032(0)] = E[Hii(0)]/(\u03ba\u2212 1), and substituting this into (82) and using Equations (81), we get\nE [ H(0) ] = \u2212 1\n\u03ba\u2212 1\n( P [ \u2126\u22121(i) \u2265 p ] C1 + P [ \u2126\u22121(i) < p ] (C2 +A 2 3 \u2212 C3) ) \u2211 i\u2032<i\u2208S (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>\n\u2212 1 \u03ba(\u03ba\u2212 1) \u2211 i\u2032<i\u2208S\n(ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>( (\u03ba\u2212 p+ 1) log ( \u03ba\n\u03ba\u2212 p\n) + (p\u2212 1) ( log ( \u03ba\n\u03ba\u2212 p+ 1\n) + log ( \u03ba\n\u03ba\u2212 p+ 1 )2)) (83)\n\u22122p log(\u03ba) 2 \u03ba(\u03ba\u2212 1) \u2211\ni\u2032<i\u2208S (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> , (84)\nwhere (83) uses \u2211p a=1 1 \u03ba\u2212a+1 \u2264 log ( \u03ba \u03ba\u2212p ) and C3 \u2265 0. Equation (84) follows from the fact that for any x > 0, log(1 + x) \u2264 x. To prove (81), we have the first order partial derivative of P(\u03b8) given by\n\u2207iP(\u03b8) = I{\u2126\u22121(i)\u2264p}P(\u03b8)\u2212 \u2211 \u03c3\u2208\u2126\n( exp (\u2211p m=1 \u03b8\u03c3(m) )\u220fp a=1 (\u2211\u03ba m\u2032=a exp ( \u03b8\u03c3(m\u2032) )) ( p\u2211 a=1 I{\u03c3\u22121(i)\u2265a} exp(\u03b8i)\u2211\u03ba m\u2032=a exp ( \u03b8\u03c3(m\u2032) ))) . (85)\nDefine constants A1, A2 and A3 such that\nA1 \u2261 P(\u03b8) \u2223\u2223 {\u03b8=0} = (p\u2212 1)! \u03ba(\u03ba\u2212 1) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 p+ 1) , (86)\nA2 \u2261 ( p\u2211 a=1 exp(\u03b8i)\u2211\u03ba m\u2032=a exp ( \u03b8\u03c3(m\u2032) ))\u2223\u2223\u2223\u2223\u2223 {\u03b8=0} = ( 1 \u03ba + 1 \u03ba\u2212 1 + \u00b7 \u00b7 \u00b7+ 1 \u03ba\u2212 p+ 1 ) , (87)\nA3 \u2261\n( (p\u2212 1)(p\u2212 2)!\n(p\u2212 1)!(\u03ba) + (p\u2212 2)(p\u2212 2)! (p\u2212 1)!(\u03ba\u2212 1) + \u00b7 \u00b7 \u00b7+ (p\u2212 2)! (p\u2212 1)!(\u03ba\u2212 p+ 2)\n) . (88)\nObserve that, for all i \u2208 [d], \u2207iP(\u03b8) \u2223\u2223 {\u03b8=0} = A1 ( I{\u2126\u22121j (i)=p}(1\u2212A2) + I{\u2126\u22121j (i)<p}(1\u2212A3)\u2212 I{\u2126\u22121j (i)>p}A2 ) . (89)\nFurther define constants B1, B2, B3 and B4 such that\nB1 \u2261\n( 1\n\u03ba2 +\n1 (\u03ba\u2212 1)2 + \u00b7 \u00b7 \u00b7+ 1 (\u03ba\u2212 p+ 1)2\n) , (90)\nB2 \u2261\n( p\u2212 1\n(p\u2212 1)\u03ba2 + p\u2212 2 (p\u2212 1)(\u03ba\u2212 1)2 + \u00b7 \u00b7 \u00b7+ 1 (p\u2212 1)(\u03ba\u2212 p+ 2)2\n) , (91)\nB3 \u2261\n( (p\u2212 1)(p\u2212 2)(p\u2212 3)!\n(p\u2212 1)!\u03ba2 + (p\u2212 2)(p\u2212 3)(p\u2212 3)! (p\u2212 1)!(\u03ba\u2212 1)2 + \u00b7 \u00b7 \u00b7+ 2(p\u2212 3)! (p\u2212 1)!(\u03ba\u2212 p+ 3)2\n) , (92)\nB4 \u2261 (p\u2212 3)! (p\u2212 1)! ( \u2211 a,b\u2208[p\u22121],b 6=a ( 1 \u03ba + 1 \u03ba\u2212 1 + \u00b7 \u00b7 \u00b7+ 1 \u03ba\u2212 a+ 1 )( 1 \u03ba + 1 \u03ba\u2212 1 + \u00b7 \u00b7 \u00b7+ 1 \u03ba\u2212 b+ 1 )) . (93)\nObserve that,\n\u22022P(\u03b8) \u2202\u03b8i\u2202\u03b8i\u2032 \u2223\u2223\u2223\u2223 \u03b8=0\n= I{ \u2126\u22121(i),\u2126\u22121(i\u2032)>p }A1((\u2212A2)(\u2212A2) +B1) + ( I{ \u2126\u22121(i)>p,\u2126\u22121(i\u2032)=p } + I{ \u2126\u22121(i)=p,\u2126\u22121(i\u2032)>p })A1((\u2212A2)(1\u2212A2) +B1)\n+ ( I{ \u2126\u22121(i)=p,\u2126\u22121(i\u2032)<p } + I{ \u2126\u22121(i)<p,\u2126\u22121(i\u2032)=p })A1((1\u2212A3) + (\u2212A2)(1\u2212A3) +B2)\n+ ( I{ \u2126\u22121(i)>p,\u2126\u22121(i\u2032)<p } + I{ \u2126\u22121(i)<p,\u2126\u22121(i\u2032)>p })A1((\u2212A2)(1\u2212A3) +B2) + I{ \u2126\u22121(i)<p,\u2126\u22121(i\u2032)<p\n}A1((1\u2212A3) + (\u2212A3) +B4 +B3) . (94) The claims in (81) are easy to verify by combining Equations (89) and (94) with (80). Also, define constants C1, C2 and C3 such that,\nC1 \u2261 ( \u03ba\u2212 1 (\u03ba)2 + \u03ba\u2212 2 (\u03ba\u2212 1)2 + \u00b7 \u00b7 \u00b7+ \u03ba\u2212 p (\u03ba\u2212 p+ 1)2 ) , (95)\nC2 \u2261\n( (p\u2212 1)(p\u2212 2)!(\u03ba\u2212 1)\n(p\u2212 1)!(\u03ba)2 + (p\u2212 2)(p\u2212 2)!(\u03ba\u2212 2) (p\u2212 1)!(\u03ba\u2212 1)2 + \u00b7 \u00b7 \u00b7+ (p\u2212 2)!(\u03ba\u2212 p+ 1) (p\u2212 1)!(\u03ba\u2212 p+ 2)2\n) , (96)\nC3 \u2261 (p\u2212 2)! (p\u2212 1)! ( \u2211 a,b\u2208[p\u22121],b=a ( 1 \u03ba + 1 \u03ba\u2212 1 + \u00b7 \u00b7 \u00b7+ 1 \u03ba\u2212 a+ 1 )( 1 \u03ba + 1 \u03ba\u2212 1 + \u00b7 \u00b7 \u00b7+ 1 \u03ba\u2212 b+ 1 )) , (97)\nsuch that,\n\u22022P(\u03b8) \u2202\u03b82i \u2223\u2223\u2223\u2223 \u03b8=0 = I{\u2126\u22121(i)>p}A1 ( (\u2212A2)(\u2212A2)\u2212 C1 ) + I{\u2126\u22121(i)=p}A1 ( (1\u2212A2)\u2212A2(1\u2212A2)\u2212 C1 )\n+ I{\u2126\u22121(i)<p}A1 ( (1\u2212A3)\u2212A3 \u2212 C2 + C3 ) . (98)\nThe claims (81) is easy to verify by combining Equations (89) and (98) with (80)."}, {"heading": "8.4 Proof of Theorem 5", "text": "The proof is analogous to the proof of Theorem 8. It differs primarily in the lower bound that is achieved for the second smallest eigenvalue of the Hessian matrix H(\u03b8), (35).\nLemma 17. Under the hypotheses of Theorem 5, if \u2211n j=1 `j \u2265 (212e6b/\u03b2\u03b12)d log d then with probability at least 1\u2212 d\u22123,\n\u03bb2(\u2212H(\u03b8)) \u2265 \u03b1 2(1 + e2b)2 1 d\u2212 1 n\u2211 j=1 `j . (99)\nUsing Lemma 10 that is derived for the general value of \u03bbj,a and pj,a, and by substituting \u03bbj,a = 1/(\u03baj\u22121) and pj,a = a for each j \u2208 [n], we get that with probability at least 1\u2212 2e3d\u22123,\n\u2016\u2207LRB(\u03b8\u2217)\u20162 \u2264 \u221a\u221a\u221a\u221a16 log d n\u2211 j=1 `j . (100)\nTheorem 5 follows from Equations (100), (99) and (40)."}, {"heading": "8.4.1 Proof of Lemma 17", "text": "Define M (j) \u2208 Sd as\nM (j) = 1 \u03baj \u2212 1 \u2211\ni<i\u2032\u2208Sj `j\u2211 a=1 I{(i,i\u2032) \u2208 Gj,a}(ei \u2212 ei\u2032)(ei \u2212 ei\u2032) >, (101)\nand let M = \u2211n j=1M\n(j). Similar to the analysis carried out in the proof of Lemma 11, we have \u03bb2(\u2212H(\u03b8)) \u2265 e2b (1+e2b)2 \u03bb2(M), when \u03bbj,a = 1/(\u03baj\u22121) is substituted in the Hessian matrix H(\u03b8), Equation (35). From Weyl\u2019s inequality we have that\n\u03bb2(M) \u2265 \u03bb2(E[M ])\u2212 \u2016M \u2212 E[M ]\u2016 . (102)\nWe will show in (107) that \u03bb2(E[M ]) \u2265 e\u22122b(\u03b1/(d\u22121)) \u2211n j=1 `j and in (112) that \u2016M\u2212E[M ]\u2016 \u2264 32eb \u221a log d \u03b2d \u2211n j=1 `j .\n\u03bb2(M) \u2265 \u03b1e\u22122b\nd\u2212 1 n\u2211 j=1\n`j \u2212 32eb \u221a\u221a\u221a\u221a log d\n\u03b2d n\u2211 j=1 `j \u2265 \u03b1e\u22122b 2(d\u2212 1) n\u2211 j=1 `j , (103)\nwhere the last inequality follows from the assumption that \u2211n j=1 `j \u2265 (212e6b/\u03b2\u03b12)d log d. This proves the desired claim.\nTo prove the lower bound on \u03bb2(E[M ]), notice that\nE[M ] = n\u2211 j=1 1 \u03baj \u2212 1 \u2211 i<i\u2032\u2208Sj E [ `j\u2211 a=1 I{(i,i\u2032)\u2208Gj,a} \u2223\u2223\u2223(i, i\u2032 \u2208 Sj)](ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> . (104)\nUsing the fact that pj,a = a for each j \u2208 [n], and the definition of rank-breaking graph Gj,a, we have that\nE [ `j\u2211 a=1 I{(i,i\u2032)\u2208Gj,a} \u2223\u2223\u2223(i, i\u2032 \u2208 Sj)] = P[I{\u03c3\u22121j (i)\u2264`j} + I{\u03c3\u22121j (i\u2032)\u2264`j} \u2265 1\u2223\u2223\u2223(i, i\u2032 \u2208 Sj)]\n\u2265 P [ (\u03c3\u22121(i) \u2264 `j \u2223\u2223\u2223(i, i\u2032 \u2208 Sj)] . (105) The following lemma provides a lower bound on P[(\u03c3\u22121(i) \u2264 `j |(i, i\u2032 \u2208 Sj)].\nLemma 18. Consider a ranking \u03c3 over a set of items S of size \u03ba. For any item i \u2208 S,\nP[(\u03c3\u22121(i) \u2264 `] \u2265 e\u22122b ` \u03ba . (106)\nTherefore, using the fact that (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> is positive semi-definite, and Equations (104), (105) and (106) we have\nE[M ] e\u22122b n\u2211 j=1 `j \u03baj(\u03baj \u2212 1) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> = e\u22122bL, (107)\nwhere L is the Laplacian defined for the comparison graphH, Definition 1. Using \u03bb2(L) = (\u03b1/(d\u22121)) \u2211n j=1 `j\nfrom (5), we get the desired bound \u03bb2(E[M ]) \u2265 e\u22122b(\u03b1/(d\u2212 1)) \u2211n j=1 `j .\nFor top-`j rank breaking, M (j) is also given by\nM (j) = 1\n\u03baj \u2212 1\n( (\u03baj \u2212 `j)diag(e{Ij}) + `jdiag(e{Sj})\u2212 e{Ij}e > {Sj} \u2212 e{Sj}e > {Ij} + e{Ij}e > {Ij} ) , (108)\nwhere e{Sj}, e{Ij} \u2208 Rd are zero-one vectors, e{Sj} has support corresponding to the set of items Sj and e{Ij} has support corresponding to the random top-`j items in the ranking \u03c3j . Ij = {\u03c3j(1), \u03c3j(2), \u00b7 \u00b7 \u00b7 , \u03c3j(`j)} for j \u2208 [n]. (M (j))2 is given by\n(M (j))2 = 1 (\u03baj \u2212 1)2 ( (\u03ba2j \u2212 `2j )diag(e{Ij}) + `j 2diag(e{Sj})\u2212\n(\u03baj + `j)(e{Ij}e > {Sj} + e{Sj}e > {Ij} \u2212 e{Ij}e > {Ij}) + `je{Sj}e > {Sj}\n) .\nNote that P[i \u2208 Ij |i \u2208 Sj ] \u2264 `je2b/\u03baj for all i \u2208 Sj . Its proof is similar to the proof of Lemma 18. Therefore, we have E[diag(e{Ij})] `je2b/\u03bajdiag(e{1}). To bound \u2016 \u2211n j=1 E[(M (j))2]\u2016, we use the fact that\nfor J \u2208 Rd\u00d7d, \u2016J\u2016 \u2264 maxi\u2208[d] \u2211d i\u2032=1 |Jii\u2032 |. Maximum of row sums of E[e{Ij}e>{Ij}] is upper bounded by\nmaxi\u2208[d] { `jP[i \u2208 Ij |i \u2208 Sj ] } \u2264 `j2e2b/\u03baj . Therefore using triangle inequality, we have,\u2225\u2225\u2225\u2225\u2225 n\u2211 j=1 E [ (M (j))2\n]\u2225\u2225\u2225\u2225\u2225 \u2264 max\ni\u2208[d] { \u2211 j:i\u2208Sj\n1\n(\u03baj \u2212 1)2\n( (\u03ba2j \u2212 `j 2)`je 2b\n\u03baj + `j\n2 + e2b(\u03baj + `j)(2`j + `j 2/\u03baj) + `j\u03baj\n)}\n\u2264 max i\u2208[d] { \u2211 j:i\u2208Sj `je 2b \u03baj ( (\u03ba2j \u2212 `j 2) (\u03baj \u2212 1)2 + `j\u03baj (\u03baj \u2212 1)2 + 2(\u03baj + `j)\u03baj (\u03baj \u2212 1)2 + (\u03baj + `j)`j (\u03baj \u2212 1)2 + \u03ba2j (\u03baj \u2212 1)2 )}\n\u2264 max i\u2208[d] { \u2211 j:i\u2208Sj `je 2b \u03baj ( (\u03ba2j \u2212 1) (\u03baj \u2212 1)2 + \u03baj(\u03baj \u2212 1) (\u03baj \u2212 1)2 + 4\u03ba2j (\u03baj \u2212 1)2 + 2\u03baj(\u03baj \u2212 1) (\u03baj \u2212 1)2 + \u03ba2j (\u03baj \u2212 1)2 )}\n\u2264 max i\u2208[d] { \u2211 j:i\u2208Sj `je 2b \u03baj ( 3 + 2 + 16 + 4 + 4 )} (109)\n\u2264 29e2b max i\u2208[d] { \u2211 j:i\u2208Sj `j \u03baj } = 29e2bDmax (110)\n= 29e2b\n\u03b2d n\u2211 j=1 `j , (111)\nwhere (109) uses the fact that \u03baj \u2265 2 and 1 \u2264 `j \u2264 \u03baj \u2212 1 for all j \u2208 [n]. (110) follows from the definition of Dmax, Definition 1 and (111) follows from the Equation (6). Also, note that \u2016Mj\u2016 \u2264 2 for all j \u2208 [n]. Applying matrix Bernstien inequality, we have,\nP [ \u2016M \u2212 E[M ]\u2016 \u2265 t ] \u2264 d exp\n( \u2212t2/2\n29e2b\n\u03b2d \u2211n j=1 `j + 4t/3\n) .\nTherefore, with probability at least 1\u2212 d\u22123, we have,\n\u2016M \u2212 E[M ]\u2016 \u2264 22eb \u221a\u221a\u221a\u221a log d\n\u03b2d n\u2211 j=1 `j + 64 log d 3 \u2264 32eb \u221a\u221a\u221a\u221a log d \u03b2d n\u2211 j=1 `j , (112)\nwhere the second inequality follows from the assumption that \u2211n j=1 `j \u2265 212d log d and \u03b2 \u2264 1."}, {"heading": "8.4.2 Proof of Lemma 18", "text": "Define imin \u2261 arg mini\u2208S \u03b8i. We claim the following. For all i \u2208 S and any 1 \u2264 ` \u2264 |S| \u2212 1,\nP[\u03c3\u22121(i) > `] \u2264 P[\u03c3\u22121(imin) > `] and P[\u03c3\u22121(imin) = `] \u2265 P[\u03c3\u22121(imin) = 1] . (113)\nTherefore P[\u03c3\u22121(i) \u2264 `] \u2265 P[\u03c3\u22121(imin) \u2264 `]. Using P[\u03c3\u22121(imin) = 1] > e\u22122b/\u03ba, we get the desired bound P[\u03c3\u22121(i) \u2264 `] > e\u22122b`/\u03ba.\nTo prove the claim (113), let \u03c3\u0302`1 denote a ranking of top-` items of the set S and P[\u03c3\u0302`1] be the probability of observing \u03c3\u0302`1. Let i \u2208 (\u03c3\u0302`1)\u22121 denote that i = (\u03c3\u0302`1)\u22121(j) for some 1 \u2264 j \u2264 `. Let\n\u21261 = { \u03c3\u0302`1 : i /\u2208 (\u03c3\u0302`1)\u22121, imin \u2208 (\u03c3\u0302`1)\u22121 } and \u21262 = { \u03c3\u0302`1 : i \u2208 (\u03c3\u0302`1)\u22121, imin /\u2208 (\u03c3\u0302`1)\u22121 } .\nWe have P[\u03c3\u22121(i) > `] \u2212 P[\u03c3\u22121(imin) > `] = \u2211 \u03c3\u0302`1\u2208\u21261 P[\u03c3\u0302`1] \u2212 \u2211 \u03c3\u0302`1\u2208\u21262\nP[\u03c3\u0302`1]. Now, take any ranking \u03c3\u0302`1 \u2208 \u21261 and construct another ranking \u03c3\u0303`1 from \u03c3\u0302 ` 1 by replacing imin with i-th item. Observe that P[\u03c3\u0302`1] \u2264 P[\u03c3\u0303`1] and \u03c3\u0303`1 \u2208 \u21262. Moreover, such a construction gives a bijective mapping between \u21261 and \u21262. Hence, the first claim is proved. For the second claim, let\n\u2126\u03021 = { \u03c3\u0302`1 : (\u03c3\u0302 ` 1) \u22121(imin) = 1 } and \u2126\u03022 = { \u03c3\u0302`1 : (\u03c3\u0302 ` 1) \u22121(imin) = ` } .\nWe have P[\u03c3\u22121(imin) = 1]\u2212P[\u03c3\u22121(imin) = `] = \u2211 \u03c3\u0302`1\u2208\u2126\u03021 P[\u03c3\u0302`1]\u2212 \u2211 \u03c3\u0302`1\u2208\u2126\u03022\nP[\u03c3\u0302`1]. Now, take any ranking \u03c3\u0302`1 \u2208 \u2126\u03021 and construct another ranking \u03c3\u0303`1 from \u03c3\u0302 ` 1 by swapping items at 1st position and `-th position. Observe that P[\u03c3\u0302`1] \u2264 P[\u03c3\u0303`1] and \u03c3\u0303`1 \u2208 \u2126\u03022. Moreover, such a construction gives a bijective mapping between \u2126\u03021 and \u2126\u03022. Hence, the claim is proved."}, {"heading": "8.5 Proof of Theorem 6", "text": "The first order partial derivative of L(\u03b8), Equation (15), is given by\n\u2207iL(\u03b8)\n= \u2211 j:i\u2208Sj `j\u2211 m=1 I{\u03c3\u22121j (i)\u2265m} [ I{\u03c3j(m)=i} \u2212\nexp(\u03b8i)\nexp(\u03b8\u03c3j(m)) + exp(\u03b8\u03c3j(m+1)) + \u00b7 \u00b7 \u00b7+ exp(\u03b8\u03c3j(\u03baj))\n] , \u2200i \u2208 [d]\nand the Hessian matrix H(\u03b8) \u2208 Sd with Hii\u2032(\u03b8) = \u2202 2L(\u03b8)\n\u2202\u03b8i\u2202\u03b8i\u2032 is given by\nH(\u03b8) = \u2212 n\u2211 j=1 \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> `j\u2211 m=1 exp(\u03b8i + \u03b8i\u2032)I{\u03c3\u22121j (i),\u03c3\u22121j (i\u2032)\u2265m} [exp(\u03b8\u03c3j(m)) + exp(\u03b8\u03c3j(m+1)) + \u00b7 \u00b7 \u00b7+ exp(\u03b8\u03c3j(\u03baj))]2 . (114)\nIt follows from the definition that \u2212H(\u03b8) is positive semi-definite for any \u03b8 \u2208 Rn. The Fisher information matrix is defined as I(\u03b8) = \u2212E\u03b8[H(\u03b8)] and given by\nI(\u03b8) = n\u2211 j=1 \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> `j\u2211 m=1 E\n[ I{\u03c3\u22121j (i),\u03c3\u22121j (i\u2032)\u2265m}\n[exp(\u03b8\u03c3j(m)) + \u00b7 \u00b7 \u00b7+ exp(\u03b8\u03c3j(\u03baj))]2\n] exp(\u03b8i + \u03b8i\u2032).\nSince \u2212H(\u03b8) is positive semi-definite, it follows that I(\u03b8) is positive semi-definite. Moreover, \u03bb1(I(\u03b8)) is zero and the corresponding eigenvector is the all-ones vector. Fix any unbiased estimator \u03b8\u0302 of \u03b8 \u2208 \u2126b. Since, \u03b8\u0302 \u2208 U , \u03b8\u0302\u2212\u03b8 is orthogonal to 1. The Crame\u0301r-Rao lower bound then implies that E[\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162] \u2265 \u2211d i=2 1 \u03bbi(I(\u03b8)) . Taking the supremum over both sides gives\nsup \u03b8 E[\u2016\u03b8\u0302 \u2212 \u03b8\u20162] \u2265 sup \u03b8 d\u2211 i=2\n1\n\u03bbi(I(\u03b8)) \u2265 d\u2211 i=2\n1\n\u03bbi(I(0)) .\nIf \u03b8 equals the all-zero vector, then\nP\u03b8[\u03c3\u22121j (i), \u03c3 \u22121 j (i\n\u2032) \u2265 m] = ( \u03baj\u2212m+1 2 )( \u03baj 2 ) = (\u03baj \u2212m+ 1)(\u03baj \u2212m) \u03baj(\u03baj \u2212 1) .\nIt follows from the definition that\nI(0) = n\u2211 j=1 \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> `j\u2211 m=1 (\u03baj \u2212m) \u03baj(\u03baj \u2212 1)(\u03baj \u2212m+ 1)\n` (\n1\u2212 1 `j `j\u2211 m=1\n1\n\u03bamax \u2212m+ 1 ) n\u2211 j=1\n1 \u03baj(\u03baj \u2212 1) \u2211\ni<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)>\ufe38 \ufe37\ufe37 \ufe38 =L ,\nwhere L is the Laplacian defined for the comparison graph H, Definition 1. By Jensen\u2019s inequality, we have\nd\u2211 i=2 1 \u03bbi(L) \u2265 (d\u2212 1) 2\u2211d i=2 \u03bbi(L) = (d\u2212 1)2 Tr(L) = (d\u2212 1)2 n ."}, {"heading": "8.6 Proof of Theorem 7", "text": "We prove a slightly more general result that implies the desired theorem. For ` \u2265 4, we can choose \u03b21 = 1/2. Then, the condition that \u03b3\u03b21 \u2264 1 implies d\u0303 \u2264 (`/2 + 1)(d\u2212 2)/(\u03ba\u2212 2), which implies d\u0303 \u2264 `d/(2\u03ba). With the choice of d\u0303 = `d/(2\u03ba), this implies Theorem 7.\nTheorem 19. Under the bottom-` separators scenario and the PL model, n partial orderings are sampled over d items parametrized by \u03b8\u2217 \u2208 \u2126b. For any \u03b21 with 0 \u2264 \u03b21 \u2264 `\u22122` , define\n\u03b3\u03b21 \u2261 d\u0303(\u03ba\u2212 2)\n(b`\u03b21c+ 1)(d\u2212 2) , (115)\nand for \u03b3\u03b21 \u2264 1,\n\u03c7\u03b21 \u2261 ( 1\u2212 b`\u03b21c /` )2( 1\u2212 exp ( \u2212 (b`\u03b21c+ 1) 2(1\u2212 \u03b3\u03b21)2\n2(\u03ba\u2212 2)\n)) . (116)\nIf\nn` \u2265 ( 212e8b\n\u03c72\u03b21\nd2\nd\u03032\n\u03ba\n`\n) d log d , (117)\nthen the rank-breaking estimator in (18) achieves\n1\u221a d\u0303\n\u2225\u2225\u0302\u0303\u03b8 \u2212 \u03b8\u0303\u2217\u2225\u2225 2 \u2264 32 \u221a 2(1 + e4b)2\n\u03c7\u03b21\nd3/2\nd\u03033/2\n\u221a d log d\nn` , (118)\nwith probability at least 1\u2212 3e3d\u22123.\nProof is very similar to the proof of Theorem 8. It mainly differs in the lower bound that is achieved for the second smallest eigenvalue of the Hessian matrix H(\u03b8\u0303) of LRB(\u03b8\u0303), Equation (17). Equation (17) can be rewritten as\nLRB(\u03b8\u0303) = n\u2211 j=1 \u2211\u0300 a=1 \u2211 i<i\u2032\u2208Sj :i,i\u2032\u2208[d\u0303] I{ (i,i\u2032)\u2208Gj,a }\u03bbj,a(\u03b8\u0303iI{ \u03c3\u22121j (i)<\u03c3 \u22121 j (i \u2032) } + \u03b8\u0303i\u2032I{ \u03c3\u22121j (i)>\u03c3 \u22121 j (i \u2032) } \u2212 log (e\u03b8\u0303i + e\u03b8\u0303i\u2032)) ,\n(119)\nwhere (i, i\u2032) \u2208 Gj,a implies either (i, i\u2032) or (i\u2032, i) belong to Ej,a. The Hessian matrix H(\u03b8\u0303) \u2208 S d\u0303 with Hii\u2032(\u03b8\u0303) =\n\u22022LRB(\u03b8\u0303) \u2202\u03b8\u0303i\u2202\u03b8\u0303i\u2032 is given by\nH(\u03b8\u0303) = \u2212 n\u2211 j=1 \u2211\u0300 a=1 \u2211 i<i\u2032\u2208Sj : i,i\u2032\u2208[d\u0303] I{ (i,i\u2032)\u2208Gj,a }((e\u0303i \u2212 e\u0303i\u2032)(e\u0303i \u2212 e\u0303i\u2032)> exp(\u03b8\u0303i + \u03b8\u0303i\u2032) [exp(\u03b8\u0303i) + exp(\u03b8\u0303i\u2032)]2 ) . (120)\nThe following lemma gives a lower bound for \u03bb2(\u2212H(\u03b8\u0303)).\nLemma 20. Under the hypothesis of Theorem 19, with probability at least 1\u2212 d\u22123,\n\u03bb2(\u2212H(\u03b8\u0303)) \u2265 \u03c7\u03b21 8(1 + e4b)2 nd\u0303`2 d2 . (121)\nObserve that although \u03b8\u0303\u2217 \u2208 Rd\u0303, Lemma 10 can be directly applied to upper bound \u2016\u2207LRB(\u03b8\u0303\u2217)\u20162. It might be possible to tighten the upper bound, given that d\u0303 \u2264 d. However, for ` \u03ba, for the smallest preference score item, imin \u2261 arg mini\u2208[d] \u03b8\u0303\u2217i , the upper bound P[\u03c3\u22121(imin) > \u03ba\u2212`] \u2264 1 is tight upto constant factor (Lemma 15). Substituting \u03bbj,a = 1 and pj,a = \u03ba \u2212 ` + a for each j \u2208 [n], a \u2208 [`], in Lemma 10, we have that with probability at least 1\u2212 2e3d\u22123,\n\u2016\u2207LRB(\u03b8\u0303\u2217)\u20162 \u2264 (`\u2212 1) \u221a 8n` log d. (122)\nTheorem 19 follows from Equations (40), (121) and (122)."}, {"heading": "8.6.1 Proof of Lemma 20", "text": "Define M\u0303 (j) \u2208 S d\u0303,\nM\u0303 (j) = \u2211\ni<i\u2032\u2208Sj :i,i\u2032\u2208[d\u0303]\n\u2211\u0300 a=1 I{(i,i\u2032)\u2208Gj,a}(e\u0303i \u2212 e\u0303i\u2032)(e\u0303i \u2212 e\u0303i\u2032) >, (123)\nand let M\u0303 = \u2211n j=1 M\u0303 (j). Similar to the analysis in Lemma 11, we have \u03bb2(\u2212H(\u03b8\u0303)) \u2265 e 4b (1+e4b)2 \u03bb2(M\u0303). Note that we have e4b instead of e2b as \u03b8\u0303 \u2208 \u2126\u03032b. We will show a lower bound on \u03bb2(E[M\u0303 ]) in (129) and an upper bound on \u2016M\u0303 \u2212 E[M\u0303 ]\u2016 in (133). Therefore using \u03bb2(M\u0303) \u2265 \u03bb2(E[M\u0303 ])\u2212 \u2016M\u0303 \u2212 E[M\u0303 ]\u2016,\n\u03bb2(M\u0303) \u2265 e\u22124b\n4 (1\u2212 \u03b21)2\n( 1\u2212 exp ( \u2212 (b`\u03b21c+ 1) 2(1\u2212 \u03b3\u03b21)2\n2(\u03ba\u2212 2) )) \ufe38 \ufe37\ufe37 \ufe38\n\u2261\u03c7\u03b21\nnd\u0303`2\nd2 \u2212 8`\n\u221a n\u03ba log d\nd . (124)\nThe desired claim follows from the assumption that n` \u2265 ( 212e8b\n\u03c72\u03b21\nd2 d\u03032 \u03ba ` ) d log d, where \u03c7\u03b21 is defined in (117).\nTo prove the lower bound on \u03bb2(E[M\u0303 ]), notice that\nE [ M\u0303 ] = n\u2211 j=1 \u2211 i<i\u2032\u2208[d\u0303] E [\u2211\u0300 a=1 I{ (i,i\u2032)\u2208Gj,a }\u2223\u2223\u2223(i, i\u2032 \u2208 Sj)]P[i, i\u2032 \u2208 Sj](e\u0303i \u2212 e\u0303i\u2032)(e\u0303i \u2212 e\u0303i\u2032)> . (125) Since the sets Sj are chosen uniformly at random, P[i, i\u2032 \u2208 Sj ] = \u03ba(\u03ba \u2212 1)/d(d \u2212 1). Using the fact that pj,a = \u03ba\u2212 `+ a for each j \u2208 [n], and the definition of rank breaking graph Gj,a, we have that\nE [\u2211\u0300 a=1 I{ (i,i\u2032)\u2208Gj,a }\u2223\u2223\u2223(i, i\u2032 \u2208 Sj)] = P[(\u03c3\u22121j (i), \u03c3\u22121j (i\u2032) > \u03ba\u2212 `)\u2223\u2223\u2223(i, i\u2032 \u2208 Sj)] . (126) The following lemma provides a lower bound on P[(\u03c3\u22121j (i), \u03c3 \u22121 j (i \u2032)) > \u03ba\u2212 `|(i, i\u2032 \u2208 Sj)].\nLemma 21. Under the hypotheses of Theorem 19, for any two items i, i\u2032 \u2208 [d\u0303], the following holds:\nP [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 ` \u2223\u2223\u2223 i, i\u2032 \u2208 S] \u2265 e\u22124b(1\u2212 \u03b21)2(1\u2212 exp(\u2212\u03b7\u03b21(1\u2212 \u03b3\u03b21)2)) 2 `2 \u03ba2 , (127)\nwhere \u03b3\u03b21 \u2261 d\u0303(\u03ba\u2212 2)/(b`\u03b21c+ 1)(d\u2212 2) and \u03b7\u03b21 \u2261 (b`\u03b21c+ 1)2/2(\u03ba\u2212 2).\nTherefore, using Equations (125), (126) and (127) we have,\nE [ M\u0303 ] e\n\u22124b(1\u2212 \u03b21)2(1\u2212 exp(\u2212\u03b7\u03b21(1\u2212 \u03b3\u03b21)2)) 2 `2 \u03ba2 \u03ba(\u03ba\u2212 1) d(d\u2212 1) n\u2211 j=1 \u2211 i<i\u2032\u2208[d\u0303] (e\u0303i \u2212 e\u0303i\u2032)(e\u0303i \u2212 e\u0303i\u2032)> . (128)\nDefine L\u0303 = \u2211n j=1 \u2211 i<i\u2032\u2208[d\u0303](e\u0303i \u2212 e\u0303i\u2032)(e\u0303i \u2212 e\u0303i\u2032)\n>. We have, \u03bb1(L\u0303) = 0 and \u03bb2(L\u0303) = \u03bb3(L\u0303) = \u00b7 \u00b7 \u00b7 = \u03bbd\u0303(L\u0303). Therefore, using \u03bb2(L\u0303) = Tr(L\u0303)/(d\u0303\u2212 1) = nd\u0303. Using the fact that E[M\u0303 ] and L\u0303 are symmetric matrices, we have,\n\u03bb2(E [ M\u0303 ] ) \u2265 e\n\u22124b(1\u2212 \u03b21)2(1\u2212 exp(\u2212\u03b7\u03b21(1\u2212 \u03b3\u03b21)2)) 4 nd\u0303`2 d2 . (129)\nTo get an upper bound on \u2016M\u0303 \u2212 E[M\u0303 ]\u2016, notice that M\u0303 (j) is also given by,\nM\u0303 (j) = ` diag(e\u0303{Ij})\u2212 e\u0303{Ij}e\u0303 > {Ij} , (130)\nwhere e\u0303{Ij} \u2208 Rd\u0303 is a zero-one vector, with support corresponding to the bottom-` subset of items in the ranking \u03c3j . Ij = {\u03c3j(\u03ba\u2212 `+ 1), \u00b7 \u00b7 \u00b7 , \u03c3j(\u03ba)} for j \u2208 [n]. (M\u0303 (j))2 is given by\n(M\u0303 (j))2 = `2 diag(e\u0303{Ij})\u2212 ` e\u0303{Ij}e\u0303 > {Ij} . (131)\nUsing the fact that sets {Sj}j\u2208[n] are chosen uniformly at random and P[i \u2208 Ij |i \u2208 Sj ] \u2264 1, we have E[diag(e\u0303{Ij})] (\u03ba/d)diag(e\u0303{1}). Maximum of row sums of E [ e\u0303{Ij}e\u0303 > {Ij} ] is upper bounded by `\u03ba/d. There-\nfore, from triangle inequality we have \u2016 \u2211n j=1 E[(M\u0303 (j))2]\u2016 \u2264 2n`2\u03ba/d. Also, note that \u2016M\u0303 (j)\u2016 \u2264 2` for all j \u2208 [n]. Applying matrix Bernstien inequality, we have that\nP [ \u2016M\u0303 \u2212 E[M\u0303 ]\u2016 \u2265 t ] \u2264 d exp ( \u2212t2/2 2n`2\u03ba/d+ 4`t/3 ) . (132)\nTherefore, with probability at least 1\u2212 d\u22123, we have,\n\u2016M\u0303 \u2212 E[M\u0303 ]\u2016 \u2264 4` \u221a 2n\u03ba log d\nd +\n64` log d\n3 \u2264 8`\n\u221a n\u03ba log d\nd , (133)\nwhere the second inequality follows from the assumption that n` \u2265 212d log d."}, {"heading": "8.6.2 Proof of Lemma 21", "text": "Without loss of generality, assume that i\u2032 < i, i.e., \u03b8\u0303\u2217i\u2032 \u2264 \u03b8\u0303\u2217i . Define \u2126 such that \u2126 = {j : j \u2208 S, j 6= i, i\u2032}. For any \u03b21 \u2208 [0, (` \u2212 2)/`], define event E\u03b21 that occurs if in the randomly chosen set S there are at most b`\u03b21c items that have preference scores less than \u03b8\u0303\u2217i , i.e.,\nE\u03b21 \u2261 {\u2211 j\u2208\u2126 I{\u03b8\u0303\u2217i>\u03b8\u0303\u2217j } \u2264 b`\u03b21c } . (134)\nWe have,\nP [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 ` \u2223\u2223\u2223 i, i\u2032 \u2208 S] > P [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 `\n\u2223\u2223\u2223 i, i\u2032 \u2208 S;E\u03b21]P[E\u03b21 \u2223\u2223\u2223 i, i\u2032 \u2208 S] (135) The following lemma provides a lower bound on P[\u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 ` | i, i\u2032 \u2208 S;E\u03b21 ].\nLemma 22. Under the hypotheses of Lemma 21,\nP [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 ` \u2223\u2223\u2223 i, i\u2032 \u2208 S;E\u03b21] \u2265 e\u22124b(1\u2212 b`\u03b21c /`)22 `2\u03ba2 . (136) Next, we provide a lower bound on P[E\u03b21 | i, i\u2032 \u2208 S]. Fix i, i\u2032 such that i, i\u2032 \u2208 S. Selecting a set uniformly at random is probabilistically equivalent to selecting items one at a time uniformly at random without replacement. Without loss of generality, assume that i, i\u2032 are the 1st and 2nd pick. Define Bernoulli random variables Yj\u2032 for 3 \u2264 j\u2032 \u2264 \u03ba corresponding to the outcome of the j\u2032-th random pick from the set of (d\u2212 j\u2032 \u2212 1) items to generate the set \u2126 such that Yj\u2032 = 1 if and only if \u03b8\u0303\u2217i > \u03b8\u0303\u2217j\u2032 .\nRecall that \u03b3\u03b21 \u2261 d\u0303(\u03ba\u22122)/(b`\u03b21c+1)(d\u22122) and \u03b7\u03b21 \u2261 (b`\u03b21c+1)2/2(\u03ba\u22122). Construct Doob\u2019s martingale (Z2, \u00b7 \u00b7 \u00b7 , Z\u03ba) from {Yk\u2032}3\u2264k\u2032\u2264\u03ba such that Zj\u2032 = E[ \u2211\u03ba k\u2032=3 Yk\u2032 | Y3, \u00b7 \u00b7 \u00b7 , Yj\u2032 ], for 2 \u2264 j\u2032 \u2264 \u03ba. Observe that,\nZ2 = E[ \u2211\u03ba k\u2032=3 Yk\u2032 ] \u2264 (i\u22122)(\u03ba\u22122) d\u22122 \u2264 \u03b3\u03b21(b`\u03b21c+ 1), where the last inequality follows from the assumption that i \u2264 d\u0303. Also, |Zj\u2032 \u2212 Zj\u2032\u22121| \u2264 1 for each j\u2032. Therefore, we have\nP [\u2211 j\u2208\u2126 I{\u03b8\u0303\u2217i>\u03b8\u0303\u2217j } \u2264 b`\u03b21c ] = P [\u2211\u03ba j\u2032=3 Yj\u2032 \u2264 b`\u03b21c ]\n= 1\u2212 P [\u2211\u03ba j\u2032=3 Yj\u2032 \u2265 b`\u03b21c+ 1 ]\n\u2265 1\u2212 P [ Z\u03ba\u22122 \u2212 Z2 \u2265 (`\u03b21 + 1)\u2212 \u03b3(b`\u03b21c+ 1) ] \u2265 1\u2212 exp ( \u2212 (b`\u03b21c+ 1) 2(1\u2212 \u03b31)2\n2(\u03ba\u2212 2) ) = 1\u2212 exp ( \u2212 \u03b7\u03b21(1\u2212 \u03b3\u03b21)2 ) , (137)\nwhere the inequality follows from the Azuma-Hoeffding bound. Since, the above inequality is true for any fixed i, i\u2032 \u2208 S, for random indices i, i\u2032 we have P[E\u03b21 | i, i\u2032 \u2208 S] \u2265 1 \u2212 exp(\u2212\u03b7\u03b21(1 \u2212 \u03b3\u03b21)2). Claim (127) follows by combining Equations (135), (136) and (137)."}, {"heading": "8.6.3 Proof of Lemma 22", "text": "Without loss of generality, assume that i\u2032 < i, i.e., \u03b8\u0303\u2217i\u2032 \u2264 \u03b8\u0303\u2217i . Define \u2126 = {j : j \u2208 S, j 6= i, i\u2032}, and event E\u03b21 = {i, i\u2032 \u2208 S; \u2211 j\u2208\u2126 I{\u03b8\u0303\u2217i>\u03b8\u0303\u2217j } \u2264 b`\u03b21c}. Since set S is chosen randomly, i, i\n\u2032 and j \u2208 \u2126 are random. Throughout this section, we condition on the random indices i, i\u2032 and the set \u2126 such that event E\u03b21 holds. To get a lower bound on P[\u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 `], define independent exponential random variables Xj \u223c exp(e\u03b8\u0303 \u2217 j ) for j \u2208 S. Observe that given event E\u03b21 holds, there exists a set \u21261 \u2286 \u2126 such that\n\u21261 = { j \u2208 S : \u03b8\u0303\u2217i \u2264 \u03b8\u0303\u2217j } , (138)\nand |\u21261| = \u03ba\u2212b`\u03b21c\u22122. In fact there can be many such sets, and for the purpose of the proof we can choose one such set arbitrarily. Note that b`\u03b21c + 2 \u2264 ` by assumption on \u03b21, so |\u21261| \u2265 \u03ba \u2212 `. From the Random Utility Model (RUM) interpretation of the PL model, we know that the PL model is equivalent to ordering the items as per random cost of each item drawn from exponential random variable with mean e\u03b8\u0303 \u2217 i . That is, we rank items according to Xj \u2019s such that the lower cost items are ranked higher. From this interpretation, we have that\nP [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 ` ] = P [\u2211 j\u2208\u2126 I{ min{Xi,Xi\u2032} > Xj } \u2265 \u03ba\u2212 `] > P\n[ \u2211 j\u2032\u2208\u21261 I{ min{Xi,Xi\u2032} > Xj\u2032 } \u2265 \u03ba\u2212 `] (139)\nThe above inequality follows from the fact that \u21261 \u2286 \u2126 and |\u21261| \u2265 \u03ba\u2212 `. It excludes some of the rankings over the items of the set S that constitute the event {\u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba \u2212 `}. Define \u21262 = {\u21261, i, i\u2032}. Observe that items i, i\u2032 have the least preference scores among all the items in the set \u21262. Therefore, the term in Equation (139) is the probability of the least two preference score items in the set \u21262, that is of size (\u03ba\u2212 b`\u03b21c), being ranked in bottom (`\u2212 b`\u03b21c) positions.\nThe following lemma shows that the probability of the least two preference score items in a set being ranked at any two positions is lower bounded by their probability of being ranked at 1st and 2nd position.\nLemma 23. Consider a set of items S and a ranking \u03c3 over it. Define imin1 \u2261 arg mini\u2208S \u03b8i, imin2 \u2261 arg mini\u2208S\\imin1 \u03b8i. For all 1 \u2264 i1, i2 \u2264 |S|, i1 6= i2, following holds:\nP [ \u03c3\u22121(imin1) = i1, \u03c3 \u22121(imin2) = i2 ] \u2265 P [ \u03c3\u22121(imin1) = 1, \u03c3 \u22121(imin2) = 2 ] . (140)\nUsing the fact that i\u2032 = arg minj\u2208\u21262 \u03b8\u0303 \u2217 j , i = arg minj\u2208\u21262\\i\u2032 \u03b8\u0303 \u2217 j , for all 1 \u2264 i1, i2 \u2264 \u03ba \u2212 b`\u03b21c, i1 6= i2, we\nhave that\nP [ \u03c3\u22121(i\u2032) = i1, \u03c3 \u22121(i) = i2 ] \u2265 P [ \u03c3\u22121(i\u2032) = 1, \u03c3\u22121(i) = 2 ] \u2265 e\u22124b 1\n\u03ba2 , (141)\nwhere the second inequality follows from the definition of the PL model and the fact that \u03b8\u0303\u2217 \u2208 \u2126\u03032b. Together with Equation (141) and the fact that there are a total of (` \u2212 b`\u03b2c)(` \u2212 b`\u03b2c \u2212 1) \u2265 (` \u2212 b`\u03b2c)2/2 pair of positions that i, i\u2032 can occupy in order to being ranked in bottom (`\u2212 b`\u03b2c), we have,\nP [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > \u03ba\u2212 ` ] \u2265 e \u22124b(1\u2212 b`\u03b21c /`)2\n2\n`2 \u03ba2 . (142)\nSince, the above inequality is true for any fixed i, i\u2032 and j \u2208 \u2126 such that event E holds, it is true for random indices i, i\u2032 and j \u2208 \u2126 such that event E holds, hence the claim is proved."}, {"heading": "8.6.4 Proof of Lemma 23", "text": "Let \u03c3\u0302 denote a ranking over the items of the set S and P[\u03c3\u0302] be the probability of observing \u03c3\u0302. Let \u2126\u03021 = { \u03c3\u0302 : \u03c3\u0302\u22121(imin1) = i1, \u03c3\u0302 \u22121(imin2) = i2 } and \u2126\u03022 = { \u03c3\u0302 : \u03c3\u22121(imin1) = 1, \u03c3 \u22121(imin2) = 2 } . (143)\nNow, take any ranking \u03c3\u0302 \u2208 \u2126\u03021 and construct another ranking \u03c3\u0303 from \u03c3\u0302 as following. If i1 = 2, i2 = 1, then swap the items at i1-th and i2-th position in ranking \u03c3\u0302 to get \u03c3\u0303. Else, if i1 < i2, then first: swap items at i1-th position and 1st position, and second: swap items at i2-th position and 2nd position, to get \u03c3\u0303; if i2 < i1, then first: swap items at i2-th position and 2nd position, and second: swap items at i1-th position and 1st position, to get \u03c3\u0303.\nObserve that P[\u03c3\u0303] \u2264 P[\u03c3\u0302] and \u03c3\u0303`1 \u2208 \u2126\u03022. Moreover, such a construction gives a bijective mapping between \u2126\u03021 and \u2126\u03022. Hence, the claim is proved."}, {"heading": "Acknowledgements", "text": "The authors thank the anonymous reviewers for their constructive feedback. This work was partially supported by National Science Foundation Grants MES-1450848, CNS-1527754, and CCF-1553452."}], "references": [{"title": "Active learning ranking from pairwise preferences with almost optimal query complexity", "author": ["N. Ailon"], "venue": "Advances in Neural Information Processing Systems, pages 810\u2013818,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "What\u2019s your choice? learning the mixed multi-nomial logit model", "author": ["A. Ammar", "S. Oh", "D. Shah", "L. Voloch"], "venue": "Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Ranking: Compare, don\u2019t score", "author": ["A. Ammar", "D. Shah"], "venue": "Communication, Control, and Computing (Allerton), 2011 49th Annual Allerton Conference on, pages 776\u2013783. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Generalized method-of-moments for rank aggregation", "author": ["H. Azari Soufiani", "W. Chen", "D. C Parkes", "L. Xia"], "venue": "Advances in Neural Information Processing Systems 26, pages 2706\u20132714,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing parametric ranking models via rank-breaking", "author": ["H. Azari Soufiani", "D. Parkes", "L. Xia"], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 360\u2013368,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Random utility theory for social choice", "author": ["H. Azari Soufiani", "D.C. Parkes", "L. Xia"], "venue": "NIPS, pages 126\u2013134,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Discrete choice analysis: theory and application to travel demand, volume 9", "author": ["M.E. Ben-Akiva", "S.R. Lerman"], "venue": "MIT press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1985}, {"title": "A Markov chain approximation to choice modeling", "author": ["J. Blanchet", "G. Gallego", "V. Goyal"], "venue": "EC, pages 103\u2013104,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Sorting from noisy information", "author": ["M. Braverman", "E. Mossel"], "venue": "arXiv preprint arXiv:0910.1191,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral mle: Top-k rank aggregation from pairwise comparisons", "author": ["Y. Chen", "C. Suh"], "venue": "arXiv preprint arXiv:1504.07218,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Magnitude-preserving ranking algorithms", "author": ["C. Cortes", "M. Mohri", "A. Rastogi"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 169\u2013176. ACM,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Essai sur l\u2019application de l\u2019analyse \u00e0 la probabilit\u00e9 des d\u00e9cisions rendues \u00e0 la pluralit\u00e9 des voix", "author": ["N. De Condorcet"], "venue": "L\u2019imprimerie royale,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1785}, {"title": "A generalization of spectral analysis with application to ranked data", "author": ["P. Diaconis"], "venue": "The Annals of Statistics, pages 949\u2013979,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning mixed membership mallows models from pairwise comparisons", "author": ["W. Ding", "P. Ishwar", "V. Saligrama"], "venue": "arXiv preprint arXiv:1504.00757,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "On the consistency of ranking algorithms", "author": ["J.C. Duchi", "L. Mackey", "M.I. Jordan"], "venue": "Proceedings of the ICML Conference, Haifa, Israel, June", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Rank aggregation methods for the web", "author": ["C. Dwork", "R. Kumar", "M. Naor", "D. Sivakumar"], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 613\u2013622. ACM,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Rank analysis of incomplete block designs: A method of paired comparisons employing unequal repetitions on pairs", "author": ["O. Dykstra"], "venue": "Biometrics, 16(2):176\u2013188,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1960}, {"title": "A data-driven approach to modeling choice", "author": ["V.F. Farias", "S. Jagabathula", "D. Shah"], "venue": "NIPS, pages 504\u2013512,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A nonparametric approach to modeling choice with limited data", "author": ["V.F. Farias", "S. Jagabathula", "D. Shah"], "venue": "Management Science, 59(2):305\u2013322,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Revenue management under the markov chain choice model", "author": ["J.B. Feldman", "H. Topaloglu"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Solution of a ranking problem from binary comparisons", "author": ["L.R. Ford Jr"], "venue": "The American Mathematical Monthly, 64(8):28\u201333,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1957}, {"title": "Eigentaste: A constant time collaborative filtering algorithm", "author": ["K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"], "venue": "Information Retrieval, 4(2):133\u2013151,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "A logit model of brand choice calibrated on scanner data", "author": ["P.M. Guadagni", "J.D. Little"], "venue": "Marketing science, 2(3):203\u2013238,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1983}, {"title": "Minimax-optimal inference from partial rankings", "author": ["B. Hajek", "S. Oh", "J. Xu"], "venue": "Advances in Neural Information Processing Systems 27, pages 1475\u20131483,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "A large-deviation inequality for vector-valued martingales", "author": ["T.P. Hayes"], "venue": "Combinatorics, Probability and Computing,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Mm algorithms for generalized bradley-terry models", "author": ["D.R. Hunter"], "venue": "Annals of Statistics, pages 384\u2013406,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Active ranking using pairwise comparisons", "author": ["K.G. Jamieson", "R. Nowak"], "venue": "Advances in Neural Information Processing Systems, pages 2240\u20132248,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Nantonac collaborative filtering: recommendation based on order responses", "author": ["T. Kamishima"], "venue": "Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 583\u2013588. ACM,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Rank matrix factorisation", "author": ["T. Le Van", "M. van Leeuwen", "S. Nijssen", "L. De Raedt"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Non-parametric modeling of partially ranked data", "author": ["G. Lebanon", "Y. Mao"], "venue": "Advances in neural information processing systems, pages 857\u2013864,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Budgeted social choice: From consensus to personalized decision making", "author": ["T. Lu", "C. Boutilier"], "venue": "IJCAI, volume 11, pages 280\u2013286,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning mallows models with pairwise preferences", "author": ["T. Lu", "C. Boutilier"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 145\u2013152,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Individualized rank aggregation using nuclear norm regularization", "author": ["Y. Lu", "S. Negahban"], "venue": "arXiv preprint arXiv:1410.0860,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Second report of the irish commission on electronic voting", "author": ["J. Lundell"], "venue": "Voting Matters, 23:13\u201317,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast and accurate inference of plackett-luce models", "author": ["L. Maystre", "M. Grossglauser"], "venue": "Advances in Neural Information Processing Systems 28 (NIPS 2015),", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust active ranking from sparse noisy comparisons", "author": ["L. Maystre", "M. Grossglauser"], "venue": "arXiv preprint arXiv:1502.05556,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional logit analysis of qualitative choice behavior", "author": ["D. McFadden"], "venue": "Frontiers in Econometrics, pages 105\u2013142,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1973}, {"title": "Econometric models for probabilistic choice among products", "author": ["D. McFadden"], "venue": "Journal of Business, 53(3):S13\u2013S29,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1980}, {"title": "Mixed mnl models for discrete response", "author": ["D. McFadden", "K. Train"], "venue": "Journal of applied Econometrics, 15(5):447\u2013470,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Collaborative filtering with the simple bayesian classifier", "author": ["K. Miyahara", "M.J. Pazzani"], "venue": "PRICAI 2000 Topics in Artificial Intelligence, pages 679\u2013689. Springer,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2000}, {"title": "Iterative ranking from pair-wise comparisons", "author": ["S. Negahban", "S. Oh", "D. Shah"], "venue": "NIPS, pages 2483\u20132491,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Rank centrality: Ranking from pair-wise comparisons", "author": ["S. Negahban", "S. Oh", "D. Shah"], "venue": "preprint arXiv:1209.1688,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning mixed multinomial logit model from ordinal data", "author": ["S. Oh", "D. Shah"], "venue": "Advances in Neural Information Processing Systems, pages 595\u2013603,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaboratively learning preferences from ordinal data", "author": ["S. Oh", "K.K. Thekumparampil", "J. Xu"], "venue": "Advances in Neural Information Processing Systems 28, pages 1900\u20131908,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Preference completion: Large-scale collaborative ranking from pairwise comparisons", "author": ["D. Park", "J. Neeman", "J. Zhang", "S. Sanghavi", "I.S. Dhillon"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, pages 1907\u20131916,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Svd-based collaborative filtering with privacy", "author": ["H. Polat", "W. Du"], "venue": "Proceedings of the 2005 ACM symposium on Applied computing, pages 791\u2013795. ACM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "A statistical convergence perspective of algorithms for rank aggregation from pairwise data", "author": ["A. Rajkumar", "S. Agarwal"], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 118\u2013126,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Independence of irrelevant alternatives", "author": ["P. Ray"], "venue": "Econometrica: Journal of the Econometric Society, pages 987\u2013991,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1973}, {"title": "Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence", "author": ["N.B. Shah", "S. Balakrishnan", "J. Bradley", "A. Parekh", "K. Ramchandran", "M.J. Wainwright"], "venue": "arXiv preprint arXiv:1505.01462,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastically transitive models for pairwise comparisons: Statistical and computational issues", "author": ["N.B. Shah", "S. Balakrishnan", "A. Guntuboyina", "M.J. Wainright"], "venue": "arXiv preprint arXiv:1510.05610,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple, robust and optimal ranking from pairwise comparisons", "author": ["N.B. Shah", "M.J. Wainwright"], "venue": "arXiv preprint arXiv:1512.08949,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "An extended transmission/disequilibrium test (tdt) for multi-allele marker loci", "author": ["P. Sham", "D. Curtis"], "venue": "Annals of human genetics, 59(3):323\u2013336,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1995}, {"title": "Generalized random utility model", "author": ["J. Walker", "M. Ben-Akiva"], "venue": "Mathematical Social Sciences, 43(3):303\u2013343,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2002}, {"title": "Clustering and inference from pairwise comparisons", "author": ["R. Wu", "J. Xu", "R. Srikant", "L. Massouli\u00e9", "M. Lelarge", "B. Hajek"], "venue": "arXiv preprint arXiv:1502.04631,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring users? preferences from crowdsourced pairwise comparisons: A matrix completion approach", "author": ["J. Yi", "R. Jin", "S. Jain", "A. Jain"], "venue": "First AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 37, "context": "Parametric probabilistic models, known collectively as Random Utility Models (RUMs), have been proposed to model such individual choices and preferences [40].", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "In some real-world elections, voters provide ranked preferences over the whole set of candidates [36].", "startOffset": 97, "endOffset": 101}, {"referenceID": 41, "context": "For such traditional data sets, there are several computationally efficient inference algorithms for finding the Maximum Likelihood (ML) estimates that provably achieve the minimax optimal performance [44, 52, 26].", "startOffset": 201, "endOffset": 213}, {"referenceID": 49, "context": "For such traditional data sets, there are several computationally efficient inference algorithms for finding the Maximum Likelihood (ML) estimates that provably achieve the minimax optimal performance [44, 52, 26].", "startOffset": 201, "endOffset": 213}, {"referenceID": 23, "context": "For such traditional data sets, there are several computationally efficient inference algorithms for finding the Maximum Likelihood (ML) estimates that provably achieve the minimax optimal performance [44, 52, 26].", "startOffset": 201, "endOffset": 213}, {"referenceID": 20, "context": ", [22, 45, 4].", "startOffset": 2, "endOffset": 13}, {"referenceID": 42, "context": ", [22, 45, 4].", "startOffset": 2, "endOffset": 13}, {"referenceID": 3, "context": ", [22, 45, 4].", "startOffset": 2, "endOffset": 13}, {"referenceID": 4, "context": "However, such a heuristic of full rank-breaking defined explicitly in (1), where all pairwise comparisons are weighted and treated equally ignoring their dependencies, has been recently shown to introduce inconsistency [5].", "startOffset": 219, "endOffset": 222}, {"referenceID": 53, "context": "The PL model is a special case of random utility models, defined as follows [56, 6].", "startOffset": 76, "endOffset": 83}, {"referenceID": 5, "context": "The PL model is a special case of random utility models, defined as follows [56, 6].", "startOffset": 76, "endOffset": 83}, {"referenceID": 36, "context": "The PL model is a special case where the noise follows the standard Gumbel distribution, and is one of the most popular model in social choice theory [39, 41].", "startOffset": 150, "endOffset": 158}, {"referenceID": 38, "context": "The PL model is a special case where the noise follows the standard Gumbel distribution, and is one of the most popular model in social choice theory [39, 41].", "startOffset": 150, "endOffset": 158}, {"referenceID": 22, "context": "PL has several important properties, making this model realistic in various domains, including marketing [25], transportation [40, 7], biology [55], and natural language processing [42].", "startOffset": 105, "endOffset": 109}, {"referenceID": 37, "context": "PL has several important properties, making this model realistic in various domains, including marketing [25], transportation [40, 7], biology [55], and natural language processing [42].", "startOffset": 126, "endOffset": 133}, {"referenceID": 6, "context": "PL has several important properties, making this model realistic in various domains, including marketing [25], transportation [40, 7], biology [55], and natural language processing [42].", "startOffset": 126, "endOffset": 133}, {"referenceID": 52, "context": "PL has several important properties, making this model realistic in various domains, including marketing [25], transportation [40, 7], biology [55], and natural language processing [42].", "startOffset": 143, "endOffset": 147}, {"referenceID": 39, "context": "PL has several important properties, making this model realistic in various domains, including marketing [25], transportation [40, 7], biology [55], and natural language processing [42].", "startOffset": 181, "endOffset": 185}, {"referenceID": 48, "context": "The PL model (i) satisfies Luce\u2019s \u2018independence of irrelevant alternatives\u2019 in social choice theory [51], and has a simple characterization as sequential (random) choices as explained below; and (ii) has a maximum likelihood estimator (MLE) which is a convex program in \u03b8 in the traditional scenarios of pairwise, best-outof-k and k-way comparisons.", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "Inappropriate selection of the paired comparisons can lead to inconsistent estimators as proved in [5], and the standard choice of the parameters can lead to a significantly suboptimal performance.", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "There are several efficient implementation tailored for this problem [22, 28, 44, 37], and under the traditional scenarios, these approaches provably achieve the minimax optimal rate [26, 52].", "startOffset": 69, "endOffset": 85}, {"referenceID": 25, "context": "There are several efficient implementation tailored for this problem [22, 28, 44, 37], and under the traditional scenarios, these approaches provably achieve the minimax optimal rate [26, 52].", "startOffset": 69, "endOffset": 85}, {"referenceID": 41, "context": "There are several efficient implementation tailored for this problem [22, 28, 44, 37], and under the traditional scenarios, these approaches provably achieve the minimax optimal rate [26, 52].", "startOffset": 69, "endOffset": 85}, {"referenceID": 34, "context": "There are several efficient implementation tailored for this problem [22, 28, 44, 37], and under the traditional scenarios, these approaches provably achieve the minimax optimal rate [26, 52].", "startOffset": 69, "endOffset": 85}, {"referenceID": 23, "context": "There are several efficient implementation tailored for this problem [22, 28, 44, 37], and under the traditional scenarios, these approaches provably achieve the minimax optimal rate [26, 52].", "startOffset": 183, "endOffset": 191}, {"referenceID": 49, "context": "There are several efficient implementation tailored for this problem [22, 28, 44, 37], and under the traditional scenarios, these approaches provably achieve the minimax optimal rate [26, 52].", "startOffset": 183, "endOffset": 191}, {"referenceID": 4, "context": "It is known that the full-rank breaking estimator is inconsistent [5]; the error is strictly bounded away from zero even with infinite samples.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "[5] recently characterized the entire set of consistent rankbreaking estimators.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "This rank-breaking graphs were introduced in [4], where it was shown that the pairwise ordinal relations that is represented by edges in the rank-breaking graphs are sufficient information for using any estimation based on the idea of rank-breaking.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Precisely, on the converse side, it was proved in [5] that any pairwise outcomes that is not present in the rank-breaking graphs Gj,a\u2019s lead to inconsistency for a general \u03b8 \u2217.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "It should be noted that rank-breaking graphs are defined slightly differently in [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Specifically, [4] introduced a different notion of rank-breaking graph, where the vertices represent positions in total ordering.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "Given such observation from the PL model, [4] and [5] prove that a rank-breaking graph is consistent if and only if it satisfies the following property.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Given such observation from the PL model, [4] and [5] prove that a rank-breaking graph is consistent if and only if it satisfies the following property.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "Although the specific definitions of rank-breaking graphs are different from our setting, the mathematical analysis of [4] still holds when interpreted appropriately.", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "Specifically, we consider only those rank-breaking that are consistent under the conditions given in [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "By changing how we weigh each rank-breaking graph (by choosing the \u03bbj,a\u2019s), the convex program (3) spans the entire set of consistent rank-breaking estimators, as characterized in [5].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "Naturally, a uniform choice of \u03bbj,a = \u03bb was proposed in [5].", "startOffset": 56, "endOffset": 59}, {"referenceID": 42, "context": "Intuitively, one can expect better accuracy when the spectral gap is larger, as evidenced in previous learning to rank results in simpler settings [45, 52, 26].", "startOffset": 147, "endOffset": 159}, {"referenceID": 49, "context": "Intuitively, one can expect better accuracy when the spectral gap is larger, as evidenced in previous learning to rank results in simpler settings [45, 52, 26].", "startOffset": 147, "endOffset": 159}, {"referenceID": 23, "context": "Intuitively, one can expect better accuracy when the spectral gap is larger, as evidenced in previous learning to rank results in simpler settings [45, 52, 26].", "startOffset": 147, "endOffset": 159}, {"referenceID": 0, "context": "This is made precise in (4), and in the main result of Theorem 2, we appropriately rescale the spectral gap and use \u03b1 \u2208 [0, 1] defined as \u03b1 \u2261 \u03bb2(L)(d\u2212 1) Tr(L) = \u03bb2(L)(d\u2212 1) \u2211n j=1 `j .", "startOffset": 120, "endOffset": 126}, {"referenceID": 0, "context": "The degree imbalance \u03b2 \u2208 [0, 1] determines how many samples are required for the analysis to hold.", "startOffset": 25, "endOffset": 31}, {"referenceID": 41, "context": "The upper bound follows from an analysis of the convex program similar to those in [44, 26, 52].", "startOffset": 83, "endOffset": 95}, {"referenceID": 23, "context": "The upper bound follows from an analysis of the convex program similar to those in [44, 26, 52].", "startOffset": 83, "endOffset": 95}, {"referenceID": 49, "context": "The upper bound follows from an analysis of the convex program similar to those in [44, 26, 52].", "startOffset": 83, "endOffset": 95}, {"referenceID": 3, "context": "We also compare performance of Generalized Method-of-Moments (GMM) proposed by [4] with our algorithm.", "startOffset": 79, "endOffset": 82}, {"referenceID": 23, "context": "This settles the question raised in [26] on whether it is possible to achieve optimal accuracy using rank-breaking under the top-` separators scenario.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "Analytically, it was proved in [26] that under the top-` separators scenario, naive rank-breaking with uniform weights achieves the same error bound as the MLE, up to a constant factor.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "We use the minorization-maximization algorithm from [28] for computing the estimates from the rank-breakings.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "Even if we use the consistent rank-breakings first proposed in [5], there is ambiguity in the choice of the weights.", "startOffset": 63, "endOffset": 66}, {"referenceID": 49, "context": "We follow the examples and experimental setup from [52] for a similar result with pairwise comparisons.", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "On real-world data sets on sushi preferences [30], we show that the data-driven rank-breaking improves over Generalized Method-of-Moments (GMM) proposed by [4].", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "On real-world data sets on sushi preferences [30], we show that the data-driven rank-breaking improves over Generalized Method-of-Moments (GMM) proposed by [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "This is a widely used data set for rank aggregation, for instance in [4, 6, 38, 31, 34, 33].", "startOffset": 69, "endOffset": 91}, {"referenceID": 5, "context": "This is a widely used data set for rank aggregation, for instance in [4, 6, 38, 31, 34, 33].", "startOffset": 69, "endOffset": 91}, {"referenceID": 35, "context": "This is a widely used data set for rank aggregation, for instance in [4, 6, 38, 31, 34, 33].", "startOffset": 69, "endOffset": 91}, {"referenceID": 28, "context": "This is a widely used data set for rank aggregation, for instance in [4, 6, 38, 31, 34, 33].", "startOffset": 69, "endOffset": 91}, {"referenceID": 31, "context": "This is a widely used data set for rank aggregation, for instance in [4, 6, 38, 31, 34, 33].", "startOffset": 69, "endOffset": 91}, {"referenceID": 30, "context": "This is a widely used data set for rank aggregation, for instance in [4, 6, 38, 31, 34, 33].", "startOffset": 69, "endOffset": 91}, {"referenceID": 3, "context": "Below, we follow the experimental scenarios of the GMM approach in [4] for fair comparisons.", "startOffset": 67, "endOffset": 70}, {"referenceID": 25, "context": "To validate our approach, we first take the estimated PL weights of the 10 types of sushi, using [28] implementation of the ML estimator, over the entire input data of 5000 complete rankings.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "We instead use scaled footrule aggregation (SFO) given in [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "We use jester data set [23] that consists of over 4.", "startOffset": 23, "endOffset": 27}, {"referenceID": 40, "context": "This data-set has been used by [43, 49, 11, 32] for rank aggregation and collaborative filtering.", "startOffset": 31, "endOffset": 47}, {"referenceID": 46, "context": "This data-set has been used by [43, 49, 11, 32] for rank aggregation and collaborative filtering.", "startOffset": 31, "endOffset": 47}, {"referenceID": 10, "context": "This data-set has been used by [43, 49, 11, 32] for rank aggregation and collaborative filtering.", "startOffset": 31, "endOffset": 47}, {"referenceID": 29, "context": "This data-set has been used by [43, 49, 11, 32] for rank aggregation and collaborative filtering.", "startOffset": 31, "endOffset": 47}, {"referenceID": 12, "context": "We perform similar experiments on American Psychological Association (APA) data-set [14].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Initially motivated by elections and voting, rank aggregation has been a topic of mathematical interest dating back to Condorcet and Borda [13, 12].", "startOffset": 139, "endOffset": 147}, {"referenceID": 37, "context": "The PL model studied in this paper is a special case of MultiNomial Logit (MNL) models commonly used in discrete choice modeling, which has a long history in operations research [40].", "startOffset": 178, "endOffset": 182}, {"referenceID": 20, "context": "Efficient inference algorithms has been proposed to either find the MLE efficiently or approximately, such as the iterative approaches in [22, 18], minorization-maximization approach in [28], and Markov chain approaches in [44, 37].", "startOffset": 138, "endOffset": 146}, {"referenceID": 16, "context": "Efficient inference algorithms has been proposed to either find the MLE efficiently or approximately, such as the iterative approaches in [22, 18], minorization-maximization approach in [28], and Markov chain approaches in [44, 37].", "startOffset": 138, "endOffset": 146}, {"referenceID": 25, "context": "Efficient inference algorithms has been proposed to either find the MLE efficiently or approximately, such as the iterative approaches in [22, 18], minorization-maximization approach in [28], and Markov chain approaches in [44, 37].", "startOffset": 186, "endOffset": 190}, {"referenceID": 41, "context": "Efficient inference algorithms has been proposed to either find the MLE efficiently or approximately, such as the iterative approaches in [22, 18], minorization-maximization approach in [28], and Markov chain approaches in [44, 37].", "startOffset": 223, "endOffset": 231}, {"referenceID": 34, "context": "Efficient inference algorithms has been proposed to either find the MLE efficiently or approximately, such as the iterative approaches in [22, 18], minorization-maximization approach in [28], and Markov chain approaches in [44, 37].", "startOffset": 223, "endOffset": 231}, {"referenceID": 41, "context": "[44] provided Rank Centrality that provably achieves minimax optimal error rate for randomly chosen pairs, which was later generalized to arbitrary pairwise comparisons in [45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[44] provided Rank Centrality that provably achieves minimax optimal error rate for randomly chosen pairs, which was later generalized to arbitrary pairwise comparisons in [45].", "startOffset": 172, "endOffset": 176}, {"referenceID": 23, "context": "This analysis was generalized to k-way comparisons in [26] and generalized to best-out-of-k comparisons with sharper bounds in [52].", "startOffset": 54, "endOffset": 58}, {"referenceID": 49, "context": "This analysis was generalized to k-way comparisons in [26] and generalized to best-out-of-k comparisons with sharper bounds in [52].", "startOffset": 127, "endOffset": 131}, {"referenceID": 9, "context": "in [10] proposed a new algorithm based on Rank Centrality that provides a tighter error bound for L\u221e norm, as opposed to the existing L2 error bounds.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "Another interesting direction in learning to rank is non-parametric learning from paired comparisons, initiated in several recent papers such as [16, 50, 53, 54].", "startOffset": 145, "endOffset": 161}, {"referenceID": 47, "context": "Another interesting direction in learning to rank is non-parametric learning from paired comparisons, initiated in several recent papers such as [16, 50, 53, 54].", "startOffset": 145, "endOffset": 161}, {"referenceID": 50, "context": "Another interesting direction in learning to rank is non-parametric learning from paired comparisons, initiated in several recent papers such as [16, 50, 53, 54].", "startOffset": 145, "endOffset": 161}, {"referenceID": 51, "context": "Another interesting direction in learning to rank is non-parametric learning from paired comparisons, initiated in several recent papers such as [16, 50, 53, 54].", "startOffset": 145, "endOffset": 161}, {"referenceID": 55, "context": "More recently, a more general problem of learning personal preferences from ordinal data has been studied [58, 33, 15].", "startOffset": 106, "endOffset": 118}, {"referenceID": 30, "context": "More recently, a more general problem of learning personal preferences from ordinal data has been studied [58, 33, 15].", "startOffset": 106, "endOffset": 118}, {"referenceID": 13, "context": "More recently, a more general problem of learning personal preferences from ordinal data has been studied [58, 33, 15].", "startOffset": 106, "endOffset": 118}, {"referenceID": 1, "context": "When users are classified into a small number of groups with same preferences, mixed MNL model can be learned from data as studied in [2, 46, 57].", "startOffset": 134, "endOffset": 145}, {"referenceID": 43, "context": "When users are classified into a small number of groups with same preferences, mixed MNL model can be learned from data as studied in [2, 46, 57].", "startOffset": 134, "endOffset": 145}, {"referenceID": 54, "context": "When users are classified into a small number of groups with same preferences, mixed MNL model can be learned from data as studied in [2, 46, 57].", "startOffset": 134, "endOffset": 145}, {"referenceID": 32, "context": "This problem was first posed as an inference problem in [35] where convex relaxation of nuclear norm minimization was proposed with provably optimal guarantees.", "startOffset": 56, "endOffset": 60}, {"referenceID": 44, "context": "This was later generalized to k-way comparisons in [47].", "startOffset": 51, "endOffset": 55}, {"referenceID": 45, "context": "A similar approach was studied with a different guarantees and assumptions in [48].", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "In an orthogonal direction, new discrete choice models with sparse structures has been proposed recently in [19] and optimization algorithms for revenue management has been proposed [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "In an orthogonal direction, new discrete choice models with sparse structures has been proposed recently in [19] and optimization algorithms for revenue management has been proposed [20].", "startOffset": 182, "endOffset": 186}, {"referenceID": 7, "context": "In a similar direction, new discrete choice models based on Markov chains has been introduced in [8], and corresponding revenue management algorithms has been studied in [21].", "startOffset": 97, "endOffset": 100}, {"referenceID": 19, "context": "In a similar direction, new discrete choice models based on Markov chains has been introduced in [8], and corresponding revenue management algorithms has been studied in [21].", "startOffset": 170, "endOffset": 174}, {"referenceID": 2, "context": "However, typically these models are analyzed in the asymptotic regime with infinite samples, with the exception of [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 47, "context": "A non-parametric choice models for pairwise comparisons also have been studied in [50, 53].", "startOffset": 82, "endOffset": 90}, {"referenceID": 50, "context": "A non-parametric choice models for pairwise comparisons also have been studied in [50, 53].", "startOffset": 82, "endOffset": 90}, {"referenceID": 8, "context": "in [9] proposed efficient approaches and provided performance guarantees.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Following this work, there has been recent advances in adaptive ranking [1, 29, 38].", "startOffset": 72, "endOffset": 83}, {"referenceID": 26, "context": "Following this work, there has been recent advances in adaptive ranking [1, 29, 38].", "startOffset": 72, "endOffset": 83}, {"referenceID": 35, "context": "Following this work, there has been recent advances in adaptive ranking [1, 29, 38].", "startOffset": 72, "endOffset": 83}, {"referenceID": 0, "context": "By the mean value theorem, there exists a \u03b8 = a\u03b8\u0302 + (1\u2212 a)\u03b8\u2217 for some a \u2208 [0, 1] such that \u03b8 \u2208 \u03a9b and LRB(\u03b8\u0302)\u2212 LRB(\u03b8)\u2212 \u3008\u2207LRB(\u03b8),\u2206\u3009 = 1 2 \u2206>H(\u03b8)\u2206 \u2264 \u2212 2 \u03bb2(\u2212H(\u03b8))\u2016\u2206\u20162, (39) where the last inequality holds because the Hessian matrix \u2212H(\u03b8) is positive semi-definite with H(\u03b8)1 = 0 and \u2206>1 = 0.", "startOffset": 74, "endOffset": 80}, {"referenceID": 24, "context": "8 in [27].", "startOffset": 5, "endOffset": 9}], "year": 2016, "abstractText": "Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. Rank-breaking is a common practice to reduce the computational complexity of learning the global ranking. The individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons. However, due to the ignored dependencies in the data, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity. Further, the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph.", "creator": "LaTeX with hyperref package"}}}