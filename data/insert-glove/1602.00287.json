{"id": "1602.00287", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2016", "title": "Additive Approximations in High Dimensional Nonparametric Regression via the SALSA", "abstract": "High dimensional 13:01 nonparametric 17.08 regression 110-volt is alvanley an 194.9 inherently difficult yamato-e problem firedrake with known wooed lower transleithania bounds bunkyo depending 4,491 exponentially in belasica dimension. A omnitel popular strategy to cuney alleviate gunsmiths this dejame curse of dimensionality kharif has been to coagulopathy use 4,246 additive models pasu of \\ emph {hmccoy first post-baccalaureate order }, which accommodate model the aahing regression function as verla a sum of i.f. independent wran functions 18,150 on hareide each frybread dimension. Though runescape useful in acquiesces controlling drafter the variance of treybig the estimate, rac\u0142awice such akmad models are often dagano too tooley restrictive non-compliance in werke practical settings. Between non - additive models pijijiapan which often misidentifications have large conservator variance ketchum and legitimisation first spritz order additive nordkapp models which ex have large aitzol bias, there pus has 18-12 been little 91,000-seat work cityhopper to milliliter exploit sriharikota the shovelled trade - off in the furtive middle miruts via epitomised additive canvassers models gilson of intermediate 17e order. hantz In emoted this work, we pruth propose disruptive SALSA, brzezinka which shes bridges this gap by allowing interactions 1963-1966 between variables, but alake controls model barpeta capacity yaht by 119.06 limiting siba the order csicop of sverre interactions. scandella SALSA schopman minimises hobbes the latrun residual shurman sum melchers of squares whaddon with carnatic squared RKHS norm penalties. kendrew Algorithmically, 65.08 it 1,537 can be viewed computerisation as open-heart Kernel h\u00f3a Ridge dermota Regression with misled an mortlock additive kernel. k-2 When paes the nivea regression function is additive, peru-bolivian the fernandes excess risk pellucid is only polynomial in dimension. Using the vinella Girard - 4,070 Newton formulae, we scriptura efficiently sum over a combinatorial number of terms thebulliondesk.com in antilock the 42.51 additive expansion. warnock Via deacetylases a cookers comparison on $ babe\u015f-bolyai 16 $ real self-perpetuating datasets, interveinal we show that kiwan our camry method herculis is habano competitive abyad against $ 21 $ other swartland alternatives.", "histories": [["v1", "Sun, 31 Jan 2016 17:32:51 GMT  (2678kb,D)", "https://arxiv.org/abs/1602.00287v1", null], ["v2", "Sun, 20 Mar 2016 23:11:13 GMT  (2020kb,D)", "http://arxiv.org/abs/1602.00287v2", null], ["v3", "Tue, 24 May 2016 23:15:24 GMT  (3375kb,D)", "http://arxiv.org/abs/1602.00287v3", "International Conference on Machine Learning (ICML) 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["kirthevasan kandasamy", "yaoliang yu"], "accepted": true, "id": "1602.00287"}, "pdf": {"name": "1602.00287.pdf", "metadata": {"source": "CRF", "title": "Additive Approximations in High Dimensional Nonparametric Regression via the SALSA", "authors": ["Kirthevasan Kandasamy", "Yaoliang Yu"], "emails": ["KANDASAMY@CS.CMU.EDU", "YAOLIANG@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Given i.i.d samples (Xi, Yi)ni=1 from some distribution PXY , on X \u00d7 Y \u2282 RD \u00d7 R, the goal of least squares regression is to estimate the regression function f\u2217(x) = E[Y |X = x]. A popular approach is linear regression which models f\u2217 as a linear combination of the variables\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nx, i.e. f(x) = \u03b2>x for some \u03b2 \u2208 RD. Linear Regression is typically solved by minimising the sum of squared errors on the training set subject to a complexity penalty on \u03b2. Such parametric methods are conceptually simple and have desirable statistical properties when the problem meets the assumption. However, the parametric assumption is generally too restrictive for many real problems.\nNonparametric regression refers to a suite of methods that typically only assume smoothness on f\u2217. They present a more compelling framework for regression since they encompass a richer class of functions than parametric models do. However they suffer from severe drawbacks in high dimensional settings. The excess risk of nonparametric methods has exponential dependence on dimension. Current lower bounds (Gyo\u0308rfi et al., 2002) suggest that this dependence is unavoidable. Therefore, to make progress stronger assumptions on f\u2217 beyond just smoothness are necessary. In this light, a common simplification has been to assume that f\u2217 decomposes into the additive form f\u2217(x) = f (1) \u2217 (x1)+f (2) \u2217 (x2)+\u00b7 \u00b7 \u00b7+f (D)\u2217 (xD) (Hastie & Tibshirani, 1990; Lafferty & Wasserman, 2005; Ravikumar et al., 2009). In this exposition, we refer to such models as first order additive models. Under this assumption, the excess risk improves significantly.\nThat said, the first order assumption is often too biased in practice since it ignores interactions between variables. It is natural to ask if we could consider additive models which permit interactions. For instance, a second order model has the expansion f\u2217(x) = f (1) \u2217 (x1, x2) + f (2) \u2217 (x1, x3) + . . . . In general, we may consider d orders of interaction which have ( D d ) terms in the expansion. If d D, we may allow for a richer class of functions than first order models, and hopefully still be able to control the excess risk.\nEven when f\u2217 is not additive, using an additive approximation has its advantages. It is a well understood statistical concept that when we only have few samples, using a simpler model to fit our data gives us a better trade-off for variance against bias. Since additive models are statistically simpler they may give us better estimates due to reduced\nar X\niv :1\n60 2.\n00 28\n7v 3\n[ st\nat .M\nL ]\n2 4\nM ay\n2 01\nvariance. In most nonparametric regression methods, the bias-variance trade-off is managed via a parameter such as the bandwidth of a kernel or a complexity penalty. In this work, we demonstrate that this trade-off can also be controlled via additive models with different orders of interaction. Intuitively, we might use low order interactions with few data points but with more data we can increase model capacity via higher order interactions. Indeed, our experiments substantiate this intuition: additive models do well on several datasets in which f\u2217 is not necessarily additive.\nThere are two key messages in this paper. The first is that we should use additive models in high dimensional regression to reduce the variance of the estimate. The second is that it is necessary to model beyond just first order models to reduce the bias. Our contributions in this paper are:\n1. We formulate additive models for nonparametric regression beyond first order models. Our method SALSA \u2013 for Shrunk Additive Least Squares Approximation\u2013 estimates a dth order additive function containing ( D d ) terms\nin its expansion. Despite this, the computational complexity of SALSA is O(Dd2).\n2. Our theoretical analysis bounds the excess risk for SALSA for (i) additive f\u2217 under reproducing kernel Hilbert space assumptions and (ii) non-additive f\u2217 in the agnostic setting. In (i), the excess risk has only polynomial dependence on D.\n3. We compare our method against 21 alternatives on synthetic and 15 real datasets. SALSA is more consistent and in many cases outperforms other methods. Our software and datasets are available at github.com/kirthevasank/salsa. Our implementation of locally polynomial regression is also released as part of this paper and is made available at github.com/kirthevasank/local-poly-reg.\nBefore we proceed we make an essential observation. When parametric assumptions are true, parametric regression methods can scale both statistically and computationally to possibly several thousands of dimensions. However, it is common knowledge in the statistics community that nonparametric regression can be reliably applied only in very low dimensions with reasonable data set sizes. Even D = 10 is considered \u201chigh\u201d for nonparametric methods. In this work we aim to statistically scale nonparametric regression to dimensions on the order 10\u2013100 while addressing the computational challenges in doing so."}, {"heading": "Related Work", "text": "A plurality of work in high dimensional regression focuses on first order additive models. One of the most popular techniques is the back-fitting algorithm (Hastie et al., 2001) which iteratively approximates f\u2217 via a sum of D one di-\nmensional functions. Some variants such as RODEO (Lafferty & Wasserman, 2005) and SpAM (Ravikumar et al., 2009) study first order models in variable selection/sparsity settings. MARS (Friedman, 1991) uses a sum of splines on individual dimensions but allows interactions between variables via products of hinge functions at selected knot points. Lou et al. (2013) model f\u2217 as a first order model plus a sparse collection of pairwise interactions. However, restricting ourselves to only to a sparse collection of second order interactions might be too biased in practice. COSSO (Lin & Zhang, 2006) study higher order models but when you need only a sparse collection of them. In Section 4 we list several other parametric and nonparametric methods used in regression.\nOur approach is based on additive kernels and builds on Kernel Ridge Regression (Steinwart & Christmann, 2008; Zhang, 2005). Using additive kernels to encode and identify structure in the problem is fairly common in Machine Learning literature. A large line of work, in what has to come to be known as Multiple Kernel Learning (MKL), focuses on precisely this problem (Bach, 2008; Go\u0308nen & Alpaydin, 2011; Xu et al., 2010). Additive models have also been studied in Gaussian process literature via additive kernels (Duvenaud et al., 2011; Plate, 1999). However, they treat the additive model just as a heuristic whereas we also provide a theoretical analysis of our methods."}, {"heading": "2. Preliminaries", "text": "We begin with a brief review of some background material. We are given i.i.d data (Xi, Yi)ni=1 sampled from some distribution PXY on a compact space X \u00d7 Y \u2282 RD \u00d7 R. Let the marginal distribution ofX onX be PX and the L2(PX) norm be \u2016f\u201622 = \u222b f2dPX . We wish to use the data to find a function f : X \u2192 R with small risk\nR(f) = \u222b X\u00d7Y (y\u2212f(x))2dPXY (x, y) = E[(Y \u2212f(X))2].\nIt is well known thatR is minimised by the regression function f\u2217(\u00b7) = EXY [Y |X = \u00b7] and the excess risk for any f is R(f)\u2212R(f\u2217) = \u2016f \u2212 f\u2217\u201622 (Gyo\u0308rfi et al., 2002). Our goal is to develop an estimate that has low expected excess risk ER(f\u0302)\u2212R(f\u2217) = E[\u2016f\u0302 \u2212 f\u2217\u201622], where the expectation is taken with respect to realisations of the data (Xi, Yi)ni=1.\nSome smoothness conditions on f\u2217 are required to make regression tractable. A common assumption is that f\u2217 has bounded norm in the reproducing kernel Hilbert space (RKHS) H\u03ba of a continuous positive definite kernel \u03ba : X \u00d7 X \u2192 R. By Mercer\u2019s theorem (Scho\u0308lkopf & Smola, 2001), \u03ba permits an eigenexpansion of the form \u03ba(x, x\u2032) =\u2211\u221e j=1 \u00b5j\u03c6j(x)\u03c6j(x\n\u2032) where \u00b51 \u2265 \u00b52 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 are the eigenvalues of the expansion and \u03c61, \u03c62, . . . are an orthonormal basis for L2(PX).\nKernel Ridge Regression (KRR) is a popular technique for nonparametric regression. It is characterised as the solution of the following optimisation problem over the RKHS of some kernel \u03ba.\nf\u0302 = argmin f\u2208H\u03ba\n\u03bb\u2016f\u20162H\u03ba + 1\nn n\u2211 i=1 (Yi \u2212 f(Xi))2. (1)\nHere \u03bb is the regularisation coefficient to control the variance of the estimate and is decreasing with more data. Via the representer theorem (Scho\u0308lkopf & Smola, 2001; Steinwart & Christmann, 2008), we know that the solution lies in the linear span of the canonical maps of the training points Xn1 \u2013 i.e. f\u0302(\u00b7) = \u2211 i \u03b1i\u03ba(\u00b7, Xi). This reduces the above objective to \u03b1\u0302 = argmin\u03b1\u2208Rn \u03bb\u03b1 >K\u03b1 + 1 n\u2016Y \u2212K\u03b1\u2016 2 2 where K \u2208 Rn\u00d7n is the kernel matrix with Kij = \u03ba(Xi, Xj). The problem has the closed form solution \u03b1\u0302 = (K + \u03bbnI)\u22121Y . KRR has been analysed extensively under different assumptions on f\u2217; see (Steinwart & Christmann, 2008; Steinwart et al., 2009; Zhang, 2005) and references therein. Unfortunately, as is the case with many nonparametric methods, KRR suffers from the curse of dimensionality as its excess risk is exponential in D.\nAdditive assumption: To make progress in high dimensions, we assume that f\u2217 decomposes into the following additive form that contains interactions of d orders among the variables. (Later on, we will analyse non-additive f\u2217.)\nf\u2217(x) = \u2211\n1\u2264i1<i2<\u00b7\u00b7\u00b7<id\u2264D\nf (j) \u2217 (xi1 , xi2 , . . . , xid), (2)\nWe will write, f\u2217(x) = \u2211Md j=1 f (j) \u2217 (x\n(j)) where Md =( D d ) , and x(j) denotes the subset (xi1 , xi2 , . . . , xid). We are primarily interested in the setting d D. While there are a large number of f (j)\u2217 \u2019s, each of them only permits interactions of at most d variables. We will show that this assumption does in fact reduce the statistical complexity of the function to be estimated. The first order additive assumption is equivalent to setting d = 1 above. A potential difficulty with the above assumption is the combinatorial computational cost in estimating all f (j)\u2217 \u2019s when d > 1. We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels."}, {"heading": "3. SALSA", "text": "To extend KRR to additive models we first define kernels k(j) that act on each subset x(j). We then optimise the fol-\nlowing objective jointly over f\u0302 (j) \u2208 Hk(j) , j = 1 . . . ,Md.\n{f\u0302 (j)}Mdj=1 = argmin f(j)\u2208H\nk(j) ,j=1,...,Md\n\u03bb Md\u2211 j=1 \u2016f (j)\u20162H k(j) +\n1\nn n\u2211 i=1 ( Yi \u2212 Md\u2211 j=1 f (j)(X (j) i ) )2 . (3)\nOur estimate for f is then f\u0302(\u00b7) = \u2211 j f\u0302\n(j)(\u00b7). At first, this appears troublesome since it requres optimising over nMd parameters (\u03b1(j)i ), j = 1, . . . ,Md, i = 1, . . . , n. However, from the work of Aronszajn (1950), we know that the solution of (3) lies in the RKHS of the sum kernel k\nk(x, x\u2032) = Md\u2211 j=1 k(j)(x(j), x(j) \u2032 ) (4)\n= \u2211\n1\u2264i1<\u00b7\u00b7\u00b7<id\u2264D\nk(j)([xi1 , . . . , xid ], [x \u2032 i1 , . . . , x \u2032 id ]).\nSee Remark 6 in Appendix A for a proof. Hence, the solution f\u0302 can be written in the form f\u0302(\u00b7) = \u2211 i \u03b1ik(\u00b7, Xi) This is convenient since we only need to optimise over n parameters despite the combinatorial number of kernels. Moreover, it is straightforward to see that the solution is obtained by solving (1) by plugging in the sum kernel k for \u03ba. Consequently f\u0302 (j) = \u2211 i \u03b1\u0302ik\n(j)(\u00b7, X(j)i ) and f\u0302 = \u2211 i \u03b1\u0302ik(\u00b7, Xi) where \u03b1\u0302 is the solution of (1). While at first sight the differences with KRR might seem superficial, we will see that the stronger additive assumption will help us reduce the excess risk for high dimensional regression. Our theoretical results will be characterised directly via the optimisation objective (3)."}, {"heading": "3.1. The ESP Kernel", "text": "While the above formulation reduces the number of optimisation parameters, the kernel still has a combinatorial number of terms which can be expensive to compute. While this is true for arbitrary choices for k(j)\u2019s, under some restrictions we can efficiently compute k. For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al. (2011). First consider a set of base kernels acting on each dimension k1, k2, . . . , . . . , kD. Define k(j) to be the product kernel of all kernels acting on each coordinate \u2013 k(j)(x(j), x(j) \u2032 ) = ki1(xi1 , x \u2032 i1 )ki2(xi2 , x \u2032 i2\n) \u00b7 \u00b7 \u00b7 kid(xid , x\u2032id). Then, the additive kernel k(x, x\u2032) becomes the dth elementary symmetric polynomial (ESP) of the D variables k1(x1, x \u2032 1), . . . , kD(xD, x \u2032 D). Concretely,\nk(x, x\u2032) = \u2211\n1\u2264i1<i2<\u00b7\u00b7\u00b7<id\u2264D ( d\u220f `=1 ki`(xi` , x \u2032 i` ) ) . (5)\nWe refer to (5) as the ESP kernel. Using the GirardNewton identities (Macdonald, 1995) for ESPs, we can compute this summation efficiently. For the D variables sD1 = s1, . . . , sD and 1 \u2264 m \u2264 D, define the mth power sum pm and themth elementary symmetric polynomial em:\npm(s D 1 ) = D\u2211 i=1 smi ,\nem(s D 1 ) = \u2211 1\u2264i1<i2<\u00b7\u00b7\u00b7<im\u2264D si1 \u00d7 si2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 sim .\nIn addition define e0(sn1 ) = 1. Then, the Girard-Newton formulae state,\nem(s D 1 ) =\n1\nm m\u2211 i=1 (\u22121)i\u22121em\u2212i(sD1 )pi(sD1 ).\nStarting with m = 1 and proceeding up to m = d, ed can be computed iteratively in just O(Dd2) time. By treating si = ki, the kernel matrix can be computed in O(n2d2D) time. While the ESP trick restricts the class of kernels we can use in SALSA, it applies for important kernel choices. For example, if each k(j) is a Gaussian kernel, then it is an ESP kernel if we set the bandwidths appropriately.\nIn what follows, we refer to a kernel such as k (5) which permits only d orders of interaction as a dth order kernel. A kernel which permits interactions of all D variables is of Dth order. Note that unlike in MKL, here we do not wish to learn the kernel. We use additive kernels to explicitly reduce the complexity of the function class over which we optimise for f\u0302 . Next, we present our theoretical results."}, {"heading": "3.2. Theoretical Analysis", "text": "We first consider the setting when f (j)\u2217 is in Hk(j) over which we optimise for f\u0302 (j). Theorem 3 generally bounds the excess risk of f\u0302 (3) in terms of RKHS parameters. Then, we specialise it to specific RKHSs in Theorem 4 and show that in many cases, the dependence on D reduces from exponential to polynomial for additive f\u2217. We begin with some assumptions.\nAssumption 1. f\u2217 has a decomposition f\u2217(x) =\u2211Md j=1 g (j)(x(j)) where each g(j) \u2208 Hk(j) .\nWe point out that the decomposition {g(j)} need not be unique. To enforce definiteness (by abusing notation) we define f (j)\u2217 \u2208 Hk(j) , j = 1, . . . ,Md to be the set of functions which minimise \u2211 j \u2016g(j)\u20162H\nk(j) . Denote the mini-\nmum value by \u2016f\u2217\u20162F . We denote it by a norm for reasons made clear in our proofs.\nLet k(j) have an eigenexpansion k(j)(x(j), x(j) \u2032 ) =\u2211\u221e\n`=1 \u00b5 (j) ` \u03c6 (j) ` (x (j))\u03c6 (j) ` (x (j)\u2032) in L2(PX(j)). Here,\n{(\u03c6(j)` )\u221e`=1} is an orthonormal basis for L2(PX(j)) and {(\u00b5(j)` )\u221e`=1} are its eigenvalues. PX(j) is the marginal distribution of the coordinates X(j). We also need the following regularity condition on the tail behaviour of the basis functions {\u03c6(j)` } for all k(j). Similar assumptions are made in (Zhang et al., 2013) and are satisfied for a large range of kernels including those in Theorem 4.\nAssumption 2. For some q \u2265 2, \u2203 \u03c1 < \u221e such that for all j = 1, . . . ,Md and ` \u2208 N, E[\u03c6(j)` (X)2q] \u2264 \u03c12q .\nWe also define the following,\n\u03b3(j)(\u03bb) = \u221e\u2211 `=1\n1\n1 + \u03bb/\u00b5 (j) `\n, \u03b3k(\u03bb) = Md\u2211 j=1 \u03b3(j)(\u03bb). (6)\nThe first term is known as the effective data dimensionality of k(j) (Zhang, 2005; Zhang et al., 2013) and captures the statistical difficulty of estimating a function inHk(j) . \u03b3k is the sum of the \u03b3(j)\u2019s. Our first theorem below bounds the excess risk of f\u0302 in terms \u2016f\u2217\u20162F and \u03b3k.\nTheorem 3. Let Assumptions 1 and 2 hold. and Y have bounded conditional variance: E[(Y \u2212 f\u2217(X))2|X] \u2264 \u03c32. Then the solution f\u0302 of (3) satisfies,\nE[R(f\u0302)]\u2212R(f\u2217) \u2264Md ( 20\u03bb\u2016f\u2217\u20162F + 12\u03c32\u03b3k(\u03bb)\nn + \u03c7(k)\n) .\nHere \u03c7(k) are kernel dependent low order terms and are given in (11) in Appendix A. Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case. We use ideas from Aronszajn (1950) to handle sum RKHSs. We consider a space F containing the tuple of functions f (j) \u2208 Hk(j) and use first order optimality conditions of (3) in F . The proof is given in Appendix A.\nThe term \u03b3k(\u03bb), which typically has exponential dependence on d, arises through the variance calculation. Therefore, by using small d we may reduce the variance of our estimate. However, this will also mean that we are only considering a smaller function class and hence suffer large bias if f\u2217 is not additive. In naive KRR, using a Dth order kernel (equivalent to setting Md = MD = 1) the excess risk depends exponentially in D. In contrast, for an additive dth order kernel, \u03b3k(\u03bb) has polynomial dependence on D if f\u2217 is additive. We make this concrete via the following theorem.\nTheorem 4. Assume the same conditions as Theorem 3. Then, suppressing log(n) terms,\n\u2022 if each k(j) has eigendecay \u00b5(j)` \u2208 O(`\u22122s/d), then by choosing \u03bb n \u22122s 2s+d , we have E[R(f\u0302)] \u2212 R(f\u2217) \u2208\nO(D2dn \u22122s 2s+d ),\n\u2022 if each k(j) has eigendecay \u00b5(j)` \u2208 O(\u03c0\u0303d exp(\u2212\u03b1`2)) for some constants \u03c0\u0303, \u03b1, then by choosing \u03bb 1/n, we have E[R(f\u0302)]\u2212R(f\u2217) \u2208 O(D 2d\u03c0\u0303d\nn ).\nWe bound \u03b3k via bounds for \u03b3(j) and use it to derive the optimal rates for the problem. The proof is in Appendix B.\nIt is instructive to compare the rates for the cases above when we use aDth order kernel \u03ba in KRR to estimate a nonadditive function. The first eigendecay is obtained if each k(j) is a Mate\u0301rn kernel. Then f (j) belongs to the Sobolev class of smoothness s (Berlinet & Thomas-Agnan, 2004; Tsybakov, 2008). By following a similar analysis, we can show that if \u03ba is in a Sobolev class, then the excess risk of KRR is O(n \u22122s 2s+D ) which is significantly slower than ours. In our setting, the rates are only exponential in d but we have an additional D2d term as we need to estimate several such functions. An example of the second eigendecay is the Gaussian kernel with \u03c0\u0303 = \u221a 2\u03c0 (Williamson et al., 2001). In the nonadditve case, the excess risk is in the Gaussian RKHS is O ( (2\u03c0)D/2 n ) which is slower than SALSA whose dependence on D is just polynomial. D, d do not appear in the exponent of n because the Gaussian RKHS contains very smooth functions. KRR is slower since we are optimising over the very large class of non-additive functions and consequently it is a difficult statistical problem. The faster rates for SALSA should not be surprising since the class of additive functions is smaller. The advantage of SALSA is its ability to recover the function at a faster rate when f\u2217 is additive. Finally we note that by taking each base kernel ki in the ESP kernel to be a 1D Gaussian, each k(j) is a Gaussian. However, at this point it is not clear to us if it is possible to recover a s-smooth Sobolev class via the tensor product of s-smooth one dimensional kernels.\nFinally, we analyse SALSA under more agnostic assumptions. We will neither assume that f\u2217 is additive nor that it lies in any RKHS. First, define the functions f (j)\u03bb , j = 1, . . . ,M which minimise the population objective.\n{f (j)\u03bb } Md j=1 = argmin\nf(j)\u2208H k(j) ,j=1,...,M\n\u03bb Md\u2211 j=1 \u2016f (j)\u20162H k(j) +\nE [( Y \u2212\nMd\u2211 j=1\nf (j)(X(j)) )2] . (7)\nLet f\u03bb = \u2211 j f (j) \u03bb , R (j) \u03bb = \u2016f (j) \u03bb \u2016Hk(j) and R\n2 d,\u03bb =\u2211\nj R (j) \u03bb\n2 . To bound the excess risk in the agnostic setting\nwe also define the class,\nHd,\u03bb = { f : X \u2192 R; f(x) = \u2211 j f (j)(x(j)), (8)\n\u2200j, f (j) \u2208 Hk(j) , \u2016f (j)\u2016Hk(j) \u2264 R (j) \u03bb\n} .\nTheorem 5. Let f\u2217 be an arbitrary measurable function and Y have bounded fourth moment E[Y 4] \u2264 \u03bd4. Further each k(j) satisfies Assumption 2. Then \u2200 \u03b7 > 0,\nE[R(f\u0302)]\u2212R(f\u2217) \u2264 (1 + \u03b7)AE + (1 + 1/\u03b7)EE,\nwhere, AE = inf f\u2208Hd,\u03bb\n\u2016f \u2212 f\u2217\u201622, EE \u2208 O (Md\u03b3k(\u03bb)\nn\n) .\nThe proof, given in Appendix C, also follows the template in Zhang et al. (2013). Loosely, we may interpret AE and EE as the approximation and estimation errors1. We may use Theorem 5 to understand the trade-offs in approximaing a non-additive function via an additive model. We provide an intuitive \u201cnot-very-rigorous\u201d explanation. Hd,\u03bb is typically increasing with d since higher order additive functions contain lower order functions. Hence, AE is decreasing with d as the infimum is taken over a larger set. On the other hand, EE is increasing with d. With more data EE decreases due to the 1/n term. Hence, we can afford to use larger d to reduce AE and balance with EE. This results in an overall reduction in the excess risk.\nThe actual analysis would be more complicated sinceHd,\u03bb is a bounded class depending intricately on \u03bb. It also depends on the kernels k(j), which differ with d. To make the above intuition concrete and more interpretable, it is necessary to have a good handle on AE. However, if we are to overcome the exponential dependence in dimension, usual nonparametric assumptions such as Ho\u0308lderian/ Sobolev conditions alone will not suffice. Current lower bounds suggest that the exponential dependence is unavoidable (Gyo\u0308rfi et al., 2002; Tsybakov, 2008). Additional assumptions will be necessary to demonstrate faster convergence. Once we control AE, the optimal rates can be obtained by optimising the bound over \u03b7, \u03bb. We wish to pursue this in future work."}, {"heading": "3.3. Practical Considerations", "text": "Choice of Kernels: The development of our algorithm and our analysis assume that the ki\u2019s are known. This is hardly the case in reality and they have to be chosen properly for good empirical performance. Cross validation is not feasible here as there are too many hyper-parameters. In our experiments we set each ki to be a Gaussian kernel ki(xi, x \u2032 i) = \u03c3Y exp(\u2212(xi \u2212 x\u2032i)2/2h2i ) with bandwidth hi = c\u03c3in \u22121/5. Here \u03c3i is the standard deviation of the ith covariate and \u03c3Y is the standard deviation of Y . The choice of bandwidth was inspired by several other kernel methods which use bandwidths on the order \u03c3in\u22121/5 (Ravikumar et al., 2009; Tsybakov, 2008). The constant c was hand tuned \u2013 we found that performance was robust to choices between 5 and 60. In our experiments we use c = 20. c\n1Loosely (and not strictly) since f\u0302 need not be in Hd,\u03bb.\nwas chosen by experimenting on a collection of synthetic datasets and then used in all our experiments. Both synthetic and real datasets used in experiments are independent of the data used to tune c.\nChoice of d, \u03bb: If the additive order of f\u2217 is known and we have sufficient data then we can use that for d in (5). However, this is usually not the case in practice. Further, even in non-additive settings, we may wish to use an additive model to improve the variance of our estimate. In these instances, our approach to choose d uses cross validation. For a given d we solve (1) for different \u03bb and pick the best one via cross validation. To choose the optimal d we cross validate on d. In our experiments we observed that the cross validation error had bi-monotone like behaviour with a unique local optimum on d. Since the optimal d was typically small we search by starting at d = 1 and keep increasing until the error begins to increase again. If d could be large and linear search becomes too expensive, a binary search like procedure on {1, . . . , D} can be used.\nWe conclude this section with a couple of remarks. First, we could have considered an alternative additive model which sums all interactions up to dth order instead of just the dth order. The excess risk of this model differs from Theorems 3, 4 and 5 only in subdominant terms and/or constant factors. The kernel can be computed efficiently using the same trick by summing all polynomials up to d. In our experiments we found that both our original model (2) and summing over all interactions performed equally well. For simplicity, results are presented only for the former.\nSecondly, as is the case with most kernel methods, SALSA requires O(n2) space to store the kernel matrix and O(n3) effort to invert it. Some recent advances in scalable kernel methods such as random features, divide and conquer techniques, stochastic gradients etc. (Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n. However, this is beyond the scope of this paper and is left to future work. For this reason, we also limit our experiments to moderate dataset sizes. The goal of this paper is primarily to introduce additive models of higher order, address the combinatorial cost in such models and theoretically demonstrate the improvements in the excess risk."}, {"heading": "4. Experiments", "text": "We compare SALSA to the following. Nonparametric models: Kernel Ridge Regression (KRR), k-Nearest Neighbors (kNN), Nadaraya Watson (NW), Locally Linear/ Quadratic interpolation (LL, LQ), -Support Vector Regression ( \u2212SVR), \u03bd-Support Vector Regression (\u03bd\u2212SVR), Gaussian Process Regression (GP), Regression Trees (RT), Gradient Boosted Regression Trees (GBRT) (Friedman,\n2000), RBF Interpolation (RBFI), M5\u2019 Model Trees (M5\u2019) (Wang & Witten, 1997) and Shepard Interpolation (SI). Nonparametric additive models: Back-fitting with cubic splines (BF) (Hastie & Tibshirani, 1990), Multivariate Adaptive Regression Splines (MARS) (Friedman, 1991), Component Selection and Smoothing (COSSO) (Lin & Zhang, 2006), Sparse Additive Models (SpAM) (Ravikumar et al., 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al., 2011). Parametric models: Ridge Regression (RR), Least Absolute Shrinkage and Selection (LASSO) (Tibshirani, 1994) and Least Angle Regression (LAR) (Efron et al., 2004). We used software from (Chang & Lin, 2011; Hara & Chellappa, 2013; Jakabsons, 2015; Lin & Zhang, 2006; Rasmussen & Williams, 2006) or from Matlab. In some cases we used our own implementation."}, {"heading": "4.1. Synthetic Experiments", "text": "We begin with a series of synthetic examples. We compare SALSA to some non-additive methods to convey intuition about our additive model. First we create a synthetic low order function of order d = 3 in D = 15 dimensions. We do so by creating a d dimensional function fd and add that function over all ( D d ) combinations of coordinates. We compare SALSA using order 3 and compare against others. The results are given in Figure 1(a). This setting is tailored to the assumptions of our method and, not surprisingly, it outperforms all alternatives.\nNext we demonstrate the bias variance trade-offs in using additive approximations on non-additive functions. We created a 15 dimensional (non-additive) function and fitted a SALSA model with d = 1, 2, 4, 8, 15 for difference choices of n. The results are given in Figure 1(b). The interesting observation here is that for small samples sizes small d performs best. However, as we increase the sample size we can also increase the capacity of the model by accommodating higher orders of interaction. In this regime, large d produces the best results. This illustrates our previous point that the order of the additive model gives us another way to control the bias and variance in a regression task. We posit that when n is extremely large, d = 15 will eventually beat all other models. Finally, we construct synthetic functions in D = 20 to 50 dimensions and compare against other methods in Figures 1(c) to 1(f). Here, we chose d via cross validation. Our method outperforms or is competitive with other methods."}, {"heading": "4.2. Real Datasets", "text": "Finally we compare SALSA against the other methods listed above on 16 datasets. The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014). Table 1 gives the average squared error\non a test set. For the Speech dataset we have indicated the training time (including cross validation for selecting hyper-parameters) for each method. For SALSA we have also indicated the order d chosen by cross validation. See the caption under the table for more details.\nSALSA performs best (or is very close to the best) in 5 of the datasets. Moreover it falls within the top 5 in all but two datasets, coming sixth in both instances. Observe that in many cases d chosen by SALSA is much smaller thanD, but importantly also larger than 1. This observation (along with Fig 1(b)) corroborates a key theme of this paper: while it is true that additive models improve the variance in high dimensional regression, it is often insufficient to confine ourselves to just first order models.\nIn Appendix D we have given the specifics on the datasets such as preprocessing, the predictors, features etc. We have also discussed some details on the alternatives used."}, {"heading": "5. Conclusion", "text": "SALSA finds additive approximations to the regression function in high dimensions. It has less bias than first order models and less variance than non-additive methods. Algorithmically, it requires plugging in an additive kernel to KRR. In computing the kernel, we use the Girard-Newton formulae to efficiently sum over a combinatorial number of terms. Our theorems show that the excess risk depends only\npolynomially on D when f\u2217 is additive, significantly better than the usual exponential dependence of nonparametric methods, albeit under stronger assumptions. Our analysis of the agnostic setting provides intuitions on the tradeoffs invovled with changing d. We demonstrate the efficacy of SALSA via a comprehensive empirical evaluation. Going forward, we wish to use techniques from scalable kernel methods to handle large datasets.\nTheorems 3,4 show polynomial dependence on D when f\u2217 is additive. However, these theorems are unsatisfying since in practice regression functions need not be additive. We believe our method did well even on non-additive settings since we could control model capacity via d. In this light, we pose the following open problem: identify suitable assumptions to beat existing lower bounds and prove faster convergence of additive models whose additive order d increases with sample size n. Our Theorem 5 might be useful in this endeavour."}, {"heading": "Acknowledgements", "text": "We thank Calvin McCarter, Ryan Tibshirani and Larry Wasserman for the insightful discussions and feedback on the paper. We also thank Madalina Fiterau for providing us with datasets. This work was partly funded by DOE grant DESC0011114."}, {"heading": "Appendix", "text": "A. Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation. Some intermediate technical results can be obtained directly from Zhang et al. (2013) but we repeat them (or provide an outline) here for the sake of completeness.\nIn addition to the definitions presented in the main text, we will also need the following quantities,\n\u03b2 (j) t = \u221e\u2211 `=t+1 \u00b5 (j) ` , \u03a8 (j) = \u221e\u2211 `=1 \u00b5 (j) ` , b(n, t, q) = max (\u221a max(q, log t) , max(q, log t) n1/2\u22121/q ) .\nHere \u03a8(j) is the trace of k(j). \u03b2(j)t depends on some t \u2208 N which we will pick later. Also define \u03b2t = \u2211 j \u03b2 (j) t and\n\u03a8 = \u2211 j \u03a8 (j).\nNote that the excess risk can be decomposed into bias and variance terms,R(f\u0302)\u2212R(f\u2217) = E[\u2016f\u0302\u2212f\u2217\u201622] = \u2016f\u2217\u2212Ef\u0302\u201622 + E[\u2016f\u0302 \u2212 Ef\u0302\u201622]. In Sections A.2 and A.3 respectively, we will prove the following bounds which will yield in Theorem 3:\n\u2016f\u2217 \u2212 Ef\u0302\u201622 \u2264Md ( 8\u03bb\u2016f\u2217\u20162F +\n8Md 3/2\u03c14\u2016f\u2217\u20162F \u03bb \u03a8\u03b2t + \u2016f\u2217\u20162F Md\u2211 j=1 \u00b5 (j) t+1 + (CMdb(n, t, q)\u03c12\u03b3k(\u03bb)\u221a n )q \u2016f\u2217\u201622 ) , (9)\nE[\u2016f\u0302 \u2212 Ef\u0302\u201622] \u2264Md ( 12\u03bb\u2016f\u2217\u20162F + 12\u03c32\u03b3k(\u03bb)\nn + (10)\n( 2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F )( Md\u2211 j=1 \u00b5 (j) t+1 + 12Md\u03c1 4 \u03bb \u03a8\u03b2t + (CMdb(n, t, q)\u03c12\u03b3k(\u03bb)\u221a n )q) \u2016f\u2217\u201622 ) .\nAccordingly, this gives the following expression for \u03c7(k),\n\u03c7(k) = inf t\n[ 8Md\n3/2\u03c14\u2016f\u2217\u20162F \u03bb \u03a8\u03b2t +\n( 2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F + 1\n)( CMdb(n, t, q)\u03c1\n2\u03b3k(\u03bb)\u221a n\n)q \u2016f\u2217\u201622 ) +\n( 2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F )( Md\u2211 j=1 \u00b5 (j) t+1 + 12Md\u03c1 4 \u03bb \u03a8\u03b2t ) + \u2016f\u2217\u20162F Md\u2211 j=1 \u00b5 (j) t+1 ] . (11)\nNote that the second term in \u03c7(k) is usually low order for large enough q due to the n\u2212q/2 term. Therefore if in our setting \u03b2\n(j) t and \u00b5 (j) t+1 are small enough, \u03c7(k) is low order. We show this for the two kernel choices of Theorem 4 in Appendix B.\nFirst, we review some well known results on RKHS\u2019s which we will use in our analysis. Let \u03ba be a PSD kernel andH\u03ba be its RKHS. Then \u03ba acts as the representer of evaluation \u2013 i.e. for any f \u2208 H\u03ba, \u3008f, \u03ba(\u00b7, x)\u3009H\u03ba = f(x). Denote the RKHS norm \u2016f\u2016H\u03ba = \u221a \u3008f, f\u3009H\u03ba and the L2 norm \u2016f\u20162 = \u221a\u222b f2.\nLet the kernel \u03ba have an eigenexpansion \u03ba(x, x\u2032) = \u2211\u221e `=1 \u00b5`\u03c6`(x)\u03c6`(x\n\u2032). Denote the basis coefficients of f in {\u03c6`} via {\u03b8`}. That is, \u03b8` = \u222b f \u00b7 \u03c6` dP and f = \u2211\u221e `=1 \u03b8`\u03c6`. The following results are well known (Scho\u0308lkopf & Smola, 2001; Steinwart & Christmann, 2008),\n\u3008\u03c6`, \u03c6`\u3009 = 1/\u00b5`, \u2016f\u201622 = \u221e\u2211 `=1 \u03b82` , \u2016f\u20162H\u03ba = \u221e\u2211 `=1 \u03b82` \u00b5` .\nBefore we proceed, we make the following remark on the minimiser of (3). Remark 6. The solution of (3) takes the form f\u0302(\u00b7) = \u2211n i=1 \u03b1ik(\u00b7, Xi) where k is the sum kernel (4).\nProof. The key observation is that we only need to consider n (and not nMd) parameters even though we are optimising overMd RKHSs. The reasoning uses a powerful result from Aronszajn (1950). Consider the class of functionsH\u2032 = {f =\u2211 j f\n(j); f (j) \u2208 Hk(j)}. In (3) we are minimising over H\u2032. Any f \u2208 H\u2032 need not have a unique additive decomposition. ConsiderH \u2282 H\u2032 which only contains the minimisers in the expression below.\n\u2016f\u20162H = inf g(j)\u2208H\nk(j) ;f=\n\u2211 g(j) M\u2211 j=1 \u2016g(j)\u20162H k(j)\nAronszajn (1950) showed that H is an RKHS with the sum kernel k = \u2211 j k\n(j) and its RKHS norm is \u2016 \u00b7 \u2016H. Clearly, the minimiser of (3) lies in H. For any g\u2032 \u2208 H\u2032, we can pick a corresponding g \u2208 H with the same sum of squared errors (as g = g\u2032) but lower complexity penalty (as g minimises the sum of norms for any g\u2032 = g). Therefore, we may optimise (3) just overH and notH\u2032. An application of Mercer\u2019s theorem concludes the proof."}, {"heading": "A.1. Set up", "text": "We first define the following function class of the product of all RKHS\u2019s, F = Hk(1) \u00d7 Hk(2) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Hk(Md) ={ f = (f (1), . . . , f (Md))\n\u2223\u2223f (j) \u2208 Hk(j) \u2200j} and equip it with the inner product \u3008f1, f2\u3009 = \u3008f (1)1 , f (1)2 \u3009Hk(1) + \u00b7 \u00b7 \u00b7 + \u3008f (Md)1 , f (Md) 2 \u3009Hk(Md) . Here, f (j) 1 are the elements of f1 and \u3008\u00b7, \u00b7\u3009Hk(j) is the RKHS inner product of Hk(j) . Therefore\nthe norm is \u2016f\u20162F = \u2211Md j=1 \u2016f (j)\u20162H k(j) . Denote \u03be(j)x = k(j)(x, \u00b7) and \u03bex(\u00b7) = K(\u00b7, x). Observe that for an additive\nfunction f = \u2211 j f (j)(x),\nf(x) = \u2211 j f (j)(x) = \u2211 j \u3008f (j), \u03be(j)x \u3009Hk(j) = \u3008f , \u03bex\u3009.\nRecall that the solution to (3) is denoted by f\u0302 and the individual functions of the solution are given by f\u0302 (j). We will also use f\u2217 and f\u0302 to denote the representations of f\u2217 and f\u0302 in F , i.e., f\u2217 = (f (1)\u2217 , . . . , f (Md)\u2217 ) and f\u0302 = (f\u0302 (1), . . . , f\u0302 (Md)). Note that \u2016f\u2217\u20162F is precisely the bound used in Theorem 3. We will also denote \u2206(j) = f\u0302 (j) \u2212 f (j) \u2217 \u2208 Hk(j) , \u2206 =\n(\u2206(1), . . . ,\u2206(Md)) \u2208 F , and \u2206 = \u2211 j \u2206 (j) = f\u0302 \u2212 f\u2217.\nFor brevity, from now on we will write k(j)(x, x\u2032) instead of k(j)(x(j), x(j) \u2032 ). Further, since d is fixed in this analysis we will write M for Md.\nA.2. Bias (Proof of Bound (9)) Note that we need to bound \u2016E[\u2206]\u20162 which by Jensen\u2019s inequality is less than E[\u2016E[\u2206|Xn1 ]\u20162]. Since, \u2016E[\u2206|Xn1 ]\u201622 \u2264 M \u2211M j=1 \u2016E[\u2206(j)|Xn1 ]\u201622, we will focus on bounding \u2211M j=1 \u2016E[\u2206(j)|Xn1 ]\u201622.\nWe can write the optimisation objective (3) as follows,\nf\u0302 = argmin f\u2208F\n1\nn n\u2211 i=1 (\u3008f , \u03beXi\u3009 \u2212 Yi) 2 + \u03bb\u2016f\u20162F (12)\nSince this is Fre\u0301chet differentiable in F in the metric induced by the inner product defined above, the first order optimality conditions for f\u0302 (j) give us,\n1\nn n\u2211 i=1 ( \u3008\u03beXi , f\u0302 \u2212 f\u2217\u3009 \u2212 i ) \u03be (j) Xi + 2\u03bbf\u0302 (j) = 0.\nHere, we have taken Yi = f\u2217(Xi) + i where E[ i|Xi] = 0. Doing this for all f\u0302 (j) we have,\n1\nn n\u2211 i=1 \u03beXi (\u3008\u03beXi ,\u2206\u3009 \u2212 i) + \u03bbf\u0302 = 0 (13)\nTaking expectations conditioned on Xn1 and rearranging we get,\n(\u03a3\u0302 + \u03bbI)E[\u2206|Xn1 ] = \u2212\u03bbf\u2217, (14)\nwhere \u03a3\u0302 = 1n \u2211 i \u03beXi \u2297 \u03beXi is the empirical covariance. Since \u03a3\u0302 0,\n\u2200j\u2032, \u2016E[\u2206(j \u2032)|Xn1 ]\u20162H\nk(j \u2032) \u2264 M\u2211 j=1 \u2016E[\u2206(j)|Xn1 ]\u20162H k(j) = \u2016E[\u2206|Xn1 ]\u20162F \u2264 \u2016f\u2217\u20162F (15)\nLet E[\u2206(j)|Xn1 ] = \u2211\u221e `=1 \u03b4 (j) ` \u03c6 (j) ` where \u03c6 (j) ` are the eigenfunctions in the expansion of k (j). Denote \u03b4(j)\u2193 = (\u03b4 (j) 1 , . . . , \u03b4 (j) t ) and \u03b4 (j) \u2191 = (\u03b4 (j) t+1, \u03b4 (j) t+2, . . . ). We will set t later. Since \u2016E[\u2206(j)|Xn1 ]\u201622 = \u2016\u03b4 (j) \u2193 \u201622 + \u2016\u03b4 (j) \u2191 \u201622 we will bound the two terms. The latter term is straightforward,\n\u2016\u03b4(j)\u2191 \u2016 2 2 \u2264 \u00b5 (j) t+1 \u221e\u2211 `=t+1 \u03b4 (j) ` 2 \u00b5 (j) ` \u2264 \u00b5(j)t+1\u2016E[\u2206(j)|Xn1 ]\u20162H k(j) \u2264 \u00b5(j)t+1\u2016f\u2217\u20162F (16)\nTo control \u2016\u03b4(j)\u2193 \u2016, let f (j) \u2217 = \u2211 ` \u03b8 (j) ` \u03c6 (j) ` . Also, define the following: \u03b8 (j) \u2193 = (\u03b8 (j) 1 , . . . , \u03b8 (j) t ), \u03a6\n(j) \u2208 Rn\u00d7t, \u03a6(j)i` = \u03c6\n(j) ` (Xi), \u03a6 (j) ` = (\u03c6 (j) ` (X1), . . . , \u03c6 (j) ` (Xn)) \u2208 Rn, M(j) = diag(\u00b5 (j) 1 , . . . , \u00b5 (j) t ) \u2208 Rt\u00d7t+ and v(j) \u2208 Rn where v (j) i =\u2211\n`>t \u03b4 (j) ` \u03c6 (j) ` (Xi) = E[\u2206 (j) \u2191 (Xi)|Xn1 ].\nFurther define, \u03a6 = [\u03a6(1) . . .\u03a6(M)] \u2208 Rn\u00d7tM , M = diag(M(1), . . . ,M(M)) \u2208 RtM\u00d7tM , vi = \u2211 j v (j), \u03b4\u2193 = [\u03b4 (1) \u2193 ; . . . ; \u03b4 (M) \u2193 ] \u2208 RtM and \u03b8\u2193 = [\u03b8 (1) \u2193 ; . . . ; \u03b8 (M) \u2193 ] \u2208 RtM .\nNow compute the F-inner product between (0, . . . , \u03c6(j)` , . . . ,0) with equation (14) to obtain,\n1\nn n\u2211 i=1 \u3008\u03c6(j)` , \u03be (j) Xi \u3009H k(j) \u3008\u03beXi ,E[\u2206|Xn1 ]\u3009+ \u03bb\u3008\u03c6 (j) ` ,E[\u2206 (j)|Xn1 ]\u3009Hk(j) = \u2212\u03bb\u3008\u03c6 (j) ` , f (j) \u2217 \u3009H k(j)\n1\nn n\u2211 i=1 \u03c6 (j) ` (Xi) M\u2211 j=1 \u2211 `\u2032\u2264t \u03c6 (j) `\u2032 (Xi)\u03b4 (j) `\u2032 + \u2211 `\u2032>t \u03c6 (j) `\u2032 (Xi)\u03b4 (j) `\u2032 + \u03bb \u03b4(j)` \u00b5 (j) ` = \u2212\u03bb \u03b8 (j) ` \u00b5 (j) `\nAfter repeating this for all j and for all ` = 1, . . . , t, and arranging the terms appropriately this reduces to( 1\nn \u03a6>\u03a6 + \u03bbM\u22121\n) \u03b4\u2193 = \u2212\u03bbM\u22121\u03b8\u2193 \u2212 1\nn \u03a6>v\nBy writing Q = (I + \u03bbM\u22121)1/2, we can rewrite the above expression as( I +Q\u22121 ( 1\nn \u03a6>\u03a6\u2212 I\n) Q\u22121 ) Q\u03b4\u2193 = \u2212\u03bbQ\u22121M\u22121\u03b8\u2193 \u2212 1\nn Q\u22121\u03a6>v.\nWe will need the following technical lemmas. The proofs are given at the end of this section. These results correspond to Lemma 5 in Zhang et al. (2013).\nLemma 7. \u2016\u03bbQ\u22121M\u22121\u03b8\u2193\u201622 \u2264 \u03bb\u2016f\u2217\u20162F .\nLemma 8. E [ \u2016 1nQ \u22121\u03a6>v\u201622 ] \u2264 1\u03bbM 3/2\u03c14\u2016f\u2217\u20162F\u03a8\u03b2t.\nLemma 9. Define the event E = {\u2016Q\u22121( 1n\u03a6 >\u03a6\u2212 I)Q\u22121\u2016op \u2264 1/2}. Then, there exists a constant C s.t.\nP(Ec) \u2264 ( max (\u221a max(q, log t) , max(q, log t)\nn1/2\u22121/q\n) \u00d7 MC\u03c1\n2\u03b3k(\u03bb)\u221a n\n)q .\nWhen E holds, by Lemma 9 and noting that Q I ,\n\u2016\u03b4\u2193\u201622 \u2264 \u2016Q\u03b4\u2193\u201622 = \u2225\u2225\u2225(I +Q\u22121( 1\nn \u03a6>\u03a6\u2212 I\n) Q\u22121 )\u22121( \u2212\u03bbQ\u22121M\u22121\u03b8\u2193 \u2212 1\nn Q\u22121\u03a6>v )\u2225\u2225\u22252 \u2264 4\u2016\u03bbQ\u22121M\u22121\u03b8\u2193 + 1\nn Q\u22121\u03a6>v\u201622. \u2264 8\u2016\u03bbQ\u22121M\u22121\u03b8\u2193\u201622 + 8\u2016\n1 n Q\u22121\u03a6>v\u201622\nNow using Lemmas 7 and 8,\nE[\u2016\u03b4\u2193\u201622|E ] \u2264 8 ( \u03bb\u2016f\u2217\u20162F +\nM3/2\u03c14\u2016f\u2217\u20162F\u03a8\u03b2t \u03bb ) Since E[\u2016\u03b4\u2193\u201622] = P(E)E[\u2016\u03b4\u2193\u201622|E ] + P(Ec)E[\u2016\u03b4\u2193\u201622|Ec] and by using the fact that \u2016\u03b4\u2193\u20162 \u2264 \u2016E[\u2206|Xn1 ]\u201622 \u2264 \u2016f\u2217\u201622, we have\nE[\u2016\u03b4\u2193\u201622] \u2264 8\u03bb\u2016f\u2217\u20162F + 8M\u03c14\u2016f\u2217\u20162F\u03a8\u03b2t\n\u03bb +(\nmax (\u221a max(q, log t) , max(q, log t)\nn1/2\u22121/q\n) \u00d7 MC\u03c1\n2\u03b3k(\u03bb)\u221a n\n)q \u2016f\u2217\u201622\nFinally using (16) and by noting that\n\u2016E[\u2206|Xn1 ]\u201622 \u2264M M\u2211 j=1 \u2016E[\u2206(j)|Xn1 ]\u201622 = M ( \u2016\u03b4\u2193\u201622 + \u2211 j \u2016\u03b4(j)\u2191 \u2016 2 2 ) \u2264M ( \u2016\u03b4\u2193\u201622 + \u2016f\u2217\u20162F \u2211 j \u00b5 (j) t+1 ) and then taking expectation over Xn1 , we obtain the bound for the bias in (9)."}, {"heading": "Proofs of Technical Lemmas", "text": ""}, {"heading": "A.2.1. PROOF OF LEMMA 7", "text": "Lemma 7 is straightforward.\n\u2016Q\u22121M\u22121\u03b8\u2193\u201622 = M\u2211 j=1 \u2016Q(j) \u22121 M(j) \u22121 \u03b8 (j) \u2193 \u2016 2 2 = M\u2211 j=1 \u03b8 (j) \u2193 > (M(j) 2 + \u03bbM(j))\u22121\u03b8(j)\u2193\n\u2264 M\u2211 j=1 \u03b8 (j) \u2193 > (\u03bbM(j))\u22121\u03b8(j)\u2193 = 1 \u03bb M\u2211 j=1 t\u2211 `=1 \u03b8 (j) ` 2 \u00b5 (j) ` \u2264 1 \u03bb \u2016f\u2217\u20162F"}, {"heading": "A.2.2. PROOF OF LEMMA 8", "text": "We first decompose the LHS as follows,\u2225\u2225\u2225\u2225 1nQ\u22121\u03a6>v \u2225\u2225\u2225\u22252\n2\n= \u2225\u2225\u2225\u2225(M + \u03bbI)\u22121/2( 1nM1/2\u03a6>v )\u2225\u2225\u2225\u22252\n2\n\u2264 1 \u03bb \u2225\u2225\u2225\u2225 1nM1/2\u03a6>v \u2225\u2225\u2225\u22252\n2\n(17)\nThe last step follows by noting that \u2016(M + \u03bbI)\u22121/2\u20162op = maxj,` 1/(\u00b5 (j) ` + \u03bb) \u2264 1/\u03bb. Further,\nE [ \u2016M1/2\u03a6>v\u201622 ] = M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` E[(\u03a6 (j) ` > v)2] \u2264 M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` E[\u2016\u03a6 (j) ` \u2016 2 2\u2016v\u201622] (18)\nNote that the term inside the summation in the RHS can be bounded by, \u221a E[\u2016\u03a6(j)` \u201642]E[\u2016v\u201642]. We bound the first expectation via,\nE [ \u2016\u03a6(j)` \u2016 4 ] = E [( n\u2211 i=1 \u03c6 (j) ` (Xi) 2 )2] \u2264 E [ n n\u2211 i=1 \u03c6 (j) ` (Xi) 4 ] \u2264 n2\u03c14\nwhere the last step follows from Assumption 2. For the second expectation we first bound \u2016v\u20164,\n\u2016v\u201642 =  n\u2211 i=1 ( M\u2211 j=1 v (j) i )22 \u2264 M n\u2211 i=1 M\u2211 j=1 v (j) i 2 2 \u2264M3n n\u2211 i=1 M\u2211 j=1 v (j) i 4\nNow by the Cauchy Schwarz inequality,\nv (j) i\n2 = (\u2211 `>t \u03b4 (j) ` \u03c6 (j) ` (Xi) )2 \u2264 (\u2211 `>t \u03b4 (j) ` 2 \u00b5 (j) ` )(\u2211 `>t \u00b5 (j) ` \u03c6 (j) ` (Xi) 2 ) .\nTherefore,\nE [ \u2016v\u20164 ] \u2264M3n n\u2211 i=1 M\u2211 j=1 E [ \u2016E[\u2206(j)|Xn1 ]\u20164H k(j) (\u2211 `>t \u00b5 (j) ` \u03c6 (j) ` (Xi) 2 )2]\n\u2264M3n\u2016f\u2217\u20164F M\u2211 j=1 n\u2211 i=1 \u2211 `,`\u2032>t E[\u00b5(j)` \u00b5 (j) `\u2032 \u03c6 (j) ` (Xi) 2\u03c6 (j) `\u2032 (Xi) 2] \u2264M3n\u03c14\u2016f\u2217\u20164F M\u2211 j=1 n\u2211 i=1 (\u2211 `>t \u00b5 (j) ` )2 \u2264M3n2\u03c14\u2016f\u2217\u20164F M\u2211 j=1 \u03b2 (j) t 2\nHere, in the first step we have used the definition of \u2016E[\u2206(j)|Xn1 ]\u2016Hk(j) , in the second step, equation (15), in the third step assumption 2 and Cauchy Schwarz, and in the last step, the definition of \u03b2t. Plugging this back into (18), we get\nE [ \u2016M1/2\u03a6>v\u20162 ] \u2264M3/2n2\u03c14\u2016f\u2217\u20162F \u221a\u221a\u221a\u221a M\u2211 j=1 \u03b2 (j) t 2 M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` \u2264M 3/2n2\u03c14\u2016f\u2217\u20162F\u03a8\u03b2t\nThis bound, along with equation (17) gives us the desired result."}, {"heading": "A.2.3. PROOF OF LEMMA 9", "text": "Define \u03c0(j)i = {\u03c6 (j) ` (xi)}t`=1 \u2208 Rt, \u03c0i = [\u03c0 (1) i ; . . . ;\u03c0 (M) i ] \u2208 RtM and the matricesAi = Q\u22121(\u03c0i\u03c0>i \u2212I)Q\u22121 \u2208 Rtm\u00d7tM . Note that Ai = A>i and E[Ai] = Q\u22121(E[\u03c0i\u03c0>i ]\u2212 I)Q\u22121 = 0.\nThen, if i, i = 1, . . . , n are i.i.d Rademacher random variables, by a symmetrization argument we have, E [\u2225\u2225\u2225Q\u22121( 1\nn \u03a6>\u03a6\u2212 I\n) Q\u22121 \u2225\u2225\u2225k op ] = E [\u2225\u2225\u2225 1 n n\u2211 i=1 Ai \u2225\u2225\u2225k op ] \u2264 2kE [\u2225\u2225\u2225 1 n n\u2211 i=1 iAi \u2225\u2225\u2225k op ] (19)\nThe above term can be bounded by the following expression.\n2q \u221aemax(q, log(t))\u03c12\u221aM\u221a n \u221a\u221a\u221a\u221a M\u2211 `=1 \u03b3(j)(\u03bb)2 + 4emax(q, log(t))\u03c12 ( M n )1\u22121/q ( M\u2211 `=1 \u03b3(j)(\u03bb)q )1/qq\n\u2264 ( C\n2\n)q max (\u221a M(max(q, log t)), M1\u22121/q max(q, log t)\nn1/2\u22121/q\n)q ( \u03c12\u03b3k(\u03bb)\u221a\nn )q The proof mimics Lemma 6 in (Zhang et al., 2013) by performing essentially the same steps over F instead of the usual Hilbert space. In many of the steps, M terms appear (instead of the one term for KRR) which is accounted for via Jensen\u2019s inequality.\nFinally, by Markov\u2019s inequality, P(Ec) \u2264 2kE [\u2225\u2225\u2225Q\u22121( 1\nn \u03a6>\u03a6\u2212 I\n) Q\u22121 \u2225\u2225\u2225q op ] \u2264 Cq max (\u221a M(max(q, log t)), M1\u22121/q max(q, log t)\nn1/2\u22121/q\n)q ( \u03c12\u03b3k(\u03bb)\u221a\nn\n)q\nA.3. Variance (Proof of Bound (10))\nOnce again, we follow Zhang et al. (2013). The tricks we use to generalise it to the additive case (i.e. over F) are the same as that for the bias. Note that since E[\u2016f\u0302\u2212Ef\u0302\u201622] \u2264 E[\u2016f\u0302\u2212g\u201622] for all g, it is sufficient to bound E[\u2016f\u0302\u2212f\u2217\u201622] = E[\u2016\u2206\u201622].\nFirst note that,\n\u03bbE[\u2016f\u0302\u20162F |Xn1 ] \u2264 E\n[ 1\nn n\u2211 i=1 ( f\u0302(Xi)\u2212 Yi )2 + \u03bb\u2016f\u0302\u20162F \u2223\u2223\u2223\u2223Xn1 ] \u2264 1 n n\u2211 i=1 E[ 2i |Xn1 ] + \u03bb\u2016f\u2217\u20162F \u2264 \u03c32 + \u03bb\u2016f\u2217\u20162F\nThe second step follows by the fact that f\u0302 is the minimiser of (12). Then, for all j,\nE[\u2016\u2206(j)\u20162H k(j) |Xn1 ] \u2264 E[\u2016\u2206\u20162F |Xn1 ] \u2264 2\u2016f\u2217\u20162F + 2E[\u2016f\u0302\u201622|Xn1 ] \u2264\n2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F (20)\nLet \u2206(j) = \u2211\u221e `=1 \u03b4 (j) ` \u03c6 (j) ` . Note that the definition of \u03b4 (j) ` is different here. Define \u03b4 (j) \u2193 , \u03b4 (j) \u2191 ,\u2206 (j) \u2193 ,\u2206 (j) \u2191 , \u03b4\u2193 analogous to the definitions in Section A.2. Then similar to before we have,\nE[\u2016\u03b4(j)\u2191 \u2016 2 2] \u2264 \u00b5 (j) t+1E[\u2016\u2206 (j) \u2191 \u2016 2 H k(j)\n] \u2264 \u00b5(j)t+1 ( 2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F ) We may use this to obtain a bound on E[\u2016\u2206\u2191\u20162]. To obtain a bound on E[\u2016\u2206\u2193\u20162], take the F inner product of (0, . . . , \u03c6\n(j) ` , . . . ,0) with the first order optimality condition (13) and following essentially the same procedure to the\nbias we get, ( 1\nn \u03a6>\u03a6 + \u03bbM\u22121\n) \u03b4\u2193 = \u2212\u03bbM\u22121\u03b8\u2193 \u2212 1\nn \u03a6>v +\n1 n \u03a6>\nwhere \u03a6,M, \u03b8\u2193 are the same as in the bias calculation. v(j) \u2208 Rn where v(j)i = \u2211 `>t \u03b4 (j) ` \u03c6 (j) ` (Xi) = E[\u2206 (j) \u2191 (Xi)|Xn1 ] (recall that \u03b4(j)` is different to the definition in the bias) and \u2208 Rn, i = Yi \u2212 f\u2217(Xi) is the vector of errors. Then we write, (\nI +Q\u22121 ( 1\nn \u03a6>\u03a6\u2212 I\n) Q\u22121 ) Q\u03b4\u2193 = \u2212\u03bbQ\u22121M\u22121\u03b8\u2193 \u2212 1\nn Q\u22121\u03a6>v +\n1 n Q\u22121\u03a6>\nwhere Q = (I + \u03bbM\u22121)1/2 as before. Following a similar argument to the bias, when the event E holds,\n\u2016\u03b4\u2193\u201622 \u2264 \u2016Q\u03b4\u2193\u201622 \u2264 4\u2016\u03bbQ\u22121M\u22121\u03b8\u2193 + 1\nn Q\u22121\u03a6>v +\n1 n Q\u22121\u03a6> \u201622\n\u2264 12\u2016\u03bbQ\u22121M\u22121\u03b8\u2193\u20162 + 12\u2016 1 n Q\u22121\u03a6>v\u20162 + 12\u2016 1 n Q\u22121\u03a6> \u201622 (21)\nBy Lemma 7, the first term can be bounded via 12\u03bb\u2016f\u2217\u20162F . For the second and third terms we use the following two lemmas, the proofs of which are given at the end of this subsection. Lemma 10. E [ \u2016 1nQ \u22121\u03a6>v\u201622 ] \u2264 1\u03bbM\u03c1 4\u03a8\u03b2t(2\u03c3 2/\u03bb+ 4\u2016f\u2217\u20162F ).\nLemma 11. E [\u2225\u2225 1\nnQ \u22121\u03a6> \u2225\u22252 2 ] \u2264 \u03c3 2 n \u03b3k(\u03bb)\nNote that E[\u2016\u03b4\u2193\u201622] \u2264 P(E)E[\u2016\u03b4\u2193\u201622|E ] + E[1(Ec)\u2016\u03b4\u2193\u201622]. The bound on the first term comes via equation (21) and Lemmas 7, 10 and 11. The second term can be bound via,\nE[1(Ec)\u2016\u03b4\u2193\u201622] \u2264 E[1(Ec)E[\u2016\u2206\u20162F |Xn1 ] \u2264 ( max (\u221a max(q, log t) , max(q, log t)\nn1/2\u22121/q\n) \u00d7 MC\u03c1\n2\u03b3k(\u03bb)\u221a n\n)q ( 2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F\n) (22)\nHere, we have used equation (20) and Lemma 9. Finally, note that\nE[\u2016\u2206\u201622] \u2264M \u2211 j E[\u2016\u2206(j)\u201622] = M ( E\u2016\u03b4\u2193\u201622 + \u2211 j E\u2016\u03b4(j)\u2191 \u2016 2 2 ) \u2264M ( E\u2016\u03b4\u2193\u201622 + (2\u03c32 \u03bb + 4\u2016f\u2217\u20162F )\u2211\nj\n\u00b5 (j) t+1\n) (23)\nWhen we combine (21), (22) and (23) we get the bound in equation (10)."}, {"heading": "Proofs of Technical Lemmas", "text": ""}, {"heading": "A.3.1. PROOF OF LEMMA 10", "text": "Note that following an argument similar to equation (25) in Lemma 8, it is sufficient to bound E\u2016M1/2\u03a6>v\u201622. We expand this as,\nE [ \u2016M1/2\u03a6>v\u201622 ] = M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` E[(\u03a6 (j) ` > v)2] \u2264 M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` E[\u2016\u03a6 (j) ` \u2016 2\u2016v\u20162]\nTo bound this term, first note that\n\u2016v\u20162 = n\u2211 i=1 ( M\u2211 j=1 v (j) i )2 \u2264M n\u2211 i=1 M\u2211 j=1 v (j) i 2 \u2264M n\u2211 i=1 M\u2211 j=1 (\u2211 `>t \u03b4 (j) ` 2 \u00b5 (j) ` )(\u2211 `>t \u00b5 (j) ` \u03c6 (j) ` (Xi) 2 ) Therefore,\nE [ \u2016M1/2\u03a6>v\u20162 ] \u2264 M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` M n\u2211 i=1 M\u2211 j\u2032=1 E [ E[\u2016\u2206(j \u2032)\u20162H k(j \u2032) |Xn1 ]\u2016\u03a6 (j) ` \u2016 2 \u2211 `\u2032>t \u00b5 (j\u2032) `\u2032 \u03c6 (j\u2032) `\u2032 (Xi) 2 ] (24)\n\u2264M ( 2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F ) M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` n\u2211 i=1 M\u2211 j\u2032=1 \u2211 `\u2032>t \u00b5 (j\u2032) `\u2032 E [ \u2016\u03a6(j)` \u2016 2\u03c6 (j\u2032) `\u2032 (Xi) 2 ]\nFor all i, the inner expectation can be bounded using assumption 2 and Jensen\u2019s inequality via,\nE [ \u2016\u03a6(j)` \u2016 2\u03c6 (j\u2032) `\u2032 (Xi) 2 ] \u2264 \u221a E [ \u2016\u03a6(j)` \u20164 ] E [ \u03c6 (j\u2032) `\u2032 (Xi) 4 ] \u2264 \u03c12 \u221a\u221a\u221a\u221aE[( n\u2211 i=1 \u03c6 (j) ` (Xi) 2 )2]\n\u2264 \u03c12 \u221a\u221a\u221a\u221aE[n n\u2211\ni=1\n\u03c6 (j) ` (Xi) 4\n] \u2264 \u03c12 \u221a n2\u03c14 = n\u03c14.\nThis yields,\nE [ \u2016M1/2\u03a6>v\u20162 ] \u2264Mn2\u03c14 ( 2\u03c32\n\u03bb + 4\u2016f\u2217\u20162F ) M\u2211 j=1 t\u2211 `=1\n\u00b5 (j) `\ufe38 \ufe37\ufe37 \ufe38\n\u2264\u03a8\nM\u2211 j\u2032=1 \u2211 `\u2032>t\n\u00b5 (j\u2032) `\u2032\ufe38 \ufe37\ufe37 \ufe38\n=\u03b2t\nFinally, we have\nE [\u2225\u2225\u2225\u2225 1nQ\u22121\u03a6>v \u2225\u2225\u2225\u22252\n2\n] \u2264 E [ 1\n\u03bb \u2225\u2225\u2225\u2225 1nM1/2\u03a6>v \u2225\u2225\u2225\u22252\n2\n] \u2264 1 \u03bb M\u03c14\u03a8\u03b2t ( 2\u03c32 \u03bb + 4\u2016f\u2217\u20162F ) (25)"}, {"heading": "A.3.2. PROOF OF LEMMA 11", "text": "We expand the LHS as follows to obtain the result.\nE [\u2225\u2225 1 n Q\u22121\u03a6> \u2225\u22252] = 1 n2 M\u2211 j=1 t\u2211 `=1 n\u2211 i=1\n1\n1 + \u03bb/\u00b5 (j) `\nE[\u03c6(j)` (Xi) 2 2i ] \u2264\n\u03c32\nn M\u2211 j=1 \u03b3(j)(\u03bb) = \u03c32 n \u03b3k(\u03bb)\nThe first step is just an expansion of the matrix. In the second step we have used E[\u03c6(j)` (Xi) 2 2i ] = E[\u03c6 (j) ` (Xi) 2 E[ 2i |Xi]] \u2264 \u03c32 since E[\u03c6(j)` (X)2] = 1. In the last two steps we have used the definitions of \u03b3(j)(\u03bb) and \u03b3k(\u03bb)."}, {"heading": "B. Proof of Theorem 4: Rate of Convergence in Different RKHSs", "text": "Our strategy will be to choose \u03bb so as to balance the dependence on n in the first two terms in the RHS of the bound in Theorem 3.\nProof of Theorem 4-1. Polynomial Decay: The quantity \u03b3k(\u03bb) can be bounded via Md \u2211\u221e `=1 1/(1 + \u03bb/\u00b5\u0303`). If we set \u03bb = n \u22122s 2s+d , then\n\u03b3k(\u03bb)\nMd = \u221e\u2211 `=1\n1\n1 + n \u22122s 2s+d /\u00b5\u0303`\n\u2264 n d 2s+d + \u2211\n`>n d 2s+d\n1\n1 + n 2s 2s+d ` 2s d\n\u2264 n d 2s+d + n\u2212 2s 2s+d \u2211 `>n d 2s+d\n1\nn \u22122s 2s+d + ` 2s d\n\u2264 n d 2s+d + n \u22122s 2s+d ( n d 2s+d + \u222b \u221e n d 2s+d u\u22122s/ddu ) \u2208 O(n d 2s+d ).\nTherefore, \u03b3k(\u03bb)/n \u2208 O(Mdn \u22122s 2s+d ) giving the correct dependence on n as required. To show that \u03c7(k) is negligible, set t = n 3d 2s\u2212d . Ignoring the poly(D) terms, both \u00b5\u0303t+1, \u03b2t \u2208 O(n \u22126s 2s\u2212d ) and \u03c7(k) is low order. Therefore, by Thereom 3 the excess risk is in O(M2dn \u22122s 2s+d ).\nProof of Theorem 4-2. Exponential Decay: By setting \u03bb = 1/n and following a similar argument to above we have,\n\u03b3k(\u03bb) Md \u2264 \u221a log n \u03b1 + 1 \u03bb \u2211 `> \u221a logn/\u03b1 \u00b5\u0303` \u2264 \u221a log n \u03b1 + n\u03c0\u0303d \u2211 `> \u221a logn/\u03b1 exp(\u2212\u03b1`2)\n\u2264 \u221a log n\n\u03b1 + n\u03c0\u0303d\n( 1\nn +\n\u222b \u221e \u221a\nlogn/\u03b1\nexp(\u2212\u03b1`2) ) = \u221a log n\n\u03b1 + \u03c0\u0303d\n( 1 + \u221a \u03c0\n2 (1\u2212 \u03a6(\n\u221a log n) ) ,\nwhere \u03a6 is the Gaussian cdf. In the first step we have bounded the first \u221a\nlogn \u03b1 terms by 1 and then bounded the second\nterm by a constant. Note that the last term is o(1). Therefore ignoring log n terms, \u03b3k(\u03bb) \u2208 O(Md\u03c0\u0303d) which gives excess risk O(M2d \u03c0\u0303d/n). \u03c7(k) can be shown to be low order by choosing t = n2 which results in \u00b5\u0303t+1, \u03b2t \u2208 O(n\u22124)."}, {"heading": "C. Proof of Theorem 5: Analysis in the Agnostic Setting", "text": "As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F . We begin by making the following crucial observation about the population minimiser (7) f\u03bb = \u2211M j=1 f (j) \u03bb ,\nf\u03bb = argmin g\u2208Hd,\u03bb\n\u2016g \u2212 f\u2217\u201622. (26)\nTo prove this, consider any g = \u2211M j=1 g\n(j) \u2208 Hd,\u03bb. Using the fact that R(g) = R(f\u2217) + \u2016g \u2212 f\u2217\u201622 for any g and that \u2016g\u2016F \u2264 Rd,\u03bb we obtain the above result as follows.\nE [ (f\u2217(X)\u2212 Y )2 ] + \u2016f\u03bb \u2212 f\u2217\u201622 + \u03bbR2d,\u03bb = E[(f\u03bb(X)\u2212 Y )2] + \u03bbR2d,\u03bb\n\u2264 E[(g(X)\u2212 Y )2] + \u03bb M\u2211 j=1 \u2016g(j)\u20162H k(j) \u2264 E [ (f\u2217(X)\u2212 Y )2 ] + \u2016g \u2212 f\u2217\u201622 + \u03bbR2d,\u03bb.\nBy using the above, we get for all \u03b7 > 0,\nE [ \u2016f\u0302 \u2212 f\u2217\u201622 ] \u2264 (1 + \u03b7)E [ \u2016f\u03bb \u2212 f\u2217\u201622 ] + (1 + 1/\u03b7)E [ \u2016f\u0302 \u2212 f\u03bb\u201622 ] = (1 + \u03b7) inf\ng\u2208Hd,\u03bb \u2016g \u2212 f\u2217\u201622\ufe38 \ufe37\ufe37 \ufe38 AE\n+(1 + 1/\u03b7)E [ \u2016f\u0302 \u2212 f\u03bb\u201622 ]\ufe38 \ufe37\ufe37 \ufe38 EE\nFor the first step, by the AM-GM inequality we have 2 \u222b (f\u0302 \u2212 f\u03bb)(f\u03bb \u2212 f\u2217) \u2264 1/\u03b7 \u222b (f\u0302 \u2212 f\u03bb)2 + \u03b7 \u222b\n(f\u03bb \u2212 f\u2217)2. In the second step we have used (26). The term AE is exactly as in Theorem 5 so we just need to bound EE.\nAs before, we consider the RKHS F . Denote the representation of f\u03bb in F by f\u03bb = (f (1)\u03bb , . . . , f (M) \u03bb ). Note that Rd,\u03bb = \u2016f\u03bb\u2016F . Analogous to the analysis in Appendix A we define \u2206(j) = f\u0302 (j) \u2212 f (j)\u03bb , \u2206 = \u2211 j \u2206\n(j) = f\u0302 \u2212 f\u03bb and \u2206 = (\u2206(1), . . . ,\u2206(M)). Note that EE = E[\u2016\u2206\u201622].\nLet \u2206(j) = \u2211\u221e `=1 \u03b4 (j) ` \u03c6 (j) ` be the expansion of \u2206\n(j) in L2(PX). For t \u2208 N, which we will select later, define \u2206(j)\u2193 =\u2211t `=1 \u03b4 (j) ` \u03c6 (j) ` , \u2206 (j) \u2191 = \u2211 `>t \u03b4 (j) ` \u03c6 (j) ` , \u03b4 (j) \u2193 = (\u03b4 (1), . . . , \u03b4(t)) \u2208 Rt and \u03b4(j)\u2191 = (\u03b4 (j) ` )`>t. Let \u2206\u2193 = \u2211 j \u2206\n(j) \u2193 and \u2206\u2191 =\u2211\nj \u2206 (j) \u2191 . Continuing the analogy, let f (j) \u03bb = \u2211M j=1 \u03b8 (j) ` \u03c6 (j) ` be the expansion of f (j) \u03bb . Let \u03b8 (j) \u2193 = (\u03b8 (j) 1 , . . . , \u03b8 (j) t ) \u2208 Rt\nand \u03b8\u2193 = [\u03b8 (1) \u2193 ; . . . ; \u03b8 (M) \u2193 ] \u2208 RtM . Let v \u2208 Rn such that v (j) i = \u2211 `>t \u03b4 (j) ` \u03c6 (j) ` (Xi) and vi = \u2211 j v (j) i . Let \u2208 Rn,\ni = Yi \u2212 f\u03bb(Xi). Also define the following quantities:\n\u03c22\u03bb(x) = E[(Y \u2212 f\u03bb(X))2|X = x], B4\u03bb = 32\u2016f\u03bb\u20164F + 8E[\u03c24\u03bb(X)]/\u03bb2.\nWe begin with the following lemmas.\nLemma 12. E[\u03c24\u03bb(X)] \u2264 8\u03a82\u2016f\u03bb\u20164F\u03c14 + 8\u03bd4.\nLemma 13. E [( E[\u2016\u2206\u20162F |Xn1 ] )2] \u2264 B4\u03bb. We first bound E[\u2016\u2206(j)\u2191 \u201622] = \u2211 `>t E\u03b4 (j) ` 2 using Lemma 13 and Jensen\u2019s inequality.\nE [ \u2016\u03b4(j)\u2191 \u2016 2 2 ] = \u2211 `>t E[\u03b4(j)` 2 ] \u2264 \u00b5(j)t+1E \u2211 `>t \u03b4 (j) ` 2 \u00b5 (j) `  \u2264 \u00b5(j)t+1E [\u2016\u2206(j)\u20162H k(j) ] \u2264 \u00b5(j)t+1E [ \u2016\u2206\u20162F ] \u2264 \u00b5(j)t+1B2\u03bb (27)\nNext we proceed to bound E[\u2016\u2206\u2193\u201622]. For this we will use \u03a6(j),\u03a6 (j) ` ,M(j),M, Q from Appendix A. The first order optimality condition can be written as,\n1\nn n\u2211 i=1 \u03beXi (\u3008\u03beXi ,\u2206\u3009 \u2212 i) + \u03bbf\u0302 = 0.\nThis has the same form as (13) but the definitions of \u2206 and i have changed. Now, just as in the variance calculation, when we take the F-inner product of the above with (0, . . . , \u03c6(j)` , . . . ,0) and repeat for all j we get,(\nI +Q\u22121 ( 1\nn \u03a6>\u03a6\u2212 I\n) Q\u22121 ) Q\u03b4\u2193 = \u2212\u03bbQ\u22121M\u22121\u03b8\u2193 \u2212 1\nn Q\u22121\u03a6>v +\n1 n Q\u22121\u03a6>\nSince \u03a6,M, Q are the same as before we may reuse Lemma 9. Then, as Q I when the event E holds,\n\u2016\u03b4\u2193\u201622 \u2264 \u2016Q\u03b4\u2193\u201622 \u2264 4\u2016\u03bbQ\u22121M\u22121\u03b8\u2193 + 1\nn Q\u22121\u03a6>v +\n1 n Q\u22121\u03a6> \u201622\n\u2264 8\u2016 1 n Q\u22121\u03a6>v\u20162 + 8\u2016\u03bbQ\u22121M\u22121\u03b8\u2193 \u2212 1 n Q\u22121\u03a6> \u201622 (28)\nWe now bound the two terms in the RHS in expectation via the following lemmas.\nLemma 14. E[\u2016 1nQ \u22121\u03a6>v\u20162] \u2264 1\u03bbMB 2 \u03bb\u03c1 4\u03a8\u03b2t\nLemma 15. E[\u2016\u03bbQ\u22121M\u22121\u03b8\u2193 \u2212 1nQ \u22121\u03a6> \u201622] \u2264 1n\u03c1 2\u03b3k(\u03bb) \u221a E[\u03c24\u03bb(X)]\nNow by Lemma 13 we have, E[\u2016\u03b4\u2193\u201622] = P(E)E[\u2016\u03b4\u2193\u201622|E ] + E[1(Ec)\u2016\u03b4\u2193\u201622] \u2264 E[\u2016\u03b4\u2193\u201622|E ] +B2\u03bbP(Ec). E[\u2016\u03b4\u2193\u201622|E ] can be bounded using Lemmas 14 and 15 while P(Ec) can be bounded using Lemma 9. Combining these results along with (27) we have the following bound for EE = E[\u2016\u2206\u201622],\nE[\u2016\u2206\u201622] \u2264 E [\u2225\u2225\u2225\u2225 M\u2211 j=1 \u2206(j) \u2225\u2225\u2225\u22252 2 ] \u2264M M\u2211 j=1 E [ \u2016\u2206(j)\u201622 ] = M E[\u2016\u03b4\u2193\u201622] + M\u2211 j=1 E[\u2016\u03b4(j)\u2193 \u2016 2 2]  \u2264 8 n M\u03c12\u03b3k(\u03bb) \u221a E[\u03c24\u03bb(X)] + 8 \u03bb M2B2\u03bb\u03c1 4\u03a8\u03b2t +B 2 \u03bbM ( CMdb(n, t, q)\u03c1 2\u03b3k(\u03bb)\u221a n )q +B2\u03bbM \u2211 j \u00b5 (j) t+1\nNow we choose t large enough so that the following are satisfied,\n\u03b2t \u2264 \u03bb\nM2nB4\u03bb ,\nM\u2211 j=1 \u00b5 (j) t+1 \u2264\n1\nMnB4\u03bb ,\n( CMdb(n, t, q)\u03c1\n2\u03b3k(\u03bb)\u221a n )q \u2264 1 MnB4\u03bb .\nThen the last three terms are O(1/nB2\u03bb) and the first term dominates. Using Lemma 12 and recalling that R2d,\u03bb =\u2211 j R (j) \u03bb 2 = \u2016f\u03bb\u20162F we get EE \u2208 O ( n\u22121M\u03b3k(\u03bb)R 2 d,\u03bb ) as given in the theorem."}, {"heading": "Proofs of Technical Lemmas", "text": ""}, {"heading": "C.1. Proof of Lemma 13", "text": "Since f\u0302 is the minimiser of the empirical objective,\nE [ \u03bb\u2016f\u0302\u20162F |Xn1 ] \u2264 E \u03bb M\u2211 j=1 \u2016f\u0302 (j)\u20162H k(j) + 1 n n\u2211 i=1  M\u2211 j=1 f\u0302 (j)(X (j) i )\u2212 Yi 2 \u2223\u2223\u2223\u2223\u2223Xn1 \n\u2264 E \u03bb M\u2211 j=1 \u2016f (j)\u03bb \u2016 2 H k(j) + 1 n n\u2211 i=1  M\u2211 j=1 f (j) \u03bb (X (j) i )\u2212 Yi 2 \u2223\u2223\u2223\u2223\u2223Xn1  \u2264 \u03bb\u2016f\u03bb\u20162F + 1n n\u2211 i=1 \u03c22\u03bb(Xi)\nNoting that \u2206 = f\u0302 \u2212 f\u03bb and using the above bound and Jensen\u2019s inequality yields,\nE[\u2016\u2206\u20162F |Xn1 ] \u2264 2\u2016f\u03bb\u20162F + 2E[\u2016f\u0302\u20162F |Xn1 ] \u2264 4\u2016f\u03bb\u20162F + 2\nn\u03bb n\u2211 i=1 \u03c22\u03bb(Xi)\nApplying Jensen\u2019s inequality once again yields,\nE[(E[\u2206\u20162F |Xn1 ])2] \u2264 E  8 n2\u03bb2 ( n\u2211 i=1 \u03c22\u03bb )2 + 32\u2016f\u03bb\u20164F  \u2264 8 n\u03bb2 n\u2211 i=1 E[\u03c24\u03bb] + 32\u2016f\u03bb\u20164F = B4\u03bb"}, {"heading": "C.2. Proof of Lemma 12", "text": "First, using Jensen\u2019s inequality twice we have E[\u03c24\u03bb(X)] = E [ E[(Y \u2212 f\u03bb(X))2|X]2 ] \u2264 E [ (Y \u2212 f\u03bb(X))4 ] \u2264 8E[f4\u03bb(X)] + 8E[Y 4] (29)\nConsider any f (j)\u03bb ,\nf (j) \u03bb (x) = \u221e\u2211 `=1 \u03b8 (j) ` \u03c6 (j) ` (x) (a) \u2264 ( \u221e\u2211 `=1 \u00b5 (j) ` 1/3 \u03b8 (j) ` 2/3 )3/4 \u221e\u2211 `=1 \u03b8 (j) ` 2 \u03c6 (j) ` (x) 4 \u00b5 (j) ` 1/4\n(b) \u2264  M\u2211 j=1 \u00b5 (j) ` 1/2 M\u2211 j=1 \u03b8 (j) ` 2 \u00b5 (j) ` 1/4 \u221e\u2211 `=1 \u03b8 (j) ` 2 \u03c6 (j) ` (x) 4 \u00b5 (j) ` 1/4 = \u03a8(j)1/2\u2016f (j)\u03bb \u20161/2H k(j)  \u221e\u2211 `=1 \u03b8 (j) ` 2 \u03c6 (j) ` (x) 4 \u00b5 (j) ` 1/4\nIn (a), we used Ho\u0308lder\u2019s inequality on \u00b5(j)` 1/4 \u03b8 (j) ` 1/2 and \u03b8(j)` 1/2 \u03c6 (j) ` (x)/\u00b5 (j) ` 1/4 with conjugates 4/3 and 4 respectively. In (b) we used Ho\u0308lder\u2019s inequality once again on \u00b5(j)` 2/3 and (\u03b8(j)` 2 /\u00b5 (j) ` )\n1/3 with conjugates 3/2 and 3. Now we expand f\u03bb in terms of the f (j) \u03bb \u2019s as follows,\nf\u03bb(x) \u2264 M\u2211 j=1 \u03a8(j) 1/2 \u2016f (j)\u03bb \u2016 1/2 H k(j)  \u221e\u2211 `=1 \u03b8 (j) ` 2 \u03c6 (j) ` (x) 4 \u00b5 (j) ` 1/4 \u2264  M\u2211 j=1 \u03a8(j) 1/2  M\u2211 j=1 \u2016f (j)\u03bb \u2016Hk(j)  \u221e\u2211 `=1 \u03b8 (j) ` 2 \u03c6 (j) ` (x) 4 \u00b5 (j) ` 1/2  1/2\nwhere we have applied Cauchy-Schwarz in the last step. Using Cauchy-Schwarz once again,\nf2\u03bb(X) \u2264 \u03a8  M\u2211 j=1 \u2016f (j)\u03bb \u2016 2 H k(j) 1/2 M\u2211 j=1 \u221e\u2211 `=1 \u03b8 (j) ` 2 \u03c6 (j) ` (X) 4 \u00b5 (j) ` 1/2\nUsing Cauchy-Schwarz for one last time, we obtain\nE[f4\u03bb(x)] \u2264 \u03a82\u2016f\u03bb\u20162F M\u2211 j=1 \u221e\u2211 `=1 \u03b8 (j) ` 2 E[\u03c6(j)` (x)]4 \u00b5 (j) ` \u2264 \u03a82\u2016f\u03bb\u20164F\u03c12\nwhere we have used Assumption 2 in the last step. When we combine this with (29) and use the fact that E[Y 4] \u2264 \u03bd4 we get the statement of the lemma."}, {"heading": "C.3. Proof of Lemma 14", "text": "The first part of the proof will mimic that of Lemma 10. By repeating the arguments for (24), we get\nE [ \u2016M1/2\u03a6>v\u20162 ] \u2264 M\u2211 j=1 t\u2211 `=1 \u00b5 (j) ` M n\u2211 i=1 M\u2211 j\u2032=1 E [ E[\u2016\u2206(j \u2032)\u20162H k(j \u2032) |Xn1 ]\u2016\u03a6 (j) ` \u2016 2 \u2211 `\u2032>t \u00b5 (j\u2032) `\u2032 \u03c6 (j\u2032) `\u2032 (Xi) 2 ]\n\u2264M M\u2211 j=1 t\u2211 `=1 n\u2211 i=1 M\u2211 j\u2032=1 \u2211 `\u2032>t \u00b5 (j) ` \u00b5 (j\u2032) `\u2032 E [ E[\u2016\u2206(j \u2032)\u20162H k(j \u2032) |Xn1 ]\u2016\u03a6 (j) ` \u2016 2\u03c6 (j\u2032) `\u2032 (Xi) 2 ]\nUsing Cauchy-Schwarz the inner expectation can be bounded via \u221a E [( E[\u2016\u2206(j\u2032)\u20162H\nk(j \u2032)\n] )2]E [\u2016\u03a6(j)` \u20164\u03c6(j\u2032)`\u2032 (Xi)4].\nLemma 13 bounds the first expectation by B4\u03bb. To bound the second expectation we use Assumption 2.\nE [ \u2016\u03a6(j)` \u2016 4\u03c6 (j\u2032) `\u2032 (Xk) 4 ] = E [( n\u2211 i=1 \u03c6 (j) ` (Xi) 2 )2 \u03c6 (j) ` (Xk) 4 ] = E \u2211 i,i\u2032 \u03c6 (j) ` (Xi) 2\u03c6 (j) ` (Xi\u2032) 2\u03c6 (j) ` (Xk) 4  \u2264 n2\u03c18\nFinally once again reusing some calculations from Lemma 10,\nE [\u2225\u2225\u2225\u2225 1nQ\u22121\u03a6>v \u2225\u2225\u2225\u22252\n2\n] \u2264 E [ 1\n\u03bb \u2225\u2225\u2225\u2225 1nM1/2\u03a6>v \u2225\u2225\u2225\u22252\n2\n] \u2264 M n2\u03bb ( n\u2211 i=1 n\u03c14 ) \ufe38 \ufe37\ufe37 \ufe38\nn2\u03c14\n M\u2211 j=1 t\u2211 `=1 \u00b5 (j) `  \ufe38 \ufe37\ufe37 \ufe38\n\u03a8\n M\u2211 j\u2032=1 \u2211 `\u2032>t \u00b5 (j\u2032) `\u2032  \ufe38 \ufe37\ufe37 \ufe38\n\u03b2t"}, {"heading": "C.4. Proof of Lemma 15", "text": "First note that we can write the LHS of the lemma as,\nE [\u2225\u2225\u2225\u2225\u03bbQ\u22121M\u22121\u03b8\u2193 \u2212 1nQ\u22121\u03a6> \u2225\u2225\u2225\u22252 ] = M\u2211 j=1 t\u2211 `=1\n1\n1 + \u03bb/\u00b5 (j) `\nE (\u03bb\u03b8(j)` \u00b5\n(j) `\n\u2212 1 n n\u2211 i=1 \u03c6 (j) ` (X (j) i ) i )2 To bound the inner expectation we use the optimality conditions of the population minimiser (7). We have,\n2E ( M\u2211 j=1 f (j) \u03bb (X (j) i )\u2212 Y ) \u03be (j) Xi + 2\u03bbf (j)\u03bb = 0 =\u21d2 E [\u03be(j)Xi i] = \u03bbf (j)\u03bb =\u21d2 E [\u03c6(j)` (X(j)i ) i] = \u03bb \u03b8(j)` \u00b5 (j) ` . (30)\nIn the last step we have taken the F-inner product with (0, . . . , \u03c6(j)` , . . . ,0). Therefore the term inside the expectation is the variance of n\u22121 \u2211 i \u03c6 (j) ` (X (j) i ) i and can be bounded via,\nV\n[ 1\nn n\u2211 i=1 \u03c6 (j) ` (X (j)) i ] \u2264 1 n E [ \u03c6 (j) ` (X (j))2 2i ] \u2264 1 n \u221a E [ \u03c6 (j) ` (X (j))4 ] E [ 4i ] \u2264 1 n \u03c12 \u221a E[\u03c24\u03bb(X)]\nHence the LHS can be bounded via,\n1 n \u03c12 \u221a E[\u03c24\u03bb(X)] M\u2211 j=1 t\u2211 `=1\n1\n1 + \u03bb/\u00b5 (j) `\n= 1\nn \u03c12\u03b3k(\u03bb)\n\u221a E[\u03c24\u03bb(X)]"}, {"heading": "D. Some Details on Experimental Setup", "text": "The function fd used in Figure 1(a) is the log of three Gaussian bumps,\nfd(x) = log ( \u03b11 1\nhdd exp\n( \u2016x\u2212 v1\u20162\n2h2d\n) + \u03b11 1\nhdd exp\n( \u2016x\u2212 v2\u20162\n2h2d\n) + (1\u2212 \u03b11 \u2212 \u03b12) 1\nhdd exp\n( \u2016x\u2212 v3\u20162\n2h2d\n)) (31)\nwhere hd = 0.01 \u221a d, \u03b11, \u03b12 \u2208 [0, 1] and vi \u2208 Rd are constant vectors. For figures 1(b)-1(f) we used fD where D is given in the figures. In all experiments, we used a test set of 2000 points and plot the mean squared test error.\nFor the real datasets, we normalised the training data so that the X, y values have zero mean and unit variance along each dimensions. We split the given dataset roughly equally to form a training set and testing set. We tuned hyper-parameters via 5-fold cross validation on the training set and report the mean squared error on the test set. For some datasets the test prediction error is larger than 1. Such datasets turned out to be quite noisy. In fact, when we used a constant predictor at 0 (i.e. the mean of the training instances) the mean squared error on the test set was typically much larger than 1.\nBelow, we list details on the dataset: the source, the used predictor and features.\n1. Housing: (UCI), Predictor: CRIM Features: All other attributes except CHAS which is a binary feature.\n2. Galaxy: (SDSS data on Luminous Red Galaxies from Tegmark et al (2006)), Predictor: Baryonic Density Features: All other attributes.\n3. fMRI: (From (Just et al., 2010)), Predictor: Noun representation Features: Voxel Intensities. Since the actual dimensionality was very large, we use a random projection to bring it down to 100 dimensions.\n4. Insulin: (From (Tu, 2012)), Predictor: Insulin levels. Features: SNP features\n5. Skillcraft: (UCI), Predictor: TotalMapExplored Features: All other attributes. The usual predictor for this dataset is LeagueIndex but its an ordinal attribute and not suitable for real valued prediction.\n6. School: (From Bristol Multilevel Modelling), Predictor: Given output Features: Given features. We don\u2019t know much about its attributes. We used the given features and labels.\n7. CCPP*: (UCI), Predictor: Hourly energy output EP Features: The other 4 features and 55 random features for the other 55 dimensions.\n8. Blog: (UCI Blog Feedback Dataset), Predictor: Number of comments in 24 hrs Features: The dataset had 280 features. The first 50 features were not used since they were just summary statistics. Our features included features 51-62 given in the UCI website and the word counts of 38 of the most frequently occurring words.\n9. Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection. We got this dataset from a private source and don\u2019t know much about its attributes. We used the given features and labels.\n10. Speech: (Parkinson Speech dataset from UCI), Predictor: Median Pitch Features: All other attributes except the mean pitch, standard deviation, minimum pitch and maximum pitches which are not actual features but statistics of the pitch.\n11. Music: (UCI), Predictor: Year of production Features: All other attributes: 12 timbre average and 78 timbre covariance\n12. Telemonit: (Parkinson\u2019s Telemonitoring dataset from UCI), Predictor: total-UPDRS Features: All other features except subject-id and motor-UPDRS (since it was too correlated with total-UPDRS). We only consider the female subjects in the dataset.\n13. Propulsion: (Naval Propulsion Plant dataset from UCI), Predictor: Lever Position Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified.\n14. Airfoil*: (Airfoil Self-Noise dataset from UCI), Predictor: Sound Pressure Level Features: The other 5 features and 35 random features.\n15. Forestfires: (UCI), Predictor: DC Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified.\n16. Brain: (From Wehbe et al. (2014)), Predictor: Story feature at a given time step Features: Other attributes\nSome experimental details: GP is the Bayesian interpretation of KRR. However, the results are different in Table 1. We believe this is due to differences in hyper-parameter tuning. For GP, the GPML package (Rasmussen & Williams, 2006) optimises the GP marginal likelihood via L-BFGS. In contrast, our KRR implementation minimises the least squares cross validation error via grid search. Some Add-GP results are missing since it was very slow compared to other methods. On the Blog dataset, SALSA took less than 35s to train and all other methods were completed in under 22 minutes. In contrast Add-GP was not done training even after several hours. Even on the relatively small speech dataset Add-GP took about 80 minutes. Among the others, BF, MARS, and SpAM were the more expensive methods requiring several minutes on datasets with large D and n whereas other methods took under 2-3 minutes. We also experimented with locally cubic and quartic interpolation but exclude them from the table since LL, LQ generally performed better. Appendix D has more details on the synthetic functions and test sets."}], "references": [{"title": "Consistency of the Group Lasso and Multiple Kernel Learning", "author": ["Bach", "Francis R"], "venue": null, "citeRegEx": "Bach and R.,? \\Q2008\\E", "shortCiteRegEx": "Bach and R.", "year": 2008}, {"title": "Reproducing kernel Hilbert spaces in Probability and Statistics", "author": ["Berlinet", "Alain", "Thomas-Agnan", "Christine"], "venue": "Kluwer Academic,", "citeRegEx": "Berlinet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Berlinet et al\\.", "year": 2004}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "author": ["Dai", "Bo", "Xie", "He", "Niao", "Liang", "Yingyu", "Raj", "Anant", "Balcan", "Maria-Florina F", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2014}, {"title": "Additive Gaussian Processes", "author": ["Duvenaud", "David K", "Nickisch", "Hannes", "Rasmussen", "Carl Edward"], "venue": "In NIPS,", "citeRegEx": "Duvenaud et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2011}, {"title": "Least Angle Regression", "author": ["Efron", "Bradley", "Hastie", "Trevor", "Johnstone", "Iain", "Tibshirani", "Robert"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Multivariate Adaptive Regression Splines", "author": ["Friedman", "Jerome H"], "venue": "Ann. Statist.,", "citeRegEx": "Friedman and H.,? \\Q1991\\E", "shortCiteRegEx": "Friedman and H.", "year": 1991}, {"title": "Greedy Function Approximation: A Gradient Boosting Machine", "author": ["Friedman", "Jerome H"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman and H.,? \\Q2000\\E", "shortCiteRegEx": "Friedman and H.", "year": 2000}, {"title": "Multiple Kernel Learning Algorithms", "author": ["G\u00f6nen", "Mehmet", "Alpaydin", "Ethem"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "G\u00f6nen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "G\u00f6nen et al\\.", "year": 2011}, {"title": "Utility of Empirical Models of Hemorrhage in Detecting and Quantifying Bleeding", "author": ["M Guillame-Bert", "A Dubrawski", "L Chen", "A Holder", "MR Pinsky", "G. Clermont"], "venue": "In Intensive Care Medicine,", "citeRegEx": "Guillame.Bert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guillame.Bert et al\\.", "year": 2014}, {"title": "A Distribution Free Theory of Nonparametric Regression", "author": ["Gy\u00f6rfi", "L\u00e1szl\u00f3", "Kohler", "Micael", "Krzyzak", "Adam", "Walk", "Harro"], "venue": "Springer Series in Statistics,", "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2002}, {"title": "Computationally efficient regression on a dependency graph for human pose estimation", "author": ["Hara", "Kentaro", "Chellappa", "Rama"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Hara et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hara et al\\.", "year": 2013}, {"title": "Generalized Additive Models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "Open source regression software for Matlab/Octave", "author": ["Jakabsons", "Gints"], "venue": null, "citeRegEx": "Jakabsons and Gints.,? \\Q2015\\E", "shortCiteRegEx": "Jakabsons and Gints.", "year": 2015}, {"title": "A neurosemantic theory of concrete noun representation based on the underlying brain", "author": ["Just", "Marcel Adam", "Cherkassky", "Vladimir L", "S Aryal", "Mitchell", "Tom M", "Aryal", "Esh"], "venue": "codes. PLoS ONE,", "citeRegEx": "Just et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Just et al\\.", "year": 2010}, {"title": "Rodeo: Sparse Nonparametric Regression in High Dimensions", "author": ["Lafferty", "John D", "Wasserman", "Larry A"], "venue": "In NIPS,", "citeRegEx": "Lafferty et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2005}, {"title": "Fastfood Approximating Kernel Expansions in Loglinear Time", "author": ["Le", "Quoc", "Sarlos", "Tamas", "Smola", "Alex"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Component selection and smoothing in smoothing spline analysis of variance models", "author": ["Lin", "Yi", "Zhang", "Hao Helen"], "venue": "Annals of Statistics,", "citeRegEx": "Lin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2006}, {"title": "Accurate Intelligible Models with Pairwise Interactions", "author": ["Lou", "Yin", "Caruana", "Rich", "Gehrke", "Johannes", "Hooker", "Giles"], "venue": "In KDD,", "citeRegEx": "Lou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lou et al\\.", "year": 2013}, {"title": "Symmetric functions and Hall polynomials", "author": ["Macdonald", "Ian Grant"], "venue": null, "citeRegEx": "Macdonald and Grant.,? \\Q1995\\E", "shortCiteRegEx": "Macdonald and Grant.", "year": 1995}, {"title": "PCA-correlated SNPs for Structure Identification", "author": ["P. Paschou"], "venue": "PLoS Genetics,", "citeRegEx": "Paschou,? \\Q2007\\E", "shortCiteRegEx": "Paschou", "year": 2007}, {"title": "Accuracy versus Interpretability in flexible modeling:implementing a tradeoff using Gaussian process models", "author": ["Plate", "Tony A"], "venue": "Behaviourmetrika, Interpreting Neural Network Models\u201d,", "citeRegEx": "Plate and A.,? \\Q1999\\E", "shortCiteRegEx": "Plate and A.", "year": 1999}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Rahimi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2009}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Sparse Additive Models", "author": ["Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Ravikumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2009}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Kernel Methods for Pattern Analysis", "author": ["Shawe-Taylor", "John", "Cristianini", "Nello"], "venue": "Cambridge university press,", "citeRegEx": "Shawe.Taylor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 2004}, {"title": "Optimal Rates for Regularized Least Squares Regression", "author": ["Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint"], "venue": "In COLT,", "citeRegEx": "Steinwart et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2009}, {"title": "Cosmological Constraints from the SDSS Luminous Red Galaxies", "author": ["M. Tegmark et al"], "venue": "Physical Review,", "citeRegEx": "al,? \\Q2006\\E", "shortCiteRegEx": "al", "year": 2006}, {"title": "Regression Shrinkage and Selection Via the Lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani and Robert.,? \\Q1994\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1994}, {"title": "Introduction to Nonparametric Estimation", "author": ["Tsybakov", "Alexandre B"], "venue": null, "citeRegEx": "Tsybakov and B.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov and B.", "year": 2008}, {"title": "Integrative Analysis of a cross-locci regulation Network identifies App as a Gene regulating Insulin Secretion from Pancreatic Islets", "author": ["Tu", "Zhidong"], "venue": "PLoS Genetics,", "citeRegEx": "Tu and Zhidong.,? \\Q2012\\E", "shortCiteRegEx": "Tu and Zhidong.", "year": 2012}, {"title": "Inducing Model Trees for Continuous Classes", "author": ["Wang", "Yong", "Witten", "Ian H"], "venue": "In ECML,", "citeRegEx": "Wang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wang et al\\.", "year": 1997}, {"title": "Simultaneously uncovering the patterns of brain regions involved in different story reading", "author": ["L. Wehbe", "B. Murphy", "P. Talukdar", "A. Fyshe", "A. Ramdas", "T. Mitchell"], "venue": "PLoS ONE,", "citeRegEx": "Wehbe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wehbe et al\\.", "year": 2014}, {"title": "Generalization Performance of Regularization Networks and Support Vector Machines via Entropy Numbers of Compact Operators", "author": ["Williamson", "Robert C", "Smola", "Alex J", "Sch\u00f6lkopf", "Bernhard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Williamson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2001}, {"title": "Simple and Efficient Multiple Kernel Learning by Group Lasso", "author": ["Xu", "Zenglin", "Jin", "Rong", "Yang", "Haiqin", "King", "Irwin", "Lyu", "Michael R"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Learning Bounds for Kernel Regression Using Effective Data Dimensionality", "author": ["Zhang", "Tong"], "venue": "Neural Computation,", "citeRegEx": "Zhang and Tong.,? \\Q2005\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2005}, {"title": "Divide and Conquer Kernel Ridge Regression", "author": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"], "venue": "In COLT,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Shrunk Additive Least Squares Approximation Appendix A. Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "but we repeat them (or provide an outline) here for the sake of completeness", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "Shrunk Additive Least Squares Approximation A.3", "author": ["Zhang"], "venue": "Variance (Proof of Bound", "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "\u03c7(k) can be shown to be low order by choosing t = n which results in \u03bc\u0303t+1, \u03b2t \u2208 O(n\u22124). C. Proof of Theorem 5: Analysis in the Agnostic Setting As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F . We begin by making the following crucial observation about the population", "author": ["d \u03c0\u0303/n"], "venue": null, "citeRegEx": "\u03c0\u0303.n..,? \\Q2013\\E", "shortCiteRegEx": "\u03c0\u0303.n..", "year": 2013}, {"title": "R are constant vectors. For figures 1(b)-1(f) we used fD where D is given in the figures. In all experiments, we used a test set of 2000 points and plot the mean squared test error", "author": ["\u221a d"], "venue": null, "citeRegEx": "d,? \\Q2000\\E", "shortCiteRegEx": "d", "year": 2000}, {"title": "Predictor: Story feature at a given time step Features: Other attributes", "author": ["Brain: (From Wehbe"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Current lower bounds (Gy\u00f6rfi et al., 2002) suggest that this dependence is unavoidable.", "startOffset": 21, "endOffset": 42}, {"referenceID": 25, "context": "In this light, a common simplification has been to assume that f\u2217 decomposes into the additive form f\u2217(x) = f (1) \u2217 (x1)+f (2) \u2217 (x2)+\u00b7 \u00b7 \u00b7+f (D) \u2217 (xD) (Hastie & Tibshirani, 1990; Lafferty & Wasserman, 2005; Ravikumar et al., 2009).", "startOffset": 153, "endOffset": 232}, {"referenceID": 25, "context": "Some variants such as RODEO (Lafferty & Wasserman, 2005) and SpAM (Ravikumar et al., 2009) study first order models in variable selection/sparsity settings.", "startOffset": 66, "endOffset": 90}, {"referenceID": 36, "context": "A large line of work, in what has to come to be known as Multiple Kernel Learning (MKL), focuses on precisely this problem (Bach, 2008; G\u00f6nen & Alpaydin, 2011; Xu et al., 2010).", "startOffset": 123, "endOffset": 176}, {"referenceID": 4, "context": "Additive models have also been studied in Gaussian process literature via additive kernels (Duvenaud et al., 2011; Plate, 1999).", "startOffset": 91, "endOffset": 127}, {"referenceID": 17, "context": "Lou et al. (2013) model f\u2217 as a first order model plus a sparse collection of pairwise interactions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "It is well known thatR is minimised by the regression function f\u2217(\u00b7) = EXY [Y |X = \u00b7] and the excess risk for any f is R(f)\u2212R(f\u2217) = \u2016f \u2212 f\u2217\u20162 (Gy\u00f6rfi et al., 2002).", "startOffset": 142, "endOffset": 163}, {"referenceID": 28, "context": "KRR has been analysed extensively under different assumptions on f\u2217; see (Steinwart & Christmann, 2008; Steinwart et al., 2009; Zhang, 2005) and references therein.", "startOffset": 73, "endOffset": 140}, {"referenceID": 4, "context": "We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels.", "startOffset": 169, "endOffset": 192}, {"referenceID": 4, "context": "We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels.", "startOffset": 169, "endOffset": 227}, {"referenceID": 43, "context": "At first, this appears troublesome since it requres optimising over nMd parameters (\u03b1 i ), j = 1, . . . ,Md, i = 1, . . . , n. However, from the work of Aronszajn (1950), we know that the solution of (3) lies in the RKHS of the sum kernel k", "startOffset": 70, "endOffset": 170}, {"referenceID": 28, "context": "The ESP Kernel While the above formulation reduces the number of optimisation parameters, the kernel still has a combinatorial number of terms which can be expensive to compute. While this is true for arbitrary choices for k\u2019s, under some restrictions we can efficiently compute k. For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al.", "startOffset": 124, "endOffset": 356}, {"referenceID": 4, "context": "For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al. (2011). First consider a set of base kernels acting on each dimension k1, k2, .", "startOffset": 78, "endOffset": 101}, {"referenceID": 38, "context": "Similar assumptions are made in (Zhang et al., 2013) and are satisfied for a large range of kernels including those in Theorem 4.", "startOffset": 32, "endOffset": 52}, {"referenceID": 38, "context": "The first term is known as the effective data dimensionality of k (Zhang, 2005; Zhang et al., 2013) and captures the statistical difficulty of estimating a function inHk(j) .", "startOffset": 66, "endOffset": 99}, {"referenceID": 29, "context": "Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case.", "startOffset": 25, "endOffset": 68}, {"referenceID": 29, "context": "Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case. We use ideas from Aronszajn (1950) to handle sum RKHSs.", "startOffset": 25, "endOffset": 133}, {"referenceID": 35, "context": "An example of the second eigendecay is the Gaussian kernel with \u03c0\u0303 = \u221a 2\u03c0 (Williamson et al., 2001).", "startOffset": 74, "endOffset": 99}, {"referenceID": 10, "context": "Current lower bounds suggest that the exponential dependence is unavoidable (Gy\u00f6rfi et al., 2002; Tsybakov, 2008).", "startOffset": 76, "endOffset": 113}, {"referenceID": 28, "context": "The proof, given in Appendix C, also follows the template in Zhang et al. (2013). Loosely, we may interpret AE and EE as the approximation and estimation errors1.", "startOffset": 32, "endOffset": 81}, {"referenceID": 25, "context": "The choice of bandwidth was inspired by several other kernel methods which use bandwidths on the order \u03c3in (Ravikumar et al., 2009; Tsybakov, 2008).", "startOffset": 107, "endOffset": 147}, {"referenceID": 3, "context": "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.", "startOffset": 0, "endOffset": 83}, {"referenceID": 16, "context": "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.", "startOffset": 0, "endOffset": 83}, {"referenceID": 38, "context": "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.", "startOffset": 0, "endOffset": 83}, {"referenceID": 25, "context": "Nonparametric additive models: Back-fitting with cubic splines (BF) (Hastie & Tibshirani, 1990), Multivariate Adaptive Regression Splines (MARS) (Friedman, 1991), Component Selection and Smoothing (COSSO) (Lin & Zhang, 2006), Sparse Additive Models (SpAM) (Ravikumar et al., 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al.", "startOffset": 256, "endOffset": 280}, {"referenceID": 4, "context": ", 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al., 2011).", "startOffset": 48, "endOffset": 71}, {"referenceID": 5, "context": "Parametric models: Ridge Regression (RR), Least Absolute Shrinkage and Selection (LASSO) (Tibshirani, 1994) and Least Angle Regression (LAR) (Efron et al., 2004).", "startOffset": 141, "endOffset": 161}, {"referenceID": 9, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 14, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 20, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 34, "context": "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).", "startOffset": 104, "endOffset": 217}, {"referenceID": 29, "context": "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950).", "startOffset": 47, "endOffset": 130}, {"referenceID": 29, "context": "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation.", "startOffset": 47, "endOffset": 193}, {"referenceID": 29, "context": "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation. Some intermediate technical results can be obtained directly from Zhang et al. (2013) but we repeat them (or provide an outline) here for the sake of completeness.", "startOffset": 47, "endOffset": 332}, {"referenceID": 43, "context": "The key observation is that we only need to consider n (and not nMd) parameters even though we are optimising overMd RKHSs. The reasoning uses a powerful result from Aronszajn (1950). Consider the class of functionsH\u2032 = {f = \u2211 j f ; f (j) \u2208 Hk(j)}.", "startOffset": 39, "endOffset": 183}, {"referenceID": 29, "context": "We will need the following technical lemmas. The proofs are given at the end of this section. These results correspond to Lemma 5 in Zhang et al. (2013).", "startOffset": 34, "endOffset": 153}, {"referenceID": 38, "context": "The proof mimics Lemma 6 in (Zhang et al., 2013) by performing essentially the same steps over F instead of the usual Hilbert space.", "startOffset": 28, "endOffset": 48}, {"referenceID": 29, "context": "Variance (Proof of Bound (10)) Once again, we follow Zhang et al. (2013). The tricks we use to generalise it to the additive case (i.", "startOffset": 62, "endOffset": 73}, {"referenceID": 29, "context": "Proof of Theorem 5: Analysis in the Agnostic Setting As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F .", "startOffset": 22, "endOffset": 114}, {"referenceID": 29, "context": "In all experiments, we used a test set of 2000 points and plot the mean squared test error. For the real datasets, we normalised the training data so that the X, y values have zero mean and unit variance along each dimensions. We split the given dataset roughly equally to form a training set and testing set. We tuned hyper-parameters via 5-fold cross validation on the training set and report the mean squared error on the test set. For some datasets the test prediction error is larger than 1. Such datasets turned out to be quite noisy. In fact, when we used a constant predictor at 0 (i.e. the mean of the training instances) the mean squared error on the test set was typically much larger than 1. Below, we list details on the dataset: the source, the used predictor and features. 1. Housing: (UCI), Predictor: CRIM Features: All other attributes except CHAS which is a binary feature. 2. Galaxy: (SDSS data on Luminous Red Galaxies from Tegmark et al (2006)), Predictor: Baryonic Density Features: All other attributes.", "startOffset": 3, "endOffset": 966}, {"referenceID": 14, "context": "fMRI: (From (Just et al., 2010)), Predictor: Noun representation Features: Voxel Intensities.", "startOffset": 12, "endOffset": 31}, {"referenceID": 9, "context": "Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection.", "startOffset": 16, "endOffset": 44}, {"referenceID": 9, "context": "Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection. We got this dataset from a private source and don\u2019t know much about its attributes. We used the given features and labels. 10. Speech: (Parkinson Speech dataset from UCI), Predictor: Median Pitch Features: All other attributes except the mean pitch, standard deviation, minimum pitch and maximum pitches which are not actual features but statistics of the pitch. 11. Music: (UCI), Predictor: Year of production Features: All other attributes: 12 timbre average and 78 timbre covariance 12. Telemonit: (Parkinson\u2019s Telemonitoring dataset from UCI), Predictor: total-UPDRS Features: All other features except subject-id and motor-UPDRS (since it was too correlated with total-UPDRS). We only consider the female subjects in the dataset. 13. Propulsion: (Naval Propulsion Plant dataset from UCI), Predictor: Lever Position Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified. 14. Airfoil*: (Airfoil Self-Noise dataset from UCI), Predictor: Sound Pressure Level Features: The other 5 features and 35 random features. 15. Forestfires: (UCI), Predictor: DC Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified. 16. Brain: (From Wehbe et al. (2014)), Predictor: Story feature at a given time step Features: Other attributes Some experimental details: GP is the Bayesian interpretation of KRR.", "startOffset": 17, "endOffset": 1422}], "year": 2016, "abstractText": "High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of first order, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on 15 real datasets, we show that our method is competitive against 21 other alternatives.", "creator": "TeX"}}}