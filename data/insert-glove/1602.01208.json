{"id": "1602.01208", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "abstract": "mcgrail In 102.25 this paper, we trevor-roper propose a kjemperud novel unsupervised univ. learning saucing method honed for billiton the jarry lexical acquisition of then-secretary words related to places visited by robots, doot from human chinnock continuous speech aucilla signals. bongao We richview address the peds problem torrevieja of learning sansar novel baie-james words by latvians a robot that unassimilated has no adere prior creach knowledge of these humeri words except for a primitive tanjavur acoustic display model. 10-inning Further, mahabharatha we cern propose ssis a golden-winged method that allows zanobi a robot paulitz to effectively dinh use unferth the lati learned words and kunjali their meanings for castagnola self - whoops localization hopsin tasks. bombarde The 26-13 proposed pekearo method is nonparametric Bayesian spatial line-of-battle concept acquisition rito method (SpCoA) 145.00 that telomeres integrates the cancer generative larijani model for self - mezzosoprano localization and the unarmored unsupervised wheezes word bosu segmentation swimmable in uttered dikili sentences wurzburg via 4nb latent garage variables fasken related 94.92 to the spatial faster concept. xis We day-day implemented belleville the giray proposed manliness method anarchy SpCoA 55.69 on SIGVerse, businessday which is a cfra simulation 1995-2006 environment, glassen and TurtleBot2, which is a t\u00e9miscamingue mobile kothamangalam robot in uphams a real environment. interossei Further, lta we dakh conducted carmarthenshire experiments for igbe evaluating 1984-1987 the rdeineh performance fulgor of SpCoA. demean The demagogic experimental results showed that SpCoA enabled lintong the robot to g6 acquire porchetta the 4,840 names fallers of beings places from legislatures speech sentences. They also revealed that the 2,615 robot ballinamallard could jointing effectively utilize the acquired spatial fazliu concepts and reduce pairc the uncertainty rambaudi in 19,980 self - asamoah localization.", "histories": [["v1", "Wed, 3 Feb 2016 06:56:51 GMT  (2922kb,D)", "https://arxiv.org/abs/1602.01208v1", "Draft submitted to IEEE Transactions on Autonomous Mental Development (TAMD)"], ["v2", "Wed, 16 Mar 2016 12:17:46 GMT  (1447kb,D)", "http://arxiv.org/abs/1602.01208v2", "Draft submitted to IEEE Transactions on Autonomous Mental Development (TAMD)"], ["v3", "Sat, 7 May 2016 11:59:51 GMT  (1453kb,D)", "http://arxiv.org/abs/1602.01208v3", "This paper was accepted in the IEEE Transactions on Cognitive and Developmental Systems. (04-May-2016)"]], "COMMENTS": "Draft submitted to IEEE Transactions on Autonomous Mental Development (TAMD)", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.RO", "authors": ["akira taniguchi", "tadahiro taniguchi", "tetsunari inamura"], "accepted": false, "id": "1602.01208"}, "pdf": {"name": "1602.01208.pdf", "metadata": {"source": "CRF", "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "authors": ["Akira Taniguchi", "Tadahiro Taniguchi", "Tetsunari Inamura"], "emails": ["a.taniguchi@em.ci.ritsumei.ac.jp;", "taniguchi@em.ci.ritsumei.ac.jp).", "inamura@nii.ac.jp)."], "sections": [{"heading": null, "text": "Index Terms\u2014Learning place names, lexical acquisition, selflocalization, spatial concept\nI. INTRODUCTION\nAUTONOMOUS robots, such as service robots, operatingin the human living environment with humans have to be able to perform various tasks and language communication. To this end, robots are required to acquire novel concepts and vocabulary on the basis of the information obtained from their sensors, e.g., laser sensors, microphones, and cameras, and recognize a variety of objects, places, and situations in an ambient environment. Above all, we consider it important for the robot to learn the names that humans associate with places in the environment and the spatial areas corresponding to these names; i.e., the robot has to be able to understand words related to places. Therefore, it is important to deal with considerable uncertainty, such as the robot\u2019s movement errors, sensor noise, and speech recognition errors.\nSeveral studies on language acquisition by robots have assumed that robots have no prior lexical knowledge. These studies differ from speech recognition studies based on a large vocabulary and natural language processing studies based on lexical, syntactic, and semantic knowledge [1], [2]. Studies on\nAkira Taniguchi and Tadahiro Taniguchi are with Ritsumeikan University, 1-1-1 Noji Higashi, Kusatsu, Shiga 525-8577, Japan (email:a.taniguchi@em.ci.ritsumei.ac.jp; taniguchi@em.ci.ritsumei.ac.jp).\nTetsunari Inamura is with National Institute of Informatics/The Graduate University for Advanced Studies, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101- 8430, Japan (e-mail:inamura@nii.ac.jp).\nlanguage acquisition by robots also constitute a constructive approach to the human developmental process and the emergence of symbols.\nThe objectives of this study were to build a robot that learns words related to places and efficiently utilizes this learned vocabulary in self-localization. Lexical acquisition related to places is expected to enable a robot to improve its spatial\nar X\niv :1\n60 2.\n01 20\n8v 3\n[ cs\n.A I]\n7 M\nay 2\n01 6\n2 cognition. A schematic representation depicting the target task of this study is shown in Fig. 1. This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes. The robot then performs self-localization while moving around in the environment, as shown in Fig. 1 (a). An utterer speaks a sentence including the name of the place to the robot, as shown in Fig. 1 (b). For the purposes of this study, we need to consider the problems of self-localization and lexical acquisition simultaneously.\nWhen a robot learns novel words from utterances, it is difficult to determine segmentation boundaries and the identity of different phoneme sequences from the speech recognition results, which can lead to errors. First, let us consider the case of the lexical acquisition of an isolated word. For example, if a robot obtains the speech recognition results \u201caporu\u201d, \u201cepou\u201d, and \u201caqpuru\u201d (incorrect phoneme recognition of apple), it is difficult for the robot to determine whether they denote the same referent without prior knowledge. Second, let us consider a case of the lexical acquisition of the utterance of a sentence. For example, a robot obtains a speech recognition result, such as \u201cthisizanaporu.\u201d The robot has to necessarily segment a sentence into individual words, e.g., \u201cthis\u201d, \u201ciz\u201d, \u201can\u201d, and \u201caporu\u201d. In addition, it is necessary for the robot to recognize words referring to the same referent, e.g., the fruit apple, from among the many segmented results that contain errors. In case of Fig. 1 (c), there is some possibility of learning names including phoneme errors, e.g., \u201cafroqtabutibe,\u201d because the robot does not have any lexical knowledge.\nOn the other hand, when a robot performs online probabilistic self-localization, we assume that the robot uses sensor data and control data, e.g., values obtained using a range sensor and odometry. If the position of the robot on the global map is unclear, the difficulties associated with the identification of the self-position by only using local sensor information become problematic. In the case of global localization using local information, e.g., a range sensor, the problem that the hypothesis of self-position is present in multiple remote locations, frequently occurs, as shown in Fig. 1 (d).\nIn order to solve the abovementioned problems, in this study, we adopted the following approach. An utterance is recognized as not a single phoneme sequence but a set of candidates of multiple phonemes. We attempt to suppress the variability in the speech recognition results by performing word discovery taking into account the multiple candidates of speech recognition. In addition, the names of places are learned by associating with words and positions. The lexical acquisition is complemented by using certain particular spatial information; i.e., this information is obtained by hearing utterances including the same word in the same place many times. Furthermore, in this study, we attempt to address the problem of the uncertainty of self-localization by improving the selfposition errors by using a recognized utterance including the name of the current place and the acquired spatial concepts, as shown in Fig. 1 (e).\nIn this paper, we propose nonparametric Bayesian spatial concept acquisition method (SpCoA) on basis of unsupervised word segmentation and a nonparametric Bayesian generative model that integrates self-localization and a clustering in both\nwords and places. The main contributions of this paper are as follows: \u2022 We have proposed a learning method for spatial concepts\nthat can perform the lexical acquisition related to places, i.e., the names of places, from a continuous speech signal in an unsupervised manner. \u2022 We have achieved relatively accurate lexical acquisition that reduced the variability and errors in phonemes by performing word discovery using the multiple candidates of the speech recognition results, i.e., by using a lattice format. \u2022 In addition to the general self-localization method of mobile robots, we showed that self-localization by the proposed method can reduce the uncertainty of selfposition by utilizing the learned spatial concepts and an uttered sentence about the current position.\nThe remainder of this paper is organized as follows: In Section II, previous studies on language acquisition and lexical acquisition relevant to our study are described. In Section III, the proposed method SpCoA is presented. In Sections IV and V, we discuss the effectiveness of SpCoA in the simulation and in the real environment. Section VI concludes this paper."}, {"heading": "II. RELATED WORKS", "text": ""}, {"heading": "A. Lexical acquisition", "text": "Most studies on lexical acquisition typically focus on lexicons about objects [1], [3]\u2013[11]. Many of these studies have not be able to address the lexical acquisition of words other than those related to objects, e.g., words about places.\nRoy et al. proposed a computational model that enables a robot to learn the names of objects from an object image and spontaneous infant-directed speech [1]. Their results showed that the model performed speech segmentation, word discovery, and visual categorization. Iwahashi et al. reported that a robot properly understands the situation and acquires the relationship of object behaviors and sentences [3]\u2013[5]. Qu & Chai focused on the conjunction between speech and eye gaze and the use of domain knowledge in lexical acquisition [7], [8]. They proposed an unsupervised learning method that automatically acquires novel words for an interactive system. Qu & Chai\u2019s method based on the IBM translation model [12] estimates the word-entity association probability.\nNakamura et al. proposed a method to learn object concepts and word meanings from multimodal information and verbal information [10]. The method proposed in [10] is a categorization method based on multimodal latent Dirichlet allocation (MLDA) that enables the acquisition of object concepts from multimodal information, such as visual, auditory, and haptic information [13]. Araki et al. addressed the development of a method combining unsupervised word segmentation from uttered sentences by a nested Pitman-Yor language model (NPYLM) [14] and the learning of object concepts by MLDA [11]. However, the disadvantage of using NPYLM was that phoneme sequences with errors did not result in appropriate word segmentation.\nThese studies did not address the lexical acquisition of the space and place that can also tolerate the uncertainty of\n3 phoneme recognition. However, for the introduction of robots into the human living environment, robots need to acquire a lexicon related to not only objects but also places. Our study focuses on the lexical acquisition related to places. Robots can adaptively learn the names of places in various human living environments by using SpCoA. We consider that the acquired names of places can be useful for various tasks, e.g., tasks with a movement of robots by the speech instruction."}, {"heading": "B. Simultaneous learning of places and vocabulary", "text": "The following studies have addressed lexical acquisition related to places. However, these studies could not utilize the learned language knowledge in other estimations such as the self-localization of a robot.\nTaguchi et al. proposed a method for the unsupervised learning of phoneme sequences and relationships between words and objects from various user utterances without any prior linguistic knowledge other than an acoustic model of phonemes [2], [15]. Further, they proposed a method for the simultaneous categorization of self-position coordinates and lexical learning [16]. These experimental results showed that it was possible to learn the name of a place from utterances in some cases and to output words corresponding to places in a location that was not used for learning.\nMilford et al. proposed RatSLAM inspired by the biological knowledge of a pose cell of the hippocampus of rodents [17]. Milford et al. proposed a method that enables a robot to acquire spatial concepts by using RatSLAM [18]. Further, Lingodroids, mobile robots that learn a language through robot-to-robot communication, have been studied [19]\u2013[21]. Here, a robot communicated the name of a place to other robots at various locations. Experimental results showed that two robots acquired the lexicon of places that they had in common. In [21], the researchers showed that it was possible to learn temporal concepts in a manner analogous to the acquisition of spatial concepts. These studies reported that the robots created their own vocabulary. However, these studies did not consider the acquisition of a lexicon by human-torobot speech interactions.\nWelke et al. proposed a method that acquires spatial representation by the integration of the representation of the continuous state space on the sensorimotor level and the discrete symbolic entities used in high-level reasoning [22]. This method estimates the probable spatial domain and word from the given objects by using the spatial lexical knowledge extracted from Google Corpus and the position information of the object. Their study is different from ours because their study did not consider lexicon learning from human speech.\nIn the case of global localization, the hypothesis of selfposition often remains in multiple remote places. In this case, there is some possibility of performing an incorrect estimation and increasing the estimation error. This problem exists during teaching tasks and self-localization after the lexical acquisition. The abovementioned studies could not deal with this problem. In this paper, we have proposed a method that enables a robot to perform more accurate self-localization by reducing the estimation error of the teaching time by using\na smoothing method in the teaching task and by utilizing words acquired through the lexical acquisition. The strengths of this study are that learning of spatial concept and self-localization represented as one generative model and robots are able to utilize acquired lexicon to self-localization autonomously."}, {"heading": "III. SPATIAL CONCEPT ACQUISITION", "text": "We propose nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates a nonparametric morphological analyzer for the lattice [23], i.e., latticelm1, a spatial clustering method, and Monte Carlo localization (MCL) [24]."}, {"heading": "A. Generative model", "text": "In our study, we define a position as a specific coordinate or a local point in the environment, and the position distribution as the spatial area of the environment. Further, we define a spatial concept as the names of places and the position distributions corresponding to these names.\nThe model that was developed for spatial concept acquisition is a probabilistic generative model that integrates a selflocalization with the simultaneous clustering of places and words. Fig. 2 shows the graphical model for spatial concept acquisition. Table I shows each variable of the graphical model. The number of words in a sentence at time t is denoted as Bt. The generative model of the proposed method is defined as equation (1-10).\n\u03c0 \u223c GEM(\u03b3) (1) Ct \u223c Mult(\u03c0) (2) W \u223c Dir(\u03b20) (3) Ot,b \u223c Mult(WCt) (4) \u03c6l \u223c GEM(\u03b1) (5) it \u223c p(it | xt,\u00b5,\u03a3, \u03c6l, Ct) (6) \u03a3 \u223c IW(\u03a3 | V0, \u03bd0) (7) \u00b5 \u223c N (\u00b5 | m0, (\u03a3/\u03ba0)) (8) xt \u223c p(xt | xt\u22121, ut) (9) zt \u223c p(zt | xt) (10)\nThen, the probability distribution for equation (6) can be defined as follows:\np(it | xt,\u00b5,\u03a3, \u03c6l, Ct)\n= N (xt | \u00b5it ,\u03a3it)Mult(it | \u03c6Ct)\u2211 it=j N (xt | \u00b5j ,\u03a3j)Mult(j | \u03c6Ct) . (11)\nThe prior distribution configured by using the stick breaking process (SBP) [25] is denoted as GEM(\u00b7), the multinomial distribution as Mult(\u00b7), the Dirichlet distribution as Dir(\u00b7), the inverse\u2013Wishart distribution as IW(\u00b7), and the multivariate Gaussian (normal) distribution as N (\u00b7). The motion model and the sensor model of self-localization are denoted as p(xt | xt\u22121, ut) and p(zt | xt) in equations (9) and (10), respectively.\n1latticelm is the name of the tool that [23] is implemented and is treated as the name of the method in this study. http://www.phontron.com/latticelm/\n4\nThis model can learn an appropriate number of spatial concepts, depending on the data, by using a nonparametric Bayesian approach. We use the SBP, which is one of the methods based on the Dirichlet process. In particular, this model can consider a theoretically infinite number of spatial concepts L \u2192 \u221e and position distributions K \u2192 \u221e. SBP computations are difficult because they generate an infinite number of parameters. In this study, we approximate a number of parameters by setting sufficiently large values, i.e., a weaklimit approximation [26].\nIt is possible to correlate a name with multiple places, e.g., \u201cstaircase\u201d is in two different places, and a place with multiple names, e.g., \u201ctoilet\u201d and \u201crestroom\u201d refer to the same place. Spatial concepts are represented by a word distribution of the names of the place Wl and several position distributions (\u00b5k, \u03a3k) indicated by a multinomial distribution \u03c6l. In other words, this model is capable of relating the mixture of Gaussian distributions to a multinomial distribution of the names of places. It should be noted that the arrows connecting it to the surrounding nodes of the proposed graphical model differ from those of ordinal Gaussian mixture model (GMM). We assume that words obtained by the robot do not change its position, but that the position of the robot affects the distribution of words. Therefore, the proposed generative process assumes that the index of position distribution it, i.e., the category of the place, is generated from the position of the robot xt. This change can be naturally introduced without any troubles by introducing equation (11)."}, {"heading": "B. Overview of the proposed method SpCoA", "text": "We assume that a robot performs self-localization by using control data and sensor data at all times. The procedure for the learning of spatial concepts is as follows:\n1) An utterer teaches a robot the names of places, as shown in Fig. 1 (b). Every time the robot arrives at a place that was a designated learning target, the utterer says a sentence, including the name of the current place. 2) The robot performs speech recognition from the uttered speech signal data. Thus, the speech recognition system\nThe procedure for self-localization utilizing spatial concepts is as follows:\n1) The words of the learned spatial concepts are registered to the word dictionary of the speech recognition system. 2) When a robot obtains a speech signal, speech recognition is performed. Then, a word sequence as the 1-best speech recognition result is obtained. 3) The robot modifies the self-localization from words obtained by speech recognition and the position likelihood obtained by spatial concepts. The details of selflocalization are provided in III-D.\nThe proposed method can learn words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences [23]. The lattice can represent to a compact the set of more promising hypotheses of a speech recognition result, such as N-best, in a directed graph format. Unsupervised word segmentation using the lattices of syllable recognition is expected to be able to reduce the variability and errors in phonemes as compared to NPYLM [14], i.e., word segmentation using the 1-best speech recognition results.\nThe self-localization method adopts MCL [24], a method that is generally used as the localization of mobile robots for simultaneous localization and mapping (SLAM) [27]. We\n5 assume that a robot generates an environment map by using MCL-based SLAM such as FastSLAM [28], [29] in advance, and then, performs localization by using the generated map. Then, the environment map of both an occupancy grid map and a landmark map is acceptable."}, {"heading": "C. Learning of spatial concept", "text": "Spatial concepts are learned from multiple teaching data, control data, and sensor data. The teaching data are a set of uttered sentences for all teaching times. Segmented words of an uttered sentence are converted into a bag-of-words (BoW) representation as a vector of the occurrence counts of words Ot,B. The set of the teaching times is denoted as To = {t1, t2, . . . , tN}, and the number of teaching data items is denoted as N . The model parameters are denoted as \u0398 = {W,\u00b5,\u03a3, \u03c6l, \u03c0}. The initial values of the model parameters can be set arbitrarily in accordance with a condition. Further, the sampling values of the model parameters from the following joint posterior distribution are obtained by performing Gibbs sampling.\np(x0:T , iTo , CTo ,\u0398 | OTo,B, u1:T , z1:T ,h) (12)\nwhere the hyperparameters of the model are denoted as h = {\u03b1, \u03b3, \u03b20,m0, \u03ba0, V0, \u03bd0}. The algorithm of the learning of spatial concepts is shown in Algorithm 1.\nThe conditional posterior distribution of each element used for performing Gibbs sampling can be expressed as follows: An index it of the position distribution is sampled for each data t \u2208 To from a posterior distribution as follows:\nit \u223c p(it = k | xt,\u00b5,\u03a3, \u03c6l, Ct) \u221d N (xt | \u00b5k=it ,\u03a3k=it)Mult(it = k | \u03c6l=Ct). (13)\nAn index Ct of the spatial concepts is sampled for each data item t \u2208 To from a posterior distribution as follows:\nCt \u223c p(Ct = l | xt, it, Ot,B,\u00b5,\u03a3, \u03c6l, \u03c0) \u221d Mult(Ot,B |Wl=Ct)Mult(it = k | \u03c6l=Ct)\nMult(Ct = l | \u03c0) (14)\nwhere Ot,B denotes a vector of the occurrence counts of words in the sentence at time t. A posterior distribution representing word probabilities of the name of place W is calculated as follows:\np(W | CTo , OTo,B) \u221d \u220f l\u2208L [\u220f l=Ct t\u2208To p(Ot,B |Wl=Ct) ] p(Wl) (15)\nwhere variables with the subscript To denote the set of all teaching times. A word probability of the name of place Wl is sampled for each l \u2208 L as follows:\nWl \u223c Mult(Ol |Wl)Dir(Wl | \u03b20) \u221d Dir(Wl | \u03b2nl) (16)\nwhere \u03b2nl represents the posterior parameter and Ol denotes the BoW representation of all sentences of Ct = l in t \u2208 To.\nA posterior distribution representing the position distribution \u00b5,\u03a3 is calculated as follows:\np(\u00b5,\u03a3 | iTo , xTo , CTo , \u03c6l) \u221d \u220f k\u2208K [\u220f k=it t\u2208To p(xt | \u00b5k=it ,\u03a3k=it) ] p(\u00b5k,\u03a3k). (17)\nA position distribution \u00b5k, \u03a3k is sampled for each k \u2208 K as follows:\n\u00b5k,\u03a3k \u223c N (xk | \u00b5k,\u03a3k)NIW(\u00b5k,\u03a3k | m0, \u03ba0, V0, \u03bd0) \u221d NIW(\u00b5k,\u03a3k | mnk , \u03bank , Vnk , \u03bdnk) (18)\nwhere NIW(\u00b7) denotes the Gaussian\u2013inverse\u2013Wishart distribution; mnk , \u03bank , Vnk , and \u03bdnk represent the posterior parameters; and xk indicates the set of the teaching positions of it = k in t \u2208 To. A topic probability distribution \u03c0 of spatial concepts is sampled as follows:\n\u03c0 \u223c Mult(CTo | \u03c0)Dir(\u03c0 | \u03b3) \u221d Dir(\u03c0 | CTo , \u03b3). (19)\nA posterior distribution representing the mixed weights \u03c6l of the position distributions is calculated as follows:\np(\u03c6l | xTo , iTo , CTo ,\u00b5,\u03a3) \u221d \u220f l\u2208L [\u220f l=Ct t\u2208To p(it | \u03c6l=Ct) ] p(\u03c6l). (20)\nA mixed weight \u03c6l of the position distributions is sampled for each l \u2208 L as follows:\n\u03c6l \u223c Mult(il | \u03c6l)Dir(\u03c6l | \u03b1) \u221d Dir(\u03c6l | il, \u03b1) (21)\nwhere il denotes a vector counting all the indices of the Gaussian distribution of Ct = l in t \u2208 To.\nSelf-positions x0:T are sampled by using a Monte Carlo fixed-lag smoother [30] in the learning phase. The smoother can estimate self-position x0:t and not p(x0:t | u1:t, z1:t), i.e., a sequential estimation from the given data u1:t, z1:t until time t, but it can estimate p(x0:t | u1:T , z1:T ), i.e., an estimation from the given data u1:T , z1:T until time T later than t (t < T ). In general, the smoothing method can provide a more accurate estimation than the MCL of online estimation. In contrast, if the self-position of a robot xt is sampled like direct assignment sampling for each time t, the sampling of xt is divided in the case with the teaching time t \u2208 To and another time t /\u2208 To as follows:\nxt \u223c  p(xt | xt\u22121, xt+1, ut, ut+1, zt) \u221d p(xt+1 | xt, ut+1)p(zt | xt)p(xt | xt\u22121, ut) (t /\u2208 To), p(xt | xt\u22121, xt+1, ut, ut+1, zt, it,\u00b5,\u03a3, \u03c6l, Ct) \u221d p(xt+1 | xt, ut+1)p(zt | xt)p(xt | xt\u22121, ut) p(it | xt,\u00b5,\u03a3, \u03c6l, Ct)\n(t \u2208 To). (22)\n6 Algorithm 1 Learning of spatial concepts 1: L = \u2205, To = \u2205 2: // Localization and speech recognition 3: for t = 0 to T do 4: x0:t\u223cMonte Carlo smoother(x0:t\u22121, u1:t, z1:t) [30] 5: if the speech signal is observed then 6: latticet = speech recognition(speech signal) 7: add latticet to L // Registering the lattice 8: add t to To // Registering the teaching time 9: end if 10: end for 11: // Word segmentation using lattices 12: OTo,B \u223c latticelm(L) [23] 13: // Gibbs sampling 14: Initialize parameters iTo , CTo , \u0398 = {W,\u00b5,\u03a3, \u03c6l, \u03c0} 15: for j = 1 to iteration number do 16: iTo \u223c p(iTo | xTo ,\u00b5,\u03a3, \u03c6l, CTo) (13) 17: CTo \u223c p(CTo | xTo , iTo , OTo,B,\u00b5,\u03a3, \u03c6l, \u03c0) (14) 18: W \u223c p(W | CTo , OTo,B) (16) 19: \u00b5,\u03a3 \u223c p(\u00b5,\u03a3 | iTo , xTo , CTo , \u03c6l) (18) 20: \u03c0 \u223c p(\u03c0 | CTo) (19) 21: \u03c6l \u223c p(\u03c6l | xTo , iTo , CTo ,\u00b5,\u03a3) (21) 22: for t = 0 to T do\n23: xt \u223c  p(xt | xt\u22121, xt+1, ut, ut+1, zt) (t /\u2208 To) p(xt | xt\u22121, xt+1, ut, ut+1, zt) p(it | xt,\u00b5,\u03a3, \u03c6l, Ct)\n(t \u2208 To)\n(22)\n24: end for 25: end for 26: return \u0398"}, {"heading": "D. Self-localization of after learning spatial concepts", "text": "A robot that acquires spatial concepts can leverage spatial concepts to self-localization. The estimated model parameters \u0398 = {W,\u00b5,\u03a3, \u03c6l, \u03c0} and a speech recognition sentence Ot,B at time t are given to the condition part of the probability formula of MCL as follows:\np(x0:t | z1:t, u1:t, O1:t,B,\u0398) \u221d p(zt | xt)p(Ot,B | xt,\u0398)p(xt | xt\u22121, ut)\np(x0:t\u22121 | z1:t\u22121, u1:t\u22121, O1:t\u22121,B,\u0398). (23)\nWhen the robot hears the name of a place spoken by the utterer, in addition to the likelihood of the sensor model of MCL, the likelihood of xt with respect to a speech recognition sentence is calculated as follows:\np(Ot,B | xt,\u0398) \u221d \u2211 Ct [ p(Ot,B|WCt) \u2211 it [ p(xt|\u00b5it ,\u03a3it)p(it|\u03c6Ct) ] p(Ct|\u03c0) ] .\n(24)\nThe algorithm of self-localization utilizing spatial concepts is shown in Algorithm 2. The set of particles is denoted as Xt, the temporary set that stores the pairs of the particle x[m]t and the weight w[m]t , i.e., \u3008x [m] t , w [m] t \u3009, is denoted as X\u0304t. The number of particles is M . The function sample motion model\nAlgorithm 2 Self-localization utilizing spatial concepts 1: procedure Localization(Xt\u22121, ut, zt, Ot,B,\u0398) 2: X\u0304t = Xt = \u2205 3: for m = 1 to M do 4: x\n[m] t = sample motion model(ut, x [m] t\u22121) (9)\n5: w [m] t = sensor model(zt, x [m] t ) (10) 6: if the speech signal is observed then 7: w\n[m] t = w [m] t \u00d7 p(Ot,B | xt,\u0398)\n8: end if 9: add \u3008x[m]t , w [m] t \u3009 to X\u0304t\n10: end for 11: for m = 1 to M do 12: draw i with probability \u221d w[i]t 13: add x[i]t to Xt 14: end for 15: return Xt 16: end procedure\nis a function that moves each particle from its previous state xt\u22121 to its current state xt by using control data. The function sensor model calculates the likelihood of each particle x[m]t using sensor data zt. These functions are normally used in MCL. For further details, please refer to [27]. In this case, a speech recognition sentence Ot,B is obtained by the speech recognition system using a word dictionary containing all the learned words."}, {"heading": "IV. EXPERIMENT I", "text": "In this experiment, we validate the evidence of the proposed method (SpCoA) in an environment simulated on the simulator platform SIGVerse2 [31], which enables the simulation of social interactions. The speech recognition is performed using the Japanese continuous speech recognition system Julius3 [32], [33]. The set of 43 Japanese phonemes defined by Acoustical Society of Japan (ASJ)\u2019s speech database committee is adopted by Julius [32]. The representation of these phonemes is also adopted in this study. The Julius system uses a word dictionary containing 115 Japanese syllables. The microphone attached on the robot is SHURE\u2019s PG27-USB. Further, an unsupervised morphological analyzer, a latticelm 0.4, is implemented [23].\nIn the experiment, we compare the following three types of word segmentation methods. A set of syllable sequences is given to the graphical model of SpCoA by each method. This set is used for the learning of spatial concepts as recognized uttered sentences OTo,B.\n(A) latticelm (proposed method) Syllable recognition results in the lattice format are segmented by using latticelm. (B) 1-best NPYLM Syllable recognition results of the 1-best method are segmented by using latticelm. In this case, latticelm [23] is almost equivalent to NPYLM [14].\n2SIGServer-2.2.2, SIGViewer-2.2.0, http://www.sigverse.com/wiki/ 3Julius dictation-kit-v4.3.1-linux, GMM-HMM decoding, http://julius.\nsourceforge.jp/\n7 /geNkaN/ /gomibako/ /kiqchiN/ /daidokoro/ /terebimae/\n/teeburunoatari/ /teeburunoatari/\n/hoNdana/ /sofaamae/\nFig. 3. Environment to be used for learning and localization on SIGVerse: This is a pseudo-room in the simulated real world. There is a robot in the center of the room. The size of the room is 500 cm \u00d7 1,000 cm, and the size of the robot is 50 cm \u00d7 50 cm.\nThe remainder of this section is organized as follows: In Section IV-A, the conditions and results of learning spatial concepts are described. The experiments performed using the learned spatial concepts are described in Section IV-B to IV-E. In Section IV-B, we evaluate the accuracy of the phoneme recognition and word segmentation for uttered sentences. In Section IV-C, we evaluate the clustering accuracy of the estimation results of index Ct of spatial concepts for each teaching utterance. In Section IV-D, we evaluate the accuracy of the acquisition of names of places. In Section IV-E, we show that spatial concepts can be utilized for effective selflocalization."}, {"heading": "A. Learning of spatial concepts", "text": "1) Conditions: We conduct this experiment of spatial concept acquisition in the environment prepared on SIGVerse. The experimental environment is shown in Fig. 3. A mobile robot can move by performing forward, backward, right rotation, or left rotation movements on a two-dimensional plane. In this experiment, the robot can use an approximately correct map of the considered environment. The robot has a range\nk=0k=7k=17\nk=1 k=2k=6\nk=4 k=3\nFig. 4. Learning result of the position distribution: A point group of each color to represent each position distribution is drawn on an map of the considered environment. The colors of the point groups are determined randomly. Each balloon shows the index number for each position distribution.\nFig. 5. Learning result of the multinomial distributions of the names of places W (top); multinomial distributions of the index of the position distribution \u03c6l (bottom): All the words obtained during the experiment are shown.\n8\nsensor in front and performs self-localization on the basis of an occupancy grid map. The initial particles are defined by the true initial position of the robot. The number of particles is M = 1000.\nThe lag value of the Monte Carlo fixed-lag smoothing is fixed at 100. The other parameters of this experiment are as follows: L = 50, K = 50, \u03b1 = 1.5, \u03b3 = 8, \u03b20 = 0.5, m0 = [0, 0]\nT, \u03ba0 = 0.001, V0 = diag(1000, 1000), and \u03bd0 = 2. The number of iterations used for Gibbs sampling is 100. This experiment does not include the direct assignment sampling of xt in equation (22), i.e., lines 22\u201324 of Algorithm 1 are omitted, because we consider that the selfposition can be obtained with sufficiently good accuracy by using the Monte Carlo smoothing. Eight places are selected as the learning targets, and eight types of place names are considered. Each uttered place name is shown in Fig. 3. These utterances include the same name in different places, i.e., \u201cteeburunoatari\u201d (which means near the table in English), and different names in the same place, i.e., \u201ckiqchiN\u201d and \u201cdaidokoro\u201d (which mean a kitchen in English). The other teaching names are \u201cgeNkaN\u201d (which means an entrance or a doorway in English); \u201cterebimae\u201d (which means the front of the TV in English); \u201cgomibako\u201d (which means a trash box in English); \u201choNdana\u201d (which means a bookshelf in English); and \u201csofaamae\u201d (which means the front of the sofa in English). The teaching utterances, including the 10 types of phrases, are spoken for a total of 90 times. The phrases in each uttered sentence are listed in Table II.\n2) Results: The learning results of spatial concepts obtained by using the proposed method are presented here. Fig. 4 shows the position distributions learned in the experimental environment. Fig. 5 (top) shows the word distributions of the names of places for each spatial concept, and Fig. 5 (bottom) shows the multinomial distributions of the indices of the position distributions. Consequently, the proposed method can learn the names of places corresponding to each place of the learning target. In the spatial concept of index Ct = 1, the highest probability of words was \u201csofamae\u201d, and the highest probability of the indices of the position distribution was k = 0; therefore, the name of a place \u201csofamae\u201d was learned to correspond to the position distribution of k = 0. In the spatial concept of index Ct = 5, \u201ckiqchi\u201d and \u201cdaidokoro\u201d were learned to correspond to the position distribution of k = 1. Therefore, this result shows that multiple names can be learned for the same place. In the spatial concept of index Ct = 0, \u201cte\u201d and \u201cdurunoatari\u201d (one word in a normal situation) were learned to correspond to the position distributions of k = 3 and k = 4. Therefore, this result shows that the same name can be learned for multiple places."}, {"heading": "B. Phoneme recognition accuracy of uttered sentences", "text": "1) Conditions: We compared the performance of three types of word segmentation methods for all the considered uttered sentences. It was difficult to weigh the ambiguous syllable recognition and the unsupervised word segmentation separately. Therefore, this experiment considered the positions of a delimiter as a single letter. We calculated the matching\nrate of a phoneme string of a recognition result of each uttered sentence and the correct phoneme string of the teaching data that was suitably segmented into Japanese morphemes using MeCab4, which is an off-the-shelf Japanese morphological analyzer that is widely used for natural language processing. The matching rate of the phoneme string was calculated by using the phoneme accuracy rate (PAR) as follows:\nPAR = 1\u2212 S +D + I N . (25)\nThe numerator of equation (25) is calculated by using the Levenshtein distance between the correct phoneme string and the recognition phoneme string. S denotes the number of substitutions; D, the number of deletions; and I , the number of insertions. N represents the number of phonemes of the correct phoneme string.\n2) Results: Table III shows the results of PAR. Table IV presents examples of the word segmentation results of the three considered methods. We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable recognition results in the lattice format."}, {"heading": "C. Estimation accuracy of spatial concepts", "text": "1) Conditions: We compared the matching rate with the estimation results of index Ct of the spatial concepts of each teaching utterance and the classification results of the correct answer given by humans. The evaluation of this experiment used the adjusted Rand index (ARI) [34]. ARI is a measure of the degree of similarity between two clustering results.\nFurther, we compared the proposed method with a method of word clustering without location information for the investigation of the effect of lexical acquisition using location information. In particular, a method of word clustering without location information used the Dirichlet process mixture (DPM) of the unigram model of an SBP representation. The parameters corresponding to those of the proposed method were the same as the parameters of the proposed method and were estimated using Gibbs sampling.\n2) Results: Fig. 6 shows the results of the average of the ARI values of 10 trials of learning by Gibbs sampling. Here, we found that the proposed method showed the best score. These results and the results reported in Section IV-B suggest that learning by uttered sentences obtained by better phoneme\n4MeCab, http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html\n9\nSpCoA SpCoA (1-best NPYLM) SpCoA (BoS)\nDPM (latticelm) DPM (1-best NPYLM) DPM (BoS)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nA R\nI\nFig. 6. Comparison of the accuracy rates of the estimation results of spatial concepts\nrecognition and better word segmentation produces a good result for the acquisition of spatial concepts. Furthermore, in a comparison of two clustering methods, we found that SpCoA was considerably better than DPM, a word clustering method without location information, irrespective of the word segmentation method used. The experimental results showed that it is possible to improve the estimation accuracy of spatial concepts and vocabulary by performing word clustering that considered location information."}, {"heading": "D. Accuracy of acquired phoneme sequences representing the names of places", "text": "1) Conditions: We evaluated whether the names of places were properly learned for the considered teaching places. This experiment assumes a request for the best phoneme sequence Ot,best representing the self-position xt for a robot. The robot moves close to each teaching place. The probability of a word Ot,best when the self-position xt of the robot is given, p(Ot,best | xt), can be obtained by using equation (24). The word having the best probability was selected. We compared the PAR with the correct phoneme sequence and a selected name of the place. Because \u201ckiqchiN\u201d and \u201cdaidokoro\u201d were taught for the same place, the word whose PAR was the higher score was adopted.\n2) Results: Fig. 7 shows the results of PAR for the word considered the name of a place. SpCoA (latticelm), the proposed method using the results of unsupervised word segmentation on the basis of the speech recognition results in the lattice format, showed the best PAR score. In the 1-best and BoS methods, a part syllable sequence of the name of a place was more minutely segmented as shown in Table IV. Therefore, the robot could not learn the name of the teaching place as a coherent phoneme sequence. In contrast, the robot could learn the names of teaching places more accurately by using the proposed method.\nSpCoA SpCoA (1-best NPYLM) SpCoA (BoS)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPA R\nFig. 7. PAR scores for the word considered the name of a place"}, {"heading": "E. Self-localization that utilizes acquired spatial concepts", "text": "1) Conditions: In this experiment, we validate that the robot can make efficient use of the acquired spatial concepts. We compare the estimation accuracy of localization for the proposed method (SpCoA MCL) and the conventional MCL. When a robot comes to the learning target, the utterer speaks out the sentence containing the name of the place once again for the robot. The moving trajectory of the robot and the uttered positions are the same in all the trials. In particular, the uttered sentence is \u201ckokowa ** dayo\u201d. When learning a task, this phrase is not used. The number of particles is M = 1000, and the initial particles are uniformly distributed in the considered environment. The robot performs a control operation for each time step.\nThe estimation error in the localization is evaluated as follows: While running localization, we record the estimation error (equation (26)) on the xy plane of the floor for each time step.\net = \u221a (x\u0304t \u2212 x\u2217t )2 + (y\u0304t \u2212 y\u2217t )2 (26)\nwhere x\u2217t ,y \u2217 t denote the true position coordinates of the robot as obtained from the simulator, and x\u0304t = \u2211M i=1 w (i) t x (i) t ,\ny\u0304t = \u2211M i=1 w (i) t y (i) t represent the weighted mean values of localization coordinates. The normalized weight w(i)t is obtained from the sensor model in MCL as a likelihood. In the utterance time, this likelihood is multiplied by the value calculated using equation (24). x(i)t , y (i) t denote the xcoordinate and the y-coordinate of index i of each particle at time t. After running the localization, we calculated the average of et.\nFurther, we compared the estimation accuracy rate (EAR) of the global localization. In each trial, we calculated the proportion of time step in which the estimation error was less than 50 cm.\n10\n2) Results: Fig. 8 shows the results of the estimation error and the EAR for 10 trials of each method. All trials of SpCoA MCL (latticelm) and almost all trials of the method using 1-best NPYLM and BoS showed relatively small estimation errors. Results of the second trial of 1-best NPYLM and the fifth trial of BoS showed higher estimation errors. In these trials, many particles converged to other places instead of the place where the robot was, based on utterance information. Nevertheless, compared with those of the conventional MCL, the results obtained using spatial concepts showed an obvious improvement in the estimation accuracy. Consequently, spatial concepts acquired by using the proposed method proved to be very helpful in improving the localization accuracy."}, {"heading": "V. EXPERIMENT II", "text": "In this experiment, the effectiveness of the proposed method was tested by using an autonomous mobile robot TurtleBot 25 in a real environment. Fig. 9 shows TurtleBot 2 used in the experiments. Mapping and self-localization are performed by the robot operating system (ROS). The speech recognition system, the microphone, and the unsupervised morphological analyzer were the same as those described in Section IV."}, {"heading": "A. Learning of spatial concepts in the real environment", "text": "1) Conditions: We conducted an experiment of the spatial concept acquisition in a real environment of an entire floor of a building. In this experiment, self-localization was performed using a map generated by SLAM. The initial particles are defined by the true initial position of the robot. The generated map in the real environment and the names of teaching places are shown in Fig. 10. The number of teaching places was\n5TurtleBot 2, http://turtlebot.com/\n19, and the number of teaching names was 16. The teaching utterances were performed for a total of 100 times.\n2) Results: Fig. 11 shows the position distributions learned on the map. Table V shows the five best elements of the multinomial distributions of the name of place WCt and the multinomial distributions of the indices of the position distribution \u03c6Ct for each index of spatial concept Ct. Thus, we found that the proposed method can learn the names of places corresponding to the considered teaching places in the real environment. For example, in the spatial concept of index Ct = 10, \u201ctorire\u201d was learned to correspond to a position distribution of k = 42. Similarly, \u201ckidanokeN\u201d corresponded to k = 8 in Ct = 29, and \u201ckaigihitsu\u201d was corresponded to k = 60 in Ct = 32. In the spatial concept of index Ct = 27, a part of the syllable sequences was minutely segmented as \u201csohatsuke\u201d, \u201cN\u201d, and \u201ctani\u201d, \u201cguchi\u201d. In this case, the robot was taught two types of names. These words were learned to correspond to the same position distribution of k = 55. In Ct = 8, \u201cgomibako\u201d showed a high probability, and it corresponded to three distributions of the position of k = 0, 36, 59. The position distribution of k = 13 had the fourth highest probability in the spatial concept Ct = 8. Therefore, \u201craqkukeN,\u201d which had the fifth highest probability in the spatial concept Ct = 8 (and was expected to relate to the spatial concept Ct = 74), can be estimated as the word drawn from spatial concept Ct = 8. However, in practice, this situation did not cause any severe problems because the spatial concept of the index Ct = 74 had the highest probabilities for the word \u201crapukeN\u201d and the position distribution k = 13 than Ct = 8. In the probabilistic model, the relative probability and the integrative information are important. When the robot listened to an utterance related to \u201craqkukeN,\u201d it could make use of the spatial concept of index Ct = 74 for selflocalization with a high probability, and appropriately updated its estimated self-location. We expected that the spatial concept of index Ct = 2 was learned as two separate spatial concepts. However, \u201cwatarirooka\u201d and \u201ckaidaNmae\u201d were learned as the same spatial concept. Therefore, the multinomial distribution \u03c62 showed a higher probability for the indices of the position distribution corresponding to the teaching places of both \u201cwatarirooka\u201d and \u201ckaidaNmae\u201d.\nThe proposed method adopts a nonparametric Bayesian method in which it is possible to form spatial concepts that allow many-to-many correspondences between names and\n11\nplaces. In contrast, this can create ambiguity that classifies originally different spatial concepts into one spatial concept as a side effect. There is a possibility that the ambiguity of concepts such as Ct = 2 will have a negative effect on selflocalization, even though the self-localization performance was (overall) clearly increased by employing the proposed method. The solution of this problem will be considered in future work.\nIn terms of the PAR of uttered sentences, the evaluation value from the evaluation method used in Section IV-B is 0.83; this value is comparable to the result in Section IV-B.\nHowever, in terms of the PAR of the name of the place, the evaluation value from the evaluation method used in Section IV-D is 0.35, which is lower than that in Section IV-D. We consider that the increase in uncertainty in the real environment and the increase in the number of teaching words reduced the performance. We expect that this problem could be improved using further experience related to places, e.g., if the number of utterances per place is increased, and additional sensory information is provided.\n12"}, {"heading": "B. Modification of localization by the acquired spatial concepts", "text": "1) Conditions: In this experiment, we verified the modification results of self-localization by using spatial concepts in global self-localization. This experiment used the learning results of spatial concepts presented in Section V-A. The experimental procedures are shown below. The initial particles were uniformly distributed on the entire floor. The robot begins to move from a little distance away to the target place. When the robot reached the target place, the utterer spoke the sentence containing the name of the place for the robot. Upon obtaining the speech information, the robot modifies the selflocalization on the basis of the acquired spatial concepts. The number of particles was the same as that mentioned in Section V-A.\n2) Results: Fig. 12 shows the results of the self-localization before (the top part of the figure) and after (the bottom part of the figure) the utterance for three places. The particle states are denoted by red arrows. The moving trajectory of the robot is indicated by a green dotted arrow. Figs. 12 (a), (b), and (c) show the results for the names of places \u201ctoire\u201d, \u201csouhatsukeN\u201d, and \u201cgomibako\u201d. Further, three spatial concepts, i.e., those at k = 0, 36, 59, were learned as \u201cgomibako\u201d. In this experiment, the utterer uttered to the robot when the robot came close to the place of k = 36. In all the examples shown in the top part of the figure, the particles were dispersed in several places. In contrast, the number of particles near the true position of the robot showed an almost accurate increase in all the examples shown in the bottom part of the figure. Thus, we can conclude that the proposed method can modify self-localization by using spatial concepts."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "In this paper, we discussed the spatial concept acquisition, lexical acquisition related to places, and self-localization using acquired spatial concepts. We proposed nonparametric Bayesian spatial concept acquisition method SpCoA that integrates latticelm [23], a spatial clustering method, and MCL. We conducted experiments for evaluating the performance of SpCoA in a simulation and a real environment. SpCoA showed good results in all the experiments. In experiments of the learning of spatial concepts, the robot could form spatial concepts for the places of the learning targets from human continuous speech signals in both the room of the simulation environment and the entire floor of the real environment. Further, the unsupervised word segmentation method latticelm could reduce the variability and errors in the recognition of phonemes in all the utterances. SpCoA achieved more accurate lexical acquisition by performing word segmentation using the lattices of the speech recognition results. In the selflocalization experiments, the robot could effectively utilize the acquired spatial concepts for recognizing self-position and reducing the estimation errors in self-localization.\nAs a method that further improves the performance of the lexical acquisition, a mutual learning method was proposed by Nakamura et al. on the basis of the integration of the learning of object concepts with a language model [35], [36]. Following\na similar approach, Heymann et al. proposed a method that alternately and repeatedly updates phoneme recognition results and the language model by using unsupervised word segmentation [37]. As a result, they achieved robust lexical acquisition. In our study, we can expect to improve the accuracy of lexical acquisition for spatial concepts by estimating both the spatial concepts and the language model.\nFurthermore, as a future work, we consider it necessary for robots to learn spatial concepts online and to recognize whether the uttered word indicates the current place or destination. Furthermore, developing a method that simultaneously acquires spatial concepts and builds a map is one of our future objectives. We believe that the spatial concepts will have a positive effect on the mapping. We also intend to examine a method that associates the image and the landscape with spatial concepts and a method that estimates both spatial concepts and object concepts."}], "references": [{"title": "Learning words from sights and sounds: A computational model,", "author": ["D. Roy", "A. Pentland"], "venue": "Cognitive science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Learning lexicons from spoken utterances based on statistical model selection,", "author": ["R. Taguchi", "N. Iwahashi", "T. Nose", "K. Funakoshi", "M. Nakano"], "venue": "in Annual Conference of the International Speech Communication Association (INTERSPEECH),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Language acquisition through a human\u2013robot interface by combining speech, visual, and behavioral information,", "author": ["N. Iwahashi"], "venue": "Information Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Robots that learn to converse: Developmental approach to situated language processing,", "author": ["N. Iwahashi", "R. Taguchi", "K. Sugiura", "K. Funakoshi", "M. Nakano"], "venue": "Proceedings of International Symposium on Speech and Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Probabilistic grounding of situated speech using plan recognition and reference resolution,", "author": ["P. Gorniak", "D. Roy"], "venue": "Proceedings of the 7th international conference on Multimodal interfaces. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Incorporating temporal and semantic information with eye gaze for automatic word acquisition in multimodal conversational systems,", "author": ["S. Qu", "J.Y. Chai"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Multimodal language acquisition based on motor learning and interaction,\u201d in From Motor Learning to Interaction Learning in Robots", "author": ["J. H\u00f6rnstein", "L. Gustavsson", "J. Santos-Victor", "F. Lacerda"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Online learning of concepts and words using multimodal LDA and hierarchical Pitman-Yor Language Model,", "author": ["T. Araki", "T. Nakamura", "T. Nagai", "S. Nagasaka", "T. Taniguchi", "N. Iwahashi"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "The mathematics of statistical machine translation: Parameter estimation,", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Multimodal categorization by hierarchical Dirichlet process,", "author": ["T. Nakamura", "T. Nagai", "N. Iwahashi"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling,", "author": ["D. Mochihashi", "T. Yamada", "N. Ueda"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Learning physically grounded lexicons from spoken utterances,", "author": ["R. Taguchi", "N. Iwahashi", "K. Funakoshi", "M. Nakano", "T. Nose", "T. Nitta"], "venue": "Human Machine Interaction\u2013Getting Closer,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Learning place-names from spoken utterances and localization results by mobile robot,", "author": ["R. Taguchi", "Y. Yamada", "K. Hattori", "T. Umezaki", "M. Hoguro", "N. Iwahashi", "K. Funakoshi", "M. Nakano"], "venue": "in Annual Conference of the International Speech Communication Association (INTERSPEECH),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "RatSLAM: a hippocampal model for simultaneous localization and mapping,", "author": ["M. Milford", "G. Wyeth", "D. Prasser"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Learning spatial concepts from RatSLAM representations,", "author": ["M. Milford", "R. Schulz", "D. Prasser", "G. Wyeth", "J. Wiles"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Lingodroids: socially grounding place names in privately grounded cognitive maps,", "author": ["R. Schulz", "G. Wyeth", "J. Wiles"], "venue": "Adaptive Behavior,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Communication between Lingodroids with different cognitive capabilities,", "author": ["S. Heath", "D. Ball", "R. Schulz", "J. Wiles"], "venue": "IEEE International Conference on Robotics and Automation (ICRA). IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Are we there yet? grounding temporal concepts in shared journeys,", "author": ["R. Schulz", "G. Wyeth", "J. Wiles"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Grounded spatial symbols for task planning based on experience,", "author": ["K. Welke", "P. Kaiser", "A. Kozlov", "N. Adermann", "T. Asfour", "M. Lewis", "M. Steedman"], "venue": "in 13th International Conference on Humanoid Robots (Humanoids). IEEE/RAS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Bayesian learning of a language model from continuous speech,", "author": ["G. Neubig", "M. Mimura", "T. Kawahara"], "venue": "IEICE TRANSACTIONS on Information and Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Monte carlo localization for mobile robots,", "author": ["F. Dellaert", "D. Fox", "W. Burgard", "S. Thrun"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "A constructive definition of Dirichlet priors,", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1994}, {"title": "A sticky HDP-HMM with application to speaker diarization,", "author": ["E.B. Fox", "E.B. Sudderth", "M.I. Jordan", "A.S. Willsky"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "FastSLAM: A factored solution to the simultaneous localization and mapping problem,", "author": ["M. Montemerlo", "S. Thrun", "D. Koller", "B. Wegbreit"], "venue": "Proceedings of the AAAI National Conference on Artificial Intelligence. American Association for Artificial Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "An efficient FastSLAM algorithm for generating maps of large-scale cyclic environments from raw laser range measurements,", "author": ["D. Hahnel", "W. Burgard", "D. Fox", "S. Thrun"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "Computational aspects of sequential Monte Carlo filter and smoother,", "author": ["G. Kitagawa"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Simulator platform that enables social interaction simulation \u2013SIGVerse: SocioIntelliGenesis simulator\u2013,", "author": ["T. Inamura", "T. Shibata", "H. Sena", "T. Hashimoto", "N. Kawai", "T. Miyashita", "Y. Sakurai", "M. Shimizu", "M. Otake", "K. Hosoda"], "venue": "IEEE/SICE International Symposium on System Integration,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Sharable software repository for Japanese large vocabulary continuous speech recognition,", "author": ["T. Kawahara", "T. Kobayashi", "K. Takeda", "N. Minematsu", "K. Itou", "M. Yamamoto", "A. Yamada", "T. Utsuro", "K. Shikano"], "venue": "Fifth International Conference on Spoken Language Processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Julius\u2014an open source realtime large vocabulary recognition engine,", "author": ["A. Lee", "T. Kawahara", "K. Shikano"], "venue": "European Conference on Speech Communication and Technology (EUROSPEECH),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Arabie, \u201cComparing partitions,", "author": ["P.L. Hubert"], "venue": "Journal of classification,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1985}, {"title": "Multimodal concept and word learning using phoneme sequences with errors,", "author": ["T. Nakamura", "T. Araki", "T. Nagai", "S. Nagasaka", "T. Taniguchi", "N. Iwahashi"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Mutual learning of an object concept and language model based on MLDA and NPYLM,", "author": ["T. Nakamura", "T. Nagai", "K. Funakoshi", "S. Nagasaka", "T. Taniguchi", "N. Iwahashi"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Iterative Bayesian Word Segmentation for Unsupervised Vocabulary Discovery from Phoneme Lattices,", "author": ["J. Heymann", "O. Walter", "R. Haeb-Umbach", "B. Raj"], "venue": "in 39th International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "These studies differ from speech recognition studies based on a large vocabulary and natural language processing studies based on lexical, syntactic, and semantic knowledge [1], [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "These studies differ from speech recognition studies based on a large vocabulary and natural language processing studies based on lexical, syntactic, and semantic knowledge [1], [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "Most studies on lexical acquisition typically focus on lexicons about objects [1], [3]\u2013[11].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "Most studies on lexical acquisition typically focus on lexicons about objects [1], [3]\u2013[11].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "Most studies on lexical acquisition typically focus on lexicons about objects [1], [3]\u2013[11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "proposed a computational model that enables a robot to learn the names of objects from an object image and spontaneous infant-directed speech [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "reported that a robot properly understands the situation and acquires the relationship of object behaviors and sentences [3]\u2013[5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "reported that a robot properly understands the situation and acquires the relationship of object behaviors and sentences [3]\u2013[5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "Qu & Chai focused on the conjunction between speech and eye gaze and the use of domain knowledge in lexical acquisition [7], [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "Qu & Chai\u2019s method based on the IBM translation model [12] estimates the word-entity association probability.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "The method proposed in [10] is a categorization method based on multimodal latent Dirichlet allocation (MLDA) that enables the acquisition of object concepts from multimodal information, such as visual, auditory, and haptic information [13].", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "addressed the development of a method combining unsupervised word segmentation from uttered sentences by a nested Pitman-Yor language model (NPYLM) [14] and the learning of object concepts by MLDA [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 7, "context": "addressed the development of a method combining unsupervised word segmentation from uttered sentences by a nested Pitman-Yor language model (NPYLM) [14] and the learning of object concepts by MLDA [11].", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "proposed a method for the unsupervised learning of phoneme sequences and relationships between words and objects from various user utterances without any prior linguistic knowledge other than an acoustic model of phonemes [2], [15].", "startOffset": 222, "endOffset": 225}, {"referenceID": 11, "context": "proposed a method for the unsupervised learning of phoneme sequences and relationships between words and objects from various user utterances without any prior linguistic knowledge other than an acoustic model of phonemes [2], [15].", "startOffset": 227, "endOffset": 231}, {"referenceID": 12, "context": "Further, they proposed a method for the simultaneous categorization of self-position coordinates and lexical learning [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "proposed RatSLAM inspired by the biological knowledge of a pose cell of the hippocampus of rodents [17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "proposed a method that enables a robot to acquire spatial concepts by using RatSLAM [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "Further, Lingodroids, mobile robots that learn a language through robot-to-robot communication, have been studied [19]\u2013[21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "Further, Lingodroids, mobile robots that learn a language through robot-to-robot communication, have been studied [19]\u2013[21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "In [21], the researchers showed that it was possible to learn temporal concepts in a manner analogous to the acquisition of spatial concepts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "proposed a method that acquires spatial representation by the integration of the representation of the continuous state space on the sensorimotor level and the discrete symbolic entities used in high-level reasoning [22].", "startOffset": 216, "endOffset": 220}, {"referenceID": 19, "context": "We propose nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates a nonparametric morphological analyzer for the lattice [23], i.", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": ", latticelm1, a spatial clustering method, and Monte Carlo localization (MCL) [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "The prior distribution configured by using the stick breaking process (SBP) [25] is denoted as GEM(\u00b7), the multinomial distribution as Mult(\u00b7), the Dirichlet distribution as Dir(\u00b7), the inverse\u2013Wishart distribution as IW(\u00b7), and the multivariate Gaussian (normal) distribution as N (\u00b7).", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "1latticelm is the name of the tool that [23] is implemented and is treated as the name of the method in this study.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": ", a weaklimit approximation [26].", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences [23].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "Unsupervised word segmentation using the lattices of syllable recognition is expected to be able to reduce the variability and errors in phonemes as compared to NPYLM [14], i.", "startOffset": 167, "endOffset": 171}, {"referenceID": 20, "context": "The self-localization method adopts MCL [24], a method that is generally used as the localization of mobile robots for simultaneous localization and mapping (SLAM) [27].", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "assume that a robot generates an environment map by using MCL-based SLAM such as FastSLAM [28], [29] in advance, and then, performs localization by using the generated map.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "assume that a robot generates an environment map by using MCL-based SLAM such as FastSLAM [28], [29] in advance, and then, performs localization by using the generated map.", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "Self-positions x0:T are sampled by using a Monte Carlo fixed-lag smoother [30] in the learning phase.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "4: x0:t\u223cMonte Carlo smoother(x0:t\u22121, u1:t, z1:t) [30] 5: if the speech signal is observed then", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "12: OTo,B \u223c latticelm(L) [23] 13: // Gibbs sampling 14: Initialize parameters iTo , CTo , \u0398 = {W,\u03bc,\u03a3, \u03c6l, \u03c0} 15: for j = 1 to iteration number do", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "In this experiment, we validate the evidence of the proposed method (SpCoA) in an environment simulated on the simulator platform SIGVerse2 [31], which enables the simulation of social interactions.", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "The speech recognition is performed using the Japanese continuous speech recognition system Julius3 [32], [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 28, "context": "The speech recognition is performed using the Japanese continuous speech recognition system Julius3 [32], [33].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "The set of 43 Japanese phonemes defined by Acoustical Society of Japan (ASJ)\u2019s speech database committee is adopted by Julius [32].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "4, is implemented [23].", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "In this case, latticelm [23] is almost equivalent to NPYLM [14].", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "In this case, latticelm [23] is almost equivalent to NPYLM [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "The evaluation of this experiment used the adjusted Rand index (ARI) [34].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "We proposed nonparametric Bayesian spatial concept acquisition method SpCoA that integrates latticelm [23], a spatial clustering method, and MCL.", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "on the basis of the integration of the learning of object concepts with a language model [35], [36].", "startOffset": 89, "endOffset": 93}, {"referenceID": 31, "context": "on the basis of the integration of the learning of object concepts with a language model [35], [36].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "proposed a method that alternately and repeatedly updates phoneme recognition results and the language model by using unsupervised word segmentation [37].", "startOffset": 149, "endOffset": 153}], "year": 2016, "abstractText": "In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Further, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot 2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.", "creator": "LaTeX with hyperref package"}}}