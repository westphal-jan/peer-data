{"id": "1602.01580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "Long-term Planning by Short-term Prediction", "abstract": "We consider ordell planning problems, that tuqay often scma arise overtones in 21.69 autonomous driving promus applications, navalny in which comanchero an neo-assyrian agent chanticleer should blasingame decide sa-16 on umfcci immediate demer actions so drink as to optimize a teno long baldick term mazzoni objective. sitt For example, mutuals when a car tries 47.86 to hnatiuk merge downloaders in berden a boldini roundabout bundesautobahn it 73-minute should decide misjudge on an oa immediate avanza acceleration / kipchumba braking kulers command, k\u00f6nigssee while the refitting long term effect of the command bracton is the success / infoline failure of the monogamy merge. larino Such problems are quitman characterized delt by continuous 1975-1990 state and action spaces, fregatidae and by interaction knoc with multiple iden agents, whose drumstick behavior sadiya can be adversarial. We argue that dual-core dual versions adeniran of the MDP bogalusa framework (dalgety that bagdhad depend tanya on the value function buckfast and matabele the $ coathanger Q $ halfa function) are punctuates problematic for autonomous driving applications backpedalling due to ponerinae the koroi non Markovian of pesnica the qamishli natural state space gouichi representation, and juggling due to 38-4 the dombroski continuous state overlake and old-time action street-fighting spaces. We breathing propose to unsc tackle berlina the planning task by decomposing erdal the elvia problem naccache into roskosmos two sahdan phases: mehle First, mehsood we leduc apply supervised stax learning for predicting torlen the golfo near coset future budgam based on the hotshots present. fledged We interregnum require pecdar that the undisputed predictor will thoreson be differentiable menapii with respect to chastened the jurvetson representation fiala of m\u00fchlacker the 4:47 present. murasaki Second, lachi we astrophysicists model heros a full trajectory of the zelasko agent freycinet using anthurium a nimmons recurrent neural officium network, where unexplained factors manzoni are gentoo modeled as (draves additive) dazhi input 38,438 nodes. This exonym allows kuro us dehere to ralambo solve the sanglah long - kift term grants planning problem mets using rcw supervised wolfram learning techniques and beheaded direct optimization over abdin the 55.85 recurrent ably neural network. Our approach pixies enables us trackman to learn immodest robust policies by resembling incorporating saint-leonard adversarial elements to confirming the environment.", "histories": [["v1", "Thu, 4 Feb 2016 08:06:59 GMT  (27kb,D)", "http://arxiv.org/abs/1602.01580v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "nir ben-zrihem", "aviad cohen", "amnon shashua"], "accepted": false, "id": "1602.01580"}, "pdf": {"name": "1602.01580.pdf", "metadata": {"source": "CRF", "title": "Long-term Planning by Short-term Prediction", "authors": ["Shai Shalev-Shwartz", "Nir Ben-Zrihem", "Aviad Cohen", "Amnon Shashua"], "emails": [], "sections": [{"heading": null, "text": "decide on immediate actions so as to optimize a long term objective. For example, when a car tries to merge in a roundabout it should decide on an immediate acceleration/braking command, while the long term effect of the command is the success/failure of the merge. Such problems are characterized by continuous state and action spaces, and by interaction with multiple agents, whose behavior can be adversarial. We argue that dual versions of the MDP framework (that depend on the value function and the Q function) are problematic for autonomous driving applications due to the non Markovian of the natural state space representation, and due to the continuous state and action spaces. We propose to tackle the planning task by decomposing the problem into two phases: First, we apply supervised learning for predicting the near future based on the present. We require that the predictor will be differentiable with respect to the representation of the present. Second, we model a full trajectory of the agent using a recurrent neural network, where unexplained factors are modeled as (additive) input nodes. This allows us to solve the long-term planning problem using supervised learning techniques and direct optimization over the recurrent neural network. Our approach enables us to learn robust policies by incorporating adversarial elements to the environment."}, {"heading": "1 Introduction", "text": "Two of the most crucial elements of autonomous driving systems are sensing and planning. Sensing deals with finding a compact representation of the present state of the environment, while planning deals with deciding on what actions to take so as to optimize future objectives. Supervised machine learning techniques are very useful for solving sensing problems. In this paper we describe a machine learning algorithmic framework for the planning part. Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.\nTypically, RL is performed in a sequence of consecutive rounds. At round t, the planner (a.k.a. the agent) observes a state, st \u2208 S, which represents the agent as well as the environment. It then should decide on an action at \u2208 A. After performing the action, the agent receives an immediate reward, rt \u2208 R, and is moved to a new state, st+1. As a simple example, consider an adaptive cruise control (ACC) system, in which a self driving vehicle should implement acceleration/braking so as to keep an adequate distance to a preceding vehicle while maintaining smooth driving. We can model the state as a pair, st = (xt, vt) \u2208 R2, where xt is the distance to the preceding vehicle and vt is the velocity of the car relative to the velocity of the preceding vehicle. The action at \u2208 R will be the acceleration command (where the car slows down if at < 0). The reward can be some function that depends on |at| (reflecting the smoothness of driving) and on st (reflecting that we keep a safe distance from the preceding vehicle). The goal of the planner is to maximize the cumulative reward (maybe up to a time horizon or a discounted sum of future rewards). To do so, the planner relies on a policy, \u03c0 : S \u2192 A, which maps a state into an action.\nSupervised Learning (SL) can be viewed as a special case of RL, in which st is sampled i.i.d. from some distribution over S and the reward function has the form rt = \u2212`(at, yt), where ` is some loss function, and the learner observes the value of yt which is the (possibly noisy) value of the optimal action to take when viewing the state st.\nThere are several key differences between the fully general RL model and the specific case of SL. These differences makes the general RL problem much harder.\n1. In SL, the actions (or predictions) taken by the learner have no effect on the environment. In particular, st+1 and at are independent. This has two important implications:\nar X\niv :1\n60 2.\n01 58\n0v 1\n[ cs\n.L G\n] 4\nF eb\n2 01\n6\n\u2022 In SL we can collect a sample (s1, y1), . . . , (sm, ym) in advance, and only then search for a policy (or predictor) that will have good accuracy on the sample. In contrast, in RL, the state st+1 usually depends on the action (and also on the previous state), which in turn depends on the policy used to generate the action. This ties the data generation process to the policy learning process.\n\u2022 Because actions do not effect the environment in SL, the contribution of the choice of at to the performance of \u03c0 is local, namely, at only affects the value of the immediate reward. In contrast, in RL, actions that are taken at round t might have a long-term effect on the reward values in future rounds.\n2. In SL, the knowledge of the \u201ccorrect\u201d answer, yt, together with the shape of the reward, rt = \u2212`(at, yt), gives us a full knowledge of the reward for all possible choices of at. Furthermore, this often enables us to calculate the derivative of the reward with respect to at. In contrast, in RL, we only observe a \u201cone-shot\u201d value of the reward for the specific choice of action we took. This is often called a \u201cbandit\u201d feedback. It is one of the main reasons for the need of \u201cexploration\u201d, because if we only get to see a \u201cbandit\u201d feedback, we do not always know if the action we took is the best one.\nBefore explaining our approach for tackling these difficulties, we briefly describe the key idea behind most common reinforcement learning algorithms. Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3]. The Markovian assumption is that the distribution of st+1 is fully determined given st and at. This yields a closed form expression for the cumulative reward of a given policy in terms of the stationary distribution over states of the MDP. The stationary distribution of a policy can be expressed as a solution to a linear programming problem. This yields two families of algorithms: optimizing with respect to the primal problem, which is called policy search, and optimizing with respect to the dual problem, whose variables are called the value function, V \u03c0 . The value function determines the expected cumulative reward if we start the MDP from the initial state s, and from there on pick actions according to \u03c0. A related quantity is the state-action value function, Q\u03c0(s, a), which determines the cumulative reward if we start from state s, immediately pick action a, and from there on pick actions according to \u03c0. The Q function gives rise to a crisp characterization of the optimal policy (using the so called Bellman\u2019s equation), and in particular it shows that the optimal policy is a deterministic function from S to A (in fact, it is the greedy policy with respect to the optimal Q function).\nIn a sense, the key advantage of the MDP model is that it allows us to couple all the future into the present using the Q function. That is, given that we are now in state s, the value of Q\u03c0(s, a) tells us the effect of performing action a at the moment on the entire future. Therefore, the Q function gives us a local measure of the quality of an action a, thus making the RL problem more similar to SL.\nMost reinforcement learning algorithms approximate the V function or theQ function in one way or another. Value iteration algorithms, e.g. the Q learning algorithm [26], relies on the fact that the V and Q functions of the optimal policy are fixed points of some operators derived from Bellman\u2019s equation. Actor-critic policy iteration algorithms aim to learn a policy in an iterative way, where at iteration t, the \u201ccritic\u201d estimates Q\u03c0t and based on this, the \u201cactor\u201d improves the policy.\nDespite the mathematical elegancy of MDPs and the conveniency of switching to the Q function representation, there are several limitations of this approach. First, as noted in [12], usually in robotics, we may only be able to find some approximate notion of a Markovian behaving state. Furthermore, the transition of states depends not only on the agent\u2019s action, but also on actions of other players in the environment. For example, in the ACC example mentioned previously, while the dynamic of the autonomous vehicle is clearly Markovian, the next state depends on the behavior of the other driver, which is not necessarily Markovian. One possible solution to this problem is to use partially observed MDPs [27], in which we still assume that there is a Markovian state, but we only get to see an observation that is distributed according to the hidden state. A more direct approach considers game theoretical generalizations of MDPs, for example the Stochastic Games framework. Indeed, some of the algorithms for MDPs were generalized to multi-agents games. For example, the minimax-Q learning [14] or the Nash-Q learning [9]. Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown\u2019s fictitious play [6], and vanishing regret learning algorithms [8, 7]. See also [25, 24, 11, 5]. As noted in [20], learning in multi-agent setting is inherently more complex than in the single agent setting.\nA second limitation of the Q function representation arises when we depart from a tabular setting. The tabular\nsetting is when the number of states and actions is small, and therefore we can express Q as a table with |S| rows and |A| columns. However, if the natural representation of S andA is as Euclidean spaces, and we try to discretize the state and action spaces, we obtain that the number of states/actions is exponential in the dimension. In such cases, it is not practical to employ the tabular setting. Instead, the Q function is approximated by some function from a parametric hypothesis class (e.g. neural networks of a certain architecture). For example, the deep-Q-networks (DQN) learning algorithm of [16] has been successful at playing Atari games. In DQN, the state space can be continuous but the action space is still a small discrete set. There are approaches for dealing with continuous action spaces (e.g. [21]), but they again rely on approximating the Q function. In any case, the Q function is usually very complicated and sensitive to noise, and it is therefore quite hard to learn it. Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17]. Intuitively, the difficulty in learning Q is that we need to implicitly understand the dynamics of the underlying Markov process.\nIn the autonomous driving domain we tackle in this paper, the multi-agent adversarial environment leads to nonMarkovianity of the natural state representation. Moreover, the natural state and action representations are continuous in nature. Taken together, we found out thatQ-based learning approaches rarely work out-of-the-box, and require long training time and advanced reward shaping.\nA radically different approach has been introduced by Schmidhuber [19], who tackled the RL problem using a recurrent neural network (RNN). Following [19], there have been several additional algorithms that rely on RNNs for RL problems. For example, Backer [1] proposed to tackle the RL problem using recurrent networks with the LSTM architecture. His approach still relies on the value function. Scha\u0308fer [18] used RNN to model the dynamics of partially observed MDPs. Again, he still relies on explicitly modeling the Markovian structure. There have been few other approaches to tackle the RL problem without relying on value functions. Most notably is the REINFORCE framework of Williams [28]. It has been recently successful for visual attention [15, 29]. As already noted by [19], the ability of REINFORCE to estimate the derivative of stochastic units can be straightforwardly combined within the RNN framework.\nIn this paper we combine Schmidhuber\u2019s approach, of tackling the policy learning problem directly using a RNN, with the notions of multi-agents games and robustness to adversarial environments from the game theory literature. Furthermore, we do not explicitly rely on any Markovian assumption. Our approach is described in the next section."}, {"heading": "2 Planning by Prediction", "text": "Throughout, we assume that the state space, S, is some subset of Rd, and the action space, A, is some subset of Rk. This is the most natural representation in many applications, and in particular, the ones we describe in Section 3.\nOur goal is to learn a policy \u03c0 : S \u2192 A. As is standard in machine learning, we bias ourselves to pick a policy function \u03c0 from a hypothesis classH. Namely,H is a predefined set of policy functions from which we should pick the best performing one. In order to learn \u03c0 using the SL framework, one would need a training set of pairs (state,optimalaction). We of course do not have such a training set. Instead, we only have an access to a \u201csimulator\u201d, that can be used to assess the quality of \u03c0. Formally, fixing a horizon T , any policy \u03c0 induces a distribution over RT , such that the probability of (r1, . . . , rT ) \u2208 RT is the probability to apply our simulator for T steps, while on step t we observe st, feed the simulator with the action at = \u03c0(st), and observe the reward rt. Denote by B the random bits used by the simulator, we note that we can write the vector r = (r1, . . . , rT ) of rewards as a deterministic function R(B, \u03c0). We use Rt(B, \u03c0) to denote the t\u2019th element of R(B, \u03c0). We can now formulate the problem of learning the policy \u03c0 as the following optimization problem:\nmax \u03c0\u2208H E B [ T\u2211 t=1 Rt(B, \u03c0) ] , (1)\nwhere the expectation is over the distribution over B. We assume that the hypothesis class,H, is the set of deep neural networks (DNN) of a certain predefined architecture, and therefore every \u03c0 \u2208 H is parametrized by a vector of weights, \u03b8. We use \u03c0\u03b8 to denote the policy associated with the vector \u03b8.\nIf we could expressR(B, \u03c0\u03b8) as a differential function of \u03b8, we could have utilized the Stochastic Gradient Descent (SGD) approach for maximizing (1). That is, starting with an initial \u03b8, at each iteration of SGD we first sample B, then we calculate the gradient of \u2211T t=1Rt(B, \u03c0\u03b8) with respect to \u03b8, and finally we update \u03b8 based on this gradient.\nOur key observation is that by solving two SL problems, described below, we can approximate R(B, \u03c0\u03b8) by a differential function of \u03b8. Hence, we can implement SGD for learning \u03c0\u03b8 directly.\nThe goal of the first SL problem is to learn a deep neural network (DNN), that represents the mapping from a (state,action) pair into the immediate reward value. We denote this DNN by DNNr and it is formally described as a function DNNr : S \u00d7 A \u2192 R. We shall later explain how to learn DNNr using SL, but for now lets just assume that we can do it and have the network DNNr such that DNNr(st, at) \u2248 rt. The goal of the second SL problem is to learn a DNN that represents the mapping from (state,action) into the next state. Formally, this DNN is the function DNNN : S \u00d7 A \u2192 S, and for now lets assume we managed to learn DNNN in a supervised manner such that DNNN (st, at) \u2248 st+1.\nEquipped with DNNr and DNNN we can describe the process of generating a randomB and calculatingR(B, \u03c0\u03b8) as follows. Initially, the simulator picks a seed for its pseudo random number generator and then it determines the initial state s1. At round t, the agent receives st from the simulator and applies \u03c0\u03b8 to generate the action at = \u03c0\u03b8(st). The simulator receives at and generates rt and st+1. At the same time, the agent applies DNNr to generate r\u0302t = DNNr(st) and applies DNNN to generate s\u0302t+1 = DNNN (st). Let us denote \u03bdt+1 = st+1 \u2212 s\u0302t+1. Therefore, if the simulator receives s\u0302t+1 it can generate \u03bdt+1 and send it to the agent.\nA single round of this process is depicted below. The entire process is obtained by repeating the shaded part of the picture T times. Solid arrows represent differentiable propagation of information while dashed arrows represent non-differentiable propagation of information.\nst\n\u03c0\u03b8\nat\nDNNN\nDNNr\nr\u0302t\ns\u0302t+1 +\n\u03bdt+1\nst+1\nSimulatort+1Simulatort Simulatort+2\nRecall that we assume that r\u0302t \u2248 rt and s\u0302t+1 \u2248 st+1. If these approximations are exact, then the dashed arrows can be eliminated from the figure above and the entire process of generating the rewards becomes a differentiable recurrent neural network. In such case, we can solve (1) using the SGD framework, and at each iteration we calculate the gradient by backpropagation in time over the recurrent neural network.\nIn most situations, we expect r\u0302t and s\u0302t+1 to slightly deviate from rt and st+1. The deviation of r\u0302t from rt is less of an issue in practice, because it is often the case that there is nothing special about the exact reward rt, and maximizing an approximation of it leads to similar performance. Therefore, for the sake of simplicity, we assume that maximizing the sum of r\u0302t is sufficiently good.\nThe more problematic part is the deviation of s\u0302t+1 from st+1. There are several possible sources for this deviation.\n1. Non-determinism: in the traditional MDP model, st+1 is a random variable whose distribution is a function of (st, at). But, the actual value of st+1 is not necessarily a deterministic function of (st, at).\n2. Non-Markovianity: it may be the case that the process is not Markovian in the state representation. It will always be Markovian in another representation, that is known to the simulator, but we do not know the Markovian representation or we do not want to model it. For example, in the ACC problem given in the next section, st+1 depends on the acceleration commands of the driver in front of us. While the simulator models this behavior in some complicated way, we do not want to model it and prefer to stick with a simple state representation that does not allow us to predict the acceleration of the other driver, but only allows us to react to the other driver\u2019s behavior.\n3. Failures of the learning process: as we will show, we are going to learn DNNN from examples, and we may suffer from the usual inaccuracies of learning algorithms (approximation error, estimation error, and optimization error). As this part is standard we ignore this issue and assume that we have managed to learn DNNN sufficiently good.\nIn any case, despite the fact that s\u0302t+1 can deviate from st+1, we can still apply backpropagation in time in order to calculate an approximate gradient of the cumulative reward w.r.t. \u03c0. In particular, the forward part of the backpropagation is correct, due to the correction made by defining st+1 as a sum of the prediction s\u0302t+1 and the correction term \u03bdt+1 (supplied by the simulator during the forward pass). In the backward part, we propagate the error through the solid arrows given in the figure, but we kill the messages that go through dashed arrows, because we refer to the simulator as a black box. As mentioned previously, we do not impose explicit probabilistic assumptions on \u03bdt. In particular, we do not require Markovian relation. Instead, we rely on the recurrent network to propagate \u201cenough\u201d information between past and future through the solid arrows. Intuitively, DNNN (st, at) describes the predictable part of the near future, while \u03bdt expresses the unpredictable aspects, mainly due to the behavior of other players in the environment. The learner should learn a policy that will be robust to the behavior of other players. Naturally, if \u2016\u03bdt\u2016 is large, the connection between our past actions and future reward values will be too noisy for learning a meaningful policy.\nAs noted in [19], explicitly expressing the dynamic of the system in a transparent way enables to incorporate prior knowledge more easily. For example, in Section 3 we demonstrate how prior knowledge greatly simplifies the problem of defining DNNN .\nFinally, it is left to explain how we can learn DNNr and DNNN within the SL framework. For this, we observe that by relying on the access to the simulator, we have the ability to generate tuples (s, a, r, s\u2032) as training examples, where s is the current state, a is the action, r is the reward, and s\u2032 is the next state. We note that it is customary to use some elements of exploration in generating the training set. Since this is a standard technique, we omit the details. Equipped with such training examples, we can learn DNNr in the SL framework by extracting examples of the form ((s, a), r) from each tuple (s, a, r, s\u2032). The key point here is that even though the action a is not necessarily optimal for s, it does not pose any problem for the task of learning the mapping from state-action into the correct reward. Furthermore, even though the reward is given in a \u201cbandit\u201d manner for the policy learning problem, it forms a \u201cfull information\u201d feedback for the problem of learning a network DNNr, such that DNNr(st, at) \u2248 rt. Likewise, we can learn DNNN in the SL framework by extracting examples of the form ((s, a), s\u2032) from each tuple (s, a, r, s\u2032). Again, the fact that a is not the optimal action for s does not pose any problem for the task of learning the near future, s\u2032, from the current state and action, (s, a).\nIt is also possible to simultaneously learn DNNr,DNNN , and \u03c0\u03b8, by defining an objective that combines the cumulative reward with supervised losses of the form \u2016s\u0302t+1 \u2212 st+1\u20162 and (r\u0302t+1 \u2212 rt)2. In the experimental section we report results for both separate and join training."}, {"heading": "2.1 Robustness to Adversarial Environment", "text": "Since our model does not impose probabilistic assumptions on \u03bdt, we can consider environments in which \u03bdt is being chosen in an adversarial manner. Of course, we must make some restrictions on \u03bdt, otherwise the adversary can make the planning problem impossible. A natural restriction is to require that \u2016\u03bdt\u2016 is bounded by a constant. Robustness against adversarial environment is quite useful in autonomous driving applications. We describe a real world aspect of adversarial environment in Section 3.\nHere, we show that choosing \u03bdt in an adversarial way might even speed up the learning process, as it can focus the learner toward the robust optimal policy. We consider the following simple game. The state is st \u2208 R, the action is at \u2208 R, and the immediate loss function is 0.1|at|+ [|st| \u2212 2]+, where [x]+ = max{x, 0} is the ReLU function. The next state is st+1 = st + at + \u03bdt, where \u03bdt \u2208 [\u22120.5, 0.5] is chosen by the environment in an adversarial manner.\nIt is possible to see that the optimal policy can be written as a two layer network with ReLU: at = \u2212[st\u2212 1.5]+ + [\u2212st \u2212 1.5]+. Observe that when |st| \u2208 (1.5, 2], the optimal action has a larger immediate loss than the action a = 0. Therefore, the learner must plan for the future and cannot rely solely on the immediate loss.\nObserve that the derivative of the loss w.r.t. at is 0.1 sign(at) and the derivative w.r.t. st is 1[|st| > 2] sign(st). Suppose we are in a situation in which st \u2208 (1.5, 2]. The adversarial choice of \u03bdt would be to set \u03bdt = 0.5, and therefore, we will have a non-zero loss on round t + 1, whenever at > 1.5 \u2212 st. In all such cases, the derivative of the loss will back-propagate directly to at. We therefore see that the adversarial choice of \u03bdt helps the learner to get a non-zero back-propagation message in all cases for which the choice of at is sub-optimal."}, {"heading": "3 Example Applications", "text": "The goal of this section is to demonstrate some aspects of our approach on two toy examples: adaptive cruise control (ACC) and merging into a roundabout."}, {"heading": "3.1 The ACC Problem", "text": "In the ACC problem, a host vehicle is trying to keep an adequate distance of 1.5 seconds to a target car, while driving as smooth as possible. We provide a simple model for this problem as follows. The state space is R3 and the action space is R. The first coordinate of the state is the speed of the target car, the second coordinate is the speed of the host car, and the last coordinate is the distance between host and target (namely, location of the host minus location of the target on the road curve). The action to be taken by the host is the acceleration, and is denoted by at. We denote by \u03c4 the difference in time between consecutive rounds (in the experiment we set \u03c4 to be 0.1 seconds).\nDenote st = (v target t , v host t , xt) and denote by a target t the (unknown) acceleration of the target. The full dynamics of\nthe system can be described by:\nvtargett = [v target t\u22121 + \u03c4 a target t\u22121 ]+\nvhostt = [v host t\u22121 + \u03c4 at\u22121]+\nxt = [xt\u22121 + \u03c4 (v target t\u22121 \u2212 vhostt\u22121)]+\nThis can be described as a sum of two vectors:\nst = ([st\u22121[0] + \u03c4a target t\u22121 ]+, [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)\n= (st\u22121[0], [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)\ufe38 \ufe37\ufe37 \ufe38 DNNN (st\u22121,at) +([st\u22121[0] + \u03c4a target t\u22121 ]+ \u2212 st\u22121[0], 0, 0)\ufe38 \ufe37\ufe37 \ufe38 \u03bdt\nThe first vector is the predictable part and the second vector is the unpredictable part.\nThe reward on round t is defined as follows:\n\u2212rt = 0.1 |at| + [|xt/x\u2217t \u2212 1| \u2212 0.3]+ where x\u2217t = max{1, 1.5 vhostt }\nThe first term above penalizes for any non-zero acceleration, thus encourages smooth driving. The second term depends on the ratio between the distance to the target car, xt, and the desired distance, x\u2217t , which is defined as the maximum between a distance of 1 meter and brake distance of 1.5 seconds. Ideally, we would like this ratio to be exactly 1, but as long as this ratio is in [0.7, 1.3] we do not penalize the policy, thus allowing the car some slack (which is important for maintaining a smooth drive)."}, {"heading": "3.2 Merging into a Roundabout", "text": "In this experiment, the goal of the agent is to pass a roundabout. An episode starts when the agent is approaching the bottom entrance of the roundabout. The episode ends when the agent reaches the second exit of the roundabout, or after a fixed number of steps. A successful episode is measured first by keeping a safety distance from all other vehicles in the roundabout at all times. Second, the agent should finish the route as quickly as possible. And third, it should adhere a smooth acceleration policy. At the beginning of the episode, we randomly place NT target vehicles on the roundabout.\nTo model a blend of adversarial and typical behavior, with probability p, a target vehicle is modeled by an \u201caggressive\u201d driving policy, that accelerates when the host tries to merge in front of it. With probability 1 \u2212 p, the target vehicle is modeled by a \u201cdefensive\u201d driving policy that deaccelerate and let the host merge in. In our experiments we set p = 0.5. The agent has no information about the type of the other drivers. These are chosen at random at the beginning of the episode.\nWe represent the state as the velocity and location of the host (the agent), and the locations, velocities, and accelerations of the target vehicles. Maintaining target accelerations is vital in order to differentiate between aggressive and defensive drivers based on the current state. All target vehicles move on a one-dimensional curve that outlines the roundabout path. The host vehicle moves on its own one-dimensional curve, which intersects the targets\u2019 curve at the merging point, and this point is the origin of both curves. To model reasonable driving, the absolute value of all vehicles\u2019 accelerations are upper bounded by a constant. Velocities are also passed through a ReLU because driving backward is not allowed. Note that by not allowing driving backward we make long-term planning a necessity (the agent cannot regret on its past action).\nRecall that we decompose the next state, st+1, into a sum of a predictable part, DNNN (st, at), and a nonpredictable part, \u03bdt+1. In our first experiment, we let DNNN (st, at) be the dynamics of locations and velocities of all vehicles (which are well defined in a differentiable manner), while \u03bdt+1 is the targets\u2019 acceleration. It is easy to\nverify that DNNN (st, at) can be expressed as a combination of ReLU functions over an affine transformation, hence it is differentiable with respect to st and at. The vector \u03bdt+1 is defined by a simulator in a non-differentiable manner, and in particular implement aggressive behavior for some targets and defensive behavior for other targets. Two frames from the simulator are show in Figure 1. As can be seen in the supplementary videos1, the agent learns to slowdown as it approaches the entrance of the roundabout. It also perfectly learned to give way to aggressive drivers, and to safely continue when merging in front of defensive ones.\nOur second experiment is more ambitious: we do not tell the network the function DNNN (st, at). Instead, we express DNNN as another learnable part of our recurrent network. Besides the rewards for the policy part, we add a loss term of the form \u2016DNNN (st, at) \u2212 st+1\u20162, where st+1 is the actual next state as obtained by the simulator. That is, we learn the prediction of the near future, DNNN , and the policy that plan for the long term, \u03c0\u03b8, concurrently. While this learning task is more challenging, as can be seen in the supplementary videos, the learning process still succeeds."}, {"heading": "4 Discussion", "text": "We have presented an approach for learning driving polices in the presence of other adversarial cars using recurrent neural networks. Our approach relies on partitioning of the near future into a predictable part and an un-predictable part. We demonstrated the effectiveness of the learning procedure for two simple tasks: adaptive cruise control and roundabout merging. The described technique can be adapted to learning driving policies in other scenarios, such as lane change decisions, highway exit and merge, negotiation of the right of way in junctions, yielding for pedestrians, as well as complicated planning in urban scenarios."}], "references": [{"title": "Reinforcement learning with long short-term memory", "author": ["Bram Bakker"], "venue": "In NIPS, pages 1475\u20131482,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Dynamic programming and lagrange multipliers", "author": ["Richard Bellman"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1956}, {"title": "Introduction to the mathematical theory of control processes, volume", "author": ["Richard Bellman"], "venue": "IMA,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1971}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "R-max\u2013a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Iterative solution of games by fictitious play", "author": ["George W Brown"], "venue": "Activity analysis of production and allocation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1951}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A simple adaptive procedure leading to correlated", "author": ["S. HART", "A. MAS-COLELL"], "venue": "equilibrium. Econometrica,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Nash q-learning for general-sum stochastic games", "author": ["Junling Hu", "Michael P Wellman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Jens Kober", "J Andrew Bagnell", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Theory and application of reward shaping in reinforcement learning", "author": ["Adam Daniel Laud"], "venue": "PhD thesis, University of Illinois at Urbana-Champaign,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["Michael L Littman"], "venue": "In Proceedings of the eleventh international conference on machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Reinforcement Learning with Recurrent Neural Network", "author": ["Anton Maximilian Sch\u00e4fer"], "venue": "PhD thesis, Universitat Osnabruck,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Reinforcement learning in markovian and non-markovian environments", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1991}, {"title": "If multi-agent learning is the answer, what is the question", "author": ["Yoav Shoham", "Rob Powers", "Trond Grenager"], "venue": "Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Algorithms for reinforcement learning", "author": ["Csaba Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Learning to play the game of chess", "author": ["S. Thrun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Adversarial reinforcement learning", "author": ["William Uther", "Manuela Veloso"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "A survey of solution techniques for the partially observed markov decision process", "author": ["Chelsea C White III"], "venue": "Annals of Operations Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1991}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 9, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 21, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 22, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 11, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 168, "endOffset": 172}, {"referenceID": 1, "context": "Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3].", "startOffset": 156, "endOffset": 162}, {"referenceID": 2, "context": "Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3].", "startOffset": 156, "endOffset": 162}, {"referenceID": 11, "context": "First, as noted in [12], usually in robotics, we may only be able to find some approximate notion of a Markovian behaving state.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "One possible solution to this problem is to use partially observed MDPs [27], in which we still assume that there is a Markovian state, but we only get to see an observation that is distributed according to the hidden state.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "For example, the minimax-Q learning [14] or the Nash-Q learning [9].", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "For example, the minimax-Q learning [14] or the Nash-Q learning [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown\u2019s fictitious play [6], and vanishing regret learning algorithms [8, 7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown\u2019s fictitious play [6], and vanishing regret learning algorithms [8, 7].", "startOffset": 169, "endOffset": 175}, {"referenceID": 6, "context": "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown\u2019s fictitious play [6], and vanishing regret learning algorithms [8, 7].", "startOffset": 169, "endOffset": 175}, {"referenceID": 24, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 23, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 10, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 4, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 19, "context": "As noted in [20], learning in multi-agent setting is inherently more complex than in the single agent setting.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "For example, the deep-Q-networks (DQN) learning algorithm of [16] has been successful at playing Atari games.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "[21]), but they again rely on approximating the Q function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 16, "context": "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 18, "context": "A radically different approach has been introduced by Schmidhuber [19], who tackled the RL problem using a recurrent neural network (RNN).", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Following [19], there have been several additional algorithms that rely on RNNs for RL problems.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "For example, Backer [1] proposed to tackle the RL problem using recurrent networks with the LSTM architecture.", "startOffset": 20, "endOffset": 23}, {"referenceID": 17, "context": "Sch\u00e4fer [18] used RNN to model the dynamics of partially observed MDPs.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "Most notably is the REINFORCE framework of Williams [28].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "It has been recently successful for visual attention [15, 29].", "startOffset": 53, "endOffset": 61}, {"referenceID": 27, "context": "It has been recently successful for visual attention [15, 29].", "startOffset": 53, "endOffset": 61}, {"referenceID": 18, "context": "As already noted by [19], the ability of REINFORCE to estimate the derivative of stochastic units can be straightforwardly combined within the RNN framework.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "As noted in [19], explicitly expressing the dynamic of the system in a transparent way enables to incorporate prior knowledge more easily.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "st = ([st\u22121[0] + \u03c4a target t\u22121 ]+, [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "st = ([st\u22121[0] + \u03c4a target t\u22121 ]+, [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "st = ([st\u22121[0] + \u03c4a target t\u22121 ]+, [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "= (st\u22121[0], [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+) } {{ } DNNN (st\u22121,at) +([st\u22121[0] + \u03c4a target t\u22121 ]+ \u2212 st\u22121[0], 0, 0) } {{ } \u03bdt", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "= (st\u22121[0], [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+) } {{ } DNNN (st\u22121,at) +([st\u22121[0] + \u03c4a target t\u22121 ]+ \u2212 st\u22121[0], 0, 0) } {{ } \u03bdt", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "= (st\u22121[0], [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+) } {{ } DNNN (st\u22121,at) +([st\u22121[0] + \u03c4a target t\u22121 ]+ \u2212 st\u22121[0], 0, 0) } {{ } \u03bdt", "startOffset": 58, "endOffset": 61}], "year": 2016, "abstractText": "We consider planning problems, that often arise in autonomous driving applications, in which an agent should decide on immediate actions so as to optimize a long term objective. For example, when a car tries to merge in a roundabout it should decide on an immediate acceleration/braking command, while the long term effect of the command is the success/failure of the merge. Such problems are characterized by continuous state and action spaces, and by interaction with multiple agents, whose behavior can be adversarial. We argue that dual versions of the MDP framework (that depend on the value function and the Q function) are problematic for autonomous driving applications due to the non Markovian of the natural state space representation, and due to the continuous state and action spaces. We propose to tackle the planning task by decomposing the problem into two phases: First, we apply supervised learning for predicting the near future based on the present. We require that the predictor will be differentiable with respect to the representation of the present. Second, we model a full trajectory of the agent using a recurrent neural network, where unexplained factors are modeled as (additive) input nodes. This allows us to solve the long-term planning problem using supervised learning techniques and direct optimization over the recurrent neural network. Our approach enables us to learn robust policies by incorporating adversarial elements to the environment.", "creator": "LaTeX with hyperref package"}}}