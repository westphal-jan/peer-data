{"id": "1602.02070", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Compressive PCA for Low-Rank Matrices on Graphs", "abstract": "Randomized algorithms cumulonimbus reduce the throttled complexity under-privileged of low - luduena rank unmade recovery methods berghaus only 1,000-dollar w. r. tonen t smooth-talking dimension cortinarius p of a aiya big dataset $ antislavery Y \\ in \\ Re ^ {21.67 p \\ defensiveness times dely n} $. However, the white-rumped case of benefiting large hermsdorf n \u00bf is a2r cumbersome testaments to s\u016btra tackle 96.80 without arinze sacrificing wolyniec the recovery. 15:32 The vancouverites recently fresh introduced Fast Robust cambuur PCA pickup6thgraf on 10,390 Graphs (danino FRPCAG) altstadt approximates 7201 a 38-2 recovery method for celicas matrices which megalomaniacs are low - rank on graphs constructed outhwaite between margherita their fezziwig rows bernheim and davud columns. detikcom In spainish this subra paper we provide oxfordians a novel pinard framework, unaligned Compressive ngam PCA on Graphs ((954) CPCA) for mutineer an approximate recovery of djebar such data symbiotes matrices from telekomunikacja sampled measurements. We mkd introduce ndong a RIP condition recyclables for low - st\u00f6\u00f0 rank matrices nocton on graphs which decapolis enables foram efficient hoopster sampling 3.1-billion of the rows yellow and imagistic columns to perform FRPCAG 2,654 on the sampled matrix. aberaeron Several efficient, parallel subcommittees and sherbini parameter - tokayev free aeronautica decoders mulyasari are presented along with waives their recieved theoretical sobbed analysis kura for the o'donovan low - rank muhajer recovery nebraska-omaha and wahyu clustering salinger applications barma of PCA. whap On kiriwina a single runnymede core hitzfeld machine, CPCA gains homers a isw speed up sinisterly of coolbrands p / k brightwood over bizness FRPCAG, rdf where k & lt; & squarer lt; veerpalu p is the subspace atoc dimension. cethegus Numerically, bozek CPCA hinault can efficiently cluster 15:09 70, 000 verdant MNIST digits bezirke in less haircuts than a mid-1600s minute kodituwakku and recover vahld a roll-out low - gamm rank chi-square matrix 6.5-hectare of pepy size aem 10304 quetzal X vaziani 1000 jenco in 15 leuschner secs, brul\u00e9 which is fremont 6 rtc and 100 schwarz times 5.7-magnitude faster than FRPCAG pyralidae and 9.88 exact recovery.", "histories": [["v1", "Fri, 5 Feb 2016 15:51:34 GMT  (1418kb,D)", "https://arxiv.org/abs/1602.02070v1", null], ["v2", "Mon, 11 Apr 2016 10:51:25 GMT  (1418kb,D)", "http://arxiv.org/abs/1602.02070v2", null], ["v3", "Mon, 2 May 2016 13:49:40 GMT  (1426kb,D)", "http://arxiv.org/abs/1602.02070v3", null], ["v4", "Tue, 4 Oct 2016 08:35:35 GMT  (1879kb,D)", "http://arxiv.org/abs/1602.02070v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nauman shahid", "nathanael perraudin", "gilles puy", "pierre vandergheynst"], "accepted": false, "id": "1602.02070"}, "pdf": {"name": "1602.02070.pdf", "metadata": {"source": "CRF", "title": "Compressive PCA for Low-Rank Matrices on Graphs", "authors": ["Nauman Shahid", "Nathanael Perraudin", "Gilles Puy", "Pierre Vandergheynst"], "emails": ["pierre.vandergheynst}@epfl.ch,", "gilles.puy@gmail.com"], "sections": [{"heading": null, "text": "Index Terms\u2014Robust PCA, graph Laplacian, spectral graph theory, compressive sampling\nI. INTRODUCTION\nIn many applications in signal processing, computer vision and machine learning, the data has an intrinsic low-rank structure. One desires to extract this structure efficiently from the noisy observations. Robust Principal Component Analysis (RPCA) [7], a linear dimensionality reduction algorithm can be used to exactly describe a dataset lying on a single linear lowdimensional subspace. Low-rank Representation (LRR) [19], on the other hand can be used for data drawn from multiple linear subspaces. However, these methods suffer from two prominent problems:\n1) They do not recover non-linear low-rank structures. 2) They do not scale for big datasets Y \u2208 <p\u00d7n (large p\nand large n, where p is the number of features). Many high dimensional datasets lie intrinsically on a smooth and very low-dimensional manifold that can be characterized by a graph G between the data samples [4]. For a matrix\nAffiliation: Signal Processing Laboratory 2 (LTS2), EPFL STI IEL, Lausanne, CH-1015, Switzerland. Phone: +41 21 69 34754. \u2020 G. Puy contributed to this work while he was at INRIA (Rennes - Bretagne Atlantique, Campus de Beaulieu, FR-35042 Rennes Cedex, France). N.Shahid and N.Perraudin are supported by the SNF grant no. 200021 154350/1 for the project \u201cTowards signal processing on graphs\u201d.G.Puy was funded by the European Research Council, PLEASE project (ERC-StG-2011-277906).\nY \u2208 <p\u00d7n, a K-nearest neighbor undirected graph between the rows or columns of Y is denoted as G = (V, E), where E is the set of edges and V is the set of vertices. The first step in the construction of G consists of connecting each yi to its K nearest neighbors yj (using Euclidean distance), resulting in |E| connections. The yi correspond to rows of Y if the graph G is the row graph or to the columns if G is a column graph. The K-nearest neighbors are non-symmetric but a symmetric weighted adjacency matrix W is computed via a Gaussian kernel as Wij = exp(\u2212\u2016(yi\u2212 yj)\u201622/\u03c32) if yj is connected to yi or vice versa and 0 otherwise. Let D be the diagonal degree matrix of G which is given as: Dii = \u2211 jWij . Then, the combinatorial Laplacian that characterizes the graph G is defined as L = D \u2212 W and its normalized form as Ln = D\u22121/2(D \u2212W )D\u22121/2 [38].\nIt is imperative to represent such datasets as a function of the smooth variations of the non-linear manifold, rather than a linear subspace. We refer to such a representation as a non-linear low-rank structure. In this context, the graph eigenvectors serve as a legitimate tool to characterize the smooth variations of the manifold. Consider the example of a 2D circle embedded in a 3D space as shown in the left most plot of Fig. 1. The noisy version of this circle qualifies as an example of a non-linear low-rank (2D) manifold embedded in a high dimensional (3D) space. Ideally one would like to recover the 2D circle as shown in the rightmost plot of Fig. 1, however, RPCA just reduces the manifold to a point in the space. Extensions of RPCA and LRR such as Robust PCA on Graphs (RPCAG) [35] and Graph Regularized LRR (GLRR) [20] propose to incorporate graph regularization as a method to recover non-linear low-rank structures. These methods still suffer from the scalability problem for big datasets.\nRandomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8]. These works improve upon the computational complexity by reducing only the data dimension p but still scale in the same manner w.r.t n. The case of large n can be tackled by using the sampling schemes accompanied with Nystrom method [40]. However, this method works efficiently only for low-rank kernel matrices and does not recover the low-rank data matrix itself. Scalable extensions of LRR such as [45] exist but they focus only on the subspace clustering application. Recently, Aravkin et. al [1] proposed to speed-up RPCA and ease the parameter selection problem, however, the variational approach does not qualify\nar X\niv :1\n60 2.\n02 07\n0v 4\n[ cs\n.L G\n] 4\nO ct\n2 01\n6\nto represent the non-linear low-rank structures. How to tackle the case of big n and non-linearity simultaneously then?\nFor many machine learning applications involving big data, such as clustering, an approximate low-rank representation might suffice. The recently introduced Fast Robust PCA on Graphs (FRPCAG) [36] approximates a recovery method for non-linear low-rank datasets, which are called Low-rank matrices on graphs. Inspired by the underlying stationarity assumption [27], the authors introduce a joint notion of lowrankness for the features and samples (rows and columns) of a data matrix. More specifically, a low-rank matrix on graphs is defined as a matrix whose rows and columns belong to the span of the first few eigenvectors of the graphs constructed between its rows and columns.\nFRPCAG does not require an SVD and scales linearly with n. It relies on fast dual graph filtering operations which involve matrix vector multiplications and can be parallelized on a GPU in every iteration. However, the size of the problem is still an issue for big datasets because the problem cannot be broken down into small sub-problems and the solution merged at the end. Thus, for the non-GPU implementation, it still suffers from 1) memory requirements 2) cost of k-means for clustering 3) the cost of parameter tuning for large p and large n and 4) scalability for very big datasets. This said, sometimes one might not even have access to the full dataset Y . This is typical, for instance for the biomedical applications, such as MRI and tomography. In such applications the number of observations are limited by the data acquisition protocols. In MRI, the number of observations is proportional to the time and dose required for the procedure. In tomography one might have access to the projections only. Thus, FRPCAG is not be usable if 1) the dataset is large and 2) only a subset of the dataset or measurements are available. Despite the above limitations of the data acquisition, one might have access to some additional information about the unobserved samples. In MRI for instance, sparsity of the samples in the Fourier domain serves as a good prior."}, {"heading": "A. The Problem Statement", "text": "In this work we answer the following questions: 1) What would be an efficient and highly scalable recovery framework, involving compression, for datasets which are jointly low-rank\non two manifolds? 2) Alternatively, given a few randomly sampled observations and features from a data matrix Y \u2208 <p\u00d7n, is it possible to efficiently recover the complete nonlinear low-rank representation? We mostly limit ourselves to the case 1 above, where a graphical prior is available or can be conveniently constructed for the complete set of observations for the application under consideration. A brief initial treatment of the 2nd case constitutes Section VII.C of this work."}, {"heading": "B. Contributions", "text": "PCA has been widely used for two different types of applications: 1) Low-rank recovery and 2) clustering in the low-dimensional space. It is crucial to point out here that the clustering is not a standard application of PCA, because PCA is just a feature extraction method. However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14]. Thus, to be consistent with the state-of-the-art, our contributions focus on both of the above applications. Below we describe our contributions in detail.\n1. Sampling & RIP for low-rank matrices on graphs: To solve the scalability problem of FRPCAG we propose to perform a dual uniform sampling of the data matrices, along rows and columns. We present a restricted isometry property (RIP) for low-rank matrices on graphs and relate it to the cumulative coherence of the graph eigenvectors. FRPCAG is then used to recover the low-rank representation for the sampled data.\n2. Decoders for low-rank recovery: We present two (ideal and alternate) convex and efficient decoders for recovering the full low-rank matrix from the corresponding low-rank matrix of the sampled data. However, our main contribution comprises the set of 3 additional parallel, low-cost and parameterfree approximate decoders, which significantly boost the speed of our framework by introducing a few approximations. Our rigorous theoretical analysis also proves that the recovery error of the above decoders depends on the spectral gaps of the row and column graph Laplacians.\n3. Low-Rank Clustering: For the clustering application of PCA, we propose a low-cost and parallel scheme based on\nCPCA. The key idea is to decode the labels of the complete dataset from the labels of a sampled low-rank dataset, without computing the complete low-rank matrix.\n4. Extensive Experimentation: Low-rank recovery experiments on 3 real video datasets and clustering experiments on 5 benchmark datasets reveal that the performance of our model is comparable to 10 different state-of-the-art PCA and nonPCA based methods. We also study some cases where CPCA fails to perform as well as the state-of-the-art.\nOur proposed framework is inspired by the recently introduced sampling of band-limited signals on graphs [28]. While we borrow several concepts from here, our framework is significantly different from [28] in many contexts. We target the low-rank recovery of matrices, whereas [28] targets the recovery of band-limited signals / vectors. For our framework it is important for the data matrix to be low-rank jointly on the row and column graphs. Thus, our sampling scheme and RIP are generalized for two graphs. The design of a sampling scheme is the major focus of [28], while we just focus on the case of uniform sampling and instead focus on how much to sample jointly given the two graphs. Of course, our method can be extended directly for the other sampling schemes in [28]. A major difference lies in the application domain and hence the experiments. Unlike [28], we target two applications related to PCA: 1) low-rank recovery and 2) clustering. Thus, contrary to [28] our proposed decoders are designed for these applications. A major contribution of our work in contrast to [28] is the design of approximate decoders for low-rank recovery and clustering which significantly boost the speed of our framework for big datasets without compromising on the performance.\nII. A GLIMPSE OF COMPRESSIVE PCA (CPCA) Let Lc \u2208 Rn\u00d7n be the Laplacian of the graph Gc connecting the different columns of Y and Lr \u2208 Rp\u00d7p the Laplacian of the graph Gr that connects the rows of Y . Furthermore, let Lc = Q\u039bcQ> = Qkc\u039bckcQ>kc + Q\u0304kc\u039b\u0304ckcQ\u0304 > kc\n, where \u039bckc \u2208 <kc\u00d7kc is a diagonal matrix of lower eigenvalues and \u039b\u0304ckc \u2208 <(n\u2212kc)\u00d7(n\u2212kc) is a diagonal matrix of higher graph eigenvalues. Similarly, let Lr = P\u039brP> = Pkr\u039brkrP>kr + P\u0304kr \u039b\u0304rkr P\u0304 > kr\n. All the values in \u039br and \u039bc are sorted in increasing order. For a K-nearest neighbors graph constructed from kc-clusterable data (along columns) one can expect \u03bbkc/\u03bbkc+1 \u2248 0 as \u03bbkc \u2248 0 and \u03bbkc \u03bbkc+1. We refer to the ratio \u03bbkc/\u03bbkc+1 as the spectral gap of Lc. The same holds for the Laplacian Lr. Then, low-rank matrices on graphs can be defined as following and recovered by solving FRPCAG [36].\nDefinition 1. A matrix Y \u2217 \u2208 Rp\u00d7n is (kr, kc)-low-rank on the graphs Lr and Lc if its columns yj \u2208 span(Pkr ) for all j = 1, . . . , n and its rows yi \u2208 span(Qkc) for all i = 1, . . . , p. The set of (kr, kc)-low-rank matrices on the graphs Lr and Lc is denoted by LR(Pkr , Qkc).\nGiven a data matrix Y \u2208 <p\u00d7n = X\u0304 + E\u0304, where X\u0304 \u2208 LR(Pkr , Qkc) and E\u0304 models the errors, the goal is to develop a method to efficiently recover X\u0304 . We propose to 1) Construct\nLaplacians Lr and Lc between the rows and columns of Y using the scheme of Section II. 2) Sample the rows and columns of Y to get a subsampled matrix Y\u0303 = Y\u0303 \u2217 + E using the sampling scheme of Section III. 3) Construct the compressed Laplacians L\u0303r, L\u0303c from Lr,Lc (Section IV-A). 4) Determine a low-rank matrix X\u0303 for Y\u0303 with L\u0303r, L\u0303c in algorithm 1 of FRPCAG:\nmin X\u0303\n\u03c6(Y\u0303 \u2212 X\u0303) + \u03b3c tr(X\u0303L\u0303cX\u0303>) + \u03b3r tr(X\u0303>L\u0303rX\u0303),\nwhere \u03c6 is a loss function (possibly lp norm), X\u0303 = X\u0303\u2217 + E\u0303 = MrX\u0304Mc + E\u0303, E\u0303 models the errors in the recovery of the subsampled low-rank matrix X\u0303 and Mr,Mc are the row and column sampling matrices whose design is discussed in Section III. 5) Use the decoders presented in Section V to decode the low-rank matrix X\u0304 = X\u0304\u2217+E\u2217 (where E\u2217 denotes the error on the recovery of optimal X\u0304\u2217) on graphs Lr,Lc if the task is low-rank recovery, or perform k-means on X\u0303 to get cluster labels C\u0303 and use the clustering algorithm (presented in Section VI) to get the cluster labels C for the full matrix X .\nThroughout this work we use the approximate nearest neighbor algorithm (FLANN [21]) for graph construction whose complexity is O(np log(n)) for p n [33] (and it can be performed in parallel).\nIII. RIP FOR LOW-RANK MATRICES ON GRAPHS\nLet Mr \u2208 <\u03c1r\u00d7p be the subsampling matrix for sampling the rows and Mc \u2208 <n\u00d7\u03c1c for sampling the columns of Y . Mc and Mr are constructed by drawing \u03c1c and \u03c1r indices \u2126c = {\u03c91 \u00b7 \u00b7 \u00b7\u03c9\u03c1c} and \u2126r = {\u03c91 \u00b7 \u00b7 \u00b7\u03c9\u03c1r} uniformly without replacement from the sets {1, 2, \u00b7 \u00b7 \u00b7 , n} and {1, 2, \u00b7 \u00b7 \u00b7 , p} and satisfy:\nM ijc = { 1 if i = \u03c9j 0 otherwise M ij r = { 1 if j = \u03c9i 0 otherwise. (1)\nNow, the subsampled data matrix Y\u0303 \u2208 <\u03c1c\u00d7\u03c1r can be written as Y\u0303 = MrYMc. CPCA requires Mr and Mc to be constructed such that the \u201clow-rankness\u201d property of the data Y is preserved under sampling. Before discussing this, we introduce a few basic definitions in the context of graphs Gc and Gr.\nDefinition 2. (Graph cumulative coherence). The cumulative coherence of order kc, kr of Gc and Gr is:\n\u03bdkc = max 1\u2264i\u2264n\n\u221a n\u2016Q>kc\u2206 c i\u20162 & \u03bdkr = max\n1\u2264j\u2264p\n\u221a p\u2016P>kr\u2206 r j\u20162,\nwhere \u2206c \u2208 {0, 1}n,\u2206r \u2208 {0, 1}p are binary vectors and \u2206ci = 1 if the i\nth entry of \u2206c is 1 and 0 otherwise. Thus, \u2206ci corresponds to a specific node of the graph.\nIn the above equations Q>kc\u2206 c i and P > kr \u2206rj characterize the first kc and kr fourier modes [38] of the nodes i and j on the graphs Gc and Gr respectively. Thus, the cumulative coherence is a measure of how well the energy of the (kr, kc) low-rank matrices spreads over the nodes of the graphs. These quantities exactly control the number of vertices \u03c1c and \u03c1r that\nneed to be sampled from the graphs Gr and Gc such that the properties of the graphs are preserved [28].\nConsider the example where a particular node i has a high coherence. Then, it implies that their exist some low-rank signals whose energy is highly concentrated on the node i. Removing this node would result in a loss of information in the data. If the coherence of this node is low then removing it in the sampling process would result in no loss of information. We already mentioned that we are interested in the case of uniform sampling. Therefore in order to be able to sample a small number of nodes uniformly from the graphs, the cumulative coherence should be as low as possible.\nWe remind that for our application we desire to sample the data matrix Y such that its low-rank structure is preserved under this sampling. How can we ensure this via the graph cumulative coherence? This follows directly from the fact that we are concerned about the data matrices which are also low-rank with respect to the two graphs under consideration Y \u2208 LR(Pkr , Qkc). In simple words, the columns of the data matrix Y belong to the span of the eigenvectors Pkr and the rows to the span of Qkc . Thus, the coherence conditions for the graph directly imply the coherence condition on the data matrix Y itself. Therefore, using these quantities to sample the data matrix Y will ensure the preservation of two properties under sampling: 1) the structure of the corresponding graphs and 2) the low-rankness of the data matrix Y . Given the above definitions, we are now ready to present the restricted-isometry theorem for the low-rank matrices on the graphs.\nTheorem 1. (Restricted-isometry property (RIP) for low-rank matrices on graphs) Let Mc and Mr be two random subsampling matrices as constructed in (1). For any \u03b4, \u2208 (0, 1), with probability at least 1\u2212 ,\n(1\u2212 \u03b4)\u2016Y \u20162F \u2264 np\n\u03c1r\u03c1c \u2016MrYMc\u20162F \u2264 (1 + \u03b4)\u2016Y \u20162F (2)\nfor all Y \u2208 LR(Pkr , Qkc) provided that\n\u03c1c \u2265 27\n\u03b42 \u03bd2kc log\n( 4kc ) & \u03c1r \u2265 27\n\u03b42 \u03bd2kr log\n( 4kr ) , (3)\nwhere \u03bdkc , \u03bdkr characterize the graph cumulative coherence as in Definition 2 and np\u03c1c\u03c1r is just a normalization constant which quantifies the norm conservation in (2).\nProof: Please refer to Appendix A. Theorem 1 is a direct extension of the RIP for k-bandlimited signals on one graph [28]. It states that the information in Y \u2208 LR(Pkr , Qkc) is preserved with overwhelming probability if the sampling matrices (1) are constructed with a uniform sampling strategy satisfying (3). Note that \u03c1r and \u03c1c depend on the cumulative coherence of the graph eigenvectors. The better spread the eigenvectors are, the smaller is the number of vertices that need to be sampled.\nIt is proved in [28] that \u03bdkc \u2265 \u221a kc and \u03bdkr \u2265 \u221a kr. Hence, when the lower bounds are attained, one only needs to sample an order of O(kc log(kc)) columns and O(kr log(kr)) rows to ensure that the RIP (eq. (3)) holds. This is the ideal scenario.\nHowever, one can also have \u03bdkc = \u221a n or \u03bdkr = \u221a p in some situations. Let us give some examples. The lower bound on \u03bdk is attained, e.g, when the graph is the regular lattice. In this case the graph Fourier transform is the \u201cusual\u201d Fourier transform and \u03bdk = \u221a k for all k. Another example where the lower bound is attained is when the graph contains k disconnected components of identical size. In this case, one can prove that \u03bdk = \u221a k. Intuitively, we guess that the coherence remains close to this lower bound when these k components are weakly interconnected.\nThe upper bound on \u03bdk is attained when, for example, the graph has one of its nodes not connected to any other node. In this case, one must sample this node. Indeed, there is no way to guess the value of the signal on this node from any neighbour. As the sampling is random, one is sure to sample this node only when all the nodes are sampled. Uniform sampling is not the best strategy in this setting. Furthermore, note that such a case is only possible if the graph is noisy or the data has strong outliers. One should resort to a more distribution aware sampling in such a case as presented in [28].\nWe choose in this paper to present the results using a uniform distribution for simplicity. Note however that one can adapt the sampling distribution to the underlying structure of the graph to ensure optimal sampling results. A consequence of the result in [28] is that there always exist distributions that ensure that the RIP holds when sampling O(kr log(kr)) rows and O(kc log(kc)) columns only. The optimal sampling distribution for which this result holds is defined in [28] (see Section 2.2). Furthermore, a fast algorithm to compute this distribution also exists (Section 4 of [28]).\nIV. COMPRESSED LOW-RANK MATRIX Once the compressed dataset Y\u0303 \u2208 <\u03c1r\u00d7\u03c1c is obtained the low-rank representation has to be extracted which takes into account the graph structures. Thus we propose the following two step strategy:\n1) Construct graphs for compressed data. 2) Run Fast Robust PCA on Graphs (FRPCAG) on the\ncompressed data. These two steps are elaborated in the following subsections."}, {"heading": "A. Graphs for Compressed data", "text": "To ensure the preservation of algebraic and spectral properties one can construct the compressed Laplacians L\u0303r \u2208 <\u03c1r\u00d7\u03c1r and L\u0303c \u2208 <\u03c1c\u00d7\u03c1c from the Kron reduction of Lr and Lc [9]. Let \u2126 be the set of sampled nodes and \u2126\u0304 the complement set and let L(Ar, Ac) denote the (row, column) sampling of L w.r.t sets Ar, Ac then the Laplacian L\u0303c for the columns of compressed matrix Y\u0303 is:\nL\u0303c = Lc(\u2126,\u2126)\u2212 Lc(\u2126, \u2126\u0304)L\u22121c (\u2126\u0304, \u2126\u0304)Lc(\u2126\u0304,\u2126).\nLet Lc has kc connected components or \u03bbkc/\u03bbkc+1 \u2248 0. Then, as argued in theorem III.4 of [9] two nodes \u03b1, \u03b2 are not connected in L\u0303c if there is no path between them in Lc via \u2126\u0304. Assume that each of the connected components has the same number of nodes. Then, if the sampling is done uniformly\nwithin each of the connected components according to the sampling bounds described in eq.(3), one can expect L\u0303c to have kc connected components as well. This is an inherent property of the Kron reduction method. However, for the case of large variation of the number of nodes among the connected components one might want to resort to a more distribution aware sampling scheme. Such schemes have been discussed in [28] and have not been addressed in this work. Nevertheless, the Kron reduction strategy mentioned here is independent of the sampling strategy used. The same concepts holds for L\u0303r as well.\nThe Kron reduction method involves the multiplication of 3 sparse matrices. The only expensive operation above is the inverse of L(\u2126\u0304, \u2126\u0304) which can be performed with O(OlKn) cost using the Lancoz method [39], where Ol is the number of iterations for Lancoz approximation."}, {"heading": "B. FRPCAG on the Compressed Data", "text": "Once the Laplacians L\u0303r \u2208 <\u03c1r\u00d7\u03c1r , L\u0303c \u2208 <\u03c1c\u00d7\u03c1c are obtained, the next step is to recover the low-rank matrix X\u0303 \u2208 <\u03c1r\u00d7\u03c1c . Let L\u0303c = Q\u0303\u039b\u0303cQ\u0303> = Q\u0303kc\u039b\u0303ckcQ\u0303>kc + \u00af\u0303Qkc \u00af\u0303\u039bckc\n\u00af\u0303Q>kc , where \u039b\u0303kc \u2208 <kc\u00d7kc is a diagonal matrix of lower eigenvalues and \u00af\u0303\u039bkc \u2208 <(\u03c1c\u2212kc)\u00d7(\u03c1c\u2212kc) is a diagonal matrix of higher graph eigenvalues. Similarly, let L\u0303r = P\u0303 \u039b\u0303rP\u0303> = P\u0303kr \u039b\u0303rkr P\u0303 > kr + \u00af\u0303Pkr \u00af\u0303\u039brkr\n\u00af\u0303P>kr . Furthermore assume that all the values in \u039b\u0303r and \u039b\u0303c are sorted in increasing order.\nAssume Y\u0303 = Y\u0303 \u2217 + E, where E models the noise in the compressed data and Y\u0303 \u2217 \u2208 LR(P\u0303kr , Q\u0303kc). The low-rank matrix X\u0303 = X\u0303\u2217+E\u0303 can be recovered by solving the FRPCAG problem as proposed in [36] and re-written below:\nmin X\u0303\n\u03c6(Y\u0303 \u2212 X\u0303) + \u03b3c tr(X\u0303L\u0303cX\u0303>) + \u03b3r tr(X\u0303>L\u0303rX\u0303), (4)\nwhere \u03c6 is a proper, positive, convex and lower semicontinuous loss function (possibly lp norm). From Theorem 1 in [36], the low-rank approximation error comprises the orthogonal projection of X\u0303\u2217 on the complement graph eigenvectors ( \u00af\u0303Qkc ,\n\u00af\u0303Pkr ) and depends on the spectral gaps \u03bb\u0303kc/\u03bb\u0303kc+1, \u03bb\u0303kr/\u03bb\u0303kr+1 as following:\n\u2016X\u0303\u2217 \u00af\u0303Qkc\u20162F + \u2016 \u00af\u0303P>krX\u0303 \u2217\u20162F = \u2016E\u0303\u20162F\n\u2264 1 \u03b3 \u03c6(E) + \u2016Y\u0303 \u2217\u20162F ( \u03bb\u0303kc \u03bb\u0303kc+1 + \u03bb\u0303kr \u03bb\u0303kr+1 ) , (5)\nwhere \u03b3 depends on the signal-to-noise ratio. Clearly, if \u03bbkc/\u03bbkc+1 \u2248 0 and \u03bbkr/\u03bbkr+1 \u2248 0 and the compressed Laplacians are constructed using the Kron reduction then \u03bb\u0303kc/\u03bb\u0303kc+1 \u2248 0 and \u03bb\u0303kr/\u03bb\u0303kr+1 \u2248 0. Thus, exact recovery is attained.\nLet g(Z) = \u03b3c tr(ZLcZ>)+\u03b3r tr(Z>LrZ), then \u2207g(Z) = 2(\u03b3cZLc + \u03b3rLrZ). Also define prox\u03bbh(Z) = Y + sgn(Z \u2212 Y ) \u25e6 max(|Z \u2212 Y | \u2212 \u03bb, 0), where \u25e6 denotes the Hadamard product, \u03bb as the step size (we use \u03bb = 1\u03b2\u2032 ), where \u03b2 \u2264 \u03b2\u2032 = 2\u03b3c\u2016Lc\u20162 +2\u03b3r\u2016Lr\u20162 and \u2016L\u20162 is the spectral norm (or maximum eigenvalue) of L, as the stopping tolerance and J the maximum number of iterations. Then FRPCAG can be solved by the FISTA in Algorithm 1.\nAlgorithm 1 FISTA for FRPCAG INPUT: Z1 = Y , S0 = Y , t1 = 1, > 0 for j = 1, . . . J do\nSj = prox\u03bbjh(Zj \u2212 \u03bbj\u2207g(Zj))\ntj+1 = 1+ \u221a 1+4t2j 2 Zj+1 = Sj + tj\u22121 tj+1\n(Sj \u2212 Sj\u22121) if \u2016Zj+1 \u2212 Zj\u20162F < \u2016Zj\u20162F then\nBREAK end if end for OUTPUT: Uj+1"}, {"heading": "V. DECODERS FOR LOW-RANK RECOVERY", "text": "Let X\u0303 \u2208 <\u03c1r\u00d7\u03c1c be the low-rank solution of (4) with the compressed graph Laplacians L\u0303r, L\u0303c and sampled data Y\u0303 . The goal is to decode the low-rank matrix X \u2208 <p\u00d7n for the full Y . We assume that X\u0303 = MrX\u0304Mc + E\u0303, where E\u0303 \u2208 <\u03c1r\u00d7\u03c1c models the noise incurred by (4).\nA. Ideal Decoder A straight-forward way to decode X on the original graphs Lr and Lc, when one knows the basis Pkr , Qkc involves solving the following optimization problem:\nmin X \u2016MrXMc \u2212 X\u0303\u20162F s.t: (X)i \u2208 span(Pkr ), (X>)j \u2208 span(Qkc). (6)\nTheorem 2. Let Mr and Mc be such that (2) holds and X\u2217 be the solution of (6) with X\u0303 = MrX\u0304Mc + E\u0303, where X\u0304 \u2208 LR(Pkr , Qkc) and E\u0303 \u2208 <\u03c1r\u00d7\u03c1c . We have:\n\u2016X\u2217 \u2212 X\u0304\u2016F \u2264 2 \u221a\nnp\n\u03c1c\u03c1r(1\u2212 \u03b4) \u2016E\u0303\u2016F , (7)\nwhere \u221a np/\u03c1c\u03c1r(1\u2212 \u03b4) is a constant resulting from the norm preservation in (2) and \u2016E\u0303\u20162F is bounded by eq. (5).\nProof: Please refer to Appendix B. Thus, the error of the ideal decoder is only bounded by the error E\u0303 in the low-rank matrix X\u0303 obtained by solving (4). In fact E\u0303 depends on the spectral gaps of L\u0303c, L\u0303r, as given in eq. (5). Hence, the ideal decoder itself does not introduce any error in the decode stage. The solution for this decoder requires projecting over the eigenvectors P and Q of Lr and Lc. This is computationally expensive because diagonalization of Lr and Lc cost O(p3) and O(n3). Moreover, the constants kr, kc are not known beforehand and require tuning."}, {"heading": "B. Alternate Decoder", "text": "As the ideal decoder is computationally costly, we propose to decode X from X\u0303 by using a convex and computationally tractable problem which involves the minimization of graph dirichlet energies.\nmin X \u2016MrXMc \u2212 X\u0303\u20162F + \u03b3\u0304c tr(XLcX>) + \u03b3\u0304r tr(X>LrX).\n(8)\nTheorem 3. Let Mr and Mc be such that (2) holds and \u03b3 > 0. Let also X\u2217 be the solution of (8) with \u03b3\u0304c = \u03b3/\u03bbkc+1, \u03b3\u0304r = \u03b3/\u03bbkr+1, and X\u0303 = MrX\u0304Mc + E\u0303, where X\u0304 \u2208 LR(Pkr , Qkc) and E\u0303 \u2208 <\u03c1r\u00d7\u03c1c . We have:\n\u2016X\u0304\u2217 \u2212 X\u0304\u2016F \u2264 \u221a\nnp\n\u03c1c\u03c1r(1\u2212 \u03b4)\n[( 2 +\n1\u221a 2\u03b3\n) \u2016E\u0303\u2016F+\n( 1\u221a 2 + \u221a \u03b3) \u221a( \u03bbkc \u03bbkc+1 + \u03bbkr \u03bbkr+1 ) \u2016X\u0304\u2016F ] , and\n\u2016E\u2217\u2016F \u2264 \u2016E\u0303\u2016F\u221a\n2\u03b3 + 1\u221a 2 \u221a( \u03bbkc \u03bbkc+1 + \u03bbkr \u03bbkr+1 ) \u2016X\u0304\u2016F , (9)\nwhere X\u0304\u2217 = ProjLR(Pkr ,Qkc )(X) and E \u2217 = X\u2217 \u2212 X\u0304\u2217. ProjLR(Pkr ,Qkc )(.) denotes the orthogonal projection onto LR(Pkr , Qkc) and \u03b3 depends on the signal to noise ratio.\nProof: Please refer to Appendix C Theorem 3 states that in addition to the error E\u0303 in X\u0303 incurred by (4) and characterized by the bound in eq. (5), the error of the alternate decoder (8) also depends on the spectral gaps of the Laplacians Lr and Lc respectively. This is the price that one has to pay in order to avoid the expensive ideal decoder. For a kr, kc clusterable data Y across the rows and columns, one can expect \u03bbkr/\u03bbkr+1 \u2248 0 and \u03bbkc/\u03bbkc+1 \u2248 0 and the solution is as good as the ideal decoder. Nevertheless, it is possible to reduce this error by using graph filters g such that the ratios g(\u03bbkc)/g(\u03bbkc+1) and g(\u03bbkr )/g(\u03bbkr+1) approach zero. However, we do not discuss this approach in our work. It is trivial to solve (8) using a conjugate gradient scheme that costs O(InpK), where I is the number of iterations for the algorithm to converge."}, {"heading": "C. Approximate Decoder", "text": "The alternate decoder proposed above has the following disadvantages: 1) It is almost as computationally expensive as FRPCAG 2) It requires tuning two model parameters.\nIn this section we describe the step-by-step construction of an approximate decoder which overcomes these limitations. The main idea is to breakdown the decode phase of low-rank matrix X into its left and right singular vectors or subspaces. Let X = U\u03a3V > and X\u0303 = U\u0303 \u03a3\u0303V\u0303 > be the SVD of X and X\u0303 . We propose to recover U from U\u0303 and V from V\u0303 in 3 steps.\n1) Split the alternate decoder to subspace learning problems. 2) Drop the orthonormality constraints on subspaces. 3) Run an efficient upsampling algorithm to solve the\nproblem of step 2.\nThe goal of this step-by-step approach is to guide the reader throughout to observe the close relationship between the alternate and approximate decoder. Now we begin to describe these steps in detail.\n1) Step 1: Splitting the alternate decoder: Using the SVD of X and X\u0303 and the invariance property of the trace under\ncyclic permutations, we can replace (8) by:\nmin U,V \u2016MrU\u03a3V >Mc \u2212 U\u0303 \u03a3\u0303V\u0303 >\u20162F + \u03b3\u0304c tr(\u03a32V >LcV )+\n\u03b3\u0304r tr(U >LrU\u03a32) s.t: U>U = Ik, V >V = Ik. (10)\nThe above eq. introduces two new variables based on the SVD of X , i.e, U \u2208 Rp\u00d7k and V \u2208 Rn\u00d7k. Clearly, with the introduction of these new variables, one needs to specify k as the dimension of the subspaces U and V . We propose the following strategy for this:\n1) First, determine \u03a3\u0303 by one inexpensive SVD of X\u0303 \u2208 R\u03c1r\u00d7\u03c1c . This costs O(\u03c12r\u03c1c) for \u03c1r < \u03c1c. 2) Then set k equal to the number of entries in \u03a3\u0303 which are above a threshold.\nIt is important to note that so far eq.(10) and the alternate decoder eq.(8) are equivalent. Also note that we did not introduce the singular values \u03a3 as an additional variable in eq.(10) because they are related to the singular values X\u0303 of X\u0303 . We argue this as following: If (9) holds for the alternate decoder then \u2016\u03a3\u0304\u2217\u2212 \u03a3\u0304\u2016F (where \u03a3\u0304\u2217, \u03a3\u0304 are the singular values of X\u0304\u2217, X\u0304) is also bounded as argued in the discussion of Appendix C. Thus, the singular values \u03a3 and \u03a3\u0303 of X and X\u0303 differ approximately by the normalization constant of theorem 1, i.e,\n\u03a3 =\n\u221a np\n\u03c1r\u03c1c(1\u2212 \u03b4) \u03a3\u0303\nNote that with the above relationship, the subspaces U, V can be solved independently of each other. Thus eq.(10) can be decoupled as following which separately solves the subspace (U and V ) learning problems.\nmin U \u2016MrU \u2212 U\u0303\u20162F + \u03b3\n\u2032\nr tr(U >LrU) s.t: U>U = Ik,\nmin V \u2016V >Mc \u2212 V\u0303 \u20162F + \u03b3\n\u2032\nc tr(V >LcV ) s.t: V >V = Ik.\n(11)\n2) Step 2: Dropping Orthonormality Constraints: Solving (11) is as expensive as (8) due to the orthonormality constraints (as explained in appendix D). Therefore, we drop the constraints and get\nmin U \u2016MrU \u2212 U\u0303\u20162F + \u03b3\n\u2032\nr tr(U >LrU), (12)\nmin V \u2016V >Mc \u2212 V\u0303 \u20162F + \u03b3\n\u2032\nc tr(V >LcV ). (13)\nThe solutions to (12) & (13) are not orthonormal anymore. The deviation from the orthonormality depends on the constants \u03b3 \u2032 r and \u03b3 \u2032 c, but X = U\u03a3V > is still a good enough (error characterized in Theorem 4) low-rank representation due to the intuitive explanation that we present here. We argue that the solutions of eqs.(12) &(13) are feasible solutions of the joint non-convex, factorized, and graph regularized low-rank optimization problem like the one presented in [31]. Let A\nand B be the subspaces that we want to recover then we can re-write the problem studied in [31] as following:\nmin A,B \u2016MrAB>M>c \u2212 X\u0303\u20162F + \u03b3\n\u2032 r tr(A >LrA) + \u03b3 \u2032 c tr(B >LcB)\nThe above non-convex problem does not require A and B to be orthonormal, but is still widely used for recovering a low-rank X = AB>. Our problem setting (eqs.(12) &(13)) is just equivalent except that it is convex as we decouple the learning of two subspaces due to the known \u03a3 that relates U and V . Thus, for any orthonormal U, V and a scaling matrix \u03a3, A = U \u221a \u03a3 and B = V \u221a \u03a3 is a feasible solution. Thus, dropping the orthonormality constraints does not effect the final solution X .\n3) Step 3: Subspace Upsampling: Eqs. (12) &(13) require the tuning of two parameters \u03b3 \u2032 r and \u03b3 \u2032\nc which can be computationally cumbersome. Therefore, our final step in the construction of the approximate decoder is to get rid of the two parameters. But before we present the final construction step we study the problems eqs.(12) &(13) and their solutions more closely.\nFirst, note that solving eqs.(12) &(13) is equivalent to making the following assumptions:\nU\u0303 = MrU\u0304 + E\u0303 u and V\u0303 = V\u0304 Mc + E\u0303v,\nwhere the columns of U\u0304 , u\u0304i \u2208 span(Pkr ), i = 1, \u00b7 \u00b7 \u00b7 , p, and the columns of V\u0304 v\u0304j \u2208 span(Qkc), j = 1, \u00b7 \u00b7 \u00b7 , n and E\u0303u \u2208 <\u03c1r\u00d7\u03c1r , E\u0303v \u2208 <\u03c1c\u00d7\u03c1c model the noise in the estimate of the subspaces.\nSecondly, the closed form solutions of eqs.(12) &(13) are given as following:\nU = (M>r Mr + \u03b3 \u2032 rLr)\u22121M>r U\u0303 , (14)\nV = (McM > c + \u03b3\n\u2032 cLc)\u22121McV\u0303 . (15)\nThus, problems (12) & (13) decode the subspaces U and V such that they are smooth on their respective graphs Lr and Lc. This can also be referred to as 1) simultaneous decoding and 2) subspace denoising stage. We call it a \u2018subspace denoising\u2019 method because the operator (M>r Mr + \u03b3 \u2032\nrLr)\u22121 can be viewed as low-pass filtering the subspace U in the graph fourier domain.\nNote that we want to decode and denoise U\u0303 and V\u0303 which are in turn determined by the SVD of X\u0303 . Furthermore, X\u0303 has been determined by solving the FRPCAG problem of eq.(4). FRPCAG is already robust to noise and outliers, therefore, it is safe to assume that the subspaces determined from it, i.e, U\u0303 and V\u0303 are also noise and outlier free. Thus, the extra denoising step (performed via graph filtering) of eqs.(12) &(13) is redundant.\nTherefore, we can directly upsample U\u0303 and V\u0303 to determine U and V without needing a margin for noise. To do this, we reformulate eq.(12) as follows:\nmin U\n1\n\u03b3\u2032r \u2016MrU \u2212 U\u0303\u20162F + tr(U>LrU).\nFor \u03b3 \u2032\nr \u2192 0, 1\u03b3\u2032r \u2192 \u221e, the emphasis on first term of the objective increases and it turns to an equality constraint MrU = U\u0303 . The same holds for eq.(13) as well. Thus, the modified problems are:\nmin U tr(U>LrU) and min V tr(V >LcV ) s.t: MrU = U\u0303 , s.t: M>c V = V\u0303 . (16)\nNote that now we have a parameter-free decode stage. It is important now to study the theoretical guarantees on eq. (16). To do this, as eqs. (16) are a specific case of eqs. (12) & (13), we first study the guarantees on eqs. (12) & (13) in Theorem 4. Then, based on this study we directly present the guarantees on the final approximate decoder of eq. (16) in Theorem 5.\nTheorem 4. Let Mr and Mc be such that (2) holds and \u03b3\u2032r, \u03b3 \u2032 c > 0. Let also U\n\u2217 and V \u2217 be respectively the solutions of (12) and (13) with U\u0303 = MrU\u0304 + E\u0303u and V\u0303 = McV\u0304 + E\u0303v , where u\u0304i \u2208 span(Pkr ), i = 1, \u00b7 \u00b7 \u00b7 , p, v\u0304j \u2208 span(Qkc), j = 1, \u00b7 \u00b7 \u00b7 , n, E\u0303u \u2208 <\u03c1r\u00d7\u03c1r , E\u0303v \u2208 <\u03c1c\u00d7\u03c1c . We have:\n\u2016U\u0304\u2217 \u2212 U\u0304\u2016F \u2264\n\u221a 2p\n\u03c1r(1\u2212 \u03b4)\n[( 2 +\n1\u221a \u03b3\u2032r\u03bbkr+1\n) \u2016E\u0303u\u2016F\n+ (\u221a \u03bbkr \u03bbkr+1 + \u221a \u03b3\u2032r\u03bbkr ) \u2016U\u0304\u2016F ] , and\n\u2016E\u2217\u2016F \u2264\n\u221a 2\n\u03b3\u2032r\u03bbkr+1 \u2016E\u0303u\u2016F + \u221a 2 \u03bbkr \u03bbkr+1 \u2016U\u0304\u2016F .\nwhere U\u0304\u2217 = PkrP > kr X and E\u2217 = U\u2217 \u2212 U\u0304\u2217. The same inequalities with slight modification also hold for V \u2217, which we omit because of space constraints.\nProof: Please refer to Appendix E.\nTheorem 5. Let Mr and Mc be such that (2) holds. Let also U\u2217 and V \u2217 be the solutions of (16) with U\u0303 = MrU\u0304 and V\u0303 = McV\u0304 , where u\u0304i \u2208 span(Pkr ), i = 1, \u00b7 \u00b7 \u00b7 , p, v\u0304j \u2208 span(Qkc), j = 1, \u00b7 \u00b7 \u00b7 , n. We have:\n\u2016U\u2217 \u2212 U\u0304\u2016F \u2264\n\u221a 2p\n\u03c1r(1\u2212 \u03b4) \u221a \u03bbkr \u03bbkr+1 \u2016U\u0304\u2016F\nwhere U\u2217 = PkrP > kr X . The same inequalities with slight modification also hold for V \u2217, which we omit because of space constraints.\nProof: The proof directly follows from the proof of Theorem 4 by using E\u0303u = 0 and \u03b3 \u2032\nr = 0. As X\u0304 = U\u0304 \u03a3\u0304V\u0304 >, we can say that the error with eqs.(16) is upper bounded by the product of the errors of the individual subspace decoders. Also note that the error again depends on the spectral gaps defined by the ratios \u03bbkc/\u03bbkc+1 and \u03bbkr/\u03bbkr+1.\nThe solution to the above problems is simply a graph upsampling operation as explained in Lemma 1.\nLemma 1. Let S \u2208 <c\u00d7r and R \u2208 <d\u00d7r be the two matrices such that d < r and d < c. Furthermore, let M \u2208 <d\u00d7c be a sampling matrix as constructed in (1) and L \u2208 <c\u00d7c be a symmetric positive semi-definite matrix. We can write S = [S>a |S>b ]>, where Sb \u2208 <d\u00d7r and Sa \u2208 <(c\u2212d)\u00d7r are the known and unknown submatrices of S. Then the exact and unique solution to the following problem:\nmin Sa\ntr(S>LS), s.t: MS = R (17)\nis given by Sa = \u2212L\u22121aaLabR.\nProof: Please refer to Appendix F. Using Lemma 1 and the notation of Section IV-A we can\nwrite:\nU =\n[ \u2212L\u22121r (\u2126\u0304r, \u2126\u0304r)Lr(\u2126\u0304r,\u2126r)U\u0303\nU\u0303 ] V = [ \u2212L\u22121c (\u2126\u0304c, \u2126\u0304c)Lc(\u2126\u0304c,\u2126c)V\u0303\nV\u0303\n] . (18)\nEqs. (18) involves solving a sparse linear system. If each connected component of the graph has at least one labeled element, Lr(\u2126\u0304r, \u2126\u0304r) is full rank and invertible. If the linear system above is not large then one can directly use eq. (18). However, to avoid inverting the big matrix we can use the standard Preconditioned Conjugate Gradient (PCG) method to solve it. Note that the eqs. (18) and even PCG can be implemented in parallel for every column of U and V . This gives a significant advantage over the alternate decoder in terms of computation time. The cost of this decoder is O(OlKkn) where Ol is the number of iterations for the PCG method. The columns of U and V are not normalized with the above solution, therefore, a unit norm normalization step is needed at the end. Once U, V are determined, one can use X = U \u03a3\u0303V > \u221a np/\u03c1r\u03c1c(1\u2212 \u03b4) to determine the required low-rank matrix X . The decoder for approximate recovery is presented in Algorithm 2.\nAlgorithm 2 Subspace Upsampling based Approximate Decoder for low-rank recovery\nINPUT: X\u0303 \u2208 <\u03c1c\u00d7\u03c1r , Lr \u2208 <p\u00d7p, Lc \u2208 <n\u00d7n 1. do SV D(X\u0303) = U\u0303 \u03a3\u0303V\u0303 > 2. find k such that \u03a3\u0303k,k/\u03a3\u03031,1 < 0.1 3. Solve eqs. (16) for every column of U, V as following: for i = 1, . . . k do\nsolve minui u > i Lrui s.t Mrui = u\u0303i using PCG solve minvi v > i Lcvi s.t M>c v = v\u0303i using PCG end for 4. Set ui = ui/\u2016ui\u2016F , vi = vi/\u2016vi\u2016F ,\u2200i = 1, \u00b7 \u00b7 \u00b7 , k 5. Set \u03a3 = \u221a np\n\u03c1r\u03c1c(1\u2212\u03b4) \u03a3\u0303\n6. Set X = U\u03a3V > OUTPUT: The full low-rank X \u2208 <p\u00d7n\nTwo other approximate decoders for low-rank recovery are presented in Appendix G.\nVI. DECODER FOR CLUSTERING As already mentioned earlier, PCA has been widely used for two types of applications: 1) low-rank recovery and 2) clustering. Therefore, in this section, we present a method to perform clustering using our framework.\nFor the clustering application we do not need the full lowrank matrix X . Thus, we propose to do k-means on the lowrank representation of the sampled data X\u0303 obtained using (4), extract the cluster labels C\u0303 and then decode the cluster labels C for X on the graphs Lr and Lc.\nLet C\u0303 \u2208 {0, 1}\u03c1c\u00d7k be the cluster labels of X\u0303 (for k clusters) which are obtained by performing k-means. Then,\nC\u0303ij = { 1 if x\u0303i \u2208 jth cluster 0 otherwise.\nNote that each of the columns c\u0303i of C\u0303 is the cluster indicator for one of the k clusters. The goal now is to decode the cluster indicator matrix C \u2208 {0, 1}n\u00d7k. We refer to the Compressive Spectral Clustering (CSC) framework [42], where the authors solve a similar problem by arguing that each of the columns of C can be obtained by assuming that it lies close to the span(Qkc), where Qkc are the first kc Laplacian eigenvectors of the graph Gc. This requires solving the following convex minimization problem:\nmin C \u2016M>c C \u2212 C\u0303\u20162F + \u03b3 tr(C>LcC) (19)\nThe above problem can be solved independently for each of the columns of C, thus,\nmin ci \u2016M>c ci \u2212 c\u0303i\u201622 + \u03b3c>i Lcci (20)\nFurthermore, note that the graph Gr is not required for this process. Eq.(20) gives a faithful solution for ci if the sampling operator Mc satisfies the restricted isometry property RIP. Thus, for any \u03b4c, c \u2208 (0, 1), with probability at least 1\u2212 c,\n(1\u2212 \u03b4c)\u2016w\u201622 \u2264 n\n\u03c1c \u2016w>Mc\u201622 \u2264 (1 + \u03b4c)\u2016w\u201622 (21)\nfor all w \u2208 span(Qkc) provided that\n\u03c1c \u2265 3\n\u03b42c \u03bd2kc log ( 2kc c ) . (22)\nThis holds true as a consequence of Theorem 1 (eq.(30) in the proof of Theorem 1 and Theorem 5 in [28]).\nEq.(20) requires the tuning of a model parameter \u03b3 which we want to avoid. Therefore, we use the same strategy as for the approximate low-rank decoder in Section V-C. The cluster labels c\u0303i are not noisy because they are obtained by running k-means on the result of FRPCAG eq.(4), which is robust to outliers. Thus, we set \u03b3 = 0 in eq.(20) and propose to solve the following problem:\nmin ci\nc>i Lcci s.t: M>c ci = c\u0303i. (23)\nAccording to Lemma 1, the solution is given by:\nci =\n[ \u2212L\u22121c (\u2126\u0304c, \u2126\u0304c)Lc(\u2126\u0304c,\u2126c)c\u0303i\nc\u0303i\n] . (24)\nIdeally, every row of the matrix C should have 1 in exactly one of the k columns, indicating the cluster membership of that data sample. However, the solution C \u2208 Rn\u00d7k obtained by solving the above problem is not binary. Thus, to finalize the cluster membership (one of the k columns), we perform a maximum pooling for each of the rows of C, i.e,\nCij \u2190 {\n1 if Cij = max{Cij \u2200 j = 1 \u00b7 \u00b7 \u00b7 k} 0 otherwise.\nAlgorithm 3 summarizes this procedure.\nAlgorithm 3 Approximate Decoder for clustering\nINPUT: X\u0303 \u2208 <\u03c1c\u00d7\u03c1r , Lc \u2208 <n\u00d7n 1. do k-means on X\u0303 to get the labels C\u0303 \u2208 {0, 1}\u03c1c\u00d7k 2. Solve eqs. (23) for every column of C as following: for i = 1, . . . k do\nsolve minci c > i Lcci s.t M>c ci = c\u0303i using PCG\nend for 3. Set Cij = 1 if max{Cij \u2200 j = 1 \u00b7 \u00b7 \u00b7 k} and 0 otherwise. OUTPUT: cluster indicators for X: C \u2208 {0, 1}n\u00d7k\nTheorem 6. Let Mc be such that (21) holds. Let also c\u2217i be the solution of (23) with c\u0303i = M>c c\u0304i, where c\u0304i \u2208 span(Qkc), i = 1, \u00b7 \u00b7 \u00b7 , n. We have:\n\u2016ci\u2217 \u2212 c\u0304i\u20162 \u2264 \u221a\nn\n\u03c1c(1\u2212 \u03b4c) \u221a \u03bbkc \u03bbkc+1 \u2016c\u0304i\u20162\nwhere c\u2217i = QkcQ > kc ci.\nProof: The proof directly follows from the proof of Theorem 3.2 in [28]. These steps have been repeated in the proof of Theorem 4 in Appendix E as well. Using c\u2217i = u\u0304 \u2217 i , c\u0304i = u\u0304i, n = p, \u03c1c = \u03c1r in eq.(41) one can get theoretical guarantees for eq. (20). Then, by using e\u0303ui = 0 and \u03b3 = 0 we get the result of above theorem.\nComputational Complexity: A summary of all the decoders and their computational complexities is presented in Table X of Appendix H. The complete CPCA algorithm and the computational complexities of different steps are presented in Table I. For K, k, \u03c1r, \u03c1c, p n CPCA algorithm scales as O(nkK) per iteration. Thus, assuming that the row and column graphs are available from external source, a speedup of p/k per iteration is obtained over FRPCAG and p2/k over RPCA. A detailed explanation regarding the calculation of complexities of CPCA and other models is presented in\nTable XI and Appendix H.\nMemory Requirements: We compare the memory requirements of CPCA with FRPCAG. For a matrix Y \u2208 Rp\u00d7n, FRPCAG and CPCA require the construction of two graphs Gr, Gc whose Laplacians Lr \u2208 Rp\u00d7p, Lc \u2208 Rn\u00d7n are used in the core algorithm. However, these Laplacians are sparse, therefore the memory requirement for Lr, Lc is O(K(|Er| + |Ec|)) respectively. The core algorithm of FRPCAG requires operation on the full matrix Y and the graph Laplacians Lr, Lc. As |Er| \u2248 Kp and |Ec| \u2248 Kn therefore, the memory\nrequirement for the regularization terms tr(XLcX>) and tr(X>LrX) is O(Knp). For the CPCA algorithm, assuming n > p and letting \u03c1r = p/b and \u03c1c = n/a, the complexity of FRPCAG on the sampled data is O(Knp/(ab)) and the approximate decode stage for subspaces of dimension k is O(Knk). Thus the overall memory requirement of CPCA is O(Kn(p/(ab) + k)). As compared to FRPCAG, an improvement of pab/(p + kab) is obtained. For example for n = 1000, p = 200, a = 10, b = 1, k = 10, a reduction of approximately 6.6 times is obtained.\nConvergence of CPCA: The CPCA based algorithm (Table I) has two main steps: 1) FRPCAG on the compressed data matrix and 2) low-rank matrix or cluster label decoding. FRPCAG is solved by the FISTA (Algorithm 1) and the decode step is solved using the PCG method. Both of these methods have been well studied in terms of their convergence guarantees. More specifically, one can refer to [3] for a detailed study on FISTA and [2] for PCG. Therefore, we do not include the convergence analysis here for brevity.\nVII. EXPERIMENTAL RESULTS We perform two types of experiments corresponding to two applications of PCA 1) Data clustering and 2) Low-rank recovery using two open-source toolboxes: the UNLocBoX [26] and the GSPBox [25]."}, {"heading": "A. Clustering", "text": "1) Experimental Setup: Datasets: We perform our clustering experiments on 5 benchmark databases (as in [35], [36]): CMU PIE, ORL, YALE, MNIST and USPS. For the USPS and ORL datasets, we further run two types of experiments 1) on subset of datasets and 2) on full datasets. The experiments on the subsets of the datasets take less time so they are used to show the efficiency of our model for a wide variety of noise types. The details of all datasets used are provided in Table XII of Appendix H.\nNoise & Errors: CPCA is a memory and computationally efficient alternative for FRPCAG. An important property of FRPCAG is its robustness to noise and outliers, just like RPCA. Therefore, it is important to study the performance of CPCA under noise and corruptions similar to those for FRPCAG and RPCA. To do so we add 3 different types of noise in all the samples of datasets in different experiments: 1) Gaussian noise and 2) Laplacian noise with standard deviation ranging from 5% to 20% of the original data 3) Sparse noise (randomly corrupted pixels) occupying 5% to 20% of each data sample.\nComparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and\n11) Fast Robust PCA on Graphs (FRPCAG) [36]. RPCA and RPCAG are not used for the evaluation of MNIST, USPS large and ORL large datasets due to computational complexity of these models.\nPre-processing: All datasets are transformed to zero-mean and unit standard deviation along the features / rows. For MMF the samples are additionally normalized to unit-norm. For NMF and GNMF only the unit-norm normalization is applied to all the samples of the dataset as NMF based models can only work with non-negative data.\nEvaluation Metric: We use clustering error as a metric to compare the clustering performance of various models. The clustering error for LE, PCA, GLPCA, MMF, NMF and GNMF is evaluated by performing k-means on the principal components V (note that these models explicitly learn V , where X = U\u03a3V >). The clustering error for RPCA, RPCAG and FRPCAG is determined by performing k-means directly on the low-rank X . For our CPCA method, k-means is performed on the small low-rank X\u0303 and then the labels for full X are decoded using Algorithm 3. Parameter Selection: To perform a fair validation for each of the models we use a range of values for the model parameters as presented in Table XIII of Appendix H. For a given dataset, each of the models is run for each of the parameter tuples in this table and the best clustering error is reported. Furthermore, PCA, GLPCA, MMF, NMF and GNMF are non-convex models so they are run 10 times for each of the parameter tuple. RPCA, RPCAG, FRPCAG and CPCA based models are convex so they are run only once. Although our CPCA approach is convex, it involves a bit of randomness due to the sampling step. Due to the extensive nature of the experimental setup, most of the clustering experiments are performed under one sampling condition. However, it is interesting to study the variation of error under different sampling scenarios. For this purpose we perform some additional experiments on the USPS dataset. For our proposed CPCA, we use a convention CPCA(a, b), where a and b denote the downsampling factors on the columns and rows respectively. A uniform sampling strategy is always used for CPCA. Graph Construction: The K-nearest neighbor graphs\nGr, Gc are constructed using FLANN [22] as discussed in Section II. The small graphs G\u0303r, G\u0303c can also be constructed using FLANN or the Kron reduction strategy of Section IV-A. For all the experiments reported in this paper we use K-nearest neighbors = 10 and Gaussian kernel for the adjacency matrices W . The smoothing parameters \u03c32 for the Gaussian kernels are automatically set to the average distance of the K-nearest neighbors.\n2) Discussion on clustering performance: We point out here that the purpose of our clustering experiments is threefold: \u2022 To show the efficiency of CPCA for a wide variety of\nnoise and errors and downsampling. \u2022 To study the conditions under which CPCA performs\nworse than the other models. \u2022 To study the variation of performance under different\nsampling scenarios. For this purpose, we test CPCA under a variety of downsampling for different datasets. Cases with p n and n p carry special interest. Therefore, we present our discussion below in the light of the above goals.\nTables II, III, IV & V present the clustering results for USPS small, USPS large, MNIST large, MNIST small, ORL small, ORL large, CMU PIE and YALE datasets. Note that not all the models are run for all the datasets due to computational constraints. The best results are highlighted in bold and the second best in blue. From Table II for the USPS dataset, it is clear that our proposed CPCA model attains comparable clustering results to the state-of-the-art RPCAG and FRPCAG models and better than the others in most of the cases. Similar observation can be made about the MNIST large dataset from Table V in comparison to FRPCAG.\nIt is important to note that for the USPS and MNIST datasets p n. Thus, for the USPS dataset, the compression is only applied along the columns (n) of the dataset. This compression results in clustering error which is comparable to the other state-of-the-art algorithms. As p = 256 for the USPS dataset, it was observed that even a 2 times downsampling on p results in a loss of information and the clustering quality deteriorates. The same observation can be made about ORL small, ORL large and YALE datasets from Tables III, IV for CPCA (2,2). i.e, two times downsampling on both rows and columns. On the other hand the performance of CPCA (1,2) is reasonable for the ORL small dataset. Recall that CPCA (a,b) means a downsampling by a and b across columns and rows (samples and features). Also note that for ORL dataset n p.\nFinally, we comment about the results on MNIST small dataset (p = 784, n = 1000) from Table II. It is clear that FRPCAG (no compression) results in the best performance. CPCA (5,1) results in a highly undersampled dataset which does not capture enough variations in the MNIST small dataset to deliver a good clustering performance. This particular case supports the fact that compression does not always yield a good performance at the advantage of reduced complexity. Therefore, we study this phenomena below.\nThe above findings for the MNIST dataset are intuitive as it only makes sense to compress both rows and columns in our CPCA based framework if a reasonable speed-up can be obtained without compromising the performance, i.e, if both n and p are large. If either n or p is small then one might only apply compression along the larger dimension, as the compression on the smaller dimension would not speed up the computations significantly. For example, for the USPS dataset, a speed-up of p/k = 256/10 \u2248 25 times would be obtained\nover FRPCAG by compressing along the samples (columns n) only without a loss of clustering quality. Our experiments showed that this speed up increased upto 30 by compressing along the features but with a loss of performance (The results are not presented for brevity).\nTables II, III, IV & V also show that CPCA is quite robust to a variety of noise and errors in the dataset. Even in the presence of higher levels of Gaussian and Laplacian noise, CPCA performs comparable to other methods for the USPS dataset. Thus, CPCA tends to preserve the robustness property of FRPCAG. This will also be clear from the lowrank recovery experiments in the next section.\n3) Computation Time vs Performance & Sampling: It is interesting to compare 1) the time needed for FRPCAG and CPCA to perform clustering 2) the corresponding clustering error and 3) the sub-sampling rates in CPCA. Table V shows such a comparison for 70,000 digits of MNIST with (10, 2) times downsampling on the (columns, rows) respectively for CPCA. The time needed by CPCA is an order of magnitude lower than FRPCAG. Note that the time reported here does not include the construction of graphs Gr, Gc as both methods use the same graphs. Furthermore, these graphs can be constructed in the order of a few seconds if parallel processing is used. The time for CPCA includes steps 2 to 5 and 7 of Table I. For the information about the graph construction time, please refer to Table VI and the discussion thereof.\nTable VI presents the computational time and number of iterations for the convergence of CPCA, FRPCAG, RPCAG & RPCA on different sizes and dimensions of the datasets. We also present the time needed for the graph construction. The computation is done on a single core machine with a 3.3 GHz processor without using any distributed or parallel computing tricks. An \u221e in the table indicates that the algorithm did not converge in 4 hours. It is notable that our model requires a very small number of iterations to converge irrespective of the size of the dataset. Furthermore, the model is orders of magnitude faster than RPCA and RPCAG. This is clearly observed from the experiments on MNIST dataset where our proposed model is 100 times faster than RPCAG. Specially for MNIST dataset with 25000 samples, RPCAG and RPCA did not converge even in 4 hours whereas CPCA converged in less than a minute.\n4) Effect of random sampling: An interesting observation can be made from Table V for the MNIST dataset: the error of CPCA is also lower than FRPCAG. Such cases can also be observed in USPS dataset (Table II). As the downsampling step is random, it might remove some spurious samples sometimes and the clustering scheme (Section VI) becomes robust to these samples. For the clustering application, the spurious samples mostly lie on the cluster borders and deteriorate the\nclustering decision. For the computational purposes (validation for all the noise scenarios) Tables II & V correspond to one run of the CPCA for one specific sampling case.\nIn order to study the effect of random sampling on the clustering performance, we perform an experiment on the full USPS dataset (256 \u00d7 10000) with different levels of artificial noise. The results in Fig. 2 correspond to 10 runs of the CPCA under different uniform sampling scenarios. For this experiment, we downsample 10 times along the columns (digits), whereas no downsampling is used across the features and then add different levels of Gaussian noise from 0 to 15% in the dataset. Thus, we downsample the dataset, run FRPCAG to get a low-rank X\u0303 \u2208 R256\u00d71000, perform k-means (k = 10) and then use the approximate clustering decoder (Algorithm 3) to decode the labels for the full dataset. This process is repeated over the whole parameter grid \u03b3 \u2032 r, \u03b3 \u2032\nc \u2208 (0, 30) and the minimum error over the grid is considered. Each of the boxplots in this figure summarize the clustering error over 10 runs of CPCA(10,1) for different levels of Gaussian noise. The mean clustering error is 15.05% for the case of no noise and 15.2%, 15.6% and 16% for 5%, 10% and 15% Gaussian noise respectively. Furthermore, the standard deviation for each of the boxplots varies between 0.4% to 0.55%. This result clearly shows that the CPCA performance is quit robust to random sampling. Similar results were observed for other datasets and are not reported here for the purpose of brevity.\nIt is interesting to study the reduction in the total time attained by using CPCA as compared to FRPCAG. Table VI can be used to perform this comparison as well. For example, for the MNIST dataset with 5000 samples, the total time (including graph construction) for FRPCAG is 28.2 secs and that for CPCA is 20.1 secs. Thus, a speed-up of 1.4 times is obtained over FRPCAG. The time required for the construction\nof the graph between the samples or features is often more than that required for the CPCA to converge. This is a small computational bottleneck of the graph construction algorithm. While graph learning or graph construction is an active and interesting field of research, it is not a part of this work. The state-of-the-art results [16], [24], [34] do not provide any scalable solutions yet.\n5) Rank Preservation Under Downsampling: An interesting question about CPCA is if it preserves the underlying rank of the dataset under the proposed sampling scheme. Table VII shows that the rank is preserved even in the presence of noise. For this experiment, we take different datasets and corrupt them with different types of noise and perform cross-validation for clustering using the parameter range for CPCA mentioned in Table XIII (see Appendices). Then, we report the rank of X\u0303 for the parameter corresponding to the minimum clustering error. As X\u0303 is approximately low-rank so we use the following strategy to determine the rank k: \u03a3\u0303k,k/\u03a3\u03031,1 < 0.1. FRPCAG assumes that the number of clusters \u2248 rank of the dataset. Our findings show that this assumption is almost satisfied for the sampled matrices even in the presence of various types of noise. Thus, the rank is preserved under the proposed sampling strategy. For clustering experiments, the lowest error with CPCA occurs when the rank \u2248 number of clusters.\n6) Clustering error vs downsampling rate: Table VIII shows the variation of clustering error of CPCA with different downsampling factors across rows and columns of the USPS\ndataset (256 \u00d7 10, 000). Obviously, higher downsampling results in an increase in the clustering error. However, note that we can downsample the samples (columns) by a factor of 5 without observing an error increase. The downsampling of features results in an error increase because the number of features for this dataset is only 256 and downsampling results in a loss of information. Similar behavior can also be observed for the ORL small and ORL large datasets in Table III where the performance of CPCA is slightly worse than FRPCAG because the number of samples n for ORL is only 400."}, {"heading": "B. Low-rank recovery", "text": "In order to demonstrate the effectiveness of our model to recover low-rank static background from videos we perform experiments on 1000 frames of 3 videos available online 7. All the frames are vectorized and arranged in a data matrix Y whose columns correspond to frames. The graph Gc is constructed between the columns of Y and the graph Gr is constructed between the rows of Y following the methodology of Section II. Fig. 3 shows the recovery of low-rank frames for one actual frame of each of the videos. The first row corresponds to a frame from the video of a shopping mall lobby, the second row to a restaurant food counter and the third row to an airport lobby. The leftmost plot in each row shows the actual frame, the other 5 show the recovered low-rank representations using RPCA, RPCAG, FRPCAG and CPCA with two different uniform downsampling rates. For CPCA Algorithm 2 is used and the rank k for the approximate decoder is set such that \u03a3\u0303k,k/\u03a3\u03031,1 < 0.1, where \u03a3\u0303 are the singular values of X\u0303 .\nFor the 2nd and 3rd rows of Fig. 3 it can be seen that our proposed model is able to separate the static backgrounds very accurately from the moving people which do not belong to the static ground truth. However, the quality is slightly\n7https://sites.google.com/site/backgroundsubtraction/test-sequences\ncompromised in the 1st row where the shadow of the person appears in the low-rank frames recovered with CPCA. In fact, this person remains static for a long time in the video and the uniform sampling compromises the quality slightly.\nTable IX presents the computational time in seconds of RPCA, RPCAG, FRPCAG and CPCA for low-rank recovery of different videos in Fig. 3. The time reported here corresponds to steps 2 to 6 of Table I, Algorithm 1 of [36] for FRPCAG, [7] for RPCA and [35] for RPCAG, excluding the construction of graphs Gr, Gc. The speed-up observed for these experiments from Table IX is 10 times over FRPCAG and 100 times over RPCA and RPCAG.\nFig. 4 presents a comparison of the quality of the lowrank static background extracted using the alternate (8) and approximate decoders discussed in (18) for a video (1st row) of Fig. 3. Clearly, the alternate decoder performs slightly better than the approximate decoders but at the price of tuning of two model parameters.\nFig. 5 presents a comparison of the quality of low-rankness for the same video extracted using the approximate decoder (18) using different downsampling factors on the pixels and frames. It is obvious that the quality of low-rankness remains intact even with higher downsampling factors."}, {"heading": "C. Low-Rank Recovery from Random Projections", "text": "Throughout this work we assume that the graphs Gc and Gr for the complete data matrix Y are either available or can be constructed directly from Y itself using the standard graph construction algorithms. This is a reasonable assumption if one wants to reduce the computational burden by downsampling on the datasets. However, often the complete data matrix Y is not available and the goal is to obtain an estimate of Y from some side information. Typical examples include Magnetic\nResonance Imaging (MRI), Computational Tomography (CT) and Electron Tomography (ET) where one only has access to the projections b of Y acquired through a known projection operator A. The purpose here is not to reduce the computational burden but to acquire a good enough estimate of Y from b. Furthermore, for such applications, there is no notion of row or column projection / sampling operators. Nevertheless, one might want to exploit the row and column smoothness assumption for the purpose of reconstruction. While, this is not a significant part of our current work, it is still an obvious open question and the answer comes from an extension of this work. Therefore, to be complete, we propose a framework for such problems which might require a low-rank reconstruction from a few projections. It is important to emphasize though that the goal is not to compare and evaluate the performance rigorously with the state-of-the-art. In fact we mention this here just to give a flavour of how the current framework can be extended for such problems.\nAssume that a CT sample, for example, a Shepp-Logan phantom of the size X \u2208 Rp\u00d7n needs to be reconstructed from its projections b \u2208 Rm, obtained via a line projection matrix A \u2208 Rm\u00d7np. Thus, b = Avec(X) + e, where e \u2208 Rm models the noise in the projections. We propose to reconstruct the sample X by solving the following optimization problem:\nmin X \u2016Avec(X)\u2212 b\u201622 + \u03b3r tr(X>LrX) + \u03b3c tr(XLcX>),\n(25) where Lr,Lc are the row and column graph Laplacians between the rows and columns of X . Since, these graphs are not available in the beginning, one can obtain an initial estimate of X by running a standard compressed sensing problem for a few iterations and then construct these graphs from this estimate. The estimated graphs can also be improved after every few iterations from the more refined X .\nFig. 6 shows the reconstruction of a 64 \u00d7 64 Modified Shepp-Logan phantom from 20% projections using eq.(25). The initial estimate of the graphs Gr,Gc between the rows and\ncolumns of the phantom is obtained from the first 3 iterations of the compressed sensing based recovery problem and then these graphs are updated every 5 iterations. Our future work will focus on a detailed study of this method.\nVIII. CONCLUSION\nWe present Compressive PCA on Graphs (CPCA) which approximates a recovery of low-rank matrices on graphs from their sampled measurements. It is supported by the proposed restricted isometry property (RIP) which is related to the coherence of the eigenvectors of graphs between the rows and columns of the data matrix. Accompanied with several efficient, parallel, parameter free and low-cost decoders for low-rank recovery and clustering, the presented framework gains a several orders of magnitude speed-up over the low-rank recovery methods like Robust PCA. Our theoretical analysis reveals that CPCA targets exact recovery for low-rank matrices which are clusterable across the rows and columns. Thus, the error depends on the spectral gaps of the graph Laplacians. Extensive clustering experiments on 5 datasets with various types of noise and comparison with 11 state-of-the-art methods reveal the efficiency of our model. CPCA also achieves stateof-the-art results for background separation from videos."}, {"heading": "A. Proof of theorem 1", "text": "We start with the sampling of the rows. Theorem 5 in [28] shows that for any \u03b4r, r \u2208 (0, 1), with probability at least 1\u2212 r,\n(1\u2212 \u03b4r)\u2016z\u201622 \u2264 p\n\u03c1r \u2016Mrz\u201622 \u2264 (1 + \u03b4r)\u2016z\u201622\nfor all z \u2208 span(Pkr ) provided that\n\u03c1r \u2265 3\n\u03b42r \u03bd2kr log ( 2kr r ) . (26)\nNotice that Theorem 5 in [28] is a uniform result. As a consequence, with probability at least 1\u2212 r,\n(1\u2212 \u03b4r)\u2016yi\u201622 \u2264 p\n\u03c1r \u2016Mryi\u201622 \u2264 (1 + \u03b4r)\u2016yi\u201622, i = 1, . . . , n, (27)\nfor all y1, . . . , yn \u2208 span(Pkr ) provided that (26) holds. Summing the previous inequalities over all i shows that, with probability at least 1\u2212 r,\n(1\u2212 \u03b4r)\u2016Y \u20162F \u2264 p\n\u03c1r \u2016MrY \u20162F \u2264 (1 + \u03b4r)\u2016Y \u20162F , (28)\nfor all Y \u2208 <p\u00d7n with column-vectors in span(Pkr ). Let us continue with the sampling of the columns. Again, Theorem 5 in [28] shows that for any \u03b4c, c \u2208 (0, 1), with probability at least 1\u2212 c,\n(1\u2212 \u03b4c)\u2016w\u201622 \u2264 n\n\u03c1c \u2016w>Mc\u201622 \u2264 (1 + \u03b4c)\u2016w\u201622\nfor all w \u2208 span(Qkc) provided that\n\u03c1c \u2265 3\n\u03b42c \u03bd2kc log ( 2kc c ) . (29)\nAs a consequence, with probability at least 1\u2212 c,\n(1\u2212 \u03b4c)\u2016zi\u201622 \u2264 n\n\u03c1c \u2016z>i Mc\u201622 \u2264 (1 + \u03b4c)\u2016zi\u201622, i = 1, . . . , \u03c1r, (30)\nfor all z1, . . . , z\u03c1r \u2208 span(Qkc) provided that (29) holds. Summing the previous inequalities over all i shows that, with probability at least 1\u2212 c,\n(1\u2212 \u03b4c)\u2016Z\u20162F \u2264 n\n\u03c1c \u2016ZMc\u20162F \u2264 (1 + \u03b4c)\u2016Z\u20162F (31)\nfor all Z \u2208 <\u03c1r\u00d7n with row-vectors in span(Qkc). In particular, this property holds, with at least the same probability, for all matrices Z of the form MrY where Y \u2208 <p\u00d7n is a matrix with row-vectors in span(Qkc).\nWe now continue by combining (28) and (31). We obtain that\n(1\u2212 \u03b4c)(1\u2212 \u03b4r)\u2016Y \u20162F \u2264 np\n\u03c1c\u03c1r \u2016MrYMc\u20162F \u2264 (1 + \u03b4c)(1 + \u03b4r)\u2016Y \u20162F (32)\nfor all Y \u2208 Rp\u00d7n with column-vectors in span(Pkr ) and row-vectors in span(Qkc), provided that (26) and (29) hold. It remains to compute the probability that (32) holds. Property (32) does not hold if (28) or (31) do not hold. Using the union bound, (32) does not hold with probability at most r + c. To finish the proof, one just need to choose r = c = /2 and \u03b4r = \u03b4c = \u03b4/3, and notice that (1 + \u03b4/3)2 \u2264 1 + \u03b4 and (1\u2212 \u03b4/3)2 \u2265 1\u2212 \u03b4 for \u03b4 \u2208 (0, 1)."}, {"heading": "B. Proof of Theorem 2", "text": "Using the optimality condition we have, for any Z \u2208 Rp\u00d7n,\n\u2016MrX\u2217Mc \u2212 X\u0303\u2016F \u2264 \u2016MrZMc \u2212 X\u0303\u2016F .\nFor Z = X\u0304 , we have\n\u2016MrX\u2217Mc \u2212 X\u0303\u2016F \u2264 \u2016MrX\u0304Mc \u2212 X\u0303\u2016F ,\nwhich gives\n\u2016MrX\u2217Mc \u2212MrX\u0304Mc \u2212 E\u0303\u2016F \u2264 \u2016E\u0303\u2016F .\nAs (2) holds, we have\n\u2016MrX\u2217Mc \u2212MrX\u0304Mc \u2212 E\u0303\u2016F \u2265 \u2016Mr(X\u2217 \u2212 X\u0304)Mc\u2016F \u2212 \u2016E\u0303\u2016F\n\u2265\n\u221a \u03c1r\u03c1c(1\u2212 \u03b4)\nnp \u2016X\u2217 \u2212 X\u0304\u2016F \u2212 \u2016E\u0303\u2016F .\nTherefore, by combining the above equations we get \u2016X\u2217 \u2212 X\u0304\u2016F \u2264 2 \u221a\nnp\n\u03c1r\u03c1c(1\u2212 \u03b4) \u2016E\u0303\u2016F ."}, {"heading": "C. Proof of Theorem 3", "text": "Using the optimality condition we have for any Z \u2208 <p\u00d7n and optimal solution X\u2217 = X\u0304\u2217 + E\u2217:\n\u2016MrX\u2217Mc \u2212 X\u0303\u20162F + \u03b3\u0304c tr(X\u2217LcX\u2217 >) + \u03b3\u0304r tr(X \u2217>LrX\u2217) \u2264 \u2016MrZMc \u2212 X\u0303\u20162F + \u03b3\u0304c tr(ZLcZ>) + \u03b3\u0304r tr(Z>LrZ) (33)\nusing Z = X\u0304 = PkrYbQ > kc as in the proof of theorem 2 in [36], where Yb \u2208 <kr\u00d7kc and it is not necessarily diagonal. Note that \u2016Yb\u2016F = \u2016X\u0304\u2016F , Q>kcQkc = Ikc , P > kr Pkr = Ikr , Q\u0304 > kc Qkc = 0, P\u0304 > kr Pkr = 0. From the proof of theorem 2 in [36] we also know that:\ntr(X\u0304Lc(X\u0304)>) \u2264 \u03bbkc\u2016X\u0304\u20162F tr(X\u0304>Lr(X\u0304)) \u2264 \u03bbkr\u2016X\u0304\u20162F\ntr(X\u2217Lc(X\u2217)>) \u2265 \u03bbkc+1\u2016X\u2217Q\u0304kc\u20162F tr(X\u2217Lr(X\u2217)>) \u2265 \u03bbkr+1\u2016P\u0304>krX\n\u2217\u20162F Now using all this information in (33) we get\n\u2016MrX\u2217Mc \u2212 X\u0303\u20162F + \u03b3\u0304c\u03bbkc+1\u2016X\u2217Q\u0304kc\u20162F + \u03b3\u0304r\u03bbkr+1\u2016P\u0304>krX \u2217\u20162F \u2264 \u2016E\u0303\u20162F + (\u03b3\u0304c\u03bbkc + \u03b3\u0304r\u03bbkr )\u2016X\u0304\u20162F\nFrom above we have: \u2016MrX\u2217Mc \u2212 X\u0303\u2016F \u2264 \u2016E\u0303\u2016F + \u221a (\u03b3\u0304c\u03bbkc + \u03b3\u0304r\u03bbkr )\u2016X\u0304\u2016F (34)\nand \u221a (\u03b3\u0304c\u03bbkc+1\u2016X\u2217Q\u0304kc\u20162F + \u03b3\u0304r\u03bbkr+1\u2016P\u0304>krX \u2217\u20162F ) \u2264 \u2016E\u0303\u2016F + \u221a (\u03b3\u0304c\u03bbkc + \u03b3\u0304r\u03bbkr )\u2016X\u0304\u2016F (35)\nusing\n\u03b3\u0304c = \u03b3 1\n\u03bbkc+1 and \u03b3\u0304r = \u03b3\n1\n\u03bbkr+1 ,\nand \u2016E\u2217\u20162F = \u2016X\u2217Q\u0304kc\u20162F = \u2016P\u0304>krX\n\u2217\u20162F we get:\n\u2016MrX\u2217Mc \u2212 X\u0303\u2016F \u2264 \u2016E\u0303\u2016F + \u221a \u03b3 ( \u03bbkc \u03bbkc+1 + \u03bbkr \u03bbkr+1 ) \u2016X\u0304\u2016F (36)\nand \u221a 2\u03b3\u2016E\u2217\u2016F \u2264 \u2016E\u0303\u2016F + \u221a \u03b3 ( \u03bbkc \u03bbkc+1 + \u03bbkr \u03bbkr+1 ) \u2016X\u0304\u2016F (37)\nwhich implies\n\u2016E\u2217\u2016F \u2264 \u2016E\u0303\u2016F\u221a\n2\u03b3 + 1\u221a 2 \u221a \u03b3 ( \u03bbkc \u03bbkc+1 + \u03bbkr \u03bbkr+1 ) \u2016X\u0304\u2016F (38)\nFocus on \u2016MrX\u2217Mc\u2212 X\u0303\u20162F now. As Mr,Mc are constructed with a sampling without replacement, we have \u2016MrE\u2217Mc\u2016F \u2264 \u2016E\u2217\u2016F . Now using the above facts and the RIP we get:\n\u2016MrX\u2217Mc \u2212 X\u0303\u2016F = \u2016Mr(X\u0304\u2217 + E\u2217)Mc \u2212MrX\u0304Mc \u2212 E\u0303\u2016F\n\u2265\n\u221a \u03c1r\u03c1c(1\u2212 \u03b4)\nnp \u2016X\u0304\u2217 \u2212 X\u0304\u2016F \u2212 \u2016E\u0303\u2016F \u2212 \u2016E\u2217\u2016F\nthis implies\n\u2016X\u0304\u2217 \u2212 X\u0304\u2016F \u2264 \u221a\nnp\n\u03c1c\u03c1r(1\u2212 \u03b4)\n[( 2 +\n1\u221a 2\u03b3\n) \u2016E\u0303\u2016F + (\n1\u221a 2 + \u221a \u03b3) \u221a( \u03bbkc \u03bbkc+1 + \u03bbkr \u03bbkr+1 ) \u2016X\u0304\u2016F ] Discussion Let A1, A2 \u2208 <p\u00d7n and A1 = U1S1V T1 , A2 = U2S2V T2 then if \u2016A1 \u2212A2\u20162F \u2192 0, then S1 \u2192 S2. We observe that\n\u2016A1 \u2212A2\u20162F = \u2016U1S1V T1 \u2212 U2S2V T2 \u20162F = \u2016UT2 U1S1V T1 V2 \u2212 S2\u20162F which implies that UT2 U1S1V T 1 V2 \u2248 S2. This is equivalent to saying that for the significant values of S2, the orthonormal matrices UT2 U1 and V T 1 V2 have to be almost diagonal. As a result, for the significant values of S2, U2 and V2 have to be aligned with U1 and V1. The same reason also implies that S1 \u2248 S2.\nD. Solution of eq. (11)\nLet us examine how to solve (11). The problem can be reformulated as:\nmin U\ntr(U>LrU) s.t: U>U = Ik, \u2016MrU \u2212 U\u0303\u20162F <\nLet U \u2032 is the zero appended matrix of U\u0303 , then we can re-write it as:\nmin U\ntr(U>LrU) s.t: U>U = Ik, \u2016Mr(U \u2212 U \u2032 )\u20162F <\nThe above problem is equivalent to (11), as the term \u2016Mr(U \u2212U \u2032 )\u20162F has been removed from the objective and introduced as a constraint. Note that the constant \u03b3r is not needed anymore. The new model parameter controls the radius of the L2 ball \u2016Mr(U \u2212 U \u2032 )\u20162F . In simple words it controls how much noise is tolerated by the projection of U on the ball that is centered at U \u2032 . To solve the above problem one needs to split it down into two sub-problems and solve iteratively between:\n1) The optimization minU tr(U>LrU) s.t: U>U = Ik. The solution to this problem is given by the lowest k eigenvectors of Lr. Thus it requires a complexity of O((n+ p)k2) for solving both problems (11). 2) The projection on the L2 ball \u2016Mr(U \u2212 U \u2032 )\u20162F whose complexity is O(\u03c1c + \u03c1r).\nThus the solution requires a double iteration with a complexity of O(Ink2) and is almost as expensive as FRPCAG."}, {"heading": "E. Proof of Theorem 4", "text": "We can write (12) and (13) as following:\nmin u1\u00b7\u00b7\u00b7up p\u2211 i=1 [ \u2016Mrui \u2212 u\u0303i\u201622 + \u03b3 \u2032 ru > i Lrui ] (39)\nmin v1\u00b7\u00b7\u00b7vn n\u2211 i=1 [ \u2016M>c vi \u2212 v\u0303i\u201622 + \u03b3 \u2032 cv > i Lcvi ] (40)\nIn this proof, we only treat Problem (39) and the recovery of U\u0304 . The proof for Problem (13) and the recovery of V\u0304 is identical. The above two problems can be solved independently for every i. From theorem 3.2 of [28] we obtain:\n\u2016u\u0304\u2217i \u2212 u\u0304i\u20162 \u2264 \u221a\np\n\u03c1r(1\u2212 \u03b4)\n[( 2 +\n1\u221a \u03b3\u2032r\u03bbkr+1\n) \u2016e\u0303ui \u20162 + (\u221a \u03bbkr \u03bbkr+1 + \u221a \u03b3\u2032r\u03bbkr ) \u2016u\u0304i\u20162 ] , (41)\nand\n\u2016e\u2217i \u20162 \u2264 1\u221a\n\u03b3\u2032r\u03bbkr+1 \u2016e\u0303ui \u20162 + \u221a \u03bbkr \u03bbkr+1 \u2016u\u0304i\u20162,\nwhich implies\n\u2016u\u0304\u2217i \u2212 u\u0304i\u201622 \u2264 2 p\n\u03c1r(1\u2212 \u03b4) (2 + 1\u221a \u03b3\u2032r\u03bbkr+1 )2 \u2016e\u0303ui \u201622 + (\u221a \u03bbkr \u03bbkr+1 + \u221a \u03b3\u2032r\u03bbkr )2 \u2016u\u0304i\u201622  , and\n\u2016e\u2217i \u201622 \u2264 2\n\u03b3\u2032r\u03bbkr+1 \u2016e\u0303ui \u201622 + 2 \u03bbkr \u03bbkr+1 \u2016u\u0304i\u201622.\nSumming the previous inequalities over all i\u2019s yields\n\u2016U\u0304\u2217 \u2212 U\u0304\u20162F \u2264 2 p\n\u03c1r(1\u2212 \u03b4) (2 + 1\u221a \u03b3\u2032r\u03bbkr+1 )2 \u2016E\u0303u\u20162F + (\u221a \u03bbkr \u03bbkr+1 + \u221a \u03b3\u2032r\u03bbkr )2 \u2016U\u0304\u20162F  , and\n\u2016E\u2217\u20162F \u2264 2\n\u03b3\u2032r\u03bbkr+1 \u2016E\u0303u\u20162F + 2 \u03bbkr \u03bbkr+1 \u2016U\u0304\u20162F .\nTaking the square root of both inequalities terminates the proof. Similarly, the expressions for V\u0304 can be derived:\n\u2016V\u0304 \u2217 \u2212 V\u0304 \u2016F \u2264\n\u221a 2n\n\u03c1c(1\u2212 \u03b4)\n[( 2 +\n1\u221a \u03b3\u2032c\u03bbkc+1\n) \u2016E\u0303v\u2016F + (\u221a \u03bbkc \u03bbkc+1 + \u221a \u03b3\u2032c\u03bbkc ) \u2016V\u0304 \u2016F ] and\n\u2016E\u2217\u2016F \u2264\n\u221a 2\n\u03b3\u2032c\u03bbkc+1 \u2016E\u0303v\u2016F + \u221a 2 \u03bbkc \u03bbkc+1 \u2016V\u0304 \u2016F ."}, {"heading": "F. Proof of Lemma 1", "text": "Let S = [S>a |S>b ]>. Further we split L into submatrices as follows:\nL = [ Laa Lab Lba Lbb ] Now (17) can be written as:\nmin Sa [ Sa Sb ]> [ Laa Lab Lba Lbb ] [ Sa Sb ] s.t: Sb = R\nfurther expanding we get: min Sa S>a LaaSa + S>a LabR+R>LbaSa +RLbbR\nusing \u2207Sa = 0, we get: 2LabR+ 2LaaSa = 0\nSa = \u2212L\u22121aaLabR"}, {"heading": "G. Other Approximate Decoders", "text": "Alternatively, if the complete data matrix Y is available then we can reduce the complexity further by performing a graphupsampling for only one of the two subspaces U or V .\n1) Approximate decoder 2 : Suppose we do the upsampling only for U , then the approximate decoder 2 can be written as:\nmin U\ntr(U>LrU) s.t: MrU = U\u0303 .\nThe solution for U is given by eq. 18. Then, we can write V as:\nV = Y >U \u03a3\u0303\u22121\n\u221a \u03c1c\u03c1r(1\u2212 \u03b4)\nnp\nHowever, we do not need to explicitly determine V here. Instead the low-rank X can be determined directly from U with the projection given below:\nX =U \u03a3\u0303\n\u221a np\n\u03c1c\u03c1r(1\u2212 \u03b4) V > = UU>Y.\n2) Approximate decoder 3 : Similar to the approximate decoder 2, we can propose another approximate decoder 3 which performs a graph upsampling on V and then determines U via matrix multiplication operation.\nmin V\ntr(V >LcV ) s.t: M>c V = V\u0303\nThe solution for V is given by eq. 18. Using the similar trick as for the approximate decoder 2, we can compute X without computing U . Therefore, X = Y V V >.\nFor the proposed approximate decoders, we would need to do one SVD to determine the singular values (\u0303\u03a3). However, note that this SVD is on the compressed matrix X\u0303 \u2208 <\u03c1r\u00d7\u03c1c . Thus, it is inexpensive O(\u03c12r\u03c1c) assuming that \u03c1r < \u03c1c."}, {"heading": "H. Computational Complexities & Additional Results", "text": "We present the computational complexity of all the models considered in this work. For a matrix X \u2208 <p\u00d7n, let I denote the number of iterations for the algorithms to converge, p is the number of features, n is the number of samples, \u03c1r, \u03c1c are the number of features and samples for the compressed data Y\u0303 and satisfy eq. (1) and theorem 1, k is the rank of the lowdimensional space or the number of clusters, K is the number of nearest neighbors for graph construction, Ol, Oc correspond to the number of iterations in the Lancoz and Chebyshev approximation methods. All the models which use the graph Gc are marked by \u2019+\u2019. The construction of graph Gr is included only in FRPCAG and CPCA. Furthermore,\n1) We assume that K, k, \u03c1r, \u03c1c, p << n and n+ p+ k +K + \u03c1r + \u03c1c \u2248 n. 2) The complexity of \u2016Y \u2212X\u20161 is O(np) per iteration and that of \u2016Y\u0303 \u2212 X\u0303\u20161 is O(\u03c1c\u03c1r). 3) The complexity of the computations corresponding to the graph regularization tr(XLcX>) + tr(X>LrX) = O(p|Ec|+\nn|Er|) = O(pnK + npK), where Er, Ec denote the number of non-zeros in Lr,Lc respectively. Note that we use the K-nearest neighbors graphs so Er \u2248 Kp and Ec \u2248 Kn. 4) The complexity for the construction of L\u0303c and L\u0303r for compressed data Y\u0303 is negligible if FLANN is used, i.e, O(\u03c1c\u03c1r log(\u03c1c)) and O(\u03c1c\u03c1r log(\u03c1r)). However, if the kron reduction strategy of Section IV-A is used then the cost is O(KOl(n+ p)) \u2248 O(KOln). 5) We use the complexity O(np2) for all the SVD computations on the matrix X \u2208 <p\u00d7n and O(\u03c1c\u03c12r) for X\u0303 \u2208 <\u03c1c\u00d7\u03c1r . 6) The complexity of \u2016MrXMc \u2212 X\u0303\u20162F is negligible as compared to the graph regularization terms tr(XLcX>) +\ntr(X>LrX). 7) We use the approximate decoders for low-rank recovery in the complexity calculations (eq. (18) in Section V-C). All the\ndecoders for low-rank recovery are summarized in Table X. 8) The complexity of k-means [10] is O(Inkp) for a matrix X \u2208 <p\u00d7n and O(I\u03c1r\u03c1ck) for a matrix X\u0303 \u2208 <\u03c1r\u00d7\u03c1c .\nTable X: A summary and computational complexities of all the decoders proposed in this work. The Lancoz method used here is presented in [39].\nType Low-rank model complexity Algo parallel\nminX \u2016MrXMc \u2212 X\u0303\u20162F O(n 3) \u2013 \u2013\nideal s.t: X> \u2208 span(Qkc ) X \u2208 span(Pkr )\nminX \u2016MrXMc \u2212 X\u0303\u20162F O(InpK) gradient no alter- +\u03b3c tr(XLcX>) descent nate +\u03b3r tr(X>LrX)\nminU \u2016MrU \u2212 U\u0303\u20162F O(InK) gradient yes +\u03b3 \u2032 r tr(U\n>LrU) descent approximate minV \u2016M>c V \u2212 V\u0303 \u20162F O(IpK) gradient yes\n+\u03b3 \u2032 c tr(V >LcV ) descent\nX = U\u03a3\u0303V > O(\u03c12r\u03c1c) SVD\nminU tr(U >LrU) O(pkOlK) PCG\ns.t:MrU = U\u0303 Subspace-\nUpsampling minV tr(V >LcV ) O(nkOlK) PCG yes s.t:M>c V = V\u0303\nX = U\u03a3\u0303V > O(\u03c12r\u03c1c) SVD\nminU tr(U >LrU) O(pkOlK) PCG yes\ns.t:MrU = U\u0303 approximate 2 X = UU>Y\nminV tr(V >LcV ) O(nkOlK) PCG yes\ns.t:M>c V = V\u0303 approximate 3 X = Y V V >\nTa bl\ne X\nI: C\nom pu\nta tio\nna l\nco m\npl ex\nity of\nal l\nth e\nm od\nel s\nco ns\nid er\ned in\nth is\nw or\nk\nM od\nel C\nom pl\nex ity\nC om\npl ex\nity C\nom pl\nex ity\nO ve\nra ll\nC om\npl ex\nity (lo\nw -r\nan k)\nO ve\nra ll\nC om\npl ex\nity (c\nlu st\ner in g) G c G r A lg or ith m Fa st SV D D ec od er To ta l km ea ns D ec od er To ta l O (n p lo g (n )) O (n p lo g (p )) fo r p n fo r p n [1 0]\nL E\n[4 ]\n+ \u2013\nO (n\n3 )\n\u2013 \u2013\n\u2013 O\n(I n k p )\n\u2013 O\n(n 3 )\nL L\nE [3\n2] \u2013\n\u2013 O\n(( p\n+ k )n\n2 )\n\u2013 \u2013\n\u2013 O\n(I n k p )\n\u2013 O\n(p n 2 )\nPC A\n\u2013 \u2013\nO (p\n2 n\n) \u2013\n\u2013 O\n(n (p\n2 lo\ng (n\n) + p 2 ))\nO (I n k p )\n\u2013 O\n(n p 2\nlo g (n\n) + n p 2 )\nG L\nPC A\n[1 4]\n+ \u2013\nO (n\n3 )\n\u2013 \u2013\nO (n\n(p lo\ng (n\n) + n 2 ))\nO (I n k p )\n\u2013 O\n(n 3 ))\nN M\nF [1\n7] \u2013\n\u2013 O\n(I n p k )\n\u2013 \u2013\nO (I n p k )\nO (I n k p )\n\u2013 O\n(I n p (k\n+ K )) G N M F [6 ] + \u2013 O (I n p k ) \u2013 \u2013 O (n p lo g (n )) O (I n k p ) \u2013 O (n p lo g (n )) M M F [4 6] + \u2013 O (( (p + k )k 2 + p k )I ) \u2013 \u2013 O (n p lo g (n )) O (I n k p ) \u2013 O (n p lo g (n )) R PC A [7 ] \u2013 \u2013 O (I n p 2 ) \u2013 \u2013 O (n p 2 I + n lo g (n )) O (I n k p ) \u2013 O (n p 2 I + n lo g (n )) R PC A G [3 5] \u2013 \u2013 O (I n p 2 ) \u2013 \u2013 O (n p 2 I + n lo g (n )) O (I n k p ) \u2013 O (n p 2 I + n lo g (n )) FR PC A G [3 6] + + O (I n p K ) \u2013 \u2013 O (n p lo g (n )) O (I n k p ) \u2013 O (n p lo g (n )) C PC A + + O (I \u03c1 c \u03c1 r K ) O (\u03c1 c \u03c1 2 r ) O (n k O lK ) O (n p lo g (n )) O (I \u03c1 c k \u03c1 r ) O (n k O l) O (n p lo g (n ))"}], "references": [{"title": "A variational approach to stable principal component pursuit", "author": ["A. Aravkin", "S. Becker", "V. Cevher", "P. Olsen"], "venue": "arXiv preprint arXiv:1406.1089,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "On the rate of convergence of the preconditioned conjugate gradient method", "author": ["O. Axelsson", "G. Lindskog"], "venue": "Numerische Mathematik, 48(5):499\u2013523,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation, 15(6):1373\u2013 1396,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 968\u2013977. Society for Industrial and Applied Mathematics,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(8):1548\u20131560,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Signal processing with compressive measurements", "author": ["M. Davenport", "P.T. Boufounos", "M.B. Wakin", "R.G. Baraniuk"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Kron reduction of graphs with applications to electrical networks", "author": ["F. Dorfler", "F. Bullo"], "venue": "Circuits and Systems I: Regular Papers, IEEE Transactions on, 60(1):150\u2013163,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["C. Elkan"], "venue": "ICML, volume 3, pages 147\u2013153,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Laplacian sparse coding, hypergraph laplacian sparse coding, and applications", "author": ["S. Gao", "I.-H. Tsang", "L.-T. Chia"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):92\u2013104,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust pca with compressed data", "author": ["W. Ha", "R.F. Barber"], "venue": "Advances in Neural Information Processing Systems, pages 1927\u20131935,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J.A. Tropp"], "venue": "SIAM review, 53(2):217\u2013288,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph-laplacian pca: Closed-form solution and robustness", "author": ["B. Jiang", "C. Ding", "J. Tang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3492\u20133498. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization with multiple hypergraph regularizers", "author": ["T. Jin", "J. Yu", "J. You", "K. Zeng", "C. Li", "Z. Yu"], "venue": "Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "How to learn a graph from smooth signals", "author": ["V. Kalofolias"], "venue": "th International Conference on Artificial Intelligence and Statistics AISTATS, Cadiz, Spain,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Identifying outliers in large matrices via randomized adaptive compressive sampling", "author": ["X. Li", "J. Haupt"], "venue": "Signal Processing, IEEE Transactions on, 63(7):1792\u20131807,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):171\u2013184,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph-regularized low-rank representation for destriping of hyperspectral images", "author": ["X. Lu", "Y. Wang", "Y. Yuan"], "venue": "IEEE transactions on geoscience and remote sensing, 51(7):4009\u20134018,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable nearest neighbour algorithms for high dimensional data", "author": ["M. Muja", "D. Lowe"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1), 2,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast randomized singular value thresholding for nuclear norm minimization", "author": ["T.-H. Oh", "Y. Matsushita", "Y.-W. Tai", "I.S. Kweon"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4484\u20134493,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Generalized laplacian precision matrix estimation for graph signal processing", "author": ["E. Pavez", "A. Ortega"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6350\u2013 6354. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "GSPBOX: A toolbox for signal processing on graphs", "author": ["N. Perraudin", "J. Paratte", "D. Shuman", "V. Kalofolias", "P. Vandergheynst", "D.K. Hammond"], "venue": "ArXiv e-prints,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "UNLocBoX A matlab convex optimization toolbox using proximal splitting methods", "author": ["N. Perraudin", "D. Shuman", "G. Puy", "P. Vandergheynst"], "venue": "ArXiv e-prints, #feb#", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Stationary signal processing on graphs", "author": ["N. Perraudin", "P. Vandergheynst"], "venue": "ArXiv e-prints, #jan#", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Random sampling of bandlimited signals on graphs", "author": ["G. Puy", "N. Tremblay", "R. Gribonval", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1511.05118,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "High dimensional low rank plus sparse matrix decomposition", "author": ["M. Rahmani", "G. Atia"], "venue": "arXiv preprint arXiv:1502.00182,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Randomized robust subspace recovery for big data", "author": ["M. Rahmani", "G.K. Atia"], "venue": "Machine Learning for Signal Processing (MLSP), 2015 IEEE 25th International Workshop on, pages 1\u20136. IEEE,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["N. Rao", "H.-F. Yu", "P.K. Ravikumar", "I.S. Dhillon"], "venue": "Advances in Neural Information Processing Systems, pages 2107\u20132115,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "A fast all nearest neighbor algorithm for applications involving large point-clouds", "author": ["J. Sankaranarayanan", "H. Samet", "A. Varshney"], "venue": "Computers &amp; Graphics, 31(2):157\u2013174,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Network topology identification from spectral templates", "author": ["S. Segarra", "A.G. Marques", "G. Mateos", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1604.02610,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust principal component analysis on graphs", "author": ["N. Shahid", "V. Kalofolias", "X. Bresson", "M. Bronstein", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1504.06151,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast robust pca on graphs", "author": ["N. Shahid", "N. Perraudin", "V. Kalofolias", "G. Puy", "P. Vandergheynst"], "venue": "IEEE Journal of Selected Topics in Signal Processing, 10(4):740\u2013756,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph dual regularization non-negative matrix factorization for co-clustering", "author": ["F. Shang", "L. Jiao", "F. Wang"], "venue": "Pattern Recognition, 45(6):2237\u2013 2250,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "Signal Processing Magazine, IEEE, 30(3):83\u201398,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated filtering on graphs using lanczos method", "author": ["A. Susnjara", "N. Perraudin", "D. Kressner", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1509.04537,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Matrix coherence and the nystrom method", "author": ["A. Talwalkar", "A. Rostamizadeh"], "venue": "arXiv preprint arXiv:1004.2008,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Low rank approximation with sparse integration of multiple manifolds for data representation", "author": ["L. Tao", "H.H. Ip", "Y. Wang", "X. Shu"], "venue": "Applied Intelligence, pages 1\u201317,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressive spectral clustering", "author": ["N. Tremblay", "G. Puy", "R. Gribonval", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1602.02018,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "On the conditioning of random subdictionaries", "author": ["J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis, 25(1):1\u201324,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Randomized algorithms for low-rank matrix factorizations: sharp performance bounds", "author": ["R. Witten", "E. Candes"], "venue": "Algorithmica, pages 1\u201318,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable sparse subspace clustering by orthogonal matching pursuit", "author": ["C. You", "D. Robinson", "R. Vidal"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, volume 1,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank matrix approximation with manifold regularization", "author": ["Z. Zhang", "K. Zhao"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(7):1717\u20131729,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Robust Principal Component Analysis (RPCA) [7], a linear dimensionality reduction algorithm can be used to exactly describe a dataset lying on a single linear lowdimensional subspace.", "startOffset": 43, "endOffset": 46}, {"referenceID": 18, "context": "Low-rank Representation (LRR) [19], on the other hand can be used for data drawn from multiple linear subspaces.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Many high dimensional datasets lie intrinsically on a smooth and very low-dimensional manifold that can be characterized by a graph G between the data samples [4].", "startOffset": 159, "endOffset": 162}, {"referenceID": 37, "context": "Then, the combinatorial Laplacian that characterizes the graph G is defined as L = D \u2212 W and its normalized form as Ln = D\u22121/2(D \u2212W )D\u22121/2 [38].", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "Extensions of RPCA and LRR such as Robust PCA on Graphs (RPCAG) [35] and Graph Regularized LRR (GLRR) [20] propose to incorporate graph regularization as a method to recover non-linear low-rank structures.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "Extensions of RPCA and LRR such as Robust PCA on Graphs (RPCAG) [35] and Graph Regularized LRR (GLRR) [20] propose to incorporate graph regularization as a method to recover non-linear low-rank structures.", "startOffset": 102, "endOffset": 106}, {"referenceID": 42, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 43, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 150, "endOffset": 154}, {"referenceID": 17, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 162, "endOffset": 166}, {"referenceID": 22, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 168, "endOffset": 172}, {"referenceID": 28, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 174, "endOffset": 178}, {"referenceID": 29, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 186, "endOffset": 190}, {"referenceID": 7, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 222, "endOffset": 225}, {"referenceID": 39, "context": "The case of large n can be tackled by using the sampling schemes accompanied with Nystrom method [40].", "startOffset": 97, "endOffset": 101}, {"referenceID": 44, "context": "Scalable extensions of LRR such as [45] exist but they focus only on the subspace clustering application.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "al [1]", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "The recently introduced Fast Robust PCA on Graphs (FRPCAG) [36] approximates a recovery method for non-linear low-rank datasets, which are called Low-rank matrices on graphs.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "Inspired by the underlying stationarity assumption [27], the authors introduce a joint notion of lowrankness for the features and samples (rows and columns) of a data matrix.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 145, "endOffset": 149}, {"referenceID": 40, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 151, "endOffset": 155}, {"referenceID": 45, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 5, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 169, "endOffset": 172}, {"referenceID": 36, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 27, "context": "Our proposed framework is inspired by the recently introduced sampling of band-limited signals on graphs [28].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "While we borrow several concepts from here, our framework is significantly different from [28] in many contexts.", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "We target the low-rank recovery of matrices, whereas [28] targets the recovery of band-limited signals / vectors.", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "The design of a sampling scheme is the major focus of [28], while we just focus on the case of uniform sampling and instead focus on how much to sample jointly given the two graphs.", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "Of course, our method can be extended directly for the other sampling schemes in [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Unlike [28], we target two applications related to PCA: 1) low-rank recovery and 2) clustering.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "Thus, contrary to [28] our proposed decoders are designed for these applications.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "A major contribution of our work in contrast to [28] is the design of approximate decoders for low-rank recovery and clustering which significantly boost the speed of our framework for big datasets without compromising on the performance.", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "Then, low-rank matrices on graphs can be defined as following and recovered by solving FRPCAG [36].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "Throughout this work we use the approximate nearest neighbor algorithm (FLANN [21]) for graph construction whose complexity is O(np log(n)) for p n [33] (and it can be performed in parallel).", "startOffset": 78, "endOffset": 82}, {"referenceID": 32, "context": "Throughout this work we use the approximate nearest neighbor algorithm (FLANN [21]) for graph construction whose complexity is O(np log(n)) for p n [33] (and it can be performed in parallel).", "startOffset": 148, "endOffset": 152}, {"referenceID": 37, "context": "In the above equations Qkc\u2206 c i and P > kr \u2206j characterize the first kc and kr fourier modes [38] of the nodes i and j on the graphs Gc and Gr respectively.", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "need to be sampled from the graphs Gr and Gc such that the properties of the graphs are preserved [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "Theorem 1 is a direct extension of the RIP for k-bandlimited signals on one graph [28].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "It is proved in [28] that \u03bdkc \u2265 \u221a kc and \u03bdkr \u2265 \u221a kr.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "One should resort to a more distribution aware sampling in such a case as presented in [28].", "startOffset": 87, "endOffset": 91}, {"referenceID": 27, "context": "A consequence of the result in [28] is that there always exist distributions that ensure that the RIP holds when sampling O(kr log(kr)) rows and O(kc log(kc)) columns only.", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "The optimal sampling distribution for which this result holds is defined in [28] (see Section 2.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "Furthermore, a fast algorithm to compute this distribution also exists (Section 4 of [28]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "To ensure the preservation of algebraic and spectral properties one can construct the compressed Laplacians L\u0303r \u2208 <\u03c1r\u00d7\u03c1r and L\u0303c \u2208 <\u03c1c\u00d7\u03c1c from the Kron reduction of Lr and Lc [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 8, "context": "4 of [9] two nodes \u03b1, \u03b2 are not connected in L\u0303c if there is no path between them in Lc via \u03a9\u0304.", "startOffset": 5, "endOffset": 8}, {"referenceID": 27, "context": "Such schemes have been discussed in [28] and have not been addressed in this work.", "startOffset": 36, "endOffset": 40}, {"referenceID": 38, "context": "The only expensive operation above is the inverse of L(\u03a9\u0304, \u03a9\u0304) which can be performed with O(OlKn) cost using the Lancoz method [39], where Ol is the number of iterations for Lancoz approximation.", "startOffset": 128, "endOffset": 132}, {"referenceID": 35, "context": "The low-rank matrix X\u0303 = X\u0303\u2217+\u1ebc can be recovered by solving the FRPCAG problem as proposed in [36] and re-written below:", "startOffset": 93, "endOffset": 97}, {"referenceID": 35, "context": "From Theorem 1 in [36], the low-rank approximation error comprises the orthogonal projection of X\u0303\u2217 on the complement graph eigenvectors (  \u0304\u0303 Qkc ,  \u0304\u0303 Pkr ) and depends on the spectral gaps \u03bb\u0303kc/\u03bb\u0303kc+1, \u03bb\u0303kr/\u03bb\u0303kr+1 as following: \u2016X\u0303\u2217  \u0304\u0303 Qkc\u2016F + \u2016  \u0304\u0303 P> krX\u0303 \u2016F = \u2016\u1ebc\u2016F \u2264 1 \u03b3 \u03c6(E) + \u2016\u1ef8 \u2016F ( \u03bb\u0303kc", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "(12) &(13) are feasible solutions of the joint non-convex, factorized, and graph regularized low-rank optimization problem like the one presented in [31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "and B be the subspaces that we want to recover then we can re-write the problem studied in [31] as following:", "startOffset": 91, "endOffset": 95}, {"referenceID": 41, "context": "We refer to the Compressive Spectral Clustering (CSC) framework [42], where the authors solve a similar problem by arguing that each of the columns of C can be obtained by assuming that it lies close to the span(Qkc), where Qkc are the first kc Laplacian eigenvectors of the graph Gc.", "startOffset": 64, "endOffset": 68}, {"referenceID": 27, "context": "(30) in the proof of Theorem 1 and Theorem 5 in [28]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "2 in [28].", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "More specifically, one can refer to [3] for a detailed study on FISTA and [2] for PCG.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "More specifically, one can refer to [3] for a detailed study on FISTA and [2] for PCG.", "startOffset": 74, "endOffset": 77}, {"referenceID": 25, "context": "We perform two types of experiments corresponding to two applications of PCA 1) Data clustering and 2) Low-rank recovery using two open-source toolboxes: the UNLocBoX [26] and the GSPBox [25].", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "We perform two types of experiments corresponding to two applications of PCA 1) Data clustering and 2) Low-rank recovery using two open-source toolboxes: the UNLocBoX [26] and the GSPBox [25].", "startOffset": 187, "endOffset": 191}, {"referenceID": 34, "context": "1) Experimental Setup: Datasets: We perform our clustering experiments on 5 benchmark databases (as in [35], [36]): CMU PIE, ORL, YALE, MNIST and USPS.", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "1) Experimental Setup: Datasets: We perform our clustering experiments on 5 benchmark databases (as in [35], [36]): CMU PIE, ORL, YALE, MNIST and USPS.", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 165, "endOffset": 168}, {"referenceID": 31, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 255, "endOffset": 259}, {"referenceID": 45, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 311, "endOffset": 315}, {"referenceID": 16, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 359, "endOffset": 363}, {"referenceID": 5, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 425, "endOffset": 428}, {"referenceID": 6, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 450, "endOffset": 453}, {"referenceID": 34, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 487, "endOffset": 491}, {"referenceID": 35, "context": "11) Fast Robust PCA on Graphs (FRPCAG) [36].", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "Gr, Gc are constructed using FLANN [22] as discussed in Section II.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "The state-of-the-art results [16], [24], [34] do not provide any scalable solutions yet.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "The state-of-the-art results [16], [24], [34] do not provide any scalable solutions yet.", "startOffset": 35, "endOffset": 39}, {"referenceID": 33, "context": "The state-of-the-art results [16], [24], [34] do not provide any scalable solutions yet.", "startOffset": 41, "endOffset": 45}, {"referenceID": 35, "context": "The time reported here corresponds to steps 2 to 6 of Table I, Algorithm 1 of [36] for FRPCAG, [7] for RPCA and [35] for RPCAG, excluding the construction of graphs Gr, Gc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "The time reported here corresponds to steps 2 to 6 of Table I, Algorithm 1 of [36] for FRPCAG, [7] for RPCA and [35] for RPCAG, excluding the construction of graphs Gr, Gc.", "startOffset": 95, "endOffset": 98}, {"referenceID": 34, "context": "The time reported here corresponds to steps 2 to 6 of Table I, Algorithm 1 of [36] for FRPCAG, [7] for RPCA and [35] for RPCAG, excluding the construction of graphs Gr, Gc.", "startOffset": 112, "endOffset": 116}], "year": 2016, "abstractText": "We introduce a novel framework for an approximate recovery of data matrices which are low-rank on graphs, from sampled measurements. The rows and columns of such matrices belong to the span of the first few eigenvectors of the graphs constructed between their rows and columns. We leverage this property to recover the non-linear low-rank structures efficiently from sampled data measurements, with a low cost (linear in n). First, a Resrtricted Isometry Property (RIP) condition is introduced for efficient uniform sampling of the rows and columns of such matrices based on the cumulative coherence of graph eigenvectors. Secondly, a state-of-the-art fast low-rank recovery method is suggested for the sampled data. Finally, several efficient, parallel and parameter-free decoders are presented along with their theoretical analysis for decoding the low-rank and cluster indicators for the full data matrix. Thus, we overcome the computational limitations of the standard linear low-rank recovery methods for big datasets. Our method can also be seen as a major step towards efficient recovery of nonlinear low-rank structures. For a matrix of size n \u00d7 p, on a single core machine, our method gains a speed up of p/k over Robust Principal Component Analysis (RPCA), where k p is the subspace dimension. Numerically, we can recover a low-rank matrix of size 10304\u00d71000, 100 times faster than Robust PCA.", "creator": "LaTeX with hyperref package"}}}