{"id": "1602.02697", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Practical Black-Box Attacks against Machine Learning", "abstract": "Deep chirambo Learning essonne is increasingly used baroud in oldstone several regurgitator machine 5-foot-7 learning tasks as Deep fairway Neural samsoe Networks (sisq\u00f3 DNNs) shinkong frequently evangelistarium outperform other oxidizers techniques. kolhapure Yet, previous work showed that, once deployed, kingshurst DNNs windspeeds are vulnerable staufer to integrity zuiverloon attacks. Indeed, randhawa adversaries can control satchels DNN outputs and ensay for braker instance force 3,531 them avaricious to begala misclassify inputs multihued by \u0161t\u011bp\u00e1n adding population a 134.00 carefully crafted delirious and chilapa undistinguishable millner perturbation. However, tugaloo these dentalium attacks assumed knowledge of 644,000 the catholique targeted DNN ' energy s nyt8 architecture and disease parameters. In 44-0 this paper intisar however, we release fokker-50 these assumptions and introduce extremis an helmore attack fullscale conducted under investigaciones the zandi more eidetic realistic, yet more 114.8 complex, telefutura threat phrygians model trebelno of launderette an oracle: adversaries pawnbroking are 2day only malayala capable 63,700 of tovey accessing oahe DNN zauberfloete label nymphet predictions for chosen inputs. We peacekeepers evaluate our kijang attack tlc in shooed real - qotbi world 117.2 settings toynbee by subshrub successfully vietnam-era forcing trocaire an oracle served by MetaMind, monos an online nyama API vasomotor for DNN classifiers, to misclassify inputs at a breakwaters 84. tugba 24% rate. ebata We tesis also public perform an alstyne evaluation 2013/2014 to openwrt fine - tune yizheng our prevoisin attack redirected strategy herself and renos maximize the oracle ' 18s s ozuna misclassification rate for viremia adversarial spac samples.", "histories": [["v1", "Mon, 8 Feb 2016 19:12:25 GMT  (5529kb,D)", "http://arxiv.org/abs/1602.02697v1", null], ["v2", "Fri, 19 Feb 2016 01:49:44 GMT  (5484kb,D)", "http://arxiv.org/abs/1602.02697v2", null], ["v3", "Mon, 7 Nov 2016 00:01:18 GMT  (5724kb,D)", "http://arxiv.org/abs/1602.02697v3", null], ["v4", "Sun, 19 Mar 2017 14:50:18 GMT  (5512kb,D)", "http://arxiv.org/abs/1602.02697v4", "Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE"]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["nicolas papernot", "patrick mcdaniel", "ian goodfellow", "somesh jha", "z berkay celik", "ananthram swami"], "accepted": false, "id": "1602.02697"}, "pdf": {"name": "1602.02697.pdf", "metadata": {"source": "CRF", "title": "On the Integrity of Deep Learning Oracles", "authors": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow"], "emails": ["ngp5056@cse.psu.edu", "mcdaniel@cse.psu.edu", "goodfellow@google.com", "jha@cs.wisc.edu", "zbc102@cse.psu.edu", "ananthram.swami.civ@mail.mil"], "sections": [{"heading": null, "text": "Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN\u2019s architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24% rate. We also perform an evaluation to finetune our attack strategy and maximize the oracle\u2019s misclassification rate for adversarial samples."}, {"heading": "1 Introduction", "text": "Deep Neural Networks are transforming the field of machine learning by outperforming humans at various tasks like classification [11] and reinforcement learning [17], or allowing for breakthroughs in promising areas like unsupervised learning [25]. This generated a wide interest in deep learning, the machine learning technique which uses neural networks to represent a hierarchy of increasingly complex concepts [3]. The benefits of using hierarchical representations of data include automatic feature extraction from the inputs [25]. Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].\nIt has however been shown that deployed deep neural networks are vulnerable to integrity attacks [10, 20, 27]. Namely, adversarial samples are inputs crafted by adding indistinguishable perturbations to force a trained and deployed DNN to misclassify them, while being undetected \u2013 correctly classified \u2013 by humans. For instance, consider a DNN used by a car\u2019s driverless system to classify signs identified on the roadside [6]. An adversary capable of crafting adversarial samples can sligtly perturb the input of such a system by altering a STOP sign, resulting in the DNN misclassifying the sign and ultimately in the car not stopping, potentially endangering lives. Previous approaches [10, 20, 27] evaluated the perturbations required to craft adversarial samples using knowledge of the DNN architecture and parameters. This limited their applicability to strong adversaries with the capability of gaining insider knowledge of the targeted deep learning based system.\nIn this paper, we instead consider a weak adversary whose capabilities are limited to accessing the targeted deep neural network\u2019s output. Specifically, the adversary has access to an oracle returning the DNN output for any input chosen by the adversary. The adversarial goal remains forcing the oracle to misclassify inputs. Our approach builds on previous attacks to craft adversarial samples under this more complex threat model.\nMore precisely, we propose using a substitute DNN approximating the model learned by the targeted oracle to skirt the lack of knowledge about the oracle\u2019s architecture and parameters. Training a substitute DNN gives us the benefit of having full access to its architecture and parameters, enabling us to employ previously introduced adversarial sample crafting techniques. We then leverage the generalizability property of adversarial samples [10]: adversarial samples crafted for a given DNN are also largely misclassified by other DNNs trained to solve the same machine learning task. Thus, adversarial samples crafted using our substitute DNN are largely misclassified by the targeted oracle, as evaluated in Section 5.\nar X\niv :1\n60 2.\n02 69\n7v 1\n[ cs\n.C R\n] 8\nF eb\n2 01\nThe contributions of this paper are the following:\n\u2022 We introduce in Section 4 an attack strategy targeting the integrity of deep learning oracles. The technique stems from training a substitute DNN using a Jacobian-based dataset augmentation. The substitute DNN allows adversaries to craft adversarial samples misclassified by the oracle without knowledge of the oracle\u2019s architecture or parameters.\n\u2022 Our dataset augmentation technique drastically reduces the number of oracle queries, thus making the attack tractable, as shown in Section 5.\n\u2022 We demonstrate the consequences of our attack by successfully forcing the online MetaMind API to misclassify inputs from the MNIST dataset with a 84.24% rate. These inputs are crafted using perturbations hard to distinguish for humans."}, {"heading": "2 About Deep Neural Network Training", "text": "In this section, we provide rudiments of deep learning to facilitate the understanding of our threat model and attack strategy. We refer readers interested by a more complete and detailed presentation of deep neural networks to the excellent book by Goodfellow et al. [3].\nA Deep Neural Network, as illustrated in its simplest form in Figure 1, is a machine learning technique that uses a hierarchical composition of n parametric functions to model a high dimensional input~x [3, 4]. Each function fi for i \u2208 1..n is modeled using a layer of neurons, which are essentially elementary computing units applying an activation function to the previous layer\u2019s weighted representation of the input to output a new representation. Each neuron layer is parameterized using a weight vector \u03b8i applied to the input of each neuron. Such weights essentially hold the knowledge of a deep neural network model F and are evaluated during its training phase, as detailed below. Thus, the composition of functions modeled by deep neural networks can be formalized as:\nF(~x) = fn (\u03b8n, fn\u22121 (\u03b8n\u22121, ... f2 (\u03b82, f1 (\u03b81,~x)))) (1)\nThe training phase of a neural network F fixes values for its set of parameters \u03b8F = {\u03b81, ...,\u03b8n}. In this paper, we focus on the case of supervised learning where the task of the DNN is to model a known input-output relation. One example of such a task is classification, where the goal is to assign inputs a label among a predefined set of labels. To do so, the network is given a large set of known input-output pairs (~x,~y) and it adjusts weight parameters to reduce a cost quantifying the prediction error between the model prediction F(~x) and the correct output~y. The adjustment is typically performed using techniques derived from the backpropagation algorithm [28].\nBriefly speaking, such techniques successively propagate error gradients with respect to network parameters from the output layer of the network to its input layer.\nDuring the test phase, the DNN is deployed with a fixed set of parameters \u03b8F and is used to make predictions on inputs unseen during training. For instance, in our paper we consider DNNs used as classifiers: for a given input~x, the neural network produces a probability vector F(~x) encoding its belief of input~x being in each of the predefined classes (cf. Figure 1). The weight parameters \u03b8F hold the knowledge acquired by the model during training. Ideally, the learning algorithm used during the training phase should allow the model to generalize so as to make accurate predictions for inputs outside of the domain explored during training. However, this is not the case in practice as shown by previous attacks manipulating DNN outputs using adversarial samples [10, 20, 27]."}, {"heading": "3 Threat Model", "text": "We now describe our threat model for attacks against deep learning oracles. The adversarial goal is to force a targeted DNN to misbehave: for instance, if the network is used for classification, the adversary seeks to force the network to misclassify an input in a class different from its correct class. To achieve this attack, we consider a weak adversary with the only adversarial capability of accessing the output of the targeted neural network. In other terms, the adversary has no knowledge of the architectural choices made to design the DNN, which include the number, type, and size of layers, nor of the training data used to evaluate the parameters of the\nDNN. As all previous attacks depended on such knowledge [10, 20, 27], this threat model is particularly more challenging and, to the best of our knowledge, it was previously unknown whether a tractable attack was possible.\nA taxonomy of threat models for deep learning deployed in adversarial settings is introduced in [20]. In the present paper, we consider attackers targeting a DNN used as a multi-class classifier. Such a DNN produces outputs taking the form of probability vectors. Each vector component encodes the network\u2019s belief of the input being part of one of the predefined classes. To illustrate our attack scenario, we consider the ongoing example of a DNN used for classifying images, as illustrated in Figure 1. Such an architecture can be used to classify handwritten digits into classes corresponding to digits between 0 and 9, images of objects in a fixed number of categories, or images of traffic signs into classes corresponding to sign types (STOP, yield, ...).\nAdversarial Goal - The adversarial goal is to take any input ~x and produce a slightly altered version of this input, named adversarial sample and denoted by ~x\u2217, that has the property of being misclassified by oracle O. This corresponds to an attack against the oracle\u2019s output integrity. Thus, the adversarial sample is such that O\u0303(~x\u2217) 6= O\u0303(~x), and solves the following optimization problem:\n~x\u2217 =~x+min{\u03b4~x : O\u0303(~x+\u03b4~x) 6= O\u0303(~x)} (2) Some examples of adversarial samples can be found in Figure 2. The first row contains legitimate samples while the second row contains corresponding adversarial samples that are misclassified. This misclassification must be achieved by adding a minimal perturbation \u03b4~x so as to evade human detection. Even with total knowledge of the network architecture used to train model O as well as the parameters resulting from training, finding such a minimal perturbation is not trivial, as properties of deep neural networks preclude the optimization problem from being linear or convex. This problem is complexified by our threat model: removing knowledge of model O\u2019s architecture and parameters makes it even harder to find a perturbation such that O\u0303(~x+\u03b4~x) 6= O\u0303(~x) holds.\nAdversarial Capabilities - The targeted DNN is referred to as the oracle and denoted by O. The name oracle refers to the only capability of the adversary: accessing the DNN\u2019s label O\u0303(~x) for any input~x by querying the oracle O. In this paper, we consider the output O\u0303(~x) to take the form of a label, which is defined as the index of the class assigned the largest probability by the DNN:\nO\u0303(~x) = arg max j\u22080..N\u22121 O j(~x) (3)\nwhere O j(~x) is the j-th component of the probability vector O(~x) output by network O on input~x. The distinction\nbetween labels and probabilities is important as it makes adversaries more realistic - users more often have access to DNN labels than output vectors in real world applications - but weaker as labels encode much less information about the model\u2019s learned characteristics than probability vectors. Accessing labels O\u0303 is the only capability of adversaries considered in our threat model, unlike previous approaches describing attacks against DNN output integrity [10, 20, 27]. As such, we do not assume that the adversary has access to the oracle\u2019s architecture, parameters, or training data. Again, this weaker adversary makes our threat model more realistic but in turn makes the attack much harder to execute."}, {"heading": "4 Attack Methodology", "text": "We introduce here an attack against DNN oracles under the threat model described in Section 3. The adversary wants to craft adversarial samples misclassified by the oracle O\u0303 using his/her sole capability of accessing the label assigned by oracle O\u0303 for any chosen input. Briefly speaking, the attack strategy consists in learning a substitute model approximating the targeted oracle and then use this substitute model to craft adversarial samples. As adversarial samples transfer between DNN architectures, we expect these samples to be misclassified by the oracle.\nTo understand the difficulty of conducting the attack under this threat model, recall Equation 2, which formalizes the adversarial goal of finding a minimal perturbation \u03b4~x resulting in an adversarial sample misclassified by the targeted DNN oracle O. Unfortunately for the attacker but fortunately for the defender, Equation 2 cannot be solved in closed form. In previous work [10, 20, 27], an approximated solution to Equation 2 was found by exploiting a precise understanding of the mapping learned by DNNs between inputs and\nexpected outputs. Such an understanding was built by exploiting gradients of the DNN\u2019s cost function with respect to network inputs [10, 27] or the DNN\u2019s Jacobian matrix [20]. However, such techniques require knowledge of the DNN architecture and parameters, making it impossible to directly implement the attack in realistic environments like the one described in our threat model. Indeed, we assumed adversaries possess no knowledge about the model learned by network O and therefore such adversaries cannot compute gradients or Jacobian matrices required in previously proposed methods.\nTo overcome this challenge, we introduce in this paper a new attack against deep neural network integrity that builds on attacks previously described [10, 20, 27], but executable under the oracle threat model described in Section 3. The intuition behind our approach comes from the transferability property of adversarial samples: an adversarial sample misclassified by a given DNN architecture A is likely to be misclassified by a different DNN architecture B [10]. Thus, our intuition is that if adversaries use oracle outputs to get an accurate approximation F of the model O learned by the oracle, then they can use this approximated model F , which we name the substitute network, to craft adversarial samples misclassified by F that will in turn transfer and be misclassified by the oracle DNN O.\nIndeed, as adversaries have full knowledge of the DNN architecture and parameters of the substitute network, they can use one of the previously described attack techniques [10, 20] to craft adversarial samples misclassified by F . As long as the transferability property holds between F and O, adversarial samples crafted for F will also be misclassified by O. This leads us to propose the following two-fold attack strategy:\n1. Substitute Model Training: the attacker uses a set of queries to the oracle to build a model F approximating the oracle model O\u2019s decision boundaries.\n2. Adversarial Sample Crafting: the attacker uses\nsubstitute network F to craft adversarial samples misclassified by oracle O.\nWe now describe both strategy folds in more details. The most crucial step is training the substitute model using the very limited knowledge available. It heavily conditions the success of adversarial crafting."}, {"heading": "4.1 Substitute Model Training", "text": "Training a model F approximating oracle O is challenging because: (1) we must select an architecture for our substitute DNN without knowledge of the targeted oracle\u2019s architecture, and (2) we must limit the number of queries made to the oracle in order to ensure that the approach is tractable. Our approach overcomes these challenges mainly by introducing a dataset augmentation technique, which we name Jacobian-based Dataset Augmentation. Let us emphasize that our augmentation technique is not aimed at improving the substitute DNN\u2019s accuracy but rather ensure it is approximates the oracle\u2019s decision boundaries. The training procedure described below is illustrated in Figure 3.\nSubstitute Architecture - Factor (1) is not strongly limiting as the adversary must at least have some partial knowledge of the targeted oracle\u2019s input nature (e.g., images, text, web documents) and the expected output (e.g., multi-class classification). The adversary can thus use an architecture adapted to the input-output relation nature. For instance, a convolutional neural network is suitable for image classification. Adversaries can also consider performing an architecture exploration and train several substitute models before selecting the one that conduces to the highest attack success rate. Exploration of the Input Domain - To better understand factor (2), note that we could potentially make an infinite number of queries to query the oracle\u2019s output O(~x) for any input~x belonging to the input domain. This would provide us with a copy of the model learned by the\noracle. However, this is simply not tractable: consider a network with M input components, each taking discrete values among a set of K possible values, the number of possible inputs to be queried is KM . The intractability is even stronger for inputs in the continuous domain. Furthermore, making large quantities of queries renders the adversarial behavior easy to detect for a defender. In order to address this issue, we therefore use an heuristic that performs an efficient exploration of the input domain and, as shown in Section 5, drastically limits the number of queries made to the oracle. Furthermore, our technique also ensures that the substitute network is an accurate approximation of the targeted oracle model, i.e. that it learns similar decision boundaries.\nThe heuristic is based on identifying directions in which the model is most sensitive, around an initial set of training points. The intuition, as illustrated in Figure 4, is that such sensitive directions will require more input-output pairs (produced by querying the oracle) to capture the output variations from model O compared to directions with lower sensitivity. Therefore, to get a substitute network accurately approximating the oracle\u2019s decision boundaries, the heuristic prioritizes samples taken in sensitive directions for queries made to the oracle. To\nAlgorithm 1 - Substitute DNN Training: the algorithm takes as inputs the oracle O\u0303, the maximum number max\u03c1 of substitute training epochs to be performed, a substitute architecture F , and an initial training set S0.\nInput: O\u0303, max\u03c1 , S0 1: Define architecture F 2: \u03c1 \u2190 0 3: while \u03c1 < max\u03c1 do 4: D = { (~x, O\u0303(~x)) :~x \u2208 S\u03c1 }\n5: Train F on D to evaluate parameters \u03b8F 6: S\u03c1+1\u2190{~x+\u03b5\u03c1+1 \u00b7 sgn(JF\u03c1 [O\u0303(~x)]) :~x \u2208 S\u03c1}\u222aS\u03c1 7: \u03c1 ++ 8: end while 9: return \u03b8F\nidentify such directions, the substitute DNN\u2019s Jacobian matrix JF is evaluated at several input points (the method used to choose these points is described later in this section). Sensitive directions of substitute network F for any input~x are then found by computing the following:\nsgn ( JF(~x)[O\u0303(~x)] ) (4)\nwhich computes the sign of the Jacobian matrix\u2019s dimension corresponding to the oracle\u2019s label for input ~x. Thus, we name this technique Jacobian-based Dataset Augmentation. We base our training algorithm on the idea of iteratively refining the model in sensitive directions by making additional queries to the oracle.\nSubstitute DNN Training Algorithm - The resulting training procedure is a five step process outlined in Algorithm 1 and described more specifically below:\n1. Substitute Training Set Initial Collection: The adversary first collects a very limited number of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit between 0 and 9. This constitutes the initial training set S0 for the substitute DNN architecture. We show in Section 5 that the substitute training set does not necessarily have to come from the distribution from which the targeted oracle was trained.\n2. Substitute DNN F Architecture Selection: The adversary selects a DNN architecture to be trained as the substitute DNN model F . Again, this can be done using high-level knowledge of the classification task performed by the targeted oracle (e.g., convolutionnal neural networks offer good performance for image classification)\n3. Substitute Dataset Labeling: Using access to labels O\u0303(~x) output by oracle O for any input ~x, the\nadversary labels its initial substitute training set S0 by querying the oracle for each~x \u2208 S0.\n4. Substitute DNN F Training: The adversary trains the architecture chosen at step (2) using the substitute training set S0. This is performed using the classical training techniques used for DNNs.\n5. Jacobian-based Dataset Augmentation: The adversary now performs Jacobian-based Dataset Augmentation on the initial substitute training set S0 to produce a new substitute training set S1 with more samples. This allows the training set to better represent the model\u2019s decision boundaries. The adversary repeats steps (3) and (4) with the augmented training dataset.\nNote that steps (3),(4), and (5) can be repeated several times to increase both the accuracy of substitute network F and the similarity of its decision boundaries with those of the oracle. We introduce the notion of substitute training epoch to refer to each iteration performed, and index epochs with \u03c1 . This leads to the following formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute DNN training algorithm:\nS\u03c1+1 = {~x+ \u03b5\u03c1+1 \u00b7 sgn(JF\u03c1 [O\u0303(~x)]) :~x \u2208 S\u03c1}\u222aS\u03c1 (5)\nwhere \u03b5\u03c1+1 is a parameter of the augmentation technique: it defines the size of the step taken in the sensitive direction identified by the sign of the Jacobian matrix to augment the dataset S\u03c1 into S\u03c1+1."}, {"heading": "4.2 Adversarial Sample Crafting", "text": "Once the adversary has successfully trained a substitute DNN, the next step of our strategy is to craft adversarial samples. This part of the attack is performed by implementing two previously introduced approaches described in [10, 20]. We now provide a brief overview of both adversarial sample crafting techniques, namely the Goodfellow et al. attack strategy and the Papernot et al. attack strategy. Both strategies share a similar intuition of evaluating the model\u2019s sensitivity to input components in order to select an small perturbation achieving the adversarial misclassification goal.\nGoodfellow et al. attack - This strategy, also known as the fast gradient sign method, is suitable for misclassification attacks [10]. Given a model F with an associated cost function c(F,~x,y), the adversary crafts an adversarial sample ~x\u2217 =~x+\u03b4~x for a given legitimate sample~x by computing the following perturbation:\n\u03b4~x = \u03b5 sgn(\u2207~xc(F,~x,y)) (6)\nwhere perturbation sgn(\u2207~xc(F,~x,y)) is the sign of the model\u2019s cost function gradient. The cost gradient is computed with respect to ~x using sample ~x and label y as inputs. The value of the intensity variation parameter factoring the sign matrix controls the perturbation\u2019s amplitude. Increasing its value increases the likelihood of ~x\u2217 being misclassified by model F but on the contrary makes adversarial samples easier to detect by humans.\nPapernot et al. attack - This strategy is suitable for source-target misclassification attacks where adversaries seek to take samples from any legitimate source class to any chosen target class [20]. The misclassification attack is a special case of the source-target misclassification attack where the target class can be any class different from the legitimate source class. Given a model F , the adversary crafts an adversarial sample ~x\u2217 =~x+ \u03b4~x for a given legitimate sample ~x by adding a perturbation. Perturbation \u03b4~x is a subset of the input components~xi.\nTo choose the input components forming perturbation \u03b4~x, components are first sorted by decreasing adversarial saliency value. The adversarial saliency value S(~x, t)[i] of input component i for an adversarial target class t is defined as:\nS(~x, t)[i] =    0 if \u2202Ft\u2202~xi (~x)< 0 or \u2211 j 6=t \u2202Fj \u2202~xi\n(~x)> 0 \u2202Ft \u2202~xi (~x) \u2223\u2223\u2223\u2211 j 6=t \u2202Fj \u2202~xi (~x) \u2223\u2223\u2223 otherwise (7)\nwhere matrix JF = [\n\u2202Fj \u2202~xi ] i j is the model\u2019s Jacobian\nmatrix. Input components i are added to perturbation \u03b4~x in order of decreasing adversarial saliency value S(~x, t)[i] until the resulting adversarial sample ~x\u2217 = ~x + \u03b4~x is misclassified by model F . The perturbation introduced for each selected input component can vary. Greater individual variations tend to reduce the number of components perturbed to achieve misclassification.\nEach attack strategy has its benefits and drawbacks. The Goodfellow strategy is well suited for fast crafting of large amounts of adversarial samples in the context of a misclassification attack. The Papernot strategy provides the flexibility of choosing the adversarial target class and reduces the perturbation introduced at the expense of a greater computing cost."}, {"heading": "5 Validation", "text": "To test the applicability of our attack strategy under realworld conditions, we first apply it to target an oracle provided online by MetaMind, a startup serving an online1 classification API built using techniques from deep\n1The MetaMind API can be accessed online at www.metamind.io\nlearning. The MetaMind API returns labels produced by a trained DNN model for any given input, but does not provide access to the model architecture and parameters. We use the MetaMind API as an oracle DNN classifier for the MNIST handwritten digit classification task [16]. We show that an adversary implementing our oracle attack strategy can reliably force the oracle provided by MetaMind to misclassify 84.24% of its inputs by introducing a perturbation indistinguishable to humans."}, {"heading": "5.1 Validation Setup", "text": "The dataset used in this set of validation experiments is the MNIST handwritten digit dataset [16]. It is made of 60,000 training samples, 10,000 validation samples, and 10,000 test samples. Each sample is an image of a handwritten digit. The task associated with this dataset is to train a classifier to identify the digit written in each image. Each 28x28 grayscale image is encoded as a vector of pixel intensities normalized in the interval [0,1] and obtained by reading the image pixel matrix row-wise.\nWe registered for an API access key on MetaMind\u2019s website, which gave us access to the following functionalities: dataset upload, model training, and model prediction querying. We uploaded the 50,000 samples included in the MNIST training set to MetaMind\u2019s servers and then used the API to train a classifier on the dataset. Again, we are not provided with details regarding the implementation of the training algorithm, or the architecture trained nor its resulting parameters. All we are given is the accuracy of the resulting model, computed by MetaMind using a validation set created by isolating 10% of the samples we provided for training. Details can be found on MetaMind\u2019s webpage for vision classifiers2.\nThe training took approximatively 36 hours and returned a classifier model showcasing a 94.97% accuracy. This classification performance cannot be improved as we are not able to access or modify the model\u2019s specifications and training algorithms. Once training is completed, we access the model\u2019s predictions, for any input of our choice, through the API. Predictions take the form of probability vectors. However, we only use the label indicating the class assigned the highest probability to serve as the oracle O\u0303. This corresponds to adversarial capabilities of the threat model described in Section 3."}, {"heading": "5.2 Attack Using a Substitute Training Set extracted from the MNIST test set", "text": "Substitute DNN Training - Using an initial substitute training set of 150 samples extracted from the MNIST test set, we train a substitute DNN as described in Section 4. Note that these samples are distinct from those\n2https://www.metamind.io/vision/train\nseen by the network during training as they are from the test set and not the training set. We assumed that they can be collected by an adversary under the threat model described in Section 3 using minimal knowledge of the classification task solved by the targeted oracle: in our case, handwritten digit classification. Later in Section 5.3, we show that substitute training also works with samples not extracted from the MNIST dataset. Among the 150 samples making up the initial substitute training set, 100 are used for training and 50 for validation.\nThe architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23]. The architecture is trained for three substitute training epochs on our local machine3. We use Theano [2, 5] together with Lasagne [8] to train all DNNs in this paper. During each of these substitute training epochs, the DNN is trained for 10 epochs from scratch with a learning rate of 10\u22122 and a momentum of 0.9, and between each substitute epoch the dataset is augmented using Jacobian-based dataset augmentations with a step of \u03b5\u03c1 = 0.1.\nIn the second column of Table 1, we report the accuracy of the substitute DNN, which we compute using the full MNIST test set (minus the 150 samples used for the initial substitute training set). Note that, the adversary does not have access to this full test set, and we solely use it to better visualize the substitute training process. One can observe that after 6 substitute training iterations, the substitute DNN achieves a 81.20% accuracy using a substitute training set of 6,400 = 100\u00d7 26 samples. Although this accuracy falls short of current state-of-the-art accuracy on this particular task, one\n3Our machine is equipped with an Intel Xeon E5-2680 v3 central processing unit and a Nvidia Tesla K5200 graphics processing unit.\nshould keep in mind that the adversary has access to a limited number of samples (in this case 6,400 instead of 50,000 for state-of-the-art architectures) and that the adversarial end goal is to craft adversarial samples misclassified by the oracle and not to train a substitute DNN matching the oracle\u2019s accuracy. As such, the adversary is principally interested in training a substitute DNN capable of mimicking the oracle\u2019s decision boundaries.\nAdversarial Sample Crafting - We now perform the second fold of our attack strategy: craft adversarial samples using the substitute DNN. These samples are crafted using the Goodfellow crafting strategy, as described in Section 4. We chose to use the 10,000 samples from the MNIST test set as our legitimate samples. Note again that the adversary does not need access to the dataset and can use any legitimate sample of his/her choice to craft adversarial samples. We use the dataset to evaluate the success of the attack on a large scale. The perturbation we add is parameterized to introduce an intensity variation of \u03b5 , and we perform our measurements for several values \u03b5 \u2208{0.05,0.2,0.25,0.3}. Once crafted against the substitute DNN, our goal is to verify whether these samples are in fact misclassified by the oracle or not, thus effectively achieving the adversarial goal.\nTo this purpose, the confusion matrix found in Figure 5 reports the label given to adversarial samples by the MetaMind oracle. Samples are organized by their legitimate class (along rows) and the class assigned by the oracle (along columns). Percentage rates shown in the cell are normalized by source class (rows). Thus, rates on the diagonal indicate the number of samples correctly classified by the oracle for each of the 10 classes. All off-diagonal rates indicate the number of samples misclassified in a wrong class. For instance, cell (8,3) in the third matrix indicates that 89% instances of a 3 are classified as a 8 by the MetaMind oracle when samples are perturbed with an intensity variation of \u03b5 = 0.25%.\nAcross all legitimate classes, the number of adversarial samples crafted with a perturbation \u03b5 = 0.25, that are misclassified by the MetaMind oracle is 73.67%. Interestingly, one can notice on Figure 5, that the confusion matrix converges to samples being classified by the oracle as 4s and 8s (except 0s still correctly classified as 0s). A possible explanation to this phenomenon is that the DNN might more easily classify adversarial samples in these classes, as already observed by previous work on adversarial samples [20]. Another explanation would come from the fact that MetaMind augments the DNN architecture with an ensemble of classifiers collectively assigning a label to the samples. Adversarial samples might display properties characteristics of an 8 like for instance an important proportion of white pixels.\nThroughout the remainder of this paper, we use transferability of adversarial samples to refer to the oracle misclassification rate of adversarial samples crafted using the substitute network. In this case, with a transferability reaching 84.24% (for perturbations of \u03b5 = 0.4) between the substitute DNN and the targeted MetaMind oracle, our attack strategy is effectively able to craft adversarial samples severely damaging the output integrity of the targeted MetaMind oracle."}, {"heading": "5.3 Attack Using a Handcrafted Substitute Training Set", "text": "To confirm this result, we repeat the entire process with a different initial substitute training set. We now use a handcrafted dataset completely independent from the MNIST dataset. This shows that even an attacker with limited knowledge about the input can effectively use our strategy to force misclassification from DNN oracles. We repeat the substitute training and adversarial sample crafting steps with this handcrafted dataset to create samples misclassified by the oracle.\nSubstitute DNN Training - We handcraft our own samples by handwriting 10 digits for each class between 0 and 9 using a laptop trackpad and adapting them to the format of 28x28 grayscale pixels used in the MNIST dataset. Example instances of these samples are shown in Figure 6. In the third column of Table 1, we report the accuracy of the substitute DNN, which we compute using the full MNIST test set (minus the 150 samples used for the previous initial substitute training set). After 4 substitute training epochs, the substitute DNN achieves an accuracy of 47.34% on the MNIST test set. Again, recall that the adversary trains the substitute DNN to mimic the oracle\u2019s decision boundaries and not necessarily to achieve good accuracy.\nAdversarial Sample Crafting - We then craft adversarial samples as described previously. The impact of using a handcrafted substitute training set compared to the previous substitute set extracted from the MNIST dataset is that we are not able to craft as many adversarial samples, as illustrated in Table 2. For instance, with an intensity variation parameter of \u03b5 = 0.25, we can only craft successfully 5132 = 0.5132\u221710,000 samples instead of 8,434 = 0.8434\u221710,000 samples previously.\nIn Table 3, we compare the transferability of adver-\nsarial samples crafted using this substitute DNN with the ones crafted using the previous substitute DNN. The transferability of adversarial samples is given for several values of the intensity variation introduced by perturbations. This second substitute DNN, showcasing similar transferability rates than the first substitute DNN, especially for perturbation intensities smaller than 0.3, shows that even with an initial training set independent and very different from the training set used by the targeted model, an adversary can achieve his adversarial goal effectively. For instance, by introducing a perturbation of \u03b5 = 0.25, the attacker can force the MetaMind oracle to misclassify 81.62% of the adversarial samples produced."}, {"heading": "6 Conclusions", "text": "We introduced in this paper an attack against DNN oracles. Our work is a first and significant step towards releasing strong assumptions made by previous attacks against DNNs regarding the adversary\u2019s capability of accessing the targeted DNN\u2019s architecture and parameters. The attack we propose builds on previous attacks and a novel substitute training technique we introduced to successfully craft adversarial samples misclassified by oracles. We validated our attack design by targeting a realworld oracle served by MetaMind, forcing it to misclassify 84.24% of inputs provided by the adversary."}, {"heading": "7 Acknowledgments", "text": "Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is\nauthorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on."}], "references": [{"title": "Predicting the sequence specificities of dna-and rna-binding proteins by deep learning. Nature biotechnology", "author": ["B. ALIPANAHI", "A. DELONG", "M.T. WEIRAUCH", "B.J. FREY"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. BASTIEN", "P. LAMBLIN", "R. PASCANU", "J. BERGSTRA", "I. GOODFELLOW", "A. BERGERON", "N. BOUCHARD", "D. WARDE- FARLEY", "Y. BENGIO"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Deep learning. Book in preparation for", "author": ["I.G.Y. BENGIO", "A. COURVILLE"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning deep architectures for ai", "author": ["Y. BENGIO"], "venue": "Foundations and trends R  \u00a9 in Machine Learning 2,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. BERGSTRA", "O. BREULEUX", "F. BASTIEN", "P. LAMBLIN", "R. PASCANU", "G. DESJARDINS", "J. TURIAN", "D. WARDE- FARLEY", "Y. BENGIO"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy) (2010),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Multi-column deep neural network for traffic sign classification", "author": ["D. CIRE\u015eAN", "U. MEIER", "J. MASCI", "J. SCHMIDHUBER"], "venue": "Neural Networks", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Largescale malware classification using random projections and neural networks", "author": ["G.E. DAHL", "J.W. STOKES", "L. DENG", "YU"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position", "author": ["K. FUKUSHIMA", "S. MIYAKE"], "venue": "Pattern recognition 15,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. GOODFELLOW", "J. SHLENS", "C. SZEGEDY"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "In IEEE International Conference on Computer Vision", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. HINTON", "L. DENG", "D. YU", "G.E. DAHL", "MOHAMED", "A.-R", "N. JAITLY", "A. SENIOR", "V. VANHOUCKE", "P. NGUYEN", "SAINATH", "T. N"], "venue": "Signal Processing Magazine, IEEE 29,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "How paypal beats the bad guys with machine learning", "author": ["E. KNORR"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LECUN", "L. BOTTOU", "Y. BENGIO", "P. HAFFNER"], "venue": "Proceedings of the IEEE 86,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Y. LECUN", "C. CORTES"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. MNIH", "K. KAVUKCUOGLU", "D. SILVER", "A. GRAVES", "I. ANTONOGLOU", "D. WIERSTRA", "M. RIEDMILLER"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. NAIR", "G.E. HINTON"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. PAPERNOT", "P. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI"], "venue": "In Proceedings of the 1st IEEE European Symposium on Security and Privacy (2016),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. PAPERNOT", "P. MCDANIEL", "X. WU", "S. JHA", "A. SWAMI"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. PENNINGTON", "R. SOCHER", "C.D. MANNING"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "A new method for mapping optimization problems onto neural networks", "author": ["C. PETERSON", "B. S\u00d6DERBERG"], "venue": "International Journal of Neural Systems 1,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. RANZATO", "F.J. HUANG", "BOUREAU", "Y.-L", "Y. LE- CUN"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Deep boltzmann machines", "author": ["R. SALAKHUTDINOV", "G.E. HINTON"], "venue": "In International Conference on Artificial Intelligence and Statistics", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Recognizing functions in binaries with neural networks", "author": ["E.C.R. SHIN", "D. SONG", "R. MOAZZEZI"], "venue": "In 24th USENIX Security Symposium (USENIX Security", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ER- HAN", "I. GOODFELLOW", "R. FERGUS"], "venue": "In Proceedings of the 2014 International Conference on Learning Representations", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. WERBOS"], "venue": "Neural Networks 1,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}], "referenceMentions": [{"referenceID": 9, "context": "Deep Neural Networks are transforming the field of machine learning by outperforming humans at various tasks like classification [11] and reinforcement learning [17], or allowing for breakthroughs in promising areas like unsupervised learning [25].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "Deep Neural Networks are transforming the field of machine learning by outperforming humans at various tasks like classification [11] and reinforcement learning [17], or allowing for breakthroughs in promising areas like unsupervised learning [25].", "startOffset": 161, "endOffset": 165}, {"referenceID": 22, "context": "Deep Neural Networks are transforming the field of machine learning by outperforming humans at various tasks like classification [11] and reinforcement learning [17], or allowing for breakthroughs in promising areas like unsupervised learning [25].", "startOffset": 243, "endOffset": 247}, {"referenceID": 2, "context": "This generated a wide interest in deep learning, the machine learning technique which uses neural networks to represent a hierarchy of increasingly complex concepts [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 22, "context": "The benefits of using hierarchical representations of data include automatic feature extraction from the inputs [25].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 112, "endOffset": 115}, {"referenceID": 12, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 214, "endOffset": 218}, {"referenceID": 19, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 248, "endOffset": 252}, {"referenceID": 6, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 281, "endOffset": 288}, {"referenceID": 23, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 281, "endOffset": 288}, {"referenceID": 8, "context": "It has however been shown that deployed deep neural networks are vulnerable to integrity attacks [10, 20, 27].", "startOffset": 97, "endOffset": 109}, {"referenceID": 17, "context": "It has however been shown that deployed deep neural networks are vulnerable to integrity attacks [10, 20, 27].", "startOffset": 97, "endOffset": 109}, {"referenceID": 24, "context": "It has however been shown that deployed deep neural networks are vulnerable to integrity attacks [10, 20, 27].", "startOffset": 97, "endOffset": 109}, {"referenceID": 5, "context": "For instance, consider a DNN used by a car\u2019s driverless system to classify signs identified on the roadside [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "Previous approaches [10, 20, 27] evaluated the perturbations required to craft adversarial samples using knowledge of the DNN architecture and parameters.", "startOffset": 20, "endOffset": 32}, {"referenceID": 17, "context": "Previous approaches [10, 20, 27] evaluated the perturbations required to craft adversarial samples using knowledge of the DNN architecture and parameters.", "startOffset": 20, "endOffset": 32}, {"referenceID": 24, "context": "Previous approaches [10, 20, 27] evaluated the perturbations required to craft adversarial samples using knowledge of the DNN architecture and parameters.", "startOffset": 20, "endOffset": 32}, {"referenceID": 8, "context": "We then leverage the generalizability property of adversarial samples [10]: adversarial samples crafted for a given DNN are also largely misclassified by other DNNs trained to solve the same machine learning task.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A Deep Neural Network, as illustrated in its simplest form in Figure 1, is a machine learning technique that uses a hierarchical composition of n parametric functions to model a high dimensional input~x [3, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 3, "context": "A Deep Neural Network, as illustrated in its simplest form in Figure 1, is a machine learning technique that uses a hierarchical composition of n parametric functions to model a high dimensional input~x [3, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 25, "context": "The adjustment is typically performed using techniques derived from the backpropagation algorithm [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "This model maps images of digits with probability vectors indicating the digit identified in the input image (Figure adapted from [21]).", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "However, this is not the case in practice as shown by previous attacks manipulating DNN outputs using adversarial samples [10, 20, 27].", "startOffset": 122, "endOffset": 134}, {"referenceID": 17, "context": "However, this is not the case in practice as shown by previous attacks manipulating DNN outputs using adversarial samples [10, 20, 27].", "startOffset": 122, "endOffset": 134}, {"referenceID": 24, "context": "However, this is not the case in practice as shown by previous attacks manipulating DNN outputs using adversarial samples [10, 20, 27].", "startOffset": 122, "endOffset": 134}, {"referenceID": 8, "context": "As all previous attacks depended on such knowledge [10, 20, 27], this threat model is particularly more challenging and, to the best of our knowledge, it was previously unknown whether a tractable attack was possible.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "As all previous attacks depended on such knowledge [10, 20, 27], this threat model is particularly more challenging and, to the best of our knowledge, it was previously unknown whether a tractable attack was possible.", "startOffset": 51, "endOffset": 63}, {"referenceID": 24, "context": "As all previous attacks depended on such knowledge [10, 20, 27], this threat model is particularly more challenging and, to the best of our knowledge, it was previously unknown whether a tractable attack was possible.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "A taxonomy of threat models for deep learning deployed in adversarial settings is introduced in [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "Accessing labels \u00d5 is the only capability of adversaries considered in our threat model, unlike previous approaches describing attacks against DNN output integrity [10, 20, 27].", "startOffset": 164, "endOffset": 176}, {"referenceID": 17, "context": "Accessing labels \u00d5 is the only capability of adversaries considered in our threat model, unlike previous approaches describing attacks against DNN output integrity [10, 20, 27].", "startOffset": 164, "endOffset": 176}, {"referenceID": 24, "context": "Accessing labels \u00d5 is the only capability of adversaries considered in our threat model, unlike previous approaches describing attacks against DNN output integrity [10, 20, 27].", "startOffset": 164, "endOffset": 176}, {"referenceID": 8, "context": "In previous work [10, 20, 27], an approximated solution to Equation 2 was found by exploiting a precise understanding of the mapping learned by DNNs between inputs and", "startOffset": 17, "endOffset": 29}, {"referenceID": 17, "context": "In previous work [10, 20, 27], an approximated solution to Equation 2 was found by exploiting a precise understanding of the mapping learned by DNNs between inputs and", "startOffset": 17, "endOffset": 29}, {"referenceID": 24, "context": "In previous work [10, 20, 27], an approximated solution to Equation 2 was found by exploiting a precise understanding of the mapping learned by DNNs between inputs and", "startOffset": 17, "endOffset": 29}, {"referenceID": 8, "context": "Such an understanding was built by exploiting gradients of the DNN\u2019s cost function with respect to network inputs [10, 27] or the DNN\u2019s Jacobian matrix [20].", "startOffset": 114, "endOffset": 122}, {"referenceID": 24, "context": "Such an understanding was built by exploiting gradients of the DNN\u2019s cost function with respect to network inputs [10, 27] or the DNN\u2019s Jacobian matrix [20].", "startOffset": 114, "endOffset": 122}, {"referenceID": 17, "context": "Such an understanding was built by exploiting gradients of the DNN\u2019s cost function with respect to network inputs [10, 27] or the DNN\u2019s Jacobian matrix [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "To overcome this challenge, we introduce in this paper a new attack against deep neural network integrity that builds on attacks previously described [10, 20, 27], but executable under the oracle threat model described in Section 3.", "startOffset": 150, "endOffset": 162}, {"referenceID": 17, "context": "To overcome this challenge, we introduce in this paper a new attack against deep neural network integrity that builds on attacks previously described [10, 20, 27], but executable under the oracle threat model described in Section 3.", "startOffset": 150, "endOffset": 162}, {"referenceID": 24, "context": "To overcome this challenge, we introduce in this paper a new attack against deep neural network integrity that builds on attacks previously described [10, 20, 27], but executable under the oracle threat model described in Section 3.", "startOffset": 150, "endOffset": 162}, {"referenceID": 8, "context": "The intuition behind our approach comes from the transferability property of adversarial samples: an adversarial sample misclassified by a given DNN architecture A is likely to be misclassified by a different DNN architecture B [10].", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": "Indeed, as adversaries have full knowledge of the DNN architecture and parameters of the substitute network, they can use one of the previously described attack techniques [10, 20] to craft adversarial samples misclassified by F .", "startOffset": 172, "endOffset": 180}, {"referenceID": 17, "context": "Indeed, as adversaries have full knowledge of the DNN architecture and parameters of the substitute network, they can use one of the previously described attack techniques [10, 20] to craft adversarial samples misclassified by F .", "startOffset": 172, "endOffset": 180}, {"referenceID": 8, "context": "This part of the attack is performed by implementing two previously introduced approaches described in [10, 20].", "startOffset": 103, "endOffset": 111}, {"referenceID": 17, "context": "This part of the attack is performed by implementing two previously introduced approaches described in [10, 20].", "startOffset": 103, "endOffset": 111}, {"referenceID": 8, "context": "attack - This strategy, also known as the fast gradient sign method, is suitable for misclassification attacks [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "attack - This strategy is suitable for source-target misclassification attacks where adversaries seek to take samples from any legitimate source class to any chosen target class [20].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "We use the MetaMind API as an oracle DNN classifier for the MNIST handwritten digit classification task [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The dataset used in this set of validation experiments is the MNIST handwritten digit dataset [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "Each 28x28 grayscale image is encoded as a vector of pixel intensities normalized in the interval [0,1] and obtained by reading the image pixel matrix row-wise.", "startOffset": 98, "endOffset": 103}, {"referenceID": 7, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 13, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 21, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 321, "endOffset": 325}, {"referenceID": 20, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 379, "endOffset": 383}, {"referenceID": 1, "context": "We use Theano [2, 5] together with Lasagne [8] to train all DNNs in this paper.", "startOffset": 14, "endOffset": 20}, {"referenceID": 4, "context": "We use Theano [2, 5] together with Lasagne [8] to train all DNNs in this paper.", "startOffset": 14, "endOffset": 20}, {"referenceID": 17, "context": "A possible explanation to this phenomenon is that the DNN might more easily classify adversarial samples in these classes, as already observed by previous work on adversarial samples [20].", "startOffset": 183, "endOffset": 187}], "year": 2016, "abstractText": "Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN\u2019s architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24% rate. We also perform an evaluation to finetune our attack strategy and maximize the oracle\u2019s misclassification rate for adversarial samples.", "creator": "LaTeX with hyperref package"}}}