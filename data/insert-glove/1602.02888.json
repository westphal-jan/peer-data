{"id": "1602.02888", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Robust Ensemble Classifier Combination Based on Noise Removal with One-Class SVM", "abstract": "cerea In afewerki machine learning area, 152,000 as tess\u00e9 the number of patak labeled campana input samples upperclass becomes very catlike large, phaethontidae it is very drying difficult to build a classification ataques model because gtp-binding of input data set wormley is not butthead fit in phytopathology a luizao memory in economical training shirreffs phase of the 1312 algorithm, therefore, 50-64 it outboards is rungkat necessary odone to lisogor utilize data partitioning defensins to ,800 handle overall data set. eyesight Bagging and trifunovi\u0107 boosting aite based data ulyanov partitioning vipin methods have been troxels broadly used in fabric-covered data mining deduces and kunstler pattern worldy recognition area. 44.02 Both of leschyshyn these saint-germain-des-pr\u00e9s methods knockoffs have johnsonville shown rivo a great wonderwall possibility lagouranis for westropp improving aplon classification model a performance. hane This warrenton study highwayman is concerned nci with brun the utility analysis swifter of data set partitioning with millburn noise \u017c\u00f3rawina removal soulfulness and podkarpackie its jarod impact on kirschstein the performance vaynerchuk of multiple 334,000 classifier adds models. 65.76 In this xestia study, we ludab propose noise tzung filtering fennessy preprocessing glion at each data set gaff partition to increment quart classifier 2-40 model performance. vir We applied Gini impurity mortimers approach to find stratacom the best split percentage of noise filter \u00f6lvir ratio. unprofessional The filtered 777 sub puggy data set is farceur then used to train rocksteady individual holzhausen ensemble tabbies models.", "histories": [["v1", "Tue, 9 Feb 2016 08:14:29 GMT  (306kb)", "http://arxiv.org/abs/1602.02888v1", "22nd International Conference, ICONIP 2015"]], "COMMENTS": "22nd International Conference, ICONIP 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ferhat \\\"ozg\\\"ur \\c{c}atak"], "accepted": false, "id": "1602.02888"}, "pdf": {"name": "1602.02888.pdf", "metadata": {"source": "CRF", "title": "Robust Ensemble Classifier Combination Based on Noise Removal with One-Class SVM", "authors": ["Ferhat \u00d6zg\u00fcr \u00c7atak"], "emails": ["ozgur.catak@tubitak.gov.tr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 88\n8v 1\n[ cs\n.L G\n] 9\nF eb\nKeywords: one-class SVM, data partitioning, noise filtering, Gini impurity, large scale data classification"}, {"heading": "1 Introduction", "text": "It\u2019s clear that we collect and store larger amounts of data in databases. The need for efficiently and effectively analyzing and utilizing the information contained in the data has been increasing. Just as big data technologies evolved, the quantity and variety of data has also increased, and becoming more focused on storing every type of data. The main purpose of the storing of the data is intended to obtain information from data using a variety of machine learning methods. One of the primary machine learning techniques is classification, which labels the new samples based on a training set whose class labels are provided [1,2]. Classification methods are applied in various areas such as bioinformatics, pattern recognition, text mining, social network analysis, etc.\nIn Big Data age, traditional classifier algorithms have new challenges to scaling up in order to address the large-scale data set training. Most of existing classification algorithms assume that the data can fit in a memory in training phase of learning. These algorithms cannot be comfortably implemented to data sets that larger than computer memory capacity. Data partitioning strategy is one of the methods that can be applied to the training of high-dimensional data\nsets that are used for the building of classification model in order to overcome the input data complexity. In order to prevent the building of weak classification model that emerged from the data chunks, the input set needs to be strengthened through various methods.\nIn this study, the noise filtering approach is applied to each individual sub data set to clean noisy input data, then, AdaBoost ensemble method is used to strength the classification model at each data partition. We applied one-class Support Vector Machine (SVM) method to filter noisy instances from each individual data partition and then AdaBoost ensemble based classification method is used to each individual data partition to increase the model accuracy.\nThe overall contributions of the study are listed as follows:\n1. Using data partitioning method, the complexity of input matrix, which is quite high for the single memory, is reduced in this manner. 2. Each individual sub-set of input matrix is reinforced with noise filtering method using one-class SVM and Gini impurity. 3. Each sub-set of input matrix is used in the training phase of the different ensemble classifier, so that each instances are considered when building a global classification model.\nGini impurity is used to calculate the uncertainty about source of input data set. This measure is applied to estimate the degree of information diversity provided by cleaned partition of sub data set.\nThe remainder of this paper is organized as follows: Section 2 briefly explains the methods that are used in this work. Section 3 describes the proposed data cleaning and partitioning method. Section 4 gives the experimental results. In Section 5, we give conclusion and future works."}, {"heading": "2 Preliminaries", "text": "The approach presented in this paper uses one-class SVM algorithm to remove noisy instances, AdaBoost to build ensemble classifier models, and data partitioning to train over all data set instances. All elements are introduced here briefly."}, {"heading": "2.1 One-Class SVM", "text": "SVM [3] method is used to find classification models using the maximum margin separating hyper plane. Scho\u0308lkopf et al. [4] proposed a training methodology that handles only one class classification called as \u201done-class\u201d classification.\nOne-class SVM algorithm is a method used to detect the outliers in the data. Basically the method finds soft boundaries of the data set, and then, model determines whether new instance belongs to this data set or not. Suppose, we are given a data set, x1, . . .xm \u2208 X drawn from an unknown underlying probability distribution P . We are interested in estimating a set S such that the probability that a test point from P lies inside in S with an a priori specified probability\nvalue. As shown in Figure 1, origin is labeled as \u22121, and the all training instances are labeled as +1.\nLet S = {(xi, yi)|xi \u2208 IR n, y \u2208 {1, . . . ,K}}mi=1 be input instances in IR n, \u03c6 : X \u2192 H be a kernel function that maps the input instances to another space. Then standard SVM method tries to find a hyper plane that solves the separation problem with an optimization problem. The objective function of the SVM classifier is formulated as follows.\nminw,\u03be,\u03c1\n(\n1 2 ||w||2 + 1 mC \u2211\ni\n\u03bei \u2212 \u03c1\n)\nsubject to\n(w.\u03c6(xi)) \u2265 \u03c1\u2212 \u03bei\n\u03bei \u2265 0, \u2200i = 1, . . . ,m\n(1)\nwherew is orthogonal to the separating hyper plane, C is smoothness parameter, xi is the i-th input instances, m is the total number of input instances, \u03bei are the slack variables, \u03c1 is the distance between origin and separating hyper plane.\nBy using Lagrange techniques, w and \u03c1 are obtained, then the decision function becomes:\nf(x) = sign ((w.\u03c6(x)) \u2212 \u03c1) (2)"}, {"heading": "2.2 AdaBoost", "text": "The AdaBoost [5] is a supervised learning algorithm designed to solve classification problems [6]. The algorithm takes as input a training set (x1, y1), ..., (xn, yn)\nwhere the input sample xi \u2208 R p, and the output value, yi, in a finite space y \u2208 1, ...K. AdaBoost algorithm assumes a set of training data sampled independently and identically distributed (i.i.d.) from some unknown distribution X .\nGiven a space of feature vectorsX and two possible class labels, y \u2208 {\u22121,+1}, AdaBoost goal is to learn a strong classifierH(x) as a weighted ensemble of weak classifiers ht(x) predicting the label of any instance x \u2208 X [7].\nH(x) = sign(f(x)) = sign\n(\nT \u2211\nt=1\n\u03b1tht(x)\n)\n(3)"}, {"heading": "2.3 Data Partitioning Strategies", "text": "The use of multiple classifiers, learning methods are applied to base classifiers with different methods. Data partitioning is used a variety of reasons. First reason is the diversity that means uncorrelated base classifiers [8,9]. Another reason is the reducing the input complexity of large-scale data sets [10]. Last one is to build classifier models for the specific part of the input instances [11].\nData partitioning is basically divided into two different groups; filter based data partitioning and wrapper based data partitioning [12]. In wrapper based data partitioning, sub-data sets are created using base classifier outputs [13]. In filter based data partitioning, sub-data sets are created before individual classifiers are trained [14]."}, {"heading": "3 Proposed Approach", "text": "In this section we provide the details of the proposed noise filter based sub data set training method. The basic idea of noise removing based on one-class SVM technique is introduced in Section 3.1. The analysis of proposed method is described in Section 3.2."}, {"heading": "3.1 Basic Idea", "text": "Our main task is to partition the input data set into sub-data sets, (Xm, Ym), and, create local classifier ensembles for each sub data chunk. Noise removing process is applied to each individual sub-data set as pre-processing. Weighted voting method is used to combine the each ensemble classifier, and then, a single classifier model is created. Overall of the proposed method is shown in Figure 2."}, {"heading": "3.2 Analysis of the proposed algorithm", "text": "Kragh et al. showed that ensemble methods of neural networks gets better accuracy performance over unseen examples [15]. The main motivation of this work is the idea that small size classifier ensembles can obtain more accurate classifier model that are comparable to individual classifiers.\nIn the proposed model, at every sub-data set, there is a set of classifier functions (ensemble classifier), H(m), that acts as a single classification model. The single model at every sub-data set, m, is defined as follows:\nH(m)(x) = argmax k\nM \u2211\nt=1\n\u03b1tht(x) (4)\nThe selected ensemble classifier models from last phase of our algorithm are combined into one single classification model, H\u0302(x), using accuracy based majority voting method.\nH\u0302(x) = argmax k\nm \u2211\ni=1\n\u03b2H(m)(x) (5)\nwhere \u03b2 is the accuracy of ensemble classifier."}, {"heading": "4 Experiments", "text": "In this section, we perform experiments on real-world data sets from the public available data set repositories. Public data sets are used to evaluate the proposed learning method. Classification models of each data set are compared for accuracy results without removing noisy samples from them."}, {"heading": "4.1 Experimental setup", "text": "In this section, our approach is applied to five different data sets to verify its model effectivity and efficiency. The data sets are summarized in Table 1, including cod-rna, ijcnn1, letter, shuttle and SensIT Vehicle. We choose 50 as the data split size, m, and 3 different classification methods including Extra Trees [16], k-nn and SVM."}, {"heading": "4.2 Effect of Noise Removing on Input Matrix", "text": "In this section, we show the impact of noise removal pre-processing on the sample data sets. In order to show the noise removing affects, we used the \u201dGini Impurity\u201d to measure the quality of procedure. Gini approaches deal appropriately with data diversity of a data. The Gini measures the class distribution of variable y = {y1, \u00b7 \u00b7 \u00b7 , ym}. The Gini impurity can be written as :\ng = 1\u2212 \u2211\nk\np2j (6)\nwhere pj is the probability of class k, in data set D.\nThe cleaning results are shown in Figure 3a and Figure 3b. As expected, the Gini impurity value decreases with the cleaning of the noisy instances from the data, and increases on separated noisy data. Our aim is to minimizing the Gini impurity on clean data set, Xclean, and maximizing the value on noisy data set\nXnoisy . As a result, this division ratio which minimizes the ratio between the two values was regarded as the optimum value.\nSplit Percantage = argmin p\nGini(Xclean, p) Gini(Xnoisy, p) (7)\nTable 2 shows the best Gini impurity performances of each data set used in our experiments."}, {"heading": "4.3 Simulation Results", "text": "The process of the experiments are as follows: Firstly, we trained our data sets without using noise removal. Then we perform classification on test data sets, and calculate the accuracy of classifiers. We repeated the experiments 50 times, and average accuracy is calculated. Table 3 shows the average accuracy of each example data sets with and without noise removing using one-class SVMmethod.\nAs can be seen on Table 3, the noise removing based partitioned proposed algorithm significantly outperforms the splitted classifier building in most cases."}, {"heading": "5 Conclusions", "text": "In this paper, we have introduced a novel data partitioning based classifier building method, which improves the sub data sets with removing the noisy instances using one-class SVM and find best noise removing ratio with Gini impurity value. We carried out a series of computer experiments to find a global ensemble\nclassifier and the performance of our proposed method. The training process of a partitioned data set is simple, fast and final classifier model handle overall training instances. Our experimental results show that the memory requirement of training phase reduced remarkably, and the accuracy increased by using the noise removal process. The proposed method is a practical multiple ensemble classifier training model to classify large-scale data sets.\nIn the future work, our plan is to study different noise removing methods to clean sub data set. We plan adaptive noise removing ratio to make our method as autonomous as possible."}], "references": [{"title": "Machine learning: An artificial intelligence approach", "author": ["J.R. Anderson", "R.S. Michalski", "J.G. Carbonell", "T.M. Mitchell"], "venue": "Volume 2. Morgan Kaufmann", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Database management systems", "author": ["R. Ramakrishnan", "J. Gehrke"], "venue": "Osborne/McGrawHill", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "Springer Science & Business Media", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural computation 13(7)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory, Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "A short introduction to boosting", "author": ["Y. Freund", "R. Schapire", "N. Abe"], "venue": "JournalJapanese Society For Artificial Intelligence 14(771-780)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Double-base asymmetric adaboost", "author": ["I. Landesa-Vzquez", "J.L. Alba-Castro"], "venue": "Neurocomputing 118(0)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Using diversity measures for generating error-correcting output codes in classifier ensembles", "author": ["L.I. Kuncheva"], "venue": "Pattern Recognition Letters 26(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Filter-based data partitioning for training multiple classifier systems", "author": ["R.A. Dara", "M. Makrehchi", "M.S. Kamel"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 22(4)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed learning with bagging-like performance", "author": ["N.V. Chawla", "T.E. Moore", "L.O. Hall", "K.W. Bowyer", "W.P. Kegelmeyer", "C. Springer"], "venue": "Pattern recognition letters 24(1)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Combination of multiple classifiers using local accuracy estimates", "author": ["K. Woods", "K. Bowyer", "W.P. Kegelmeyer Jr"], "venue": "Computer Vision and Pattern Recognition, 1996. Proceedings CVPR\u201996, 1996 IEEE Computer Society Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Selection of relevant features and examples in machine learning", "author": ["A.L. Blum", "P. Langley"], "venue": "Artificial intelligence 97(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "Schapire", "R.E"], "venue": "ICML. Volume 96.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning 24(2)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["A. Krogh", "J. Vedelsby"], "venue": "Advances in Neural Information Processing Systems, MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine learning 63(1)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "One of the primary machine learning techniques is classification, which labels the new samples based on a training set whose class labels are provided [1,2].", "startOffset": 151, "endOffset": 156}, {"referenceID": 1, "context": "One of the primary machine learning techniques is classification, which labels the new samples based on a training set whose class labels are provided [1,2].", "startOffset": 151, "endOffset": 156}, {"referenceID": 2, "context": "1 One-Class SVM SVM [3] method is used to find classification models using the maximum margin separating hyper plane.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "[4] proposed a training methodology that handles only one class classification called as \u201done-class\u201d classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "2 AdaBoost The AdaBoost [5] is a supervised learning algorithm designed to solve classification problems [6].", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "2 AdaBoost The AdaBoost [5] is a supervised learning algorithm designed to solve classification problems [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "Given a space of feature vectorsX and two possible class labels, y \u2208 {\u22121,+1}, AdaBoost goal is to learn a strong classifierH(x) as a weighted ensemble of weak classifiers ht(x) predicting the label of any instance x \u2208 X [7].", "startOffset": 220, "endOffset": 223}, {"referenceID": 7, "context": "First reason is the diversity that means uncorrelated base classifiers [8,9].", "startOffset": 71, "endOffset": 76}, {"referenceID": 8, "context": "First reason is the diversity that means uncorrelated base classifiers [8,9].", "startOffset": 71, "endOffset": 76}, {"referenceID": 9, "context": "Another reason is the reducing the input complexity of large-scale data sets [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "Last one is to build classifier models for the specific part of the input instances [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Data partitioning is basically divided into two different groups; filter based data partitioning and wrapper based data partitioning [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "In wrapper based data partitioning, sub-data sets are created using base classifier outputs [13].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "In filter based data partitioning, sub-data sets are created before individual classifiers are trained [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "showed that ensemble methods of neural networks gets better accuracy performance over unseen examples [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "We choose 50 as the data split size, m, and 3 different classification methods including Extra Trees [16], k-nn and SVM.", "startOffset": 101, "endOffset": 105}], "year": 2016, "abstractText": "In machine learning area, as the number of labeled input samples becomes very large, it is very difficult to build a classification model because of input data set is not fit in a memory in training phase of the algorithm, therefore, it is necessary to utilize data partitioning to handle overall data set. Bagging and boosting based data partitioning methods have been broadly used in data mining and pattern recognition area. Both of these methods have shown a great possibility for improving classification model performance. This study is concerned with the analysis of data set partitioning with noise removal and its impact on the performance of multiple classifier models. In this study, we propose noise filtering preprocessing at each data set partition to increment classifier model performance. We applied Gini impurity approach to find the best split percentage of noise filter ratio. The filtered sub data set is then used to train individual ensemble models.", "creator": "LaTeX with hyperref package"}}}