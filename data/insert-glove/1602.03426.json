{"id": "1602.03426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Automatic Sarcasm Detection: A Survey", "abstract": "Automatic kendig detection prostitute of 5:28 sarcasm gorillaz has witnessed kirchmedia interest 4,670 from 78-73 the sentiment moscopole analysis patillo research community. 3-0-18-0 With laraine diverse approaches, datasets 6,395 and phanor analyses that walzel have been closers reported, g\u00f6kt\u00fcrks there repressurize is an panama-pacific essential need fvr to have a collective 5-44 understanding 145-pound of 22x the filthy research in strage this chalon area. yongsan In this akwa survey hinman of ecfa automatic gamblin sarcasm baldo detection, kilronan we describe special-purpose datasets, kobayashi approaches (corday both dynein supervised and rule - based ), triptychs and 1,817 trends vympel in sarcasm detection research. We kushk-e also present palanquin a research matrix deejay that summarizes past work, and list 5-ht1b pointers to outplayed future nakas work.", "histories": [["v1", "Wed, 10 Feb 2016 16:02:46 GMT  (178kb,D)", "http://arxiv.org/abs/1602.03426v1", "11 pages. This paper is likely to submitted to a NLP conference in the near future, with the same author list. All necessary regulations of arxiv and the conference will be followed"], ["v2", "Tue, 20 Sep 2016 22:15:52 GMT  (326kb,D)", "http://arxiv.org/abs/1602.03426v2", "This paper is likely to be submitted to ACM CSUR. This copy on arXiv is to obtain feedback from stakeholders"]], "COMMENTS": "11 pages. This paper is likely to submitted to a NLP conference in the near future, with the same author list. All necessary regulations of arxiv and the conference will be followed", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aditya joshi", "pushpak bhattacharyya", "mark james carman"], "accepted": false, "id": "1602.03426"}, "pdf": {"name": "1602.03426.pdf", "metadata": {"source": "CRF", "title": "Automatic Sarcasm Detection: A Survey", "authors": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman"], "emails": ["adityaj@cse.iitb.ac.in,", "pb@cse.iitb.ac.in,", "mark.carman@monash.edu"], "sections": [{"heading": "1 Introduction", "text": "Sarcasm is a peculiar form of sentiment expression, where surface sentiment differs from the implied sentiment. The Free Dictionary1 defines sarcasm as a form of verbal irony that is intended to ridicule2. Sarcasm is an often-quoted challenge for sentiment analysis (Liu, 2010) because sarcasm intends a negative sentiment, but a positive surface sentiment. This led to introduction of sarcasm detection as a research problem. Automatic sarcasm detection refers to computational approaches to detect sarcasm in text. This problem is hard because of nuanced ways in which sarcasm may be expressed. These nuances arise in several ways: general understanding (\u2018I love to get ignored\u2019), in terms of an author\u2019s background (\u2018I love solving math problems all weekend\u2019), in terms of a culture (popular use of honorifics in Chinese sarcasm), etc.\nStarting from the earliest known work by Tepperman et al. (2006) which deals with speech and text-related features, sarcasm detection research has seen wide interest. Following that, sarcasm detection from text has extended to different data forms (tweets, reviews, TV series dialogues), and spanned several approaches (rule-based, supervised, semi-supervised). This synergy has resulted in interesting innovations for automatic sarcasm detection.\nWith such diversity of approaches, there is a need to have a collective understanding of the research in\n1www.thefreedictionary.com 2That sarcasm is a form of verbal irony explains the relationship between sarcasm and irony. Sarcasm is essentially hurtful or intended to be hurtful\nsarcasm detection. This paper collates past works in automatic sarcasm detection. Through tabular summarization, we provide views of the current state-of-art. One past work to summarize computational irony is by Wallace (2013). However, they focus on linguistic challenges of computational irony, and deliberate on possible future work. Their paper is, however, limited to linguistic challenges and theories in sarcasm. On the contrary, we focus on the computational angle, and present a survey of computational sarcasm detection techniques.\nOur paper looks at sarcasm detection research in terms of four parameters: datasets, approaches, trends and issues. We present an illustration that shows current research, and a detailed matrix that describes papers dealing with sarcasm detection. Both will serve as useful resources for future work. We also prescribe future directions based on current papers.\nThe rest of the paper is organized as follows. Section 2 describes sarcasm studies in linguistics. Section 3 presents different problem definitions for sarcasm detection. Sections 4 and 5 discuss datasets and approaches reported for sarcasm detection. Section 6 highlights trends underlying sarcasm detection research, while Section 7 discusses key recurring issues in this research. Finally, Section 8 concludes the paper."}, {"heading": "2 Sarcasm in Linguistics", "text": "Sarcasm as a linguistic phenomenon has been widely studied. Before we proceed to approaches for sarcasm detection, we present an introduction to sarcasm studies in linguistics.\nCampbell and Katz (2012) state that sarcasm occurs along several dimensions namely failed expectation, pragmatic insincerity, negative tension, and presence of a victim. Eisterhold et al. (2006) state that sarcasm is peculiar because of the response it elicits. They observe that the responses to sarcasm may be: laughter, zero response, smile, sarcasm (in return), a change of topic (because the listener was not happy with the caustic sarcasm), literal reply and non-verbal reactions. Camp (2012) show that there are four types of sarcasm: (1) Propositional: Such sarcasm appears to be a nonsentiment proposition but has an implicit sentiment involved, (2) Embedded: Such sarcasm has an embed-\nar X\niv :1\n60 2.\n03 42\n6v 1\n[ cs\n.C L\n] 1\n0 Fe\nb 20\n16\nded sentiment incongruity in the form of words and phrases itself, (3) Like-prefixed: A like-phrase provides an emphatic implied denial of the argument being made before the sarcasm is expressed, and (4) Illocutionary: This sarcasm involves non-textual clues that indicate an attitude opposite to a sincere utterance. Therefore, in such cases, prosodic variations play a role in sarcasm expression.\nSeveral theories have been proposed to represent sarcasm. Some of them are:\n1. Situational disparity theory: According to Wilson (2006), sarcasm arises when there is situational disparity between text and a contextual information.\n2. Negation theory of sarcasm: Giora (1995) state that irony/sarcasm is a form of negation in which an explicit negation marker is lacking. In other words, when one expresses sarcasm, a negation is intended, without putting a negation word like \u2018not\u2019.\n3. 6-tuple representation: Ivanko and Pexman (2003) define sarcasm as a 6-tuple consisting of <S, H, C, u, p, p\u2019> where:\nS = Speaker H = Hearer/Listener\nC = Context u = Utterance\np = Literal Proposition p\u2019 = Intended Proposition\nThe tuple can be read as \u2018Speaker S generates an utterance u in Context C meaning proposition p but intending that hearer H understands p\u2019. Consider the following example. If a teacher says to a student, \u201cThat\u2019s how assignments should be done!\u201d and if the student knows that (s)he has barely completed the assignment, the student would understand the sarcasm. In context of the 6-tuple above, the properties of this sarcasm would be: S: Teacher H: Student C: The student has not completed his/her assignment. u: \u201cThat\u2019s how assignments should be done!\u201d p: You have done a good job at the assignment. p\u2019: You have done a bad job at the assignment.\nThis linguistic background helps to establish the challenges for sarcasm detection. In the context of the theories described here, some challenges are: (1) Identification of common knowledge, (2) Intent to ridicule, (3) Speaker-listener context. As we will see in the next section, these challenges have been partly handled by different reported works in automatic sarcasm detection."}, {"heading": "3 Problem Definition", "text": "We now look at how computational sarcasm detection has been defined, in past work. The most common formulation for sarcasm detection is as a classification task. Given a piece of text, the goal is to predict whether or not it is sarcastic. However, alternate definitions are also possible. Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm. Reyes et al. (2013) use a similar formulation and provide pair-wise classification performance for these labels. Even in the context of classification, there have been interesting variations in the data units that will be annotated and hence, classified. Tepperman et al. (2006) is an early paper in sarcasm detection that looks at occurrences of a common sarcastic phrase \u2018yeah right\u2019 and classifies each occurrence of \u2018yeah right\u2019 as sarcastic or not. Veale and Hao (2010) annotate similes such as \u2018as excited as a patient before a surgery\u2019 with sarcastic or not labels.\nOther formulations for sarcasm detection have also been reported. Ghosh et al. (2015b) model sarcasm detection as a sense disambiguation task. They state that a word may have a literal sense and a sarcastic sense. Their goal is to identify the sense of a word in order to detect sarcasm. Wang et al. (2015) formulate the problem as a sequence labeling problem. For a sequence of tweets in a conversation, they estimate the most probable sequence of three labels: happy, sad and sarcastic.\nTable 1 shows a matrix that summarizes sarcasm detection papers. The matrix is a useful resource to understand current work in sarcasm detection. The rest of this paper elaborates different columns of this matrix, i.e., we compare past work across five parameters: datasets, approach, annotation technique, features and extra-textual context, in the next sections.\nA note on languages Most research in sarcasm detection exists for English. Liu et al. (2014) use Chinese and English social media content. Barbieri et al. (2014a) present a first approach to detect sarcasm in Italian tweets. Pta\u0301cek et al. (2014) deal with Czech along with English tweets. Liebrecht et al. (2013) explore sarcasm detection for Dutch."}, {"heading": "4 Datasets", "text": ""}, {"heading": "4.1 Short text", "text": "Social media makes available several forms of data. Due to availability of the Twitter API and popularity of twitter as a medium, sarcasm-labeled datasets of tweets are popular. One approach to obtain labels for tweets is manual annotation. Riloff et al. (2013) use a dataset of tweets, manually annotated as sarcastic or not. Maynard and Greenwood (2014) study sarcastic tweets and their impact to sarcasm classification. They experiment with around 600 tweets which are marked for subjectivity, sentiment and sarcasm. Pta\u0301cek et al. (2014) present a dataset of 7,000 manually labeled tweets in Czech.\nThe second approach is the use of hashtag-based supervision. Many approaches use hashtags in tweets as indicators of sarcasm. The popularity of this approach can be attributed to various factors: (a) No one but the author of a tweet can determine if it was sarcastic. A hashtag is a label provided by authors themselves, (b) The approach allows creation of large-scale datasets. In order to create such a dataset, tweets containing particular hashtags are labeled as sarcastic. Davidov et al. (2010) use a dataset of tweets, which are labeled\nwith hashtags such as #sarcasm, #sarcastic, #not, etc. Gonza\u0301lez-Iba\u0301nez et al. (2011) also use hashtag-based supervision for tweets. However, they eliminate cases where the hashtag is a part of the running text. For example, \u2018#sarcasm is popular in india\u2019 is eliminated. Reyes et al. (2012) use hashtag-based supervision for tweets. Reyes et al. (2013) use a dataset of 40000 tweets labeled as sarcastic or not, using hashtags. Barbieri et al. (2014a) introduce a dataset of 25K Italian tweets. Joshi et al. (2015) present a dataset of tweets\nlabeled with hashtag-based supervision. Ghosh et al. (2015a) use hashtag-based dataset of tweets. 1000 trial, 4000 development and 8000 test tweets were introduced. Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags. Liebrecht et al. (2013) identify how the hashtag \u2018#not\u2019 is a popular modern form of expressing sarcasm. The dataset involves tweets that contain such a hashtag. Bouazizi and Ohtsuki (2015) use Sentiment140 dataset of tweets. Wang et al. (2015) use a dataset of tweets marked as happy, sad and sarcastic. The tweets are labelled using a supervised classifier since the main task is to detect sequence of labels. Barbieri et al. (2014b) create a dataset using hashtag-based supervision to create multiple labels: politics, sarcasm, humor and irony.\nUse of distant supervision using hashtags presents challenges, and may require quality control. To ensure quality, Bamman and Smith (2015) use hashtag-based dataset of tweets. The positive tweets are the ones containing #sarcasm the negative tweets are assumed to be the one not containing these labels. Fersini et al. (2015) use a dataset of 8K tweets marked using hashtags. To ensure quality, these tweets are additionally labelled by annotators.\nTwitter also provides access to additional context. Hence, in order to predict sarcasm, datasets may look at tweets by the same author. Khattri et al. (2015) use a dataset of tweets, introduced in the past. They\nlook up the complete twitter timeline (limited to 3200 tweets, by Twitter) to establish context. Rajadesingan et al. (2015) use a dataset of tweets, labeled by hashtagbased supervision along with a historical context of 80 tweets per author. Using tweets in an ongoing conversation in order to predict sarcasm has not been explored yet.\nOther social media text includes reddits. Wallace et al. (2014) create a corpus of reddit posts of 10K sentences, from 6 reddit topics. Wallace (2015) present a dataset of reddit comments: 5625 sentences."}, {"heading": "4.2 Long text", "text": "Reviews and discussion forum posts have also been used for sarcasm-labeled datasets. Lukin and Walker (2013) use Internet Argument Corpus that marks a dataset of discussion forum posts with multiple labels one of the labels is related to sarcasm. Reyes and Rosso (2014) use a dataset of movies, book reviews and news articles marked with sarcasm and sentiment. Reyes and Rosso (2012) deal with products that saw a spate of sarcastic reviews all of a sudden. They consider 11000 reviews in total. Filatova (2012) use a dataset of around 1000 reviews, marked as sarcastic or not. Buschmeier et al. (2014) use 1254 Amazon reviews, out of which 437 are ironic. Tsur et al. (2010) consider a large dataset of 66000 amazon reviews. Liu et al. (2014) use a dataset from multiple sources such as Amazon, Twitter, Netease and Netcena."}, {"heading": "4.3 Other datasets", "text": "Several other novel datasets have been used. Tepperman et al. (2006) use 131 call center transcripts. Each occurrence of \u2018yeah right\u2019 is marked as sarcastic or not. The goal is to find features in a transcripts that identify which \u2018yeah right\u2019 is sarcastic. Kreuz and Caucci (2007) use 20 sarcastic excerpts and 15 non-sarcastic excerpts, which are marked by 101 students. The goal is to identify lexical indicators of sarcasm. Veale and Hao (2010) focus on identifying which similes are sarcastic. Hence, they first use google for the pattern \u2018* as a *\u2019. This results in 20,000 distinct similes which are then marked as sarcastic or not. Rakov and Rosenberg (2013) create a crowdsourced dataset of sentences from a MTV show, Daria. Ghosh et al. (2015b) use a crowdsourcing tool to obtain a non-sarcastic version of a sentence if applicable. For example \u2018Who doesn\u2019t love being ignored\u2019 is expected to be corrected to \u2018Not many love being ignored\u2019."}, {"heading": "5 Approaches", "text": "Figure 1 shows key milestones in sarcasm detection. Only initial work in each area are indicated in the figure. Following foundational studies, there were many reported works on supervised/semi-supervised sarcasm classification that focused on using specific patterns or novel features. Then, as twitter emerged as a viable\nsource of data, hashtag-based supervision became popular. Recently, using context beyond the text to be classified has become popular. This can be done in several ways, as we describe later.\nIn general, approaches to sarcasm detection can be classified into: rule-based and statistical approaches. We look at these approaches in the next subsections."}, {"heading": "5.1 Rule-based Approaches", "text": "Rule-based approaches aim to identify sarcasm through specific evidences. As can be seen, these rules use peculiar indicators of sarcasm. Veale and Hao (2010) focus on identifying whether a given simile (of the form \u2018* as a *\u2019) is intended to be sarcastic. They use Google search in order to determine how likely a simile is. They present a 9-step approach where at each step/rule, a simile is validated using the number of search results. A strength of this approach is that they present an error analysis corresponding to multiple rules. Maynard and Greenwood (2014) propose that hashtag sentiment is a key indicator of sarcasm. Hashtags are often used by authors to highlight the sarcasm, and hence, if the sentiment expressed by a hashtag does not agree with rest of the tweet, the tweet is predicted as sarcastic. They use a hashtag tokenizer to split hashtags made of concatenated words. Bharti et al. (2015) present two rule-based\nclassifiers. The first uses a parse\u2013based lexicon generation algorithm that creates parse trees of sentences and identifies situation phrases that bear sentiment. If a negative phrase occurs in a positive sentence, it is predicted as sarcastic. The second algorithm aims to capture hyperboles by using interjection and intensifiers occur together. Riloff et al. (2013) present rule-based classifiers that look for a positive verb and a negative situation phrase in a sentence. They experiment with different configurations such as restricting the order of the verb and situation phrase."}, {"heading": "5.2 Statistical Approaches", "text": "Several statistical approaches have been explored for sarcasm detection. These approaches vary in terms of features and classifiers. We look at the two in forthcoming subsections."}, {"heading": "5.2.1 Features Used", "text": "In this subsection, we look at the set of features that have been reported for sarcasm detection. Table 3 summarizes some salient features for statistical approaches. In this subsection, we focus on features related to the text to be classified. Contextual features (i.e., features that use information beyond the text to be classified) are described in a latter subsection. Most approaches\nuse bag-of-words as features. However, in addition to these, there are peculiar features introduced in different works. We look at them hereafter.\nTsur et al. (2010) use pattern-based features extracted from a sarcasm-labeled corpus. These patternbased features are included as numeric features that take three possible values: exact match, partial overlap and no match. Gonza\u0301lez-Iba\u0301nez et al. (2011) use sentiment lexicon-based features. In addition, pragmatic features like emoticons and user mentions are also used. Reyes et al. (2012) use features related to ambiguity, unexpectedness, emotional scenario, etc. Ambiguity features cover structural, morpho-syntactic, semantic ambiguity, while unexpectedness features measure semantic relatedness. Riloff et al. (2013) use a set of patterns, specifically positive verbs and negative situation phrases, as features for a classifier. Liebrecht et al. (2013) introduce bigrams and trigrams as features. Reyes et al. (2013) explore skip-gram and character n-gram-based features. Maynard and Greenwood (2014) include seven sets of features. Some peculiar features include maximum/minimum/gap of intensity of adjectives and adverbs, max/min/average number of synonyms and synsets for words in the target text, etc. Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pta\u0301cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes. Rajadesingan et al. (2015) use extensions of words, number of flips, readability features in addition to others. Herna\u0301ndez-Far\u0131\u0301as et al. (2015) present features that measure semantic relatedness between words using Wordnet::similarity. Liu et al. (2014) introduce POS sequences and semantic imbalance as features. Since they also experiment with Chinese datasets, they use language-typical features like use of homophony, use of honorifics, etc."}, {"heading": "5.3 Classifiers", "text": "A variety of classifiers have been experimented for sarcasm detection. Gonza\u0301lez-Iba\u0301nez et al. (2011) use SVM with SMO and logistic regression. Chi-squared test is used to identify discriminating features. Reyes and Rosso (2012) use Naive Bayes and SVM. They also show Jaccard similarity between labels and the features. Riloff et al. (2013) compare rule-based techniques with a SVM-based classifier. Liebrecht et al. (2013) use balanced winnow algorithm in order to determine high-ranking features. Reyes et al. (2013) use Naive Bayes and decision trees for multiple pairs of labels among irony, humor, politics and education. Bamman and Smith (2015) use binary logistic regression. Wang et al. (2015) use SVM-HMM in order to incor-\nporate sequence nature of output labels in a conversation. Liu et al. (2014) compare several classification approaches including bagging, boosting, etc. and show results on five datasets."}, {"heading": "5.4 Key Findings", "text": "This section describes the observations that past sarcasm detection approaches make. These observations will be useful pointers for future work. Gonza\u0301lezIba\u0301nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors like incongruity due to numbers and granularity of annotation. Rajadesingan et al. (2015) show that historical features along with flip-based features are the most discriminating features. These are also the features that Khattri et al. (2015) use. Bharti et al. (2015) report a F-score of around 84%."}, {"heading": "6 Trends in Sarcasm Detection Approaches", "text": "Two research trends can be observed in approaches for sarcasm detection. They are (a) discovery of sarcastic patterns, and use of these patterns as features, and (b) use of contextual information i.e., information beyond the target text for sarcasm detection. We describe the two trends in detail in the forthcoming subsections."}, {"heading": "6.1 Pattern discovery", "text": "Discovering sarcastic patterns was an early trend in sarcasm detection. Several approaches dealt with extracting patterns that are indicative of sarcasm, or carry implied sentiment. These patterns may then be used as features for a statistical classifier, or as rules in a rule-based classifier. Tsur et al. (2010) extract sarcastic patterns from a seed set of labeled sentences. They first select words that either occur more than an upper threshold or less than a lower threshold. Among these words, identify a large set of candidate patterns. The patterns which occur discriminatively in either classes are then selected. Pta\u0301cek et al. (2014) also use a similar approach for Czech and English tweets.\nRiloff et al. (2013) hypothesize that sarcasm occurs due to a contrast between positive verbs and negative situation phrases. To discover a lexicon of these verbs and phrases, they propose an iterative algorithm. Starting with a seed set of positive verbs, they identify discriminative situation phrases that occur with these verbs in sarcastic tweets. These phrases are then used\nto identify other verbs. The algorithm iteratively appends to the list of known verbs and phrases. Joshi et al. (2015) adapt this algorithm by eliminating subsumption, and show that it adds value.\nLukin and Walker (2013) begin with a seed set of nastiness and sarcasm patterns, created using Amazon Mechanical Turk. They train a high precision sarcastic post classifier, followed by a high precision nonsarcastic post classifier. These two classifiers are then used to generate a large labeled dataset from a bootstrap set of patterns."}, {"heading": "6.2 Role of context in sarcasm detection", "text": "A recent advancement in sarcasm detection is the use of context. The term context must viewed as any information beyond the text to be predicted, and beyond common knowledge. As we will see, this context may be incorporated in a variety of ways. In the rest of this section, we refer to the textual unit to be classified as \u2018target text\u2019.\nWallace et al. (2014) is an annotation study that first highlighted the need of context for sarcasm detection. The annotators mark reddit comments with sarcasm labels. During this annotation, annotators often request for additional context in the form of reddit comments. The authors also present a transition matrix that shows how many times authors change their labels after the context is displayed to them.\nFollowing this observation and the promise of context for sarcasm detection, several recent approaches have looked at ways of incorporating it. The contexts that have been reported are of three types:\n1. Historical context refers to text by this author in the past. Khattri et al. (2015) follow the intuition \u2018A tweet is sarcastic either because it has words of contrasting sentiment in it, or because there is sentiment that contrasts with historical sentiment\u2019.\nHistorical tweets by the same author are considered as the context. Named entity phrases in the target tweet are looked up in the timeline of the author in order to gather the true sentiment of the author. This historical sentiment is then used to predict whether the author is likely to be sarcastic, given the sentiment expressed towards the entity in the target tweet. Rajadesingan et al. (2015) incorporate context about author using the author\u2019s past tweets. This context is captured as features for a classifier. The features deal with various dimensions. They use features about author\u2019s familiarity with twitter (in terms of use of hashtags), familiarity with language (in terms of words and structures), and familiarity with sarcasm. Bamman and Smith (2015) consider historical context in features such as historical salient terms, historical topic, profile info, historical sentiment (how likely is he/she to be negative), etc.\n2. Conversation context refers to text in the conversation of which the target text is a part. This incorporates the discourse structure of a conversation. Bamman and Smith (2015) capture conversational context using pairwise Brown features between the previous tweet and the target tweet. In addition, they also use \u2018audience\u2019 features. These are author features of the tweet author who responded to the target tweet. Joshi et al. (2015) show that concatenation of the previous post in a discussion forum thread along with the target post leads to an improvement in precision. Wallace (2015) look at comments in the thread structure to obtain context for sarcasm detection. To do so, they use the subreddit name, and noun phrases from the thread to which the target post belongs.\n3. Topical context: This context follows the intuition that some topics are likely to evoke sarcasm\nmore commonly than others. Wang et al. (2015) use three types of context: history, conversation and topic. Topical context refers to how likely the topic of this tweet is, in terms of sarcastic occurrences in the past."}, {"heading": "7 Issues in Sarcasm Detection", "text": "This section describes three important issues related to current techniques in automatic sarcasm detection. The first set of issues deal with annotation: hashtag-based supervision, data imbalance and inter-annotator agreements. The second issue deals with a specific kind of features that have been used for classification: sentiment as a label. Finally, the third issue lies in the context of classification techniques where we look at how past works handle dataset skews."}, {"heading": "7.1 Issues in Annotation", "text": "In addition to specific issues described in the previous subsections, some issues in sarcasm annotation have been analyzed.\n\u2022 Hashtag-based supervision: Although hashtagbased labeling can provide large-scale supervision, the quality of the dataset may be debatable. This is particularly true in case of use of #not to indicate insincere sentiment. (Liebrecht et al., 2013) show how #not can be used to express sarcasm - while the rest of the sentence is non-sarcastic. For example, \u2018I love cooking Italian food. #not\u2019. The speaker expresses sarcasm through #not. In most cases that use hashtag-based supervision, the hashtag is removed in the preprocessing step. This reduces the sentence above to \u2019I love cooking Italian food\u2019 - which may not have a sarcastic interpretation at all.\n\u2022 Data imbalance: Sarcasm is an infrequent phenomenon of sentiment expression. This skew also reflects in the datasets. Tsur et al. (2010) use a dataset with a small set of sentences are marked as sarcastic. 12.5% of tweets in the Italian dataset given by Barbieri et al. (2014a) are sarcastic. On the other hand, Rakov and Rosenberg (2013) present a balanced dataset of 15k tweets. Liu et al. (2014) focus on this imbalance in data, and present sarcasm detection that is robust to this imbalance.\n\u2022 Inter-annotator agreement: Since sarcasm is a subjective phenomenon, the inter-annotator agreement values reported in past work are diverse. Tsur et al. (2010) indicate an agreement of 0.34. Tepperman et al. (2006) observe an agreement of 52.73%. Fersini et al. (2015) report an agreement of 0.79. Riloff et al. (2013) observe an agreement of 0.81."}, {"heading": "7.2 Sarcasm before sentiment or sentiment before sarcasm?", "text": "The motivation behind sarcasm detection is often pointed as sarcastic sentences misleading a sentiment classifier. However, several approaches use sentiment as an input to the sarcasm classifier. It must, however, be noted that these approaches require \u2018surface polarity\u2019 the apparent polarity of a sentence. Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture polarity value in terms of two emotion dimensions: activation and pleasantness. Buschmeier et al. (2014) incorporate sentiment imbalance as a feature. Sentiment imbalance is a situation where star rating of a review disagrees with the surface polarity. Bouazizi and Ohtsuki (2015) cascade sarcasm detection and sentiment detection and observes an improvement of about 4% in accuracy when sentiment detection is aware of sarcastic nature."}, {"heading": "7.3 Dealing with Dataset Skews", "text": "Data skew in sarcasm-labeled datasets is a critical challenge to sarcasm classifiers. Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this skew. In Liu et al. (2014), a multi-strategy ensemble learning approach is used that uses ensembles and majority voting. Similarly, in order to deal with sparse features and skew of data, Wallace (2015) introduce a LSS-regularization strategy. Thus, they use a sparsifying L1 regularizer over contextual features and L2-norm for bag of word features. Liebrecht et al. (2013) report AUC for balanced as well as skewed datasets, to demonstrate the benefit of their classifier."}, {"heading": "8 Conclusion & Future Directions", "text": "In this paper, we presented a literature survey of current approaches for automatic sarcasm detection. We observe three trends in sarcasm detection research: semisupervised pattern extraction to identify implicit sentiment, use of hashtag-based supervision, and use of context beyond target text. We tabulated datasets and approaches that have been reported. We also highlight two key issues in sarcasm detection: the relationship between sentiment and sarcasm, and data skew in case of sarcasm-labeled datasets.\nSarcasm detection research has flourished significantly in the past few years, necessitating a look-back at the overall picture that these individual works have led to. Based on our survey of these works, we propose following possible directions for future:\n1. Implicit sentiment detection & sarcasm: Based on past work, it is well-established that sarcasm is closely linked to sentiment incongruity (Liebrecht et al., 2013). Several related works exist for detection of implicit sentiment in sentences, as in the case of \u2018The phone gets heated quickly\u2019 v/s \u2018The induction cooktop gets heated quickly\u2019. This will help sarcasm detection, following the line of semisupervised pattern discovery.\n2. Incongruity in numbers: Joshi et al. (2015) point out how numerical values convey sentiment and hence, is related to sarcasm. Consider the example of \u2019Took 6 hours to reach work today. #yay\u2019. This sentence is sarcastic, as opposed to \u2018Took 10 minutes to reach work today. #yay\u2019.\n3. Coverage of different forms of sarcasm: In Section 2, we described four species of sarcasm: propositional, lexical, like-prefixed and illocutionary sarcasm. We observe that current approaches are limited in handling the last two forms of sarcasm: like-prefixed and illocutionary. Future work may focus on these forms of sarcasm.\n4. Culture-specific aspects of sarcasm detection: As shown in Liu et al. (2014), sarcasm is closely related to language/culture-specific traits. Future approaches to sarcasm detection in new languages will benefit from understanding such traits, and incorporating them into their classification frameworks."}], "references": [{"title": "Contextualized sarcasm detection on twitter", "author": ["Bamman", "Smith2015] David Bamman", "Noah A Smith"], "venue": "In Ninth International AAAI Conference on Web and Social Media", "citeRegEx": "Bamman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2015}, {"title": "Italian irony detection in twitter: a first approach", "author": ["Francesco Ronzano", "Horacio Saggion"], "venue": "In The First Italian Conference on Computational Linguistics CLiC-it", "citeRegEx": "Barbieri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2014}, {"title": "Modelling sarcasm in twitter, a novel approach", "author": ["Horacio Saggion", "Francesco Ronzano"], "venue": "ACL", "citeRegEx": "Barbieri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2014}, {"title": "Parsing-based sarcasm sentiment recognition in twitter data", "author": ["Korra Sathya Babu", "Sanjay Kumar Jena"], "venue": "In Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Anal-", "citeRegEx": "Bharti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bharti et al\\.", "year": 2015}, {"title": "Opinion mining in twitter how to make use of sarcasm to enhance", "author": ["Bouazizi", "Ohtsuki2015] Mondher Bouazizi", "Tomoaki Ohtsuki"], "venue": null, "citeRegEx": "Bouazizi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouazizi et al\\.", "year": 2015}, {"title": "An impact analysis of features in a classification approach to irony detection in product reviews", "author": ["Philipp Cimiano", "Roman Klinger"], "venue": null, "citeRegEx": "Buschmeier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2014}, {"title": "Sarcasm, pretense, and the semantics/pragmatics distinction", "author": ["Elisabeth Camp"], "venue": "Nou\u0302s,", "citeRegEx": "Camp.,? \\Q2012\\E", "shortCiteRegEx": "Camp.", "year": 2012}, {"title": "Are there necessary conditions for inducing a sense of sarcastic irony? Discourse Processes, 49(6):459\u2013480", "author": ["Campbell", "Katz2012] John D Campbell", "Albert N Katz"], "venue": null, "citeRegEx": "Campbell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Campbell et al\\.", "year": 2012}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Oren Tsur", "Ari Rappoport"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Reactions to irony in discourse: Evidence for the least disruption principle", "author": ["Salvatore Attardo", "Diana Boxer"], "venue": "Journal of Pragmatics,", "citeRegEx": "Eisterhold et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Eisterhold et al\\.", "year": 2006}, {"title": "Detecting irony and sarcasm in microblogs: The role of expressive signals and ensemble classifiers", "author": ["Federico Alberto Pozzi", "Enza Messina"], "venue": "In Data Science and Advanced Analytics (DSAA),", "citeRegEx": "Fersini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fersini et al\\.", "year": 2015}, {"title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing", "author": ["Elena Filatova"], "venue": "In LREC,", "citeRegEx": "Filatova.,? \\Q2012\\E", "shortCiteRegEx": "Filatova.", "year": 2012}, {"title": "2015a. Semeval-2015 task 11: Sentiment analysis of figurative language in twitter", "author": ["Guofu Li", "Tony Veale", "Paolo Rosso", "Ekaterina Shutova", "Antonio Reyes", "John Barnden"], "venue": "In Int. Workshop on Semantic Evaluation", "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "Sarcastic or not: Word embeddings to predict the literal or sarcastic meaning of words", "author": ["Weiwei Guo", "Smaranda Muresan"], "venue": null, "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "On irony and negation", "author": ["Rachel Giora"], "venue": "Discourse processes,", "citeRegEx": "Giora.,? \\Q1995\\E", "shortCiteRegEx": "Giora.", "year": 1995}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["Smaranda Muresan", "Nina Wacholder"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Gonz\u00e1lezIb\u00e1nez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gonz\u00e1lezIb\u00e1nez et al\\.", "year": 2011}, {"title": "Applying basic features from sentiment analysis for automatic irony detection", "author": ["Jos\u00e9-Miguel Bened\u0131", "Paolo Rosso"], "venue": "In Pattern Recognition and Image Analysis,", "citeRegEx": "Hern\u00e1ndez.Far\u0131\u0301as et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Far\u0131\u0301as et al\\.", "year": 2015}, {"title": "Context incongruity and irony processing", "author": ["Ivanko", "Pexman2003] Stacey L Ivanko", "Penny M Pexman"], "venue": "Discourse Processes,", "citeRegEx": "Ivanko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ivanko et al\\.", "year": 2003}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["Joshi et al.2015] Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "citeRegEx": "Joshi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Your sentiment precedes you: Using an authors historical tweets to predict sarcasm", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman"], "venue": "In 6TH WORKSHOP ON COMPUTATIONAL", "citeRegEx": "Khattri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khattri et al\\.", "year": 2015}, {"title": "Lexical influences on the perception of sarcasm", "author": ["Kreuz", "Caucci2007] Roger J Kreuz", "Gina M Caucci"], "venue": "In Proceedings of the Workshop on computational approaches to Figurative Language,", "citeRegEx": "Kreuz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kreuz et al\\.", "year": 2007}, {"title": "The perfect solution for detecting sarcasm in tweets", "author": ["FA Kunneman", "APJ van den Bosch"], "venue": null, "citeRegEx": "Liebrecht et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liebrecht et al\\.", "year": 2013}, {"title": "Sarcasm detection in social media based on imbalanced classification", "author": ["Liu et al.2014] Peng Liu", "Wei Chen", "Gaoyan Ou", "Tengjiao Wang", "Dongqing Yang", "Kai Lei"], "venue": "In Web-Age Information Management,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Sentiment analysis and subjectivity", "author": ["Bing Liu"], "venue": "Handbook of natural language processing,", "citeRegEx": "Liu.,? \\Q2010\\E", "shortCiteRegEx": "Liu.", "year": 2010}, {"title": "Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue", "author": ["Lukin", "Walker2013] Stephanie Lukin", "Marilyn Walker"], "venue": "In Proceedings of the Workshop on Language Analysis in Social Me-", "citeRegEx": "Lukin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lukin et al\\.", "year": 2013}, {"title": "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis", "author": ["Maynard", "Greenwood2014] Diana Maynard", "Mark A Greenwood"], "venue": "In Proceedings of LREC", "citeRegEx": "Maynard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maynard et al\\.", "year": 2014}, {"title": "Identification of nonliteral language in social media: A case study on sarcasm", "author": ["Roberto Gonzalez-Ibanez", "Debanjan Ghosh", "Nina Wacholder"], "venue": "Journal of the Association", "citeRegEx": "Muresan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Muresan et al\\.", "year": 2016}, {"title": "Sarcasm detection on czech and english twitter", "author": ["Pt\u00e1cek et al.2014] Tom\u00e1\u0161 Pt\u00e1cek", "Ivan Habernal", "Jun Hong"], "venue": "In Proceedings COLING 2014. COLING", "citeRegEx": "Pt\u00e1cek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pt\u00e1cek et al\\.", "year": 2014}, {"title": "Sarcasm detection on twitter: A behavioral modeling approach", "author": ["Reza Zafarani", "Huan Liu"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Rajadesingan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2015}, {"title": "sure, i did the right thing\u201d: a system for sarcasm detection in speech", "author": ["Rakov", "Rosenberg2013] Rachel Rakov", "Andrew Rosenberg"], "venue": "In INTERSPEECH,", "citeRegEx": "Rakov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rakov et al\\.", "year": 2013}, {"title": "Making objective decisions from subjective data: Detecting irony in customer reviews", "author": ["Reyes", "Rosso2012] Antonio Reyes", "Paolo Rosso"], "venue": "Decision Support Systems,", "citeRegEx": "Reyes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2012}, {"title": "On the difficulty of automatically detecting irony: beyond a simple case of negation", "author": ["Reyes", "Rosso2014] Antonio Reyes", "Paolo Rosso"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Reyes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2014}, {"title": "From humor recognition to irony detection: The figurative language of social media", "author": ["Reyes et al.2012] Antonio Reyes", "Paolo Rosso", "Davide Buscaldi"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "Reyes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2012}, {"title": "A multidimensional approach for detecting irony in twitter", "author": ["Reyes et al.2013] Antonio Reyes", "Paolo Rosso", "Tony Veale"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Reyes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2013}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Riloff et al.2013] Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang"], "venue": null, "citeRegEx": "Riloff et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": " yeah right\u201d: sarcasm recognition for spoken dialogue systems", "author": ["David R Traum", "Shrikanth Narayanan"], "venue": "In INTERSPEECH. Citeseer", "citeRegEx": "Tepperman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tepperman et al\\.", "year": 2006}, {"title": "Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews", "author": ["Tsur et al.2010] Oren Tsur", "Dmitry Davidov", "Ari Rappoport"], "venue": null, "citeRegEx": "Tsur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsur et al\\.", "year": 2010}, {"title": "Detecting ironic intent in creative comparisons", "author": ["Veale", "Hao2010] Tony Veale", "Yanfen Hao"], "venue": "In ECAI,", "citeRegEx": "Veale et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Veale et al\\.", "year": 2010}, {"title": "Humans require context to infer ironic intent (so computers probably do, too)", "author": ["Laura Kertz Do Kook Choe", "Eugene Charniak"], "venue": "In Proceedings of the Annual Meeting of the Association", "citeRegEx": "Wallace et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wallace et al\\.", "year": 2014}, {"title": "Computational irony: A survey and new perspectives", "author": ["Byron C Wallace"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Wallace.,? \\Q2013\\E", "shortCiteRegEx": "Wallace.", "year": 2013}, {"title": "Sparse, contextually informed models for irony detection: Exploiting user communities,entities and sentiment", "author": ["Byron C Wallace"], "venue": null, "citeRegEx": "Wallace.,? \\Q2015\\E", "shortCiteRegEx": "Wallace.", "year": 2015}, {"title": "Twitter sarcasm detection exploiting a context-based model", "author": ["Wang et al.2015] Zelin Wang", "Zhijian Wu", "Ruimin Wang", "Yafeng Ren"], "venue": "In Web Information Systems Engineering\u2013WISE", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "The pragmatics of verbal irony: Echo or pretence? Lingua, 116(10):1722\u20131743", "author": ["Deirdre Wilson"], "venue": null, "citeRegEx": "Wilson.,? \\Q2006\\E", "shortCiteRegEx": "Wilson.", "year": 2006}], "referenceMentions": [{"referenceID": 23, "context": "Sarcasm is an often-quoted challenge for sentiment analysis (Liu, 2010) because sarcasm intends a negative sentiment, but a positive surface sentiment.", "startOffset": 60, "endOffset": 71}, {"referenceID": 23, "context": "Sarcasm is an often-quoted challenge for sentiment analysis (Liu, 2010) because sarcasm intends a negative sentiment, but a positive surface sentiment. This led to introduction of sarcasm detection as a research problem. Automatic sarcasm detection refers to computational approaches to detect sarcasm in text. This problem is hard because of nuanced ways in which sarcasm may be expressed. These nuances arise in several ways: general understanding (\u2018I love to get ignored\u2019), in terms of an author\u2019s background (\u2018I love solving math problems all weekend\u2019), in terms of a culture (popular use of honorifics in Chinese sarcasm), etc. Starting from the earliest known work by Tepperman et al. (2006) which deals with speech and text-related features, sarcasm detection research has seen wide interest.", "startOffset": 61, "endOffset": 698}, {"referenceID": 39, "context": "One past work to summarize computational irony is by Wallace (2013). However, they focus on linguistic challenges of computational irony, and deliberate on possible future work.", "startOffset": 53, "endOffset": 68}, {"referenceID": 42, "context": "Situational disparity theory: According to Wilson (2006), sarcasm arises when there is situational disparity between text and a contextual information.", "startOffset": 43, "endOffset": 57}, {"referenceID": 14, "context": "Negation theory of sarcasm: Giora (1995) state that irony/sarcasm is a form of negation in which an explicit negation marker is lacking.", "startOffset": 28, "endOffset": 41}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm.", "startOffset": 65, "endOffset": 89}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm. Reyes et al. (2013) use a similar formulation and provide pair-wise classification performance for these labels.", "startOffset": 65, "endOffset": 184}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm. Reyes et al. (2013) use a similar formulation and provide pair-wise classification performance for these labels. Even in the context of classification, there have been interesting variations in the data units that will be annotated and hence, classified. Tepperman et al. (2006) is an early paper in sarcasm detection that looks at occurrences of a common sarcastic phrase \u2018yeah right\u2019 and classifies each occurrence of \u2018yeah right\u2019 as sarcastic or not.", "startOffset": 65, "endOffset": 443}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm. Reyes et al. (2013) use a similar formulation and provide pair-wise classification performance for these labels. Even in the context of classification, there have been interesting variations in the data units that will be annotated and hence, classified. Tepperman et al. (2006) is an early paper in sarcasm detection that looks at occurrences of a common sarcastic phrase \u2018yeah right\u2019 and classifies each occurrence of \u2018yeah right\u2019 as sarcastic or not. Veale and Hao (2010) annotate similes such as \u2018as excited as a patient before a surgery\u2019 with sarcastic or not labels.", "startOffset": 65, "endOffset": 639}, {"referenceID": 12, "context": "Ghosh et al. (2015b) model sarcasm detection as a sense disambiguation task.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Ghosh et al. (2015b) model sarcasm detection as a sense disambiguation task. They state that a word may have a literal sense and a sarcastic sense. Their goal is to identify the sense of a word in order to detect sarcasm. Wang et al. (2015) formulate the problem as a sequence labeling problem.", "startOffset": 0, "endOffset": 241}, {"referenceID": 20, "context": "Liu et al. (2014) use Chinese and English social media content.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Barbieri et al. (2014a) present a first approach to detect sarcasm in Italian tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Barbieri et al. (2014a) present a first approach to detect sarcasm in Italian tweets. Pt\u00e1cek et al. (2014)", "startOffset": 0, "endOffset": 107}, {"referenceID": 21, "context": "Liebrecht et al. (2013) explore sarcasm detection for Dutch.", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "Riloff et al. (2013) use a dataset of tweets, manually annotated as sarcastic or not.", "startOffset": 0, "endOffset": 21}, {"referenceID": 33, "context": "Riloff et al. (2013) use a dataset of tweets, manually annotated as sarcastic or not. Maynard and Greenwood (2014) study sarcastic tweets and their impact to sarcasm classification.", "startOffset": 0, "endOffset": 115}, {"referenceID": 27, "context": "Pt\u00e1cek et al. (2014) present a dataset of 7,000 manually labeled tweets in Czech.", "startOffset": 0, "endOffset": 21}, {"referenceID": 36, "context": "(Tsur et al., 2010) X X X X X", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "(Davidov et al., 2010) X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "(Reyes et al., 2012) X X X X X X X", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "(Filatova, 2012) X X", "startOffset": 0, "endOffset": 16}, {"referenceID": 34, "context": "(Riloff et al., 2013) X X X X X X", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "(Liebrecht et al., 2013) X X X X X X", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "(Reyes et al., 2013) X X X X X X X", "startOffset": 0, "endOffset": 20}, {"referenceID": 38, "context": "(Wallace et al., 2014) X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "(Buschmeier et al., 2014) X X X X X X X", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "(Joshi et al., 2015) X X X X X X X X X", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "(Khattri et al., 2015) X X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "(Rajadesingan et al., 2015) X X X X X X X X", "startOffset": 0, "endOffset": 27}, {"referenceID": 40, "context": "(Wallace, 2015) X X X X X X X X", "startOffset": 0, "endOffset": 15}, {"referenceID": 16, "context": "(Hern\u00e1ndez-Far\u0131\u0301as et al., 2015) X X X X X X X", "startOffset": 0, "endOffset": 32}, {"referenceID": 41, "context": "(Wang et al., 2015) X X X X X", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "(Liu et al., 2014) X X X X X X X X", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Bharti et al., 2015) X X X X X X", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "(Fersini et al., 2015) X X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "(Muresan et al., 2016) X X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) also use hashtag-based supervision for tweets.", "startOffset": 0, "endOffset": 150}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) also use hashtag-based supervision for tweets. However, they eliminate cases where the hashtag is a part of the running text. For example, \u2018#sarcasm is popular in india\u2019 is eliminated. Reyes et al. (2012) use hashtag-based supervision for tweets.", "startOffset": 0, "endOffset": 355}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) also use hashtag-based supervision for tweets. However, they eliminate cases where the hashtag is a part of the running text. For example, \u2018#sarcasm is popular in india\u2019 is eliminated. Reyes et al. (2012) use hashtag-based supervision for tweets. Reyes et al. (2013) use a dataset of 40000 tweets labeled as sarcastic or not, using hashtags.", "startOffset": 0, "endOffset": 417}, {"referenceID": 1, "context": "Barbieri et al. (2014a) introduce a dataset of 25K Italian tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Barbieri et al. (2014a) introduce a dataset of 25K Italian tweets. Joshi et al. (2015) present a dataset of tweets", "startOffset": 0, "endOffset": 87}, {"referenceID": 34, "context": "Tweets Manual: (Riloff et al., 2013; Maynard and Greenwood, 2014; Pt\u00e1cek et al., 2014) Hashtag-based: (Davidov et al.", "startOffset": 15, "endOffset": 86}, {"referenceID": 27, "context": "Tweets Manual: (Riloff et al., 2013; Maynard and Greenwood, 2014; Pt\u00e1cek et al., 2014) Hashtag-based: (Davidov et al.", "startOffset": 15, "endOffset": 86}, {"referenceID": 8, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 30, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 33, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 18, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 3, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 21, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 41, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 10, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 19, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 28, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 38, "context": "Reddits (Wallace et al., 2014; Wallace, 2015)", "startOffset": 8, "endOffset": 45}, {"referenceID": 40, "context": "Reddits (Wallace et al., 2014; Wallace, 2015)", "startOffset": 8, "endOffset": 45}, {"referenceID": 5, "context": ") (Lukin and Walker, 2013; Reyes and Rosso, 2014; Reyes and Rosso, 2012; Buschmeier et al., 2014; Liu et al., 2014; Filatova, 2012)", "startOffset": 2, "endOffset": 131}, {"referenceID": 22, "context": ") (Lukin and Walker, 2013; Reyes and Rosso, 2014; Reyes and Rosso, 2012; Buschmeier et al., 2014; Liu et al., 2014; Filatova, 2012)", "startOffset": 2, "endOffset": 131}, {"referenceID": 11, "context": ") (Lukin and Walker, 2013; Reyes and Rosso, 2014; Reyes and Rosso, 2012; Buschmeier et al., 2014; Liu et al., 2014; Filatova, 2012)", "startOffset": 2, "endOffset": 131}, {"referenceID": 35, "context": "Other datasets (Tepperman et al., 2006; Kreuz and Caucci, 2007; Veale and Hao, 2010; Rakov and Rosenberg, 2013; Ghosh et al., 2015b)", "startOffset": 15, "endOffset": 132}, {"referenceID": 11, "context": "Ghosh et al. (2015a) use hashtag-based dataset of tweets.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags. Liebrecht et al. (2013) identify how the hashtag \u2018#not\u2019 is a popular modern form of expressing sarcasm.", "startOffset": 0, "endOffset": 96}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags. Liebrecht et al. (2013) identify how the hashtag \u2018#not\u2019 is a popular modern form of expressing sarcasm. The dataset involves tweets that contain such a hashtag. Bouazizi and Ohtsuki (2015) use Sentiment140 dataset of tweets.", "startOffset": 0, "endOffset": 261}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags. Liebrecht et al. (2013) identify how the hashtag \u2018#not\u2019 is a popular modern form of expressing sarcasm. The dataset involves tweets that contain such a hashtag. Bouazizi and Ohtsuki (2015) use Sentiment140 dataset of tweets. Wang et al. (2015) use a dataset of tweets marked as happy, sad and sarcastic.", "startOffset": 0, "endOffset": 316}, {"referenceID": 1, "context": "Barbieri et al. (2014b) create a dataset using hashtag-based supervision to create multiple labels: politics, sarcasm, humor and irony.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Fersini et al. (2015) use a dataset of 8K tweets marked using hashtags.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "Khattri et al. (2015) use a dataset of tweets, introduced in the past.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "Khattri et al. (2015) use a dataset of tweets, introduced in the past. They look up the complete twitter timeline (limited to 3200 tweets, by Twitter) to establish context. Rajadesingan et al. (2015) use a dataset of tweets, labeled by hashtagbased supervision along with a historical context of 80 tweets per author.", "startOffset": 0, "endOffset": 200}, {"referenceID": 38, "context": "Wallace et al. (2014) create a corpus of reddit posts of 10K sentences, from 6 reddit topics.", "startOffset": 0, "endOffset": 22}, {"referenceID": 38, "context": "Wallace et al. (2014) create a corpus of reddit posts of 10K sentences, from 6 reddit topics. Wallace (2015) present a dataset of reddit comments: 5625 sentences.", "startOffset": 0, "endOffset": 109}, {"referenceID": 10, "context": "Filatova (2012) use a dataset of around 1000 reviews, marked as sarcastic or not.", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "Buschmeier et al. (2014) use 1254 Amazon reviews, out of which 437 are ironic.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Buschmeier et al. (2014) use 1254 Amazon reviews, out of which 437 are ironic. Tsur et al. (2010) consider a large dataset of 66000 amazon reviews.", "startOffset": 0, "endOffset": 98}, {"referenceID": 5, "context": "Buschmeier et al. (2014) use 1254 Amazon reviews, out of which 437 are ironic. Tsur et al. (2010) consider a large dataset of 66000 amazon reviews. Liu et al. (2014) use a dataset from multiple sources such as Amazon, Twitter, Netease and Netcena.", "startOffset": 0, "endOffset": 166}, {"referenceID": 35, "context": "Tepperman et al. (2006) use 131 call center transcripts.", "startOffset": 0, "endOffset": 24}, {"referenceID": 35, "context": "Tepperman et al. (2006) use 131 call center transcripts. Each occurrence of \u2018yeah right\u2019 is marked as sarcastic or not. The goal is to find features in a transcripts that identify which \u2018yeah right\u2019 is sarcastic. Kreuz and Caucci (2007) use 20 sarcastic excerpts and 15 non-sarcastic", "startOffset": 0, "endOffset": 237}, {"referenceID": 12, "context": "Ghosh et al. (2015b) use a crowdsourcing tool to obtain a non-sarcastic version of a sentence if applicable.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) present two rule-based classifiers.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) present two rule-based classifiers. The first uses a parse\u2013based lexicon generation algorithm that creates parse trees of sentences and identifies situation phrases that bear sentiment. If a negative phrase occurs in a positive sentence, it is predicted as sarcastic. The second algorithm aims to capture hyperboles by using interjection and intensifiers occur together. Riloff et al. (2013) present rule-based classifiers that look for a positive verb and a negative situation phrase in a sentence.", "startOffset": 0, "endOffset": 413}, {"referenceID": 20, "context": "Reyes et al. (2012) use features related to ambiguity, unexpectedness, emotional scenario, etc.", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "Reyes et al. (2012) use features related to ambiguity, unexpectedness, emotional scenario, etc. Ambiguity features cover structural, morpho-syntactic, semantic ambiguity, while unexpectedness features measure semantic relatedness. Riloff et al. (2013) use a set of patterns, specifically positive verbs and negative situation phrases, as features for a classifier.", "startOffset": 0, "endOffset": 252}, {"referenceID": 16, "context": "Liebrecht et al. (2013) introduce bigrams and trigrams as features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "Liebrecht et al. (2013) introduce bigrams and trigrams as features. Reyes et al. (2013) explore skip-gram and character n-gram-based features.", "startOffset": 0, "endOffset": 88}, {"referenceID": 16, "context": "Liebrecht et al. (2013) introduce bigrams and trigrams as features. Reyes et al. (2013) explore skip-gram and character n-gram-based features. Maynard and Greenwood (2014) include seven sets of features.", "startOffset": 0, "endOffset": 172}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators.", "startOffset": 30, "endOffset": 54}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance.", "startOffset": 30, "endOffset": 128}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity.", "startOffset": 30, "endOffset": 234}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes.", "startOffset": 30, "endOffset": 415}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes. Rajadesingan et al. (2015) use extensions of words, number of flips, readability features in addition to others.", "startOffset": 30, "endOffset": 515}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes. Rajadesingan et al. (2015) use extensions of words, number of flips, readability features in addition to others. Hern\u00e1ndez-Far\u0131\u0301as et al. (2015) present features that measure semantic relatedness between words using Wordnet::similarity.", "startOffset": 30, "endOffset": 633}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes. Rajadesingan et al. (2015) use extensions of words, number of flips, readability features in addition to others. Hern\u00e1ndez-Far\u0131\u0301as et al. (2015) present features that measure semantic relatedness between words using Wordnet::similarity. Liu et al. (2014) introduce POS sequences and semantic imbalance as features.", "startOffset": 30, "endOffset": 743}, {"referenceID": 29, "context": "Riloff et al. (2013) compare rule-based techniques with a SVM-based classifier.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "Liebrecht et al. (2013) use balanced winnow algorithm in order to determine high-ranking features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "Liebrecht et al. (2013) use balanced winnow algorithm in order to determine high-ranking features. Reyes et al. (2013) use", "startOffset": 0, "endOffset": 119}, {"referenceID": 39, "context": "Wang et al. (2015) use SVM-HMM in order to incorporate sequence nature of output labels in a conversation.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "Liu et al. (2014) compare several classification approaches including bagging, boosting, etc.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon.", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features.", "startOffset": 0, "endOffset": 359}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.", "startOffset": 0, "endOffset": 490}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors like incongruity due to numbers and granularity of annotation.", "startOffset": 0, "endOffset": 543}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors like incongruity due to numbers and granularity of annotation. Rajadesingan et al. (2015) show that historical features along with flip-based features are the most discriminating features.", "startOffset": 0, "endOffset": 663}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors like incongruity due to numbers and granularity of annotation. Rajadesingan et al. (2015) show that historical features along with flip-based features are the most discriminating features. These are also the features that Khattri et al. (2015) use.", "startOffset": 0, "endOffset": 817}, {"referenceID": 3, "context": "Bharti et al. (2015) report a F-score of around 84%.", "startOffset": 0, "endOffset": 21}, {"referenceID": 35, "context": "Tsur et al. (2010) extract sarcastic patterns from a seed set of labeled sentences.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "Pt\u00e1cek et al. (2014) also use a similar approach for Czech and English tweets.", "startOffset": 0, "endOffset": 21}, {"referenceID": 36, "context": "(Tsur et al., 2010) Sarcastic patterns, Punctuations", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "(Reyes et al., 2012) Ambiguity-based, semantic relatedness", "startOffset": 0, "endOffset": 20}, {"referenceID": 34, "context": "(Riloff et al., 2013) Sarcastic patterns (Positive verbs, negative phrases)", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "(Liebrecht et al., 2013) N-grams, emotion marks, intensifiers", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "(Reyes et al., 2013) Skip-grams, Polarity skip-grams", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "(Buschmeier et al., 2014) Interjection, ellipsis, hyperbole, imbalance-based", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "(Joshi et al., 2015) Unigrams, Implicit incongruity-based, Explicit incongruity-based", "startOffset": 0, "endOffset": 20}, {"referenceID": 28, "context": "(Rajadesingan et al., 2015) Readability, flips, etc.", "startOffset": 0, "endOffset": 27}, {"referenceID": 16, "context": "(Hern\u00e1ndez-Far\u0131\u0301as et al., 2015) Length, capitalization, semantic similarity", "startOffset": 0, "endOffset": 32}, {"referenceID": 22, "context": "(Liu et al., 2014) POS sequences, Semantic imbalance.", "startOffset": 0, "endOffset": 18}, {"referenceID": 27, "context": "(Pt\u00e1cek et al., 2014) Word shape, Pointedness, etc.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Joshi et al. (2015) adapt this algorithm by eliminating subsumption, and show that it adds value.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "Khattri et al. (2015) follow the intuition \u2018A tweet is sarcastic either because it has words of contrasting sentiment in it, or because there is sentiment that contrasts with historical sentiment\u2019.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "Khattri et al. (2015) follow the intuition \u2018A tweet is sarcastic either because it has words of contrasting sentiment in it, or because there is sentiment that contrasts with historical sentiment\u2019. Historical tweets by the same author are considered as the context. Named entity phrases in the target tweet are looked up in the timeline of the author in order to gather the true sentiment of the author. This historical sentiment is then used to predict whether the author is likely to be sarcastic, given the sentiment expressed towards the entity in the target tweet. Rajadesingan et al. (2015) incorporate context about author using the author\u2019s past tweets.", "startOffset": 0, "endOffset": 597}, {"referenceID": 19, "context": "Khattri et al. (2015) follow the intuition \u2018A tweet is sarcastic either because it has words of contrasting sentiment in it, or because there is sentiment that contrasts with historical sentiment\u2019. Historical tweets by the same author are considered as the context. Named entity phrases in the target tweet are looked up in the timeline of the author in order to gather the true sentiment of the author. This historical sentiment is then used to predict whether the author is likely to be sarcastic, given the sentiment expressed towards the entity in the target tweet. Rajadesingan et al. (2015) incorporate context about author using the author\u2019s past tweets. This context is captured as features for a classifier. The features deal with various dimensions. They use features about author\u2019s familiarity with twitter (in terms of use of hashtags), familiarity with language (in terms of words and structures), and familiarity with sarcasm. Bamman and Smith (2015) consider historical context in features such as historical salient terms, historical topic, profile info, historical sentiment (how likely is he/she to be negative), etc.", "startOffset": 0, "endOffset": 965}, {"referenceID": 18, "context": "Joshi et al. (2015) show that concatenation of the previous post in a discussion forum thread along with the target post leads to an improvement in precision.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "Joshi et al. (2015) show that concatenation of the previous post in a discussion forum thread along with the target post leads to an improvement in precision. Wallace (2015) look at comments in the thread structure to obtain context for sarcasm detection.", "startOffset": 0, "endOffset": 174}, {"referenceID": 41, "context": "Wang et al. (2015) use three types of context: history, conversation and topic.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "(Liebrecht et al., 2013) show how #not can be used to express sarcasm - while the rest of the sentence is non-sarcastic.", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "Tsur et al. (2010) use a dataset with a small set of sentences are marked as sarcastic.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "5% of tweets in the Italian dataset given by Barbieri et al. (2014a) are sarcastic.", "startOffset": 45, "endOffset": 69}, {"referenceID": 1, "context": "5% of tweets in the Italian dataset given by Barbieri et al. (2014a) are sarcastic. On the other hand, Rakov and Rosenberg (2013) present a balanced dataset of 15k tweets.", "startOffset": 45, "endOffset": 130}, {"referenceID": 1, "context": "5% of tweets in the Italian dataset given by Barbieri et al. (2014a) are sarcastic. On the other hand, Rakov and Rosenberg (2013) present a balanced dataset of 15k tweets. Liu et al. (2014) focus on this imbalance in data, and present sarcasm detection that is robust to this imbalance.", "startOffset": 45, "endOffset": 190}, {"referenceID": 34, "context": "Tsur et al. (2010) indicate an agreement of 0.", "startOffset": 0, "endOffset": 19}, {"referenceID": 34, "context": "Tepperman et al. (2006) observe an agreement of 52.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Fersini et al. (2015) report an agreement", "startOffset": 0, "endOffset": 22}, {"referenceID": 34, "context": "Riloff et al. (2013) observe an agreement of 0.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015).", "startOffset": 119, "endOffset": 210}, {"referenceID": 18, "context": "In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015).", "startOffset": 119, "endOffset": 210}, {"referenceID": 28, "context": "In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015).", "startOffset": 119, "endOffset": 210}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm.", "startOffset": 0, "endOffset": 180}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture polarity value in terms of two emotion dimensions: activation and pleasantness.", "startOffset": 0, "endOffset": 477}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture polarity value in terms of two emotion dimensions: activation and pleasantness. Buschmeier et al. (2014) incorporate sentiment imbalance as a feature.", "startOffset": 0, "endOffset": 590}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture polarity value in terms of two emotion dimensions: activation and pleasantness. Buschmeier et al. (2014) incorporate sentiment imbalance as a feature. Sentiment imbalance is a situation where star rating of a review disagrees with the surface polarity. Bouazizi and Ohtsuki (2015) cascade sarcasm detection and sentiment detection and observes an improvement of about 4% in accuracy when sentiment detection is aware of sarcastic nature.", "startOffset": 0, "endOffset": 766}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this skew. In Liu et al. (2014), a multi-strategy ensemble learning approach is used that uses ensembles and majority voting.", "startOffset": 0, "endOffset": 169}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this skew. In Liu et al. (2014), a multi-strategy ensemble learning approach is used that uses ensembles and majority voting. Similarly, in order to deal with sparse features and skew of data, Wallace (2015) introduce a LSS-regularization strategy.", "startOffset": 0, "endOffset": 345}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this skew. In Liu et al. (2014), a multi-strategy ensemble learning approach is used that uses ensembles and majority voting. Similarly, in order to deal with sparse features and skew of data, Wallace (2015) introduce a LSS-regularization strategy. Thus, they use a sparsifying L1 regularizer over contextual features and L2-norm for bag of word features. Liebrecht et al. (2013) report AUC for balanced as well as skewed datasets, to demonstrate the benefit of their classifier.", "startOffset": 0, "endOffset": 517}, {"referenceID": 21, "context": "Implicit sentiment detection & sarcasm: Based on past work, it is well-established that sarcasm is closely linked to sentiment incongruity (Liebrecht et al., 2013).", "startOffset": 139, "endOffset": 163}, {"referenceID": 18, "context": "Incongruity in numbers: Joshi et al. (2015) point out how numerical values convey sentiment and hence, is related to sarcasm.", "startOffset": 24, "endOffset": 44}, {"referenceID": 22, "context": "Culture-specific aspects of sarcasm detection: As shown in Liu et al. (2014), sarcasm is closely related to language/culture-specific traits.", "startOffset": 59, "endOffset": 77}], "year": 2016, "abstractText": "Automatic detection of sarcasm has witnessed interest from the sentiment analysis research community. With diverse approaches, datasets and analyses that have been reported, there is an essential need to have a collective understanding of the research in this area. In this survey of automatic sarcasm detection, we describe datasets, approaches (both supervised and rule-based), and trends in sarcasm detection research. We also present a research matrix that summarizes past work, and list pointers to future work.", "creator": "LaTeX with hyperref package"}}}