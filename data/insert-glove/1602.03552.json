{"id": "1602.03552", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Learning privately from multiparty data", "abstract": "guiterrez Learning aggressively a 5.85 classifier from limewood private elizondo data collected by multiple remixing parties western-style is kleeb an larysa important slowed-down problem that knockout has many potential applications. scantlin How vamped can we build helmrich an jimena accurate matopos and 138.00 differentially pi\u00e8ce private global classifier by combining basuo locally - trained goalball classifiers bie from isin different parties, without access to marquessate any martinova party ' kairouan s kronoberg private hostel data? 3,920 We propose krasnoyarsk-26 to 1,261 transfer kaliyuga the ` bellerive knowledge ' of nannilam the maliku local isobutane classifier ensemble 2008-present by first creating labeled singareni data from auxiliary kelin unlabeled rabalais data, and then relabel train a global $ \\ epsilon $ - paradises differentially private 23-29 classifier. We show that colorize majority dinajpur voting dscc is too 787-8 sensitive myp and a-10 therefore kansas-nebraska propose cope a 9.49 new zero-emissions risk fairness weighted by class pritam probabilities almer estimated from tanaro the 3,702 ensemble. agaja Relative to a florian non - keet private diamandouros solution, our private asf solution directo has mjr a lubrano generalization gustan error bounded anglicization by $ O (\\ epsilon ^ {- 2} M ^ {- noncommunicable 2} ) $ ouadda where $ M $ theramenes is 4-1-0 the dramatisations number of 33.12 parties. numeros This allows l.y. strong zeluco privacy pneumatically without plateaued performance 373.5 loss when $ grandpappy M $ is chargeable large, such as petrushova in loincloths crowdsensing applications. reenactors We suita demonstrate the performance of asato our 20-goal method anslinger with mary-ann realistic 320si tasks peaceville of activity biberach recognition, fikir network crisman intrusion sriram detection, electrostatics and jainal malicious portacio URL quarks detection.", "histories": [["v1", "Wed, 10 Feb 2016 22:02:43 GMT  (352kb,D)", "http://arxiv.org/abs/1602.03552v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["jihun hamm", "yingjun cao", "mikhail belkin"], "accepted": true, "id": "1602.03552"}, "pdf": {"name": "1602.03552.pdf", "metadata": {"source": "META", "title": "Learning Privately from Multiparty Data", "authors": ["Jihun Hamm", "Paul Cao", "Mikhail Belkin"], "emails": ["HAMMJ@CSE.OHIO-STATE.EDU", "YIC242@ENG.UCSD.EDU", "MBELKIN@CSE.OHIO-STATE.EDU"], "sections": [{"heading": "1. Introduction", "text": "Consider the problem of performing machine learning with data collected by multiple parties. In many settings, the parties may not wish to disclose the private information. For example, the parties can be medical institutions who aim to perform collaborative research using sensitive patient information they hold. For another example, the parties can be computer users who aim to collectively build a malware detector without sharing their usage data. A conventional approach to learning from multiparty data is to first collect data from all parties and then process them centrally. When privacy is a major concern, this approach\nis not always an appropriate solution since it is vulnerable to attacks during transmission, storage, and processing of data. Instead, we will consider a setting in which each party trains a local classifier from its private data without sending the data. The goal is to build a global classifier by combining local classifiers efficiently and privately. We expect the global classifier to be more accurate than individual local classifiers, as it has access to more information than individual classifiers.\nThis problem of aggregating classifiers was considered in (Pathak et al., 2010), where the authors proposed averaging of the parameters of local classifiers to get a global classifier. To prevent the leak of private information from the averaged parameters, the authors used a differentially private mechanism. Differential privacy measures maximal change in the probability of any outcome of a procedure when any item is added to or removed from a database. It provides a strict upper bound on the privacy loss against any adversary (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006). Parameter averaging is a simple and practical procedure that can be implemented by Secure Multiparty Computation (Yao, 1982). However, averaging is not applicable to classifiers with non-numerical parameters such as decision trees, nor to a collection of different classifier types. This raises the question if there are more flexible and perhaps better ways of aggregating local classifiers privately.\nIn this paper, we propose a method of building a global differentially private classifier from an ensemble of local classifier in two steps (see Fig. 1.) In the first step, locally-trained classifiers are collected by a trusted entity. A naive approach to use the collected classifiers is to release (the parameters of) the classifiers after sanitization by differentially private mechanisms, which is impractical (Sec. 3.2.) Instead, we use the classifier ensemble to generates (pseudo)labels for auxiliary unlabeled data, thus transferring the knowledge of the ensemble to the auxiliary data. In the second step, we use the labeled auxiliary data to find\nar X\niv :1\n60 2.\n03 55\n2v 1\n[ cs\n.L G\n] 1\n0 Fe\nb 20\nan empirical risk minimizer, and release a differentially private classifier using output perturbation (Chaudhuri et al., 2011).\nWhen generating labels for auxiliary data using an ensemble of classifiers, majority voting is the simplest choice. However, we show quantitatively that a global classifier trained from majority-voted labels is highly sensitive to individual votes of local classifiers. Consequently, the final classifier after differentially-private sanitization suffer a heavy loss in its performance. To address this, we propose a new risk insensitive to individual votes, where each sample is weighted by the confidence of the ensemble. We provide an interpretation of the weighted risk in terms of random hypothesis of an ensemble (Breiman, 1996a) in contrast to deterministic labeling rule of majority voting. One of our main results is in Theorem 4: we can achieve -differential privacy with a generalization error of O( \u22122M\u22122) and O(N\u22121) terms, relative to the expected risk of a non-private solution, where M is the number of parties and N is the number of samples in auxiliary data. This result is especially useful in a scenario where there are a large number of parties with weak local classifiers such as a group of connected smart devices with limited computing capability. We demonstrate the performance of our approach with several realistic tasks: activity recognition, network intrusion detection, and malicious URL detection. The results show that it is feasible to achieve both accuracy and privacy with a large number of parties.\nTo summarize, we propose a method of building a global differentially private classifier from locally-trained classifiers of multiple parties without access to their private data. The proposed method has the following advantages: 1) it\ncan use local classifiers of any (mixed) types and therefore is flexible; 2) its generalization error converges to that of a non-private solution with a fast rate of O( \u22122M\u22122) and O(N\u22121); 3) it also provides -differential privacy to all samples of a party and not just a single sample.\nIn Sec. 2, we formally describe privacy definitions. In Sec. 3, we discuss the first step of leveraging unlabeled data, and in Sec. 4, we present the second step of finding a global private classifier via empirical risk minimization in two different forms. In Sec. 5, we discuss related works. We evaluate the methods with real tasks in Sec. 6 and conclude the paper in Sec. 7. Appendix contains omitted proofs and several extensions of the algorithms."}, {"heading": "2. Preliminary", "text": ""}, {"heading": "2.1. Differential privacy", "text": "A randomized algorithm that takes dataD as input and outputs a function f is called -differentially private if\nP (f(D) \u2208 S) P (f(D\u2032) \u2208 S) \u2264 e (1)\nfor all measurable S \u2282 T of the output range and for all datasets D and D\u2032 differing in a single item, denoted by D \u223c D\u2032. That is, even if an adversary knows the whole dataset D except for a single item, she cannot infer much about the unknown item from the output f of the algorithm. When an algorithm outputs a real-valued vector f \u2208 RD, its global L2 sensitivity (Dwork et al., 2006) can be defined as\nS(f) = max D\u223cD\u2032\n\u2016f(D)\u2212 f(D\u2032)\u2016 (2)\nwhere \u2016\u00b7\u2016 is theL2 norm. An important result from (Dwork et al., 2006) is that a vector-valued output f with sensitivity S(f) can be made -differentially private by perturbing f with an additive noise vector \u03b7 whose density is\nP (\u03b7) \u221d e\u2212 S(f) \u2016\u03b7\u2016. (3)"}, {"heading": "2.2. Output perturbation", "text": "When a classifier which minimizes empirical risk is released in public, it leaks information about the training data. Such a classifier can be sanitized by perturbation with additive noise calibrated to the sensitivity of the classifier, known as output perturbation method (Chaudhuri et al., 2011). Specifically, the authors show the following. If ws is the minimizer of the regularized empirical risk\nR\u03bbS(w) = 1\nN \u2211 (x,y)\u2208S l(h(x;w), y) + \u03bb 2 \u2016w\u20162, (4)\nThen the perturbed outputwp = ws+\u03b7, p(\u03b7) \u221d e\u2212 N\u03bb 2 \u2016\u03b7\u2016 is -differentially private for a single sample. Output perturbation was used to sanitize the averaged parameters in (Pathak et al., 2010). We also use output perturbation to sanitize global classifiers. One important difference of our setting to previous work is that we consider -differential privacy of all samples of a party, which is much stronger than -differential privacy of a single sample.\nThere are conditions on the loss function for this guarantee to hold. We will assume the following conditions for our global classifier1 similar to (Chaudhuri et al., 2011).\n\u2022 The loss-hypothesis has a form l(h(x;w), v)) = l(vwT\u03c6(x)), where \u03c6 : X \u2192 Rd is a fixed map. We will consider only linear classifiers l(vwTx), where any nonlinear map \u03c6 is absorbed into the ddimensional features.\n\u2022 The surrogate loss l(\u00b7) is convex and continuously differentiable. \u2022 The derivative l\u2032(\u00b7) is bounded: |l\u2032(t)| \u2264 1, \u2200t \u2208 R, and c-Lipschitz: |l\u2032(s)\u2212 l\u2032(t)| \u2264 c|s\u2212 t|, \u2200s, t \u2208 R.\n\u2022 The features are bounded: supx\u2208X \u2016x\u2016 \u2264 1.\nThese conditions are satisfied by, e.g., logistic regression loss (c = 1/4) and approximate hinge loss."}, {"heading": "3. Transferring knowledge of ensemble", "text": ""}, {"heading": "3.1. Local classifiers", "text": "In this paper, we treat local classifiers as M black boxes h1(x), ..., hM (x). We assume that a local classifier hi(x)\n1Local classifiers are allowed to be of any type.\nis trained using its private i.i.d. training set S(i)\nS(i) = {(x(i)1 , y (i) 1 ), ... , (x (i) Ni , y (i) Ni )}, (5)\nwhere (x(i)j , y (i) j ) \u2208 X \u00d7{\u22121, 1} is a sample from a distribution P (x, y) common to all parties. We consider binary labels y \u2208 {\u22121, 1} in the main paper, and present a multiclass extension in Appendix B.\nThis splitting of training data across parties is similar to the Bagging procedure (Breiman, 1996a) with some differences. In Bagging, the training set S(i) for party i is sampled with replacement from the whole dataset D, whereas in our setting, the training set is sampled without replacement from D, more similar to the Subagging (Politis et al., 1999) procedure."}, {"heading": "3.2. Privacy issues of direct release", "text": "In the first step of our method, local classifiers from multiple parties are first collected by a trusted entity. A naive approach to use the ensemble is to directly release the local classifier parameters to the parties after appropriate sanitization. However, this is problematic in efficiency and privacy. Releasing all M classifier parameters is an operation with a constant sensitivity, as opposed to releasing insensitive statistics such as an average whose sensitivity is O(M\u22121). Releasing the classifiers requires much stronger perturbation than necessary, incurring steep loss of performance of sanitized classifiers. Besides, efficient differentially private mechanisms are known only for certain types of classifiers so far (see (Ji et al., 2014) for a review.) Another approach is to use the ensemble as a service to make predictions for test data followed by appropriate sanitization. Suppose we use majority voting to provide a prediction for a test sample. A differentially private mechanism such as Report Noisy Max (Dwork & Roth, 2013) can be used to sanitize the votes for a single query. However, answering several queries requires perturbing all answers with noise linearly proportional to the number of queries, which is impractical in most realistic settings."}, {"heading": "3.3. Leveraging auxiliary data", "text": "To address the problems above, we propose to transfer the knowledge of the ensemble to a global classifier using auxiliary unlabeled data. More precisely, we use the ensemble to generate (pseudo)labels for the auxiliary data, which in turn are used to train a global classifier. Compared to directly releasing local classifiers, releasing a global classifier trained on auxiliary data is a much less sensitive operation with O(M\u22121) (Sec. 4.4) analogous to releasing an average statistic. The number of auxiliary samples does not affect privacy, and in fact the larger the data the closer the global classifier is to the original ensemble with O(N\u22121) bound (Sec. 4.4). Also, compared to using the ensemble to an-\nswer prediction queries, the sanitized global classifier can be used as many times as needed without its privacy being affected.\nWe argue that the availability of auxiliary unlabeled data is not an issue, since in many settings they are practically much easier to collect than labeled data. Furthermore, if the auxiliary data are obtained from public repositories, privacy of such data is not an immediate concern. We mainly focus on the privacy of local data, and discuss extensions for preserving the privacy of auxiliary data in Sec. 4.5."}, {"heading": "4. Finding a global private classifier", "text": "We present details of training a global private classifier. As the first attempt, we use majority voting of an ensemble to assign labels to auxiliary data, and find a global classifier from the usual ERM procedure. In the second attempt, we present a better approach where we use the ensemble to estimate the posterior P (y|x) of the auxiliary samples and solve a \u2018soft-labeled\u2019 weighted empirical minimization."}, {"heading": "4.1. First attempt: ERM with majority voting", "text": "As the first attempt, we use majority voting ofM local classifiers to generate labels of auxiliary data, and analyze its implications. Majority voting for binary classification is the following rule\nv(x) =\n{ 1, if \u2211M i=1 I[hi(x) = 1] \u2265 M 2\n\u22121, otherwise . (6)\nTies can be ignored by assuming an odd number M of parties. Regardless of local classifier types or how they are trained, we can consider the majority vote of the ensemble {h1, ..., hM} as a deterministic target concept to train a global classifier.\nThe majority-voted auxiliary data are\nS = {(x1, v(x1)), ..., (xN , v(xN ))}, (7)\nwhere xi \u2208 X is an i.i.d. sample from the same distribution P (x) as the private data. We train a global classifier by minimizing the (regularized) empirical risk associated with a loss and a hypothesis class:\nR\u03bbS(w) = 1\nN \u2211 (x,v)\u2208S l(h(x;w), v) + \u03bb 2 \u2016w\u20162. (8)\nThe corresponding expected risks with and without regularization are\nR\u03bb(w) = Ex[l(h(x;w), v(x))] + \u03bb\n2 \u2016w\u20162, (9)\nand R(w) = Ex[l(h(x;w), v(x))]. (10)\nAlgorithm 1 DP Ensemble by Majority-voted ERM Input: h1, ..., hM (local classifiers), X (auxiliary unlabeled samples), , \u03bb Output: wp Begin\nfor i = 1, ... , N do Generate majority voted labels v(xi) by (6) end for Find the minimizer ws of (8) with S = {(xi, v(xi))} Sample a random vector \u03b7 from p(\u03b7) \u221d e\u22120.5\u03bb \u2016\u03b7\u2016 Output wp = ws + \u03b7\nAlgorithm 1 summarizes the procedure.\nApplying output perturbation to our multiparty setting gives us the following result.\nTheorem 1. The perturbed output wp = ws + \u03b7 from Algorithm 1 with p(\u03b7) \u221d e\u2212\u03bb 2 \u2016\u03b7\u2016 is -differentially private.\nThe proof of the theorem and others are in the Appendix A."}, {"heading": "4.2. Performance issues of majority voting", "text": "We briefly discuss the generalization error of majorityvoted ERM. In (Chaudhuri et al., 2011), it is shown that the expected risk of an output-perturbed ERM solution wp with respect to the risk of any reference hypothesis w0 is bounded by two terms \u2013 one due to noise and another due to the gap between expected and empirical regularized risks. This result is applicable to the majority-voted ERM with minor modifications. The sensitivity of majority-voted ERM from Theorem 1 is 2\u03bb compared to 2 N\u03bb of a standard ERM, and corresponding the error bound is\nR(wp) \u2264 R(w0) +O( \u22122) +O(N\u22121), (11)\nwith high probability, ignoring other variables. Unfortunately, the bound does not guarantee a successful learning due to the constant gap O( \u22122), which can be large for a small .\nWhat causes this is the worst-case scenario of multiparty voting. Suppose the votes of M \u2212 1 local classifiers are exactly ties for all auxiliary samples {x1, ..., xN}. If we replace a local classifier hi(x) with the \u2018opposite\u2019 classifier h\u2032i(x) = \u2212hi(x), then the majority-voted labels {v1, ..., vN} become {\u2212v1, ...,\u2212vN}, and the resultant global classifier is entirely different. However unlikely this scenario may be in reality, differential privacy requires that we calibrate our noise to the worst case sensitivity."}, {"heading": "4.3. Better yet: weighted ERM with soft labels", "text": "The main problem with majority voting was its sensitivity to the decision of a single party. Let \u03b1(x) be the fraction of\npositive votes from M classifiers given a sample x:\n\u03b1(x) = 1\nM M\u2211 j=1 I[hj(x) = 1]. (12)\nIn terms of \u03b1, the original loss l(wTxv(x)) for majority voting can be written as\nl(ywTx) = I[\u03b1(x) \u2265 0.5] l(wTx) + I[\u03b1(x) < 0.5] l(\u2212wTx), (13)\nwhich changes abruptly when the fraction \u03b1(x) crosses the boundary \u03b1 = 0.5. We remedy the situation by introducing the new weighted loss:\nl\u03b1(\u00b7) = \u03b1(x)l(wTx) + (1\u2212 \u03b1(x))l(\u2212wTx). (14)\nThe new loss has the following properties. When the M votes given a sample x are unanimously positive (or negative), then the weighted loss is l\u03b1(\u00b7) = l(wTx) (or l(\u2212wTx)), same as the original loss. If the votes are almost evenly split between positive and negative, then the weighted loss is l\u03b1(\u00b7) ' 0.5 l(wTx)+0.5 l(\u2212wTx) which is insensitive to the change of label by a single vote, unlike the original loss. Specifically, a single vote can change l\u03b1(\u00b7) only by a factor of 1/M (see Proof of Theorem 3.)\nWe provide a natural interpretation of \u03b1(x) and the weighted loss in the following. For the purpose of analysis, assume that the local classifiers h1(x), ..., hM (x) are from the same hypothesis class.2 Since the local training data are i.i.d. samples from P (x, y), the local classifiers {h1(x), ..., hM (x)} can be considered random hypotheses, as in (Breiman, 1996a). Let Q(j|x) be the probability of such a random hypothesis h(x) predicting label j given x:\nQ(j|x) = P (h(x) = j|x), (15)\nThen the fraction \u03b1(x) = 1M \u2211M j=1 I[hj(x) = 1] is an unbiased estimate of Q(1|x). Furthermore, the weighted loss is directly related to the unweighted loss:\nLemma 2. For any w, the expectation of the weighted loss (14) is asymptotically the expectation of the unweighted loss:\nlim M\u2192\u221e\nEx[l \u03b1(w)] = Ex,v[l(w Txv)]. (16)\nProof. The expected risk Ex,v[l(vwTx)] is\n= ExEv|x[l(vw Tx)]\n= Ex[Q(1|x)l(wTx) +Q(\u22121|x)l(\u2212wTx)] = Ex[ lim\nM\u2192\u221e \u03b1(x)l(wTx) + (1\u2212 lim M\u2192\u221e \u03b1(x))l(\u2212wTx)]\n2Our differential privacy guarantee holds whether they are from the same hypothesis class or not.\nAlgorithm 2 DP Ensemble by Weighted ERM Input: h1, ..., hM (local classifiers), X (auxiliary unlabeled samples), , \u03bb Output: wp Begin\nfor i = 1, ... , N do Compute \u03b1(xi) by (12) end for Find the minimizer of ws of (19) with {(xi, \u03b1(xi))} Sample a random vector \u03b7 from p(\u03b7) \u221d e\u22120.5M\u03bb \u2016\u03b7\u2016 Output wp = ws + \u03b7\n(the law of large numbers)\n= lim M\u2192\u221e\nEx[\u03b1(x)l(w Tx) + (1\u2212 \u03b1(x))l(\u2212wTx)]\n(bounded \u03b1 and l for \u2200x \u2208 X ) = lim\nM\u2192\u221e Ex[l\n\u03b1(w)]. (17)\nThis shows that minimizing the expected weighted loss is asymptotically the same as minimizing the standard expected loss, when the target v is a probabilistic concept from P (h(x) = v) of the random hypothesis, as opposed to a deterministic concept v(x) from majority voting.\nThe auxiliary dataset with \u2018soft\u2019 labels is now\nS = {(x1, \u03b1(x1)), ... , (xN , \u03b1(xN ))}. (18)\nwhere xi \u2208 X is an i.i.d. sample from the same distribution P (x) as the private data, and 0 \u2264 \u03b1 \u2264 1. Note that we are not trying to learn a regression function X \u2192 [0, 1] but to learn a classifier X \u2192 {\u22121, 1} using \u03b1 as a real-valued oracle on P (y = 1|x). Consequently, we find a global classifier by minimizing the regularized weighted empirical risk\nR\u03bbS(w) = 1\nN N\u2211 i=1 l\u03b1(h(xi;w), \u03b1i) + \u03bb 2 \u2016w\u20162, (19)\nwhere \u03b1i = \u03b1(xi). The corresponding expected risks with and without regularization are\nR\u03bb(w) = Ex[l \u03b1(h(x;w), \u03b1(x))] +\n\u03bb 2 \u2016w\u20162, (20)\nand R(w) = Ex[l \u03b1(h(x;w), \u03b1(x))]. (21)\nWe again use output perturbation to make the classifier differentially private as summarized in Algorithm 2."}, {"heading": "4.4. Privacy and performance", "text": "Compared to Theorem 1 for majority-voted ERM with a noise of P (\u03b7) \u221d e\u2212\u03bb 2 \u2016\u03b7\u2016, we have the following result: Theorem 3. The perturbed output wp = ws + \u03b7 from Algorithm 2 with p(\u03b7) \u221d e\u2212M\u03bb 2 \u2016\u03b7\u2016 is -differentially private.\nThat is, we now require 1/M times smaller noise to achieve the same -differential privacy. This directly impacts the performance of the corresponding global classifier as follows.\nTheorem 4. Letw0 be any reference hypothesis. Then with probability of at least 1\u2212 \u03b4p \u2212 \u03b4s over the privacy mechanism (\u03b4p) and over the choice of samples (\u03b4s),\nR(wp) \u2264 R(w0) + 4d2(c+ \u03bb) log2(d/\u03b4p)\n\u03bb2M2 2\n+ 16(32 + log(1/\u03b4s)) \u03bbN + \u03bb 2 \u2016w0\u20162. (22)\nThe generalization error bound above has the O(M\u22122 \u22122) term compared to theO( \u22122) term for majority-voted ERM (11). This implies that by choosing a largeM , Algorithm 2 can find a solution whose expected risk is close to the minimum of a non-private solution for any fixed privacy level > 0.\nWe remind the user that the results should be interpreted with a caution. The bounds in (11) and (22) indicate the goodness of private ERM solutions relative to the best nonprivate solutions with deterministic and probability concepts which are not the same task. Also, they do not indicate the goodness of the ensemble approach itself relative to a centrally-trained classifier using all private data without privacy consideration. We leave this comparison to empirical evaluation in the experiment section."}, {"heading": "4.5. Extensions", "text": "We discuss extensions of Algorithms 1 and 2 to provide additional privacy for auxiliary data. More precisely, those algorithms can be made -differentially private for all private data of a single party and a single sample in the auxiliary data, by increasing the amount of perturbation as necessary. We outline the proof as follows. In the previous sections, a global classifier was trained on auxiliary data whose labels were generated either by majority voting or soft labeling. A change in the local classifier affects only the labels {vi} of the auxiliary data but not the features {xi}. Now assume in addition that the feature of one sample from the auxiliary data can also change arbitrarily, i.e., xj 6= x\u2032j for some j and xi = x\u2032i for all i \u2208 {1, ..., N} \\ {j}. The sensitivity of the resultant risk minimizer can be computed similarly to the proofs of Theorems 1 and 3 in Appendix A. Briefly, the sensitivity is upper-bounded by the absolute sum of the\ndifference of gradients\n\u2016\u2207g(w)\u2016 \u2264 1 N N\u2211 i=1 \u2016\u2207l(yiwTxi)\u2212\u2207l(y\u2032iwTx\u2032i)\u2016. (23)\nFor majority voting, one term in the sum (23) is\n\u2016v(xj)xj l\u2032(v(xj)wTxj)\u2212 v\u2032(x\u2032j)x\u2032j l\u2032(v\u2032(x\u2032j)wTx\u2032j)\u2016 (24) which is at most 2 for any xj , x\u2032j \u2208 X , and therefore the sensitivity is the same whether xj = x\u2032j or not. As a result, Algorithm 1 is already -differentially private for both labeled and auxiliary data without modification. Furthermore, the privacy guarantee remains the same if we allow xj 6= x\u2032j for any number of samples. For soft labeling, one term in the sum (23) is\n\u2016\u03b1jxj l\u2032(wTxj)\u2212 (1\u2212 \u03b1j)xj l\u2032(\u2212wTxj) \u2212\u03b1\u2032jx\u2032j l\u2032(wTx\u2032j) + (1\u2212 \u03b1\u2032j)x\u2032j l\u2032(\u2212wTx\u2032j)\u2016 (25)\nwhich is also at most 2 for any xj , x\u2032j \u2208 X and 2M when xj = x \u2032 j . When only a single auxiliary sample changes, i.e., xj 6= x\u2032j for one j, the overall sensitivity increases by a factor of N+M\u22121N . By increasing the noise by this factor, Algorithm 2 is -differentially private for both labeled and auxiliary data. Note that this factor N+M\u22121N can be bounded close to 1 if we increase the number of auxiliary samples N relative to the number of parties M ."}, {"heading": "5. Related work", "text": "To preserve privacy in data publishing, several approaches such as k-anonymity (Sweeney, 2002) and secure multiparty computation (Yao, 1982) have been proposed (see (Fung et al., 2010) for a review.) Recently, differential privacy (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al., 2008), and gained popularity as a quantifiable measure of privacy risk. The measure provides a bound on the privacy loss regardless of any additional information an adversary might have. Differential privacy has been used for a privacy-preserving data analysis platform (McSherry, 2009), and for sanitization of learned model parameters from a standard ERM (Chaudhuri et al., 2011). This paper adopts output perturbation techniques from the latter to sanitize non-standard ERM solutions from multiparty settings.\nPrivate learning from multiparty data has been studied previously. In particular, several differentially-private algorithms were proposed, including parameter averaging through secure multiparty computation (Pathak et al., 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al., 2015). Our paper is motivated by\n(Pathak et al., 2010) but uses a very different approach to aggregate local classifiers. In particular, we use an ensemble approach and average the classifier decisions (Breiman, 1996a) instead of parameters, which makes our approach applicable to arbitrary and mixed classifier types. Advantages of ensemble approaches in general have been analyzed previously, in terms of bias-variance decomposition (Breiman, 1996b), and in terms of the margin of training samples (Schapire et al., 1998).\nFurthermore, we are using unlabeled data to augment labeled data during training, which can be considered a semisupervised learning method (Chapelle et al., 2006). There are several related papers in this direction. Augmenting private data with non-private labeled data to lower the sensitivity of the output is straightforward, and was demonstrated in medical applications (Ji et al., 2014). Using nonprivate unlabeled data, which is more general than using labeled data, was demonstrated specifically to assist learning of random forests (Jagannathan et al., 2013). Our use of auxiliary data is not specific to classifier types. Furthermore, we present an extension to preserve the privacy of auxiliary data as well."}, {"heading": "6. Experiments", "text": "We use three real-world datasets to compare the performance of the following algorithms:\n\u2022 batch: classifier trained using all data ignoring privacy \u2022 soft: private ensemble using soft-labels (Algorithm 2) \u2022 avg: parameter averaging (Pathak et al., 2010) \u2022 vote: private ensemble using majority voting (Algo-\nrithm 1) \u2022 indiv: individually trained classifier using local data\nWe can expect batch to perform better than any private algorithm since it uses all private data for training ignoring privacy. In contrast, indiv uses only the local data for training and will perform significantly worse than batch, but it achieves a perfect privacy as long as the trained classifiers are kept local to the parties. We are interested in the range of where private algorithms (soft, avg, and vote) perform better than the baseline indiv.\nTo compare all algorithms fairly, we use only a single type of classier \u2013 binary or multiclass logistic regression. For Algorithms 1 and 2, both local and global classifiers are of this type as well. The only hyperparameter of the model is the regularization coefficient \u03bb which we fixed to 10\u22124 after performing some preliminary experiments.About 10% of the original training data are used as auxiliary unlabeled data, and the rest 90% are randomly distributed to M parties as private data. We report the mean and s.d. over 10\ntrials for non-private algorithms and 100-trials for private algorithms."}, {"heading": "6.1. Activity recognition using accelerometer", "text": "Consider a scenario where wearable device users want to train a motion-based activity classifier without revealing her data to others. To test the algorithms, we use the UCI Human Activity Recognition Dataset (Anguita et al., 2012), which is a collection of motion sensor data on a smart device by multiple subjects performing 6 activities (walking, walking upstairs, walking downstairs, sitting, standing, laying). Various time and frequency domain variables are extracted from the signal, and we apply PCA to get d = 50 dimensional features. The training and testing samples are 7K and 3K, respectively.\nWe simulate a case with M = 1K users (i.e., parties). Each user can use only 6 samples to train a local classifier. The remaining 1K samples are used as auxiliary unlabeled data. Figure 2 shows the test accuracy of using different algorithms with varying privacy levels. For nonprivate algorithms, the top solid line (batch) shows the accuracy of a batch-trained classifier at around 0.90, and the bottom dashed line (indiv) shows the averaged accuracy of local classifiers at around 0.47. At 1/ = 0, the private algorithms achieve test accuracy of 0.79 (vote), 0.76 (soft) and 0.67 (avg), and as the the privacy level 1/ increases, the performance drops for all private algorithms. As expected from the bound (11), vote becomes useless even at 1/ = 0.1, while soft and avg are better than indiv until 1/ = 1. We fixed M to 1000 in this experiment due to the limited number of samples, the tendency in the graph is similar to other experiments with larger M \u2019s."}, {"heading": "6.2. Network intrusion detection", "text": "Consider a scenario where multiple gateways or routers collect suspicious network activities independently, and aim to collaboratively build an accurate network intrusion detector without revealing local traffic data. For this task we use the KDD-99 dataset, which consists of examples of \u2018bad\u2019 connections, called intrusions or attacks, and \u2018good\u2019 normal connections. Features of this dataset consists of continuous values and categorical attributes. To apply logistic regression, we change categorical attributes to onehot vectors to get d = 123 dimensional features. The training and testing samples are 493K and 311K, respectively.\nWe simulate cases withM = 5K/10K/20K parties. Each party can only use 22 samples to train a local classifier. The remaining 43K samples are used as auxiliary unlabeled data. Figure 3 shows the test accuracy of using different algorithms with varying privacy levels. For each of M = 5K/10K/20K the tendency of algorithms is similar to Figure 2 \u2013 for a small 1/ , private algorithms perform roughly in between batch and indiv, and as 1/ increases private algorithms start to perform worse than indiv. When M is large (e.g., M = 20K), private algorithms soft and avg hold their accuracy better than when M is small (e.g., M = 5K.) In particular, soft performs nearly as well as the\nnon-private batch until around 1/ = 10 compared to avg ."}, {"heading": "6.3. Malicious URL prediction", "text": "In addition to network intrusion detection, multiple parties such as computer users can collaborate on detecting malicious URLs without revealing the visited URLs. The Malicious URL Dataset (Ma et al., 2009) is a collection of examples of malicious URLs from a large Web mail provider. The task is to predict whether a URL is malicious or not by various lexical and host-based features of the URL. We apply PCA to get d = 50 dimensional feature vectors. We choose days 0 to 9 for training, and days 10 to 19 for testing, which amount to 200K samples for training and 200K samples for testing.\nWe simulate cases withM = 5K/10K/20K parties. Each party can use only 9 samples to train a local classifier. The remaining 16K samples are used as auxiliary unlabeled data. Figure 4 shows the test accuracy of using different algorithms with varying privacy levels. The gap between batch and other algorithms is larger compared to the previous experiment (Figure 3), most likely due to the smaller number (=9) of samples per party. However, the overall tendency is very similar to previous experiments."}, {"heading": "7. Conclusion", "text": "In this paper, we propose a method of building global differentially private classifiers from local classifiers using two new ideas: 1) leveraging unlabeled auxiliary data to transfer the knowledge of the ensemble, and 2) solving a weighted ERM using class probability estimates from the ensemble. In general, privacy comes with a loss of classification performance. We present a solution to minimize the performance gap between private and non-private ensembles demonstrated with real world tasks."}, {"heading": "A. Proofs", "text": "A.1. Proof of Theorem 1\nTheorem 1: The perturbed output wp = ws + \u03b7 from Algorithm 1 with p(\u03b7) \u221d e\u2212\u03bb 2 \u2016\u03b7\u2016 is -differentially private.\nProof. We will compute the sensitivity of the minimizer ws of the regularized empirical risk with majority-voted labels (8). Suppose D = (S(1), ..., S(M)) is the ordered set of private training data (5) for M parties, and D\u2032 = ((S\u2032)(1), ..., S(M)) is a neighboring set which differs from D only at party 1\u2019s data, without loss of generality. The local classifiers after training with D and D\u2032 are\nH = (h1, ..., hM ) and H \u2032 = (h\u20321, ..., hM ), respectively, which are again different only for classifier 1. The majority votes v(x) and v\u2032(x) from D and D\u2032 generates two auxiliary training sets S = {(xi, v(xi))} and S\u2032 = {(xi, v\u2032(xi)} which have the same features but possibly different labels.\nLet R\u03bbS(w) and R \u03bb S\u2032(w) be the regularized empirical risks for training sets S and S\u2032, and let ws and ws\u2032 be the minimizers of the respective risks. From Corollaries 7 and 8 (Chaudhuri et al., 2011), the L2 difference of ws and ws\u2032 is bounded by\n\u2016ws \u2212 ws\u2032\u2016 \u2264 1\n\u03bb max w \u2016\u2207g(w)\u2016, (26)\nwhere g(w) is the risk difference R\u03bbS(w)\u2212R\u03bbS\u2032(w), which, in our case, satisfies\n\u2016\u2207g(w)\u2016 \u2264 1 N N\u2211 i=1 \u2016v(xi)xil\u2032(v(xi)wTxi)\n\u2212v\u2032(xi)xil\u2032(v\u2032(xi)wTxi)\u2016.\n\u2264 1 N N\u2211 i=1 \u2016xi\u2016 \u00d7\n|l\u2032(wTxi) + l\u2032(\u2212wTxi)|. (27)\nRecall that \u2016x\u2016 \u2264 1 and |l\u2032(\u00b7)| \u2264 1 by assumption. In the worst case, v(xi) 6= v\u2032(xi) for all i = 1, ..., N , and therefore the RHS of (27) is bounded by 2. Consequently, the L2 sensitivity of the minimizer ws is\nmax S,S\u2032 \u2016ws \u2212 ws\u2032\u2016 \u2264\n2 \u03bb . (28)\n-differential privacy follows from the sensitivity result (3).\nA.2. Proof of Theorem 3\nTheorem 3: The perturbed output wp = ws + \u03b7 from Algorithm 2 with p(\u03b7) \u221d e\u2212M\u03bb 2 \u2016\u03b7\u2016 is -differentially private.\nProof. The proof parallels the proof of Theorem 1. We again assume D = (S(1), ..., S(M)) is the ordered set of private training data (5) for M parties, and D\u2032 = ((S\u2032)(1), ..., S(M)) is a neighboring set which differs from D only at party 1\u2019s data, without loss of generality. Let S = {(xi, \u03b1i)} and S\u2032 = {(xi, \u03b1\u2032i)} be the two resulting datasets which have the same the features but possibly different \u03b1\u2019s. We first compute the sensitivity of the minimizer of the weighted regularized empirical risk (19). Let R\u03bbS(w) and R \u03bb S\u2032(w) be the regularized empirical risks for training sets S and S\u2032, and let ws and ws\u2032 be the minimizers of the respective risks. Also let g(w) be the difference\nR\u03bbS(w)\u2212R\u03bbS\u2032(w) of two risks\ng(w) = 1\nN N\u2211 i=1 [\u03b1il(w Txi) + (1\u2212 \u03b1i)l(\u2212wTxi) \u2212\u03b1\u2032il(wTxi)\u2212 (1\u2212 \u03b1\u2032i)l(\u2212wTxi)]. (29)\nThe gradient of g(w) is bounded by\n\u2016\u2207g(w)\u2016 \u2264 1 N N\u2211 i=1 [|\u03b1i \u2212 \u03b1\u2032i|\u2016xi\u2016|l\u2032(wTxi)|\n+|\u03b1i \u2212 \u03b1\u2032i|\u2016xi\u2016|l\u2032(\u2212wTxi)|] (30)\n\u2264 1 N N\u2211 i=1 2|\u03b1i \u2212 \u03b1\u2032i|. (31)\nIn the worst case, \u03b1i 6= \u03b1\u2032i for all i = 1, ..., N . Since \u03b1i is the fraction of positive votes, |\u03b1i \u2212 \u03b1\u2032i| \u2264 1/M holds for all i = 1, ..., N . Therefore the L2 sensitivity of the minimizer ws is at most 2\u03bbM and the -differential privacy follows.\nA.3. Lemma 5\nWe use the following lemma.\nLemma 5 (Lemma 17 of (Chaudhuri et al., 2011)). If X \u223c \u0393(k, \u03b8), where k is an integer, then with probability of at least 1\u2212 \u03b4,\nX \u2264 k\u03b8 log(k/\u03b4).\nA.4. Lemma 6\nLemma 6. If ws is the minimizer of (19) and wp is the - differentially private version from Algorithm 2, then with probability of at least 1\u2212 \u03b4p over the privacy mechanism,\nR\u03bbS(wp) \u2264 R\u03bbS(ws) + 2d2(c+ \u03bb) log2(d/\u03b4)\n\u03bb2M2 2 (32)\nProof. A differentiable function f : Rd \u2192 R is called \u03b2smooth, if \u2203\u03b2 > 0 such that \u2016\u2207f(v)\u2212\u2207f(u)\u2016 \u2264 \u03b2\u2016v\u2212u\u2016 for all u, v. From the Mean Value Theorem, such a function satisfies\nf(v) \u2264 f(u) +\u2207T f(u)(v \u2212 u) + \u03b2 2 \u2016v \u2212 u\u20162, \u2200u, v.\nSince |l\u2032(\u00b7)| is c-Lipschitz, R\u03bbS(w) is (c+ \u03bb)-smooth:\n\u2016\u2207R\u03bbS(v)\u2212\u2207R\u03bbS(u)\u2016 \u2264 1 N \u2211 i \u2225\u2225\u03b1ixil\u2032(vTxi)\u2212 (1\u2212 \u03b1i)xil\u2032(\u2212vTxi) \u2212\u03b1ixil\u2032(uTxi) + (1\u2212 \u03b1i)xil\u2032(\u2212uTxi)\n\u2225\u2225 +\u03bb\u2016v \u2212 u\u2016\n\u2264 1 N \u2211 i [ \u03b1ic\u2016(v \u2212 u)Txi\u2016+\n(1\u2212 \u03b1i)c\u2016(u\u2212 v)Txi\u2016 ]\n+ \u03bb\u2016v \u2212 u\u2016 \u2264 (c+ \u03bb)\u2016u\u2212 v\u2016. (33)\nBy setting v = wp and u = ws and using the (c + \u03bb)smoothness of R\u03bbS(w), we have\nR\u03bbS(wp) \u2264 R\u03bbS(ws) +\u2207TR\u03bbS(ws)(wp \u2212 ws)\n+ (c+ \u03bb)\n2 \u2016wp \u2212 w\u2217s\u20162\n= R\u03bbS(ws) + (c+ \u03bb)\n2 \u2016wp \u2212 ws\u20162. (34)\nSince\nP ( \u2016wp \u2212 w\u2217s\u2016 \u2264 2d log(d/\u03b4)\n\u03bbM\n) \u2265 1\u2212 \u03b4p (35)\nfrom Lemma 5 with k = d and \u03b8 = 2\u03bbM , we have the desired result.\nA.5. Proof of Theorem 4\nTheorem 4: Let w0 be any reference hypothesis. Then with probability of at least 1\u2212 \u03b4p \u2212 \u03b4s over the privacy mechanism (\u03b4p) and over the choice of samples (\u03b4s),\nR(wp) \u2264 R(w0) + 4d2(c+ \u03bb) log2(d/\u03b4p)\n\u03bb2M2 2\n+ 16(32 + log(1/\u03b4s)) \u03bbN + \u03bb 2 \u2016w0\u20162. (36)\nProof. Let ws and w\u2217 be the minimizers of the regularized empirical risk (19) and the regularized expected risk (20), respectively. The risk at wp relative to a reference classifier w0 can be written as\nR(wp)\u2212R(w0) = R\u03bb(wp)\u2212R\u03bb(w\u2217) +R\u03bb(w\u2217)\u2212R\u03bb(w0)\n+ \u03bb\n2 \u2016w0\u20162 \u2212\n\u03bb 2 \u2016wp\u20162\n\u2264 R\u03bb(wp)\u2212R\u03bb(w\u2217) + \u03bb\n2 \u2016w0\u20162.\n(37)\nThe inequality above follows from R\u03bb(w\u2217) \u2264 R\u03bb(w0) by definition. Note that since \u2016x\u2016 \u2264 1 and |l\u2032| \u2264 1 by assumption, the weighted loss \u03b1(x)l(wTx) + (1 \u2212 \u03b1(x))l(wTx) is 1-Lipschitz in w. From Theorem 1 of (Sridharan et al., 2009) with a = 1, we can also bound R\u03bb(wp) \u2212 R\u03bb(w\u2217) as\nR\u03bb(wp)\u2212R\u03bb(w\u2217) \u2264 2(R\u03bbS(wp)\u2212R\u03bbS(w\u2217s))\n+ 16(32 + log(1/\u03b4s))\n\u03bbN (38)\nwith probability of 1 \u2212 \u03b4s over the choice of samples. By combining this inequality with Lemma 6 using the union bound, we have\nR\u03bb(wp)\u2212R\u03bb(w\u2217) \u2264 4d2(c+ \u03bb) log2(d/\u03b4p)\n\u03bb2M2 2\n+ 16(32 + log(1/\u03b4s))\n\u03bbN . (39)\nThe theorem follows from (37)."}, {"heading": "B. Differentially private multiclass logistic regression", "text": "We extend our methods to multiclass classification problems and provide a sketch of -differential privacy proofs for multiclass logistic regression loss.\nB.1. Standard ERM\nSuppose y \u2208 1, ...,K, and let w = [w1; ... ;wK ] be a stacked (d K)\u00d7 1 vector. The multiclass logistic loss (i.e. softmax) is\nl(h(x), y) = \u2212wTy x+ log( \u2211 l ew T l x), (40)\nand the regularized empirical risk is\nR\u03bbS(w) = \u2212 1\nN \u2211 i [wTyixi \u2212 log( \u2211 l ew T l xi)] + \u03bb 2 \u2016w\u20162.\n(41) Note that R\u03bbS(w) is \u03bb-strongly convex in w.\nThe sensitivity of ws which minimizes (41) can be computed as follows. Suppose S and S\u2032 are two different datasets which are not necessarily neighbors: S = {(xi, yi))} and S\u2032 = {(x\u2032i, y\u2032i)}. Let g(w) be the difference R\u03bbS(w) \u2212 R\u03bbS\u2032(w) of the two risks. Then the partial gradient w.r.t. wk is\n\u2207wkR\u03bbS(w) = \u2212 1\nN \u2211 i xi\u2206k(xi, yi, w) + \u03bbwk, (42)\nwhere\n\u2206k(xi, yi, w) = I[yi = k]\u2212 ew T k xi\u2211\nl e wTl xi\n= I[yi = k]\u2212Pk(xi).\n(43) Since I[yi = k] can be non-zero (i.e. 1) for only one k, and\u2211 k Pk(xi) = 1 with 0 \u2264 Pk(xi) \u2264 1, we have\u2211 k \u22062k = \u2211 k (Ik \u2212 Pk)2 \u2264 \u2211 k (I2k + P 2 k ) \u2264 2, (44)\nLet \u2206(xi, yi, w) = [\u22061(xi, yi, w), ...,\u2206K(xi, yi, w)] be a K\u00d71 vector (which depends on xi, yi, w.) The gradient of the risk difference g(w) is then\n\u2207g(w) = \u2212 1 N \u2211 i \u2206(xi, yi, w)\u2297 xi\u2212\u2206(x\u2032i, y\u2032i, w)\u2297 x\u2032i, (45) where \u2297 is a Kronecker product of two vectors. Note that\n\u2016\u2206\u2297x\u20162 = \u2211 k \u2016\u2206kx\u20162 \u2264 \u2016x\u20162 \u2211 k \u22062k \u2264 2\u2016x\u20162. (46)\nWithout loss of generality, we assume that only (x1, y1) and (x\u20321, y \u2032 1) are possibly different and (xi, yi) = (x \u2032 i, y \u2032 i) for all i = 2, ..., N . In this case we have\n\u2016\u2207g(w)\u2016 \u2264 1 N \u2016\u2206(x1, y1, w)\u2297 x1\u2016\n+ 1\nN \u2016\u2206(x\u20321, y\u20321, w)\u2297 x\u20321\u2016\n\u2264 \u221a 2\nN (\u2016x1\u2016+ \u2016x\u20321\u2016) \u2264\n2 \u221a 2\nN , (47)\nand the therefore the L2 sensitive of the minimizer of a multiclass logistic regression is\n2 \u221a 2 N\u03bb (48)\nfrom Corollaries 7 and 8 (Chaudhuri et al., 2011). Note that the sensitivity does not depend on the number of classesK.\nB.2. Majority-voted ERM\nLet S = {(xi, vi)} and S\u2032 = {(xi, v\u2032i)} be two datasets with the same features but with possibly different labels for all i = 1, ..., N . Then the partial gradient of the risk difference g(w) is\n\u2207wkg(w) = \u2212 1\nN \u2211 i xi[I[vi = k]\u2212 I[v\u2032i = k]]\n= \u2212 1 N \u2211 i xiak(vi, v \u2032 i), (49)\nwhere ak(vi, v\u2032i) is\nak(vi, v \u2032 i) = I[vi = k]\u2212 I[v\u2032i = k] \u2208 {\u22121, 0, 1}. (50)\nLet a = [a1, ..., aK ] be a K \u00d7 1 vector (which depends on vi, v\u2032i.) Note that at most two elements of a can be nonzero (i.e. \u00b11.) The gradient can be rewritten using the Kronecker product \u2297 as\n\u2207g(w) = \u2212 1 N \u2211 i a(vi, v \u2032 i)\u2297 xi, (51)\nand its norm is bounded by\n\u2016\u2207g(w)\u2016 \u2264 1 N \u2211 i \u221a 2\u2016xi\u2016 \u2264 \u221a 2. (52)\nTherefore the L2 sensitivity of the minimizer of majoritylabeled multiclass logistic regression is\n\u221a 2\n\u03bb . (53)\nB.3. Weighted ERM\nA natural multiclass extension of the weighted loss (14) is l\u03b1(w) = \u2211 k \u03b1k(x)l(wTk x), (54)\nwhere \u03b1k(x) is the unbiased estimate of the probability P (v = k|x). The corresponding weighted regularized empirical risk is\nR\u03bbS(w) = 1\nN \u2211 i \u2211 k \u03b1k(xi)l(w T k x) + \u03bb 2 \u2016w\u20162\n= 1\nN \u2211 i \u2211 k \u03b1k(xi)[log( \u2211 l ew T l xi)\u2212 wTk xi]\n+ \u03bb\n2 \u2016w\u20162\n= \u2212 1 N \u2211 i [ \u2211 k \u03b1k(xi)w T k xi \u2212 log( \u2211 l ew T l xi)]\n+ \u03bb\n2 \u2016w\u20162, (55)\nand its partial gradient is\n\u2207wkR\u03bbS(w) = \u2212 1\nN \u2211 i xi\n[ \u03b1k(xi)\u2212 ew T k xi\u2211\nl e wTl xi\n] + \u03bbwk.\n(56)\nLet S = {(xi, \u03b1i)} and S\u2032 = {(xi, \u03b1\u2032i)} be two datasets with the same features but with possibly different labels for all i = 1, ..., N . Then the partial gradient of the risk difference g(w) is\n\u2207wkg(w) = \u2212 1\nN \u2211 i xi[\u03b1 k(xi)\u2212 (\u03b1\u2032)k(xi)]\n= \u2212 1 N \u2211 i xibk(\u03b1 k i , (\u03b1 \u2032)ki ), (57)\nwhere bk(\u03b1ki , (\u03b1 \u2032)ki ) = \u03b1 k(xi) \u2212 (\u03b1\u2032)k(xi). Let b = [b1, ..., bK ] be aK\u00d71 vector (which depends \u03b1i, \u03b1\u2032i.) Note that at most two elements of b can be nonzero (i.e.,\u00b11/M .) The gradient can then be rewritten as\n\u2207g(w) = \u2212 1 N \u2211 i b(\u03b1i, \u03b1 \u2032 i)\u2297 xi, (58)\nand its norm is bounded by\n\u2016\u2207g(w)\u2016 \u2264 1 N \u2211 i \u221a 2 M \u2016xi\u2016 \u2264 \u221a 2 M . (59)\nTherefore the L2 sensitivity of the minimizer of the weighted multiclass logistic regression is\n\u221a 2 M\u03bb . (60)\nB.4. Parameter averaging\nFor the purposes of comparison, we also derive the sensitivity of parameter averaging (Pathak et al., 2010) for multiclass logistic regression. Let the two neighboring datasets be W = (w1, w2, ..., wM ) and W \u2032 = (w\u20321, w \u2032 2, ..., w \u2032 M ), which are collections of parameters from M parties. The corresponding averages for the two sets are w\u0304 = 1M \u2211 i wi\nand w\u0304\u2032 = 1M \u2211 i w \u2032 i. Without loss of generality, we assume the parameters w1 and w\u20321 differ only for party 1 and wi = w \u2032 i for others i = 2, ...,M . Since \u2016w\u0304 \u2212 w\u0304\u2032\u2016 =\n1 M \u2016w1\u2212w \u2032 1\u2016, the L2 sensitivity is 1/M times the sensitivity of the minimizer of the minimizer of a single classifier, when all training samples of party 1 are allowed to change. Therefore the L2 sensitivity of the average parameters for multiclass logistic regression is 2 \u221a 2\nM\u03bb ."}], "references": [{"title": "Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine", "author": ["Anguita", "Davide", "Ghio", "Alessandro", "Oneto", "Luca", "Parra", "Xavier", "Reyes-Ortiz", "Jorge L"], "venue": "In International Workshop of Ambient Assisted Living (IWAAL", "citeRegEx": "Anguita et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anguita et al\\.", "year": 2012}, {"title": "Bias, variance, and arcing classifiers", "author": ["Breiman", "Leo"], "venue": "Technical report, Statistics Department,", "citeRegEx": "Breiman and Leo.,? \\Q1996\\E", "shortCiteRegEx": "Breiman and Leo.", "year": 1996}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "Zien", "A. (eds"], "venue": "URL http: //www.kyb.tuebingen.mpg.de/ssl-book", "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Differentially private empirical risk minimization", "author": ["Chaudhuri", "Kamalika", "Monteleoni", "Claire", "Sarwate", "Anand D"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Differential privacy. In Automata, languages and programming", "author": ["Dwork", "Cynthia"], "venue": null, "citeRegEx": "Dwork and Cynthia.,? \\Q2006\\E", "shortCiteRegEx": "Dwork and Cynthia.", "year": 2006}, {"title": "Privacy-preserving datamining on vertically partitioned databases", "author": ["Dwork", "Cynthia", "Nissim", "Kobbi"], "venue": "In Advances in Cryptology\u2013CRYPTO", "citeRegEx": "Dwork et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2004}, {"title": "The algorithmic foundations of differential privacy", "author": ["Dwork", "Cynthia", "Roth", "Aaron"], "venue": "Theoretical Computer Science,", "citeRegEx": "Dwork et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2013}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam"], "venue": "In Theory of cryptography,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Privacypreserving data publishing: A survey of recent developments", "author": ["Fung", "Benjamin", "Wang", "Ke", "Chen", "Rui", "Yu", "Philip S"], "venue": "ACM Comp. Surveys (CSUR),", "citeRegEx": "Fung et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2010}, {"title": "Composition attacks and auxiliary information in data privacy", "author": ["Ganta", "Srivatsava Ranjit", "Kasiviswanathan", "Shiva Prasad", "Smith", "Adam"], "venue": "In Proc. ACM SIGKDD,", "citeRegEx": "Ganta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganta et al\\.", "year": 2008}, {"title": "Crowd-ML: A privacy-preserving learning framework for a crowd of smart devices", "author": ["Hamm", "Jihun", "Champion", "Adam", "Chen", "Guoxing", "Belkin", "Mikhail", "Xuan", "Dong"], "venue": "In Proceedings of the 35th IEEE International Conference on Distributed Computing Systems (ICDCS). IEEE,", "citeRegEx": "Hamm et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hamm et al\\.", "year": 2015}, {"title": "A semi-supervised learning approach to differential privacy", "author": ["Jagannathan", "Geetha", "Monteleoni", "Claire", "Pillaipakkamnatt", "Krishnan"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "Jagannathan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jagannathan et al\\.", "year": 2013}, {"title": "Differentially private distributed logistic regression using private and public data", "author": ["Ji", "Zhanglong", "Jiang", "Xiaoqian", "Wang", "Shuang", "Xiong", "Li", "Ohno-Machado", "Lucila"], "venue": "BMC medical genomics,", "citeRegEx": "Ji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "Identifying suspicious urls: an application of largescale online learning", "author": ["Ma", "Justin", "Saul", "Lawrence K", "Savage", "Stefan", "Voelker", "Geoffrey M"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Ma et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2009}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["McSherry", "Frank D"], "venue": "In Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,", "citeRegEx": "McSherry and D.,? \\Q2009\\E", "shortCiteRegEx": "McSherry and D.", "year": 2009}, {"title": "Multiparty differential privacy via aggregation of locally trained classifiers", "author": ["Pathak", "Manas", "Rane", "Shantanu", "Raj", "Bhiksha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Pathak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2010}, {"title": "A differentially private stochastic gradient descent algorithm for multiparty classification", "author": ["Rajkumar", "Arun", "Agarwal", "Shivani"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rajkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rajkumar et al\\.", "year": 2012}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["Schapire", "Robert E", "Freund", "Yoav", "Bartlett", "Peter", "Lee", "Wee Sun"], "venue": "Annals of statistics,", "citeRegEx": "Schapire et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1998}, {"title": "Fast rates for regularized objectives", "author": ["Sridharan", "Karthik", "Shalev-Shwartz", "Shai", "Srebro", "Nathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}, {"title": "k-anonymity: A model for protecting privacy", "author": ["Sweeney", "Latanya"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Sweeney and Latanya.,? \\Q2002\\E", "shortCiteRegEx": "Sweeney and Latanya.", "year": 2002}, {"title": "Protocols for secure computations", "author": ["Yao", "Andrew C"], "venue": "IEEE Symp. Found. Comp. Sci.,", "citeRegEx": "Yao and C.,? \\Q2013\\E", "shortCiteRegEx": "Yao and C.", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "This problem of aggregating classifiers was considered in (Pathak et al., 2010), where the authors proposed averaging of the parameters of local classifiers to get a global classifier.", "startOffset": 58, "endOffset": 79}, {"referenceID": 7, "context": "It provides a strict upper bound on the privacy loss against any adversary (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006).", "startOffset": 75, "endOffset": 130}, {"referenceID": 3, "context": "an empirical risk minimizer, and release a differentially private classifier using output perturbation (Chaudhuri et al., 2011).", "startOffset": 103, "endOffset": 127}, {"referenceID": 7, "context": "When an algorithm outputs a real-valued vector f \u2208 R, its global L2 sensitivity (Dwork et al., 2006) can be defined as S(f) = max D\u223cD\u2032 \u2016f(D)\u2212 f(D\u2032)\u2016 (2)", "startOffset": 80, "endOffset": 100}, {"referenceID": 7, "context": "An important result from (Dwork et al., 2006) is that a vector-valued output f with sensitivity S(f) can be made -differentially private by perturbing f with an additive noise vector \u03b7 whose density is", "startOffset": 25, "endOffset": 45}, {"referenceID": 3, "context": "Such a classifier can be sanitized by perturbation with additive noise calibrated to the sensitivity of the classifier, known as output perturbation method (Chaudhuri et al., 2011).", "startOffset": 156, "endOffset": 180}, {"referenceID": 15, "context": "Output perturbation was used to sanitize the averaged parameters in (Pathak et al., 2010).", "startOffset": 68, "endOffset": 89}, {"referenceID": 3, "context": "We will assume the following conditions for our global classifier1 similar to (Chaudhuri et al., 2011).", "startOffset": 78, "endOffset": 102}, {"referenceID": 12, "context": "Besides, efficient differentially private mechanisms are known only for certain types of classifiers so far (see (Ji et al., 2014) for a review.", "startOffset": 113, "endOffset": 130}, {"referenceID": 3, "context": "In (Chaudhuri et al., 2011), it is shown that the expected risk of an output-perturbed ERM solution wp with respect to the risk of any reference hypothesis w0 is bounded by two terms \u2013 one due to noise and another due to the gap between expected and empirical regularized risks.", "startOffset": 3, "endOffset": 27}, {"referenceID": 8, "context": "To preserve privacy in data publishing, several approaches such as k-anonymity (Sweeney, 2002) and secure multiparty computation (Yao, 1982) have been proposed (see (Fung et al., 2010) for a review.", "startOffset": 165, "endOffset": 184}, {"referenceID": 7, "context": ") Recently, differential privacy (Dwork & Nissim, 2004; Dwork et al., 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al.", "startOffset": 33, "endOffset": 88}, {"referenceID": 9, "context": ", 2006; Dwork, 2006) has addressed several weaknesses of k-anonymity (Ganta et al., 2008), and gained popularity as a quantifiable measure of privacy risk.", "startOffset": 69, "endOffset": 89}, {"referenceID": 3, "context": "Differential privacy has been used for a privacy-preserving data analysis platform (McSherry, 2009), and for sanitization of learned model parameters from a standard ERM (Chaudhuri et al., 2011).", "startOffset": 170, "endOffset": 194}, {"referenceID": 15, "context": "In particular, several differentially-private algorithms were proposed, including parameter averaging through secure multiparty computation (Pathak et al., 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al.", "startOffset": 140, "endOffset": 161}, {"referenceID": 10, "context": ", 2010), and private exchange of gradient information to minimize empirical risks incrementally (Rajkumar & Agarwal, 2012; Hamm et al., 2015).", "startOffset": 96, "endOffset": 141}, {"referenceID": 15, "context": "(Pathak et al., 2010) but uses a very different approach to aggregate local classifiers.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Advantages of ensemble approaches in general have been analyzed previously, in terms of bias-variance decomposition (Breiman, 1996b), and in terms of the margin of training samples (Schapire et al., 1998).", "startOffset": 181, "endOffset": 204}, {"referenceID": 2, "context": "Furthermore, we are using unlabeled data to augment labeled data during training, which can be considered a semisupervised learning method (Chapelle et al., 2006).", "startOffset": 139, "endOffset": 162}, {"referenceID": 12, "context": "Augmenting private data with non-private labeled data to lower the sensitivity of the output is straightforward, and was demonstrated in medical applications (Ji et al., 2014).", "startOffset": 158, "endOffset": 175}, {"referenceID": 11, "context": "Using nonprivate unlabeled data, which is more general than using labeled data, was demonstrated specifically to assist learning of random forests (Jagannathan et al., 2013).", "startOffset": 147, "endOffset": 173}, {"referenceID": 15, "context": "\u2022 batch: classifier trained using all data ignoring privacy \u2022 soft: private ensemble using soft-labels (Algorithm 2) \u2022 avg: parameter averaging (Pathak et al., 2010) \u2022 vote: private ensemble using majority voting (Algorithm 1) \u2022 indiv: individually trained classifier using local data", "startOffset": 144, "endOffset": 165}, {"referenceID": 0, "context": "To test the algorithms, we use the UCI Human Activity Recognition Dataset (Anguita et al., 2012), which is a collection of motion sensor data on a smart device by multiple subjects performing 6 activities (walking, walking upstairs, walking downstairs, sitting, standing, laying).", "startOffset": 74, "endOffset": 96}, {"referenceID": 13, "context": "The Malicious URL Dataset (Ma et al., 2009) is a collection of examples of malicious URLs from a large Web mail provider.", "startOffset": 26, "endOffset": 43}], "year": 2016, "abstractText": "Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party\u2019s private data? We propose to transfer the \u2018knowledge\u2019 of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global -differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by O( \u22122M\u22122) where M is the number of parties. This allows strong privacy without performance loss whenM is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection.", "creator": "LaTeX with hyperref package"}}}