{"id": "1602.05179", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation", "abstract": "This work festwochen follows allemagne Bengio and oligarchs Fischer (11,050 2015) j.r.r. in m\u00fcnchner which theoretical foundations 25,000,000 were chaurasia laid to visigothic show hexachaeta how iterative cappagh inference benmont can backpropagate error signals. Neurons tippi move waggons their activations 26-member towards coscia configurations corresponding to lower risperidone energy and smaller prediction error: 1.4340 a kelley new oneida observation burntisland creates luchu a circunstancias perturbation kunta at jaina visible neurons that propagates 35,625 into maccormick hidden layers, with these propagated carbon perturbations fade-out corresponding to enthoven the auchterarder back - propagated koschei gradient. palliative This smooches avoids yokogawa the backhander need moveset for bingeing a lengthy circumnavigates relaxation in the positive phase 55,800 of training (when both inputs vinicius and renoirs targets gunpoint are lluvia observed ), as mechner was believed nfi with obtusa previous laxmipur work colbourn on insecure fixed - eighth-seeded point hawsers recurrent jfranksptimes.com networks. We cisf show 2045 experimentally sighs that werts energy - based birdying neural moyal networks adductor with several hidden agami layers nantou can chacin be trained shined at discriminative tasks winrock by shaunie using iterative danged inference subquadrate and an derwent STDP - like shichang learning sidewinder rule. The 9-kilometer main 6.96 result of this fmt paper doz is kregel that 2000-5s we can nonideological train genki neural networks with 3,448 1, kiefel 2 and 3 hidden b.s. layers dasu on atiku the permutation - invariant MNIST task heathcliff and get rui'an the raes training error down to 0. warsi 00% . 1186 The results presented mcdowell here make it more guangde biologically plausible that whipper a elledge mechanism similar habbaniya to blogger back - propagation may apisai take place in brains +11.00 in tuor order to achieve jesuit-run credit assignment switchblade in sobakam deep widescale networks. The 50.19 paper berthon also discusses some of the whitticase remaining open cgwic problems pantulu to 2,850 achieve a warschauer biologically aileron plausible marries implementation 1.3666 of ahistorical backprop dvu in brains.", "histories": [["v1", "Tue, 16 Feb 2016 20:46:51 GMT  (300kb,D)", "http://arxiv.org/abs/1602.05179v1", null], ["v2", "Wed, 24 Feb 2016 11:13:08 GMT  (301kb,D)", "http://arxiv.org/abs/1602.05179v2", null], ["v3", "Tue, 20 Sep 2016 16:15:26 GMT  (436kb,D)", "http://arxiv.org/abs/1602.05179v3", null], ["v4", "Mon, 26 Sep 2016 09:55:15 GMT  (439kb,D)", "http://arxiv.org/abs/1602.05179v4", null], ["v5", "Tue, 28 Mar 2017 18:31:11 GMT  (388kb,D)", "http://arxiv.org/abs/1602.05179v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["benjamin scellier", "yoshua bengio"], "accepted": false, "id": "1602.05179"}, "pdf": {"name": "1602.05179.pdf", "metadata": {"source": "CRF", "title": "Towards a Biologically Plausible Backprop", "authors": ["Benjamin Scellier", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.e., moving towards configurations that better \u201cexplain\u201d the observed sensory data. We can think of the configuration of internal neurons (hidden units or latent variables) as an \u201cexplanation\u201d for the observed sensory data.\nThis work is only a stepping stone towards a full theory of learning in deep biological networks that performs a form of credit assignment that would be credible from a machine learning point of view and would scale to very large networks. We focus on a simple setup in which inputs are clamped, the network relaxes to a fixed point, at which predictions are read out. When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al. (2015a). Several points, elaborated at the end of this paper, still need to be elucidated before a complete theory of learning, inference and credit assignment is elaborated that is both biologically plausible and makes sense from a machine learning point of view for whole networks (with global optimization of the whole network and not being limited to learning of visible neurons that receive a target). In particular, the proposed energy-based model requires symmetry of connections, but note that hidden units in the model need not correspond exactly to actual neurons in the brain (it could be groups of neurons in a cortical microcircuit, for example). It remains to be shown how a form of symmetry could arise from the learning procedure itself or if a different formulation could eliminate the symmetry requirement.\nWe believe that the contributions of this article are the following.\n\u2022 We lay theoretical foundations that guarantee that our model (and in particular the STDP learning rule) makes sense from a mathematical and machine learning point of view, i.e., that the proposed STDP update rule corresponds to stochastic gradient descent on a prediction error.\n\u2022 This shows that leaky integrator neural computation in a recurrent neural network can be interpreted as performing both inference and back-propagation of errors. This avoids the need for a side network that is not biologically plausible for implementing back-propagation.\n\u2217Y.B. is also a Senior Fellow of CIFAR\nar X\niv :1\n60 2.\n05 17\n9v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\n16\n\u2022 We show experimentally that with such a procedure, it is possible to train a model with 1, 2 and 3 hidden layers on MNIST and get the training error down to 0.00%.\n\u2022 The code for the model is available for replicating and extending the experiments."}, {"heading": "2 Previous work (revisited)", "text": "In this section, we present the model first introduced in Bengio and Fischer (2015); Bengio et al. (2015a,b). The model is a continuous-time process { (\u03b8(t), s(t)) : t \u2208 R } where, as usual in neural networks, s is the vector that represents the\nstates of the units and \u03b8 = (W, b) represents the set of free parameters, which includes the synaptic weights Wi,j and the neuron biases bi (which control the activation threshold for each unit i and also correspond to the weight from a virtual constant input). The units are continuous-valued and would correspond to averaged voltage potential across time, spikes, and possibly neurons in the same minicolumn. The units in the model need not correspond exactly to actual neurons in the brain. Finally, \u03c1 is an activation function such that \u03c1(si) represents the firing rate of unit i. Next we will define neural computation (subsection 2.1) and see how it performs both inference (subsection 2.2) and error back-propagation (subsection 2.3)."}, {"heading": "2.1 Neural computation as leaky integrator", "text": "As usual in models of biological neurons, we assume that the neurons are performing leaky temporal integration of their past inputs. The time evolution of the neurons is assumed to follow the leaky integration equation\ndsi dt = Ri(s)\u2212 si \u03c4 , (1)\nwhere Ri(s) represents the pressure on neuron i from the rest of the network (and the value to which si would converge if Ri(s) would not change) while \u03c4 is a characteristic integration time. Moreover Ri(s) is assumed to be of the form\nRi(s) \u221d \u2211 j 6=i Wj,i\u03c1(sj) + bi. (2)"}, {"heading": "2.2 Neural computation does inference: going down the energy", "text": "One hypothesis in computational neuroscience is that biological neurons perform iterative inference. In our model, that means that the hidden units of the network gradually move towards configurations that are more probable, given the sensory input and according to the current \"model of the world\" associated with the parameters of the model.\nOne class of models based on iterative inference is the class of energy-based models, in which an energy function E(\u03b8, s) drives the states of the units according to a dynamics of the form\nds dt \u221d \u2212\u2202E \u2202s .\nWith this dynamics, the network spontaneously moves toward low-energy configurations. Consider the following energy function, studied by Bengio et al. (2015a):\nE(\u03b8, s) := 1\n2 \u2211 i s2i \u2212 1 2 \u2211 i 6=j Wi,j\u03c1(si)\u03c1(sj)\u2212 \u2211 i bi\u03c1(si). (3)\nWe have\n\u2202E \u2202si = si \u2212 \u03c1\u2032(si) \u2211 j 6=i 1 2 (Wi,j +Wj,i)\u03c1(sj) + bi  . (4) Thus, by defining\nds dt = \u2212 1 \u03c4 \u2202E \u2202s , (5)\nwe get for the i-th unit dsi dt = \u2212 1 \u03c4 \u2202E \u2202si = Ri(s)\u2212 si \u03c4 , (6)\nwhich is consistent with Eq. 1, with\nRi(s) = \u03c1 \u2032(si) \u2211 j 6=i 1 2 (Wi,j +Wj,i)\u03c1(sj) + bi  . (7) For Ri(s) to have the same form as in Eq. 2, we need to impose symmetric connections, i.e. Wi,j = Wj,i. Finally, we get\nRi(s) := \u03c1 \u2032(si) \u2211 j 6=i Wj,i\u03c1(sj) + bi  . (8) The factor \u03c1\u2032(si) would suggest that when a neuron is saturated (either being shut off or firing at the maximal rate), the external inputs have no impact on its state. In this case, the dynamics of si becomes dsidt = \u2212 si \u03c4\n, driving si towards 0 and bringing it out of the saturation region and back into a regime where the neuron is sensitive to the outside feedback, so long as \u03c1(0) is not a saturated value."}, {"heading": "2.3 Early inference recovers backpropagation", "text": "In Bengio and Fischer (2015), it is shown how iterative inference can also backpropagate error signals in a multi-layer network. In this subsection we revisit this result and adapt it to neural networks with a general architecture where the units are split in visible and hidden units, i.e. s = (v, h). Like in previous work inspired by the Boltzmann machine, we will use the terminology of \u201cpositive phase\u201d and \u201cnegative phase\u201d to distinguish two phases of training, the positive phase being with v fully observed (or clamped) and the negative phase with v partially or fully unobserved. They actually correspond to the network following the gradient of the energy function, but with or without a term that drives some or all of the visible units towards the value of external signals (inputs and target outputs of the network).\nFor this purpose, we introduce a new term to the energy function that drives the neuron, a term that corresponds to prediction error and that can push visible units towards observed values for any subset of the visible units:\nC\u03b2(v) := 1\n2 \u2211 i \u03b2i(vdata,i \u2212 vi)2 (9)\nwhere \u03b2i \u2265 0 controls whether vi is pushed towards vdata,i or not, and by how much. The total actual energy is thus\nF (\u03b8, s, \u03b2) := E(\u03b8, s) + C\u03b2(v) = 1\n2 \u2211 i s2i \u2212 1 2 \u2211 i6=j Wi,j\u03c1(si)\u03c1(sj)\u2212 \u2211 i bi\u03c1(si) + C\u03b2(v), (10)\nand the state s of the network evolves according to\nds dt \u221d \u2212\u2202F \u2202s . (11)\nWe may see F as an energy function for \"generalized phases\". The case \u03b2 = 0 corresponds to the negative phase where all the units evolve freely according to the dynamics of the network, i.e. ds\ndt \u221d \u2212 \u2202E \u2202s . When \u03b2i > 0, an additional term\n\u221d \u2212 \u2202C \u2202vi = vdata,i\u2212 vi is added that drives the visible unit vi towards the data vdata,i. In the limit \u03b2i \u2192 +\u221e, the visible unit vi move infinitely fast towards vdata,i, i.e. it is immediately clamped to vdata,i and is not sensitive to the pressure of its surroundings neurons anymore. The case when \u03b2i \u2192 +\u221e for all visible units vi corresponds to the positive phase in the terminology of Boltzmann machines. Now we can define a notion of \"\u03b2-fixed point\": given \u03b2 (with \u03b2i \u2265 0 for all i), we call \"\u03b2-fixed point\" and denote by s\u03b2 a state that satisfies\n\u2202F \u2202s (\u03b8, s\u03b2 , \u03b2) = 0. (12)\nThe state s\u03b2 is the fixed point to which the network eventually settles when it is driven by the dynamics of Eq. 11. For \u03b2 = 0 on the visible units that we want the network to predict, we call s0 the negative fixed point and also denote it by s\u2212. For \u03b2 = +\u221e (here we mean that \u03b2i \u2192 +\u221e for all visible unit i), we call s+\u221e the positive fixed point and also denote it by s+. \u03b2 = +\u221e corresponds to \u201cclamping\u201d the visible units.\nWhen addressing the supervised learning scenario where one wants to predict ydata from xdata, we distinguish two groups of visible units, the inputs x and the outputs y, with their respective \u03b2:\nC\u03b2(v) := 1\n2 \u03b2x||xdata,i \u2212 xi||2 +\n1 2 \u03b2y||ydata,i \u2212 yi||2 (13)\nwhere (x, y) = v and \u03b2x and \u03b2y are non-negative scalars. In the supervised setting studied here, during the negative phase we have \u03b2x \u2192 +\u221e and \u03b2y = 0, i.e., the inputs are observed but not the target outputs.\nSuppose that the network is settled to a (negative phase) equilibrium point s = s0 = s\u2212 = (xdata, h\u2212, y\u2212), that is\n\u2202F \u2202h (xdata, h \u2212, y\u2212) = 0 and \u2202F \u2202y (xdata, h \u2212, y\u2212) = 0. (14)\nThen in the positive phase, ydata is observed and we change \u03b2y from 0 to a positive value, gradually driving the output units y from their fixed point value y\u2212, towards ydata. This happens because the output units are also leaky integrator neurons, meaning that their state gradually changes based on the input they receive, in direction of the driving signal (now not only Rv(s\u2212) but also ydata). Notice that because s\u2212 is already at equilibrium for all the units except those for which a new observation is made, just after that observation is made (we are starting the positive phase), we have\ndy dt = ydata \u2212 y\u2212 \u03c4 = \u2212 1 \u03c4 \u2202C\u03b2 \u2202y (y\u2212). (15)\nThen, starting from Eq. 5 and differentiating wrt time, we have\nd2h\ndt2 = \u2212 1 \u03c4 d dt \u2202F \u2202h = \u2212 1 \u03c4\n( \u22022F\n\u2202y\u2202h\ndy dt + \u22022F \u2202h2 dh dt ) where we use that\ndf dt = \u2202f \u2202y dy dt + \u2202f \u2202h dh dt , (16)\nwith the function f(h, y) = \u2202F \u2202h (xdata, h, y). Injecting the initial values of dydt and dh dt at the point s\u2212 in Eq. 2.3 we get\nd2h dt2 = 1 \u03c42 \u22022F \u2202h\u2202y \u2202C\u03b2 \u2202y\n(17)\nwhere we use that the cross derivatives of F are symmetric. Indeed we are still at the fixed point, where the \u201cposition\u201d has not changed, and only \u201cvelocity\u201d on y is non-zero, due to a change in the energy function introduced by the novel observation. Since y has not changed, the pressure on h has not changed (yet). Therefore we have\nd2h\ndt2 = \u2212 1 \u03c42 \u2202Ry \u2202h \u2202C \u2202y = \u2212 1 \u03c42 \u2202C\u0302 \u2202h , (18)\nwhere we define C\u0302(h) as the value of C(yh) taken at the point yh that is at equilibrium given h, i.e. yh is such that \u2202F \u2202y (h, yh) = 0. We used Eq. 6 for y rather than only for s, so that \u2202 2F \u2202h\u2202y = \u2212 \u2202Ry(s) \u2202h .\nFrom the above, we see that the early change of h from its fixed point (where dh dt is initially 0) is in the direction opposite to the error gradient: early propagation of pertubations due to the output units moving towards their target propagates into perturbations of hidden units in the direction opposite to the gradient of the prediction error C. A related demonstration was made by Bengio and Fischer (2015) in the case of a regular feedforward network and the discrete-time setting, relying on small difference approximations."}, {"heading": "2.4 STDP learning rule", "text": "Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes. Although it is the result of experimental observations in biological neurons, its generalization (outside of the experimental setups in which it was measured) and interpretation as part of a learning procedure that could explain learning in deep networks remains a topic where more exploration is needed.\nExperimental results in Bengio et al. (2015a) show that if the weight changes satisfy\ndWij dt \u221d \u03c1(si) dsj dt , (19)\nthen we recover the biological observations made in Bi and Poo (2001) about Spike-Timing Dependent Plasticity. For this reason we refer to Eq. 19 as the STDP learning rule.\nIn this paper, we change the STDP learning rule from Bengio et al. (2015a) into\ndWij dt \u221d \u03c1(si) d\u03c1(sj) dt . (20)\nThe two rules are the same up to a factor \u03c1\u2032(sj). However, with the hard sigmoid nonlinearity \u03c1(s) = max(0,min(1, s)) chosen for our experiments, s is forced to stay in the \u201cactive\u201d range where \u03c1\u2032(s) = 1. Note that this form of the STDP update rule is the same as the one studied by Xie and Seung (2000).\nAn advantage of this form of the STDP update rule is that it leads to a more natural view of the update for the tied symmetric value Wij = Wji. Assuming this constraint to be somehow enforced, the update should take into account the pressures from both the i to j and j to i synapses, so that the total update under constraint is\ndWij dt \u221d \u03c1(si) d\u03c1(sj) dt + \u03c1(sj) d\u03c1(si) dt = d dt \u03c1(si)\u03c1(sj). (21)\nWe will call Eq. 21 the symmetric STDP learning rule.\nAs an aside, let us show that the symmetric STDP learning rule can be expressed in terms of the following quantity J (a kind of kinetic energy) :\nJ := \u2225\u2225\u2225\u2225dsdt \u2225\u2225\u2225\u22252 . (22)\nUsing (5), J can be rewritten\nJ = \u2212 \u2329 \u2202E\n\u2202s , ds dt\n\u232a = \u2212 dE\ndt \u2223\u2223\u2223\u2223 W\n(23)\nwhere the notation |W indicates that the differentiation is performed with fixed W . Differentiating J with respect to W we get\n\u2202J \u2202Wij = \u2212 \u2202 \u2202Wij dE dt = \u2212 d dt \u2202E \u2202Wij = d dt \u03c1(si)\u03c1(sj). (24)\nTherefore the symmetric STDP learning rule can be rewritten:\ndW dt \u221d \u2202J \u2202W . (25)"}, {"heading": "2.5 Deriving Contrastive Hebbian Learning from the STDP learning rule", "text": "A connection between Backpropagation and Contrastive Hebbian Learning was shown previously in Xie and Seung (2003). We have seen in subsection 2.3 a connection between iterative inference and backpropagation. In this subsection we show a connection between the STDP learning rule and Contrastive Hebbian Learning in energy-based models. In a differential form, the STDP learning rule (21) can be written\ndWij \u221d d (\u03c1(si)\u03c1(sj)) . (26)\nAs in subsection 2.3, we denote the state of the network by s = (x, h, y) where x, h and y are the input, hidden and output units respectively. Consider the following procedure:\n1. negative phase (\u03b2x = +\u221e and \u03b2y = 0): let the network relax and settle to a fixed point s\u2212 = (xdata, h\u2212, y\u2212); don\u2019t update the weights during this phase;\n2. positive phase (\u03b2 = (\u03b2x, \u03b2y) with \u03b2x = +\u221e and \u03b2y > 0): starting from s\u2212, let the network relax and settle to a fixed point s\u03b2 = (xdata, h\u03b2 , y\u03b2); update the weights according to the STDP rule (26) on the path from s\u2212 to s\u03b2 .\nBy integrating (26) on the path from s\u2212 to s\u03b2 , we get\n\u2206Wij \u221d \u03c1(s\u03b2i )\u03c1(s \u03b2 j )\u2212 \u03c1(s \u2212 i )\u03c1(s \u2212 j ). (27)\nIf we assume that the weight updates on the path from s\u2212 to s\u03b2 are too small to influence the trajectory of s, then the two following update procedures must be equivalent:\n1. update the weights according to (26) continuously on the path from s\u2212 to s\u03b2 ;\n2. update the weights according to (27) at the end of the positive phase.\nNotice that, if we choose \u03b2y = +\u221e in the positive phase (i.e. ydata is clamped), Eq. 27 is the Contrastive Hebbian Learning rule:\n\u2206Wij \u221d \u03c1(s+i )\u03c1(s + j )\u2212 \u03c1(s \u2212 i )\u03c1(s \u2212 j )."}, {"heading": "3 Link to Recurrent Back-Propagation and Getting Rid of the Positive Phase Relaxation", "text": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients. We present an explanation for this discrepency here.\nWe have seen in Section 2.3 that the early steps of inference in the energy-based model recovers backpropagation of errors through the network. To obtain a full back-propagation algorithm we also show that such a short inference, combined with our STDP update rule, gives rise to stochastic gradient descent on the prediction error. To achieve this, we will consider a value of \u03b2 that is only barely greater than zero, which corresponds to only nudging the output units towards a value that would reduce prediction error.\nBesides computational efficiency, another reason for avoiding the positive phase relaxation suggested by Eq. 32 is that it does not follow exactly the same kind of dynamics as the negative phase relaxation because it uses a linearization of the neural activation rather than the fully non-linear activation. From a biological plausibility point of view, having to use a different kind of hardware and computation for the \u201cforward\u201d and \u201cbackward\u201d phases is not satisfying. This issue is \u201caddressed\u201d by Xie and Seung (2003) by assuming that the feedback weights are tiny compared to the feedforward weights (thus making the feedback weights only indice infinitesimal perturbations on the hidden units\u2019 state). Again, this introduces a hypothesis which seems to hardly match biology.\nWe use a different trick that both gets rid of the need for a positive phase relaxation and avoids the assumption of infinitesimal feedback weights. The idea relies on the observation that we are only looking for the gradient of C, which only asks how a small change in h\u2212 or \u03b8 would yield a small change in C. Thus we do not need to relax y all the way to ydata: we only need to nudge it in the direction \u2212 \u2202C\u2202\u2202y . This corresponds to picking \u03b2y = positive but infinitesimal."}, {"heading": "3.1 Reformulation of the problem", "text": "Recall that we write the state s = (x, h, y) where x, h and y are the inputs, the hiddens and the outputs respectively. The \"negative phase\" corresponds to the choice of \u03b2 = (\u03b2x, \u03b2y) with \u03b2x = +\u221e and \u03b2y = 0 for the \"generalized energy function\" F . We denote the negative phase fixed point by s\u2212. We will argue that, after the network has settled to s\u2212, the correct thing to do from a machine learning perspective is not a positive phase with \u03b2y = +\u221e, but a positive phase with a \"small\" \u03b2y > 0.\nWe can frame the training objective as the following constrained optimization problem:\nfind min \u03b8,s C(s)\nsubject to \u2202E\n\u2202s (\u03b8, s) = 0.\nAs usual for constrained optimization problems, we introduce the Lagrangian:\nL(\u03b8, s, \u03bb) := C(s) + \u03bb \u00b7 \u2202E \u2202s (\u03b8, s). (28)\nAs usual in this setting, starting from the current parameter \u03b8, we first find s\u2217 and \u03bb\u2217 such that\n\u2202L \u2202\u03bb (\u03b8, s\u2217, \u03bb\u2217) = 0 and\n\u2202L \u2202s (\u03b8, s\u2217, \u03bb\u2217) = 0, (29)\nand then we do one step of gradient descent on L with respect to \u03b8, i.e.\n\u2206\u03b8 \u221d \u2212\u2202L \u2202\u03b8 (\u03b8, s\u2217, \u03bb\u2217) = \u2212\u03bb\u2217 \u00b7 \u2202 2E \u2202\u03b8\u2202s (\u03b8, s\u2217). (30)\nThe first condition translates into \u2202E \u2202s (\u03b8, s\u2217) = 0, i.e. s\u2217 = s\u2212 is the \"negative phase\" fixed point. The second condition translates into\n\u2202C \u2202s (s\u2212) + \u03bb\u2217 \u00b7 \u2202\n2E\n\u2202s2 (\u03b8, s\u2212) = 0. (31)\nNote that solving the above equation in \u03bb\u2217 can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987):\n\u2202C \u2202s (s\u2212) + (I \u2212R\u2032(s))\u03bb\u2217 = 0\u21d2 \u03bb\u2217 = R\u2032(s)\u03bb\u2217 \u2212 \u2202C \u2202s . (32)\nInstead, here, we will introduce and study a shortcut that does not require such a long relaxation, only the propagation of perturbations across the layers.\nFirst, let us consider another equation whose solution is equivalent in order to find a \u03bb\u2217 that satisfies Eq. 31. We will show shortly that this solution is \u03bb\u2217 = ds \u03b2\nd\u03b2 \u2223\u2223\u2223 \u03b2=0\n, which in practice we will approximate by \u03bb\u2217 \u221d s\u03b2 \u2212 s\u2212 for a small \u03b2 > 0. Recall the definition of a \u03b2-fixed point (Eq. 12). Taking the differential of Eq. 12 for fixed \u03b8, we get\n\u22022F\n\u2202s2 \u00b7 ds\u03b2 + \u2202\n2F\n\u2202\u03b2\u2202s d\u03b2 = 0, (33)\nwhich can be rewritten ( \u22022E\n\u2202s2 + \u03b2\n\u22022C\n\u2202s2\n) \u00b7 ds\u03b2 + \u2202C\n\u2202s d\u03b2 = 0 (34)\nsince \u2202F \u2202\u03b2 = C. Finally, dividing by d\u03b2 and evaluating at \u03b2 = 0 we get (recall that s\u03b2 = s\u2212 for \u03b2 = 0)\n\u22022E\n\u2202s2 (\u03b8, s\u2212) \u00b7 ds\n\u03b2\nd\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 + \u2202C \u2202s (s\u2212) = 0. (35)\nThus ds \u03b2\nd\u03b2 \u2223\u2223\u2223 \u03b2=0 satisfies the same equation as \u03bb\u2217 (Eq. 31) and we conclude that \u03bb\u2217 = ds \u03b2 d\u03b2 \u2223\u2223\u2223 \u03b2=0 . Finally, injecting s\u2217 = s\u2212\nand \u03bb\u2217 = ds \u03b2\nd\u03b2 \u2223\u2223\u2223 \u03b2=0 in Eq. 30 we get\n\u2206Wij \u221d \u2212\u03bb\u2217 \u00b7 \u22022E\n\u2202s\u2202Wij =\nds\u03b2\nd\u03b2 \u2223\u2223\u2223\u2223 \u03b2=0 \u00b7 \u2202 \u2202s \u03c1(si)\u03c1(sj) \u2223\u2223\u2223\u2223 s=s\u2212 = d d\u03b2 \u03c1(s\u03b2i )\u03c1(s \u03b2 j ) \u2223\u2223\u2223\u2223 \u03b2=0\n(36)\nusing \u2202E \u2202Wij = \u03c1(si)\u03c1(sj).\nIn practice, after running the negative phase and reaching the state s\u2212, we run a \u03b2-phase (with small \u03b2) and reach s\u03b2 , and finally we update\n\u2206Wij \u221d \u03c1(s\u03b2i )\u03c1(s \u03b2 j )\u2212 \u03c1(s \u2212 i )\u03c1(s \u2212 j ). (37)\nAs argued in subsection 2.5 this is equivalent to update the weights according to the symmetric STDP learning rule\ndWij dt \u221d d dt \u03c1(si)\u03c1(sj) (38)\nall the way from s\u2212 to s\u03b2 . In fact we don\u2019t even need to let the network relax until it reaches the beta fixed point s\u03b2 during the \u03b2-phase. Indeed in Subsection 2.3 we showed that early moves of s at the point s\u2212 go in the direction that minimizes C. That means that they go in the direction that minimizes F since E does not contribute to these moves at s = s\u2212.\nHence we have found that (a) stochastic gradient descent of the prediction error in a recurrent network with clamped inputs can be achieved with a very brief relaxation (just enough for signals to propagate from outputs into all the hidden layers) in which the output units are slightly driven towards their target and that (b) the update corresponds to the STDP update rule from Bengio et al. (2015a) as well as (when incorporating the symmetry constraint) to the contrastive Hebbian learning update."}, {"heading": "4 Implementation of the model", "text": "In this section, we provide experimental evidence that our model is trainable, and we analyze the influence of various factors on the training time, the generalization error as well as the biological plausibility of the model.\nWe show that the STDP learning rule (21) can be used to train a neural network with several layers of hidden units to classify the MNIST digits.\nFor each example, training proceeds in two phases. During the first phase (that we call negative phase by analogy with Boltzmann Machines), we clamp x = xdata and let the network relax until it settles to a (negative) fixed point s\u2212 = (xdata, h\n\u2212, y\u2212). During the negative phase, the weights W remain fixed. Then, during a second phase (called positive phase, or \u03b2-phase as discussed in section 3) we drive y towards ydata according to updates y \u2190 (1\u2212 )y+ ydata and let the network relax for \"a little bit\" (see the next subsection for the a discussion about the duration of this relaxation)."}, {"heading": "4.1 Finite difference method", "text": "We choose \u03c4 = 1 for the characteristic time. The obvious way to implement the iterative inference procedure as defined in (5) is to discretize time into short time lapses of duration and update each unit si according to\nsi \u2190 si \u2212 \u2202E\n\u2202si = (1\u2212 )si + Ri(s),\nthat is\nsi \u2190 (1\u2212 )si + \u03c1\u2032(si) \u2211 j 6=i Wj,i\u03c1(sj) + bi  . (39) This is simply one step of gradient descent on the energy, with step size , as described in Bengio and Fischer (2015).\nFor our experiments (see subsection 4.2) we choose the hard sigmoid as an activation function, i.e. \u03c1(s) = 0 \u2228 s \u2227 1, where \u2228 denotes the max and \u2227 the min. Also, rather than the standard gradient descent (39), we will use a slightly modified version:\nsi \u2190 0 \u2228 (1\u2212 )si + \u03c1\u2032(si) \u2211 j 6=i Wj,i\u03c1(sj) + bi  \u2227 1. (40) Indeed, for this choice of \u03c1, the form of Ri(s) (Eq. 8) shows that when si reaches si = 0, the fact that \u03c1\u2032(0\u2212) = 0 prevents si from going further in the range of negative values. Similarly, si cannot reach values above 1. Therefore si always remains in the domain 0 \u2264 si \u2264 1. Hence Eq. 40. This little detail is more important than it seems at first glance: if at some point the ith unit was in the state si < 0, then Eq. 39 would simplify to si \u2190 (1 \u2212 )si, which would give again si < 0. As a consequence si would be doomed to remain in the negative range forever, being totally insensitive to the pressure of the surrounding units. This issue was already mentioned in Bengio and Fischer (2015).\nTwo questions arise:\n1. What step size should we choose ?\n2. How long do the negative phase and positive phase relaxations need to last (as functions of \u03c4 )?\nStep size . Figure 1 shows that the choice of has little influence as long as 0 < < 1. In our experiments we choose = 0.5 to avoid extra unnecessary computations.\nDuration of the negative phase relaxation. We find experimentally that the number of iterations required for the negative phase relaxation is large and grows fast as the number of layers increases, which could slow down training. More\nexperimental and theoretical investigation would be needed to analyze the number of iterations required, but we leave that for future work.\nDuration of the positive phase. As discussed in section 3, during the positive phase we only need to initiate the move of the units. Notice that the characteristic time \u03c4 represents the time needed for a signal to propagate from a layer to the next one with \"significant amplitude\". So the time needed for the error signals to backpropagate in the network is n\u03c4 , where n is the number of layers."}, {"heading": "4.2 Implementation details and results", "text": "We train multi-layer neural networks with 1, 2 and 3 hidden layers. Each hidden layer has 500 units. There is no connection within a layer and no skip connections. For efficiency of the experiments, we use mini batches of 20 training examples and we use = 0.5 as the step size for each iteration. Only one global weight update is done at the end of the positive phase according to Eq. 27, rather than several weight updates after each step of the positive phase (Eq. 26).\nTo tackle the problem of the long negative phase relaxation and speed-up the simulations, we use \"persistent particles\" for the latent variables to re-use the previous fixed point configuration for a particular example as a starting point for the next negative phase relaxation on that example. This means that for each training example in the dataset, we store the state of the hidden layers at the end of the negative phase, and we use this to initialize the state of the network at the next epoch. This method is similar in spirit to the PCD algorithm for sampling from other energy-based models like the Boltzmann machine (Tieleman, 2008).\nFor the model with 1 hidden layer, we run 20 iterations for the negative phase and 4 iterations for the positive phase. For the model with 2 hidden layers, we run 100 iterations for the negative phase and 6 iterations for the positive phase. For the model with 3 hidden layers, we run 500 iterations for the negative phase and 8 iterations for the positive phase.\nWe find that it is important to choose different learning rates for the weight matrices of different layers. We denote by Wi the weight matrix between the layers Li\u22121 and Li. We choose the learning rate \u03b1i for Wi so that the quantities \u2016\u2206Wi\u2016 \u2016Wi\u2016\nfor i = 1, \u00b7 \u00b7 \u00b7 , n are approximately the same in average, where \u2016\u2206Wi\u2016 represents the weight change of Wi after seeing a minibatch. For the model with 1 hidden layer, we use \u03b11 = 0.1 and \u03b12 = 0.05. For the model with 2 hidden layers, we use \u03b11 = 0.4, \u03b12 = 0.1 and \u03b13 = 0.01. For the model with 3 hidden layers, we use \u03b11 = 0.128, \u03b12 = 0.032, \u03b13 = 0.008 and \u03b14 = 0.002."}, {"heading": "4.3 Speeding up the relaxation", "text": "In this subsection we introduce an algorithm to accelerate the relaxation in both the negative phase and the positive phase. With the finite difference method (subsection 4.1), the number of iterations required to reach a negative fixed point with enough accuracy becomes very large as the number of hidden layers increases. The algorithm that we design here enables to train our networks 5 times as fast by speeding up convergence towards a fixed point. This algorithm is less biologically plausible but it establishes a link with forward propagation and backpropagation in feedforward networks, as well as Gibbs sampling in Boltzmann machines. The spirit of this algorithm is similar to going from ordinary Gibbs sampling (where only one unit is updated at a time) to block Gibbs sampling (where a whole layer is updated in parallel, given the layer below and the one above). Given the state of the layer above and the layer below some layer, we can analytically solve for the fixed point solution of the middle layer. By iterating these analytical fixed point solutions (conditional on clamping the other layers), we can greatly speed-up convergence of the relaxations.\nGiven the states s\u2212i of all units but unit i, one denotes by s\u2217i the value of si that minimizes the energy:\ns\u2217i := arg min si E(\u03b8, s\u2212i, si) = arg min si s2i \u2212 \u03c1(si) \u2211 j 6=i Wij\u03c1(sj) + bi  . (41) The value s\u2217i represents the equilibrium potential of unit i subjected to the constant external pressure pi = \u2211 j 6=iWij\u03c1(sj)+ bi. The value s\u2217i is a function of pi, so we denote by \u03c0 the mapping from pi to s \u2217 i , i.e. s \u2217 i = \u03c0(pi). If \u03c1 is differentiable everywhere, then to achieve ds \u2217 i\ndt = 0, we must have s\u2217i such that\ns\u2217i = \u03c1 \u2032(s\u2217i ) pi. (42)\nFigure 3 shows the shape of the function \u03c0 in two cases. When \u03c1 is the hard sigmoid defined by \u03c1(s) = 0 \u2228 s \u2227 1 (where \u2228 denotes the max and \u2227 denotes the min), then \u03c0 is the hard sigmoid itself. When \u03c1 is a soft sigmoid such\nas \u03c1(s) = 1/(1 + e2\u22124s), then \u03c0 has no analytical expression and must be computed numerically. For this reason we choose the hard sigmoid for our experiments.\nThe function \u03c0 can be used to update the units sequentially in a way similar to the Hopfield net or the Gibbs sampling algorithm in Boltzmann machines. Given the state s\u2212i of all units except unit i, the update rule for si is\nsi \u2190 \u03c0 \u2211 j 6=i Wji\u03c1(sj)  . (43) In the case of a multi-layer network with no connection within a layer, one can update in parallel all the units within a layer given the state of the units in the other layers. We denote by L0, L1, \u00b7 \u00b7 \u00b7 , Ln the layers of the network, where L0 = x and Ln = y. For the negative relaxation phase, we use the algorithm 1 (Forwardprop). For the positive phase, we use the algorithm 2 (Backprop). These algorithms are similar in spirit to the block Gibbs sampling procedure in Deep Boltzmann Machines. Also notice that these algorithms still work with skip connections.\nAlgorithm 1 Forwardprop Clamp L0 \u2190 xdata. repeat\nfor k = 1 to n\u2212 1 do Lk \u2190 \u03c0 ( Wl\u22121Lk\u22121 +W T l Lk+1 ) end for Ln \u2190 \u03c0 (Wn\u22121Ln\u22121)\nuntil convergence.\nAlgorithm 2 Backprop Clamp L0 \u2190 xdata and Ln \u2190 ydata. repeat\nfor k = n\u2212 1 down to 1 do Lk \u2190 \u03c0 ( Wl\u22121Lk\u22121 +W T l Lk+1 ) end for\nuntil desired."}, {"heading": "5 Discussion and future work", "text": "In the CD algorithm for RBMs, one starts from a positive equilibrium sample and then, during a negative phase, we do a few steps of iterative inference to get an approximate negative sample. However, for discriminative tasks, it makes more sense to do the contrary: start from a negative state and drive the prediction made towards the correct target during a positive phase. This is what our model does.\nThe same algorithms can be adapted to the unsupervised learning setting, and this is the subject of future work.\nWe could imagine introducing different types of units with different characteristic times \u03c4 to integrate the signals from the surrounding units.\nWe could introduce connections within each layer. The property of subsection 2.3 still applies in this setting. Therefore the finite difference method ought to work. Notice that the trick to speed up the relaxation (the forwardprop and backprop algorithms) does not work in this setting, but it is less biologically plausible anyway.\nA troubling issue from the point of view of biological plausibility is our model requires symmetric weights between the units. We need to find a way to either untie those weights or figure out how a symmetry in the connections could naturally arise, for example from autoencoder-like unsupervised learning. Encouraging cues from the observation that denoising autoencoders without tied weights often end up learning symmetric weights (Vincent et al., 2010). Another encouraging piece of evidence, also linked to autoencoders, is the theoretical result from Arora et al. (2015), showing that the symmetric solution minimizes the autoencoder reconstruction error between two successive layers of rectifying (ReLU) units.\nMore open problems remains. Both from a machine learning point of view and from a biological plausibility point of view, we would like to get rid of the requirement to have to run a full relaxation to a fixed point in the negative phase. From a machine learning point of view it makes computations much slower than in traditional feedforward neural networks. From a biological point of view it would not be practical for a brain to have to wait for the whole brain to settle near a fixed point before processing the next stimuli. This question is related to the issue of running a deterministic simulation. A more likely simulation would include some form of noise. As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function. Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks.\nAlso connected to the above question of having to wait for a negative phase fixed point is the question of timevarying input. Although this work makes back-propagation more plausible for the case of a static input, the brain is a recurrent network with time-varying inputs, and back-propagation through time seems even less plausible than static back-propagation. An encouraging direction is that proposed by Ollivier et al. (2015), which shown that computationally efficient estimators of the gradient can be obtained using a forward method (online estimation of the gradient), which avoids to need to store all past states in training sequences, at the price of a noisy estimator of the gradient."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Akram Erraqabi, Samira Shabanian and Asja Fischer for feedback and discussions, as well as NSERC, CIFAR, Samsung and Canada Research Chairs for funding, and Compute Canada for computing resources."}], "references": [{"title": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment", "author": ["L.B. Almeida"], "venue": "IEEE International Conference on Neural Networks,", "citeRegEx": "Almeida,? \\Q1987\\E", "shortCiteRegEx": "Almeida", "year": 1987}, {"title": "Why are deep nets reversible: a simple theory, with implications for training", "author": ["S. Arora", "Y. Liang", "T. Ma"], "venue": "Technical report,", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Early inference in energy-based models approximates back-propagation", "author": ["Y. Bengio", "A. Fischer"], "venue": "arXiv preprint arXiv:1510.02777", "citeRegEx": "Bengio and Fischer,? \\Q2015\\E", "shortCiteRegEx": "Bengio and Fischer", "year": 2015}, {"title": "STDP as presynaptic activity times rate of change of postsynaptic activity", "author": ["Y. Bengio", "T. Mesnard", "A. Fischer", "S. Zhang", "Y. Wu"], "venue": "arXiv preprint arXiv:1509.05936", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Towards biologically plausible deep learning", "author": ["Y. Bengio", "Lee", "D.-H", "J. Bornschein", "Z. Lin"], "venue": "arXiv preprint arXiv:1502.04156", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment", "author": ["P. Berkes", "G. Orban", "M. Lengyel", "J. Fiser"], "venue": null, "citeRegEx": "Berkes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berkes et al\\.", "year": 2011}, {"title": "Synaptic modification by correlated activity: Hebb\u2019s postulate revisited", "author": ["Bi", "G.-q", "Poo", "M.-m"], "venue": "Annual review of neuroscience,", "citeRegEx": "Bi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2001}, {"title": "A neuronal learning rule for sub-millisecond temporal coding", "author": ["W. Gerstner", "R. Kempter", "J.L. van Hemmen", "H. Wagner"], "venue": null, "citeRegEx": "Gerstner et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Gerstner et al\\.", "year": 1996}, {"title": "Learning and releaming in boltzmann machines. Parallel distributed processing: Explorations in the microstructure", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "of cognition,", "citeRegEx": "Hinton and Sejnowski,? \\Q1986\\E", "shortCiteRegEx": "Hinton and Sejnowski", "year": 1986}, {"title": "Action potentials propagating back into dendrites triggers changes in efficacy", "author": ["H. Markram", "B. Sakmann"], "venue": "Soc. Neurosci. Abs,", "citeRegEx": "Markram and Sakmann,? \\Q1995\\E", "shortCiteRegEx": "Markram and Sakmann", "year": 1995}, {"title": "Training recurrent networks online without backtracking", "author": ["Y. Ollivier", "C. Tallec", "G. Charpiat"], "venue": "Technical report,", "citeRegEx": "Ollivier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ollivier et al\\.", "year": 2015}, {"title": "Generalization of back-propagation to recurrent neural networks", "author": ["F.J. Pineda"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Pineda,? \\Q1987\\E", "shortCiteRegEx": "Pineda", "year": 1987}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Tieleman,? \\Q2008\\E", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "J. Machine Learning Res.,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Spike-based learning rules and stabilization of persistent neural activity", "author": ["X. Xie", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Xie and Seung,? \\Q2000\\E", "shortCiteRegEx": "Xie and Seung", "year": 2000}, {"title": "Equivalence of backpropagation and contrastive hebbian learning in a layered network", "author": ["X. Xie", "H.S. Seung"], "venue": "Neural computation,", "citeRegEx": "Xie and Seung,? \\Q2003\\E", "shortCiteRegEx": "Xie and Seung", "year": 2003}], "referenceMentions": [{"referenceID": 2, "context": "Abstract This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals.", "startOffset": 27, "endOffset": 53}, {"referenceID": 8, "context": "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 40, "endOffset": 116}, {"referenceID": 5, "context": "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 40, "endOffset": 116}, {"referenceID": 2, "context": "When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al.", "startOffset": 188, "endOffset": 214}, {"referenceID": 2, "context": "When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al. (2015a). Several points, elaborated at the end of this paper, still need to be elucidated before a complete theory of learning, inference and credit assignment is elaborated that is both biologically plausible and makes sense from a machine learning point of view for whole networks (with global optimization of the whole network and not being limited to learning of visible neurons that receive a target).", "startOffset": 188, "endOffset": 381}, {"referenceID": 2, "context": "In this section, we present the model first introduced in Bengio and Fischer (2015); Bengio et al.", "startOffset": 58, "endOffset": 84}, {"referenceID": 3, "context": "Consider the following energy function, studied by Bengio et al. (2015a):", "startOffset": 51, "endOffset": 73}, {"referenceID": 2, "context": "3 Early inference recovers backpropagation In Bengio and Fischer (2015), it is shown how iterative inference can also backpropagate error signals in a multi-layer network.", "startOffset": 46, "endOffset": 72}, {"referenceID": 2, "context": "A related demonstration was made by Bengio and Fischer (2015) in the case of a regular feedforward network and the discrete-time setting, relying on small difference approximations.", "startOffset": 36, "endOffset": 62}, {"referenceID": 9, "context": "4 STDP learning rule Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes.", "startOffset": 124, "endOffset": 174}, {"referenceID": 7, "context": "4 STDP learning rule Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes.", "startOffset": 124, "endOffset": 174}, {"referenceID": 3, "context": "Experimental results in Bengio et al. (2015a) show that if the weight changes satisfy dWij dt \u221d \u03c1(si) dsj dt , (19)", "startOffset": 24, "endOffset": 46}, {"referenceID": 3, "context": "In this paper, we change the STDP learning rule from Bengio et al. (2015a) into dWij dt \u221d \u03c1(si) d\u03c1(sj) dt .", "startOffset": 53, "endOffset": 75}, {"referenceID": 15, "context": "Note that this form of the STDP update rule is the same as the one studied by Xie and Seung (2000). An advantage of this form of the STDP update rule is that it leads to a more natural view of the update for the tied symmetric value Wij = Wji.", "startOffset": 78, "endOffset": 99}, {"referenceID": 15, "context": "5 Deriving Contrastive Hebbian Learning from the STDP learning rule A connection between Backpropagation and Contrastive Hebbian Learning was shown previously in Xie and Seung (2003). We have seen in subsection 2.", "startOffset": 162, "endOffset": 183}, {"referenceID": 11, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 58, "endOffset": 87}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 58, "endOffset": 87}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 73, "endOffset": 250}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 73, "endOffset": 566}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients. We present an explanation for this discrepency here. We have seen in Section 2.3 that the early steps of inference in the energy-based model recovers backpropagation of errors through the network. To obtain a full back-propagation algorithm we also show that such a short inference, combined with our STDP update rule, gives rise to stochastic gradient descent on the prediction error. To achieve this, we will consider a value of \u03b2 that is only barely greater than zero, which corresponds to only nudging the output units towards a value that would reduce prediction error. Besides computational efficiency, another reason for avoiding the positive phase relaxation suggested by Eq. 32 is that it does not follow exactly the same kind of dynamics as the negative phase relaxation because it uses a linearization of the neural activation rather than the fully non-linear activation. From a biological plausibility point of view, having to use a different kind of hardware and computation for the \u201cforward\u201d and \u201cbackward\u201d phases is not satisfying. This issue is \u201caddressed\u201d by Xie and Seung (2003) by assuming that the feedback weights are tiny compared to the feedforward weights (thus making the feedback weights only indice infinitesimal perturbations on the hidden units\u2019 state).", "startOffset": 73, "endOffset": 1750}, {"referenceID": 10, "context": "(31) Note that solving the above equation in \u03bb\u2217 can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987): \u2202C \u2202s (s) + (I \u2212R(s))\u03bb = 0\u21d2 \u03bb = R(s)\u03bb \u2212 \u2202C \u2202s .", "startOffset": 186, "endOffset": 200}, {"referenceID": 0, "context": "(31) Note that solving the above equation in \u03bb\u2217 can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987): \u2202C \u2202s (s) + (I \u2212R(s))\u03bb = 0\u21d2 \u03bb = R(s)\u03bb \u2212 \u2202C \u2202s .", "startOffset": 201, "endOffset": 216}, {"referenceID": 3, "context": "Hence we have found that (a) stochastic gradient descent of the prediction error in a recurrent network with clamped inputs can be achieved with a very brief relaxation (just enough for signals to propagate from outputs into all the hidden layers) in which the output units are slightly driven towards their target and that (b) the update corresponds to the STDP update rule from Bengio et al. (2015a) as well as (when incorporating the symmetry constraint) to the contrastive Hebbian learning update.", "startOffset": 380, "endOffset": 402}, {"referenceID": 2, "context": "This is simply one step of gradient descent on the energy, with step size , as described in Bengio and Fischer (2015).", "startOffset": 92, "endOffset": 118}, {"referenceID": 2, "context": "This issue was already mentioned in Bengio and Fischer (2015).", "startOffset": 36, "endOffset": 62}, {"referenceID": 13, "context": "This method is similar in spirit to the PCD algorithm for sampling from other energy-based models like the Boltzmann machine (Tieleman, 2008).", "startOffset": 125, "endOffset": 141}, {"referenceID": 14, "context": "Encouraging cues from the observation that denoising autoencoders without tied weights often end up learning symmetric weights (Vincent et al., 2010).", "startOffset": 127, "endOffset": 149}, {"referenceID": 1, "context": "Another encouraging piece of evidence, also linked to autoencoders, is the theoretical result from Arora et al. (2015), showing that the symmetric solution minimizes the autoencoder reconstruction error between two successive layers of rectifying (ReLU) units.", "startOffset": 99, "endOffset": 119}, {"referenceID": 12, "context": "Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks.", "startOffset": 106, "endOffset": 131}, {"referenceID": 2, "context": "As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function.", "startOffset": 18, "endOffset": 44}, {"referenceID": 2, "context": "As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function. Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks. Also connected to the above question of having to wait for a negative phase fixed point is the question of timevarying input. Although this work makes back-propagation more plausible for the case of a static input, the brain is a recurrent network with time-varying inputs, and back-propagation through time seems even less plausible than static back-propagation. An encouraging direction is that proposed by Ollivier et al. (2015), which shown that computationally efficient estimators of the gradient can be obtained using a forward method (online estimation of the gradient), which avoids to need to store all past states in training sequences, at the price of a noisy estimator of the gradient.", "startOffset": 18, "endOffset": 903}], "year": 2016, "abstractText": "This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hidden layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with several hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00%. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains.", "creator": "LaTeX with hyperref package"}}}