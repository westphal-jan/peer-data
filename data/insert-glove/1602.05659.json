{"id": "1602.05659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification", "abstract": "badgers This paper tufano proposes bubka a cul universal method, Boost Picking, vogosca to train supervised classification sotha models murmurs mainly pleistocene by mcneely un - labeled walbert data. praline Boost Picking compiled only quasi-independent adopts two weak shiners classifiers to \u03b5 estimate duang and laraine correct kadhem the error. wisler It \u014dta is theoretically sueif proved liebknecht that Boost Picking could mcgeechan train a severny supervised harun model asili mainly by un - 1880-81 labeled citronella data microglia as bianco effectively as vick the same djerba model trained advertise by arkoff 100% labeled data, grandville only riata if raupp recalls of the two zangana weak itma classifiers 29.10 are all greater spherion than euro940 zero vanni and humiliati the inbar sum minner of homelike precisions franey is late-1920s greater than one. falck Based cobol on blanchet Boost Picking, 116.4 we present \" harmut Test hispanic-american along with sapru Training (beiersdorf TawT) \" judaization to pinkus improve frenzies the generalization of supervised spinosum models. Both Boost Picking upsc and TawT are 2,099 successfully tested in varied little data opportunities sets.", "histories": [["v1", "Thu, 18 Feb 2016 02:24:54 GMT  (851kb,D)", "http://arxiv.org/abs/1602.05659v1", "9 pages, 9 figures"], ["v2", "Mon, 29 Feb 2016 13:16:23 GMT  (841kb,D)", "http://arxiv.org/abs/1602.05659v2", "9 pages, 9 figures"], ["v3", "Sat, 12 Nov 2016 09:25:54 GMT  (0kb,I)", "http://arxiv.org/abs/1602.05659v3", "This paper has been withdraw by the author due to format error"]], "COMMENTS": "9 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["fuqiang liu", "fukun bi", "yiding yang", "liang chen"], "accepted": false, "id": "1602.05659"}, "pdf": {"name": "1602.05659.pdf", "metadata": {"source": "META", "title": "Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification", "authors": ["FUQIANG LIU", "FUKUN BI", "YIDING YANG", "LIANG CHEN"], "emails": ["FQLIU@OUTLOOK.COM", "BIFUKUN@NCUT.EDU.CN", "YANGYDI9G3@BIT.EDU.CN", "CHENL@BIT.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "Supervised learning is the machine learning task of inferring a function from labeled training data (Mohri et al., 2012). The achievements of supervised learning rely on labeled data bases, like LFW (Huang et al., 2007), ImageNet (Fei-Fei, 2010) and Microsoft COCO (Lin et al., 2014). Humans academically called \u201dcrowds\u201d (Attenberg et al., 2011; Burton et al., 2012), who label the data in practice, greatly contribute to the development of machine learning. However, labeling data artificially is not a completely ideal strategy, especially when data erupt. There are two harsh\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nfacts:\n\u2022 In practice, the scale of un-labeled data is far larger than that of labeled data, and the un-labeled data are substantially produced at any time.\n\u2022 Labeling data by humans is too expensive when the scale of data increases.\nGenerally, using abundant data could improve the performance especially generalization of supervised learning models. Based on the above two harsh facts and the benefit of abundant data, we carried out the preliminary research on utilizing un-labeled data to train classical supervised classification models without any internal change, in both theory and practice.\nWe present a universal method named \u201dBoost Picking\u201d to train supervised classification models mainly by un-labeled data. Boost Picking just adds external modules rather than creates internal changes in the traditional models. Furthermore, the external modules are just two weak classifiers to estimate and correct the error of supervised models. We use two weak classifiers to train a strong classifier by unlabeled data (section 3.1) as well as a classifier trained by 100% labeled data. Boost Picking would work effectively only if the sum of the two weak classifiers precisions is greater than one (section 3.2). Precisions of the two weak classifiers determine whether Boost Picking works effectively. And the efficiency of Boost Picking is affected by both precisions and recalls of the two weak classifiers (section 3.3).\nBased on Boost Picking, we present a new way named \u201dTest along with Training, (TawT)\u201d (section 4) to improve\nar X\niv :1\n60 2.\n05 65\n9v 1\n[ cs\n.C V\n] 1\nthe generalization of supervised classification models. Data to be classified are un-labeled, as a result, before classifying these data, train the supervised model using these unlabeled data by Boost Picking. The effect of TawT is theoretically analyzed in section 4.\nThe structure of Boost Picking refers to \u201dP-N Learning\u201d in \u201dTracking-Learning-Detection\u201d (Kalal et al., 2012). Kalal\u2019s work inspired Boost Picking (summarized in section 2). P-N Learning ignores the bias between the estimated labels and the true labels of examples in training data set. But the assumption of Boost Picking is looser and easier. Our contributions are listed as follows.\n\u2022 Theoretically prove the establishment condition of Boost Picking.\n\u2022 Present a theoretical guide on designing the weak classifiers to improve the efficiency of Boost Picking.\n\u2022 Theoretically analyze the generalization improvement of the supervised models when they are used as TawT.\nBoost Picking involves a small amount of labeled data and a large amount of un-labeled data in training the supervised learning models. This peculiarity is as same as that of Semi-supervised learning (Chapelle et al., 2006). Different from previous semi-supervised learning works like (Zhu, 2005; Zhu & Goldberg, 2009; Fazakis et al., 2015), Boost Picking, a universal method on training traditional supervised models using un-labeled data, adopts no internal change and has no limitation on the supervised models. Logistic regression (Hosmer Jr & Lemeshow, 2004), neural network (Haykin & Network, 2004), support vector machine (SVM) (Suykens & Vandewalle, 1999) and Adaboost (Ra\u0308tsch et al., 2001) are all tested with varied data sets in experiments (section 5). Using Boost Picking, these models can be trained by 75% un-labeled data as effectively as same models trained by 100% labeled data. And TawT could improve generalizations of these models. Boost Picking can be regard as a universal method on converting supervised classification to semi-supervised classification."}, {"heading": "2. Prior Work", "text": "This section introduces Kalal\u2019s works. In TLD (Kalal et al., 2012), P-N Learning is an online learning method to learn the new form of the tracked object. It adopts two classifiers, which are named P-expert and N-expert respectively, to evaluate the error of the detector (a classifier) and correct it. P-expert estimates the false negatives from the negative outputs of the detector, while N-expert estimates the false positives from the positive outputs. Boost Picking refers to this thought.\nKalal theoretically proved that P-N Learning could train a detector whose error converges into zero (Kalal et al., 2010; 2012). Define \u2212\u2212\u2192 d(k) as the errors in kth iteration,\n\u2212\u2212\u2192 d(k) = [\u03b1(k), \u03b2(k)]T (1)\nwhere \u03b1(k) represents false positives and \u03b2(k) represents false negatives in outputs of the detector.\nDefine P+,R+ as the precision and the recall of P-expert respectively. Similarly, define P\u2212,R\u2212 as the precision and the recall of N-expert. The recursive equations are as equation (2) (Kalal et al., 2012). \u03b1(k + 1) = \u03b1(k)\u2212R\u2212 \u00b7 \u03b1(k) + 1\u2212 P + P+ R+ \u00b7 \u03b2(k) \u03b2(k + 1) = \u03b2(k)\u2212R+ \u00b7 \u03b2(k) + 1\u2212 P \u2212\nP\u2212 R\u2212 \u00b7 \u03b1(k)\n(2)\nReconstruct equation (1) and (2),\n\u2212\u2192 d (k + 1) =M \u2212\u2192 d (k) (3)\nwhere M = [ 1\u2212R\u2212 1\u2212P\n+\nP+ R +\n1\u2212P\u2212 P\u2212 R\n\u2212 1\u2212R+ ].\nKalal concludes that d will converge to zero only if eigenvalues of the transformation matrix M are all smaller than one (Kalal et al., 2010). It should be noted that P-N Learning try training a classifier without error, d \u2192 0. In practice, it is difficult for the error to converge into zero, because the classifier almost cannot be trained to be errorless even through P-N experts could find out all false positives and false negatives from the outputs. The conclusion in Kalal\u2019s works is based on an assumption that the trained classifier can perfectly classify the data in training set. P-N Learning ignores the bias of the classifier in training set."}, {"heading": "3. Boost Picking", "text": "This section details Boost Picking, drives its establishment condition and analyzes its efficiency in theory."}, {"heading": "3.1. Description of Boost Picking", "text": "The structure of Boost Picking is showed as Fig 1. Supposing x is an example from a feature-space X, and y is its corresponding label from a label space Y = {1,\u22121}, the training set of Boost Picking is composed by a labeled set Xl with its label Ll and an unlabeled set Xu. Supposing that Lu are the true labels of Xu (do not exist in fact), define F : x \u2192 y as an supervised learning model trained by 100% labeled data set X(100%) = {Xl, Xu} with L(100%) = {Ll, Lu}. The goal of Boost Picking is to train an effective model f \u2192 F by Xl, Ll and Xu.\nFirst, the supervised model is trained by labeled data Xl, which is named \u201dinitialization\u201d ( 1\u00a9 in Fig 1). Through this step, the model is initialed as f0.\nSecond, the un-labeled dataXu are classified by the trained model ( 2\u00a9 in Fig 1).\nyk = fk\u22121(x), x \u2208 Xu, (4)\nwhere yk is the estimated label in kth iteration and fk\u22121 is the result of the k-1th iteration.\nThen use two classifiers, FP Finder and FN Finder, to estimate the error in the outputs. Here, functions of FP Finder and FN Finder are similar with N-expert and P-expert. FP Finder detects the false positives in positive outputs and FN Finder detects the false negatives in negative outputs.{\nXf+ = FP (X+) Xf\u2212 = FN(X\u2212) (5)\nwhere X+ = {x \u2208 Xu|f(x) = 1} and X\u2212 = {x \u2208 Xu|f(x) = \u22121}\nAnd then, the supervised learning model is re-trained by a new data set. The new data set is composed by randomly selected labeled data and the results of FP Finder and FN Finder. Xnew = {Xran, Xf+, Xf\u2212}, where Xran \u2282 Xl. In the new data set, labels of FP Finder results are set as -1 and labels of FN Finder results are set as 1.\nThe first step \u201dinitialization\u201d ( 1\u00a9 in Fig 1) is executed only one time. And the second step ( 2\u00a9 in Fig 1) is circularly executed until satisfying the convergence condition. The establishment condition of Boost Picking is theoretically proved in section 3.2\nIn implement, one of the convergence conditions is that there is almost no difference between current outputs and last outputs, which can be written as equation (6).\nsum(|yki \u2212 yk\u22121i |)/2Nu < \u03b8diff (6)\nwhere Nu is the size of Xu. Another convergence condition is that the classification results of part of labeled data\nAlgorithm 1 Boost Picking Input: data Xl with label Ll, data Xu Initialize trainset = Xl, Ynow = 0 repeat\nTrain f with trainset Ypre = Ynow Ynow = f(Xu) Xf+ = FP (X+), Xf\u2212 = FN(X\u2212) Xran = randomselect(Xl) trainset = {Xran, Xf+, Xf\u2212} con1 = sum(|Ynow \u2212 Ypre|)/2Nu < \u03b8diff Xran = randomselect(Xl) con2 = sum(|Lran \u2212 f(Xran)|)/2Nran < \u03b8bias\nuntil con1 + con2 == 2\nhas a relatively small bias.\nsum(|lran \u2212 f(xran)|)/2Nran < \u03b8bias (7)\nwhere xran \u2208 Xran and Nran is the scale of Xran. It should be noted that each Xran including the one in Xnew is randomly selected from Xl anew.\nUnlike the equation (6), equation (7) is an empirical solution for the problem that FP Finder and FN Finder might be designed so poor that the establishment condition cannot be always satisfied just like our experiments (section 5.2). In section 3.2.3, why this condition is needed is detailed. Algorithm 1 is the format pseudocode of Boost Picking in practice."}, {"heading": "3.2. Establishment Condition", "text": "This subsection drives the establishment condition of Boost Picking. It includes:\n\u2022 Prove that Boost Picking could train supervised models mainly by un-labeled data as effectively as same models trained by all labeled data, only if the eigenvalues of the transformation matrix (defined in section 2) are all smaller than one.\n\u2022 Derive that Boost Picking would work effectively only if sum of two weak classifiers precisions is greater than one and recalls are all greater than zero.\n\u2022 Discuss the instantaneous situation."}, {"heading": "3.2.1. CONVERGENCE DERIVATION", "text": "The conclusion in prior work (section 2) is established based on an assumption that the trained model could perfectly classify the data in training set. Under most conditions, this assumption cannot be established. However, when Boost Picking aims to train f \u2192 F by Xl, Ll and\nXu, the assumption can be looser. Here, the conclusion that f \u2192 F only if the eigenvalues of the transformation matrix M are smaller than one is derived.\nWhat should be paid attention is that both f and F are a same model and all their parameters are same. The only difference between f and F is the training data set (section 3.1). Define XT is the data set where examples in X100% can be correctly classified by F , and XF is the data set where examples cannot be correctly classified by F ,{\nXT = {x \u2208 X100%|F (x) = l} XF = {x \u2208 X100%|F (x) 6= l}\n(8)\nwhere l is the true label of x. Define 4e as the difference between f and F , and4e is valued by the number of examples whose classification results by f and F are different.{\n4ei = |f(xi)\u2212 F (xi)|, xi \u2208 X100% 4e = \u2211 4ei/2N\n(9)\nwhere N is the size of X100%.\n4e can be reconstructed as\n4 e = d+ b (10)\nwhere di = |f(xi) \u2212 F (xi)|, xi \u2208 XT , d = \u2211 di/2N\nand bi = |f(xi) \u2212 F (xi)|, xi \u2208 XF , b = \u2211 bi/2N . d means the number of examples that can be correctly classified by F but cannot be correctly classified by f . b means the number of examples that can be correctly classified by f but cannot be correctly classified by F . Define biasi = |li \u2212 F (xi)|, xi \u2208 X100% and bias =\u2211 biasi/2N . bias represents the bias between the labels estimated by F and the true labels. When xi \u2208 XT ,biasi = 0. In contrast, when xi \u2208 XF ,biasi = 2.\nbi = 0 when xi \u2208 XF \u2229 f(xi) 6= li and bi = 2 when xi \u2208 XF \u2229 f(xi) = li, as a result, b = \u2211 |l \u2212 F (x)|/2N, x \u2208 XF \u2229 f(x) = l. Based on the above conclusion, bias can be reconstructed as equation (11).\nbias = b+ n (11)\nwhere ni = |l \u2212 F (xi)|, xi \u2208 XF \u2229 f(xi) 6= l and n =\u2211 ni/2N .\nGenerally, bias can be reduced by increasing the complexity of models, while it will be almost constant if the model is determined (including the model\u2019s parameters). Define ei = |l \u2212 f(xi)|, xi \u2208 X100% and e = \u2211 ei/2N . e represents the \u201derror\u201d between the labels estimated by f and the true labels.\ne = bias+4e\u2212 2b = d+ bias\u2212 b \u2264 d+ bias (12)\nTable 1 shows the relationship among 4ei, di, bi, biasi ni and ei. Take bi for instance, bi appears in (f(xi) = li, F (xi) 6= li), and this means that \u2200xi \u2208 X100%, if f(xi) = li&F (xi) 6= li, bi = 2. As for the quadrants where bi does not exit, bi is not defined or equal to zero. 4ei, di, biasi ni and ei are similar. From Table 1, equation (10), (11) and (12) can be easily derived.\nAs above mentioned, b represents the improvement between the performance of f and the performance of F in training set X100%. b might be the cost to make the model F avoid being over-fitting. However, because f and F are a same model (b is relatively small) and f is un-fitting in early iteration, we temporarily ignore the negative impact of b on the generalization and regard b is helpful for model improvement. Furthermore, even if b harms the generalization, section 4 details how to improve the generalization of the supervised learning models using Boost Picking.\nAs for d, it can be proved that d\u2192 0 as section 2, because xi can be perfectly classified (F (xi) = li) when xi \u2208 XT .\n\u2212\u2192 d (k + 1) =M \u2212\u2192 d (k) (13)\nd \u2192 0 when the eigenvalues of transformation matrix M are smaller than 1.\nAs a result, if b = 0, d\u2192 0,4e = 0, and f \u2192 F ; if b 6= 0, d \u2192 0,4e = b, and f is better than F , as least in training set. This part gets the conclusion that f could be trained by Xl, Ll and Xu as effectively as F trained by X100% and L100%, only if eigenvalues of M are smaller than one."}, {"heading": "3.2.2. FP FINDER AND FN FINDER DESIGN", "text": "This part theoretically analyzes how to design FN Finder and FP Finder to make sure eigenvalues of M are all smaller than one and then f \u2192 F . Assuming \u03bb1, \u03bb2 are eigenvalues of transformation matrix M , the expression of the two eigenvalues are as equation (14).\n\u03bb1, \u03bb2 = (2\u2212R\u2212 \u2212R+)\u00b1\n\u221a 4\n2 (14)\nwhere P+, R+, P\u2212, R\u2212 \u2208 [0, 1] (defined in section 2), and\n4 = (R+ \u2212R\u2212)2 + 4(1\u2212 P +)(1\u2212 P\u2212) P+P\u2212 R+R\u2212 (15)\n4 \u2265 0 is consistently satisfied because (R+ \u2212 R\u2212)2 \u2265 0 and (1\u2212P\n+)(1\u2212P\u2212) P+P\u2212 R +R\u2212 \u2265 0. 4 = 0 under the condition\nthat R+ = R\u2212&&(P+ = 1||P\u2212 = 1) or R+ = R\u2212 = 0. When the above condition is established, \u03bb1 = \u03bb2.\nAssuming \u03bb2 \u2264 \u03bb1, because 2 \u2212 R\u2212 \u2212 R+ \u2265 0, |\u03bb2| \u2264 |\u03bb1|. As section 3.2.1 referred, (|\u03bb1|, |\u03bb2|) < 1 leads to d \u2192 0. \u03bb1 \u2265 0 is consistently since \u221a 4 \u2265 0. If \u03bb1 < 1, |\u03bb2| \u2264 |\u03bb1| < 1, and d\u2192 0.\nIf \u03bb1 < 1, (2\u2212R\u2212\u2212R+)2+\n\u221a 4\n2 < 1, and derive\u221a 4 < R\u2212 +R+ (16)\nSimplify equation (16) and obtain\n1\u2212 P+ \u2212 P\u2212\nP+P\u2212 R+R\u2212 < 0 (17)\nBased on equation (17), FP Finder and FN Finder should be carefully designed as equation (18). Then eigenvalues of M are all smaller than one and d\u2192 0 is guaranteed.{\nR+R\u2212 6= 0 P+ + P\u2212 > 1\n(18)\nFig 2 shows the relationship between the maximal eigenvalue of M and precisions of the two classifiers when R+ = R\u2212. The red line is P+ + P\u2212 = 1, and the maximal eigenvalue in the red line is definitely equal to one. When P+ + P\u2212 > 1, the max eigenvalue is smaller than one. Fig 3 shows that no matter what the recalls of the two classifiers are, the maximal eigenvalue is definitely equal to one when P+ + P\u2212 = 1. And the maximal eigenvalue is smaller than one when P+ + P\u2212 > 1.\nAs a result, the sum of precisions of FP Finder and FN Finder determines whether Boost Picking can train f as effectively as F . Under the condition that recalls of the two\nweak classifiers are neither equal to zero, the differences between f and F will drop to zero in the iteration process only if P+ + P\u2212 > 1.\nHere, based on the above theoretical anlysis and Fig 2 3, it is concluded that if recalls of the two weak classifiers are both greater than zero and sum of precisions is greater than one, Boost Picking could train an effective supervised classification models mainly by un-labeled data as effectively as the same model trained by all labeled data."}, {"heading": "3.2.3. INSTANTANEOUS SITUATION", "text": "In section 3.2.2, P+,P\u2212,R+,R\u2212 are constant, while the precisions and recalls are usually varied with the iteration processing in practice. Assuming that P+(k),P\u2212(k),R+(k),R\u2212(k) are the instantaneous values of the precisions and recalls, expression of P+ is written as equation (19).\nP+ = 1\nT T\u2211 k=1 P+(k) (19)\nwhere T is the generation of the iteration process. P\u2212,R+,R\u2212 are defined similarly. The ideal condition is that FP Finder and FN Finder satisfy equation (19) in every generation, {\nR+(k)R\u2212(k) 6= 0 P+(k) + P\u2212(k) > 1\n(20)\nEquation (20) is relatively strict compared to equation (18). In practice, P+(k),P\u2212(k),R+(k),R\u2212(k) are likely to be choppy, and d is choppily decreasing. If k \u2208 [K,T ], P+(k)+P\u2212(k) > 1 and T\u2212K is relatively long enough, it\nwould achieve d\u2192 0 and f \u2192 F . Because of the shock of P+ + P\u2212, equation (7) is another convergence condition to guarantee that d converges into zero rather than other values. Adopting random selection in equation (7) aims to avoid being over-fitting."}, {"heading": "3.3. Performance Analysis", "text": "This section discusses how to design FP Finder and FN Finder to ensure Boost Picking works efficiently. Two aspects are involved in the discussion. The first one is focused on the whole iteration process and discusses the shock. And the second one is focused on single iteration and states how to reduce d as much as possible in one generation."}, {"heading": "3.3.1. EFFICIENCY IMPROVEMENT FROM GLOBAL ASPECT", "text": "According to section 3.2.3, d would be choppy because P+(k) + P\u2212(k) might not be always greater than one. Define K+ = {k|P+(k) + P\u2212(k) \u2264 1}, and K\u2212 = {k|P+(k) + P\u2212(k) > 1}. Assuming 4d(k) is the increment of d in kth iteration, if d\u2192 0,\n\u2211 k\u2208K\u2212 |4d(k)|\u2212\u2211\nk\u2208K+ | 4 d(k)| \u2265 \u2212\u2192 d (0) . As a result, large K+ has an obvious negative impact on reducing the generation of convergence. Thus, FP Finder and FN Finder should satisfy equation (20) as much as they could."}, {"heading": "3.3.2. EFFICIENCY IMPROVEMENT FROM ONE GENERATION", "text": "Based on the theory of dynamical system (Ogata, 2001), d converges to zero more quickly if absolute values of the eigenvalues are smaller. There is a negative correlation between the speed of error dropping and (|\u03bb1|, |\u03bb2|). It can be written as equation (21).\n4 d \u221d (\u2212|\u03bb1|,\u2212|\u03bb2|) (21)\nThe way to make d converge to zero quickly is to design |\u03bb1|,|\u03bb2| as small as possible. Supposing |\u03bb1|+ |\u03bb2| as the cost function, the optimal FN Finder and FP Finder should be designed as equation (22).\nR+ \u2217 , R\u2212 \u2217 , P+ \u2217 , P\u2212 \u2217 = arg min\nR+,R\u2212,P+,P\u2212 |\u03bb1|+ |\u03bb2|\ns.t.P+ + P\u2212 > 1, 0 < R+, R\u2212, P+, P\u2212 \u2264 1 (22)\nSupposing \u03bb2 \u2264 \u03bb1, \u03bb1 \u2265 0 based on equation (14). If \u03bb1\u03bb2 < 0, \u03bb2 < 0, and the determinate of M is smaller than 0.\n|M | = 1\u2212 (R+ +R\u2212) + (P + + P\u2212)\u2212 1 P+P\u2212 R+R\u2212 < 0\n(23)\nAssuming C(R+, R\u2212) = |M |, C < 0 \u2192 \u03bb2 < 0 and C \u2265 0 \u2192 (\u03bb1, \u03bb2 \u2265 0) (shown in Fig 4(a)). When\n(R+, R\u2212) = (1,0) or (0,1), C(R+, R\u2212) is definitely equal to zero. No matter what P+, P\u2212 is, all curves C = 0 go through (0,1) and (1,0) (shown in Fig 4(b-d)). Larger P+ or P\u2212 makes \u03bb2 more likely to be greater than zero, because C = 0 is closer to (1,1) when P+ or P\u2212 is larger and that means more area where C > 0 (shown in Fig 4(b,c)); the smaller |P+ \u2212 P\u2212| is, the less likely (\u03bb1\u03bb2 > 0) (shown in Fig 4(d)).\nBased on equation (14) and relationship between \u03bb1\u03bb2 and C(R+, R\u2212),\n|\u03bb1|+ |\u03bb2| =\n{ 2\u2212R+ \u2212R\u2212,C(R+, R\u2212) \u2265 0\u221a\n4,C(R+, R\u2212) < 0 (24)\nWhen C(R+, R\u2212) \u2265 0, |\u03bb1| + |\u03bb2| will decrease if R+ + R\u2212 increases. And if R+ + R\u2212 is constant, the efficiency of Boost Picking is similar (In Fig 5, contour lines are in planes R+ + R\u2212 = P , when C(R+, R\u2212) > 0). When C(R+, R\u2212) < 0, |\u03bb1| + |\u03bb2| = \u221a 4 that is an increasing function for bothR+ andR\u2212. And ifR++R\u2212 is constant, the smaller |R+\u2212R\u2212| is, the smaller |\u03bb1|+|\u03bb2| is (In Fig 5, the contours bulge to (1,1)). Consequently, if P+ + P\u2212 > 1, the optimal design of FN Finder and FP Finder is R+ = R\u2212 \u2229 C(R+, R\u2212) = 0. Especially, when P+ = P\u2212, the best design is R+ = R\u2212 = P+ = P\u2212 and the efficiency of Boost Picking is positively correlation to the precisions.\nIt should be noted that for certain P+ and P\u2212, R+ and R\u2212 are not as larger as better. If P+, P\u2212 are larger, the C(R+, R\u2212) (purple line in Fig5) would shift close to (1,1), which means R+, R\u2212 can be larger to reduce |\u03bb1|+ |\u03bb2|.\nCombining section(3.2.2), the above theoretical analysis and Fig 5, we concluded that\n\u2022 Precisions of FP Finder and FN Finder determine the upper boundary of the performance of Boost Picking, including effectiveness and efficiency.\n\u2022 If recalls of FP Finder and FN Finder are relatively high, it is better to adjust the balance of the two Finders or improve the precisions to make Boost Picking more efficient.\n\u2022 If the recalls are not ideal, the best way to improve Boost Picking performance is to improve the recalls, rather than keep balance or improve the precisions."}, {"heading": "4. Test along with Training", "text": "This section states a method to improve the generalization of the supervised classification models by Boost Picking. Because boost picking can use un-labeled data to train the supervised classification models, before classifying the unlabeled data in test set, use these un-labeled data to train the model. In fact, these data are classified during the Boost Picking process. As a result, this methods is named \u201dTest along with Training (TawT)\u201d.\nFig 6 shows the concept of TawT. In traditional process, supervised learning models are trained by labeled data in the training set and classify the un-labeled data in test set. As for Boost Picking, the training set involves un-labeled data.\nBased on Boost Picking, TawT cuts across the boundary between training set and test set to improve the generalization.\nBased on equation (12), when d \u2192 0, relationship among \u201dbias\u201d, \u201dvariance\u201d and \u201derror\u201d (Geman et al., 1992; James, 2003) could be simplified as Fig 7. In Fig 7, e (defined as equation (12)) represents the \u201derror\u201d between the labels estimated by f and the true labels. It should be noted that left \u201dbias\u201d and right \u201dbias\u201d are some different. The \u201dnew training set\u201d used to compute right \u201dbias\u201d is an assumed set. This set includes the training set used to compute left \u201dbias\u201d and the test set with its assumed true label set. In theory, TawT could eliminate the influence of variance and improve the performance of supervised models (equation (12) and Fig 7), which is tested in section 5."}, {"heading": "5. Experiment", "text": "This section details experiments to verify functions of Boost Picking and TawT. Logistic regression(lr), bpneural network(nn), support vector machine(svm) and adaboost(ada) are involved in these experiments. For certain model, parameters are same among tradition supervised classification, Boost Picking and TawT. Two data sets, heart scale and wine, are used. For one data set X with its label set L, it is separated into two part, Xrest and Xtest. Xrest/Xtest means the ratio of number of examples and its results is 3/1. Xtest is the constant test data set for all experiments. And for all experiments, the measurement of model performance is f1 = 2\u2217Precision\u2217RecallPrecision+Recall ."}, {"heading": "5.1. Baseline", "text": "This subsection illustrates that only small amount of labeled data cannot train an effective supervised model. Classical models are trained by different training sets Xtrain with different scales and the trained models are test in Xtest. Xtrain \u2282 Xrest and Xtrain/Xrest ranges from 0.1 to 1. Fig 9 shows the experiment. The horizontal axis represents Xtrain/Xrest. Furthermore, this experiment also determines the baseline or goal for Boost Picking. According to Fig 9, generally, larger Xtrain/Xrest leads to larger f1 in Xtest. We choose the models trained by whole Xrest as baselines."}, {"heading": "5.2. FP Finder and FN Finder in Experiments", "text": "Here, a very poor FP Finder and FN Finder are designed to verify equation (18). Compute the center of positive out-\nputs and the examples that are the farthest 20% from the center are regarded as false positives. FP is designed as the above statement and FN is designed similarly. This two classifiers are very weak and their performances wave. In these experiments, the average sum of precisions is 1.06 and the average sum of recalls is 0.12."}, {"heading": "5.3. Verification", "text": "This subsection tests the functions of Boost Picking and TawT in practice, whose results are shown as Fig 8. For Boost Picking experiments,Xrest are all used to train models but only examples from Xlabel have labels. Xlabel \u2282 Xrest and Xlabel/Xrest ranges from 5% to 50%. As for TawT experiments, all X including Xtest is used to train models and only labels of Xlabel can be used. Corresponding Boost Picking experiments, the index is also Xlabel/Xrest.\nGenerally, when Xlabel/Xrest is more than 0.25, models can be trained close to or super than the corresponding baseline (shown in Fig 8). As a result, using Boost Picking can train a supervised classification model by 75% unlabeled data as well as the same model trained by 100% labeled data. When using supervised classification models as TawT, performances of these models are improved. In Fig 8, the green line (TawT result) is usually higher than the blue line (Boost Picking without test set).\nIn these experiments, the convergence generation is long because of the two poorly designed classifiers. To improve the efficiency of Boost Picking, the two classifiers should be designed as the guide in section 3.3."}], "references": [{"title": "Beat the machine: Challenging workers to find the unknown unknowns", "author": ["Attenberg", "Josh", "Ipeirotis", "Panagiotis G", "Provost", "Foster J"], "venue": "Human Computation,", "citeRegEx": "Attenberg et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Attenberg et al\\.", "year": 2011}, {"title": "Self-trained lmt for semisupervised learning", "author": ["Fazakis", "Nikos", "Karlos", "Stamatis", "Kotsiantis", "Sotiris", "Sgarbas", "Kyriakos"], "venue": "Computational Intelligence and Neuroscience,", "citeRegEx": "Fazakis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fazakis et al\\.", "year": 2015}, {"title": "Imagenet: crowdsourcing, benchmarking & other cool things", "author": ["L. Fei-Fei"], "venue": "In CMU VASC Seminar,", "citeRegEx": "Fei.Fei,? \\Q2010\\E", "shortCiteRegEx": "Fei.Fei", "year": 2010}, {"title": "Neural networks and the bias/variance dilemma", "author": ["Geman", "Stuart", "Bienenstock", "Elie", "Doursat", "Ren\u00e9"], "venue": "Neural computation,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Applied logistic regression", "author": ["Hosmer Jr.", "David W", "Lemeshow", "Stanley"], "venue": null, "citeRegEx": "Jr et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jr et al\\.", "year": 2004}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "Learned-Miller", "Erik"], "venue": "Technical Report 07-49,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Variance and bias for general loss functions", "author": ["James", "Gareth M"], "venue": "Machine Learning,", "citeRegEx": "James and M.,? \\Q2003\\E", "shortCiteRegEx": "James and M.", "year": 2003}, {"title": "Pn learning: Bootstrapping binary classifiers by structural constraints", "author": ["Kalal", "Zdenek", "Matas", "Jiri", "Mikolajczyk", "Krystian"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kalal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalal et al\\.", "year": 2010}, {"title": "Tracking-learning-detection. Pattern Analysis and Machine Intelligence", "author": ["Kalal", "Zdenek", "Mikolajczyk", "Krystian", "Matas", "Jiri"], "venue": "IEEE Transactions on,", "citeRegEx": "Kalal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalal et al\\.", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Foundations of machine learning", "author": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Modern control engineering", "author": ["Ogata", "Katsuhiko"], "venue": "Prentice Hall PTR,", "citeRegEx": "Ogata and Katsuhiko.,? \\Q2001\\E", "shortCiteRegEx": "Ogata and Katsuhiko.", "year": 2001}, {"title": "Soft margins for adaboost", "author": ["R\u00e4tsch", "Gunnar", "Onoda", "Takashi", "M\u00fcller", "K-R"], "venue": "Machine learning,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2001\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2001}, {"title": "Least squares support vector machine classifiers", "author": ["Suykens", "Johan AK", "Vandewalle", "Joos"], "venue": "Neural processing letters,", "citeRegEx": "Suykens et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Suykens et al\\.", "year": 1999}, {"title": "Semi-supervised learning literature survey", "author": ["Zhu", "Xiaojin"], "venue": null, "citeRegEx": "Zhu and Xiaojin.,? \\Q2005\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2005}, {"title": "Introduction to semi-supervised learning", "author": ["Zhu", "Xiaojin", "Goldberg", "Andrew B"], "venue": "Synthesis lectures on artificial intelligence and machine learning,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "Supervised learning is the machine learning task of inferring a function from labeled training data (Mohri et al., 2012).", "startOffset": 100, "endOffset": 120}, {"referenceID": 5, "context": "The achievements of supervised learning rely on labeled data bases, like LFW (Huang et al., 2007), ImageNet (Fei-Fei, 2010) and Microsoft COCO (Lin et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 2, "context": ", 2007), ImageNet (Fei-Fei, 2010) and Microsoft COCO (Lin et al.", "startOffset": 18, "endOffset": 33}, {"referenceID": 9, "context": ", 2007), ImageNet (Fei-Fei, 2010) and Microsoft COCO (Lin et al., 2014).", "startOffset": 53, "endOffset": 71}, {"referenceID": 0, "context": "Humans academically called \u201dcrowds\u201d (Attenberg et al., 2011; Burton et al., 2012), who label the data in practice, greatly contribute to the development of machine learning.", "startOffset": 36, "endOffset": 81}, {"referenceID": 8, "context": "The structure of Boost Picking refers to \u201dP-N Learning\u201d in \u201dTracking-Learning-Detection\u201d (Kalal et al., 2012).", "startOffset": 89, "endOffset": 109}, {"referenceID": 1, "context": "Different from previous semi-supervised learning works like (Zhu, 2005; Zhu & Goldberg, 2009; Fazakis et al., 2015), Boost Picking, a universal method on training traditional supervised models using un-labeled data, adopts no internal change and has no limitation on the supervised models.", "startOffset": 60, "endOffset": 115}, {"referenceID": 12, "context": "Logistic regression (Hosmer Jr & Lemeshow, 2004), neural network (Haykin & Network, 2004), support vector machine (SVM) (Suykens & Vandewalle, 1999) and Adaboost (R\u00e4tsch et al., 2001) are all tested with varied data sets in experiments (section 5).", "startOffset": 162, "endOffset": 183}, {"referenceID": 8, "context": "In TLD (Kalal et al., 2012), P-N Learning is an online learning method to learn the new form of the tracked object.", "startOffset": 7, "endOffset": 27}, {"referenceID": 7, "context": "Kalal theoretically proved that P-N Learning could train a detector whose error converges into zero (Kalal et al., 2010; 2012).", "startOffset": 100, "endOffset": 126}, {"referenceID": 8, "context": "The recursive equations are as equation (2) (Kalal et al., 2012).", "startOffset": 44, "endOffset": 64}, {"referenceID": 7, "context": "Kalal concludes that d will converge to zero only if eigenvalues of the transformation matrix M are all smaller than one (Kalal et al., 2010).", "startOffset": 121, "endOffset": 141}, {"referenceID": 3, "context": "Based on equation (12), when d \u2192 0, relationship among \u201dbias\u201d, \u201dvariance\u201d and \u201derror\u201d (Geman et al., 1992; James, 2003) could be simplified as Fig 7.", "startOffset": 86, "endOffset": 119}], "year": 2017, "abstractText": "This paper proposes a universal method, Boost Picking, to train supervised classification models mainly by un-labeled data. Boost Picking only adopts two weak classifiers to estimate and correct the error. It is theoretically proved that Boost Picking could train a supervised model mainly by un-labeled data as effectively as the same model trained by 100% labeled data, only if recalls of the two weak classifiers are all greater than zero and the sum of precisions is greater than one. Based on Boost Picking, we present \u201dTest along with Training (TawT)\u201d to improve the generalization of supervised models. Both Boost Picking and TawT are successfully tested in varied little data sets.", "creator": "LaTeX with hyperref package"}}}