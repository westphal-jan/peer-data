{"id": "1602.05682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Audio Recording Device Identification Based on Deep Learning", "abstract": "avellone In this paper surfwatch we present lgv a 28-32 study on fighters identification distributable of mueller-stahl audio 4kids recording devices from background noise, aami thus ragini providing a ternana method westcountry for forensics photosynthetically and copyright slader disputes related. affluenza The naeim audio crotone signal waat is oshiomole the haeberlin sum 55-11-30233331 of lacker speech abelisaurids signal osbi and 1,242 noise zibel signal. thence Since comenius usually, it ' amiraults s the speech gilsland which espaillat is forthe regarded keizer as the nets information truthiness to be foreningen passed acknowledged that people care padmini about, a dfe great amount shibito of researches have been vanuatu dedicated in-order to getting higher SNR. odiham So there are many speech enhancement 576 algorithms wherewith to frapp\u00e9 improve the grrrr quality of tengiz the speech signals, 4,436 which is 1958/59 generally hotze can be seen leaf much tmc like oq reducing ruengroj the dsrna noise. prophetically However, betsky we t\u1eed make the underlying hypothesis that piercer the noise can be oracion regarded sevastyanov as the jocara intrinsic fingerprint traces bucholz of an unferth audio recording xanthippe device 60-gun in atmatzidis a way. These reverby digital traces kowit can be characterized and identified by guardhouse new crosscourt machine learning techniques. Therefore, rial the noise al-ghazi can serve as the intrinsic features for the 202-887-8307 identification. As 78.6 for marmalade the 77th classifier, chopp a sar101 Softmax classifier mohs and an kuzmuk MLP 1.091 classifier doeschate are used inadequacies and multi-media compared. The identification carnivorans accuracy priscu is moodiness up wooden-hulled to euro118 93 \\% innaloo among cephalus nine different panforte devices, hamoed and ourso shows boys-only the method boonies of getting feature inori vector from the glucosidase noise stoeckl of each device and relativamente identifying sabaragamuwa with deeplearning 45-centimeter techniques is kamke viable, tuivaga and afato well - zyman preformed.", "histories": [["v1", "Thu, 18 Feb 2016 05:49:37 GMT  (191kb,D)", "https://arxiv.org/abs/1602.05682v1", null], ["v2", "Wed, 27 Apr 2016 02:32:38 GMT  (692kb)", "http://arxiv.org/abs/1602.05682v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["simeng qi", "zheng huang", "yan li", "shaopei shi"], "accepted": false, "id": "1602.05682"}, "pdf": {"name": "1602.05682.pdf", "metadata": {"source": "CRF", "title": "Audio Recording Device Identification Based on Deep Learning", "authors": ["Simeng Qi", "Zheng Huang", "Yan Li", "Shaopei Shi"], "emails": ["huang-zheng}@sjtu.edu.cn", "shisp}@ssfjd.cn"], "sections": [{"heading": null, "text": "of audio recording devices from background noise, thus providing a method for forensics. The audio signal is the sum of speech signal and noise signal. Usually, people pay more attention to speech signal, because it carries the information to deliver. So a great amount of researches have been dedicated to getting higher Signal-Noise-Ratio (SNR). There are many speech enhancement algorithms to improve the quality of the speech, which can be seen as reducing the noise. However, noises can be regarded as the intrinsic fingerprint traces of an audio recording device. These digital traces can be characterized and identified by new machine learning techniques. Therefore, in our research, we use the noise as the intrinsic features. As for the identification, multiple classifiers of deep learning methods are used and compared. The identification result shows that the method of getting feature vector from the noise of each device and identifying them with deep learning techniques is viable, and well-preformed.\nKeywords-audio forensic; device detection; deep learning\nI. INTRODUCTION\nThe identification of audio recording devices has numerous applications. In criminology and forensics, determining the audio recording device can help determining whether a certain record is from a proper device and thus determining its validity. In copyright disputes, finding out the actual ownership of a certain record may help deal with multiple claims of ownership. Also, if different pieces of one record show different recording devices, we can infer that the record may have been modified. However, at the same time, the advent of modern digital era adds up the difficulty and complexity to the identification. Thus, demand for efficient methods to assure the authenticity of audio signal is becoming more and more important.\nMany people have done the identification of audio recording devices for numerous proposes in numerous conditions. Luca Cuccovillo et al. [1] used microphone classification to perform audio tampering detection, and the underlying algorithm was based on blind channel estimation and applied to detect a specific type of tampering. Constantine Kotropoulos et al. [2] performed research on mobile phone identification using recorded speech signals, and they used Mel frequency cepstral coefficients extracted from recorded speech signals to train a Gaussian Mixture Model with diagonal covariance matrices, thus providing\ntemplates for each device. Also, Ling Zou et al. [3] had similar ideas and utilized Gaussian mixture model-universal background model as the classifier, and showed that Mel frequency cepstral coefficients are more effective than Power-normalized cepstral coefficients. Xavier Valero et al. [4] compared Gammatone cepstral coefficients to Mel frequency cepstral coefficients in non-speech audio classification and found the GTCC more effective than MFCC, especially at low frequencies. Some people focus on the identification method themselves. Daniel Garcia-Romero et al. [5] proposed a method of automatic identification of acquisition devices when only get access to the output speech recordings, which used a support vector machine classifier to perform closed-set identification experiments and focused on two classes of acquisition devices. Robert Buchholz et al. [6] extracted a Fourier coefficient histogram of near-silence segments of the recording as the feature vector and used machine learning techniques for the classification. As for the features used to perform classification, people have different ideas. Yannis Panagakis et al. [7] chose random spectral features extracted from each speech signal as an intrinsic fingerprint for device identification, and Constantine Kotropoulos [8] chose the sketches of spectral features. Many other people chose the background noise of the audio recording devices to be the feature. Sohaib Ikram et al. [9] had a great idea about leakage signal, which is actually in the removed noise from speech enhancement, and we find the idea really inspiring. Huy Quan Vu et al. [10] identified microphone from noisy recordings by using representative instance One ClassClassification approach, and proposed a representative instance classification framework to improve performance of OCC algorithms. Chang-Bae Moon et al. [11] proposed an audio recorder identification method as one of digital forensic technologies, as well as a new feature reduction method, where Wiener filter was used to extract noise sounds of recorders and their features were extracted by MIRtoolbox. Rachit Aggarwal et al. [12] used features based on estimates of noise associated with recordings and classified them using sequential minimal optimization based Support Vector Machine. In this paper, we choose background noise as the feature, use classifiers of deep learning methods, improve former methods with new ideas and experiments, and show a pretty satisfying result.\nSince in most cases, only the recorded audio signals are accessible, the identification should be based totally on the recorded audio signals themselves. This fact makes the problem pretty challenging since the audio signals we can get contain two parts: the speech signals and the noise signals, and the speech signals have their own variability based on the content. Usually, it's the speech which is regarded as the information to be passed that people mainly care about, and a great amount of researches have been dedicated to getting higher SNR. There are many speech enhancement algorithms to improve the quality of the speech signals, which can be seen as reducing the noise. However, noises can be regarded as the intrinsic fingerprint traces of an audio recording device. These digital traces can be characterized and identified by new machine learning techniques. Therefore, the noise can serve as the intrinsic features for the identification.\nDeep learning is a set of algorithms in machine learning. It attempts to model high-level abstractions in data by using model architectures, which are composed of multiple nonlinear transformations. Softmax regression is an important method in deep learning area for multi-class classification. The Softmax regression model generalizes logistic regression to classification problems where the class label can take on more than two possible values. This will be useful for problems where the goal is to distinguish between multiple outputs, in our case, multiple audio recording devices. Multilayer perceptron (MLP) is also appropriate for our situation. It is a modification of the standard linear perceptron and can distinguish data that are not linearly separable. With a feedforward artificial neural network model, MLP maps input data sets onto appropriate output set. It uses a backpropagation algorithm, and turns out to be a pretty proper algorithm for any supervised learning pattern recognition process.\nThus we present a research on identification of audio recording devices from background noise in the audio signals. Multiple classifiers of deep learning methods are used and compared to perform identifications. Furthermore, we also perform several enhanced methods such as model averaging and voting model to get better results.\nThe remaining paper is structured as follows: In Section II we introduce the audio files collection and the preprocessing of the dataset. Section III presents the methodology, including the background noise extraction, feature extraction, and classifiers. The whole experiment processing, as well as the results and corresponding analysis are shown in Section IV and we get the conclusion in Section V.\nII. DATASET\nWe use nine devices to record audio signals and the classification is among these nine categories. The devices are listed as Table I.\nFor each device, we generate three recordings and label them with 1, 2 and 3. Each recording\u2019s length varies from six minutes to seven minutes. For each audio file, we randomly split it and generate segments to be our dataset, with 4096 sample points in each miner segment. Among the three recordings for each equipment, the first two are used to generate the training set and each recording results in 1000 segments. The third file is used for the testing set, and each one generates 100 segments. The data instances extracted from the recordings are assigned with the label of the device by the file name. In this way, a total of 18000 identification trials for training and 900 for testing are obtained. Each of the segments will form data instances with feature vector.\nIII. BACKGROUND NOISE CLASSIFICATION METHODOLOGY\nThe overview of our experiment is shown in Fig. 1."}, {"heading": "A. Background Noise Extraction", "text": "As long as we have the audio segments, we need to extract the intrinsic characteristic of the device, which in our\ncase, is the noise of the signals. The input signal is a\npassively received audio signal which is the sum of speech\nsignal and noise signal. This can be expressed as:\n sn = fn + en  \nin which, time n is equally spaced. sn is the audio signal we get from former processing. fn is the speech signal, which is the information people usually care about. en is the noise signal that we need here to identify the device.\nA great amount of researches have been dedicated to getting higher SNR. So there are many speech enhancement algorithms to improve the quality of speech signals, which can be seen as reducing the noise. In our experiment, we get the de-noised version of the input signal, which is expressed in the equation as fn.\nWe use an automatic de-noising process of a 1-D signal using wavelets. The de-noise objective is to suppress the noise part of the signal e and to recover f.\nAfter doing the subtraction, we can obtain the noise signal en.\nFor the baseline, which is to generate features directly from the original audio signals, this part is unnecessary. We just assign sn to en."}, {"heading": "B. Feature Exaction", "text": "After obtaining the signal en, we need to turn them into feature vectors for the classification. Our feature extraction is based only on frequency domain features of a recording, which are mathematical representation reflecting characteristics of audio signal. To do so, we extract a Fourier coefficient histogram of the signal as the feature vector, which has powerful descriptive capability for audio signals. The corresponding Fourier coefficients for all those segments are summed up to yield a Fourier coefficient histogram that is then used as the global feature vector.\nThis process can be expressed as:\n F(en) = FFT(en)  \nAfter the Fast Fourier Transform, we normalize it as:\n N(F(en)) = log(F(en) + 1) \nIn this way, the segments of the speech recording are represented by a point in a high-dimensional vector space. The data after the transformation are saved as the dataset x. For training, x is a matrix of 2049*18000 and for testing, x is a matrix of 2049*900. These matrixes provide the features that we need. At the meantime, the file names of the audios are written into vector y and recorded in a map, which are also assigned with the label of the device by the file name."}, {"heading": "C. Classifiers", "text": "Deep learning is a set of algorithms in machine learning. It can model high-level abstractions in data by using model architectures, which are composed of multiple non-linear transformations. An observation, in our case, audio signal, can be represented as a vector, and thus can be processed by multiple standard algorithms.\nWe'll give a brief introduction to the methods we used in our experiments. These are not necessarily independent. To obtain a better solution, some mixed-up may get surprising result.\n1) Softmax: Softmax regression model is one of the\nmodels in the field of deep learning, which generalizes\nlogistic regression and is expanded to perform classification\namong more than two classes.\nFor a training set {(x (1) ,y (1) ),\u2026,(x (m) ,y (m) )}, where the\ninput feature xi  Rn+1. The difference from the logistic regression here is that instead of {0, 1}, we now have yi{1, 2,...,k} so that the label y can be set among k different value.\nFor a given test input x, we need our function to estimate\nthe probability that p(y=j|x) for j = 1...k, and thus to\ndetermine which label should y|x be. So, our hypothesis is to\noutput a k-dimensional vector, each element in the vector\npresents the estimated probability for one of the k possible\nvalues of yi, and these k probabilities should sum to 1. The hypothesis h\u03b8 can be described as:\n\n( ) ( )\n( ) ( )\n( )\n( ) ( )\n( 1| ; )\n( 2 | ; ) ( )\n...\n( | ; )\ni i\ni i\ni\ni i\np y x\np y x h x\np y k x\n\n\n\n\n              \nThe cost function is:\n\n( )\n( )\n( ) 2\n1 1 1 0 1\n1 ( ) [ 1{ }log\n2\nT i j\nT i l\nxm k k n i ijk x i j i j\nl\ne J y j\nm e\n\n\n  \n    \n      \nin which 1{*} is called indicator and can be calculated\nas 1{a true statement} = 1 and 1{a false statement} = 0. In\nthe cost function, the former part is the naive cost and the\nlater part is a weight decay term which is to penalize large\nvalues of the parameters. To get a working implementation\nof Softmax regression, we will minimize J\u03b8.\n2) MLP: Multilayer perceptron (MLP) is another model\nin the field of deep learning. It maps input data sets onto\noutput data sets. A MLP has multiple layers, where nodes\nconnecting to every node in the next layer and the last layer\nis the output we need. With these connection, MLP has the\nability to classify data which are not linearly separated.\nEach node in MLP, except for those in the input layer,\nhas a nonlinear activation function that maps the weighted\ninputs to the outputs. Apart from the input and output layers,\nMLP consists of one or more hidden layers. Since the\nactivation functions are not linear, these layers cannot be\nreduced to the standard two-layer input-output model. In\nthis way, it is considered a deep neural network.\nEach node in one layer connects to every node in the\nfollowing layer, with a certain weight. These weight will be\nchanged after each piece of data is processed, by\nminimizing the error, which is presented as:\n 21( ) ( ( ) ( ))\n2 i i\ni\nn d n y n    \nwhere di is the target value and yi is the produced value. In this way, MLP utilizes a supervised learning technique\ncalled backpropagation.\n3) CNN: In machine learning, a convolutional neural\nnetwork (CNN) is a type of feed-forward artificial neural\nnetwork. Convolutional networks were inspired by\nbiological processes and are variations of MLP designed to\nuse minimal amounts of preprocessing.\nCNN consists of multiple layers of small neuron\ncollections. These collections process portions of the input\ndata, and the outputs of these collections are then tiled so\nthat they overlap, to obtain a better representation of the\noriginal data. This is repeated for every such layer.\nCompared to other classification algorithms, CNNs use\nrelatively little pre-processing. This means that the network\nis responsible for learning the filters that in traditional\nalgorithms were hand-engineered. The lack of dependence\non prior knowledge and human effort in designing features\nis a major advantage for CNNs. Considering this superb\nfeature of CNN, we also performed several experiments\nwithout the noise-extraction using wavelets.\n4) Model Averaging: Since as a baseline, we put our\ngenerated data into several hidden layers all together, and\nthese data are actually not consecutive, it is reasonable to\nconsider putting them separately through separate hidden\nlayers, and then concatenating them all together for the\nsucceeding processes.\n5) Voting Model: One miner signal segment is easily to\nget biased randomly. So we consider to generate multiple\nsets of segments as input data for our classification. After all\nthe predictions, we sum the results up and let them vote for\nthe final classification result.\nIV. EXPERIMENT AND RESULT\nWe did several experiments to compare classifiers, and to compare parameters in one certain classifier. Open source code, with which the most important results and figures can be reproduced, is available at https://github.com/SMartQi/identification.\nTable II shows the comparison of Softmax classifier and MLP classifier, according to the processes shown in Fig. 2. This indicates that MLP beats Softmax classifier in audio device classification in this certain condition, and that three hidden layers seem to work better, but not necessarily more layers leads to better result.\nTable III shows the results without performing background noise extraction first before classification, according to the processes shown in Fig. 3. No matter which classifier is used, background noise signal shows better accuracy than the recording signal itself. Also, both Softmax and MLP have certain ability to extract features from the original recording signals automatically, and perform the\nclassification all by themselves. As we introduced, compared to other classification algorithms, CNNs use relatively little pre-processing. This means that CNN network is responsible for learning the filters that in traditional algorithms were hand-engineered, and does not care so much about whether the input data have noise mixed. Notice, the noise here is not the noise signals in the audio signals. On the contrary, features should be generated from our noise signals, and the noise here refers to the speech signal, which is not helpful to the classification.\nTable IV shows the comparison of whether to introduce model averaging, according to the process shown in Fig. 4. Unfortunately, the model averaging strategy does not have significant improvement.\nTable V shows the comparison of whether to introduce the voting model, according to the process shown in Fig. 5. In these experiments, we set three hidden layers. We can clearly see that more voters lead to higher accuracy. But notice that more voters also lead to more time to generate datasets and perform classification.\nTable VI shows the detailed result of MLP classification. The confusion matrix result is given as Fig. 6. Fig. 7 is the dimensionality-reduced result using t-distributed Stochastic Neighbor Embedding (t-SNE). We can clearly see that the classification result is pretty satisfying. One thing to notice is that the error is not evenly distributed among all devices. Device aigo and PHILIPS get pretty high recall, as well as good precision and f1-score. These are also shown in Fig. 6 with dark blocks and in Fig. 7 with clustered dots. It\u2019s relatively easy to confuse device Allbar and Shinco, as shown in Fig. 6 that the yellow block is relatively obvious for their two corresponding positions. Some mixed-up dots for them are shown in Fig. 7.\nV. CONCLUSION\nA research on identification of audio recording devices from background noise in the audio signals is presented. It showed the method of getting feature vectors from the noise of each device and identifying those using deep learning methods is viable, and well-preformed.\nAlthough pretty much work has been done, many issues in the field of identification of audio recording devices are still open. Some other topics are still promising. On the one hand, our underlying theory is that the input signal is a passively received audio signal which is the sum of speech signal and noise signal, and by doing the subtraction, we can get the noise signal; on the other hand, the current de-noise methods pay more attention to the quality of the speech, instead of the noise signal. Thus, there may be a bias between the real noise signal and the signal after the subtraction. More work can be done in the direction of getting purer noise signal."}, {"heading": "ACKNOWLEDGMENT", "text": "This work has been supported by the \u201c12th Five-Year Plan\u201d National Science and Technology Support Program under the grant 2012BAK16B05. The authors would like to\nthank our co-worker Jingyao Luo, who helped a lot during the collection of audio recording materials."}], "references": [{"title": "Mobile phone identification using recorded speech signals", "author": ["C. Kotropoulos", "S. Samaras"], "venue": "in: Digital Signal Processing (DSP), 2014 19th International Conference on, IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic cell phone recognition fromspeech recordings", "author": ["L. Zou", "J. Yang", "T. Huang"], "venue": "in: Signal and Information Processing (ChinaSIP), 2014 IEEE China Summit & International Conference on, IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Gammatone cepstral coefficients: biologically inspired features for non-speech audio classification", "author": ["X. Valero", "F. Alias"], "venue": "Multimedia, IEEE Transactions on 14 (6) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic acquisition device identification from speech recordings", "author": ["D. Garcia-Romero", "C.Y. Espy-Wilson"], "venue": "in: Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Microphone classification using fourier coefficients", "author": ["R. Buchholz", "C. Kraetzer", "J. Dittmann"], "venue": "in: Information Hiding, Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic telephone handset identification by sparse representation of random spectral features", "author": ["Y. Panagakis", "C. Kotropoulos"], "venue": "in: Proceedings of the on Multimedia and security, ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Telephone handset identification using sparse representations of spectral feature sketches", "author": ["C. Kotropoulos"], "venue": "in: Biometrics and Forensics (IWBF), 2013 International Workshop on, IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Digital audio forensics using background noise", "author": ["S. Ikram", "H. Malik"], "venue": "in: Multimedia and Expo (ICME), 2010 IEEE International Conference on, IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying microphone from noisy recordings by using representative instance one classclassification approach", "author": ["H.Q. Vu", "S. Liu", "X. Yang", "Z. Li", "Y. Ren"], "venue": "Journal of networks 7 (6) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio recorder identification using reduced noise features", "author": ["C.-B. Moon", "H. Kim", "B.M. Kim"], "venue": "in: Ubiquitous Information Technologies and Applications, Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Cellphone identification using noise estimates from recorded audio", "author": ["R. Aggarwal", "S. Singh", "A.K. Roul", "N. Khanna"], "venue": "in: Communications and Signal Processing (ICCSP), 2014 International Conference on, IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "[2] performed research on mobile phone identification using recorded speech signals, and they used Mel frequency cepstral coefficients extracted from recorded speech signals to train a Gaussian Mixture Model with diagonal covariance matrices, thus providing templates for each device.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] had similar ideas and utilized Gaussian mixture model-universal background model as the classifier, and showed that Mel frequency cepstral coefficients are more effective than Power-normalized cepstral coefficients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] compared Gammatone cepstral coefficients to Mel frequency cepstral coefficients in non-speech audio classification and found the GTCC more effective than MFCC, especially at low frequencies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] proposed a method of automatic identification of acquisition devices when only get access to the output speech recordings, which used a support vector machine classifier to perform closed-set identification experiments and focused on two classes of acquisition devices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] extracted a Fourier coefficient histogram of near-silence segments of the recording as the feature vector and used machine learning techniques for the classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] chose random spectral features extracted from each speech signal as an intrinsic fingerprint for device identification, and Constantine Kotropoulos [8] chose the sketches of spectral features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] chose random spectral features extracted from each speech signal as an intrinsic fingerprint for device identification, and Constantine Kotropoulos [8] chose the sketches of spectral features.", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "[9] had a great idea about leakage signal, which is actually in the removed noise from speech enhancement, and we find the idea really inspiring.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] identified microphone from noisy recordings by using representative instance One ClassClassification approach, and proposed a representative instance classification framework to improve performance of OCC algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] proposed an audio recorder identification method as one of digital forensic technologies, as well as a new feature reduction method, where Wiener filter was used to extract noise sounds of recorders and their features were extracted by MIRtoolbox.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] used features based on estimates of noise associated with recordings and classified them using sequential minimal optimization based Support Vector Machine.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In this paper we present a research on identification of audio recording devices from background noise, thus providing a method for forensics. The audio signal is the sum of speech signal and noise signal. Usually, people pay more attention to speech signal, because it carries the information to deliver. So a great amount of researches have been dedicated to getting higher Signal-Noise-Ratio (SNR). There are many speech enhancement algorithms to improve the quality of the speech, which can be seen as reducing the noise. However, noises can be regarded as the intrinsic fingerprint traces of an audio recording device. These digital traces can be characterized and identified by new machine learning techniques. Therefore, in our research, we use the noise as the intrinsic features. As for the identification, multiple classifiers of deep learning methods are used and compared. The identification result shows that the method of getting feature vector from the noise of each device and identifying them with deep learning techniques is viable, and well-preformed. Keywords-audio forensic; device detection; deep learning", "creator": "Microsoft\u00ae Office Word 2007"}}}