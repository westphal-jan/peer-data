{"id": "1602.06183", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2016", "title": "Node-By-Node Greedy Deep Learning for Interpretable Features", "abstract": "attini Multilayer asymmetrically networks fail-safe have canines seen dace a peregrym resurgence myrrha under 16:31 the cayo umbrella of stosur deep learning. vagos Current deep learning algorithms 618-907 train the kastan layers gaylords of ascended the network sequentially, gree improving gavras algorithmic performance zodiac as oslofjord well as providing some iriki regularization. cynthia We disengagement present a punk-rock new training aillagon algorithm kayan for deep 818,000 networks kristinehamn which trains \\ emph {each boishakh node eberron in the network} itihaad sequentially. blk Our algorithm begam is orders of reales magnitude faster, creates more stammler interpretable internal cruithne representations 933 at 1,500-mile the zanier node h2-a level, 1-point while not 1,210 sacrificing on the ultimate megalon out - synyster of - englewood sample performance.", "histories": [["v1", "Fri, 19 Feb 2016 15:36:38 GMT  (1172kb,D)", "http://arxiv.org/abs/1602.06183v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ke wu", "malik magdon-ismail"], "accepted": false, "id": "1602.06183"}, "pdf": {"name": "1602.06183.pdf", "metadata": {"source": "CRF", "title": "Node-By-Node Greedy Deep Learning for Interpretable Features", "authors": ["Ke Wu", "Malik Magdon-Ismail"], "emails": ["WUK3@RPI.EDU", "MAGDON@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "Multilayer neural networks have gone through ups and downs since their arrival in (Rosenblatt, 1958; Widrow, 1960; Hoff Jr, 1962). The resurgence in \u201cdeep\u201d networks is largely due to the efficient greedy layer by layer algorithms for training, that create meaningful hierarchical representations of the data. Particularly in the era of \u201cbig data\u201d from diverse applications, efficient training to create data representations that provide insight into the complex features captured by the neurons are important. We explore these two dimensions of training a deep network. Assume a standard machine learning from data setup (Abu-Mostafa et al., 2012), withN datapoints (x1, y1), . . . , (xN , yN ); xn \u2208 Rd and yn \u2208 {0, 1, . . . , c\u2212 1} (multi-class setting).\nDeep Network We refer to Abu-Mostafa et al. (2012, Chapter 7) for the basics of multilayer networks, including notation which we very quickly summarize here. On the right, we show a feedforward network architecture. Such a network is \u201cdeep\u201d because it has many ( 2) layers. We assume that a network architecture has been fixed. The network implements a function whereby in each layer (`), the output of the previous layer (` \u2212 1) is transformed into the output of the layer `\nLearn W(1) Learn W(2) Learn W(3) Fine tuning\nFigure 1. Layer-by-layer greedy deep learning algorithm.\nuntil one reaches the final layer on top, which is the output of the network. The function implemented by layer ` is\nx(`) = tanh(W(`)x(`\u22121)),\nwhere x(`) is the output of layer `, and the weight-matrix W(`) (of appropriate dimensions to map a vector from layer ` \u2212 1 to a vector in `) are parameters to be learned from data. The training phase uses the data to identify all the parameters {W(1),W(2), . . . ,W(L)} of the deep network.\nBackpropagation which trains all the weights simultaneously, allowing for maximum flexibility, was the popular approach to training a deep network (Rumelhart et al., 1986). The current approach is layer-by-layer: train the first layer weights W(1); then train the second layer weights W(2), keeping the first layer weights fixed; and so on until all the weights are learned. In practice, once all the weights have been learned in the greedy-layer-by-layer manner (often referred to as pre-training), the best results are obtained by fine tuning all the weights using a few iterations of backpropagation (Figure 1).\nHow one should train the internal layers? The two popular approaches are: (1) Each layer is an unsupervised nonlinear auto-encoder (Cottrell & Munro, 1988; Bengio et al., 2007); this approach is appealing to build meaningful hierarchical representations of the data in the internal layers. (2) Each layer is a supervised encoder; this approach primarly targets performance. Deep learning enjoys success in several applications and hence considerable effort\nar X\niv :1\n60 2.\n06 18\n3v 1\n[ cs\n.L G\n] 1\n9 Fe\nb 20\n16\nhas been expended in optimizing the pre-training. The two main considerations are:\n1. Pre-training time and training/test performance of the final solution. The greedy layer-by-layer pre-training is significantly more efficient than full backpropagation, and appears to be better at avoiding bad local minima (Erhan et al., 2010). Our algorithms will show an order of magnitude speed gain over greedy layer-by-layer pre-training.\n2. Interpretability of the feature representations in the internal layers. We use the USPS digits data (10 classes) as a strawman benchmark to illustrate our approach. The weights going into each node of the first layer identify the pixels contributing to the \u201chigh-level\u201d feature represented by that hidden node. Figure 2 compares our features with the layer-by-layer features in the unsupervised setting, and Figure 4 compares the features in the supervised setting. The layer-by-layer features do not capture the essence of the digits as do our features. This has to do with the simultaneous training of all the hidden nodes in the layer. Our approach can be viewed as a nonlinear extension of PCA, which \u201dgreedily\u201d constructs each linear features. (In the supplementary material, we show the features captured by linear PCA; they are comparable to our features.)\nGreedy Node-by-Node Pre-Training. The thrust of our approach is to learn the weights into each node in a sequential greedy manner: greedy-by-node (GN) for the unsupervised setting and greedy-by-class-by-node (GCN) for the supervised setting. Figure 3 illustrates the first 5 steps\nfor a network. Our approach mimics a human who hardly\nbuilds all features at once from all objects. Instead, features are built sequentially, while processing the data. Our algorithm learns one feature at a time, using a part of the data to learn each feature. Our contributions are\n1. The specific algorithm to train each internal node.\n2. How to select the training data for each internal node.\nWe do not improve the accuracy of deep learning. Rather, we improve efficiency and the interpretability of features, while maintaining accuracy. A standard deep learning algorithm uses every data point to process every weight in the network. Our algorithm uses only a subset of the data to process a particular weight. By training each node using \u201drelevant\u201d data, our algorithm produces more interpretable features. Our algorithm gets more intuitive features, in the unsupervised and supervised setting (Figures 2 and 4).\nRelated Work. To help motivate our approach, it helps to start back at the very beginning of neural networks, with Rosenblatt (1958) and Widrow (1960). They introduced the adaline, the adaptive linear (hard threshold element), and the combination of multiple elements came in Hoff Jr (1962), the madaline (the precursor to the multilayer perceptron). Things cooled off a little because fitting data with multiple hard threshold elements was a combinatorial nightmare. There is no doubt softening the hard threshold to a sigmoid and the arrival of a new efficient training algorithm, backpropagation (Rumelhart et al., 1986), was a huge part of the resurgence of neural networks in the 1980s/1990s. But, again, neural networks receded, taking a back seat to modern techniques like the support vector machine (Vapnik, 2000). In part, this was due to the facts that multilayer feedforward networks were still hard to train iteratively due to convergence issues and multiple local minima (Gori & Tesi, 1992; Fukumizu & Amari, 2000), are extremely powerful (Hornik et al., 1989) and easy to overfit to data. For these reasons, and despite the complexity theoretic advantages of deep networks (see for example the short discussion in (Bengio et al., 2007)), application of neural networks was limited mostly to shallow two layer networks. Multi-layer (deep) neural networks are back in the guise of deep learning/deep networks, and again because of a leap in the methods used to train the network (Hinton et al., 2006). In a nutshell, rather that address the full problem of learning the weights in the net-\nwork all at once, train each layer of the network sequentially. In so doing, training becomes manageable (Hinton et al., 2006; Bengio et al., 2007), the local minima problem when training a single layer is significantly diminished as compared to the whole network and the restriction to layer by layer learning reigns in the power of the network, helping with regularizing it(Erhan et al., 2010). A side benefit has also emerged, which is that each layer successively has the potential to learn hierarchical representations (Erhan et al., 2010; Lee et al., 2009a). As a result of these algorithmic advances, deep networks have found a host of modern applications, ranging from sentiment classification (Glorot et al., 2011), to audio (Lee et al., 2009b), to signal and information processing (Yu & Deng, 2011), to speech (Deng et al., 2013), and even to the unsupervised and transfer settings (Bengio, 2012). Optimization of such deep networks is also an active area, for example (Ngiam et al., 2011; Martens, 2010).\nMost work has focused on better representations of the input data. Besides the original deep belief network (Hinton et al., 2006) and autoencoder (Bengio et al., 2007), the stacked denoising autoencoder (Vincent et al., 2010; 2008) has been widely used as a variant to the classic autoencoder, where corrupted data is used in pre-training. Sparse encoding (Boureau et al., 2008; Poultney et al., 2006) has also been used to prevent the system from activating the same subset of nodes constantly (Poultney et al., 2006). This approach has been shown capable of learn-\ning local and meaningful features, however the efficiency is worse. For images, convolutional neural networks (LeCun & Bengio, 1995) and the deep convolutional neural network (Krizhevsky et al., 2012) are widely used, however the computation cost is significantly more, and the feature learning is based on the local filter defined by the modeler. Our methods apply to a general deep network. Our algorithms can be roughly seen as simultaneously clustering the data while extracting the features. Deep networks have been used for unsupervised clustering (Chen, 2015) and clustering has been used in classic deep networks by Weston et al. (2012) which are targeted for semi-supervised learning. Using clusters from K-means to train a deep network was reported (Faraoun & Boukelif, 2006), which improves the training speed while ignoring some data - not recommended in the supervised setting with scarce data. Pre-clustering may have efficiency-advantages in large-scale systems, but it may not be an effective way to learn good representations (Coates & Ng, 2012).\nIn this paper, we are proposing a new algorithmic enhancement to the deep network which is to consider each node separately. It not only explicitly achieves the sparsity of node activation but also requires much shorter computation time. We have found no such approaches in the literature."}, {"heading": "2. Greedy Node-by-Node Deep Learning", "text": "The basic step for pre-training is to train a two layer network. The network is trained to reproduce the input (unsupervised) or the final target (supervised). The two algorithms are very similar in structure. For concreteness, we describe unsupervised pre-training (auto-encoding).\nIn a classic auto-encoder with one hidden layer using stochastic gradient descent (SGD), the number of operations (fundamental arithmetic operations) p for one training example is: p = (2d1d2 + d1 + 2d2) + (3d1d2 + 2d1 + 3d2)\n(Forward Propagation) (Backpropagation) = 5d1d2 + 3d1 + 5d2,\n(1)\nwhere d1 is the dimension of the input layer, (d1+1 including the bias) and d2 is the dimension of the second layer. Forward propagation computes the output and backpropagation computes the gradient of the loss w.r.t. the weights (see Abu-Mostafa et al. (2012, Chapter 7)). We use the Euclidean distance between the reconstructed input x\u0302 and the original input x (auto-encoder target) as loss, loss = \u2016x\u2212 x\u0302\u20162. For N training example and E epochs of SGD, the total running time is O(NpE) = O(NEd1d2).\nIn our algorithm, the basic step is also to train a 2-layer network. However, we train each node sequentially in a greedy fashion as illustrated in Figure 5. The red (middle) layer is\nbeing trained (it has dimension d2). The inputs come from the outputs of the previous layer, having dimension d1. The output-dimension is also d1 (auto-encoder). We use linear output-nodes and SGD optimizing the auto-encoder (the algorithm is easy to adapt to sigmoid output-nodes).\nThe standard layer-by-layer algorithm trains all the weights at the same time, using the whole data set. We are going to use a fraction of the data to learn one feature at a time; different features are learned on different data. So, the training of each layer is done in multiple stages. At each stage, we only update the weights corresponding to one node. After all the features are obtained (all the weights learned) for a layer, a forward propagation with all data computes the outputs of the layer, for use in training the next layer (as in standard pre-training). To make this greedy learning algorithm work, we must address three questions:\n1. How to learn features sequentially? 2. How to distribute the training data into each node? 3. How to obtain non-overlapping features?\nLet us address the question 1, assuming we have already created d2 subsets of the data of sizeK, S1, . . . , Sd2 . These subsets need not be disjoint, but our discussion is for the case of disjoint subsets, so Kd2 = N . If each node is fed a random subset of K data points, then each node will be roughly learning the same feature, that is close to the top principle component. We will address this issue is questions 2 and 3 which encourage learning different features by implementing a form of orthogonality and using different data for each node.\nA simple idea which works but is inefficient: use data Si to train the weights into hidden node i (assuming weights into nodes 1, . . . , (i\u2212 1) have been trained). The situation is illustrated in Figure 5. The weights into and out of node 1 are fixed (black). Node 2 is being trained (the red weights into and out of node 2). We use data S2 to train these red weights, so we forward propagate the data through both nodes, but we only need to backpropagate through the red nodes. Effectively, we are forward propagating through a network of i hidden nodes, but we have K data points, so\nthe run-time is of just the forward propagations is\nd2\u2211 i=1 O(KEd1i) = O(KEd1d 2 2).\nSince d2 could be large, it is inefficient to have a quadratic in d2 run-time. There is a simple fix which relies on the fact that the outputs of the hidden layer are linearly summed before feeding into the output layer. This means that after the weights for node 1 are learned (and fixed), one can do a single forward propagation of all the data once through this node and store a running sum of the signal being fed into each output-layer node, which is the sunning contribution from all previously trained nodes in the hidden layer. This restores the linear dependence on d2 as in equation (1).\nDistributing data into nodes: We now address the 2nd question. We propose two methods corresponding to the two new algorithms GN and GCN. There are many ways to extend these two basic methods, so we present only the rudimentary forms. Each node i is trained on subset Si. In the unsupervised setting (GN), we train node 1 to learn a \u201cglobal feature\u201d, and use this single global feature to reconstruct all the data. The reconstruction error from this single feature is then used to rank the data. A small reconstruction error means the data point is captured well by the current feature. Large reconstruction errors mean a new feature is needed for those data. So the reconstruction error can be used as an approximate proxy for partitioning the data into different features: data with drastically different reconstruction error correspond to different features, so that data will be used to train its own feature. After sorting the data according to reconstruction error, the first N/(d2 \u2212 1) will be used to train node 2, the next N/(d2 \u2212 1) to train node 3 and so on up to node d2. One may do more sophisticated things, like cluster the reconstruction errors into d2\u22121 disjoint clusters and use those clusters as the subsets S2, . . . , Sd2 . We present the simplest approach.\nIn the supervised setting (GCN), the only change is to modify the distribution of the data into nodes using the class labels. If there are c classes, then d2/c of the nodes will be dedicated to each class, and the data points in each class are distributed evenly among the nodes dedicated to that class.\nEnsuring Non-Overlapping Features. If every node is trained independently on all or a random sample of the data, then each node will learn more-or-less the same feature. Our method of distributing the data among the nodes to some extent breaks this tie. We also introduce an explicit coordination mechanism between the nodes which we call the amnesia factor \u2013 when training the next node, how much of the prior training is \u201cforgotten\u201d. Recall that from the previous i\u22121 (already trained) nodes, we store the running contribution to the output. SGD is applied based on the distance between the input and the sum of the cur-\nrent output and running stored value from the previous i\u22121 nodes. The running stored value can be viewed as a constraint on the training of node i, forcing the newly learned feature to be \u201dorthogonal\u201d to the previous ones. This is because the previous features are encoded in the running stored value. The current value produced by node i will try to contribute additional information toward reconstruct the data in Si. Since, in our algorithms, the weights from the previous nodes have been learned and fixed, due to their optimization, it may prematurely saturate the output layer and make the new ith node redundant. The amnesia factor gives the ability to add just the right amount of coordination between the training of node i and the nodes that have already been trained, leading to stability of the features while at the same time maintaining the ability to get non-redundant features. Our implementation of amnesia is simple. The output value, OB , used for backpropagation to train the weights into node i are the stored (already learned) running output, O(1:i\u22121), scaled by the amnesia factor plus the output from the currently being trained node Oi,\nOB = A \u00b7O(1:i\u22121) +Oi. The amnesia factor controls how \u201corthogonal\u201d each successive feature will be to the previously trained features. A higher amnesia factor results in strongly coupled features that are more orthogonal. A zero amnesia factor means independent training of the nodes which is likely to result in redundant features. Here is a high-level summary of the full algorithm. Detailed pseudo-code is presented in the supplementary materials.\n1: Distribute the data into subsets S1, . . . , Sd2 2: Train hidden node i on data Si using amnesia factor A 3: Perform one forward propagation with Si on nodes\n1, . . . , i after training, and add the output values to the running output value.\nTheorem 1 GN and GCN run inO(NEd1+Nd1d2) time.\n(The detailed derivation of the running time is in the supplementary materials.) The run-time of the classic deepnetwork algorithm is O(NEd1d2). As d2 and E (the number of iterations of SGD on each data point) increase, the efficiency gain increases, and can be orders of magnitude."}, {"heading": "3. Results and Discussion", "text": "We use a variety of data sets to compare our new algorithms GN (unsupervised) and GCN (supervised) against some standard deep-network training algorithms. Our aim is to demonstrate that\n\u2022 Features from GN and GCN are more interpretable. \u2022 The classification performance of our algorithms is\ncomparable to layer-by-layer training, despite being orders of magnitude more efficient."}, {"heading": "3.1. Quality of Features", "text": "First, we appeal to the results on the USPS digits data in the Figures 2 and 4 to illustrate the qualitative difference between our greedy node-by-node features and the features produced by the standard layer-by-layer pre-training. When you learn all the features in a layer simultaneously, there are several local minima that do not correspond to the natural intuitive features that might be present. Because all the nodes are being trained simultaneously, multiple features can get mixed within one node and this mixing can be compensated for in other nodes. This results in somewhat counter-intuitive features as are obtained in Figures 2 and 4 for the layer-by-layer approach. The node-by-node approaches do not appear to suffer from this problem, as a simple visual inspection of the features implemented at each node yield recognizable digits. It is clear that our new algorithms find more interpretable features.\nEven though the visual inspection of the features is quite difference, we may investigate more closely the dimension reduction from the original pixel-space to the new feature space. A common approach is to use scatter plots of one feature against another, and we give such scatter plots in Figure 6, for the unsupervised setting. For this experiment, we consider a smaller handwritten digit data, gener-\nated from the scikit-learn package (Pedregosa et al., 2011). Readers can refer to the supplementary information for the details on the generation procedure. The simplified handwritten digit data is a 8 by 8 pixel handwritten digits. It is known that this data admits good classification accuracy using the top-2 PCA features. This means that we should see good class separation from a scatter plot of the features of one node against another. (The higher dimensional digits data cannot effectively be visualized in 2-dimensions.)\nIn Figure 6, the top row shows scatter plots of one node\u2019s features against another node\u2019s features for the standard layer-by-layer algorithm. Good class separation is obtained even after projecting to 2-dimensions (there are 5 classes). The bottom row shows the similar plots for our unsupervised algorithms. The similarity between the scatter plots indicates that our features and layer-by-layer features are comparable in terms of classification accuracy (we will investigate this in depth in the next section).\nSummary: Our node-by-node features are more interpretable and appear as effective as layer-by-layer features."}, {"heading": "3.2. Efficiency and Classification Performance", "text": "We have argued theoretically that our algorithms are more efficient than layer-by-layer pre-training. The rationale is\nthat the training is distributed onto each inner node by the partitioning of the data. We first compare the running time in practice of supervised and unsupervised pre-training for our algorithms (GN and GCN) with the standard layer-bylayer algorithms. For a concrete comparison, we use a fixed number of iterations (300) for pre-training with an initial learning rate 0.001; we use 500 iterations to train the final output layer with logistic regression, with an initial learning rate 0.002; and finally, we use 20 iterations of backpropagation to fine-tune all the weights with fixed learning rate 0.001. We do not use significant learning rate decay.\nAn L2 regularization term is used with all the algorithms to help with overfitting, with a regularization parameter of \u03bb = 1. It should be noted that this \u201dweight decay\u201d regularization term is not always necessary with deep learning since the flexibility of the network is already significantly diminished. In this paper, we are not trying to optimize all hyperparameters to get the highest possible test performance for certain data sets. Instead we try to compare the four algorithm with one predefined set of parameters. Our goal is to see whether there is a deterioration in classification accuracy due to additional constraints placed by training the deep-network using greedy node-by-node learning. Table 1 shows the results for running time. A 256-200- 150-10 network structure was used. RT and PT are the times for the entire algorithm and for only the pre-training respectively (the entire algorithm includes fine-tuning and data pre-processing). 7291 data are in the training set and 2007 are in the test set. All the experiment was done using a single Intel i-7 3370 3.4GHz CPU. As shown in Table 1, all algorithms show comparable training and test performance, however our algorithms can give an order of magnitude speed-up.\nImpact of Amnesia The amnesia factor A is used to train node i, given the stored results of forward propagation for the previous i \u2212 1 nodes. The stored output is similar to keeping a \u201cmemory\u201d of previous learned knowledge within the same representation layer. Node i should learn the new feature with the previous information \u201cin mind\u201d. We found that best results are obtained with an amnesia factor be-\ntween 0 (no memory) and 1.0 (no loss of memory). This suggests that some coordination between nodes in a layer is useful, but too much coordination adds too many constraints. Our results indicate that amnesia is very important for both our new algorithms, and it is not a surprise. Table 2 shows the result of a 256-200-150-10 network with learning rate 0.001 and a variety of amnesia factors. All the other settings not changed from the previous experiment.\nThe results in Table 2 suggest that a resonable choice for the amnesia factor is A \u2208 [0.4, 0.5], which is far from both 1 (full coordination) and 0 (no memory that can result in redundant features). A non-zero amnesia factor applies a balance between two types of representations. A higher AF will lean to generative model of inputs but induce numerical issue for training. A lower AF will learn a local model of inputs but result in loss of information through redundant features. The optimal amnesia factor may depend on size and depth of the layer. We did not investigate this issue, keeping A constant for each layer. We also did not investigate different ways to distribute the data among the nodes. Our simple method works well and is efficient.\nFinally, we give an extensive comparison of the out-ofsample performance between the four algorithms on several additional data sets in Table 3. Nine data sets obtained from UCI machine learning repository (Lichman, 2013) were used to compare the algorithms. Test performance is computed using a validation set which is typically approximately 30% of the data unless the test set is provided by the repository. For GCN, it is convenient to set the number of nodes in each hidden layer to be divisible by the number of classes. We used the same architecture for all algorithms. Here are some relevant information for each data set.\nDATA SET N d \u03b7 A\nMUSK 6598 166 0.01 0.4 ISOLET 6238 617 0.0001 0.4 CNAE-9 1080 856 0.01-0.1 0.4 MADELON 4400 500 0.01 0.4 CANCER 699 10 0.01 0.4\nBANK 4521 16 0.01 0.4 NEWS 39797 60 0.01 0.4 POKER 78211 10 0.01 0.4 CHESS 9149 6 0.01 0.4\n(N = # data points; d = dimension (# attributes); \u03b7 = initial learning rate of SGD; A = amnesia factor.)\nMUSK (version 2) is a molecule classification task with data on molecules that are judged by experts as musks and non-musks. ISOLET contains audio information for 26 letters spoken by human speakers. CNAE-9 contains 1080 documents of text business descriptions for Brazilian companies categorized into 9 categories. CNAE-9 is very sparse, so a learning rate up to 0.1 can produce a reasonable performance for our new algorithms while the classical algorithms use a learning rate of 0.01. Better performance results if one specializes the learning rate to the algorithm (for example, GCN with learning rate of 0.15 has the better test score of 0.935). MADELON is an artificial dataset used in the NIPS 2003 feature selection challenge. CANCER is the Wisconsin breast cancer data. BANK contains bank marketing data. NEWS contains news popularity data. POKER is part of the poker hand data set (class 0 and 1 are removed to get a better class balance). CHESS is the king-rook vs. king data set (KRK) with 4 classes (\u201cdraw\u201d, \u201ceight\u201d, \u201celeven\u201d, \u201cfifteen\u201d) from the original data set with 6 attributes. Table 3 shows the comparison between the layer-by-layer supervised, unsupervised, GN and GCN algorithms.\nSummary: The performance of our algorithms is comparable with standard layer-by-layer deep network algorithms, significant speedup and more interpretable features. The\nresults on additional data sets are are consistent with the USPS handwriting data set we used throughout this paper."}, {"heading": "4. Conclusion and Future Work", "text": "Our goal was to develop a greedy node-by-node deep network algorithm that learns feature representations in each layer of a deep network sequentially (similar to PCA in the linear setting, but different because we partition data among features). Our two novel deep learning algorithms originate from the idea of simulating a human\u2019s learning process, namely building features in a streaming fashion incrementally, using part of the data to learn each feature. This is in contrast to classical deep learning algorithms which obtain all the hierarchical features in a layer for all the objects at the same time (train all nodes of a layer simultaneously on all data) \u2013 the human learner learns from one or few objects at a time and is able to learn new features while leveraging the features it has learned on other data. Our two new methods, corresponding to supervised learning (GCN) and unsupervised learning (GN), do indeed learn one feature of one group of training data at a time. Such a design helps to construct more human-recognizable features. We also developed amnesia, an ability for the greedy algorithm to control the coordination among the features. The results on several datasets reveal that our algorithms have a prediction performance comparable with the standard layer-by-layer methods plus the advertised benefits of speed-up and a more interpretable features.\nIn the future, we would like to investigate two subproblems. First, whether it is possible to further exploit the node-bynode paradigm by optimizing the hyper-parameters (learning rate and amnesia) to obtain superior performance. Several questions need to be answered here: Is there a better way to partition the data? How to choose the optimal amnesia factor? Should the learning rate be adjusted differently for each inner node or each layer?\nScalability: our algorithms are in some sense learning in an\nonline fashion, and so cannot exploit the matrix-vector and matrix-matrix multiplication approaches to training that can be easily implemented for multicore or GPU architectures. One way to handle such difficulty could be to learn several features at the same time in different machines and exchange information (as memorization constraints) every few epochs (to simulate a group of human learners). Distributing our algorithm over different models of parallel computing appears to be a challenging problem. The adaption of the algorithm to parallel computing will surely require creative upgrades in the algorithm design."}], "references": [{"title": "Learning From Data: A Short Course", "author": ["Abu-Mostafa", "Yaser", "Magdon-Ismail", "Malik", "Lin", "Hsuan-Tien"], "venue": "amlbook.com,", "citeRegEx": "Abu.Mostafa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abu.Mostafa et al\\.", "year": 2012}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Bengio", "Yoshua"], "venue": "Unsup. and Transfer Learning Challenges in ML,", "citeRegEx": "Bengio and Yoshua.,? \\Q2012\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2012}, {"title": "Sparse feature learning for deep belief networks", "author": ["Boureau", "Y-lan", "Cun", "Yann L"], "venue": "In NIPS, pp", "citeRegEx": "Boureau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2008}, {"title": "Deep learning with nonparametric clustering", "author": ["Chen", "Gang"], "venue": "arXiv preprint arXiv:1501.03084,", "citeRegEx": "Chen and Gang.,? \\Q2015\\E", "shortCiteRegEx": "Chen and Gang.", "year": 2015}, {"title": "Learning feature representations with k-means", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Coates et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2012}, {"title": "Principal components analysis of images via back propagation", "author": ["Cottrell", "Garrison", "Munro", "Paul"], "venue": "In Proc. SPIE 1001,", "citeRegEx": "Cottrell et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Cottrell et al\\.", "year": 1988}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["Deng", "Li", "Hinton", "Geoffrey", "Kingsbury", "Brian"], "venue": "In Proc. ICASSP,", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Neural networks learning improvement using the k-means clustering algorithm to detect network intrusions", "author": ["KM Faraoun", "A. Boukelif"], "venue": "INFOCOMP Journal of Computer Science,", "citeRegEx": "Faraoun and Boukelif,? \\Q2006\\E", "shortCiteRegEx": "Faraoun and Boukelif", "year": 2006}, {"title": "Local minima and plateaus in hierarchical structures of multilayer perceptrons", "author": ["Fukumizu", "Kenji", "Amari", "Shun-ichi"], "venue": "Neural Networks,", "citeRegEx": "Fukumizu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2000}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proc. ICML, pp", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "On the problem of local minima in backpropagation", "author": ["Gori", "Marco", "Tesi", "Alberto"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gori et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Gori et al\\.", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Learning phenomena in networks of adaptive switching circuits", "author": ["Hoff Jr.", "ME"], "venue": "PhD thesis,", "citeRegEx": "Jr and ME.,? \\Q1962\\E", "shortCiteRegEx": "Jr and ME.", "year": 1962}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Yann", "Bengio", "Yoshua"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee", "Honglak", "Grosse", "Roger", "Ranganath", "Rajesh", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Lee", "Honglak", "Pham", "Peter", "Largman", "Yan", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proc. ICML, pp", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "On optimization methods for deep learning", "author": ["Ngiam", "Jiquan", "Coates", "Adam", "Lahiri", "Ahbik", "Prochnow", "Bobby", "Le", "Quoc V", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Poultney", "Christopher", "Chopra", "Sumit", "Cun", "Yann L"], "venue": "In NIPS,", "citeRegEx": "Poultney et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poultney et al\\.", "year": 2006}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological review,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1958}, {"title": "Learning internal representation by back propagation. Parallel distributed processing: exploration in the microstructure", "author": ["DE Rumelhart", "GE Hinton", "Williams", "RJ"], "venue": "of cognition,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "The nature of statistical learning theory", "author": ["Vapnik", "Vladimir"], "venue": null, "citeRegEx": "Vapnik and Vladimir.,? \\Q2000\\E", "shortCiteRegEx": "Vapnik and Vladimir.", "year": 2000}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proc. ICML, pp", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Deep learning via semi-supervised embedding", "author": ["Weston", "Jason", "Ratle", "Fr\u00e9d\u00e9ric", "Mobahi", "Hossein", "Collobert", "Ronan"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Adaptive switching circuits", "author": ["B. Widrow"], "venue": "In IRE WESCON Convention Record, pp", "citeRegEx": "Widrow,? \\Q1960\\E", "shortCiteRegEx": "Widrow", "year": 1960}, {"title": "Deep learning and its applications to signal and information processing", "author": ["Yu", "Dong", "Deng", "Li"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 28, "context": "Multilayer neural networks have gone through ups and downs since their arrival in (Rosenblatt, 1958; Widrow, 1960; Hoff Jr, 1962).", "startOffset": 82, "endOffset": 129}, {"referenceID": 0, "context": "Assume a standard machine learning from data setup (Abu-Mostafa et al., 2012), withN datapoints (x1, y1), .", "startOffset": 51, "endOffset": 77}, {"referenceID": 23, "context": "Backpropagation which trains all the weights simultaneously, allowing for maximum flexibility, was the popular approach to training a deep network (Rumelhart et al., 1986).", "startOffset": 147, "endOffset": 171}, {"referenceID": 7, "context": "The greedy layer-by-layer pre-training is significantly more efficient than full backpropagation, and appears to be better at avoiding bad local minima (Erhan et al., 2010).", "startOffset": 152, "endOffset": 172}, {"referenceID": 23, "context": "There is no doubt softening the hard threshold to a sigmoid and the arrival of a new efficient training algorithm, backpropagation (Rumelhart et al., 1986), was a huge part of the resurgence of neural networks in the 1980s/1990s.", "startOffset": 131, "endOffset": 155}, {"referenceID": 27, "context": "To help motivate our approach, it helps to start back at the very beginning of neural networks, with Rosenblatt (1958) and Widrow (1960). They introduced the adaline, the adaptive linear (hard threshold element), and the combination of multiple elements came in Hoff Jr (1962), the madaline (the precursor to the multilayer perceptron).", "startOffset": 123, "endOffset": 137}, {"referenceID": 27, "context": "To help motivate our approach, it helps to start back at the very beginning of neural networks, with Rosenblatt (1958) and Widrow (1960). They introduced the adaline, the adaptive linear (hard threshold element), and the combination of multiple elements came in Hoff Jr (1962), the madaline (the precursor to the multilayer perceptron).", "startOffset": 123, "endOffset": 277}, {"referenceID": 14, "context": "due to the facts that multilayer feedforward networks were still hard to train iteratively due to convergence issues and multiple local minima (Gori & Tesi, 1992; Fukumizu & Amari, 2000), are extremely powerful (Hornik et al., 1989) and easy to overfit to data.", "startOffset": 211, "endOffset": 232}, {"referenceID": 12, "context": "Multi-layer (deep) neural networks are back in the guise of deep learning/deep networks, and again because of a leap in the methods used to train the network (Hinton et al., 2006).", "startOffset": 158, "endOffset": 179}, {"referenceID": 12, "context": "In so doing, training becomes manageable (Hinton et al., 2006; Bengio et al., 2007), the local minima problem when training a single layer is significantly diminished as compared to the whole network and the restriction to layer by layer learning reigns in the power of the network, helping with regularizing it(Erhan et al.", "startOffset": 41, "endOffset": 83}, {"referenceID": 7, "context": ", 2007), the local minima problem when training a single layer is significantly diminished as compared to the whole network and the restriction to layer by layer learning reigns in the power of the network, helping with regularizing it(Erhan et al., 2010).", "startOffset": 235, "endOffset": 255}, {"referenceID": 7, "context": "A side benefit has also emerged, which is that each layer successively has the potential to learn hierarchical representations (Erhan et al., 2010; Lee et al., 2009a).", "startOffset": 127, "endOffset": 166}, {"referenceID": 10, "context": "As a result of these algorithmic advances, deep networks have found a host of modern applications, ranging from sentiment classification (Glorot et al., 2011), to audio (Lee et al.", "startOffset": 137, "endOffset": 158}, {"referenceID": 6, "context": ", 2009b), to signal and information processing (Yu & Deng, 2011), to speech (Deng et al., 2013), and even to the unsupervised and transfer settings (Bengio, 2012).", "startOffset": 76, "endOffset": 95}, {"referenceID": 20, "context": "Optimization of such deep networks is also an active area, for example (Ngiam et al., 2011; Martens, 2010).", "startOffset": 71, "endOffset": 106}, {"referenceID": 12, "context": "Besides the original deep belief network (Hinton et al., 2006) and autoencoder (Bengio et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 26, "context": ", 2007), the stacked denoising autoencoder (Vincent et al., 2010; 2008) has been widely used as a variant to the classic autoencoder, where corrupted data is used in pre-training.", "startOffset": 43, "endOffset": 71}, {"referenceID": 2, "context": "Sparse encoding (Boureau et al., 2008; Poultney et al., 2006) has also been used to prevent the system from activating the same subset of nodes constantly (Poultney et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 21, "context": "Sparse encoding (Boureau et al., 2008; Poultney et al., 2006) has also been used to prevent the system from activating the same subset of nodes constantly (Poultney et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 21, "context": ", 2006) has also been used to prevent the system from activating the same subset of nodes constantly (Poultney et al., 2006).", "startOffset": 101, "endOffset": 124}, {"referenceID": 15, "context": "For images, convolutional neural networks (LeCun & Bengio, 1995) and the deep convolutional neural network (Krizhevsky et al., 2012) are widely used, however the computation cost is significantly more, and the feature learning is based on the local filter defined by the modeler.", "startOffset": 107, "endOffset": 132}, {"referenceID": 15, "context": "For images, convolutional neural networks (LeCun & Bengio, 1995) and the deep convolutional neural network (Krizhevsky et al., 2012) are widely used, however the computation cost is significantly more, and the feature learning is based on the local filter defined by the modeler. Our methods apply to a general deep network. Our algorithms can be roughly seen as simultaneously clustering the data while extracting the features. Deep networks have been used for unsupervised clustering (Chen, 2015) and clustering has been used in classic deep networks by Weston et al. (2012) which are targeted for semi-supervised learning.", "startOffset": 108, "endOffset": 577}], "year": 2016, "abstractText": "Multilayer networks have seen a resurgence under the umbrella of deep learning. Current deep learning algorithms train the layers of the network sequentially, improving algorithmic performance as well as providing some regularization. We present a new training algorithm for deep networks which trains each node in the network sequentially. Our algorithm is orders of magnitude faster, creates more interpretable internal representations at the node level, while not sacrificing on the ultimate out-of-sample performance.", "creator": "LaTeX with hyperref package"}}}