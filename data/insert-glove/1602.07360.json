{"id": "1602.07360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size", "abstract": "Recent research on deep fcca neural vwo networks fsas has focused 91.40 primarily muhu on andenes improving accuracy. For loop a turtling given bostridge accuracy cup-shaped level, muchadehama is typically possible to identify hentig multiple paonta DNN architectures redoubling that fse achieve 5.31 that wainui accuracy servlet level. muntasser With equivalent accuracy, 77-kilogram smaller jaikishan DNN 1969-1974 architectures shipbuilders offer dimorphism at least mvcc three kisses advantages: (imad-ul-mulk 1) Smaller DNNs nonprofessionals require less communication across eight-wheeled servers dvb during 300-person distributed training. (acquire 2) mohsin Smaller DNNs sutiya require less karlin bandwidth tympani to priorat export bardi a rw04 new ammaccapane model 25-month from the ntsaluba cloud to an gehn autonomous car. (3) aos Smaller inves DNNs are 10-course more feasible to deploy 3-for-13 on FPGAs jarratt and other hardware with v\u00e4in\u00f6 limited latasa memory. To provide all of hasmah these advantages, sukam we mcgough propose clonal a 17-13 small p-9 DNN kpnlaf architecture called skeptical SqueezeNet. hallow SqueezeNet chose achieves AlexNet - preclear level nagambie accuracy on ImageNet with 50x fewer second-place parameters. Additionally, ivybridge with rsis model prompts compression beji techniques 58,750 we 6,002 are able 2,279 to hollybush compress SqueezeNet yunis to aniversario less aff than 1MB (461x dilfer smaller than zagunis AlexNet ).", "histories": [["v1", "Wed, 24 Feb 2016 00:09:45 GMT  (402kb,D)", "http://arxiv.org/abs/1602.07360v1", null], ["v2", "Sat, 27 Feb 2016 20:24:20 GMT  (513kb,D)", "http://arxiv.org/abs/1602.07360v2", null], ["v3", "Wed, 6 Apr 2016 07:21:49 GMT  (1006kb,D)", "http://arxiv.org/abs/1602.07360v3", "Added macro- and microarchitectural design space exploration. Dense-&gt;sparse-&gt;dense training. Further compression. Expanded from 5 pages to 9 pages"], ["v4", "Fri, 4 Nov 2016 21:26:08 GMT  (533kb,D)", "http://arxiv.org/abs/1602.07360v4", "In ICLR Format"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["forrest n iandola", "song han", "matthew w moskewicz", "khalid ashraf", "william j dally", "kurt keutzer"], "accepted": false, "id": "1602.07360"}, "pdf": {"name": "1602.07360.pdf", "metadata": {"source": "CRF", "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size", "authors": ["Forrest N. Iandola", "Matthew W. Moskewicz", "Khalid Ashraf", "Song Han", "William J. Dally", "Kurt Keutzer"], "emails": ["keutzer}@eecs.berkeley.edu", "dally}@stanford.edu"], "sections": [{"heading": null, "text": "The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet"}, {"heading": "1. Introduction and Motivation", "text": "Much of the recent research on deep neural networks has focused on increasing accuracy on computer vision datasets. For a given accuracy level, there typically exist multiple DNN architectures that achieve that accuracy level. Given equivalent accuracy, a DNN architecture with fewer parameters has several advantages:\n\u2022 More efficient distributed training. Communication among servers is the limiting factor to the scalability of distributed DNN training. For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model [11]. In short, small models train faster due to requiring less communication.\n\u2022 Less overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers\u2019 cars. With AlexNet, this would require 240MB\n\u2217http://deepscale.ai\nof communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\n\u2022 Feasible FPGA and embedded deployment. FPGAs often have less than 10MB1 of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA, while video frames stream through the FPGA in real time.\nAs you can see, there are several advantages of smaller DNN architectures. With this in mind, we focus directly on the problem of identifying a DNN architecture with fewer parameters but equivalent accuracy compared to a wellknown model. We have discovered such an architecture, which we call SqueezeNet.\nThe rest of the paper is organized as follows. In Section 2, we describe SqueezeNet. We review related work in Section 3, and we evaluate SqueezeNet in Section 4. We conclude in Section 5."}, {"heading": "2. SqueezeNet: preserving accuracy with", "text": "few parameters\nIn this section, we begin by outlining our design strategies for DNN architectures with few parameters. Then, we introduce the Fire module, our building block out of which to build DNN architectures. Finally, we use our design strategies to construct SqueezeNet, which is comprised largely of Fire modules."}, {"heading": "2.1. Architectural Design Strategies", "text": "Our overarching objective in this paper is to identify DNN architectures which have few parameters while maintaining competitive accuracy. To achieve this, we work toward three main strategies when designing DNN architectures:\nStrategy 1. Replace 3x3 filters with 1x1 filters. Given a budget of a certain number of convolution filters, we will choose to make the majority of these filters 1x1, since a 1x1 filter has 9x fewer parameters than a 3x3 filter.\n1For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory.\nar X\niv :1\n60 2.\n07 36\n0v 1\n[ cs\n.C V\n] 2\n4 Fe\nb 20\nh\"p://www.presenta.on0process.com/lego0blocks0in0powerpoint.html88\nsquee ze8\nexpan d8\n1x18convolu.on8filters8\n1x18and83x38convolu.on8filters8\nReLU8\nReLU8\nFigure 1. Organization of convolution filters in the Fire module. In this example, s1x1 = 3, e1x1 = 4, and e3x3 = 4. We illustrate the convolution filters but not the activations.\nStrategy 2. Decrease the number of input channels to 3x3 filters. Consider a convolution layer that is comprised entirely of 3x3 filters. The total quantity of parameters in this layer is (number of input channels) * (number of filters) * (3*3). So, to maintain a small total number of parameters in a DNN, it is important not only to decrease the number of 3x3 filters (see Strategy 1 above), but also to decrease the number of input channels to the 3x3 filters. We decrease the number of input channels to 3x3 filters using squeeze modules, which we describe in the next section.\nStrategy 3. Downsample late in the network so that convolution layers have large activation maps. In a convolutional network, each convolution layer produces an output activation map with a spatial resolution that is at least 1x1 and often much larger than 1x1. The height and width of these activation maps are controlled by: (1) the size of the input data (e.g. 256x256 images) and (2) the choice of layers in which to downsample in the DNN architecture. Most commonly, downsampling is engineered into DNN architectures by setting the (stride > 1) in some of the convolution or pooling layers. If early2 layers in the network have large strides, then most layers will have small activation maps. Conversely, if most layers in the network have a stride of 1, and the strides greater than 1 are concentrated toward the end3 of the network, then many layers in the network will have large activation maps. Our intuition is that large activation maps due to delayed downsampling can lead to higher classification accuracy, with all else held equal. Indeed, K. He and H. Sun applied delayed downsampling to four different DNN architectures, and in each case delayed downsampling led to higher classification accuracy [10].\nStrategies 1 and 2 are about judiciously decreasing the quantity of parameters in a DNN while attempting to preserve accuracy. Strategy 3 is about maximizing accuracy on a limited budget of parameters. Next, we describe the Fire module, which is our building block for DNN architectures that achieve Strategies 1, 2, and 3.\n2In our terminology, an \u201cearly\u201d layer is close to the input data. 3In our terminology, the \u201cend\u201d of the network is the classifier."}, {"heading": "2.2. The Fire Module", "text": "We define the Fire module as follows. A Fire module is comprised of: a squeeze convolution module comprised of 1x1 filters, feeding into an expand module that is comprised of a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1. The liberal use of 1x1 filters in Fire modules allows us to leverage Strategy 1 from Section 2.1.\nWe expose three tunable dimensions (hyperparameters) in a Fire module: s1x1, e1x1, and e3x3. In a Fire module, s1x1 is the number of filters in the squeeze module (all 1x1), e1x1 is the number of 1x1 filters in the expand module, and e3x3 is the number of 3x3 filters in the expand module. When we use Fire modules we set s1x1 to be less than (e1x1 + e3x3), so the squeeze module helps to limit the number of input channels to the 3x3 filters, enabling Strategy 2 from Section 2.1."}, {"heading": "2.3. The SqueezeNet architecture", "text": "We now describe the SqueezeNet DNN architecture. We illustrate in Figure 2 that SqueezeNet begins with a standalone convolution layer (conv1), followed by 8 Fire modules (fire2-9), ending with a final conv layer (conv10). We gradually increase the number of filters per fire module from the beginning to the end of the network. SqueezeNet performs max-pooling with a stride of 2 after conv1, fire4, fire8, and conv10; these relatively late placements of pooling allow us to achieve Strategy 3 from Section 2.1. We present the full SqueezeNet architecture in Table 1."}, {"heading": "2.3.1 Other SqueezeNet details", "text": "\u2022 So that the output activations from 1x1 and 3x3 filters\nhave the same height and width, we add a 1-pixel border of zero-padding in the input data to 3x3 filters of expand modules.\n\u2022 ReLU [18] is applied to activations from squeeze and expand modules.\n\u2022 Dropout [20] with a ratio of 50% is applied after the fire9 layer.\n\u2022 Note the lack of fully-connected layers in SqueezeNet; this design choice was inspired by the NiN [17] architecture.\n\u2022 When training SqueezeNet, we use a polynomial learning rate much like the one that we described in the FireCaffe paper [11]. For details on the training protocol (e.g. batch size, learning rate, parameter initialization), please refer to our Caffe [15] configuration files located here: https://github.com/DeepScale/SqueezeNet.\n\u2022 The Caffe framework does not natively support a convolution layer that contains multiple filter resolutions (e.g. 1x1 and 3x3). To get around this, we implement our expand modules with two separate convolution layers: a layer with 1x1 filters, and a layer with 3x3 filters. Then, we concatenate the outputs of these layers together in the channel dimension. This is numerically equivalent to implementing one layer that contains both 1x1 and 3x3 filters.\nDetailed(dimensions(of(FireNet(w/(squeeze(ra6o(=(0.125("}, {"heading": "3. Related Work", "text": "The overarching goal of our work is to identify a model that has very few parameters while preserving accuracy. To\naddress this problem, a sensible approach is to take an existing DNN model and compress it in a lossy fashion. In fact, a research community has emerged around the topic of model compression, and several approaches have been reported. A fairly straightforward approach by Denton et al. is to apply singular value decomposition (SVD) to a pretrained DNN model [4]. Han et al. developed Network Pruning, which begins with a pretrained model, then replaces parameters that are below a certain threshold with zeros to form a sparse matrix, and finally performs a few iterations of training on the sparse DNN [9]. Recently, Han et al. extended their work by combining Network Pruning with quantization (to 8 bits or less) and huffman encoding to create an approach called Deep Compression [8]."}, {"heading": "4. Evaluation", "text": "We now turn our attention to evaluating SqueezeNet. In each of the DNN model compression papers reviewed in the previous section, the goal was to compress an AlexNet [16] model that was trained to classify images using the ImageNet [3] (ILSVRC 2012) dataset. Therefore, we use AlexNet4 and the associated model compression results as a basis for comparison when evaluating SqueezeNet.\nIn Table 2, we review SqueezeNet in the context of recent model compression results. The SVD-based approach is able to compress a pretrained AlexNet model by a factor of 5x, while diminishing accuracy to 56.0% top-1 accuracy [4]. Network Pruning achieves a 9x reduction in model size while maintaining the baseline of 57.2% top-1 and 80.3% top-5 accuracy on ImageNet [9]. Deep Compression achieves a 35x reduction in model size, while still maintaining the baseline accuracy level [8]. Now, with SqueezeNet, we achieve a 50x reduction in model size compared to AlexNet, while meeting or exceeding the top-1 and top-5 accuracy of AlexNet. We summarize all of the aforementioned results in Table 2.\nIt appears that we have surpassed the state-of-the-art results from the model compression community: even when using uncompressed 32-bit values to represent the model, SqueezeNet has a 1.4x smaller model size than the best efforts from the model compression community while maintaining or exceeding the baseline accuracy. Until now, an open question has been: are small models amenable to compression, or do small models \u201cneed\u201d all of the representational power afforded by dense floating-point values? To find out, we applied Deep Compression [8] to SqueezeNet, using 50% sparsity5 and 8-bit quantization. This yields a 0.92 MB model (258x smaller than 32-bit AlexNet) with equivalent accuracy to AlexNet. Further, applying Deep Compression with 6-bit quantization and 45% sparsity on SqueezeNet, we produce a 0.52MB model (461x smaller than 32-bit AlexNet) with equivalent accuracy. It appears that our small model is indeed amenable to compression.\n4Our baseline is bvlc alexnet from the Caffe codebase [15]. 5Note that, due to the storage overhead of storing sparse matrix indices, increasing sparsity from 0% to 50% leads to somewhat less than a 2x decrease in model size.\nIn addition, these results demonstrate that Deep Compression [8] not only works well on DNN architectures with many parameters (e.g. AlexNet and VGG [19]), but it is also able to compress the already compact, fully convolutional SqueezeNet architecture. Deep Compression compressed SqueezeNet by 9.2x while preserving the baseline accuracy. By combining DNN architectural innovation (SqueezeNet) with state-of-the-art compression techniques (Deep Compression), we achieved a 461x reduction in model size with no decrease in accuracy compared to the baseline."}, {"heading": "5. Conclusions and Future Work", "text": "We have presented SqueezeNet, a DNN architecture that has 50x fewer parameters than AlexNet and maintains AlexNet-level accuracy on ImageNet. In this paper, we focused on ImageNet as a target dataset. However, it has become common practice to apply ImageNet-trained DNN representations to a variety of applications such as fine-grained object recognition [21, 5], logo identification in images [13], and generating sentences about images [6]. ImageNettrained DNNs have also been applied to a number of applications pertaining to autonomous driving, including pedestrian and vehicle detection in images [12, 7] and videos [2], as well as segmenting the shape of the road [1]. We think SqueezeNet will be a good candidate DNN architecture for a variety of applications, especially those in which small model size is of importance.\nSqueezeNet is one of several new DNNs that we have discovered while broadly exploring the design space of DNN architectures. We hope that SqueezeNet will inspire the reader to consider and explore the broad range of possibilities in the design space of DNN architectures."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Bill Dally for helpful discussions. Research partially funded by DARPA Award Number HR0011-12-2-0016, plus ASPIRE industrial sponsors and affiliates Intel, Google, Huawei, Nokia, NVIDIA, Oracle, and Samsung. This research used resources of the Oak Ridge Leadership Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of\nthe U.S. Department of Energy under Contract No. DEAC05-00OR22725."}], "references": [{"title": "SegNet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arxiv:1511.00561", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "3d object proposals for accurate object class detection", "author": ["X. Chen", "K. Kundu", "Y. Zhu", "A.G. Berneshawi", "H. Ma", "S. Fidler", "R. Urtasun"], "venue": "NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv:1310.1531", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deformable part models are convolutional neural networks", "author": ["R.B. Girshick", "F.N. Iandola", "T. Darrell", "J. Malik"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compression: Compressing DNNs with pruning", "author": ["S. Han", "H. Mao", "W. Dally"], "venue": "trained quantization and huffman coding. arxiv:1510.00149v3", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks at constrained time cost", "author": ["K. He", "J. Sun"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "FireCaffe: near-linear acceleration of deep neural network training on compute clusters", "author": ["F.N. Iandola", "K. Ashraf", "M.W. Moskewicz", "K. Keutzer"], "venue": "arxiv:1511.00175", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "DenseNet: Implementing efficient convnet descriptor pyramids", "author": ["F.N. Iandola", "M.W. Moskewicz", "S. Karayev", "R.B. Girshick", "T. Darrell", "K. Keutzer"], "venue": "arXiv:1404.1869", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "DeepLogo: Hitting logo recognition with the deep neural network hammer", "author": ["F.N. Iandola", "A. Shen", "P. Gao", "K. Keutzer"], "venue": "arXiv:1510.02131", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "JMLR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACM Multimedia", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv:1312.4400", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deformable part descriptors for fine-grained recognition and attribute prediction", "author": ["N. Zhang", "R. Farrell", "F. Iandola", "T. Darrell"], "venue": "ICCV", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "Sun applied delayed downsampling to four different DNN architectures, and in each case delayed downsampling led to higher classification accuracy [10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "\u2022 ReLU [18] is applied to activations from squeeze and expand modules.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "\u2022 Dropout [20] with a ratio of 50% is applied after the fire9 layer.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "\u2022 Note the lack of fully-connected layers in SqueezeNet; this design choice was inspired by the NiN [17] architecture.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "\u2022 When training SqueezeNet, we use a polynomial learning rate much like the one that we described in the FireCaffe paper [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "batch size, learning rate, parameter initialization), please refer to our Caffe [15] configuration files located here: https://github.", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "(The formatting of this table was inspired by the Inception2 paper [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "is to apply singular value decomposition (SVD) to a pretrained DNN model [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "developed Network Pruning, which begins with a pretrained model, then replaces parameters that are below a certain threshold with zeros to form a sparse matrix, and finally performs a few iterations of training on the sparse DNN [9].", "startOffset": 229, "endOffset": 232}, {"referenceID": 7, "context": "extended their work by combining Network Pruning with quantization (to 8 bits or less) and huffman encoding to create an approach called Deep Compression [8].", "startOffset": 154, "endOffset": 157}, {"referenceID": 15, "context": "In each of the DNN model compression papers reviewed in the previous section, the goal was to compress an AlexNet [16] model that was trained to classify images using the ImageNet [3] (ILSVRC 2012) dataset.", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "In each of the DNN model compression papers reviewed in the previous section, the goal was to compress an AlexNet [16] model that was trained to classify images using the ImageNet [3] (ILSVRC 2012) dataset.", "startOffset": 180, "endOffset": 183}, {"referenceID": 3, "context": "0% top-1 accuracy [4].", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "3% top-5 accuracy on ImageNet [9].", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "Deep Compression achieves a 35x reduction in model size, while still maintaining the baseline accuracy level [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "Until now, an open question has been: are small models amenable to compression, or do small models \u201cneed\u201d all of the representational power afforded by dense floating-point values? To find out, we applied Deep Compression [8] to SqueezeNet, using 50% sparsity5 and 8-bit quantization.", "startOffset": 222, "endOffset": 225}, {"referenceID": 14, "context": "4Our baseline is bvlc alexnet from the Caffe codebase [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "3% AlexNet SVD [4] 32 bit 240MB \u2192 48MB 5x 56.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "4% AlexNet Network Pruning [9] 32 bit 240MB \u2192 27MB 9x 57.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "AlexNet Deep Compression[8] 5-8 bit 240MB \u2192 6.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "In addition, these results demonstrate that Deep Compression [8] not only works well on DNN architectures with many parameters (e.", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "AlexNet and VGG [19]), but it is also able to compress the already compact, fully convolutional SqueezeNet architecture.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "However, it has become common practice to apply ImageNet-trained DNN representations to a variety of applications such as fine-grained object recognition [21, 5], logo identification in images [13], and generating sentences about images [6].", "startOffset": 154, "endOffset": 161}, {"referenceID": 4, "context": "However, it has become common practice to apply ImageNet-trained DNN representations to a variety of applications such as fine-grained object recognition [21, 5], logo identification in images [13], and generating sentences about images [6].", "startOffset": 154, "endOffset": 161}, {"referenceID": 12, "context": "However, it has become common practice to apply ImageNet-trained DNN representations to a variety of applications such as fine-grained object recognition [21, 5], logo identification in images [13], and generating sentences about images [6].", "startOffset": 193, "endOffset": 197}, {"referenceID": 5, "context": "However, it has become common practice to apply ImageNet-trained DNN representations to a variety of applications such as fine-grained object recognition [21, 5], logo identification in images [13], and generating sentences about images [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 11, "context": "ImageNettrained DNNs have also been applied to a number of applications pertaining to autonomous driving, including pedestrian and vehicle detection in images [12, 7] and videos [2], as well as segmenting the shape of the road [1].", "startOffset": 159, "endOffset": 166}, {"referenceID": 6, "context": "ImageNettrained DNNs have also been applied to a number of applications pertaining to autonomous driving, including pedestrian and vehicle detection in images [12, 7] and videos [2], as well as segmenting the shape of the road [1].", "startOffset": 159, "endOffset": 166}, {"referenceID": 1, "context": "ImageNettrained DNNs have also been applied to a number of applications pertaining to autonomous driving, including pedestrian and vehicle detection in images [12, 7] and videos [2], as well as segmenting the shape of the road [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "ImageNettrained DNNs have also been applied to a number of applications pertaining to autonomous driving, including pedestrian and vehicle detection in images [12, 7] and videos [2], as well as segmenting the shape of the road [1].", "startOffset": 227, "endOffset": 230}], "year": 2016, "abstractText": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNetlevel accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 1MB (461x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet", "creator": "LaTeX with hyperref package"}}}