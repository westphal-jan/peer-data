{"id": "1602.08448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Simple Bayesian Algorithms for Best Arm Identification", "abstract": "This recai paper 249.2 considers hathersage the optimal lampang adaptive jys allocation of measurement effort for embley identifying the best among noughts a dourly finite set of options 80.54 or designs. aiguille An experimenter stornoway sequentially companionate chooses designs aude to measure and observes noisy signals weixing of their quality with the goal of then-governor confidently identifying cuscus the cuyo best design gavilan after problema a munchausen small number enfilade of hmy measurements. I propose vural three b\u00e2timent simple Bayesian algorithms akabusi for brunius adaptively vandalise allocating measurement effort. One is lpf Top - Two Probability sampling, privately which computes the two designs sivanath with lonergan the highest posterior probability luaus of being mtp02 optimal, and xuanhua then randomizes to select \u00e7aml\u0131dere among these two. One rebelled is estadounidenses a mikkelson variant a top - tenso two pankey sampling which montanans considers pacrim not shanteau only florus the selbyville probability a d\u00fcbendorf design is tiptoeing optimal, spoonbills but kinugasa the kungayeva expected amount by atsayev which it exceeds goumba other fenceline designs. The somtas final algorithm is wattled a modified macular version zhangke of Thompson sampling lunchbox that is wudang tailored for 1.5875 identifying zawinul the best posey design. tcdd I kantilal prove that 1953-54 these levitin simple exposici\u00f3n algorithms peladeau satisfy pc1 a strong jassin optimality property. corke In suno a arizona frequestist setting where the distinctiveness true ellijay quality combusts of mzembi the aborting designs is fixed, the powerbroker posterior asche is matalan said klepis to hadrat be sterno consistent jeancourt if lowcountry it 1,049 correctly clamshells identifies forties the optimal design, in lochleven the sense that umbriel that conflagrations the finanziaria posterior 2,564 probability assigned to guimaras the mhuire event ashqelon that mdb some other design serifovic is optimal converges chicagoland to zazzo zero virens as monticello measurements dom\u00e8nech are mekeisha collected. I bosnjak show that under 39.22 the proposed hommes algorithms peaty this export-oriented convergence nikkel occurs at an exponential rate, punked and refuseniks the peschke corresponding alecks exponent aliwal is the best possible 72-run among buhay all thibauld allocation", "histories": [["v1", "Fri, 26 Feb 2016 19:39:01 GMT  (46kb,D)", "https://arxiv.org/abs/1602.08448v1", null], ["v2", "Mon, 29 Feb 2016 07:28:19 GMT  (46kb,D)", "http://arxiv.org/abs/1602.08448v2", null], ["v3", "Wed, 7 Dec 2016 21:30:55 GMT  (70kb,D)", "http://arxiv.org/abs/1602.08448v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel russo"], "accepted": false, "id": "1602.08448"}, "pdf": {"name": "1602.08448.pdf", "metadata": {"source": "CRF", "title": "Simple Bayesian Algorithms for Best-Arm Identification", "authors": ["Daniel Russo"], "emails": [], "sections": [{"heading": null, "text": "the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort, and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant of top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation rules."}, {"heading": "1 Introduction", "text": "This paper considers the optimal adaptive allocation of measurement effort in order to identify the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes independent noisy signals of their quality. The goal is to allocate measurement effort intelligently so that the best design can be identified confidently after a small number of measurements. Just as the multi-armed bandit problem crystallizes the tradeoff between exploration and exploitation in sequential decision-making, this \u201cpure\u2013exploration\u201d problem crystallizes the challenge of efficiently gathering information before committing to a final decision. It serves as a fundamental abstraction of issues faced in many practical settings. For example:\n\u2022 Efficient A/B/C Testing: An e-commerce platform is considering a change to its website and would like to identify the best performing candidate among many potential new designs. To do this, the platform runs an experiment, displaying different designs to different users who visit the site. How should the platform decide what percentage of traffic to allocate to each website design?\n\u2022 Simulation Optimization: An engineer would like to identify the best performing aircraft design among several proposals. She has access to a realistic simulator through which she can assess the quality of the designs, but each simulation trial is very time consuming and produces only noisy output. How should she allocate simulation effort among the designs?\nar X\niv :1\n60 2.\n08 44\n8v 3\n[ cs\n.L G\n] 7\nD ec\n2 01\n6\n\u2022 Design of Clinical Trials: A medical research organization would like to find the most effective treatment out of several promising candidates. They run a clinical trail in which they experiment with the treatments. The results of the study may influence practice for many years to come, and so it is worth reaching a definitive conclusion. At the same time, clinical trails are extremely expensive, and careful experimentation can help to mitigate the associated costs.1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al., 2015]. Can we develop adaptive rules with better performance?\nWe study Bayesian algorithms for adaptively allocating measurement effort. Each begins with a prior distribution over the unknown quality of the designs. The experimenter learns as measurements are gathered, and beliefs are updated to form a posterior distribution. This posterior distribution gives a principled mechanism for reasoning about the uncertain quality of designs, and for assessing the probability any given design is optimal. By formulating this problem as a Markov decision process whose state-space tracks posterior beliefs about the true quality of each design, dynamic programming could in principle be used to optimize many natural measures of performance. Unfortunately, computing or even storing an optimal policy is usually infeasible due to the curse of dimensionality. Instead, this work proposes three simple and intuitive rules for adaptively allocating measurement effort, and by characterizing fundamental limits on the performance of any algorithm, formalizes a sense in which these seemingly na\u00efve rules are the best possible.\nThe first algorithm we propose is called top\u2013two probability sampling. It computes at each time-step the two designs with the highest posterior probability of being optimal. It then randomly chooses among them, selecting the design that appears most likely to be optimal with some fixed probability, and selecting the second most likely otherwise. Beliefs are updated as observations are collected, so the top-two designs change over time. The long run fraction of measurement effort allocated to each design depends on the true quality of the designs, and the distribution of observation noise. Top\u2013two value sampling proceeds in a similar manner, but in selecting the top-two designs it considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm we propose is a toptwo sampling version of the Thompson sampling algorithm for multi-armed bandits. Thompson sampling has attracted a great deal of recent interest in both academia and industry [Scott, 2016, Tang et al., 2013, Graepel et al., 2010, Chapelle and Li, 2011, Agrawal and Goyal, 2012, Kauffmann et al., 2012, Gopalan et al., 2014, Russo and Van Roy, 2014], but it is designed to maximize the cumulative reward earned while sampling. As a result, in the long run it allocates almost all effort to measuring the estimated-best design, and requires a huge number of total measurements to certify that none of the alternative designs offer better performance. We introduce a natural top-two variant of Thompson sampling that avoids this issue and as a result offers vastly superior performance for the best-arm identification problem.\nRemarkably, these simple heuristic algorithms satisfy a strong optimality property. Our analysis focuses on frequentist consistency and rate convergence of the posterior distribution, and therefore takes place in a setting where the true quality of the designs is fixed, but unknown to the experimenter. One hopes that as measurements are collected the posterior distribution definitively identifies the true best design, in the sense that the posterior probability assigned to the event that some other design is optimal converges to zero. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is essentially the best\n1Interpreted the context of clinical trials, this paper\u2019s results are stated in terms of the number of patients required to reach a confided conclusion of the best treatment. However, we will see that optimal rules from this perspective also allocate fewer patients to very poor treatments, potentially leading to more ethical trials [Berry, 2004].\npossible among all allocation rules."}, {"heading": "1.1 Main Contributions", "text": "This paper makes both algorithmic and theoretical contributions. On the algorithmic side, we develop three new adaptive measurement rules. The top-two Thompson sampling rule, in particular, could have an immediate impact in application areas where Thompson sampling is already in use. For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004]. But practitioners in these domains typically hope to commit to a decision after definitive period of experimentation, and top-two Thompson sampling can greatly reduce the number of measurements required to do so. In addition, because of their simplicity, the proposed allocation rules can be easily adapted to treat problems beyond the scope of this paper\u2019s problem formulation. See Section 7 for examples.\nThe paper also makes several theoretical contributions. Most importantly, it is of broad scientific interest to understand when very simple measurement strategies are the best possible. This paper provides a sharp result of this type by proving that three top-two sampling rules attain an optimal rate of posterior convergence across a broad class of problems. In establishing this result, we exactly characterize the optimal rate of posterior convergence attainable by an adaptive algorithm, and provide interpretable bounds on this rate when measurement distributions are sub-Gaussian. The analysis also provides several intermediate results which may be of independent interest, including establishing consistency and exponential rates of convergence for posterior distributions with nonconjugate priors and under adaptive measurement rules."}, {"heading": "1.2 Related Literature", "text": "Sequential Bayesian Best-Arm Identification. There is a sophisticated literature on algorithms for Bayesian multi-armed bandit problems. In discounted bandit problems with independent arms, Gittins indices characterize the Bayes optimal policy [Gittins and Jones, 1974, Gittins, 1979]. Moreover, a variety of simpler Bayesian allocation rules have been developed, including Bayesian upper-confidence bound algorithms [Kaufmann et al., 2012, Srinivas et al., 2012, Kaufmann, 2016], Thompson sampling [Agrawal and Goyal, 2012, Korda et al., 2013, Gopalan et al., 2014, Johnson et al., 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016]. These heuristic algorithms can be applied effectively to complicated learning problems beyond the specialized settings in which the Gittins index theorem holds, have been shown to have strong performance in simulation, and have theoretical performance guarantees. In several cases, they are known to attain sharp asymptotic limits on the performance of any adaptive algorithm due to Lai and Robbins [1985].\nThe pure-exploration problem studied in this paper is not nearly as well understood. Recent work has cast this problem in a decision-theoretic framework [Chick and Gans, 2009]. However, because the conditions required for the Gittins index theorem do not hold, computing an optimal policy via dynamic programming is generally infeasible due to the curse of dimensionality. Papers in this area typically focus on problems with Gaussian observations and priors. They formulate simpler problems that can be solved exactly \u2013 like a problem where only a single measurement can be gathered [Gupta and Miescke, 1996, Frazier et al., 2008, Chick et al., 2010] or a continuous-time problem with only two alternatives [Chick and Frazier, 2012] \u2013 and then extend those solutions heuristically to build measurement and stopping rules in more general settings.\nFor problems with Gaussian priors and noise distributions, the expected-improvement (EI) algorithm is a popular Bayesian approach to sequential information-gathering. Interesting recent\nwork by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al. [2000]. This contribution is very similar in spirit to this paper, as it relates the longrun behavior of a simple Bayesian measurement strategy to a notion of an approximately optimal allocation. Unfortunately, EI cannot match the performance guarantees in this paper. In fact, under EI the posterior converges only at a polynomial rate, instead of the exponential rate attained by the algorithms proposed here. See appendix C for a more precise discussion.\nClassical Ranking and Selection. The problem of identifying the best system has been studied for many decades under the names ranking and selection or ordinal optimization. See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews. Part of this literature focuses on a problem called subset-selection, where the goal is not to identify the best-design, but to find a fairly small subset of designs that is guaranteed to contain the best design. Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4)\n) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein. However, in a sense described below, Jennison et al. [1982] show formally that there are problems with Gaussian observations where any sequential-elimination algorithm will require substantially more samples than optimal adaptive allocation rules. See Section 7 for modified top-two sampling algorithms designed for an indifference zone criterion.\nThe asymptotic complexity of best-arm identification. We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004]. A great deal of work has sought \u201cproblem dependent\u201d bounds, which reveal that the best-arm can be identified more rapidly when the true problem instance is easier. This is the case, for example, when some arms are of very low quality, and can be distinguished from the best using a small number of measurements. Asymptotic measures of the complexity of best-arm identification appear to have been derived independently in statistics [Chernoff, 1959, Jennison et al., 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016]. Each of these papers studies a slightly different objective, but each captures a notion of the number of samples required to identify the best-arm as a function of the problem instance \u2013 i.e. as a function the number of designs, each design\u2019s true quality, and the distribution of measurement noise.\nGlynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue.\nThis paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing. Chernoff\u2019s asymptotic derivations give great insight best-arm identification, which can be formulated as a multiple-hypothesis testing problem with sequentially chosen experiments, but surprisingly this connection does not seem to be discussed in the literature. Chernoff looks at a different scaling than Glynn and Juneja [2004]. Rather than take the budget of available measurements to infinity, he allows the algorithm to stop and declare the hypothesis true or false at any time, but takes the cost of gathering measurements to zero while the cost of an incorrect terminal decision stays fixed. He constructs rules that minimize expected total costs in this limit. Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013].\nJennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting.\nA large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone.\nThe current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al. [1982]. While the\ncomplexity measure we derive is similar to past work, the proposed algorithms differ substantially. The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, \u201csidestep the issue of how to experiment in the early stages.\u201d By contrast, we show simple and natural adaptive rules automatically reach a notion of asymptotically optimal performance. See Subsection 6.5 for a more precise discussion of these approaches."}, {"heading": "2 Problem Formulation", "text": "Consider the problem of efficiently identifying the best among a finite set of designs based on noisy sequential measurements of their quality. At each time n \u2208 N, a decision-maker chooses to measure the design In \u2208 {1, ..., k}, and observes a measurement Yn,In . The measurement Yn,i \u2208 R associated with design i and time n is drawn from a fixed, unknown, probability distribution, and the vector Yn , (Yn,1, ..., Yn,k) is drawn independently across time. The decision-maker chooses a policy, or adaptive allocation rule, which is a (possibly randomized) rule for choosing a design In to measure as a function of past observations I1, Y1,I1 , ...In\u22121, Yn\u22121,In\u22121 . The goal is to efficiently identify the design with highest mean.\nWe will restrict attention to problems where measurement distributions are in the canonical one dimensional exponential family. The marginal distribution of the outcome Yn,i has density p(y|\u03b8\u2217i ) with respect to a base measure \u03bd, where \u03b8\u2217i \u2208 R is an unknown parameter associated with design i. This density takes the form\np(y|\u03b8) = b(y) exp{\u03b8T (y)\u2212A(\u03b8)} \u03b8 \u2208 R (1)\nwhere b, T , and A are known functions, and A(\u03b8) is assumed to be twice differentiable. We will assume that T is a strictly increasing function so that \u00b5(\u03b8) , \u00b4 yp(y|\u03b8)d\u03bd(y) is a strictly increasing function of \u03b8. Many common distributions can be written in this form, including Bernoulli, normal (with known variance), Poisson, exponential, chi-squared, and Pareto (with known minimal value).\nThroughout the paper, \u03b8\u2217 , (\u03b8\u22171, ..., \u03b8\u2217k) will denote the unknown true parameter vector, and \u03b8 and \u03b8\u2032 will be used to denote possible alternative parameter vectors. Let I\u2217 = arg max1\u2264i\u2264k \u03b8\u2217i denote the unknown best design. We will assume throughout that \u03b8\u2217i 6= \u03b8\u2217j for i 6= j so that I\u2217 is unique, although this can be relaxed by considering an indifference zone formulation where the goal is to identify an \u2013optimal design, for some specified tolerance level > 0.\nPrior and Posterior Distributions. The policies studied in this paper make use of a prior distribution \u03a01 over a set of possible parameters \u0398 that contains \u03b8\u2217. Based on a sequence of observations (I1, Y1,I1 , ..., In\u22121, Yn\u22121,In\u22121), beliefs are updated to attain a posterior distribution \u03a0n. We assume \u03a01 has density \u03c01 with respect to Lebesgue measure. In this case, the posterior distribution \u03a0n has corresponding density\n\u03c0n(\u03b8) = \u03c01(\u03b8)Ln\u22121(\u03b8)\u00b4\n\u0398 \u03c01(\u03b8\u2032)Ln\u22121(\u03b8\u2032)d\u03b8\u2032 n \u2265 2, (2)\nwhere\nLn\u22121(\u03b8) = n\u22121\u220f l=1 p(Yl,Il |\u03b8Il)\nis the likelihood function. While this formulation enforces some technical restrictions to facilitate theoretical analysis, it allows for very general prior distributions, and in particular allows for the quality of different designs to be correlated under the priors.\nOptimal Action Probabilities. Let\n\u0398i , { \u03b8 \u2208 \u0398 \u2223\u2223\u2223\u2223\u03b8i > maxj 6=i \u03b8j }\ndenote the set of parameters under which design i is optimal, and let\n\u03b1n,i , \u03a0n(\u0398i) = \u02c6\n\u0398i\n\u03c0n(\u03b8)d\u03b8 (3)\ndenote the posterior probability assigned to the event that action i is optimal. Our analysis will focus on \u03a0n(\u0398cI\u2217) = 1\u2212\u03b1I\u2217 , which is the posterior probability assigned to the event that an action other than I\u2217 is optimal. The next section will introduce policies under which \u03a0n(\u0398cI\u2217) \u2192 0 as n\u2192\u221e, and the rate of convergence is essentially optimal.\nFurther Notation. Before proceeding, we introduce some further notation. Let Fn denote the sigma algebra generated by (I1, Y1,I1 , ...In, Yn,In). For all i \u2208 {1, ..., k} and n \u2208 N, define\n\u03c8n,i , P(In = i|Fn\u22121) \u03a8n,i , n\u2211 `=1 \u03c8`,i \u03c8n,i , n \u22121\u03a8n,i.\nEach of these measures the effort allocated to design i up to time n."}, {"heading": "3 Algorithms", "text": "This section proposes three algorithms for allocating measurement effort. Each depends on a tuning parameter \u03b2 > 0, which will sometimes be set to a default value of 1/2. Each algorithm is based on the same high level principle. At every time step, each algorithm computes an estimate I\u0302 \u2208 {1, ..., k} of the optimal design, and measures that with probability \u03b2. Otherwise, we consider a counterfactual: in the (possibly unlikely) event that I\u0302 is not the best design, which alternative J\u0302 6= I\u0302 is most likely to be the best design? With probability 1 \u2212 \u03b2, the algorithm measures the alternative J\u0302 . The algorithms differ in how they compute I\u0302 and J\u0302 . The most computationally efficient is the modified version of Thompson sampling, under which I\u0302 and and J\u0302 are themselves randomly sampled from a probability distribution.\nWe will see that asymptotically all three algorithms allocate faction \u03b2 of measurement effort to measuring the estimated-best design, and the remaining fraction to gathering evidence about alternatives. The algorithms adjust how measurement effort is divided among these alternative designs as evidence is gathered, allocating less effort to measuring clearly inferior designs and greater effort to measuring designs that are more difficult to distinguish from the best."}, {"heading": "3.1 Top-Two Probability Sampling (TTPS)", "text": "With probability \u03b2, the top-two probability sampling (TTPS) policy plays the action I\u0302n = arg maxi \u03b1n,i which, under the posterior, is most likely to be optimal. When the algorithm does not play I\u0302n, it plays the most likely alternative J\u0302n = arg maxj 6=I\u0302n \u03b1n,j , which is the action that is second most likely to be optimal under the posterior. Put differently, the algorithm sets \u03c8n,I\u0302n = \u03b2, and \u03c8n,J\u0302n = 1\u2212\u03b2."}, {"heading": "3.2 Top-Two Value Sampling (TTVS)", "text": "We now propose a variant of top-two sampling that considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. In particular, we will define below a measure Vn,i of the value of design i under the posterior distribution at time n. Top-two value sampling computes the top-two designs under this measure: I\u0302n = arg maxi Vn,i and J\u0302n = arg maxj 6=I\u0302n Vn,j . It then plays the top design I\u0302n with probability \u03b2 and the best alternative J\u0302n otherwise. As observations are gathered, beliefs are updated and so the top two designs change over time. The measure of value Vn,i is defined below.\nThe definition of TTVS depends on a choice of (utility) function u : \u03b8 7\u2192 R, which encodes a measure of the value of discovering a design with quality \u03b8i. Two natural choices of u are u(\u03b8) = \u03b8 and u(\u03b8) = \u00b5(\u03b8). The paper\u2019s theoretical results allow u to be a general function, but we assume that it is continuous and strictly increasing. For a given choice of u, and any i \u2208 {1, ..., k}, the function\nvi(\u03b8) = max j u(\u03b8j)\u2212max j 6=i u(\u03b8j) = { 0 if \u03b8 /\u2208 \u0398i u(\u03b8i)\u2212maxj 6=i u(\u03b8j) if \u03b8 \u2208 \u0398i\nprovides a measure of the value of design i when the true parameter is \u03b8. It captures the improvement in decision quality due to design i\u2019s inclusion in the choice set. Let\nVn,i = \u02c6\n\u0398\nvi(\u03b8)\u03c0n(\u03b8)d\u03b8 = \u02c6\n\u0398i\nvi(\u03b8)\u03c0n(\u03b8)d\u03b8 (4)\ndenote the expected value of vi(\u03b8) under the posterior distribution at time n. This can be viewed as the option-value of design i: it is the expected additional value of having the option to choose design i when it is revealed to be the best design. Note that the integral (4) defining Vn,i is a weighted version of the integral defining \u03b1n,i. The paper will formalize a sense in which Vn,i and \u03b1n,i are asymptotically equivalent as n \u2192 \u221e, and as a result the asymptotic analysis of top-two value sampling essentially reduces to the analysis of top-two probability sampling."}, {"heading": "3.3 Thompson Sampling", "text": "Thompson sampling is an old and popular heuristic for multi-armed problems. The algorithm simply samples actions according to the posterior probability they are optimal. In particular, it selects action i with probability \u03c8n,i = \u03b1n,i, where \u03b1n,i denotes the probability action i is optimal under under a parameter drawn from the posterior distribution.\nThompson sampling can have very poor asymptotic performance for the best arm identification problem. Intuitively, this is because once it estimates that a particular arm is the best with reasonably high probability, it selects that arm in almost all periods at the expense of refining its knowledge of other arms. If \u03b1n,i = .95, then the algorithm will only select an action other than i roughly once every 20 periods, greatly extending the time it takes until \u03b1n,i > .99. This insight can be made formal; our results imply that Thompson sampling attains a only attains a polynomial, rather exponential, rate of posterior convergence. A similar reasoning applies to other multi-armed bandit algorithms. The work of Bubeck et al. [2009] shows formally that algorithms satisfying regret bounds of order log(n) are necessarily far from optimal for the problem of identifying the best arm.\nWith this in mind, it is natural to consider a modification of Thompson sampling that simply restricts the algorithm from sampling the same action too frequently. One version of this idea is proposed below."}, {"heading": "3.4 Top-Two Thompson Sampling (TTTS)", "text": "This section proposes top-two Thompson sampling (TTTS), which modifies standard Thompson sampling by adding a re-sampling step. As with TTPS and TTVS, this algorithm depends on a tuning parameter \u03b2 > 0 that will sometimes be set to a default value of 1/2.\nAs in Thompson sampling, at time n, the algorithm samples a design I \u223c \u03b1n. Design I is measured with probability \u03b2, but, in order to prevent the algorithm from exclusively focusing on one action, with probability 1\u2212 \u03b2, an alternative design is measured. To generate this alternative, the algorithm continues sampling designs J \u223c \u03b1n until the first time J 6= I. This can be viewed as a top-two sampling algorithm, where the top-two are chosen by executing Thompson sampling until two distinct designs are drawn.\nUnder top-two Thompson sampling, the probability of measuring design i at time n is\n\u03c8n,i = \u03b1n,i \u03b2 + (1\u2212 \u03b2)\u2211 j 6=i \u03b1n,j 1\u2212 \u03b1n,j  . This expression simplifies as the algorithm definitively identifies the best design. As \u03b1n,I\u2217 \u2192 1, \u03c8n,I\u2217 \u2192 \u03b2, and for each i 6= I\u2217,\n\u03c8n,i 1\u2212 \u03c8n,I\u2217 \u223c \u03b1n,i1\u2212 \u03b1n,I\u2217 .\nIn this limit, the true best design is sampled with probability \u03b2. The probability i is sampled given I\u2217 is not is equal to the posterior probability i is optimal given I\u2217 is not."}, {"heading": "3.5 Computing and Sampling According to Optimal Action Probabilities", "text": "Here we provide some insight into how to efficiently implement the proposed top-two rules in important problem classes. We begin with top-two Thompson sampling, which is often the easiest to implement. Note that given an ability to sample from \u03a0n, it is easy to sample from the posterior distribution over the optimal design \u03b1n. In particular, if \u03b8\u0302 \u223c \u03a0n is drawn randomly from the posterior, then arg maxi \u03b8\u0302i is a random sample from \u03b1n. Either through the choice of conjugate prior distributions, or through the use of Markov chain Monte Carlo, it is possible to efficiently sample from the posterior for many interesting models. Algorithm 1 shows how to directly sample an action according to TTTS by sampling from the posterior distribution. It is worth highlighting that this algorithm does not require computing or approximating the distribution \u03b1n.\nAlgorithm 1 Top-Two Thompson Sampling (\u03b2)\n1: Sample \u03b8\u0302 \u223c \u03a0n and set I \u2190 arg maxi \u03b8\u0302i . Apply Thompson sampling 2: Sample B \u223c Bernoulli(\u03b2) 3: if B = 1 then . Occurs with probability \u03b2. 4: Play I 5: else 6: repeat 7: Sample \u03b8\u0302 \u223c \u03a0n and set J \u2190 arg maxj \u03b8\u0302j . Repeat Thompson sampling 8: until J 6= I 9: Play J\n10: end if\nThe optimal action probabilities \u03b1n,i and values Vn,i are defined by k-dimensional integrals, which may be difficult to compute in general even if the posterior \u03a0n has a closed form. Algorithm 2 shows how to approximate \u03b1n,i and Vn,i using samples \u03b81 . . .\u03b8M , which enables efficient approximations to TTPS and TTVS whenever posterior samples can be efficiently generated.\nAlgorithm 2 SampleApprox(K,M, u,\u03b81, . . . ,\u03b8M ) 1: Si \u2190 {m|i = arg maxj \u03b8mj } \u2200i \u2208 {1, ..,K} 2: \u03b1\u0302i \u2190 |Si|/m \u2200i \u2208 {1, ..K} 3: V\u0302i \u2190M\u22121 \u2211 m\u2208Si ( u(\u03b8mi )\u2212maxj 6=i u(\u03b8mj ) ) \u2200i \u2208 {1, ..,K}\n4: return \u03b1\u0302, V\u0302\nThankfully, the computation of \u03b1n,i and Vn,i simplifies when the algorithm begins with an independent prior over the qualities \u03b81, ...\u03b8k of the k designs. To understand this fact, suppose X1, ..., Xk \u2208 R are independently distributed and continuous random variables. Then\nP(X1 = max i Xi) =\n\u02c6\nx\u2208R\nf1(x) k\u220f j=2 Fj(x)dx (5)\nwhere f1 denotes the PDF of X1 and F2, ..., FK are the CDFs of X2, .., Xk. In particular, P(X1 = maxiXi) can be computed by solving a 1-dimensional integral. Based on this insight, Appendix B provides an efficient implementation of TTPS and TTVS for a problem with independent Beta priors and binary observations. That implementation approximates integrals like (5) using quadrature with n points, and has the time and space complexity that scale as O(kn)."}, {"heading": "4 A Numerical Experiment", "text": "Some of the paper\u2019s main insights are reflected in a simple numerical experiment. Consider a problem where observations are binary Yn,i \u2208 {0, 1}, and the unknown vector \u03b8\u2217 = (.1, .2, .3, .4, .5) defines the true success probability of each design. Each algorithm begins with an independent uniform prior over the components of \u03b8\u2217. The experiment compares the performance of top-two probability sampling (TTPS), top-two value sampling (TTVS)2, and top-two Thompson sampling (TTTS) with \u03b2 = 1/2 against Thompson sampling and a uniform allocation rule which allocates equal measurement effort (\u03c8n,i = 1/5) to each design. The uniform allocation is an especially natural benchmark, as it is the most commonly used strategy in practice.\nFigure 1 displays the average number of measurements required for the posterior to reach a given confidence level. In particular, the experiment tracks the first time when maxi \u03b1n,i \u2265 c for various confidence levels c \u2208 (0, 1). Figure 1 displays the average number of measurements required for each algorithm to reach each fixed confidence level, where the average was taken over 100 trials in Panel (a) and 500 in Panel (b). Even for this simple problem with five designs, the proposed algorithms can reach the same confidence level using fewer than half the measurements required by a uniform allocation rule. While all the top-two rules attain the same asymptotic rate of convergence, we can see that top-two probability sampling is slightly outperformed in this experiment. Panel (a) compares Thompson sampling to Top-Two Thompson sampling. TS appears to reach low confidence levels as rapidly as top-two TS, but as suggested in Subsection 3.3, is very slow to reach high levels of confidence. It requires over than 60% more measurements to reach\n2TTVS is executed with the utility function u(\u03b8) = \u03b8\nconfidence .95 and over 250% more measurements to reach confidence .99. TS requires an onerous number of measurements to reach confidence .999, and so we omit this experiment.\nFigure 2 provides insight into how the proposed algorithms differ from the uniform allocation. It displays the distribution of measurements and posterior beliefs at the first time when a confidence level of .999 is reached. Again, all results are averaged across 500 trials. Panel (a) displays the average number of measurements collected from each design. It is striking that although TTTS, TTPS, and TTVS seem quite different, they all settle on essentially the same distribution of measurement effort. Because \u03b2 = 1/2, roughly one half of the measurements are collected from I\u2217 = 5. Moreover, fewer measurements are collected from designs that are farther from optimal, and most of the remaining half of measurement effort is allocated to design 4. Notice that using the same number of noisy samples it is much more difficult certify that \u03b8\u22174 < \u03b8\u22175 than that \u03b8\u22171 < \u03b8\u22175, both because \u03b8\u22174 is closer to \u03b8\u22175, and because observations from a Bernoulli distribution with parameter .4 have higher variance than under a Bernoulli distribution with parameter .1.\nPanel (b) investigates the posterior probability \u03b1n,i assigned to the event that design i is optimal. To make the insights more transparent, these are plotted on log-scale, where the value log(1/\u03b1n,i) can roughly be interpreted as the magnitude of evidence that alternative i is not optimal. By using an equal allocation of measurement effort across the designs, the uniform sampling rule gathers an enormous amount of evidence to rule out design 1, but an order of magnitude less evidence to rule out design 4. Instead of allocating measurement effort equally across the alternatives, TTTS, TTPS, and TTVS appear to exactly adjust measurement effort to gather equal evidence that each of the first four designs is not optimal.\nIntuitively, in the long run each of the proposed algorithms will allocate measurement effort to design 5\u2013the true best design\u2013and to whichever other designs could most plausibly be optimal. If too much measurement effort has been allocated to a particular design, then the posterior will indicate that it is clearly suboptimal, and effort will be allocated elsewhere until a similar amount of evidence has been gathered about other designs. In this way, measurement effort is automatically adjusted to the appropriate level."}, {"heading": "5 Main Theoretical Results", "text": "Our main theoretical results concern the frequentist consistency and rate of convergence of the posterior distribution. Recall that\n\u03a0n(\u0398cI\u2217) = \u2211 i 6=I\u2217 \u03b1n,i\ncaptures the posterior mass assigned to the event that an action other than I\u2217 is optimal. One hopes that \u03a0n(\u0398cI\u2217) \u2192 0 as the number of observations n tends to infinity, so that the posterior distribution converges on the truth. We will show that under the TTTS, TTPS, and TTVS allocation rules, \u03a0n(\u0398cI\u2217) converges to zero an exponential rate and that the exponent governing the rate of convergence is nearly the best possible.\nTo facilitate theoretical analysis, we will make three additional boundedness assumptions, which are assumed throughout all formal proofs. This rules out some cases of interest, such the use of multivariate Gaussian prior. However, we otherwise allow for quite general correlated priors, expressed in terms of a density over a compact set. Assumption 1 is used only in establishing posterior concentration results, and it is likely that these can be established under less restrictive technical conditions.\nAssumption 1. The parameter space is a bounded open hyper-rectangle \u0398 = (\u03b8, \u03b8)k, the prior density is uniformly bounded with\n0 < inf \u03b8\u2208\u0398 \u03c01(\u03b8) < sup \u03b8\u2208\u0398 \u03c01(\u03b8) <\u221e,\nand the log-partition function has bounded first derivative with sup\u03b8\u2208[\u03b8,\u03b8] |A \u2032(\u03b8)| <\u221e.\nThe paper\u2019s main results, as stated in the next theorem, characterize the rate of posterior convergence under the proposed algorithms, formalize a sense in which this is the fastest possible rate, and bound the impact of the tuning parameter \u03b2 \u2208 (0, 1). The statement depends on distribution-dependent constants \u0393\u2217\u03b2 > 0 and \u0393\u2217 > 0 that will be explicitly characterized in Section 6.\nThe first part of the theorem shows that there is an exponent \u0393\u2217 > 0 such that \u03a0n(\u0398cI\u2217) cannot converge to zero at a rate faster than e\u2212n\u0393\u2217 under any allocation rule, and shows that TTPS, TTVS and TTTS attain this optimal rate of convergence when the tuning parameter \u03b2 is set optimally.\nThe remainder of the theorem investigates the role of the tuning parameter \u03b2 \u2208 (0, 1). Part 2 shows that there is an exponent \u0393\u2217\u03b2 > 0 such that \u03a0n(\u0398cI\u2217)\u2192 0 at rate e\n\u2212n\u0393\u2217\u03b2 under TTPS, TTVS, or TTTS with parameter \u03b2, and this is shown to be optimal among a restricted class of allocation rules. In particular, we observe that \u03b2 controls the fraction of measurement effort allocated to the true best design I\u2217, in the sense that \u03c8n,I\u2217 \u2192 \u03b2 as n\u2192\u221e under each of the proposed algorithms. A lower bound establishes that no algorithm that allocates a faction \u03b2 of overall effort to measuring I\u2217 can converge at rate faster than e\u2212n\u0393 \u2217 \u03b2 . In this sense, while a tuning parameter controls the long-run measurement effort allocated to the true best design, TTPS, TTVS, and TTTS all automatically adjust how the remaining the measurement effort is allocated among the k\u2212 1 suboptimal designs in an asymptotically optimal manner. The final part of the theorem shows that \u0393\u2217\u03b2 is close to the largest possible exponent \u0393\u2217 whenever \u03b2 is close to the optimal value. The choice of \u03b2 = 1/2 is particularly robust: \u0393\u22171/2 is never more than a factor of 2 away from the optimal exponent.\nTheorem 1. There exist constants {\u0393\u2217\u03b2 > 0 : \u03b2 \u2208 (0, 1)} such that \u0393\u2217 = max\u03b2 \u0393\u2217\u03b2 exists, \u03b2\u2217 = arg max\u03b2 \u0393\u2217\u03b2 is unique, and the following properties are satisfied with probability 1:\n1. Under TTTS, TTPS, or TTVS with parameter \u03b2\u2217,\nlim n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) = \u0393\u2217.\nUnder any adaptive allocation rule,\nlim sup n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) \u2264 \u0393\u2217.\n2. Under TTTS, TTPS, or TTVS with parameter \u03b2 \u2208 (0, 1),\nlim n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) = \u0393\u2217\u03b2 and limn\u2192\u221e\u03c8n,I\u2217 = \u03b2.\nUnder any adaptive allocation rule,\nlim sup n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) \u2264 \u0393\u2217\u03b2 on any sample path with limn\u2192\u221e\u03c8n,I\u2217 = \u03b2.\n3. \u0393\u2217 \u2264 2\u0393\u22171 2 and\n\u0393\u2217 \u0393\u2217\u03b2 \u2264 max\n{ \u03b2\u2217\n\u03b2 , 1\u2212 \u03b2\u2217 1\u2212 \u03b2\n} .\nThis theorem is established in a sequence of results in Section 6. The lower bounds in parts 1 and 2 are given respectively in Propositions 5 and 6. Proposition 7 shows the top-two rules attain these optimal exponents. Part 3 is stated as Lemma 2 in Section 6."}, {"heading": "5.1 An upper bound on the error exponent", "text": "Before proceeding, we will state an upper bound on the error exponent when \u03b2 = 1/2 that is closely related to complexity terms that have appeared in the literature on best\u2013arm identification (e.g. Audibert and Bubeck [2010]). This bound depends on the gaps between the means of the different observation distributions.\nWe say that a real valued random variable X is \u03c3\u2013sub\u2013Gaussian if E [exp{\u03bb(X \u2212E[X])}] \u2264 exp { \u03bb2\u03c32\n2\n} so that the moment generating function of X \u2212 E[X] is dominated by that of a zero\nmean Gaussian random variable with variance \u03c32. Gaussian random variables are sub-Gaussian, as are uniformly bounded random variables. The next result applies to both Bernoulli and Gaussian distributions, as each can be parameterized with sufficient statistic T (y) = y.\nProposition 1. Suppose the exponential family distribution is parameterized with T (y) = y and that each \u03b8 \u2208 [\u03b8, \u03b8], if Y \u223c p(y|\u03b8), then Y is sub-Gaussian with parameter \u03c3. Then\n\u0393\u22171 2 \u2265 1 16\u03c32 \u2211 i 6=I\u2217 \u2206\u22122i\nwhere for each i \u2208 {1, ..., k}, \u2206i = E[Yn,I\u2217 ]\u2212E[Yn,i]\nis the difference between the mean under \u03b8\u2217I\u2217 and the mean under \u03b8\u2217i .\nThis shows that \u03a0n(\u0398cI\u2217) decays at asymptotic rate faster than exp{\u2212 nmini \u22062i\n16k\u03c32 }, so convergence is rapid when there is a large gap between the means of different designs. In fact, Proposition 1 replaces the dependence on (1/k) times the smallest gap \u2206i with a dependence on (\u2211k i=2 \u2206\u22122i )\u22121 , which captures the average inverse gap. This rate is attained only by an intelligent adaptive algorithm which allocates more measurement effort to designs that are nearly optimal and less to designs that are clearly suboptimal. In fact, the next result shows that the asymptotic performance of uniform allocation rule depends only on the smallest gap mini 6=I\u2217 \u22062i , and therefore even if some designs could be quickly ruled out, the algorithm can\u2019t leverage this to attain a faster rate of convergence.\nProposition 2. If Yn,I\u2217 \u223c N (0, \u03c32) and Yn,i \u223c N (\u2212\u2206i, \u03c32) for each i 6= I\u2217,\nlim n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) = \u2212nmini \u22062i 4k\u03c32\nunder a uniform allocation rule which sets \u03c8n,i = 1/k for each i and n."}, {"heading": "6 Analysis", "text": ""}, {"heading": "6.1 Asymptotic Notation.", "text": "To simplify the presentation, it is helpful to introduce additional asymptotic notation. We say two sequences an and bn taking values in R are logarithmically equivalent, denoted by an\n.= bn, if 1 n log( an bn\n) \u2192 0 as n \u2192 \u221e. This notation means that an and bn are equal up to first order in the exponent. With this notation, Theorem 1 implies the top-two sampling rules with parameter \u03b2 attain the convergence rate \u03a0n(\u0398cI\u2217) .= e\u2212n\u0393 \u2217 \u03b2 . This is an equivalence relation, in the sense that if an .= bn and bn .= cn then an .= cn. Note that an + bn .= max{an, bn}, so that the sequence with\nthe largest exponent dominates. In addition for any positive constant c, can .= an, so that constant multiples of sequences are equal up to first order in the exponent. When applied to sequences of random variables, these relations are understood to apply almost surely.\nIt is natural to wonder whether the proposed algorithms asymptotically minimize expressions like \u2211 i 6=I\u2217(\u03b8\u2217I\u2217 \u2212 \u03b8i)\u03b1n,i, which account for how far some designs are from optimal. We note in\npassing, that \u2211 i 6=I\u2217 ci\u03b1n,i .= max i 6=I\u2217 \u03b1n,i\nfor any positive costs ci > 0, and so any such performance measures are equal to first order in the exponent. Similar observations have been used to justify the study of the probability of incorrect selection, rather than notions of the expected cost of an incorrect decision [Glynn and Juneja, 2004, Audibert and Bubeck, 2010]."}, {"heading": "6.2 Posterior Consistency", "text": "The next proposition provides a consistency and anti-consistency result for the posterior distribution. The first part says that if design i receives infinite measurement effort, the marginal posterior distribution of its quality concentrates around the true value \u03b8\u2217i . The second part says that when restricted to designs that are not measured infinitely often, the posterior does not concentrate around any value. The posterior converges to the truth as infinite evidence is collected, but nothing can be ruled out with certainty based on finite evidence.\nProposition 3. With probability 1, for any i \u2208 {1, .., k} if \u03a8n,i \u2192\u221e, then, for all > 0\n\u03a0n({\u03b8 \u2208 \u0398|\u03b8i /\u2208 (\u03b8\u2217i \u2212 , \u03b8\u2217i + )})\u2192 0.\nIf I = {i \u2208 {1, ..., k}| limn\u2192\u221e\u03a8n,i <\u221e} is nonempty, then\ninf n\u2208N \u03a0n({\u03b8 \u2208 \u0398|\u03b8i \u2208 (\u03b8\u2032i, \u03b8\u2032\u2032i ) \u2200i \u2208 I}) > 0\nfor any collections of open intervals (\u03b8\u2032i, \u03b8\u2032\u2032i ) \u2282 (\u03b8, \u03b8) ranging over i \u2208 I.\nThis result is the key to establishing that \u03b1n,I\u2217 \u2192 1 under each of the proposed algorithm. The next subsection gives a more refined result that allows us to to characterize the rate of convergence."}, {"heading": "6.3 Posterior Large Deviations", "text": "This section provides an asymptotic characterization of posterior probabilities \u03a0n(\u0398\u0303) for any open set \u0398\u0303 \u2282 \u0398 and under any adaptive measurement strategy. The characterization depends on the notion of Kullback-Leibler divergence. For two parameters \u03b8, \u03b8\u2032 \u2208 R, the log-likelihood ratio, log (p(y|\u03b8)/p(y|\u03b8\u2032)), provides a measure of the amount of information y provides in favor of \u03b8 over \u03b8\u2032. The Kullback-Leibler divergence\nd(\u03b8||\u03b8\u2032) , \u02c6 log ( p(y|\u03b8) p(y|\u03b8\u2032) ) p(y|\u03b8)d\u03bd(y).\nis the expected value of the log-likelihood under observations drawn p(y|\u03b8). Then, if the design to measure is chosen by sampling from a probability distribution \u03c8 over {1, .., k},\nD\u03c8(\u03b8||\u03b8\u2032) , k\u2211 i=1 \u03c8id(\u03b8i||\u03b8\u2032i)\nis the average Kullback-Leibler divergence between \u03b8 and \u03b8\u2032 under \u03c8. Under the algorithms we consider, the effort allocated to measuring design i, \u03c8n,i , P(In =\ni|Fn\u22121), changes over time as data is collected. Recall that \u03c8n,i , n\u22121 \u2211n `=1 \u03c8`,i captures the fraction of overall effort allocated to measuring design i over the first n periods. Under an adaptive allocation rule, \u03c8n is function of the history (I1, Y1,I1 , ...In\u22121, Yn\u22121,In\u22121) and is therefore a random variable. Given that measurement effort has been allocation according to \u03c8n, D\u03c8n(\u03b8\n\u2217||\u03b8) quantifies the average information acquired that distinguishes \u03b8 from the true parameter \u03b8\u2217. The following proposition relates the posterior mass assigned to \u0398\u0303 to inf\u03b8\u2208\u0398\u0303D\u03c8n(\u03b8\n\u2217||\u03b8), which captures the element in \u0398\u0303 that is hardest to distinguish from \u03b8\u2217 based on samples from \u03c8n.\nProposition 4. For any open set \u0398\u0303 \u2282 \u0398,\n\u03a0n(\u0398\u0303) .= exp { \u2212n inf\n\u03b8\u2208\u0398\u0303 D\u03c8n\n(\u03b8\u2217||\u03b8) } .\nTo understand this result, consider a simpler setting where the algorithm measures design i in every period, and consider some \u03b8 with \u03b8i 6= \u03b8\u2217i . Then the log-ratio of posteriors densities\nlog ( \u03c0n(\u03b8) \u03c0n(\u03b8\u2217) ) = log ( \u03c01(\u03b8) \u03c01(\u03b8\u2217) ) + n\u22121\u2211 `=1 log ( p(Y`,i|\u03b8i) p(Y`,i|\u03b8\u2217i ) )\ncan be written as the sum of the log-prior-ratio and the log-likelihood-ratio. The log-likelihood ratio is negative drift random walk: it is the sum of n\u2212 1 i.i.d terms, each of which has mean\nE [ log ( p(Y1,i|\u03b8i) p(Y1,i|\u03b8\u2217i ) )] = E [ \u2212 log ( p(Y1,i|\u03b8\u2217i ) p(Y1,i|\u03b8i) )] = \u2212d(\u03b8\u2217i ||\u03b8i).\nTherefore, by the law of large numbers, as n\u2192\u221e, n\u22121 log (\u03c0n(\u03b8)/\u03c0n(\u03b8\u2217))\u2192 \u2212d(\u03b8\u2217i ||\u03b8i), or equivalently, the ratio of the posterior densities decays exponentially as\n\u03c0n(\u03b8) \u03c0n(\u03b8\u2217) .= exp{\u2212nd(\u03b8\u2217i ||\u03b8i}.\nThis calculation can be carried further to show that if the designs measured (I1, I2, I3, ...) are drawn independently of the observations (Y1,Y2,Y3, ...) from a fixed probability distribution \u03c8, then\n\u03c0n(\u03b8) \u03c0n(\u03b8\u2217) .= exp {\u2212nD\u03c8(\u03b8\u2217||\u03b8)} . (6)\nNow, by a Laplace approximation, one might expect that the integral \u00b4\n\u0398\u0303 \u03c0n(\u03b8)d\u03b8 is extremely well approximated by integrating around a vanishingly small ball around the point\n\u03b8\u0302 = arg min \u03b8\u2208\u0398\u0303\nD\u03c8(\u03b8\u2217||\u03b8).\nThese are the main ideas behind Proposition 4, but there are several additional technical challenges involved in a rigorous proof. First, we need that a property like (6) holds when the allocation rule is adaptive to the data. Next, convergence of the integral of the posterior density requires a form of uniform convergence in (6). Finally, since \u03c8n changes over time, the point arg min\n\u03b8\u2208\u0398\u0303 D\u03c8n\n(\u03b8\u2217||\u03b8)\nchanges over time and basic Laplace approximations don\u2019t directly apply."}, {"heading": "6.4 Characterizing the Optimal Allocation", "text": "Throughout this paper, an experimenter wants to gather enough evidence to certify that I\u2217 is optimal, but since she does not know \u03b8\u2217, she does not know which measurements will provide the most information. To characterize the optimal exponent \u0393\u2217, however, it is useful to consider the easier problem of gathering the most effective evidence when \u03b8\u2217 is known. We can cast this as a game between two players:\n\u2022 An experimenter, who knows the true parameter \u03b8\u2217, chooses a (possibly adaptive) measurement rule.\n\u2022 A referee observes the resulting sequence of observations (I1, Y1,I1 , ..., In\u22121, Yn\u22121,In\u22121) and computes posterior beliefs (\u03b1n,1, .., \u03b1n,k) according to Bayes rule (2, 3).\n\u2022 How can the experimenter gather the most compelling evidence? A rule which is optimal asymptotically should maximize the rate at which \u03b1n,I\u2217 \u2192 1 as n\u2192\u221e.\nIn order to drive the posterior probability \u03b1n,I\u2217 to 1, the decision-maker must be able to rule out all parameters in \u0398cI\u2217 under which the optimal action is not I\u2217. Our analysis shows that the posterior probability assigned to \u0398cI\u2217 is dominated by the parameter that is hardest to distinguish from \u03b8\u2217 under \u03c8n. In particular, by Proposition 4,\n\u03a0n(\u0398cI\u2217) .= exp { \u2212n ( min \u03b8\u2208\u0398c\nI\u2217 D\u03c8n\n(\u03b8\u2217||\u03b8) )}\nas n\u2192\u221e. Therefore, the solution to the max-min problem\nmax \u03c8 min \u03b8\u2208\u0398c I\u2217 D\u03c8(\u03b8\u2217||\u03b8) (7)\nrepresents an asymptotically optimal allocation rule. As highlighted in the literature review, the max-min problem (7) closely mirrors the main sample complexity term in Chernoff\u2019s classic paper on the sequential design of experiments (Chernoff [1959]).\nSimplifying the optimal exponent. Thankfully, the best-arm identification problem has additional structure which allows us to simplify the optimization problem (7). Much of our analysis involves the posterior probability probability assigned to the event some action i 6= I\u2217 is optimal. This can be difficult to evaluate, since the set of parameter vectors under which i is optimal\n\u0398i = {\u03b8 \u2208 \u0398|\u03b8i \u2265 \u03b81, ...\u03b8i \u2265 \u03b8k}\ninvolves k separate constraints. Consider instead a simpler problem of comparing the parameter \u03b8\u2217i against \u03b8\u2217I\u2217 . For each i 6= I\u2217 define the set\n\u0398i , {\u03b8 \u2208 \u0398|\u03b8i \u2265 \u03b8I\u2217} \u2283 \u0398i\nunder which the value at i exceeds that at I\u2217. Since, ignoring the boundary of the set, \u0398cI\u2217 = \u222ai 6=I\u2217\u0398i,\nmax i 6=I\u2217 \u03a0n(\u0398i) \u2264 \u03a0n(\u0398cI\u2217) \u2264 kmax i 6=I\u2217 \u03a0n(\u0398i)\nand therefore \u03a0n(\u0398cI\u2217)\n.= max i 6=I\u2217 \u03a0n(\u0398i). (8)\nThis yields an analogue of (7) that will simplify our subsequent analysis. Combining (8) with Proposition 4 shows the solution to the max-min problem\n\u0393\u2217 , max \u03c8 min i 6=I\u2217 min \u03b8\u2208\u0398i D\u03c8(\u03b8\u2217||\u03b8) (9)\nrepresents an asymptotically optimal allocation rule. Because the set \u0398i involves only a constraints on \u03b8i and \u03b8I\u2217 , we can derive an expression the inner minimization problem over \u03b8 in terms of the measurement effort allocated to i and I\u2217. Define\nCi(\u03b2, \u03c8) , min x\u2208R \u03b2d(\u03b8\u2217I\u2217 ||x) + \u03c8d(\u03b8\u2217i\u2217 ||x). (10)\nThe next lemma shows that the function Ci arises as the solution to the minimization problem over \u03b8 \u2208 \u0398i in (9). It also shows that the minimum in (10) is attained by a parameter \u03b8 under which the mean observation is a weighted combination of the means under \u03b8\u2217I\u2217 and \u03b8\u2217i . Recall that, for an exponential family distribution A\u2032(\u03b8) = \u00b4 T (y)p(y|\u03b8)d\u03bd(y) is the mean observation of the sufficient statistic T (y) under \u03b8.\nLemma 1. For any i \u2208 {1, .., k} and probability distribution \u03c8 over {1, ..., k}\nmin \u03b8\u2208\u0398i\nD\u03c8(\u03b8\u2217||\u03b8) = Ci(\u03c8I\u2217 , \u03c8i)"}, {"heading": "In addition, each Ci is a strictly increasing concave function satisfying", "text": "Ci(\u03c8I\u2217 , \u03c8i) = \u03c8I\u2217d(\u03b8\u2217I\u2217 ||\u03b8) + \u03c8id(\u03b8\u2217i ||\u03b8),\nwhere \u03b8 \u2208 [\u03b8\u2217i , \u03b8\u2217I\u2217 ] is the unique solution to\nA\u2032(\u03b8) = \u03c8I \u2217A\u2032(\u03b8\u2217I\u2217) + \u03c8iA\u2032(\u03b8\u2217i )\n\u03c8I\u2217 + \u03c8i .\nLemma 1 and equation (9) immediately imply\n\u0393\u2217 = max \u03c8 min i 6=I\u2217 Ci(\u03c8I\u2217 , \u03c8i). (11)\nThe function Ci(\u03b2, \u03c8) captures the effectiveness with which one can certify \u03b8\u2217I\u2217 \u2265 \u03b8\u2217i using an allocation rule that measures actions I\u2217 and i with respective frequencies \u03b2 and \u03c8. Naturally, it is an increasing function of the measurement effort (\u03b2, \u03c8) allocated to designs I\u2217 and i. For given \u03b2 and \u03c8, Ci(\u03b2, \u03c8) \u2265 Cj(\u03b2, \u03c8) when \u03b8\u2217i \u2264 \u03b8\u2217j , reflecting that \u03b8\u2217i is easier to distinguish from \u03b8\u2217I\u2217 than \u03b8\u2217j .\nExample 1. (Gaussian Observations) Suppose each outcome distribution p(y|\u03b8\u2217i ) is Gaussian with unknown mean \u03b8\u2217i . Then direct calculation using Lemma 1 shows\nCi(\u03b2, \u03c8i) = (\n\u03b2\u03c8i \u03b2 + \u03c8i ) (\u03b8\u2217I\u2217 \u2212 \u03b8\u2217i )2 2 .\nTo understand this formula, imagine we use a deterministic allocation rule that collects n\u03b2 and n\u03c8i observations from I\u2217 and i. Let XI\u2217 and Xi denote the respective sample means. The empirical difference is normally distributed XI\u2217 \u2212 Xi \u223c N ( \u2206, \u03c32/n ) where \u2206 = \u03b8\u2217I\u2217 \u2212 \u03b8\u2217i and \u03c32 = 1/\u03b2 + 1/\u03c8i = (\u03b2 + \u03c8i)/(\u03b2\u03c8i). Standard Gaussian tail bounds imply that as n \u2192 \u221e, P(XI\u2217 \u2212Xi < 0) .= exp(\u2212n/2(\u03c3\u2206)2), and so Ci(\u03b2, \u03c8i) appears to characterize the probability of error.\nThe next proposition formalizes the derivations in this section, and states that the solution to the above maximization problem attains the optimal error exponent. Recall that \u03c8n,i , P(In = i|Fn\u22121) denotes the measurement effort assigned design i at time n.\nProposition 5. Let \u03c8\u2217 denote the optimal solution to the maximization problem (11). If \u03c8n = \u03c8\u2217 for all n, then\n\u03a0n(\u0398cI\u2217) .= exp{\u2212n\u0393\u2217}.\nMoreover under any other adaptive allocation rule,\nlim sup n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) \u2264 \u0393\u2217.\nThis shows that under the fixed allocation rule \u03c8\u2217 error decays as e\u2212n\u0393\u2217 , and that no faster rate of decay is possible, even under an adaptive allocation.\nAn Optimal Constrained Allocation. Because the algorithms studied in this paper always allocate \u03b2\u2013fraction of their samples to measuring I\u2217 in the long run, they may not exactly attain the optimal error exponent. To make rigorous claims about their performance, consider a modified version of the error exponent (11) given by the constrained max-min problem\n\u0393\u2217\u03b2 , max \u03c8:\u03c8I\u2217=\u03b2 min i 6=I\u2217 Ci(\u03b2, \u03c8i). (12)\nThis optimization problem yields the optimal allocation subject to a constraint that \u03b2\u2013fraction of the samples are spent on I\u2217. The next subsection will show that TTTS, TTPS, and TTVS attain the error exponent \u0393\u2217\u03b2. The next proposition formalizes that the solution to this optimization problem represents an optimal constrained allocation. In addition, it shows that the solution is the unique feasible allocation under which Ci(\u03b2, \u03c8i) is equal for all suboptimal designs i 6= I\u2217. To understand this result, consider the case where there are three designs and \u03b8\u22171 > \u03b8\u22172 > \u03b8\u22173. If \u03c82 = \u03c83, then C2(\u03b2, \u03c82) < C3(\u03b2, \u03c83), reflecting that it is more difficult to certify that \u03b8\u22172 \u2264 \u03b8\u2217I\u2217 than \u03b8\u22173 \u2264 \u03b8\u2217I\u2217 . The next proposition shows it is optimal to decrease \u03c82 and increase \u03c81, until the point when C2(\u03b2, \u03c82) = C3(\u03b2, \u03c83). Instead of allocating equal measurement effort to each alternative, it is optimal to adjust measurement effort to gather equal evidence to rule out each suboptimal alternative. The results in this proposition are closely related to those in Glynn and Juneja [2004], in which large deviations rate functions take the place of the functions Ci.\nProposition 6. The solution to the optimization problem (12) is the unique allocation \u03c8\u2217 satisfying \u03c8\u2217I\u2217 = \u03b2 and\nCi(\u03b2, \u03c8i) = Cj(\u03b2, \u03c8j) \u2200 i, j 6= I\u2217.\nIf \u03c8n = \u03c8\u2217 for all n, then \u03a0n(\u0398cI\u2217) .= exp{\u2212n\u0393\u2217\u03b2}.\nMoreover under any other adaptive allocation rule, if \u03c8n,I\u2217 \u2192 \u03b2 then\nlim sup n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) \u2264 \u0393\u2217\u03b2\nalmost surely.\nThe following lemma relates the constrained exponent \u0393\u2217\u03b2 to \u0393\u2217.\nLemma 2. For \u03b2\u2217 = arg max\u03b2 \u0393\u2217\u03b2 and any \u03b2 \u2208 (0, 1),\n\u0393\u2217 \u0393\u2217\u03b2 \u2264 max\n{ \u03b2\u2217\n\u03b2 , 1\u2212 \u03b2\u2217 1\u2212 \u03b2\n} .\nTherefore \u0393\u2217 \u2264 2\u0393\u22171/2."}, {"heading": "6.5 Optimal Adaptive Allocation", "text": "Two Stage Procedures. While the last subsection describes an asymptotically optimal exploration strategy, implementing this strategy requires knowledge of the parameter vector \u03b8\u2217. One simple approach to attaining the rate (11) is to split the experiment into two phases. For the first o(n) periods the algorithm selects actions uniformly at random, after which it constructs a point estimate \u03b8\u0302 of \u03b8\u2217. In the second phase, it solves the optimization problem (11) with \u03b8\u0302 in place of of \u03b8\u2217, and follows that allocation for the remaining periods. In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982].\nThese two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures \u201ctypically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.\u201d In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but \u201cthe approach is very coarse for moderate sample size problems.\u201d He writes that two-stage procedures of Kiefer and Sacks [1963], \u201csidestep the issue of how to experiment in the early stages,\u201d while constructing the optimal allocations based on point estimates \u201ctreats estimates of \u03b8 based on a few observations with as much respect as that based on many observations.\u201d\nConvergence of Top-Two Algorithms. Instead of attempting to directly solve the optimization problem (11), this paper focuses on simple and intuitive sequential strategies. These algorithms have the potential to explore much more intelligently in early stages, as they carefully measure and reason about uncertainty. While they ostensibly have no connection to the derivations earlier in this section, we establish that remarkably all three automatically converge to the unknown optimal allocation. This is shown formally in the next result.\nWe are now ready to establish the paper\u2019s main claim, which shows that TTTS, TTPS, and TTVS each attain the error exponent \u0393\u2217\u03b2.\nProposition 7. Under the TTTS, TTPS, or TTVS algorithm with parameter \u03b2 > 0, \u03c8n \u2192 \u03c8\u03b2, where \u03c8\u03b2 is the unique allocation with \u03c8\u03b2I\u2217 = \u03b2 satisfying\nCi(\u03b2, \u03c8\u03b2i ) = Cj(\u03b2, \u03c8 \u03b2 j ) \u2200i, j 6= I \u2217.\nTherefore, \u03a0n(\u0398cI\u2217) .= e\u2212n\u0393 \u2217 \u03b2 .\nTo understand this result, imagine that n is very large, and \u03c8n,I\u2217 \u2248 \u03b2. If the algorithm has allocated too much measurement effort to a suboptimal action i, with \u03c8n,i > \u03c8 \u03b2 i + \u03b4 for a constant \u03b4 > 0, then it must have allocated too little measurement effort to at least one other suboptimal\ndesign j 6= i. Since much less evidence has been gathered about j than i, we expect \u03b1j,n >> \u03b1j,i. When this occurs, TTTS, TTPS and TTVS essentially never sample action i until the average effort \u03c8n,i allocated to design i dips back down toward \u03c8 \u03b2 i . This seems to suggest that the algorithm cannot allocate too much effort to any alternative, but that in turn implies that it never allocates too little effort to measuring any alternative."}, {"heading": "6.6 Asymptotics of the Value Measure", "text": "The proof for top-two value sampling relies on the following lemma, which shows that the posteriorvalue of any suboptimal design is logarithmically equivalent to its probability of being optimal.\nLemma 3. For any i 6= I\u2217, Vn,i .= \u03b1n,i\nNote that by this lemma, \u03a0n(\u0398cI\u2217) = \u2211 i 6=I\u2217 \u03b1n,i .= \u2211 i 6=I\u2217 Vn,i,\nand so all of the asymptotic results in this could be reformulated as statements concerning the value assigned to suboptimal alternatives under the posterior.\nThe lemma is not so surprising, as Vn,i = \u00b4 \u0398i vi(\u03b8)\u03c0n(\u03b8)d\u03b8 differs from \u03b1n,i = \u00b4\n\u0398i \u03c0n(\u03b8)d\u03b8 only because of the function vi(\u03b8). The \u03c0n(\u03b8) term dominates this integral as n\u2192\u221e, since it tends to zero at an exponential rate in n whereas vi(\u03b8) is a fixed function of n."}, {"heading": "7 Extensions and Open Problems", "text": "This paper studies efficient adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. We propose three simple Bayesian algorithms. Each is a variant of what we call top-two sampling, which, at each time-step, measures one of the two designs that appear most promising given current evidence. Surprisingly, these seemingly naive algorithms are shown to satisfy a strong asymptotic optimality property.\nTop two sampling appears to be a general design principle that can be extended to address a variety of problems beyond to the scope of this paper. To spur research in this area, we briefly discuss a number of extensions and open questions below.\nTop-Two Sampling Via Constrained MAP Estimation. Here we present a version of toptwo sampling that uses MAP estimation. This can simplify computations, as MAP estimates can be computed without solving for the normalizing constant of the posterior density \u03c0n(\u03b8). Consider the following procedure for selecting a design at time n:\n1. Compute \u03b8\u0302 \u2208 arg max\u03b8\u2208\u0398 \u03c0n(\u03b8) and set I\u0302n = arg maxi \u03b8\u0302i.\n2. Compute \u03b8\u0302\u2032 \u2208 arg max\u03b8\u2208\u0398c I\u0302n \u03c0n(\u03b8) and set J\u0302n = arg maxi \u03b8\u0302\u2032i.\n3. Play (I\u0302n, J\u0302n) with respective probabilities (\u03b2, 1\u2212 \u03b2).\nThe first step uses MAP estimation to make a prediction I\u0302n of the best design, while the second uses constrained MAP estimation to identify the alternative design that is most likely to be optimal when I\u0302n is not. Many of the asymptotic calculations in the previous section appear to extend to this algorithm, but proving this formally is left as an open problem.\nIndifference Zone Criterion. Suppose our goal is to confidently identify an \u2013optimal arm, for a user specified indifference parameter > 0. Much of the paper investigates the set of parameters \u0398i under which arm i is optimal, and studies the rate at which \u03a0n(\u0398I\u2217)\u2192 1. Now, let us instead consider the set of parameters\n\u0398 ,i = {\u03b8|\u03b8i \u2265 max j \u03b8j \u2212 }\nunder which i is \u2013optimal. It is easy to develop a variety of modified top-two sampling rules under which maxi \u03a0n(\u0398 ,i) \u2192 1 rapidly. For example, we can extend TTPS as follows: set I\u0302n = arg maxi \u03a0n(\u0398 ,i). Define J\u0302n = arg maxj 6=I\u0302n \u03a0n(\u03b8|\u03b8j = maxi \u03b8i & \u03b8j > \u03b8I\u0302n + ) to be the alternative design that is most likely to be optimal and offer an \u2013improvement over I\u0302n. A top-two Thompson sampling approach might instead continue sampling \u03b8 \u223c \u03a0n until maxi \u03b8i > \u03b8I\u0302n + and then set Jn = arg maxi \u03b8i.\nTop m\u2013arm identification. Suppose now that our goal is to identify the top m < k designs. Consider choosing a design to measure at time n by the following steps:\n1. Sample \u03b8 \u223c \u03a0n and compute the top m designs under \u03b8.\n2. Continue sampling \u03b8\u2032 \u223c \u03a0n until the top m designs under \u03b8\u2032 differ from those under \u03b8.\n3. Identify the set of designs that are in the top m under \u03b8 or under \u03b8\u2032, but not under both. Choose a design to measure by sampling one uniformly at random from this set.\nThis is the natural extension of top-two Thompson sampling to the top-m arm problem. In fact, when m = 1, this is exactly TTTS with \u03b2 = 1/2. I conjecture that like the case where m = 1, this algorithm attains a rate of posterior convergence within a factor of 2 of optimal for general m. The optimal exponent for this problem can be calculated by mirroring the steps in Subsection 6.4.\nExtremely Correlated Designs. While our results apply in the case of correlated priors, the proposed algorithms may be wasteful when there are a large number of designs whose qualities are extremely correlated. As an example, consider an extension of our techniques to a pure-exploration variant of a linear bandit problem. Here we associate each action i with a feature vector xi \u2208 Rd and seek an action that maximizes xTi \u03b8. The vector \u03b8 \u2208 Rd is unknown, but we begin with a prior \u03b8 \u223c N(0, I) and see noisy observations of xTi \u03b8 whenever action i is selected. To apply top-two sampling to this problem, we should modify the algorithm\u2019s second step. For example, under toptwo Thompson sampling, we usually begin drawing a design according to i\u0302 \u223c \u03b1n, and then continue drawing designs j\u0302 \u223c \u03b1n until i\u0302 6= j\u0302. These are played with respective probabilities (\u03b2, 1\u2212 \u03b2). But even if i\u0302 6= j\u0302, their features may be nearly identical. A more natural extension of top-two Thompson sampling would modify the second step, and continue sampling j\u0302 \u223c \u03b1n, until a sufficiently different action is drawn \u2013 for example until the angle between xj\u0302 and xi\u0302 exceeds a threshold.\nTuning \u03b2. The most glaring gap in this work may be arbitrary choice of tuning parameter \u03b2. Optimal asymptotic rates can be attained by adjusting this parameter over time by solving for an optimal allocation as in (11). It is an open problem to instead develop simple algorithms that set \u03b2 automatically through value of information calculations, or avoid the need for such a parameter altogether.\nAdaptive Stopping. This paper proposed only an allocation rule, which determines the sequence of measurements to draw, but this can be coupled with a rule that determines when to stop sampling. One natural stopping rule in a Bayesian framework is to stop when maxi \u03b1n,i > 1 \u2212 \u03b4 for some \u03b4 > 0. Let \u03c4\u03b4 be a random variable indicating the stopping time under constraint \u03b4. Since 1 \u2212maxi \u03b1n,i .= e\u2212n\u0393 \u2217 \u03b2 under top-two sampling, our results imply that for each sample path \u03c4\u03b4 \u223c \u0393\u2217\u03b2 log(1/\u03b4) as \u03b4 \u2192 0. It is natural to conjecture that E[\u03c4\u03b4] \u223c \u0393\u2217\u03b2 log(1/\u03b4) as well. This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(\u03b4) as \u03b4 \u2192 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[\u03c4\u03b4] as in Jennison et al. [1982] or Kaufmann [2016]."}, {"heading": "A Outline", "text": "This technical appendix is organized as follows.\n1. Section B describes a numerical algorithm that can be used to implement TTPS.\n2. Section C provides a more precise discussion of related work by Ryzhov [2016].\n3. The theoretical analysis begins in Section D. There we begin by noting some basic facts of exponential family distributions, as well as some results relating martingales to their quadratic variation process.\n4. Section E establishes results related to the concentration of the posterior distribution, including the proofs of Prop. 3, Prop. 4, and Lemma 3.\n5. Section F studies and simplifies the optimal exponents \u0393\u2217 and \u0393\u2217\u03b2, including the proofs of Lemma 1, Prop. 6, Lemma 2, Prop. 1, and Prop. 2.\n6. We conclude with Section G, which studies the top-two allocation rules and provides a proof of Prop. 7."}, {"heading": "B An Implementation of TTPS", "text": "This section describes an implementation of the top-two probability sampling for a problem with a Beta prior and binary observations. In this problem, measurements are binary with success probability given by P(Yn,i = 1) = \u03b8\u2217i . The algorithm begins with an independent prior, under which the ith component of \u03b8 follows a Beta distribution with parameters (\u03bb1i , \u03bb2i ). When \u03bb2i = \u03bb2i = 1, this specifies a uniform prior over [0, 1]. This prior distribution can be easily updated to form a posterior distribution according to the update rule given in line 19 of Algorithm 3.\nThis algorithm uses quadrature to approximate the integral defining \u03b1n,i. To understand this implementation, consider a random vector (X1, .., XK) whose components are independently distributed with Xi \u223c Beta(\u03bb1i , \u03bb2i ). Then, the probability component i is maximal can be computed according to\nP(Xi = max j Xj) =\n\u02c6\nx\u2208R\nP(\u2229j 6=i{Xj \u2264 x})P(Xi = dx)\n= \u02c6\nx\u2208R\n\u220f j 6=i P(Xj \u2264 x) P(Xi = dx) = \u02c6\nx\u2208R\n K\u220f j=1 P(Xj \u2264 x)  /P(Xi \u2264 x) P(Xi = dx).\nAlgorithm 3 takes as input a vector of x consisting of M points in (0, 1) and approximates the above integral using quadrature at these points. The algorithm computes and updates the posterior PDF and CDF of \u03b8i in an M dimensional vectors fi and Fi. It also stores and updates a vector F = \u220fK i=1 Fi,m, where Fm is the posterior probability all the designs have quality below xm. Using these quantities, the posterior probability design i is optimal is approximated by a sum in line 8. Lines 11-15 select an action according to TTPS and lines 18-21 update the stored statistics of the posterior using Bayes rule. The algorithm continues for N time steps, and upon stopping returns the posterior parameters \u03bb1 and \u03bb2, which summarize all evidence gathered throughout the measurement process. The algorithm has O(NKM) space and time complexity. It is worth noting that most operations in this algorithm can be implemented in a \u201cvectorized\u201d fashion in languages like MATLAB, NumPy, and Julia.\nAlgorithm 3 BernoulliTTPS(\u03b2,K,M,N,\u03bb1,\u03bb2,x) 1: \\\\Initialize: 2: fi,m \u2190 Beta.pdf(xm|\u03bb1i , \u03bb2i ) \u2200i,m 3: Fi,m \u2190 Beta.cdf(xm|\u03bb1i , \u03bb2i ) \u2200i,m 4: Fm \u2190 \u220f i Fi,m \u2200m\n5: 6: for n = 1 . . . N do 7: \\\\Compute Optimal Action Probabilities: 8: \u03b1i \u2190 \u2211 m fi,mFm/Fi,m \u2200i\n9: 10: \\\\Act and Observe: 11: J1 \u2190 arg maxi \u03b1i 12: J2 \u2190 arg maxi 6=J1 \u03b1i 13: Sample B \u223c Bernoulli(\u03b2) 14: I \u2190 BJ1 + (1\u2212B)J2. 15: Play I and Observe Yn,I \u2208 {0, 1}. 16: 17: \\\\Update Statistics: 18: (\u03bb1I , \u03bb2I)\u2190 (\u03bb1I , \u03bb2I) + (Yn,I , 1\u2212 Yn,I) 19: Fm \u2190 (Fm/FI,m)\u00d7 Beta.cdf(xm|\u03bb1I , \u03bb2I) \u2200m 20: FI,m \u2190 Beta.cdf(xm|\u03bb1I , \u03bb2I) \u2200m 21: fI,m \u2190 Beta.pdf(xm|\u03bb1I , \u03bb2I) \u2200m 22: end for 23: return V ,\u03bb1,\u03bb2"}, {"heading": "C Discussion of the Expected Improvement Algorithm", "text": "Here, we briefly discuss interesting recent results of Ryzhov [2016]. He studies a setting with an uncorrelated Gaussian prior, and Gaussian observation noise Yn,i \u223c N(\u03b8i, \u03c32i ). To simplify our discussion, let us restrict attention to the case of common variance \u03c31 = ... = \u03c3k = \u03c3. Ryzhov [2016] shows that under the the expected-improvement algorithm, in the limit as n\u2192\u221e\u2211\ni 6=I\u2217 \u03a8n,i = O(logn) (13)\nand \u03a8n,i(\u03b8\u2217I \u2212 \u03b8i)2 \u223c \u03a8n,j(\u03b8\u2217I \u2212 \u03b8j)2 \u2200i, j 6= I\u2217 (14)\nRecall that \u03a8n,i = \u2211n `=1 \u03c8n,i denotes the total measurement effort allocated to design i. The sampling ratios (14) are the ratios suggested in the optimal computing budget allocation of Chen et al. [2000]. This work therefore establishes an interesting link between EI and OCBA, which appear quite different on the surface.\nUnfortunately, property (13) is not suggested by the OCBA, and implies that \u03a0n(\u0398cI\u2217) cannot tend to zero at an exponential rate. To see this precisely, assume without loss of generality that I\u2217 = 1. Then (13) implies \u03c8n \u2192 ei \u2261 (1, 0, 0, ..., 0). It is easy to show that min\u03b8\u2208\u0398c1 Dei(\u03b8\n\u2217||\u03b8) = 0 and therefore, by Proposition 4,\nlim n\u2192\u221e \u2212n\u22121 log \u03a0n(\u0398c1) = 0.\nIt is also worth noting that the sampling ratios in (14) are not actually optimal for any finite number of designs k. Specifying our calculations as in Example 1, one can show that under an optimal fixed allocation (\u03c8i, ..., \u03c8k),\n(\u03b8\u2217I\u2217 \u2212 \u03b8\u2217i )2\n1/\u03c8I\u2217 + 1/\u03c8i =\n(\u03b8\u2217I\u2217 \u2212 \u03b8\u2217j )2\n1/\u03c8I\u2217 + 1/1/\u03c8j \u2200i, j 6= I\u2217.\nThese calculations match those in Glynn and Juneja [2004] and Jennison et al. [1982]. As a result, there is no problem with finite k for which the sampling ratios in (14) are optimal. One can show, in fact, that any optimal multi-armed bandit algorithm that attains the lower bound of Lai and Robbins [1985] also satisfies equations (13) and (14). The main innovation in this paper is to show how to build on such bandit algorithms to attain near-optimal rates for the best-arm identification problem.\nRyzhov [2016] also studies the knowledge gradient policy, which could offer improved performance as (13) no longer holds, but shows that as n\u2192\u221e\n\u03a8n,i(\u03b8\u2217I \u2212 \u03b8i) \u223c \u03a8n,j(\u03b8\u2217I \u2212 \u03b8j) \u2200i, j 6= I\u2217,\nwhich could be very far from the optimal sampling proportions."}, {"heading": "D Preliminaries", "text": "This section presents some basic results which will be used in the subsequent analysis. First, unless clearly specified, all statements about random variables are meant to hold with probability 1. So for sequences of random variables {Xn} and {Yn}, if we say that Xn \u2192\u221e whenever Yn \u2192\u221e, this means that the set {\u03c9 : Yn(\u03c9)\u2192\u221e, Xn(\u03c9) 9\u221e} has measure zero.\nFacts about the exponential family. The log partition function A(\u03b8) is strictly convex and differentiable, with\nA\u2032(\u03b8) = \u02c6 T (y)p(y|\u03b8)d\u03bd(y) (15)\nequal to the mean under \u03b8. The Kullback-Leibler divergence is equal to\nd(\u03b8||\u03b8\u2032) = (\u03b8 \u2212 \u03b8\u2032)A\u2032(\u03b8)\u2212A(\u03b8) +A(\u03b8\u2032) (16)\nand satisfies\n\u03b8\u2032\u2032 > \u03b8\u2032 \u2265 \u03b8 =\u21d2 d(\u03b8||\u03b8\u2032\u2032) > d(\u03b8||\u03b8\u2032) (17) \u03b8\u2032\u2032 < \u03b8\u2032 \u2264 \u03b8 =\u21d2 d(\u03b8||\u03b8\u2032\u2032) < d(\u03b8||\u03b8\u2032). (18)\nFinally, since [\u03b8, \u03b8] is bounded, and we have assumed sup\u03b8\u2208[\u03b8,\u03b8] |A \u2032(\u03b8)| <\u221e,\nsup \u03b8\u2208[\u03b8,\u03b8] |A(\u03b8)| <\u221e and sup \u03b8,\u03b8\u2032\u2208[\u03b8,\u03b8]\nd(\u03b8||\u03b8\u2032) <\u221e. (19)\nThis effectively guarantees no single observation can provide enough information to completely rule out a parameter.\nSome martingale convergence results. The next fact relates the behavior of a martingaleMn to its quadratic variation \u3008M\u3009n.\nFact 1. (Williams [1991], 12.13-12.14) Let {Mn} be a square-integrable martingale adapted to the filtration {Hn} and let\n\u3008M\u3009n = n\u2211 `=1 E[(M` \u2212M`\u22121)2 |H`\u22121]\ndenote the corresponding quadratic variation process. Then\nMn \u3008M\u3009n \u2192\u221e\nalmost surely if \u3008M\u3009n \u2192\u221e and limn\u2192\u221eMn exists and is finite almost surely if limn\u2192\u221e\u3008M\u3009n <\u221e.\nThe next lemma is crucial to our analysis. To draw the connection with our setting, imagine an adaptive-randomized rule is used to determine when to draw samples from a population. Here Yn \u2208 R denotes the sample at time n, Xn \u2208 {0, 1} indicates whether the sample was measured, and Zn \u2208 [0, 1] determines the probability of measurement conditioned on the past. This lemma provides a law of large numbers when measurement effort \u2211n `=1 Z` tends to infinity, but shows that\nif measurement effort is finite then \u2211\u221e `=1X`Y` is also finite; in this sense the observations collected from Yn are inconclusive when measurement effort is finite.\nLemma 4. Let {Yn} be an i.i.d sequence of real-valued random variables with finite variance and let {Xn} be a sequence of binary random variables. Suppose each sequence is adapted to the filtration {Hn}, and define Zn = P(Xn = 1|Hn\u22121). If, conditioned on Hn\u22121, each Yn is independent of Xn, then with probability 1,\nlim n\u2192\u221e n\u2211 `=1 Z` =\u221e =\u21d2 lim n\u2192\u221e \u2211n `=1X`Y`\u2211n `=1 Z` = E[Y1]\nand lim n\u2192\u221e n\u2211 `=1 Z` <\u221e =\u21d2 sup n\u2208N \u2223\u2223\u2223\u2223\u2223 n\u2211 `=1 X`Y` \u2223\u2223\u2223\u2223\u2223 <\u221e. Proof. Let \u00b5 = E[Y1] and \u03c32 = E[(Y1 \u2212 E[Y1])2] denote the mean and variance of each Yn. Define the martingale\nMn = n\u2211 `=1 (X`Y` \u2212 Z`\u00b5)\nwith M0 = 0 and put Sn = \u2211n `=1 Z`. This martingale has quadratic variation\n\u3008M\u3009n = n\u2211 `=1 E[(M` \u2212M`\u22121)2|H`\u22121]\n= n\u2211 `=1 E[(X`(Y` \u2212 \u00b5) + (Y` \u2212 Z`)\u00b5)2 |H`\u22121]\n= n\u2211 `=1 Z`\u03c3 2 + n\u2211 `=1 Z`(1\u2212 Z`)\u00b52 \u2264 (\u03c32 + \u00b52)Sn.\nWe use the shorthand S\u221e = limn\u2192\u221e Sn and \u3008M\u3009\u221e = limn\u2192\u221e\u3008M\u3009n. Suppose S\u221e <\u221e so \u3008M\u3009\u221e <\u221e. By Fact 1, limn\u2192\u221eMn exists and is finite almost surely, which\nimplies supn\u2208N |Mn| <\u221e. Since | \u2211n `=1X`Y`| \u2264 |Mn|+ |\u00b5S\u221e|, this shows supn\u2208N | \u2211n `=1X`Y`| <\u221e as desired. Now, suppose S\u221e = \u221e. If \u3008M\u3009\u221e < \u221e, then again by Fact 1, limn\u2192\u221eMn < \u221e and it is immediate that S\u22121n Mn \u2192 0. However, if \u3008M\u3009\u221e =\u221e then Mn \u3008M\u3009n \u2192 0,\nwhich implies S\u22121n Mn \u2192 0 since Sn \u2265 (\u03c32 + \u00b52)\u3008M\u3009n.\nTaking Yn = 1 in the lemma above yields Levy\u2019s extension of the Borel\u2013Cantelli lemmas (Williams [1991], 12.15). Specialized to our setting, this result relates the long run measurement effort \u03a8n,i = \u2211n `=1 \u03c8n,i to the number of times alternative i is actually measured \u2211n `=1 1(In = i).\nCorollary 1. For i \u2208 {1, ..., k}, set Sn,i = \u2211n `=1 1(In = i). Then, with probability 1,\n\u03a8n,i \u2192\u221e \u21d0\u21d2 Sn,i \u2192\u221e\nand \u03a8n,i \u2192\u221e =\u21d2\nSn,i \u03a8n,i \u2192 1.\nProof. Apply Lemma 4 with Yn = 1, Xn = 1(In = 1), and Hn = Fn. Then Zn = \u03c8n,i by definition."}, {"heading": "E Posterior Concentration and anti-Concentration", "text": ""}, {"heading": "E.1 Uniform Convergence of the Log-Likelihood", "text": "We study the log-likelihood\n\u039bn(\u03b8\u2217||\u03b8) , log ( Ln(\u03b8\u2217) Ln(\u03b8) ) = n\u2211 `=1 log ( p(Y`,I` |\u03b8\u2217I`) p(Y`,I` |\u03b8I`) ) and the log-likelihood from observations of design i\n\u039bn,i(\u03b8\u2217i ||\u03b8i) , n\u2211 `=1 1(In = i) log ( p(Yn,i|\u03b8\u2217i ) p(Yn,i|\u03b8i) ) .\nA Doob-decomposition expresses \u039bn,i(\u03b8i) = An(\u03b8i) + Mn(\u03b8i) as the sum of an Fn\u22121 predictable process An(\u03b8i) and a MartingaleMn(\u03b8i). Moreover, an easy calculation shows An(\u03b8i) = \u03a8n,id(\u03b8\u2217i ||\u03b8i) and Mn(\u03b8i) = \u039bn,i(\u03b8\u2217i ||\u03b8i) \u2212 \u03a8n,id(\u03b8\u2217i ||\u03b8i). Applying Lemma 4 shows \u03a8\u22121n,iMn(\u03b8i) \u2192 0 if \u03a8n,i \u2192 \u221e, which shows the log-likelihood ratio tends to infinity at rate \u03a8n,id(\u03b8\u2217i ||\u03b8i). The next lemma strengthens this, and provides a link between these quantities that holds uniformly in \u03b8i.\nLemma 5. With probability 1, if \u03a8n,i \u2192\u221e then\nsup \u03b8i\u2208[\u03b8,\u03b8] \u03a8\u22121n,i |\u039bn,i(\u03b8 \u2217 i ||\u03b8i)\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8i)| \u2192 0,\nand if limn\u2192\u221e\u03a8n,i <\u221e then\nsup \u03b8i\u2208[\u03b8,\u03b8] sup n\u2208N |\u039bn,i(\u03b8i)|+ |\u03a8n,id(\u03b8\u2217i ||\u03b8i)| <\u221e.\nProof. Define \u03ben , T (Yn,i) \u2212 E[T (Yn,i)] and Xn , 1(In = i). Note that E[\u03ben|Fn\u22121] = 0, E[Xn|Fn\u22121] = \u03c8n,i, and, conditioned on Fn\u22121, Xn is independent of \u03ben. Using the form of the exponential family density given in equation (1), and the form of the KL-divergence given in equation (16), the log-likelihood ratio can be written as\nlog ( p(Yn,i|\u03b8\u2217i ) p(Yn,i|\u03b8i) ) = (\u03b8\u2217i \u2212 \u03b8i)T (Yn,i)\u2212 (A(\u03b8\u2217i )\u2212A(\u03b8i))\n= d(\u03b8\u2217i ||\u03b8i) + (\u03b8\u2217i \u2212 \u03b8i) (T (Yn,i)\u2212E[T (Yn,i)]) = d(\u03b8\u2217i ||\u03b8i) + (\u03b8\u2217i \u2212 \u03b8i)\u03ben\nTherefore,\n\u039bn,i(\u03b8\u2217i ||\u03b8i)\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8i) = n\u2211 `=1 X` log ( p(Y`,i|\u03b8\u2217i ) p(Y`,i|\u03b8i) ) \u2212 n\u2211 `=1 \u03c8`,id(\u03b8\u2217i ||\u03b8i)\n= n\u2211 `=1 (X` \u2212 \u03c8`,i)d(\u03b8\u2217i ||\u03b8i) + n\u2211 `=1 X`\u03be`(\u03b8\u2217i \u2212 \u03b8i).\nHere |\u03b8\u2217i \u2212 \u03b8i| \u2264 \u03b8 \u2212 \u03b8 \u2261 C2 is bounded uniformly. Similarly, as shown in Appendix D, d(\u03b8\u2217i ||\u03b8i) is bounded uniformly in \u03b8i by\nC1 \u2261 max \u03b8\u2032\u2208[\u03b8,\u03b8] d(\u03b8\u2217i ||\u03b8\u2032i) <\u221e.\nThis implies,\n|\u039bn,i(\u03b8i)\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8i)| \u2264 C1 \u2223\u2223\u2223\u2223\u2223 n\u2211 `=1 (X` \u2212 \u03c8`,i) \u2223\u2223\u2223\u2223\u2223+ C2 \u2223\u2223\u2223\u2223\u2223 n\u2211 `=1 X`\u03be` \u2223\u2223\u2223\u2223\u2223 (20) |\u039bn,i(\u03b8i)| \u2264 C1\u03a8n,i + C1\n\u2223\u2223\u2223\u2223\u2223 n\u2211 `=1 (X` \u2212 \u03c8`,i) \u2223\u2223\u2223\u2223\u2223+ C2 \u2223\u2223\u2223\u2223\u2223 n\u2211 `=1 X`\u03be` \u2223\u2223\u2223\u2223\u2223 . (21) Since E[\u03be2n] < \u221e, the result then follows by applying Lemma 4 and Corollary 1. In particular, when \u03a8n,i \u2192\u221e,\nlim n\u2192\u221e \u03a8\u22121n,i n\u2211 `=1 (X` \u2212 \u03c8`,i) = 0 and lim n\u2192\u221e \u03a8\u22121n,i n\u2211 `=1 X`\u03be` = 0\nWhen limn\u2192\u221e\u03a8n,i <\u221e,\nsup n\u2208N \u2223\u2223\u2223\u2223\u2223 n\u2211 `=1 (X` \u2212 \u03c8`,i) \u2223\u2223\u2223\u2223\u2223 <\u221e and supn\u2208N \u2223\u2223\u2223\u2223\u2223 n\u2211 `=1 X`\u03be` \u2223\u2223\u2223\u2223\u2223 <\u221e. It is also immediate that d(\u03b8\u2217i ||\u03b8i)\u03a8n,i \u2264 C1\u03a8n,i 9\u221e, which by (21) implies the second part of the result.\nA corollary of the previous lemma relates the log-likelihood ratio \u039bn(\u03b8\u2217||\u03b8) to the KullbackLeibler divergence D\u03c8n(\u03b8 \u2217||\u03b8).\nCorollary 2. With probability 1,\nsup \u03b8\u2208\u0398 |n\u22121\u039bn(\u03b8\u2217||\u03b8)\u2212D\u03c8n(\u03b8 \u2217||\u03b8)| \u2192 0\nProof.\n\u2223\u2223\u2223n\u22121\u039bn(\u03b8\u2217||\u03b8)\u2212D\u03c8n(\u03b8\u2217||\u03b8)\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223n\u22121 k\u2211 i=1 (\u039bn,i(\u03b8\u2217i ||\u03b8i)\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8i)) \u2223\u2223\u2223\u2223\u2223\n\u2264 k\u2211 i=1 n\u22121|\u039bn,i(\u03b8\u2217i ||\u03b8i)\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8i)|.\nLemma 5 implies sup\n\u03b8i\u2208[\u03b8,\u03b8] n\u22121|\u039bn,i(\u03b8\u2217i ||\u03b8i)\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8i)| \u2192 0,\nwhich completes the proof."}, {"heading": "E.2 Posterior Consistency: Proof of Prop. 3", "text": "Proposition 3. For any i \u2208 {1, .., k} if \u03a8n,i \u2192\u221e, then, for all > 0\n\u03a0n({\u03b8 \u2208 \u0398|\u03b8i /\u2208 (\u03b8\u2217i \u2212 , \u03b8\u2217i + )})\u2192 0,\nwith probability 1. If I = {i \u2208 {1, ..., k}| limn\u2192\u221e\u03a8n,i <\u221e} is nonempty, then\ninf n\u2208N \u03a0n({\u03b8 \u2208 \u0398|\u03b8i \u2208 (\u03b8\u2032i, \u03b8\u2032\u2032i ) \u2200i \u2208 I}) > 0\nfor any collections of open intervals (\u03b8\u2032i, \u03b8\u2032\u2032i ) \u2282 (\u03b8, \u03b8) ranging over i \u2208 I.\nBecause we don\u2019t assume an independent prior across the designs, \u03a01 is not a product measure and therefore neither is \u03a0n. This makes it challenging to reason about the marginal posterior of each design, which is required for Proposition 3. Thankfully, since the prior density is bounded, \u03a0n behaves like a product measure. Note that the likelihood function can be written as the product of k terms:\nLn(\u03b8) = k\u220f i=1 Ln,i(\u03b8i)\nwhere Ln,i(\u03b8i) , \u220f `\u2264n I`=i p(Y`,1|\u03b8i)\nwith the convention that Ln,i(\u03b8i) = 1 when \u2211n `=1 1(I` = i) = 0. Therefore Ln(\u03b8) forms the density of a product measure. By normalizing, this induces a probability measure over \u0398,\nLn(\u0398\u0303) , \u00b4\n\u0398\u0303 Ln(\u03b8)d\u03b8\u00b4 \u0398 Ln(\u03b8)d\u03b8\n\u0398\u0303 \u2282 \u0398,\nwhich, as we argue in the next lemma, behaves like the posterior \u03a0n.\nLemma 6. For any set \u0398\u0303 \u2282 \u0398,\nC\u22121Ln(\u0398\u0303) \u2264 \u03a0n+1(\u0398\u0303) \u2264 CLn(\u0398\u0303),\nwhere C = sup\u03b8\u2208\u0398 \u03c01(\u03b8)inf\u03b8\u2208\u0398 \u03c01(\u03b8) <\u221e\nis independent of n and \u0398\u0303.\nProof. This follows immediately by bounding \u03c01(\u03b8) from above and below in the relation\n\u03a0n+1(\u0398\u0303) = \u00b4\n\u0398\u0303 \u03c01(\u03b8)Ln(\u03b8)d\u03b8\u00b4 \u0398 \u03c01(\u03b8)Ln(\u03b8)d\u03b8 .\nWe can now prove Proposition 3.\nProof of Proposition 3. We begin with the first part of the result. For simplicity of notation, we focus on the upper interval \u0398\u0303 = {\u03b8 \u2208 \u0398 : \u03b8i > \u03b8\u2217i + }, but results follow identically for the lower interval. We want to show \u03a0n(\u0398\u0303) \u2192 0, which occurs if and only if Ln(\u0398\u0303) \u2192 0. Since Ln is a product measure,\nLn(\u0398\u0303) =\n\u00b4 \u03b8 \u03b8\u2217i +\nLn,i(\u03b8)d\u03b8 \u00b4 \u03b8 \u03b8 Ln,i(\u03b8)d\u03b8 =\n\u00b4 \u03b8 \u03b8\u2217i +\n(Ln,i(\u03b8)/Ln,i(\u03b8\u2217i )) d\u03b8\u00b4 \u03b8 \u03b8 (Ln,i(\u03b8)/Ln,i(\u03b8 \u2217 i )) d\u03b8 =\n\u00b4 \u03b8 \u03b8\u2217i +\nexp{\u2212\u039bn,i(\u03b8\u2217i ||\u03b8)}d\u03b8\u00b4 \u03b8 \u03b8 exp{\u2212\u039bn,i(\u03b8 \u2217 i ||\u03b8)}d\u03b8\n(22)\nwhere \u039bn,i(\u03b8\u2217i ||\u03b8i) = log(Ln,i(\u03b8\u2217i )/Ln,i(\u03b8i). By Lemma 5, with probability 1 there is a sequence an \u2192 0 such that |\u039bn,i(\u03b8\u2217i ||\u03b8)\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8)| \u2264 an for all \u03b8. Then, for bn = ean/e\u2212an \u2192 1, one has\nLn(\u0398\u0303) \u2264 bn \u00b4 \u03b8 \u03b8\u2217i +\nexp{\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8)}d\u03b8\u00b4 \u03b8 \u03b8 exp{\u2212\u03a8n,id(\u03b8 \u2217 i ||\u03b8)}d\u03b8\n\u2264 bn \u00b4 \u03b8 \u03b8\u2217i +\nexp{\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8)}d\u03b8\u00b4 \u03b8\u2217i + /2 \u03b8\u2217i exp{\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8)}d\u03b8 .\nThe integral in the numerator is upper bounded by (\u03b8 \u2212 \u03b8\u2217i \u2212 ) exp{\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8\u2217i + ) while the integral in the denominator is lower bounded by ( /2) exp{\u2212\u03a8n,id(\u03b8\u2217i ||\u03b8\u2217i + /2)}. This shows\nLn(\u0398\u0303) \u2264 c0bn exp{\u2212\u03a8n,i (d(\u03b8\u2217i ||\u03b8\u2217i + )\u2212 d(\u03b8\u2217i ||\u03b8\u2217i + /2))} \u2192 0\nwhere c0 = 2 \u22121(\u03b8 \u2212 \u03b8\u2217i \u2212 ). The second part of the claim follows from the lower bound in Lemma 6 of\n\u03a0n+1({\u03b8 \u2208 \u0398|\u03b8i \u2208 (\u03b8\u2032, \u03b8\u2032\u2032) \u2200i \u2208 I}) \u2265 C\u22121Ln({\u03b8 \u2208 \u0398|\u03b8i \u2208 (\u03b8\u2032i, \u03b8\u2032\u2032i ) \u2200i \u2208 I}) (23) = C\u22121 \u220f i\u2208I Ln({\u03b8 \u2208 \u0398|\u03b8i \u2208 (\u03b8\u2032i, \u03b8\u2032\u2032i )}). (24)\nAs in (22),\nLn({\u03b8 \u2208 \u0398|\u03b8i \u2208 (\u03b8\u2032i, \u03b8\u2032\u2032i )}) = \u00b4 \u03b8\u2032\u2032 \u03b8\u2032 exp{\u2212\u039bn,i(\u03b8 \u2217 i ||\u03b8)}d\u03b8\u00b4 \u03b8\n\u03b8 exp{\u2212\u039bn,i(\u03b8 \u2217 i ||\u03b8)}d\u03b8\n.\nWhen limn\u2192\u221e\u03a8n,i <\u221e, Lemma 5 shows that for each i \u2208 I,\nsup \u03b8i\u2208[\u03b8,\u03b8] sup n\u2208N |\u039bn,i(\u03b8\u2217i ||\u03b8i)| <\u221e.\nThis implies inf n Ln({\u03b8 \u2208 \u0398|\u03b8i \u2208 (\u03b8\u2032i, \u03b8\u2032\u2032i )}) > 0\nand establishes the claim."}, {"heading": "E.3 Large Deviations: Proof of Proposition 4", "text": "All statements in this section hold when observations are drawn under the parameter \u03b8\u2217. Since \u03b8\u2217 is fixed throughout, we simplify notation and write\nWn(\u03b8) , D\u03c8n(\u03b8 \u2217||\u03b8).\nNote thatWn(\u03b8\u2217) = 0. As shown in the next lemma n\u22121 log (\u03c0n(\u03b8)/\u03c0n(\u03b8\u2217))\u2212Wn(\u03b8)\u2192 0 uniformly in \u03b8.\nLemma 7. With probability 1,\nsup \u03b8\u2208\u0398 n\u22121 \u2223\u2223\u2223\u2223log(\u03c0n(\u03b8\u2217)\u03c0n(\u03b8) ) \u2212Wn(\u03b8) \u2223\u2223\u2223\u2223\u2192 0. Proof. We have\nlog ( \u03c0n(\u03b8\u2217) \u03c0n(\u03b8) ) \u2212Wn(\u03b8) = log ( \u03c01(\u03b8\u2217) \u03c01(\u03b8) ) + (\u039bn\u22121(\u03b8\u2217||\u03b8)\u2212Wn\u22121(\u03b8)) + (Wn\u22121(\u03b8)\u2212Wn(\u03b8)) .\nSince inf\u03b8\u2208\u0398 \u03c01(\u03b8) > 0 and sup\u03b8\u2208\u0398 \u03c01(\u03b8) < \u221e, n\u22121 log (\u03c01(\u03b8)/\u03c01(\u03b8\u2217)) \u2192 0 uniformly in \u03b8. By Corollary 2, n\u22121 (\u039bn\u22121(\u03b8)\u2212Wn\u22121(\u03b8)) \u2192 0 uniformly as well. Finally, by equation (19), n\u22121(Wn(\u03b8)\u2212Wn\u22121(\u03b8)) \u2264 n\u22121 maxi d(\u03b8\u2217i ||\u03b8i)\u2192 0 uniformly in \u03b8.\nThe remaining proof of Proposition 4 follows from a sequence of lemmas. The next observes a form of uniform continuity of Wn that follows from the uniform bound on A\u2032(\u03b8) in Assumption 1.\nLemma 8. For all > 0, there exists \u03b4 > 0 such that for \u03b8,\u03b8\u2032 \u2208 \u0398\n\u2016\u03b8 \u2212 \u03b8\u2032\u2016\u221e \u2264 \u03b4 =\u21d2 sup n\u2208N |Wn(\u03b8)\u2212Wn(\u03b8\u2032)| \u2264 .\nProof. We have that\n|Wn(\u03b8)\u2212Wn(\u03b8\u2032)| \u2264 max 1\u2264i\u2264k |d(\u03b8\u2217i ||\u03b8i)\u2212 d(\u03b8\u2217i ||\u03b8\u2032i)|\n= max 1\u2264i\u2264k \u2223\u2223(\u03b8\u2032i \u2212 \u03b8i)A\u2032(\u03b8\u2217i ) +A(\u03b8i)\u2212A(\u03b8\u2032i)\u2223\u2223 \u2264 2C\u03b4\nwhere C = sup\u03b8\u2208(\u03b8,\u03b8) |A \u2032(\u03b8)| <\u221e.\nLemma 9. For any open set \u0398\u0303 \u2282 \u0398, \u02c6\n\u03b8\u2208\u0398\u0303\n\u03c0n(\u03b8) \u03c0n(\u03b8\u2217)\nd\u03b8 .= \u02c6\n\u03b8\u2208\u0398\u0303\nexp{\u2212nWn(\u03b8)}d\u03b8.\nProof. By Corollary 2, we can fix a sequence n \u2265 0 with n \u2192 0 such that,\nexp{\u2212n(Wn(\u03b8) + n)} \u2264 \u03c0n(\u03b8) \u03c0n(\u03b8\u2217) \u2264 exp{\u2212n(Wn(\u03b8)\u2212 n)}.\nIntegrating over \u0398\u0303 yields,\nexp{\u2212n n} \u02c6\n\u0398\u0303\nexp{\u2212nWn(\u03b8)}d\u03b8 \u2264 \u02c6\n\u0398\u0303\n\u03c0n(\u03b8) \u03c0n(\u03b8\u2217)\nd\u03b8 \u2264 exp{n n} \u02c6\n\u0398\u0303\nexp{\u2212nWn(\u03b8)}d\u03b8.\nTaking the logarithm of each side implies\n1 n \u2223\u2223\u2223\u2223\u2223\u2223\u2223log \u02c6\n\u0398\u0303\n\u03c0n(\u03b8) \u03c0n(\u03b8\u2217)\nd\u03b8 \u2212 log \u02c6\n\u0398\u0303\nexp{\u2212nWn(\u03b8)}d\u03b8 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 n \u2192 0.\nLemma 10. For any open set \u0398\u0303 \u2282 \u0398, \u02c6\n\u03b8\u2208\u0398\u0303\nexp{\u2212nWn(\u03b8)}d\u03b8 .= exp{\u2212n inf\n\u03b8\u2208\u0398\u0303 Wn(\u03b8)}\nProof. Let \u03b8\u0302n be a point in the closure of \u0398\u0303, satisfying\nWn(\u03b8\u0302n) = inf \u03b8\u2208\u0398\u0303 Wn(\u03b8).\nSuch a point always exists, since Wn is continuous, and the closure of \u0398\u0303 is compact. Let\n\u03b3n , \u02c6\n\u03b8\u2208\u0398\u0303\nexp{\u2212nWn(\u03b8)}d\u03b8.\nOur goal is to show 1 n log(\u03b3n) +Wn(\u03b8\u0302n)\u2192 0.\nWe have \u03b3n \u2264 Vol(\u0398\u0303) exp{\u2212nWn(\u03b8\u0302n)} where for any \u0398\u2032 \u2282 \u0398, Vol(\u0398\u2032) = \u00b4\n\u0398\u0303 d\u03b8 \u2208 (0,\u221e) denotes the volume of \u0398. This shows\nlim sup n\u2192\u221e ( 1 n log(\u03b3n) +Wn(\u03b8\u0302n) ) \u2264 0.\nWe now show the reverse. Fix an arbitrary > 0. By Lemma 8, there exists \u03b4 > 0 such that\n|Wn(\u03b8)\u2212Wn(\u03b8\u0302n)| \u2264 \u2200n \u2208 N\nfor any \u03b8 \u2208 \u0398 with \u2016\u03b8 \u2212 \u03b8\u0302n\u2016\u221e \u2264 \u03b4.\nNow, choose a finite \u03b4\u2013cover O of \u0398\u0303 in the norm \u2016\u00b7\u2016\u221e. Remove any set in O that does not intersect \u0398\u0303. Then, for each o \u2208 O,\nVol(o \u2229 \u0398\u0303) > 0 =\u21d2 C\u03b4 , min o\u2208O Vol(o \u2229 \u0398\u0303) > 0.\nChoose on \u2208 O with \u03b8\u0302n \u2208 closure(on). Then, for every \u03b8 \u2208 on, Wn(\u03b8) \u2264Wn(\u03b8\u0302n) + . This shows\n\u03b3n \u2265 \u02c6 o exp{\u2212nWn(\u03b8}d\u03b8 \u2265 C\u03b4 exp{\u2212n(Wn(\u03b8\u0302n)\u2212 )).\nTaking the logarithm of both sides implies\n1 n log(\u03b3n) +Wn(\u03b8\u0302n) \u2265 C\u03b4 n \u2212 \u2192 \u2212 .\nSince was chosen arbitrarily, this shows\nlim inf n\u2192\u221e ( 1 n log(\u03b3n) +Wn(\u03b8\u0302n) ) \u2265 0,\nand completes the proof.\nWe now complete the proof of Proposition 4.\nProof of Proposition 4. We begin with a simple observation. For any sequences of real numbers {an}, {bn}, and {a\u0303n}, {b\u0303n}, if an .= a\u0303n and bn .= b\u0303n \u2208 R, then an/bn\n.= a\u0303n/b\u0303n. Therefore, we have\n\u03a0n(\u0398\u0303) = \u03a0n(\u0398\u0303) \u03a0n(\u0398)\n= \u00b4\n\u0398\u0303 \u03c0n(\u03b8)d\u03b8\u00b4 \u0398 \u03c0n(\u03b8)d\u03b8\n= \u00b4 \u0398\u0303 (\u03c0n(\u03b8)/\u03c0n(\u03b8 \u2217))d\u03b8\u00b4\n\u0398 (\u03c0n(\u03b8)/\u03c0n(\u03b8\u2217))d\u03b8 .= exp{\u2212n inf\u03b8\u2208\u0398\u0303Wn(\u03b8)} exp{\u2212n inf\u03b8\u2208\u0398Wn(\u03b8)}\nwhere the final equality follows from the previous two lemmas. Since Wn(\u03b8) \u2265 0 and Wn(\u03b8\u2217) = 0, exp{\u2212n inf\u03b8\u2208\u0398Wn(\u03b8)} = 1."}, {"heading": "E.4 Large Deviations of the Value Measure: Proof of Lemma 3", "text": "Lemma 3. For any i 6= I\u2217, Vn,i .= \u03b1n,i.\nProof. First, since\nVn,i = \u02c6\n\u0398i\nvi(\u03b8)\u03c0n(\u03b8)d\u03b8 \u2264 (u(\u03b8)\u2212 u(\u03b8)) \u02c6\n\u0398i\n\u03c0n(\u03b8)d\u03b8 = (u(\u03b8)\u2212 u(\u03b8))\u03b1n,i\nit is immediate that lim sup n\u2192\u221e n\u22121(log Vn,i \u2212 log\u03b1n,i) \u2264 0. (25)\nThe other direction more subtle. Define \u0398i,\u03b4 \u2282 \u0398i by\n\u0398i,\u03b4 = {\u03b8 \u2208 \u0398 : \u03b8i \u2265 max j 6=i \u03b8j + \u03b4}.\nFor any \u03b8 \u2208 \u0398i,\u03b4, vi(\u03b8) \u2265 C\u03b4 where\nC\u03b4 \u2261 min \u03b8\u2208[\u03b8,\u03b8] u(\u03b8 + \u03b4)\u2212 u(\u03b8) > 0.\nBecause u(\u03b8 + \u03b4) \u2212 u(\u03b8) is continuous and is strictly positive for each \u03b8, this minimum exists and the objective value is strictly positive. Then\nVn,i \u2265 \u02c6\n\u0398i,\u03b4\nvi(\u03b8)\u03c0n(\u03b8)d\u03b8 \u2265 C\u03b4 \u02c6\n\u0398i,\u03b4\n\u03c0n(\u03b8)d\u03b8 = C\u03b4\u03a0n(\u0398i,\u03b4) \u2200\u03b4 > 0.\nCombining this with Proposition 4 shows\nlim inf n\u2192\u221e 1 n (log Vn,i\u2212log\u03b1n,i) \u2265 lim inf n\u2192\u221e 1 n (log \u03a0n(\u0398i,\u03b4)\u2212log \u03a0n(\u0398i)) = \u2212 min \u03b8\u2208\u0398i,\u03b4 D\u03c8n (\u03b8\u2217||\u03b8)\u2212min \u03b8\u2208\u0398i D\u03c8n (\u03b8\u2217||\u03b8).\nThe final term can be made arbitrarily small by taking \u03b4 \u2192 0. Precisely, by Lemma 8, for any > 0, there exists \u03b4 > 0 such that for all n \u2208 N and \u03b8,\u03b8\u2032 \u2208 \u0398 satisfying \u2016\u03b8 \u2212 \u03b8\u2032\u2016\u221e \u2264 \u03b4 ,\nD\u03c8n (\u03b8\u2217||\u03b8) \u2264 .\nTherefore, for each > 0 one can choose \u03b4 > 0 such that\nmin \u03b8\u2208\u0398i,\u03b4 D\u03c8n (\u03b8\u2217||\u03b8) \u2264 min \u03b8\u2208\u0398i D\u03c8n (\u03b8\u2217||\u03b8) + .\nThis shows lim inf n\u22121(log Vn,i \u2212 log\u03b1n,i) \u2265 \u2212 for all > 0, and hence\nlim inf n\u2192\u221e\nn\u22121(log Vn,i \u2212 log\u03b1n,i) \u2265 0."}, {"heading": "F Simplifying and Bounding the Error Exponent", "text": ""}, {"heading": "F.1 Proof of Lemma 1", "text": "To begin, we restate the results of Lemma 1 in the order in which they will be proved. Recall, from Section D that A(\u03b8) is increasing and strictly convex, and, by (15), A\u2032(\u03b8) is the mean observation under \u03b8.\nLemma 1. Define for each i 6= I\u2217,\u03c8 \u2265 0,\nCi(\u03b2, \u03c8) , min x\u2208R \u03b2d(\u03b8\u2217I\u2217 ||x) + \u03c8d(\u03b8\u2217i ||x). (26)\n(a) For any i 6= I\u2217 and probability distribution \u03c8 over {1, ..., k}\nmin \u03b8\u2208\u0398i\nD\u03c8(\u03b8\u2217||\u03b8) = Ci(\u03c8I\u2217 , \u03c8i).\nwhere \u0398i , {\u03b8 \u2208 \u0398|\u03b8i \u2265 \u03b8I\u2217}. (b) Each Ci is a concave function. (c) The unique solution to the minimization problem (26) is \u03b8 \u2208 R satisfying\nA\u2032(\u03b8) = \u03c8I \u2217A\u2032(\u03b8\u2217I\u2217) + \u03c8iA\u2032(\u03b8\u2217i )\n\u03c8I\u2217 + \u03c8i .\nTherefore, Ci(\u03c8I\u2217 , \u03c8i) = \u03c8I\u2217d(\u03b8\u2217I\u2217 ||\u03b8) + \u03c8id(\u03b8\u2217i ||\u03b8).\n(d) Each Ci is a strictly increasing function.\nProof. (a)\nmin \u03b8\u2208\u0398i D\u03c8(\u03b8\u2217||\u03b8) = min \u03b8\u2208\u0398:\u03b8i\u2265\u03b8I\u2217 k\u2211 j=1 \u03c8n,jd(\u03b8\u2217j ||\u03b8j)\n= min \u03b8\u2265\u03b8i\u2265\u03b8I\u2217\u2265\u03b8\n\u03c8I\u2217d(\u03b8\u2217I\u2217 ||\u03b8I\u2217) + \u03c8id(\u03b8\u2217i ||\u03b8i) + \u2211\nj /\u2208{i,I\u2217} min \u03b8j \u03c8n,jd(\u03b8\u2217j ||\u03b8j)\n= min \u03b8\u2265\u03b8i\u2265\u03b8I\u2217\u2265\u03b8 \u03c8I\u2217d(\u03b8\u2217I\u2217 ||\u03b8I\u2217) + \u03c8id(\u03b8\u2217i ||\u03b8i)\nwhere the last equality uses that the minimum occurs when \u03b8j = \u03b8\u2217j for j /\u2208 {I\u2217, i}, and this is feasible for any choice of (\u03b8i, \u03b8I\u2217). Then, by the monotonicity properties of KL-divergence (see Section D, equation (17)), there is always a minimum with \u03b8i = \u03b8I\u2217 . Therefore this objective value is equal to\nmin \u03b8\u2208[\u03b8,\u03b8] \u03c8I\u2217d(\u03b8\u2217I\u2217 ||\u03b8) + \u03c8id(\u03b8\u2217i ||\u03b8) = min x\u2208R \u03c8I\u2217d(\u03b8\u2217I\u2217 ||x) + \u03c8id(\u03b8\u2217i ||x) = Ci(\u03c8I\u2217 , \u03c8i).\n(b) Ci is the minimum over a family of linear functions and therefore is concave (See Chapter 3.2 of Boyd and Vandenberghe [2004]). In particular Ci(\u03b2, \u03c8) = minx\u2208R g((\u03b2, \u03c8);x) where g((\u03b2, \u03c8);x) = \u03b2d(\u03b8\u2217I\u2217 ||x) + \u03c8d(\u03b8\u2217i ||x) is linear in (\u03b2, \u03c8).\n(c) Direct calculation using the formula for KL divergence in exponential families (see (16) in Section D) shows\n\u03b2d(\u03b8\u2217I\u2217 ||x) + \u03c8id(\u03b8\u2217i ||x) = (\u03b2 + \u03c8i)A(x)\u2212 (\u03b2A\u2032(\u03b8\u2217I\u2217) + \u03c8iA\u2032(\u03b8\u2217i ))x+ f(\u03b2, \u03b8\u2217I\u2217 , \u03c8i, \u03b8\u2217i )\nwhere f(\u03b2, \u03b8\u2217I\u2217 , \u03c8i, \u03b8\u2217i ) captures terms that are independent of x. Setting the derivative with respect to x to zero yields the result since A(x) is strictly convex.\n(d) We will show Ci is strictly increasing in the second argument. The proof that it is strictly increasing in its first argument follows by symmetry. Set\nf(\u03c8i, x) = \u03b2d(\u03b8\u2217I\u2217 ||x) + \u03c8id(\u03b8\u2217i ||x)\nso that Ci(\u03b2, \u03b3i) = minx\u2208R f(\u03c8i, x). Since KL divergences are non-negative, f(\u03c8i, x) is weakly increasing in \u03c8i. To establish the claim, fix two nonnegative numbers \u03c8\u2032 < \u03c8\u2032\u2032. Let x\u2032 = arg minx f(\u03c8\u2032, x) and x\u2032\u2032 = arg minx f(\u03c8\u2032\u2032, x). By part (c), these are unique and x\u2032 < x\u2032\u2032. Then\nf(\u03c8\u2032, x\u2032) < f(\u03c8\u2032, x\u2032\u2032) \u2264 f(\u03c8\u2032\u2032, x\u2032\u2032)\nwhere the first inequality uses that x\u2032 6= x\u2032\u2032 and x\u2032 is a unique minimum and the second uses the f is non-decreasing."}, {"heading": "F.2 Proof of Proposition 6", "text": "We will begin by restating Proposition 6.\nProposition 6. The solution to the optimization problem (12) is the unique allocation \u03c8\u2217 satisfying \u03c8\u2217I\u2217 = \u03b2 and\nCi(\u03b2, \u03c8i) = Cj(\u03b2, \u03c8j) \u2200 i, j 6= I\u2217. (27)\nIf \u03c8n = \u03c8\u2217 for all n, then \u03a0n(\u0398cI\u2217) .= exp{\u2212n\u0393\u2217\u03b2}.\nMoreover under any other adaptive allocation rule, if \u03c8n,I\u2217 \u2192 \u03b2 as n\u2192\u221e then\nlim sup n\u2192\u221e \u2212 1 n log \u03a0n(\u0398cI\u2217) \u2264 \u0393\u2217\u03b2\nalmost surely.\nProof. By Lemma 1, each function Ci is continuous, and therefore mini 6=I\u2217 Ci(\u03b2, \u03c8i) is continuous in (\u03c8i : i 6= I\u2217). Since continuous functions on a compact space attain their minimum, there exists an optimal solution \u03c8\u2217 to (12), which satisfies\nmin i 6=I\u2217 Ci(\u03b2, \u03c8\u2217i ) = max \u03c8:\u03c8I\u2217=\u03b2 min i 6=I\u2217 Ci(\u03b2, \u03c8i).\nSuppose \u03c8\u2217 does not satisfy (27), so for some j 6= I\u2217,\nCj(\u03b2, \u03c8\u2217j ) > min i 6=I\u2217 Ci(\u03b2, \u03c8\u2217i ).\nThis yields a contradiction. Consider a new vector \u03c8 with \u03c8 j = \u03c8\u2217j \u2212 and \u03c8 i = \u03c8\u2217i + /(k \u2212 2) for each i /\u2208 {I\u2217, j}. For sufficiently small , one has\nCj(\u03b2, \u03c8 j) > min i 6=I\u2217 Ci(\u03b2, \u03c8 i ) > min i 6=I\u2217 Ci(\u03b2, \u03c8\u2217i )\nand so \u03c8 attains a higher objective value. To show the solution to (27) must be unique, imagine \u03c8 and \u03c8\u2032 both satisfy (27) and \u03c8I\u2217 = \u03c8\u2032I\u2217 = \u03b2. If \u03c8j > \u03c8\u2032j for some j, then Cj(\u03b2, \u03c8j) > Cj(\u03b2, \u03c8\u2032j) since Cj is strictly increasing. But by (27) this implies that Cj(\u03b2, \u03c8j) > Cj(\u03b2, \u03c8\u2032j) for every j 6= I\u2217, which implies \u03c8j > \u03c8\u2032j for every j, and contradicts that that \u2211 j 6=I\u2217 \u03c8j = \u2211 j 6=I\u2217 \u03c8 \u2032 j = 1\u2212 \u03b2.\nThe remaining claims follow immediately from Propoosition 4 and Lemma 1, which together show that under any adaptive allocation rule\n\u03a0n(\u0398cI\u2217) .= exp{\u2212nmin\ni 6=I\u2217 Ci(\u03c8n,I\u2217 , \u03c8n,i)}.\nThis implies that if \u03c8n = \u03c8\u2217 for all n, then \u03a0n(\u0398cI\u2217) .= exp{\u2212n\u0393\u2217\u03b2}. Similarly, by the continuity of each Ci, if \u03c8n,I\u2217 \u2192 \u03b2, then\n\u03a0n(\u0398cI\u2217) .= exp{\u2212nmin i 6=I\u2217 Ci(\u03b2, \u03c8n,i)} \u2265 exp{\u2212n\u0393\u2217\u03b2}\nwhich establishes the final claim."}, {"heading": "F.3 Proof of Lemma 2", "text": "Recall, the notation\n\u0393\u2217 = max \u03c8 min i 6=I\u2217 Ci(\u03c8I\u2217 , \u03c8i) \u0393\u2217\u03b2 , max \u03c8:\u03c8I\u2217=\u03b2 min i 6=I\u2217 Ci(\u03b2, \u03c8i)\nwhere Ci(\u03b2, \u03c8) = min\nx\u2208R \u03b2d(\u03b8\u2217I\u2217 ||x) + \u03c8d(\u03b8\u2217i\u2217 ||x).\nLemma 2. For \u03b2\u2217 = arg max\u03b2 \u0393\u2217\u03b2 and any \u03b2 \u2208 (0, 1),\n\u0393\u2217 \u0393\u2217\u03b2 \u2264 max\n{ \u03b2\u2217\n\u03b2 , 1\u2212 \u03b2\u2217 1\u2212 \u03b2\n} .\nTherefore \u0393\u2217 \u2264 2\u0393\u22171/2 Proof. Define for each non-negative vector \u03c8,\nf(\u03c8) = min i 6=I\u2217 Ci(\u03c8I\u2217 , \u03c8i)\nThe optimal exponent \u0393\u2217 is the maximum of f(\u03c8) over probability vectors \u03c8. Here, we instead define f for all non-negative vectors, and proceed by varying the total budget of measurement effort available \u2211k i=1 \u03c8i.\nBecause each Ci is non-decreasing (see Lemma 1), f is non-decreasing. Since the minimum over x in the definition of Ci only depends on the relative size of the components of \u03c8, f is homogenous of degree 1. That is f(c\u03c8) = cf(\u03c8) for all c \u2265 1. For each c1, c2 > 0 define\ng(c1, c2) = max{f(\u03c8) : \u03c8I\u2217 = c1, \u2211 i 6=I\u2217 \u03c8i \u2264 c2,\u03c8 \u2265 0}.\nThe function g inherits key properties of f ; it is also non-decreasing and homogenous of degree 1. We have\n\u0393\u2217\u03b2 = max{f(\u03c8) : \u03c8I\u2217 = \u03b2, k\u2211 i=1 \u03c8i = 1,\u03c8 \u2265 0}\n= max{f(\u03c8) : \u03c8I\u2217 = \u03b2, \u2211 i 6=I\u2217 \u03c8i \u2264 1\u2212 \u03b2,\u03c8 \u2265 0}\n= g(\u03b2, 1\u2212 \u03b2)\nwhere the second equality uses that f is non-decreasing. Similarly, \u0393\u2217 = g(\u03b2\u2217, 1\u2212 \u03b2\u2217). Setting\nr := max { \u03b2\u2217\n\u03b2 , 1\u2212 \u03b2\u2217 1\u2212 \u03b2 } implies r\u03b2 \u2265 \u03b2\u2217 and r(1\u2212 \u03b2) \u2265 1\u2212 \u03b2\u2217. Therefore\nr\u0393\u2217\u03b2 = rg(\u03b2, 1\u2212 \u03b2) = g(r\u03b2, r(1\u2212 \u03b2)) \u2265 g(\u03b2\u2217, 1\u2212 \u03b2\u2217) = \u0393\u2217."}, {"heading": "F.4 Sub-Gaussian Bound: Proof of Proposition 1", "text": "The proof of Proposition 1 relies on the following variational form of Kullback\u2013Leibler divergence, which is given in Theorem 5.2.1 of Robert Gray\u2019s textbook Entropy and Information Theory Gray [2011].\nFact 2. Fix two probability measures P and Q defined on a common measureable space (\u2126,F). Suppose that P is absolutely continuous with respect to Q. Then\nD (P||Q) = sup X\n{ EP[X]\u2212 log EQ[eX ] } ,\nwhere the supremum is taken over all random variables X such that the expectation of X under P is well defined, and eX is integrable under Q.\nWhen comparing two normal distributions N (\u03b8, \u03c32) and N (\u03b8\u2032, \u03c32) with common variance, the KL-divergence can be expressed as d(\u03b8||\u03b8\u2032) = (\u03b8 \u2212 \u03b8\u2032)2/(2\u03c32). We follow Russo and Zou [2015] in deriving the following corollary of Fact 2, which provides and analogous lower bound on the KL-divergences when distributions are sub-Gaussian. Recall that, \u00b5(\u03b8) = \u00b4 yp(y|\u03b8)d\u03bd(y) denotes the mean observation under \u03b8.\nCorollary 3. Fix any \u03b8, \u03b8\u2032 \u2208 [\u03b8, \u03b8]. If when Y \u223c p(y|\u03b8\u2032), Y is sub-Gaussian with parameter \u03c3, then,\nd(\u03b8||\u03b8\u2032) \u2265 (\u00b5(\u03b8)\u2212 \u00b5(\u03b8 \u2032))2\n2\u03c32 Proof. Consider two alternate probability distributions for a random variable Y , one where Y \u223c p(y|\u03b8) and one where Y \u223c p(y|\u03b8\u2032) We apply Fact 2 where X = \u03bb(Y \u2212E\u03b8\u2032 [Y ]), P is the probability measure when Y \u223c p(y|\u03b8) and Q is the measure when Y \u223c p(y|\u03b8\u2032). By the sub-Gaussian assumption log E\u03b8\u2032 [exp{X}] \u2264 \u03bb2\u03c32/2. Therefore, Fact 2 implies\nd(\u03b8||\u03b8\u2032) \u2265 \u03bb(E\u03b8[X])\u2212 \u03bb2\u03c32\n2 = \u03bb(E\u03b8[Y ]\u2212E\u03b8 \u2032 [Y ])\u2212 \u03bb\n2\u03c32\n2 .\nThe result follows by choosing \u03bb = (E\u03b8[Y ]\u2212E\u03b8\u2032 [Y ])/\u03c32 which minimizes the right hand side.\nWe are now ready to prove Proposition 1. Recall that in an exponential family, A\u2032(\u03b8) =\u00b4 T (y)p(y|\u03b8)d\u03bd(y), so if T (y) = y then A\u2032(\u03b8) = \u00b5(\u03b8).\nProof of Proposition 1. By Lemma 1,\n\u0393\u22171/2 = max \u03c8:\u03c8I\u2217=1/2 min i 6=I\u2217 Ci(1/2, \u03c8i)\nLet \u00b5I\u2217 = A\u2032(\u03b8\u2217I\u2217) and \u00b5i = A\u2032(\u03b8\u2217i ) denote the means of designs I\u2217 and i so \u2206i = \u00b5I\u2217 \u2212 \u00b5i. By Lemma 1, Ci(1/2, \u03c8i) = (1/2)d(\u03b8\u2217I\u2217 ||\u03b8) + \u03c8id(\u03b8\u2217i ||\u03b8). where \u03b8 is the unique parameter with mean\nA\u2032(\u03b8) = (1/2)\u00b5I \u2217 + \u03c8i\u00b5i\n1/2 + \u03c8i .\nFor \u03c8i \u2264 1/2, A\u2032(\u03b8) \u2265 \u00b5I\n\u2217 + \u00b5i 2 = \u00b5i + \u2206i/2.\nNow, using Corollary 3 and the non-negativity of KL-divergence\nCi(1/2, \u03c8i) \u2265 \u03c8id(\u03b8\u2217i ||\u03b8) \u2265 \u03c8i(\u00b5i \u2212 \u00b5i + \u2206i/2)2 2\u03c32 = \u03c8i\u22062i 8\u03c32 .\nChoosing \u03c8I\u2217 = 1/2, and \u03c8i \u221d \u2206\u22122i , so\n\u03c8i = 1 2  k\u2211 j\u22122 \u2206\u22122j \u22121 \u2206\u22122i yields\nmin i 6=I\u2217\nCi(1/2, \u03c8i) \u2265 1 16\u03c32 \u2211k 2 \u2206\u22122j ."}, {"heading": "F.5 Convergence of Uniform Allocation: Proof of Proposition 2", "text": "Proof. Without loss of generality, assume the problem is parameterized so that the mean of design i is \u03b8\u2217i By Proposition 5, we have\n\u03a0n(\u0398cI\u2217) .= exp{\u2212nmin i 6=I\u2217 Ci(k\u22121, k\u22121)}\nBy Lemma 1, Ci(k\u22121, k\u22121) = k\u22121d(\u03b8\u2217I\u2217 ||\u03b8) + k\u22121d(\u03b8\u2217i ||\u03b8)\nwhere \u03b8 = (\u03b8\u2217I\u2217 + \u03b8\u2217i )/2. Therefore, using the formula for the KL-divergence of standard Gaussian random variables\nCi(k\u22121, k\u22121) = (\u03b8\u2217I\u2217 \u2212 \u03b8)2 2\u03c32 + (\u03b8\u2217i \u2212 \u03b8)2 2\u03c32 = (\u03b8\u2217I\u2217 \u2212 \u03b8\u2217i )2 4\u03c32 = \u22062i 4\u03c32 .\nG Analysis of the Top-Two Allocation Rules: Proof of Proposition 7\nProposition 7. Under the TTTS, TTPS, or TTVS algorithm with parameter \u03b2 > 0, \u03c8n \u2192 \u03c8\u03b2, where \u03c8\u03b2 is the unique allocation with \u03c8\u03b2I\u2217 = \u03b2 satisfying\nCi(\u03b2, \u03c8\u03b2i ) = Cj(\u03b2, \u03c8 \u03b2 j ) \u2200i, j 6= I \u2217. (28)\nTherefore, \u03a0n(\u0398cI\u2217) .= e\u2212n\u0393 \u2217 \u03b2 . (29)\nBecause each Ci is continuous, if \u03c8n \u2192 \u03c8\u03b2 then Ci(\u03c8n,I\u2217 , \u03c8n,i) \u2192 Ci(\u03b2, \u03c8 \u03b2 i ) for all i 6= I\u2217. Equation (29) then follows by invoking Proposition 6, which establishes the optimality of the allocation \u03c8\u03b2.\nThe remainder of this section establishes that \u03c8n \u2192 \u03c8\u03b2 almost surely the proposed top-two rules. The proof is broken into a number of steps. In order to provide a nearly unified treatment of the three algorithms, we begin with several results that hold for any allocation rule.\nG.1 Results for a general allocation rule\nAs in other sections, all arguments here hold for any sample path (up to a set of measure zero). The first result provides a sufficient condition under which \u03c8n \u2192 \u03c8\u03b2. Roughly speaking, if \u03c8n,j \u2265 \u03c8 \u03b2 j +\u03b4, then too much measurement effort has been allocated to design j relative to the optimal proportion \u03c8\u03b2j . Algorithms satisfying (30) allocate negligible measurement effort to such designs, and therefore the average measurement effort they receive must decrease toward the optimal proportion.\nLemma 11 (Sufficient condition for optimality). Consider any adaptive allocation rule. If \u03c8n,I\u2217 \u2192 \u03b2 and \u2211\nn\u2208N \u03c8n,j1(\u03c8n,j \u2265 \u03c8 \u03b2 j + \u03b4) <\u221e \u2200 j 6= I \u2217, \u03b4 > 0, (30)\nthen \u03c8n \u2192 \u03c8\u03b2.\nProof. Fix a sample path for which \u03c8n,I\u2217 \u2192 \u03b2, and (30) holds. Fix some j 6= I\u2217. We first show lim inf n\u2192\u221e\n\u03c8n,j \u2264 \u03c8\u2217j . Suppose otherwise. Then, with positive probability, for some \u03b4 > 0, there exists N such that for all n \u2265 N , \u03c8n,j \u2265 \u03c8\u2217j + \u03b4. But then,\n\u2211 n\u2208N \u03c8n,j = N\u2211 n=1 \u03c8n,j + \u221e\u2211 n=N+1 1(\u03c8n,j \u2265 \u03c8\u2217j + \u03b4)\u03c8n,j <\u221e.\nBut since \u03c8n,j = \u2211n `=1 \u03c8n,j/n this implies \u03c8n,j \u2192 0.\nNow, we show lim sup n\u2192\u221e \u03c8n,j \u2264 \u03c8\u2217j . Proceeding by contradiction again, suppose otherwise. Then, with positive probability\nlim sup n\u2192\u221e \u03c8n,j > \u03c8 \u03b2 j & lim infn\u2192\u221e \u03c8n,j \u2264 \u03c8 \u03b2 j .\nOn any sample path where this occurs, for some \u03b4 > 0, there exists an infinite sequence of times N1 < N2 < N3 < ... such that \u03c8N`,j \u2265 \u03c8 \u03b2 j + 2\u03b4 when ` is odd and \u03c8N`,j \u2264 \u03c8 \u03b2 j + \u03b4 when ` is even.\nThis can only occur if, \u2211 n\u2208N \u03c8n,j1(\u03c8n,j \u2265 \u03c8\u2217j + \u03b4) =\u221e,\nwhich violates the hypothesis. Together with the hypothesis that \u03c8n,I\u2217 \u2192 \u03b2, this implies that for all i \u2208 {1, ..., k}, lim sup\nn\u2192\u221e \u03c8n,i \u2264 \u03c8\u03b2i . But since \u2211 i \u03c8n,i = \u2211 i \u03c8 \u03b2 i , this implies \u03c8n \u2192 \u03c8\u03b2.\nThe next lemma will be used to establish that (30) holds for each of the proposed algorithms. It shows that if too much measurement effort has been allocated to some design i 6= I\u2217, in the sense that \u03c8n,i > \u03c8 \u03b2 i + \u03b4 for a constant \u03b4 > 0, then \u03b1n,i is exponentially small compared maxj 6=I\u2217 \u03b1n,j .\nLemma 12 (Over-allocation implies negligible probability). Fix any \u03b4 > 0 and j 6= I\u2217. With probability 1, under any allocation rule, if \u03c8n,I\u2217 \u2192 \u03b2, there exists \u03b4\u2032 > 0 and a sequence n with n \u2192 0 such that for any n \u2208 N,\n\u03c8n,j \u2265 \u03c8 \u03b2 j + \u03b4 =\u21d2 \u03b1n,j maxi 6=I\u2217 \u03b1n,i \u2264 e\u2212n(\u03b4\u2032+ n).\nProof. Since \u03a0n(\u0398cI\u2217) = \u2211 i 6=I\u2217 \u03b1n,i, \u03a0n(\u0398cI\u2217)\n.= maxi 6=I\u2217 \u03b1n,i. Then, by invoking Proposition (6), since \u03c8n,I\u2217 \u2192 \u03b2,\nlim sup n\u2192\u221e \u2212 1 n\nlog (\nmax i 6=I\u2217 \u03b1n,i\n) \u2264 \u0393\u2217\u03b2.\nRecall the definition \u0398i , {\u03b8|\u03b8i \u2265 \u03b8I\u2217}. Now, by Proposition 4 and Lemma 1,\n\u03b1n,j = \u03a0n(\u0398j) \u2264 \u03a0n(\u0398j) .= exp{\u2212nCj(\u03c8n,I\u2217 , \u03c8n,j)} .= exp{\u2212nCj(\u03b2, \u03c8n,j)}.\nCombining these equations implies that there exists a non-negative sequence n \u2192 0 with\n\u03b1n,j maxi 6=I\u2217 \u03b1n,i \u2264 exp{\u2212n(Cj(\u03b2, \u03c8n,j)\u2212 n/2)} exp{\u2212n(\u0393\u2217\u03b2 + n/2)} = exp\n{ \u2212n ( (Cj(\u03b2, \u03c8n,j)\u2212 \u0393\u2217\u03b2)\u2212 n )} Since Cj(\u03b2, \u03c8j) is strictly increasing in \u03c8j (See lemma 1) and Cj(\u03b2, \u03c8\u03b2j ) = \u0393\u2217\u03b2, there exists some \u03b4\u2032 > 0 such that\n\u03c8n,j \u2265 \u03c8 \u03b2 j + \u03b4 =\u21d2 Cj(\u03b2, \u03c8n,j)\u2212 \u0393 \u2217 \u03b2 > \u03b4 \u2032.\nThe next result builds on Proposition 3. It shows that the quality of any design which receives infinite measurement effort is identified to arbitrary precision. On the other hand, for designs receiving finite measurement effort, there is always nonzero probability under the posterior that one of them significantly exceeds the highest quality that has been confidently identified. Therefore, \u03b1n,i and Vn,i remain bounded away from 0 for designs that receive finite measurement effort. This result will be used to show that all designs receive infinite measurement effort under the proposed top-two allocation rules, and as a result the posterior converges on the truth asymptotically. Lemma 13 (Implications of finite measurement). Let\nI = {i \u2208 {1, .., k} : \u221e\u2211 n=1 \u03c8n,i <\u221e}\ndenote the set of designs to which a finite amount of measurement effort is allocated. Then, for any i /\u2208 I \u03a0n ({\u03b8 : \u03b8i \u2208 (\u03b8\u2217i \u2212 , \u03b8\u2217i + ))\u2192 1, (31) and if I is empty\nVn,i \u2192 { 0 if i 6= I\u2217\nvI\u2217(\u03b8\u2217) > 0 if i = I\u2217 and \u03b1n,i \u2192\n{ 0 if i 6= I\u2217\n1 if i = I\u2217.\nIf I is nonempty, then for every i \u2208 I,\nlim inf n\u2192\u221e \u03b1n,i > 0 and lim inf n\u2192\u221e Vn,i > 0.\nProof. Equation (31) is implied by by Proposition 3. Now, set\n\u0398i, = {\u03b8 \u2208 \u0398 : \u03b8i \u2265 max j 6=i \u03b8j + }\nto be the set of parameters under which the quality of design i exceeds that of all others by at least . Let \u03c1\u2217 = maxi/\u2208I \u03b8\u2217i denote the quality of the best design among those that are sampled infinitely often, and choose > 0 small enough that \u03c1\u2217 + 2 < \u03b8. For i \u2208 I, we have\n\u03a0n(\u0398i, ) \u2265 \u03a0n(A)\u2212\u03a0n (B)\nfor A \u2261 {\u03b8|\u03b8i \u2265 \u03c1\u2217 + 2 & \u03b8j < \u03c1\u2217 \u2200j \u2208 I \\ {i}}\ndefined to be parameters under which \u03b8i \u2265 \u03c1\u2217 + 2 but none of the other designs in I exceed \u03c1\u2217, and\nB \u2261 {\u03b8 : max i/\u2208I\n\u03b8i \u2265 \u03c1\u2217 + }\ndefined to be the parameter vectors under which there is no design in Ic with quality exceeding \u03c1\u2217 + . By (31), \u03a0n (B)\u2192 0, but by the second part of Proposition 3, the set of parameters A cannot be completely ruled based on a finite amount of measurement effort, and\ninf n\u2208N \u03a0n(A) > 0.\nTogether this shows lim inf n\u2192\u221e \u03a0n(\u0398i, ) > 0, which implies the result.\nG.2 Results specific to the proposed algorithms\nWe now leverage the general results of the previous subsection to show \u03c8 \u2192 \u03c8\u03b2 under each proposed top-two allocation rule. Proofs are provided separately for each of the three algorithms, but they follow a similar structure. In the first step, we use Lemma 13 to argue that \u03c8n,I\u2217 \u2192 \u03b2 almost surely. The proof then uses Lemma 12 to show (30) holds, which by Lemma 11 is sufficient to establish that \u03c8n \u2192 \u03c8\u03b2."}, {"heading": "G.2.1 Top-Two Thompson Sampling", "text": "Recall that under top-two Thompson sampling, for every i \u2208 {1, ..., k},\n\u03c8n,i = \u03b1n,i \u03b2 + (1\u2212 \u03b2)\u2211 j 6=i \u03b1n,j 1\u2212 \u03b1n,j  ."}, {"heading": "Proof for TTTS.", "text": "Step 1: Show \u03c8n,I\u2217 \u2192 \u03b2. To begin, we show \u2211 n\u2208N \u03c8n,i =\u221e for each design i. Suppose otherwise.\nLet I = {i \u2208 {1, .., k} : \u2211\u221e\n1 \u03c8n,i < \u221e} be the set of designs to which finite measurement effort is allocated. Under the TTTS sampling rule, \u03c8n,i \u2265 \u03b2\u03b1n,i. Therefore, by Lemma 13, if i \u2208 I then lim inf n\u2192\u221e \u03b1n,i > 0, which implies \u2211 n\u2208N \u03c8n,i =\u221e, a contradiction.\nSince \u2211\u221e\n1 \u03c8n,i = \u221e for all i, by applying Lemma 13 we conclude that \u03b1n,I\u2217 \u2192 1. For TTTS, this implies \u03c8n,I\u2217 \u2192 \u03b2.\nStep 2: Show (30) holds. By Lemma 11, it is enough to show that (30) holds under TTTS. Let I\u0302n = arg maxi \u03b1n,i, and J\u0302n = arg maxi 6=I\u0302n \u03b1n,i. Since \u03b1n,I\u2217 \u2192 1, for each sample path there is a finite time \u03c4 < \u221e such that for all n \u2265 \u03c4 , I\u0302n = I\u2217 and therefore J\u0302n = arg maxi 6=I\u2217 \u03b1n,i. Under TTTS,\n\u03c8n,i \u2264 \u03b2\u03b1n,i + (1\u2212 \u03b2) \u03b1n,i \u03b1n,Jn \u2264 \u03b1n,i \u03b1n,Jn ,\nwhere the first inequality follows since\n\u2211 j 6=i \u03b1n,j 1\u2212 \u03b1n,j \u2264 \u2211 j 6=i \u03b1n,i 1\u2212 \u03b1n,I\u0302n \u2264 \u2211 j 6=i \u03b1n,j \u03b1n,J\u0302n \u2264 1 \u03b1n,J\u0302n .\nFor n \u2265 \u03c4 , this means \u03c8n,i \u2264 \u03b1n,i/(maxj 6=I\u2217 \u03b1n,i) for any i 6= I\u2217. By Lemma 12, there is a constant \u03b4\u2032 > 0 and a sequence n \u2192 0 such that\n\u03c8n,i \u2265 \u03c8 \u03b2 i + \u03b4 =\u21d2 \u03b1n,i maxj 6=I\u2217 \u03b1n,j \u2264 e\u2212n(\u03b4\u2032\u2212 n).\nTherefore for all i 6= I\u2217 \u2211 n\u2265\u03c4 \u03c8n,i1(\u03c8n,i \u2265 \u03c8 \u03b2 i + \u03b4) \u2264 \u2211 n\u2265\u03c4 e\u2212n(\u03b4 \u2032\u2212 n) <\u221e."}, {"heading": "G.2.2 Top-Two Probability Sampling", "text": "Recall that top-two probability sampling sets \u03c8n,I\u0302n = \u03b2 and \u03c8n,J\u0302n = 1\u2212\u03b2 where I\u0302n = arg maxi \u03b1n,i and J\u0302n = arg maxj 6=I\u0302n \u03b1n,i are the two designs with the highest posterior probability of being optimal."}, {"heading": "Proof for TTPS.", "text": "Step 1: Show \u03c8n,I\u2217 \u2192 \u03b2. To begin, we show \u2211 n\u2208N \u03c8n,i =\u221e for each design i. Suppose otherwise.\nLet I = {i \u2208 {1, .., k} : \u2211\u221e\n1 \u03c8n,i < \u221e} be the set of designs to which finite measurement effort is allocated. Proceeding by contradiction, suppose I is nonempty. By Lemma 13, there is a time \u03c4 and some probability \u03b1\u2032 > 0 such that \u03b1n,i > \u03b1\u2032 for all n \u2265 \u03c4 and i \u2208 I. However, because of the assumption that \u03b8\u2217i 6= \u03b8\u2217j , for i 6= j, I = arg maxi/\u2208I \u03b8\u2217i is unique. By (31), the algorithm identifies arg maxi/\u2208I \u03b8\u2217i with certainty, and \u03b1n,i \u2192 0 for every i /\u2208 I except for I. This means there is a time \u03c4 \u2032 > \u03c4 such that for n \u2265 \u03c4 \u2032\n\u03b1n,i > \u03b1 \u2032 if i \u2208 I \u03b1n,i \u2264 \u03b1\u2032 if i /\u2208 I and i 6= I.\nWhen this occurs at least one of the two designs with highest probability \u03b1n,i of being optimal must be in the set I, which implies designs in I receive infinite measurement effort, yielding a contradiction.\nSince \u2211\u221e\n1 \u03c8n,i = \u221e for all i, Lemma 13 implies \u03b1n,I\u2217 \u2192 1. Therefore, there is a finite time \u03c4 such that I\u0302n , arg maxi \u03b1n,i = I\u2217 for all n \u2265 \u03c4 . By the definition of the algorithm \u03c8n,I\u0302n = \u03b2, and so \u03c8n,I\u2217 = \u03b2 for all n \u2265 \u03c4 . We conclude that \u03c8n,I\u2217 \u2192 \u03b2.\nStep 2: Show (30) holds. As argued above, for each sample path there is a finite time \u03c4 < \u221e such that for all n \u2265 \u03c4 , I\u0302n = I\u2217 and therefore J\u0302n = arg maxi 6=I\u2217 \u03b1n,i. By Lemma 12, one can choose \u03c4 \u2032 \u2265 \u03c4 such that for all n \u2265 \u03c4 \u2032,\n\u03c8n,j \u2265 \u03c8 \u03b2 j + \u03b4 =\u21d2 \u03b1n,j < max\ni 6=I\u2217 \u03b1n,i\nand therefore by definition J\u0302n 6= j. This concludes the proof, as it shows that for each sample path there is a finite time \u03c4 \u2032 after which TTPS never allocates any measurement effort to design j when \u03c8n,j \u2265 \u03c8 \u03b2 j + \u03b4."}, {"heading": "G.2.3 Top-Two Value Sampling", "text": "Recall that top-two value sampling sets and \u03c8n,I\u0302n = \u03b2 and \u03c8n,J\u0302n = 1\u2212 \u03b2 where I\u0302n = arg maxi Vn,i and J\u0302n = arg maxj 6=I\u0302n Vn,i are the two designs with the highest posterior value.\nProof for TTVS. Step 1: Show \u03c8n,I\u2217 \u2192 \u03b2. The proof is essentially identical to that for TTPS.To begin, we show \u2211 n\u2208N \u03c8n,i = \u221e for each design i. Suppose otherwise. Let I = {i \u2208 {1, .., k} :\u2211\u221e\n1 \u03c8n,i < \u221e} be the set of designs to which finite measurement effort is allocated. Proceeding by contradiction, suppose I is nonempty. By Lemma 13, there is a time \u03c4 and some v > 0 such that Vn,i > v for all n \u2265 \u03c4 and i \u2208 I. However, because of the assumption that \u03b8\u2217i 6= \u03b8\u2217j , for i 6= j, I = arg maxi/\u2208I \u03b8\u2217i is unique3. By (31), the algorithm identifies arg maxi/\u2208I \u03b8\u2217i with certainty, and\n3If the arg-max is not unique, then one could show that Vn,i \u2192 0 for all i /\u2208 I, and that therefore there is a finite time after both of the top-two designs are always in the set I, yielding a contradiction\nVn,i \u2192 0 for every i /\u2208 I except for I. Then there is a time \u03c4 \u2032 > \u03c4 such that for n \u2265 \u03c4 \u2032\nVn,i > v if i \u2208 I Vn,i \u2264 v if i /\u2208 I and i 6= I\u2217.\nWhen this occurs at least one of the two designs with highest value Vn,i must be in the set I, which implies designs in I receive infinite measurement effort, yielding a contradiction.\nSince \u2211\u221e\n1 \u03c8n,i =\u221e for all i, Lemma 13 implies Vn,I\u2217 \u2192 vI\u2217(\u03b8\u2217) > 0 and Vn,i \u2192 0 for all i 6= I\u2217. Therefore, there is a finite time \u03c4 such that arg maxi Vn,i = I\u2217 for all \u03c4 \u2265 n. By the definition of the algorithm arg maxi Vn,i is sampled with probability \u03b2, and so \u03c8n,I\u2217 = \u03b2 for all n \u2265 \u03c4 . We conclude that \u03c8n,I\u2217 \u2192 \u03b2.\nStep 2: Show (30) holds. Again, the proof is essentially identical to that for TTPS. As argued above, for each sample path there is a finite time \u03c4 < \u221e such that for all n \u2265 \u03c4 , I\u0302n = I\u2217 and therefore J\u0302n = arg maxi 6=I\u2217 Vn,i. By Lemma 3, Vn,i\n.= \u03b1n,i. Combining this with Lemma 12 shows one can choose \u03c4 \u2032 \u2265 \u03c4 such that for all n \u2265 \u03c4 \u2032,\n\u03c8n,j \u2265 \u03c8 \u03b2 j + \u03b4 =\u21d2 Vn,j < max\ni 6=I\u2217 Vn,i\nand therefore by definition J\u0302n 6= j. This concludes the proof, as it shows that for each sample path there is a finite time \u03c4 \u2032 after which TTVS never allocates any measurement effort to design j 6= I\u2217 when \u03c8n,j \u2265 \u03c8 \u03b2 j + \u03b4."}], "references": [{"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "The sequential design of experiments for infinitely many states of nature", "author": ["A.E. Albert"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Albert.,? \\Q1961\\E", "shortCiteRegEx": "Albert.", "year": 1961}, {"title": "Best arm identification in multi-armed bandits", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "In COLT-23th Conference on Learning Theory-2010, pages 13\u2013p,", "citeRegEx": "Audibert and Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2010}, {"title": "A single-sample multiple decision procedure for ranking means of normal populations with known variances", "author": ["R.E. Bechhofer"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Bechhofer.,? \\Q1954\\E", "shortCiteRegEx": "Bechhofer.", "year": 1954}, {"title": "Bayesian statistics and the efficiency and ethics of clinical trials", "author": ["D.A. Berry"], "venue": "Statistical Science,", "citeRegEx": "Berry.,? \\Q2004\\E", "shortCiteRegEx": "Berry.", "year": 2004}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Simulation budget allocation for further enhancing the efficiency of ordinal optimization", "author": ["C.-H. Chen", "J. Lin", "E. Y\u00fccesan", "S.E. Chick"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Chen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2000}, {"title": "Sequential design of experiments", "author": ["H. Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Chernoff.,? \\Q1959\\E", "shortCiteRegEx": "Chernoff.", "year": 1959}, {"title": "Approaches in sequential design of experiments. A Survey of Statistical Design and Linear Models (Edited by J. N", "author": ["H. Chernoff"], "venue": null, "citeRegEx": "Chernoff.,? \\Q1975\\E", "shortCiteRegEx": "Chernoff.", "year": 1975}, {"title": "Sequential sampling with economics of selection procedures", "author": ["S.E. Chick", "P. Frazier"], "venue": "Management Science,", "citeRegEx": "Chick and Frazier.,? \\Q2012\\E", "shortCiteRegEx": "Chick and Frazier.", "year": 2012}, {"title": "Economic analysis of simulation selection problems", "author": ["S.E. Chick", "N. Gans"], "venue": "Management Science,", "citeRegEx": "Chick and Gans.,? \\Q2009\\E", "shortCiteRegEx": "Chick and Gans.", "year": 2009}, {"title": "Sequential sampling to myopically maximize the expected value of information", "author": ["S.E. Chick", "J. Branke", "C. Schmidt"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "Chick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chick et al\\.", "year": 2010}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "A fully sequential elimination procedure for indifference-zone ranking and selection with tight bounds on probability of correct selection", "author": ["P.I. Frazier"], "venue": "Operations Research,", "citeRegEx": "Frazier.,? \\Q2014\\E", "shortCiteRegEx": "Frazier.", "year": 2014}, {"title": "A knowledge-gradient policy for sequential information collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Frazier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Frazier et al\\.", "year": 2008}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Optimal best arm identification with fixed confidence", "author": ["Aur\u00e9lien Garivier", "Emilie Kaufmann"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Garivier and Kaufmann.,? \\Q2016\\E", "shortCiteRegEx": "Garivier and Kaufmann.", "year": 2016}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J.C. Gittins"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "D.M. Jones"], "venue": "In J. Gani, editor, Progress in Statistics,", "citeRegEx": "Gittins and Jones.,? \\Q1974\\E", "shortCiteRegEx": "Gittins and Jones.", "year": 1974}, {"title": "A large deviations perspective on ordinal optimization", "author": ["P. Glynn", "S. Juneja"], "venue": "In Simulation Conference,", "citeRegEx": "Glynn and Juneja.,? \\Q2004\\E", "shortCiteRegEx": "Glynn and Juneja.", "year": 2004}, {"title": "Ordinal optimization-empirical large deviations rate estimators, and stochastic multi-armed bandits", "author": ["P. Glynn", "S. Juneja"], "venue": "arXiv preprint arXiv:1507.04564,", "citeRegEx": "Glynn and Juneja.,? \\Q2015\\E", "shortCiteRegEx": "Glynn and Juneja.", "year": 2015}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Web-scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft\u2019s Bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Graepel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Graepel et al\\.", "year": 2010}, {"title": "Entropy and information theory", "author": ["R.M. Gray"], "venue": null, "citeRegEx": "Gray.,? \\Q2011\\E", "shortCiteRegEx": "Gray.", "year": 2011}, {"title": "Bayesian look ahead one-stage sampling allocations for selection of the best population", "author": ["S.S. Gupta", "K.J. Miescke"], "venue": "Journal of statistical planning and inference,", "citeRegEx": "Gupta and Miescke.,? \\Q1996\\E", "shortCiteRegEx": "Gupta and Miescke.", "year": 1996}, {"title": "Optimistic gittins indices", "author": ["E. Gutin", "V. Farias"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Gutin and Farias.,? \\Q2016\\E", "shortCiteRegEx": "Gutin and Farias.", "year": 2016}, {"title": "Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting", "author": ["K. Jamieson", "R. Nowak"], "venue": "In Information Sciences and Systems (CISS),", "citeRegEx": "Jamieson and Nowak.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson and Nowak.", "year": 2014}, {"title": "Asymptotically optimal procedures for sequential adaptive selection of the best of several normal means", "author": ["C. Jennison", "I.M. Johnstone", "B.W. Turnbull"], "venue": "Statistical decision theory and related topics III,", "citeRegEx": "Jennison et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Jennison et al\\.", "year": 1982}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Z. Karnin", "T. Koren", "O. Somekh"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "Thompson sampling: an asymptotically optimal finite time analysis", "author": ["E. Kauffmann", "N. Korda", "R. Munos"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Kauffmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kauffmann et al\\.", "year": 2012}, {"title": "On bayesian index policies for sequential resource allocation", "author": ["E. Kaufmann"], "venue": "arXiv preprint arXiv:1601.01190,", "citeRegEx": "Kaufmann.,? \\Q2016\\E", "shortCiteRegEx": "Kaufmann.", "year": 2016}, {"title": "Information complexity in bandit subset selection", "author": ["E. Kaufmann", "S. Kalyanakrishnan"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Kaufmann and Kalyanakrishnan.,? \\Q2013\\E", "shortCiteRegEx": "Kaufmann and Kalyanakrishnan.", "year": 2013}, {"title": "On Bayesian upper confidence bounds for bandit problems", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "On the complexity of best arm identification in multiarmed bandit models", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "arXiv preprint arXiv:1407.4443,", "citeRegEx": "Kaufmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2014}, {"title": "Second order efficiency in the sequential design of experiments", "author": ["R. Keener"], "venue": "The Annals of Statistics,", "citeRegEx": "Keener.,? \\Q1984\\E", "shortCiteRegEx": "Keener.", "year": 1984}, {"title": "Asymptotically optimum sequential inference and design", "author": ["J. Kiefer", "J. Sacks"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Kiefer and Sacks.,? \\Q1963\\E", "shortCiteRegEx": "Kiefer and Sacks.", "year": 1963}, {"title": "Selecting the best system", "author": ["S.-H. Kim", "B.L. Nelson"], "venue": "Handbooks in operations research and management science,", "citeRegEx": "Kim and Nelson.,? \\Q2006\\E", "shortCiteRegEx": "Kim and Nelson.", "year": 2006}, {"title": "Recent advances in ranking and selection. In Proceedings of the 39th conference on Winter simulation: 40 years! The best is yet to come, pages 162\u2013172", "author": ["S.-H. Kim", "B.L. Nelson"], "venue": null, "citeRegEx": "Kim and Nelson.,? \\Q2007\\E", "shortCiteRegEx": "Kim and Nelson.", "year": 2007}, {"title": "Thompson sampling for one-dimensional exponential family bandits", "author": ["N. Korda", "E. Kaufmann", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Korda et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Korda et al\\.", "year": 2013}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "Active sequential hypothesis testing", "author": ["M. Naghshvar", "T. Javidi"], "venue": "The Annals of Statistics,", "citeRegEx": "Naghshvar and Javidi,? \\Q2013\\E", "shortCiteRegEx": "Naghshvar and Javidi", "year": 2013}, {"title": "Controlled sensing for multihypothesis testing", "author": ["S. Nitinawarat", "G. K Atia", "V.V. Veeravalli"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Nitinawarat et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nitinawarat et al\\.", "year": 2013}, {"title": "A sequential procedure for selecting the population with the largest mean from k normal populations", "author": ["E. Paulson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Paulson.,? \\Q1964\\E", "shortCiteRegEx": "Paulson.", "year": 1964}, {"title": "On two-stage selection procedures and related probability-inequalities", "author": ["Y. Rinott"], "venue": "Communications in Statistics-Theory and methods,", "citeRegEx": "Rinott.,? \\Q1978\\E", "shortCiteRegEx": "Rinott.", "year": 1978}, {"title": "Learning to optimize via information-directed sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "How much does your data exploration overfit? Controlling bias via information usage", "author": ["D. Russo", "J. Zou"], "venue": "arXiv preprint arXiv:1511.05219,", "citeRegEx": "Russo and Zou.,? \\Q2015\\E", "shortCiteRegEx": "Russo and Zou.", "year": 2015}, {"title": "On the convergence rates of expected improvement methods", "author": ["I.O. Ryzhov"], "venue": "Operations Research,", "citeRegEx": "Ryzhov.,? \\Q2016\\E", "shortCiteRegEx": "Ryzhov.", "year": 2016}, {"title": "The knowledge gradient algorithm for a general class of online learning problems", "author": ["I.O. Ryzhov", "W.B. Powell", "P.I. Frazier"], "venue": "Operations Research,", "citeRegEx": "Ryzhov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ryzhov et al\\.", "year": 2012}, {"title": "Overview of content experiments: Multi-armed bandit experiments, 2016", "author": ["S.L. Scott"], "venue": "URL https: //support.google.com/analytics/answer/2844870?hl=en. [Online; accessed 9-November2016]", "citeRegEx": "Scott.,? \\Q2016\\E", "shortCiteRegEx": "Scott.", "year": 2016}, {"title": "Information-theoretic regret bounds for Gaussian process optimization in the bandit setting", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Srinivas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2012}, {"title": "Automatic ad format selection via contextual bandits", "author": ["L. Tang", "R. Rosales", "A. Singh", "D. Agarwal"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges", "author": ["S.S. Villar", "J. Bowden", "J. Wason"], "venue": "Statistical Science,", "citeRegEx": "Villar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Villar et al\\.", "year": 2015}, {"title": "Probability with martingales", "author": ["D. Williams"], "venue": "Cambridge university press,", "citeRegEx": "Williams.,? \\Q1991\\E", "shortCiteRegEx": "Williams.", "year": 1991}, {"title": "As a result, there is no problem with finite k for which the sampling ratios in (14) are optimal. One can show, in fact, that any optimal multi-armed bandit algorithm that attains the lower bound of Lai and Robbins [1985] also satisfies equations (13) and (14). The main innovation in this paper is to show how to build on such bandit algorithms to attain near-optimal rates for the best-arm identification", "author": ["Glynn", "Juneja", "Jennison"], "venue": null, "citeRegEx": "Glynn et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Glynn et al\\.", "year": 1982}, {"title": "2016] also studies the knowledge gradient policy, which could offer improved performance", "author": ["problem. Ryzhov"], "venue": null, "citeRegEx": "Ryzhov,? \\Q2016\\E", "shortCiteRegEx": "Ryzhov", "year": 2016}], "referenceMentions": [{"referenceID": 54, "context": "1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al., 2015].", "startOffset": 178, "endOffset": 199}, {"referenceID": 53, "context": "1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al.", "startOffset": 60, "endOffset": 76}, {"referenceID": 4, "context": "However, we will see that optimal rules from this perspective also allocate fewer patients to very poor treatments, potentially leading to more ethical trials [Berry, 2004].", "startOffset": 159, "endOffset": 172}, {"referenceID": 50, "context": "For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004].", "startOffset": 86, "endOffset": 99}, {"referenceID": 4, "context": "For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004].", "startOffset": 123, "endOffset": 136}, {"referenceID": 49, "context": ", 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016].", "startOffset": 89, "endOffset": 110}, {"referenceID": 26, "context": ", 2012], and optimistic Gittins indices [Gutin and Farias, 2016].", "startOffset": 40, "endOffset": 64}, {"referenceID": 11, "context": "Recent work has cast this problem in a decision-theoretic framework [Chick and Gans, 2009].", "startOffset": 68, "endOffset": 90}, {"referenceID": 10, "context": ", 2010] or a continuous-time problem with only two alternatives [Chick and Frazier, 2012] \u2013 and then extend those solutions heuristically to build measurement and stopping rules in more general settings.", "startOffset": 64, "endOffset": 89}, {"referenceID": 0, "context": ", 2012, Kaufmann, 2016], Thompson sampling [Agrawal and Goyal, 2012, Korda et al., 2013, Gopalan et al., 2014, Johnson et al., 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016]. These heuristic algorithms can be applied effectively to complicated learning problems beyond the specialized settings in which the Gittins index theorem holds, have been shown to have strong performance in simulation, and have theoretical performance guarantees. In several cases, they are known to attain sharp asymptotic limits on the performance of any adaptive algorithm due to Lai and Robbins [1985]. The pure-exploration problem studied in this paper is not nearly as well understood.", "startOffset": 44, "endOffset": 700}, {"referenceID": 47, "context": "work by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 7, "context": "work by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al. [2000]. This contribution is very similar in spirit to this paper, as it relates the longrun behavior of a simple Bayesian measurement strategy to a notion of an approximately optimal allocation.", "startOffset": 188, "endOffset": 207}, {"referenceID": 33, "context": "See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews.", "startOffset": 4, "endOffset": 26}, {"referenceID": 33, "context": "See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews.", "startOffset": 4, "endOffset": 52}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal.", "startOffset": 15, "endOffset": 32}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation.", "startOffset": 15, "endOffset": 552}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average.", "startOffset": 15, "endOffset": 838}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound.", "startOffset": 15, "endOffset": 1061}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance.", "startOffset": 15, "endOffset": 1173}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence.", "startOffset": 15, "endOffset": 1272}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein.", "startOffset": 15, "endOffset": 1526}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein. However, in a sense described below, Jennison et al. [1982] show formally that there are problems with Gaussian observations where any sequential-elimination algorithm will require substantially more samples than optimal adaptive allocation rules.", "startOffset": 15, "endOffset": 1614}, {"referenceID": 20, "context": ", 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016].", "startOffset": 33, "endOffset": 57}, {"referenceID": 17, "context": ", 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016].", "startOffset": 129, "endOffset": 158}, {"referenceID": 11, "context": "We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004].", "startOffset": 95, "endOffset": 118}, {"referenceID": 11, "context": "We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004]. A great deal of work has sought \u201cproblem dependent\u201d bounds, which reveal that the best-arm can be identified more rapidly when the true problem instance is easier.", "startOffset": 95, "endOffset": 151}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation.", "startOffset": 83, "endOffset": 102}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge.", "startOffset": 83, "endOffset": 640}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue.", "startOffset": 83, "endOffset": 714}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue. This paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing.", "startOffset": 83, "endOffset": 834}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue. This paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing. Chernoff\u2019s asymptotic derivations give great insight best-arm identification, which can be formulated as a multiple-hypothesis testing problem with sequentially chosen experiments, but surprisingly this connection does not seem to be discussed in the literature. Chernoff looks at a different scaling than Glynn and Juneja [2004]. Rather than take the budget of available measurements to infinity, he allows the algorithm to stop and declare the hypothesis true or false at any time, but takes the cost of gathering measurements to zero while the cost of an incorrect terminal decision stays fixed.", "startOffset": 83, "endOffset": 1235}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design.", "startOffset": 102, "endOffset": 228}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance.", "startOffset": 102, "endOffset": 335}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit.", "startOffset": 102, "endOffset": 769}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al.", "startOffset": 102, "endOffset": 1467}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings.", "startOffset": 102, "endOffset": 1494}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting.", "startOffset": 102, "endOffset": 1865}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone.", "startOffset": 102, "endOffset": 2045}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al.", "startOffset": 102, "endOffset": 2774}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al.", "startOffset": 102, "endOffset": 2872}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al. [1982]. While the", "startOffset": 102, "endOffset": 2944}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al.", "startOffset": 33, "endOffset": 49}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, \u201csidestep the issue of how to experiment in the early stages.", "startOffset": 33, "endOffset": 73}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, \u201csidestep the issue of how to experiment in the early stages.", "startOffset": 33, "endOffset": 101}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, \u201csidestep the issue of how to experiment in the early stages.", "startOffset": 33, "endOffset": 217}, {"referenceID": 5, "context": "The work of Bubeck et al. [2009] shows formally that algorithms satisfying regret bounds of order log(n) are necessarily far from optimal for the problem of identifying the best arm.", "startOffset": 12, "endOffset": 33}, {"referenceID": 2, "context": "Audibert and Bubeck [2010]).", "startOffset": 0, "endOffset": 27}, {"referenceID": 8, "context": "As highlighted in the literature review, the max-min problem (7) closely mirrors the main sample complexity term in Chernoff\u2019s classic paper on the sequential design of experiments (Chernoff [1959]).", "startOffset": 116, "endOffset": 198}, {"referenceID": 20, "context": "The results in this proposition are closely related to those in Glynn and Juneja [2004], in which large deviations rate functions take the place of the functions Ci.", "startOffset": 64, "endOffset": 88}, {"referenceID": 33, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959].", "startOffset": 65, "endOffset": 89}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al.", "startOffset": 116, "endOffset": 132}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section.", "startOffset": 116, "endOffset": 226}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures \u201ctypically.", "startOffset": 116, "endOffset": 468}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures \u201ctypically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.\u201d In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but \u201cthe approach is very coarse for moderate sample size problems.", "startOffset": 116, "endOffset": 721}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures \u201ctypically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.\u201d In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but \u201cthe approach is very coarse for moderate sample size problems.\u201d He writes that two-stage procedures of Kiefer and Sacks [1963], \u201csidestep the issue of how to experiment in the early stages,\u201d while constructing the optimal allocations based on point estimates \u201ctreats estimates of \u03b8 based on a few observations with as much respect as that based on many observations.", "startOffset": 116, "endOffset": 1048}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al.", "startOffset": 40, "endOffset": 56}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016].", "startOffset": 40, "endOffset": 80}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(\u03b4) as \u03b4 \u2192 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[\u03c4\u03b4] as in Jennison et al.", "startOffset": 40, "endOffset": 100}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(\u03b4) as \u03b4 \u2192 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[\u03c4\u03b4] as in Jennison et al. [1982] or Kaufmann [2016].", "startOffset": 40, "endOffset": 403}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(\u03b4) as \u03b4 \u2192 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[\u03c4\u03b4] as in Jennison et al. [1982] or Kaufmann [2016].", "startOffset": 40, "endOffset": 422}], "year": 2016, "abstractText": "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort, and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant of top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation rules.", "creator": "LaTeX with hyperref package"}}}