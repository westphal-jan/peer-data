{"id": "1602.08671", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2016", "title": "Lie Access Neural Turing Machine", "abstract": "5,791 Recently, seafloor Neural 15.80 Turing Machine kirkhill and Memory krankl Networks blackburn have shown hallo that adding satpura an external memory can greatly mortuaries ameliorate chlorosis a traditional woldegiorgis recurrent neural raushan network ' s tendency wti to ladki forget dinefwr after a pogo\u0144 long period of qimonda time. arwal Here perdanakusumah we present a new yellow-headed design of 214.8 an external 51-year memory, wherein memories ledin are xandra stored gilbert in an sunseri Euclidean key space $ \\ wavefronts mathbb minutemen R ^ n $. matherne An LSTM capulin controller sase performs read 1.0001 and write via specialized structures called read and steadfast write sixto heads, oyston following radoi the lagarde design renowed of sonjay Neural gudino Turing balcerowicz Machine. campfield It mcgarty can kwaramba move a sensorineural head by either providing vajgl a 16.40 new address \u1e29\u0101jj in haviva the 57,100 key tokyu space (d'errico aka ve\u013ek\u00e1 random llotja access) or loughead moving from its pnp previous paymasters position via wrr a Lie 3,391 group action (willies aka Lie kushan access ). tretiakov In church this loquasto way, schofields the \" ethology L \" and \" phd R \" world instructions mynamar of cabe\u00e7a a dzongkha traditional stereograms Turing Machine zable is generalized bombes to arbitrary elements of a missabe fixed pouliot Lie arnica group action. alhazen For trebbia this reason, we tred name this winrich new saitta model ouda the Lie freedoms Access swagged Neural Turing Machine, 943,000 or LANTM.", "histories": [["v1", "Sun, 28 Feb 2016 04:55:19 GMT  (1474kb,D)", "http://arxiv.org/abs/1602.08671v1", null], ["v2", "Tue, 23 Aug 2016 01:23:46 GMT  (781kb,D)", "http://arxiv.org/abs/1602.08671v2", null], ["v3", "Tue, 6 Sep 2016 14:42:56 GMT  (14976kb,AD)", "http://arxiv.org/abs/1602.08671v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["greg yang"], "accepted": true, "id": "1602.08671"}, "pdf": {"name": "1602.08671.pdf", "metadata": {"source": "CRF", "title": "Lie Access Neural Turing Machine", "authors": ["Greg Yang"], "emails": ["gyang@college.harvard.edu"], "sections": [{"heading": null, "text": "We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. As LANTM is differentiable end-to-end, training was done with RMSProp. We found the right configuration of LANTM to be capable of learning different permutation and arithmetic tasks and extrapolating to at least twice the input size, all with the number of parameters 2 orders of magnitude below that for the LSTM baseline. In particular, we trained LANTM on addition of k-digit numbers for 2 \u2264 k \u2264 16, but it was able to generalize almost perfectly to 17 \u2264 k \u2264 32."}, {"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) are powerful devices that, unlike conventional neural networks, are able to keep state across time. They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on. However, despite such advances, RNNs still cannot maintain memory for long periods of time, presenting an obstacle to attaining human-like general intelligence.\nRecently, the Neural Turing Machine (NTM) of Graves et al. [5] and memory networks of Sukhbaatar et al. [14] achieved breakthroughs toward this goal by utilizing external memories. This capability allowed NTM to learn and generalize simple algorithms, and the memory network to complete simple QA tasks. Later on, Grefenstette et al. [6] introduced neural stacks, queues, and deques. It followed in the trend for external memories: as NTM\u2019s external memory could be treated as a smooth version of a random access memory array, neural stack/queues/deques could be treated as smooth versions of the corresponding discrete data structures, to be operated by LSTM controllers. These models were able to learn language-like tasks over very long input sequences and generalize to even longer sequences.\nWe here present a new design of an external memory based on a dictionary with keys residing in an Euclidean key space Rn. Read and write heads move around in this key space either by random \u2217email: gyang@college.harvard.edu\nar X\niv :1\n60 2.\n08 67\n1v 1\n[ cs\n.N E\n] 2\n8 Fe\nb 20\naccess or via an action of a fixed Lie group G and perform their functions with the current address. In analogy to the traditional Turing machine, the tape has become a continuous space and the \u201cL\u201d and \u201cR\u201d movement instructions have become elements of G.\nCombining this external memory with an LSTM controller, we call such a model Lie Access Neural Turing Machine, or LANTM.\nWith a concrete implementation with the key space being R2 and G being the translation group, we show that LANTM, with the right design choices, learns simple algorithms and generalizes very well. In particular, it learns addition of \u2264 16 digit integers but can answer 99% of randomly selected \u2264 32 digit addition problems correctly. Furthermore, while the same model trained on \u2264 16 digit addition could no longer correctly compute 64 digit additions, the outputs given are only wrong in a small fraction (5%) of total digits."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Lie groups", "text": "We here review basic concepts of (Lie) group theory.\nA group is a set S with operations \u2217 (multiplication), (.)\u22121 (inverse), and e (unit) of arity respectively 2, 1, 0, such that\n\u2022 (associativity) for all a, b, c \u2208 G, (a \u2217 b) \u2217 c = a \u2217 (b \u2217 c)\n\u2022 (inverse) for all a \u2208 G, a \u2217 a\u22121 = a\u22121 \u2217 a = e\n\u2022 (identity) for all a \u2208 G, a \u2217 e = e \u2217 a = a\nThe classical examples are (Zn,+,\u2212(.), 0), (Rn,+,\u2212(.), 0), matrix groups like GL(n), and cyclic groups Z/nZ.\nA group often \u201cacts on\u201d another object or set, like a hand twists a rubik\u2019s cube. For example, imagine an equilateral triangle with its vertices colored differently. Rotating the triangle by 120 degrees permutes the vertex color but leaves the overall shape unchanged. If we let 0, 1, 2 \u2208 Z/3Z correspond respectively to rotations of the equilateral triangle by 0, 120, or 240 degrees, and addition in Z/3Z corresponds to applying two such rotations consecutively, then Z/3Z is said to act on the set of color permutations of the triangle, because it maps one such permutation to another by a rotation. Or, consider A = R2 as a set of vectors and B = R2 as a set of points. One may drag an element of B by a vector from A, thus mapping it to another element of B. Then we say A acts on B by vector addition. As this example illustrates, a group G always acts on itself by the group multiplication (in the example, this is addition of R2 vectors). So in fact, every group acts on another set. Formally, a group action of group G on set X is defined as a mapping \u03c6 : G\u00d7X \u2192 X : (g, x) 7\u2192 g \u00b7 x such that\n\u2022 e \u00b7 x = x for all x \u2208 X\n\u2022 (a \u2217 b) \u00b7 x = a \u00b7 (b \u00b7 x) for all a, b \u2208 G, x \u2208 X.\nIt is the ubiquity of group action that explains the ubiquity of groups in mathematics. In this paper, we only borrow the language of groups and group actions to the extent it neatly expresses many ideas central to our design. No advanced ideas from mathematics are used.\nA Lie group is a group with a smooth manifold structure such that multiplication and inverse operations are smooth maps. Similarly, a smooth group action of a Lie group G on smooth manifold M is just a group action \u03c6 : G\u00d7M \u2192M that is smooth. In the context of smooth Lie group action, we also call elements of G Lie actions.\nThe reader who has had no experience with smooth topology need not worry too much about the precise meaning of these definitions beyond the intuition that \u201cLie group is a group such that\nmost things you do to it are differentiable\u201d and \u201csmooth Lie group action is a differentiable group action\u201d. Indeed, the only reason we require a Lie group rather than a group is so that its group action yields to gradient descent. (To that end, it is not strictly necessary for the groups to be infinitely differentiable, but as all common differentiable groups are Lie groups and all groups explored in this paper are Lie group, this distinction is not needed.) The reader hoping to learn the basics of smooth manifolds and Lie groups can consult John Lee\u2019s excellent Introduction to Smooth Manifolds [12]."}, {"heading": "2.2 Recurrent Neural Networks", "text": "Unlike the conventional feedforward neural network, a recurrent neural network (RNN) has selfconnections. In other words, it maintains an internal state similar to the latent variables of a hidden Markov model.\nMathematically, an RNN is a function \u03c1 : X \u00d7H \u2192 Y \u00d7H, where X is the input space, Y the output space, and H the space of internal states. On input (x(1), . . . , x(T )) \u2208 XT and with initial state h(0) \u2208 H, the RNN transitions into states h(1), . . . , h(T ) (internally) and returns a sequence (y(1), . . . , y(T )) (externally) defined recursively by\n(y(t), h(t)) = R(x(t), h(t\u22121)).\nA simple implementation of R would just be a 2-layer neural network\nh(t) := \u03c4(Wxhx (t) +Whhh (t\u22121) + bh)\ny(t) := Whyh (t) + by\nwhere \u03c4 is a transfer function, W\u00b7\u00b7 are matrices and b\u00b7 are bias vectors. \u03c4 is usually differentiable everywhere or almost everywhere, so that the RNN can be trained by backpropagation through time (BPTT) [17]. In this work, we use a particular variant of RNN called the Long Short Term Memory (LSTM) [7]. LSTM\u2019s hidden state consists of two variables (c(t), h(t)), where h(t) is also the output to the external world (i.e. it fills the role of y(t) in the above description). The c(t) is the \u201cmemory\u201d of the machine, designed to be maintained for a long time when necessary. There are many variants of LSTM. In this paper we define the function LSTM : (x(t), c(t\u22121), h(t\u22121)) 7\u2192 (y(t), c(t), h(t)) as follows:\ni(t) := \u03c3(Wxix (t) +Whih (t\u22121) + bi)\nf (t) := \u03c3(Wxfx (t) +Whfh (t\u22121) + bf ) c(t) := f (t)c(t\u22121) + i(t) tanh(Wxcx (t) +Whch (t\u22121) + bc)\no(t) := \u03c3(Wxox (t) +Whoh (t\u22121) + bo)\nh(t) := o(t) tanh(c(t))\ny(t) := h(t)\nwhere \u03c3 is the logistic function. i(t), f (t), o(t) are called the input, forget, and output gates, respectively, which modulate multiplicatively different quantities in the computation. The weights W\u00b7\u00b7 are trainable through BPTT. The undashed parts of figure 1 show a schematic of the equations above.\nIn models with external memories, LSTM often serves as the controller [5, 6, 20]. This means that 1) the entire system carries state over time from both the LSTM and the external memory, 2) the LSTM controller collects reading from and computes additional instructions to the external memory, and 3) the LSTM possibly performs extra processing F to return the desired output at each time point. The dashed parts of figure 1 demonstrate a typical such arrangement, in which \u03a3(t) represents the state of the memory, \u03c1(t) represents the reading from the memory, RW represents a\nsubroutine used for reading from and writing to the memory. The entire system is now described by the recurrence TM : (x(t),\u03a3(t\u22121), c(t\u22121), \u03c1(t\u22121), h(t\u22121)) 7\u2192 (y(t),\u03a3(t), c(t), \u03c1(t), h(t)) defined by\n(e(t) \u2295 h(t), c(t), e(t) \u2295 h(t)) := LSTM(x(t), c(t\u22121), \u03c1(t\u22121) \u2295 h(t\u22121)) y(t) := F (h(t))\n(\u03a3(t), \u03c1(t)) := RW(\u03a3(t\u22121), e(t)),\nwhere e(t) is a set of instructions to read from and write to the memory, as illustrated in figure 1. F is usually a softmax layer that produces a distribution over all possible symbols in a language task such as those explored in this paper, and this is indeed the case with LANTM. In the next section, we show how LANTM implements RW."}, {"heading": "3 Lie Access Memory", "text": "As mentioned in the introduction, Lie Access Neural Turing Machine (LANTM) is based off of the external memory architecture of Neural Turing Machine (NTM). This architecture can be summarized by figure 2: a neural network controller reads from and writes to a memory structure via specially designed functions called \u201cheads\u201d, paying respect to terminologies of Turing machine literature. The heads themselves do not have any trainable parameters, so the only learning done is by the controller.\nIn a LANTM, the memory structure is a dictionary, with keys in an Euclidean space Rn for a fixed n, called the key space or address space; and with values (called memory vectors) in another Euclidean space Rm for a fixed m (m is called the memory width). At time step t, each read head converts instructions from the controller to a read address k (t) r \u2208 Rn that retrieves a reading \u03c1(t) from the memory by a weighted inverse squared (or polynomial in general) law, to be elaborated below. Each write head converts instructions from the controller to a new memory vector m(t) \u2208 Rm\nand a new address k (t) w \u2208 Rn, along with a scalar s(t) \u2208 [0, 1], called the memory strength of the vector. Such a triple (k (t) w ,m(t), s(t)) is essentially appended to the memory.\nAside from m and n, another hyperparameter of a LANTM is its choice of Lie group G that acts on Rn. At time t+ 1, the controller may emit new addresses for each head (random access) or issue Lie actions g \u2208 G that change the old addresses (Lie access).\nOne may imagine the key space to be a piece of paper, and the read and write heads to be stones placed on this paper. The controller is a hand that moves the stones from turn to turn. Sometimes it may lift a stone up and place it somewhere completely unrelated to its original position (random access); other times it may drag a stone along a chosen direction (Lie access).\nIn the design discussed in this paper, there is no explicit erasure. However, the machine can theoretically store the exact negation of a memory vector at the same location to cancel out that memory, albeit the required precision to do so would probably be overwhelming.\nWhat follows are details of the overview given above."}, {"heading": "3.1 Read", "text": "Let M (t) denote the set of memory vectors stored in the key space by time t. We choose a canonical ordering on this set, for example by time added, and write M (t)(i) for the ith vector in this order. Denote by a(t)(i) the corresponding addresses of M (t)(i) and by S(t)(i) the corresponding memory strength of M (t)(i). In this section we introduce two weight schemes for retrieving a value from the memory via an address. The main idea of both is summarized by figure 3.\nThe read key k (t) r produces weightings w (t) r (i) over all memory vectors M (t)(i), each with address\na(t)(i), by normalizing their inverse squared distances:\nw(t)r (i) := S(t)(i)\u2016k(t)r \u2212 a(t)(i)\u2016\u22122\u2211 j S (t)(j)\u2016k(t)r \u2212 a(t)(j)\u2016\u22122\nwith the convention that it takes the limit value when k (t) r \u2192 a(t)(i) for some i (e.g. if all the keys are distinct and all strengths are nonzero, then w (t) r (i) = 1 and w (t) r (j) = 0 for j 6= i)\nIn practice, as the formula for w (t) r can induce numerical instability as k (t) r \u2192 a(t)(i) for some i,\nwe adjust the formula with a small , e.g. 10\u22129, so that\nw(t)r (i) := S(t)(i)(\u2016k(t)r \u2212 a(t)(i)\u20162 + )\u22122\u2211 j S (t)(j)(\u2016k(t)r \u2212 a(t)(j)\u20162 + )\u22122 .\nObviously, \u2211 j w (t) r (j) = 1 and each w (t) r (i) is nonnegative, so the reading\n\u03c1(t) := \u2211 j w(t)r (j)M (t)(j)\nis a convex combination of the stored memories. In general, the 2 in the power of the absolute distances can be replaced by any exponent \u03b1 > 0 to yield\nw(t)r\u03b1(i) := S(t)(i)\u2016k(t)r \u2212 a(t)(i)\u2016\u2212\u03b1\u2211 j S (t)(j)\u2016k(t)r \u2212 a(t)(j)\u2016\u2212\u03b1 .\nWe call this method of converting a read key to a set of weighting via a polynomial law InvNormalize, or InvNorm for short, in contrast with the use of exponential law in the case of SoftMax weight scheme, which computes the weights w (t) r (i) as\nw(t)r (i) := S(t)(i) exp(\u2212\u2016k(t)r \u2212 a(t)(i)\u20162/T (t))\u2211 j S (t)(j) exp(\u2212\u2016k(t)r \u2212 a(t)(j)\u20162/T (t))\nwhere T (t) is a temperature emitted by the controller at time t that represent the certainty of its reading. The higher T (t) is, the more w (t) r tends to the uniform distribution.\nGiven the ubiquity of SoftMax in the machine learning literature, one may consider it a natural choice for the weight scheme. But as will be seen in the experiments, InvNorm is crucial in making the Euclidean space work as an address space."}, {"heading": "3.2 Write", "text": "There is no extra ingredient to writing other than adding the produced memory vector m(t), its strength s(t), and its address k (t) w to the collection of memory vectors, strengths, and addresses.\nTo ensure that memory selection by weighted average works well, we squash the values of m(t) to [\u22121, 1] by tanh, but squashing by the logistic sigmoid function is also conceivable. Without such squashing, a memory vector M (t)(i) with large values can dominate the output of a weight method despite having low weight w (t) r (i)."}, {"heading": "3.3 Addressing procedure", "text": "Here we describe how the keys k (t) r and k (t) w are produced. The procedure is the same for both read and write keys, so we assume that we are to compute a single key k(t). We first describe the abstraction of the process over any fixed Lie group G acting smoothly on the key space Rn. In particular, the exact mechanism for producing an element of G is left unspecified at the moment. In the next few subsections we discuss the particular choices of G.\nThe controller emits 3 things: a candidate key k\u0303(t) \u2208 Rn, a mixing coefficient, or gate, g(t) \u2208 [0, 1], and an action v(t) \u2208 G that we also call step.\nThe gate g mixes the previous key k(t\u22121) with the candidate key to produce a pre-action key k\u0304(t), which is transformed by v(t) to produce the final key k(t)\nk\u0304(t) := g(t)k\u0303(t) + (1\u2212 g(t))k(t\u22121)\nk(t) := v(t) \u00b7 k\u0304(t)\nwhere \u00b7 denotes group action.\n3.3.1 Example: The scaling rotation group R\u2217 \u00d7 SO(2) The scaling rotation group R\u2217\u00d7SO(2) is the group of linear transformations of R2 that decomposes into a rotation followed by a dilation (or contraction).\nIn the specific case of G = R\u2217\u00d7SO(2), the controller would produce 2 numbers a = a(t), b = b(t), which represents the element\nv = ( a \u2212b b a ) of the group. The matrix acts on a key k = (x, y)T \u2208 R2 by left matrix multiplication\nv \u00b7 k = ( a \u2212b b a )( x y ) This is the same as scaling by the scalar c = \u221a \u2016a\u20162 + \u2016b\u20162 and then rotating (i.e. left multipli-\ncation) by the orthogonal matrix ( a/c \u2212b/c b/c a/c ) Another viewpoint is to treat (a, b) \u2208 R2 \u2212 {0} as the complex number a+ bi \u2208 C\u2212 {0}. Then one can view the action v \u00b7 k for k = (x, y)T \u2208 R2 as the complex multiplication (a+ bi)(x+ yi).\n3.3.2 Example: The rotation group SO(2)\nThe rotation, or special orthogonal, group SO(2) is as its name suggests, the group of all linear transformations of R2 expressable as a rotation.\nWhen G = SO(2), we can just modify the scheme from the last example by scaling (a, b) to unit norm, (a\u0304, b\u0304) = (a, b)/c. The rest will follow just the same."}, {"heading": "3.3.3 Example: The translation group R2 acting on R2", "text": "The case of the translation group is simpler. Again the controller outputs 2 numbers a = a(t), b = b(t), so that v = (a, b) acts upon a key k = (x, y) by\nv \u00b7 k = (x, y) + (a, b) = (x+ a, y + b)"}, {"heading": "3.4 Interpolation of Lie action", "text": "For groups like (Rn,+), there is a well-defined convex interpolation between two elements that stays in the group. For some others like R\u2217\u00d7SO(2), the straight-line interpolation tv+(1\u2212t)w for t \u2208 [0, 1], v, w \u2208 G sometimes produce elements outside the group (in this case sometimes the elements cancel out and get 0), but does so with probability zero in a suitable sense.\nThen, as for keys, we can let the controller output a candidate action v\u0303(t) \u2208 G and a mixing coefficient h(t) to smoothly mix with the previous action v(t\u22121) to produce a final action\nv(t) := h(t)v\u0303(t) + (1\u2212 h(t))v(t\u22121). This allows the controller to \u201cmove in a straight line within the group of actions\u201d by merely left saturating (i.e. squash to 0) the gates g(t) and h(t) for all t, so that v(1) = v(2) = v(3) = \u00b7 \u00b7 \u00b7 . Of course, the \u201cstraight line\u201d can be actually curved depending on the group. For example, when G = R\u2217 \u00d7 SO(2), a typical \u201cstraight line\u201d will be a spiral tending exponentially toward the origin or growing exponentially unbounded. In a more precise mathematical terminology, the keys of the read/write head will be on the orbit of the initial key by a Z-parametrized subgroup of G.\nEven if a group doesn\u2019t have a natural straight-line interpolation, there may be another way to mix two actions. In the case of G = SO(2) \u223c= S1, we can just project a straight-line interpolation onto the circle (barring a measure zero chance of intepolating into (0, 0) \u2208 R2).\nThe method of geodesics allow for interpolation between a wider class of Lie groups, like compact groups, but it may be too unwieldy and slow. In the end, any interpolation method should be dictated by performance first.\nThe final addressing mechanism is shown in figure 4. All together, the interaction of the controller with the external memory is shown in figure 5."}, {"heading": "4 Experiments", "text": "The baselines of our experiments are LSTMs in an encoder-decoder setup as described in [15]. We tested 2 variations of LANTM with an InvNorm and a SoftMax address mechanism, along with the LSTM baseline, on the permutation and arithmetic tasks to be described. The Lie group for both types of LANTM is the translation group R2 acting on R2 1. For both LANTMs and LSTM, we embed the input vocabulary continuously via a real embedding matrix into an Euclidean space before feeding into the models; we also pass the outputs through a softmax layer to arrive at probability distributions over the vocabulary set (this is the F box in figure 1). As usual, prediction is performed via argmax but training is done by minimizing negative log likelihood.\nThe machines are first fed a learnable initial state and then provided with the input sequence, flanked by a start-of-input (SOI) symbol \u3008s\u3009 and a repetition of an end-of-input (EOI) symbol \u3008/s\u3009. The machines are to output the correct sequence during the response phase, which starts when they receive the first \u3008/s\u3009. The repetition of \u3008/s\u3009 effectively ensures that the correct symbols are not shown to the machines during answering. The machine also must correctly emit an end-of-output\n1We early on experimented with the scaling rotation group R\u2217 \u00d7 SO(2), which produced acceptable results when input lengths were small but encountered numerical problems when input lengths were large due to exponentiating scale.\n(EOO) symbol \u3008e\u3009 to terminate their answers. Figure (6) is an example of inputs and correct outputs during a copy task.\nTasks. Each task has a length parameter k. The permutation tasks include\n1. copy\ninput: a1a2a3 \u00b7 \u00b7 \u00b7 ak output: a1a2a3 \u00b7 \u00b7 \u00b7 ak\n2. reverse\ninput: a1a2a3 \u00b7 \u00b7 \u00b7 ak output: akak\u22121ak\u22122 \u00b7 \u00b7 \u00b7 a1\n3. bigramFlip\ninput: a1a2a3a4 \u00b7 \u00b7 \u00b7 a2k\u22121a2k output: a2a1a4a3 \u00b7 \u00b7 \u00b7 a2ka2k\u22121\nThe arithmetic tasks include the following. Note that all numbers, input or output, are formatted with the least significant digits on the left and with zero padding.\n1. double. Let x be an integer in the range [0, 10k], with zero padding in front (on the right) to make up k digits.\ninput: x in base 10, zero padded to k digits\noutput: 2x in base 10, zero padded to k + 1 digits\n2. addition. Let x and y be integers in the range [0, 10k], with zero padding in front (on the right) to make up k digits. If they have digits x1x2 \u00b7 \u00b7 \u00b7xk and y1y2 \u00b7 \u00b7 \u00b7 yk, respectively, with the least significant digits on the left, then\ninput: x1y1x2y2 \u00b7 \u00b7 \u00b7xkyk output: x+ y in base 10, zero padded to k + 1 digits, with least significant digits on the left\nIn other words, we interleave the inputs. Thus this is a different encoding of the addition problem from previous works like [19] and [8].\nTask parameters and hyperparameters. We trained the models on the above tasks for input sizes summarized by table 1. For all tasks, the LANTM has a single-layer, 50-cell or 100-cell LSTM controller. The Lie group for all LANTMs is the translation group R2 acting on the key space R2, as mentioned above. The memory width (i.e. the size of each memory vector) is 20. For all tasks, the LSTM baseline has 4 layers, each with 256 cells. The exact setting of parameters for each model in each task is listed in table 2.\nTraining and testing. We seek to minimize the negative log likelihood of the individual output characters given the input\nNLL(W) := L\u2211 i=1 \u2212 log Pr[yi = y\u0303i|~x,W]\nwhere W is the network weights, ~x is input, L is the length of the correct output, yi is the ith output character, and y\u0303i is the corresponding correct answer. All models are trained through RMSProp with momentum .95. Every epoch has 10 batches, and every batch has 32 instances of the task. For the LANTM models, after 100 epochs, we half the learning rate if the best error so far is not improved in 30 epochs. The LSTMs are trained with learning rate 0.0002, with no learning rate adjustments during training.\nSince the training sets are large and separate from the test sets, we train until convergence, testing the models periodically \u2014 every 20 epochs for the LANTM models, and every 200 epochs for the LSTM baseline. After training is complete, the best test scores are tabulated.\nWe tested the models by drawing 100 batches of random problems and computing fine and coarse scores as in [6]. Fine score refers to the percentage of digit or characters (including the EOO marker) that the model correctly outputs. Coarse score refers to the percentage of total problems that the model answers completely correctly.\nTweaks to the LANTM model. We applied two tweaks to the LANTM model described in the previous section: 1) we initialized the mix coefficients for write address and action to strong negative values. This means that the LANTM would tend to write in a straight line. 2) We normalized the step sizes to approximately 1 but did not normalize the initial state step sizes. We found that these two tweaks improved convergence speed and consistency 2. Note that with the second tweak, the \u201cgroup\u201d of actions is no longer a group. This is akin to restricting the head shifts of an NTM to +1 and \u22121 [5]."}, {"heading": "5 Results", "text": "We trained the models as specified in the last section. The results are listed in table 3. LANTM-InvNorm was able to master all tasks and generalize nearly perfectly to 2x the training sizes. LANTM-SoftMax did as well on the copy and double tasks but utterly failed at all the others, having performed worse than the LSTM baseline. The baseline itself learned tasks with smaller training input sizes (bigramFlip, double, addition) almost flawlessly, but generalization to 2x training size was pathetic on all tasks, with coarse score not exceeding 6%.\nWe tested the learned InvNorm model on larger, arbitrarily selected input sizes. The results are summarized by table 4. On permutation tasks, it generalized quite well when challenged by 4 times the training size, able to get more than 90% of test problems correct. On the double task,\n2A gif of the read and writes of a LANTM-InvNorm learning the copy task with no biases (tweak 1) is available at https://gfycat.com/MedicalKeyGalapagostortoise. Compare with the corresponding gif with biases at https: //gfycat.com/WigglyUnlawfulCaudata. Details of the gifs can be found in appendix A.\nits extrapolation performance was similar, with 86% coarse score on 4x training size. Notice that LANTM-InvNorm on several of the tasks (8x bigramFlip, 8x double, 4x addition) achieved high fine scores when extrapolating to large input sizes despite having low coarse scores. This suggests that the extrapolation errors systematically occur at the end of each output on those tasks.\nWe have created gifs of the read and write locations of LANTM-InvNorm and LANTM-SoftMax while learning each of the 5 tasks, tracking their progress over time. They are available at https: //gfycat.com/geyang/lie_access_neural_turing_machine, with details explained in appendix A. One sees clearly that LANTM-InvNorm learned to write in a straight line (which is not surprising given our tweaks to the model) and then read along that same line. On the other hand, LANTMSoftMax tended to quarantine its read locations to one end of the write line in the reverse, bigramFlip, and addition tasks. In the copy and double tasks, the read line doesn\u2019t stick to the write line as closely with LANTM-Softmax as with LANTM-InvNorm. This is expected since SoftMax assigns a memory vector with high value just if its location a is closer to the read location k than any other memory vector, whereas InvNorm requires k to be very close to a.\nIn appendix B, we look at the behaviors of trained LANTM-InvNorm through their read and write locations, gate values, and example input/output to analyze what exactly did the they learn and where their extrapolation errors come from when challenged by extreme input lengths."}, {"heading": "6 Related Works and Comparison of Results", "text": "Many recent works have been devoted to the research of external memories and learning algorithmic tasks. We briefly summarize some of the most relevant below, and collected results of experiments similar to our copy and addition tasks in table 5.\nLSTM-based models. Zaremba et al. [19] taught LSTM to evaluate simple python programs via curriculum learning, which included copy and addition tasks. They reported 99% accuracy in the addition of 9-digit integers but trouble with copying long sequences. Their version of the tasks had Python programs as inputs, like \u201cprint((125+203))\u201d for addition (thus differing from our interleaved inputs in the addition task) and \u201cprint(2053)\u201d for copy (thus with much smaller vocabulary \u2014 10 \u2014 than in our copy task \u2014 128). They also relied on teacher forcing, that is, \u201ceven if the LSTM made an incorrect prediction in the ith output digit, the LSTM will be provided as input the correct ith output digit for predicting the (i + 1)th digit\u201d [19]. In contrast, in our experiments, we made no indication of the correct answer during the response phase. Nevertheless, LANTM-InvNorm was able to not only learn 16-digit addition but also correctly answer 32-digit problems 99% of the time, and it had no difficulty with copying long sequences.\nIn Grid Long Short-Term Memory [9], Kalchbrenner et al. arranged LSTM cells in a multidimensional grid which can take as input higher dimensional data such as images. They trained the machine in 15-digit addition (with inputs separated, unlike in this paper) and length 20 copy tasks. Their best model achieved > 99% accuracy only with 18 layers of 400 cells each and .55 million\nsamples on the addition task, while, if we assume for the moment that the experimental conditions are close enough to allow meaningful comparison, our LANTM-InvNorm learned 16-digit addition with about .16 million samples and orders of magnitude less parameters, and generalized to 32 digitaddition with 99% accuracy. Similarly, their best model reached 100% accuracy in the copy task with many more parameters and samples than our LANTM-InvNorm used to do the same, despite their machine only having to copy length 20 sequences compared to (up to) 64 in our version.\nNTM-based models. As mentioned in the introduction, Graves et al. [5] constructed the Neural Turing Machine by tying an LSTM or feedforward controller to a fixed size memory array via read and write heads. These heads are responsible for taking and carrying out read and write instructions from the controller. An important feature is that the model is differentiable end-to-end, allowing it to be trained via gradient-descent methods. Our LANTM model adopts the core ideas of NTM, but with the following important differences: 1) NTM\u2019s hardwired content addressing system is replaced by LANTM\u2019s key system that gives each memory vector a key, or address, in an Euclidean space Rn. 2) NTM moves its head location, which is a probability vector over the memory slots, via convolution with another probability vector, but LANTM does so by emitting a Lie action that acts upon its head location, which is a coordinate in Rn. In the implementation of this paper, this only involves computing a translation 2-vector and adding it to the 2-vector head location, which can be done much more quickly than a convolution of two size-128 vectors. 3) There is no bound on the number of memory vectors LANTM can have. 4) There is no erasure function in LANTM.\nGraves et al. tested NTM on a copy task very similar to ours, with no input during the response phase, but all input and output are in bit strings. They used a memory array of 128 memory vectors each of dimension 20 in order to learn length 20 copy task, with good but imperfect generalization to length 30, 50, and 120 3. LANTM used half the memory size (about 64\u00d7 20, as the input is only about length 64, and a new memory vector is created for each input character) to learn length 64 copy task, generalizing to length 128 and 256 perfectly, and even with 91% coarse accuracy and 100% fine accuracy on length 320. One caveat here is that each symbol in NTM\u2019s copy task is an 8-bit\n3[5] gave no statistics on the actual performance comparable to the fine and coarse scores here, but here we assume that sample input/output of figure 4 of that paper is illustrative of NTM\u2019s performance on the copy task.\nstring, so its effective vocabulary size is 256, compared to 128 atomic symbols in our experiment. In Learning to Transduce with Unbounded Memory, Grefenstette et al. [6] designed smooth versions of stack, queue, and deque as external memories to an LSTM controller, and these models were respectively named the Neural Stack, Queue, and DeQue. The models have the distinguishing property of having constant time access to their external memories, which were also unbounded, like LANTM. The models were trained on the copy, reverse, and bigramFlip tasks, and we adopted their setup in this paper for the most part; the other tasks in that paper were two transduction tasks with synthetic grammars. The Neural DeQue was able to learn and generalize all tasks except bigramFlip. In contrast, LANTM-InvNorm excelled at all three permutation tasks with a much smaller model size, with the caveat that our bigramFlip task was half the size of that in Grefenstette et al.\nIn Reinforcement Learning Neural Turing Machines, Zaremba et al. [20] tweaked the NTM model so that it does not need to involve the entire memory array every time memory is accessed. This was done via an intricate application of reinforcement learning, and the model is named RL-NTM. They had RL-NTM learn simple sequence tasks like reverse and repeatCopy with modest success.\nOthers. Weston et al. [18] wrote about ideas similar to those in Neural Turing Machines in the paper Memory Networks, released around the same time. They gave a general construction of a machine with external memory and implemented a specific version, testing it on QA tasks. Sukhbaatar et al. [14] improved upon the paper to give a memory network trainable via gradient descent end-to-end, and obtained good results in language modelling along with weaker results in QA. Kumar et al. [11] added a episodic memory module on top of a memory network to be able to recursively focus on parts of specific facts relevant to the question at hand."}, {"heading": "7 Discussion", "text": "Two things about our experimental results are unusual in view of the current trend in neural network research. One is that the go-to method for assigning weights \u2014 the softmax layer \u2014 could not yield effective learning of tasks other than the relatively simple copy and double tasks. The other is that our key space, R2, was surprisingly effective, when one perhaps would at first picked a relatively large dimension.\nWe are uncertain what factors contribute to these phenomena, but nevertheless here mention some possible starting points to look into.\nInvNorm performs better. Recall that InvNorm has the following formula\nw(t)r (i) := S(t)(i)\u2016k(t)r \u2212 a(t)(i)\u2016\u22122\u2211 j S (t)(j)\u2016k(t)r \u2212 a(t)(j)\u2016\u22122\nwhere a(t)(i) is the address of and S(t)(i) is the memory strength of the memory vector M (t)(i). Notice w (t) r (i) \u2192 1 for some i only when k(t)r \u2192 a(t)(i). Thus reading a memory with high fidelity means that the read location must be very close to the memory location. But when k (t) r is far from all points a(t)(i), w (t) r (i) tends to the uniform distribution. Consider a simplified memory retrieval setting where a controller tries to emit an address k such that the resultant memory vector \u03c1 produced by the InvNorm scheme minimizes the squared error \u2016\u03c1\u2212M (t)(i)\u20162 for some i. If memory vectors are linearly independent, then k = a(t)(i) is the unique global error minimizer, with error 0. This means that, at least for k in a neighborhood of a(t)(i), the gradient of k (under regularity conditions) points more or less toward a(t)(i), and the controller will eventually, under gradient descent, hone k in toward a(t)(i).\nOn the other hand, SoftMax has the formula\nw(t)r (i) := S(t)(i) exp(\u2212\u2016k(t)r \u2212 a(t)(i)\u20162/T (t))\u2211 j S (t)(j) exp(\u2212\u2016k(t)r \u2212 a(t)(j)\u20162/T (t))\nso w (t) r (i)\u2192 1 for some i only when \u2016k(t)r \u2212a(t)(i)\u2016 is much smaller than \u2016k(t)r \u2212a(t)(j)\u2016 for all j 6= i. Naturally then, there are decision regions Ri \u2208 Rn such that k(t)r \u2208 Ri implies argmaxjw(t)r (j) = i. Reading a memory with high fidelity with SoftMax only requires that the read location be in the interior of some decision region, far enough from the decision boundaries, and the temperature T (t) is sufficiently low. In the memory retrieval setting described above (modified so that the controller now emits also a temperature T for use in the SoftMax scheme), there is no longer a global minimum (k, T ) minimizing the squared error; any such \u201cminimum\u201d would take T to infinity, while k just needs to be within the appropriate decision region for M (t)(i). Within the wider setting of LANTM learning via BPTT, this inability to precisely hone in on a(t)(i) can negatively impact the precision of Lie access.\nFor example, suppose we have memory vectors M(j) stored at a(j) in a straight line at regular intervals, and the task is to output M(1),M(2), . . . ,M(m) in order. Under gradient descent, LANTM-InvNorm is likely to learn the addresses of two consecutive memories, from which it learns the Lie action step that connect every pair of consecutive memories (here we are appealing more to intuition than rigor). This allows the machine to quickly learn to traverse the entire memory array. But LANTM-SoftMax cannot follow the same strategy as it has less propensity to learn the exact addresses, and therefore less propensity to learn the Lie action step between consecutive memory addresses. In short, while InvNorm makes it harder to retrieve individual memories by requiring the emitted keys to be exact, this same requirement makes it easier to go from one memory location to the next.\nPlausibly, this difference between InvNorm and SoftMax is magnified when the task at hand becomes more complicated, causing the perceived differences in experimental results in tasks like addition.\nR2 works well. We cannot quite explain why R2 as key space has induced good performance, but we can take an educated guess as to the difference between high and low dimensions in regards to InvNorm.\nUnder InvNorm, a high dimension n (possibly) makes memory retrieval of any particular memory vector M (t)(i) more difficult. Indeed, to read M (t)(i) with high fidelity, the read key must be very close to a(t)(i) for all n of its coordinates, which is certainly easier for small n than for large n.\nWhatever the reason, the fact that R2 serves as a good key space has allowed good visualization of the memory operations in a LANTM, not only over the course of its execution but also over its evolution through training (see appendix A). This opens up the possibility of monitoring training progress through real time (per epoch) visualization of a LANTM\u2019s reads and writes, which can give a better indication of training set performance than the error suggests. Indeed, the author has implemented such a system, and the gifs in appendix A are essentially the real time visualizations strung together.\nBased on our experimental results, LANTM has potential for application to episodic tasks like machine translation of sentences or question answering. However, the fact that the memory is unbounded limits its applicability in continuous tasks like modeling large corpus of text. In the future, we hope to develop a version of bounded memory for such applications. Possible ideas include discounting the memory strength exponentially through time according to disuse and removing all memories with strength less than a threshold, or fixing the memory locations beforehand and use InvNorm to compute memory weights, similar to how NTM uses cosine-distance and SoftMax to perform content addressing.\nNevertheless, it is important to note that in episodic tasks, LANTM\u2019s memory usage may be no larger, or even smaller, than comparable fixed memory systems. For example, the NTM of [5] used a memory array of 128 memory vectors each of dimension 20 in order to learn length 20 copy task, but LANTM used only half the memory size (about 64 \u00d7 20, as the input is only about length 64, and a new memory vector is created for each input character) to learn length 64 copy task.\nWhile LANTM has succeeded in all of the tasks here, what is perhaps dissatisfying is that its strategy remains more or less the same: writing and reading in a straight line. This begs question,\ncan LANTM learn tasks for which this strategy is no longer adequate? If not, what modification can be made toward this end? These are topics for further investigation, and we already have some nascent ideas on expanding and tweaking the LANTM model: 1) adding a \u201ccontent\u201d addressing system, for example, as in NTM; 2) researching an efficient and effective erasure mechanism; 3) trying other Lie groups, such as other subgroups of the Mobius transformation group, and seeing if another smooth manifold works better than Rn as the key space."}, {"heading": "8 Conclusion", "text": "Our experiments showed that, in the 5 tasks selected, a small LANTM-InvNorm was able to learn and generalize almost perfectly, while the large LSTM baseline failed in many of them. In addition, InvNorm was clearly better than SoftMax as a weight scheme, as LANTM-SoftMax could not learn or generalize tasks other than copy and double.\nOur results suggest that a smooth key space, with no fixed bound on the number of memories, dramatically increases the learnability and generalizability of problems involving long term correlations \u2014 but only with the right weight scheme."}, {"heading": "A Gifs", "text": "For each task and each of LANTM-InvNorm and LANTM-SoftMax, we created a gif of sample read and writes over the course of learning; the entire album is available at https://gfycat.com/ geyang/lie_access_neural_turing_machine. Each gif was created as follows:\n1. At the end of each epoch, we randomly selected an input of the maximium training length specific to that task (for example, in the case of addition task, two 16-digit numbers interleaved).\n2. We ran the model, with all weights set as trained so far, on this input and record the read and write locations in the key space, along with the strength of each memory vector.\n3. When training is complete, we plot the recording of each epoch in a separate frame, and string them together into a gif file. The write locations are marked by red circles, and filled so that a darker fill color means higher memory strength. The read locations are marked by blue disks and connected together by a blue line chronologically (the read line).\nEven though we did not explicitly indicate the directionality of the read line, one may infer the directionality of the write sequence by noting that a red circle with white filling marks the beginning of the writes. Then the read sequence will follow this directionality in all tasks other than the reverse task."}, {"heading": "B Close analysis", "text": "In this section, we discuss the performance of LATNM-InvNorm through various statistics and example input/outputs.\nB.1 Permutation tasks\nB.1.1 copy\nFigure 7a shows the read and write locations of such a LANTM-InvNorm, trained on length 1 to 64 input, running on a typical length 320 input. As one might expect, the reads and writes proceed along straight lines in the key space. The actual read locations keep close to the corresponding write locations. In this execution, the LANTM made no errors (figure 7c).\nFigure 7b shows the values of the 4 gates governing the computation of read and write keys. A value of 0 means the gate takes the previous step or key location, while a value of 1 means the gate takes the newly computed step or key location. While the write location gates during the input phase and the read location gates during the response phase were as expected pushed to 0, the write step and read step gates were unexpectedly pushed to 1. Thus the LANTM must have memorized a fixed step size and used it for both reads and writes.\nB.1.2 reverse\nThe counterparts of these graphs for the reverse task are exhibited in figure 8. On the left we have data for length 128 input, demonstrating a correct execution, while on the right we have data for length 300 input, demonstrating what goes on when extrapolating to higher input sizes.\nWe see that LANTM trained on the reverse task functions much like that trained on the copy task, with read and write heads traversing on straight lines, except now the directionalities are opposed. However, when running on length 300 input, the read line, i.e. the curve connecting the read locations in sequence, bends suddenly toward the end, causing almost all reads at the end to diverge from the writes and making almost all outputs at the end to be incorrect. This is somewhat surprising, for one might have expected error to come in the form of the accumulation of a small difference between the slopes of the read and write lines. Along with the sudden dip in read step gate value at the end (blue line in figure 8d), the bending of the read line suggests that the LSTM controller started to forget its learned program as the answering phase drew toward a conclusion.\nB.1.3 bigramFlip\nThe same phenomena appear with the bigramFlip task, where reads and writes happen along 2 closely aligned lines, but when tested by a long input, the reads will abruptly fall out of order: while in the reverse task, the read line visibly bends away from the write line, here the lines stay straight but each step in the read line is elongated, starting around the 187th read (figure 9b).\nOne might be surprised to see that the read happens along a line instead of zigzagging inside the write line. On closer inspection, we find that LANTM works as follows:\n1. LANTM stores representations of the inputs in input order.\n2. Meanwhile it memorizes the first two input characters and outputs them in the reverse order after reading the first two EOI symbols.\n3. When it sees the first EOI symbols, it starts reading the second bigram, i.e. it reads characters 3 and 4 (or their representations in memory; this corresponds to the 5th and 6th memory vectors) after seeing the first and second EOI symbols. This effectively allows it to \u201clook ahead\u201d and have each bigram on hand before having to output the flipped image of it.\n4. The LSTM flips each \u201clook ahead\u201d bigram and outputs it in order. Repeat for each bigram.\nUnique to the LANTM trained on bigramFlip is the oscillation of the read step gate between 0 and 1 (figure 9c and 9d). This seems like more an artifact of the learning process than a feature of the learned computation, as it would also imply that the controller memorized a single fixed read step, and that the error that occurs with extrapolation seems to stem from the adulteration of this memory.\nB.2 Arithmetic tasks\nIn the double task, the LANTM behaved much like it did in the copy task. It stored the input in a line and then computed the doubling with carry digitwise.\nIn the addition task, the LANTM learned to compress each pair of digits of the input numbers (which, as mentioned above, are interleaved) and store them in the odd write locations; the even write locations had vanishing memory strength (figure 11a and 11b). The LANTM then read off the information by skipping through the odd memory locations.\nAs with copy and reverse tasks, the read step gate values during the response phase were all close to 1, meaning that the LANTM kept the read step in the LSTM controller memory. This suggests that the read step gate might be an unnecessary design."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Describing Multimedia Content using Attention-based Encoder\u2013Decoder Networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "[cs],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder- Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "[cs],", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Neural Turing Machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "Jrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Grid Long Short-Term Memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "[cs],", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "[cs],", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Introduction to Smooth Manifolds", "author": ["John Lee"], "venue": "Graduate Texts in Mathematics 218. Springer,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille"], "venue": "[cs],", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "[cs],", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "[cs],", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Memory Networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning to Execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 84, "endOffset": 94}, {"referenceID": 2, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 84, "endOffset": 94}, {"referenceID": 0, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 84, "endOffset": 94}, {"referenceID": 3, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 115, "endOffset": 121}, {"referenceID": 1, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 115, "endOffset": 121}, {"referenceID": 12, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 140, "endOffset": 152}, {"referenceID": 9, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 140, "endOffset": 152}, {"referenceID": 15, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 140, "endOffset": 152}, {"referenceID": 4, "context": "[5] and memory networks of Sukhbaatar et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] achieved breakthroughs toward this goal by utilizing external memories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] introduced neural stacks, queues, and deques.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": ") The reader hoping to learn the basics of smooth manifolds and Lie groups can consult John Lee\u2019s excellent Introduction to Smooth Manifolds [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "\u03c4 is usually differentiable everywhere or almost everywhere, so that the RNN can be trained by backpropagation through time (BPTT) [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "In this work, we use a particular variant of RNN called the Long Short Term Memory (LSTM) [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "In models with external memories, LSTM often serves as the controller [5, 6, 20].", "startOffset": 70, "endOffset": 80}, {"referenceID": 5, "context": "In models with external memories, LSTM often serves as the controller [5, 6, 20].", "startOffset": 70, "endOffset": 80}, {"referenceID": 0, "context": "and a new address k (t) w \u2208 R, along with a scalar s \u2208 [0, 1], called the memory strength of the vector.", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "The controller emits 3 things: a candidate key k\u0303 \u2208 R, a mixing coefficient, or gate, g \u2208 [0, 1], and an action v \u2208 G that we also call step.", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": "For some others like R\u2217\u00d7SO(2), the straight-line interpolation tv+(1\u2212t)w for t \u2208 [0, 1], v, w \u2208 G sometimes produce elements outside the group (in this case sometimes the elements cancel out and get 0), but does so with probability zero in a suitable sense.", "startOffset": 81, "endOffset": 87}, {"referenceID": 14, "context": "The baselines of our experiments are LSTMs in an encoder-decoder setup as described in [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Let x be an integer in the range [0, 10], with zero padding in front (on the right) to make up k digits.", "startOffset": 33, "endOffset": 40}, {"referenceID": 9, "context": "Let x and y be integers in the range [0, 10], with zero padding in front (on the right) to make up k digits.", "startOffset": 37, "endOffset": 44}, {"referenceID": 18, "context": "Thus this is a different encoding of the addition problem from previous works like [19] and [8].", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "Thus this is a different encoding of the addition problem from previous works like [19] and [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "We tested the models by drawing 100 batches of random problems and computing fine and coarse scores as in [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "This is akin to restricting the head shifts of an NTM to +1 and \u22121 [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 18, "context": "[19] taught LSTM to evaluate simple python programs via curriculum learning, which included copy and addition tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "They also relied on teacher forcing, that is, \u201ceven if the LSTM made an incorrect prediction in the ith output digit, the LSTM will be provided as input the correct ith output digit for predicting the (i + 1)th digit\u201d [19].", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "In Grid Long Short-Term Memory [9], Kalchbrenner et al.", "startOffset": 31, "endOffset": 34}, {"referenceID": 18, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "The asterisk (*) indicates that [5] gave no statistics on the actual performance comparable to the fine and coarse scores here, but here we assume that sample input/output of figure 4 of that paper is illustrative of NTM\u2019s performance on the copy task.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "[5] constructed the Neural Turing Machine by tying an LSTM or feedforward controller to a fixed size memory array via read and write heads.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "One caveat here is that each symbol in NTM\u2019s copy task is an 8-bit 3[5] gave no statistics on the actual performance comparable to the fine and coarse scores here, but here we assume that sample input/output of figure 4 of that paper is illustrative of NTM\u2019s performance on the copy task.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "[6] designed smooth versions of stack, queue, and deque as external memories to an LSTM controller, and these models were respectively named the Neural Stack, Queue, and DeQue.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] wrote about ideas similar to those in Neural Turing Machines in the paper Memory Networks, released around the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] improved upon the paper to give a memory network trainable via gradient descent end-to-end, and obtained good results in language modelling along with weaker results in QA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] added a episodic memory module on top of a memory network to be able to recursively focus on parts of specific facts relevant to the question at hand.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "For example, the NTM of [5] used a memory array of 128 memory vectors each of dimension 20 in order to learn length 20 copy task, but LANTM used only half the memory size (about 64 \u00d7 20, as the input is only about length 64, and a new memory vector is created for each input character) to learn length 64 copy task.", "startOffset": 24, "endOffset": 27}], "year": 2016, "abstractText": "Recently, Neural Turing Machine and Memory Networks have shown that adding an external memory can greatly ameliorate a traditional recurrent neural network\u2019s tendency to forget after a long period of time. Here we present a new design of an external memory, wherein memories are stored in an Euclidean key space R. An LSTM controller performs read and write via specialized structures called read and write heads, following the design of Neural Turing Machine. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \u201cL\u201d and \u201cR\u201d instructions of a traditional Turing Machine is generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. As LANTM is differentiable end-to-end, training was done with RMSProp. We found the right configuration of LANTM to be capable of learning different permutation and arithmetic tasks and extrapolating to at least twice the input size, all with the number of parameters 2 orders of magnitude below that for the LSTM baseline. In particular, we trained LANTM on addition of k-digit numbers for 2 \u2264 k \u2264 16, but it was able to generalize almost perfectly to 17 \u2264 k \u2264 32.", "creator": "LaTeX with hyperref package"}}}