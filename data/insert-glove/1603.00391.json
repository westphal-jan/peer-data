{"id": "1603.00391", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Noisy Activation Functions", "abstract": "639-2 Common 10-piece activation tounsi functions sainte-chapelle used five-way in lafraie neural 7,000 networks can yield imataca to lohberg training nsg difficulties blavatsky due to the saturation behavior jumpoff of 500-a the activation congke function, which may 1,195 hide donets dependencies which cordovan are mazzante not visible to pasturage first order (using \u00e1rp\u00e1d only gradients ). jui Gating mechanisms hongze that use softly despising saturating activation tat functions to cocktail emulate kumar the discrete switching slurps of digital logic 78 circuits tempel are v\u00edtor good examples of this. We propose to afikoman exploit suffix the 86.80 injection of appropriate stebnar noise naughton so that montolivo some misbehave gradients may kayseri sometimes trakl flow, bronce even swathed if the gamay noiseless application breuker of yas the activation function would yield mumbled zero gaumont gradient. Large sayeda noise aliu will dominate proponents the k\u014dichi noise - free scenes gradient ventricosa and tjuta allow 21,500 stochastic gradient deisher descent yelda to alkalic be more exploratory. g\u00e9ologie By adding noise only crinkles to apollonio the exacerbated problematic parts of the brassard activation function spender we lamela allow the shikapwasha optimization procedure to gengler explore the l'equipe boundary between the degenerate (finely saturating) sayf and the well - retch behaved parts llb of tage the darach activation 43,125 function. We mcfeely also 84.65 establish nicety connections marjana to mitoizumi simulated elkies annealing, chute when the audry amount of ascorbic noise is annealed down, making nechai it tappet easier fingerless to multiday optimize phil hard 89.58 objective functions. harcourt We find zygote experimentally that replacing unicom such saturating activation molander functions contortionists by by noisy vagrant variants yammering helps streamer training palaiologan in cars.com many wilmerhale contexts, landsgemeinde yielding state - of - knife-edge the - art hunchback results frogmen on recertification several datasets, velimirovi\u0107 especially when ioane training seems albiol to be the most sovereignity difficult, 94-92 e. \u017ei\u017ekov g. , peddling when curriculum learning madnodje is necessary 0.568 to obtain 26.84 good m\u00e1xima results.", "histories": [["v1", "Tue, 1 Mar 2016 18:30:15 GMT  (784kb,D)", "http://arxiv.org/abs/1603.00391v1", null], ["v2", "Sun, 6 Mar 2016 20:51:57 GMT  (784kb,D)", "http://arxiv.org/abs/1603.00391v2", null], ["v3", "Sun, 3 Apr 2016 21:41:47 GMT  (788kb,D)", "http://arxiv.org/abs/1603.00391v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["\u00e7aglar g\u00fcl\u00e7ehre", "marcin moczulski", "misha denil", "yoshua bengio"], "accepted": true, "id": "1603.00391"}, "pdf": {"name": "1603.00391.pdf", "metadata": {"source": "META", "title": "Noisy Activation Functions", "authors": ["Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio"], "emails": ["GULCEHRC@IRO.UMONTREAL.CA", "MARCIN.MOCZULSKI@STCATZ.OX.AC.UK", "MISHA.DENIL@GMAIL.COM", "BENGIOY@IRO.UMONTREAL.CA"], "sections": [{"heading": "1. Introduction", "text": "The introduction of the piecewise-linear activation functions such as ReLU and Maxout (Goodfellow et al., 2013) units had a profound effect on deep learning, and was a major catalyst in allowing the training of much deeper networks. It is thanks to ReLU that for the first time a few\nyears ago it was shown (Glorot et al., 2011) that very deep purely supervised networks can be trained, whereas using tanh nonlinearity only allowed to train shallow networks. A plausible hypothesis about the recent surge of interest on these piecewise-linear activation functions (Glorot et al., 2011), is due to the fact that they are easier to optimize with SGD and backpropagation than smooth activation functions, such as sigmoid and tanh. The recent successes of piecewise linear functions is particularly evident in computer vision, where the ReLU has become the default choice in convolutional networks.\nIn this paper, we propose a new technique to train networks with activation functions which strongly saturate when their input is large. This is achieved by adding noise to the activation function in its saturated regime. Using this approach, we have found that it was possible to train neural networks with much wider family of activation functions. Adding noise to the activation function has been considered for ReLU units and was explored in (Bengio et al., 2013; Nair & Hinton, 2010) for feed-forward networks and Boltzmann machines to encourage units to explore more and make the optimization easier.\nMore recently there has been a resurgence of interest in more elaborate \u201cgated\u201d architectures such as LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014), but also encompassing neural attention mechanisms that have been used in the NTM (Graves et al., 2014), Memory Networks (Weston et al., 2014), automatic image captioning (Xu et al., 2015) and wide areas of applications (LeCun et al., 2015). A common thread running through these works is the use of soft-saturating non-linearities, such as the sigmoid or softmax, to emulate the hard decisions of digital logic circuits. In spite of its success, there are two key problems with this approach.\n1. Since the non-linearities still saturate there are problems with vanishing gradient information flowing\nar X\niv :1\n60 3.\n00 39\n1v 1\n[ cs\n.L G\n] 1\nM ar\nthrough the gates; and\n2. since the non-linearities only softly saturate they do not allow one to take hard decisions.\nAlthough gates often operate in the soft-saturated regime (Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al., 2015) the architecture prevents them from being fully open or closed. In this paper we take a novel approach to addressing both of these problems. Our method addresses the second problem through the use of hard-saturating nonlinearities, which allow gates to make perfectly on or off decisions when they saturate. Since the gates are able to be completely open or closed, no information is lost through the leakiness of the soft-gating architecture.\nBy introducing hard-saturating nonlinearities we have exacerbated the problem of gradient flow, since gradients in the saturated regime are now precisely zero instead of merely very small. However, by introducing noise into the activation function that grows based on the depth of saturation we encourage random exploration when the units saturate, and normal behavior when they do not.\nAt test time the noise in the activation functions can be removed, and as our experiments show, the resulting deterministic networks clearly outperform their soft-saturating counterparts on a wide variety of tasks, and allow to reach state-of-the-art performance by simple drop-in replacement of the nonlinearities in existing training code.\nThe technique that we propose in this paper, addresses the difficulty of optimization and having hard-activations at the test time for gating units and we propose a way of performing simulated annealing for neural networks."}, {"heading": "2. Saturating Activation Functions", "text": "Definition 2.1. (Activation Function). An activation function is a function h : R \u2192 R that is differentiable almost everywhere.\nDefinition 2.2. (Saturation). An activation function h(x) with derivative h\u2032(x) is said to right (resp. left) saturate if its limit as x\u2192\u221e (resp. x\u2192 \u2212\u221e) is zero. An activation function is said to saturate (without qualification) if it both left and right saturates.\nMost common activation functions used in recurrent networks (for example, tanh and sigmoid) are saturating. In particular they are soft saturating, meaning that they achieve saturation only in the limit.\nDefinition 2.3. Hard and Soft Saturation. Let c be a constant such that x > c implies h\u2032(x) = 0 and left hard saturates of x < c implies h\u2032(x) = 0, \u2200x. We say that h(\u00b7) hard saturates (without qualification) if it both left and right hard\nsaturates. An activation function that saturates but achieves zero gradient only in the limit is said to soft saturate.\nWe can construct hard saturating versions of soft saturating activation functions by taking a first-order Taylor expansion about zero and clipping the results to an appropriate range.\nFor example, expanding tanh and sigmoid around 0, with x \u2248 0, we obtain linearized functions ut and us of tanh and sigmoid respectively:\nsigmoid(x) \u2248 us(x) = 0.25x+ 0.5 (1) tanh(x) \u2248 ut(x) = x. (2)\nClipping the linear approximations result to,\nhard-sigmoid(x) = max(min(us(x), 1), 0) (3)\nhard-tanh(x) = max(min(ut(x), 1), \u2212 1). (4)\nThe motivation behind this construction is to introduce linear behavior around zero to allow gradients to flow easily when the unit is not saturated, while providing a crisp decision in the saturated regime.\nFor the rest of the document we will refer use h(x) to refer to a generic activation function and use u(x) to denote its linearization based on the first-order Taylor expansion about zero. hard-sigmoid saturates when x 6 \u22122 or x > 2 and hard-tanh saturates when x 6 \u22121 or x > 1. We denote xt the threshold, in absolute value, xt = 2 for hard-sigmoid and xt = 1 for the hard-tanh.\nThe ability of the hard-sigmoid and hard-tanh to make crisp decisions comes at the cost of exactly 0 gradients in the saturated regime. This can cause difficulties during training: a small but not infinitesimal change of the pre-activation (before the nonlinearity) may help to reduce the objective function, but this will not be reflected in the gradient.\nRemark. Let us note that both hard-sigmoid(x), sigmoid(x) and tanh(x) are all contractive mapping. hard-tanh(x), becomes a contractive mapping only when its input is greater than the threshold. An important difference among these activation functions is their fixed points. hard-sigmoid(x) has a fixed point at x = 32 . However the fixed-point of sigmoid is x = 0. Any x \u2208 R between \u22121 and 1 can be the fixed-point of hard-tanh(x), but the fixed-point of tanh(x) is 0. tanh(x) and sigmoid(x) have point attractors at their fixed-points. Those mathematical differences among the saturating activation functions can make them behave very differently with RNNs and deep networks.\nThe highly non-smooth gradient descent trajectory may bring the parameters in a state that pushes the activations of a given unit inside the 0 gradient regime for a particular example, from where it becomes difficult to escape.\nWhen units saturate and gradients vanish, an algorithm may require many training examples and a lot of computation to recover."}, {"heading": "3. Annealing with Noisy Activation Functions", "text": "Consider a noisy activation function \u03c6(x, \u03be) in which we have injected iid noise \u03be, to replace a saturating nonlinearity such as the hard-sigmoid and hard-tanh introduced in the previous section. In the next section we describe the proposed noisy activation function which has been used for our experiments, but here we want to consider a larger family of such noisy activation functions, when we use some variant of stochastic gradient descent (SGD) for training.\nLet \u03be have variance \u03c32 and mean 0. We want to characterize what happens as we gradually anneal the noise, going from large noise levels (\u03c3 \u2192\u221e) to no noise at all (\u03c3 \u2192 0).\nFurthermore, we will assume that \u03c6 is such that when the noise level becomes large, so does its derivative with respect to x:\nlim |\u03be|\u2192\u221e |\u2202\u03c6(x, \u03be) \u2202x | \u2192 \u221e. (5)\nIn the 0 noise limit, we recover a deterministic nonlinearity, \u03c6(x, 0), which in our experiments is piecewise linear and allows us to capture the kind of complex function we want to learn. As illustrated in Figure 2, in the large noise limit, very large gradients are obtained because backpropagating through \u03c6 gives rise to very large derivatives. Hence, the noise drowns the signal: the example-wise gradient on parameters is much larger than it would have been with \u03c3 = 0. SGD therefore just sees noise and can move around anywhere in parameter space without \u201cseeing\u201d any trend.\nAnnealing is also related to the signal to noise ratio where SNR can be defined as the ratio of the variance of noise\n\u03c3signal and \u03c3noise, SNR = \u03c3signal \u03c3noise . If SNR \u2192 0, the model will do pure random exploration. As we anneal SNR will increase, and when \u03c3noise converges to 0, the only source of exploration during the training will come from the noise of Monte Carlo estimates of stochastic gradients.\nThis is precisely what we need for methods such as simulated annealing (Kirkpatrick et al., 1983) and continuation methods (Allgower & Georg, 1980) to be helpful, in the context of the optimization of difficult non-convex objectives. With high noise, SGD is free to explore all parts of space. As the noise level is decreased, it will prefer some regions where the signal is strong enough to be \u201cvisible\u201d by SGD: given a finite number of SGD steps, the noise is not averaged out, and the variance continues to dominate. Then, as the noise level is reduced, SGD spends more time in \u201cglobally better\u201d regions of parameter space. As it approaches zero, we are fine-tuning the solution and converging near a minimum of the noise-free objective function. A related approach of adding noise to gradients and annealing the noise was investigated in (Neelakantan et al., 2015) as well."}, {"heading": "4. Adding Noise Where the Unit Would Saturate", "text": "A novel idea behind the proposed noisy activation is that the noise added to the nonlinearity is proportional to the magnitude of saturation of the nonlinearity. For hard-sigmoid(x) and hard-tanh(x), due to our parametrization of the noise, that translates into the fact that the noise is only added when the hard-nonlinearity saturates. This is different from previous proposals such as the noisy rectifier from Bengio (2013) where noise is added just before a rectifier (ReLU) unit, independently of whether the input is in the linear regime or in the saturating regime of the nonlinearity.\nThe motivation is to keep the training signal clean when the\nunit is in the non-saturating (typically linear) regime and provide some noisy signal when the unit is in the saturating regime.\nIn this paper, h(x) will refer to hard saturation activation function such as the hard-sigmoid and hard-tanh introduced in Sec. 2, we consider noisy activation functions of the following form:\n\u03c6(x, \u03be) = h(x) + s (6)\nwhere xt is the saturation threshold, and s = \u00b5+ \u03c3\u03be. Here \u03be is an iid random variable drawn from some generating distribution, and the parameters \u00b5 and \u03c3 (discussed below) are used to generate a location scale family from \u03be.\nIntuitively when the unit saturates we pin its output to the threshold value t and add noise. The exact behavior of the method depends on the type of noise \u03be and the choice of \u00b5 and \u03c3, which we can pick as functions of x in order to let some gradients be propagated even when we are in the saturating regime.\nA desirable property we would like \u03c6 to approximately satisfy is that, in expectation, it is equal to the hard-saturating activation function, i.e.\nE\u03be\u223cN (0,1)[\u03c6(x, \u03be)] \u2248 h(x) (7)\nIf the \u03be distribution has zero mean then this property can be satisfied by setting \u00b5 = 0, but for biased noise it will be necessary to make other choices for \u00b5. In practice, we used slightly biased \u03c6 with good results, though.\nIntuitively we would like to add more noise when x is far into the saturated regime, since a large change in parameters would be required desaturate h. Conversely, when x is close to the saturation threshold a small change in parameters would be sufficient for it to escape. To that end we make use of the difference between the original activation function h and its linearization u\n\u2206 = h(x)\u2212 u(x) (8)\nwhen choosing the scale of the noise. See Eqs.1 for definitions of u for the hard-sigmoid and hard-tanh respectively. The quantity \u2206 is zero in the unsaturated regime, and when h saturates it grows proportionally to the distance between |x| and the saturation threshold xt.\nWe experimented with several ways of scaling \u03c3 with \u2206, and empirically we found that the following formulation gave good results:\n\u03c3(x) = c (g(p\u2206)\u2212 0.5)2 g(x) = sigmoid(x). (9)\nIn Equation 9 a free scalar parameter p is learned during the course of training. By changing p, the model is able to adjust the magnitude of the noise and that also effects the sign of the gradient as well. The hyper-parameter c changes the scale of the standard deviation of the noise."}, {"heading": "4.1. Derivatives in the saturated regime", "text": "In the simplest case of our method we draw \u03be from an unbiased distribution, such as a standard normal. In this case we choose \u00b5 = 0 to satisfy Equation 7 and therefore we will have,\n\u03c6(x, \u03be) = h(x) + \u03c3(x)\u03be\nDue to our parameterization of \u03c3(x), when |x| \u2264 xt our stochastic activation function behaves exactly as the hardsaturating activation h, leading to familiar territory. Because \u2206 will be 0. Let us for the moment restrict our attention to the case when |x| > xt and h saturates. In this case the derivative of h(x) is precisely zero, however, if we condition on the sample \u03be we have\n\u03c6\u2032(x, \u03be) = \u2202\n\u2202x \u03c6(x, \u03be) = \u03c3\u2032(x)\u03be (10)\nwhich is non-zero almost surely.\nIn the non-saturated regime, where \u03c6\u2032(x, \u03be) = h\u2032(x) the optimization can exploit the linear structure of h near the origin in order to tune its output. In the saturated regime the randomness in \u03be drives exploration, and gradients still flow back to x since the scale of the noise still depends on x. To reiterate, we get gradient information at every point in spite of the saturation of h, and the variance of the\ngradient information in the saturated regime depends on the variance of \u03c3\u2032(x)\u03be."}, {"heading": "4.2. Pushing Activations Towards Linear Regime", "text": "An unsatisfying aspect of the formulation with unbiased noise is that, depending on the value of \u03be occasionally the gradient of \u03c6 will point the wrong way. This can cause a backwards message that would push x in a direction that would worsen the objective function, in average over \u03be. Intuitively we would prefer these messages to \u201cpush back\u201d the saturated unit towards a non-saturated state where the gradient of h(x) can be used safely.\nA simple way to achieve this is to make sure that the noise \u03be is always positive, and adjust its sign manually to match the sign of x. In particular we could set\nd(x) =\u2212 sgn(x) sgn(1\u2212 \u03b1) s =\u00b5(x) + d(x)\u03c3(x)|\u03be|.\nwhere \u03be and \u03c3 are as before and sgn is the sign function, such that sgn(x) is 1 if x is greater than or equal to 0 otherwise it is \u22121. We also use the absolute value of \u03be in the reparametrization of the noise, such that the noise is being sampled from a half-Normal distribution. We ignored the sign of \u03be, such that the direction that the noise pushes the activations are determined by d(x). Matching the sign of the noise to the sign of x would ensure that we avoid the sign cancellation between the noise and the gradient message from backpropagation. sgn(1\u2212\u03b1) is required to push the activations towards h(x) when the bias from \u03b1 is introduced.\nIn practice we use a hyperparameter \u03b1 that influences the mean of the added term, such that \u03b1 near 1 approximately satisfies the above condition, as seen in Fig. 4. We can rewrite the noisy term s in a way that the noise can either be added to the linearized function or h(x). The relationship between \u2206, u(x) and h(x) is visualized Figure 4.1 can be expressed as in Eqn 11.\nWe have experimented with different types of noise. Empirically, in terms of performance we found, half-normal and normal noise to be better. In Eqn 11, we provide the formulation for the activation function where = |\u03be| if the noise is sampled from half-normal distribution, = \u03be if the noise is sampled from normal distribution.\n\u03c6(x, \u03be) = u(x) + \u03b1\u2206 + d(x)\u03c3(x) (11)\nBy using Eqn 11, we arrive to the noisy activations, which we used in our experiments.\n\u03c6(x, \u03be) = \u03b1h(x) + (1\u2212 \u03b1)u(x) + d(x)\u03c3(x) (12)\nAs can be seen in Eqn 12, there are three paths that gradients can flow through the neural network, the linear path, nonlinear path and the stochastic path. The flow of gradients through these different pathways across different layers makes the optimization of our activation function easier.\nAt test time, we used the expectation of Eqn 12 in order to get deterministic units,\nE\u03be[\u03c6(x, \u03be)] = \u03b1h(x)+(1\u2212\u03b1)u(x)+d(x)\u03c3(x)E\u03be[ ] (13)\nIf = \u03be, then E\u03be[ ] is 0. Otherwise if = |\u03be|, then E\u03be[ ] will be \u221a 2 \u03c0 .\nAlgorithm 1 Noisy Activations with Half-Normal Noise for Hard-Saturating Functions\n1: \u2206\u2190 h(x)\u2212 u(x) 2: d(x)\u2190 \u2212 sgn(x) sgn(1\u2212 \u03b1) 3: \u03c3(x)\u2190 c (g(p\u2206)\u2212 0.5)2 4: \u03be \u223c N (0, 1) 5: \u03c6(x, \u03be)\u2190 \u03b1h(x) + (1\u2212 \u03b1)u(x) + (d(x)\u03c3(x)|\u03be|)\nTo illustrate the effect of \u03b1 and noisy activation of the hard-tanh, We provide plots of our stochastic activation functions in Fig. 4."}, {"heading": "5. Adding Noise to Input of the Function", "text": "Adding noise with fixed standard deviation to the input of the activation function has been investigated for ReLU activation functions (Nair & Hinton, 2010; Bengio et al., 2013).\n\u03c6(x, \u03be) = h(x+ \u03c3\u03be) and \u03be \u223c N (0, 1). (14)\nIn Eqn 14, we provide a parametrization of the noisy activation function. \u03c3 can be either learned as in Eqn 9 or fixed as a hyperparameter. The condition in Eqn 5 is satisfied only when \u03c3 is learned. Experimentally we found small values of \u03c3 to work better. When \u03c3 is fixed and small, as x gets larger and further away from the threshold xt, noise will less likely be able to push the activations back to the linear regime. We also investigated the effect of injecting input noise when the activations saturate:\n\u03c6(x, \u03be) = h(x+ 1|x|\u2265|xt|(\u03c3\u03be)) and \u03be \u223c N (0, 1). (15)"}, {"heading": "6. Experimental Results", "text": "In our experiments, we used noise only during training: at test time we replaced the noise variable by its expected value. We performed all our experiments with just a dropin replacement of the activation functions in existing experimental setups, without changing the previously set hyperparameters. Hence it is plausible one could obtain better results by performing a careful hyper-parameter tuning for the models with noisy activation functions. In all our experiments, we initialized p uniform randomly from the range [\u22121, 1].\nWe provide experimental results using noisy activations with normal (NAN), half-normal noise (NAH), normal noise at the input of the function (NANI) and normal noise injected to the input of the function when the unit saturates (NANIS)."}, {"heading": "6.1. Exploratory Analysis", "text": "As a sanity-check, we performed small-scale control experiments, in order to observe the behavior of the noisy units. In Fig 5, we showed the learning curves of different types of activations with various types of noise in contrast to the tanh and hard-tanh units. The models are single-layer MLPs trained on MNIST for classification and we show the average negative log-likelihood \u2212 logP (correct class|input). Models that are using noisy activations converge faster than those using tanh and hard-tanh activation functions, and to lower NLL than the tanh network.\nWe trained 3\u2212layer MLP on a dataset generated from a mixture of 3 Gaussian distributions with different means and standard deviations. Each layer of the MLP contains 8-hidden units. Both the model with tanh and noisy\u2212 tanh activations was able to solve this task almost perfectly. By using the learned p values, in Figure 6 and 7, we showed the scatter plot of the activations of each unit at each layer and the derivative function of each unit at each layer with respect to its input.\nWe further investigated the performances of network with activation functions using NAN, NANI and NANIS on pen-\n\u2202xki at the end of training\nfor ith unit at kth layer. \u03bek is sampled from Normal distribution for \u03b1 = 1.\nntreebank character-level language modeling. We used a GRU language model over sequences of length 200. We used the same model and train all the activation functions with the same hyperparameters except we ran a grid-search for \u03c3 for NANI and NANIS from [1, 0.01] with 8 values. We choose the best \u03c3 based on the validation bit-percharacter(BPC). We have not observed any big difference among NAN and NANS in terms of training performance as seen on Figure 8."}, {"heading": "6.2. Learning to Execute", "text": "The problem of predicting the output of a short program introduced in (Zaremba & Sutskever, 2014)1 proved chal-\n1The code is residing at https://github.com/ wojciechz/learning_to_execute. We thank authors\nlenging for modern deep learning architectures. The authors had to use curriculum learning (Bengio et al., 2009) to let the model capture knowledge about the easier examples first and increase the level of difficulty of the examples further down the training.\nWe replaced all sigmoid and tanh non-linearities in the reference model with their noisy counterparts. We changed the default gradient clipping to 5 from 10 in order to avoid numerical stability problems. When evaluating a network, the length (number of lines) of the executed programs was set to 6 and nesting was set to 3, which are default settings in the released code for these tasks. Both the reference model and the model with noisy activations were trained with \u201ccombined\u201d curriculum which is the most sophisticated and the best performing one.\nOur results show that applying the proposed activation\nfor making it publicly available.\nfunction leads to significantly better performance than that of the reference model. Moreover it shows that the method is easy to combine with a non-trivial learning curriculum. The results are presented in Table 1 and in Figure 10"}, {"heading": "6.3. Penntreebank Experiments", "text": "We trained a 2\u2212layer word-level LSTM language model on Penntreebank. We used the same model proposed by Zaremba et al. (2014)2. We simply replaced all sigmoid and tanh units with noisy hard-sigmoid and hard-tanh units. The reference model is a well-finetuned strong baseline from (Zaremba et al., 2014). For the noisy experiments we used exactly the same setting, but decreased the gradient clipping threshold to 5 from 10. We provide the results of different models in Table 2. In terms of validation and test performance we did not observe big difference between the additive noise from Normal and half-Normal distributions, but there is a substantial improvement due to noise, which makes this result the new state-of-the-art on this task, as far as we know."}, {"heading": "6.4. Neural Machine Translation Experiments", "text": "We have trained a neural machine translation (NMT) model on the Europarl dataset with the neural attention model\n2We used the code provided in https://github.com/ wojzaremba/lstm\nTable 3. Image Caption Generation on Flickr8k. This time we added noisy activations in the code from (Xu et al., 2015) and obtain substantial improvements on the higher-order BLEU scores and the METEOR metric, as well as in NLL. Soft attention and hard attention here refers to using backprop versus REINFORCE when training the attention mechanism.\nBLEU -1 BLEU-2 BLEU-3 BLEU-4 METEOR Test NLL\nSoft Attention (Sigmoid and Tanh) (Reference) 67 44.8 29.9 19.5 18.9 40.33 Soft Attention (Noisy Sigmoid and Tanh-NAH) 66 45.8 30.9 20.9 20.5 40.17\nHard Attention (Sigmoid and Tanh) 67 45.7 31.4 21.3 19.5 -\n60 80 100 120 140 160 180 Epochs\n47%\n48%\n48%\n48%\n49%\n50%\n50%\n50%\n51%\nA cc\nur ac\ny\nNoisy Network Train accuracy Reference Model Train accuracy\nFigure 10. Training curves of the reference model (Zaremba & Sutskever, 2014) and its noisy variant on the \u201cLearning To Execute\u201d problem. The noisy network converges faster and reaches a higher accuracy, showing that the noisy activations help to better optimize for such hard to optimize tasks.\n(Bahdanau et al., 2014)3. We have replace all sigmoid and tanh units in the model with the noisy counterparts. We scaled down the weight matrices initialized to be orthogonal by the factor of 0.01. Evaluation is done on the newstest2011 test set. All models are trained with early-stopping. We also compared with a model with hard-tanh and hard-sigmoid units and our model with noisy activations was able to outperform both, as shown in Table 4. Again, we see a substantial improvement (more\n3Again, we have used existing code, provided in https: //github.com/kyunghyuncho/dl4mt-material, and only changed the nonlinearities\nSigmoid and Tanh NMT (Reference) 65.26 20.18 Hard-Tanh and Hard-Sigmoid NMT 64.27 21.59 Noisy (NAH) Tanh and Sigmoid NMT 63.46 22.57\nthan 2 BLEU points) with respect to the reference."}, {"heading": "6.5. Image Caption Generation Experiments", "text": "We evaluated our noisy activation functions on a network trained on the Flickr8k dataset. We used the soft neural attention model proposed in (Xu et al., 2015) as our reference model. 4 We scaled down the weight matrices initialized to be orthogonal by the factor of 0.01. As shown in Table 3, we were able to obtain better results than the reference model and our model also outperformed the best model provided in (Xu et al., 2015) in terms of Meteor score."}, {"heading": "6.6. Experiments with Continuation", "text": "We performed experiments to validate the effect of annealing the noise to obtain a continuation method for neural networks.\nWe designed a new task where, given a random sequence of integers, the objective is to predict the number of unique elements in the sequence. We use an LSTM network over the input sequence, and performed a time average pooling over the hidden states of LSTM to obtain a fixed-size vector. We feed the pooled LSTM representation into a simple (one hidden-layer) ReLU MLP in order to predict the unique number of elements in the input sequence. In the experiments we fixed the length of input sequence to 26 and the input values are between 0 and 10. In order to anneal the noise, we started training with the scale hyperparameter\n4We used the code provided at https://github.com/ kelvinxu/arctic-captions.\nTest Error %\nNoisy LSTM + MLP (NAN) Annealed Noise 9.53\nof the standard deviation of noise with c = 30 and annealed it down to 0.5 with the schedule of c\u221a\nt+1 where t is being\nincremented at every 200 minibatch updates. When noise annealing is combined with a curriculum strategy (starting with short sequences first and gradually increase the length of the training sequences), the best models are obtained. Code for generating this data and running the experiments is available at http://anonymous.\nAs a second test, we used the same annealing procedure in order to train a neural turing machine (NTM) on the associative recall task (Graves et al., 2014). We trained our model with a minimum of 2 items and a maximum of 16 items. We show results of the NTM with noisy activations in the controller, with annealed noise, and compare with a regular NTM in terms of validation error. As can be seen in Figure 11, he noisy activation network converges much faster and nails the task, whereas the original network failed to approach a low error."}, {"heading": "7. Conclusion", "text": "Nonlinearities in neural networks are both a blessing and a curse. A blessing because they allow to represent more complicated functions and a curse because that makes the optimization more difficult. For example, we have found in several of our experiments that using a hard version (hence more nonlinear) of the sigmoid and tanh nonlinearities often improved results. In the past, various strategies have been proposed to help deal with the difficult optimization problem involved in training some deep networks, including curriculum learning, which is an approximate form of continuation method. Earlier work also included softened versions of the nonlinearities that would be gradually be made harder during training. Motivated by this prior work, we introduce and formalize the concept of noisy activations as a general framework for injecting noise in nonlinear functions so that large noise allows SGD to be more exploratory. We propose a specific noisy activation unit which only injects noise in the region where the unit would otherwise saturate, and allow gradients to flow even in that case. Even with a fixed noise level, we found the proposed noisy activations to outperform their sigmoid or tanh counterpart on many tasks and datasets, yielding new state-ofthe-art results, for example on PennTreebank. In addition, we found that annealing the noise to obtain a continuation method could further improved performance."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec, Compute Canada, Samsung, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano 5, for developing such a powerful tool for scientific computing. Caglar Gulcehre also thanks to IBM Watson Research and Statistical Knowledge Discovery Group at IBM research for supporting this work during his internship.\n5http://deeplearning.net/software/theano/"}], "references": [{"title": "Numerical Continuation Methods", "author": ["E.L. Allgower", "K. Georg"], "venue": "An Introduction. Springer-Verlag,", "citeRegEx": "Allgower and Georg,? \\Q1980\\E", "shortCiteRegEx": "Allgower and Georg", "year": 1980}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Bengio", "Yoshua"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "Jerome", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML\u201909,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc V", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Kurach", "Karol", "Martens", "James"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "It is thanks to ReLU that for the first time a few years ago it was shown (Glorot et al., 2011) that very deep purely supervised networks can be trained, whereas using tanh nonlinearity only allowed to train shallow networks.", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "A plausible hypothesis about the recent surge of interest on these piecewise-linear activation functions (Glorot et al., 2011), is due to the fact that they are easier to optimize with SGD and backpropagation than smooth activation functions, such as sigmoid and tanh.", "startOffset": 105, "endOffset": 126}, {"referenceID": 4, "context": "Adding noise to the activation function has been considered for ReLU units and was explored in (Bengio et al., 2013; Nair & Hinton, 2010) for feed-forward networks and Boltzmann machines to encourage units to explore more and make the optimization easier.", "startOffset": 95, "endOffset": 137}, {"referenceID": 5, "context": "More recently there has been a resurgence of interest in more elaborate \u201cgated\u201d architectures such as LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014), but also encompassing neural attention mechanisms that have been used in the NTM (Graves et al.", "startOffset": 150, "endOffset": 168}, {"referenceID": 7, "context": ", 2014), but also encompassing neural attention mechanisms that have been used in the NTM (Graves et al., 2014), Memory Networks (Weston et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 13, "context": ", 2014), automatic image captioning (Xu et al., 2015) and wide areas of applications (LeCun et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 10, "context": "Although gates often operate in the soft-saturated regime (Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al., 2015) the architecture prevents them from being fully open or closed.", "startOffset": 58, "endOffset": 126}, {"referenceID": 1, "context": "Although gates often operate in the soft-saturated regime (Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al., 2015) the architecture prevents them from being fully open or closed.", "startOffset": 58, "endOffset": 126}, {"referenceID": 8, "context": "Although gates often operate in the soft-saturated regime (Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al., 2015) the architecture prevents them from being fully open or closed.", "startOffset": 58, "endOffset": 126}, {"referenceID": 12, "context": "A related approach of adding noise to gradients and annealing the noise was investigated in (Neelakantan et al., 2015) as well.", "startOffset": 92, "endOffset": 118}, {"referenceID": 4, "context": "Adding noise with fixed standard deviation to the input of the activation function has been investigated for ReLU activation functions (Nair & Hinton, 2010; Bengio et al., 2013).", "startOffset": 135, "endOffset": 177}, {"referenceID": 3, "context": "The authors had to use curriculum learning (Bengio et al., 2009) to let the model capture knowledge about the easier examples first and increase the level of difficulty of the examples further down the training.", "startOffset": 43, "endOffset": 64}, {"referenceID": 14, "context": "The reference model is a well-finetuned strong baseline from (Zaremba et al., 2014).", "startOffset": 61, "endOffset": 83}, {"referenceID": 14, "context": "We used the same model proposed by Zaremba et al. (2014)2.", "startOffset": 35, "endOffset": 57}, {"referenceID": 13, "context": "This time we added noisy activations in the code from (Xu et al., 2015) and obtain substantial improvements on the higher-order BLEU scores and the METEOR metric, as well as in NLL.", "startOffset": 54, "endOffset": 71}, {"referenceID": 14, "context": "We only replaced in the code from Zaremba et al. (2014) the sigmoid and tanh by corresponding noisy variants and observe a substantial improvement in perplexity, which makes this the state-of-theart on this task.", "startOffset": 34, "endOffset": 56}, {"referenceID": 1, "context": "(Bahdanau et al., 2014)3.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Using existing code from (Bahdanau et al., 2014) with nonlinearities replaced by their noisy versions, we find much improved performance (2 BLEU points is considered a lot for machine translation).", "startOffset": 25, "endOffset": 48}, {"referenceID": 13, "context": "We used the soft neural attention model proposed in (Xu et al., 2015) as our reference model.", "startOffset": 52, "endOffset": 69}, {"referenceID": 13, "context": "As shown in Table 3, we were able to obtain better results than the reference model and our model also outperformed the best model provided in (Xu et al., 2015) in terms of Meteor score.", "startOffset": 143, "endOffset": 160}, {"referenceID": 7, "context": "As a second test, we used the same annealing procedure in order to train a neural turing machine (NTM) on the associative recall task (Graves et al., 2014).", "startOffset": 134, "endOffset": 155}], "year": 2016, "abstractText": "Common activation functions used in neural networks can yield to training difficulties due to the saturation behavior of the activation function, which may hide dependencies which are not visible to first order (using only gradients). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that some gradients may sometimes flow, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent to be more exploratory. By adding noise only to the problematic parts of the activation function we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by by noisy variants helps training in many contexts, yielding state-of-the-art results on several datasets, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.", "creator": "LaTeX with hyperref package"}}}