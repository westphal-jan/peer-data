{"id": "1603.00957", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Question Answering on Freebase via Relation Extraction and Textual Evidence", "abstract": "Existing knowledge - hiroyasu based karasyov question answering systems dungar often 66.11 rely on schwartlander small wustlich annotated training data. While shallow officio methods vectorization like information extraction biasone techniques are petrolia robust gorgonio to number-three data boroujerdi scarcity, they are less fulbright expressive than huntin deep understanding zealot methods, thereby pakhi failing thrashed at 73.05 answering great-great-grandchildren questions involving jetboat multiple baa2/bbb constraints. Here senga we sovereign alleviate this syamsuddin problem by empowering 61.88 a denzongpa relation extraction fore-and-aft method 20.02 with blundetto additional compelling evidence from Wikipedia. We raes first swinburn present hez a re-developed novel 18:56 neural reppert network based relation abdominals extractor divisoria to retrieve qawasmi the nece candidate 68.15 answers from Freebase, and zampella then 2,612 develop a refinement model to pistoia validate answers using 700-ton Wikipedia. 27,000-ton We shaurya achieve 53. 3 accomplishment F1 nambi on nimbyism WebQuestions, alb-donau a kreuzlingen substantial improvement anisoptera over uar the yearlings state - laberge of - the - art.", "histories": [["v1", "Thu, 3 Mar 2016 03:22:01 GMT  (636kb,D)", "http://arxiv.org/abs/1603.00957v1", null], ["v2", "Wed, 8 Jun 2016 11:05:53 GMT  (1341kb,D)", "http://arxiv.org/abs/1603.00957v2", null], ["v3", "Thu, 9 Jun 2016 15:12:19 GMT  (1335kb,D)", "http://arxiv.org/abs/1603.00957v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kun xu", "siva reddy", "yansong feng", "songfang huang", "dongyan zhao"], "accepted": true, "id": "1603.00957"}, "pdf": {"name": "1603.00957.pdf", "metadata": {"source": "CRF", "title": "Enhancing Freebase Question Answering Using Textual Evidence", "authors": ["Kun Xu", "Yansong Feng", "Siva Reddy", "Songfang Huang", "Dongyan Zhao"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "As very large structured knowledge bases (KBs) have become available, e.g., Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Auer et al., 2007), KBs have become a new source of potential answers for people to mine. Answering factoid questions over such structured KBs, known as KB-based question answering (or KBQA), is attracting increasing research efforts from both information retrieval and natural language processing communities.\nThe state-of-the-art methods for this task can be categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated\nmeaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures which is practically impossible for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014).\nOn the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry. We propose a method based on text based evidence which help solve this problem without having to do this.\nKnowledge bases like Freebase capture real world facts which people are interested in, and web like Wikipedia provides a large repository of sentences\nar X\niv :1\n60 3.\n00 95\n7v 1\n[ cs\n.C L\n] 3\nM ar\nthat validate or support these facts. For example, a sentence in Wikipedia says, Denali (also known as Mount McKinley, its former official name) is the highest mountain peak in North America, with a summit elevation of 20,310 feet (6,190 m) above sea level. To answer our example question on a KB using a relation extractor, we can use this sentence as external evidence, and filter out wrong answers and pick the correct one.\nUsing external evidence not only mitigates representational issues in relation extraction, it also alleviates data scarcity problem to some extent. Consider the question, Who was queen isabella\u2019s mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is the answer should be the parent of Isabella, and the other is the answer\u2019s gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction approaches, and requires larger training data (this phenomenon is coined as sub-lexical compositionally by Wang et al. (2015)). Most systems are good at triggering the parent constraint, but fail on the other. The textual evidence from Wikipedia, . . . her mother was Isabella of Barcelos . . . , can act as a further constraint to answer the question.\nWe present a novel method for answering factoid questions, which infers both on structured and unstructured resources. Figure 1 gives an overview. First we employ a relation extractor to find answer entities from a KB (indicated by KB-QA box). Then a refinement model uses textual evidence to select answer entities (indicated by Answer Refinement box). The main contributions of this paper are two-fold.\n\u2022 We treat the KB-QA problem as a joint task of relation extraction and entity linking, and further propose an Multi Channel Convolutional Neural Network (MCCNN) (Section 4.2) to learn a robust relation representation. By performing a joint inference approach (Section 4.3), our KB-based approach obtains competitive results on WEBQUESTIONS.\n\u2022 We introduce a refinement model to further validate the answers (Section 5). Experimental results show that our system outperforms the state-of-the-\nart systems on WEBQUESTIONS."}, {"heading": "2 Framework", "text": "Consider the question \u201cwho did shaq first play for\u201d, we first identify the topic entity in the question, and then employ an MCCNN model to recognize the KB relation between the topic entity and the answer. For both entity linking and relation extraction components, we keep up to 5 top ranked entities and relations, and develop a joint inference model to find the best entity-relation configuration, and accordingly retrieve candidate answers from Freebase. Finally, we refine these candidate answers by applying a refinement model which takes the Wikipedia page of the topic entity into consideration."}, {"heading": "3 Question Decomposition", "text": "A question often involves multiple constraints to its answers, e.g., who plays anakin skywalker in star wars 1. All actors as the answers of this question should satisfy the following two constraints: (1)\nthe actor played anakin skywalker; and (2) the actor played in star wars 1. Inspired by (Bao et al., 2014), we design a dependency tree-based method to handle such multiple-constraint questions. In general, we first decompose the original question into a set of sub-questions using syntactic-based patterns. Then the final answers of the original question can be obtained by intersecting the answers of all subquestions. For the example question, its two subquestions are who plays anakin skywalker and who plays in star wars 1."}, {"heading": "4 KB-based Question Answering", "text": "Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation with the entity found in the question, and predict a single KB triple for each sub-question. The QA problem is thus formulated as an information extraction problem that involves two sub-tasks, i.e., entity linking and relation extraction, which are challenging enough by themselves, and whose results are even expected to be compatible to each other."}, {"heading": "4.1 Entity Linking", "text": "For each question, we use hand-built sequences of part-of-speech categories to identify all possible named entity mention spans, e.g., the sequence NN (shaq) may indicate an entity. For each mention span, we employ an entity linking tool S-MART (Yang and Chang, 2015) to retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will be eventually disambiguated in the joint inference step."}, {"heading": "4.2 Relation Extraction", "text": "We now proceed to identify the relation between the answer and the entity in the question. Recently, a large number of methods have been proposed to learn the mapping from relational phrases to KB relations, such as Naive Bayes based (Yao and Van Durme, 2014), logistic regression based (Berant et al., 2013), and neural network based (Yih et al., 2015). These methods usually take the whole question to predict the relation, which may be effected by noisy information that are unrelated to the relation, especially for those short questions. Syntactic\n1who, when, what, where, how, which, why, whom, whose.\nfeatures such as the shortest dependency path have been proven to be more concise and representative in relation extraction (Liu et al., 2015; Xu et al., 2015). Therefore, we develop a multi-channel convolutional neural networks (MCCNNs) to learn the relation representations from both the syntactic level and sentence level."}, {"heading": "4.2.1 Syntactic Level Features", "text": "We use the shortest path between an entity mention and the question word in the dependency tree2 as input to the first channel. We treat the path as a concatenation of vectors of words, dependency edge directions, and dependency labels. Note that, the entity mention and the question word are excluded from the dependency path so as to learn a more general relation representation in syntactic level. As shown in Figure 2, the dependency path between shaq and who is\u2190 nsubj \u2013 play \u2013 dobj\u2192."}, {"heading": "4.2.2 Sentence Level Features", "text": "The other channel takes the context of the entities in the question as input, i.e., the words of the question after removing the entity mention and the ques-\n2We use Stanford CoreNLP dependency parser.\ntion word. As illustrated in Figure 2, did first play for is fed into the other channel to learn the relation representation at a sentence level."}, {"heading": "4.2.3 MCCNNs for Relation Classification", "text": "For each channel, the network structure illustrated in Figure 2 is used to tackle the input of various length, which returns a fixed length vector representation. Finally the two feature vectors are concatenated and then fed into a softmax classifier, the output dimension of which is equal to the number of predefined relation types. The value of each dimension indicates the confidence score of the corresponding relation. In the syntactic feature representations, the vectors of the dependency edge direction and dependency label are randomly initialized and optimized through back-propagation. The word embeddings are shared across two channels and also updated in the training process."}, {"heading": "4.2.4 Objective Function and Learning", "text": "The model is learned using pairs of question and its corresponding simulated gold relation from the training data. Given an input question x with an annotated entity mention, the network outputs a vector o(x), where the entry ok(x) is the probability that there exists the k-th relation between the entity and the expected answer. We denote t(x) \u2208 RK\u00d71 as the target distribution vector, in which the value for gold relation is set 1, others are set 0. We compute the cross entropy error between t(x) and o(x), and further define the objective function over all training data:\nJ(\u03b8) = \u2212 \u2211 x K\u2211 k=1 tk(x) log ok(x) + \u03bb||\u03b8||2\nwhere \u03b8 is the set of model parameters to be learned, and \u03bb is a vector of regularization parameters. The model parameters \u03b8 can be efficiently computed via back-propagation through network structures. To minimize J(\u03b8), we apply stochastic gradient descent (SGD) with AdaGrad (Duchi et al., 2011) in our experiments."}, {"heading": "4.3 Joint Inference", "text": "A pipeline of entity linking and relation extraction may suffer from error propagations. As we know,\nentities and relations have strong selectional preferences that certain entities do not appear with certain relations and vice versa. Locally optimized models are not able to exploit these implicit bi-directional preferences. We thus exploit a ranking based joint inference model to find a globally optimal entityrelation assignment from local predictions. The key idea behind is to leverage various clues from the two local models and the KB available to rank a correct entity-relation assignment higher than those problematic ones."}, {"heading": "4.3.1 Features", "text": "For a given entity-relation assignment (e, r), we exploit from the KB the following clues:\nEntity Clues The score of predicted entity e returned by the entity linking system is directly used as a feature. In addition, the mention\u2019s text overlap with the Freebase name for entity e is also included as a feature. In Freebase, most entities have a relation fb:description that describes the basic introduction of this entity. This description serves as an important clue to distinguish the entity, since in some sense, the description could be treated as a bag of relational phrases related to this entity. For instance, in the running example, shaq is linked to three potential entities m.06 ttvh (an American reality television show), m.05n7bp (a multiplayer video game) and m.012xdf (a famous NBA basketball player). Interestingly, the relational phrase play only appears in the description of m.012xdf and occurs three times. Therefore, we count how many times the relational phrase appearing in e\u2019s description, and include it as a feature.\nRelation Clues The score of relation r returned by the MCCNNs is used as a feature. Furthermore, we view each relation as a document which consists of the training questions that this relation is expressed in. For a given question, we use the sum of the tf-idf scores of its words with respect to the relation r as a feature.\nAnswer Clues In a natural language question, question word itself often indicates the answer type. For example, the question word when usually indicates that the answer entity has a Freebase type type.datetime. We thus use the co-occurrences of\nthe question word and the answer types as a feature. Secondly, a Freebase relation r is a concatenation of a series of fragments r = r1.r2.r3. For instance, the three fragments of people.person.parents are people, person and parents. The first two fragments indicate the Freebase type of the subject of this relation, and the third fragment indicates the object type, in our case the answer type. We use an indicator feature to denote if the surface form of third fragment r appears in the question."}, {"heading": "4.3.2 Learning", "text": "To train a ranking based joint inference model needs entity-relation assignments (e, r). We take all our entity/relation predictions, and create a ranked list as follows. Suppose that (egold, rgold) is the gold entity/relation pair for question q. Let (ea, ra) be a candidate entity-relation assignment for q. Specifically, if both ea and ra are correct (i.e., ea = egold, ra = rgold), we assign it with the highest ranking score 3. In contrast, if only the entity or relation equals to the gold one (i.e., ea = egold, ra 6= rgold or ea 6= egold, ra = rgold), we assign it with score 2. When both entity and relation assignments are wrong, we assign it with score 1. The intuition behind is that even if an assignment is not completely correct, it is still preferred than some other totally incorrect assignments. This gives the training data for our svm-ranker (Joachims, 2006), which we hope the gold-standard entity-relation assignments rank higher than incorrect ones."}, {"heading": "5 Refining Answers Using Wikipedia", "text": "After the joint inference, we are able to generate a KB query for the question, thus retrieve a collection of candidate answers from the KB. To further refine the answers, we use Wikipedia as the unstructured knowledge resource, where most statements are validated by multiple people.\nOur refinement model is inspired by the intuition of how people refine their answers. If you ask someone: who did shaq first play for, and give them four candidate answers (Los Angeles Lakers, Boston Celtics, Orlando Magic and Miami Heat), as well as access to Wikipedia, that person might first determine that the question is about Shaquille O\u2019Neal, then go to O\u2019Neal\u2019s Wikipedia page, and search for the sentences that contain the candidate answers as\nevidences. By analyzing these sentences, one can figure out whether a candidate answer is correct or not."}, {"heading": "5.1 Finding Evidence from Wikipedia", "text": "As mentioned above, we should first link topic entity selected by joint inference model and answers retrieved from Freebase into the Wikipedia3. In the topic entity\u2019s Wikipedia page, we utilize wikifier (Cheng and Roth, 2013) to recognize the Wikipedia entities in the sentences. Then, we collect the sentences that mention the candidate answers as the evidences, which are the input of the refinement model. For example, in the Wikipedia page of O\u2019Neal, we can find a sentence \u201cO\u2019Neal was drafted by the Orlando Magic with the first overall pick in the 1992 NBA draft\u201d, which is treated as an evidence that supports the answer Orlando Magic. This sentence is then taken into account by the refinement model to discriminate whether Orlando Magic is the answer of the question."}, {"heading": "5.2 Lexical Features", "text": "We treat the refinement process as a binary classification task, i.e., correct (positive) and incorrect (negative). In particular, we rely on the lexical features extracted from the question and the evidence. Formally, given a question q = <q1, .., qn> and an evidence sentence s = <s1, .., sm>, we denote the token of q and s by qi and sj , respectively. For each pair (q, s), we identify a set of all possible token pairs (qi, sj), the occurrences of which are used as features. In this way, we hope to to learn a higher weight for features like: (first, drafted) and lower weight for (first, played)."}, {"heading": "5.3 Learning", "text": "We prepare the training data for the refinement model as follows. On the training dataset, we first use our KB-based approach to retrieve the answers from the KB. Then we use the gold answers of these questions to supervise the training of the refinement model. Specifically, we treat the sentences that contain correct/incorrect answers as positive/negative examples for the refinement model. We then employ\n3We apply the Freebase API to retrieve the corresponding Wikipedia id of a Freebase entity.\nLIBSVM (Chang and Lin, 2011) to learn the weights for all the token pairs.\nNote that, in the Wikipedia page of the topic entity, we may collect more than one sentence that contain a candidate answer. However, some of these sentences may be irrelevant to the question, we thus consider this candidate answer as correct if at least one evidence can be predicted positive by our refinement model. On the other hand, sometimes, we may not find any evidence for the candidate answer. In these cases, we fall back to the results of the KBbased approach."}, {"heading": "6 Experiments", "text": "In this section we introduce the experimental setup, the main results and detailed analysis of our system."}, {"heading": "6.1 Data", "text": "We use the WEBQUESTIONS dataset, which contains 5,810 questions crawled via Google Suggest service, with answers annotated on Amazon Mechanical Turk. The questions are split into training and test sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively. We further split the training questions into the training set and the development set by 80%/20%.\nNotice that, to train the MCCNNs and the joint inference model, we need the gold standard relations of the questions. Since this dataset only contains question-answer pairs and annotated topic entities, we retrieve the simulated gold relations in three steps: for a given question, we first locate the topic entity e in the Freebase graph, then view the 1-hop and 2-hop predicates connected to the topic entity as the gold relation candidates. For each relation candidate r, we issue the query (e, r, ?) to the KB, and label the one that has the answers which have the highest F1 value compared with the annotated answers, as the simulated gold relation."}, {"heading": "6.2 Parameter Settings", "text": "The dimension of the word embedding is set to 50. We initialized the word embeddings with the word vectors trained with the model of Turian et al. (2010). The hyper parameters in our model are tuned using the development set. The window size of MCCNNs is 3. The sizes of hidden layer 1 and\nhidden layer 2 of two channels in MCCNNs are set to 200 and 100, respectively."}, {"heading": "6.3 Results and Discussion", "text": "We use Berant\u2019s official evaluation script to compute the average question-wise F1, and compare our model DeepJRQA, which performs both the Joint inference and answer Refinement, to existing models in the literature. Note that, DeepJRQA treats the executed results of 2 top ranked KB triples generated by the joint inference model as candidate answers.\nMoreover, we use three baselines. The first baseline is DeepQA, which finds the answers by taking the best predictions from a pipeline of EL and RE to query the KB. The second baseline is DeepJQA, which takes the joint inference approach but without answer refinement. The third baseline is DeepRQA, which performs the answer refinement but without joint inference. Table 1 summarizes the results.\nWe can see that when adding the joint inference and answer refining step, i.e., DeepJRQA, our performance is improved by 9.2% over DeepQA, outperforming all systems including the current stateof-the-art (Yih et al., 2015). We also report Yih et al. removing its external resource ClueWeb."}, {"heading": "6.3.1 Detailed analysis", "text": "Now let us explore the contributions of each component of our system.\nJoint inference From Table 1, we can see that adding the joint inference step make a substantial performance increase around 3%. The question of interest is how much performance gain the EL and RE components contribute during the joint inference approach.\nWe first evaluate the entity linking step with the gold entity annotation on the development set. As shown in Table 2, for 79.8% questions, our entity linker can correctly find the gold standard topic entities. We also evaluate DeepJQA to see if the joint inference helps entity linking. We see 3.4% improvement on the best prediction revealing that the joint inference is indeed beneficial. Next we use the simulated gold relations to evaluate the performance of RE component on the development set. As shown in Table 2, the relation prediction accuracy increases by 9.4% when using the joint inference approach.\nFeature variations Table 3 shows the results of feature ablation studies. In particular, we present the results using single-channel network , i.e., tuning the parameters of one channel while switching off the other. As seen, the sentence level features are found to be more important than syntactic features. This could be due to short and noisy nature of WEBQUESTIONS questions which causes the shortest dependency path inaccessible to sufficient information to predict a relation. By mixing the syntax and context channels, the joint inference model boosts its performance by 5.7% from 40.1% to 45.8%.\nAnswer refinement As shown in Table 1, when augmented with the answer refinement model, DeepRQA and DeepJRQA obtain significant performance improvements, 2.9% and 6.2%, respectively.\nOn the test set, we find 192 examples get a higher F1 value of the answers after taking the refinement method. Table 4 lists some questions and the corresponding answers of our system.\nWe manually analyze these examples and find out that, as we expected, the refinement model improves the KB-based approach mainly on 2 types of questions: (i) the questions that involve aggregation operations. Examples are Question 1-3 in Table 4. For Question 3, DeepJQA takes the Freebase relation fb:teams..from as the prediction of the joint inference approach. This relation describes the first years that ray allen joined the basketball teams including in the college and NBA. To answer such a question, the min(\u00b7) operation should be performed on these years. Note that, Ray Allen joined a college basketball team (University of Connecticut) in the 1993 rather than a NBA team. Interestingly, the refinement model can not only correctly choose the earliest year but also distinguish his basketball career period. (ii) the questions that query for fine-grained relations such as Question 4- 5 in Table 4. Take Question 5 as an example, the user queries for the college that John Steinbeck attended. However, Freebase only uses a single relation fb:education..institution to describe the person\u2019s general education information,\nwithout discriminating the specific periods such as high school or college.\nThese empirical findings demonstrate again that it is appropriate to exploit the Wikipedia for answer refinement.\nError analysis We analyze the errors our model makes on the development set. Around 15% of the errors are caused by the incorrect entity linking. Around 50% of the errors are due to incorrect relation predictions. We analyze these relation prediction errors and find out two reasons: (i) in some short-length questions, both the dependency path and the sentence are too short to provide sufficient information to the relation classifier; (ii) the insufficiency and unbalanced distribution of the training data for the relation classifier (3022 training examples for 461 relations) heavily influences the performance of relation classification.\nThe remaining errors are due to the failure of our refinement model for two reasons. First, for some questions, we cannot find evidences with respect to the candidate answers, and our system fall back to the KB-based approach, which may obtain a collection of answers in low accuracy. Second, recall that we only use the lexical features to refine the answers, the feature sparse problem inevitably hurts the performance of our refinement model ."}, {"heading": "7 Related Work", "text": "Over time, the QA task has evolved into two main streams \u2013 QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent focus has shifted to large scale structured KBs like Freebase, DBPedia (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas)\nat the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). QALD tasks and linked data initiatives are contributing to this trend.\nOur model combines the best of both worlds by inferring over structured and unstructured data. Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answer single relation queries.\nOur work also intersects with relation extraction methods. While these methods aim to predict a relation between two entities in order to populate KBs (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering. Krishnamurthy and Mitchell (2012) and Fader et al. (2014) adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries. Closest to our extraction method is (Yao and Van Durme, 2014; Yao, 2015) who also uses sentence level relation extraction for QA. Unlike them, we can predict multiple relations per question, and our MCCNN architecture is more robust to unseen contexts compared to their logistic regression models."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper, we presented a novel method that exploits both Freebase and Wikipedia to answer natural language questions. We introduced a joint entity linking and relation extraction method to retrieve answers from Freebase, and then refine these answers using additional evidence from Wikipedia. Our method outperforms the state-of-the-art on the WEBQUESTIONS dataset.\nAlthough the refinement model helps, our performance is limited by the coverage of Freebase. Moreover, in the future, to answer subjective questions such as what is the most popular attraction in America which cannot be answered using Freebase only, we should consult the unstructured knowledge."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] Sren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary G. Ives"], "venue": "In ISWC/ASWC", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Open information extraction for the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni"], "venue": "In IJCAI", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Knowledge-based question answering as machine translation", "author": ["Bao et al.2014] Junwei Bao", "Nan Duan", "Ming Zhou", "Tiejun Zhao"], "venue": null, "citeRegEx": "Bao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2014}, {"title": "More accurate question answering on freebase", "author": ["Bast", "Haussmann2015] Hannah Bast", "Elmar Haussmann"], "venue": "In CIKM", "citeRegEx": "Bast et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bast et al\\.", "year": 2015}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": null, "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks. CoRR, abs/1506.02075", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Cai", "Yates2013] Qingqing Cai", "Alexander Yates"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM TIST,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Relational inference for wikification", "author": ["Cheng", "Roth2013] Xiao Cheng", "Dan Roth"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2013}, {"title": "Question answering over freebase with multi-column convolutional neural networks. In ACLIJCNLP", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni"], "venue": null, "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader et al.2014] Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In SIGKDD", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions", "author": ["Heilman", "Smith2010] Michael Heilman", "Noah A Smith"], "venue": null, "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": null, "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Training linear svms in linear time. In SIGKDD", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Knowledge graph and corpus driven segmentation and answer inference for telegraphic entity-seeking", "author": ["Joshi et al.2014] Mandar Joshi", "Uma Sawant", "Soumen Chakrabarti"], "venue": null, "citeRegEx": "Joshi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2014}, {"title": "Weakly supervised training of semantic parsers", "author": ["Krishnamurthy", "Tom M Mitchell"], "venue": "EMNLPCoNLL", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer"], "venue": null, "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": null, "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Grounded semantic parsing for complex knowledge extraction", "author": ["Hoifung Poon", "Kristina Toutanova"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2015}, {"title": "Large-scale semantic parsing without question-answer pairs", "author": ["Reddy et al.2014] Siva Reddy", "Mirella Lapata", "Mark Steedman"], "venue": "Transactions of the Association of Computational Linguistics,", "citeRegEx": "Reddy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": null, "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Template-based question answering over rdf data", "author": ["Lorenz B\u00fchmann", "Jens Lehmann", "Axel-Cyrille Ngonga Ngomo", "Daniel Gerber", "Philipp Cimiano"], "venue": null, "citeRegEx": "Unger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Unger et al\\.", "year": 2012}, {"title": "The trec-8 question answering track report", "author": ["Voorhees", "Tice1999] Ellen M Voorhees", "Dawn M. Tice"], "venue": null, "citeRegEx": "Voorhees et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Voorhees et al\\.", "year": 1999}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["Wang et al.2007] Mengqiu Wang", "Noah A Smith", "Teruko Mitamura"], "venue": "EMNLPCoNLL", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Building a semantic parser overnight", "author": ["Wang et al.2015] Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Xu et al.2015] Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Natural language questions for the web of data", "author": ["Yahya et al.2012] Mohamed Yahya", "Klaus Berberich", "Shady Elbassuoni", "Maya Ramanath", "Volker Tresp", "Gerhard Weikum"], "venue": null, "citeRegEx": "Yahya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yahya et al\\.", "year": 2012}, {"title": "S-mart: Novel tree-based structured learning algorithms applied to tweet entity linking", "author": ["Yang", "Chang2015] Yi Yang", "Ming-Wei Chang"], "venue": "ACLIJNLP", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for opendomain question answering", "author": ["Yang et al.2015] Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Yao", "Van Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Answer extraction as sequence tagging with tree edit distance", "author": ["Yao et al.2013] Xuchen Yao", "Benjamin Van Durme", "Peter Clark"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Lean question answering over freebase from scratch", "author": ["Xuchen Yao"], "venue": "In NAACL", "citeRegEx": "Yao.,? \\Q2015\\E", "shortCiteRegEx": "Yao.", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih et al.2013] Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base. In ACL-IJCNLP", "author": ["Yih et al.2015] Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection. arXiv preprint arXiv:1412.1632", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["Zelle", "Mooney1996] John M Zelle", "Raymond J Mooney"], "venue": null, "citeRegEx": "Zelle et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Zelle et al\\.", "year": 1996}, {"title": "A joint model for question answering over multiple knowledge bases", "author": ["Zhang et al.2016] Yuanzhe Zhang", "Shizhu He", "Kang Liu", "Jun Zhao"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": ", Freebase (Bollacker et al., 2008), YAGO (Suchanek et al.", "startOffset": 11, "endOffset": 35}, {"referenceID": 29, "context": ", 2008), YAGO (Suchanek et al., 2007) and DBpedia (Auer et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": ", 2007) and DBpedia (Auer et al., 2007), KBs have become a new source of potential answers for people to mine.", "startOffset": 20, "endOffset": 39}, {"referenceID": 5, "context": "The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language.", "startOffset": 39, "endOffset": 86}, {"referenceID": 23, "context": "The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language.", "startOffset": 39, "endOffset": 86}, {"referenceID": 23, "context": "Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 27, "context": "Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 42, "context": "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al.", "startOffset": 173, "endOffset": 253}, {"referenceID": 40, "context": "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al.", "startOffset": 173, "endOffset": 253}, {"referenceID": 7, "context": ", 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015).", "startOffset": 76, "endOffset": 116}, {"referenceID": 13, "context": ", 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015).", "startOffset": 76, "endOffset": 116}, {"referenceID": 8, "context": "Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015).", "startOffset": 71, "endOffset": 117}, {"referenceID": 32, "context": "Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction approaches, and requires larger training data (this phenomenon is coined as sub-lexical compositionally by Wang et al. (2015)).", "startOffset": 230, "endOffset": 249}, {"referenceID": 2, "context": "Inspired by (Bao et al., 2014), we design a dependency tree-based method to handle such multiple-constraint questions.", "startOffset": 12, "endOffset": 30}, {"referenceID": 5, "context": "Recently, a large number of methods have been proposed to learn the mapping from relational phrases to KB relations, such as Naive Bayes based (Yao and Van Durme, 2014), logistic regression based (Berant et al., 2013), and neural network based (Yih et al.", "startOffset": 196, "endOffset": 217}, {"referenceID": 43, "context": ", 2013), and neural network based (Yih et al., 2015).", "startOffset": 34, "endOffset": 52}, {"referenceID": 24, "context": "features such as the shortest dependency path have been proven to be more concise and representative in relation extraction (Liu et al., 2015; Xu et al., 2015).", "startOffset": 124, "endOffset": 159}, {"referenceID": 34, "context": "features such as the shortest dependency path have been proven to be more concise and representative in relation extraction (Liu et al., 2015; Xu et al., 2015).", "startOffset": 124, "endOffset": 159}, {"referenceID": 14, "context": "To minimize J(\u03b8), we apply stochastic gradient descent (SGD) with AdaGrad (Duchi et al., 2011) in our experiments.", "startOffset": 74, "endOffset": 94}, {"referenceID": 20, "context": "This gives the training data for our svm-ranker (Joachims, 2006), which we hope the gold-standard entity-relation assignments rank higher than incorrect ones.", "startOffset": 48, "endOffset": 64}, {"referenceID": 5, "context": "(Berant et al., 2013) 48.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "9 (Bao et al., 2014) - - 37.", "startOffset": 2, "endOffset": 20}, {"referenceID": 7, "context": "5 (Bordes et al., 2014) - - 39.", "startOffset": 2, "endOffset": 23}, {"referenceID": 13, "context": "2 (Dong et al., 2015) - - 40.", "startOffset": 2, "endOffset": 21}, {"referenceID": 40, "context": "8 (Yao, 2015) - - 44.", "startOffset": 2, "endOffset": 13}, {"referenceID": 43, "context": "3 (Yih et al., 2015) 52.", "startOffset": 2, "endOffset": 20}, {"referenceID": 43, "context": "5 (Yih et al., 2015)(w/o ClueWeb) 50.", "startOffset": 2, "endOffset": 20}, {"referenceID": 43, "context": "2% over DeepQA, outperforming all systems including the current stateof-the-art (Yih et al., 2015).", "startOffset": 80, "endOffset": 98}, {"referenceID": 32, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 39, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 41, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 44, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 36, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 18, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 30, "context": "While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent focus has shifted to large scale structured KBs like Freebase, DBPedia (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on noisy KBs (Banko et al.", "startOffset": 189, "endOffset": 277}, {"referenceID": 5, "context": "While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent focus has shifted to large scale structured KBs like Freebase, DBPedia (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on noisy KBs (Banko et al.", "startOffset": 189, "endOffset": 277}, {"referenceID": 23, "context": "While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent focus has shifted to large scale structured KBs like Freebase, DBPedia (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on noisy KBs (Banko et al.", "startOffset": 189, "endOffset": 277}, {"referenceID": 1, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 10, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 15, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 26, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 35, "context": "An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016).", "startOffset": 138, "endOffset": 198}, {"referenceID": 16, "context": "An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016).", "startOffset": 138, "endOffset": 198}, {"referenceID": 46, "context": "An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016).", "startOffset": 138, "endOffset": 198}, {"referenceID": 5, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time.", "startOffset": 61, "endOffset": 179}, {"referenceID": 27, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time.", "startOffset": 61, "endOffset": 179}, {"referenceID": 43, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time.", "startOffset": 61, "endOffset": 179}, {"referenceID": 4, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data.", "startOffset": 96, "endOffset": 293}, {"referenceID": 25, "context": "While these methods aim to predict a relation between two entities in order to populate KBs (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering.", "startOffset": 92, "endOffset": 156}, {"referenceID": 19, "context": "While these methods aim to predict a relation between two entities in order to populate KBs (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering.", "startOffset": 92, "endOffset": 156}, {"referenceID": 28, "context": "While these methods aim to predict a relation between two entities in order to populate KBs (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering.", "startOffset": 92, "endOffset": 156}, {"referenceID": 40, "context": "Closest to our extraction method is (Yao and Van Durme, 2014; Yao, 2015) who also uses sentence level relation extraction for QA.", "startOffset": 36, "endOffset": 72}, {"referenceID": 17, "context": ", 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering. Krishnamurthy and Mitchell (2012) and Fader et al.", "startOffset": 8, "endOffset": 159}, {"referenceID": 15, "context": "Krishnamurthy and Mitchell (2012) and Fader et al. (2014) adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries.", "startOffset": 38, "endOffset": 58}], "year": 2017, "abstractText": "Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like information extraction techniques are robust to data scarcity, they are less expressive than deep understanding methods, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a novel neural network based relation extractor to retrieve the candidate answers from Freebase, and then develop a refinement model to validate answers using Wikipedia. We achieve 53.3 F1 on WEBQUESTIONS, a substantial improvement over the state-of-theart.", "creator": "LaTeX with hyperref package"}}}