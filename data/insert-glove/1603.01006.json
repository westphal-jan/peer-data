{"id": "1603.01006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Automatic learning of gait signatures for people identification", "abstract": "lenau This endonuclease work gca targets doleac people titulescu identification yivo in gorp video .1310 based on cintra the way pro-ana they walk (phule i. e. gait ). While classical cedrela methods albedos typically entrepreneurs derive kilmuir gait borknagar signatures manella from sequences of binary silhouettes, mooradian in this aquan work we explore passbooks the eucommia use of zinny convolutional neural tjilatjap networks (CNN) for nahkt learning high - level keast descriptors from low - passia level nune motion prosequi features (37.6 i. aizlewood e. optical fredrik flow components ). We mantega carry tranches out holocaust a malena thorough strategem experimental alaron evaluation stupefyingly of the proposed vereins CNN thol architecture on righthanders the challenging ribozymes TUM - GAID redshift dataset. The -52 experimental bankcorp results maezumi indicate that chinasat using shelagh spatio - summerwind temporal cuboids of sabinov optical flow meitner as m16a1 input data dominie for asmerom CNN allows to bergstroem obtain state - olympio of - the - dunwich art menou results 1-minute on 1,260 the keyuraphan gait task with an twigg image resolution ------ eight times workrate lower 1,880-mile than 335 the kaikai previously thumbtack reported taiwans results (shoko i. maglie e. whackers 80x60 pixels ).", "histories": [["v1", "Thu, 3 Mar 2016 08:07:14 GMT  (7633kb,D)", "http://arxiv.org/abs/1603.01006v1", "Proof of concept paper. Technical report on the use of ConvNets (CNN) for gait recognition"], ["v2", "Tue, 14 Jun 2016 16:07:07 GMT  (7634kb,D)", "http://arxiv.org/abs/1603.01006v2", "Proof of concept paper. Technical report on the use of ConvNets (CNN) for gait recognition. Data and code:this http URL"]], "COMMENTS": "Proof of concept paper. Technical report on the use of ConvNets (CNN) for gait recognition", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["f m castro", "m j marin-jimenez", "n guil", "n perez de la blanca"], "accepted": false, "id": "1603.01006"}, "pdf": {"name": "1603.01006.pdf", "metadata": {"source": "CRF", "title": "Automatic learning of gait signatures for people identification", "authors": ["F.M. Castro", "M.J. Mar\u0131\u0301n-Jim\u00e9nez"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "The goal of gait recognition is to identify people by the way they walk. This type of biometric approach is considered non-invasive, since it is performed at a distance, and does not require the cooperation of the subject that has to be identified, in contrast to other methods as irisor fingerprint-based approaches. Gait recognition has application in the context of video surveillance, ranging from control access in restricted areas to early detection of persons of interest as, for example, v.i.p. customers in a bank office.\nFrom a computer vision point of view, gait recognition could be seen as a particular case of human action recognition. However, gait recognition requires more fine-grained features than action recognition, as differences between different gait styles are usually much more subtle than between common action categories (e.g. \u2018high jump\u2019 vs. \u2018javelin throw\u2019) included in state-of-the-art datasets [29].\nIn last years, great effort has been put into the problem of people identification based on gait recognition [18]. However, previous approaches have mostly used hand-crafted features for representing the human gait, which are not easily scalable to diverse datasets. Therefore, we propose an end-to-end approach based on convolutional neural net-\nworks that given low-level optical flow maps, directly extracted from video frames (see Fig. 1), is able to learn and extract higher-level features suitable for representing human gait: gait signature.\nTo the best of our knowledge, this is the first work where convolutional neural networks are applied to the problem of gait identification using as input optical flow features. Therefore, our main contributions are: (i) a preprocessing stage to extract, organize and normalize low-level motion features for defining the input data; (ii) a convolutional neural network architecture to extract discriminative gait signatures from low-level motion features; and, (iii) a thorough experimental study to validate the proposed framework on the standard TUM-GAID dataset for gait identification, obtaining state-of-the-art results with video frames whose size is eight times smaller than the ones used in previously reported results.\nThe rest of the paper is organized as follows. We start by reviewing related work in Sec. 2. An overview of the fundamentals of convolutional neural networks is presented in Sec. 3. Sec. 4 explains our approach for learning gait signatures and identifying people. Sec. 5 contains the experiments and results. Finally, we present the conclusions and future work in Sec.6.\n1\nar X\niv :1\n60 3.\n01 00\n6v 1\n[ cs\n.C V\n] 3\nM ar\n2 01\n6"}, {"heading": "2. Related work", "text": "Hand-crafted features. This is the traditional representation used in gait recognition. Two main approaches stand out over the rest: silhouette-based and dense trajectoriesbased. Silhouette-based descriptors are the most used in the state-of-the-art frameworks. In this sense, the most popular silhouette-based gait descriptor is the called Gait Enery Image (GEI) [13]. The key idea is to compute a temporal averaging of the binary silhouette of the target subject. To improve the performance of gait recognition, Liu et al. [22] propose the computation of HOG descriptors from GEI and the Chrono-Gait Image (CGI). Martin-Felez and Xiang [24], using GEI as the basic gait descriptor, propose a new ranking model that allows to leverage training data from different datasets. Hu proposes in [16] the use of a regularized local tensor discriminant analysis method with the Enhanced Gabor representation of the GEI. In addition, the same author defines in [17] a method to identify camera viewpoints at test time from patch distribution features. Lately, Guan et al. [11] proposed a novel approach to deal with covariate factors (e.g. clothing, elapsed time, carrying condition, shoe type) in gait recognition using the GEI descriptor as basis. Although the use of binary silhouettes is widely extended and has shown excellent results in several scenarios, the computation of noiseless silhouettes is a critical issue, not always easy to achieve. Therefore, in this paper we choose not to use those features. Recently, an increasing number of publications based on dense trajectories have appeared in the context of action recognition in video. The main idea of these approaches is to compute short-term trajectories of densely sampled points for describing, mainly, human motion. Dense trajectories are described with the concatenation of different histograms, like Histograms of Oriented Gradients (HOG), Histograms of Optical Flow (HOF) and Motion Boundary Histograms (MBH) [31]. An alternative to this representation is the proposed by Jain et al. [19] where instead of using HOG, HOF and MBH, they use a new kind of descriptor (DivergenceCurl-Shear) based on partial derivatives of the optical flow. Finally all these trajectories are summarized at video level by using Fisher Vectors [25] as in [10]. A successful gait descriptor based on this approach is the called \u2018Pyramidal Fisher Motion\u2019 [4], which has reported state-of-the-art results on several gait datasets [3, 23]. However, it requires the application of a set of carefully selected feature extraction steps and machine learning techniques, what it is against the goal of this paper. Deep-learnt features. Traditionally, deep learning approaches based in Convolutional Neural Networks (CNN) have been used in image-based tasks with great success [21, 28, 34]. In the last years, deep architectures for video have appeared, specially focused on action recognition, where the inputs of the CNN are subsequences of stacked\nframes. In [27], Simonyan and Zisserman proposed to use as input to a CNN a volume obtained as the concatenation of frames with two channels that contain the optical flow in the x-axis and y-axis respectively. To normalize the size of the inputs, they split the original sequence in subsequences of 10 frames, considering each subsample independently. Donahue et al. [7] propose another point of view in deep learning using a novel architecture called \u201cLong-term Recurrent Convolutional Networks\u201d. This new architecture combines CNN (specialized in spatial learning) with Recurrent Neural Networks (specialized in temporal learning) to obtain a new model able to deal with visual and temporal features at the same time. Recently, Wang et al. [32] combined dense trajectories with deep learning. The idea is to obtain a powerful model that combines the deep-learnt features with the temporal information of the trajectories. They train a traditional CNN and use dense trajectories to extract the deep features to build a final descriptor that combines the deep information over time. On the other hand, Perronnin et al. [26] propose a more traditional approach using Fisher Vectors as input to a Deep Neural Network instead of using other classifiers like SVM. Although several papers can be found for the task of human action recognition using deep learning techniques, it is hard to find such type of approaches applied to the problem of gait recognition. In [15], Hossain and Chetty propose the use of Restricted Boltzmann Machines to extract gait features from binary silhouettes, but a very small probe set (i.e. only ten different subjects) were used for validating their approach. Our approach takes the idea of Simonyan and Zisserman [27] and uses a spatio-temporal volume of optical flow as input to a CNN specially designed for gait recognition."}, {"heading": "3. CNN overview", "text": "The convolutional neural network (CNN) model is an important type of feed-forward neural network with special success on applications where the target information can be represented by a hierarchy of local features (see [2]). A CNN is defined as the composition of several convolutional layers and several fully connected layers. Each convolutional layer is, in general, the composition of a non-linear layer and a pooling or sub-sampling layer to get some spatial invariance. For images, the non-lineal layer of the CNN takes advantage, through local connections and weight sharing, of the 2D structure present in the data. These two conditions impose a very strong regularization on the total number of weights in the model, which allows a successful training of the model by using back-propagation. In our approach, although we do not feed the model directly with the RGB image pixels, the CNN approach remains relevant since the optical flow information also shares the local dependency property as the pixels do.\nIn the last years, CNN models are achieving state-of-the-\nCompute OF Crop and stack CNN Classifier\nart results on many different complex applications (e.g. object detection, text classification, natural language processing, scene labeling, etc.) [6, 21, 8, 36]. However, to the extent of our knowledge, CNN has not been applied to the problem of gait recognition yet. The great success of the CNN model is in part due to its use on data where the target can be represented through a feature hierarchy of increasing semantic complexity. When a CNN is successfully trained, the output of the last hidden layer can be seen as the coordinates of the target in a high level representation space. The fully connected layers, on top of the convolutional ones, allow us to reduce the dimensionality of such representation and, therefore, to improve the classification accuracy."}, {"heading": "4. Proposed approach", "text": "In this section we describe our proposed framework to address the problem of gait recognition using CNN. The pipeline proposed for gait recognition based on CNN is represented in Fig. 2: (i) compute optical flow (OF) along the whole sequence; (ii) build up a data cuboid from consecutive OF maps; (iii) feed the CNN with OF cuboid to extract the gait signature; and, (iv) apply a classifier to decide the subject identity."}, {"heading": "4.1. Input data", "text": "The use of optical flow (OF) as input data for action representation in video with CNN has already shown excellent results [27]. Nevertheless human action is represented by a wide, and usually well defined, set of local motions. In our case, the set of motions differentiating one gait style from another is much more subtle and local. An important ques-\ntion here is whether the gait information can be decoded from simple and low resolution (e.g. 80\u00d7 60) optical flow.\nLet Ft be an OF map computed at time t and, therefore, Ft(x, y, c) be the value of the OF vector component c located at coordinates (x, y), where c can be either the horizontal or vertical component of the corresponding OF vector. The input data IL for the CNN are cuboids built by stacking L consecutive OF maps Ft, where IL(x, y, 2k\u22121) and IL(x, y, 2k) corresponds to the value of the horizontal and vertical OF components located at spatial position (x, y) and time k, respectively, ranging k in the interval [1, L].\nSince each original video sequence will probably have a different temporal length, and CNN requires a fixed size input, we extract subsequences of L frames from the fulllength sequences. In Fig. 3 we show five frames distributed every six frames along a subsequence of twenty-five frames in total (i.e. frames 1, 7, 13, 19, 25). Top row frames show the horizontal component of the OF (x-axis displacement) and bottom row frames show the vertical component of the OF (y-axis displacement). It can be observed that most of the flow is concentrated in the horizontal component, due to the displacement of the person. In order to remove noisy OF located in the background, as it can be observed in Fig. 3, we might think in applying a preprocessing step for filtering out those vectors whose magnitude is out of a given interval. However, since our goal in this work is to minimize the manual intervention in the process of gait signature extraction, we will use those OF maps as returned by the OF algorithm. Implementation details First of all, we resize the RGB\nvideo frames to a common size of 80 \u00d7 60 pixels, keeping the original aspect ratio of the video frames. Then, we compute dense OF on pairs of frames by using the method of Farneback [9] implemented in OpenCV library. In parallel, people are located in a rough manner along the video sequences by background substraction [20]. Then, we crop the video frames to remove part of the background, obtaining video frames of 60\u00d760 pixels (full height is kept) and to align the subsequences (people are x-located in the middle of the central frame, #13) as in Fig. 3.\nFinally, from the cropped OF maps, we build subsequences of 25 frames by stacking OF maps with an overlap of O% frames. In our case, we chose O = 80%, that is, to build a new subsequence, we use 20 frames of the previous subsequence and 5 new frames. For most state-of-the-start datasets, 25 frames cover almost one complete gait cycle, as stated by other authors [1]."}, {"heading": "4.2. CNN architecture for gait signature extraction", "text": "The CNN architecture we propose for gait recognition is based on the one described in [27] for general action recognition in video. However, in our case, the input has a size of 60\u00d7 60\u00d7 50, obtained from the sequence of 25 OF frames with their corresponding two channels, as explained in the previous section.\nThe proposed CNN is composed by the following sequence of layers (Fig. 4): \u2018conv1\u2019, 96 filters of size 7 \u00d7 7 applied with stride 1 followed by a normalization and max pooling 2\u00d72; \u2018conv2\u2019, 192 filters of size 5\u00d75 applied with stride 2 followed by max pooling 2\u00d7 2; \u2018conv3\u2019, 512 filters of size 3\u00d7 3 applied with stride 1 followed by max pooling 2\u00d72; \u2018conv4\u2019, 4096 filters of size 2\u00d72 applied with stride 1; \u2018full5\u2019, fully-connected layer with 4096 units and dropout; \u2018full6\u2019, fully-connected layer with 2048 units and dropout; and, \u2018softmax\u2019, softmax layer with as many units as subject identities. All convolutional layers use the rectification (ReLU) activation function. Implementation details We use the implementation of CNN provided in MatConvNet library [30]. This library\nallows to develop CNN architectures in an easy and fast manner using the Matlab environment. In addition, it takes advantage of CUDA and cuDNN [5] to improve the performance of the algorithms.\nWe perform CNN training following an iterative process to speed up and to facilitate the convergence. In this iterative process, initially, we train a simplified version of our CNN (i.e. \u2018conv1\u2019 without normalization, \u2018conv4\u2019 512 filters, \u2018full5\u2019 512 units, \u2018full6\u2019 256 units and no dropout) and, then, we use its weights for initializing the layers of a more complex version of that simpler CNN (i.e. adding normalization, 0.1 dropout and more filters and units). By this way, we train four incremental CNN versions using the previous weights until we obtain the final CNN architecture represented in Fig. 4. During the training of the CNN, the weights are learnt using mini-batch stochastic descent algorithm with momentum equal to 0.9 in the first three CNN version iterations, and 0.95 during the last one. We set weight decay to 5 \u00b7 10\u22124 and dropout to 0.4. The learning rate is initially set to 10\u22122 and divided by 10 when the validation error become stagnant. At each epoch, a minibatch of 150 samples is constructed by random selection over a balanced training set (i.e. almost same proportion of samples per class)."}, {"heading": "4.3. Classification strategies", "text": "Once we have obtained the gait signatures, the final stage consists in classifying those signatures to derive a subject identity. Although the softmax layer of the CNN is already a classifier (i.e. each unit represents the probability of belonging to a class), the fully-connected layers can play the role of gait signatures that can be used as input of a Support Vector Machine (SVM) classifier. Since we are dealing with a multiclass problem, we define an ensemble of C binary SVM classifiers with linear kernel in an \u2018one-vs-all\u2019 fashion, where C is the number of possible subject identities. Previous works (e.g. [4]) indicate that this configuration of binary classifiers is suitable to obtain top-tier results in this problem. Note that we L2-normalize the top fullyconnected layer before using it as feature vector.\nA classical alternative to discriminative classifiers is the nearest neighbour (NN) classifier, which does not require any training step. Actually, we can easily extend our gait\nrecognition system by just adding samples of the new subjects to our gallery set (i.e. the models).\nNote that in Sec. 4.1, we split the whole video sequence into overlapping subsequences of a fixed length, and those subsequences are classified independently. Therefore, in order to derive a final identity for the subject walking along the whole sequence, we apply a majority voting strategy on the labels assigned to each subsequence."}, {"heading": "5. Experiments and results", "text": "We present here the experiments designed to validate our approach and the results obtained on the selected dataset for gait recognition."}, {"heading": "5.1. Dataset", "text": "We run our experiments on the recent \u2018TUM Gait from Audio, Image and Depth\u2019 (TUM-GAID) dataset [14] for gait recognition. In TUM-GAID 305 subjects perform two walking trajectories in an indoor environment. The first trajectory is performed from left to right and the second one from right to left. Therefore, both sides of the subjects are recorded. Two recording sessions were performed, one in January, where subjects wore heavy jackets and mostly winter boots, and the second in April, where subjects wore different clothes. The action is captured by a Microsoft Kinect sensor which provides a video stream with a resolution of 640\u00d7 480 pixels with a frame rate of approximately 30 fps. Some examples can be seen in Fig. 5 depicting the different conditions included in the dataset.\nHereinafter the following nomenclature is used to refer each of the four walking conditions considered: normal walk (N), carrying a backpack of approximately 5 kg (B), wearing coating shoes (S), as used in clean rooms for hygiene conditions, and elapsed time (TN-TB-TS). Each subject of the dataset is composed of: six sequences of normal walking (N1, N2, N3, N4, N5, N6), two sequences carrying a bag (B1, B2) and two sequences wearing coating shoes (S1, S2). In addition, 32 subjects were recorded in both sessions (i.e. January and April) so they have 10 additional sequences (TN1, TN2, TN3, TN4, TN5, TN6, TB1, TB2, TS1, TS2). Therefore, the overall amount of videos is 3400.\nTo standardize the experiments performed on the dataset, the authors have defined three subsets of subjects: training, validation and testing. The training set is used for obtaining a robust model against the different covariates of the dataset. This partition is composed of 100 subjects and the sequences N1 to N6, B1, B2, S1 and S2. The validation set is used for validation purposes and contains 50 different subjects with the sequences N1 to N6, B1, B2, S1 and S2. Finally, the test set contains other 155 different subjects used in the test phase. As the set of subjects is different between the test set and the training set, a new training of the identification model must be performed. For this purpose, the au-\nthors reserve the sequences N1 to N4, from the subject test set, to train the model again and the rest of sequences are used for testing and to obtain the accuracy of the model. In the elapsed time experiment, the temporal sequences (TN1, TN2, TN3, TN4, TN5, TN6, TB1, TB2, TS1, TS2) are used instead of the normal ones and the subsets are: 10 subjects in the training set, 6 subjects in the validation set and 16 subjects in the test set.\nFor the viability of our experiments with CNN, we resized all the videos to a resolution of 80 \u00d7 60 pixels (i.e. 8 times lower resolution). Nevertheless, we will show in the experimental results (Sec. 5.4), that we obtain state-of-theart results with such low resolution, what, in our opinion, highlights the potential of CNN for gait recognition."}, {"heading": "5.2. Performance evaluation", "text": "For each test sample, we return a sorted list of possible identities, where the top one identity corresponds to the largest scored one. Therefore, we use the following metrics to quantitative measure the performance of the proposed system: rank-1 and rank-5. Metric rank-1 measures the percentage of test samples where the top one assigned identity corresponds to the right one. Whereas rank-5 measures the percentage of test samples where the ground truth identity is included in the first five ranked identities for the corresponding test sample. Note that rank-5 is less strict than rank-1 and, in a real system, it would allow to verify if the target subject is any of the top 5 most probably ones."}, {"heading": "5.3. Experimental setup", "text": "We describe here the experiments we carried out on the dataset with the proposed approach. Experiment A: gait recognition with clothing and carrying conditions. This is the core experiment of this paper, where we aim at evaluating the capacity of the proposed CNN model to extract gait signatures robust enough to deal with covariate factors as clothing changes (e.g. long coats or coating shoes) or carrying conditions (e.g. backpacks). In fact, the CNN model trained here will be used for the subsequent experiments.\nTraining of the CNN convolutional filters is carried out by using only sequences of the standard training and validation subject partitions (i.e. 100 + 50 subjects) of TUMGAID, including the three scenarios. Once the CNN model is trained with those samples, the learnt weights from layers \u2018conv1\u2019 to \u2018full6\u2019 are frozen (i.e. not modified any more). In order to evaluate the performance of the CNN-based gait signatures on the test subject partition (i.e. 155 subjects), only the softmax layer will be fine-tuned by using the training sequences of scenario \u2018N\u2019 from the test subject partition, as the subject identities have changed. However, when we use SVM or NN classifiers, no CNN fine-tuning is needed, as we will use the output of layer \u2018full6\u2019 directly as our gait signature (i.e. the automatic gait descriptor extracted from the input sequence).\nThe results of this experiment are summarized in Tab. 1, where each row corresponds to a different combination of features and classifiers: softmax \u2018SM\u2019, support vector machine \u2018SVM\u2019 and nearest neighbour \u2018NN\u2019. Each column contains the recognition results of the diverse scenarios included in the dataset (N, B, S) plus the average on the three scenarios (\u2018Avg\u2019). For completeness, we report rank-1 (\u2018R1\u2019) and rank-5 (\u2018R5\u2019) results.\nMoreover, for comparison purposes, we have implemented the \u2018Pyramidal Fisher Motion\u2019 (PFM) descriptor, as described in [4], since it does not need binary silhouettes as input for its computation and has previously reported stateof-the-art results for the problem of gait recognition [3]. Note that we have used the PFM descriptor both in the original resolution video sequences (row \u2018PFM@640 \u00d7 480\u2019) and in the low resolution version of the sequences (row \u2018PFM@80\u00d760\u2019), to allow a fair comparison with our CNNbased gait signatures that use the low resolution version. For \u2018PFM@640 \u00d7 480\u2019, we have used the whole video sequence to compute a single descriptor, as in the original paper [4]. Whereas in \u2018PFM@80 \u00d7 60\u2019, we have computed several PFM using the same set of subsequences extracted for CNN, making an even much fairer comparison. After the classification of each PFM of the sequence, majority voting was applied to obtain a final identity. Experiment B: elapsed time. The goal of this experiment is to evaluate the robustness of the CNN-based gait signatures against changes of people appearance at different periods of time. In this experiment, we apply the CNN model trained in \u2018Experiment A\u2019 on the \u2018elapsed time\u2019 subset of TUM-GAID (Sec. 5.1), which is composed of 16 subjects for training and validation, and 16 for testing. From the training sequences of the \u2018normal\u2019 scenario TN of the 16 test subjects, we obtained 10620 samples that were used to fine-tune the softmax layer of the CNN trained in the previous experiment, as the subject identities changed. Then, we used the test sequences of the three elapsed time scenarios to evaluate the performance.\nThe results of this experiment are summarized in Tab. 2, where each row corresponds to a different combination of features and classifiers, including PFM. Each column presents the recognition results of the diverse scenarios included in the elapsed time subset (TN, TB, TS) plus the average on the three scenarios (\u2018Avg\u2019). For completeness, we report rank-1 (\u2018R1\u2019) and rank-5 (\u2018R5\u2019) results. Experiment C: gait-based gender recognition. Gender recognition based on gait signatures is considered a kind of soft biometric, which allows to prune a subset of subjects for a subsequent finer identification. The goal of this experiment is to validate the quality of the gait signatures learnt in the first experiment to train a binary linear SVM for gender classification. For evaluation purposes, we train the gender classifier only on the gait sequences included in the training and validation subject partitions. In TUM-GAID, which provides labels at video level for this task, the proportion of male and female subjects in the test set is 62.6% and 37.4%, respectively.\nThe results of this experiment are summarized in Tab. 4, where we show both the confusion matrices for each scenario, plus the overall accuracy of the classifier. For comparison purposes, bottom row contains the accuracy reported for this task in paper [14]."}, {"heading": "5.4. Results and discussion", "text": "We ran our experiments on a computer with 32 cores at 2 GHz, 256 GB of RAM and a GPU Nvidia Tesla K40c, with MatConvNet library running on Matlab 2014b for Ubuntu 14.04. After splitting the training sequences (of the training subjects) into subsequences, we got a training set composed of 269352 samples used for learning the filters from \u2018conv1\u2019 to \u2018full6\u2019 layers (see Fig. 4); and a second training set composed of 108522 samples for training the softmax layer from the subset of test subjects1. With all these samples, the whole training process (from the first CNN model until the fine-tuning of the softmax layer of the final model) took about 60 hours.\nDue to the specificity of the dataset, in the training step we had to balance the number of samples of the different kinds of walking (i.e. normal, carrying a bag and wearing coating shoes). To do this, we defined different training subsets with the same number of samples of each walking scenario, and when the CNN converged, we continued the training with a different subset. At the end of the training phase, all samples of the original training set had been passed through the CNN at least twice to guarantee a good performance of the model. If this step is not performed, the CNN would learn mainly specialized filters for \u2018normal\u2019 walk, as we have four times more samples of this kind than\n1Note that TUM-GAID distinguishes between training/test subjects and training/test sequences. Test sequences are never used for training or validation of the model.\nthe others. We show in Fig. 6 the 96 convolutional filters learnt at the first CNN layer during training for \u2018Experiment A\u2019. Each row shows a pair of set of filters corresponding to the horizontal (left) and vertical (right) components of the OF. Each represented filter component spans 7 \u00d7 7 pixels. The first aspect that is appreciable in this set of filters is that there seems to be two main types of filters: filters acting as spatial derivatives, where patterns are distinguishable like in rows \u2018a\u2019 and \u2018d\u2019 (the pattern evolves along time); and, filters acting as temporal derivatives, where the mask in each frame is mainly uniform but changes its intensity along frames like in rows \u2018b\u2019 and \u2018c\u2019. These observations are shared with the\nones made by Simonyan and Zisserman in [27] applied to action datasets. The second aspect that we can perceive is the difference between x-flow filters and y-flow filters. The set of x-flow filters exhibits a structure more defined than the y-flow filters, which are more noisy and blurry. In our opinion, this difference is due to the fact that the main motion in the gait is located in the horizontal axis, as the displacement of the subject is along such axis. In contrast, vertical movements (i.e. body limbs) are softer and subtler, getting filters less defined.\nFocusing on \u2018Experiment A\u2019, the results in Tab. 1 indicate that from low resolution frames (i.e. 80 \u00d7 60) the trained CNN model is able to extract gait signatures that used in combination with standard SVM classifiers, it is attained an average of 98% rank-1 correct recognition (see row \u2018CNN-SVM\u2019), and 99.6% of rank-5 accuracy. Comparing SVM with SM, we can see that the obtained results are quite similar, although SVM accuracy is slightly better, indicating a good linear separability of the test subjects given the extracted gait signatures. For speeding-up the NN classifier, the 2048-dimensional gait descriptors were compressed with the standard principal components analysis (PCA) algorithm \u2013 vectors are L2-normalized and mean is subtracted before PCA \u2013 obtaining compact signatures of 64, 128 and 256 dimensions. The average results reported in rows \u2018CNN-NN+PCAx\u2019 are comparable to the ones yielded by the parametric classifiers (i.e. SVM and SM), making attractive the use of NN in combination with these CNNbased signatures as no training stage is needed if adding new identities to our recognition system is required. Furthermore, our proposal outperforms PFM descriptor when used on the same low resolution video sequences (i.e. 59.5% vs. 98%), although average rank-1 accuracy for PFM at full resolution is around 1% better that CNN. Nevertheless, our CNN-based signature extractor has been trained in a fully automatic manner, in contrast to the hand-crafted steps need for computing PFM. Focusing on the results on scenarios \u2018B\u2019 and \u2018S\u2019, we can conclude that our CNN signatures are\nable to successfully represent the discriminative motion patterns that characterize the different subjects regardless the clothing or shoes worn or the bags carried. Remember that we used a set of individuals totally different for training the CNN filters than the one used for testing the signatures obtained with them.\nMoving to \u2018Experiment B\u2019, the \u2018elapsed time\u2019 experiment proposed in TUM-GAID is more challenging than the previous one, as there is a temporal gap of months between recordings of the same subjects. This higher level of difficulty is reflected in the results of Tab. 2, where we directly use the CNN previously trained in \u2018Experiment A\u2019. In terms of rank-1 accuracy, SVM behaves on average better than SM, as previously reflected in \u2018Experiment A\u2019. However, NN classifiers improves on SVM, what suggests that this set of subjects is not linearly separable given the gait signatures. Comparing to PFM, only the full resolution version obtains average results slightly better than CNN, due to the good results achieved in the normal (TN) scenario. Note that results reported for TB and TS are equal or lower than the CNN ones.\nComparing our best results with previously published ones, we observe in Tab. 3 that our accuracy (rank-1) is on\na par with those methods, even though we are using video frames with a resolution eight times lower than the others. Note that our average accuracy (columns \u2018Avg\u2019) in both sets of experiments is greater than the ones reported in all the compared papers, but PFM, what emphasizes the quality of the gait signatures returned by the proposed CNN.\nFinally, the results in Tab. 4 suggest that the problem of gender recognition can be successfully addressed based on just motion features (i.e. optical flow). Regardless the scenario, accuracy for female recognition is lower than male recognition (i.e. \u2248 77% vs. \u2248 96%). This can be due to the ratio among female and male samples in the dataset. Comparing to the results reported by Hofmann et al. in [14], the average on the three scenarios (N, B, S) for our method is 88.9%, whereas the average for their method is 87.8%, despite the lower resolution of our video inputs."}, {"heading": "6. Conclusions and future work", "text": "This paper has presented a thorough study of convolutional neural networks applied to the demanding problem of people identification based on gait. The experimental validation has been carried out on the challenging dataset TUM-GAID, by using a low resolution version of the original video sequences (i.e. eight times lower). The results indicate that starting from just sequences of optical flow, the proposed CNN is able to extract meaningful gait signatures (i.e. L2-normalized top fully-connected layer) that allow to obtain high recognition rates on the available scenarios (i.e. different clothing and wearing bags), achieving state-of-the-art results, in contrast to classical approaches for gait recognition that use hand-crafted features, mainly based on binary silhouettes or dense tracklets. In terms of classification strategies, an ensemble of \u2018one-vs-all\u2019 linear SVM is a good choice, although a NN approach on PCA compressed descriptors offers similar accuracy, not requiring any training step. Finally, we have shown that our automatically learnt gait signatures are suitable for gender recognition, what would allow to filter out a set of individuals before running a finer identification procedure. As future work, we plan to extend our study to other datasets for gait recognition where multiple viewpoints are available and other CNN architectures combining OF with RGB data."}], "references": [{"title": "Frontal-view gait recognition by intra- and inter-frame rectangle size distribution", "author": ["O. Barnich", "M.V. Droogenbroeck"], "venue": "Pattern Recognition Letters, 30(10):893 \u2013 901,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning", "author": ["Y. Bengio", "I.J. Goodfellow", "A. Courville"], "venue": "Book in preparation for MIT Press,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical study of audio-visual features fusion for gait recognition", "author": ["F.M. Castro", "Mar\u0131\u0301n-Jim\u00e9nez", "N. Guil"], "venue": "In Proc. CAIP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Pyramidal Fisher Motion for multiview gait recognition", "author": ["F.M. Castro", "M. Mar\u0131\u0301n-Jim\u00e9nez", "R. Medina-Carnicer"], "venue": "In Proc. ICPR, pages 1692\u20131697,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "CoRR, abs/1103.0398,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE PAMI, 35(8):1915\u20131929,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-frame motion estimation based on polynomial expansion", "author": ["G. Farneb\u00e4ck"], "venue": "Proc. of Scandinavian Conf. on Image Analysis, volume 2749, pages 363\u2013370,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Fisher tensor decomposition for unconstrained gait recognition", "author": ["W. Gong", "M. Sapienza", "F. Cuzzolin"], "venue": "Proc. of Tensor Methods for Machine Learning, Workshop of the European Conference of Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "On reducing the effect of covariate factors in gait recognition: a classifier ensemble method", "author": ["Y. Guan", "C. Li", "F. Roli"], "venue": "IEEE PAMI, 37(7):1521\u20131528,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A robust speed-invariant gait recognition system for walker and runner identification", "author": ["Y. Guan", "C.-T. Li"], "venue": "Intl. Conf. on Biometrics (ICB), pages 1\u20138,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Individual recognition using gait energy image", "author": ["J. Han", "B. Bhanu"], "venue": "IEEE PAMI, 28(2):316\u2013322,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "The TUM Gait from Audio, Image and Depth (GAID) database: Multimodal recognition of subjects and traits", "author": ["M. Hofmann", "J. Geiger", "S. Bachmann", "B. Schuller", "G. Rigoll"], "venue": "J. of Visual Com. and Image Repres., 25(1):195 \u2013 206,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal feature learning for gait biometric based human identity recognition", "author": ["E. Hossain", "G. Chetty"], "venue": "Neural Information Processing, pages 721\u2013728,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Enhanced gabor feature based classification using a regularized locally tensor discriminant model for multiview gait recognition", "author": ["H. Hu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on, 23(7):1274\u20131286, July", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview gait recognition based on patch distribution features and uncorrelated multilinear sparse local discriminant canonical correlation analysis", "author": ["H. Hu"], "venue": "Circuits  and Systems for Video Technology, IEEE Transactions on, 24(4):617\u2013630, April", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on visual surveillance of object motion and behaviors", "author": ["W. Hu", "T. Tan", "L. Wang", "S. Maybank"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 34(3):334\u2013352,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Better exploiting motion for better action recognition", "author": ["M. Jain", "H. Jegou", "P. Bouthemy"], "venue": "CVPR, pages 2555\u20132562,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "An improved adaptive background mixture model for real-time tracking with shadow detection", "author": ["P. KaewTraKulPong", "R. Bowden"], "venue": "Video-Based Surveillance Systems, pages 135\u2013144.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple HOG templates for gait recognition", "author": ["Y. Liu", "J. Zhang", "C. Wang", "L. Wang"], "venue": "Proc. ICPR, pages 2930\u2013 2933. IEEE,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "On how to improve tracklet-based gait recognition systems", "author": ["M.J. Mar\u0131\u0301n-Jim\u00e9nez", "F.M. Castro", "A. Carmona-Poyato", "N. Guil"], "venue": "Pattern Recognition Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Uncooperative gait recognition by learning to rank", "author": ["R. Mart\u0131\u0301n-F\u00e9lez", "T. Xiang"], "venue": "Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "CVPR, pages 1\u20138. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Fisher vectors meet neural networks: A hybrid classification architecture", "author": ["F. Perronnin", "D. Larlus"], "venue": "CVPR, pages 3743\u20133752,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, pages 568\u2013576,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "UCF101: A dataset of 101 human action classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": "CRCV-TR-12-01, November", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "CVPR, pages 3169\u2013 3176,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Action recognition with trajectory-pooled deep-convolutional descriptors", "author": ["L. Wang", "Y. Qiao", "X. Tang"], "venue": "CVPR, pages 4305\u20134314,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic distance-based shape features for gait recognition", "author": ["T. Whytock", "A. Belyaev", "N. Robertson"], "venue": "Journal of Mathematical Imaging and Vision, 50(3):314\u2013326,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "CoRR, abs/1311.2901,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Silhouette-based gait recognition via deterministic learning", "author": ["W. Zeng", "C. Wang", "F. Yang"], "venue": "Pattern Recognition, 47(11):3568 \u2013 3584,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "CoRR, abs/1509.01626,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "\u2018javelin throw\u2019) included in state-of-the-art datasets [29].", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "In last years, great effort has been put into the problem of people identification based on gait recognition [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "In this sense, the most popular silhouette-based gait descriptor is the called Gait Enery Image (GEI) [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "[22] propose the computation of HOG descriptors from GEI and the Chrono-Gait Image (CGI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Martin-Felez and Xiang [24], using GEI as the basic gait descriptor, propose a new ranking model that allows to leverage training data from different datasets.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "Hu proposes in [16] the use of a regularized local tensor discriminant analysis method with the Enhanced Gabor representation of the GEI.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "In addition, the same author defines in [17] a method to identify camera viewpoints at test time from patch distribution features.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "[11] proposed a novel approach to deal with covariate factors (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Dense trajectories are described with the concatenation of different histograms, like Histograms of Oriented Gradients (HOG), Histograms of Optical Flow (HOF) and Motion Boundary Histograms (MBH) [31].", "startOffset": 196, "endOffset": 200}, {"referenceID": 18, "context": "[19] where instead of using HOG, HOF and MBH, they use a new kind of descriptor (DivergenceCurl-Shear) based on partial derivatives of the optical flow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Finally all these trajectories are summarized at video level by using Fisher Vectors [25] as in [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "Finally all these trajectories are summarized at video level by using Fisher Vectors [25] as in [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 3, "context": "A successful gait descriptor based on this approach is the called \u2018Pyramidal Fisher Motion\u2019 [4], which has reported state-of-the-art results on several gait datasets [3, 23].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "A successful gait descriptor based on this approach is the called \u2018Pyramidal Fisher Motion\u2019 [4], which has reported state-of-the-art results on several gait datasets [3, 23].", "startOffset": 166, "endOffset": 173}, {"referenceID": 22, "context": "A successful gait descriptor based on this approach is the called \u2018Pyramidal Fisher Motion\u2019 [4], which has reported state-of-the-art results on several gait datasets [3, 23].", "startOffset": 166, "endOffset": 173}, {"referenceID": 20, "context": "Traditionally, deep learning approaches based in Convolutional Neural Networks (CNN) have been used in image-based tasks with great success [21, 28, 34].", "startOffset": 140, "endOffset": 152}, {"referenceID": 27, "context": "Traditionally, deep learning approaches based in Convolutional Neural Networks (CNN) have been used in image-based tasks with great success [21, 28, 34].", "startOffset": 140, "endOffset": 152}, {"referenceID": 33, "context": "Traditionally, deep learning approaches based in Convolutional Neural Networks (CNN) have been used in image-based tasks with great success [21, 28, 34].", "startOffset": 140, "endOffset": 152}, {"referenceID": 26, "context": "In [27], Simonyan and Zisserman proposed to use as input to a CNN a volume obtained as the concatenation of frames with two channels that contain the optical flow in the x-axis and y-axis respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "[7] propose another point of view in deep learning using a novel architecture called \u201cLong-term Recurrent Convolutional Networks\u201d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[32] combined dense trajectories with deep learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] propose a more traditional approach using Fisher Vectors as input to a Deep Neural Network instead of using other classifiers like SVM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In [15], Hossain and Chetty propose the use of Restricted Boltzmann Machines to extract gait features from binary silhouettes, but a very small probe set (i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Our approach takes the idea of Simonyan and Zisserman [27] and uses a spatio-temporal volume of optical flow as input to a CNN specially designed for gait recognition.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "The convolutional neural network (CNN) model is an important type of feed-forward neural network with special success on applications where the target information can be represented by a hierarchy of local features (see [2]).", "startOffset": 220, "endOffset": 223}, {"referenceID": 5, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 20, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 7, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 35, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 26, "context": "The use of optical flow (OF) as input data for action representation in video with CNN has already shown excellent results [27].", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Then, we compute dense OF on pairs of frames by using the method of Farneback [9] implemented in OpenCV library.", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "In parallel, people are located in a rough manner along the video sequences by background substraction [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "For most state-of-the-start datasets, 25 frames cover almost one complete gait cycle, as stated by other authors [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 26, "context": "The CNN architecture we propose for gait recognition is based on the one described in [27] for general action recognition in video.", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "Implementation details We use the implementation of CNN provided in MatConvNet library [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "In addition, it takes advantage of CUDA and cuDNN [5] to improve the performance of the algorithms.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "[4]) indicate that this configuration of binary classifiers is suitable to obtain top-tier results in this problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We run our experiments on the recent \u2018TUM Gait from Audio, Image and Depth\u2019 (TUM-GAID) dataset [14] for gait recognition.", "startOffset": 95, "endOffset": 99}, {"referenceID": 3, "context": "Moreover, for comparison purposes, we have implemented the \u2018Pyramidal Fisher Motion\u2019 (PFM) descriptor, as described in [4], since it does not need binary silhouettes as input for its computation and has previously reported stateof-the-art results for the problem of gait recognition [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 2, "context": "Moreover, for comparison purposes, we have implemented the \u2018Pyramidal Fisher Motion\u2019 (PFM) descriptor, as described in [4], since it does not need binary silhouettes as input for its computation and has previously reported stateof-the-art results for the problem of gait recognition [3].", "startOffset": 283, "endOffset": 286}, {"referenceID": 3, "context": "For \u2018PFM@640 \u00d7 480\u2019, we have used the whole video sequence to compute a single descriptor, as in the original paper [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 13, "context": "For comparison purposes, bottom row contains the accuracy reported for this task in paper [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "ones made by Simonyan and Zisserman in [27] applied to action datasets.", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "6 4 0 \u00d7 4 8 0 SDL [35] - - - - 96.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "9 - - GEI [14] 99.", "startOffset": 10, "endOffset": 14}, {"referenceID": 32, "context": "SEIM [33] 99.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "6 GVI [33] 99.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "9 SVIM [33] 98.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "0 RSM [12] 100.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "3 PFM [4] 99.", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "[14] (640 \u00d7 480) 95.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "in [14], the average on the three scenarios (N, B, S) for our method is 88.", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "This work targets people identification in video based on the way they walk (i.e. gait). While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e. optical flow components). We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset. The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e. 80\u00d7 60 pixels).", "creator": "LaTeX with hyperref package"}}}