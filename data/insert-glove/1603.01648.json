{"id": "1603.01648", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Getting More Out Of Syntax with PropS", "abstract": "nasuta Semantic NLP applications meur often embarrassment rely on dependency balancers trees to kibbey recognize snes major 2040 elements idlewild of the batukayev proposition 79.07 structure salvio of odioma sentences. 227.6 Yet, shigenori while much 500.00 semantic ancom structure dholloway is highlighter indeed expressed dubrow by scanian syntax, many phenomena are 4x50 not easily read out of dependency trees, 800-1000 often leading canclini to further unplugged ad - sangam hoc blister heuristic segev post - processing 2:33 or to maranhao information loss. peroxidation To directly linked address the needs chunuk of oliy semantic pluralization applications, rrustem we present PropS - - demel an output tayyib representation designed queuing to explicitly totalfina and uniformly picea express mool much horseflesh of the proposition sawarka structure which hasbaya is implied 43.02 from syntax, devolves and wangchen an associated tool fraysse for extracting 3,597 it bead from laverick dependency trees.", "histories": [["v1", "Fri, 4 Mar 2016 22:47:46 GMT  (246kb,D)", "http://arxiv.org/abs/1603.01648v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gabriel stanovsky", "jessica ficler", "ido dagan", "yoav goldberg"], "accepted": false, "id": "1603.01648"}, "pdf": {"name": "1603.01648.pdf", "metadata": {"source": "CRF", "title": "Getting More Out Of Syntax with PROPS", "authors": ["Gabriel Stanovsky", "Jessica Ficler", "Ido Dagan", "Yoav Goldberg"], "emails": ["gabriel.satanovsky@gmail.com", "jessica.ficler@gmail.com", "yoav.goldberg@gmail.com", "dagan@cs.biu.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Propositions, statements for which a truth value can be assigned (e.g., \u201cBob loves Mary\u201d), constitute the primary unit of information conveyed in texts. Accordingly, recognizing and matching proposition structure is a central component in systems and algorithms that attempt to extract semantic information from text, such as question answering, summarization, or recognizing textual entailment, collectively referred as semantic applications.\nSeveral practices exist for recovering the proposition structure of sentences. Dependency trees (de Marneffe and Manning, 2008b) are attractive as they directly connect verbal predicates to their arguments, while deep syntax extensions of dependency trees also mark long distance dependencies, further broadening predicate-argument coverage (Ballesteros et al., 2014). Other annotations choose a more semantic approach. Notable examples are Semantic Role Labeling (SRL) (Carreras\nand Ma\u0300rquez, 2005a), which extracts frames linking predicates with their semantic arguments, or, more recently, Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which aims to extract a graph representation capturing the semantic structure of the sentence.\nCurrently, dependency trees are perhaps the most commonly used representation, mainly for the high accuracy with which they can be obtained, in contrast to the relatively lower performance for current SRL systems and AMR parsers.\nDespite the attractive properties of (deep) dependency parses, it is quite hard to read out from them the complete structure of all propositions expressed in a sentence, for several reasons: (1) Different predications are represented in a non-uniform manner (e.g, passive vs. active, verbal vs. adjectival predication, appositions vs. copula) (2) Proposition boundaries are not easy to detect; and (3) Substantial parts of the dependency structure represent syntactic detail that is not core to proposition structure. For these reasons, it is common for NLP systems to tailor sets of rules and heuristics to unify and extract specific information from the parse structure. While these heuristics are created mostly in an ad-hoc manner and differ from application to application (e.g., (Culotta and Sorensen, 2004)), the possibility of creating such heuristics rests on rather strong linguistic grounds \u2013 the syntactic structure reliably expresses a wide range of predications. This stands in contrast with the deeper analysis required in order to obtain semantic annotations such as AMR or SRL, which are not readily attainable from syntax.\nOur goal in this work is to alleviate the need for tailoring such heuristic post-processing of parse trees on a case by case basis. We do this by provid-\nar X\niv :1\n60 3.\n01 64\n8v 1\n[ cs\n.C L\n] 4\nM ar\n2 01\ning a convenient output format which directly represents the proposition structure that can be induced from syntax, while following several desired design principles. In particular, we uniformly represent propositions headed by different types of predicates, verbal or not. We canonicalize different syntactic constructions that correspond to the same proposition structure, and decouple independent propositions while clearly marking proposition boundaries. Finally, the scheme masks non-core syntactic detail, yielding cleaner compact structures. Overall, we enable simple access to the represented propositions by a uniform graph traversal, offering an appealing alternative input to common uses of dependency structures.\nWe bundle an implementation of these principles in PROPS (Proposition Structure) \u2013 an automatic converter which takes as input Stanford dependency trees, and outputs proposition structures with our notation. In most cases. PROPS stays close to the syntactic level, keeping the relatively high accuracy and robustness levels currently attainable by syntactic parsing, while still providing a significant improvement over the bare syntactic structure for downstream semantic processing. In some other cases, where information is currently not reliably produced by automatic parsers (such as difficult conjunctions or differentiating between raising and control), PROPS applies a set of heuristics to obtain the output structure. We demonstrate the out-of-thebox attractiveness of PROPS on a reading comprehension task.\nIn addition to providing an automatic tool to convert dependency trees we also provide a large semiautomatic annotation of the WSJ portion of the Penn Tree Bank (PTB) (Bies et al., 1995). This recovers our structures with higher accuracy, and includes an accurate handling for the previously mentioned semantically hard decisions. We do so by utilizing gold phrase-based annotations and other annotated resources, such as Propbank (Kingsbury and Palmer, 2003) or Vadas and Curran\u2019s (2007) NP structure annotation. We intrinsically test the quality of our predicted structures over the PTB by measuring their accuracy against a small manually annotated subset.\nBy identifying and clearly enumerating the typical properties that semantic applications would need from syntactic structures, we hope to trigger re-\nsearch on an application-oriented syntactic middleground, as opposed to current mostly ad-hoc and under-documented post-processing solutions."}, {"heading": "2 Design Principles", "text": "We focus our output representation around the proposition structure needed for semantic applications, instead of the syntactic focus of dependency trees. To that end, we suggest five desired structural principles that we find missing or lacking in dependency trees. This section presents these principles, as well as consequent formulation of the PROPS output format. The next two sections describe the corresponding dependency tree conversions implemented in PROPS. Section 3 describes the bigger bulk of conversions. These can be realized with high precision in a principled way from dependency trees attainable from current dependency parsers. Few cases, in which we applied heuristics to address some harder decisions, are described in Section 4.\nMasking non-core syntactic detail (Section 3.1) We want to focus the application on the semantic components of the sentence. While dependency trees are often over specified, and include a node for every token, we would like to remove auxiliary words and instead encode their syntactic function as features in a canonicalized way. Additionally, we would like to group atomic units (such as noun compounds) within a single node.\nRepresenting propositions in a uniform manner (Section 3.2) We would like the application to uniformly access a wide range of propositions, headed by different types of predicates, unlike the verbcentric representation of input dependency trees.\nCanonicalizing and differentiating syntactic constructions (Sections 3.3, 4.1) To ease the semantic handling for downstream applications, we want our structures to (1) Unify the representation of propositions which are semantically equivalent, yet look different in dependency trees, due to syntactic subtleties, and (2) Differentiate syntacticallysimilar, yet semantically-different, constructions.\nMarking proposition boundaries (Section 3.4) Clearly marking the minimal span of a standalone proposition and its elements (predicate, arguments, and modifiers) is beneficial for semantic tasks (e.g.,\n(Angeli et al., 2015)), and not trivially available in depdendency trees.\nPropagating Relations (Section 4.2) We would like that every relation which is inferable through parse tree traversal (for instance, through conjunctions) would be explicitly marked in our representation. This way we can save an application the need to perform subsequent passes and propagations over the input representation."}, {"heading": "2.1 Output Format", "text": "To achieve our desired principles, we choose a representation format that resembles dependency structures, with the following changes:\nTyped nodes In order to clearly identify the propositions in our structures, we differentiate between two types of nodes (compared with syntactic dependencies, where there is only one type of nodes): (1) Predicates, which evoke a proposition and (2) Non-predicates, which can be either arguments or modifiers. Additionally, we delegate function words, such as modality and tense, to features of nodes.\nFigure 1 depicts the sentence \u201cBarack Obama, the young candidate, was elected president\u201d in dependency representation (top) verus PROPS output (bottom). PROPS clearly marks the predicates elect, young and SameAs (representing apposition) as shaded nodes in the graph, along with their features (for instance, past tense indicated in subscript for the predicate elect), and their direct arguments and modifiers.\nBreaking the correspondence between nodes and words We simplify the graph structure by allowing multi-word nodes (e.g., Barack Obama), versus having each node corresponding to a single word in dependency trees. In some cases (as in the SameAs node), nodes do not correspond to specific words in the sentence (see Section 3.2).\nGraph Structure Similar to the deep variant of dependency trees, our resulting structures are no longer limited to trees. Instead, our structures are directed graphs, as seen in Figure 1.\nFocused edge label set In order to further simplify the reading of the graph, we introduce a label set of 14 relations (compared with approximately 50 in Stanford dependencies). These enable the user to focus on a more general set of relations between the proposition elements. See table 1 for the complete inventory, along with an index to examples in the paper."}, {"heading": "3 The PROPS Converter", "text": "In this section we describe the transformations carried out in PROPS, a rule-based converter of Stanford dependency-trees.1 These transformations fulfill the guiding principles described in the previous section. Specifically, we target phenomena which we find to be both feasibly attainable from dependency trees, as well as common enough2 to be of\n1We will make the code available upon acceptance. 2As we identified by frequency analyses over the PTB.\npractical significance for semantic applications, as demonstrated in Section 7.1."}, {"heading": "3.1 Masking Non-Core Syntactic Detail", "text": "In addition to single word nodes, we produce structures containing multi-word nodes, for cases of multi-word predicates (e.g., take a picture) or noun compounds (e.g., New York). In certain syntactic formalisms, these constructs are represented as multiple nodes joined by some designated label. In particular, PROPS relies on the nn and mwe relations in Stanford dependencies for noun phrases and multiword expressions, respectively. Joining such entities in a single node reduces the size of the tree, reduces the label set, and overall simplifies its processing.\nAdditionally, we write words in their base form and encode features for modality, negation, definiteness, tense, and passive or active voice for each node in a flat key=value structure (e.g., \u201cwas elected\u201d appears as a single elect node with a past tense feature in Figure 1). Recovering such properties can be done with high accuracy based on lexical and syntactic cues in a few deterministic rules (see Section 7.1 for an intrinsic evaluation). These feature encodings provide abstraction over the different ways in which the features are realized in the surface forms. In addition, we unify markers which are not traditionally thought of as tense. For example, we reduce constructions such as going to + verb to indicate future tense (e.g., in \u201cI am going to dance\u201d)."}, {"heading": "3.2 Representing Propositions in a Uniform Manner", "text": "Beyond verbal predicates, we deal with three common types of predications which do not adhere to the simple verbal-oriented representation in dependency trees. We re-arrange the tree structure such that each predicate heads its arguments and modifiers.3\nAdjectival predication While syntactic analyses treat adjectives as modifiers, we explicitly represent also their propositional meaning (for example sentences such as \u201cthe boy is tall\u201d and \u201cthe tall boy\u201d evoke the proposition tall(boy)). Adjectival propositions are connected using the \u201cprop of\u201d relation, denoting the 1-ary predication. We also retain the modification status of an adjective using the \u201cmod\u201d rela-\n3The rather complex case of nominal predication is left to be integrated in future work.\ntion, which we will elaborate on in Section 3.3. This results in the following structures:\n(1) She said that the boy is tall (2) She saw a tall boy\nShe say boy tall\nprop ofsubj comp\nShe see boy tall\nprop of\nmodsubj dobj\nNotice that in (2) we mark \u201ctall\u201d as a modifier of \u201cboy\u201d for the completeness of the see(she,tall boy) proposition, but \u201ctall\u201d is also used as a predicate in a separate tall(boy) proposition. Non-lexical propositions Some propositions do not correspond to a specific word in the sentence, but are rather implied from the syntactic structure. We introduce handling for two such recurring examples: appositions and existentials.\nAn appositive construction, like \u201cBarack Obama, the U.S. president, lives in Washington\u201d, implies that \u201cBarack Obama is the U.S. president\u201d, without a word mediating this relation in the sentence. We deal with such cases by introducing a synthetic SAMEAS node for apposition (and copular) constructions. Similarly, we introduce an EXISTS node for existentials (example 4).\n(3) Barack Obama, the U.S. president, lives in Washington\nlive Present\nBarack Obama SAMEAS U.S president\nlive Present\nWashington\nprep-in\nprep-in\nsubjSameAs arg\nSameAs arg mod\nsubj\n(4) She says that there are signs for rain\nShe say EXISTS signs rain\nsubj comp subj prep-for\nConditionals Conditionals are a syntactic construction in which a word (syntactically referred as \u201cmarker\u201d) introduces a logical relation between two discrete propositions, where one of the clauses conditions the other. While there has been extensive linguistic study of the subject (Stalnaker, 1981; Bennett, 2003), NLP applications tend to overlook these constructions (a recent exception is (Berant et al., 2014)).\nWe treat conditional constructions as propositions, as they can be assigned a truth value. We consider the marker word (e.g, \u201cbecause\u201d, \u201calthough\u201d,\n\u201cunless\u201d, etc.) as a predicate, taking as argument both clauses, using the \u201ccondition\u201d and \u201coutcome\u201d labels, as it introduces the logical relation between them (example 5).\n(5) (a) If you build it, they will come (b) They will come, if you build it\nif you build Present it they come Future\noutcome\ncondition\nsubj dobj\nsubj"}, {"heading": "3.3 Canonicalizing and Differentiating Syntactic Constructions", "text": "Different syntactic realizations may evoke the same proposition structure (e.g., passive versus active voice), while sentences with similarly looking syntactic structures may evoke different proposition structures.We aim to present a unified proposition structure when applicable, and different structures when needed. The major cases we consider are detailed below.\nAdjectival modification We unify restrictive adjectival modifications that are expressed as prenominal adjectives (\u201cbroken pipe\u201d) or as relative clauses (\u201cpipe which was broken\u201d), representing both using the \u201cmod\u201d relation.\n(6) (a) The janitor didn\u2019t fix the broken pipe (b) The janitor did not fix the pipe which was broken\njanitor fix Negated broken pipe\nsubj dobj\nmod\nprop of\nAdjectival predication We provide a unified representation for adjectival predications, which are represented using a \u201cprop of\u201d label connecting a predicate node to its argument. This structure is evoked by adjectives (\u201cJohn is nice\u201d) and adjectival phrases (\u201cJohn is a nice man\u201d) in the non-restrictive cases of prenominal adjectives, relative clauses, appositive and copular constructions.\n(7) (a) John, who is a nice man (b) John is a nice man (c) John, a nice man\nJohn nice man\nprop of mod\nprop of\nIn a more subtle canonicalization, we follow Huddleston et al (2002), and unify, under the above rep-\nresentation, adjectival complements as also evoking an adjectival predication.\n(8) (a) You looked very impatient yesterday (b) You are impatient\nyou look Past very impatient yesterday\nmod source\nprop of\ntime\nyou impatient\nprop of\nAdjectival complements occur where an adjective appears as a complement of a verbal predicate. For instance, in the sentence: \u201cyou look impatient\u201d. Dependency representation marks the verb (look) as the head of the sentence, and the adjective (impatient) as depending on it.\nInstead, in PROPS we mark the adjective as the predicate, and the verb as a modifier. We introduce a \u201csource\u201d edge indicating this modification relation. This analysis focuses the attention on the main predicate and maintains a uniform predication structure, as demonstrated by the structural resemblance to \u201cYou are impatient\u201d (example 8).4\nAdjectival complements are similar to raisingto-subject constructions (\u201cyou seem to be impatient\u201d). Indeed, we make an attempt to represent both cases in a similar manner, having impatient as the main predicate and seem as \u201csource\u201d. Unfortunately, distinguishing raising from control constructions is more nuanced, and is not handled by current syntactic parers. We handle this heuristically in PROPS, as described in Section 4.1. Copulas and appositions We would like to canonicalize the semantically similar cases of copulas and appositions. Syntactic copular (\u201cX is Y\u201d) and appositive (\u201cX, Y\u201d) constructions may both evoke either an equivalence relation (\u201cObama, the president\u201d; \u201cObama is the president\u201d) or class - subclass / membership relation (\u201cObama, an american citizen\u201d).\nThe different cases are distinguished based on the syntactic categories and definiteness status of both X and Y, regardless of whether an apposition or a copula appeared in the surface form.\n4While one may think of modality, and hence \u201csource\u201d, as being a feature of a predicate rather than a relation, the relation is needed for cases where the modality itself has a rich internal structure, as seen in example (8).\nWe represent the equivalence case using the SAMEAS predication node (example (3) above) and the membership case as an adjectival predication (as in example (7)).5"}, {"heading": "3.4 Marking Proposition Boundaries", "text": "While syntactic dependency representations contain much of the proposition structure, they do not clearly mark the boundaries of different propositions and their arguments, making it hard to focus on individual propositions. We use restrictive versus nonrestrictive modification to properly bound the scope of arguments (Kamp and Reyle, 1993).\nConsider, for instance, the following sentences: -\u201cThe director who edited \u2018Rear Window\u2019 released Psycho\u201d -\u201cHitchcock, who edited \u2018Rear Window\u2019, released Psycho\u201d. While syntactic analysis will assign similar structure for both instances, each induces different proposition boundaries. In the first sentence, the director who edited Rear-Window specifies the minimal scope of a single argument, yielding the proposition released(the director who edited Rear Window, Psycho). However, in the second sentence we can separate the relative clause from the entity it modifies, yielding two distinct propositions: edited(Hitchcock, Rear Window) and released(Hitchcock, Psycho).\nThe difference between the two cases is that the first exhibits a restrictive while the second exhibits a non-restrictive modification. Restrictive modifiers are used to select an item from a set and are an integral part of the proposition. We represent these with a \u201cmod\u201d edge (example 9). Non-restrictive modification, on the other hand, \u201cbreaks\u201d the propositions and provides additional information on a selected entity, therefore forming a separate predication (example 10).\nSimilar to the differentiation we make in copulas and appositions, we identify restrictive modification by definiteness status of the entity being modified.\n(9) the director who edited \u2018Rear Window\u2019 released Psycho\nRear Window edit Past director release Past Psycho\nsubj\nmod\nobj subj obj\n5The \u201cprop of\u201d relation is used for both adjectival complements (\u201cJohn is beautiful\u201d) as well as noun complements (\u201cJohn is a citizen\u201d).\n(10) Hitchcock, who edited \u2018Rear Window\u2019, released Psycho\nRear Window edit Past Hitchcock release Past Psycho\nsubjobj subj obj"}, {"heading": "4 Heuristically Dealing with Harder Cases", "text": "One of the guiding principles for choosing the phenomena described in Section 3 is the ability to provide principled rules for handling them based on automatic parsing output. Yet, certain phenomena are difficult to directly identify from the output of current parsers, and fall out of this scope. However, we find that by applying a few heuristics we can target them with relatively high accuracy, and provide downstream semantic applications with a more practical representation.\nWhile not currently handled by syntactic parsers, most of these distinctions are annotated in existing linguistic datasets. In Section 6 we make use of these annotations and thus are able to provide a goldstandard corpus that covers these phenomena reliably."}, {"heading": "4.1 Differentiating Raising-to-Subject from Control", "text": "Following Huddleston et al (2002)\u2019s analysis, we would like to canonicalize syntactically-different sentences like:\n(11)\nYou seem impatient\nnsubj acomp\n(12)\nYou seem to be impatient\nnsubj aux xcomp acomp\nIdeally, in both cases we would like the complement impatient to become the main predicate, and mark the verb seem as its modifier, as we previously analyzed for adjectival complements (example 8). However, using the output of syntactic parsers it is hard to differentiate example 12 from cases such as example 13, in which we would like to keep the nested dependency representation.\n(13)\nYou want to be impatient\nnsubj aux xcomp acomp\nLinguistic theories identify example 12 as raising-to-subject, differentiating them from con-\ntrol constructions (e.g., example 13), by the correspondence between syntactic and semantic arguments (Davies and Dubinsky, 2008) (i.e., in example 13 \u201cyou\u201d is a syntactic and a semantic argument of want, while in 12 \u201cyou\u201d is only a syntactic argument of seem). However, current syntactic parsers and annotations standards do not make this distinction.\nIn order to identify raising-to-subject constructions in PROPS, we heuristically use a set of approximately 30 verbs which were found by (Chrupa\u0142a and van Genabith, 2007) to frequently occur in raising constructions (including verbs such as \u201cseems\u201d, \u201clooks\u201d, \u201cremains\u201d, etc.). For these verbs, similarly to the handling of adjectival complements, we do not produce a proposition, but instead use a \u201csource\u201d edge indicating a modification relation."}, {"heading": "4.2 Propagating Relations", "text": "In some cases, most notably in coordination constructions, predicate-argument relations can be propagated according to the syntactic structure, resulting in new propositions. For example in \u201cDell makes and distributes products\u201d the propagation results in the three propositions make(Dell, products), distribute(Dell, products) and make and distribute(Dell, products). Similarly, for \u201cDell sells laptops and servers\u201d we get sell(Dell, laptos), sell(Dell, servers) and sell(Dell, laptops and servers).\nOur representation of coordination follows the Prague-Treebank style (Popel et al., 2013; Bo\u0308hmova\u0301 et al., 2003) and posits the coordinating word as the main node of the coordinating-conjunction structure, while the different conjuncts are attached to it using \u201cconj\u201d edges.\n(14) Kim and Pat speak Spanish / Kim and Pat are a couple\nKim and Pat speak Spanish\nconj-and conj-and\nsubj subj\nsubj\ndobj dobj\ndobj\nKim and Pat couple\nprop of\nconj-and conj-and\nThe semantic of coordination nodes is that the node represents the entire conjunction. Following relation propagation, relations may involve the coordination node (i.e. the entire coordinated structure) and/or the individual conjuncts.\nA similar approach is taken by the \u201cpropagated\u201d variant of Stanford Dependencies (de Marneffe and\nManning, 2008a), but we differ in choosing the coordination as the main element in the conjunction, and take more care in distinguishing different kinds of coordination constructions and determining the conjunction scope. As one example, we distinguish between distributive and joint coordination (Huddleston et al., 2002). In distributive coordination (\u201cKim and Pat speak Spanish\u201d) we fully propagate the relations, while for cases of joint coordination (\u201cKim and Pat are a couple\u201d)6 we do not propagate, as the relation is meant only for the combination of both entities. When the relation propagation results in multiple propositions involving the same predicate, the predicate node is duplicated in order to distinguish the different propositions and keep with the 1-1 correspondence between propositions and predicate nodes.\nWe also propagate relations in appositive and copular constructions involving the SAMEAS node (example 3), e.g. \u201cAmazon, the retail giant, sells products\u201d will evoke both sell(Amazon, products) and sell(retail giant, products)."}, {"heading": "4.3 Syntactic assertedness", "text": "Certain propositions are asserted by the sentence, while others are attributed (compare \u201cJohn passed the test\u201d versus \u201cthe teacher said that John passed the test\u201d). This property was extensibility studied in the PDTB (Prasad et al., 2006) and PARC (Pareti, 2012) corpora, and was shown to be useful in semantic applications (e.g., (Aramaki et al., 2009)).\nUsing a dedicated node feature, we mark cases where it is possible to syntactically determine if a proposition is asserted. While hierarchical structure generally implies dependence of the nested proposition, in certain constructions the nested proposition is in fact independent of its subordinating clause. This is the case for non restrictive relative clause (\u201cAlfred Hitchcock, who directed Psycho\u201d in which the directed proposition is syntactically asserted), certain conditionals (\u201cGlaciers are melting because temperature rises\u201d in which both rise and melt predications are asserted), and several other constructions.\n6Distinguished, in this case, by the copula are in the second sentence."}, {"heading": "5 Concrete Example", "text": "We conclude the discussion of PROPS structures with a final example, taken from the PTB, which exhibits several of our transformations. Figure 2 compares PROPS with Stanford-dependency analysis for a typical WSJ sentence. The dependency representation contains 27 nodes (and therefore 27 edges), while our representation contains 17 nodes and 19 edges. A simple traversal from all predicate nodes in our graph yields the following propositions:\n(1) lower wine prices have come about [asserted] (2) hit wine dramatically increase in price (3) producers see (2) (4) producers don\u2019t like (3) [asserted] (5) Mr Pratt is the head of marketing [asserted] (6) (1) happens because of (4) (7) Mr Pratt thinks that (6) [asserted] (8) the head of marketing thinks that (6) [asserted]\nIn contrast, extracting this set of propositions and their assertion status from the dependency tree requires a non-trivial amount of processing."}, {"heading": "6 Annotated Corpus", "text": "In addition to the PROPS converter described in previous sections, we also created a large PROPS corpus, which extracts with high precision both the core structures described in Section 3, as well as the harder cases described in Section 4.7\nTo obtain this corpus we adapted the conversions described in Section 3 and ran them on full gold constituency trees, available from the Penn Tree Bank (PTB) (Bies et al., 1995). Using the PTB along with Propbank (which also annotated the WSJ), allows us\n7The corpus will be made available upon publication.\nto explicitly recover some of the phenomena which were only heuristically handled in the PROPS converter (Section 4). In order to recover argument propagation and conjunction handling we make use of the rich syntactic information available in the gold trees (including traces, empty elements and functional annotations). To distinguish raising-to-subject from control, we made use of Propbank annotations, as depicted in (Chrupa\u0142a and van Genabith, 2007)."}, {"heading": "7 Evaluation", "text": "We evaluate the PROPS converter intrinsically (by matching its output to a gold standard) and extrinsically (by demonstrating the utility of the corresponding structure in a text-comprehension task)."}, {"heading": "7.1 Intrinsic Evaluation", "text": "In order to intrinsically test the performance of PROPS, we manually annotated a batch of 100 sentences of the WSJ, composed of the first 50 consecutive sentences of sections 2 (train) and 22 (dev). The annotation was performed by manually verifying and fixing the output of the automatic conversion of the WSJ, described in Section 6.\nIn order to test our labeled attachment accuracy we adapt the metric used in Ballesteros et al (2014) to account for PROPS\u2019 non 1-1 correspondence between nodes and sentence tokens (see Section 2). To this end, we define an edge as a triplet of (head-span, mod-span, label), where head-span and mod-span are each an ordered list of token indices that contribute to the node.8 For an edge to be considered\n8For example, an edge of the form ((1, 2, 3), (4, 5), subj) indicates that a node over tokens 1-3 is the head of a node over tokens 4-5 with a label subj.\ncorrect, both the nodes and the label should match. More formally, given a gold reference graph g, its approximation g\u0302, and the gold standard corpus GOLD (a list of reference graphs), we take the list of edges E(g) = {(i1, ..., ik), (j1, ..., jl), label}, and calculate precision and recall metrics between E(g) and E(g\u0302), in the following manner:\nPlas = \u03a3g\u2208GOLD|E(g) \u2229 E(g\u0302)|\n\u03a3g\u2208GOLD|E(g\u0302)| (1)\nRlas = \u03a3g\u2208GOLD|E(g) \u2229 E(g\u0302)|\n\u03a3g\u2208GOLD|E(g)| (2)\nWe take a similar approach for computing feature accuracy.9 Given the list of features F (g) = {(i1, ..., ik), (key, value)} we calculate feature precision and recall metrics between F (g) and F (g\u0302) :\nPfeat = \u03a3g\u2208GOLD|F (g) \u2229 F (g\u0302)|\n\u03a3g\u2208GOLD|F (g\u0302)| (3)\nRfeat = \u03a3g\u2208GOLD|F (g) \u2229 F (g\u0302)|\n\u03a3g\u2208GOLD|F (g)| (4)\nResults We tested both the WSJ conversion (against the gold standard), as well as our automatic tool (against the gold standard and against the entire WSJ conversion). The results are shown in table 2.\nWe find that the WSJ conversion performs with 91% F1 score of for labeled attachment, and 96% F1 score for feature computation. Note that our evaluation metric is rather harsh, as a small node segmentation error will penalize both in terms of recall and precision all of the edges that include the incorrectly segmented nodes.10\nFurther manual inspection of the errors revealed that the vast majority (> 80%) stem either from errors in the underlying annotations, or disagreement about annotation standards (e.g. PropBank marks \u201cbusiness\u201d in \u201cthe business grew\u201d as an object, while we treat it as a subject). This high accuracy of the automatic conversion, which stems from the use of the manual annotations, warrants its use as a large\n9To be clear, a sentence fragment such as \u201cdid not walk\u201d will result in a node \u201cwalk\u201d with features tense=past, negated=True, covering three sentence tokens.\n10For example, failing to combine \u201cNew England Journal of Medicine\u201d into a single node in \u201cresults appear in today\u2019s New England Journal of Medicine\u201d is penalized with 6 edge errors.\nreference corpus for the development and evaluation of future parsers for PROPS.\nAdditionally, we test the accuracy of converting PROPS over automatic dependency trees derived by running the Stanford converter on top of the Berkeley parser (Petrov and Klein, 2007). While the PROPS parser is only capable of producing an approximation of the complete representation and is bounded by the accuracy of the underlying dependency parser (82%F1 for labeled attachment and 89%F1 for feature computation on the gold standard), it does succeed in recovering the major parts of the structure, as we demonstrate in the following section."}, {"heading": "7.2 Extrinsic Evaluation", "text": "To extrinsically evaluate our automatic converter we use the MCTest corpus for machine comprehension (Richardson et al., 2013), composed of 500 short stories, each followed by 4 multiple choice questions. The MCTest comprehension task does not require extensive world knowledge. This makes it ideal for testing underlying representations, as performance will mostly depend on accuracy and informativeness of the representation. We focus our tests on questions which are marked in the corpus as answerable from a single sentence in the story (905 questions followed by 3620 candidate answers).\nOur goal in this evaluation is to exemplify the usability of PROPS in an out-of-the-box application. We therefore focus on a simple format-independent algorithm, which allows us to control our evaluation around the underlying representation. The intention is to emphasize the attractive features of PROPS for semantic applications, rather than providing an elaborate, state-of-the-art text comprehension algorithm.\nRichardson et al (2013) introduce a lexical matching algorithm, which we adapt to use either dependency or PROPS structures. They convert a question and a possible answer into a candidate answer (CA)\n(e.g., \u201cBilly is the name of the boy\u201d is obtained from the question/answer pair \u201cwhat is the name of the boy?\u201d/\u201cBilly\u201d), counts lexical matches between CA and a sliding window over the story, and returns the answer with the maximal number of matches.\nOur adaptation works by counting matches of representation-units instead of words: The CA is compared against each story sentence (instead of a sliding window), and instead of counting lexical matches counts the number of identical representation-units in the CA and the sentence.11 Table 3 shows the adaptation is effective: using dependencies improve over the lexical baseline reported in the MCTest paper, and using PROPS further improves results over dependency trees.\nPROPS outperforms dependencies in answering questions involving phenomena addressed by our framework (e.g., in appositives: \u201cJohn and his best friend, Rick, shared their love for peaches.\u201d to answer: \u201cWhat did John and Rick both love?\u201d, or adjectival complements as in \u201cshe was wearing a costume that looked like a kitten\u201d to answer the question \u201cWhat did Tessa\u2019s costume look like?\u201d)."}, {"heading": "8 Related Work", "text": "Abstracting away from syntax and providing a more semantically-oriented sentence representations has been the focus of several research efforts. In terms of the produced outputs, perhaps the most similar to ours is UCCA (Abend and Rappoport, 2013). They proposed a new representation scheme geared towards semantic applications and a small accompanying gold standard corpus. However, UCCA differs in their main motivations. Namely they aim to be a cognitive interlingua representation. The design choices of UCCA seem to focus on descriptiveness and universality rather than immediate use, resulting in a cognitively-motivated label-set that, for\n11Both PROPS and Stanford-dependency trees were obtained by using Berkeley Parser (Petrov and Klein, 2007) as the base parser. In both cases the representation-units are (sourceword(s), label, target-word(s)) triplets.\nexample, does not distinguish between subjects and objects giving both a \u201cparticipant\u201d status. Finally, to the best of our knowledge there is no automated system capable of inferring UCCA structures from text, while we provide a concrete implementation and demonstrate its effectiveness.\nSemantic Role Labeling (SRL) (Carreras and Ma\u0300rquez, 2005b) is also centered around proposition structures, with the goal of identifying predicates and linking each predicate to its semantic arguments (agent, patient, etc.) and adjuncts (locative, temporal, etc.). While SRL does not cover the full scope of propositions and unifications handled by PROPS, for the propositions it does cover it tackles a challenging semantic task that PROPS does not attempt to address, namely assigning arguments with semantic roles and grounding predicates to a semantic lexicon. PROPS graphs may be extended with SRL annotations, which can be incorporated on top of PROPS structures by adding semantic-role labels to predicate-argument relations.\nOther notable semantic representations include GMB (Basile et al., 2012), UNL (Uchida and Zhu, 2001) and recently AMR (Banarescu et al., 2013). While AMR\u2019s rooted tree cannot capture independent propositions within a single sentence, it otherwise subsumes PROPS and SRL, aiming to provide the complete semantic structure of the sentence (along with inner structure and inter-relations). Producing accurate AMR structures requires in-depth semantic analysis, and it is currently produced with relatively low accuracy (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; Artzi et al., 2015). In contrast, PROPS is designed to be relatively easy to induce. Generally, PROPS can be viewed as a step between syntax and more semantic representations, providing a strong foundation on top of which more elaborate semantic annotations can be added."}, {"heading": "9 Conclusion", "text": "We presented PROPS \u2013 a large set of linguistically motivated conversions. PROPS allows semantic applications to easily explore a broad range of the proposition structure, oblivious to the way it was expressed in the surface syntactic form. In addition, we produced an automatic high accuracy conversion of the WSJ, and showed the out-of-the-box applicability of the PROPS converter."}], "references": [{"title": "Universal conceptual cognitive annotation (ucca)", "author": ["Omri Abend", "Ari Rappoport."], "venue": "ACL (1), pages 228\u2013238.", "citeRegEx": "Abend and Rappoport.,? 2013", "shortCiteRegEx": "Abend and Rappoport.", "year": 2013}, {"title": "Leveraging linguistic structure for open domain information extraction", "author": ["Gabor Angeli", "Melvin Johnson Premkumar", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Angeli et al\\.,? 2015", "shortCiteRegEx": "Angeli et al\\.", "year": 2015}, {"title": "Text2table: medical text summarization system based on named entity recognition and modality identification", "author": ["Eiji Aramaki", "Yasuhide Miura", "Masatsugu Tonoike", "Tomoko Ohkuma", "Hiroshi Mashuichi", "Kazuhiko Ohe."], "venue": "Proceedings of the Workshop", "citeRegEx": "Aramaki et al\\.,? 2009", "shortCiteRegEx": "Aramaki et al\\.", "year": 2009}, {"title": "Broad-coverage ccg semantic parsing with amr", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1699\u20131710, Lisbon, Portugal, September. Association", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Deepsyntactic parsing", "author": ["Miguel Ballesteros", "Bernd Bohnet", "Simon Mille", "Leo Wanner."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics (COLING), Dublin, Ireland.", "citeRegEx": "Ballesteros et al\\.,? 2014", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2014}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Developing a large semantically annotated corpus", "author": ["Valerio Basile", "Johan Bos", "Kilian Evang", "Noortje Venhuizen."], "venue": "LREC, volume 12, pages 3196\u2013 3200.", "citeRegEx": "Basile et al\\.,? 2012", "shortCiteRegEx": "Basile et al\\.", "year": 2012}, {"title": "A philosophical guide to conditionals", "author": ["Jonathan Bennett"], "venue": null, "citeRegEx": "Bennett.,? \\Q2003\\E", "shortCiteRegEx": "Bennett.", "year": 2003}, {"title": "Modeling biological processes for reading comprehension", "author": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Berant et al\\.,? 2014", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Bracketing guidelines for treebank ii style penn treebank project", "author": ["Ann Bies", "Mark Ferguson", "Karen Katz", "Robert MacIntyre", "Victoria Tredinnick", "Grace Kim", "Mary Ann Marcinkiewicz", "Britta Schasberger."], "venue": "University of Pennsylvania, 97:100.", "citeRegEx": "Bies et al\\.,? 1995", "shortCiteRegEx": "Bies et al\\.", "year": 1995}, {"title": "The prague dependency treebank", "author": ["Alena B\u00f6hmov\u00e1", "Jan Haji\u010d", "Eva Haji\u010dov\u00e1", "Barbora Hladk\u00e1."], "venue": "Treebanks, pages 103\u2013127. Springer.", "citeRegEx": "B\u00f6hmov\u00e1 et al\\.,? 2003", "shortCiteRegEx": "B\u00f6hmov\u00e1 et al\\.", "year": 2003}, {"title": "Introduction to the conll-2005 shared task: Semantic role labeling", "author": ["Xavier Carreras", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proceedings of CONLL,", "citeRegEx": "Carreras and M\u00e0rquez.,? \\Q2005\\E", "shortCiteRegEx": "Carreras and M\u00e0rquez.", "year": 2005}, {"title": "Introduction to the conll-2005 shared task: Semantic role labeling", "author": ["Xavier Carreras", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras and M\u00e0rquez.,? \\Q2005\\E", "shortCiteRegEx": "Carreras and M\u00e0rquez.", "year": 2005}, {"title": "Using very large corpora to detect raising and control verbs", "author": ["Grzegorz Chrupa\u0142a", "Josef van Genabith."], "venue": "Proceedings of the LFG07 Conference.", "citeRegEx": "Chrupa\u0142a and Genabith.,? 2007", "shortCiteRegEx": "Chrupa\u0142a and Genabith.", "year": 2007}, {"title": "Dependency tree kernels for relation extraction", "author": ["Aron Culotta", "Jeffrey Sorensen."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 423. Association for Computational Linguistics.", "citeRegEx": "Culotta and Sorensen.,? 2004", "shortCiteRegEx": "Culotta and Sorensen.", "year": 2004}, {"title": "The grammar of raising and control: A course in syntactic argumentation", "author": ["William D Davies", "Stanley Dubinsky."], "venue": "John Wiley & Sons.", "citeRegEx": "Davies and Dubinsky.,? 2008", "shortCiteRegEx": "Davies and Dubinsky.", "year": 2008}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine de Marneffe", "Christopher D Manning."], "venue": "Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008a", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "The stanford typed dependencies representation", "author": ["Marie-Catherine de Marneffe", "Christopher D Manning."], "venue": "Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1\u20138.", "citeRegEx": "Marneffe and Manning.,? 2008b", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["J. Flanigan", "S. Thomson", "J. Carbonell", "C. Dyer", "N.A. Smith."], "venue": "Proc. of ACL, Baltimore, Maryland, June. Association for Computational Linguistics.", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "The cambridge grammar of english", "author": ["Rodney Huddleston", "Geoffrey K Pullum"], "venue": null, "citeRegEx": "Huddleston and Pullum,? \\Q2002\\E", "shortCiteRegEx": "Huddleston and Pullum", "year": 2002}, {"title": "From discourse to logic: Introduction to model theoretic semantics of natural language, formal logic and discourse representation theory", "author": ["Hans Kamp", "Uwe Reyle."], "venue": "Number 42. Springer Science & Business Media.", "citeRegEx": "Kamp and Reyle.,? 1993", "shortCiteRegEx": "Kamp and Reyle.", "year": 1993}, {"title": "Propbank: the next level of treebank", "author": ["Paul Kingsbury", "Martha Palmer."], "venue": "Proceedings of Treebanks and lexical Theories, volume 3.", "citeRegEx": "Kingsbury and Palmer.,? 2003", "shortCiteRegEx": "Kingsbury and Palmer.", "year": 2003}, {"title": "A database of attribution relations", "author": ["Silvia Pareti."], "venue": "LREC, pages 3213\u20133217. Citeseer.", "citeRegEx": "Pareti.,? 2012", "shortCiteRegEx": "Pareti.", "year": 2012}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "HLT-NAACL, volume 7, pages 404\u2013411.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Coordination structures in dependency treebanks", "author": ["Martin Popel", "David Marecek", "Jan Step\u00e1nek", "Daniel Zeman", "Zdenek Zabokrtsk\u1ef3."], "venue": "Proc. of ACL, pages 517\u2013527.", "citeRegEx": "Popel et al\\.,? 2013", "shortCiteRegEx": "Popel et al\\.", "year": 2013}, {"title": "Annotating attribution in the penn discourse treebank", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Aravind Joshi", "Bonnie Webber."], "venue": "Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 31\u201338. Association for Computational Linguistics.", "citeRegEx": "Prasad et al\\.,? 2006", "shortCiteRegEx": "Prasad et al\\.", "year": 2006}, {"title": "Parsing english into abstract meaning representation using syntaxbased machine translation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Pust et al\\.,? 2015", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."], "venue": "EMNLP, pages 193\u2013203.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "A theory of conditionals", "author": ["Robert C Stalnaker."], "venue": "Ifs, pages 41\u201355. Springer.", "citeRegEx": "Stalnaker.,? 1981", "shortCiteRegEx": "Stalnaker.", "year": 1981}, {"title": "The universal networking language beyond machine translation", "author": ["Hiroshi Uchida", "Meiying Zhu."], "venue": "International Symposium on Language in Cyberspace, Seoul, pages 26\u201327.", "citeRegEx": "Uchida and Zhu.,? 2001", "shortCiteRegEx": "Uchida and Zhu.", "year": 2001}, {"title": "Adding noun phrase structure to the penn treebank", "author": ["David Vadas", "James Curran."], "venue": "ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 240.", "citeRegEx": "Vadas and Curran.,? 2007", "shortCiteRegEx": "Vadas and Curran.", "year": 2007}, {"title": "A transition-based algorithm for amr parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366\u2013", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "Dependency trees (de Marneffe and Manning, 2008b) are attractive as they directly connect verbal predicates to their arguments, while deep syntax extensions of dependency trees also mark long distance dependencies, further broadening predicate-argument coverage (Ballesteros et al., 2014).", "startOffset": 262, "endOffset": 288}, {"referenceID": 5, "context": "Notable examples are Semantic Role Labeling (SRL) (Carreras and M\u00e0rquez, 2005a), which extracts frames linking predicates with their semantic arguments, or, more recently, Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which aims to extract a graph representation capturing the semantic structure of the sentence.", "startOffset": 210, "endOffset": 234}, {"referenceID": 14, "context": ", (Culotta and Sorensen, 2004)), the possibility of creating such heuristics rests on rather strong linguistic grounds \u2013 the syntactic structure reliably expresses a wide range of predications.", "startOffset": 2, "endOffset": 30}, {"referenceID": 9, "context": "In addition to providing an automatic tool to convert dependency trees we also provide a large semiautomatic annotation of the WSJ portion of the Penn Tree Bank (PTB) (Bies et al., 1995).", "startOffset": 167, "endOffset": 186}, {"referenceID": 21, "context": "We do so by utilizing gold phrase-based annotations and other annotated resources, such as Propbank (Kingsbury and Palmer, 2003) or Vadas and Curran\u2019s (2007) NP structure annotation.", "startOffset": 100, "endOffset": 128}, {"referenceID": 9, "context": "In addition to providing an automatic tool to convert dependency trees we also provide a large semiautomatic annotation of the WSJ portion of the Penn Tree Bank (PTB) (Bies et al., 1995). This recovers our structures with higher accuracy, and includes an accurate handling for the previously mentioned semantically hard decisions. We do so by utilizing gold phrase-based annotations and other annotated resources, such as Propbank (Kingsbury and Palmer, 2003) or Vadas and Curran\u2019s (2007) NP structure annotation.", "startOffset": 168, "endOffset": 489}, {"referenceID": 1, "context": "(Angeli et al., 2015)), and not trivially available in depdendency trees.", "startOffset": 0, "endOffset": 21}, {"referenceID": 28, "context": "While there has been extensive linguistic study of the subject (Stalnaker, 1981; Bennett, 2003), NLP applications tend to overlook these constructions (a recent exception is (Berant et al.", "startOffset": 63, "endOffset": 95}, {"referenceID": 7, "context": "While there has been extensive linguistic study of the subject (Stalnaker, 1981; Bennett, 2003), NLP applications tend to overlook these constructions (a recent exception is (Berant et al.", "startOffset": 63, "endOffset": 95}, {"referenceID": 8, "context": "While there has been extensive linguistic study of the subject (Stalnaker, 1981; Bennett, 2003), NLP applications tend to overlook these constructions (a recent exception is (Berant et al., 2014)).", "startOffset": 174, "endOffset": 195}, {"referenceID": 20, "context": "We use restrictive versus nonrestrictive modification to properly bound the scope of arguments (Kamp and Reyle, 1993).", "startOffset": 95, "endOffset": 117}, {"referenceID": 15, "context": ", example 13), by the correspondence between syntactic and semantic arguments (Davies and Dubinsky, 2008) (i.", "startOffset": 78, "endOffset": 105}, {"referenceID": 24, "context": "Our representation of coordination follows the Prague-Treebank style (Popel et al., 2013; B\u00f6hmov\u00e1 et al., 2003) and posits the coordinating word as the main node of the coordinating-conjunction structure, while the different conjuncts are attached to it using \u201cconj\u201d edges.", "startOffset": 69, "endOffset": 111}, {"referenceID": 10, "context": "Our representation of coordination follows the Prague-Treebank style (Popel et al., 2013; B\u00f6hmov\u00e1 et al., 2003) and posits the coordinating word as the main node of the coordinating-conjunction structure, while the different conjuncts are attached to it using \u201cconj\u201d edges.", "startOffset": 69, "endOffset": 111}, {"referenceID": 25, "context": "This property was extensibility studied in the PDTB (Prasad et al., 2006) and PARC (Pareti, 2012) corpora, and was shown to be useful in semantic applications (e.", "startOffset": 52, "endOffset": 73}, {"referenceID": 22, "context": ", 2006) and PARC (Pareti, 2012) corpora, and was shown to be useful in semantic applications (e.", "startOffset": 17, "endOffset": 31}, {"referenceID": 2, "context": ", (Aramaki et al., 2009)).", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": "To obtain this corpus we adapted the conversions described in Section 3 and ran them on full gold constituency trees, available from the Penn Tree Bank (PTB) (Bies et al., 1995).", "startOffset": 158, "endOffset": 177}, {"referenceID": 23, "context": "Additionally, we test the accuracy of converting PROPS over automatic dependency trees derived by running the Stanford converter on top of the Berkeley parser (Petrov and Klein, 2007).", "startOffset": 159, "endOffset": 183}, {"referenceID": 27, "context": "2 Extrinsic Evaluation To extrinsically evaluate our automatic converter we use the MCTest corpus for machine comprehension (Richardson et al., 2013), composed of 500 short stories, each followed by 4 multiple choice questions.", "startOffset": 124, "endOffset": 149}, {"referenceID": 0, "context": "In terms of the produced outputs, perhaps the most similar to ours is UCCA (Abend and Rappoport, 2013).", "startOffset": 75, "endOffset": 102}, {"referenceID": 23, "context": "Both PROPS and Stanford-dependency trees were obtained by using Berkeley Parser (Petrov and Klein, 2007) as the base parser.", "startOffset": 80, "endOffset": 104}, {"referenceID": 6, "context": "Other notable semantic representations include GMB (Basile et al., 2012), UNL (Uchida and Zhu, 2001) and recently AMR (Banarescu et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 29, "context": ", 2012), UNL (Uchida and Zhu, 2001) and recently AMR (Banarescu et al.", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": ", 2012), UNL (Uchida and Zhu, 2001) and recently AMR (Banarescu et al., 2013).", "startOffset": 53, "endOffset": 77}, {"referenceID": 18, "context": "Producing accurate AMR structures requires in-depth semantic analysis, and it is currently produced with relatively low accuracy (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 129, "endOffset": 210}, {"referenceID": 31, "context": "Producing accurate AMR structures requires in-depth semantic analysis, and it is currently produced with relatively low accuracy (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 129, "endOffset": 210}, {"referenceID": 26, "context": "Producing accurate AMR structures requires in-depth semantic analysis, and it is currently produced with relatively low accuracy (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 129, "endOffset": 210}, {"referenceID": 3, "context": "Producing accurate AMR structures requires in-depth semantic analysis, and it is currently produced with relatively low accuracy (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 129, "endOffset": 210}], "year": 2016, "abstractText": "Semantic NLP applications often rely on dependency trees to recognize major elements of the proposition structure of sentences. Yet, while much semantic structure is indeed expressed by syntax, many phenomena are not easily read out of dependency trees, often leading to further ad-hoc heuristic postprocessing or to information loss. To directly address the needs of semantic applications, we present PROPS \u2013 an output representation designed to explicitly and uniformly express much of the proposition structure which is implied from syntax, and an associated tool for extracting it from dependency trees.", "creator": "LaTeX with hyperref package"}}}