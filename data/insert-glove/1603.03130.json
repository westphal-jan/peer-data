{"id": "1603.03130", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning", "abstract": "hardware In PU wrington learning, a dinnie binary dinatale classifier torode is trained only patisserie from positive (chowrasia P) softwoods and unlabeled (rayna U) panpacs data bentwood without negative (N) gilberton data. fpcc Although N data dhungel is missing, it entree sometimes outperforms sdv PN learning (mouring i. fontanez e. , exordium supervised motion-sensing learning) niverville in cambron experiments. revillon In this paper, 217 we theoretically dumel compare mynamar PU (hawkins and the triennale opposite debridement NU) kuroyedov learning aban against benaco PN boren learning, and yubo prove eur2004-por that, elly one utting of nuo PU crazing and kilday NU learning armario given platte infinite U agios data will almost sidelight always gung improve on PN helvetic learning. Our wimbledons theoretical bhokin finding is ok also validated inspiral experimentally.", "histories": [["v1", "Thu, 10 Mar 2016 02:53:52 GMT  (47kb)", "https://arxiv.org/abs/1603.03130v1", null], ["v2", "Mon, 23 May 2016 04:35:47 GMT  (169kb,D)", "http://arxiv.org/abs/1603.03130v2", null], ["v3", "Fri, 28 Oct 2016 13:37:46 GMT  (169kb,D)", "http://arxiv.org/abs/1603.03130v3", "NIPS 2016 camera-ready version"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["gang niu", "marthinus christoffel du plessis", "tomoya sakai", "yao ma", "masashi sugiyama"], "accepted": true, "id": "1603.03130"}, "pdf": {"name": "1603.03130.pdf", "metadata": {"source": "CRF", "title": "Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning", "authors": ["Gang Niu", "Marthinus C. du Plessis", "Tomoya Sakai", "Yao Ma", "Masashi Sugiyama"], "emails": ["gang@ms.,", "christo@ms.,", "sakai@ms.,", "yao@ms.,"], "sections": [{"heading": "1 Introduction", "text": "Positive-unlabeled (PU) learning, where a binary classifier is trained from P and U data, has drawn considerable attention recently [1, 2, 3, 4, 5, 6, 7, 8]. It is appealing to not only the academia but also the industry, since for example the click-through data automatically collected in search engines are highly PU due to position biases [9, 10, 11]. Although PU learning uses no negative (N) data, it is sometimes even better than PN learning (i.e., ordinary supervised learning, perhaps with class-prior change [12]) in practice. Nevertheless, there is neither theoretical nor experimental analysis for this phenomenon, and it is still an open problem when PU learning is likely to outperform PN learning. We clarify this question in this paper.\nProblem settings For PU learning, there are two problem settings based on one sample (OS) and two samples (TS) of data respectively. More specifically, let X \u2208 Rd and Y \u2208 {\u00b11} (d \u2208 N) be the input and output random variables and equipped with an underlying joint density p(x, y). In OS [3], a set of U data is sampled from the marginal density p(x). Then if a data point x is P, this P label is observed with probability c, and x remains U with probability 1\u2212 c; if x is N, this N label is never observed, and x remains U with probability 1. In TS [4], a set of P data is drawn from the positive marginal density p(x | Y = +1) and a set of U data is drawn from p(x). Denote by n+ and nu the sizes of P and U data. As two random variables, they are fully independent in TS, and they satisfy n+/(n+ + nu) \u2248 c\u03c0 in OS where \u03c0 = p(Y = +1) is the class-prior probability. Therefore, TS is slightly more general than OS, and we will focus on TS problem settings.\nSimilarly, consider TS problem settings of PN and NU learning, where a set of N data (of size n\u2212) is sampled from p(x | Y = \u22121) independently of the P/U data. For PN learning, if we enforce that n+/(n+ + n\u2212) \u2248 \u03c0 when sampling the data, it will be ordinary supervised learning; otherwise, it is supervised learning with class-prior change, a.k.a. prior probability shift [12].\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 3.\n03 13\n0v 3\n[ cs\n.L G\n] 2\n8 O\nct 2\nIn [7], a cost-sensitive formulation for PU learning was proposed, and its risk estimator was proven unbiased if the surrogate loss is non-convex and satisfies a symmetric condition. Therefore, we can naturally compare empirical risk minimizers in PU and NU learning against that in PN learning.\nContributions We establish risk bounds of three risk minimizers in PN, PU and NU learning for comparisons in a flavor of statistical learning theory [13, 14]. For each minimizer, we firstly derive a uniform deviation bound from the risk estimator to the risk using Rademacher complexities (see, e.g., [15, 16, 17, 18]), and secondly obtain an estimation error bound. Thirdly, if the surrogate loss is classification-calibrated [19], an excess risk bound is an immediate corollary. In [7], there was a generalization error bound similar to our uniform deviation bound for PU learning. However, it is based on a tricky decomposition of the risk, where surrogate losses for risk minimization and risk analysis are different and labels of U data are needed for risk evaluation, so that no further bound is implied. On the other hand, ours utilizes the same surrogate loss for risk minimization and analysis and requires no label of U data for risk evaluation, so that an estimation error bound is possible.\nOur main results can be summarized as follows. Denote by g\u0302pn, g\u0302pu and g\u0302nu the risk minimizers in PN, PU and NU learning. Under a mild assumption on the function class and data distributions,\n\u2022 Finite-sample case: The estimation error bound of g\u0302pu is tighter than that of g\u0302pn whenever \u03c0/ \u221a n+ + 1/ \u221a nu < (1 \u2212 \u03c0)/ \u221a n\u2212, and so is the bound of g\u0302nu tighter than that of g\u0302pn if\n(1\u2212 \u03c0)/\u221an\u2212 + 1/ \u221a nu < \u03c0/ \u221a n+.\n\u2022 Asymptotic case: Either the limit of bounds of g\u0302pu or that of g\u0302nu (depending on \u03c0, n+ and n\u2212) will improve on that of g\u0302pn, if n+, n\u2212 \u2192\u221e in the same order and nu \u2192\u221e faster in order than n+ and n\u2212.\nNotice that both results rely on only the constant \u03c0 and variables n+, n\u2212 and nu; they are simple and independent of the specific forms of the function class and/or the data distributions. The asymptotic case is from the finite-sample case that is based on theoretical comparisons of the aforementioned upper bounds on the estimation errors of g\u0302pn, g\u0302pu and g\u0302nu. To the best of our knowledge, this is the first work that compares PU learning against PN learning.\nThroughout the paper, we assume that the class-prior probability \u03c0 is known. In practice, it can be effectively estimated from P, N and U data [20, 21, 22] or only P and U data [23, 24].\nOrganization The rest of this paper is organized as follows. Unbiased estimators are reviewed in Section 2. Then in Section 3 we present our theoretical comparisons based on risk bounds. Finally experiments are discussed in Section 4."}, {"heading": "2 Unbiased estimators to the risk", "text": "For convenience, denote by p+(x) = p(x | Y = +1) and p\u2212(x) = p(x | Y = \u22121) partial marginal densities. Recall that instead of data sampled from p(x, y), we consider three sets of data X+, X\u2212 and Xu which are drawn from three marginal densities p+(x), p\u2212(x) and p(x) independently.\nLet g : Rd \u2192 R be a real-valued decision function for binary classification and ` : R\u00d7 {\u00b11} \u2192 R be a Lipschitz-continuous loss function. Denote by\nR+(g) = E+[`(g(X),+1)], R\u2212(g) = E\u2212[`(g(X),\u22121)] partial risks, where E\u00b1[\u00b7] = EX\u223cp\u00b1 [\u00b7]. Then the risk of g w.r.t. ` under p(x, y) is given by\nR(g) = E(X,Y )[`(g(X), Y )] = \u03c0R+(g) + (1\u2212 \u03c0)R\u2212(g). (1) In PN learning, by approximating R(g) based on Eq. (1), we can get an empirical risk estimator as\nR\u0302pn(g) = \u03c0 n+ \u2211 xi\u2208X+ `(g(xi),+1) + 1\u2212\u03c0 n\u2212 \u2211 xj\u2208X\u2212 `(g(xj),\u22121).\nFor any fixed g, R\u0302pn(g) is an unbiased and consistent estimator to R(g) and its convergence rate is of order Op(1/ \u221a n+ + 1/ \u221a n\u2212) according to the central limit theorem [25], where Op denotes the order in probability.\nIn PU learning, X\u2212 is not available and then R\u2212(g) cannot be directly estimated. However, [7] has shown that we can estimate R(g) without any bias if ` satisfies the following symmetric condition:\n`(t,+1) + `(t,\u22121) = 1. (2)\nSpecifically, let Ru,\u2212(g) = EX [`(g(X),\u22121)] = \u03c0E+[`(g(X),\u22121)] + (1\u2212 \u03c0)R\u2212(g) be a risk that U data are regarded as N data. Given Eq. (2), we have E+[`(g(X),\u22121)] = 1\u2212R+(g), and hence\nR(g) = 2\u03c0R+(g) +Ru,\u2212(g)\u2212 \u03c0. (3)\nBy approximating R(g) based on (3) using X+ and Xu, we can obtain R\u0302pu(g) = \u2212\u03c0 + 2\u03c0n+ \u2211 xi\u2208X+ `(g(xi),+1) + 1 nu \u2211 xj\u2208Xu `(g(xj),\u22121).\nAlthough R\u0302pu(g) regards Xu as N data and aims at separating X+ and Xu if being minimized, it is an unbiased and consistent estimator to R(g) with a convergence rate Op(1/ \u221a n+ + 1/ \u221a nu) [25].\nSimilarly, in NU learning R+(g) cannot be directly estimated. Let Ru,+(g) = EX [`(g(X),+1)] = \u03c0R+(g) + (1\u2212 \u03c0)E\u2212[`(g(X),+1)]. Given Eq. (2), E\u2212[`(g(X),+1)] = 1\u2212R\u2212(g), and\nR(g) = Ru,+(g) + 2(1\u2212 \u03c0)R\u2212(g)\u2212 (1\u2212 \u03c0). (4)\nBy approximating R(g) based on (4) using Xu and X\u2212, we can obtain R\u0302nu(g) = \u2212(1\u2212 \u03c0) + 1nu \u2211 xi\u2208Xu `(g(xi),+1) + 2(1\u2212\u03c0) n\u2212 \u2211 xj\u2208X\u2212 `(g(xj),\u22121).\nOn the loss function In order to train g by minimizing these estimators, it remains to specify the loss `. The zero-one loss `01(t, y) = (1\u2212 sign(ty))/2 satisfies (2) but is non-smooth. [7] proposed to use a scaled ramp loss as the surrogate loss for `01 in PU learning:\n`sr(t, y) = max{0,min{1, (1\u2212 ty)/2}},\ninstead of the popular hinge loss that does not satisfy (2). Let I(g) = E(X,Y )[`01(g(X), Y )] be the risk of g w.r.t. `01 under p(x, y). Then, `sr is neither an upper bound of `01 so that I(g) \u2264 R(g) is not guaranteed, nor a convex loss so that it gets more difficult to know whether `sr is classificationcalibrated or not [19].1 If it is, we are able to control the excess risk w.r.t. `01 by that w.r.t. `. Here we prove the classification calibration of `sr, and consequently it is a safe surrogate loss for `01. Theorem 1. The scaled ramp loss `sr is classification-calibrated (see Appendix A for the proof)."}, {"heading": "3 Theoretical comparisons based on risk bounds", "text": "When learning is involved, suppose we are given a function class G, and let g\u2217 = arg ming\u2208G R(g) be the optimal decision function in G, g\u0302pn = arg ming\u2208G R\u0302pn(g), g\u0302pu = arg ming\u2208G R\u0302pu(g), and g\u0302nu = arg ming\u2208G R\u0302nu(g) be arbitrary global minimizers to three risk estimators. Furthermore, let R\u2217 = infg R(g) and I\u2217 = infg I(g) denote the Bayes risks w.r.t. ` and `01, where the infimum of g is over all measurable functions.\nIn this section, we derive and compare risk bounds of three risk minimizers g\u0302pn, g\u0302pu and g\u0302nu under the following mild assumption on G, p(x), p+(x) and p\u2212(x): There is a constant CG > 0 such that\nRn,q(G) \u2264 CG/ \u221a n (5)\nfor any marginal density q(x) \u2208 {p(x), p+(x), p\u2212(x)}, where Rn,q(G) = EX\u223cqnE\u03c3 [ supg\u2208G 1 n \u2211 xi\u2208X \u03c3ig(xi) ] is the Rademacher complexity of G for the sampling of size n from q(x) (that is, X = {x1, . . . , xn} and \u03c3 = {\u03c31, . . . , \u03c3n}, with each xi drawn from q(x) and each \u03c3i as a Rademacher variable) [18]. A special case is covered, namely, sets of hyperplanes with bounded normals and feature maps:\nG = {g(x) = \u3008w, \u03c6(x)\u3009H | \u2016w\u2016H \u2264 Cw, \u2016\u03c6(x)\u2016H \u2264 C\u03c6}, (6)\nwhereH is a Hilbert space with an inner product \u3008\u00b7, \u00b7\u3009H, w \u2208 H is a normal vector, \u03c6 : Rd \u2192 H is a feature map, and Cw > 0 and C\u03c6 > 0 are constants [26].\n1A loss function ` is classification-calibrated if and only if there is a convex, invertible and nondecreasing transformation \u03c8` with \u03c8`(0) = 0, such that \u03c8`(I(g)\u2212 infg I(g)) \u2264 R(g)\u2212 infg R(g) [19]."}, {"heading": "3.1 Risk bounds", "text": "Let L` be the Lipschitz constant of ` in its first parameter. To begin with, we establish the learning guarantee of g\u0302pu (the proof can be found in Appendix A). Theorem 2. Assume (2). For any \u03b4 > 0, with probability at least 1\u2212 \u03b4,2\nR(g\u0302pu)\u2212R(g\u2217) \u2264 8\u03c0L`Rn+,p+(G) + 4L`Rnu,p(G) + 2\u03c0 \u221a 2 ln(4/\u03b4) n+ + \u221a 2 ln(4/\u03b4) nu , (7)\nwhere Rn+,p+(G) and Rnu,p(G) are the Rademacher complexities of G for the sampling of size n+ from p+(x) and the sampling of size nu from p(x). Moreover, if ` is a classification-calibrated loss, there exists nondecreasing \u03d5 with \u03d5(0) = 0, such that with probability at least 1\u2212 \u03b4,\nI(g\u0302pu)\u2212I\u2217 \u2264 \u03d5 ( R(g\u2217)\u2212R\u2217+8\u03c0L`Rn+,p+(G)+4L`Rnu,p(G)+2\u03c0 \u221a 2 ln(4/\u03b4) n+ + \u221a 2 ln(4/\u03b4) nu ) . (8)\nIn Theorem 2, R(g\u0302pu) and I(g\u0302pu) are w.r.t. p(x, y), though g\u0302pu is trained from two samples following p+(x) and p(x). We can see that (7) is an upper bound of the estimation error of g\u0302pu w.r.t. `, whose right-hand side (RHS) is small if G is small; (8) is an upper bound of the excess risk of g\u0302pu w.r.t. `01, whose RHS also involves the approximation error of G (i.e., R(g\u2217)\u2212R\u2217) that is small if G is large. When G is fixed and satisfies (5), we have Rn+,p+(G) = O(1/ \u221a n+) and Rnu,p(G) = O(1/ \u221a nu), and then R(g\u0302pu)\u2212R(g\u2217)\u2192 0, I(g\u0302pu)\u2212 I\u2217 \u2192 \u03d5(R(g\u2217)\u2212R\u2217) in Op(1/ \u221a n+ + 1/ \u221a nu). On the other hand, when the size of G grows with n+ and nu properly, those complexities of G vanish slower in order than O(1/\u221an+) and O(1/ \u221a nu) but we may have R(g\u0302pu)\u2212R(g\u2217)\u2192 0, I(g\u0302pu)\u2212 I\u2217 \u2192 0, which means g\u0302pu approaches the Bayes classifier if ` is a classification-calibrated loss, in an order slower than Op(1/ \u221a n+ + 1/ \u221a nu) due to the growth of G.\nSimilarly, we can derive the learning guarantees of g\u0302pn and g\u0302nu for comparisons. We will just focus on estimation error bounds, because excess risk bounds are their immediate corollaries. Theorem 3. Assume (2). For any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\nR(g\u0302pn)\u2212R(g\u2217) \u2264 4\u03c0L`Rn+,p+(G) + 4(1\u2212 \u03c0)L`Rn\u2212,p\u2212(G) + \u03c0 \u221a 2 ln(4/\u03b4) n+ + (1\u2212 \u03c0) \u221a 2 ln(4/\u03b4) n\u2212 , (9) where Rn\u2212,p\u2212(G) is the Rademacher complexity of G for the sampling of size n\u2212 from p\u2212(x). Theorem 4. Assume (2). For any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\nR(g\u0302nu)\u2212R(g\u2217) \u2264 4L`Rnu,p(G)+8(1\u2212\u03c0)L`Rn\u2212,p\u2212(G)+ \u221a 2 ln(4/\u03b4) nu +2(1\u2212\u03c0) \u221a 2 ln(4/\u03b4) n\u2212 . (10)\nIn order to compare the bounds, we simplify (9), (7) and (10) using Eq. (5). To this end, we define f(\u03b4) = 4L`CG + \u221a 2 ln(4/\u03b4). For the special case of G defined in (6), define f(\u03b4) accordingly as\nf(\u03b4) = 4L`CwC\u03c6 + \u221a\n2 ln(4/\u03b4). Corollary 5. The estimation error bounds below hold separately with probability at least 1\u2212 \u03b4:\nR(g\u0302pn)\u2212R(g\u2217) \u2264 f(\u03b4) \u00b7 {\u03c0/ \u221a n+ + (1\u2212 \u03c0)/ \u221a n\u2212}, (11) R(g\u0302pu)\u2212R(g\u2217) \u2264 f(\u03b4) \u00b7 {2\u03c0/ \u221a n+ + 1/ \u221a nu}, (12) R(g\u0302nu)\u2212R(g\u2217) \u2264 f(\u03b4) \u00b7 {1/ \u221a nu + 2(1\u2212 \u03c0)/ \u221a n\u2212}. (13)"}, {"heading": "3.2 Finite-sample comparisons", "text": "Note that three risk minimizers g\u0302pn, g\u0302pu and g\u0302nu work in similar problem settings and their bounds in Corollary 5 are proven using exactly the same proof technique. Then, the differences in bounds reflect the intrinsic differences between risk minimizers. Let us compare those bounds. Define\n\u03b1pu,pn = ( \u03c0/ \u221a n+ + 1/ \u221a nu ) / ( (1\u2212 \u03c0)/\u221an\u2212 ) , (14)\n\u03b1nu,pn = ( (1\u2212 \u03c0)/\u221an\u2212 + 1/ \u221a nu ) / ( \u03c0/ \u221a n+ ) . (15)\nEqs. (14) and (15) constitute our first main result. 2Here, the probability is over repeated sampling of data for training g\u0302pu, while in Lemma 8, it will be for\nevaluating R\u0302pu(g).\nTheorem 6 (Finite-sample comparisons). Assume (5) is satisfied. Then the estimation error bound of g\u0302pu in (12) is tighter than that of g\u0302pn in (11) if and only if \u03b1pu,pn < 1; also, the estimation error bound of g\u0302nu in (13) is tighter than that of g\u0302pn if and only if \u03b1nu,pn < 1.\nProof. Fix \u03c0, n+, n\u2212 and nu, and then denote by Vpn, Vpu and Vnu the values of the RHSs of (11), (12) and (13). In fact, the definitions of \u03b1pu,pn and \u03b1nu,pn in (14) and (15) came from\n\u03b1pu,pn = Vpu \u2212 \u03c0f(\u03b4)/\n\u221a n+\nVpn \u2212 \u03c0f(\u03b4)/ \u221a n+\n, \u03b1nu,pn = Vnu \u2212 (1\u2212 \u03c0)f(\u03b4)/\n\u221a n\u2212\nVpn \u2212 (1\u2212 \u03c0)f(\u03b4)/ \u221a n\u2212 .\nAs a consequence, compared with Vpn, Vpu is smaller and (12) is tighter if and only if \u03b1pu,pn < 1, and Vnu is smaller and (13) is tighter if and only if \u03b1nu,pn < 1.\nWe analyze some properties of \u03b1pu,pn before going to our second main result. The most important property is that it relies on \u03c0, n+, n\u2212 and nu only; it is independent of G, p(x, y), p(x), p+(x) and p\u2212(x) as long as (5) is satisfied. Next, \u03b1pu,pn is obviously a monotonic function of \u03c0, n+, n\u2212 and nu. Furthermore, it is unbounded no matter if \u03c0 is fixed or not. Properties of \u03b1nu,pn are similar, as summarized in Table 1.\nImplications of the monotonicity of \u03b1pu,pn are given as follows. Intuitively, when other factors are fixed, larger nu or n\u2212 improves g\u0302pu or g\u0302pn respectively. However, it is complicated why \u03b1pu,pn is monotonically decreasing with n+ and increasing with \u03c0. The weights of the empirical average of X+ is 2\u03c0 in R\u0302pu(g) and \u03c0 in R\u0302pn(g), as in R\u0302pu(g) it also joins the estimation of (1\u2212 \u03c0)R\u2212(g). It makes X+ more important for R\u0302pu(g), and thus larger n+ improves g\u0302pu more than g\u0302pn. Moreover, (1\u2212 \u03c0)R\u2212(g) is directly estimated in R\u0302pn(g) and the concentration Op((1\u2212 \u03c0)/ \u221a n\u2212) is better if\n\u03c0 is larger, whereas it is indirectly estimated through Ru,\u2212(g)\u2212 \u03c0(1\u2212R+(g)) in R\u0302pu(g) and the concentration Op(\u03c0/ \u221a n+ + 1/ \u221a nu) is worse if \u03c0 is larger. As a result, when the sample sizes are fixed g\u0302pu is more (or less) favorable as \u03c0 decreases (or increases).\nA natural question is what the monotonicity of \u03b1pu,pn would be if we enforce n+, n\u2212 and nu to be proportional. To answer this question, we assume n+/n\u2212 = \u03c1pn, n+/nu = \u03c1pu and n\u2212/nu = \u03c1nu where \u03c1pn, \u03c1pu and \u03c1nu are certain constants, then (14) and (15) can be rewritten as\n\u03b1pu,pn = (\u03c0 + \u221a \u03c1pu)/((1\u2212 \u03c0) \u221a \u03c1pn), \u03b1nu,pn = (1\u2212 \u03c0 + \u221a \u03c1nu)/(\u03c0/ \u221a \u03c1pn).\nAs shown in Table 1, \u03b1pu,pn is now increasing with \u03c1pu and decreasing with \u03c1pn. It is because, for instance, when \u03c1pn is fixed and \u03c1pu increases, nu is meant to decrease relatively to n+ and n\u2212.\nFinally, the properties will dramatically change if we enforce \u03c1pn = \u03c0/(1\u2212 \u03c0) that approximately holds in ordinary supervised learning. Under this constraint, we have\n\u03b1pu,pn = (\u03c0 + \u221a \u03c1pu)/ \u221a \u03c0(1\u2212 \u03c0) \u2265 2 \u221a \u03c1pu + \u221a \u03c1pu,\nwhere the equality is achieved at \u03c0\u0304 = \u221a\u03c1pu/(2 \u221a \u03c1pu + 1). Here, \u03b1pu,pn decreases with \u03c0 if \u03c0 < \u03c0\u0304 and increases with \u03c0 if \u03c0 > \u03c0\u0304, though it is not convex in \u03c0. Only if nu is sufficiently larger than n+ (e.g., \u03c1pu < 0.04), could \u03b1pu,pn < 1 be possible and g\u0302pu have a tighter estimation error bound."}, {"heading": "3.3 Asymptotic comparisons", "text": "In practice, we may find that g\u0302pu is worse than g\u0302pn and \u03b1pu,pn > 1 given X+, X\u2212 and Xu. This is probably the consequence especially when nu is not sufficiently larger than n+ and n\u2212. Should we then try to collect much more U data or just give up PU learning? Moreover, if we are able to have as many U data as possible, is there any solution that would be provably better than PN learning?\nWe answer these questions by asymptotic comparisons. Notice that each pair of (n+, nu) yields a value of the RHS of (12), each (n+, n\u2212) yields a value of the RHS of (11), and consequently each triple of (n+, n\u2212, nu) determines a value of \u03b1pu,pn. Define the limits of \u03b1pu,pn and \u03b1nu,pn as\n\u03b1\u2217pu,pn = limn+,n\u2212,nu\u2192\u221e \u03b1pu,pn, \u03b1 \u2217 nu,pn = limn+,n\u2212,nu\u2192\u221e \u03b1nu,pn.\nRecall that n+, n\u2212 and nu are independent, and we need two conditions for the existence of \u03b1\u2217pu,pn and \u03b1\u2217nu,pn: n+ \u2192\u221e and n\u2212 \u2192\u221e in the same order and nu \u2192\u221e faster in order than them. It is a bit stricter than what is necessary, but is consistent with a practical assumption: P and N data are roughly equally expensive, whereas U data are much cheaper than P and N data. Intuitively, since \u03b1pu,pn and \u03b1nu,pn measure relative qualities of the estimation error bounds of g\u0302pu and g\u0302nu against that of g\u0302pn, \u03b1\u2217pu,pn and \u03b1 \u2217 nu,pn measure relative qualities of the limits of those bounds accordingly.\nIn order to illustrate properties of \u03b1\u2217pu,pn and \u03b1 \u2217 nu,pn, assume only nu approaches infinity while n+ and n\u2212 stay finite, so that \u03b1\u2217pu,pn = \u03c0 \u221a n\u2212/((1 \u2212 \u03c0) \u221a n+) and \u03b1\u2217nu,pn = (1 \u2212 \u03c0) \u221a n+/(\u03c0 \u221a n\u2212). Thus, \u03b1\u2217pu,pn\u03b1 \u2217 nu,pn = 1, which implies \u03b1 \u2217 pu,pn < 1 or \u03b1 \u2217 nu,pn < 1 unless n+/n\u2212 = \u03c0\n2/(1 \u2212 \u03c0)2. In principle, this exception should be exceptionally rare since n+/n\u2212 is a rational number whereas \u03c02/(1\u2212 \u03c0)2 is a real number. This argument constitutes our second main result. Theorem 7 (Asymptotic comparisons). Assume (5) and one set of conditions below are satisfied:\n(a) n+ <\u221e, n\u2212 <\u221e and nu \u2192\u221e. In this case, let \u03b1\u2217 = (\u03c0 \u221a n\u2212)/((1\u2212 \u03c0) \u221a n+);\n(b) 0 < limn+,n\u2212\u2192\u221e n+/n\u2212 <\u221e and limn+,n\u2212,nu\u2192\u221e(n+ + n\u2212)/nu = 0. In this case, let \u03b1\u2217 = \u03c0/((1\u2212 \u03c0) \u221a \u03c1\u2217pn) where \u03c1 \u2217 pn = limn+,n\u2212\u2192\u221e n+/n\u2212. Then, either the limit of estimation error bounds of g\u0302pu will improve on that of g\u0302pn (i.e., \u03b1\u2217pu,pn < 1) if \u03b1\u2217 < 1, or the limit of bounds of g\u0302nu will improve on that of g\u0302pn (i.e., \u03b1\u2217nu,pn < 1) if \u03b1\n\u2217 > 1. The only exception is n+/n\u2212 = \u03c02/(1\u2212 \u03c0)2 in (a) or \u03c1\u2217pn = \u03c02/(1\u2212 \u03c0)2 in (b). Proof. Note that \u03b1\u2217 = \u03b1\u2217pu,pn in both cases. The proof of case (a) has been given as an illustration of the properties of \u03b1\u2217pu,pn and \u03b1 \u2217 nu,pn. The proof of case (b) is analogous.\nAs a result, when we find that g\u0302pu is worse than g\u0302pn and \u03b1pu,pn > 1, we should look at \u03b1\u2217 defined in Theorem 7. If \u03b1\u2217 < 1, g\u0302pu is promising and we should collect more U data; if \u03b1\u2217 > 1 otherwise, we should give up g\u0302pu, but instead g\u0302nu is promising and we should collect more U data as well. In addition, the gap between \u03b1\u2217 and one indicates how many U data would be sufficient. If the gap is significant, slightly more U data may be enough; if the gap is slight, significantly more U data may be necessary. In practice, however, U data are cheaper but not free, and we cannot have as many U data as possible. Therefore, g\u0302pn is still of practical importance given limited budgets."}, {"heading": "3.4 Remarks", "text": "Theorem 2 relies on a fundamental lemma of the uniform deviation from the risk estimator R\u0302pu(g) to the risk R(g): Lemma 8. For any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\nsupg\u2208G |R\u0302pu(g)\u2212R(g)| \u2264 4\u03c0L`Rn+,p+(G) + 2L`Rnu,p(G) + 2\u03c0 \u221a ln(4/\u03b4) 2n+ + \u221a ln(4/\u03b4) 2nu .\nIn Lemma 8,R(g) is w.r.t. p(x, y), though R\u0302pu(g) is w.r.t. p+(x) and p(x). Rademacher complexities are also w.r.t. p+(x) and p(x), and they can be bounded easily for G defined in Eq. (6). Theorems 6 and 7 rely on (5). Thanks to it, we can simplify Theorems 2, 3 and 4. In fact, (5) holds for not only the special case of G defined in (6), but also the vast majority of discriminative models in machine learning that are nonlinear in parameters such as decision trees (cf. Theorem 17 in [16]) and feedforward neural networks (cf. Theorem 18 in [16]).\nTheorem 2 in [7] is a similar bound of the same order as our Lemma 8. That theorem is based on a tricky decomposition of the risk\nE(X,Y )[`(g(X), Y )] = \u03c0E+[\u02dc\u0300(g(X),+1)] + E(X,Y )[\u02dc\u0300(g(X), Y )],\nwhere the surrogate loss \u02dc\u0300(t, y) = (2/(y + 3))`(t, y) is not ` for risk minimization and labels of Xu are needed for risk evaluation, so that no further bound is implied. Lemma 8 uses the same ` as risk minimization and requires no label of Xu for evaluating R\u0302pu(g), so that it can serve as the stepping stone to our estimation error bound in Theorem 2."}, {"heading": "4 Experiments", "text": "In this section, we experimentally validate our theoretical findings.\nArtificial data Here, X+, X\u2212 and Xu are in R2 and drawn from three marginal densities\np+(x) = N(+12/ \u221a 2, I2), p\u2212(x) = N(\u221212/ \u221a\n2, I2), p(x) = \u03c0p+(x) + (1\u2212 \u03c0)p\u2212(x), where N(\u00b5,\u03a3) is the normal distribution with mean \u00b5 and covariance \u03a3, 12 and I2 are the all-one vector and identity matrix of size 2. The test set contains one million data drawn from p(x, y).\nThe model g(x) = \u3008w, x\u3009+ b where w \u2208 R2, b \u2208 R and the scaled ramp loss `sr are employed. In addition, an `2-regularization is added with the regularization parameter fixed to 10\u22123, and there is no hard constraint on \u2016w\u20162 or \u2016x\u20162 as in Eq. (6). The solver for minimizing three regularized risk estimators comes from [7] (refer also to [27, 28] for the optimization technique).\nThe results are reported in Figure 1. In (a)(b), n+ = 45, n\u2212 = 5, \u03c0 = 0.5, and nu varies from 5 to 200; in (c)(d), n+ = 45, n\u2212 = 5, nu = 100, and \u03c0 varies from 0.05 to 0.95. Specifically, (a) shows \u03b1pu,pn and \u03b1nu,pn as functions of nu, and (c) shows them as functions of \u03c0. For the experimental results, g\u0302pn, g\u0302pu and g\u0302nu were trained based on 100 random samplings for every nu in (b) and \u03c0 in (d), and means with standard errors of the misclassification rates are shown, as `sr is classificationcalibrated. Note that the empirical misclassification rates are essentially the risks w.r.t. `01 as there were one million test data, and the fluctuations are attributed to the non-convex nature of `sr. Also, the curve of g\u0302pn is not a flat line in (b), since its training data at every nu were exactly same as the training data of g\u0302pu and g\u0302nu for fair experimental comparisons.\nIn Figure 1, the theoretical and experimental results are highly consistent. The red and blue curves intersect at nearly the same positions in (a)(b) and in (c)(d), even though the risk minimizers in the experiments were locally optimal and regularized, making our estimation error bounds inexact.\nBenchmark data Table 2 summarizes the specification of benchmarks, which were downloaded from many sources including the IDA benchmark repository [29], the UCI machine learning repository, the semi-supervised learning book [30], and the European ESPRIT 5516 project.3 In Table 2, three rows describe the number of features, the number of data, and the ratio of P data according to the true class labels. Given a random sampling of X+, X\u2212 and Xu, the test set has all the remaining data if they are less than 104, or else drawn uniformly from the remaining data of size 104.\nFor benchmark data, the linear model for the artificial data is not enough, and its kernel version is employed. Consider training g\u0302pu for example. Given a random sampling, g(x) = \u3008w, \u03c6(x)\u3009+ b is used where w \u2208 Rn++nu , b \u2208 R and \u03c6 : Rd \u2192 Rn++nu is the empirical kernel map [26] based on X+ and Xu for the Gaussian kernel. The kernel width and the regularization parameter are selected by five-fold cross-validation for each risk minimizer and each random sampling.\n3See http://www.raetschlab.org/Members/raetsch/benchmark/ for IDA, http://archive.ics. uci.edu/ml/ for UCI, http://olivier.chapelle.cc/ssl-book/ for the SSL book and https://www. elen.ucl.ac.be/neural-nets/Research/Projects/ELENA/ for the ELENA project.\nThe results by varying nu and \u03c0 are reported in Figures 2 and 3 respectively. Similarly to Figure 1, in Figure 2, n+ = 25, n\u2212 = 5, \u03c0 = 0.5, and nu varies from 10 to 300, while in Figure 3, n+ = 25, n\u2212 = 5, nu = 200, and \u03c0 varies from 0.05 to 0.95. Figures 2(a) and 2(k) depict \u03b1pu,pn and \u03b1nu,pn as functions of nu and \u03c0, and all the remaining subfigures depict means with standard errors of the misclassification rates based on 100 random samplings for every nu and \u03c0.\nThe theoretical and experimental results based on benchmarks are still highly consistent. However, unlike in Figure 1(b), in Figure 2 only the errors of g\u0302pu decrease with nu, and the errors of g\u0302nu just fluctuate randomly. This may be because benchmark data are more difficult than artificial data and hence n\u2212 = 5 is not sufficiently informative for g\u0302nu even when nu = 300. On the other hand, we can see that Figures 2(k) and 1(c) look alike, and so do all the remaining subfigures in Figure 3 and Figure 1(d). Nevertheless, three intersections in Figure 2(k) are closer than those in Figure 1(c), as nu = 200 in Figure 2(k) and nu = 100 in Figure 1(c). The three intersections will become a single one if nu =\u221e. By observing the experimental results, three curves in Figure 3 are also closer than those in Figure 1(d) when \u03c0 \u2265 0.6, which demonstrates the validity of our theoretical findings."}, {"heading": "5 Conclusions", "text": "In this paper, we studied a fundamental problem in PU learning, namely, when PU learning is likely to outperform PN learning. Estimation error bounds of the risk minimizers were established in PN, PU and NU learning. We found that under the very mild assumption (5): The PU (or NU) bound is tighter than the PN bound, if \u03b1pu,pn in (14) (or \u03b1nu,pn in (15)) is smaller than one (cf. Theorem 6); either the limit of \u03b1pu,pn or that of \u03b1nu,pn will be smaller than one, if the size of U data increases faster in order than the sizes of P and N data (cf. Theorem 7). We validated our theoretical findings experimentally using one artificial data and nine benchmark data."}, {"heading": "Acknowledgments", "text": "GN was supported by the JST CREST program and Microsoft Research Asia. MCdP, YM, and MS were supported by the JST CREST program. TS was supported by JSPS KAKENHI 15J09111."}, {"heading": "A Proofs", "text": "In this appendix, we prove Theorem 1 in Section 2, and Lemma 8, Theorem 2, and Corollary 5 in Section 3. The proofs of Theorems 3 and 4 are omitted, since they are essentially similar to that of Theorem 2 relying on slightly different uniform deviation bounds.\nA.1 Proof of Theorem 1\nThe proof is straightforward. Denote by \u03c0+(x) = p(Y = +1 | X = x), \u03c0\u2212(x) = p(Y = \u22121 | X = x),\nthen the conditional risk is EY [`sr(g(X), Y ) | X = x] = \u03c0+(x)`sr(g(x),+1) + \u03c0\u2212(x)`sr(g(x),\u22121)\n=  \u03c0+(x), g(x) \u2264 \u22121, 1/2\u2212 (\u03c0+(x)\u2212 \u03c0\u2212(x))g(x)/2, \u22121 < g(x) < +1, \u03c0\u2212(x), g(x) \u2265 +1.\nThe minimum is achieved by g(x) = sign(\u03c0+(x)\u2212 \u03c0\u2212(x)), which is actually the Bayes classifier. Therefore, `sr is classification-calibrated according to Theorem 1.3.c in [19].\nA.2 Proof of Lemma 8\nSimilarly to the decomposition in Eq. (3) such that R(g) = 2\u03c0R+(g) +Ru,\u2212(g)\u2212 \u03c0,\nwe have seen in the definition of R\u0302pu(g) that it can also be decomposed into\nR\u0302pu(g) = 2\u03c0R\u0302+(g) + R\u0302u,\u2212(g)\u2212 \u03c0, where\nR\u0302+(g) = 1 n+ \u2211 xi\u2208X+ `(g(xi),+1), R\u0302u,\u2212(g) = 1 nu \u2211 xj\u2208Xu `(g(xj),\u22121)\nare the empirical averages corresponding to R+(g) and Ru,\u2212(g). Due to the sub-additivity of the supremum operators, it holds that\nsupg\u2208G |R\u0302pu(g)\u2212R(g)| \u2264 2\u03c0 supg\u2208G |R\u0302+(g)\u2212R+(g)|+ supg\u2208G |R\u0302u,\u2212(g)\u2212Ru,\u2212(g)|. As a result, in order to prove Lemma 8, it suffices to show that with probability at least 1\u2212 \u03b4/2, the uniform deviation bounds below hold separately:\nsupg\u2208G |R\u0302+(g)\u2212R+(g)| \u2264 2L`Rn+,p+(G) + \u221a ln(4/\u03b4) 2n+ , (16)\nsupg\u2208G |R\u0302u,\u2212(g)\u2212Ru,\u2212(g)| \u2264 2L`Rnu,p(G) + \u221a ln(4/\u03b4) 2nu . (17)\nIn the following we prove (16), and then (17) can be proven using the same proof technique.\nSince the surrogate loss ` is bounded by 0 and 1 according to (2), the change of R\u0302+(g) will be no more than 1/n+ if some xi in X+ is replaced with x\u2032i. Thus McDiarmid\u2019s inequality [31] implies\nPr [ |R\u0302+(g)\u2212R+(g)| \u2265 ] \u2264 2 exp ( \u2212 2 2\nn+(1/n+)2 ) for any fixed g. Equivalently, for any fixed g, with probability at least 1\u2212 \u03b4/2,\n|R\u0302+(g)\u2212R+(g)| \u2264 \u221a\nln(4/\u03b4) 2n+ .\nThen, according to the basic uniform deviation bound using the Rademacher complexity [18], with probability at least 1\u2212 \u03b4/2,\nsupg\u2208G |R\u0302+(g)\u2212R+(g)| \u2264 2Rn+,p+(` \u25e6 G) + \u221a ln(4/\u03b4) 2n+ , (18)\nwhere Rn+,p+(` \u25e6 G) is the Rademacher complexity of the composite function class (` \u25e6 G) for the sampling of size n+ from p+(x) defined by\nRn+,p+(` \u25e6 G) = EX+\u223cpn++ E\u03c3 [ supg\u2208G 1 n+ \u2211 xi\u2208X+ \u03c3i`(g(xi),+1) ] .\nAs `(t, y) is L`-Lipschitz-continuous in t for every y, we have Rn+,p+(` \u25e6 G) \u2264 L`Rn+,p+(G) by Talagrand\u2019s contraction lemma [32], which proves (16).\nA.3 Proof of Theorem 2\nBased on Lemma 8, the estimation error bound (7) is proven through R(g\u0302pu)\u2212R(g\u2217) = ( R\u0302pu(g\u0302pu)\u2212 R\u0302pu(g\u2217) ) + ( R(g\u0302pu)\u2212 R\u0302pu(g\u0302pu) ) + ( R\u0302pu(g \u2217)\u2212R(g\u2217) )\n\u2264 0 + 2 supg\u2208G |R\u0302pu(g)\u2212R(g)| \u2264 8\u03c0L`Rn+,p+(G) + 4L`Rnu,p(G) + 2\u03c0 \u221a 2 ln(4/\u03b4) n+ + \u221a 2 ln(4/\u03b4) nu ,\nwhere we have used R\u0302pu(g\u0302pu) \u2264 R\u0302pu(g\u2217) by the definition of g\u0302pu. Moreover, if ` is classification-calibrated, Theorem 1 in [19] implies that there will exist a convex, invertible and nondecreasing transformation \u03c8` with \u03c8`(0) = 0, such that\n\u03c8`(I(g\u0302pu)\u2212 I\u2217) \u2264 R(g\u0302pu)\u2212R\u2217.\nHence, let \u03d5 = \u03c8\u22121` , we have\nI(g\u0302pu)\u2212 I\u2217 \u2264 \u03d5(R(g\u0302pu)\u2212R\u2217) = \u03d5(R(g\u2217)\u2212R\u2217 +R(g\u0302pu)\u2212R(g\u2217)),\nand subsequently the excess risk bound (8) is an immediate corollary of (7).\nA.4 Proof of Corollary 5\nGiven (5), the estimation error bound (7) can be rewritten into\nR(g\u0302pu)\u2212R(g\u2217) \u2264 8\u03c0L`CG/ \u221a n+ + 2\u03c0 \u221a 2 ln(4/\u03b4) n+ + 4L`CG/ \u221a nu + \u221a 2 ln(4/\u03b4) nu\n= 2\u03c0f(\u03b4)/ \u221a n+ + f(\u03b4)/ \u221a nu,\nwhere f(\u03b4) = 4L`CG + \u221a\n2 ln(4/\u03b4). This proves (12). In exactly the same way, we could get (11) from (9) and (13) from (10).\nConsider the special case of G defined in (6). Recall that Rn,q(G) is the Rademacher complexity of G for X = {x1, . . . , xn} with each xi drawn from q(x). Given any such X , denote by R\u0302X (G) the empirical Rademacher complexity of G conditioned on X [18]:\nR\u0302X (G) = E\u03c3 [ supg\u2208G 1 n \u2211 xi\u2208X \u03c3ig(xi) ] .\nIt is known that R\u0302X (G) \u2264 CwC\u03c6/ \u221a n and thus Rn,q(G) = EX [R\u0302X (G)] \u2264 CwC\u03c6/ \u221a n [18]. Then, letting CG = CwC\u03c6 completes the proof."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data<lb>without negative (N) data. Although N data is missing, it sometimes outperforms<lb>PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor<lb>experimental analysis has been given to explain this phenomenon. In this paper,<lb>we theoretically compare PU (and NU) learning against PN learning based on the<lb>upper bounds on estimation errors. We find simple conditions when PU and NU<lb>learning are likely to outperform PN learning, and we prove that, in terms of the<lb>upper bounds, either PU or NU learning (depending on the class-prior probability<lb>and the sizes of P and N data) given infinite U data will improve on PN learning.<lb>Our theoretical findings well agree with the experimental results on artificial and<lb>benchmark data even when the experimental setup does not match the theoretical<lb>assumptions exactly.", "creator": "LaTeX with hyperref package"}}}