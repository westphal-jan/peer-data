{"id": "1603.04402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2016", "title": "Controlling Search in Very large Commonsense Knowledge Bases: A Machine Learning Approach", "abstract": "howett Very large allsvenskan commonsense knowledge bases (luganda KBs) often 9.60 have hibel thousands to elvie millions new-born of centreboard axioms, of commuter which 283.4 relatively few tarquinia are 242,000 relevant 11.33 for test-fired answering any given 300-point query. A silliest large uehara number of irrelevant a614 axioms can falna easily curs overwhelm resolution - based jocelin theorem submit provers. jean-loup Therefore, 25,000,000 methods that huang-lao help dalassenos the laming reasoner 27,400 identify useful viktorovna inference paths taney form an ether essential part vegetable of large - toastmasters scale utsava reasoning systems. riebeeck In wsbk this paper, we describe carlita two bouchareb ordering heuristics for optimization ishino of a71 reasoning linking in such meralco systems. First, fealy we bruijn discuss sergels how decision trees quintaglios can chalcedony be daozi used to clein select itzig inference brodus steps pesce that salsabil are pizzerias more lim\u00f3n likely 49.13 to oversensitive succeed. goldsberry Second, lexcen we identify undershirts a small set of problem instance features via that corpuscles suffice provokes to guide tyutin searches away ethnographies from intractable d.o.o. regions cassiopeia of the search space. We instituted show the efficacy hasbrouck of these 02107-2378 techniques via smartcard experiments on hajela thousands camondo of gibowski queries from the Cyc KB. stettin Results show that mac-10 these methods balkars lead to an order herman of mcelroy magnitude stankovi\u0107 reduction in friskies inference time.", "histories": [["v1", "Mon, 14 Mar 2016 19:20:36 GMT  (681kb)", "http://arxiv.org/abs/1603.04402v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["abhishek sharma", "michael witbrock", "keith goolsbey"], "accepted": false, "id": "1603.04402"}, "pdf": {"name": "1603.04402.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Commonsense reasoning has always been a core problem for artificial intelligence (AI) systems. Effective reasoning about the external world often involves drawing deductively valid conclusions from known facts. Unfortunately, given the combinatorial explosiveness of reasoning in expressive knowledge-based systems (KBS), even simple queries might get \u201clost\u201d in millions of seemingly relevant inference paths. Efficient reasoning in such systems is critical for building large-scale AI systems. Ordering heuristics play an important role in optimization of reasoning in KBS for at least two reasons: First, inference algorithms of KBS (e.g., backward chaining [Russell and Norvig 2003] in Cyc, tableaux algorithms in description logic (DL)) typically represent the search space as a graph, the structure of which is determined by the rules applicable to the given node in the graph. Generally, many rules might simultaneously apply to a given vertex, and the order of rule expansion can have a significant effect on efficiency [Tsarkov and Horrocks 2005]. Second, researchers have used first-order logic (FOL) theorem provers as tools for inference\nwith very expressive languages (e.g., OWL DL, the Semantic Web Rule Language (SWRL)) where reasoning with the complete language is beyond the scope of existing DL algorithms or the language does not correspond to any decidable fragment of FOL [Tsarkov et al. 2004, Horrocks and Voronkov 2006]. Very large FOL systems often have thousands to millions of axioms, of which only a few are relevant for answering any given query [Hoder and Voronkov 2011, Tsarkov et al. 2004]. Since hundreds of thousands of axioms that are irrelevant for a given query might overwhelm resolution-based theorem provers, the reasoner is expected to assess the utility of further expanding numerous incomplete inference paths. A na\u00efve ordering of paths can lead to potentially infinite subtrees and cause unproductive backtracking. To make inferences more efficient, this paper suggests two types of ordering heuristics. First, we discuss how implausible search paths are created when domain-specific axioms are used to prove queries involving fairly general predicates. We argue that decision trees can be used to represent the semantic context in which a rule is likely to contribute to a proof. We show that ordering nodes with the help of decision trees helps in guiding search toward the solution. Second, a key impediment in the development of fast broad-application first-order reasoning systems has been an insufficient understanding of what makes problems difficult. We propose a comprehensive set of features that correlate with the answerability of nodes. We run the inference engine on a large number of queries, sample nodes from the resulting search graphs, and record values for their instance features. We use statistical regression methods to derive a model for predicting the answerability of nodes. We use the resulting model to order nodes during search and demonstrate that this improves search performance.\nThis paper is organized as follows: We start by discussing relevant previous work. Our decision tree algorithm and statistical regression methods are discussed next. We conclude by discussing our results and plans for future work.\n2 Related Work\nPrior research has examined the use of machine learning to identify best heuristics1 for problems [Bridge et al. 2014] and\nto select a small set of axioms/lemmas that are most relevant for answering a set of queries [Hoder and Voronkov 2011, Sharma and Forbus 2013, Meng and Paulson 2009, Kaliszyk et al. 2015, Kaliszyk and Urban 2015, Alama et al. 2014]. In contrast, we focus on ordering heuristics that enable inference algorithms to reason with all axioms. In [Taylor et al. 2007], the authors use reinforcement learning to guide inference, whereas in [Tsarkov and Horrocks 2005], the authors study different types of rule-ordering heuristics (e.g., preference between \u2203 and \u2a06 rules) and expansion-ordering heuristics (e.g., descending order of frequency of usage of each of the concepts in the disjunction). This paper proposes that ruleordering heuristics should be based on the search state, and we use a regression-based model to learn the effects of different features on the answerability of nodes. Work in\nother fields (e.g., database community [Chaudhuri 1998], SAT reasoning [Hutter et al. 2014], answer set programming [Brewka et al. 2011]) is less relevant because the studies do not address the complexity of deep and cyclic search graphs that arise from expressive first-order reasoning. To the best of our knowledge, no work in the AI community has used decision trees and statistical regression-based methods to control inference in large commonsense reasoning systems."}, {"heading": "3 Background", "text": "We assume familiarity with the Cyc representation language [Lenat and Guha 1990, Matuszek et al. 2006, Taylor et al. 2007]. In Cyc, concept hierarchies are represented by the \u2018genls\u2019 relation. For instance, (genls Person Mammal) holds. During backward inference, the rule P(x) \u2192 Q(x) is used to transform a query like Q(a) into P(a). The link between Q(a) and P(a) is a type of transformation link. A node like s0: (and (performedBy ?x JohnMcCarthy-ComputerScientist) (isa ?x Buying)) leads to sub-goals like s1: (isa JohnMcCarthyBuysABook-012 Buying) and s2: (isa JohnMcCarthyWritesAPaper-087 Buying), some of which may be satisfiable. The links between s0 and s1 and s0 and s2 are examples of restriction links. Transformation and restriction links play a major role in determining the outdegree of nodes. Every node in the search graph is timestamped with an id. A node y is called a successor of x if there is a path consisting of transformation links from x to y and id(x) < id(y). A node x is a parent of y if a transformation link exists between x and y and id(x) < id(y). Parents(x) and Successors(x) denote the sets of all parents and successors of node x respectively. Let S be the set of all nodes in a search graph. Then, a transformation link set p = {a(1), a(2), \u2026,a(n)} is a set of transformation links that transform an initial state s0 to an intermediate state sn. Rule(a) and\n1 Examples of heuristics (or strategies) include \u201cgive priority to\naxioms in clause selection\u201d and \u201csort symbols by inverse frequency\u201d. 2 For instance, the negated query is often used as the set of support.\nSubstitutions(a) denote the rule and bindings associated with the transformation link a. Transitive inference is well supported in Cyc. The query (genls ?x Person) has more than 6,700 answers because the predicate \u2018genls\u2019 allows transitive inference in its first argument position. The aforementioned query has one open transitive argument position. Reasoning in Cyc KB is difficult due to the sheer size of the KB and the expressiveness of the CycL representation language. In its default inference mode, the Cyc inference engine uses the following types of axioms/facts during backward inference: (i) 21,743 role inclusion axioms (e.g., P(x, y) \u2192 Q(x, y)), (ii) 2,601 inverse role axioms (e.g., P(x, y) \u2192 Q (y, x)), (iii) 365,593 concepts and 986,965 concept inclusion axioms (i.e., \u2018genls\u2019 facts), (iv) 817 transitive roles, (v) 99,238 complex role inclusion axioms (e.g., P(x, y) \u02c4 Q (y, z) \u2192 R (x, z)), and (vi) 31,897 binary roles and 7,980 roles with arities greater than two. The KB has 21.7 million assertions and 652,037 individuals. To control search in such a large KBS, inference algorithms often use different control strategies. They distinguish between a set of clauses known as the set of support2 that define the imporant facts about the problem and a set of usable axioms that are outside the set of support (e.g., see the OTTER theorem prover [Russell and Norvig 2003]). At every step, such theorem provers resolve an element of the set of support against one of the usable axioms. To perform best-first search, a heuristic control strategy mesures the \u201cweight\u201d of each clause in the set of support, picks the \u201cbest\u201d clause, and adds to the set of support the immediate consequences of resolving it with the elements of the usable list [Russell and Norvig 2003]. Cyc uses a set of heuristic modules to identify the best clause from the set of support. A heuristic module is a tuple hi = (wi, fi), where fi is a function fi: S \u2192 \u211d that assesses the quality of a node, and wi is the weight of hi. The net score of a node s is \u03a3iwifi(s), and the node with the highest score is selected for further expansion. In next two sections, we discuss two heuristic modules for focusing search."}, {"heading": "4 Decision Trees for Focused Search", "text": "The basic idea behind this approach is best explained with a few examples. Consider the rules shown below3: (sitTypeIsSpecWithTypeRestrictionOnRolePlayer ?absorption PhotonAbsorption absorber ?type) \u02c4 (sitTypeIsSpecWithTypeRestrictionOnRolePlayer ?excitation ChemicalObjectExcitation objectOfStateChange ?type)\n\u2192 (cotemporalProperSubEventTypes ?absorption ?excitation) (Rule A1)\n(objectFoundInLocation ?ARG1 ?ARG2) \u02c4 (geopoliticalSubdivision ?OTHER ?ARG2) \u2192 (objectFoundInLocation ?ARG1 ?OTHER) (Rule A2)\nTo answer the query (properSubEventTypes BirthdayParty ?x), an inference engine would backchain on rule A1 (see footnote\n3 A sentence of the form (sitTypeIsSpecWithTypeRestrictionOnRolePlayer SPEC\nSIT-TYPE ROLE TYPE) means that SPEC is the unique specialization of SITTYPE, a specialization of \u2018Situation\u2019, such that all objects that play ROLE in instances of SPEC are instances of TYPE.\n3) because \u2018cotemporalProperSubEventTypes\u2019 is a sub-role of \u2018properSubEventTypes\u2019. This transformation would lead us to reason about photon absorption4. Similarly, we would backchain on rule A2 to answer the query (objectFoundInLocation ?x MesophyllCell-001). Such search paths are unlikely to succeed. General knowledge bases often have heavily used predicates with hundreds of specializations. These specializations partition the space into several domains. For example, while rule A1 is expected to be useful for na\u00efve physics, A2 is expected to be useful when reasoning about geographic sites. Implausible search paths arise when a mismatch exists between the query and the implied context in which an axiom is likely to work. In this paper, we suggest that type/concept based decision trees are the right representation choice for this problem because rules are expected to fire for a certain class (or type) of things. Therefore, we associate restrictive information with the variables of axioms. Although the variables are expected to range over their entire domain, the restrictive information specifies a subset of the domain over which the rule has been observed to work. These restrictions are specified in terms of sorts or concepts. They derive from the results of successful uses of the given rule. A small set of successful bindings for rule 2 is shown in Table 1. The fact that Minneapolis, Anaheim, and Rochester are US cities helps us derive the sorted generalization [Page and Frisch 1992] that ?ARG2 is likely to range over the set USCity. Formally, a restriction condition is a pair, x:\u03c4, where x is a variable and \u03c4 is a concept. Let \u03a3 denote the set of sentences that represent relationships among the concepts. Then, a substitution \u03b8 satisfies the restriction condition x:\u03c4 if it maps x to a ground term t and \u03a3 \u255e \u03c4(t). The jth restriction condition for axiom a, RC(a, j), can be represented as \u22c0 \ud835\udc65(\ud835\udc56): \ud835\udf0f(\ud835\udc56)\ud835\udc65(\ud835\udc56)\ud835\udf16 \ud835\udc49\ud835\udc4e\ud835\udc5f\ud835\udc60(\ud835\udc4e) , where Vars(a) is the set of variables in a. A disjunction of such constraints can be specified as \u22c1 \ud835\udc45\ud835\udc36(\ud835\udc4e, \ud835\udc56)\ud835\udc56 , and decision trees are a natural representation for such constraints. Our algorithm for constructing a decision tree from a set of successful rule bindings is shown in Figure 1. The compact decision tree (induced from 1900 bindings) for rule 2 is simply: ?ARG2:GeopoliticalEntity \u02c4 ?OTHER:GeographicalRegion \u02c4 ?ARG1:TerrestrialFunctioningObject\nThe algorithm CreateTree (shown in Figure 1) takes as input a training set and the variables that occur in the rule. The training set is generated by querying the antecedent of the\n4 Readers might wonder about domain constraints. The first\nargument to \u2018sitTypeIsSpecWithTypeRestrictionOnRolePlayer\u2019 is expected to be a specialization of \u2018Situation\u2019, and the concept \u2018BirthdayParty\u2019 satisfies this condition. The generality of some domain constraints ensures that it is difficult to identify implausible sub-queries.\nrule for a fixed duration of time. The bindings returned by the query results form the TrainingSet. Given a tuple from the training set (see Table 1), we compute the generalizations of the bindings.5 In step 3 of the algorithm, membership in the most specific maximally covering generalization is chosen as the branching test. When a tuple satisfies this test, we explore constraints for other variables in the AND branch (step 6). Otherwise, other values for the variable are considered in the OR branch (step 7). We stop growing the tree (step 2) when the number of unexplained training examples is less than a pre-determined fraction of the full training set. The complexity of top-down decision tree induction is O(m2.n) where m is the number of attributes, and n is the size of the training set [Kent and Hirschberg 1996]. To use decision trees during search we can define a heuristic module with the following function, fDT(s), for assesing the quality of nodes: \u2211 \u2211 1\n|\ud835\udc5d| \ud835\udc3c(\ud835\udc46\ud835\udc62\ud835\udc4f\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60(\ud835\udc4e(\ud835\udc56)), \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc52(\ud835\udc45\ud835\udc62\ud835\udc59\ud835\udc52(\ud835\udc4e(\ud835\udc56))))\ud835\udc4e(\ud835\udc56)\u2208\ud835\udc5d\ud835\udc5d\u2208\ud835\udc3f(\ud835\udc60) ..(1)\nIn (1), L(s) is the set of all transformation link sets from node s to the root, and I(\u03b8, tree) is 1 when the substitution \u03b8 satisfies the restriction conditions specified by the tree, and 0 otherwise6. This module will prioritize search paths that satisfy the restriction conditions. For instance, the path that uses rule A2 to answer the query (objectFoundInLocation ?x MesophyllCell-001) would be expanded late because MesophyllCell-001 is not transitively a GeographicalRegion (see the constraint for variable ?OTHER). This helps in early evaluation of inference steps that use rules from the domains that are pertinent for the given query. However, since the KB might have thousands of rules that are relevant for a query, we also need other ways to steer the search toward more productive states. In the next section, we propose a statistical approach to solving this problem.\n5 The generalization of a substitution, s, is the set Gen(s) = {c |\n\u03a3 \u255e (isa s c)}. For instance, PopulatedPlace is a generalization of Minnesota-State. 6 The \u2018Tree\u2019 function in (1) returns the decision tree for a given rule."}, {"heading": "5 Statistical Meta-Search Learning", "text": "Is it possible to predict whether an inference engine will be able to solve an arbitrary node generated during search? In this section, we show how supervised machine learning methods can be used to build models that predict the number of answers for a problem instance. Such models can be used by the inference engine to decide how to allocate computational resources. Moreover, by shedding light on the sources of hardness in problem instances, they help in improving knowledge representation and fuel development of new algorithms.\nTo build such models, we take the following steps: (i) Identification of features: First, we identify key parameters that represent all known relevant features of problem instances. (ii) Data collection: Next, we run the inference engine on a large set of queries and sample nodes from the generated search graph. For each sampled node, the number of answers and a set of feature values are recorded. (iii) Learning: Finally, we learn a model that maps from instance features to the inference engine\u2019s performance, and evaluate it on a test set of queries. After introducing some notation that is used in Figure 2, we discuss each of these steps in detail. Notation: Let P and TERMS denote the set of predicates and terms mentioned in the query respectively. For any predicate p, let NumGafs(p) and NumRules(p) denote the number of ground atomic formulas and number of relevant rules for p. Moreover, Cyc maintains an estimate of generality of any\nterm based on its position in the ontology. Let TermGenerality (t) denote the generality of any term t. Feature Identification: Our features and their cost of computation are shown in Figure 2. Broadly, they can be divided into ten groups. The first group includes wellunderstood problem size and type features including number of literals and number of variables. The second group contains those attributes that involve examining the path that led to the node. This includes important features that help in maintaining the right shape of the search space. While the feature \u201cdepth\u201d is critical in ensuring that the inference engine is not trapped in depth-first infinite regress, the feature \u201cnumber of transformation links\u201d helps us control the outdegree of nodes. Figures 3 and 4 show the trade-off between depth-first and breadth-first search. We see that the conditional probability of success of a node decreases rapidly with depth. Similarly, Figure 4 shows that most of the successful transformation links are added in the initial phase and the utility of adding an additional transformation link drops rapidly. Table 2 shows the conditional probability of success of nodes as a function of number of literals.\nFigure 4: The x-axis shows the index range of transformation links, and the y-axis shows the number of successful transformation links in the given range.\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n1-5 6-10 11-15 16-20 21-25 26-30 31-100\nTransformation Link Index Range (i)\nN u\nm b\ner o\nf Su\ncc es\nsf u\nl T -l\nin k\ns\nin R\na n\ng e\ni\nThe potential fan-out score of a node is a function of the number of rules that can potentially be used with it. Formally it is \u03a3P log10 (1+NumRules(p)). The next group includes features that encode the level of knowledge the KB has for predicates and terms mentioned in the query. The generality estimate mentioned in the third group is defined as \u220fP log10 (1+TermGenerality(p)). In the fourth group, we include attributes for understanding the cost of transitive queries. The probing feature is a special kind of feature that examines the neighborhood of the node to assess its quality. Since locally available information at a node is insufficient for gauging the complexity of the search space below it, in the fifth group, we include features that pick a random path originating at the given node and record descriptive statistics of various properties of interest (e.g., number of literals, out-degree of nodes). For example, the third feature of the fifth group is computed by finding the median out degree of nodes encountered in the randomly selected path.The sixth group captures the balance of the node in two ways: we measure the ratio of number of variables and literals, and the ratio of number of positive and negative literals. Because we expect disjunctive queries to be more difficult, we also note whether non-Horn axioms were used in deriving the state. In the seventh and eighth group, we included quadratic and interaction terms for some salient features. Data Collection: The Cyc KB contains thousands of stored queries of various level of difficulty. We gathered a large amount of data by sampling and running these queries. Forty percent of the nodes from the resulting space were sampled, and the values of the 46 features shown in Figure 2 were recorded. This produced 2.5 million data points. Data Transformation and Learning: Recall that the number of answers is the performance measure, and all other features shown in Figure 2 are predictor features. We performed z-score normalization of the predictor variables by subtracting the mean and dividing the difference by their standard deviation7. Given the extreme variability in the number of answers, we use a log-transformation on the result feature (i.e., we predict log10(1+ number of answers)). In our initial unpublished work, we experimented with classification techniques (e.g., na\u00efve Bayes, logistic regression) to predict the likelihood of success of a node. Since the results were not very encouraging, we switched to multiple linear regression. In linear regression, the aim is to learn a function of the form fSL(s) = \u03a3 wigi (s), where wi is the weight for the ith feature of node s, gi(s). The function fSL(s) can then be used by a heuristic module to assess the quality of nodes. The values of wi are determined by minimizing the metric root mean squared error (RMSE). The R software was used to estimate the values of wi [R Core Team 2015]. We used the repeated random sub-sampling approach and 10- fold cross validation to validate our model. While using the former method, 90% of the data was selected at random for\n7 Missing feature values are ignored during normalization, and then set to zero during training. This ensures that they are minimally informative because they are equal to the mean of the distribution [Hutter et al. 2014].\ntraining and the rest were used as the test set. This process was repeated 50 times. The mean RMSE from the two validation methods was 0.47 and 0.76 respectively. Multiple R2 and adjusted R2 for this model were equal to 0.76, and the F-statistic was 1.8*105. For a variety of reasons, the features can be uninformative, correlated or redundant. Therefore, we use feature selection methods to identify a small set of features that explains the variance in data as well as the full set of features8. Such analysis helps us to identify properties of nodes that strongly affect empirical performance. The set of best 10 features as identified by an exhaustive subset selection method is shown in bold in Figure 2. The R2 value of subset models with these 10 features converged to that of models with all inputs. The presence of features such as \u201cKnuth\u2019s tree size estimate\u201d [Knuth 1975], \u201cratio of number of positive and negative literals\u201d and \u201c(number of transformation links)2\u201d in the selected list suggests the following: (i) Locally available information is insufficient for predicting the complexity of search space, and probing features play an important role in guiding a search. (ii) ensuring that the search graph has the right shape is of critical importance, and reasoners need to find the balance between \u201cdepth-first\u201d and \u201cbreadth-first\u201d search; (iii) negated literals and disjunctive queries are more difficult to answer. In the next section, we evaluate how these heuristics help the inference engine in answering queries."}, {"heading": "6 Experimental Results", "text": "The selection of benchmark instances for testing the efficacy of heuristics is an important factor in any empirical analysis. Our selection of problem instances was guided by two principles: (i) The benchmark set should consist of queries that are intrinsically difficult to solve for the inference engine. Therefore, we excluded simple queries that can be answered without any backchaining in a few milliseconds (e.g., (isa MarvinMinsky Person), (genls Dog Carnivore)). We focused on queries that needed several transformations (i.e., depth of rule back-chaining) to be answered. (ii) While artificially crafted and randomly generated problem instances are very useful for understanding how syntactic properties affect the behavior of algorithms, the right methodology for generating such instances has not received much attention in the commonsense reasoning community. Therefore, this work focused on problems from real-world applications. The Cyc KB has thousands of queries that have been created by knowledge engineers and programmers for various projects (e.g., Project HALO [Friedland et al. 2004], HPKB project [Cohen et al. 1998]) and for testing the question-answering capability of the system. These queries are of varying levels of difficulty: some of them need just one transformation, others required the inference engine to back-chain on heavily used predicates that can lead to huge fan-out and high search\n8 We have experimented with forward, backward and exhaustive subset selection methods. All three methods lead to very similar set of selected features.\ncost. We ensured that queries of both types were well represented in our test sets9. Based on the terms mentioned in them, the queries were divided into three test sets: (i) Test Set 1: Military and asymmetrical warfare domain, (ii) Test Set 2: Biology domain, and (iii) Test Set 3: Others (e.g., commonsense queries). The English translation of a query from test set 2 is shown below:\nRecall that the inference engine uses a set of heuristics for ordering nodes during search, and the net score of a node can be written as f(s) = w0+ w1.fDT(s)+ w2.fSL(s). Here, fDT(s)and fSL(s) refer to the scores returned by decision tree and statistical learning models discussed above. The first term, w0, is the score returned by heuristics not discussed in this paper. In Table 3, the \u201cbaseline\u201d version is obtained by setting both w1 and w2 to zero. By setting w2 to 0, we can assess the efficacy of decision tree heuristics (rows labeled \u201cDT\u201d in Table 3). Similarly, we can study the utility of statistical learning models by setting w1 to 0 (rows labeled \u201cSL\u201d in Table 3). The net contribution of both methods is shown in rows labeled \u201cDT+SL.\u201d The experimental data was collected on a 4-core 3.40 GHz Intel processor with 32 GB of RAM. We used 18,383 decision trees, and ten best features identified by subset selection in these experiments. Due to\n9 The difficulty level of these queries can be gauged by looking at the average time requirements of the \u201cbaseline\u201d version in Table 3. Initially some of the queries in our test set could not be answered in 20 minutes.\nthe large time requirements of these queries, we restricted the cutoff time of each query to 5 minutes. Table 3 contains the results for three test sets. We see that both decision trees and multiple regression based models have led to significant speedups. The average speedup is a factor of 14. Since these heuristics steer the inference engine towards more productive parts of the search space, they improve question-answering (Q/A) performance too. The fifth column in Table 3 (labeled \u201cQ/A Imp. (%)\u201d) shows the improvement in Q/A performance with respect to the baseline."}, {"heading": "7 Conclusion", "text": "Deep deductive reasoning over large commonsense knowledge bases is critical for modern AI systems. The intractability of first-order logic has presented interesting research opportunities for understanding the causes of problem hardness and developing new algorithms for surmounting them. In this article, we have described two techniques to make reasoning more efficient. The first uses decision trees to guide the search toward germane rules by representing the semantic context in which a rule is expected to produce results. The second uses statistical regression techniques to provide an estimate of the number of answers a node is expected to provide based on search meta-features. The inference engine uses these heuristics to order nodes during search. Experimental results over thousands of queries show an order of magnitude speedup. These results suggest several lines of future work. First, we need to test these heuristics over even larger set of queries to understand their dynamics. Second, we want to extend our decision tree implementation to make probabilistic assessments. Next, we would like to experiment with other statistical models (e.g., regression splines, random forests) to improve the model quality. Finally, we believe that coupling this approach with a decision-theoretic model [Smith 1989, Greiner 1991] could yield a more complete theoretical model for making reasoning more efficient."}], "references": [{"title": "Premise Selection for Mathematics by Corpus Analysis and Kernel Methods", "author": ["J. Alama", "T. Heskes", "D. Kulhwein", "E. Tsivtsivadze", "J. Urban"], "venue": "Journal of Automated Reasoning, 52(2):191\u2013213", "citeRegEx": "Alama et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Answer set Programming at a Glance", "author": ["G. Brewka", "T. Eiter", "M. Truszcynski"], "venue": "Communications of the ACM, 54(12). pages 91-103", "citeRegEx": "Brewka et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "An Overview of Query Optimization in Relational Systems", "author": ["S. Chaudhuri"], "venue": "Proceedings of PODS, pages 34-43", "citeRegEx": "Chaudhuri 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "The DARPA HighPerformance Knowledge Bases Project", "author": ["Cohen"], "venue": "AI Magazine,", "citeRegEx": "Cohen,? \\Q1998\\E", "shortCiteRegEx": "Cohen", "year": 1998}, {"title": "B", "author": ["N.S. Friedland", "P.G. Allen", "G. Matthews", "M. Witbrock", "J. Curtis"], "venue": "Shepard, et al.. Project Halo: Towards a Digitial Aristotle. AI Magazine, 25(4), pages 29-47", "citeRegEx": "Friedland et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Finding Optimal Derivation Strategies in Redundant Knowledge Bases", "author": ["R. Greiner"], "venue": "Artificial Intelligence, 50(1), pages 95-115", "citeRegEx": "Greiner 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "Sine qua non for Large Theory Reasoning", "author": ["K. Hoder", "A. Voronkov"], "venue": "Proceedings of the CADE-23, pages 299-314", "citeRegEx": "Hoder and Voronkov 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Reasoning Support for Expressive Ontology Languages Using A Theorem Prover", "author": ["I. Horrocks", "A. Voronkov"], "venue": "Foundations of Information and Knowledge Systems, pages 201-218, Springer", "citeRegEx": "Horrocks and Voronkov 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithm Runtime Prediction: Methods and Evaluation", "author": ["Hutter"], "venue": "Artificial Intelligence,", "citeRegEx": "Hutter,? \\Q2014\\E", "shortCiteRegEx": "Hutter", "year": 2014}, {"title": "Efficient Semantic Features for Automated Reasoning Over Large Theories", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "Proceedings of IJCAI", "citeRegEx": "Kaliszyk et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Assisted Theorem Proving with Millions of Lemmas", "author": ["C. Kaliszyk", "J. Urban"], "venue": "Journal of Symbolic Computation, 69:109-128", "citeRegEx": "Kaliszyk and Urban 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Complexity of Learning Decision Trees", "author": ["M.J. Kent", "D.S. Hirschberg"], "venue": "International Symposium on Artificial Intelligence and Mathematics, pages 112-115", "citeRegEx": "Kent and Hirschberg 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Estimating the Efficiency of Backtrack Programs Mathematics of Computation", "author": ["D. Knuth"], "venue": "29(129):121-136", "citeRegEx": "Knuth 1975", "shortCiteRegEx": null, "year": 1975}, {"title": "Buliding Knowledge-based Systems: Representation and Inference in the Cyc Project", "author": ["D.B. Lenat", "R.V. Guha"], "venue": "Addison Wesley", "citeRegEx": "Lenat and Guha 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "An Introduction to the Syntax and Content of Cyc. In AAAI Spring Symposium: Formalizing and Compiling Background Knowledge and its Applications to Knowledge Representation", "author": ["Matuszek"], "venue": null, "citeRegEx": "Matuszek,? \\Q2006\\E", "shortCiteRegEx": "Matuszek", "year": 2006}, {"title": "Lightweight Relevance Filtering for Machine-generated Resolution Problems", "author": ["J. Meng", "L.C. Paulson"], "venue": "Journal of Applied Logic, 7(1):4157", "citeRegEx": "Meng and Paulson 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalization and Learnability: A Study of Constrained Atoms", "author": ["C.D. Page", "A.M. Frisch"], "venue": "Inductive Logic Programming, Academic Press", "citeRegEx": "Page and Frisch 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "R: A Language and Environment for Scientific Computing", "author": ["R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna, Austria", "citeRegEx": "R Core Team 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Artificial Intelligence- A Modern Approach", "author": ["Stuart Russell", "Peter Norvig"], "venue": "Pearson Education,", "citeRegEx": "Russell and Norvig 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic Extraction of Efficient Axiom Sets from Large Knowledge Bases", "author": ["A. Sharma", "K.D. Forbus"], "venue": "Proceedings of the AAAI", "citeRegEx": "Sharma and Forbus 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Controlling Backward Inference", "author": ["D.E. Smith"], "venue": "Aritificial Intelligence, 39(2):145-208", "citeRegEx": "Smith 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Guiding Inference with Policy Search Reinforcement Learning", "author": ["M.E. Taylor", "C. Matuszek", "P.R. Smith", "M.J. Witbrock"], "venue": "Proceedings of the FLAIRS, pages 146-151", "citeRegEx": "Taylor et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Ordering Heuristics for Description Logic Reasoning", "author": ["D. Tsarkov", "I. Horrocks"], "venue": "Proceedings of the IJCAI, pages 609-614", "citeRegEx": "Tsarkov and Horrocks 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "S", "author": ["D. Tsarkov", "A. Riazanov"], "venue": "Bechhofer and I. Horrocks. Using Vampire to reason with OWL. In Proceedings of the Semantic Web-ISWC, pages 471-485", "citeRegEx": "Tsarkov et al. 2004", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 18, "context": ", backward chaining [Russell and Norvig 2003] in Cyc, tableaux algorithms in description logic (DL)) typically represent the search space as a graph, the structure of which is determined by the rules applicable to the given node in the graph.", "startOffset": 20, "endOffset": 45}, {"referenceID": 22, "context": "Generally, many rules might simultaneously apply to a given vertex, and the order of rule expansion can have a significant effect on efficiency [Tsarkov and Horrocks 2005].", "startOffset": 144, "endOffset": 171}, {"referenceID": 21, "context": "In [Taylor et al. 2007],", "startOffset": 3, "endOffset": 23}, {"referenceID": 22, "context": "the authors use reinforcement learning to guide inference, whereas in [Tsarkov and Horrocks 2005], the authors study", "startOffset": 70, "endOffset": 97}, {"referenceID": 2, "context": ", database community [Chaudhuri 1998],", "startOffset": 21, "endOffset": 37}, {"referenceID": 1, "context": "[Brewka et al. 2011]) is less relevant because the studies do not address the complexity of deep and cyclic search graphs", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": ", see the OTTER theorem prover [Russell and Norvig 2003]).", "startOffset": 31, "endOffset": 56}, {"referenceID": 18, "context": "To perform best-first search, a heuristic control strategy mesures the \u201cweight\u201d of each clause in the set of support, picks the \u201cbest\u201d clause, and adds to the set of support the immediate consequences of resolving it with the elements of the usable list [Russell and Norvig 2003].", "startOffset": 254, "endOffset": 279}, {"referenceID": 16, "context": "The fact that Minneapolis, Anaheim, and Rochester are US cities helps us derive the sorted generalization [Page and Frisch 1992] that ?ARG2 is likely to range over the set USCity.", "startOffset": 106, "endOffset": 128}, {"referenceID": 11, "context": "n) where m is the number of attributes, and n is the size of the training set [Kent and Hirschberg 1996].", "startOffset": 78, "endOffset": 104}, {"referenceID": 17, "context": "The R software was used to estimate the values of wi [R Core Team 2015].", "startOffset": 53, "endOffset": 71}, {"referenceID": 12, "context": "The presence of features such as \u201cKnuth\u2019s tree size estimate\u201d [Knuth 1975], \u201cratio of number of positive and negative literals\u201d and \u201c(number of transformation links)\u201d in the selected list suggests the following: (i) Locally available information is insufficient for predicting the complexity of search space, and probing features play an important role in guiding a search.", "startOffset": 62, "endOffset": 74}, {"referenceID": 4, "context": ", Project HALO [Friedland et al. 2004], HPKB project [Cohen et al.", "startOffset": 15, "endOffset": 38}], "year": 2016, "abstractText": "Very large commonsense knowledge bases (KBs) often have thousands to millions of axioms, of which relatively few are relevant for answering any given query. A large number of irrelevant axioms can easily overwhelm resolution-based theorem provers. Therefore, methods that help the reasoner identify useful inference paths form an essential part of large-scale reasoning systems. In this paper, we describe two ordering heuristics for optimization of reasoning in such systems. First, we discuss how decision trees can be used to select inference steps that are more likely to succeed. Second, we identify a small set of problem instance features that suffice to guide searches away from intractable regions of the search space. We show the efficacy of these techniques via experiments on thousands of queries from the Cyc KB. Results show that these methods lead to an order of magnitude reduction in inference time.", "creator": "Microsoft\u00ae Word 2016"}}}