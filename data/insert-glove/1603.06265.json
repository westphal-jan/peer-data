{"id": "1603.06265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2016", "title": "Collaborative prediction with expert advice", "abstract": "verveer We zukas consider maroons a o'nuts collaborative chocolates variant hazu of prediction masspirg with expert salee advice. kooks In granger each paned round, o&o one turnback user wishes reuther to joropo make a prediction, jimmy and brf must choose which 3,070 expert to millerwise follow. nyeon The one-and-a-half-story users wrappers would like to share 14.72 their nantyglo experiences 13.95 in hojas order 88.51 to ganilau learn pre-chorus faster - - ideally single/ep they prowlers could heraldry amortize the gatv same total regret cross flavel the defendent whole community of users. However, nominalist some fossilized of gasbags the users obama-biden may behave firdos maliciously, distorting their meijer reported payoffs in order to manipulate the honest users childhoods of falso the verae system. And even if all caribous users behave kss honestly, heterosexuality different experts may perform o.e. better flugzeugbau for muscogee different tallest users, such that eliecer sharing sinusoidal data understudied can hadouken be counterproductive.", "histories": [["v1", "Sun, 20 Mar 2016 20:34:32 GMT  (14kb)", "http://arxiv.org/abs/1603.06265v1", null], ["v2", "Wed, 6 Apr 2016 20:41:12 GMT  (22kb)", "http://arxiv.org/abs/1603.06265v2", null], ["v3", "Fri, 8 Apr 2016 00:36:06 GMT  (22kb)", "http://arxiv.org/abs/1603.06265v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["paul christiano"], "accepted": false, "id": "1603.06265"}, "pdf": {"name": "1603.06265.pdf", "metadata": {"source": "CRF", "title": "Robust Collaborative Online Learning", "authors": ["Paul Christiano"], "emails": ["paulfchristiano@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 26\n5v 1\n[ cs\n.L G\nWe present a robust collaborative algorithm for prediction with expert advice, which guarantees that every subset of users H perform nearly as well as if they had shared all of their data, and ignored all data from users outside of H. This algorithm limits the damage done\nby the dishonest users to O (\u221a T ) , compared to the O (T ) we would\nobtain by naively aggregating data. We also extend our results to general online convex optimization. The resulting algorithm achieves low regret, but is computationally intractable. This demonstrates that there is no statistical obstruction to generalizing robust collaborative online learning, but leaves the design of efficient algorithms as an open problem."}, {"heading": "1 Introduction", "text": "Modern machine learning systems often aggregate data from many users. This facilitates rapid learning, but leaves these systems vulnerable to manipulation by malicious users. Traditional results in learning theory make very weak guarantees about robustness to manipulation.\nThis is not an abstract concern. Machine learning systems are used to make a range of economically significant decisions, from aggregated ratings that shape what we buy to search rankings that shape what we read, and the incentives to manipulate these systems can be very strong.\nWe consider a collaborative version of prediction with expert advice, a fundamental problem in online learning. In our collaborative formulation, each round involves a particular user ut, who can make a decision based on all of the information reported by users in previous rounds.\nWe would like to guarantee that every set of users performs relatively well. That is, if we choose an arbitrary subset H of users, and restrict our attention to the set of rounds involving a user ut \u2208 H , we would still like to compete with the best fixed expert. This ensures that adding additional malicious or varied users can\u2019t significantly reduce the performance of existing users.\nThe chief difficulty is achieving this bound for every set H simultaneously. We are not only ignorant of what expert will perform best, we are ignorant of what payoffs we actually care about.\nWe show how to transform any traditional algorithm for prediction with expert advice into a robust collaborative version. This transformation ensures that the original algorithm\u2019s regret bound applies for every subset of users H . It does this at the expense of some additional regret\u2014if there are\nN users in total, the additional regret is at most O (\u221a TN ) . The additional\nregret improves significantly when most users are honest, or when few users are; we present precise statements in Section 3.\nThe resulting algorithms automatically compete with the best strategy which divides the honest users into m groups and makes a different recommendation for each group. These bounds may be of interest even in domains where there is no adversarial behavior.\nWe extend our results to general online convex optimization, at the expense of computational tractability. This result indicates that it is statistically possible to implement very efficient robust collaborative learning in a very broad range of settings."}, {"heading": "1.1 Our model", "text": "In this section we present our model for online convex optimization over a convex set S. Online convex optimization is an extremely general learning problem that captures many traditional learning problems as special cases ([11]). Throughout, we assume that the losses are in [\u22121, 1].\nOnline convex optimization includes prediction with expert advice as a special case. We only consider the decision-theoretic setting for prediction with expert advice ([7]). This is the special case of online convex optimization where S is the set of probability distributions over a finite set of \u201cexperts\u201d X , and the loss functions \u2113t have the form \u2113t (p) = \u2211 x\u2208X \u2113 x t p\nx. We imagine ourselves in the position of a central recommendation service, which must provide advice to some fixed set of users U , with |U| = N . We make our decisions in a sequence of rounds, t = 1, 2, . . . , T . At the beginning of each round, we are given the identity of a user ut \u2208 U . We then choose an element pt \u2208 S, and nature reveals a convex loss function \u2113t : S \u2192 [\u22121, 1]. The user ut and loss \u2113t may be adversarial, depending on our choices in previous rounds. The loss \u2113t may also depend on pt.\nOur loss in round t is simply \u2113t (pt). The loss of a set of users H \u2282 U is the total loss in all rounds involving a user ut \u2208 H :\n\u2113H\u2264T = \u2211\nt\u2264T :ut\u2208H\n\u2113t (pt)\nTo measure our performance, we compare to the performance of the best single point in S:\nOPTH\u2264T = min p\u2208S\n\u2211\nt\u2264T :ut\u2208H\n\u2113t (p) .\nWe are interested in minimizing the regret \u2113H\u2264T \u2212 OPTH\u2264T . We will prove bounds that hold simultaneously for every set H .\nRather than producing algorithms for this problem from scratch, we will implement transformations that start with an algorithm for the single-user case, and produce a robust collaborative version.\nAlgorithms for online convex optimization typically make some assumption about the loss functions \u2113t, for example that they are Lipschitz in an appropriate norm. Our transformation will apply for any convex optimization problem, under arbitrary assumptions about the loss functions \u2113t. The transformed algorithm will make exactly the same assumptions as the original algorithm.\nMore precisely, we take as given an algorithm OCO for online convex optimization over the set S, with some set of admissible loss functions \u2113t. Write regret RT (OCO) for the worst-case regret of that algorithm over the first T rounds. We produce an algorithm OCO for collaborative online convex optimization over S, with the same set of admissible loss functions. OCO satisfies a bound of the form\n\u2113H\u2264T \u2264 OPTH\u2264T +RT (OCO) +R\u2032T (H)\nfor every set H . We call R\u2032T (H) the additional regret of the transformation. Our goal is to prove bounds on the additional regret.\nWe will also consider a stronger benchmark, in which we divide the honest users into m groups and choose the optimal p for each group:\nOPTH,m\u2264T = min p1,...,pm\u2208S\n\u2211\nu\u2208H\nmin p\u2208{p1,...,pm}\n\u2211\nt\u2264T :ut=u\n\u2113t (p) .\nRegret bounds against this class of strategies will follow automatically from our other results.\nMany of our bounds include an additive term in O (\u221a T log log T ) . We write O\u0303 (\u00b7) to hide these additive terms. Typically they will not affect the asymptotics unless T is \u03c9 (exp (N)), which is not a regime in which we expect our results to be meaningful. We expect that these terms can be removed with some additional work."}, {"heading": "1.2 Our contributions", "text": "Prediction with expert advice. For prediction with expert advice, we exhibit a transformation that introduces additional regret of O\u0303 (\u221a TN ) .\nThe key technical idea is reversing the relationship between the experts and the users: rather than having the experts always offer advice, we have each expert learn which users to offer advice to. The goal of the expert is to offer advice if and only if their advice will be helpful. The resulting experts are \u201cspecialists,\u201d who sometimes abstain from offering advice, and we can apply a standard technique to compete with the best specialist ([8]).\nBy improving the learning algorithm used by the experts, we can significantly improve this bound when there are either few or many malicious users.\nFor example, when only an \u01eb < 1/2 fraction of users are malicious (respectively honest), and only an \u01eb fraction of rounds involve a malicious (respec-\ntively honest) user, we prove that the additional regret is O\u0303 ( \u01eb \u221a TN log (\u01eb) ) .\nThe parameter \u01eb is not known to the algorithm; the strengthened guarantee still holds simultaneously for every H .\nAs an immediate consequence, the same algorithm is also competitive with the best strategy that divides the honest users into m > 1 groups and chooses a different expert for each group. In this case, we obtain re-\ngret mRT (PEA) + O\u0303 (\u221a TN log (m) ) , where RT (PEA) is the regret of an\nalgorithm for single-user prediction with expert advice over T steps. Online convex optimization. For online convex optimization with losses in [\u22121, 1], we exhibit a transformation that introduces additional regret O\u0303 (\u221a TN ) . However, this transformation requires exponential time.\nWe demonstrate that there is no statistical obstruction to robust collaborative online optimization, but leave open the problem of designing efficient algorithms.\nThis result can also be improved whenever H is small or large, and can also be improved if we have side information about the set H .\nThe key idea behind the transformation is to introduce an expert for every subset of the users. Each expert uses the underlying online convex optimization algorithm to make recommendations to the corresponding subset of users. We then aggregate these recommendations using an algorithm for learning from specialists ([8]).\nThis result can also be applied to the case where we want to divide the honest users into m subgroups. We obtain regret mRT/m (OCO) + O\u0303 (\u221a TN log (m) ) , where RT/m (OCO) is the regret of an algorithm for\nsingle-user online convex optimization over T/m steps."}, {"heading": "1.3 Related work", "text": "Collaborative filtering: In the collaborative filtering problem, a set of users interact with a set of resources, and exploit their common tastes to more efficiently predict which resources each of them will rate highly. This problem has been studied at length; see ([12]) for an overview. A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).\nOur work differs from this literature in two respects. First, we focus on\nachieving very strong robustness and non-manipulability guarantees without sacrificing statistical efficiency. Second, we consider the general problem of prediction with expert advice rather than predicting which resources a user will rate highly. As a result, we require completely different techniques.\nRobust collaborative filtering. The most closely related work is ([5]). Their results fit in our model of robust collaborative learning, but they study a different problem (a particular model of the collaborative filtering problem) and use different techniques.\nAlong similar lines, ([3]) provides a robust collaborative algorithm for the multi-armed bandit problem, under an additional stochastic assumption. The multi-armed bandit problem is more similar to, and motivated by, applications to collaborative filtering than to our setting. Moreover, it is not clear how we would apply similar techniques to our setting.\nAdversarial learning. Another literature deals with learning problems in which an adversary has some influence over the training or testing data ([4]). Robust collaborative learning can be viewed within this framework, as an attack model in which an adversary controls the data associated with some users. The unique characteristic of our model is that we only care about the payoff associated with the uncorrupted users; in our view this is a very natural model of an important class of attacks. This allows us to obtain extremely strong regret bounds, and also leads us to use a novel set of techniques."}, {"heading": "2 Preliminaries: learning algorithms as sub-", "text": "routines\nOur results are transformations, from single-user learning algorithms to collaborative versions.\nThe transformed algorithm invokes several learning algorithms as subroutines, (one of which is the algorithm PEA or OCO that we are transforming to a collaborative version). We will typically use the Roman letters A,B,C, . . . to represent these subroutines. Each of them is solving an independent learning problem.\nThe \u201couter\u201d learning problem that we are trying to solve proceeds in a series of rounds t = 1, 2, . . . , T, . . .. In general, the \u201cinner\u201d learning problems will proceed more slowly; some but not all of the subroutines will make\na prediction and receive a loss function in each round of the outer learning problem. We say that a subroutines is \u201cactive\u201d in a given round if it produces an output and receives a loss. Write \u03c4A (T ) = {t \u2264 T : A is active}.\nFor t \u2208 \u03c4A (T ), we write At for the output of A, at the beginning of the tth round of the outer learning problem. In the case of prediction with expert advice, At is a distribution over experts, and we write At (x) for the probability assigned to expert x.\nSimilarly, for t \u2208 \u03c4A (T ), we write \u2113At for the loss function observed by A during round t of the outer learning problem. In the case of prediction with expert advice, we abuse notation and write \u2113At (x) for the loss of expert x, i.e. the loss of the distribution that assigns probability 1 to x.\nMany of our subroutines will be algorithms for prediction with expert advice and non-uniform priors. The chief subtlety of such algorithms, compared to classical algorithms for prediction with expert advice, is adapting the learning rate appropriately. We will use the algorithm Squint from ([10]). We write A \u2190 Squint (D) to indicate that A is a new instance of Squint with experts D. We may also specify some initial weights B0 (d) for d \u2208 D; if we don\u2019t specify initial weights, then B0 (d) = 1/ |D|.\nRecalling that \u03c4B (T ) is the set of rounds t \u2264 T where B makes a prediction and receives a loss, we have:\nLemma 1. ([10, Theorem 4]) If B is an instance of Squint with experts D, then for any d\u2217 \u2208 D we have\n\u2211\nt\u2208\u03c4B(T )\n\u2113Bt (Bt) \u2264 \u2211\nt\u2208\u03c4B(T )\n\u2113Bt (d \u2217) + O\u0303 (\u221a |\u03c4B (T )| log (B0 (d\u2217)) ) .\nMoreover, if V = \u2211\nt\u2208\u03c4B(T ) ( \u2113Bt (B t)\u2212 \u2113Bt (d\u2217) )2 , then we have\n\u2211\nt\u2208\u03c4B(T )\n\u2113Bt (Bt) \u2264 \u2211\nt\u2208\u03c4B(T )\n\u2113Bt (d \u2217) + O\u0303 (\u221a V log (B0 (d\u2217)) ) .\nFor most of our purposes Squint is overkill, and we could obtain the same results using any algorithm that satisfies a similar non-uniform regret bound. In Section 4, we will make use of the improved bound in terms of the regret variance V , and so we use Squint throughout for simplicity.\nSquint is one source of our O (\u221a T log log (T ) ) regret term, which we\nhide in O\u0303 (\u00b7); it is not the only source."}, {"heading": "3 Collaborative prediction with expert ad-", "text": "vice\nRecall the model discussed in section 1.1. N = |U| is the number of users. We will take as given an algorithm PEA for the single-user case, with regret RT (PEA), and exhibit a protocol PEA with the following guarantee:\nTheorem 1. Let H be any set of users. Then PEA satisfies:\n\u2113H\u2264T \u2264 OPTH\u2264T +RT (PEA) + O\u0303 (\u221a TN ) .\nIf we write D = U\\H and TS = |{t \u2264 T : ut \u2208 S}|, then PEA satisfies the stronger bounds:\n\u2113H\u2264T \u2264 OPTH\u2264T +RT (PEA) + O\u0303 (\u221a TH |H| log ( T\nTH\nN\n|H|\n) + TH |H|\n)\nand\n\u2113H\u2264T \u2264 OPTH\u2264T +RT (PEA) + O\u0303 (\u221a TD |D| log ( T\nTD\nN\n|D|\n) + TD |D|\n)\nIn all of these bounds, O\u0303 (\u00b7) hides additive terms that are O\u0303 (\u221a T log log T ) .\nFor example, if |H| = \u0398 (N), then the total additional regret per honest user is O (\u221a k ) , where k = T/N is the average number of rounds per user.\nIf the fraction of honest users is \u01eb, and an \u01eb fraction of rounds involve an\nhonest user, then the total additional regret per honest user isO (\u221a k log ( 1 \u01eb )) . Similarly, if the fraction of dishonest users is \u01eb, and an \u01eb fraction of rounds involve a dishonest user, then total additional regret per dishonest user is\nO (\u221a k log ( 1 \u01eb )) (this additional regret is amortized over all of the honest users). For any set of users H , recall the stronger benchmark OPTH,m\u2264T which can choose up to m different experts, and then pick a different expert from this set for each user in H .\nWe can compete with this benchmark without modifying our algorithm. In Appendix C we prove\nCorollary 1. For any set of users H and any m > 1, PEA satisfies:\n\u2113H\u2264T \u2264 OPTH,m\u2264T +mRT (PEA) + O\u0303 (\u221a TN log (m) ) ."}, {"heading": "It also satisfies the stronger bound:", "text": "\u2113H\u2264T \u2264 OPTH,m\u2264T +mRT (PEA) + O\u0303 (\u221a TH |H| log ( m T\nTH\nN\n|H|\n) + T\n)\nwhere as before TH = |{t \u2264 T : ut \u2208 H}|.\nIn our result for online convex optimization, we obtain a similar regret bound that depends on mRT/m (PEA) rather than mRT (PEA). However, the algorithm that achieves this bound is not efficient. We leave closing this gap as an open problem. The most likely approach is replacing the term RT (PEA) in Theorem 1 with RTH (PEA)."}, {"heading": "3.1 Basic algorithm", "text": "Our basic algorithm is given in Figure 1. We also informally explain the behavior of the algorithm in this section. The full algorithm, for which we prove Theorem 1, is discussed in the next section and analyzed in Appendix B.\nFor each x \u2208 X and each round t, we compute a quantity, zxt reflecting the probability that expert x should offer advice in round t. We imagine these quantities as being computed \u201cby the experts\u201d but this has no substantive effect on the algorithm. We will describe how the experts compute the quantities zxt later.\nOnce the experts have decided whether to participate in a round, we use a standard algorithm for combining \u201cspecialists,\u201d experts who are only active in some rounds ([8]). In order to make a prediction, we renormalize the weights of the active experts. We compute the losses of the active experts as usual, and for each inactive expert we provide a loss equal to the average loss of the active experts (weighted by their current weights). We then update the weights using the algorithm PEA.\nThe experts choose zxt in order to minimize their losses and maximize the weight given to them by PEA. Each expert instantiates one Squint instance per user, with one expert that always recommends \u201cactive\u201d and one expert that always recommends \u201cinactive.\u201d In round t, each expert consults the\ninstance corresponding to ut to decide whether to be active. The loss for the \u201cactive\u201d expert is \u2113t (x), while the loss for the \u201cinactive\u201d expert is the average loss of the other experts (weighted by their current weights).\nA\u2190PEA (X ); for x \u2208 X , u \u2208 U do\nBxu\u2190Squint ({0, 1}); end for t = 1, 2, . . . do Observe ut \u2208 U ; for x \u2208 X do\nzxt \u2190Bxutt (1); wxt \u2190zxt At (x);\nend Wt\u2190 \u2211 x w x t ; Play pt (x) = w x t /Wt \u2208 \u2206(X ); Observe \u2113t : X \u2192 [\u22121, 1]; for x \u2208 X do\n\u2113At (x)\u2190zxt \u2113t (x) + (1\u2212 zxt )\u2113t (pt); \u2113B xut\nt (1)\u2190\u2113t (x); \u2113B xut\nt (0)\u2190\u2113t (pt); end\nend\nAlgorithm 1: Collaborative prediction with expert advice\nTo analyze this algorithm, we consider the excess performance of expert x in rounds where they are active: \u2211 t z x t (\u2113t (x)\u2212 \u2113t (pt)). We show that this quantity is nearly as negative as if we had taken zxt = 1 precisely when ut \u2208 H . On the other hand, we show that this quantity can\u2019t be more than RT (PEA). This places a bound on the excess performance of expert x in the rounds where ut \u2208 H , which is precisely what we want to establish.\nLemma 2. For every x \u2208 X we prove: \u2211\nt\u2264T\nzxt (\u2113t (x)\u2212 \u2113t (pt)) \u2264 \u2211\nt\u2264T :ut\u2208H\n(\u2113t (x)\u2212 \u2113t (pt)) + O\u0303 (\u221a TN )\nProof. We apply Lemma 1 to each Squint instance Bxu, and sum the resulting\ninequalities. Write hx = 1 if x \u2208 H , and 0 otherwise.\u2211\nt\u2264T\nzxt (\u2113t (x)\u2212 \u2113t (pt)) = \u2211\nu\u2208U\n\u2211\nt\u2264T :ut=u\nzxt (\u2113t (x)\u2212 \u2113t (pt))\n\u2264 \u2211\nu\u2208U\n( \u2211\nt\u2264T :ut=u\nhx (\u2113t (x)\u2212 \u2113t (pt)) + O\u0303 (\u221a |{t \u2264 T : ut = u}| ))\n\u2264 \u2211\nt\u2264T :ut\u2208H\n(\u2113t (x)\u2212 \u2113t (pt)) + O\u0303 (\u221a TN )\nWhere the last inequality follows by Jensen\u2019s.\nLemma 3. For any x \u2208 X , \u2211\nt\u2264T\nzxt (\u2113t (pt)\u2212 \u2113t (x)) \u2264 RT (PEA)\nProof. First, observe that the loss of A is exactly equal to the loss of pt: We compute the loss of A:\n\u2113At (A) = \u2211\nx\nAt (x) (z x t \u2113t (x) + (1\u2212 zxt )\u2113t (pt))\n= \u2211\nx\nAt (x) z x t \u2113t (x) + \u2113t (pt)\n\u2211\nx\nAt (x)\u2212 \u2113t (pt) \u2211\nx\nAt (x) z x t\n= \u2211\nx\nwxt \u2113t (x) + \u2113t (pt)\u2212 \u2113t (pt) \u2211\nx\nwxt\n= Wt\u2113t (pt) + \u2113t (pt)\u2212Wt\u2113t (pt) = \u2113t (pt) .\nSo we can apply the regret bound for A, and obtain: \u2211\nt\u2264T\n\u2113t (pt) = \u2211\nt\u2264T\n\u2113At (A)\n\u2264 \u2211\nt\u2264T\n\u2113At (x) +RT (PEA)\n= \u2211\nt\u2264T\n(zxt \u2113t (x) + (1\u2212 zxt )\u2113t (pt)) +RT (PEA)\n= \u2211\nt\u2264T\n\u2113t (pt) + \u2211\nt\u2264T\nzxt (\u2113t (x)\u2212 \u2113t (pt)) +RT (PEA)\n0 \u2264 \u2211\nt\u2264T\nzxt (\u2113t (x)\u2212 \u2113t (pt)) +RT (PEA) .\nCombining these lemmas, we have that for any expert x \u2208 X : \u2211\nt\u2264T :ut\u2208H\n(\u2113t (x)\u2212 \u2113t (pt)) \u2264 \u2211\nt\u2264T\nzxt (\u2113t (x)\u2212 \u2113t (pt)) + O\u0303 (\u221a TN )\n\u2264 RT (PEA) + O\u0303 (\u221a TN )\nas desired."}, {"heading": "3.2 Improving the algorithm", "text": "In the previous algorithm, the experts treat each user as a separate learning problem. We can improve the algorithm by having the experts learn what fraction of the users are honest, rather than implicitly expecting half of all users to be honest.\nWe introduce a new learning algorithm A\u03b8 for solving a simultaneous prediction with expert advice problem for each user u \u2208 U . We instantiate A\u03b8 as A \u2190 A\u03b8 (U). For each round t, Autt is a probability distribution over {0, 1}. Write \u2113At for the loss function given to A in round t. We imagine a separate learning problem for each user u \u2208 U , with ut indicating which learning problem is being addressed in round t. However, rather than simply using an independent instance of Squint for each user, A\u03b8 learns a parameter \u03b8 reflecting a prior distribution over {0, 1}.\nIn Appendix B, we define A\u03b8 and prove the following result: Theorem 2. Let A be an instance of A\u03b8. For any H \u2282 U , we have: \u2211\nt\u2264T\n\u2113At (A ut t ) \u2264\n\u2211\nt\u2264T :ut\u2208H\n\u2113At (1)+ \u2211\nt\u2264T :ut 6\u2208H\n\u2113At (0)+O\u0303 (\u221a TH |H| log ( TN\nTH |H|\n) + TH |H|\n)\nThis bound also holds if we replace each occurrence of H in the regret term with U\\H.\nWith A\u03b8 in hand, it is straightforward to improve Algorithm 1. Rather than having each expert instantiate a separate instance Bxu of Squint for each user u, we instantiate a single instance Bx of A\u03b8. The analysis of the improved algorithm is then identical to the analysis of Algorithm 1, except that the conclusion of Lemma 2 is strengthened appropriately. The result is precisely the strengthened conclusion in Theorem 1. The full algorithm and analysis are given in Appendix B."}, {"heading": "4 Online convex optimization", "text": "In this section, we present a general method for transforming an online convex optimization algorithm OCO into a collaborative version OCO. Both algorithms are optimizing over some convex set S.\nOCO maintains a separate instance of OCO for every subset of the users. Each of these instances aggregates data over all users in the corresponding subset. We then use a standard algorithm for sleeping experts to aggregate across these different OCO instances; the experts awake at step t are precisely those corresponding to sets that contain ut.\nA\u2190Squint ( 2U ) , where 2U is the power set of U , with A0 (S) = p (S); for S \u2286 U do BS\u2190OCO();\nend for t = 1, 2, . . . do Observe ut \u2208 U ; Wt\u2190 \u2211 S\u2286U :ut\u2208S At (S);\nPlay pt = \u2211\nS\u2286U :ut\u2208S At (S)B S t /Wt;\nObserve \u2113t : S \u2192 [\u22121, 1]; for S \u2286 U do\nif ut \u2208 S then \u2113At (S)\u2190\u2113t ( BSt ) ;\n\u2113B S t \u2190\u2113t; else\n\u2113At (S)\u2190\u2113t (pt); end\nend\nend\nAlgorithm 2: OCO\nTheorem 3. For any distribution p over subsets of U , there is an algorithm OCOp such that, for every set H:\n\u2113H\u2264T \u2264 OPTH\u2264T +RTH (OCO) + O\u0303 (\u221a THp (H) )\nProof. We have\n\u2113At (At) = \u2211\nS\u2282U\nAt (S) \u2113 A t (S)\n= \u2211\nS\u2282U :ut\u2208S\nAt (S) \u2113t ( BSt ) + \u2211\nS\u2282U :ut 6\u2208S\nAt (S) \u2113t (pt)\n= Wt \u2211\nS\u2282U :ut\u2208S\n( At (S) \u2113t ( BSt ) /Wt ) + (1\u2212Wt)\u2113t (pt)\n\u2265 Wt\u2113t (pt) + (1\u2212Wt)\u2113t (pt) = \u2113t (pt)\nwhere the inequality follows from the convexity of \u2113t. Moreover, by Lemma 1, we have:\n\u2211\nt\n\u2113At (At) \u2264 \u2211\nt\n\u2113At (H) + O\u0303 (\u221a THp (H) )\n\u2211\nt:ut\u2208H\n( \u2113At (A)\u2212 \u2113At (H) ) = \u2211\nt\n( \u2113At (A)\u2212 \u2113At (H) )\n\u2264 O\u0303 (\u221a THp (H) )\n\u2211\nt:ut\u2208H\n\u2113At (At) \u2264 \u2211\nt:ut\u2208H\n\u2113At (H) + O\u0303 (\u221a Tp (H) )\n\u2211\nt:ut\u2208H\n\u2113t (pt) \u2264 \u2211\nt:ut\u2208H\n\u2113At (H) + O\u0303 (\u221a Tp (H) )\nWe obtain TH in the bound on the first line, rather than T , because \u2113 A t (H) = \u2113t (pt) = \u2113 A t (A) for all t such that ut 6\u2208 H . Thus V = \u2211 t\u2264T ( \u2113At (H)\u2212 \u2113At (A)\n)2 is upper-bounded by TH .\nThe second line is valid because \u2113At (A) = \u2113 A t (H) for t such that ut 6\u2208 H . Finally, for any x \u2208 S: \u2211\nt:ut\u2208H\n\u2113At (H) = \u2211\nt:ut\u2208H\n\u2113t ( BHt )\n\u2264 \u2211\nt:ut\u2208H\n\u2113t (x) +RTH (OCO)\nWhere the inequality is valid because \u2113t = \u2113 BH\nt . Combining these inequalities yields\n\u2211\nt:ut\u2208H\n\u2113t (pt) \u2264 \u2211\nt:ut\u2208H\n\u2113t (x) +RTH (OCO) + O\u0303 (\u221a THp (H) ) ,\nas desired.\nWe define OCO by plugging in an appropriate p: first draw \u03b8 from the distribution \u00b5 defined in Appendix B (roughly log uniform), and then sample a random set with density \u03b8. We obtain:\nCorollary 2. For every set H, OCO satisfies:\n\u2113H\u2264T \u2264 OPTH\u2264T +RTH (OCO) + O\u0303 (\u221a TH |H| log ( N\n|H|\n) + TH |H|\n)\nProof. All we need to show is that\nlog (p (H)) = O\u0303 ( |H| log ( N\n|H|\n) + |H|+ log log T ) .\nBut this is easily verified by direct calculation, as in the proof of Theorem 2.\nFinally, in Appendix C we show that OCO competes with OPTH,m\u2264T . We assume that RT (OCO) is a convex function of T\u2014this is true for essentially all learning algorithms used in practice.\nCorollary 3. For every set H, and any m > 1, OCO satisfies:\n\u2113H\u2264T \u2264 OPTH\u2264T +RT/m (OCO) + O\u0303 (\u221a TH |H| log ( m N\n|H|\n)) ,\nprovided that RT (OCO) is a convex function of T ."}, {"heading": "5 Open questions", "text": "The robust collaborative learning framework provides a general transformation from single-user learning problem to robust collaborative learning problems. We have answered a few fundamental questions, but we leave many more open.\n\u2022 Efficient online convex optimization. Our algorithm for online convex optimization is intractable. It may be possible to produce a general transformation from a tractable online convex optimization algorithm to a tractable collaborative online convex optimization algorithm; this would be a major improvement over our results. Even if such a general transformation is impossible, we can seek efficient algorithms for a broader range of online convex optimization problems, for example online learning over combinatorial structures.\n\u2022 Exploiting information about users. Our additional regret bounds depend on a quantity like |H|, reflecting the prior probability of the set H under an appropriate distribution. If we can find a better probability distribution p over subsets, potentially exploiting other information available about the users, then the statistically optimal additional regret is \u221a p(H)TH (as in Theorem 3). For example, if we know that\nmalicious users tend to have few honest friends, then we can use this information to reduce our additional regret bounds, potentially down to negligible levels per user. Our algorithm for online convex optimization is able to make use of this kind of information, but we dont\u2019 propose any efficient algorithms that can nor do we consider any promising sources of information.\n\u2022 Parallel expert problems. Suppose the same set of users participate in many online services 1, 2, . . . , k, with each user participating in some arbitrary subset. We would like to be able to amortize the additional regret over all of these services, rather than running a separate collaborative learning algorithm for each of them. This corresponds to an experts problem with a simple combinatorial structure: an \u201cexpert\u201d corresponds to a choice of expert in each of the k underlying problems. ([5]) develops robust collaborative algorithms for this problem when the\nnumber of experts in each problem is O\u0303 (1). But their regret bounds are suboptimal, and moreover the general problem remains open.\n\u2022 Translating results from the single-user setting. The robust collaborative learning framework provides a transformation from singleuser learning problems to collaborative learning problems. It is natural to try to extend desirable properties to the collaborative case, such as performing well on \u201ceasy\u201d instances, achieving quantile bounds, or competing with enlarged spaces of of single-user algorithms to the collaborative case. Natural contenders are tighter bounds for \u201ceasy\u201d instances, quantile bounds, or competing with enlarged spaces of strategies (such as switching experts).\n\u2022 Better regret against OPTH,m\u2264T . For prediction with expert advice, our regret bounds against OPTH,m\u2264T depend on mRT (PEA), while our intractable algorithm for online convex optimization has regret that depends on mRT/m (PEA). For practical learning algorithms this corresponds to a significant \u221a m slowdown.\n\u2022 Contextual bandits. Our algorithms all require full feedback. It seems likely that they can be extended to the contextual bandits setting, which would be important for many practical applications. Without some additional stochastic assumptions, we expect that the addi-\ntional regret will have to likely scale like O\u0303 (\u221a TAN ) , where A is the\nnumber of available actions; but even this result would be a significant generalization of our algorithm for predicting with expert advice.\n\u2022 Memory requirements. Our algorithm for prediction with expert advice requires maintaining one weight for each (expert, user) pair. When the number of users and experts is large, this may be infeasible. A more efficient algorithm might only require O (|U|+ |X |) storage rather than O (|U| \u2217 |X |) storage."}, {"heading": "A Defining A\u03b8", "text": "We will assume that T is known in advance. This assumption can be removed with a standard doubling trick.\nLet \u0398 be the smallest set such that:\n\u2022 0 \u2208 \u0398\n\u2022 2\u2212i \u2208 \u0398 for each i \u2264 log (NT )\n\u2022 1\u2212 x \u2208 \u0398, for each x \u2208 \u0398\nand let \u00b5 be a uniform distribution over \u0398.\nA\u2190Squint instance with experts \u0398 and initial weights A0 (\u03b8) = \u00b5 (\u03b8).; for \u03b8 \u2208 \u0398N , u \u2208 U do\nB\u03b8u\u2190Squint instance with experts 0 and 1, with initial weight B\u03b8u0 (1) = \u03b8;\nend for t = 1, 2, . . . do Observe ut \u2208 U ; Play pt = \u2211 \u03b8\u2208\u0398 A (\u03b8)B\n\u03b8ut (1); Observe \u2113t : {0, 1} \u2192 [\u22121, 1]; for \u03b8 \u2208 \u0398 do\n\u2113At (\u03b8)\u2190\u2113t ( B\u03b8ut ) ; \u2113B \u03b8ut\nt \u2190\u2113t; end\nend\nAlgorithm 3: A\u03b8\nA\u03b8 is defined in Figure 3. Recall that our goal is to bound the regret of A\u03b8, compared to the best\nstrategy corresponding to a fixed set H , by\nO\u0303 (\u221a TH |H| log ( TN\nTH |H|\n) + TH |H| ) ,\nand a similar term with H replaced by U\\H .\nTheorem 2. A\u03b8 is symmetric under swapping 0 and 1 (since \u00b5 is symmetric), and so is symmetric under swapping H and U\\H . So it suffices to prove the regret bound with H .\nNote that A\u03b8 has regret of at most O\u0303 (\u221a T log log (NT ) ) with respect to\neach set of experts B\u03b8u. Since O\u0303 (\u221a T log log T ) = O\u0303 (1), and O\u0303 (\u221a T log logN ) is dominated by the desired regret bound for every set H , it suffices to prove that at least one of the families B\u03b8u has the claimed regret.\nWrite \u01eb = TH T |H| N . If \u01eb > 1/2, then TN = O (TH |H|), and so the family of predictors B1/2u have the desired regret bound. So assume \u01eb < 1/2. Note that there is a unique \u03b8 \u2208 (\u01eb/2, \u01eb] \u2229 \u0398. We will show that B\u03b8u satisfies the desired regret bound. If \u03b8 = 0, then this claim is trivial since the corresponding predictors are constants. So assume \u03b8 > 0.\nFor each user u \u2208 H who has been involved in Tu rounds, the regret is upper-bounded by\nO\u0303 (\u221a Tu log (\u03b8) ) = O\u0303 (\u221a Tu log (\u01eb) ) .\nFor each user u 6\u2208 H , it is bounded by\nO\u0303 (\u221a Tu log (1\u2212 \u03b8) ) = O\u0303 (\u221a Tu log (1\u2212 \u01eb) ) = O\u0303 (\u221a Tu\u01eb )\nBy Jensen\u2019s inequality, the total regret is maximized when Tu is equal to TH/ |H| for all u \u2208 H and (T \u2212 TH)/(N \u2212 |H|) for all u 6\u2208 H .\nThe total regret of the instances B\u03b8,u is thus at most\nO\u0303 ( |H| \u221a TH |H| log (\u01eb) +N \u221a T/N\u01eb ) \u2264 O\u0303 (\u221a TH |H| log (\u01eb) + \u221a TU\\H |U\\H| \u01eb )\n\u2264 O\u0303 (\u221a TH |H| log (\u01eb) + \u221a TN\u01eb ) = O\u0303 (\u221a TH |H| log (\u01eb) + \u221a TH |H| ) ,\nas desired."}, {"heading": "B Proof of Theorem 1", "text": "The algorithm PEA is defined in Figure 4. The algorithm is precisely analogous to the algorithm in Section 3.1, and its analyis is precisely parallel to the analysis in that section.\nA\u2190PEA (X ); for x \u2208 X do\nBx\u2190A\u03b8 (U); end for t = 1, 2, . . . do Observe ut \u2208 U ; for x \u2208 X do\nzxt \u2190Bxutt (1); wxt \u2190zxt At (x);\nend Wt\u2190 \u2211 x w x t ; Play pt (x) = w x t /Wt \u2208 \u2206(X ); Observe \u2113t : X \u2192 [\u22121, 1]; for x \u2208 X do\n\u2113At (x)\u2190zxt \u2113t (x) + (1\u2212 zxt )\u2113t (pt); \u2113B x\nt (1)\u2190\u2113t (x); \u2113B x\nt (0)\u2190\u2113t (pt); end\nend\nAlgorithm 4: PEA\nLemma 4.\n\u2211\nt\u2264T\nzxt (\u2113t (x)\u2212 \u2113t (pt)) \u2264 \u2211\nt\u2264T :ut\u2208H\n(\u2113t (x)\u2212 \u2113t (pt))+O\u0303 (\u221a TH |H| log ( TN\nTH |H|\n) + TH |H| ) ,\nand similarly if we replace each occurence of H in the final term with U\\H. Proof. If we subtract \u2211\nt\u2264T \u2113t (pt) to both sides, then the left hand side is\nprecisely \u2211\nt \u2113 Bx t (B x t ). We can therefore apply Theorem 2, from which the\ndesired result follows immediately.\nLemma 5. For any x \u2208 X , \u2211\nt\u2264T\nzxt (\u2113t (pt)\u2212 \u2113t (x)) \u2264 RT (PEA)\nProof. This proof is identical to the proof of Lemma 3.\nCombining these lemmas, we have that for any expert x \u2208 X :\n\u2211\nt\u2264T :ut\u2208H\n(\u2113t (x)\u2212 \u2113t (pt)) \u2264 \u2211\nt\u2264T\nzxt (\u2113t (x)\u2212 \u2113t (pt)) + O\u0303 (\u221a TH |H| log ( TN\nTH |H|\n) + TH |H|\n)\n\u2264 RT (PEA) + O\u0303 (\u221a TH |H| log ( TN\nTH |H|\n) + TH |H|\n)\nas desired.\nC Competing with OPT H,m \u2264T\nWe first show that OCO competes with OPTH,m\u2264T , which is slightly easier than analyzing PEA.\nCorollary 3. Fix an optimizing set {p1, . . . , pm}. Let Hi be the set of users for which the minimum in the definition of OPTH,m\u2264T is obtained at pi. We assume that the Hi are a partition, we can break ties lexicographically if needed. We apply Corollary 2 to each of the Hi, and add up the resulting inequalities.\nOn the LHS we obtain \u2113H\u2264T .\nOn the RHS, for the benchmark terms we obtain \u2211\ni OPT Hi \u2264T = OPT H,m \u2264T .\nFor the RT (OCO) terms, we obtain \u2211\ni RTHi (OCO). Under our convexity assumption, this is at most mRT/m (OCO).\nFinally, we have the sum of regret terms O\u0303 (\u221a THi |Hi| log ( N |Hi| )) (the\nsecond summand in the regret terms is dominated on average). Optimizing\nwith respect to THi , we can see that this is maximized if THi \u221d |Hi| log ( N |Hi| ) . Substituting in these values, the resulting expression is a convex function of |Hi| (essentially it is a constant times the entropy of the numbers |Hi| / |H|), and so is maximized when |Hi| = |H| /m. Finally, plugging in these values we obtain a total regret of\nmO\u0303 (\u221a\nTH m |H| m log (m) log\n( N\n|H|\n))\n=O\u0303 (\u221a TH |H| log (m) log ( N\n|H|\n)) .\nAdding up these three terms on the RHS, we obtain precisely the desired inequality.\nThe analysis of PEA is essentially identical. There are two differences:\n\u2022 The regret term in Theorem 1 depends on RT (PEA) rather than RTH (PEA), and so we obtain mRT (PEA) rather than mRT/m (PEA). \u2022 The regret term in Theorem 1 depends on log (\nN |H| T TH\n) rather than on\nlog (\nN |H|\n) . However, we can split up this log into two summands, and\nbound each summand in exactly the same way that we bound the sum of regret terms in the proof of Corollary 3."}], "references": [{"title": "Tell me who I am: An interactive recommendation system", "author": ["Alon", "Awerbuch", "Azar", "Patt-Shamir"], "venue": "In SPAA: Annual ACM Symposium on Parallel Algorithms and Architectures", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Collaborate with strangers to find own preferences", "author": ["Awerbuch", "Azar", "Lotker", "Patt-Shamir", "Tuttle"], "venue": "MST: Mathematical Systems Theory", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Competitive collaborative learning", "author": ["Awerbuch", "Kleinberg"], "venue": "In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Can machine learning be secure", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security (New York, NY, USA,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Provably manipulation-resistant reputation systems", "author": ["P. Christiano"], "venue": "CoRR abs/1411.1127", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Competitive recommendation systems", "author": ["Drineas", "Kerenidis", "Raghavan"], "venue": "In STOC: ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Schapire"], "venue": "JCSS: Journal of Computer and System Sciences", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Using and combining predictors that specialize", "author": ["Freund", "Schapire", "Singer", "Warmuth"], "venue": "In STOC: ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Near-optimal algorithms for online matrix prediction", "author": ["E. Hazan", "S. Kale", "S. Shalev-Shwartz"], "venue": "CoRR abs/1204.0136", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Second-order quantile methods for experts and combinatorial games", "author": ["W.M. Koolen", "T. van Erven"], "venue": "CoRR abs/1502.08009", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning 4,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Online convex optimization is an extremely general learning problem that captures many traditional learning problems as special cases ([11]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "We only consider the decision-theoretic setting for prediction with expert advice ([7]).", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "The resulting experts are \u201cspecialists,\u201d who sometimes abstain from offering advice, and we can apply a standard technique to compete with the best specialist ([8]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 7, "context": "We then aggregate these recommendations using an algorithm for learning from specialists ([8]).", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "The most closely related work is ([5]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Along similar lines, ([3]) provides a robust collaborative algorithm for the multi-armed bandit problem, under an additional stochastic assumption.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "Another literature deals with learning problems in which an adversary has some influence over the training or testing data ([4]).", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "We will use the algorithm Squint from ([10]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "Once the experts have decided whether to participate in a round, we use a standard algorithm for combining \u201cspecialists,\u201d experts who are only active in some rounds ([8]).", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "([5]) develops robust collaborative algorithms for this problem when the number of experts in each problem is \u00d5 (1).", "startOffset": 1, "endOffset": 4}], "year": 2017, "abstractText": "We consider a collaborative variant of prediction with expert advice. In each round, one user wishes to make a prediction, and must choose which expert to follow. The users would like to share their experiences in order to learn faster\u2014ideally they could amortize the same total regret cross the whole community of users. However, some of the users may behave maliciously, distorting their reported payoffs in order to manipulate the honest users of the system. And even if all users behave honestly, different experts may perform better for different users, such that sharing data can be counterproductive. We present a robust collaborative algorithm for prediction with expert advice, which guarantees that every subset of users H perform nearly as well as if they had shared all of their data, and ignored all data from users outside of H. This algorithm limits the damage done by the dishonest users to O (\u221a T ) , compared to the O (T ) we would obtain by naively aggregating data. We also extend our results to general online convex optimization. The resulting algorithm achieves low regret, but is computationally intractable. This demonstrates that there is no statistical obstruction to generalizing robust collaborative online learning, but leaves the design of efficient algorithms as an open problem.", "creator": "LaTeX with hyperref package"}}}