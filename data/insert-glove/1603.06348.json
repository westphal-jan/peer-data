{"id": "1603.06348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstration", "abstract": "sego Dexterous multi - alen fingered europeanisation hands can tresser accomplish comrade-in-arms fine manipulation http://libertyforelian.org behaviors alamaro that are 46-39 infeasible willimantic with 2006/7 simple robotic noble grippers. biddy However, sophisticated blood-related multi - fingered hands enraght are a.p often expensive and fragile. Low - cost soft polyphemus hands keysar offer an 21-lap appealing 100km alternative eliscu to miyazawa more conventional devices, but workbook present considerable qabatiya challenges in 82.91 sensing winstone and actuation, bitmaps making bacellar them difficult to sheelagh apply kodacolor to more 1 complex manipulation schuette tasks. renala In maha this paper, we lalang describe an approach to learning from demonstration reanalysis that can be used splinter to schlossplatz train onecomm soft primo\u017e robotic hands breslov to alogoskoufis perform dexterous manipulation tasks. kabua Our method deoghar uses object - irakli centric carignano demonstrations, saggart where a mccleave human iturbe demonstrates the desired khandi motion of manipulated kordestani objects ncert with humourist their own thebes hands, and dhananjoy the robot mannikin autonomously gilovich learns sakya to imitate these prajnaparamita demonstrations frittatas using reinforcement huy learning. network-attached We actuelle propose khara a novel upanishadic algorithm evzen that allows pallu us to blend and 10:05 select 1,911 a subset yiquan of 4-34 the most refreshers feasible hannele demonstrations troop to tna learn to imitate nolet on the hardware, swoop which we use with heideman an croatto extension 115.00 of ariano the openid guided wayport policy barnstorm search vilseck framework to 117 use 1,105 multiple khunjerab demonstrations clorinde to learn generalizable emblem neural network policies. tiarra We demonstrate lymnaeidae our reemphasized approach on 58.34 the cleans RBO lower-powered Hand mapquest.com 2, alunan with bellhorn learned motor skills for turning tolle a valve, dannemora manipulating an abacus, dash and 783 grasping.", "histories": [["v1", "Mon, 21 Mar 2016 08:00:44 GMT  (5198kb,D)", "https://arxiv.org/abs/1603.06348v1", "In submission at International Conference on Intelligent Robots and Systems(IROS) 2016"], ["v2", "Tue, 9 Aug 2016 22:52:49 GMT  (5201kb,D)", "http://arxiv.org/abs/1603.06348v2", "Accepted at International Conference on Intelligent Robots and Systems(IROS) 2016"], ["v3", "Mon, 20 Mar 2017 17:11:44 GMT  (5200kb,D)", "http://arxiv.org/abs/1603.06348v3", "Accepted at International Conference on Intelligent Robots and Systems(IROS) 2016. Pdf file updated for stylistic consistency"]], "COMMENTS": "In submission at International Conference on Intelligent Robots and Systems(IROS) 2016", "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["abhishek gupta", "clemens eppner", "sergey levine", "pieter abbeel"], "accepted": false, "id": "1603.06348"}, "pdf": {"name": "1603.06348.pdf", "metadata": {"source": "CRF", "title": "Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstrations", "authors": ["Abhishek Gupta", "Clemens Eppner", "Sergey Levine", "Pieter Abbeel"], "emails": ["pabbeel}@berkeley.edu", "clemens.eppner@tu-berlin.de"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nControl of multi-fingered hands for fine manipulation skills is exceedingly difficult, due to the complex dynamics of the hand, the challenges of non-prehensile manipulation, and under-actuation. Furthermore, the mechanical design of multi-finger hands tends to be complex and delicate. Although a number of different hand designs have been proposed in the past [1], [2], many of these hands are expensive and fragile.\nIn this work, we address the problem of autonomously learning dexterous manipulation skills with an inexpensive and highly compliant multi-fingered hand \u2014 the RBO Hand 2 [3]. This hand (see Fig. 1) is actuated by inflating airfilled chambers. We cannot accurately control the kinematic finger motion, but can only actuate the hand by inflating or deflating the air chambers. Lack of sensing and precise actuation makes standard control methods difficult to apply directly to devices like this. Instead, we use an approach based on learning from demonstrations (LfD) and reinforcement learning (RL).\nIn LfD, the robot observes a human teacher solving a task and learns how to perform the demonstrated task and\n1Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, CA, USA. This research was funded in part by ONR through a Young Investigator Program award and by the Berkeley Vision and Learning Center (BVLC). {abhigupta, svlevine, pabbeel}@berkeley.edu\n2Robotics and Biology Laboratory, Technische Universita\u0308t Berlin, Germany. The author gratefully acknowledges financial support by the European Commission (SOMA, H2020-ICT-645599). clemens.eppner@tu-berlin.de\napply it to new situations. Demonstrations are typically given visually, by kinesthetic teaching, or through teleoperation. However, these techniques are difficult in case of the RBO Hand 2. A demonstrator cannot manually move all of the fingers of the hand for kinesthetic teaching, and the hand lacks position sensing to store such demonstrations. Even direct teleoperation via intuitive interfaces, such as gloves or motion capture, is non-trivial, because although the RBO Hand 2 is anthropomorphic in design, its degrees of freedom do not match those of the human hand well enough to enable direct mapping of human hand motion.\nHowever, the goal of most dexterous manipulation is to manipulate the poses of objects in the world, and a task can often be fully defined by demonstrating the motion of these objects. These demonstrations can be provided by putting trackers on the objects being manipulated and using a human demonstrator to physically move the objects along the desired trajectories. The object-centric demonstrations consist of just the trajectories of the object trackers, without any other states or actions. These kinds of \u201cobject-centric\u201d demonstrations are intuitive and easy to provide, but because the robot does not directly control the degrees of freedom of moving objects in the world, they cannot be imitated directly. Instead, we use reinforcement learning to construct controllers that reproduce object-centric demonstrations.\nOne crucial challenge that we must address to utilize object-centric demonstrations is to account for the mismatch between the morphology of the human expert and the robot. Since the robot cannot always reproduce all objectcentric demonstrations, we propose a novel algorithm that automatically selects and blends those demonstrations that the robot can follow most closely, while ignoring irrelevant demonstrations that cannot be reproduced.\nar X\niv :1\n60 3.\n06 34\n8v 3\n[ cs\n.L G\n] 2\n0 M\nar 2\n01 7\nIndividual controllers may start from different initial conditions, such as different relative configurations of the hand and manipulated object, and thus may only be able to realize some subset of the demonstrated object-centric trajectories. Furthermore, some demonstrations may be difficult to achieve even from the same initial conditions as the ones they were demonstrated in, due to morphological differences between the robot and the human providing demonstrations, so the only way to determine if a demonstration is achievable is to attempt to imitate it. Our algorithm automatically determines which of the demonstrations are actually feasible to achieve from each initial state by alternating between reinforcement learning to learn the controllers and correspondence assignment to choose which demonstrations to follow from each state.\nOur goal is to find a unified control policy that can generalize to a variety of initial states. To achieve generalization, we train a single nonlinear neural network policy to reproduce the behavior of multiple object-centric demonstrations. This approach follows the framework of guided policy search (GPS [4]), where multiple local controllers are unified into a single high-dimensional policy. This method is chosen because of its effectiveness in learning high dimensional control policies for real robots using a very small number of samples. However, unlike standard GPS, our approach requires only a set of object-centric demonstrations from a human expert to learn a new skill, rather than hand-specified cost functions.\nThe contributions of this paper are: 1) We propose a novel algorithm for learning from object-\ncentric demonstrations. This algorithm enables complex dexterous manipulators to learn from multiple human demonstrations, selecting the most suitable demonstration to imitate for each initial state during training. The algorithm alternates between softly assigning demonstrations to individual controllers, and optimizing those controllers with an efficient trajectory-centric reinforcement learning algorithm. 2) We demonstrate that a single generalizable policy can be learned from this collection of controllers by extending the guided policy search algorithm to learning from demonstrations. 3) We evaluate our approach by learning a variety of dexterous manipulation skills with the RBO Hand 2, showing that our method can effectively acquire complex behaviors for soft robots with limited sensing and challenging actuation mechanisms."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Dexterous Manipulation using Planning", "text": "A variety of methods for generating manipulation behaviors with multi-fingered hands are based on planning. These approaches assume that a detailed model of the hand and object is available a priori. They generate open-loop trajectories that can be executed on real hardware. There exist planners that integrate contact kinematics, non-holonomic motion\nconstraints, and grasp stability to come up with manipulation plans based on finger gaits [5], rolling and sliding fingertip motions [6], or nonprehensile actions involving only the palm [7]. Optimization-based techniques [8] have also been used for in-hand manipulation tasks. All of these approaches rely on detailed models, or make simplifying assumptions about the system. Modelling and simulating the behavior of a soft hand like the RBO Hand 2 is computationally expensive, since it requires finite-element method models [9] to achieve accuracy. Moreover, it is extremely hard to do accurate system identification on such systems. In order to tackle this problem, our approach does not rely on detailed apriori models but learns the task-specific consequences of actions from interactions of the real hardware with the environment, during a task."}, {"heading": "B. Reinforcement Learning for Manipulation", "text": "In order to avoid planning with fixed handcrafted models, control policies that solve continuous manipulation problems can be found using reinforcement learning. A widely used approach is to learn the parameters of a dynamic motor primitive [10] (DMP) with relative entropy policy search [11] or PI2 [12]. This has been used to learn striking movements [13] and bi-manual transportation tasks [14]. Although DMPs are often used to describe the kinematic state of a system, they can be used to generate compliant behavior for picking up small objects or opening doors [15]. However, DMP\u2019s typically require either a model of the system or the ability to control kinematic state, neither of which is straightforward on a soft hand that lacks position sensing.\nControllers for reaching and grasping have been learned by approximating the Q-function with a multilayer perceptron [16]. Policy search methods have succeeded in training neural network controllers to solve contact-rich peg-in-holelike tasks [17] based on positional or visual feedback [18].\nSome RL methods for manipulation have been applied to in-hand manipulation. Van Hoof et al. [19] learn a policy based on tactile feedback which lets an under-actuated hand slide cylindrical objects horizontally while being rolled between two fingers. Similar to our work is the learning method for an in-hand rotation tasks by Kumar et al. [20]. In contrast, we learn global policies that aim to generalize local solutions."}, {"heading": "C. Exploiting Human Demonstrations for Learning Manipulation Skills", "text": "Learning from demonstrations has been effective in teaching robots to perform manipulation tasks with a limited amount of human supervision. By building statistical models of human demonstrations, gestures [21] and dual-arm manipulations [22] have been reproduced on robotic systems. Pure LfD can lead to suboptimal behavior when demonstrator and imitator do not share the same embodiment. To circumvent this problem the learning objective is often extended with additional feedback. This can be provided by a human, e.g. in the case of iteratively improving grasp adaptation [23]. Alternatively, demonstrations can provide the coarse structure of a solution, while the details are iteratively refined and learned\nby the imitator itself. This has been shown for dexterous manipulation [24] where an in-hand manipulation is broken down into a sequence of canonical grasps. In combination with reinforcement learning, demonstrations often serve as an initial policy rollout or they constrain the search space by providing building blocks. This has been applied to reaching motions [25] and dynamic manipulation tasks."}, {"heading": "III. ALGORITHM OVERVIEW", "text": "To find manipulation strategies for the RBO Hand 2 that solve different manipulation tasks, we take advantage of two general concepts: imitating human demonstrations and reinforcement learning. In order to learn from human demonstrations, we exploit task-specific information offered by human demonstrators using object-centric demonstrations, i.e. we only capture the motion of the object being manipulated, not hand-specific information. We use reinforcement learning to learn a policy which imitates these object centric demonstrations. However, due to kinematic and dynamic differences between the human hand and the RBO Hand 2, following some of these demonstrations might not be possible, and hence trying to imitate them closely is undesirable. We describe a novel demonstration selection algorithm that selects which demonstration should be imitated, and use a reinforcement learning method to solve the problem of how to imitate.\nWe define our learning problem as optimizing a policy \u03c0\u03b8 to perform the demonstrated task by learning from demonstrations. In order to learn this policy, we first train multiple different local controllers to imitate the most closely achievable demonstration from their respective initial states. This involves solving the joint problem of selecting the appropriate demonstration for each controller, and using reinforcement learning to train each controller to actually follow its chosen demonstration. By modeling the objective as a minimization of KL divergence between a distribution of controllers and a mixture of demonstrations modeled as Gaussians, as shown in Section IV, this joint problem reduces to an alternating optimization between computing correspondence weights assigning a demonstration to each controller, and optimizing each controller using an optimal control algorithm. This algorithm can be used within the BADMMbased guided policy search framework [18], to train a neural network policy \u03c0\u03b8 to generalize over the learned controllers. We propose a novel learning from demonstrations algorithm based on the GPS framework, which consists of three phases described in sections IV, V, and VI:\n1) Perform a weight assignment which computes soft correspondences between demonstrations and individual controllers (Sec. IV). 2) With the soft correspondences fixed, solve an optimal control problem based on the correspondences and deviations from individual demonstrations (Sec. V). 3) Perform supervised learning over the trajectory distributions from the optimal control phase, using the framework of BADMM-based GPS (Sec. VI).\nAlgorithm 1 Guided policy search with demonstration selection\n1: for iteration k = 1 to K do 2: Generate samples {\u03c4\u0304 j} from each controller p j(\u03c4\u0304) by\nrunning it on the soft hand. 3: Compute soft correspondence weights ai j 4: Estimate system dynamics p(xt+1|xt ,ut) from {\u03c4 j} 5: for iteration inner = 1 to n do 6: Perform optimal control to optimize objective defined\nin Section IV 7: Perform supervised learning to match \u03c0\u03b8 with the\nsamples {\u03c4\u0304 j} 8: return \u03b8 . the optimized policy parameters"}, {"heading": "IV. LEARNING CONTROLLERS FROM MULTIPLE DEMONSTRATIONS", "text": "As the first step to generalizing dexterous manipulation skills, we learn a collection of controllers starting from different initial conditions, such that each controller imitates the demonstration which is most closely achievable from its initial condition. This problem can be cast as minimizing the divergence between two distributions: one corresponding to the demonstrated trajectories, and one to the controllers.\nFor our given dynamical system, we define the states to be xt , and the actions to be ut at every time step t. The system dynamics are specified by the model p(xt+1|xt ,ut). Each controller j is defined in terms of a conditional distribution p j(ut |xt), which along with the dynamics model p(xt+1|xt ,ut) induces a distribution p j(\u03c4) = p j(x0)\u220f p(xt+1|xt ,ut)p j(ut |xt) over trajectories \u03c4 = x1,u1, ...,xT ,uT , where T is the length of an episode. We define p(\u03c4) = \u2211Cj=1 1 C p j(\u03c4) to be the uniform mixture of C controllers p j(\u03c4). Our state xt can be expressed as xt = [x\u0304t ,x\u2032t ], where x\u0304t denotes the \u201dobject-centric\u201d parts tracking the manipulated objects and x\u2032t is the rest of the state. In our experimental setting, x\u0304t consists of positions and velocities of motion capture markers placed on manipulated objects.\nAs we are using object-centric demonstrations, our objective is to match our controllers with the demonstrations but only over the object centric elements (x\u0304) of the state. For each controller p(\u03c4), we can marginalize to obtain p(\u03c4\u0304), which is a uniform mixture of C distributions p j(\u03c4\u0304), such that p(\u03c4\u0304) = \u2211Cj=1 w j p j(\u03c4\u0304) where w j = 1 C and \u03c4\u0304 = {x\u03041, x\u03042, ...., x\u0304T}. This distribution is over just the object-centric trajectories \u03c4\u0304 . The distribution of D demonstrations over the trajectories \u03c4\u0304 is also modeled as a mixture, given by d(\u03c4\u0304) = \u2211Di vidi(\u03c4\u0304). Each di(\u03c4\u0304) is defined as a multivariate Gaussian, constructed according to di(\u03c4\u0304) = N (\u00b5i,\u03a3i), where \u00b5i = {x\u03041, x\u03042, ..., x\u0304T} is the trajectory of the objects recorded in each demonstration, and the covariance \u03a3i is a parameter that decides how closely the demonstration needs to be tracked by the controller. The number of demonstrations and controllers do not have to be the same.\nOur goal is to match the distribution of demonstrations with the distribution of controllers, which we formalize as a KL divergence objective: minp(\u03c4) DKL(p(\u03c4\u0304)||d(\u03c4\u0304)). Although the objective is defined with respect to the object-centric\ndistributions p(\u03c4\u0304), the optimization is done with respect to the entire controller mixture p(\u03c4) which includes other parts of the state, and actions.\nDue to the mode seeking behavior of the KL divergence, this objective encourages each p j(\u03c4\u0304) to match the closest achievable demonstration. However, the KL divergence between mixtures cannot be evaluated analytically. Methods such as MCMC sampling can be used to estimate it, but we find a variational upper bound [26] to be the most suitable for our formulation. In order to simplify our objective, we decompose each mixture weight w j and vi into individual variational parameters ai j and bi j, such that \u2211i ai j = w j and \u2211 j bi j = vi. We can rewrite\nDKL (p(\u03c4\u0304)||d(\u03c4\u0304)) = \u222b\np(\u03c4\u0304) log p(\u03c4\u0304) d(\u03c4\u0304)\n= \u222b \u2212p(\u03c4\u0304) log\n\u2211i, j bi jdi(\u03c4\u0304) p(\u03c4\u0304)\n=\u2212 \u222b\np(\u03c4\u0304) log\u2211 i, j bi jdi(\u03c4\u0304)ai j p j(\u03c4\u0304) ai j p j(\u03c4\u0304)p(\u03c4\u0304) .\nFrom Jensen\u2019s inequality we get an upper bound as follows:\nDKL (p(\u03c4\u0304)||d(\u03c4\u0304))\u2264\u2212 \u222b\np(\u03c4\u0304)\u2211 i, j ai j p j(\u03c4\u0304) p(\u03c4\u0304) log bi jdi(\u03c4\u0304) ai j p j(\u03c4\u0304)\n=\u2212\u2211 i, j\n\u222b p j(\u03c4\u0304)ai j log\nbi jdi(\u03c4\u0304) ai j p j(\u03c4\u0304)\n= [ \u2211 i, j ai j \u222b p j(\u03c4\u0304) log p j(\u03c4\u0304) di(\u03c4\u0304) ] \u2212 [ \u2211 i, j ai j log bi j ai j ] = \u2211\ni, j ai jDKL (p j(\u03c4\u0304)||di(\u03c4\u0304))+DKL (a||b)\nThus, our optimization problem becomes\nmin p(\u03c4),a,b \u2211 i, j ai jDKL (p j(\u03c4\u0304)||di(\u03c4\u0304))+DKL (a||b) . (1)\nWhile the first term \u2211i, j ai jDKL(p j(\u03c4\u0304)||di(\u03c4\u0304)) depends on the distribution p(\u03c4), the second term DKL(a||b) depends on the mixing components ai j and bi j but is independent of the distribution p(\u03c4).\nWe can perform the optimization in two alternating steps, where we first optimize DKL(p(\u03c4\u0304)||d(\u03c4\u0304)) with respect to a, b, followed by an optimization of DKL(p(\u03c4\u0304)||d(\u03c4\u0304)) with respect to p(\u03c4), giving us a block coordinate descent algorithm in {a,b} and p. The convergence of the algorithm is guaranteed by the convergence of a block coordinate descent method on a quasiconvex function and the fact that KL divergence is quasiconvex. The convergence properties of the weight assignment phase is shown in [26].\nIntuitively, the first optimization with respect to a,b is a weight assignment with the correspondence weight ai j representing the probability of assigning demonstration i to controller j. The second optimization with respect to p(\u03c4), keeps the correspondence parameters a, b fixed, and finds optimal controllers using an optimal control algorithm to minimize a weighted objective specified in Eq. 3.\n1) Weight assignment phase: The objective function DKL(p(\u03c4\u0304)||d(\u03c4\u0304)) is convex in both a and b, so we can optimize it by keeping one variable fixed while optimizing the other, and vice versa. We refer the reader to [26] for further details on this optimization. This yields the following closed form solutions:\nbi j = viai j\n\u2211 j\u2032 ai j\u2032 and ai j =\nw jbi je\u2212DKL(p j(\u03c4\u0304)||di(\u03c4\u0304))\n\u2211i\u2032 bi\u2032 je\u2212DKL(p j(\u03c4\u0304)||di\u2032 (\u03c4\u0304)) .\nIn order to compute the optimal a and b, we alternate between these updates for a and b until convergence.\n2) Controller optimization phase: Once the optimal values for a and b have been computed, we fix these as correspondences between demonstrations and controllers and optimize Eq. 1 to recover the optimal p(\u03c4). As a and b are fixed, DKL(a||b) is independent of p. Hence, our optimization becomes:\nmin p(\u03c4) \u2211 i, j ai jDKL(p j(\u03c4\u0304)||di(\u03c4\u0304))\n= \u2211 i, j\nai j ( \u2212Ep j(\u03c4\u0304) [logdi(\u03c4\u0304)]\u2212H (p j(\u03c4\u0304)) ) = \u2211\nj \u2212w jH(p j(\u03c4\u0304))\u2212\u2211 i, j ai jEp j(\u03c4\u0304) [logdi(\u03c4\u0304)] .\nFactorizing the optimization to be independently over each of the controller distributions, for each controller p j(\u03c4), we optimize the objective:\n\u2212w jH(p j(\u03c4\u0304))\u2212\u2211 i ai jEp j(\u03c4\u0304)[logdi(\u03c4\u0304)] (2)\n=\u2212w j ( H(p j(\u03c4\u0304))+\u2211\ni\nai j w j Ep j(\u03c4\u0304)[logdi(\u03c4\u0304)]\n) (3)\nIn practice, the weight assignment is performed independently per time step, as the controllers we consider are time varying."}, {"heading": "V. CONTROLLER OPTIMIZATION WITH AN LFD OBJECTIVE", "text": "While the controller optimization phase could be performed with a variety of optimal control and reinforcement learning methods, we choose a simple trajectory-centric reinforcement learning algorithm that allows us to control systems with unknown dynamics, such as soft hands. Building on prior work, we learn time-varying linear Gaussian controllers by using iteratively refitted time-varying local linear models [4]. This is predicated on the assumption that the system has Gaussian noise, which has been shown to work well in practice for robotic systems [4]. The derivation follows prior work, but is presented here for the specific case of our LfD objective. Action-conditionals for the timevarying linear-Gaussian controllers are given by\np j(ut |xt) = N (K jtxt + k jt ,C jt)\nwhere K jt is a feedback term and k jt is an open loop term. Given this form, the maximum entropy objective (Eq. 3), can be optimized using differential dynamic programming [27],\n[28]. As di(\u03c4\u0304) is a multivariate Gaussian N (\u00b5i,\u03a3i), we can rewrite the optimization problem in Eq. 3 as\nmin p j(\u03c4) \u2211 t,i ai jt \u2211i\u2032 ai\u2032 jt Ex\u0304t\u223cp j(\u03c4\u0304) [ 1 2 (x\u0304t\u2212\u00b5it)T \u03a3\u22121i (x\u0304t\u2212\u00b5it) ] \u2212H(p j(\u03c4\u0304))\nwhere we express the objective as a sum over individual time steps. In this maximum entropy objective, the cost function defined as the expectation of a sum of l2 distances of trajectory samples to each demonstration, weighted by the normalized correspondence weights ai j\u2211i\u2032 ai\u2032 j . The trajectory samples denote the trajectories of the objects which we recover through object markers, and we compute the l2 distance of these samples to the object-centric demonstrations. Under linearized dynamics, this objective can be locally optimized using LQG [29]. However, for robots like the RBO Hand 2, the dynamics are complex and difficult to model analytically. Instead, we can fit a time-varying locally linear model of the dynamics, of the form p(xt+1|xt ,ut) = N ( fxtxt + futut |Cd), to samples obtained by running the physical system in the real world. The dynamics matrices fxt and fut can then be used in place of the system linearization to optimize the controller objective using LQG (for details see [29], [4]). Important to note here is that the iLQG optimization learns K jt , k jt and C jt for the trajectory controller, while the term ai j\u2211i\u2032 ai\u2032 j is learned in the weight assignment phase and kept fixed for the iLQG optimization.\nOne issue with optimizing a controller using fitted local dynamics models is that the model is only valid close to the previous controller. The new controller might visit very different states where the fitted dynamics are no longer valid, potentially causing the algorithm to diverge. To avoid this, we bound the maximum amount the controller changes between iterations. This can be expressed as an additional constraint on the optimization:\nDKL (p j (\u03c4) ||p\u0302 j (\u03c4))< \u03b5, (4)\nwhere p\u0302 j(\u03c4) is the previous trajectory-controller and p j(\u03c4) the new one. As shown in [18], this constrained optimization problem can be formulated in the same maximum entropy form as Eq. 3, using Lagrange multipliers, and solved via dual gradient descent (for details and a full derivation see [4], [18]). In practice, each iteration of this controller optimization algorithm involves generating N samples on the real physical system by running the previous controller, fitting a time-varying linear dynamics model to these samples as in previous work [4], and optimizing a new controller p j(\u03c4) by solving the constrained optimization using dual gradient descent, with LQG used to optimize with respect to the primal variables K jt , k jt , and C jt . This can be viewed as an instance of model-based reinforcement learning."}, {"heading": "VI. SUPERVISED LEARNING USING GPS", "text": "The multiple controllers defined in the previous section learn to imitate the most closely imitable demonstration from their individual starting positions. However, given an unseen initial state, it is not clear which controller p j(\u03c4) should be\nused. For effective generalization, we need to obtain a single policy \u03c0\u03b8 (ut |xt) that will succeed under various conditions. To do this, we extend the framework of GPS [18] to combine controllers into a single nonlinear neural network policy.\nWe learn the parameters \u03b8 of a neural network \u03c0\u03b8 to match the behavior shown by the individual controllers by regressing from the state xt to the actions ut taken by the controllers at each of the N samples generated on the physical system. Simply using supervised learning is not in general guaranteed to produce a policy with good long-horizon performance. In fact, supervised learning is effective only when the state distribution of \u03c0\u03b8 matches that of the controllers p j(\u03c4). To ensure this, we use the BADMM-based variant of GPS [18], which modifies the cost function for the controllers to include a KL-divergence term to penalize deviation of the controllers from the latest policy \u03c0\u03b8 at each iteration. This is illustrated in Algorithm (1), by first assigning correspondences between demonstrations and controllers, then alternating between trajectory optimization and supervised learning at every iteration, eventually leading to a good neural network policy \u03c0\u03b8 . For further details on the guided policy search algorithm, we refer the reader to prior work [18]."}, {"heading": "VII. RBO HAND 2 AND SYSTEM SETUP", "text": "The RBO Hand 2 is an inexpensive, compliant, underactuated robotic hand which has been shown to be effective for a variety of grasping tasks [3]. The hand consists of a polyamide scaffold to which multiple pneumatic actuators are attached (see Fig. 2). Each of the four fingers is a single actuator, while the thumb consists of three independent pneumatic actuators. This makes the thumb the most dexterous part of the hand, achieving seven out of eight configurations of the Kapandji test [30]. The actuators are controlled via external air valves and a separate air supply. Control is challenging since the air valves can only be either fully closed or open and have a switching time of \u223c 0.02s. Each actuator has a pressure sensor located close to the air valve.\nThe hand is controlled by specifying valve opening durations to either inflate or deflate a single actuator. We turn the discrete valve actions into a continuous control signal using pulse width modulation. Given a constant frequency of 5Hz, the control signal is interpreted as the duration the inflation (if it is positive) or deflation (negative) valve is opened during a single time step. To ensure that the control signal does not exceed the duration of a single time step we apply a sigmoid function to the commands from the learning algorithm.\nThe positions and velocities of the manipulated objects are captured in real time with a PhaseSpace Impulse system, which relies on active LED markers. The state xt of our system is the concatenation of the seven pressure readings of the hand, their time derivatives, the 3D positions and velocities of markers attached to the object, and joint angles of the robot arm (depending on the task). We placed no LED markers on the hand itself, only the object was equipped to record object-centric demonstrations, and the positions and velocities of these markers constitute object-centric state x\u0304t ."}, {"heading": "VIII. EXPERIMENTS", "text": "We evaluated our algorithm on a variety of manipulation and grasping tasks. Our experiments aim to show that\n1) It is possible to perform fine manipulation with the RBO Hand 2. 2) Our algorithm can learn feedback policies from demonstrations that perform nearly as well as an oracle with the correct demonstrations (depending on the context) manually assigned to controllers. 3) A single neural network policy learned from demonstrations with our method is able to generalize to different initial states.\nWe will evaluate our learning approach on three different tasks: turning a valve, pushing beads on an abacus, and grasping a bottle (see Fig. 3). A video of the experiments can be found at https://youtu.be/XyZFkJWu0Q0. For the first two tasks, we compare our method to the following baselines:\nHand designed baseline: A controller with a hand-designed open loop policy. In the case of the abacus task, we evaluate the performance of two different strategies for simple handdesigned baselines. Single demo baseline: A single controller trained to imitate a single demonstration. We use two separate baselines which are trained to follow different demonstrations. Oracle: Depending on the context we manually assign the correct achievable demonstration to controllers. This comparison is useful to test whether the correspondence assignments are accurate."}, {"heading": "A. Turning a valve", "text": "1) Experimental setup: Rotating a gas valve is a challenging task since it involves coordinating multiple fingers. Our valve consists of a blue horizontal lever that increases its range of motion (Fig. 3). Varying wrist positions along the lever require different finger motions to rotate it.\nWe mound the RBO Hand 2 on a PR2 robot arm, with the objective to rotate the valve away from the initial center position in either direction, using just its fingers. The arm is kept stationary for each episode, but changes positions between the training of different controllers. The joint angles are part of the state to determine the relative position of the hand with respect to the valve.\nA human demonstrated three different valve rotations with their own hand, while two LED markers tracked the motion of the lever. Two demonstrations were of the valve rotating clockwise and anti-clockwise at the same position, and a third demonstration with the valve placed at a different position and rotated anticlockwise. All three demonstrations are valid for the task, but not all of them are achievable from every training position. Our algorithm trained three individual controllers and a single neural network policy to generalize over them.\n2) Results and Discussion: During evaluation, the policy learned by each method was sampled ten times at four positions of the hand relative to the valve. The results in Fig. 4 show that our method generates the most robust policy compared with the baselines, which each fail to turn the valve significantly in at least one position. Our method does nearly as well as the oracle, for which demonstrations are assigned to controllers manually. While learning the correspondence weights and the individual controller policies, our method determines which of the demonstration it can actually perform from its initial positions, and disregards distant unachievable demonstrations.\nOur method is able to learn distinctly different behavior at various test positions. At position 1, the policy pushes the lever using its last two fingers, with support given by the thumb. At position 2, the policy uses the thumb to rotate the valve by pushing the lever as the fingers are blocked. At position 3, our policy extends the thumb out of the way and pushes strongly with the index finger to rotate the valve. Simple open loop hand-designed strategies and the baselines learned from a single demonstration fail to learn this distinctly different behavior needed to generalize to different positions along the valve lever. By learning that different joint angles of the arm require different behaviors to be performed, our method is able to perform the task in various positions."}, {"heading": "B. Pushing the beads of an abacus", "text": "1) Experimental setup: The RBO Hand 2 is required to push particular beads on an abacus while leaving other beads stationary. This task is challenging due to the precise\nindividual finger motions needed to move only the desired beads. The hand is mounted on a stationary PR2 arm (Fig. 3), while the abacus is moved to several positions. The beads of relevance here are the central yellow, orange and red ones. Markers were attached to each of the three beads to capture their motion. As the position of the abacus with respect to the hand changes, different fingers need to be used. During demonstrations a human pushed only the yellow beads along their spindle at each of the three positions shown in Fig. 5.\n2) Results and Discussion: We evaluated ten samples of each policy at each of the three positions and recorded the distances that each bead moved. The results are shown in Fig. 5. Our method moves the bead closer to the target position than the single demonstration and hand designed baselines for all the test positions. Only the oracle policy produces equally good performance. By interleaving selection of the right demonstration to imitate, with optimal control and supervised learning, our algorithm is able to learn a policy which uses discretely different fingers depending on the positions of the abacus relative to the hand. On the other hand, the hand-designed baselines being open loop can never learn different behaviors for different fingers. The controller trained at a single position fails because it has no notion of\ngeneralization."}, {"heading": "C. Grasping a bottle", "text": "1) Experimental setup: This task involves using the soft hand mounted on a moving arm, to grasp a deodorant bottle placed on a table. The arm has a scripted motion of moving up after 8 seconds, and we use reinforcement learning to learn the behavior of the fingers to go with this arm motion. The objective of the task is to grasp the bottle before the arm starts moving and keep it grasped until the end of the episode at the final arm location.\nAs grasping tasks for several objects largely succeed in open loop, our aim is to demonstrate that we can match the performance of a hand-designed baseline with a controller learned from a human demonstration through optimal control. This experiment is challenging for reinforcement learning algorithms due to the delayed nature of the reward signal in grasping.\nWe provide a demonstration of the bottle being lifted by a human, and use it to define the cost function for trajectory optimization as the l2 distance of trajectory samples from the provided demonstration. We also apply a Gaussian filter to the noise generated in the controllers to be more temporally\ncoherent, allowing tight grasping. The resulting learned control policy is then tested on 10 sample trajectories in order to evaluate whether a successful grasp has occurred where the objected is lifted and kept at the maximum arm height.\n2) Results and Discussion: We find that on the grasping task, the control policy learned through optimal control does just as well as a hand-designed policy on ten samples of grasping the bottle. Both the hand-designed policy and the learned policy were able to grasp the bottle for all 10 test samples. This indicates that the learning has comparable results to a hand-designed baseline, despite not having prior information besides a human-provided demonstration."}, {"heading": "D. Limitations", "text": "Although the LfD algorithm shows good performance on several tasks using the RBO Hand 2, there are many directions for future work. Instead of using a motion capture system, we hope to use better computer vision techniques such as deep convolutional nets to track trajectories of relevant feature points in future work. Extending the neural network policy to learn policies dependent on just the pressure sensors in the fingers and/or additional tactile sensors, would be an exciting future direction."}, {"heading": "IX. CONCLUSIONS", "text": "We presented an algorithm for learning dexterous manipulation skills with a soft hand from object-centric demonstrations. Unlike standard LfD methods, our approach only requires the human expert to demonstrate the desired behaviors with their own hand. Our method automatically determines the most relevant demonstrations to track, using reinforcement learning to optimize a collection of controllers together with controller to demonstration correspondences. To generalize the demonstrations to new initial conditions, we utilize the GPS framework to train nonlinear neural network policies that combine the capabilities of all of the controllers. We evaluate our method on the RBO Hand 2, and show that it is capable of learning a variety of dexterous manipulation skills, including valve turning, moving beads on an abacus, and grasping."}], "references": [{"title": "Anthropomorphic robot hand: Gifu hand iii", "author": ["T. Mouri", "H. Kawasaki", "K. Yoshikawa", "J. Takai", "S. Ito"], "venue": "Proc. Int. Conf. ICCAS, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A novel type of compliant and underactuated robotic hand for dexterous grasping", "author": ["R. Deimel", "O. Brock"], "venue": "The International Journal of Robotics Research, vol. 35, no. 1-3, pp. 161\u2013185, March 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "NIPS, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Dextrous manipulation by rolling and finger gaiting", "author": ["L. Han", "J.C. Trinkle"], "venue": "ICRA, vol. 1. IEEE, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Planning quasi-static fingertip manipulations for reconfiguring objects", "author": ["M. Cherif", "K.K. Gupta"], "venue": "Robotics and Automation, IEEE Transactions on, vol. 15, no. 5, pp. 837\u2013848, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Dexterous manipulation using both palm and fingers", "author": ["Y. Bai", "C.K. Liu"], "venue": "ICRA 2014. IEEE, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Contact-invariant optimization for hand manipulation", "author": ["I. Mordatch", "Z. Popovi\u0107", "E. Todorov"], "venue": "Proc. of the ACM SIGGRAPH, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Modeling of soft fiber-reinforced bending actuators", "author": ["P. Polygerinos", "Z. Wang", "J.T.B. Overvelde", "K.C. Galloway", "R.J. Wood", "K. Bertoldi", "C.J. Walsh"], "venue": "IEEE T-RO, vol. 31, no. 3, pp. 778\u2013789, June 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning attractor landscapes for learning motor primitives", "author": ["A.J. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "Tech. Rep., 2002.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Relative entropy policy search.", "author": ["J. Peters", "K. M\u00fclling", "Y. Altun"], "venue": "in AAAI. Atlanta,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning policy improvements with path integrals", "author": ["E.A.B.J.S.S. Theodorou"], "venue": "AISTATS 2010, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["K. M\u00fclling", "J. Kober", "O. Kroemer", "J. Peters"], "venue": "IJRR, vol. 32, no. 3, pp. 263\u2013279, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards learning hierarchical skills for multi-phase manipulation tasks", "author": ["O. Kroemer", "C. Daniel", "G. Neumann", "H. van Hoof", "J. Peters"], "venue": "ICRA. IEEE, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning force control policies for compliant manipulation", "author": ["M. Kalakrishnan", "L. Righetti", "P. Pastor", "S. Schaal"], "venue": "IROS. IEEE, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Acquiring visual servoing reaching and grasping skills using neural reinforcement learning", "author": ["T. Lampe", "M. Riedmiller"], "venue": "IJCNN, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning contact-rich manipulation skills with guided policy search", "author": ["S. Levine", "N. Wagener", "P. Abbeel"], "venue": "ICRA, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning robot in-hand manipulation with tactile features", "author": ["H. van Hoof", "T. Hermans", "G. Neumann", "J. Peters"], "venue": "Humanoid Robots (Humanoids). IEEE, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental learning of gestures by imitation in a humanoid robot", "author": ["S. Calinon", "A. Billard"], "venue": "Proceedings of the ACM/IEEE international conference on Human-robot interaction. ACM, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Imitation learning of dual-arm manipulation tasks in humanoid robots", "author": ["T. Asfour", "P. Azad", "F. Gyarfas", "R. Dillmann"], "venue": "International Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183\u2013202, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative learning of grasp adaptation through human corrections", "author": ["E.L. Sauser", "B.D. Argall", "G. Metta", "A.G. Billard"], "venue": "Robotics and Autonomous Systems, vol. 60, no. 1, pp. 55\u201371, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Modeling and planning high-level in-hand manipulation actions from human knowledge and active learning from demonstration", "author": ["U. Prieur", "V. Perdereau", "A. Bernardino"], "venue": "IROS. IEEE, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Reinforcement learning for imitating constrained reaching movements", "author": ["F. Guenter", "M. Hersch", "S. Calinon", "A. Billard"], "venue": "Advanced Robotics, vol. 21, no. 13, pp. 1521\u20131544, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Approximating the Kullback Leibler divergence between Gaussian mixture models", "author": ["J. Hershey", "P. Olsen"], "venue": "ICASSP, vol. 4, April 2007, pp. IV\u2013317\u2013IV\u2013320.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Differential dynamic programming", "author": ["D.H. Jacobson", "D.Q. Mayne"], "venue": "New York: Elsevier,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1970}, {"title": "Guided policy search", "author": ["S. Levine", "V. Koltun"], "venue": "Proceedings of The 30th International Conference on Machine Learning, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative linear quadratic regulator design for nonlinear biological movement systems.", "author": ["W. Li", "E. Todorov"], "venue": "ICINCO", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Cotation clinique de lopposition et de la contreopposition du pouce", "author": ["I.A. Kapandji"], "venue": "Annales de Chirurgie de la Main, vol. 5, no. 1, pp. 68\u201373, 1986.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 0, "context": "Although a number of different hand designs have been proposed in the past [1], [2], many of these hands are expensive and fragile.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "In this work, we address the problem of autonomously learning dexterous manipulation skills with an inexpensive and highly compliant multi-fingered hand \u2014 the RBO Hand 2 [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "This approach follows the framework of guided policy search (GPS [4]), where multiple local controllers are unified into a single high-dimensional policy.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "There exist planners that integrate contact kinematics, non-holonomic motion constraints, and grasp stability to come up with manipulation plans based on finger gaits [5], rolling and sliding fingertip motions [6], or nonprehensile actions involving only the palm [7].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "There exist planners that integrate contact kinematics, non-holonomic motion constraints, and grasp stability to come up with manipulation plans based on finger gaits [5], rolling and sliding fingertip motions [6], or nonprehensile actions involving only the palm [7].", "startOffset": 210, "endOffset": 213}, {"referenceID": 5, "context": "There exist planners that integrate contact kinematics, non-holonomic motion constraints, and grasp stability to come up with manipulation plans based on finger gaits [5], rolling and sliding fingertip motions [6], or nonprehensile actions involving only the palm [7].", "startOffset": 264, "endOffset": 267}, {"referenceID": 6, "context": "Optimization-based techniques [8] have also been used for in-hand manipulation tasks.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "Modelling and simulating the behavior of a soft hand like the RBO Hand 2 is computationally expensive, since it requires finite-element method models [9] to achieve accuracy.", "startOffset": 150, "endOffset": 153}, {"referenceID": 8, "context": "A widely used approach is to learn the parameters of a dynamic motor primitive [10] (DMP) with relative entropy policy search [11] or PI2 [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "A widely used approach is to learn the parameters of a dynamic motor primitive [10] (DMP) with relative entropy policy search [11] or PI2 [12].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "A widely used approach is to learn the parameters of a dynamic motor primitive [10] (DMP) with relative entropy policy search [11] or PI2 [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "This has been used to learn striking movements [13] and bi-manual transportation tasks [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "This has been used to learn striking movements [13] and bi-manual transportation tasks [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Although DMPs are often used to describe the kinematic state of a system, they can be used to generate compliant behavior for picking up small objects or opening doors [15].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "Controllers for reaching and grasping have been learned by approximating the Q-function with a multilayer perceptron [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "Policy search methods have succeeded in training neural network controllers to solve contact-rich peg-in-holelike tasks [17] based on positional or visual feedback [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "Policy search methods have succeeded in training neural network controllers to solve contact-rich peg-in-holelike tasks [17] based on positional or visual feedback [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": "[19] learn a policy based on tactile feedback which lets an under-actuated hand slide cylindrical objects horizontally while being rolled between two fingers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "By building statistical models of human demonstrations, gestures [21] and dual-arm manipulations [22] have been reproduced on robotic systems.", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "By building statistical models of human demonstrations, gestures [21] and dual-arm manipulations [22] have been reproduced on robotic systems.", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "in the case of iteratively improving grasp adaptation [23].", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "manipulation [24] where an in-hand manipulation is broken down into a sequence of canonical grasps.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "This has been applied to reaching motions [25] and dynamic manipulation tasks.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "This algorithm can be used within the BADMMbased guided policy search framework [18], to train a neural network policy \u03c0\u03b8 to generalize over the learned controllers.", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "Methods such as MCMC sampling can be used to estimate it, but we find a variational upper bound [26] to be the most suitable for our formulation.", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "The convergence properties of the weight assignment phase is shown in [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "We refer the reader to [26] for further details on this optimization.", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "Building on prior work, we learn time-varying linear Gaussian controllers by using iteratively refitted time-varying local linear models [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "This is predicated on the assumption that the system has Gaussian noise, which has been shown to work well in practice for robotic systems [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 24, "context": "3), can be optimized using differential dynamic programming [27],", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Under linearized dynamics, this objective can be locally optimized using LQG [29].", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "The dynamics matrices fxt and fut can then be used in place of the system linearization to optimize the controller objective using LQG (for details see [29], [4]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 2, "context": "The dynamics matrices fxt and fut can then be used in place of the system linearization to optimize the controller objective using LQG (for details see [29], [4]).", "startOffset": 158, "endOffset": 161}, {"referenceID": 16, "context": "As shown in [18], this constrained optimization problem can be formulated in the same maximum entropy", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "3, using Lagrange multipliers, and solved via dual gradient descent (for details and a full derivation see [4], [18]).", "startOffset": 107, "endOffset": 110}, {"referenceID": 16, "context": "3, using Lagrange multipliers, and solved via dual gradient descent (for details and a full derivation see [4], [18]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "fitting a time-varying linear dynamics model to these samples as in previous work [4], and optimizing a new controller", "startOffset": 82, "endOffset": 85}, {"referenceID": 16, "context": "To do this, we extend the framework of GPS [18] to combine controllers into a single nonlinear neural network policy.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "To ensure this, we use the BADMM-based variant of GPS [18], which modifies the cost function for the controllers to include a KL-divergence term to penalize deviation of the controllers from the latest policy \u03c0\u03b8 at each iteration.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "For further details on the guided policy search algorithm, we refer the reader to prior work [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "The RBO Hand 2 is an inexpensive, compliant, underactuated robotic hand which has been shown to be effective for a variety of grasping tasks [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 27, "context": "This makes the thumb the most dexterous part of the hand, achieving seven out of eight configurations of the Kapandji test [30].", "startOffset": 123, "endOffset": 127}], "year": 2017, "abstractText": "Dexterous multi-fingered hands can accomplish fine manipulation behaviors that are infeasible with simple robotic grippers. However, sophisticated multi-fingered hands are often expensive and fragile. Low-cost soft hands offer an appealing alternative to more conventional devices, but present considerable challenges in sensing and actuation, making them difficult to apply to more complex manipulation tasks. In this paper, we describe an approach to learning from demonstration that can be used to train soft robotic hands to perform dexterous manipulation tasks. Our method uses object-centric demonstrations, where a human demonstrates the desired motion of manipulated objects with their own hands, and the robot autonomously learns to imitate these demonstrations using reinforcement learning. We propose a novel algorithm that allows us to blend and select a subset of the most feasible demonstrations, which we use with an extension of the guided policy search framework that learns generalizable neural network policies. We demonstrate our approach on the RBO Hand 2, with learned motor skills for turning a valve, manipulating an abacus, and grasping.", "creator": "LaTeX with hyperref package"}}}