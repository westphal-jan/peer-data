{"id": "1603.07886", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity", "abstract": "dwi Integration between biology melanella and musicomh information padus science soundproofing benefits recollected both fields. slingshots Many warracknabeal related models territorially have been proposed, kyzer such as computational visual reflecting cognition stabilizer models, fleabag computational navales motor ren?e control models, cen\u00e9l integrations of dergue both 19.68 and kosko so concluye on. In general, 2,689 the 2.4-billion robustness weezy and precision of blumenschein recognition is one of gefter the 81-67 key 2,696 problems for takamura object recognition models.", "histories": [["v1", "Fri, 25 Mar 2016 11:47:16 GMT  (4849kb,D)", "http://arxiv.org/abs/1603.07886v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["peijie yin", "hong qiao", "wei wu", "lu qi", "yinlin li", "shanlin zhong", "bo zhang"], "accepted": false, "id": "1603.07886"}, "pdf": {"name": "1603.07886.pdf", "metadata": {"source": "CRF", "title": "A Novel Biologically Mechanism-Based Visual Cognition Model \u2013Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity", "authors": ["Peijie Yin", "Hong Qiao", "Wei Wu", "Lu Qi", "YinLin Li", "Shanlin Zhong", "Bo Zhang"], "emails": ["hong.qiao@ia.ac.cn)."], "sections": [{"heading": null, "text": "In this paper, inspired by features of human recognition process and their biological mechanisms, a new integrated and dynamic framework is proposed to mimic the semantic extraction, concept formation and feature re-selection in human visual processing. The main contributions of the proposed model are as follows: (1) Semantic feature extraction: Local semantic features are\nlearnt from episodic features that are extracted from raw images through a deep neural network; (2) Integrated concept formation: Concepts are formed with local semantic information and structural information learnt through network. (3) Feature re-selection: When ambiguity is detected during recognition process, distinctive features according to the difference between ambiguous candidates are re-selected for recognition.\nExperimental results on hand-written digits and facial shape dataset show that, compared with other methods, the new proposed model exhibits higher robustness and precision for visual recognition, especially in the condition when input samples are smantic ambiguous. Meanwhile, the introduced biological mechanisms further strengthen the interaction between neuroscience and information science.\nIndex Terms\u2014Biologically inspired model, object recognition, semantic learning, structural learning\nI. INTRODUCTION\nW ITH the integration of neuroscience and informationscience, more and more biological mechanisms have been applied in computational models, which promotes the development of biologically inspired models. On the one hand, inspired by recent findings in biology, these models outperform classic algorithms in performance and efficiency. On the other hand, related neural mechanisms, which are introduced into\nH. Qiao, W. Wu, L. Qi, Y. Li, S. Zhong are with the State Key Lab of Management and Control for Complex Systems,Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China (e-mail: hong.qiao@ia.ac.cn).\nP. Yin, B. Zhang are with the Institute of Applied Mathematics, Academy of Mathematics and Systems Science, Chinese Academy of Science.\nThis work was supported by the National Science Foundation of China under grant 61210009, and the Strategic Priority Research Program of the CAS (grant XDB02080003).\ncomputational models, could be validated and testified to promote the development in neuroscience.\nVision is one of the key interdisciplinary research directions between neuroscience and information science. Mechanisms of primate and human in visual processing and cognition have been introduced into computational cognition model.\nHMAX model tries to mimic the functions of primate visual system layer by layer [1].\nThe main difference between HMAX and other hierarchical architectures (such as hand-crafted hierarchical features [2], convolutional neural networks [3], and etc.) is that it focused on reproducing anatomical, physiological and psychophysical properties of the ventral pathway of visual system [4], which consists of V 1, V 2, V 4 and inferior temporal (IT) cortical areas.\nAfter its first publication in 1999, this well-known model has been further developed and improved in different aspects [5]\u2013[8]. For example, many researchers modified the original HMAX model by adding feedback process to improve the recognition precision [9], [10].\nOther modifications include adding sparsity to the convolutional layer [8], enhancing the architecture by adding specific layers to the model [7], [11], and changing strategies of feature selection and filtering properties [12]. Frontier researchers try to introduce mechanisms of attention into visual model. Itti proposed a saliency-based model based on the saliency map theory in human visual system [13] and combined attention with object recognition [14], [15]. Spatial information of an object is introduced by modeling the dorsal pathway in vision system. It has been implement by Bayesian inference [16] and saliency model [17]. Mechanisms in middle and high level cortices are also a hot topic in the area. Based on HMAX and deep neural network, Qiao et al. developed a series of models introducing association [18], attention [19] to the model. The introduced mechanisms show good performance on object classification and identification tasks.\nRobustness, i.e., the ability of generalization is one of the key objectives and motivations in these visual cognition models. However, recent findings [20], [21] point out that even the state-of-art deep hierarchical networks suffer from tiny disturbance and transformation. It is shown that tiny perturbation may cause significant difference in the output of hierarchical network models [21].\nHowever, human has extraordinary ability to deal with dif-\nar X\niv :1\n60 3.\n07 88\n6v 1\n[ cs\n.C V\n] 2\n5 M\nar 2\n01 6\n2 ficult object recognition task with various viewpoints, scales, deformation, and ambiguity. According to biological findings, objection recognition tasks involve multiple cortices and many sophisticated mechanisms including preliminary cognition, top-down attention [22], semantic and conceptual memory [23]\u2013[25]. Lake, Salakhutdinov and Tenenbaum recently [26] employs semantics and concepts explicitly and achieves significant improvement in robustness of one-shot character recognition.\nIn this paper, we build a Biological mechanism based Semantic Neural Network model (BSNN), which extracts semantic information hierarchically and forms concepts with corresponding probabilities. The model is trained sequentially and generate hierarchical information layer by layer.\nTo mimic the biological mechanisms, the model firstly trains a neural network to extract episodic features; then it integrates the learnt episodic features into the semantic features. To encode the structure information, the model learns structural relationships between semantics and represents them as population vectors. With the population vectors, concepts for categories are formed in a probabilistic way. The proposed model also applies two dynamic updating strategies, feature re-selection for adaption to ambiguous condition, and online training for new concepts. By mimicking and implementing neural mechanisms in visual processing, the model achieves robustness to various ambiguous images with small training samples. It is also more efficient to diminish uncertainty by semantics and concepts with the ability of generalization.\nThe rest of this paper is organized as follows. Section II introduces biological evidence of the proposed model. Section III explains the framework and methods in the BSNN. Section IV presents how the experiments are conducted and shows experimental results. Section V summarizes current work and points out future direction."}, {"heading": "II. BIOLOGICAL EVIDENCE", "text": "In this paper, several biological mechanisms are introduced into the new framework to mimic semantic extraction, concept formation and feature re-selection process in human visual processing. Here, related biological evidence has been reviewed and discussed for its validity of later implementation."}, {"heading": "A. Semantic Feature Extraction", "text": "Two different types of memory are stored in the brain: episodic memory and semantic memory [25], [27]. Episodic memory stores events and detailed contextual information, while semantic memory extracts regularities from different spatial-temporal events and forms perceptual categories, complex concepts and relations [27]. This requires that extraction of regularities or semantics should be carried out over episodes [28], [29]. Since hippocampus is involved with storage of episodic memory and prefrontal cortex contributes to organization of information, the extraction process could be achieved via hippocampus and mPFC (medial prefrontal cortex) interaction [30]\u2013[32]. The extracted semantic information could be used for later tasks."}, {"heading": "B. Structural Information", "text": "It has been proposed that objects could be described with parts and their positional and connectional relationships [33], [34]. For example, neurons in V4 are tuned for contour fragment orientation with specific object-relative position [35]. In other words, one V4 neuron could respond to convex curvature at bottom right (such as \u2019b\u2019), but not to that at top right (such as \u2019p\u2019). Thus, in V4 area, neurons respond to individual contour fragments and their relationships are encoded in population responses [36]. In PIT (posterior inferior temporal cortex), neurons integrate information on multiple fragments [35]. Thus, the integrated explicit representations of multi-part configurations could be encoded in IT."}, {"heading": "C. Selective Attention", "text": "Attention is required when people carry out various tasks, since relevant environmental stimuli and information should be selected and processed in the brain [22], [37]. Several brain areas are activated in the attention process, such as frontal eye fields (FEF), anterior cingulate, frontal cortex, and etc. [38]. Visual attention usually consists of active exploration of the environment, selection of task-related information and suppression of distraction. When visual stimuli is not clear for the task, visual attention process could suppress distraction from location of previous attention focus and find new positions for the search of related information [39]."}, {"heading": "III. THE FRAMEWORK", "text": "In this section, we present the structure of the proposed framework. Firstly, the outline of the framework is described. Secondly, the algorithms of semantic feature extraction, integrated concept formation and feature re-selection are given in details."}, {"heading": "A. Outline of the Framework", "text": "Figure 1 shows the training procedure of the model. Figure 2 shows the recognition and online updating procedure.\n1) Block1: Primary episodic feature extraction block, where episodic features are extracted from the original image directly. Block 1 includes a four-layer convolutional deep belief network (CDBN). The network is trained layer by layer without supervision. The output of the layer is the activation states of the last layer (for block 2) and the learnt connection weights (for block 3).\n2) Block2: Semantic feature extraction block, which extracts semantic features from the learnt episodic features in Block 1.\nA cluster-based method is applied here, which provides a better abstract description of the object than the feature maps.\n3) Block3: Semantic spatial information learning block, where spatial positions of semantic features are learnt based on the output of Block 1 and Block 2. Spatial information is encoded based on position-related population neurons.\n4) Block4: Structural concept formation block, where relationships between semantic features are formed from the spatial information in Block 3. Relationships are encoded via orientation-related population neurons. For each input sample, a relationship matrix is generated to represent the global structure.\n5) Block5: Integrated recognition block, which combines episodic and semantic features together for recognition. Block 5 in Fig. 1 learns weights of episodic and semantic features from different pathways, which uses for integrated classification in Block 5 of Fig. 2.\n6) Block6: Feature re-selection block, which copes with ambiguous situations dynamically. During the training procedure in Fig. 1, the correlation between extracted features and categories will be learnt. When there are two or more candidate recognition results, features that are more discriminative between candidates will be selected for further classification."}, {"heading": "B. Episodic Features Learning with Unsupervised Deep Neural Network (Block 1)", "text": "In human, episodic memory is the memory that represents experiences and specific events in time, from which people can reconstruct the actual events that took place at any given\ntime [27]. Episodic memory is one of the basic forms of explicit memory and considered as the source of other forms of memory [40].\nIn this paper, episodic features are extracted via an unsupervised deep neural network. Unsupervised convolutional deep belief network (CDBN) is first introduced in Ng\u2019s work [41] for feature extraction tasks. In our previous work [42], CDBN has been used to extract episodic information from the image. As an unsupervised model, CDBN is able to extract good\nlocal features and encode common components by minimizing the reconstruction error, which ensures good performance in recognition. CDBN is composed of stacked Convolutional Restricted Boltzmann Machine (CRBM). CRBM, as a variant of RBM, can infer the original input from the activation and minimize the reconstruction error. Thus, the visual information could be retrieved from memory, which is similar to human. The structure of CRBM is showed in Fig 3.\nAs shown in Fig. 3, each CRBM includes three layers: visible layer V, hidden layer H and pooling layer P. nv and nh are the widths of V and H, respectively. H has K groups of feature maps, which is denoted by Hk (k = 1, 2, \u00b7 \u00b7 \u00b7 ,K). H is connected with V by the shared local weights W k with width nw. So the width of Hk is calculated as nh = nv\u2212nw+1. Let vki,j represent an unit in layer V with row index i and column index j, and hki,j stands for an unit in layer H\nk. Layer P is the pooling layer of H. The unit pk\u03b1 is obtained by pooling from a specific c \u00d7 c block in Hk denoted by B\u03b1. So P also has K groups of feature maps P k (k = 1, 2, \u00b7 \u00b7 \u00b7 ,K) with width np = nh/c. In mathematics, the CRBM model is a special type of energy-based models. Given inputs V and hidden layer H with binary feature maps Hk, the energy of each possible state (v, h), where v \u2208 Rnv\u00d7nv and h \u2208 Bnh\u00d7nh\u00d7K (B = {0, 1}), is defined in (1)\nE(v, h) = \u2212 K\u2211 k=1 nh\u2211 i,j=1 hki,j(W\u0303 k \u2217 v)i,j \u2212 K\u2211 k=1 bk nh\u2211 i,j=1 hki,j\n\u2212a nv\u2211 i,j=1 vi,j + 1 2 nv\u2211 i,j=1 v2i,j , (1)\nwhere hki,j meets the constraint\n\u2211 (i,j)\u2208B\u03b1 hki,j \u2264 1,\u2200k, \u03b1. (2)\nHere, W\u0303 k, representing the 180-degree rotation of matrix W k, is the convolutional kernel, \u2217 denotes the convolution operation, bk is the shared basis of all units in Hk, and a is the shared basis of visible layer units.\nThe CRBM can be trained with Contrastive Divergence (CD), which is an approximate Maximum-Likelihood learning algorithm [43]. By training CRBMs sequentially, the CDBN is also trained.\nHowever, since it is trained without supervision, the features may be not distinctive enough for classification. Meanwhile, useful features sometimes could shrink to a small set and\n5 thus the generalization ability is limited. Moreover, because it focuses on minimization of the reconstruction error, it could not go deeper to the semantic level and extract the structure information. To overcome this drawback, this paper introduces extraction of semantic features to enhance the ability of distinction of features.\nAs a generative model, CDBN also has the ability to achieve reconstruction from activation. So the model can recall the original input image by reconstruction as augmentation of training data.\nIn this paper, we train a two-layer CDBN and apply it to extract features from original images. We also reconstruct the input images from the activation of the output layer for visualization and data augmentation when the number of training samples is relatively small. The visual reconstruction v\u2032 is defined by\nv\u2032 = \u2211 k sk(W k \u2217Hk) (3)\nwhere sk denotes the weights of the connection between W k and Hk. sk is learned by feature re-selection, which is described in detail in Block 6."}, {"heading": "C. Semantic Representation Learning based on Episodic Features (Block 2)", "text": "Semantics has multiple definitions in different fields, such as linguistics [44], [45], cognitive science [46], [47], artificial intelligence [48], [49] and etc. In cognitive science, semantic memory is about facts that capture the internal properties about an object [27], [47]. Human use semantic memory to store the category and abstract information about the object and distinguish one category of objects from others. Binder and Desai [47] proposed that modality-specific semantic memory is encoded in the corresponding cortex. Convergence of these findings, semantic information in vision is represented in the similar form of visual episodic features, but more abstract and discriminative.\nInspired by the above mentioned properties of semantic memory in neuroscience, a reasonable hypothesis is that semantic features for visual task are formed based on those learnt hierachical episodic features. Semantic features are more likely to activated by diverse properties of an object. For visual recognition, each property, like a stroke or a shape, represents a general cluster of patches rather than a certain patch. In this way, a formalized description of semantic features is given as follows:\nDenote the reconstruction function frecon : F \u2192 I, where F is the space of episodic features, I is the space of input images.\nGiven patches V\u2032 = {v\u20321, v\u20322, ..., v\u2032n} (v\u2032i is reconstructed from the ith episodic feature, where 1 \u2264 i \u2264 n and n is the number of episodic features), find K groups in V\u2032 based on similarities of patches (K is a relative small number compared to the size of V). Divide corresponding episodic features into groups according to their reconstructions. For each group of patches, find vj (vj \u2208 frecon(F), j = 1, ..,K) as a representative of the group which minimizes the loss of\nimformation. Representatives denoted as {Sj , j = 1, ..,K}, {Sj} are semantic features abstracted from previous episodic features.\nThe objective of semantic clustering is to find:\nargmin S K\u2211 j=1 \u2211 v\u2032i\u2208Sj \u2016v\u2032i \u2212 Sj\u2016 2\n(4)\nwhere \u2016\u00b7\u2016 refers to the metric from I restricted to frecon(F). In our model, for computational convenience, we use kmeans and L2 metrics to iteratively find the desired Si. Given an initial set of k-means {m(1)1 , ...,m (1) k }, the algorithm proceeds by alternating between two steps: Assignment step: Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS).\nS (t) j = {v \u2032 i : \u2225\u2225\u2225v\u2032i \u2212m(t)j \u2225\u2225\u22252 \u2264 \u2225\u2225\u2225v\u2032i \u2212m(t)l \u2225\u2225\u22252 \u2200l, 1 \u2264 l \u2264 k} (5)\nwhere each v\u2032i is assigned to exactly one S(t), even if it could be assigned to two or more of them.\nUpdate step: Calculate the new means to be the centroids of the observations in the new clusters.\nm (t+1) j = 1\u2223\u2223\u2223S(t)j \u2223\u2223\u2223 \u2211\nv\u2032i\u2208S (t) j\nv\u2032i (6)\nwhere \u2223\u2223\u2223S(t)j \u2223\u2223\u2223 denotes the number of elements in S(t)j .\nWith a few iterations as mentioned above, the patches could form different clusters. We use the center of clusters as semantic features."}, {"heading": "D. Structural Learning with Population Coding (Block 3 &", "text": "Block 4)\nIn the context of object recognition, spatial structure information is highly valuable but difficult to find a proper representation. Neuroscience researches [50] reveal that human brain processes this kind of information with a population of neurons. Taking the population of neurons related to orientation as an example, each neuron has a preferred direction; the closer to the preferred direction, the more a direction of stimulus will activate a certain neuron. The relationship between preferred stimulus and the rate of activation can be represented as a Gaussian-like curve. With many populations together, not only the input stimulus but also the uncertainty of the stimulus could be encoded by the rate of activations among the population of neurons.\nIn this paper, we define two kinds of structural features, (1) position features, the relative position of a component to the object center (2) relationship features, a relative structure which consists of spatial directions and distances between semantic components. The former feature captures the spatial positions of different semantic features in an input sample. The latter feature represent global concepts of how different features are organized together.\nInspired by the population coding mechanism in biological neural system, the structure is encoded by two population of neurons, one population encodes the positions whereas\n6 the other population encodes the relative relationship between different components. In this paper, the two populations of neurons are denoted as position neurons PNeurons and the relationship neurons RNeurons.\nFig. 4 and Fig. 5 give examples of PNeurons. A single neuron is like a gaussian filter (Fig. 4), in consistent with neuroscience researches. Each neuron has its own preferred position, the position that mostly activates the neuron. With multiple PNeurons together (e.g. 16 PNeurons as shown in Fig. 5), the population will output spatial representations of semantic features.\nThe activation process is almost the same with RNeurons\u2019 except RNeurons prefer orientations rather than positions. Fig. 6 shows the detailed process of how RNeurons are activated. Each neuron has a preferred direction that maximizes its activation. For a certain neuron, the activation responding to a direction is characterized as a Gaussian, depending on the difference between the input and the preferred direction.\nFrom the output of semantic layer (semantic feature maps), one can locate the position of each semantic feature via population of PNeurons. The tuning function of a PNeuron is a 2D Gaussian function centered in a certain position, which represents its probability of activation with different input position. With multiple PNeurons, each semantic feature map could be encoded as a new map about where the semantic\n(a) (b) (c)\n+ =\nfeatures are most likely located in the image. In this paper, for computational consideration, the center position of PNeurons are uniformly distributed over the map and the Gaussian functions are discretization with the same size of semantic feature maps. For a input feature map, the aggregated output of all PNeurons forms a matrix according to their center position, (denoted as position matrix or PM in the rest of paper).\nWith the semantic features and their positions (by sampling from the PM), the population of RNeurons will output the relative relationship between different features. For every two semantic features, the paper defines a relationship matrix to describe their position relationships. As is shown in Fig. 7 with 8 RNeurons, for an input direction, the output of all the RNeurons can be represented as a feature in the relationship matrix. The center node of the matrix is to encode the distance between the input position.\nThe output of relationship neurons is structural semantic information of input sample. As is shown above, structural information is distributed encoded by a population of neurons. Each neuron responds to two specific semantic features and one preferred direction. Thus, encoded structural features actually contain both semantic and structural information.\nThe structural concepts then can be learnt from one or multiple samples. In our model, the concept of one category\n7 is a distribution of position and relationship neurons for the category based on experiences. The sample distribution is used to approximated the prior distribution. The concepts will be further utilized in Block 6 to judge between possible candidates of recognition results.\nE. Integrated Recognition with Bayesian Learning (Block 5)\nPrior work has shown that perception can be interpreted as a Bayesian inference process from different pathways. Related model can predict human eye movements well in visual search tasks without any further assumptions or parameter tuning [45].\nIn this paper, the object recognition is considered as a Bayesian inference process based on models trained with different kinds of features. Firstly, recognition models, like softmax classifiers, are built based on different pathways, that is, different features, including episodic, semantic, and structural features. Each model outputs a vector of probabilities of all categories for an input sample. In training process, the correlations between features and categories are also learnt for feature-selection in Block 6.\nThe recognition results are then inferred from the output probabilities of these recognition models by Bayesian learning. For computational convenience, this paper assumes that recognition based on different features are independent. The detailed computation process is as follows.\nP (Oi|M1,M2, . . .) = P (M1,M2, . . . |Oi) P (Oi)\u2211 j P (M1,M2, . . . |Oj) P (Oj)\n(7)\nP (M1,M2, . . . |Oi)P (Oi) = P (Oi) \u220f k P (Mk|Oi) (8)\nOi is the category of a certain object, Mi are the output of recognition models based on different recognition features. For an object, the prior probabilities of each pathway are initialized as \u03b5n , \u03b5 is a relative small number and n is the total number of pathways. During training, the prior distribution is updated by the sample distribution.\nDuring recognition, post probabilities of potential categories are calculated and the category that maximizes the post probability is the output.\nOoutput = argmax i P (M1,M2, . . . |Oi) (9)\nIn short, by mimicking population coding and visual perception process, the proposed model can integrated different information extracted from original samples by Bayesian learning."}, {"heading": "F. Feature Re-selection (Block 6)", "text": "During recognition, human brain is not static but always adjusts and adapts dynamically to new stimuli. This paper especially focuses on the ambiguity of images. As the findings in visual systems suggest [51], for an ambiguous image which has multiple competitive candidates, human will pay more attention to the difference between the candidates. When a\nnew category of images appears, the brain tends to form a new concept, based on existing semantic memory [52]\nInspired by the principles mentioned above, a feature reselection strategy is applied to cope with ambiguous condition, in which the outputs of classification have more than one results with high confidence. The recognition process will then go backward to Block 4 to choose more distinctive structural features. For example, when the model cannot decide whether a handwritten digit is \"5\" or \"6\", it will go back to Block 4 to choose the \"horizon line\" and \"half circle\" with a vertical relationship and focus on these features to distinguish between \"5\" and \"6\". These relationships between categories and features are learnt in Block 3 and 4.\nThe recognition models based on spatial positions and structural relationships are trained in Block 5. The significance of different features is stored in the weights of those models. Given potential two candidate categories, the model will select features with more discriminative ability among two candidates, which is evaluated by the absolute differences of weights for the categories. If there are more than 2 candidates, the model averages the differences and use the mean as the significance of features. In this paper, to better utilize the features, we consider one block in the feature matrix (e.g. different positions of one semantic feature) as a whole. The corresponding weights of these features are summed up to get the total weight for the block. In re-selection process, the model automatically selects blocks with larger weights than average of all weights.\nWith the re-selected features, the judgment between \"5\" and \"6\" is achieved by comparing structural information on the current sample and learnt concepts. The above mentioned concepts are the distribution of positions of semantic features and structural relationships. As shown in Fig. 8(a), structural concept of digit \"6\" is represented in a relationship matrix, which is generated by averaging all relationship matrices of category \"6\". It reveals how likely the neurons are activated when the input is \"6\". Fig. 8(b) shows the selected significant features of \"6\" to distinguish \"6\" from \"5\". When an input image is ambiguous, only the activations of discriminative features would be feed to the classification models after reselection.\n8 By applying the feature re-selection strategy, the proposed model achieves the ability of generalization and adaption to ambiguous input stimuli."}, {"heading": "IV. EXPERIMENT", "text": "Several experiments are conducted to verify the effectiveness of the proposed biologically inspired model, and each module is tested and analyzed in details. The experiments are focused on three aspects: 1) visualize the episodic and semantic features that are extracted by the proposed model; 2) investigate the structure information learned by the proposed model; 3) evaluate the classification performance on different datasets.\nA. Extraction of Episodic Features\nThis experiment is to visualize the extracted episodic features and to verify that these features can capture the critical information of the original image. Here, MNIST dataset is used as an example. The visualizations of the learnt weights of CDBN are given in Fig. 9, which corresponds to the episodic features in our model. Here, two visualization techniques are used, including the deconvolution method and the average of max activations used in [41].\nAs is shown in Fig. 9, the proposed deconvolution method could achieve clearer edges and parts than the method in [41]. Furthermore, it is clear that the CRBM model can extract episodic features hierarchically from the original dataset. In details, features learnt by the first layer of the CRBM model are mostly edges and small details of the input digits, whereas the second layer of the CDBN model extracts more sophisticated components like circles and turning strokes. So it is reasonable to use the outputs of second layer to learn semantic features. From Fig. 9, one may find out that some features are highly similar. One possible reason is that those features are\ntrained without supervision, so some of them may more likely be attracted to the most significant features at the same time.\nTo further verify that CDBN can extract and learn critical information of image, experiments on reconstruction from the episodic features are conducted. Some examples are given in Fig. 10, which illustrates the high similarity between the original image and its reconstruction.\nIn addition, Fig. 10 illustrates how the learned weights encode episodic features. Original images can be directly reconstructed from high-level features, which indicates that most of the detailed information is captured by the second layer."}, {"heading": "B. Semantic Features Extraction and Structure Learning", "text": "This experiment is conducted to show the abstraction process from episodic features to structural semantic outputs.\nSemantic features are clustered from extracted episodic features, as visualized in Fig. 11. The number of clusters is set as 8. In Fig. 11, different semantic features are less similar with each other, which enhances the variety of features and captures more information with less features.\nAfter extracting semantic features, we then calculate the activations of PNeurons, by applying position tuning functions on the features. Here, a position tuning function is a 2D Gaussian function, with different mean but the same covariance matrix. For computational convenience, we use the discretized version of it, as illustrated in Fig. 12. For each feature, there are 16 PNeurons, which form a 4\u00d7 4 position matrix.\nAn example of position matrix is shown in Fig. 12, which is similar with the mixture of several Gaussian distributions.\nVisualization of structural outputs is generated from the structural relationship matrix, which encodes the distributions and relative spatial relationships between features.\nFig. 13 illustrates an example of structural relationship matrix, which is randomly selected from 100 training samples of MNIST dataset. Each small square includes eight direction neurons (surrounding) and one distance neuron (center).\n9"}, {"heading": "C. Feature Re-selection Experiment", "text": "The learnt semantic features and structures are not static during the testing process. Here we show how the features are re-selected to deal with the ambiguity and unfamiliarity.\nWhen the input is ambiguous, it is easy to output as multiple candidates, the model could re-select features to achieve accurate classification.\nTo illustrate the process in a better way, Fig. 14 shows examples of ambiguous images, whereas the input images are misclassified as \"6\" by a convolutional neural network. These ambiguous images are generated by the method proposed in [21], so-called \"adversarial images\". That is, optimizing and modifying the original image, such as the one labeled as \"7\", to be misclassified as \"6\" by a convolutional neural network. By applying the back-propagation to the input space and limiting the martingale of gradients, we are able to generate tiny perturbations to the original images which could mislead the model. Images with perturbation are originally designed for a convolutional neural network, but it could also affect the\nrecognition by a CDBN, which is consistent with the results in [21].\nFollowing the strategy in Section III-F, the significance of different features is learnt from training dataset. Fig. 15 illustrates the learnt position matrix and structural relationships of digits \"5\" and \"6\". As is shown in Fig. 15, although \"5\" and \"6\" activate similar semantic features, the position and structural relationships between the features are quite different. Hence, by evaluating the differences of the position and structural relationship, we could find more distinctive features to build a new classifier, which is specific to separate \"5\" and \"6\". Fig. 16 shows the chosen features after the reselection to distinguish \"5\" from \"6\"."}, {"heading": "D. Classification Performance", "text": "1) MNIST with small training set: The first comparison experiment is conducted between our model and some bi-\n10\nologically inspired models (such as traditional CDBN [41], HMAX [1], and etc.) on MNIST dataset. In total, MNIST includes 60000 hand-writing digit images for training and 10000 for testing. In this experiment, small training set is randomly and uniformly chosen from MNIST training data for ten categories. The code of HMAX model is obtained from the author\u2019s website. The traditional CDBN model has the same configuration and structure of our model, but without semantic features and structural information.\nIn this section, we use a two-layer CDBN with 40 feature maps in the first layer and 40 feature maps in the second layer. The pooling size is 2 in both layers. The outputs of second layer are episodic features. From these episodic features, 8 semantic features are extracted and then processed by the position tuning functions and structural relationship neurons. Two types of PNeurons are used with size of 16 (4 \u00d7 4) and 64 (8 \u00d7 8). For each pair of semantic features, we use 9 RNeurons (8 for direction, 1 for distance).\nThe results are shown in Table 1, performance of the proposed BSNN is better than HMAX and traditional CDBN. The main reason is because the semantic features are more discriminative even with a small number and integrated . Moreover, the performance is further improved by introducing the position neurons.\n2) Ambiguous Images from MNIST: The ambiguous data set is generated by adding a relative small perturbation to the original MNIST data sets [53]. Details of the method are described in IV-C. Note that all the networks and classifiers are trained on the original MNIST data set.\nTable 2 is the classification error rate of different models on the ambiguous images. Compared with the results in Table 1, for HMAX and traditional CDBN, performances on ambiguous images are worse than those on the original images. After feature re-selection, the performance of BSNN has increased because the selected features are more discriminative than before.\n3) Facial shape dataset: The third comparison experiments are conducted on a facial shape dataset. The artificial face is composed of key components with different shapes. Some examples are shown in Fig. 17. Compared to hand-written digits in MNIST, the facial shape dataset has more stable global structure, but also more scale- and shape-variant properties together with local transformations.\nMore specifically, the dataset consists of 5 classes of faces. As is shown in Fig. 17, each face has four key components, including one mouth, one nose and two eyes. The main\ndifferences between classes are the different shapes of the components, where locations and scales of the components are also distributed in a wide range. Fig. 18 shows some examples in one class.\nConducting experiments as mentioned above, we compare the performance between HMAX, traditional CDBN and our model, with training data of different sizes (30, 100, 300 samples). The results are shown in Table 3, which illustrate that the proposed model can successfully learn discriminative features even with a very small dataset."}, {"heading": "V. CONCLUSION", "text": "In this paper, a novel biologically inspired model is proposed for robust visual recognition, mimicking the visual processing system in human brain. By introducing semantics and\n11\nstructural conceptual outputs to the traditional CDBN network, the model gains more ability of generalization, especially for a small training dataset. The procedure of feature re-selection provides the model more robustness to ambiguity. During the cognition process, when ambiguity is detected during recognition process, new features according to the difference between ambiguous candidates are re-selected online for later cognition.\nIn the future, the proposed model will be further improved by extracting spatiotemporal semantics and concepts for sequential analysis, which is more similar to human neural system. Another approach to enhance the model is to further introduce the biological mechanisms in higher level perception and inference. A more flexible and robust classifier, such as the function of prefrontal cortex in human, is useful to process different outputs in an integrated manner."}], "references": [{"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nat. Neuroscience, vol. 2, no. 11, pp. 1019\u20131025, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Primal sketch: Integrating structure and texture", "author": ["C.-e. Guo", "S.-C. Zhu", "Y.N. Wu"], "venue": "Computer Vision and Image Understanding, vol. 106, no. 1, pp. 5\u201319, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Top-down feedback in an hmax-like cortical model of object perception based on hierarchical bayesian networks and belief propagation", "author": ["S. Dura-Bernal", "T. Wennekers", "S.L. Denham"], "venue": "PloS one, vol. 7, no. 11, p. e48216, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Realistic modeling of simple and complex cell tuning in the hmax model, and implications for invariant object recognition in cortex", "author": ["T. Serre", "M. Riesenhuber"], "venue": "DTIC Document, Tech. Rep., 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Automated mitosis detection using texture, sift features and hmax biologically inspired approach", "author": ["H. Irshad", "S. Jalali", "L. Roux", "D. Racoceanu", "L.J. Hwee", "G. Le Naour", "F. Capron"], "venue": "J. Pathology informatics, vol. 4, no. Suppl, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Extended coding and pooling in the hmax model", "author": ["C. Theriault", "N. Thome", "M. Cord"], "venue": "IEEE Trans. Image Process., vol. 22, no. 2, pp. 764\u2013777, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparsity-regularized hmax for visual recognition", "author": ["X. Hu", "J. Zhang", "J. Li", "B. Zhang"], "venue": "PloS one, vol. 9, no. 1, p. e81813, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1813}, {"title": "Enhanced hmax model with feedforward feature learning for multiclass categorization", "author": ["Y. Li", "W. Wu", "B. Zhang", "F. Li"], "venue": "Frontiers in computational neuroscience, vol. 9, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Hmax model: A survey", "author": ["C. Liu", "F. Sun"], "venue": "Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1\u20137.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Biologically plausible saliency mechanisms improve feedforward object recognition", "author": ["S. Han", "N. Vasconcelos"], "venue": "Vision research, vol. 50, no. 22, pp. 2295\u20132307, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical model for object recognition based on natural-stimuli adapted filters", "author": ["P. Mishra", "B.K. Jenkins"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 950\u2013953.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., no. 11, pp. 1254\u20131259, 1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "A neural model combining attentional orienting to object recognition: Preliminary explorations on the interplay between where and what", "author": ["F. Miau", "L. Itti"], "venue": "Proc. of the 23rd Annu. International Conference of the IEEE, vol. 1. IEEE, 2001, pp. 789\u2013792.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Attentional selection for object recognition: gentle way", "author": ["D. Walther", "L. Itti", "M. Riesenhuber", "T. Poggio", "C. Koch"], "venue": "Biologically Motivated Computer Vision. Springer, 2002, pp. 472\u2013479.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "What and where: A bayesian inference theory of attention", "author": ["S. Chikkerur", "T. Serre", "C. Tan", "T. Poggio"], "venue": "Vision research, vol. 50, no. 22, pp. 2233\u20132247, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "What/where to look next? modeling top-down visual attention in complex interactive environments", "author": ["A. Borji", "D.N. Sihite", "L. Itti"], "venue": "IEEE Trans. Syst., Man, Cybern., A, vol. 44, no. 5, pp. 523\u2013538, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Introducing memory and association mechanism into a biologically inspired visual model", "author": ["H. Qiao", "Y. Li", "T. Tang", "P. Wang"], "venue": "IEEE Trans. Cybern., vol. 44, no. 9, pp. 1485\u20131496, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Biologically inspired visual model with preliminary cognition and active attention adjustment", "author": ["H. Qiao", "X. Xi", "Y. Li", "W. Wu", "F. Li"], "venue": "IEEE Trans. Cybern., vol. 45, no. 11, pp. 2612\u20132624, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "arXiv preprint arXiv:1412.1897, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1897}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural mechanisms of selective visual attention", "author": ["R. Desimone", "J. Duncan"], "venue": "Annu. review of neuroscience, vol. 18, no. 1, pp. 193\u2013222, 1995.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "A biologically inspired system for action recognition", "author": ["H. Jhuang", "T. Serre", "L. Wolf", "T. Poggio"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on. Ieee, 2007, pp. 1\u20138.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "The Representation of Meaning in Memory (PLE: Memory)", "author": ["W. Kintsch"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Where do you know what you know? the representation of semantic knowledge in the human brain", "author": ["K. Patterson", "P.J. Nestor", "T.T. Rogers"], "venue": "Nature Reviews Neuroscience, vol. 8, no. 12, pp. 976\u2013987, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science, vol. 350, no. 6266, pp. 1332\u20131338, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "How many memory systems are there?", "author": ["E. Tulving"], "venue": "American psychologist,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1985}, {"title": "Functional neuroanatomy of remote episodic, semantic and spatial memory: a unified account based on multiple trace theory", "author": ["M. Moscovitch", "R.S. Rosenbaum", "A. Gilboa", "D.R. Addis", "R. Westmacott", "C. Grady", "M.P. McAndrews", "B. Levine", "S. Black", "G. Winocur"], "venue": "J. Anatomy, vol. 207, no. 1, pp. 35\u201366, 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural mechanisms supporting the extraction of general knowledge across episodic memories", "author": ["C.C. Sweegers", "A. Takashima", "G. Fern\u00e1ndez", "L.M. Talamini"], "venue": "Neuroimage, vol. 87, pp. 138\u2013146, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Coherent theta oscillations and reorganization of spike timing in the hippocampal-prefrontal network upon learning", "author": ["K. Benchenane", "A. Peyrache", "M. Khamassi", "P.L. Tierney", "Y. Gioanni", "F.P. Battaglia", "S.I. Wiener"], "venue": "Neuron, vol. 66, no. 6, pp. 921\u2013936, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Tracking the emergence of conceptual knowledge during human decision making", "author": ["D. Kumaran", "J.J. Summerfield", "D. Hassabis", "E.A. Maguire"], "venue": "Neuron, vol. 63, no. 6, pp. 889\u2013901, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Persistent schema-dependent hippocampal-neocortical connectivity during memory encoding and postencoding rest in humans", "author": ["M.T. van Kesteren", "G. Fern\u00e1ndez", "D.G. Norris", "E.J. Hermans"], "venue": "Proc. Natl. Acad. Sci. U.S.A., vol. 107, no. 16, pp. 7550\u20137555, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Recognition-by-components: a theory of human image understanding.", "author": ["I. Biederman"], "venue": "Psychological review,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1987}, {"title": "Representation and recognition of the spatial organization of three-dimensional shapes", "author": ["D. Marr", "H.K. Nishihara"], "venue": "Proc. the Royal Society of London B: Biological Sciences, vol. 200, no. 1140, pp. 269\u2013 294, 1978.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1978}, {"title": "Shape representation in area v4: position-specific tuning for boundary conformation", "author": ["A. Pasupathy", "C.E. Connor"], "venue": "J. neurophysiology, vol. 86, no. 5, pp. 2505\u20132519, 2001.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Population coding of shape in area v4", "author": ["\u2014\u2014"], "venue": "Nature neuroscience, vol. 5, no. 12, pp. 1332\u20131338, 2002.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "The attention system of the human brain: 20 years after", "author": ["S.E. Petersen", "M.I. Posner"], "venue": "Annu. review of neuroscience, vol. 35, p. 73, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Using genetic data in cognitive neuroscience: from growing pains to genuine insights", "author": ["A.E. Green", "M.R. Munaf\u00f2", "C.G. DeYoung", "J.A. Fossella", "J. Fan", "J.R. Gray"], "venue": "Nature Reviews Neuroscience, vol. 9, no. 9, pp. 710\u2013720, 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "The native coordinate system of spatial attention is retinotopic", "author": ["J.D. Golomb", "M.M. Chun", "J.A. Mazer"], "venue": "J. Neuroscience, vol. 28, no. 42, pp. 10 654\u201310 662, 2008.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Episodic memory: From mind to brain", "author": ["E. Tulving"], "venue": "Annu. review of psychology, vol. 53, no. 1, pp. 1\u201325, 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Proc. the 26th Annu. Int. Conference on Mach. Learning. ACM, 2009, pp. 609\u2013616.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Biologically inspired model for visual cognition achieving unsupervised episodic and semantic feature learning", "author": ["H. Qiao", "Y. Li", "F. Li", "W. Wu"], "venue": "IEEE Trans. Cybern., 2015.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.  12", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2002}, {"title": "Is semantics still possible?", "author": ["J. Berg"], "venue": "J. pragmatics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2002}, {"title": "How comparative is semantics? a unified parametrictheory of bare nouns and proper names", "author": ["G. Longobardi"], "venue": "Natural language semantics, vol. 9, no. 4, pp. 335\u2013369, 2001.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive science, vol. 34, no. 8, pp. 1388\u20131429, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "The neurobiology of semantic memory", "author": ["J.R. Binder", "R.H. Desai"], "venue": "Trends in cognitive sciences, vol. 15, no. 11, pp. 527\u2013536, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining answer set programming with description logics for the semantic web", "author": ["T. Eiter", "G. Ianni", "T. Lukasiewicz", "R. Schindlauer", "H. Tompits"], "venue": "Artificial Intelli., vol. 172, no. 12, pp. 1495\u20131539, 2008.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning the semantics of object\u2013action relations by observation", "author": ["E.E. Aksoy", "A. Abramov", "J. D\u00f6rr", "K. Ning", "B. Dellen", "F. W\u00f6rg\u00f6tter"], "venue": "The International J. of Robotics Research, p. 0278364911410459, 2011.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Top-down activation of shape-specific population codes in visual cortex during mental imagery", "author": ["M. Stokes", "R. Thompson", "R. Cusack", "J. Duncan"], "venue": "The J. of Neuroscience, vol. 29, no. 5, pp. 1565\u20131572, 2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "How recent experience affects the perception of ambiguous objects", "author": ["V. Daelli", "N.J. van Rijsbergen", "A. Treves"], "venue": "Brain research, vol. 1322, pp. 81\u201391, 2010.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "The parallel distributed processing approach to semantic cognition", "author": ["J.L. McClelland", "T.T. Rogers"], "venue": "Nature Reviews Neuroscience, vol. 4, no. 4, pp. 310\u2013322, 2003.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2003}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "Learning Representation, 2014. ICLR 2014. International Conference on, 2014, pp. 1\u20138.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "HMAX model tries to mimic the functions of primate visual system layer by layer [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "The main difference between HMAX and other hierarchical architectures (such as hand-crafted hierarchical features [2], convolutional neural networks [3], and etc.", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "The main difference between HMAX and other hierarchical architectures (such as hand-crafted hierarchical features [2], convolutional neural networks [3], and etc.", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": ") is that it focused on reproducing anatomical, physiological and psychophysical properties of the ventral pathway of visual system [4], which consists of V 1, V 2, V 4 and inferior temporal (IT) cortical areas.", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "After its first publication in 1999, this well-known model has been further developed and improved in different aspects [5]\u2013[8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "After its first publication in 1999, this well-known model has been further developed and improved in different aspects [5]\u2013[8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "For example, many researchers modified the original HMAX model by adding feedback process to improve the recognition precision [9], [10].", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "For example, many researchers modified the original HMAX model by adding feedback process to improve the recognition precision [9], [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "Other modifications include adding sparsity to the convolutional layer [8], enhancing the architecture by adding specific layers to the model [7], [11], and changing strategies of feature selection and filtering properties [12].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Other modifications include adding sparsity to the convolutional layer [8], enhancing the architecture by adding specific layers to the model [7], [11], and changing strategies of feature selection and filtering properties [12].", "startOffset": 142, "endOffset": 145}, {"referenceID": 10, "context": "Other modifications include adding sparsity to the convolutional layer [8], enhancing the architecture by adding specific layers to the model [7], [11], and changing strategies of feature selection and filtering properties [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "Other modifications include adding sparsity to the convolutional layer [8], enhancing the architecture by adding specific layers to the model [7], [11], and changing strategies of feature selection and filtering properties [12].", "startOffset": 223, "endOffset": 227}, {"referenceID": 12, "context": "Itti proposed a saliency-based model based on the saliency map theory in human visual system [13] and combined attention with object recognition [14], [15].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Itti proposed a saliency-based model based on the saliency map theory in human visual system [13] and combined attention with object recognition [14], [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "Itti proposed a saliency-based model based on the saliency map theory in human visual system [13] and combined attention with object recognition [14], [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "It has been implement by Bayesian inference [16] and saliency model [17].", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "It has been implement by Bayesian inference [16] and saliency model [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "developed a series of models introducing association [18], attention [19] to the model.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "developed a series of models introducing association [18], attention [19] to the model.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "However, recent findings [20], [21] point out that even the state-of-art deep hierarchical networks suffer from tiny disturbance and transformation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 20, "context": "However, recent findings [20], [21] point out that even the state-of-art deep hierarchical networks suffer from tiny disturbance and transformation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "It is shown that tiny perturbation may cause significant difference in the output of hierarchical network models [21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "According to biological findings, objection recognition tasks involve multiple cortices and many sophisticated mechanisms including preliminary cognition, top-down attention [22], semantic and conceptual memory [23]\u2013[25].", "startOffset": 174, "endOffset": 178}, {"referenceID": 22, "context": "According to biological findings, objection recognition tasks involve multiple cortices and many sophisticated mechanisms including preliminary cognition, top-down attention [22], semantic and conceptual memory [23]\u2013[25].", "startOffset": 211, "endOffset": 215}, {"referenceID": 24, "context": "According to biological findings, objection recognition tasks involve multiple cortices and many sophisticated mechanisms including preliminary cognition, top-down attention [22], semantic and conceptual memory [23]\u2013[25].", "startOffset": 216, "endOffset": 220}, {"referenceID": 25, "context": "Lake, Salakhutdinov and Tenenbaum recently [26] employs semantics and concepts explicitly and achieves significant improvement in robustness of one-shot character recognition.", "startOffset": 43, "endOffset": 47}, {"referenceID": 24, "context": "Two different types of memory are stored in the brain: episodic memory and semantic memory [25], [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "Two different types of memory are stored in the brain: episodic memory and semantic memory [25], [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "Episodic memory stores events and detailed contextual information, while semantic memory extracts regularities from different spatial-temporal events and forms perceptual categories, complex concepts and relations [27].", "startOffset": 214, "endOffset": 218}, {"referenceID": 27, "context": "This requires that extraction of regularities or semantics should be carried out over episodes [28], [29].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "This requires that extraction of regularities or semantics should be carried out over episodes [28], [29].", "startOffset": 101, "endOffset": 105}, {"referenceID": 29, "context": "Since hippocampus is involved with storage of episodic memory and prefrontal cortex contributes to organization of information, the extraction process could be achieved via hippocampus and mPFC (medial prefrontal cortex) interaction [30]\u2013[32].", "startOffset": 233, "endOffset": 237}, {"referenceID": 31, "context": "Since hippocampus is involved with storage of episodic memory and prefrontal cortex contributes to organization of information, the extraction process could be achieved via hippocampus and mPFC (medial prefrontal cortex) interaction [30]\u2013[32].", "startOffset": 238, "endOffset": 242}, {"referenceID": 32, "context": "It has been proposed that objects could be described with parts and their positional and connectional relationships [33], [34].", "startOffset": 116, "endOffset": 120}, {"referenceID": 33, "context": "It has been proposed that objects could be described with parts and their positional and connectional relationships [33], [34].", "startOffset": 122, "endOffset": 126}, {"referenceID": 34, "context": "For example, neurons in V4 are tuned for contour fragment orientation with specific object-relative position [35].", "startOffset": 109, "endOffset": 113}, {"referenceID": 35, "context": "Thus, in V4 area, neurons respond to individual contour fragments and their relationships are encoded in population responses [36].", "startOffset": 126, "endOffset": 130}, {"referenceID": 34, "context": "In PIT (posterior inferior temporal cortex), neurons integrate information on multiple fragments [35].", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "Attention is required when people carry out various tasks, since relevant environmental stimuli and information should be selected and processed in the brain [22], [37].", "startOffset": 158, "endOffset": 162}, {"referenceID": 36, "context": "Attention is required when people carry out various tasks, since relevant environmental stimuli and information should be selected and processed in the brain [22], [37].", "startOffset": 164, "endOffset": 168}, {"referenceID": 37, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "When visual stimuli is not clear for the task, visual attention process could suppress distraction from location of previous attention focus and find new positions for the search of related information [39].", "startOffset": 202, "endOffset": 206}, {"referenceID": 26, "context": "In human, episodic memory is the memory that represents experiences and specific events in time, from which people can reconstruct the actual events that took place at any given time [27].", "startOffset": 183, "endOffset": 187}, {"referenceID": 39, "context": "Episodic memory is one of the basic forms of explicit memory and considered as the source of other forms of memory [40].", "startOffset": 115, "endOffset": 119}, {"referenceID": 40, "context": "Unsupervised convolutional deep belief network (CDBN) is first introduced in Ng\u2019s work [41] for feature extraction tasks.", "startOffset": 87, "endOffset": 91}, {"referenceID": 41, "context": "In our previous work [42], CDBN has been used to extract episodic information from the image.", "startOffset": 21, "endOffset": 25}, {"referenceID": 42, "context": "The CRBM can be trained with Contrastive Divergence (CD), which is an approximate Maximum-Likelihood learning algorithm [43].", "startOffset": 120, "endOffset": 124}, {"referenceID": 43, "context": "Semantics has multiple definitions in different fields, such as linguistics [44], [45], cognitive science [46], [47], artificial intelligence [48], [49] and etc.", "startOffset": 76, "endOffset": 80}, {"referenceID": 44, "context": "Semantics has multiple definitions in different fields, such as linguistics [44], [45], cognitive science [46], [47], artificial intelligence [48], [49] and etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 45, "context": "Semantics has multiple definitions in different fields, such as linguistics [44], [45], cognitive science [46], [47], artificial intelligence [48], [49] and etc.", "startOffset": 106, "endOffset": 110}, {"referenceID": 46, "context": "Semantics has multiple definitions in different fields, such as linguistics [44], [45], cognitive science [46], [47], artificial intelligence [48], [49] and etc.", "startOffset": 112, "endOffset": 116}, {"referenceID": 47, "context": "Semantics has multiple definitions in different fields, such as linguistics [44], [45], cognitive science [46], [47], artificial intelligence [48], [49] and etc.", "startOffset": 142, "endOffset": 146}, {"referenceID": 48, "context": "Semantics has multiple definitions in different fields, such as linguistics [44], [45], cognitive science [46], [47], artificial intelligence [48], [49] and etc.", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "In cognitive science, semantic memory is about facts that capture the internal properties about an object [27], [47].", "startOffset": 106, "endOffset": 110}, {"referenceID": 46, "context": "In cognitive science, semantic memory is about facts that capture the internal properties about an object [27], [47].", "startOffset": 112, "endOffset": 116}, {"referenceID": 46, "context": "Binder and Desai [47] proposed that modality-specific semantic memory is encoded in the corresponding cortex.", "startOffset": 17, "endOffset": 21}, {"referenceID": 49, "context": "Neuroscience researches [50] reveal that human brain processes this kind of information with a population of neurons.", "startOffset": 24, "endOffset": 28}, {"referenceID": 44, "context": "Related model can predict human eye movements well in visual search tasks without any further assumptions or parameter tuning [45].", "startOffset": 126, "endOffset": 130}, {"referenceID": 50, "context": "As the findings in visual systems suggest [51], for an ambiguous image which has multiple competitive candidates, human will pay more attention to the difference between the candidates.", "startOffset": 42, "endOffset": 46}, {"referenceID": 51, "context": "When a new category of images appears, the brain tends to form a new concept, based on existing semantic memory [52]", "startOffset": 112, "endOffset": 116}, {"referenceID": 40, "context": "Here, two visualization techniques are used, including the deconvolution method and the average of max activations used in [41].", "startOffset": 123, "endOffset": 127}, {"referenceID": 40, "context": "9, the proposed deconvolution method could achieve clearer edges and parts than the method in [41].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "These ambiguous images are generated by the method proposed in [21], so-called \"adversarial images\".", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "Images with perturbation are originally designed for a convolutional neural network, but it could also affect the recognition by a CDBN, which is consistent with the results in [21].", "startOffset": 177, "endOffset": 181}, {"referenceID": 40, "context": "ologically inspired models (such as traditional CDBN [41], HMAX [1], and etc.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "ologically inspired models (such as traditional CDBN [41], HMAX [1], and etc.", "startOffset": 64, "endOffset": 67}, {"referenceID": 52, "context": "2) Ambiguous Images from MNIST: The ambiguous data set is generated by adding a relative small perturbation to the original MNIST data sets [53].", "startOffset": 140, "endOffset": 144}], "year": 2016, "abstractText": "Integration between biology and information science benefits both fields. Many related models have been proposed, such as computational visual cognition models, computational motor control models, integrations of both and so on. In general, the robustness and precision of recognition is one of the key problems for object recognition models. In this paper, inspired by features of human recognition process and their biological mechanisms, a new integrated and dynamic framework is proposed to mimic the semantic extraction, concept formation and feature re-selection in human visual processing. The main contributions of the proposed model are as follows: (1) Semantic feature extraction: Local semantic features are learnt from episodic features that are extracted from raw images through a deep neural network; (2) Integrated concept formation: Concepts are formed with local semantic information and structural information learnt through network. (3) Feature re-selection: When ambiguity is detected during recognition process, distinctive features according to the difference between ambiguous candidates are re-selected for recognition. Experimental results on hand-written digits and facial shape dataset show that, compared with other methods, the new proposed model exhibits higher robustness and precision for visual recognition, especially in the condition when input samples are smantic ambiguous. Meanwhile, the introduced biological mechanisms further strengthen the interaction between neuroscience and information science.", "creator": "LaTeX with hyperref package"}}}