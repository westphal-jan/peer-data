{"id": "1603.07893", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction", "abstract": "We explore the kaiser-wilhelm effectiveness webos of 27-24 using Long Short Term c-119s Memory tavoy Networks (LSTM) for gust predicting sheens stock lgonzalez prices. subscale We will pro-southern construct, test 109.7 and compare a miren range kokang of gynoid architectures lieutenancies of 135.00 LSTMs compactors trained cyb\u00e8le via smestow backpropagation through electronegativity time (BPTT ).", "histories": [["v1", "Fri, 25 Mar 2016 12:28:02 GMT  (5kb)", "http://arxiv.org/abs/1603.07893v1", "8 pages"], ["v2", "Thu, 31 Mar 2016 11:28:45 GMT  (5kb)", "http://arxiv.org/abs/1603.07893v2", "7 pages"], ["v3", "Sun, 28 Aug 2016 09:56:23 GMT  (4kb)", "http://arxiv.org/abs/1603.07893v3", "6 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["hengjian jia"], "accepted": false, "id": "1603.07893"}, "pdf": {"name": "1603.07893.pdf", "metadata": {"source": "CRF", "title": "Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction", "authors": ["Hengjian Jia"], "emails": ["henryjia18@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n07 89\n3v 1\n[ cs\n.N E\n] 2\n5 M\nar 2\n01 6\nInvestigation Into The Effectiveness Of\nLong Short Term Memory Networks For\nStock Price Prediction\nHengjian Jia\nColyton Grammar School\nE-mail: henryjia18@gmail.com"}, {"heading": "1 Acknowledgements", "text": "I would like to thank Alfie Howard for providing me with the necessary code to preprocess data. I would also like to thank Franc\u0327ois Chollet for creating the Keras framework which was used to create all the necessary code for this paper."}, {"heading": "2 Abstract", "text": "We explore the effectiveness of using Long Short Term Memory Networks (LSTM) for predicting stock prices. We will construct, test and compare a range of architectures of LSTMs trained via backpropagation through time (BPTT)."}, {"heading": "3 Introduction", "text": "Stock prices are a form of time series data. There have been many existing business and economics based methods for predicting stock prices. These methods can be classed as fundamental and technical analysis. Technical analysis is based on the observing patterns in stock prices based on psychological effects (fear and greed) changing supply and demand. Fundamental analysis is based on observing current news and events such as the corporate profits and economic situation. However, all of these models ultimately rely on human judgement of the situation to make predictions and are not learned [1].\nThere are also other learning algorithms which have been used to takle the problem of predicting stock prices. These include using deep multilayer perceptrons [2] and convolutional neural networks [3]. However, these methods have limited capability for temporal memory which can be provided through a fixed sized sliding window for predicting future stock prices as a function of historical prices.\nOn the other hand, recurrent neural networks have a cycle feeds activations from the previous time step back in as an input and influences the activations of the current time step. Therefore the activations create an internal state. This in theory can store temporal information for a dynamic indefinite number of time steps in contrast to the fixed number of time steps of feed forward networks. LSTMs are a specific type of recurrent neural network which overcomes some of the problems of recurrent networks.[5]."}, {"heading": "4 Long Short Term Memory", "text": "LSTM hidden layers are made up of special cells with sigmoidal input, output and forget gates. This allows the network to learn when to forget, take input and output. The LSTM cell has an internal state which is updated based on the previous activations of the layer and inputs through connections to the previous layer and self connections[4].\nA layer of LSTM cells takes a sequence of vectors as input x = (x1, x2, x3, . . . , xT ) and outputs a sequence of vectors y = (y1, y2, y3, . . . , yT ). The output vectors are calculated by iterating through the following equations from t = 1 to T :\nct = g(Wcxxt +Wcyyt\u22121 + bc)\nit = \u03c3(Wixxt +Wiyyt\u22121 + bi)\nft = \u03c3(Wfxxt +Wfyyt\u22121 + bf )\not = \u03c3(Woxxt +Woyyt\u22121 + bo)\nSt = it \u2299 ct + ft \u2299 St\u22121\nyt = ot \u2299 \u03c6(St)\nWe define \u03c3(x) as a hard sigmoid function which can output 0 and 1. This means that the gates can fully close or open.\n\u03c3(x) =\n\n \n  0 x \u2264 \u22122.5 0.2x+ 0.5 \u22122.5 \u2264 x \u2264 2.5 1 2.5 \u2265 x\nAnd\n\u03c6(x) = g(x) = tanh(x)\n5 Network Topology\nWe will first use LSTM Layers as a set of learned feature generators. This will then allow us to stack a dense network on top of the LSTM layers to accumulate the outputs of the LSTM layers into the predictions of the network. As the LSTM layers are already nonlinear, only one dense layer is needed to accumulate their output. However, we varied the number of LSTM layers and the number of cells within them to experiment and see which archticture would perform the best."}, {"heading": "6 Weight Initialisations", "text": "We initialised our feed forward weights by sampling from the uniform distribution [6]\nW \u223c U\n[ \u2212 \u221a 6\n\u221a nin + nout ,\n\u221a 6\n\u221a nin + nout\n]\nWe initialised our recurrent weights by first sampling from Gaussian distribution with 0 mean and unit variance. Then we performed singular vale decomposition"}, {"heading": "W = USV T", "text": "Which gives us 2 random orthonormal matrices U and V , so we set\nW := U\nThis gives us recurrent weight matrices with unit maximal eigenvalue. As a result, the internal state does not explode as multiplying by the weight matrices repeatedly does not increase the Euclidean norm of the internal state vector.\nFinally, we initialised the forget bias units to 1 and the other biases to 0."}, {"heading": "7 Hyperparameters, Feature Selection and Tar-", "text": "gets\nIn our experiments, we set the learning rate to 0.01 and trained our network for 200 epochs.\nWe used Google\u2019s daily stock prices from January 1st 2005 to December 31st 2014 to create our training set and from January 1st 2015 to June 21st 2015 to form our test set. Th data was optained from Yahoo finance. We only used the open, high, low, close and volume as features for our inputs. This is because any other technical indicators used in technical analysis are simply calculated from those. Therefore our network should learn any necessary patterns that are represented by the indicators.\nWe also normalised our data to aid training by converting it into returns through a percentage change calculation.\nx\u0302t = xt\nxt\u22121 \u2212 1\nThis scaled the values to make them all small but also transforms the time series to become bounded. This is due to the fact that stocks move very little from day to day. This allows our network to learn a pattern easier."}, {"heading": "8 Training Methodology", "text": "Although LSTMs have mostly solved the vanishing/exploding gradient problem, they are still inadequate for sequences of such great lengths. Therefore we trained our network using the algorithm described as follows.\nAlgorithm 1 LSTM Pretraining Algorithm\ni \u2190 0 m \u2190 sequence length for 2i < m do\nReset network\u2019s internal state Truncate sequence using a sliding window of length 2i Train network on truncated sequence i \u2190 i+ 1\nend for Train sequence on whole dataset Reset network\u2019s internal state Test network on test sequence\nHowever, we did not need the sequence length for training to be as long as the data goes back in time. So we truncated to a length of 256 using sliding windows as this was a little more than the number of trading days in one year. It is unlikely that data any further back than one year will affect future data.\nWe trained one epoch on each truncated sequence and then 100 epochs on the 256 days sequence. The batch size was kept constant at 20 and the Adam optimiser was used with 0.001 learning rate and default paramters [7].\n9 Experiments"}, {"heading": "10 Discussion", "text": "As shown in Table 1, the returns data is generally resistant to overfitting. The network generally performs better when made deeper and wider with a few exceptions even when the number of parameters in the network are far greater than then number of unique training examples.\nAlthough to know in absolute terms whether the network performed, we compare the RMSE of the networks to the RMS of the returns itself. If the RMS of the returns itself is greater, then it would imply that the network is performing better than an algorithm that simply predicts no change and vice versa. The RMS of the test data is 0.0265 which shows that the networks are all performing nearly or more than twice as successfully as the benchmark.\nTherefore from this we can conclude that LSTM networks trained with accelerated first order methods are an effective method of predicting stock returns. We also conclude that returns data and LSTM networks are generally resistant to overfitting and larger networks perform better."}], "references": [{"title": "ANN Model to Predict Stock Prices at Stock Exchange Markets", "author": ["Wanjawa Barack Wamkaya", "Muchemi Lawrence"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Long Short-Term Memory in Recurrent Neural Networks", "author": ["Felix Gers"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "These include using deep multilayer perceptrons [2] and convolutional neural networks [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The LSTM cell has an internal state which is updated based on the previous activations of the layer and inputs through connections to the previous layer and self connections[4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "6 Weight Initialisations We initialised our feed forward weights by sampling from the uniform distribution [6]", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "001 learning rate and default paramters [7].", "startOffset": 40, "endOffset": 43}], "year": 2016, "abstractText": "Stock prices are a form of time series data. There have been many existing business and economics based methods for predicting stock prices. These methods can be classed as fundamental and technical analysis. Technical analysis is based on the observing patterns in stock prices based on psychological effects (fear and greed) changing supply and demand. Fundamental analysis is based on observing current news and events such as the corporate profits and economic situation. However, all of these models ultimately rely on human judgement of the situation to make predictions and are not learned [1].", "creator": "LaTeX with hyperref package"}}}