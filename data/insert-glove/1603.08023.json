{"id": "1603.08023", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "abstract": "low-friction We lipping investigate cassino evaluation metrics for end - to - darrieux end monorails dialogue 92-76 systems dieselboy where supervised labels, such recessionary as artech task flyer completion, sarbanes are not horrell available. laxminarayan Recent works intimidating in total-goals end - to - end dialogue systems have gmo adopted finals metrics 58.25 from machine garad translation and carafe text szczawnica summarization irtf to 212.4 compare a model ' counter-piracy s generated response fire-control to cantius a single ekstra target resentfully response. We show that synechococcus these kumsusan metrics correlate bolande very weakly 139,000 or not kharge at rajeev all with subanon human elshafay judgements carhop of seigneur the light-heavyweight response 10:12 quality in both technical and 65-64 non - technical domains. We bottlings provide aremissoft quantitative and ciment qualitative bretherton results 10-bit highlighting specific impossible weaknesses alasania in motsepe existing ossipee metrics, sorey and provide seafront recommendations kazel for future development 55.13 of ilaiyaraaja better automatic evaluation metrics for trossingen dialogue 91-92 systems.", "histories": [["v1", "Fri, 25 Mar 2016 20:32:21 GMT  (787kb,D)", "http://arxiv.org/abs/1603.08023v1", "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. Submitted to ACL 2016"], ["v2", "Tue, 3 Jan 2017 18:28:32 GMT  (723kb,D)", "http://arxiv.org/abs/1603.08023v2", "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. EMNLP 2016"]], "COMMENTS": "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. Submitted to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["chia-wei liu", "ryan lowe", "iulian serban", "michael noseworthy", "laurent charlin", "joelle pineau"], "accepted": true, "id": "1603.08023"}, "pdf": {"name": "1603.08023.pdf", "metadata": {"source": "CRF", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "authors": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V. Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "emails": ["chia-wei.liu@mail.mcgill.ca", "ryan.lowe@mail.mcgill.ca", "michael.noseworthy@mail.mcgill.ca", "jpineau}@cs.mcgill.ca", "iulian.vlad.serban@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Significant progress has been made in learning end-to-end systems directly from large amounts of text data for a variety of natural language tasks, such as question answering (Weston et al., 2015), machine translation (Cho et al., 2014), and dialogue response generation systems (Sordoni et al., 2015), in particular through the use of neural network models. In the case of dialogue systems, an important challenge is to provide a reliable evaluation of the learned systems. Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; Mo\u0308ller et al., 2006). We call models that are trained to optimize for such supervised objectives supervised dialogue models, while those that are not unsupervised dialogue models.\n\u2217Denotes equal contribution.\nWhile supervised models have historically been the method of choice, supervised labels are difficult to collect on a large scale due to the cost of human labour. Further, for free-form types of dialogues (e.g., chatbots), the notion of task completion is ill-defined since it may differ from one human user to another.\nUnsupervised dialogue models are receiving increased attention. These models are typically trained (end-to-end) to predict the next utterance of a conversation, given several context utterances (Serban et al., 2015). This task is referred to as response generation. However automatically evaluating the quality of unsupervised models remains an open question. Automatic evaluation metrics would help accelerate the deployment of unsupervised dialogue systems.\nFaced with similar challenges, other natural language tasks have successfully developed automatic evaluation metrics. For example, BLEU (Papineni et al., 2002a) and METEOR (Banerjee and Lavie, 2005) are now standard for evaluating machine translation models, and ROUGE (Lin, 2004) is often used for automatic summarization. Since the machine translation task appears similar to the dialogue response generation task, dialogue researchers have adopted the same metrics for evaluating the performance of their models. However, the applicability of these methods has not been validated for dialoguerelated tasks. A particular challenge in dialogues is the significant diversity in the space of valid responses to a given conversational context. This is illustrated in Table 1, where two reasonable proposed responses are given to the context; however, these responses do not share any words in common and do not have the same semantic meaning.\nWe investigate several evaluation metrics for dialogue response generation systems, including both statistical word based similarity met-\nar X\niv :1\n60 3.\n08 02\n3v 1\n[ cs\n.C L\n] 2\n5 M\nar 2\n01 6\nrics such as BLEU, METEOR, and ROUGE, and word-embedding based similarity metrics derived from word embedding models such as Word2Vec (Mikolov et al., 2013). The BLEU metric, in particular, has been used recently to evaluate dialogue models (Li et al., 2015a; Galley et al., 2015a; Sordoni et al., 2015; Ritter et al., 2011a; Li et al., 2016).\nWe study the applicability of these metrics by using them to evaluate a variety of end-to-end dialogue models, including both retrieval models such as the Dual Encoder (Lowe et al., 2015) and generative models that incorporate some form of recurrent decoder (Serban et al., 2015). We use these models to produce a proposed response given the context of the conversation and compare them to the ground-truth response (the actual next response) using the above metrics.\nWhen evaluating these models with the embedding-based metrics, we find that even though some models significantly outperform others across several metrics and domains, the metrics only very weakly correlate with human judgement, as determined by human evaluation of the responses. This is despite the fact that metrics such as BLEU have seen significant recent use in evaluating unsupervised dialogue systems (Ritter et al., 2011a; Sordoni et al., 2015; Li et al., 2015b; Li et al., 2016). We highlight the shortcomings of these metrics using: a) a statistical analysis of our survey\u2019s results; b) a qualitative analysis of examples taken from our data; and c) an exploration of the sensitivity of the metrics."}, {"heading": "2 Related Work", "text": "Evaluation methods for supervised dialogue systems include the PARADISE framework (Walker et al., 1997), which simultaneously optimizes for task completion and alternative costs, such as number of utterances and agent response delay. Similarly, MeMo (Mo\u0308ller et al., 2006) evaluates dialogue systems through interactions with simulated users. An extensive overview of such metrics\ncan be found in (Jokinen and McTear, 2009). We focus on metrics that are modelindependent, i.e. where the model generating the response does not also evaluate its quality; thus, we do not consider word perplexity, although it has been used to evaluate unsupervised dialogue models (Serban et al., 2015). This is because it is not computed on a per-response basis, and cannot be computed for retrieval models. Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015).\nSeveral recent works on unsupervised dialogue systems adopt the BLEU score for evaluation. Ritter et al. (2011b) formulate the unsupervised learning problem as one of translating a context into a candidate response. They use a statistical machine translation (SMT) model to generate responses to various contexts using Twitter data, and show that it outperforms information retrieval baselines according to both BLEU and human evaluations. Sordoni et al. (2015) extend this idea using a recurrent language model to generate responses in a context-sensitive manner. They also evaluate using BLEU, however they produce multiple ground truth responses by retrieving 15 responses from elsewhere in the corpus, using a simple bag-of-words model. Li et al. (2015b) evaluate their proposed diversity-promoting objective function for neural network models using BLEU score with only a single ground truth response. A modified version of BLEU, deltaBLEU (Galley et al., 2015b), which takes into account several humanevaluated ground truth responses, is shown to have a weak to moderate correlation to human judgement using Twitter dialogues. However, such human annotation is often infeasible to obtain in practice. Galley et al (2015b) also show that, even with several ground truth responses available, the standard BLEU metric correlates at best weakly with human judgements."}, {"heading": "3 Evaluation Metrics", "text": "Given the context of a conversation and a proposed response, our goal is to automatically evaluate how appropriate and relevant the proposed response is to the conversation. We focus on metrics that compare it to the ground truth response of the\nconversation. In particular, we investigate two approaches: word based similarity metrics and wordembedding based similarity metrics."}, {"heading": "3.1 Word Overlap-based Metrics", "text": "We first consider metrics that evaluate the amount of word-overlap between the proposed response and the ground-truth response. We examine the BLEU and METEOR scores that have been used for machine translation, and the ROUGE score that has been used for automatic summarization.1 While these metrics have been shown to correlate with human judgement in their target domains (Papineni et al., 2002a; Lin, 2004), they have not been evaluated for dialogue systems.2\nWe denote the set of ground truth responses as R and the set of proposed responses as R\u0302. The size of both sets is N (that is we assume that there is a single candidate ground truth response) and individual sentences are indexed using i (ri \u2208 R). The j\u2019th token in sentence ri is denoted wij .\nBLEU. BLEU (Papineni et al., 2002a) analyzes the co-occurences of n-grams in the ground truth and the proposed responses. It first computes an ngram precision for the whole dataset (we assume that there is a single candidate ground truth response per context):\nPn(R, R\u0302) =\n\u2211 i \u2211 k min(h(k, ri), h(k, r\u0302i))\u2211\ni \u2211 k h(k, ri)\nwhere k indexes all possible n-grams of length n and h(k, ri) is the number of n-grams k in ri. To avoid the drawbacks of using a precision score, namely that it favours shorter (candidate) sentences, the authors introduce a brevity penalty. BLEU-N where N is the maximum length of ngrams considered is defined as:\nBLEU-N := b(R, R\u0302) exp( N\u2211\nn=1\n\u03b2n logPn(R, R\u0302))\n\u03b2n is a weighting that is usually uniform, and b(\u00b7) is the brevity penalty. Note that BLEU is usually\n1We only provide summaries of the metrics; we will add the mathematical details of all metrics using the extra page available at publication time.\n2To the best of our knowledge, only BLEU has been evaluated in the dialogue system setting quantitatively by Galley et al. (2015a) on the Twitter domain. However, they carried out their experiments in a very different setting with multiple ground truth responses, which are rarely available in practice, and without providing any qualitative analysis of their results.\ncalculated at the corpus-level, and has been shown to correlate with human judgement in the translation domain when there are multiple ground truth candidates available.\nMETEOR. The METEOR metric (Banerjee and Lavie, 2005) was introduced to address several weaknesses in BLEU. It creates an explicit alignment between the candidate and target responses. The alignment is based on exact token matching, followed by WordNet synonyms, stemmed tokens, and then paraphrases. Given a set of alignments, the METEOR score is the harmonic mean of precision and recall between the proposed and ground truth sentence.\nROUGE. ROUGE (Lin, 2004) is a set of evaluation metrics used for automatic summarization. We consider ROUGE-L, which is a Fmeasure based on the Longest Common Subsequence (LCS) between a candidate and target sentence. The LCS is a set of words which occur in two sentences in the same order; however, unlike n-grams the words do not have to be contiguous, i.e. there can be other words in between the words of the LCS."}, {"heading": "3.2 Embedding-based Metrics", "text": "An alternative to using word-overlap based metrics is to consider the meaning of each word as defined by a word embedding, which assigns a vector to each word. Methods such as Word2Vec (Mikolov et al., 2013) calculate these embeddings using distributional semantics; that is, they approximate the meaning of a word by considering how often it co-occurs with other words in the corpus3. These embedding-based metrics usually approximate sentence-level embeddings using some heuristic to combine the vectors of the individual words in the sentence. The sentence-level embeddings between the candidate and target response are compared using a measure such as cosine distance. This does not depend on exact wordoverlap between generated and actual responses, but still allows a quantitative comparison between responses generated by a dialogue system and the actual response of the conversation.\n3To maintain statistical independence between the task and each performance metric, it is important that the word embeddings used are trained on corpora which do not overlap with the task corpus. Otherwise the assumptions of independent and identically distributed (i.i.d.) training and test data examples are incorrect, which could lead to spurious and potentially misleading correlations between data examples.\nGreedy Matching. Greedy matching is the one embedding-based metric that does not compute sentence-level embeddings. Instead, given two sequences r and r\u0302, each token w \u2208 r is greedily matched with a token w\u0302 \u2208 r\u0302 based on the cosine similarity of their word embeddings (ew), and the total score is then averaged across all words:\nG(r, r\u0302) =\n\u2211 w\u2208r; maxw\u0302\u2208r\u0302 cos(ew, ew\u0302)\n|r|\nGM(r, r\u0302) = G(r, r\u0302) +G(r\u0302, r)\n2\nThis formula is asymmetric, thus we must average the greedy matching scores G in each direction. This was originally introduced for intelligent tutoring systems (Rus and Lintean, 2012). The greedy approach favours responses with key words that are semantically similar to those in the ground truth response.\nEmbedding Average. The embedding average metric calculates sentence-level embeddings using additive composition, a method for computing the meanings of phrases by averaging the vector representations of their constituent words (Foltz et al., 1998; Landauer and Dumais, 1997; Mitchell and Lapata, 2008). This method has been widely used in other domains, for example in textual similarity tasks (Wieting et al., 2015). The embedding average, e\u0304, is defined as the mean of the word embeddings of each token in a sentence r:\ne\u0304r =\n\u2211 w\u2208r ew\n| \u2211 w\u2032\u2208r ew\u2032 | .\nTo compare a ground truth response r and retrieved response r\u0302, we compute the cosine similarity between their respective sentence level embeddings: EA := cos(e\u0304r, e\u0304r\u0302).\nVector Extrema. Another way to calculate sentence-level embeddings is using vector extrema (Forgues et al., 2014). For each dimension of the word vectors, take the most extreme value amongst all word vectors in the sentence, and use that value in the sentence-level embedding:\nerd = {\nmaxw\u2208r ewd if ewd > |minw\u2032\u2208r ew\u2032d| minw\u2208r ewd otherwise\nwhere d indexes the dimensions of a vector; ewd is the d\u2019th dimensions of ew (w\u2019s embedding).\nSimilarity between response vectors is again computed using cosine distance. Intuitively, this\ncan be thought of as prioritizing informative words over common ones; words that appear in similar contexts will be close together in the vector space. Thus, common words are pulled towards the origin because they occur in various contexts, while words carrying important semantic information will lie further away. By taking the extrema along each dimension, we are therefore more likely to ignore common words."}, {"heading": "4 End-to-End Dialogue Models", "text": "We now describe a variety of models that can be used to produce a response given the context of a conversation. These fall into two categories: retrieval models, and generative models. While we do not consider all available models, those selected cover a diverse range of end-to-end models that appear in recent literature, and provide a good sample of models for illustrating evaluation with existing metrics."}, {"heading": "4.1 Retrieval Models", "text": "Ranking or retrieval models for dialogue systems are typically evaluated based on whether they can retrieve the correct response from a corpus of predefined responses, which includes the ground truth response to the conversation (Schatzmann et al., 2005). Such systems can be evaluated using recall or precision metrics. However, when deployed in a real setting these models will not have access to the correct response given an unseen conversation. Thus, in the results presented below we remove the correct response from the corpus and ask the model to retrieve the most appropriate response from the remaining utterances.\nWe then evaluate each model by comparing the retrieved response to the ground truth response of the conversation. This closely imitates real-life deployment of these models, as it tests the ability of the model to generalize to unseen contexts.\nTF-IDF. We consider a simple Term Frequency - Inverse Document Frequency (TF-IDF) retrieval model (Lowe et al., 2015). TF-IDF is a statistic that intends to capture how important a given word is to some document, which is calculated as: tfidf(w, c, C) = f(w, c) \u00d7 log N|{c\u2208C:w\u2208c}| , where C is the set of all contexts in the corpus, f(w, c) indicates the number of times word w appeared in context c, N is the total number of dialogues, and the denominator represents the number of dialogues in which the word w appears.\nIn order to apply TF-IDF as a retrieval model for dialogue, we first compute the TF-IDF vectors for each context and response in the corpus. We then produce two models: C-TFIDF computes the cosine similarity between an input context and all other contexts in the corpus, and returns the response from the context with the highest score, while R-TFIDF computes the cosine similarity between the input context and each response directly.\nDual Encoder. Next we consider the recurrent neural network (RNN) based architecture called the Dual Encoder (DE) model (Lowe et al., 2015). The DE model consists of two RNNs which respectively compute the vector representation of an input context and response, c, r \u2208 Rn. The model then calculates the probability that the given response is the ground truth response given the context, by taking a weighted dot product: p(r is correct|c, r,M) = \u03c3(cTMr + b) where M is a matrix of learned parameters and b is a bias. The model is trained using negative sampling to minimize the cross-entropy error of all (context, response) pairs. To our knowledge, our application of neural network models to large-scale retrieval in dialogue systems is novel."}, {"heading": "4.2 Generative Models", "text": "In addition to retrieval models, we also consider generative models. In this context, we refer to a model as generative if it is able to generate entirely new sentences that are unseen in the training set.\nLSTM language model. The baseline model is an LSTM language model (Hochreiter and Schmidhuber, 1997) trained to predict the next\nword in the (context, response) pair. During test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013). During test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013).\nHRED. Finally we consider the Hierarchical Recurrent Encoder-Decoder (HRED) (Serban et al., 2015). In the traditional Encoder-Decoder framework, all utterances in the context are concatenated together before encoding. Thus, information from previous utterances is far outweighed by the most recent utterance. The HRED model uses a hierarchy of encoders; each utterance in the context passes through an \u2018utterance-level\u2019 encoder, and the output of these encoders is passed through another \u2018context-level\u2019 encoder. This enables the handling of longer-term dependencies compared to a conventional Encoder-Decoder."}, {"heading": "4.3 Conclusions from an Incomplete Analysis", "text": "When evaluation metrics are not explicitly correlated to human judgement, it is possible to draw misleading conclusions by examining how the metrics rate different models.\nTo illustrate this point, we compare the performance of selected models according to the embedding metrics on two different domains: the Ubuntu Dialogue Corpus (Lowe et al., 2015), which con-\ntains technical vocabulary and where conversations are often oriented towards solving a particular problem or obtaining specific information, and a non-technical Twitter corpus collected following the procedure of Ritter et al. (2010), where the dialogues cover a diverse set of topics often without any particular goal. We consider these two datasets since they cover contrasting dialogue domains, i.e. technical help vs casual chit-chat, and because they are amongst the largest publicly available corpora, making them good candidates for building data-driven dialogue systems.\nResults on the proposed embedding metrics are shown in Table 2. For the retrieval models, we observe that the DE model significantly outperforms both TFIDF baselines on all metrics across both datasets. Further, the HRED model significantly outperforms the basic LSTM generative model in both domains, and appears to be of similar strength as the DE model.\nBased on these results, one might be tempted to conclude that there is some information being captured by these metrics, that significantly differentiates models of different quality. However, as we show in the next section, the embeddingbased metrics correlate only weakly with human judgement on the Twitter corpus, and not at all on the Ubuntu Dialogue Corpus. Although this may not come as a surprise to some researchers in the dialogue system community, the fact remains that BLEU has been frequently used to evaluate unsupervised dialogue systems (Li et al., 2015a; Li et al., 2016; Galley et al., 2015a; Ritter et al., 2011a)."}, {"heading": "5 Human Correlation Analysis", "text": ""}, {"heading": "5.1 Data collection", "text": "We conducted a human survey to determine the correlation between human judgement on the\nquality of responses, and the score assigned by each metric. We aimed to follow the procedure for the evaluation of BLEU (Papineni et al., 2002a). 25 volunteers from the Computer Science department at the author\u2019s institution were given a context and one proposed response, and were asked to judge the response quality on a scale of 1 to 5 4 ; in this case, a 1 indicates that the response is either not appropriate or sensible given the context, and a 5 indicates that the response is very reasonable.\nEach volunteer was given 100 questions for each of the Ubuntu and Twitter datasets. These questions correspond to 20 unique contexts, with 5 different responses: one utterance randomly drawn from elsewhere in the test set, the response selected from each of the TF-IDF, DE, and HRED models, and a response written by a human annotator. These were chosen as they cover the range of qualities approximately uniformly, as seen in Figure 1. The questions were randomly permuted within each dataset during the experiments. Out of the 25 respondents, 23 had Cohen\u2019s kappa scores \u03ba > 0.2 w.r.t. the other respondents, which is a standard measure for inter-rater agreement (Cohen, 1968). The 2 respondents with low kappa scores were excluded from the analysis below."}, {"heading": "5.2 Survey Results", "text": "We present correlation results between the human judgements and each metric in Table 3. We compute the Pearson correlation, which estimates lin-\n4Studies asking humans to evaluate text often rate different aspects separately, such as \u2018adequacy\u2019, \u2018fluency\u2019 and \u2018informativeness\u2019 of the text (Hovy, 1999; Papineni et al., 2002b). Our evaluation focuses on adequacy. We did not consider fluency because 4 out of the 5 proposed responses to each context were generated by a human. We did not consider informativeness because in the domains considered, it is not necessarily important (in Twitter), or else it seems to correlate highly with adequacy (in Ubuntu).\near correlation, and Spearman correlation, which estimates any monotonic correlation.\nThe first observation is that in both domains the BLEU-4 score, which has previously been used to evaluate unsupervised dialogue systems, shows very weak if any correlation with human judgement. In fact we found that in our analysis, the BLEU-3 and BLEU-4 scores were near-zero for a majority of response pairs; for BLEU-4, only four examples had a score > 10\u22129.5 Some of the embedding metrics and BLEU-2 show some positive correlation in the non-technical Twitter domain. However there is no metric that significantly correlates with humans on the Ubuntu Dialogue Corpus, quite possibly because the correct Ubuntu responses contain specific technical words, which are less likely to be generated or retrieved by a learned model.\nFigure 1 illustrates the relationship between metrics and human judgements. We include only the best performing metric using word-overlaps, i.e. the BLEU-2 score (left), and the best performing metric using word embeddings, i.e. the vector average (center). These plots show how weak the correlation is: in both cases, they appear to be ran-\n5The reason BLEU-3 and BLEU-4 have any correlation is because of the smoothing constant, which gives a tiny weight to unigrams and bigrams despite the absence of higher-order n-grams. Thus, they behave as a scaled version of BLEU-2.\ndom noise. It seems as though the BLEU score obtains a positive correlation because of the large number of responses that are given a score of 0 (bottom left corner of the first plot). This is in stark contrast to the inter-rater agreement, which is plotted between two randomly sampled halves of the raters (right-most plots).\nWe also calculated the BLEU scores after removing stopwords and punctuation from the responses. As shown in Table 4, this weakens the correlation with human judgement for BLEU2 compared to the values in Table 3, and suggests that BLEU is sensitive to factors that do not change the semantics of the response.\nFinally, we examined the effect of response length on the metrics, by considering changes in scores for the cases where the response length differ in terms of words, between the ground truth response and the proposed response. Table 4 shows that BLEU and METEOR are particularly sensitive to this aspect, compared to the Embedding Average metric and human judgement."}, {"heading": "5.3 Qualitative Analysis", "text": "In order to determine specifically why the metrics fail, we examine some qualitative samples where there is a disagreement between the metrics and human rating. We present in Figure 2 two examples where all of the embedding-based metrics and\nBLEU-1 score the proposed response significantly differently than the humans.\nThe left of Figure 2 shows an example where the embedding-based metrics score the proposed response lowly, while humans rate it highly. It is clear from the context that the proposed response is reasonable \u2013 indeed both responses intend to express gratitude. However, the proposed response has a different wording than the ground truth response, and therefore the metrics are unable to separate the salient words from the rest. This suggests that the embedding-based metrics would benefit from a weighting of word saliency.\nThe right of the figure shows the reverse scenario: the embedding-based metrics score the proposed response highly, while humans do not. This is most likely due to the frequently occurring \u2018i\u2019 token, and the fact that \u2018happy\u2019 and \u2018welcome\u2019 may be close together in the embedding space. However, from a human perspective there is a significant semantic difference between the responses as they pertain to the context. Metrics that take into account the context may be required in order to differentiate these responses. Note that in both responses in Figure 2, there are no overlapping ngrams greater than unigrams between the ground truth and proposed responses; thus, all of BLEU2,3,4 would assign a score near 0 to the response."}, {"heading": "6 Discussion", "text": "We have shown that many metrics commonly used in the literature for evaluating unsupervised dialogue systems do not correlate strongly with human judgement. Here we elaborate on important issues arising from our analysis.\nConstrained tasks. Our analysis focuses on relatively unconstrained domains. Other work, which separates the dialogue system into a di-\nalogue planner and a natural language generation component, for applications in constrained domains, may find stronger correlations with the BLEU metric. Wen et al. (2015) provide an example of this, when they propose a model to map from dialogue acts to natural language sentences and use BLEU to evaluate the quality of the generated sentences. Since the mapping from dialogue acts to natural language sentences is more constrained and more similar to the machine translation task, it seems likely that BLEU will correlate better with human judgements. However, an empirical investigation is still necessary to justify this.\nIncorporating multiple responses. Our correlation results assume that only one ground truth response is available given each context. Indeed, this is the common setting in most of the recent literature on training end-to-end conversation models. There has been some work on using a larger set of automatically retrieved plausible responses when evaluating with BLEU (Sordoni et al., 2015). However, there is no standard method for doing this in the literature. Future work should examine how retrieving additional responses affects the correlation with word-overlap metrics.\nSearching for suitable metrics. While we provide evidence against existing metrics, we do not yet provide good alternatives for unsupervised evaluation. We do believe that embedding-based metrics hold the most promise, if they can be extended to take into account more complex models for modeling sentence-level compositionality. For example, the skip-thought vectors of Kiros et al. (2015) could be considered. Metrics that take into account the context of the conversation, or other utterances in the corpus could also be considered. Finally, a model could be learned using the data\ncollected from the human survey in order to provide human-like scores to proposed responses."}, {"heading": "Appendix: Full scatter plots", "text": "We present the scatterplots for all of the metrics consider and their correlation with human judgement, in Figures 3-7 below. As previously emphasized, there is very little correlation for any of the metrics, and the BLEU-3 and BLEU-4 scores are often close to zero."}], "references": [{"title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures", "author": ["Banerjee", "Lavie2005] S. Banerjee", "A. Lavie"], "venue": null, "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit", "author": ["Jacob Cohen"], "venue": "Psychological bulletin,", "citeRegEx": "Cohen.,? \\Q1968\\E", "shortCiteRegEx": "Cohen.", "year": 1968}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["Foltz et al.1998] P.W. Foltz", "W. Kintsch", "T.K. Landauer"], "venue": "Discourse processes,", "citeRegEx": "Foltz et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Bootstrapping dialog systems with word embeddings", "author": ["Forgues et al.2014] G. Forgues", "J. Pineau", "J.-M. Larcheveque", "R. Tremblay"], "venue": null, "citeRegEx": "Forgues et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Forgues et al\\.", "year": 2014}, {"title": "2015a. deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse", "author": ["Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret l", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "2015b. deltableu: A discriminative metric for generation tasks with intrinsically diverse tar", "author": ["Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["A. Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Toward finely differentiated evaluation metrics for machine translation", "author": ["Eduard Hovy"], "venue": "In Proceedings of the Eagles Workshop on Standards and Evaluation", "citeRegEx": "Hovy.,? \\Q1999\\E", "shortCiteRegEx": "Hovy.", "year": 1999}, {"title": "Spoken Dialogue Systems", "author": ["Jokinen", "McTear2009] K. Jokinen", "M. McTear"], "venue": null, "citeRegEx": "Jokinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jokinen et al\\.", "year": 2009}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "2015a. A diversity-promoting objective function for neural conversation models. CoRR, abs/1510.03055", "author": ["Li et al.2015a] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "2015b. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055", "author": ["Li et al.2015b] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A personabased neural conversation model", "author": ["Li et al.2016] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1603.06155", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Text summarization branches out: Proceedings of the ACL-04 workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Lowe et al.2015] Ryan Lowe", "Nissan Pow", "Iulian V. Serban", "Joelle Pineau"], "venue": "In SIGDIAL", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Memo: towards automatic usability evaluation of spoken dialogue services by user error simulations", "author": ["M\u00f6ller et al.2006] S. M\u00f6ller", "R. Englert", "K.P. Engelbrecht", "V.V. Hafner", "A. Jameson", "A. Oulasvirta", "A. Raake", "N. Reithinger"], "venue": null, "citeRegEx": "M\u00f6ller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "M\u00f6ller et al\\.", "year": 2006}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["S. Roukos", "T Ward", "W Zhu"], "venue": "In Proceedings of the 40th annual meeting on Association for Computational Linguistics (ACL)", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results", "author": ["Salim Roukos", "Todd Ward", "John Henderson", "Florence Reeder"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Unsupervised modeling of twitter conversations. In North American Chapter of the Association for Computational Linguistics (NAACL)", "author": ["Ritter et al.2010] A. Ritter", "C. Cherry", "B. Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011a] Alan Ritter", "Colin Cherry", "William B. Dolan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011b] Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics", "author": ["Rus", "Lintean2012] V. Rus", "M. Lintean"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Applications Us-", "citeRegEx": "Rus et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2012}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["K. Georgila", "S. Young"], "venue": "In 6th Special Interest Group on Discourse and Dialogue (SIGDIAL)", "citeRegEx": "Schatzmann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Building EndTo-End Dialogue Systems Using Generative Hierarchical Neural Networks", "author": ["Serban et al.2015] I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Sordoni et al.2015] A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Nie", "J. Gao", "B. Dolan"], "venue": "In Conference of the North American", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Paradise: A framework for evaluating spoken dialogue agents", "author": ["Walker et al.1997] M.A. Walker", "D.J. Litman", "C.A. Kamm", "A. Abella"], "venue": "In Proceedings of the eighth conference on European chapter of the Association for Computational Lin-", "citeRegEx": "Walker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisit toy tasks. arXiv preprint arXiv:1502.05698", "author": ["Weston et al.2015] J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings. CoRR, abs/1511.08198", "author": ["Wieting et al.2015] J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "end-to-end systems directly from large amounts of text data for a variety of natural language tasks, such as question answering (Weston et al., 2015), machine translation (Cho et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 1, "context": ", 2015), machine translation (Cho et al., 2014), and dialogue response generation systems (Sordoni et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 27, "context": ", 2014), and dialogue response generation systems (Sordoni et al., 2015), in particular through the use of neural network models.", "startOffset": 50, "endOffset": 72}, {"referenceID": 28, "context": "Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; M\u00f6ller et al., 2006).", "startOffset": 132, "endOffset": 174}, {"referenceID": 18, "context": "Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; M\u00f6ller et al., 2006).", "startOffset": 132, "endOffset": 174}, {"referenceID": 26, "context": "trained (end-to-end) to predict the next utterance of a conversation, given several context utterances (Serban et al., 2015).", "startOffset": 103, "endOffset": 124}, {"referenceID": 15, "context": ", 2002a) and METEOR (Banerjee and Lavie, 2005) are now standard for evaluating machine translation models, and ROUGE (Lin, 2004) is often used for automatic summarization.", "startOffset": 117, "endOffset": 128}, {"referenceID": 17, "context": "rics such as BLEU, METEOR, and ROUGE, and word-embedding based similarity metrics derived from word embedding models such as Word2Vec (Mikolov et al., 2013).", "startOffset": 134, "endOffset": 156}, {"referenceID": 16, "context": "We study the applicability of these metrics by using them to evaluate a variety of end-to-end dialogue models, including both retrieval models such as the Dual Encoder (Lowe et al., 2015)", "startOffset": 168, "endOffset": 187}, {"referenceID": 26, "context": "and generative models that incorporate some form of recurrent decoder (Serban et al., 2015).", "startOffset": 70, "endOffset": 91}, {"referenceID": 27, "context": "This is despite the fact that metrics such as BLEU have seen significant recent use in evaluating unsupervised dialogue systems (Ritter et al., 2011a; Sordoni et al., 2015; Li et al., 2015b; Li et al., 2016).", "startOffset": 128, "endOffset": 207}, {"referenceID": 14, "context": "This is despite the fact that metrics such as BLEU have seen significant recent use in evaluating unsupervised dialogue systems (Ritter et al., 2011a; Sordoni et al., 2015; Li et al., 2015b; Li et al., 2016).", "startOffset": 128, "endOffset": 207}, {"referenceID": 18, "context": "Similarly, MeMo (M\u00f6ller et al., 2006) evaluates dialogue systems through interactions with simulated users.", "startOffset": 16, "endOffset": 37}, {"referenceID": 26, "context": "it has been used to evaluate unsupervised dialogue models (Serban et al., 2015).", "startOffset": 58, "endOffset": 79}, {"referenceID": 25, "context": "Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015).", "startOffset": 224, "endOffset": 268}, {"referenceID": 16, "context": "Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015).", "startOffset": 224, "endOffset": 268}, {"referenceID": 21, "context": "Ritter et al. (2011b) formulate the unsupervised", "startOffset": 0, "endOffset": 22}, {"referenceID": 27, "context": "Sordoni et al. (2015) extend this idea using a recurrent language model to generate responses in a context-sensitive manner.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Li et al. (2015b) evaluate their proposed diversity-promoting objective function for neural network models using BLEU score with only a single ground truth response.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "A modified version of BLEU, deltaBLEU (Galley et al., 2015b), which takes into account several humanevaluated ground truth responses, is shown to have a weak to moderate correlation to human judgement using Twitter dialogues. However, such human annotation is often infeasible to obtain in practice. Galley et al (2015b) also show that, even with several ground truth responses available, the standard BLEU metric correlates at best weakly with human judgements.", "startOffset": 39, "endOffset": 321}, {"referenceID": 15, "context": "1 While these metrics have been shown to correlate with human judgement in their target domains (Papineni et al., 2002a; Lin, 2004), they have not been evaluated for dialogue systems.", "startOffset": 96, "endOffset": 131}, {"referenceID": 5, "context": "To the best of our knowledge, only BLEU has been evaluated in the dialogue system setting quantitatively by Galley et al. (2015a) on the Twitter domain.", "startOffset": 108, "endOffset": 130}, {"referenceID": 15, "context": "ROUGE (Lin, 2004) is a set of evaluation metrics used for automatic summarization.", "startOffset": 6, "endOffset": 17}, {"referenceID": 17, "context": "Methods such as Word2Vec (Mikolov et al., 2013) calculate these", "startOffset": 25, "endOffset": 47}, {"referenceID": 3, "context": "meanings of phrases by averaging the vector representations of their constituent words (Foltz et al., 1998; Landauer and Dumais, 1997; Mitchell and Lapata, 2008).", "startOffset": 87, "endOffset": 161}, {"referenceID": 31, "context": "This method has been widely used in other domains, for example in textual similarity tasks (Wieting et al., 2015).", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "Another way to calculate sentence-level embeddings is using vector extrema (Forgues et al., 2014).", "startOffset": 75, "endOffset": 97}, {"referenceID": 25, "context": "are typically evaluated based on whether they can retrieve the correct response from a corpus of predefined responses, which includes the ground truth response to the conversation (Schatzmann et al., 2005).", "startOffset": 180, "endOffset": 205}, {"referenceID": 16, "context": "- Inverse Document Frequency (TF-IDF) retrieval model (Lowe et al., 2015).", "startOffset": 54, "endOffset": 73}, {"referenceID": 16, "context": "neural network (RNN) based architecture called the Dual Encoder (DE) model (Lowe et al., 2015).", "startOffset": 75, "endOffset": 94}, {"referenceID": 7, "context": "During test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013).", "startOffset": 135, "endOffset": 149}, {"referenceID": 7, "context": "test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013).", "startOffset": 128, "endOffset": 142}, {"referenceID": 26, "context": "Recurrent Encoder-Decoder (HRED) (Serban et al., 2015).", "startOffset": 33, "endOffset": 54}, {"referenceID": 16, "context": "To illustrate this point, we compare the performance of selected models according to the embedding metrics on two different domains: the Ubuntu Dialogue Corpus (Lowe et al., 2015), which con-", "startOffset": 160, "endOffset": 179}, {"referenceID": 21, "context": "ing the procedure of Ritter et al. (2010), where the dialogues cover a diverse set of topics often without any particular goal.", "startOffset": 21, "endOffset": 42}, {"referenceID": 14, "context": "Although this may not come as a surprise to some researchers in the dialogue system community, the fact remains that BLEU has been frequently used to evaluate unsupervised dialogue systems (Li et al., 2015a; Li et al., 2016; Galley et al., 2015a; Ritter et al., 2011a).", "startOffset": 189, "endOffset": 268}, {"referenceID": 2, "context": "the other respondents, which is a standard measure for inter-rater agreement (Cohen, 1968).", "startOffset": 77, "endOffset": 90}, {"referenceID": 9, "context": "Studies asking humans to evaluate text often rate different aspects separately, such as \u2018adequacy\u2019, \u2018fluency\u2019 and \u2018informativeness\u2019 of the text (Hovy, 1999; Papineni et al., 2002b).", "startOffset": 144, "endOffset": 180}, {"referenceID": 29, "context": "Wen et al. (2015) provide an example of this, when they propose a model to map", "startOffset": 0, "endOffset": 18}, {"referenceID": 27, "context": "There has been some work on using a larger set of automatically retrieved plausible responses when evaluating with BLEU (Sordoni et al., 2015).", "startOffset": 120, "endOffset": 142}], "year": 2016, "abstractText": "We investigate evaluation metrics for endto-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model\u2019s generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.", "creator": "LaTeX with hyperref package"}}}