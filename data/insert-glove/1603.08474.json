{"id": "1603.08474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Deep Embedding for Spatial Role Labeling", "abstract": "zainol This paper jennie introduces download-only the vullo visually informed lyre embedding 1,417 of nscn word (burbury VIEW ), a continuous gantcher vector unten representation for wilga a word extracted vieillot from psychical a petrolati deep cybersquatter neural model pantaenus trained using ozamiz the Microsoft COCO 62.59 data set to forecast the spatial bennison arrangements between visual objects, given mahram a raba textual description. chaturthi The haut-brion model is composed etibar of a deep ruey multilayer perceptron (MLP) stacked on avoided the phocian top kachina of a elna Long redshift Short Term Memory (launderers LSTM) network, 76.40 the k\u014dmeit\u014d latter wonderfully being preceded by linha an domack embedding skylight layer. tasker The connotative VIEW is eumetsat applied riau to transferring khda multimodal background chimichurri knowledge beefing to cry-baby Spatial teasdale Role Labeling (10-truck SpRL) algorithms, signorini which recognize mandoline spatial vercoe relations thellusson between objects shekhdar mentioned cilley in senate the text. 41.07 This work also clinchers contributes tadahiro with 2,154 a 86-yard new method careca to select complementary lobola features and whynott a bodor fine - tuning abbemuseum method puype for shukrijumah MLP muirfield that 3,110 improves the $ tullamore F1 $ measure motormen in dniepr classifying the 1907-1908 words bleidelis into piscataquis spatial bega roles. mcdaid The VIEW strymon is evaluated with the Task 3 onyszkiewicz of SemEval - sti 2013 abubakarov benchmark data purinergic set, swiss SpaceEval.", "histories": [["v1", "Mon, 28 Mar 2016 18:38:46 GMT  (370kb,D)", "http://arxiv.org/abs/1603.08474v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG cs.NE", "authors": ["oswaldo ludwig", "xiao liu", "parisa kordjamshidi", "marie-francine moens"], "accepted": false, "id": "1603.08474"}, "pdf": {"name": "1603.08474.pdf", "metadata": {"source": "CRF", "title": "Deep Embedding for Spatial Role Labeling", "authors": ["Oswaldo Ludwig", "Xiao Liu", "Parisa Kordjamshidi", "Marie-Francine Moens"], "emails": [], "sections": [{"heading": null, "text": "This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the F1 measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval."}, {"heading": "1 Introduction", "text": "One of the essential functions of natural language is to describe location and translocation of objects in space. Spatial language can convey complex spatial relations along with polysemy and ambiguity inherited in natural language. Therefore, a formal spatial model is required, to focus on some particular spatial aspects. This paper address a layer of linguistic conceptual representation, called spatial role labeling (SpRL), which predicts the existence of spatial information at the sentence level by identifying the words that play a particular spatial role as well as their spatial relationship Kordjamshidi & Moens (2015).\nAn issue in extracting spatial semantics from natural language is the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations. Current SpRL algorithms rely strongly on feature engineering, which has the advantage of encoding human knowledge, thus compensating for the lack of annotated training data. This work preserves the previous contributions on feature engineering of Kordjamshidi & Moens (2015) while adding a new set of features learned from multimodal data, i.e. the visually informed embedding of word (VIEW).\nMultimodal data is usually associated with multimodal representation learning, which has been studied by various authors, such as Srivastava & Salakhut-\nar X\niv :1\n60 3.\n08 47\n4v 1\n[ cs\n.C L\n] 2\n8 M\nar 2\ndinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images.\nSimilar to the work Kiros et al. (2014) our model learns embedding from multimodal data and applies LSTM to process textual information. However, unlike most of the works on multimodal representation learning, which jointly map visual and textual information into a common embedding space, our work aims at providing embeddings only for words, but encoding spatial information extracted from the image annotations. The idea is to learn VIEW by pipelining an embedding layer into a deep architecture trained by back propagation to predict the spatial arrangement between the visual objects annotated in the pictures, given the respective textual descriptions. In this sense, unlike Karpathy et al. (2014) and Kiros et al. (2014), we don\u2019t need a CNN, because the spatial information which is relevant for our purpose is provided directly by the position of the bounding boxes containing the visual objects annotated in the images, as detailed in Section 3. Our VIEW is used as a vehicle to transfer spatial information from multimodal data to SpRL algorithms.\nThe paper is organized as follows. Section 3, we describe the model setting, from the annotation style through the modeling of the deep neural network, whose training algorithm is described in Section 4. Section 5 describes how the spatial embedding is applied in SpRL, as well as the algorithm developed to select the best complementary embedding features and the fine-tuning method able to deal with the tradeoff of precision and recall, aiming at the largest F1. Section 6 reports and discusses the experiments, while Section 7 summarizes the major findings."}, {"heading": "2 Problem definition and research questions", "text": "The SpRL algorithm recognizes spatial objects in language (i.e. trajector and landmark) and their spatial relation signaled by the spatial indicator. The trajector is a spatial role label assigned to a word or a phrase that denotes an object of a spatial scene, more specifically, an object that moves. A landmark is a spatial role label assigned to a word or a phrase that denotes the location of this trajector object in the spatial scene. The spatial indicator is a spatial role label assigned to a word or a phrase that signals the spatial relation trajector and landmark.\nIn this work we apply the SpRL algorithm developed for the work Kordjamshidi & Moens (2015), which models this problem as a structured prediction task Taskar et al. (2005), that is, it jointly recognizes the spatial relation and its\ncomposing role elements in text. The SpRL algorithm receives as input a natural language sentence, such as \u201cThere is a white large statue with spread arms on a hill\u201d, having a number of words, in this case identified as w = {w1, . . . ,w12}, where wi is the i\nth word in the sentence. Each word in the sentence that can be part of a spatial relation (e.g., nouns, prepositions) is described by a vector of the local features denoted by \u03c6word(wi), including linguistically motivated lexical, syntactical and semantical features of words, such as the lexical surface form, its semantic role, its part-of-speech and the lexical surface form of words in the neighborhood. This feature vector is used to relate a word with its spatial role, i.e. spatial indicator, trajector or landmark, hence further represented by sp, tr and lm, respectively.\nThere are also descriptive vectors of pairs of words, referred to as \u03c6pair(wi,wj), encoding the linguistically motivated features of pairs of words and their relational features, such as their relative position in the sentence, their distance in terms of number of words and the path between them obtained with a syntactic parser. The SpRL model is trained on a training set of sentences annotated with the above output labels. Following training, the system outputs all spatial relations found in a sentence of a test set composed of the sentences and their corresponding spatial roles.\nThe main research question approached in this paper regards the possibility of improving the quality of \u03c6word(wi) by concatenating the VIEW in this feature vector. It derives a secondary research question on the possibility of encoding visual information from COCO images into the word embeddings by learning a model able to map from the captions to a simplified representation of the visual objects annotated in the corresponding image and their relative position. We assume that the necessary condition to correctly forecast the visual output, given the textual description, is that the embedding layer (which is in the model pipeline) is successfully encoding the spatial information of the textual description, assuring a suitable word embedding for this specific task related with SpRL.\nAnother research question regards the importance of feature selection in order to discard embedding features that are not directly related to the SpRL task, since our data set derived from COCO is not created for SpRL; therefore, some features can act as noise for SpRL."}, {"heading": "3 The model setting", "text": "The Microsoft COCO data set Lin et al. (2014) is a collection of images featuring complex everyday scenes which contain common visual objects in their natural context. COCO contains photos of 91 object types with a total of 2.5 million labeled instances in 328k images. Each image has five written caption descriptions. The visual objects within the image are tight-fitted by bounding boxes from annotated segmentation masks, as can be seen in Fig.1.\nOur annotation system automatically derives a less specific spatial annotation about the relative position between the center points of the bounding\nboxes containing visual objects, given COCO\u2019s annotation of the coordinates of the bounding boxes. Our annotation style is ruled by the predicates alone(v1), below(v1, v2) and beside(v1, v2), where v1 and v2 are visual objects, see Fig.2. This information is encoded in a sparse target vector, Y , where the first three positions encode the predicate in one-hot vector style, and the ensuing positions encode the index of the visual objects, also in one-hot vector style, i.e. 3 positions to encode the predicates plus 91 positions to encode the index of the first argument (visual object) plus other 91 positions for the second argument, totalizing 185 positions in the target vector. If the predicate is alone(v1), i.e. when a single visual object is annotated in the image, the last 91 positions are all zeros.\nDespite having a large number of annotated objects per image, MS-COCO has several objects belonging to the same category per image, and so, a small number of categories per image. On average MS-COCO data set contains 3.5 categories per image, yielding 7.7 instances per image (see Section 5 of Lin et al. (2014)). Our annotation system is based on two assumptions: 1) learning the spatial arrangement between visual objects belonging to the same category is not useful: 2) objects placed at the center of the image are more salient, and\nso, they are likely to be present in the captions. Therefore, our system ranks the bounding boxes from the most centered box to the least centered box and starts by selecting the most centered bounding box as the first visual object. Then, it searches from the second higher ranked bounding box to those with a lower rank, until it reaches a visual object belonging to a different category or the end of the list of instances, i.e. our system selects only a pair of visual objects belonging to different categories.\nIn summary, the system learns how to map the captions to the spatial relation between the most salient pair of objects belonging to different categories. For instance, in Fig.1 the system selects the man and the most centered sheep (not the dog in the corner of the image) to generate the target output for all the five captions, yielding five training examples. Notice that the dog is only cited in one of the five captions."}, {"heading": "3.1 The embedding model", "text": "The input of our deep model is the textual information provided by COCO\u2019s captions, i.e., a sequence1 of words encoded in a set of K-dimensional one-hot vectors, where K is the vocabulary size, here assumed as 8000 words. Since COCO\u2019s captions are shorter than 30 words, our system cuts texts after the limit of 30 words; therefore, the data pair for the ith caption is composed by a sparse matrix Xi of dimension 8000\u00d730 per target vector, yi, of 185 dimensions, as explained in the previous paragraph. Captions with less than 30 words are\n1The words, encoded in one-hot vectors, are provided sequentially to the model.\nencoded into a matrix Xi whose the first columns are filled with all-zeros. Each figure has five associated captions, which yields five training examples with the same target vector, but different input vectors.\nOur deep model is trained to forecast the visual objects and their spatial relation (i.e. alone(v1), below(v1, v2) and beside(v1, v2)), given the textual description in the caption.\nThe model was implemented in Keras2 and it is pipeline composed of a linear embedding layer, an original version of LSTM network, as proposed in Hochreiter & Schmidhuber (1997), and a deep multilayer perceptron (MLP), see Fig.3. The embedding layer receives the sparse input Xi, representing a sentence, and encodes the one-hot vector representation of the words (i.e. the columns of Xi) into a set of 30 dense vectors of Nw dimensions that are provided sequentially to the LSTM, which extracts a Ns-dimensional vector from the sentence (after 30 iterations). This vector representation of the sentence (i.e. a sentence-level embedding) is mapped to the sparse spatial representation yi by the MLP.\nWe also evaluated an architecture where the LSTM directly predicts the sparse output vector (without MLP), but the result was better using the MLP. The latent representation of sentences, i.e. the sentence-level embedding, makes possible the choice of a suitable dimension for the LSTM output, rather than forcing the LSTM output to be the sparse 185-dimensional target vector.\nThe ith caption yields a matrix Xi from which a set of 30 input vectors, here represented by xt (t = 1, . . . , 30), are sequentially given to the model.\n2http://keras.io/\nOur model starts by computing the word embedding:\net = Wext (1)\nwhere the adjustable embedding matrix, We, has the dimension of 8000\u00d7Nw, i.e. it maps from the 8000-dimensional one-hot vector representation of words to a Nw-dimensional continuous vector representation et. Then the system calculates the state variables of the LSTM, starting by the input gate it and the candidate value for the states of the memory cells C\u0303t at iteration t, as follows:\nit = \u03c3(Wiet + Uiht\u22121 + bi) (2)\nC\u0303t = tanh(Wcet + Ucht\u22121 + bc) (3)\nwhere \u03c3(\u00b7) represents the sigmoid function, Wi, Ui, Wc, Uc, bi and bc are adjustable parameters and ht\u22121 is the LSTM output at the previous iteration. Having it and C\u0303t the system can compute the activation of the memory forget gates, ft, at iteration t:\nft = \u03c3(Wfet + Ufht\u22121 + bf ) (4)\nwhere Wf , Uf and bf are adjustable parameters. Having it, ft and the candidate state value C\u0303t, the system can compute the new state of the memory cells, Ct, at iteration t: Ct = it \u2217 C\u0303t + ft \u2217 Ct\u22121 (5) where \u2217 represents the point-wise multiplication operation. With the new state of the memory cells, Ct, the system can compute the value of their output gates, ot:\not = \u03c3(Woet + Uoht\u22121 + bo) (6)\nwere Wo, Uo and bo are adjustable parameters. Having ot, the system can finally calculate the output of the LSTM, ht:\nht = ot \u2217 tanh(Ct) (7)\nAt this point we have both word-level and sentence-level embeddings, given by (1) and (7), respectively. Each Ns-dimensional sentence embedding is produced after 30 iteration of LSTM (the adopted sentence length), when it is ready to be processed by the MLP. Therefore, the LSTM output is sub-sampled in the rate of 1/30, yielding hi, i.e. the input for the MLP (note the changing of the index variable in relation to (7)).\nThe adopted MLP has two sigmoidal hidden layers and a sigmoidal output layer. The optimal number of hidden layers was empirically determined based on the performance on MS-COCO. The MLP model is given by:\nyh(1,i) = \u03c3 (W1hi + b1) yh(2,i) = \u03c3 ( W2yh(1,i) + b2 ) y\u0302i = \u03c3 ( W3yh(2,i) + b3\n) (8) where Wj and bj are the weight matrix and bias vector of layer j, respectively, and yh(j,i) is the output vector of the hidden layer j."}, {"heading": "4 Model Training", "text": "After evaluating different objective functions combined with different activity and weight regularizers available in Keras, we decided to implement a custom objective function, gathering some ideas from the support vector learning. Our training method yields the following constrained optimization problem:\nmin We,\u03b8l,\u03b8m\n1\nNeNo Ne\u2211 i=1 No\u2211 j=1 max ( 1\u2212 y\u0302(i,j) ( 2y(i,j) \u2212 1 ) , 0 )\n(9)\nsubject to: \u2016Wneul \u2016 \u2264 1, l=1,2,3 \u2200neu (10)\nwhere the objective function (9) is the Hinge loss with the jth position of the intended output vector for the ith caption, y(i,j) \u2208 {0, 1}, scaled and shifted to assume the values \u22121 or 1, Ne is the cardinality of the training data set, No = 185 is the dimension of the output vector, We is the weight matrix of the embedding, \u03b8l = {Wi,Wf ,Wc,Wo, Ui, Uf , Uc, Uo, bi, bf , bc, bo} is the set of adjustable LSTM parameters, \u03b8m = {W1,W2,W3, b1, b2, b3} is the set of adjustable MLP parameters, y\u0302(i,j) is the j th position of the output vector estimated by our model for the ith caption and Wneul is the vector of synaptic weights of the neuron neu of the layer l.\nKeras allows to set constraints on network parameters during optimization. The adopted set of constraints (10) regularizes the model by upper bounding the norm of the vector of synaptic weights of the MLP neurons. Note that the adopted loss function (9) only penalizes examples that violate a given margin or are misclassified, i.e. an estimated output smaller than 1 in response to a positive example or an estimated output larger than -1 in response to a negative example (these training examples can be understood as support vectors). The other training examples are ignored during the optimization, i.e. they don\u2019t participate in defining the decision surface.\nThe best optimization algorithm for our model was Adam Kingma & Ba (2014), an algorithm for first-order gradient-based optimization based on adaptive estimates of lower-order moments."}, {"heading": "5 Applying the spatial-specific embedding in SpRL", "text": "We apply VIEW in SpRL by simply concatenating it with the original feature vector, \u03c6word(wi), generated by the SpRL algorithm Kordjamshidi & Moens (2015) for the words that are candidate for sp, tr and lm."}, {"heading": "5.1 Selecting complementary Features", "text": "Our aim is to select complementary features from the VIEW so as to maximize the mutual information between the target output, here represented by\nthe scalar random variable r, and the selected features, represented by the random variables x1 \u2208 X1, . . . , xn \u2208 Xn, while minimizing the mutual information between the selected features and the original SpRL features, \u03c6word(\u00b7).\nThe method introduced in this section requires a scalar random variable, r, as target output. However, the target output of SemEval is a 3-dimensional one-hot vector indicating the spatial roles, i.e. sp, tr and lm. Therefore, we convert this binary number with 3 binary digits into a decimal number, i.e. a scalar.\nThe mutual information is given by:\nI(x1, . . . , xn; r) = H(x1, . . . , xn) +H(r)\u2212H(x1, . . . , xn, r)\n(11)\nwhere n is the arbitrary number of selected features and H(.) represents the entropy of a set of random variables, given by:\nH(x1, . . . , xn) = \u2212 \u222b X1 . . . \u222b Xn p(x1, . . . , xn) log p(x1, . . . , xn)dx1 . . . dxn\n(12)\nEven assuming a discrete approximation for the joint density, p(x1, . . . , xn), e.g. normalized histograms, the calculation of (12) for several random variables is computationally unfeasible. Therefore, we adopt an indirect approach by applying the principle of Max-Relevance and Min-Redundancy Peng et al. (2005). According to this principle it is possible to maximize (12) by jointly solving the following two problems:\nmax i1,...,in \u03c6(i1, . . . , in) (13)\nwhere in is the index of the n th selected feature, \u03c6(i1, . . . , in) = V (i1, . . . , in)\u2212 D(i1, . . . , in),\nV (i1, . . . , in) = 1\nn n\u2211 k=1 I(xik ; r), (14)\nD(i1, . . . , in) = 1\nn2 n\u2211 j=1 n\u2211 k=1 I(xij ;xik) (15)\nThe idea is to find the set of indexes, i1, . . . , in, that simultaneously maximize the relevance (14) and minimize the redundancy (15). Notice that this procedure requires the calculation of a matrix S \u2208 Rn\u00d7n whose the elements are the mutual information values I(xij ;xik). However, this is a naive approach, since this method doesn\u2019t take into account the redundancy between the embedding features and the original 8099 SpRL features of \u03c6word(\u00b7). The computational cost increases significantly by considering the whole problem. Let xSpRLk be the kth original feature from \u03c6word(\u00b7), then the complete problem can be modeled as:\nmax i1,...,in \u03c6(i1, . . . , in) (16)\nV (i1, . . . , in) = 1\nn+m ( n\u2211 k=1 I(xik ; r)+\nm\u2211 j=1 I(xSpRLij ; r)\n , (17)\nD(i1, . . . , in) = 1\nn2 + nm+m2  n\u2211 j=1 n\u2211 k=1 I(xij ;xik)+\nn\u2211 l=1 m\u2211 z=1 I(xil ;x SpRL iz ) + m\u2211 w=1 m\u2211 q=1 I(xSpRLiw ;x SpRL iq )\n) (18)\nwhere m is the number of original features from \u03c6word(\u00b7). Fortunately, the terms \u2211m j=1 I(x SpRL ij ; y) and \u2211m w=1 \u2211m q=1 I(x SpRL iw\n;xSpRLiq ) are constant in relation to the manipulated indexes, i.e. the mutual information between the original SpRL features and the output, as well as the mutual information between pairs of SpRL features, don\u2019t matter for this optimization problem, alleviating the computational cost. Therefore, (16)-(18) can be simplified as follows:\nmax i1,...,in \u03c6(i1, . . . , in) (19)\nV (i1, . . . , in) = 1\nn n\u2211 k=1 I(xik ; r), (20)\nD(i1, . . . , in) = 1\nn2 + nm  n\u2211 j=1 n\u2211 k=1 I(xij ;xik)+\nn\u2211 l=1 m\u2211 z=1 I(xil ;x SpRL iz )\n) (21)\nThe optimization problem (19)-(21) requires the calculation of a matrix with the pairwise mutual information values of dimension n\u00d7m, in the place of the m \u00d7 m matrix required by (18). In our case it means a computational effort around 100 times smaller.\nWe solved (19)-(21) by slightly adapting the Feature Selector based on Genetic Algorithm and Information Theory3 Ludwig & Nunes (2010)."}, {"heading": "5.2 Maximizing the F1", "text": "After having the selected features from the embedding concatenated with the original SpRL features, we train an MLP on SemEval annotated data to predict\n3http://www.mathworks.com/matlabcentral/\nthe spatial role of the words. The adopted MLP has a single sigmoid hidden layer and a linear output layer.\nOne of the issues that we observe in applying MLP trained with MSE on SpRL data is the unbalanced relation between precision and recall that worsens with the use of the embedding, resulting in damage on the F1. This issue is usually solved by manipulating the threshold; however, a larger gain on F1 can be obtained by manipulating all the parameters of the output layer. Therefore, we propose a fine-tuning of the output layer, by maximizing an approximation of F1 squared. We start by analyzing the simplest approach:\nmax w,b\nF12 (22)\nwhere w and b are the adjustable parameters of the linear output layer of the MLP. F1 is function of the true positive (TP ) and true negative examples (TN), as follows:\nF1 = 2TP\nN + TP \u2212 TN (23)\nTP = 1\n2 Np\u2211 i=1 (1 + \u03d5(wxpi + b)) (24)\nTN = 1\n2 Nn\u2211 j=1 ( 1\u2212 \u03d5(wxnj + b) ) (25)\nwhere \u03d5(\u00b7) = sign(\u00b7) returns 1 or \u22121 according to the sign of the argument, N is the number of examples, Np and Nn are the number of positive and negative examples, respectively, and xpi and x n j are the outputs of the hidden layer of the MLP (i.e. the inputs of the output layer) for the ith positive example and the jth negative example, respectively.\nUnfortunately, the sign function is not suitable for gradient based optimization methods; therefore, we approximate this function by hyperbolic tangent, yielding the approximate \u02dcTP , \u02dcTN and F\u03031, whose relation is given by:\nF\u03031 = 2 \u02dcTP\nN + \u02dcTP \u2212 \u02dcTN (26)\nTo derive a lower bound on F1 as a function of F\u03031, \u02dcTP and \u02dcTN , let us analyze the bounds on the difference (sign(\u00b7)\u2212 tanh(\u00b7)) by one-sided limit:\nlim v\u21920\u2212\n(sign(v)\u2212 tanh(v)) = \u22121 (27)\nlim v\u21920+\n(sign(v)\u2212 tanh(v)) = 1 (28)\nFrom (27), (28) and (24) we can derive bounds on TP as a function of its approximation \u02dcTP :\n\u02dcTP \u2212 Np 2 < TP < \u02dcTP + Np 2\n(29)\nNotice that we can relax these bounds by substituting Np by the largest value between Np and Nn, henceforward called Nl, i.e.:\n\u02dcTP \u2212 Nl 2 < TP < \u02dcTP + Nl 2\n(30)\nSimilar bounds can be derived for TN :\n\u02dcTN \u2212 Nl 2 < TN < \u02dcTN + Nl 2\n(31)\nBy substituting the lower bounds of (30) and (31) into (23) and using (26) we can derive a lower bound on F1:\nF1 > (1\u2212 Nl 2 \u02dcTP )F\u03031 (32)\nFrom (32) we conclude that it is desirable to have (1\u2212 Nl 2T\u0303P ) > 0, since this is a necessary condition (but not sufficient) to have the lower bound of F1 increasing together with F\u03031. Therefore, we propose the constrained optimization problem:\nmax w,b,\u03b6\nF\u03031 2 + C\u03b6\ns.t. \u02dcTP > Nl 2 + \u03b6\n(33)\nwhere \u03b6 is a slack variable. To solve this optimization problem we need the following derivatives:\n\u2202F\u03031 2\n\u2202w = 2F\u03031\n( 2 \u2202 \u02dcTP\n\u2202w\n( N + \u02dcTP \u2212 \u02dcTN )\u22121 +\n2 \u02dcTP ( \u02dcTN \u2212 \u02dcTP \u2212N )\u22122(\u2202 \u02dcTN\n\u2202w \u2212 \u2202\n\u02dcTP\n\u2202w\n)) (34)\n\u2202F\u03031 2\n\u2202b = 2F\u03031\n( 2 \u2202 \u02dcTP\n\u2202b\n( N + \u02dcTP \u2212 \u02dcTN )\u22121 +\n2 \u02dcTP ( \u02dcTN \u2212 \u02dcTP \u2212N )\u22122(\u2202 \u02dcTN\n\u2202b \u2212 \u2202\n\u02dcTP\n\u2202b\n)) (35)\n\u2202 \u02dcTP\n\u2202w =\n1\n2 Np\u2211 i=1 (tanh\u2032(wxpi + b)x p i ) (36)\n\u2202 \u02dcTN\n\u2202w = \u22121 2 Nn\u2211 j=1 ( tanh\u2032(wxnj + b)x n j ) (37)\n\u2202 \u02dcTP\n\u2202b =\n1\n2 Np\u2211 i=1 (tanh\u2032(wxpi + b)) (38)\n\u2202 \u02dcTN\n\u2202b = \u22121 2 Nn\u2211 j=1 ( tanh\u2032(wxnj + b) ) (39)\nwhere tanh\u2032(\u00b7) = 1\u2212 tanh2(\u00b7)."}, {"heading": "6 Experimental Settings", "text": "In this section our methods are evaluated by means of experiments in the SemEval-2013 benchmark data set, the Task 3, SpaceEval. We start by evaluating the embedding model on COCO, then we evaluate the contribution of the spatial-specific embedding in the multiclass classification of words into spatial roles and finally in the structured prediction of spatial triplets by using the algorithm of Kordjamshidi & Moens (2015)."}, {"heading": "6.1 Evaluating the embedding model on COCO", "text": "This subsection reports the performance indexes of the deep model described in Section 3.1 in predicting our annotation on the COCO testing data. The model was trained on 135015 captions and evaluated on a test set composed by 67505 captions.\nAccording to our experiments, the deep model has its best performance on the test data when having a 200-dimensional word embedding and a 300- dimensional sentence embedding, meaning that the embedding matrix has dimension 8000\u00d7200 and the LSTM receives a 200-dimensional vector and outputs a 300-dimensional vector, i.e. Nw = 200 and Ns = 300. The best setup for the MLP is 300\u00d7 250\u00d7 200\u00d7 185, i.e. with two sigmoidal hidden layers containing 250 and 200 neurons, respectively.\nThe performance indexes on the test data are provided for the model trained with mean squared error (MSE) and our Hinge-like multiclass loss (9) for the sake of comparison, as shown in Table 1.\nAs can be seen in Table 1, the model has a better performance in predicting the spatial relation, since it can assume only 3 discrete values, while the visual\nobject can assume 91 discrete values. The Hinge-like multiclass loss presents slightly better performance indexes.\nAfter training, the word embedding extracted from We enables the clustering of words into classes, as can be seen in Fig.4, which shows the PCA projection on the two first eigen directions of the embedding representing visual objects.\nFig.5 shows the PCA projection of the sentence-level embedding of four pairs of sentences describing the same scene in different manners, except for a pair of sentences whose spatial meaning was changed, in order to check the sensitivity of the model to the spatial information, i.e. (\u201cA red book on the top of a bottle of wine\u201d, \u201cA bottle of wine on top of a red book\u201d), which was plotted with a larger dispersion than the other pairs."}, {"heading": "6.2 Multiclass classification of word roles on SemEval", "text": "In this set of experiments on multiclass classification an MLP is trained to classify words into spatial roles. The adopted MLP has a single sigmoid hidden layer with 10 neurons and a linear output layer with 3 neurons, which encode the output (i.e. the predicted class: sp, tr, lm or no spatial role) in one-hot vector\nstyle. The MLP receives as input the original features, \u03c6word(\u00b7), extracted by the same feature function as used by the SpRL algorithm of Kordjamshidi & Moens (2015). These features are concatenated with features from the VIEW, in order to access the gains of using the embedding.\nThe MLP was trained using 15092 sentences and evaluated using 3711 sentences which compose the train and test data sets of SemEval, Task 3, SpaceEval. The results are summarized in Tables 2 and 3. The gains obtained by using\nVIEW are improved by selecting only 100 complementary features from the embedding employing the method explained in Section 5.1, as can be seen in Table 4. Table 5 presents the results after the application of the fine tuning method\nintroduced in Section 5.2 to improve the F1 measure. To compare the perfor-\nmance of VIEW with the usual Word2Vec embedding Mikolov et al. (2013), we trained a skip-gram model4 on the same COCO captions as we trained VIEW (but without visual information) and concatenated it to the original SpRL features to generate the results of Table 6. The VIEW yields performance gains in\npredicting all the spatial roles. These gains are improved by the application of the methods described in Sections 5.1 and 5.2."}, {"heading": "6.3 Structured prediction of spatial triplets on SemEval", "text": "In this set of experiments on structured prediction we used the original SpRL algorithm of Kordjamshidi & Moens (2015) not only to predict the spatial role of the words, but also to compose words into triplets (sp, tr, lm), i.e. the structured\n4https://code.google.com/p/word2vec/\noutput. As explained in Section 2, the algorithm uses descriptive vectors of words, \u03c6word(\u00b7), and pairs of words, \u03c6pair(\u00b7, \u00b7). The VIEW is concatenated only to \u03c6word(\u00b7), having a secondary role in this set of experiments.\nThe VIEW yields performance gains in classifying words into the roles sp and lm, as can be seen in Tables 7 and 8, which summarize the performance indexes using only the original features from \u03c6word(\u00b7) and \u03c6pair(\u00b7, \u00b7) and using \u03c6word(\u00b7) concatenated with the VIEW. Table 9 shows the performance of the\nusual Word2Vec embedding Mikolov et al. (2013) concatenated to the original SpRL features, using the SpRL algorithm with the same setup assumed for the experiments with VIEW."}, {"heading": "7 Conclusion", "text": "This paper introduces a new approach in transferring spatial knowledge from multimodal data through the VIEW. The experiments provide evidence for the effectiveness of our method in transferring information useful in improving the performance of SpRL algorithms, specially in classifying words into spatial roles.\nThe experiments also provide evidence for the effectiveness of the algorithms for complementary feature selection and F1 maximization, introduced in Sections 5.1 and 5.2, in improving the gains obtained by using the VIEW.\nAs for future work we aim at developing a method for F1 optimization in the structured prediction setting by extending the work Joachims (2005) for the structured classification setting.\nWe believe that the results reported in this paper may improve with the increasing amount of annotated data. Notice that despite having a large cardinality, the COCO data set has a small variety of visual objects in its gold standard, i.e. it has only 91 object categories (including the super-categories)."}], "references": [{"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A support vector method for multivariate performance measures", "author": ["Joachims", "Thorsten"], "venue": "In Proceedings of the 22Nd International Conference on Machine Learning,", "citeRegEx": "Joachims and Thorsten.,? \\Q2005\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2005}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy", "Andrej", "Joulin", "Armand", "Li", "Fei Fei F"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Global machine learning for spatial ontology population", "author": ["Kordjamshidi", "Parisa", "Moens", "Marie-Francine"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Kordjamshidi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kordjamshidi et al\\.", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Novel maximum-margin training algorithms for supervised neural networks", "author": ["Ludwig", "Oswaldo", "Nunes", "Urbano"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Ludwig et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ludwig et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Srivastava", "Nitish", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Taskar", "Ben", "Chatalbashev", "Vassil", "Koller", "Daphne", "Guestrin", "Carlos"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al.", "startOffset": 136, "endOffset": 159}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images.", "startOffset": 136, "endOffset": 359}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images. Similar to the work Kiros et al. (2014) our model learns embedding from multimodal data and applies LSTM to process textual information.", "startOffset": 136, "endOffset": 680}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images. Similar to the work Kiros et al. (2014) our model learns embedding from multimodal data and applies LSTM to process textual information. However, unlike most of the works on multimodal representation learning, which jointly map visual and textual information into a common embedding space, our work aims at providing embeddings only for words, but encoding spatial information extracted from the image annotations. The idea is to learn VIEW by pipelining an embedding layer into a deep architecture trained by back propagation to predict the spatial arrangement between the visual objects annotated in the pictures, given the respective textual descriptions. In this sense, unlike Karpathy et al. (2014) and Kiros et al.", "startOffset": 136, "endOffset": 1344}, {"referenceID": 2, "context": "dinov (2012), which uses deep Boltzmann machines for representing joint multimodal probability distributions over images and sentences, Karpathy et al. (2014), which then embed fragments of images (objects) and fragments of sentences (dependency tree relations) into a common space for bidirectional retrieval of images and sentences, and Kiros et al. (2014), which unify joint imagetext embedding models with multimodal neural language models to rank images and sentences (as well as to generate descriptions for images) using a long short term memory (LSTM) network to process text and deep convolutional network (CNN) to process images. Similar to the work Kiros et al. (2014) our model learns embedding from multimodal data and applies LSTM to process textual information. However, unlike most of the works on multimodal representation learning, which jointly map visual and textual information into a common embedding space, our work aims at providing embeddings only for words, but encoding spatial information extracted from the image annotations. The idea is to learn VIEW by pipelining an embedding layer into a deep architecture trained by back propagation to predict the spatial arrangement between the visual objects annotated in the pictures, given the respective textual descriptions. In this sense, unlike Karpathy et al. (2014) and Kiros et al. (2014), we don\u2019t need a CNN, because the spatial information which is relevant for our purpose is provided directly by the position of the bounding boxes containing the visual objects annotated in the images, as detailed in Section 3.", "startOffset": 136, "endOffset": 1368}, {"referenceID": 11, "context": "In this work we apply the SpRL algorithm developed for the work Kordjamshidi & Moens (2015), which models this problem as a structured prediction task Taskar et al. (2005), that is, it jointly recognizes the spatial relation and its", "startOffset": 151, "endOffset": 172}, {"referenceID": 6, "context": "The Microsoft COCO data set Lin et al. (2014) is a collection of images featuring complex everyday scenes which contain common visual objects in their natural context.", "startOffset": 28, "endOffset": 46}, {"referenceID": 6, "context": "Figure 1: This figure shows an image from COCO with annotated bounding boxes and captions Lin et al. (2014).", "startOffset": 90, "endOffset": 108}, {"referenceID": 6, "context": "7 instances per image (see Section 5 of Lin et al. (2014)).", "startOffset": 40, "endOffset": 58}, {"referenceID": 9, "context": "Therefore, we adopt an indirect approach by applying the principle of Max-Relevance and Min-Redundancy Peng et al. (2005). According to this principle it is possible to maximize (12) by jointly solving the following two problems: max i1,.", "startOffset": 103, "endOffset": 122}, {"referenceID": 8, "context": "mance of VIEW with the usual Word2Vec embedding Mikolov et al. (2013), we trained a skip-gram model on the same COCO captions as we trained VIEW (but without visual information) and concatenated it to the original SpRL features to generate the results of Table 6.", "startOffset": 48, "endOffset": 70}, {"referenceID": 8, "context": "usual Word2Vec embedding Mikolov et al. (2013) concatenated to the original SpRL features, using the SpRL algorithm with the same setup assumed for the experiments with VIEW.", "startOffset": 25, "endOffset": 47}], "year": 2016, "abstractText": "This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the F1 measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.", "creator": "LaTeX with hyperref package"}}}