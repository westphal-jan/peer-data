{"id": "1603.08705", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms", "abstract": "tricia In pamper this paper, we describe 49-40 ROOT13, segoe a french-trained supervised plosive system kawasaki for the tusken classification maylene of hypernyms, hassidic co - kirwan hyponyms gortat and notches random haba words. guildford The artemije system wanga relies on 161st a Random sandbaggers Forest thunderwing algorithm and consolidating 13 milegi unsupervised corpus - based westmead features. sardo We 2,039 evaluate oxime it kristie with o'gill a prakasam 10 - hajji fold mcelheny cross lydian validation rantoul on 9, 600 partner-in-crime pairs, equally jovo distributed belanger among the three 12-6 classes and involving rera several Parts - Of - 2085 Speech (periplasm i. 106.83 e. earth-moving adjectives, .690 nouns boost and carentan verbs ). When cmap all rauparaha the symphysis classes bajevic are peregrination present, 114.55 ROOT13 achieves 3.3-mile an 51b F1 prion score gizmos of 88. germanna 3% , cryns against first-wave a uptakes baseline of lusitanica 57. icea 6% (mordehai vector cosine ). When panzerdivision the classification perisphere is pencader binary, nault ROOT13 928 achieves herse the following vautin results: chytrid hypernyms - hiett co - bigwigs hyponyms (cemac 93. darra 4% exulting vs. prgf 60. lovers 2% ), dorstenia hypernymsrandom (1682 92. matkovic 3% niyitegeka vs. treasonable 65. 5%) garnsey and co - hyponyms - dmsa random (a-row 97. orientals 3% wkcr vs. simpfendorfer 81. 5% ). tagetes Our results lamivudine are tahir competitive with khyam stateof - the - tribune-review art models.", "histories": [["v1", "Tue, 29 Mar 2016 10:05:05 GMT  (151kb)", "http://arxiv.org/abs/1603.08705v1", "in AAAI 2016"]], "COMMENTS": "in AAAI 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["enrico santus", "tin-shing chiu", "qin lu", "alessandro lenci", "chu-ren huang"], "accepted": true, "id": "1603.08705"}, "pdf": {"name": "1603.08705.pdf", "metadata": {"source": "CRF", "title": "ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms", "authors": ["Enrico Santus", "Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang"], "emails": ["esantus@gmail.com,", "cstschiu@comp.polyu.edu.hk,", "churen.huang}@polyu.edu.hk", "alessandro.lenci@ling.unipi.it"], "sections": [{"heading": "Introduction and Related Work", "text": "Distinguishing hypernyms (e.g. dog-animal) from cohyponyms (e.g. dog-cat) and, in turn, discriminating them from random words (e.g. dog-fruit) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment (Geffet and Dagan, 2005). Cohyponymy (or coordination), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on (Weeds et al., 2014). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers\u2019 ability in such discrimination, generally achieving promising results (Weeds et al., 2014; Rimmel,\nCopyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n2014; Geffet and Dagan, 2005). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper, we propose a supervised method, based on a Random Forest algorithm and 13 corpus-based features. In our evaluation, carried out using the 10-fold cross validation on 9,600 pairs, we achieved an accuracy of 88.3% when the three classes are present, and of 92.3% and 97.3% when only two classes are present. Such results are competitive with the state-of-the-art (Weeds et al., 2014)."}, {"heading": "Method and Evaluation", "text": "ROOT13 uses the Random Forest algorithm implemented in Weka (Breiman, 2001), with the default settings. It relies on 13 features that are described below. Each of them is automatically extracted from a window-based Vector Space Model (VSM), built on a combination of ukWaC and WaCkypedia corpora (around 2.7 billion words) and recording word co-occurrences within the 5 nearest content words to the left and right of each target. FEATURES. The feature set was designed to identify several distributional properties characterizing the terms in the pairs. On top of the standard features (e.g. vector cosine, co-occurrence and frequencies), we have added several features capturing the generality of the terms and of their contexts1, plus two unsupervised measures for capturing similarity (Santus et al., 2014b-c). All the features are normalized in the range 0-1: \u2022 Cos: vector cosine (Turney and Pantel, 2010); \u2022 Cooc: co-occurrence frequency; \u2022 Freq 1, 2: two features storing the frequency the terms; \u2022 Entr 1, 2: two features storing the entropy of the terms;\n1 Generality is measured as for Santus et al. (2014a).\n\u2022 Shared: extent of the intersection between the top 1k most mutually related contexts of the two terms2; \u2022 APSyn: for every context in the intersection between the top 1k most mutually related contexts of the two terms, this measure adds 1, divided by its average rank (Santus et al. 2014b-c); \u2022 Diff Freqs: difference between the terms frequencies; \u2022 Diff Entrs: difference between the terms entropies3; \u2022 C-Freq 1, 2: two features storing the average frequency\namong the top 1k most mutually related contexts for each term; \u2022 C-Entr 1, 2: two features, storing the average entropy among the top 1k most mutually related contexts for each term. DATASET. We have used 9,600 pairs, randomly extracted from three datasets: Lenci/Benotto (Santus et al., 2014b), BLESS (Baroni and Lenci, 2011) and EVALution (Santus et al., 2015). The pairs are equally distributed among the three classes (i.e. hypernyms, co-hyponyms and random words) and involve several Parts-Of-Speech. TASKS. Four classification tasks have been carried out. One involving all classes and three tasks involving only two of them. F1 score on a 10-fold cross validation was chosen as accuracy measure. BASELINE. Vector cosine is used as baseline. It achieves a reasonable accuracy, which is anyway far below the results obtained by our model. As it will be shown below, our model does not benefit from its use. RESULTS. Table 1 describes the features\u2019 contributions in the four classification tasks. As can be seen, when all the classes are involved, every feature contributes for an increment between 0.4% and 4.8%, except for the feature Shared, whose contribution is +12.5%. The relevance of this feature is confirmed also in the other three tasks. Interestingly, instead, the vector cosine does not contribute to our score. It instead penalizes it in three tasks out of four. The only task in which it actually contributes for 0.1% is the discrimination between co-hyponyms and randoms, which is its main function."}, {"heading": "Conclusions", "text": "In this paper, we have described ROOT13, a classifier for hypernyms, co-hyponyms and random words. The classifier, based on the Random Forest algorithm, uses only 13 unsupervised corpus-based features, which have been described and their contribution reported. Our classifier is competitive with the state-of-the-art (Weeds et al., 2014). In a second run of tests, we have noticed the Levy et al. (2015)\u2019s effect, that is the classification of switched hypernyms as hypernyms (e.g. dog-vehicle, car-animal). How2 Ranking is calculated with Local Mutual Information (Evert, 2005) 3 Entropy was calculated using the formula in Santus et al. (2014a)\never, we were able to remove it \u2013 without any sensible loss in accuracy \u2013 by training the model also on switched hypernyms labeled as randoms.4"}], "references": [{"title": "How we BLESSed distributional semantic evaluation", "author": ["M. Baroni", "A. Lenci"], "venue": "EMNLP 2011.", "citeRegEx": "Baroni and Lenci,? 2011", "shortCiteRegEx": "Baroni and Lenci", "year": 2011}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning. 45(1).", "citeRegEx": "Breiman,? 2001", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "The Statistics of Word Cooccurrences: Word Pairs and Collocations", "author": ["S. Evert"], "venue": "Dissertation, University of Stuttgart.", "citeRegEx": "Evert,? 2005", "shortCiteRegEx": "Evert", "year": 2005}, {"title": "The Distributional Inclusion Hypotheses and Lexical Entailment", "author": ["M. Geffet", "I. Dagan"], "venue": "ACL 2005.", "citeRegEx": "Geffet and Dagan,? 2005", "shortCiteRegEx": "Geffet and Dagan", "year": 2005}, {"title": "Do Supervised Distributional Methods Really Learn Lexical Inference Relations", "author": ["O. Levy", "S. Remus", "C. Biemann", "I. Dagan"], "venue": "NAACL 2015.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "The Big Book of Concepts", "author": ["Murphy", "Gregory L.."], "venue": "The MIT Press, Cambridge, MA.", "citeRegEx": "Murphy and L..,? 2002", "shortCiteRegEx": "Murphy and L..", "year": 2002}, {"title": "Distributional Lexical Entailment by Topic Coherence", "author": ["L. Rimmel"], "venue": "EACL 2014..", "citeRegEx": "Rimmel,? 2014", "shortCiteRegEx": "Rimmel", "year": 2014}, {"title": "Chasing Hypernyms in Vector Spaces with Entropy", "author": ["E. Santus", "A. Lenci", "Q. Lu", "Schulte im Walde", "S.."], "venue": "EACL 2014.", "citeRegEx": "Santus et al\\.,? 2014a", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Unsupervised Antonym-Synonym Discrimination in Vector Space", "author": ["E. Santus", "Q. Lu", "A. Lenci", "Huang", "C-R."], "venue": "CLICIT 2014.", "citeRegEx": "Santus et al\\.,? 2014b", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Taking Antonymy Mask off in Vector Space", "author": ["E. Santus", "Q. Lu", "A. Lenci", "Huang", "C-R."], "venue": "PACLIC 2014.", "citeRegEx": "Santus et al\\.,? 2014c", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Articial Intelligence Research, Vol. 37. 141-188.", "citeRegEx": "Turney and Pantel,? 2010", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Learning to Distinguish Hypernyms and Co-Hyponyms", "author": ["J. Weeds", "D. Clarke", "J. Reffin", "D. Weir", "B. Keller"], "venue": "Proceedings of COLING 2014.", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment (Geffet and Dagan, 2005).", "startOffset": 200, "endOffset": 224}, {"referenceID": 11, "context": "Cohyponymy (or coordination), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014).", "startOffset": 149, "endOffset": 169}, {"referenceID": 11, "context": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on (Weeds et al., 2014).", "startOffset": 211, "endOffset": 231}, {"referenceID": 11, "context": "Such results are competitive with the state-of-the-art (Weeds et al., 2014).", "startOffset": 55, "endOffset": 75}, {"referenceID": 3, "context": "2014; Geffet and Dagan, 2005). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al.", "startOffset": 6, "endOffset": 174}, {"referenceID": 3, "context": "2014; Geffet and Dagan, 2005). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.", "startOffset": 6, "endOffset": 206}, {"referenceID": 1, "context": "ROOT13 uses the Random Forest algorithm implemented in Weka (Breiman, 2001), with the default settings.", "startOffset": 60, "endOffset": 75}, {"referenceID": 10, "context": "All the features are normalized in the range 0-1: \u2022 Cos: vector cosine (Turney and Pantel, 2010); \u2022 Cooc: co-occurrence frequency; \u2022 Freq 1, 2: two features storing the frequency the terms; \u2022 Entr 1, 2: two features storing the entropy of the terms;", "startOffset": 71, "endOffset": 96}, {"referenceID": 7, "context": "1 Generality is measured as for Santus et al. (2014a).", "startOffset": 32, "endOffset": 54}, {"referenceID": 8, "context": "We have used 9,600 pairs, randomly extracted from three datasets: Lenci/Benotto (Santus et al., 2014b), BLESS (Baroni and Lenci, 2011) and EVALution (Santus et al.", "startOffset": 80, "endOffset": 102}, {"referenceID": 0, "context": ", 2014b), BLESS (Baroni and Lenci, 2011) and EVALution (Santus et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 11, "context": "Our classifier is competitive with the state-of-the-art (Weeds et al., 2014).", "startOffset": 56, "endOffset": 76}, {"referenceID": 4, "context": "In a second run of tests, we have noticed the Levy et al. (2015)\u2019s effect, that is the classification of switched hypernyms as hypernyms (e.", "startOffset": 46, "endOffset": 65}, {"referenceID": 2, "context": "2 Ranking is calculated with Local Mutual Information (Evert, 2005) 3 Entropy was calculated using the formula in Santus et al.", "startOffset": 54, "endOffset": 67}, {"referenceID": 2, "context": "2 Ranking is calculated with Local Mutual Information (Evert, 2005) 3 Entropy was calculated using the formula in Santus et al. (2014a) ever, we were able to remove it \u2013 without any sensible loss in accuracy \u2013 by training the model also on switched hypernyms labeled as randoms.", "startOffset": 55, "endOffset": 136}], "year": 2015, "abstractText": "In this paper, we describe ROOT13, a supervised system for the classification of hypernyms, co-hyponyms and random words. The system relies on a Random Forest algorithm and 13 unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-OfSpeech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%, against a baseline of 57.6% (vector cosine). When the classification is binary, ROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%), hypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Our results are competitive with stateof-the-art models. Introduction and Related Work Distinguishing hypernyms (e.g. dog-animal) from cohyponyms (e.g. dog-cat) and, in turn, discriminating them from random words (e.g. dog-fruit) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment (Geffet and Dagan, 2005). Cohyponymy (or coordination), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on (Weeds et al., 2014). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers\u2019 ability in such discrimination, generally achieving promising results (Weeds et al., 2014; Rimmel, Copyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 2014; Geffet and Dagan, 2005). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper, we propose a supervised method, based on a Random Forest algorithm and 13 corpus-based features. In our evaluation, carried out using the 10-fold cross validation on 9,600 pairs, we achieved an accuracy of 88.3% when the three classes are present, and of 92.3% and 97.3% when only two classes are present. Such results are competitive with the state-of-the-art (Weeds et al., 2014). Method and Evaluation ROOT13 uses the Random Forest algorithm implemented in Weka (Breiman, 2001), with the default settings. It relies on 13 features that are described below. Each of them is automatically extracted from a window-based Vector Space Model (VSM), built on a combination of ukWaC and WaCkypedia corpora (around 2.7 billion words) and recording word co-occurrences within the 5 nearest content words to the left and right of each target. FEATURES. The feature set was designed to identify several distributional properties characterizing the terms in the pairs. On top of the standard features (e.g. vector cosine, co-occurrence and frequencies), we have added several features capturing the generality of the terms and of their contexts, plus two unsupervised measures for capturing similarity (Santus et al., 2014b-c). All the features are normalized in the range 0-1: \u2022 Cos: vector cosine (Turney and Pantel, 2010); \u2022 Cooc: co-occurrence frequency; \u2022 Freq 1, 2: two features storing the frequency the terms; \u2022 Entr 1, 2: two features storing the entropy of the terms; 1 Generality is measured as for Santus et al. (2014a). \u2022 Shared: extent of the intersection between the top 1k most mutually related contexts of the two terms; \u2022 APSyn: for every context in the intersection between the top 1k most mutually related contexts of the two terms, this measure adds 1, divided by its average rank (Santus et al. 2014b-c); \u2022 Diff Freqs: difference between the terms frequencies; \u2022 Diff Entrs: difference between the terms entropies; \u2022 C-Freq 1, 2: two features storing the average frequency among the top 1k most mutually related contexts for each term; \u2022 C-Entr 1, 2: two features, storing the average entropy among the top 1k most mutually related contexts for each term. DATASET. We have used 9,600 pairs, randomly extracted from three datasets: Lenci/Benotto (Santus et al., 2014b), BLESS (Baroni and Lenci, 2011) and EVALution (Santus et al., 2015). The pairs are equally distributed among the three classes (i.e. hypernyms, co-hyponyms and random words) and involve several Parts-Of-Speech. TASKS. Four classification tasks have been carried out. One involving all classes and three tasks involving only two of them. F1 score on a 10-fold cross validation was chosen as accuracy measure. BASELINE. Vector cosine is used as baseline. It achieves a reasonable accuracy, which is anyway far below the results obtained by our model. As it will be shown below, our model does not benefit from its use. RESULTS. Table 1 describes the features\u2019 contributions in the four classification tasks. As can be seen, when all the classes are involved, every feature contributes for an increment between 0.4% and 4.8%, except for the feature Shared, whose contribution is +12.5%. The relevance of this feature is confirmed also in the other three tasks. Interestingly, instead, the vector cosine does not contribute to our score. It instead penalizes it in three tasks out of four. The only task in which it actually contributes for 0.1% is the discrimination between co-hyponyms and randoms, which is its main function.", "creator": "Word"}}}