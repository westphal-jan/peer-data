{"id": "1603.08776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "COCO: The Experimental Procedure", "abstract": "marinac We bagnoud present imminence an experimental a51 setup destron and procedure for benchmarking numerical beart optimization kamber algorithms ngam in masci a 42.48 black - guthrum box scenario. This 63.05 procedure can anthropological be applied anastassia with brownstone the 7.90 COCO benchmarking platform. We duggal describe initialization of stairwells and input to the parkus algorithm and flattops touch sycamore upon remond the relevance of zuber termination demonic and 62.21 restarts. We haryana introduce the 114.75 concept memorializing of fabisch recommendations wieniawski for benchmarking with mulhearn COCO. Recommendations 24-percent detach the solutions danisco used endgame in function calls willans from sesame the mons anytime lindt performance assessment of the algorithm.", "histories": [["v1", "Tue, 29 Mar 2016 14:10:14 GMT  (20kb,D)", "http://arxiv.org/abs/1603.08776v1", null], ["v2", "Thu, 19 May 2016 11:58:22 GMT  (20kb,D)", "http://arxiv.org/abs/1603.08776v2", "ArXiv e-prints,arXiv:1603.08776"]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["nikolaus hansen", "tea tusar", "olaf mersmann", "anne auger", "dimo brockhoff"], "accepted": false, "id": "1603.08776"}, "pdf": {"name": "1603.08776.pdf", "metadata": {"source": "CRF", "title": "COCO: The Experimental Procedure", "authors": ["Nikolaus Hansen", "Tea Tusar", "Olaf Mersmann", "Anne Auger", "Dimo Brockhoff"], "emails": [], "sections": [{"heading": null, "text": "Contents"}, {"heading": "1 Introduction 2", "text": "1.1 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2"}, {"heading": "2 Conducting the Experiment 2", "text": "2.1 Initialization and Input to the Algorithm . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Budget, Termination Criteria, and Restarts . . . . . . . . . . . . . . . . . . . . . . 4"}, {"heading": "3 Parameter Setting and Tuning of Algorithms 4", "text": ""}, {"heading": "4 Recommendations 5", "text": "5 Time Complexity Experiment 5\nar X\niv :1\n60 3.\n08 77\n6v 1\n[ cs\n.A I]\n2 9"}, {"heading": "1 Introduction", "text": "Based on [HAN2009] and [HAN2010], we describe a comparatively simple experimental set-up for black-box optimization benchmarking. We recommend to use this procedure within the COCO platform [HAN2016co]. 1\nOur central measure of performance, to which the experimental procedure is adapted, is the number of evaluations of the objective function to reach a certain solution quality (function value or \ud835\udc53 -value or indicator value), also denoted as runtime."}, {"heading": "1.1 Terminology", "text": "function We talk about a function as a parametrized mapping R\ud835\udc5b \u2192 R\ud835\udc5a with scalable input space, that is, \ud835\udc5b is not (yet) determined, and usually \ud835\udc5a \u2208 {1, 2}. Functions are parametrized such that different instances of the \u201csame\u201d function are available, e.g. translated or shifted versions.\nproblem We talk about a problem, coco_problem_t, as a specific function instance on which the optimization algorithm is run. Specifically, a problem can be described as the triple (dimension, function, instance). A problem can be evaluated and returns an \ud835\udc53 -value or -vector. In the context of performance assessment, a target \ud835\udc53 - or indicator-value is attached to each problem. That is, a target value is added to the above triple to define a single problem in this case.\nruntime We define runtime, or run-length [HOO1998] as the number of evaluations conducted on a given problem, also referred to as number of function evaluations. Our central performance measure is the runtime until a given target value is hit [CocoPerf].\nsuite A test- or benchmark-suite is a collection of problems, typically between twenty and a hundred, where the number of objectives \ud835\udc5a is fixed."}, {"heading": "2 Conducting the Experiment", "text": "The optimization algorithm to be benchmarked is run on each problem of the given test suite once. On each problem, the very same algorithm with the same parameter setting, the same initialzation procedure, the same budget, the same termination and/or restart criteria etc. is used. There is no prescribed minimal or maximally allowed budget. The longer the experiment, the more data are available to assess the performance accurately. See also Section Budget, Termination Criteria, and Restarts.\n1 The COCO platform provides several (single and bi-objective) test suites with a collection of black-box optimization problems of different dimensions to be minimized. COCO automatically collects the relevant data to display the performance results after a post-processing is applied."}, {"heading": "2.1 Initialization and Input to the Algorithm", "text": "An algorithm can use the following input information from each problem. For initialization:\nInput and output dimensions as a defining interface to the problem, specifically:\n\u2022 The search space (input) dimension via coco_problem_get_dimension,\n\u2022 The number of objectives via coco_problem_get_number_of_objectives, which is the \u201coutput\u201d dimension of coco_evaluate_function. All functions of a single benchmark suite have the same number of objectives, currently either one or two.\n\u2022 The number of constraints via coco_problem_get_number_of_constraints, which is the \u201coutput\u201d dimension of coco_evaluate_constraint. All problems of a single benchmark suite have either no constraints, or one or more constraints.\nSearch domain of interest defined from coco_problem_get_largest_values_of_interest and coco_problem_get_smallest_values_of_interest. The optimum (or each extremal solution of the Pareto set) lies within the search domain of interest. If the optimizer operates on a bounded domain only, the domain of interest can be interpreted as lower and upper bounds 2.\nFeasible (initial) solution provided by coco_problem_get_initial_solution.\nThe initial state of the optimization algorithm and its parameters shall only be based on these input values. The initial algorithm setting is considered as part of the algorithm and must therefore follow the same procedure for all problems of the suite. The problem identifier or the positioning of the problem in the suite or any (other) known characteristics of the problem are not allowed as input to the algorithm, see also Section Parameter Setting and Tuning of Algorithms.\nDuring an optimization run, the following (new) information is available to the algorithm:\n1. The result, i.e. the \ud835\udc53 -value(s), from evaluating the problem at a given search point via coco_evaluate_function.\n2. The result from evaluating the constraints of the problem at a given search point via coco_evaluate_constraint.\n3. The result of coco_problem_final_target_hit, which can be used to terminate a run conclusively without changing the performance assessment in any way. Currently, if the number of objectives \ud835\udc5a > 1, this function returns always zero.\nThe number of evaluations of the problem and/or constraints are the search costs, also referred to as runtime, and used for the performance assessment of the algorithm. 3\n2 Note, however, that the Pareto set in the bi-objective case is not always guaranteed to lie in its entirety within the region of interest.\n3 coco_problem_get_evaluations(const coco_problem_t * problem) is a convenience function that returns the number of evaluations done on problem. Because this information is available to the optimization algorithm anyway, the convenience function might be used additionally."}, {"heading": "2.2 Budget, Termination Criteria, and Restarts", "text": "We consider the budget, termination criteria, and restarts to be part of the benchmarked algorithm. Algorithms with any budget of function evaluations are eligible. The choice of termination is a relevant part of the algorithm. On the one hand, allowing a larger number of function evaluations increases the chance to achieve better function values. On the other hand, a timely termination of a stagnating run can improve the performance, as these evaluations can be used more effectively. 4\nTo exploit a large number of function evaluations effectively, we encourage to use independent restarts 5, in particular for algorithms which terminate naturally within a comparatively small budget. Independent restarts do not change the central performance measure 6, however, they improve the reliability, comparability 7, precision, and \u201cvisibility\u201d of the measured results.\nMoreover, any multistart procedure (which relies on an interim termination of the algorithm) is encouraged. Multistarts may not be independent as they can feature a parameter sweep (e.g., increasing population size [HAR1999] [AUG2005]) or can be based on the outcome of the previous starts.\nAfter a multistart procedure has been established, a recommended procedure is to use a budget proportional to the dimension, \ud835\udc58 \u00d7 \ud835\udc5b, and run repeated experiments with increase \ud835\udc58, e.g. like 3, 10, 30, 100, 300, . . ., which is a good compromise between availability of the latest results and computational overhead.\nAn algorithm can be conclusively terminated if coco_problem_final_target_hit returns 1. 8 This saves CPU cycles without affecting the performance assessment, because there is no target left to hit."}, {"heading": "3 Parameter Setting and Tuning of Algorithms", "text": "Any tuning of algorithm parameters to the test suite should be described and the approximate overall number of tested parameter settings or algorithm variants and the approximate overall invested budget should be given.\nOn all functions the very same parameter setting must be used (which might well depend on the dimensionality, see Section Initialization and Input to the Algorithm). That means, the a priori use of function-dependent parameter settings is prohibited (since 2012). The function ID or any function characteristics (like separability, multi-modality, ...) cannot be considered as input parameter\n4 In the single objective case care should be taken to apply termination conditions that allow to hit the final target on the most basic functions, like the sphere function \ud835\udc531, that is on the problems 0, 360, 720, 1080, 1440, and 1800 of the bbob suite.\n5 The COCO platform provides example code to implement independent restarts. 6 Therefore we call the experimental approach budget-free. This claim however makes the assumption that the runtime distribution is the same on all instances of a function in a given dimension. This assumption cannot be proven in general and might be violated in some cases for some algorithms.\n7 Algorithms are only comparable up to the smallest budget given to any of them. 8 For the bbob-biobj suite this is however currently never the case.\nto the algorithm.\nOn the other hand, benchmarking different parameter settings as \u201cdifferent algorithms\u201d on an entire test suite is encouraged."}, {"heading": "4 Recommendations", "text": "The performance assessment is based on a set of evaluation counts associated with the \ud835\udc53 -value or -vector of a solution. By default, each evaluation count is associated with the respectively evaluated solution and hence its \ud835\udc53 -value. The solution associated to the current (last) evaluation can be changed by calling coco_recommend_solution, thereby associating the \ud835\udc53 -value of the recommended solution (instead of the evaluated solution) with the current evaluation count. A recommendation is best viewed as the currently best known approximation of the optimum 9 delivered by the optimization algorithm, or as the currently most desirable return value of the algorithm.\nRecommendations allow the algorithm to explore solutions without affecting the performance assessment. For example, a surrogate-based algorithm can explore (i.e. evaluate) an arbitrarily bad solution, update the surrogate model and then recommend the (new) model optimizer. On nonnoisy suites it is neither necessary nor advantageous to recommend the same solution repeatedly."}, {"heading": "5 Time Complexity Experiment", "text": "In order to get a rough measurement of the time complexity of the algorithm, the wall-clock or CPU time should be measured when running the algorithm on the benchmark suite. The chosen setup should reflect a \u201crealistic average scenario\u201d. 10 The time divided by the number of function evaluations shall be presented separately for each dimension. The chosen setup, coding language, compiler and computational architecture for conducting these experiments are to be described."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Raymond Ros, Steffen Finck, Marc Schoenauer, Petr Posik and Dejan Tusar for their many invaluable contributions to this work.\n9 In the multi-objective scenario not only the last solution, but all solutions are taken into account for this approximation. In the noisy scenario, a small number of the most current solutions will be taken into account in future assessements.\n10 The example experiment code provides the timing output measured over all problems of a single dimension by default. It also can be used to make a record of the same timing experiment with \u201cpure random search\u201d, which can serve as additional base-line data. On the bbob test suite, also only the first instance of the Rosenbrock function \ud835\udc538 had been used for this experiment previously, that is, the suite indices 105, 465, 825, 1185, 1545, 1905.\nThe authors also acknowledge support by the grant ANR-12-MONU-0009 (NumBBO) of the French National Research Agency."}], "references": [{"title": "A restart CMA evolution strategy with increasing population size", "author": ["A. Auger", "N. Hansen"], "venue": "In Proceedings of the IEEE Congress on Evolutionary Computation", "citeRegEx": "Auger and Hansen.,? \\Q2005\\E", "shortCiteRegEx": "Auger and Hansen.", "year": 2005}, {"title": "Real-Parameter Black-Box Optimization Benchmarking 2009: Experimental Setup", "author": ["N. Hansen", "A. Auger", "S. Finck", "R. Ros"], "venue": "Inria Research Report RR-6828", "citeRegEx": "Hansen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2009}, {"title": "Real-Parameter Black-Box Optimization Benchmarking 2010: Experimental Setup", "author": ["N. Hansen", "A. Auger", "S. Finck", "R. Ros"], "venue": "Inria Research Report RR-7215", "citeRegEx": "Hansen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2010}, {"title": "COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting", "author": ["N. Hansen", "A. Auger", "O. Mersmann", "T. Tusar", "D. Brockhoff"], "venue": null, "citeRegEx": "Hansen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2016}, {"title": "A parameter-less genetic algorithm", "author": ["G.R. Harik", "F.G. Lobo"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO),", "citeRegEx": "Harik and Lobo.,? \\Q1999\\E", "shortCiteRegEx": "Harik and Lobo.", "year": 1999}, {"title": "Evaluating Las Vegas algorithms: pitfalls and remedies", "author": ["H.H. Hoos", "T. St\u00fctzle"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hoos and St\u00fctzle.,? \\Q1998\\E", "shortCiteRegEx": "Hoos and St\u00fctzle.", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "We present an experimental setup and procedure for benchmarking numerical optimization algorithms in a black-box scenario. This procedure can be applied with the COCO benchmarking platform. We describe initialization of and input to the algorithm and touch upon the relevance of termination and restarts. We introduce the concept of recommendations for benchmarking with COCO. Recommendations detach the solutions used in function calls from the any-time performance assessment of the algorithm.", "creator": "LaTeX with hyperref package"}}}