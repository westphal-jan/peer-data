{"id": "1603.09509", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Learning Multiscale Features Directly From Waveforms", "abstract": "Deep learning has lindner dramatically improved preacher the mexican performance largemouth of hemostasis speech snpl recognition systems issan through learning grada\u010dac hierarchies of delsarte features adagia optimized mandamus for the 62-1 task discounters at kister hand. However, true dusheti end - marisela to - inspectional end petzl learning, where vdovin features sochaczew are bireun learned baigelman directly from licences waveforms, has prsc only recently elusa reached pileup the soldatova performance of temptress hand - giovanny tailored gossett representations inuinnaqtun based blister on plutarco the Fourier transform. zwerling In this 1997-1999 paper, chiri we 1994-2004 detail an approach to detar use convolutional kishkindha filters nitkowski to r\u00e1di\u00f3 push depreist past yuuji the inherent tradeoff of temporal and hogge frequency resolution that exists zemeri for hyflex spectral cubisme representations. nakdong At eyestripe increased gmpte computational cost, we show that lipovans increasing temporal flecha resolution thayne via caudally reduced ruethling stride and p\u00e9ret increasing onu frequency d'ambra resolution via additional five-factor filters delivers -6.0 significant criolla performance sparty improvements. Further, we priests find 58.25 more mon-el efficient nadeem-shravan representations vaulters by 2,578 simultaneously learning bluffer at multiple brined scales, denber leading to 2,400,000 an agriculture-based overall decrease in word error rate on a difficult crasher internal speech test set by 20. 7% ruetten relative to barrie networks benedict with the lucelia same brezje number of parameters trained on theodoret spectrograms.", "histories": [["v1", "Thu, 31 Mar 2016 09:54:44 GMT  (2977kb,D)", "https://arxiv.org/abs/1603.09509v1", null], ["v2", "Tue, 5 Apr 2016 14:17:09 GMT  (2977kb,D)", "http://arxiv.org/abs/1603.09509v2", "\"fix typo in the title\""]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE cs.SD", "authors": ["zhenyao zhu", "jesse h engel", "awni hannun"], "accepted": false, "id": "1603.09509"}, "pdf": {"name": "1603.09509.pdf", "metadata": {"source": "CRF", "title": "Learning Multiscale Features Directly From Waveforms", "authors": ["Zhenyao Zhu", "Jesse H. Engel", "Awni Hannun"], "emails": ["zhuzychn@gmail.com,", "jengel@baidu.com,", "awnihannun@baidu.com"], "sections": [{"heading": null, "text": "of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-toend learning, where features are learned directly from waveforms, has only recently reached the performance of handtailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms. Index Terms: Speech Recognition, Multiscale Learning, Convolutional Neural Networks, Raw Waveforms"}, {"heading": "1. Introduction", "text": "Raw speech waveforms are densely sampled in time, and thus require downsampling to make many analysis techniques computationally tractable. For speech recognition, this presents the challenge to reduce the number of timesteps in the signal without throwing away relevant information. Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction. Deep learning\u2019s recent success in speech recognition is based on learning feature hierarchies atop these representations [1, 2].\nThere has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform. A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8]. While some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand-engineered features on large vocabulary speech recognition tasks [4].\nFeatures based on the Fourier transform are computationally efficient, but exhibit an intrinsic tradeoff of temporal and frequency resolution. Convolutional filters decouple time and frequency resolution as the number of filters and stride are chosen independently. Despite this, a filter bank is constrained by its window size to a single scale. Herein, we explore jointly learning filter banks at multiple different scales on raw wave-\nforms. Multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [9], scene labeling [10], and gesture detection [11]. These successful applications exploit structure at different scales, which encourages us to explore multiscale representations for waveforms as well.\nFurther, multiscale convolutions enable us to split the spectrum into different filter banks with independent choice of stride, window size, and number of filters. To learn high frequency features, filters with a short window are applied at a small stride on the waveforms. Low-frequency features, on the contrary, employ a long window that can be applied at a larger stride. Finally, based on the needs of the speech recognition system we can then independently tune the number of filters for the different frequency bands.\nWhile much research has already been conducted on learn-\nar X\niv :1\n60 3.\n09 50\n9v 2\n[ cs\n.C L\n] 5\nA pr\n2 01\n6\ning directly from waveforms for speech recognition, the unique contributions of this paper are threefold:\n\u2022 We perform with an in-depth analysis of scaling to low strides and large numbers of filters and discover that a convolutional front end can significantly outperform Fourier features by independently tuning temporal and frequency resolution, at the cost of additional computation and memory.\n\u2022 We propose a new multiscale convolutional front end, composed of concatenated filters with different window sizes, that requires less computation and outperforms features learned on a single scale (20.7% relative to spectrogram baseline).\n\u2022 We find that multiscale features naturally learn the frequencies they can most efficiently represent, with large and small windows learning low and high frequencies respectively. This contrasts with the single scale features which try to cover the entire frequency spectrum regardless of window size."}, {"heading": "2. Experimental Setup", "text": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, 2]. However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data. The basic architecture is shown in Table 1. While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13], is employed between each layer, but not between individual timesteps [2]. Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps. We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14].\nTraining is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data. The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech [2]. At each epoch, 40% of the utterances are randomly selected to have background noise\n(superpositions of YouTube clips) added at signal-to-noise ratios ranging from 0dB to 15dB [12]. All input data (either spectrogram or waveform) is sampled at 16kHz, and normalized so that each input feature has zero mean and unit variance. For models that learn directly from waveforms, no other preprocessing is applied and the network inputs have 1 feature per 1/16ms frame. For models that learn from spectrograms, linear FFT features are extracted with a hop size of 10ms, and window size of 20ms. The network inputs are thus spectral magnitude maps ranging from 0-8kHz with 161 features per 10ms frame.\nWe train using stochastic gradient descent with Nesterov momentum and a batch size of 128. Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following [2], we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function. Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data.\nAs previously observed [2], deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M. We are aware that the results are not directly comparable to literature due to the use of proprietary datasets. However, we attempt to tightly control our experiments rather than focus on overall\nperformance, with the aim that the conclusions will generalize to other architectures and datasets as well. If optimizing for performance, it is worth noting that the WER drops by \u223c50% when training on all 12,000 hours of data with 7 bidirectional layers (70M parameters) in the backend."}, {"heading": "3. Results", "text": ""}, {"heading": "3.1. Decoupling temporal resolution", "text": "While many studies have compared convolution on raw waveforms to features such as spectrograms, MFCCs, and melscale filterbanks, they often compare the two at a similar strides and window sizes [5, 6]. Spectrograms can employ a high stride because of the unique analytic structure of the basis functions. Integrating twice, once over the real cosine and once over its imaginary sine counterpart, identifies both the phase and magnitude of the response. This is in many ways similar to performing a convolution over every timestep and max-pooling, with the phase represented by the index at which the max occurs, but is much more computationally efficient to perform with an FFT.\nWe decided to test this hypothesis that a convolution with low stride and pooling can find at least as good a basis as the spectrogram. Figure 2 and Table 2 show the effects of replacing the spectrogram with a single convolution and pooling layer of the same number of filters and window size. For each decrease in stride of the convolution, the stride of the pooling is increased to give a consistent total stride of 20ms, which is the same as the stride of the spectrogram with pooling. At comparable strides to the spectrogram (10ms) the convolution is unable to perform as well, likely due to needing to represent phase shifts as well as frequency variation. However, as the stride dips below 2ms, the convolution asymptotically reaches a superior level of performance to the spectrogram. To be fair, this improved performance comes at increased computational cost and memory usage."}, {"heading": "3.2. Multiscale features are more efficient", "text": "Representing high frequency information with a large filter is difficult because of the many places the information can occur in the filter window. Similarly, representing low frequency information with a small filter is challenging because separate filters are required for separate parts of the wave. We hypothe-\nsized that applying convolution simultaneously at several scales could allow each scale to learn filters selective to the frequencies that it can most efficiently represent. To test our hypothesis, we compare the performance of convolutional front ends with a constant number of filters at three different scales (High: (1ms window, 1/4ms stride), Mid: (4ms, 1ms), and Low: (40ms, 10ms)), to a front end employing all three scales (Diagramed in Figure 1.\nTable 3 shows that WER significantly decreases from High to Mid to Low frequency filter banks. The improved performance of the lower frequency banks is evidence for the importance of low frequency vocalization features in speech recognition [4]. Figure 3 displays the spectral centroids of each filter bank sorted by frequency, with the mean value printed alongside. It is clear that the High (2800Hz), Mid (1700Hz), and Low (940Hz) filter banks live up to their names, each learning filters that capture different frequency bands. When we jointly learn several scales, the filters exhibit a an heightened preference for representing different bands. Relaxing the requirement of each bank to cover the entire spectrum causes the High (3500Hz) banks to increase in mean frequency, and the Mid (1300Hz) and Low (480Hz) banks to decrease. Further, the WER is lowest for the multiscale features, despite the fact that it has fewer parameters and three times less large filters."}, {"heading": "3.3. Decoupling frequency resolution", "text": "With methods based on the Fourier transform, there is an intrinsic tradeoff of temporal and frequency resolution. As the number of basis functions increases, helping identify which fre-\nquencies are present, so does the window size, which smears knowledge of when they occur. Decreasing the stride cannot increase temporal resolution, it can only provide more samples of the smeared signal. Convolutions do not suffer from this same tradeoff, as the temporal resolution is limited by the number of filters and temporal resolution by the stride, both of which are independent. This advantage comes at the added cost of increased computation and memory, both by increasing number of filters or decreasing stride.\nWe explore the value of increased resolution by performing the same multiscale experiments as the previous section, but increasing the number of filters. Table 4 demonstrates that increasing the number of filters, even by a factor of 3, leads to a significant (8% relative) improvement in WER. A fullyconnected layer with output dimension of 161 is added above the concatenated feature maps in order to produce the same number of features as the previous experiments. Since using more filters at short strides is more costly in terms of both computation and memory, we specifically increase the number of filters at long strides. Further increasing the number of filters, and expanding the size of the bottle neck leads to smaller gains. showing an impressive 20.7% cumulative improvement in WER relative to the spectrogram baseline."}, {"heading": "4. Discussion", "text": "In this paper, we have consistently demonstrated that learning features directly from waveforms can outperform spectrograms, especially when applied at multiple scales. However, several interesting research questions remain to be answered before such techniques likely see widespread adoption.\nLearning convolutional filters overcomes the time/frequency tradeoff of Fourier representations, but with considerable computational and memory cost. Many modern state of the art systems train on clusters of GPUs, where memory is precious, as requiring memory transfer between GPU and CPU can be prohibitively slow for training. This is especially problematic for training on long utterances, where the amount of memory required to save all the activations increases both with the number of filters and the reduction of stride. It remains to be seen whether the power of learned input features can be combined with the efficiency of analytic signal transformations such as the Fourier transform. One such approach could be to learn basis functions in the real and imaginary domain by performing backpropagation through the Hilbert transformation, enabling the use of larger strides. Alternatively, learned features can be fixed and used to augment other fixed features at train time. Sainath et al. [4] found noticeable improvements from supplementing log-mel filterbanks in such a manner.\nWhile learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs [2, 16]. Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.\nIn our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers. This temporal pooling only takes into account local structure and has no explicit knowledge of what information to preserve based on longrange dependencies. Recently proposed architectures that operate simultaneously at different timescales, such as the Clockwork RNN [18], could provide a more elegant way of combining multiscale signals. Beyond incorporating recurrence, low frequency features that require fewer temporal samples could then also require less recurrent computation and facilitate modeling long-range structure.\nFinally, from observing representative filters learned at each scale in Figure 4, we can see that there is some redundancy in the representation. Some filter shapes appear similar at multiple scales. An interesting future direction could be to investigate learning features in a scale-free basis, similar to wavelets, where a reduced basis set could be applied across a range of scales."}, {"heading": "5. References", "text": "[1] G. Hinton, L. Deng, D. Yu, G. Dahl, A. rahman Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition,\u201d Signal Processing Magazine, 2012.\n[2] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. C. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen, J. Engel, L. Fan, C. Fougner, T. Han, A. Y. Hannun, B. Jun, P. LeGresley, L. Lin, S. Narang, A. Y. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh, D. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao, D. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d CoRR, vol. abs/1512.02595, 2015. [Online]. Available: http://arxiv.org/abs/1512.02595\n[3] D. Palaz, R. Collobert et al., \u201cAnalysis of cnn-based speech recognition system using raw speech as input,\u201d in Proceedings of Interspeech, no. EPFL-CONF-210029, 2015.\n[4] T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals, \u201cLearning the speech front-end with raw waveform cldnns,\u201d in Sixteenth Annual Conference of the International Speech Communication Association, 2015.\n[5] P. Golik, Z. Tu\u0308ske, R. Schlu\u0308ter, and H. Ney, \u201cConvolutional neural networks for acoustic modeling of raw time signal in lvcsr,\u201d in Sixteenth Annual Conference of the International Speech Communication Association, 2015.\n[6] Z. Tu\u0308ske, P. Golik, R. Schlu\u0308ter, and H. Ney, \u201cAcoustic modeling with deep neural networks using raw time signal for lvcsr,\u201d in Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.\n[7] N. Jaitly and G. E. Hinton, \u201cLearning a better representation of speech soundwaves using restricted boltzmann machines.\u201d in ICASSP. IEEE, 2011, pp. 5884\u20135887.\n[8] S. Dieleman and B. Schrauwen, \u201cEnd-to-end learning for music audio,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, May 2014, pp. 6964\u2013 6968.\n[9] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \u201cGoing deeper with convolutions,\u201d arXiv preprint arXiv:1409.4842, 2014.\n[10] C. Farabet, C. Couprie, L. Najman, and Y. LeCun, \u201cLearning hierarchical features for scene labeling,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1915\u2013 1929, 2013.\n[11] N. Neverova, C. Wolf, G. W. Taylor, and F. Nebout, \u201cMulti-scale deep learning for gesture detection and localization,\u201d in Computer Vision-ECCV 2014 Workshops. Springer, 2014, pp. 474\u2013490.\n[12] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., \u201cDeepspeech: Scaling up end-to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014.\n[13] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d CoRR, vol. abs/1502.03167, 2015. [Online]. Available: http: //arxiv.org/abs/1502.03167\n[14] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.\n[15] K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn, \u201cScalable modified Kneser-Ney language model estimation,\u201d in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, 8 2013. [Online]. Available: http://kheafield.com/professional/edinburgh/estimate paper.pdf\n[16] T. Sercu, C. Puhrsch, B. Kingsbury, and Y. LeCun, \u201cVery deep multilingual convolutional neural networks for LVCSR,\u201d CoRR, vol. abs/1509.08967, 2015. [Online]. Available: http: //arxiv.org/abs/1509.08967\n[17] M. Ranzato, F. J. Huang, Y. L. Boureau, and Y. LeCun, \u201cUnsupervised learning of invariant feature hierarchies with applications to object recognition,\u201d in Computer Vision and Pattern Recognition, 2007. CVPR \u201907. IEEE Conference on, June 2007, pp. 1\u20138.\n[18] J. Koutn\u0131\u0301k, K. Greff, F. J. Gomez, and J. Schmidhuber, \u201cA clockwork RNN,\u201d CoRR, vol. abs/1402.3511, 2014. [Online]. Available: http://arxiv.org/abs/1402.3511"}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. rahman Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B.C. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A.Y. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A.Y. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": "CoRR, vol. abs/1512.02595, 2015. [Online]. Available: http://arxiv.org/abs/1512.02595", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["D. Palaz", "R. Collobert"], "venue": "Proceedings of Interspeech, no. EPFL-CONF-210029, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning the speech front-end with raw waveform cldnns", "author": ["T.N. Sainath", "R.J. Weiss", "A. Senior", "K.W. Wilson", "O. Vinyals"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr", "author": ["P. Golik", "Z. T\u00fcske", "R. Schl\u00fcter", "H. Ney"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr", "author": ["Z. T\u00fcske", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a better representation of speech soundwaves using restricted boltzmann machines.", "author": ["N. Jaitly", "G.E. Hinton"], "venue": "in ICASSP. IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, May 2014, pp. 6964\u2013 6968.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1915\u2013 1929, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1915}, {"title": "Multi-scale deep learning for gesture detection and localization", "author": ["N. Neverova", "C. Wolf", "G.W. Taylor", "F. Nebout"], "venue": "Computer Vision-ECCV 2014 Workshops. Springer, 2014, pp. 474\u2013490.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, vol. abs/1502.03167, 2015. [Online]. Available: http: //arxiv.org/abs/1502.03167", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, 8 2013. [Online]. Available: http://kheafield.com/professional/edinburgh/estimate paper.pdf", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep multilingual convolutional neural networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "CoRR, vol. abs/1509.08967, 2015. [Online]. Available: http: //arxiv.org/abs/1509.08967", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.L. Boureau", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR \u201907. IEEE Conference on, June 2007, pp. 1\u20138.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "A clockwork RNN", "author": ["J. Koutn\u0131\u0301k", "K. Greff", "F.J. Gomez", "J. Schmidhuber"], "venue": "CoRR, vol. abs/1402.3511, 2014. [Online]. Available: http://arxiv.org/abs/1402.3511", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning\u2019s recent success in speech recognition is based on learning feature hierarchies atop these representations [1, 2].", "startOffset": 121, "endOffset": 127}, {"referenceID": 1, "context": "Deep learning\u2019s recent success in speech recognition is based on learning feature hierarchies atop these representations [1, 2].", "startOffset": 121, "endOffset": 127}, {"referenceID": 2, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 3, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 4, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 5, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 6, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 7, "context": "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8].", "startOffset": 222, "endOffset": 240}, {"referenceID": 3, "context": "While some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand-engineered features on large vocabulary speech recognition tasks [4].", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": "Multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [9], scene labeling [10], and gesture detection [11].", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "Multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [9], scene labeling [10], and gesture detection [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 10, "context": "Multiscale convolutions have already been successfully applied to address tasks in the computer vision field, such as image classification [9], scene labeling [10], and gesture detection [11].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, 2].", "startOffset": 107, "endOffset": 114}, {"referenceID": 1, "context": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, 2].", "startOffset": 107, "endOffset": 114}, {"referenceID": 12, "context": "Batch normalization [13], is employed between each layer, but not between individual timesteps [2].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "Batch normalization [13], is employed between each layer, but not between individual timesteps [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14].", "startOffset": 178, "endOffset": 182}, {"referenceID": 1, "context": "The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 11, "context": "(superpositions of YouTube clips) added at signal-to-noise ratios ranging from 0dB to 15dB [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "Following [2], we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository.", "startOffset": 214, "endOffset": 218}, {"referenceID": 1, "context": "As previously observed [2], deep neural networks trained on sufficient data perform better as the model size grows.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "While many studies have compared convolution on raw waveforms to features such as spectrograms, MFCCs, and melscale filterbanks, they often compare the two at a similar strides and window sizes [5, 6].", "startOffset": 194, "endOffset": 200}, {"referenceID": 5, "context": "While many studies have compared convolution on raw waveforms to features such as spectrograms, MFCCs, and melscale filterbanks, they often compare the two at a similar strides and window sizes [5, 6].", "startOffset": 194, "endOffset": 200}, {"referenceID": 3, "context": "The improved performance of the lower frequency banks is evidence for the importance of low frequency vocalization features in speech recognition [4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "[4] found noticeable improvements from supplementing log-mel filterbanks in such a manner.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs [2, 16].", "startOffset": 179, "endOffset": 186}, {"referenceID": 15, "context": "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs [2, 16].", "startOffset": 179, "endOffset": 186}, {"referenceID": 16, "context": "Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "Recently proposed architectures that operate simultaneously at different timescales, such as the Clockwork RNN [18], could provide a more elegant way of combining multiscale signals.", "startOffset": 111, "endOffset": 115}], "year": 2016, "abstractText": "Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-toend learning, where features are learned directly from waveforms, has only recently reached the performance of handtailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms.", "creator": "LaTeX with hyperref package"}}}