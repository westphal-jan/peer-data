{"id": "1603.09727", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Neural Language Correction with Character-Based Attention", "abstract": "Natural dosso language correction has nzx-50 the hailsham potential somjai to ecotrust help landung language hoberman learners improve their writing uaa skills. klopas While approaches barks with separate utstarcom classifiers for lorinda different error types armonico have chile high basicos precision, theatro they faubourg do unsaid not thumbing flexibly handle biotransformation errors melts such katsuobushi as redundancy or non - christology idiomatic phrasing. biosis On the shaoguang other hand, treforest word yasina and nemov phrase - based machine qasim translation methods are ust-kamenogorsk not talitha designed to khathran cope vld with taobao orthographic errors, herbivore and have deering recently arraigned been destructors outpaced by neller neural lydd models. potidaea Motivated by haetzni these issues, korba we pinatubo present 114.81 a neural network - based mateyo approach to hapless language schiedam correction. barty The tokara core fashion-conscious component thinktanks of trohman our lovers method is an passionate encoder - decoder 52-17 recurrent neural non-jew network with an attention tidally mechanism. batalla By operating 1555 at the character leutwiler level, matn the kurimoto network avoids the cabactulan problem undergraduate of out - pakka of - 1,542 vocabulary words. We esler illustrate shapour the flexibility channelside of neuhausen our ziff-davis approach on shilowa dataset of writedowns noisy, user - leve generated text collected sealink from an English colding learner forum. When contemporaneously combined with kuteesa a comco language model, our chepchumba method egyptian achieves a lec state - of - moira the - chazzan art $ mittal F_ {achuthanandan 0. 5} $ - score punx on the yoido CoNLL aytoun 2014 skoch Shared mezvinsky Task. sportcoat We mechanicsburg further demonstrate nasolabial that slamed training the castellani network on additional toxemia data with wangqing synthesized errors infrasonic can 36-member improve cavalier-smith performance.", "histories": [["v1", "Thu, 31 Mar 2016 19:16:54 GMT  (191kb,D)", "http://arxiv.org/abs/1603.09727v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ziang xie", "anand avati", "naveen arivazhagan", "dan jurafsky", "rew y ng"], "accepted": false, "id": "1603.09727"}, "pdf": {"name": "1603.09727.pdf", "metadata": {"source": "CRF", "title": "Neural Language Correction with Character-Based Attention", "authors": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y. Ng"], "emails": ["zxie@cs.stanford.edu,", "avati@cs.stanford.edu,", "naveen67@cs.stanford.edu,", "ang@cs.stanford.edu,", "jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Systems that provide writing feedback have great potential to assist language learners as well as native writers. Although tools such as spell checkers have been useful, detecting and fixing errors in natural language, even at the sentence level, remains far from solved.\nMuch of the prior research focuses solely on training classifiers for a small number of error\ntypes, such as article or preposition errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recent methods that consider a broader range of error classes often rely on language models to score n-grams or statistical machine translation approaches (Ng et al., 2014). These methods, however, do not flexibly handle orthographic errors in spelling, capitalization, and punctuation.\nAs a motivating example, consider the following incorrect sentence: \u201cI visitted Tokyo on Nov 2003. :)\u201d. Several errors in this sentence illustrate the difficulties in the language correction setting. First, the sentence contains a misspelling, visitted, an issue for systems with fixed vocabularies. Second, the sentence contains rare words such as 2003 as well as punctuation forming an emoticon :), issues that may require special handling. Finally, the use of the preposition on instead of in when not referring to a specific day is non-idiomatic, demonstrating the complex patterns that must be\nar X\niv :1\n60 3.\n09 72\n7v 1\n[ cs\n.C L\n] 3\n1 M\nar 2\n01 6\ncaptured to suggest good corrections. In hopes of capturing such complex phenomena, we use a neural network-based method.\nBuilding on recent work in language modeling and machine translation, we propose an approach to natural language error correction based on an encoder-decoder recurrent neural network trained on a parallel corpus containing \u201cgood\u201d and \u201cbad\u201d sentences (Figure 1). When combined with a language model, our system obtains state-of-the-art results on the CoNLL 2014 Shared Task, beating systems using statistical machine translation systems, rule-based methods, and task-specific features. Our system naturally handles orthographic errors and rare words, and can flexibly correct a variety of error types. We further find that augmenting the network training data with sentences containing synthesized errors can result in significant gains in performance."}, {"heading": "2 Model Architecture", "text": "Given an input sentence x that we wish to map to an output sentence y, we seek to model P (y|x). Our model consists of an encoder and a decoder (Sutskever et al., 2014; Cho et al., 2014). The encoder maps the input sentence to a higher-level representation with a pyramidal bidirectional RNN architecture similar to that of Chan et al. (2015). The decoder is also a recurrent neural network that uses a content-based attention mechanism (Bahdanau et al., 2014) to attend to the encoded representation and generate the output sentence one character at a time."}, {"heading": "2.1 Character-Level Reasoning", "text": "Our neural network model operates at the character level, in the encoder as well as the decoder. This is for two reasons, as illustrated by our motivating example. First, we do not assume that the inputs are spell-checked and often find spelling errors in the sentences written by English learners in the datasets we consider. Second, wordlevel neural MT models with a fixed vocabulary are poorly suited to handle OOVs such as multidigit numbers, emoticons, and web addresses (Graves, 2013), though recent work has proposed workarounds for this problem (Luong et al., 2014). Despite longer sequences in the character-based model, optimization does not seem to be a significant issue, since the network often only needs to copy characters from source to target."}, {"heading": "2.2 Encoder Network", "text": "Given the input vector xt, the forward, backward, and combined activations of the jth hidden layer are computed as:\nf (j) t = GRU(f (j) t\u22121, c (j\u22121) t ), b (j) t = GRU(b (j) t+1, c (j\u22121) t ), h (j) t = f (j) t + b (j) t\nwhere GRU denotes the gated recurrent unit function, which, similar to long short-term memory units (LSTMs), have shown to improve the performance of RNNs (Cho et al., 2014; Hochreiter and Schmidhuber, 1997).\nThe input from the previous layer input c(0)t = xt and\nc (j) t = tanh ( W (j)pyr [ h (j\u22121) 2t , h (j\u22121) 2t+1 ]> + b(j)pyr ) for j > 0. The weight matrix Wpyr thus reduces the number of hidden states for each additional hidden layer by half, and hence the encoder has a pyramid structure. At the final hidden layer we obtain the encoded representation c consisting of \u2308 T/2N\u22121 \u2309 hidden states, where N denotes the number of hidden layers."}, {"heading": "2.3 Decoder Network", "text": "The decoder network is recurrent neural network using gated recurrent units with M hidden layers. After the final hidden layer the network also conditions on the encoded representation c using an attention mechanism.\nAt the jth decoder layer the hidden activations are computed as\nd (j) t = GRU(d (j) t\u22121, d (j\u22121) t ),\nwith the output of the final hidden layer d(M)t then being used as part of the content-based attention mechanism similar to that proposed by Bahdanau et al. (2014):\nutk = \u03c61(d (M) t ) >\u03c62(ck) \u03b1tk = utk\u2211 j utj\nat = \u2211 j \u03b1tjcj\nwhere \u03c61 and \u03c62 represent feedforward affine transforms followed by a tanh nonlinearity. The\nweighted sum of the encoded hidden states at is then concatenated with d(M)t , and passed through another affine transform followed by a ReLU nonlinearity before the final softmax output layer.\nThe loss function is the cross-entropy loss per time step summed over the output sequence y:\nL(x, y) = \u2212 T\u2211 t=1 logP (yt|x, y<t).\nNote that during training the ground truth yt\u22121 is fed into the network to predict yt, while at test time the most probable y\u0302t\u22121 is used. Figure 1 illustrates the model architecture."}, {"heading": "2.4 Attention and Pyramid Structure", "text": "In preliminary experiments, we found that having an attention mechanism was crucial for the model to be able to generate outputs characterby-character that did not diverge from the input. While character-based approaches have not attained state-of-the-art performance on large scale translation and language modeling tasks, in this setting the decoder network simply needs to copy input tokens during the majority of time steps.\nAlthough character-level models reduce the softmax over the vocabulary at each time step over word-level models, they also increase the total number of time-steps of the RNN. The contentbased attention mechanism must then consider all the encoder hidden states c1:T at every step of the decoder. Thus we use a pyramid architecture, which reduces computational complexity (as shown by Chan et al. (2015)). For longer batches, we observe over a 2\u00d7 speedup for the same number of parameters when using a 400 hidden unit per layer model with 3 hidden layers (4\u00d7 reduction of steps in c)."}, {"heading": "3 Decoding", "text": "While it is simpler to integrate a language model by using it as a re-ranker, here the language model probabilities are combined with the encoderdecoder network through beam search. This is possible because the attention mechanism in the decoder network prevents the decoded output from straying too far from the source sentence."}, {"heading": "3.1 Language Model", "text": "To model the distribution\nPLM(y1:T ) = T\u220f t=1 P (yt|y<t) (1)\nwe build a Kneser-Ney smoothed 5-gram language model on a subset of the Common Crawl Repository1 collected during 2012 and 2013. After pruning, we obtain 2.2 billion n-grams. To build and query the model, we use the KenLM toolkit (Heafield et al., 2013)."}, {"heading": "3.2 Beam Search", "text": "For inference we use a beam search decoder combining the neural network and the language model likelihood. Similar to Hannun et al. (2014), at step k, we rank the hypotheses on the beam using the score\nsk(y1:k|x) = logPNN(y1:k|x) + \u03bb logPLM(y1:k)\nwhere the hyper-parameter \u03bb determines how much the language model is weighted. To avoid penalizing longer hypotheses, we additionally normalize scores by the number of words in the hypothesis |y|. Since decoding is done at the character level, the language model probability PLM(\u00b7) is only incorporated after a space or end-of-sentence symbol is encountered."}, {"heading": "3.3 Controlling Precision", "text": "For many error correction tasks, precision is emphasized more than recall; for users, an incorrect suggestion is worse than a missed mistake.\nIn order to filter spurious edits, we train an edit classifier to classify edits as correct or not. We run our decoder on uncorrected sentences from our training data to generate candidate corrected sentences. We then align the candidate sentences to the uncorrected sentences by minimizing the word-level Levenshtein distance between each candidate and uncorrected sentence. Contiguous segments that do not match are extracted as proposed edits2. We repeat this alignment and edit extraction process for the gold corrected sentences and the uncorrected sentences to obtain the gold edits. \u201cGood\u201d edits are defined as the intersection of the proposed and gold edits and \u201cbad\u201d edits are defined as the proposed edits not contained in the gold edits. We compute edit features and train a multilayer perceptron binary classifier on the extracted edits to predict the probability of an edit being correct. The features computed on an edit s\u2192 t are: 1http://commoncrawl.org 2Note this is an approximation and cannot distinguish sideby-side edits as separate edits.\n\u2022 edit distance features: normalized word and character lengths of s and t, normalized word and character insertions, deletions, and substitutions between s and t.\n\u2022 embedding features: sum of 100 dimensional GloVe (Pennington et al., 2014) vectors of words in s and t, GloVe vectors of left and right context words in s.\nIn order to filter incorrect edits, we only accept edits whose predicted probability exceeds a threshold pmin. This assumes that classifier probabilities are reasonably calibrated (Niculescu-Mizil and Caruana, 2005). Edit classification improves precision with a small drop in recall; most importantly, it helps filter edits where the decoder network misbehaves and t deviates wildly from s."}, {"heading": "4 Experiments", "text": "We perform experiments using two datasets of corrected sentences written by English learners. The first is the Lang-8 Corpus, which contains erroneous sentences and their corrected versions collected from a social language learner forum (Tajiri et al., 2012). Due to the online user-generated setting, the Lang-8 data is noisy, with sentences often containing misspellings, emoticons, and other loose punctuation. Sample sentences are show in Table 4.\nThe other dataset we consider comes from the CoNLL 2013 and 2014 Shared Tasks, which contain about 60K sentences from essays written by English learners with corrections and error type annotations. We use the larger Lang-8 Corpus primarily to train our network, then evaluate on the CoNLL Shared Tasks."}, {"heading": "4.1 Training and Decoding Details", "text": "Our pyramidal encoder has 3 layers, resulting in a factor 4 reduction in the sequence length at its output, and our decoder RNN has 3 layers as well. Both the encoder and decoder use a hidden size of 400 and gated recurrent units (GRUs), which along with LSTMs (Hochreiter and Schmidhuber, 1997) have been shown to be easier to optimize and preserve information over many time steps better than vanilla recurrent networks.\nOur vocabulary includes 98 characters: the printing ASCII character set and special \u3008sos\u3009, \u3008eos\u3009, and \u3008unk\u3009 symbols indicating the start-ofsentence, end-of-sentence, and unknown symbols, respectively.\nTo train the encoder-decoder network we use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0003, default decay rates \u03b21 and \u03b22, and a minibatch size of 128. We train for up to 40 epochs, selecting the model with the lowest perplexity on the Lang-8 development set. We found that using dropout (Srivastava et al., 2014) at a rate of 0.15 on the non-recurrent connections (Pham et al., 2014) helped reduce perplexity. We use uniform initialization of the weight matrices in the range [\u22120.1, 0.1] and zero initialization of biases.\nDecoding parameter \u03bb and edit classifier threshold pmin were chosen to maximize performance on the development sets of the datasets described. All results were obtained using a beam width of 64, which seemed to provide a good trade-off between speed and performance."}, {"heading": "4.2 Noisy Data: Lang-8 Corpus", "text": "We use the train-test split provided by the Lang8 Corpus of Learner English (Tajiri et al., 2012), which contains 100K and 1K entries with about 550K and 5K parallel sentences, respectively. We also split 5K sentences from the training set to use as a separate development set for model and parameter selection.\nSince we do not have gold annotations that distinguish side-by-side edits as separate edits, we report BLEU score3 using just the encoder-decoder network as well as when combined with the ngram language model (Table 1). Note that since there may be multiple ways to correct an error and some errors are left uncorrected, the baseline of using uncorrected sentences is more difficult to improve upon than it may initially appear. As another baseline we apply the top suggestions from\n3Using case-sensitive multi-bleu.perl from Moses.\na spell checker with default configurations4. We suspect due to proper nouns, acronyms, and inconsistent capitalization conventions in Lang-8, however, this actually decreased BLEU slightly. To the best of our knowledge, no other work has reported results on this challenging task."}, {"heading": "4.3 Main Results: CoNLL Shared Tasks", "text": "Description For our second set of experiments we evaluate on the CoNLL 2014 Shared Task on Grammatical Error Correction (Ng et al., 2013; Ng et al., 2014). We use the revised CoNLL 2013 test data with all error types as a development set for parameter and model selection with the 2014 test data as our test set. The 2013 test data contains 1381 sentences with 3470 errors in total, and the 4Hunspell v1.3.4, https://hunspell.github.io\n2014 test data contains 1312 sentences with 3331 errors. The CoNLL 2014 training set contains 57K sentences with the corresponding gold edits by a single annotator. The 2013 test set is also only labeled by a single annotator, while the 2014 test set has two separate annotators.\nWe use the NUS MaxMatch scorer (Dahlmeier and Ng, 2012) v3.2 in order to compute the precision (P ), recall (R), and F -score for our corrected sentences. Since precision is considered more important than recall for the error correction task, F0.5 score is reported as in the CoNLL 2014 Challenge. We compare to the top submissions in the 2014 Challenge as well as the method by Susanto (2014), which combines 3 of the weaker systems to achieve the state-of-the-art result. All results reported on the 2014 test set exclude alternative corrections submitted by the participants.\nSynthesizing Errors In addition to the Lang-8 training data, we include the CoNLL 2014 training data in order to train the encoder-decoder network. Following prior work, we additionally explore synthesizing additional sentences containing errors using the CoNLL 2014 training data (Felice and Yuan, 2014; Rozovskaya et al., 2012). Our data augmentation procedure generates synthetic errors for two of the most common error types in the development set: article or determiner errors (ArtOrDet) and noun number errors (Nn). Similar to Felice and Yuan (2014), we first collect error distribution statistics from the CoNLL 2014 training data. For ArtOrDet errors, we estimate the probability that an article or determiner is deleted, replaced with another determiner, or inserted before the start of a noun phrase. For Nn errors, we estimate the probability that it is replaced with its singular or plural form. To obtain sentence parses we use the Stanford CoreNLP Toolkit (Manning et al., 2014). Example synthesized errors:\n\u2022 ArtOrDet: They will generate and brainstorm the innovative ideas.\n\u2022 Nn: Identification is becoming more important in our society\u2192 societies.\nErrors are introduced independently according to their estimated probabilities by iterating over the words in the training sentences, and we produce two corrupted versions of each training sentence whenever possible. The original Lang-8 training data contains 550K sentence pairs. Adding the\nCoNLL 2014 training data results in about 610K sentence pairs, and after data augmentation we obtain a total of 720K sentence pairs. We examine the benefits of synthesizing errors in Section 5.\nResults Results for the development set are shown in Table 2, and results for the CoNLL 2014 test set in Table 3. On the CoNLL 2014 test set, which contains the full set of 28 error types, our method achieves a state-of-the-art result, beating all systems from the 2014 Challenge as well as a system combination method (Susanto, 2014). Methods from the 2014 Challenge used statistical machine translation, language model ranking, rule-based approaches, and error type-specific features and classifiers, often in combination. System descriptions for participating teams are given in Ng et al. (2014)."}, {"heading": "5 Discussion", "text": "Qualitative Analysis We present examples of correct and incorrect edits on Lang-8 development set in Table 4 and Table 5. Despite operating at the character level, the network is occasionally able to perform rearrangements of words to form common phrases (e.g. I and my roommate\u2192my roommate and I) and insert and delete words where appropriate. On the other hand, the network can also sometimes mangle rare words (Moodysson\u2192 Moodysnot) and fail to split common words missing a separating space (withthe \u2192 withth), suggesting that while common patterns are captured, the network lacks semantic understanding.\nPerformance Breakdown While the encoderdecoder network can itself produce modifications, on less noisy datasets such as the CoNLL\nChallenge datasets a language model can greatly improve performance. Increasing the language model weight \u03bb tends to improves recall at the expense of precision. On the other hand, using edit classification to filter spurious edits increases precision, often with smaller drops in recall. We do not observe a trend of decreasing F -score for a wide range of sentence lengths (Figure 2), likely due to the attention mechanism, which helps to prevent the decoded output from diverging from the input sentence.\nWe report the inter-annotator agreement in Table 3, which gives a possible bound on the F -score for this task.\nEffects of Data Augmentation We obtain promising improvements using data augmentation, boosting F0.5-score on the development set from 31.55 to 34.81. For the two error types where we synthesize data (article or determiner and noun number) we observe significant increases in recall, as shown in Table 6. The same phenomenon has been observed by Rozovskaya et al. (2012). Interestingly, the recall of other error types (see Ng et al. (2014) for descriptions) decreases. We surmise this is because the additional training data contains only ArtOrDet and Nn errors, and hence the network is encouraged to simply copy the output when those error types are not present. We hope synthesizing data with a variety of other error types may fix this issue and improve performance.\nChallenging Error Types We now examine a few illustrative error types from the CoNLL Challenges that originally motivated our approach: orthographic (Mec), redundancy (Rloc-), and idiomatic errors (Wci). Since the 2013 Challenge\ndid not score these error types, we compare our recall to those of participants in the 2014 Challenge (Ng et al., 2014).5 Note that systems only predict corrected sentences and not error types, and hence precision is not compared. We use the results from our final system, including both data augmentation and edit classification. Some examples of these error types are shown in Table 7.\n\u2022 Mec: We obtain a recall of 37.17 on the Mec error type, higher than all the 2014 Challenge teams besides one team (RAC) that used rulebased methods to attain 43.51 recall. The word/phrase-based translation and language modeling approaches do not seem to perform as well for fixing orthographic errors.\n\u2022 Rloc-: Redundancy is difficult to capture 5The team that placed 9th overall did not disclose their method; thus we only compare to the 12 remaining teams.\nusing just rule-based approaches and classifiers; our approach obtains 17.47 recall which places second among the 12 teams. The top system obtains 20.16 recall using a combination MT, LM, and rule-based method.\n\u2022 Wci: Although there are 340 collocation errors, all teams performed poorly on this category. Our recall places 3rd behind two teams (AMU and CAMB) whose methods both used an MT system. Again, this demonstrates the difficulty of capturing whether a sentence is idiomatic through only classifiers and rule-based methods.\nWe note that our system obtains significantly higher precision than any of the top 10 teams in the 2014 Challenge (49.24 vs. 41.78), which comes at the expense of recall.\nLimitations A key limitation of our method as well as most other translation-based methods is that it is trained on just parallel sentences, despite some errors requiring information about the surrounding text to make the proper correction. Even within individual sentences, when longer context is needed to make a correction (for example in many subject-verb agreement errors), the performance is hit-and-miss. The edits introduced by the system tend to be fairly local.\nOther errors illustrate the need for natural language understanding, for example in Table 5 the correction Broke my heart\u2192 I broke my heart and I want to big size bag\u2192 I want to be a big size bag. Finally, although end-to-end approaches have the potential to fix a wide variety of errors, it is not straightforward to then classify the types of errors being made. Thus the system cannot easily provide error-specific feedback."}, {"heading": "6 Related Work", "text": "Our work primarily builds on prior work on training encoder-decoder RNNs for machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The attention mechanism, which allows the decoder network to copy parts of the source sentence and cope with long inputs, is based on the content-based attention mechanism introduced by Bahdanau et al. (2014), and the overall network architecture is based on that described by Chan et al. (2015). Our model is also inspired by character-level models as proposed by Graves (2013). More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al., 2015; Maas et al., 2015; Chan et al., 2015).\nTreating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences\nwas used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). The work of Felice et al. (2014) achieved the best F0.5-score of 37.33 in that year\u2019s challenge using a combination of rule-based, language-model ranking, and statistical machine translation techniques. Many other teams used a language model for re-ranking hypotheses as well. Other teams participating in the CoNLL 2014 Challenge used techniques ranging from rule-based systems to type-specific classifiers, as well as combinations of the two (Rozovskaya et al., 2014; Lee and Lee, 2014). The rule-based systems often focus on only a subset of the error types. The previous state of the art was achieved by Susanto (2014) using the system combination method proposed by Heafield and Lavie (2010) to combine three weaker systems.\nFinally, our work uses data collected and shared through the generous efforts of the teams behind the CoNLL and Lang-8 datasets (Mizumoto et al., 2011; Mizumoto et al., 2012; Ng et al., 2013; Ng et al., 2014). Prior work has also proposed data augmentation for the language correction task (Felice and Yuan, 2014; Rozovskaya et al., 2012).\nConclusion\nWe present a neural network-based model for performing language correction. Our system is able correct errors on noisy data collected from an English learner forum and attains state-of-the-art performance on the CoNLL 2014 Challenge dataset of annotated essays. Key to our approach is the use of a character-based model with an attention mechanism, which allows for orthographic errors to be captured and avoids the OOV problem suffered by word-based neural machine translation methods. We hope the generality of this approach will also allow it to be applied to other tasks that must deal with noisy text, such as in the online user-generated setting."}, {"heading": "Acknowledgments", "text": "We thank Kenneth Heafield, Jiwei Li, Thang Luong, Peng Qi, and Anshul Samar for helpful discussions. We additionally thank the developers of Theano (Bergstra et al., 2010). Some GPUs used in this work were donated by NVIDIA Corporation. ZX was supported by an NDSEG Fellowship. This project was funded in part by DARPA MUSE award FA8750-15-C-0242 AFRL/RIKF."}, {"heading": "In Association for Computational Linguistics (ACL)", "text": "System Demonstrations.\n[Mizumoto et al.2011] Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2011. Mining revision log of language learning sns for automated japanese error correction of second language learners. In International Joint Conference on Natural Language Processing (IJCNLP).\n[Mizumoto et al.2012] Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, Masaaki Nagata, and Yuji Matsuomto. 2012. The effect of learner corpus size in grammatical error correction of ESL writings. In International Conference on Computational Linguistics.\n[Ng et al.2013] Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-2013 shared task on grammatical error correction.\n[Ng et al.2014] Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction.\n[Niculescu-Mizil and Caruana2005] Alexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabilities with supervised learning. In International Conference on Machine learning (ICML).\n[Pennington et al.2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP).\n[Pham et al.2014] Vu Pham, The\u0301odore Bluche, Christopher Kermorvant, and Je\u0301ro\u0302me Louradour. 2014. Dropout improves recurrent neural networks for handwriting recognition. In Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on.\n[Rozovskaya and Roth2010] Alla Rozovskaya and Dan Roth. 2010. Generating confusion sets for contextsensitive error correction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language (EMNLP).\n[Rozovskaya et al.2012] Alla Rozovskaya, Mark Sammons, and Dan Roth. 2012. The ui system in the hoo 2012 shared task on error correction. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP.\n[Rozovskaya et al.2014] Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, Dan Roth, and Nizar Habash. 2014. The illinois-columbia system in the conll-2014 shared task. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task.\n[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research.\n[Susanto2014] Raymond Hendy Susanto. 2014. Systems combination for grammatical error correction. In Empirical Methods in Natural Language Processing (EMNLP).\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Neural Information Processing Systems (NIPS).\n[Tajiri et al.2012] Toshikazu Tajiri, Mamoru Komachi, and Yuji Matsumoto. 2012. Tense and aspect error correction for esl learners using global context. In Association for Computational Linguistics: Short Papers."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Better evaluation for grammatical error correction. In North American Chapter of the Association for Computational Linguistics (NAACL)", "author": ["Dahlmeier", "Ng2012] Daniel Dahlmeier", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Dahlmeier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2012}, {"title": "Generating artificial errors for grammatical error correction", "author": ["Felice", "Yuan2014] Mariano Felice", "Zheng Yuan"], "venue": "In EACL", "citeRegEx": "Felice et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Zheng Yuan", "istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar"], "venue": "Proceedings of the Eighteenth Conference on Computational", "citeRegEx": "Felice et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Detecting errors in english article usage by non-native speakers", "author": ["Han et al.2006] Na-Rae Han", "Martin Chodorow", "Claudia Leakcock"], "venue": "Natural Language Engineering", "citeRegEx": "Han et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Han et al\\.", "year": 2006}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun et al.2014] Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos"], "venue": "arXiv preprint arXiv:1412.5567", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Cmu multi-engine machine translation for wmt 2010", "author": ["Heafield", "Lavie2010] Kenneth Heafield", "Alon Lavie"], "venue": "In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,", "citeRegEx": "Heafield et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2010}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Heafield et al.2013] K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "In ACLHLT,", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The amu system in the conll-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation", "author": ["Junczys-Dowmunt", "Roman Grundkiewicz"], "venue": null, "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Postech grammatical error correction system in the conll-2014 shared task", "author": ["Lee", "Lee2014] Kyusong Lee", "Gary Geunbae Lee"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Characterbased neural machine translation", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": "arXiv preprint arXiv:1511.04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1410.8206", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Maas et al.2015] Andrew L. Maas", "Ziang Xie", "Dan Jurafsky", "Andrew Y. Ng"], "venue": "In Proceedings the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Maas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Mining revision log of language learning sns for automated japanese error correction of second language learners", "author": ["Mamoru Komachi", "Masaaki Nagata", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Mizumoto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mizumoto et al\\.", "year": 2011}, {"title": "The effect of learner corpus size in grammatical error correction of ESL writings", "author": ["Yuta Hayashibe", "Mamoru Komachi", "Masaaki Nagata", "Yuji Matsuomto"], "venue": null, "citeRegEx": "Mizumoto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mizumoto et al\\.", "year": 2012}, {"title": "The CoNLL-2013 shared task on grammatical error correction", "author": ["Ng et al.2013] Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "Predicting good probabilities with supervised learning", "author": ["Niculescu-Mizil", "Rich Caruana"], "venue": "In International Conference on Machine learning (ICML)", "citeRegEx": "Niculescu.Mizil et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Generating confusion sets for contextsensitive error correction", "author": ["Rozovskaya", "Roth2010] Alla Rozovskaya", "Dan Roth"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language (EMNLP)", "citeRegEx": "Rozovskaya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2010}, {"title": "The ui system in the hoo 2012 shared task on error correction", "author": ["Mark Sammons", "Dan Roth"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP", "citeRegEx": "Rozovskaya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2012}, {"title": "The illinois-columbia system in the conll-2014 shared task", "author": ["Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural", "citeRegEx": "Rozovskaya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Systems combination for grammatical error correction", "author": ["Raymond Hendy Susanto"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Susanto.,? \\Q2014\\E", "shortCiteRegEx": "Susanto.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Tense and aspect error correction for esl learners using global context. In Association for Computational Linguistics: Short Papers", "author": ["Mamoru Komachi", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Tajiri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tajiri et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "types, such as article or preposition errors (Han et al., 2006; Rozovskaya and Roth, 2010).", "startOffset": 45, "endOffset": 90}, {"referenceID": 32, "context": "Our model consists of an encoder and a decoder (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 47, "endOffset": 89}, {"referenceID": 0, "context": "The decoder is also a recurrent neural network that uses a content-based attention mechanism (Bahdanau et al., 2014) to attend to the encoded representation and generate the output sentence one character at a time.", "startOffset": 93, "endOffset": 116}, {"referenceID": 1, "context": "The encoder maps the input sentence to a higher-level representation with a pyramidal bidirectional RNN architecture similar to that of Chan et al. (2015). The decoder is also a recurrent neural network that uses a content-based attention mechanism (Bahdanau et al.", "startOffset": 136, "endOffset": 155}, {"referenceID": 7, "context": "Second, wordlevel neural MT models with a fixed vocabulary are poorly suited to handle OOVs such as multidigit numbers, emoticons, and web addresses (Graves, 2013), though recent work has proposed workarounds for this problem (Luong et al.", "startOffset": 149, "endOffset": 163}, {"referenceID": 18, "context": "Second, wordlevel neural MT models with a fixed vocabulary are poorly suited to handle OOVs such as multidigit numbers, emoticons, and web addresses (Graves, 2013), though recent work has proposed workarounds for this problem (Luong et al., 2014).", "startOffset": 226, "endOffset": 246}, {"referenceID": 0, "context": "with the output of the final hidden layer d t then being used as part of the content-based attention mechanism similar to that proposed by Bahdanau et al. (2014):", "startOffset": 139, "endOffset": 162}, {"referenceID": 2, "context": "Thus we use a pyramid architecture, which reduces computational complexity (as shown by Chan et al. (2015)).", "startOffset": 88, "endOffset": 107}, {"referenceID": 11, "context": "To build and query the model, we use the KenLM toolkit (Heafield et al., 2013).", "startOffset": 55, "endOffset": 78}, {"referenceID": 9, "context": "Similar to Hannun et al. (2014), at step k, we rank the hypotheses on the beam using the score", "startOffset": 11, "endOffset": 32}, {"referenceID": 25, "context": "\u2022 embedding features: sum of 100 dimensional GloVe (Pennington et al., 2014) vectors of words in s and t, GloVe vectors of left and right context words in s.", "startOffset": 51, "endOffset": 76}, {"referenceID": 33, "context": "The first is the Lang-8 Corpus, which contains erroneous sentences and their corrected versions collected from a social language learner forum (Tajiri et al., 2012).", "startOffset": 143, "endOffset": 164}, {"referenceID": 30, "context": "We found that using dropout (Srivastava et al., 2014) at a rate of 0.", "startOffset": 28, "endOffset": 53}, {"referenceID": 26, "context": "15 on the non-recurrent connections (Pham et al., 2014) helped reduce perplexity.", "startOffset": 36, "endOffset": 55}, {"referenceID": 33, "context": "We use the train-test split provided by the Lang8 Corpus of Learner English (Tajiri et al., 2012), which contains 100K and 1K entries with about 550K and 5K parallel sentences, respectively.", "startOffset": 76, "endOffset": 97}, {"referenceID": 31, "context": "33 Susanto (2014) 53.", "startOffset": 3, "endOffset": 18}, {"referenceID": 23, "context": "Description For our second set of experiments we evaluate on the CoNLL 2014 Shared Task on Grammatical Error Correction (Ng et al., 2013; Ng et al., 2014).", "startOffset": 120, "endOffset": 154}, {"referenceID": 31, "context": "We compare to the top submissions in the 2014 Challenge as well as the method by Susanto (2014), which combines 3 of the weaker systems to achieve the state-of-the-art result.", "startOffset": 81, "endOffset": 96}, {"referenceID": 28, "context": "Following prior work, we additionally explore synthesizing additional sentences containing errors using the CoNLL 2014 training data (Felice and Yuan, 2014; Rozovskaya et al., 2012).", "startOffset": 133, "endOffset": 181}, {"referenceID": 20, "context": "To obtain sentence parses we use the Stanford CoreNLP Toolkit (Manning et al., 2014).", "startOffset": 62, "endOffset": 84}, {"referenceID": 26, "context": "Following prior work, we additionally explore synthesizing additional sentences containing errors using the CoNLL 2014 training data (Felice and Yuan, 2014; Rozovskaya et al., 2012). Our data augmentation procedure generates synthetic errors for two of the most common error types in the development set: article or determiner errors (ArtOrDet) and noun number errors (Nn). Similar to Felice and Yuan (2014), we first collect error distribution statistics from the CoNLL 2014 training data.", "startOffset": 157, "endOffset": 408}, {"referenceID": 31, "context": "On the CoNLL 2014 test set, which contains the full set of 28 error types, our method achieves a state-of-the-art result, beating all systems from the 2014 Challenge as well as a system combination method (Susanto, 2014).", "startOffset": 205, "endOffset": 220}, {"referenceID": 23, "context": "System descriptions for participating teams are given in Ng et al. (2014).", "startOffset": 57, "endOffset": 74}, {"referenceID": 26, "context": "The same phenomenon has been observed by Rozovskaya et al. (2012). Interestingly, the recall of other error types (see Ng et al.", "startOffset": 41, "endOffset": 66}, {"referenceID": 23, "context": "Interestingly, the recall of other error types (see Ng et al. (2014) for descriptions) decreases.", "startOffset": 52, "endOffset": 69}, {"referenceID": 32, "context": "Our work primarily builds on prior work on training encoder-decoder RNNs for machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 97, "endOffset": 171}, {"referenceID": 17, "context": "More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al., 2015; Maas et al., 2015; Chan et al., 2015).", "startOffset": 197, "endOffset": 254}, {"referenceID": 19, "context": "More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al., 2015; Maas et al., 2015; Chan et al., 2015).", "startOffset": 197, "endOffset": 254}, {"referenceID": 2, "context": "More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al., 2015; Maas et al., 2015; Chan et al., 2015).", "startOffset": 197, "endOffset": 254}, {"referenceID": 0, "context": "The attention mechanism, which allows the decoder network to copy parts of the source sentence and cope with long inputs, is based on the content-based attention mechanism introduced by Bahdanau et al. (2014), and the overall network architecture is based on that described by Chan et al.", "startOffset": 186, "endOffset": 209}, {"referenceID": 0, "context": "The attention mechanism, which allows the decoder network to copy parts of the source sentence and cope with long inputs, is based on the content-based attention mechanism introduced by Bahdanau et al. (2014), and the overall network architecture is based on that described by Chan et al. (2015). Our model is also inspired by character-level models as proposed by Graves (2013).", "startOffset": 186, "endOffset": 296}, {"referenceID": 0, "context": "The attention mechanism, which allows the decoder network to copy parts of the source sentence and cope with long inputs, is based on the content-based attention mechanism introduced by Bahdanau et al. (2014), and the overall network architecture is based on that described by Chan et al. (2015). Our model is also inspired by character-level models as proposed by Graves (2013). More recent work has applied character-level models to machine translation and speech recognition as well, suggesting that it may be applicable to many other tasks that involve the problem of OOVs (Ling et al.", "startOffset": 186, "endOffset": 379}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014).", "startOffset": 205, "endOffset": 266}, {"referenceID": 29, "context": "Other teams participating in the CoNLL 2014 Challenge used techniques ranging from rule-based systems to type-specific classifiers, as well as combinations of the two (Rozovskaya et al., 2014; Lee and Lee, 2014).", "startOffset": 167, "endOffset": 211}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). The work of Felice et al. (2014) achieved the best F0.", "startOffset": 206, "endOffset": 301}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). The work of Felice et al. (2014) achieved the best F0.5-score of 37.33 in that year\u2019s challenge using a combination of rule-based, language-model ranking, and statistical machine translation techniques. Many other teams used a language model for re-ranking hypotheses as well. Other teams participating in the CoNLL 2014 Challenge used techniques ranging from rule-based systems to type-specific classifiers, as well as combinations of the two (Rozovskaya et al., 2014; Lee and Lee, 2014). The rule-based systems often focus on only a subset of the error types. The previous state of the art was achieved by Susanto (2014) using the system combination method proposed by Heafield and Lavie (2010) to combine three weaker systems.", "startOffset": 206, "endOffset": 891}, {"referenceID": 5, "context": "Treating grammatical error correction as a statistical machine translation problem is an old idea; the method of mapping \u201cbad\u201d to \u201cgood\u201d sentences was used by many of the teams in the CoNLL 2014 Challenge (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). The work of Felice et al. (2014) achieved the best F0.5-score of 37.33 in that year\u2019s challenge using a combination of rule-based, language-model ranking, and statistical machine translation techniques. Many other teams used a language model for re-ranking hypotheses as well. Other teams participating in the CoNLL 2014 Challenge used techniques ranging from rule-based systems to type-specific classifiers, as well as combinations of the two (Rozovskaya et al., 2014; Lee and Lee, 2014). The rule-based systems often focus on only a subset of the error types. The previous state of the art was achieved by Susanto (2014) using the system combination method proposed by Heafield and Lavie (2010) to combine three weaker systems.", "startOffset": 206, "endOffset": 965}, {"referenceID": 21, "context": "Finally, our work uses data collected and shared through the generous efforts of the teams behind the CoNLL and Lang-8 datasets (Mizumoto et al., 2011; Mizumoto et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 128, "endOffset": 208}, {"referenceID": 22, "context": "Finally, our work uses data collected and shared through the generous efforts of the teams behind the CoNLL and Lang-8 datasets (Mizumoto et al., 2011; Mizumoto et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 128, "endOffset": 208}, {"referenceID": 23, "context": "Finally, our work uses data collected and shared through the generous efforts of the teams behind the CoNLL and Lang-8 datasets (Mizumoto et al., 2011; Mizumoto et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 128, "endOffset": 208}, {"referenceID": 28, "context": "Prior work has also proposed data augmentation for the language correction task (Felice and Yuan, 2014; Rozovskaya et al., 2012).", "startOffset": 80, "endOffset": 128}, {"referenceID": 1, "context": "We additionally thank the developers of Theano (Bergstra et al., 2010).", "startOffset": 47, "endOffset": 70}], "year": 2016, "abstractText": "Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art F0.5-score on the CoNLL 2014 Shared Task. We further illustrate that training the network on additional data with synthesized errors can improve performance.", "creator": "LaTeX with hyperref package"}}}