{"id": "1604.00126", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "abstract": "Traditional topic models do not taves account for refrigerator semantic regularities in bhang language. hellevoetsluis Recent distributional representations easterly of personification words behaviorism exhibit troubadours semantic consistency chikurubi over kayce directional metrics markleeville such morven as sz\u00e9ll cosine similarity. However, thurl neither categorical herodium nor 70s Gaussian observational martyr distributions waistlines used in existing topic schweiz-osterzgebirge models tsvangarai are appropriate smadi to kudarat leverage such correlations. uii In caetera this hohnstein paper, tinton we haise propose sawt to wakering use woodin the 106.25 von Mises - Fisher toadstools distribution balrog to kipkorir model amra the m-22 density 83.28 of jcp words muawiya over a unit vejar sphere. flutist Such a representation is retrenchment well - bojang suited jleicester for directional data. chanteau We rathenow use a bustros Hierarchical Dirichlet Process for our radislav base topic model floatable and ambalat propose cocktail an efficient inference gbs algorithm based 22-umit on Stochastic Variational anaphoras Inference. mugan This model forbach enables us 226.4 to a naturally obtrusive exploit botello the semantic structures girija of word interceded embeddings mankoff while flexibly admont discovering the qafl number akhtyrka of topics. nevena Experiments demonstrate nurcan that johnston our truehd method outperforms demilitarize competitive approaches in terms of topic coherence on cusa two different cani text smedes corpora while offering myla efficient inference.", "histories": [["v1", "Fri, 1 Apr 2016 04:36:58 GMT  (529kb,D)", "http://arxiv.org/abs/1604.00126v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG stat.ML", "authors": ["kayhan batmanghelich", "ardavan saeedi", "karthik narasimhan", "samuel gershman"], "accepted": true, "id": "1604.00126"}, "pdf": {"name": "1604.00126.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "authors": ["Kayhan Batmanghelich", "Ardavan Saeedi", "Karthik Narasimhan", "Sam Gershman"], "emails": ["kayhan@mit.edu", "ardavans@mit.edu", "karthikn@mit.edu", "gershman@fas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "Prior work on topic modeling has mostly involved the use of categorical likelihoods (Blei et al., 2003; Blei and Lafferty, 2006; Rosen-Zvi et al., 2004). Applications of topic models in the textual domain treat words as discrete observations, ignoring the semantics of the language. Recent developments in distributional representations of words (Mikolov et al., 2013; Pennington et al., 2014) have succeeded in capturing certain semantic regularities, but have not been explored exten-\n\u2217Authors contributed equally and listed alphabetically.\nsively in the context of topic modeling. In this paper, we propose a probabilistic topic model with a novel observational distribution that integrates well with directional similarity metrics.\nOne way to employ semantic similarity is to use the Euclidean distance between word vectors, which reduces to a Gaussian observational distribution for topic modeling (Das et al., 2015). The cosine distance between word embeddings is another popular choice and has been shown to be a good measure of semantic relatedness (Mikolov et al., 2013; Pennington et al., 2014). The von Mises-Fisher (vMF) distribution is well-suited to model such directional data (Dhillon and Sra, 2003; Banerjee et al., 2005) but has not been previously applied to topic models.\nIn this work, we use vMF as the observational distribution. Each word can be viewed as a point on a unit sphere with topics being canonical directions. More specifically, we use a Hierarchical Dirichlet Process (HDP) (Teh et al., 2006), a Bayesian nonparametric variant of Latent Dirichlet Allocation (LDA), to automatically infer the number of topics. We implement an efficient inference scheme based on Stochastic Variational Inference (SVI) (Hoffman et al., 2013).\nWe perform experiments on two different English text corpora: 20 NEWSGROUPS and NIPS and compare against two baselines - HDP and Gaussian LDA. Our model, spherical HDP (sHDP), outperforms all three systems on the measure of topic coherence. For instance, sHDP obtains gains over Gaussian LDA of 97.5% on the NIPS dataset and 65.5% on the 20 NEWSGROUPS dataset. Qualitative inspection reveals consistent topics produced by sHDP. We also empirically demonstrate that employing SVI leads to efficient topic inference.\nar X\niv :1\n60 4.\n00 12\n6v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\n6"}, {"heading": "2 Related Work", "text": "Topic modeling and word embeddings Das et al. (2015) proposed a topic model which uses a Gaussian distribution over word embeddings. By performing inference over the vector representations of the words, their model is encouraged to group words that are semantically similar, leading to more coherent topics. In contrast, we propose to utilize von Mises-Fisher (vMF) distributions which rely on the cosine similarity between the word vectors instead of euclidean distance.\nvMF in topic models The vMF distribution has been used to model directional data by placing points on a unit sphere (Dhillon and Sra, 2003). Reisinger et al. (2010) propose an admixture model that uses vMF to model documents represented as vector of normalized word frequencies. This does not account for word level semantic similarities. Unlike their method, we use vMF over word embeddings. In addition, our model is nonparametric.\nNonparametric topic models HDP and its variants have been successfully applied to topic modeling (Paisley et al., 2015; Blei, 2012; He et al., 2013); however, all these models assume a categorical likelihood in which the words are encoded as one-hot representation."}, {"heading": "3 Model", "text": "In this section, we describe the generative process for documents. Rather than one-hot representation of words, we employ normalized word embeddings (Mikolov et al., 2013) to capture semantic meanings of associated words. Word n from document d is represented by a normalized M - dimensional vector xdn and the similarity between words is quantified by the cosine of angle between the corresponding word vectors.\nOur model is based on the Hierarchical Dirichlet Process (HDP). The model assumes a collection of \u201ctopics\u201d that are shared across documents in the corpus. The topics are represented by the topic centers \u00b5k \u2208 RM . Since word vectors are normalized, the \u00b5k can be viewed as a direction on unit sphere. Von Mises\u2212Fisher (vMF) is a distribution that is commonly used to model directional data. The likelihood of the topic k for word xdn is:\nf(xdn;\u00b5k;\u03bak) = exp ( \u03bak\u00b5 T k xdn ) CM (\u03bak)\nwhere \u03bak is the concentration of the topic k, the CM (\u03bak) := \u03ba M/2\u22121 k / ( (2\u03c0)M/2IM/2\u22121(\u03bak) ) is the normalization constant, and I\u03bd(\u00b7) is the modified Bessel function of the first kind at order \u03bd. Interestingly, the log-likelihood of the vMF is proportional to \u00b5Tk xdn (up to a constant), which is equal to the cosine distance between two vectors. This distance metric is also used in Mikolov et al. (2013) to measure semantic proximity.\nWhen sampling a new document, a subset of topics determine the distribution over words. We let zdn denote the topic selected for the word n of document d. Hence, zdn is drawn from a categorical distribution: zdn \u223c Mult(\u03c0d), where \u03c0d is the proportion of topics for document d. We draw \u03c0d from a Dirichlet Process which enables us to estimate the the number of topics from the data. The generative process for the generation of new document is as follows:\n\u03b2 \u223c GEM(\u03b3) \u03c0d \u223c DP(\u03b1, \u03b2) \u03bak \u223c log-Normal(m,\u03c32) \u00b5k \u223c vMF(\u00b50, C0) zdn \u223c Mult(\u03c0d) xdn \u223c vMF(\u00b5k, \u03bak) where GEM(\u03b3) is the stick-breaking distribution with concentration parameter \u03b3, DP(\u03b1, \u03b2) is a Dirichlet process with concentration parameter \u03b1 and stick proportions \u03b2 (Teh et al., 2012). We use log-normal and vMF as hyper-prior distributions for the concentrations (\u03bak) and centers of the topics (\u00b5k) respectively. Figure 1 provides a graphical illustration of the model.\nStochastic variational inference In the rest of the paper, we use bold symbols to denote the variables of the same kind (e.g., xd = {xdn}n, z := {zdn}d,n). We employ stochastic variational mean-field inference (SVI) (Hoffman et al., 2013) to estimate the posterior distributions of the latent variables. SVI enables us to sequentially process batches of documents which makes it appropriate in large-scale settings.\nTo approximate the posterior distribution of the latent variables, the mean-field approach finds the optimal parameters of the fully factorizable q (i.e., q(z, \u03b2,\u03c0,\u00b5,\u03ba) := q(z)q(\u03b2)q(\u03c0)q(\u00b5)q(\u03ba)) by maximizing the Evidence Lower Bound (ELBO),\nL(q) = Eq [log p(X, z, \u03b2,\u03c0,\u00b5,\u03ba)]\u2212 Eq [log q]\nwhere Eq[\u00b7] is expectation with respect to q, p(X, z, \u03b2,\u03c0,\u00b5,\u03ba) is the joint likelihood of the model specified by the HDP model.\nThe variational distributions for z,\u03c0,\u00b5 have the following parametric forms,\nq(z) = Mult(z|\u03d5) q(\u03c0) = Dir(\u03c0|\u03b8) q(\u00b5) = vMF(\u00b5|\u03c8,\u03bb),\nwhere Dir denotes the Dirichlet distribution and \u03d5,\u03b8,\u03c8 and \u03bb are the parameters we need to optimize the ELBO. Similar to (Bryant and Sudderth, 2012), we view \u03b2 as a parameter; hence, q(\u03b2) = \u03b4\u03b2\u2217(\u03b2). The prior distribution \u03ba does not follow a conjugate distribution; hence, its posterior does not have a closed-form. Since \u03ba is only one dimensional variable, we use importance sampling to approximate its posterior. For a batch size of one (i.e., processing one document at time), the update equations for the parameters are:\n\u03d5dwk \u221d exp{Eq[log vMF(xdw|\u03c8k, \u03bbk)] + Eq[log \u03c0dk]}\n\u03b8dk \u2190 (1\u2212 \u03c1)\u03b8dk + \u03c1(\u03b1\u03b2k +D W\u2211 n=1 \u03c9wj\u03d5dwk) t\u2190 (1\u2212 \u03c1)t+ \u03c1s(xd, \u03d5dk) \u03c8 \u2190 t/\u2016t\u20162, \u03bb\u2190 \u2016t\u20162\nwhere D, \u03c9wj , W , \u03c1 are the total number of documents, number of word w in document j, the total number of words in the dictionary, and the step size, respectively. t is a natural parameter for vMF and s(xd, \u03d5dk) is a function computing the sufficient statistics of vMF distribution of the topic k.\nWe use numerical gradient ascent to optimize for \u03b2\u2217 (see Gopal and Yang (2014) for exact forms of Eq log[vMF(xdw|\u03c8k, \u03bbk)] and Eq[log \u03c0dk] )."}, {"heading": "4 Experiments", "text": "Setup We perform experiments on two different text corpora: 11266 documents from 20 NEWSGROUPS1 and 1566 documents from the NIPS corpus2. We utilize 50-dimensional word embeddings trained on text from Wikipedia using word2vec3. The vectors are post-processed to have unit `2- norm. We evaluate our model using the measure of topic coherence (Newman et al., 2010), which has been shown to effectively correlate with human judgement (Lau et al., 2014). For this, we compute the Pointwise Mutual Information (PMI) using a reference corpus of 300k documents from Wikipedia. The PMI is calculated using cooccurence statistics over pairs of words (ui, uj) in 20-word sliding windows:\nPMI(ui, uj) = log p(ui, uj)\np(ui) \u00b7 p(uj)\nWe compare our model with two baselines: HDP and the Gaussian LDA model . We ran G-LDA with various number of topics (k).\nResults Table 2 details the topic coherence averaged over all topics produced by each model. We observe that our sHDP model outperforms GLDA by 0.08 points on 20 NEWSGROUPS and by 0.17 points on the NIPS dataset. We can also see that the individual topics inferred by sHDP\n1http://qwone.com/\u02dcjason/20Newsgroups/ 2http://www.cs.nyu.edu/\u02dcroweis/data.\nhtml 3https://code.google.com/p/word2vec/\nmake sense qualitatively and have higher coherence scores than G-LDA (Table 1). This supports our hypothesis that using the vMF likelihood helps in producing more coherent topics. sHDP produces 16 topics for the 20 NEWSGROUPS and 92 topics on the NIPS dataset.\nFigure 2 shows a plot of normalized loglikelihood against the runtime of sHDP and G-\nLDA.4 We calculate the normalized value of loglikelihood by subtracting the minimum value from it and dividing it by the difference of maximum and minimum values. We can see that sHDP converges faster than G-LDA, requiring only around five iterations while G-LDA takes longer to converge."}, {"heading": "5 Conclusion", "text": "Classical topic models do not account for semantic regularities in language. Recently, distributional representations of words have emerged that exhibit semantic consistency over directional metrics like cosine similarity. Neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this work, we demonstrate the use of the von Mises-Fisher distribution to model words as points over a unit sphere. We use HDP as the base topic model and propose an efficient algorithm based on Stochastic Variational Inference. Our model naturally exploits the semantic structures of word embeddings while flexibly inferring the number of topics. We show that our method outperforms three competitive approaches in terms of topic coherence on two different datasets.\n4Our sHDP implementation is in Python and the G-LDA code is in Java."}], "references": [{"title": "Joydeep Ghosh", "author": ["Arindam Banerjee", "Inderjit S Dhillon"], "venue": "and Suvrit Sra.", "citeRegEx": "Banerjee et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Dynamic topic models", "author": ["Blei", "Lafferty2006] David M Blei", "John D Lafferty"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "2003", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u2013", "citeRegEx": "Blei et al.2003", "shortCiteRegEx": null, "year": 1022}, {"title": "Probabilistic topic models", "author": ["David M Blei"], "venue": "Communications of the ACM,", "citeRegEx": "Blei.,? \\Q2012\\E", "shortCiteRegEx": "Blei.", "year": 2012}, {"title": "Truly nonparametric online variational inference for hierarchical dirichlet processes", "author": ["Bryant", "Sudderth2012] Michael Bryant", "Erik B Sudderth"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bryant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bryant et al\\.", "year": 2012}, {"title": "Manzil Zaheer", "author": ["Rajarshi Das"], "venue": "and Chris Dyer.", "citeRegEx": "Das et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling data using directional distributions. Technical report, Technical Report TR03-06, Department of Computer Sciences, The University of Texas at Austin", "author": ["Dhillon", "Sra2003] Inderjit S Dhillon", "Suvrit Sra"], "venue": "URL ftp://ftp. cs. utexas", "citeRegEx": "Dhillon et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2003}, {"title": "Von mises-fisher clustering models", "author": ["Gopal", "Yang2014] Siddarth Gopal", "Yiming Yang"], "venue": null, "citeRegEx": "Gopal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopal et al\\.", "year": 2014}, {"title": "Wei Gao", "author": ["Yulan He", "Chenghua Lin"], "venue": "and Kam-Fai Wong.", "citeRegEx": "He et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Chong Wang", "author": ["Matthew D Hoffman", "David M Blei"], "venue": "and John Paisley.", "citeRegEx": "Hoffman et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "David Newman", "author": ["Jey Han Lau"], "venue": "and Timothy Baldwin.", "citeRegEx": "Lau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Greg S Corrado", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen"], "venue": "and Jeff Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Karl Grieser", "author": ["David Newman", "Jey Han Lau"], "venue": "and Timothy Baldwin.", "citeRegEx": "Newman et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "David M Blei", "author": ["John Paisley", "Chingyue Wang"], "venue": "and Michael I Jordan.", "citeRegEx": "Paisley et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Christopher D", "author": ["Jeffrey Pennington", "Richard Socher"], "venue": "Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bryan Silverthorn", "author": ["Joseph Reisinger", "Austin Waters"], "venue": "and Raymond J Mooney.", "citeRegEx": "Reisinger et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mark Steyvers", "author": ["Michal Rosen-Zvi", "Thomas Griffiths"], "venue": "and Padhraic Smyth.", "citeRegEx": "Rosen.Zvi et al.2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Matthew J Beal", "author": ["Yee Whye Teh", "Michael I Jordan"], "venue": "and David M Blei.", "citeRegEx": "Teh et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Matthew J Beal", "author": ["Yee Whye Teh", "Michael I Jordan"], "venue": "and David M Blei.", "citeRegEx": "Teh et al.2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.", "creator": "LaTeX with hyperref package"}}}